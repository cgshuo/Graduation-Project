 The paper introduces stability as a new measure of the recommender systems performance. In general, we define a recommendation algorithm to be  X  X table X  if its predictions for the same items are consistent over a period of time, assuming that any new ratings that have been submitted to the recommender system over the same period of time are in complete agreement with system X  X  prior predictions. In this paper, we advocate that stability should be a desire d property of recommendation algorithms, because unstable recommendations can lead to user confusion and, therefore, reduce trust in recommender systems. Furthermore, we empirically eval uate stability of several popular recommendation algorithms. Our results suggest that model-based recommendation techniques demonstrate higher stability than memory-based collaborative filtering heuristics. We also find that the stability measure for recommendation techniques is influenced by many factors, including the sparsity of the initial rating data, the number of new in coming ratings (representing the length of the time period over which the stability is being measured), the distribution of th e newly added rating values, and the rating normalization procedures employed by the recommendation algorithms. H.3.3 [ Information Search and Retrieval ]: Information Filtering, H.2.8.d [ Information Technology and Systems ]: Database Applications  X  Data Mining, I.2.6 [ Artificial Intelligence ]: Learning. Algorithms, Measurement, Performance, Reliability Evaluation of recommender systems, stability of recommendation algorithms, performance measures, collaborative filtering The objective of a recommender system typically is to recommend items that best fit user X  X  personal preferences. In one of the more common formulations of the recommendation problem, recommendation techniques aim to estimate ratings for items that have not yet been consumed by users, based on the known ratings users provided in the past. Recommender systems then recommend the items with highly predicted ratings. Personalized recommendations can a dd value to users X  experience in a variety of settings; as a result, recommender systems have become standard components in many e-commerce applications, such as the ones used by Amazon and Netflix. Much of the research in recommender systems literature has been focusing on enhancing the predictive accuracy of recommendation algorithms. However, we believe that there exist a number of other important aspects of reco mmender systems performance that have been largely overlooked in res earch literature. In particular, in this paper we explore the notion of the stability of recommendation algorithms, which can be illustrated by the following simple example. Suppos e that user Alice has entered the ratings 4, 2, 1 (on the scale from 1 to 5) into a recommender system for the three movies that she saw recently and, in return, the system provided for her the following rating predictions for four other movies: 4, 4, 5, and 1. Following the recommendations, Alice went and saw the first three of the movies (because they were highly predicted by the system), but avoided the fourth one (because it was predicted only as 1 out of 5). It turns out that, in Alice X  X  case, the system was extremely accurate for the three movies that were predicted highly, and Alice indeed liked them as 4, 4, and 5, respectively. Imagine Alice X  X  surprise when, after submitting these latest ratings (which were exactly the same as the system X  X  own predictions), she saw that the recommender system is now predicting the final movie for her (which earlier was 1 out of 5) as 5 out of 5. Figure 1 gives a simple numeric example illustrating that this scenario is possible in real-world recommender systems that use popular recommendation algorithms. Assume that we have five users and seven items, and the recommender system uses the simple variation of the traditional user-oriented collaborative filtering approach to make predictions. As Figure 1 indicates, the system initially knows A lice X  X  ratings only for items 1, 2, 3, but not for items 4, 5, 6 and 7. At th at time, the system indicates that users Bob and Carol are perfect neighbors for Alice, as both of them provided the same exact rati ngs to common items (i.e., items 1, 2, and 3). Therefore, Alice X  X  ratings on items 4-7 are predicted to be the average of Bob and Carol X  X  ratings on these items, i.e., Alice is predicted to rate items 4-7 as 4, 4, 5, and 1, respectively. Then, following our earlier example, let X  X  assume that Alice has consumed recommended items 4-6 and submitted ratings on these items that were exactly the same as the system X  X  predictions. With the new incoming ratings, th e system now finds the most similar users for Alice to be Dave and Eve, because they rated all common items exactly the same. Following the same user-based collaborative filtering approach, the prediction for Alice X  X  rating on item 7 is the average of Dave and Eve X  X  ratings on this item. Thus, the predicted rating for Ali ce on item 7 changes from 1 to 5 (from the least liked to most liked). For a regular user like Alice, who gave ratings exactly as predicted, such instability in rating predictions (i.e., such internal inconsistency of the recommendation algorithm) may seem odd and confusing. Meanwhile, it is well recognized both in academia and industry that inconsistent statements will be discredited by customers, regardless whether they are accura te or not. The downgrade in users X  confidence will directly reduce users X  perception of system X  X  personalizati on competence. Previous studies found that user X  X  trust and perception of personalization competence are the keys that lead to user X  X  accep tance of recommendations [10, 18, 19, 21]. Hence the instability of a recommender system will have negative impact on users X  acceptance and, therefore, harm the success of the system. Alice 4 2 1 ? ? ? ? Eve ? 2 ? 4 4 5 5 Alice 4 2 1 4 4 5 ? Eve ? 2 ? 4 4 5 5 In summary, this study emphasizes the importance of stability of recommender systems. In this paper, we investigate the stability properties of several famili es of popular recommendation algorithms and explore several factors and their impact on stability. In the machine learning literature, the stability of a predictive algorithm is the degree to which it generates repeatable results, given different sub-samples of the entire dataset [20]. Intuitively, an algorithm is stable if it induc es approximately the same model from two samples with the same probability distribution. Turney X  X  work [20] focused on the stability of binary classifiers (in particular, decision trees), and he described a method to quantify stability based on the expected agreement between predictions from two different samples. This notion of stability is hence closely associated with the representativeness of the random sub-samples of original da ta (i.e., sampling effects) as well as the strength of the underl ying patterns in the original dataset. Also, this notion of stab ility focuses on fixed datasets and does not account for the dynamics of the data, more specifically, for the impact of the model X  X  predictions on its future performance. In the recommender systems literature, attack detection represents a research area that has some connection to the issue of recommendation stability. In particular, since recommender systems depend heavily on input from users, they are subject to manipulations and attacks [13-17]. In this literature, the vulnerabilities of recommender system s are studied in terms of the robustness of such systems in the face of malicious attacks and are evaluated via two metrics: robustness and stability . Robustness measures the performance of the system before and after an attack (e.g., difference in predictive a ccuracy). Stability looks at the shift in system X  X  predictions for the attacked items. Stability is often measured as the average prediction shift , which is a metric that is similar to the one used in this paper. However, in the recommendation system attack detection literature, stability of a recommender system measures the dynamics in system X  X  predictions when facing attacks from outside. In this paper, we measure a very different aspect of  X  X tability X  (which can also be thought of as  X  X nternal consiste ncy X ) which represents not a consequence of an external force that attempts to manipulate the recommender system for some purpose, but rather an inherent property of the recommendation algorithm. We define the predictive model to be stable if its predictions for the same items are consistent ove r a period of time, assuming that any new ratings that have been submitted to the recommender system over that period are in complete agreement with system X  X  prior predictions. Hence, quantifying stability of the predictive model involves multiple time periods to compare predictions. In this study, we adopt a two-phase approach to compute the stability of a recommender algorithm, which is illustrated in Figure 2. In phase 1, given a set of known ratings R 1 , a predictive model is built using R 1 , and predictions for all unknown ratings are made and denoted as P 1 , where P 1 ( u , i ) represents a system-predicted rating for user u and item i . Then, a set of hypothetical incoming ratings is added to the original set of known ratings R assuming that the recommendation system was highly accurate and was able to make predictions P 1 that are identical to users X  true preferences. Therefore, in phase 2, some subset S of predictions P 1 is added as the newly incoming known ratings. Thus, in phase 2 the set of known ratings becomes R and the set of unknown ratings becomes P 2 = P 1 \ S . Based on R a second predictive model is bu ilt using the same recommendation algorithm, and predictions on unknown ratings P 2 are made. Stability is then measured by comparing the two predictions, i.e., P and P 2 , to compute their mean absolute difference or root mean squared difference, which we call mean absolute shift (MAS) or root mean squared shift (RMSS), respectively. More prediction difference is denoted by: Then the above mentioned stabilit y measures are computed as: MAS =  X  X  To illustrate the intuition behind the stability computation, consider an example data set co ntaining 10 data points as shown in Figure 3. In this example, given these 10 data points, we are trying to build predictive models to predict f ( x ) for the entire space, i.e., for any x  X  R . Figures 3a, 3b, and 3c represent model-based techniques (represented by 0-, 1-, and 2-degree polynomials, respectively) that fine tune their parameters using global optimization to fit the data in a way that minimizes sum of squared errors. For example, Figur e 3a represents the best model of the form f ( x ) = b 0 , where b 0 is the simple average of f ( x ) values for the 10 data points; similarly Figures 3b and 3c represent the best models of the forms f ( x ) = b 0 + b 1 x and f ( x ) = b respectively. Let X  X  assume that these models have perfect accuracy and any newly incoming data points will be in agreement with these models, i. e., any new data points would appear on the fitted curve. Note that re-estimating the above three models using this type of addition al data (along with the original 10 data points) will not change any of the underlying model parameters. Therefore, model-ba sed approaches that are based on the sum-of-squared-errors minimi zation represent one illustrative example of stable predictive models. In contrast, Figure 3d shows the predictions made by the k nearest neighbor heuristic, where k is set to 3, given the same set of 10 average of the three nearest neighbors of x , and the resulting model is indicated by lines. Figure 3e further illustrates original data as well as several hypothetical new incoming data points that are consistent with the model show n in Figure 3d. Finally, the 3 nearest neighbor algorithm is used again to re-estimate f ( x ) after incorporating the new data (i.e., using all data points from Figure 3e), and Figure 3f provides the comparison between predictions based on the original data (thin line) and predictions based on the original data plus additional data (thick line). The graph clearly shows that predictions made using neighborhood heuristics at different stages differ from each other, even though newly added data points perfectly agree with prior predictions. Predictive techniques for recommender systems can be classified into two broad families: model-based and memory-based techniques. Model-based learners (e.g., matrix factorization, regression analysis algorithms) typically try to fit the data into a predictive model from a pre-defi ned category (e.g., linear regression, decision tree) using some optimization approach for error minimization (e.g., sum of squared error minimization). As we saw from the above example, one may expect that such learners may have better stability than memory-based heuristics (such as neighborhood-based CF algorithms), which do not have an underlying global optimization-based model and the learning process is purely data driven (which may make them more sensitive to new data additions, even when they are consistent with prior predictions). We inves tigate this issue in this paper. We used the publicly available MovieLens 100K dataset [7] to test the stability of several popular recommendation algorithms. The dataset consists of 100,000 ratings for 1682 movies by 943 users (i.e., data density is 6.3%). All ratings are integer values between 1 and 5, where 1 represen ts the least liked movies and 5 represent the most liked movies. We tested the stability of recommendation algorithms in a variety of settings. 
Step 1. Train the recommendation algorithm based on the 
Step 2. Select and add a subset of the predicted ratings as 
Step 3. Re-train the algorithm based on the new data, and 
Step 4. Compare predictions from Steps 1 and 3 and Our experiment followed the ge neral process of stability computation described in Sectio n 3. Figure 4 provides a high-level summary of the main steps in each of our experiments. Please note that, in order to obtain robust empirical results, we ran the experiments 5 times for each condition and report the average stability measure of the 5 runs. In this study, we test the stability of six popular recommendation techniques, including simple user-and item-based averages, user-and item-based variations of neighborhood-based collaborative filtering approaches, and the model-based matrix factorization method. Table 1 summarizes the techniques used in the paper. Methodology Description 
Item Average (Item_Avg) 
User Average (User_Avg) User-Item 
Average (User_Item_Avg) 
Item-based CF (CF_Item) 
User-based CF (CF_User) Matrix 
Factorization (SVD) As often suggested in research lit erature, it is useful to normalize rating data by eliminating user a nd item effects be fore applying any prediction technique [1, 2, 8]. Therefore, recommendation algorithms often involve a pre-processing step to remove  X  X lobal effects X . For example, some users may systematically tend to give higher ratings than others, and some universally liked items might receive higher ratings than others. Without normalization, such user and item effects could bias system X  X  predictions. One common practice of normalization sugge sted in the literature is to estimate and remove three effects in sequence, i.e., overall mean, main effect for items, and main e ffect for users, and then make predictions based on the residuals [2, 3]. In all experiments below, except where explicitly indicated to the contrary, the known ratings were normalized by removing global effects, including overall mean a nd the main effects of users and items. Specifically, a baseline estimate for each known rating denoted by b ui is computed to account fo r global effects, i.e., where  X  is the overall average rating, b u is the observed deviations processing step estimates b ui in order to remove it from original rating R ui when making predictions. This section summarizes our expe rimental results which explore the stability of several recomme ndation algorithms (and some of their variations) in different settings. We ran experiments with six recommendation algorithms described earlier on the MovieLens 100K dataset. This dataset has 1682 movies and 943 users. Thus, the total number of possible ratings is about 1.6M , of which 100,000 are known and are provided in this dataset. For our initial stability calculations, we used these 100K ratings as the input to the implemented recommendation algorithms in order to predict the remaining 1.5M unknown ratings. From these predicted ratings, we drew a random sample of 100K ratings and treated them as new incoming ratings, and the remaining 1.4M rati ngs were used to calculate the prediction shift, as described in Section 3. The predictive accuracy of the six recommendation techniques was estimated using 5-fold cross validation on the 100K known ratings and measured using standard accuracy measures of mean absolute error (MAE) and root mean squared error (RMSE) [9]. Experimental results are summarized in Table 2, where, for the sake of completeness, we provide accuracy and stability numbers both as measured by the mean absolute error and shift (MAE and MAS) as well as the root mean squared error and shift (RMSE and RMSS). However, we note that bot h sets of measures were highly consistent with each other throughout our experiments, therefore, we will use only RMSE and RMSS measures in the remainder of the paper. Also, Figure 5 provides a visual performance illustration of all algorithms as the accuracy-stability plot . Table 2 . Accuracy and Stability of Recommendation Algorithms 
Recommendation algorithm SVD 0.94 0.74 0.11 0.08 CF_Item 0.94 0.73 0.25 0.17 CF_User 0.95 0.74 0.37 0.26 User_Item_Avg 0.98 0.80 0.11 0.09 Item_Avg 1.02 0.81 0.00 0.00 User_Avg 1.04 0.84 0.00 0.00 Figure 5 . Accuracy-Stability Plot of Popular Recommenders Predictably, more sophisticated recommendation algorithms (such as SVD, CF_Item, and CF_User) demonstrated better predictive accuracy than much simpler average-based techniques. In terms of stability, as expected, both user-based and item-based averages had prediction shift values of zero , i.e., adding new ratings that are identical to the current item (or user) average does not change the item (or user) average, thus, leaving the original predictions Furthermore, while the SVD, CF _Item, and CF_User techniques were comparable in terms of their predictive accuracy, the model-based matrix factorization (SVD) technique, which is based on the global optimization appr oach, demonstrated higher stability than the memory-based neighborhood techniques (CF_Item and CF_User) that are based on the  X  X ocal X , nearest neighbor heuristics. Data sparsity is often cited as one of the reasons for inadequate recommender systems accuracy. In this section we perform an empirical investigation on whether data sparsity/density has an impact on the stability as well, by observing the stability and accuracy of recommendation algorithms at different data density levels. Density of original M ovieLens 100K dataset is 6.3%, and to manipulate this density leve l, we draw random samples from the original dataset that contained 30%, 40%, 50%, ..., 90% of ratings, thus, obtaining rating data sets ranging from 1.9% to 5.7% in density. Also, because only subsets of actual ratings in the dataset were used for training, we were able to use the rest of the data as part of the predic tive accuracy evaluation. Figure 6(a) shows the comparison of accuracy of recommendation algorithms with varied levels of data density. Our results are consistent with prior literature in that the recommendation algorithms typically demonstrate higher predictive accuracy on denser rating datasets. More im portantly, Figure 6(b) shows that the density level of the ratings data also has a significant influence on recommendation stability  X  in ge neral, RMS shift increases as the ratings data becomes sparser. This is especially true for the neighborhood-based CF techniques (CF_Item and CF_User). In the context of sparser data, these techniques are increasingly more sensitive to additions of new ratings (even though they are in complete agreement with the al gorithms X  own predictions), which leads to higher variance in rating predictions to the same items over time. In particular, in our sparsest dataset (1.9% rating density), both CF_User and CF _Item demonstrate a very significant 0.5 RMS shift (on th e rating scale from 1 to 5). Meanwhile, the stability performance of the model-based matrix factorization (SVD) approach is much more advantageous and more consistent across different de nsity levels, always staying in the 0.1-0.15 RMS shift range. In order to further understand the dynamics of recommendation stability, in the next set of expe riments we varied the number of new incoming ratings (that are in complete agreement with the system X  X  prior predictions). Note that these experiments can be thought of as simulating the length of the time period over which the stability is being measured. More and more ratings are made available to the system as time passes by, t hus, longer time period is associated with more incoming ratings. The number of new ratings (to be added to the existing set of 100K known ratings) ranged from 1K to 500K ratings, a nd these ratings were randomly drawn from all the available 1.5M predicted ratings. Figure 7 shows the dynamics of recommenda tion stability (as measured by RMS shift) for different numbers of new ratings. Figure 7 . Stability of Recommendation Techniques for Different In particular, as before, we find that the prediction shift for the simple user-and item-average approaches is always zero, regardless of how many predictions were added to rating matrix. For all other techniques, including the matrix factorization (SVD), user-and item-based collaborative filtering (CF_User and CF_Item), and baseline estimates using user and item averages (User_Item_Avg), the prediction shift curves indicate a convex shape. In particular, with only very few newly introduced ratings the prediction shift is small, but the instability rises very rapidly until the number of newly introduced ratings reaches about 20% of the original data, at which point the rise of the prediction shift slows down and later starts exhib iting a slow continuous decrease. Our experiments suggest that th e stability of memory-based neighborhood techniques is more sensitive to the number of new (a) (b) incoming ratings than the stability of model-based techniques that are based on global optimization approaches (including both matrix factorization and simpler average-based models). When only a very small number of new ra tings is added, the original rating patterns persist in the data and the neighborhood of each user (or item) does not change dr amatically, resulting in similar predictions for the same items (i.e., higher stability) . In contrast, when more and more new ratings are made available, the neighborhoods can be affected very dramatically. This is supported by the observed more rapid decrease of stability in neighborhood-based techniques as compared to the model-based approaches, which are based on gl obal optimization and, thus, are less malleable to additions of new ratings (that are in agreement of what the algorithm had predicted earlier). As also observed in earlier experiments, the item-based CF technique in this application domain exhibi ts more stability than the user-based CF. In addition, it is important to note that, after the initial increase in prediction shift, its subseq uent slow decrease (for all recommendation algorithms) can be attributed to the sheer numbers of additional new ratings that are introduced, all of which are in agreement with the initial recommendation model. Whether the recommendation algorith m is more computationally sophisticated (e.g., SVD) or less (e .g., CF_User), providing it with increasingly more data that is in consistent agreement with some specific model will make this algorithm learn this model better and make more accurate and more stable recommendations. We also varied the sampling strategy that was used to select the new incoming ratings. The objective was to test the impact of the incoming rating distribution on the stability of recommendation algorithms. We applied five differ ent sampling strategies to draw samples of the same size (i.e., 100K) as the original MovieLens 100K data from all the available 1. 5M system X  X  pr edicted ratings: Random, High, HighHalf, Low, and LowHalf. Table 3 . Summary of Sampling Strategies to Select New Ratings Strategy Details 
Random Randomly add a sample of predicted ratings to 
High Add highest predicted ratings (e.g., 5) to existing 
HighHalf Add a random sample of predicted ratings with 
Low Add lowest predicted ratings (e.g., 1) to existing 
LowHalf Add a random sample of predicted ratings with Random strategy draws a sample of predictions from each user at random to be added as new incoming ratings to the set of original ratings (i.e., corresponding to the situation where the users will choose to watch new movies at random); High strategy sorts all predictions for each user and onl y chooses those with highest predicted rating values (i.e., the users will exactly follow the recommendation algorithm to choose movies); HighHalf sorts predictions for each user and then draws a random sample of predictions with values greater than the median prediction (i.e., users will follow the recommender algorithm to separate good movies from the bad, but then will choose among the good movies at random); Low sorts predictions for each user and only adds lowest predictions (i.e., opposit e of High; included here for completeness); and LowHalf sorts predictions and draws a random sample from ratings whose values are lower than the median prediction (i.e., opposite of HighHa lf; included for completeness). The five strategies are summarized in Table 3. In summary, we wanted to test whether adding sk ewed rating samples (e.g., high ratings only or low ratings only) will bias the original rating distribution and, as a result, decrease recommendation algorithm stability, as compared to the random samples of new predictions. Comparison of recommendation algorithms in terms of their stability for different incoming rating distributions is presented in Figure 8. As can be seen from the figure, the distribution of new incoming ratings significantly influences the stability of recommendation algorithms, except for simple user-and item-based average techniques, which are always perfectly stable, as discussed earlier. In partic ular, among the five sampling strategies, Random strategy demonstrated the highest stability for nearly all recommendation algorithms. Moreover, all recommendation algorithms exhibited an increase in instability with High or Low sampling strategies, as compared to the Random strategy, and this differe nce was especially substantial for memory-based neighborhood CF techniques (CF_User and CF_Item). For example, the prediction shift of user-based CF method rises from about 0.4 with Random strategy all the way to about 0.8 with Low strategy (on th e scale of length 4, i.e., from 1 to 5). Furthermore, adding new ratings selected by HighHalf and LowHalf strategies led to moderate prediction shift for all algorithms. In summary, Random rating samples that are in complete agreement with previous predictions have more favorable impact on stability than samples with skewed distribution. Figure 8 . Stability of Recommendation Techniques with Varied Our results also suggest that the impact of adding skewed samples of new ratings on recommendation stability can be asymmetric for some algorithms, e.g., User_Item_A vg had the best stability using HighHalf strategy. One explanati on for this phenomenon is that the original ratings data is skew ed toward high ratings, i.e., mean of ratings in the original data is 3.53. Because of this, among the five sampling strategies, the HighHalf strategy used with User_Item_Avg algorithm provided the ratings that turned out to have the distribution that was the most similar to the distribution of the original data and, ther efore, introduced least prediction shift. This is illustrated in Figure 9, which presents the difference between rating means in the original data and the data resulting from all five sampling strategies for the User_Item_Avg technique. In all previous experiments, befo re applying collaborative filtering approaches or matrix factorization method, the rating data was normalized by removing  X  X lobal effects X  (including the overall mean, user average, and item average) in the way that is often used in recommender systems literature [2]. In this section, we investigated the impact of normalization on the stability of recommendation algorithms. Figure 10 summarizes our findings. In particular, we implemented two versions of matrix factorization (SVD) algorithm, with normalizati on (i.e., removing the baseline) and without. For the two neighborhood-based approaches (CF_User and CF_Item), we implemented three versions of each: with normalization by removing the baseline, with normalization by removing the user average for CF_User or item average for CF_Item, and without normalization. Simple average heuristics essentially represent the normalizat ion procedures and, thus, we only have a single version of each. 
Figure 10 . Stability of Recommendation Techniques with and The results suggest that normaliz ing rating data in general can improve predictive accuracy fo r all recommendation algorithms. Meanwhile, normalization also impacts the stability of these algorithms in different ways. For example, normalizing data by removing baseline estimate slightly reduced the stability of matrix factorization algorithm compared to no normalization. However, normalization by removing only it em average or user average dramatically improved both accuracy and stability of neighborhood-based approaches as compared to their non-normalized versions. On top of this, removing all the global effects (i.e., baseline estimate) resulted in a comparable accuracy and only very slight stability im provement compared to removing only one main effect (i.e., user average or item average). All previous experiments were ba sed on Movielens 100K dataset. Because this dataset is relatively small and dense, in this section we test the stability of recommendation algorithms on two larger and sparser datasets to test the generalizability of our findings. The first dataset we used is a sample from the Movielens 1M dataset. The original Movielen s 1M dataset consists of 1,000,000 ratings for 6040 movies by 3952 us ers (4.2% data density) [7]. All ratings are integer values betw een 1 and 5. From this dataset we sub-sampled a dataset of 3000 users and 3000 movies with 153,938 known ratings (i.e., 1.71% data density). The second dataset that we used is sampled from Netflix 100M dataset [5]. Similar to previous approach, we sub-sampled the dataset of 3000 users and 3000 movies. The resu lt data sample consists of 108,631 known ratings (i.e., 1.21% data density). Both datasets described above are larger in both user and item dimensions and much sparser compared to the Movielens 100K dataset. Figure 11 shows that the stability patterns obtained using these larger datasets are essentially the same as the ones presented earlier (e.g., in Figure 7, Section 5.4). More specifically, the more skewed the distribution of new in coming ratings is, the less stable the recommendation algorithms become. Also, similar to what was discussed in previous sect ion, model-based approaches (including matrix factorization and simple average estimates) are substantially more stable than neighborhood-based heuristics, and this difference is even more pronounced in larger, sparser datasets. In this paper, we introduced and investigated the notion of stability of recommendation algorithms. Stability measures the extent to which recommendation algorithms provide consistent predictions over time, assuming that new incoming ratings available to system are in complete agreement with system X  X  prior predictions. We believe that the stability should be a desired property for recommender syst ems designers, because it represents the internal consis tency of a recommendation algorithm (as measured by the impact of algorithm X  X  predictions on its own future performance). We advocate that stability is another important dimension to measur e performance of recommendation algorithms; providing unstable, contradictory predictions could negatively affect the users X  confidence and trust in the system. The results of our experiments s how that model-based techniques (e.g., matrix factorization, user average, item average, and baseline estimates using both user and item average) are more consistent in making predictions than memory-based collaborative filtering heuristics. We also found that normalizing rating data before applying any algorithms not only improves accuracy for all recommendation algorithms, but also plays a critical role that affects the stability of recommenda tion algorithms. Moreover, the sparsity of original rating data, the number of new incoming ratings, and the distribution of incoming ratings all have substantial impact on the stability of the system. In summary, we believe that addressing the issue of stability of recommendation algorithms dese rves our attention and exploration in both research and pr actice. We believe that this paper represents just the first step in studying stability-related issues in recommender systems and that significant additional work is needed to explore this issue in a more comprehensive manner. This work is supported in part by the National Science Foundation grant IIS-0546443. [1] Adomavicius, G. and A. Tuzhilin. Toward the Next [2] Bell, R.M. and Y. Koren. Scalable Collaborative Filtering [3] Bell, R.M. and Y. Koren. Improved Neighborhood-based [4] Bell, R.M. and Y. Koren. Lessons from the Netflix prize [5] Bennet, J. and S. Lanning. The Netflix Prize. In Proceeding [6] Funk, S. Netflix Update: Try This at Home . Netflix Update: [7] Grouplens. Movielens Data Sets. 2006. [8] Herlocker, J., J. Kostan, A. Borchers, and J. Riedl. An [9] Herlocker, J., J. Konstan, K. Terveen, and J. Riedl. [10] Komiak, S. and I. Benbasat. The effects of personalization [11] Koren, Y., R. Bell, and C. Volinsky. Matrix Factorization [12] Kostan, J., B. Miller, D. Maltz, J. Herlocker, L. Gordon, and [13] Lam, S. and J. Riedl. Shilling Recommender Systems for [14] Massa, P. and B. Bhattacharje e. Using trust in recommender [15] Mobasher, B., R. Burke, a nd J.J. Sandvig. Model-Based [16] Mobasher, B., R. Burke, C. Williams, and R. Bhaumik. [17] Mobasher, B., R. Burke, R. Bhaumik, and C. Williams. [18] O'Donovan, J. and B. Smyth. Mining trust values from [19] O X  X onovan, J. and B. Smyth. Trust in recommender systems. [20] Turney, P. Technical Note: Bias and the Quantification of [21] Wang, W. and I. Benbasat. Trust in and adoption of online 
