 term occurs more than once, stores only one mapping from term to document instead; (ii) its primitives are composed of docids, document frequencies and positions, these elements can be stored separately or combined arbitrarily; (iii) the posting lists of query terms can be processed in parallel to accelerate the procedure, also, various list intersection and skipping algorithms have been implemented to reach early termination [ 1  X  3 ].
 it can be hardly fi t into the main memory, thus compression is needed to save space and reduce the number of disk access. Different ordering schemes and traversal strategies have a large impact on the performance of query response. Also, the inverted index is inferior in searching for substrings and in languages which the terms are not as discrete in English. All these issues have forced variant implementations of inverted index. Different orderings in the lists of documents associated with a term, and different auxiliary information, fi t widely different IR tasks. Index designers have to choose the right order for one such task, rendering the index dif fi cult to use for others. Dual-Sorted Index [ 7 , 8 ] are two successful works that have been continuously studied by researchers. Block-Max Index fi rst partitions the sequence of each term into blocks of fi xed size(say, 64 or 128 elements), and compresses each block independently with faster list-oriented encoders like simple-X or PFD [ 9 ]; then stores the maximum impact value and the head docid for each block in uncompressed form, enabling to skip large parts of the lists. It is simple with little space occupation, but leads to considerable performance gains in conjunctive query and DAAT style pruning approaches.
 Dual-Sorted Index is a variant of inverted index using wavelet tree, a balanced binary tree-like compact data structure. The wavelet tree can store a sequence (e.g., the posting list) from a symbol universe (e.g., the docid) within asymptotically the same space required by a plain representation of the sequence [ 10  X  12 ]. Dual-Sorted Index allows combining an ordering by decreasing term frequency with an ordering by increasing docid, more importantly, it supports not only typical query scheme, but also sophis-ticated operations in pattern matching. While researchers keep improving both tech-niques continuously, missing from the literature is a study that thoroughly measures properties and performances of these two indexes. In this paper, we provide a com-prehensive comparison and analysis of the space occupation and response ef fi ciency for different query schemes of these two indexes, using an open source search engine platform-Terrier [ 13 ].
 processing strategies and wavelet trees; Sect. 3 summarizes both Block-Max Index and Dual-Sorted Index; Sect. 4 describes our experimental setup and comparison results; Conclusions and future work follow in Sect. 5 . 2.1 Query Processing Strategies Given a query, the most basic processing form is called Boolean query processing , which intersects or merges posting lists of query terms according to their logical pointers of other lists are aligned to it, scoring is executed incrementally from top to bottom, once we fi nd current document fail to pass the threshold, scoring is aborted and another docid is picked until essential lists reach their ends. 2.2 Wavelet Tree Recent years, several researchers have been making efforts to bring the compact data structure to bear on the problems in IR, in particular ranked document retrieval. Brisaboa et al. [ 18 ] present an encoding scheme called Directly Addressable Codes (DACs), which enables direct access to any element of the encoded sequences without the need of sampling method, but the symbols must be stored in complete form, making it hard to compress; Culpepper et al. [ 19 , 20 ] adopt the HSV data structure in combination with a wavelet tree-base representation to retrieve top-k results, but it is not safe; Petri et al. [ 21 ] describe a hybrid index consisting of a pruned suf fi xof document-level posting lists to ef fi ciently handle large intervals, and fast sequential exhaustive processing of smaller sections to create document-level lists on-the-fl y. Although quite different in their details, the common vision of these work is to use breakthroughs in compressed pattern matching as an ef fi cient algorithmic base on which the more sophisticated operations required by IR systems can be built. alphabet R 0 r  X  , within a space requirement of nlog r 1  X  o 1  X  X   X  X  bits. Figure 2 gives an example of the structure of wavelet tree. The tree is a complete balanced binary tree, where each node handles a range of symbols. The root handles 0  X  r  X  , its children nodes bisect the range recursively until reach the leaves which only hold a single symbol. The detail procedure is as follows: each node v in the tree handling the range [  X  and B v [ i ] = 1 otherwise. Then the alphabet interval breaks into two roughly equal current list that is equal to or greater than a given one and it usually involves a block decompression; shallow pointer movement only moves the current pointer to the cor-responding block without decompression. BMW picks a candidate pivot using the list maxscores as in WAND, but then it uses shallow movement to check if it is necessary to decompress the block and evaluate the pivot based on the maxscores of the block, which helps fi ltering out most of the candidates and achieving much faster query response. Also, if a pivot d fails to make it into the top results, instead of picking the docid next to it, the pointer is moved to the header of the next block, since d is ruled out based on the maxscore of current block. Different from BMW, BMM uses a prepro-cessing step rather than an online one to detect and align block boundaries. The blocks are repartitioned into intervals with interval boundary, then BMM runs Maxscore within each interval, selecting non-essential lists and doing partial scoring using block maxscores instead of list maxscores.
 such as PageRank and the IR score into one score to give a correct and better estimation of document  X  s upper bound score, they also recalibrate candidate documents using local BMW and BMM to omit invalid scoring. Dimopoulos et al. [ 6 ] compare per-formances of WAND-and Maxscore-based algorithms with and without BMI, then build on their observations by designing and implementing new techniques for exploiting BMI, in particular docid-oriented block selection schemes, on-the-fl y gen-eration of BMI, and a new recursive query processing algorithm that uses a hierarchical partitioning of inverted lists into blocks. The core idea of their improvement is to decouple the choice of blocks for storing block maxscores from the choice of blocks for inverted index compression, making the BMI as a structure separate from the inverted lists. In some cases, much smaller blocks are chosen to get better pruning power, however, this also results in a much larger space occupation. To solve the problem, they have de fi ned block boundaries based not on the number of postings in a block, but based on docid space and carefully tuned the block size to achieve good space-time tradeoffs, moreover, on-the-fl y Block-Max generation is proposed to further shrink the size of BMI with little time overhead. 3.2 Dual-Sorted Index The main data structure used in Dual-Sorted Index (DSI) is wavelet tree. To make it suitable for document retrieval, symbols of letters are substituted by docids in the range [1, D ], where D denotes the total number of documents in the collection. Let L t [1, df t ] be the list of docids in which term t appears, in decreasing tf order. Let N =  X  t df t be the total number of occurrences of distinct terms in the documents. All the posting lists L t are concatenated into a unique list L [1, N ], and the starting position s t of list L t within L is also stored. The sequence L of docids is then represented with a wavelet tree. Note that the structure is a complete balanced binary tree with D leaves. The leaves are labeled left-to-right with the symbols [1, D ] in increasing order. For any internal node v of the wavelet tree, let L v be a subsequence of L containing only the docids on the leaves in the subtree with root v . At each node v of depth  X  , the docids are split by their DAAT and TAAT traversals with different weighting models, we will show the result that although inferior to the two state-of-the-art indexes, it does gain performance achievement after carefully tuned.
 million web pages and about 32.8 million terms in the vocabulary crawled from the gov Internet domain. The uncompressed size of these web pages is 426 GB. The collection is indexed using Terrier IR platform, all terms have the Porter stemmer applied, and stopwords have been removed. The docids are assigned by the sequence of their occurrence.
 track queries and classi fi ed into different categories according to the number of distinct terms in the query. Unless stated otherwise, the default number of documents retrieved for each query equals to 20 that a result page will contain and BM25 is used as ranking function in order to keep consistent with work in [ 4 , 6 ].
 running at 2.40 GHz with 128 GB of RAM and 12,288 KB of cache. The default physical block size is 16 KB, unless stated otherwise algorithms are implemented using C ++ and compiled with GCC 4.8.1 with  X  O3 optimizations. In all our runs, the whole inverted index is completely loaded into main memory, in order to warm up the execution environment, each query set is run 4 times for each experiment, and the response times only measured for the last run. Our implementations are available at https://github.com/Sparklexs/Dualsorted-master . 4.2 Index Size Comparison First we compare the index size of each structure. The baseline is compressed using both Gamma and OptPFD. As depicted in [ 4 ], BMI compresses docid-gaps and fre-quencies using OptPFD, compared with baseline OptPFD, BMI barely expands its size, as it only augments the index with local maxscores and pointers to the header of blocks. For DSI, its docids and frequencies are stored separately, the frequency lists are compressed using Gamma codec, however, the docid lists are hard to compress since they are stored in wavelet trees in primitive form. Moreover, in order to achieve direct access to any symbol in the tree, some additional structures are also stored, all these result in the size of DSI grows nearly 4 times larger than the rest indexes. Table 1 gives the detail of each index. We also show the results after docid reassignment for the collection. The idea of docid reassignment is to reorder the documents in the collection so that similar documents are clustered together, thus shrinking the gap between docids and the index size, it also improves the speed of dynamic pruning for that both potential and invalid documents are batched up to reach an early termination, here we choose to reorder the documents based on an alphabetic sorting of their URLs. As the right hand side of Table 1 shows, baseline and BMI reduce nearly 20 % size after docid reas-signment, however, DSI seems insensitive to reassignment as it hardly changes size in both situations, this is mainly due to the fact that DSI separately stores docids in their primitive form and frequencies in decreasing order without relying on docid order. Also note that BMI cuts out the same size as baseline OptPFD, which can be explained concatenation operations are inevitable to decode an integer using bit-oriented codecs, however, list-oriented codecs can decode a batch of integers with a single operation. BMI with dynamic pruning methods further improves processing ef fi ciency, nonetheless, BMM achieves better performance than BMW, which is different from the result in [ 21 ], after an inspection into the procedure of both methods, we conclude that the list sorting consumes much time in pivot selection phase of BMW, also BMM selects candidate documents in the essential lists which are likely to belong to more important terms, while BMW selects candidate documents based on their docids without considering the terms  X  importance, with more query terms, it causes more mis-scoring candidates that slows down the performance of BMW. Here we only implement the basic BMI, some other methods like on-the-fl y BMI generation and hierarchical-layered blocks adopted in [ 6 ] remain future investigation. When it comes to DSI, we get an interesting result that its time consume is rarely low compared with other strategies, which are an order of magnitude higher than DSI; another surprising phenomenon is that it stays considerably stable when query length grows while other strategies raise their time consume to different extent, the box part is contracted into a solid line which can be hardly noticed. It can be concluded without doubt that DSI outperforms all the others. However, this performance is achieved at the cost that the huge size structure of DSI is fully loaded into the main memory and occupies nearly 50 GB space, while others keep low memory occupation within 6 GB. This result is also consistent with the inference before, we postulate that scoring a document and maintaining a priority queue for the result cost constant time, then the only factor that in fl uences time ef fi ciency is the size of the collection, as the three basic operations can be implemented in O ( logD ) time, so is the intersection and merge of the posting lists. Also the documents in DSI are sorted by term frequency in decreasing order, the potential candidates to be ranked in top-k lie in the front of each posting list. With this in mind, we can just evaluate few documents rather than the whole list and the query time is signi fi cantly reduced. Another thing to be noticed is that the outliers always appear below the box, which is caused by the fact that posting lists for uncommon terms are missing or pruned by dynamic pruning strategies.
 results for the case of reordered indexes, as expected, performance of structures which use dynamic pruning techniques shows promising bene fi ts in term of query response time compared to the prior. However, DAAT and DSI remain unaffected, as DAAT processes the query in an exhaustive way and documents in DSI are ordered by term frequency actually. Reordering works extremely well for BMW, performance gap between it and BMM is sharply reduced. Indeed, reordering clusters the related doc-uments and shorten the time of pivot alignment. For query length no longer than 5, BMM and BMW are about the same ef fi ciency, their average response times fall below 10 2 ms, as query length grows, the performance gap rises again.
