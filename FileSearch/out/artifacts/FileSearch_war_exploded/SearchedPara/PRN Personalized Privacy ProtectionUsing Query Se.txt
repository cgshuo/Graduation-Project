 With advances in wireless communication and mobile positioning technologies, location-based services (LBSs) have s een wide-spread adoption. These applica-tions provide users with a great convenience, and improve the quality of work and personal life significantly. However, the increasing collections of individual X  X  information ( e.g. , location) open the door for potential privacy disclosure.
In general, existing work on privacy preserving in LBSs protects users three kinds of information: identity , location ,and sensitive information .Inorderto protect these information , different models and methods are proposed. To hide the user X  X  identity, location k -anonymity model is the most acceptable model. For example, in Fig. 1(a), u 1 , u 2 and u 3 constitute a cloaking set. The locations of u 1 , u 2 and u 3 are represented by a segments set S . u 1 is indistinguishable from u 2 and u 3 in the cloaking set, thus the users identities are successfully protected.
To hide the exact locations, based on the location k -anonymity model, location cloaking is the popular methodology. Its main idea is to reduce the spatial and temporal resolution of the user X  X  location. In Euclidean space, exact locations are usually extended to a rectangle or a circle [1,3,4]. While in a constrained space ( e.g. road networks), exact locations are usually published as a segments set [2,13,10]. In the previous example, the adversary cannot sure where is the exactly location of u 1 on the segments in S .

Most existing work focuses on the iden tity and location protection, but few literatures pay attention to prevent the sen sitive information disclosure. The sen-sitive information is disclosed using two kinds of published information: location semantics [3,7,16] and query semantics [14]. Fig. 1(b) shows an example of the first case. Considering the public geographical context, the segments in S are totally covered by a hospital. Thus, each user X  X  visited place is disclosed. Fig. 1(a) illustrates the other case. When the adversary considers the query seman-tics, three users issue hospital-related queries. The attackers get the inquirer X  X  health conditions. In this paper, we consider to protect the sensitive information using query semantics over road networks.

In order to protect the sensitive information, besides location k -anonymity model, a cloaking set also follows l -diversity model [9]. In our scenario, according to the l -diversity model, the query contents issued from a cloaking set are at least l different. However, location k -anonymity model and l -diversity model are not sufficient to protect the sensitive information.
As we known, privacy requirements are personalized. Whether a query is sensitive or not depends on the personalized privacy requirements. Fig. 2(a) shows an example of the personalized privacy profiles. Fig. 2(b) is a cloaking set following location 3-anonymity model and 3-diversity model. In u 4  X  X  side, three queries issued from the cloaking set are all sensitive. u 4 doesn X  X  want any of these queries being associated with hi mself. The case, that Fig. 2(b) shows, is unacceptable for u 4 . We call this kind of attacks as personalized homogeneity attacks , which consider personalized privacy profiles and query semantics.
This paper focuses on preventing personalized homogeneity attacks on road networks, which aiming at protection identity, location and sensitive informa-tion. We face one major challenge. What is the proper users partition strategy which incurs less re-computing? Different protection targets lead to re-compute cloaking sets iteratively. For exampl e, some users in a ca ndidate cloaking set satisfy location k -anonymity model originally, but other users in the same set fail to protect the sensitive information. However, when the sensitive information is protected by adjustment, some user fails to follow the location k -anonymity model.

To address the above issues, we propose a ( K, L, P )-anonymity model to pro-tect identity, location and sensitive information. Thanks to [10], a user location on the road network is mapped to a 1-D value. Our P 3 RN finds cloaking set based on the 1-D values considering the persona lized privacy requirements. Specifically, users are first partitioned into groups as the maximum anonymity level. Then, conservative users shift from one group to another one considering the sensitiv-ity requirements. Finally, the segments covered by each group are published to protect location information.

The contributions we make in this paper can be summarized as follows:  X  We propose a ( K, L, P )-anonymity model, which aims to protect user X  X  iden- X  We propose a P 3 RN cloaking algorithm over road networks, which supports  X  Some preliminary experiments are conducted to evaluate the performance of
The rest of the paper is organized as follows. We review the related work in Section 2. The problem under investigation is formally defined in Section 3. The cloaking algorithm P 3 RN is proposed in Section 4 . Section 5 presents the preliminary performance evaluation results. Finally, the paper is concluded in Section 6. Hu et al. [4] first observed the problem of sensitive information disclosure, when the anonymized location data joins with a reference data set. [4] only follows loca-tion k -anonymity model. Location l -diversity model was introduced in [1] which requires that a cloaked region contains k mobile users and l places. However, [1] doesn X  X  distinguish the place type. [15] refined the definition of l -diversity as the number of different places types. Then, [3] further classified the places into sen-sitive places and non-sensitive places. [16] presented an approach to the privacy preserving sharing of sensitive position s in urban settings. All of previous work protects individual sensitive information using location semantics. [14] first pointed out the problem of the homogeneity attack using query semantics. A p -sensitive model is proposed, and a PE-tree is constructed for im-plementing the model. The drawback of [14] is as follows. First, the maintenance cost for the PE-tree is high. Second, every u ser has the same privacy requirement. Third, the system unifies to define whether a query is sensitive or not. [6] fur-ther categorized service attribute values ( e.g. , burgers and pizzas are categorized as the fast food) and enhanced the cloaking algorithms by defining the query l -diversity concept in LBSs. [14] and [6] only apply to Euclid ean space, whereas our work pays attention on privacy protection against personalized homogeneity attacks over road networks.

In terms of the anonymization methods on road networks, existing work is classified into four categories: tree-type index-based cloaking algorithms [8], graph traversal-based cloaking algorithms [13,10,2], mix-zone based cloaking al-gorithms [12], and PIR-based anoymization algorithms [11]. The cloaking sets, generated from the second method, refl ect the feature of road networks, which is helpful for reducing query cost in LBSs server. Meanwhile, the cloaking set has k -sharing property, which is benefit for cases of high workloads, preventing query sampling attacks [10] and replay attacks [13]. Consequently, the graph traversal-based cloaking algorithms are the most widely used privacy preserving approach over road networks so far. Our proposed P 3 RN algorithm is in line with the graph traversal-based methods. 3.1 Preliminary Like most existing work, we employ a centr alized system, which consists of mobile users, a trusted anonymizing proxy (TAP ), and an un-trusted service provider. A category-sensitivity relation CaSR is stored in the TAP, which is a many-to-one relation from a query categories relation to a sensitivity relation. Through CaSR , each query is labeled with a sensitivity 1 . CaSR is defined as follows. Definition 1. ( Category-Sensitivity Relation )Let D ( CaSet ) and D ( SSet ) be the domain of query categories and sensitivities respectively. Then, For example, CaSR = { (sensitive location navigation, top secret), (emergency call, more secret), (location sensitive billing, secret), (infotainment services, less secret), (shopping guides, non-s ecret), (travel guides, non-secret) } ,where D ( CaSet )= { sensitive location navigation, emergency call, location sensitive billing, infotainment services, shopping guides, travel guides } , D ( SSet )= { top secret, more secret, secret , less secret, non-secret } 2 .
 The relation CaSR has two properties.

Property 1: For any two queries q i and q j ,if q i and q j belongtoasame category, q s i = q s j ,where q s i is the sensitivity of the query q i .
Property 2: For any two queries q i and q j ,if q s i = q s j , the categories of q i and q j are different.

From Property 1 and Property 2, quer ies with the same contents are the special case of queries with the same sensitivity. Thus, in the following sections, we only focus on the query sensitivity instead of the specific query content.
In order to represent users locations on the road network, like the most existing work, we define a road network as an un-directional graph G ( V,E ). V is a vertices d ( v ) is the vertex degree. E is an un-directional edges set, representing road segments between two vertices.
 The privacy profile is formalized as follows.
 Definition 2. ( Privacy Profile ) In order to protect user X  X  personalized pri-vacy on road networks, each user specifies three kinds of parameters:  X  Anonymity requirement k : It is the anonymity level in the location  X  Location diversity requirement l : It is the minimum number of road segments 3  X  Sensitivity requirements ( t s ,p ): t s is the user X  X  maximum tolerable query
Note that t s is defined for one query, indicating whether a query is sensitive or not. p is based on a users set, implying the number of sensitive queries in the set. For example, a user u sets the privacy profile as (3, 4,(0.5, 0.4)). u requires that the cloaking set contains at least 3 users. At least 4 different road segments are published as the cloaked location. If a query sensitivity is larger than 0.5, u regards this query as a sensitive query. u requires that the portion of sensitive queries in the cloaking set is not larger than 0.4. 3.2 Attack Model and Privacy Model Definition 3. ( Personalized Homogeneity Attack )Let CS be a users set. For  X  u  X  CS ,from u  X  X  point of view, the number of the sensitive query in CS is denoted as Count SQ u .If where | CS | is the number of users in CS . This attack is termed as personalized homogeneity attacks.

Fig.3showsausersset CS = { u 1 ,u 2 ,u 3 } . Assume that u 1 .t s =0.25 and u .p =0.9. From u 1  X  X  view, all queries issued from CS are sensitive, which is un-acceptable. CS suffers from personalized homogeneity attacks.
 Now let X  X  define the ( K, L, P )-anonymity model.
 Definition 4. ( ( K, L, P ) -anonymity model )Let CS be a cloaking set accom-panied with a set of issued queries QSet and the cloaked location RS .If  X  K  X | CS | ,where K = MAX  X  RS is a set of road segments, and L  X | RS | ,where L = MAX Users in CS satisfy ( K, L, P )-anonymity model .

The first condition guarantees location K -anonymity model. The second con-dition ensures the diversity of the published location. Finally, the third one implies that the number of sensitive queri es is acceptable for each user. Besides, users following the above three conditions has K -sharing proterty. Fig. 4(a) shows a cloaking set. Users privacy profiles are shown in Fig. 4(b). According to Definition 4, { u 1 ,u 2 ,u 3 } satisfies ( K, L, P )-anonymity model. P
RN aims to protect the identity, location and sensitive information for each user. Therefore, we divide the problem into three sub-problems. First, to protect identities, we partition users into diff erent groups accord ing to the maximum anonymity level. Then, to protect sensit ive information, sev eral conservative users shift from one group to another according to each user X  X  sensitivity require-ment. Finally, to protect the exact locati ons, segments, which users in cloaking sets are on or near by, are published as t he cloaked location. We elaborate each sub-problem as follows.
 4.1 User Identity Protection As we known, the network distance com puting is a costly work. Thus, we adopt users partition strategies on road networks in [10]. Let moSet be the users set and GK MAX = MAX number according to the road segments tranveral order. Thus, the user X  X  loca-tion on the road network is mapped to a 1-D value. Based on the user orders, users are partitioned into | moSet | GK MAX groups. Each group has GK MAX users except the last one with less than 2* GK MAX users. Obviously, the anonymity requirements for users in each group are satisfied.

Let X  X  take the users in Fig. 5(a) as a running example. Suppose that the segment &lt;n 1 ,n 2 &gt; is selected. From &lt;n 1 ,n 2 &gt; , do a depth-first traver-sal on the road network. The number on the edge is the order of edge vis-iting. Then, users on the edge are labeled with a number according to the edge visiting order. The subscript of each user is the user order. Fig. 5(b) shows the 10 users privacy profiles. GK MAX =3. 10 users are partitioned { u 4.2 Sensitive Information Protection Intuitively, a user with the constr ained sensitivity requirement ( t s ,p )ishard to find the cloaking set. If these users are not anonymized successfully at the beginning of the algorithm, they are hardly to be anonymized later. Therefore, we build a min-heap for users with t s  X  p as the sorting key. Then, pop the top user topu from the heap, and find the group group i where topu is. Next, for each are found and inserted into group i . Repeat the inserting step until group i has no unsafe users.

However, the inserting process would result in re-computing iteratively. Sup-pose that there exist an unsafe user nu and a safe user u in group i . nu regards aquery q as non-sensitivity, but u holds the opposite. If users issuing q are inserted into group i , the safe user u may become unsafe with the number of sensitive queries increasing. In order to avoiding this case, conservative users are inserted into a group. We define conservative users as follows.
 Definition 5. ( Conservative Users )Foragroup group i , group i .t s = MIN  X  u  X  group i ;  X  u.q s  X  group i .t s . u is called as a conservative user w.r.t. group i . In the previous example, group 1 = { u Theorem 1 :Foragroup group i ,if MAX users are inserted, the personalized ho mogeneity attack is prevented, where uus is the set of unsafe users whose number of s ensitive queries is beyond expected. Proof. With conservative users inserting, Count SQ u remains unchange for each user u ,and | group i | increases. Therefore, for a safe user u  X  group i  X  uus , of conservative users inserted. We hope that Count SQ u group derivation, x u  X  Count SQ u u.p  X  X  group i | .For group i ,if MAX users are inserted, group i has no un-safe users. Proof done.

If conservative users are redundant, the nearest ones are inserted. Since the network distance computing is costly, we employ the order distance for proximate calculation. Specifically, we define the group order ord gp i as the average order of users in the group. The nearest conservative user is defined as the conservative user u with the minimum order distance | ord u  X  ord gp i | . Still using the example a safe cloaking set. If there is not enough conservative users, in order to gurantee 100% success rates, dummies 4 with q s =0and t s = 1 are inserted.

Since some users ( e.g. u 5 ) shift from one group ( e.g. group 2 ) to another group ( e.g. group 1 ), the user number of a group ( e.g. group 2 ) decreases. Such that, the anonymity requirement may fail to be satisfied. For convenience, we call this kind of groups as shrunk-groups . We merge the shrunk-groups into its neighbor group. Specifically, suppose a shrunk-group is group i . group i is merged with group i  X  1 or group i +1 randomly. Repeat this merging process until each user X  X  anonymity requirement is satisfied.

Continuing the previous example, the second top user popped from the min-is a shrunk-group. Therefore, merge users in group 2 with group 3 . Finally, users in the merged group constitute a cloaking set.

The algorithm is shown in Algorithm 1. In order to improve the efficiency, we use the bucketization technique. Users who issuing queries with the same sensitivity are in a same bucket. For a top user u , the group number is computed (line 1). Then users in this group are found (line 2). If the group is a shrunk-group, merge it with the neighbor group (line 4 and line 5). Otherwise, check whether each user X  X  sensitivity requirem ent is satisfied (line 8). If not, compute the number of consecutive users needed as Theorem 1 (line 9). If conservative users are enough, shift the max nd nearest consecutive users to the group i (line 14); otherwise, dummies are g enerated and inserted into group i (line 16). Users in group i constitute a cloaking set. 4.3 Cloaked Locations Computing The third target of P 3 RN is to generate the cloaked locations. Let CS be a cloaking where ll max = MAX first choice is the road segment whose nodes are both in sg on . The second choice is the segment one of whose nodes is in sg on . We continue running the example in n Algorithm 1. Algorithm for finding cloaking sets 4.4 Strict Users Anonymization D ( SSet )(1  X  i&lt;j  X  n ), S j &gt;S i . Now let X  X  define strict users. position of u.t s in D ( SSet ) , u is termed as a strict user.
 requirement ( t s ,p ) as (0.25,0.5). For any a query q , q has 0.6(=3/5) probability as a sensitive query to u . However, u only sets the sensivity profile p =0 . 5. u is a strict user. From Theorem 1, the stri ct users need great conservative users to be anonymized togerther, as a result, the cloaking set is huge. Huge cloaking sets not only results in high query cost, but also draws the attacker X  X  attention resulting in privacy disclosure.

In order to avoid huge cloaking sets, we bring strict users together for anonymization. Strict users strU are sorted by their user orders ascendingly. Then, strU is divided into strU GK MAX groups. For each group, if there exists unsafe users, we insert dummies as the conservative users. The algorithm is simple, so we omit the detailed algorithm for space limited.

In summary, P 3 RN is shown in Algorithm 2. Line 1 employs [10] to sort mobile users as the segments traversal order. Then, insert users in moSet into a min-heap with t s  X  p as the sorting key (line 2). Next, pop the top user u from the min-heap (line 3). If u is a strict user, insert u into the strict users set strU (line 6); otherwise invoke Algorithm 1 to find the cloaking set (line 8). After, invoke the algorithm in Section 4.4 to find cloaking sets for users in strU (line 9). Finally, applying the segment selecting strategy in Section 4.3 to generate cloaked locations (line 10 to line 13). Repeat line 4 to line 14 until the min-heap is empty.
 Algorithm 2. P 3 RN We compare our algorithm P 3 RN with DF, which is the DFS-based cloaking al-gorithm [10] incuring the fewest query co st. DF cannot prevent personalized ho-mogeneity attacks, but it is included for comparison to show the cost required for defending against personalized homogeneity attacks. Both cloaking algorithms are implemented in C++ and run on a desktop PC with a dual AMD 2.0GMHz processor and 2GB main memory.

We adapt a real data set, California Road Network and Points of Interest (POI) [5], to validate the effectiveness of our cloaking algorithm. The road map contains 21,048 nodes and 21,693 edges; moreover, it is associated with a real dataset of 32,399 POIs. All the POIs are empolyed to simulate LBS queries. Table 1 lists the default system settings.

We investigate the impact of anonymity level k on the performance of cloaking algorithms by enlarging the anonymity level range. Enlarging the anonymity level range implies not only more constrained but also more diversified privacy requirements. In Fig. 6, we measure the i nformation entropy [13] of anonymous locations, which indicates the protectio n strength. Larger information entropy implies that the attackers is more uncertain of the user X  X  specific location. With k increasing, more users and segments are included in each cloaking set. Thus, the protection strength for two metho ds is both improved. However, P 3 RN provides better protection than DF at all settings.
 Recall that we use dummy queries to achieve a 100% success rate. Thus, in Fig. 7, we measure the portion of dummys generated in the total mobile users . We observe that DF doesn X  X  need dummies since the anonymity requiremented is only considered. In order to prevent p ersonalized homogeneity attacks, about 7% dummies are generated by P 3 RN, which we think is acceptable.

We empoly the query cost model proposed in [2] to evaluate the average query cost, which is measured by the road segment length and the number of boundary nodes in the cloaked locations. From Fig. 8, the query costs for both algorithms increase with k increasing. Though the query cost of P 3 RN increases worse than DF, 0.2  X  query cost is sacrificed for preve nting personalized homogeneity attacks on average. Fig. 9 shows the effect of k on the average cloaking time. Since the time spending on traversing road segments and users, when users are sorted as the order of segments tranversal, dominates in the overall cloaking time, the time difference between DF and P 3 RN is slight (about 0.3 ms ). The cloaking time of P 3 RN is only 6 ms for each user on average. In this paper, we investigated a cloaking algorithm that can protect privacy against personalized homogeneity attacks over road networks. To address this problem, we propose a ( K, L, P )-anonymity model to protect the identity, lo-cation and sensitive information. Base d on personalized privacy requirements, we have proposed a cloaking algorithm called P 3 RN to generate cloaking sets over road networks. A prelimiary of experiments has been conducted to evaluate the effectiveness and effciency of P 3 RN. Experimental results demonstrate that P
RN provides better protection strengh than the existing algorithm. The aver-age cloaking time is only 6 ms . The price paid for defending against personalized homogeneity attacks is small.
 Acknowledgement. This research was partially supported by the grant from the Natural Science Foundation of Ch ina (No. 61303017), and the Hebei Educa-tion Department (No. Q2012131), and Shandong Province Higher Educational Science and Technology Program (No. J12LN05).

