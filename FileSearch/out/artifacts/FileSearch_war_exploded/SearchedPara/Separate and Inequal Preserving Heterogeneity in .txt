 Web pages, like people, are often known by others in a variety of contexts. When those contexts are sufficiently distinct, a p age X  X  importance may be better represented by multiple domains of au-thority, rather than by one that indiscriminately mixes rep utations. In this work we determine domains of authority by examining t he contexts in which a page is cited. However, we find that it is no t enough to determine separate domains of authority; our mode l ad-ditionally determines the local flow of authority based upon the relative similarity of the source and target authority doma ins. In this way, we differentiate both incoming and outgoing hyper links by topicality and importance rather than treating them indi scrim-inately. We find that this approach compares favorably to oth er topical ranking methods on two real-world datasets and prod uces an approximately 10% improvement in precision and quality o f the top ten results over PageRank.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Infor-mation Search and Retrieval Algorithms, Performance Web search engine, link analysis, reputation, PageRank
Human recommendations are typically made within a particul ar context. For example, we say that person is rare to recommend someone without qualification (that is, with-out regard to purpose or topic). In addition, a person may als o be known in multiple contexts: a successful scientist might al so be an amateur musician.

Thus one might argue that reputation (or equally, importanc e or authority) is context-sensitive. This idea is equally appl icable to the analysis of web authority. Traditional link analysis sc hemes treat hyperlinks indiscriminately and make the assumption that ev-ery target page is equally recommended. Under such methods, a page X  X  importance is measured by the sum total of authority fl owing through incoming links without considering from which comm uni-ties that authority is derived. Similarly, in traditional l ink analy-sis methods, a page divides its authority equally among all t argets without considering the context in which those links were cr eated.
However, when a content creator places a hyperlink from one web page to another, often the link is meant to refer a particu lar aspect or topic of the target page instead of the page in gener al. Importantly, a single page often covers multiple topics, an d so an indiscriminate analysis of an incoming link might mistaken ly give importance to unintended topics.

In this paper we propose to model reputation within distinct do-mains. A page can be known in multiple areas, and thus we track a page X  X  reputation separately within each domain. In addit ion, we consider to what reputation domain an outgoing link is heade d, and propagate topical authority appropriately. To accomplish this, we break a node into a number of heterogeneous, domain-specific sub-structures and consider authority flow between those units ( both within and between pages) instead of at the page-to-page lev el.
By modeling the structure of the web at this finer level of deta il, we can separate authority flow on different topics and preser ve such heterogeneity during authority propagation. As a result, a page X  X  importance is able to be represented within multiple domain s of authority, rather than by one that indiscriminately mixes r eputa-tions. Furthermore, by considering the relative similarit y between the source and target domains, a page can propagate its topic al au-thority among the targets in a more intelligent and appropri ate way.
We evaluate our approach by conducting experiments on two real-world web datasets and involve studies on context repr esenta-tion as well as domain identification. We show that by introdu cing this fine-grained graph model, we can improve the retrieval q uality of the PageRank algorithm.
 The contributions of the paper are:
The remainder of this paper is organized as follows: the back -ground and related work will be introduced in Section 2, with a focus on combining text with link analysis and community ide nti-fication. The proposed model is then detailed. The experimen tal framework and results will be presented in Sections 4 and 5 re spec-tively. We conclude with a discussion and summary of contrib u-tions.
We begin with brief connections to related work, which can be put into four categories: web reputation representation, l ink anal-ysis for community discovery, topicality in link analysis a nd web page partitioning.
In this work we explicitly represent the various domains in w hich a page X  X  reputation exists. Rafiei and Mendelzon [18, 17] als o in-vestigated the reputation of a page, and proposed a method to gen-erate a description of a page X  X  reputation by considering th e contri-butions of all terms in predecessors that add authority to th e page. In contrast, we consider distinct domain reputations withi n a set of topics, rather than a single cumulative reputation over all text. Our approach for generating reputation domains from link conte xts is based on our prior work on identification of communities of pa r-ent pages [12, 13] but here we additionally consider extende d an-chortext and communities of child pages in order to maintain the heterogeneous flow of topical authority.
Instead of classifying links into topical domains, one coul d par-tition links by the communities in which the link endpoints a re placed. Processes for discovering communities on the web ha ve been extensively studied. Kumar et al. [9] find expanded bipar-tite subgraphs to represent web communities. Similarly, Fl ake et al. [5] and Andersen and Lang [1] also detect communities purely based on link structure, while our approach takes page conte nt into account when deciding communities. Moreover, these approa ches utilize link analysis to detect web communities, while we us e com-munity information to assist with link analysis.

Roberts and Rosenthal [20] find clusters of web pages based on their outlink sets; in their model the authority value of a pa ge is proportional to the number of clusters which tend to link to i t rather than the number of pages which link to it. This is somewhat sim ilar in spirit to our approach, except that we utilize page conten t when generating communities, and do not consider each community to be of equal value.
This paper is one of many that have proposed methods to in-tegrate topical analysis with link analysis. Haveliwala X  X  Topic-Sensitive PageRank (TSPR) [6] was the first algorithm to inco r-porate topical information into link analysis. In his model , multi-ple PageRank calculations are performed, one per category, each biased by jumping exclusively to pages in the corresponding cate-gory rather than to any web page. Rafiei and Mendelzon [18] app ly a similar idea at the term level.

In addition to biasing the random jump on a per-term relevanc e basis, Richardson and Domingos X  X  Intelligent Surfer (IS) [ 19] bi-ases the selection among the outgoing links based on the rele vance of the target to the term. Pal and and Narayan [16] similarly f avor links leading to pages on the topic of interest.

In past work [11] we also incorporated topicality into web ra nk-ings in our Topical PageRank model. This approach calculate s an authority score vector for each page to distinguish the cont ribu-tion from different topics. A topical random surfer probabi listically combines page topical distribution and link structure.

All of the approaches above incorporate topics within the ra n-dom surfer model. In contrast, the approach introduced in th is paper retains the original random surfer model but applies i t to a finer-grained link graph for better modeling of authority pr opaga-tion though a page X  X  reputation domains.
Our proposed model partitions the set of outgoing links for e ach page (by domain). Others have similarly considered breakin g a page into subpages. Chakrabarti et al. [3] propose segmenting a page into small pagelets containing contiguous subsets of l inks con-sistently on a particular topic. Cai et al. [2] use a vision-based al-gorithm to partition a page into blocks. In contrast, our pro posed approach breaks the set of outlinks by the reputation domain s to which they point, independent of the source page X  X  structur e.
Web pages, like people, are often known by others in a variety of contexts. When those contexts are sufficiently distinct, a page X  X  importance may be better represented by multiple domains of au-thority, rather than by one that indiscriminately mixes rep utations. To achieve this, a page can be regarded as mapping into multip le  X  X uthority X  units, in which each unit/subnode is used to acc umu-late authority flows from a particular domain. In this way, re pu-tation flows from various domains will stay on their topics ra ther than diffusing into the entire page.

Similarly, a single page normally covers multiple topics; t hus hyperlinks from different contexts within this page may poi nt to different topics and carry different importance as well. In stead of giving each possible outlink the same amount of authority no matter whether the target X  X  topic is relevant to the authority X  X  co ntext or not, it is advantageous to separate the targets into distinc t domains and distribute authority among them based on relevance. Jus t as in the previous mechanism, we can separate the page into seve ral domain-specific  X  X ub X  units, in which each hub includes outg oing links leading to targets in a particular domain.

In this way, we map the authority propagation from the web pag e level into a level consisting of domain-specific subnodes. S uch a fine-grained model explores the hidden semantic structure w ithin web pages between which a hyperlink really takes place, pres erves the heterogeneity in topical reputation flow and provides a m ore effective method of reputation calculation. The rest of the section gives a detailed introduction of our model, Heterogeneous T opic Rank (denoted as HTR). For better understanding, we use a sma ll web made up of four pages in Figure 1 as an example.
As introduced above, the premise to identify the  X  X uthority  X  and  X  X ub X  units for a given page is to find out various distinct dom ains surrounding it. Web pages are often known by others or refer t o oth-ers in a variety of contexts. To disambiguate various contex ts, we classify them into separate categories. We predefine twelve broad categories (Table 1), chosen from the top level of the dmoz Op en Directory Project (ODP) [14], and use a textual classifier to deter-mine the category of each context. As shown in the second step in Figure 1, the classification process will assign a label (d epicted by different colors in this example) to each hyperlink. In th is way, given a page separated into several domains (topics) based on their clas sification labels.

Besides the classification solution given above, we could al so use clustering techniques to disambiguate contexts based o n their textual similarity. Compared to classification, clusterin g would be more flexible. However, it is an expensive process, and in our case, would need to be applied to the set of parents for each documen t; even with parallelization and optimization, this will alwa ys be ex-pensive for large datasets.

There are a number of options for representing a hyperlink X  X  con-text. In this paper, we examine three variations:
With the various topics/domains surrounding a given page id en-tified, we can easily partition the page from both directions , into domain-specific  X  X uthority X  and  X  X ub X  units (denoted as in what follows).

This process is demonstrated in the 3rd step of Figure 1. Node is linked from two relatively distinct domains, and domain resents 66% of the links and dependent within node rately directed to a page into several domain-specific ous domains to which it points. As reflected in the example, si nce node units, e.g., ) are introduced into node different domains. A similar process is applied to every nod e in the collection.
When node decomposition is complete, each page has been mapped into a bipartite graph consisting of two sets of verti ces: units lar domain. The decomposition processes of given page are symmetric, determined by various domains/to pics pointing to or linked from this page respectively. To descri be the domain referred by a particular straction. Given an know that it is a unit within page contains several contexts on topic each context is represented in the form of a term vector, thus we can describe the domain by the average of all the belonging ve c-tors, denoted as integrates information from various members and renders a g lobal view of this domain. Alternatively, since each context in th e do-main is labeled with the same category tag tag can be represented using the textual average vector ( X  X erm l evel X ) or classification label ( X  X ategory level X ) of all the From the perspective of link analysis, the introduction of units provides a fine-grained representation for hyperlink struc-ture. In our model, each hyperlink is no longer a simple conne ction between two entire pages cal unit unit
In our model, we uncover the hidden internal structure of a pa ge as a complete bipartite graph, with each unit within the same page. In this way, a particular its current authority to the outside via some intermediate  X  exits X  ( all the target domains no matter whether they are relevant to the domain of authority or not, a particular its authority to an targets in a relevant domain. As the example shows, the either propagate its authority to the target domain to target domain unrelated domains. If we did not consider the relevance and e qually split authority among the three outgoing links, 66% of autho rity on topic unrelated domain getting undeserved high authority, we sel ect units based on the relevance between the source and target do main. Suppose the relevance between the weight labeled on edges within the page. As a result, doma in inherits 85% of the authority of
To measure the relevance (A,H) units, e.g., their associated domains. As mentioned above, the domain ca n be described in two levels: the category level or the term level .
In the last phase, we normalize the relevance measurement to represent the transition probability from weight the corresponding edge in the page X  X  bipartite graph . In the above equation, gory labels of
The next question comes as how to propagate authority among the newly constructed web structure. The solution is to imit ate and alter the behavior of PageRank X  X   X  X andom surfer X . Imagi ne a web surfer wanders on the web, who at each time is at some unit within a page With probability unit from the entire web with probability 1/ is the number of surfer will pick one of the outgoing links, saying, the link t owards . However, notice that the outgoing links cannot be reached by the surfer directly, since there are several the middle. The solution is to take a 2-step transition: the s urfer first chooses an likelihood to choose a particular relevance with the current from from introduced in last section, which can be calculated in eithe r  X  X at-egory level X  or  X  X erm level X . After arriving at select uniformly one of its outlinks links from
Since the authority score can be defined as the stationary pro b-ability of finding the random surfer at authority tion of category labels of By replacing lation can be finalized as When the authority propagation converges, every signed an authority score with respect to its associated dom ain. The next task is performed at query time; to be able to rank result s for a particular query score for the page. This can be achieved by merging the scores of units that belong to a page weighted by their affinity to this q uery. The composite score can be calculated as follows: where represents nent in query egories, which can be generated by a textual classifier.
In this section, we describe the datasets and experimental m eth-ods used to evaluate the performance. We will compare our ap-proach versus well-known ranking algorithms, especially t hose combining text and link analysis. Experimental results wil l be pre-sented in Section 5.
To avoid a corpus bias, two different data collections were u sed in our experiments. One is the TREC GOV collection, which is a 1.25 million page crawl of the .gov domain in the year 2002. Among them, 1,053,372 are text/html files, which were used in our experiments. This corpus was used as the data collection of t he TREC Web Track for a number of years. The second data set is a 2005 crawl from the Stanford WebBase [7, 4]. It contains 58M pages and approximately 900M links.

When conducting the experiments on the GOV corpus, we chose the 2003 topic distillation task to test these algorithms, w hich con-tains 50 queries. When doing experiments on the WebBase corp us, we selected 50 queries from a set of consisting of those frequ ently used by previous researchers, ODP category names, and popul ar queries from Lycos and Google (shown in Table 2). Since there are no standard relevance judgments available f or WebBase dataset, the relevance between query and search res ults has to be inspected manually. For each randomly assigned que ry, evaluators (members of our research lab) are required to det ermine the relevance for every URL result associated with this quer y (blind to the source ranking algorithm), using a five-value scale wh ich were translated into the integers from 0 to 4. If the average s core for this pair is more than 2.5, it is marked as relevant. The av er-age fraction of relevant URLs within the top ten results acro ss all queries is defined as Precision@10. To further explore the qu ality of retrieval, we also evaluated the ranking algorithms over the Nor-malized Discounted Cumulative Gain (NDCG) [8] metric. NDCG credits systems with high precision at top ranks by weightin g rele-vant documents according to their rankings in the returned s earch results; this characteristic is crucial in web search.

For GOV data, TREC provides relevance assessments; there ar e 10.32 relevant documents per query on average for the topic d istil-lation task of TREC 2003. In addition to P@10 and NDCG@10, we add Mean Average Precision (MAP) as evaluation metric sin ce it is widely used in TREC and not restricted to the top 10 resul ts.
We compare our proposed approach HTR with five ranking algo-rithms: PageRank (PR) [15], Topical PageRank (TPR) [11], To pic-Sensitive PageRank (TSPR) [6], Intelligent Surfer (IS) [19 ] and CommunityRank (CR) [12]. PR is used as the baseline; we ad-ditionally chose TPR, TSPR, IS and CR because, similar to our model, they measure a page X  X  reputation with respect to diff erent aspects (topic or term) instead of a generic reputation.

As discussed previously, our model may have several options ; the various resulting combinations are shown in Table 3. In t he Table 2: Set of fifty queries used for relevance evaluation in WebBase.
 experimental section below, we study and compare their retr ieval quality over multiple performance metrics.

For each query, we rank all documents using the combination vance score and the other is the authority score calculated b y link analysis algorithms. The relevance score is calculated usi ng the OKAPI BM2500 [21] weighting function, and the parameters ar e from the combined list as the final outputs. The combination c ould be score-based, where a page X  X  final score is a weighted summa -tion of its authority score and relevance score; it could alt ernately be order-based, where ranking positions based on importanc e score and relevance score are combined together. In our implement ation, we choose the order-based option; all ranking results prese nted in this paper are already combined with IR scores.
We use a well-known naive Bayes classifier,  X  X ainbow X  [10], t o decide the category for each hyperlink X  X  context for the pur pose of domain recognition as well as a given query X  X  affinity to va ri-ous topics. The classifier is trained on 19,000 pages from eac h of twelve categories of the ODP hierarchy. We apply it to both co n-texts and queries and get a topic distribution for each. We la bel the context by the dominant dimension of its topic distribution vector.
To evaluate the behavior of our proposed HTR model, we com-pare its retrieval performance versus well-known ranking a lgo-rithms. Results demonstrate that by preserving the heterog eneity in topical authority flows, we can produce more accurate sear ch results.
As described in Section 3, each node in the web graph will be partitioned into several  X  X uthority X  ( respect to the different domains surrounding it. As a result , the 1.05 million nodes in GOV collection are mapped into 1.45 million units and 3.16 million Base are mapped into 82.3 million units. (These numbers reflect the use of  X  X xtendedAnchor X  co n-text representation.)
We define domain in-degree as the number of domains point-ing to a page, and domain out-degree as the number of domains pointed by a page. To illustrate, in Figure 2 we show the distr ibu-tion of the domain in-degree and the domain out-degree when u sing extended anchortexts on GOV. (Distribution on WebBase exib its a similar pattern.) The distribution of domain out-degree is smoother than the domain in-degree, indicating that it is more common for a document to link to multiple topics than being known within m any domains.

Many pages on the web are referenced by contexts from different domains. For example, in our GOV dataset, the Figure 2: Histogram of domain degrees for extended anchort-exts on GOV. page http://www.tourism.wa.gov/ is the official site of Washington X  X  state tourism. We classified its parent page s into either  X  X ecreation X  or  X  X usiness X  domain. Another, http://oa.od.nih.gov/ , is the home page of  X  X IH Office of Administration X , for which some parents belongs to  X  X eal th X , while others belongs to the domain of  X  X usiness X . A broader e x-ample is http://dc.gov/ , the governmental homepage of the District of Columbia. Information related to different top ics can be found on this page, and correspondingly we found parent pa ges from various topics, such as  X  X ecreation X ,  X  X ports X ,  X  X usi ness X ,  X  X ealth X  and  X  X omputers X , pointing to it.
When determining how to distribute the authority introduce d through a particular incoming link among the targets, our mo del makes its choice based on the relevance of each source to each target. Instead of simply measuring the relevance based on t he contexts provided by individual inlinks and outlinks, we ch oose to examine the relevance between domains. In this experimen t, we investigate whether a global view of context (provided by a c om-bination of all of the members of a link X  X  domain) is more help ful than a local view of context (provided by a single link alone) in determining relevance.

We randomly sampled 966 linked pages from the GOV web document corpus. The relevance between source and target ca n be measured by the similarity between the link pair X  X  local c on-texts (in  X  X xtendedAnchor X  representation and with  X  X erm l evel X  measurement). Unfortunately, the description provided by a sin-gle hyperlink is always short and not very informative, gene rat-ing an extremely sparse term vector space. As a result, 604 ou t of 966 sample pairs end with zero relevance score, indicatin g that local context alone is not informative enough for relevance judg-ment. Global context provides a more detailed and comprehen sive interpretation by synthesizing viewpoints from multiple m embers. When using global context, 323 out of 966 pairs have zero scor e. Compared to local context, the global representation reduc es cases of zero by almost half or equivalently, increases cases of no n-zero relevance scores by more than 75%. In this experiment, the av er-age length of local context is 7.9 terms, versus an average of 187.1 terms for global context. In the remaining experiments, we u se the global context to describe domain-specific reputation and e stimate the inter-domain similarity.
The baseline performance (PageRank) on the three evaluatio n metrics introduced in Section 4.2 is shown at the top of Table 4. Figure 3: Precision@10 performance on GOV as the combina-tion of IR and importance scores are varied.
 Below the baseline we present performance of our model with d if-ferent configurations. All but one result is better than the b aseline. Thus we conclude that our model certainly has the potential t o im-prove search quality. HTR(EC) gets the best performance on all measures. We also find that using  X  X ategory level X  to determi ne do-main relevance outperforms or matches  X  X erm level X  measure ment in all cases.

Table 4 additionally shows (when using  X  X ategory level X ) th at us-ing  X  X xtendedAnchor X  to represent context exhibits better perfor-mance than using  X  X ulltext X , while the performance of  X  X nch ort-ext X  lags behind. One possible reason is that the classifier p erforms poorly on short documents (i.e., anchortexts) since they ar e less informative and distinguishable, which may bring inaccura cy into domain identification and relevance measurement. Expandin g the anchortext with surrounding information seems to improve p erfor-mance to be even better than using  X  X ulltext X .
 Notably, when using  X  X ulltext X  representation, only a sing le unit is generated for each page because every outgoing link s hares an identical contextual representation X  X he current page X  s full con-tent. Since the outgoing targets cannot be discriminated in this case, the authority score is always equally divided among them, co ntra-dicting our intuition. Under this configuration, the HTR mod el de-generates into our previous model CommunityRank, and regar dless of whether in  X  X ategory level X  or  X  X erm level X  measurement, they exhibit the same performance as CommunityRank. Compared to  X  X ulltext X ,  X  X xtendedAnchor X  provides a more efficient and spe-cific way to represent hyperlink contexts.

After considering both quality and efficiency issues, we cho ose to use  X  X xtendedAnchor X  to represent contexts in the follow ing ex-periments.

In the following experiments, we compare the best performan ce of our model, with the other five rankers: PageRank, Communi-tyRank, Topical PageRank, Topic-Sensitive PageRank and In telli-gent Surfer.
 Table 4: Ranking performance of different HTR variations on GOV. Figure 4: Comparison of overall performance for GOV data.
We first conduct a parameter study to investigate how differ-ent weights for importance and relevance scores will affect rank-ing systems X  performance. Figure 3 shows the Precision@10 a s is varied for the four ranking approaches, where is the weight of BM2500 score in the combination of text and authority. As can be seen, HTR curve is almost always equal to or above other curves in the graph, showing that our approach generally out per-forms other approaches. All curves converged to the baselin e when is 1, which corresponds to the performance of BM2500. In GOV dataset, for each approach, we tune the combining parameter for the best P@10 and output its results with this optimal combin ation as final results. In contrast, for experiments on WebBase, we fix the weight of IR score as 0.8 to save the cost of manual evaluat ion across different values of .

Figure 4 shows the overall performance comparison. HTR out-performs other approaches on all metrics. An observation is that IS does not work well on TREC data, as it performs even more poorly than PageRank. To determine whether these improveme nts tests to compare HTR with all other approaches. A t-test show s that HTR significantly exceeds both baseline and IS at a 90% co n-fidence level.
From experiments conducted on the TREC dataset, we drew the conclusion that using  X  X xtendedAnchor X  to represent co ntext provides appropriate descriptions and a significant cost sa vings compared to using full content. For WebBase, we only use  X  X x-tendedAnchor X  to represent context, but we still compare th e two different options for relevance measurement:  X  X ategory le vel X  and  X  X erm level X , as presented in Table 5. The baseline performa nce is listed in the top row.

Again, both the results presented in Table 5 are better than t he baseline. HTR(ET) gets the best performance by outperforming the baseline by 10.8% on P@10 and 5.3% on NDCG@10. In contrast to the results shown in GOV dataset,  X  X erm-Relevance X  optio n out-performed  X  X ategory-Relevance X  on WebBase.
 Table 5: Ranking performance for different HTR approaches on WebBase.
 Figure 5: Comparison of overall performance on WebBase data.

Figure 5 shows the overall performance comparison. HTR out-performs the other approaches on both metrics. Again we per-formed t-tests to compare HTR with all the other approaches. A t-test shows that HTR significantly outperformed most appro aches with a confidence level of at least 90% except for Intelligent Surfer. However, Intelligent Surfer is quite expensive, since it ne eds to be calculated for each dictionary term, while our model, like P ageR-ank, only need to calculated once. In addition, different fr om HTR X  X  consistent superiority on GOV and WebBase, Intellige nt Surfer shows drastically different performance on the two d atasets, from the worst to nearly the best.
From the above experiments, we find that  X  X erm level X  rel-evance measurement outperforms  X  X ategory level X  measurem ent on WebBase, but not on GOV. Intuitively, queries in WebBase are broad and have lots of relevant documents, while queries in TREC are specific with only 10.32 relevant documents on aver-age. As a result, there are different policies for  X  X arrow X  q ueries and  X  X road X  queries. On one hand, we expand the similarity ju dg-ment from term-level to category-level for the purpose of in cluding more potential candidates for the  X  X arrow queries X  on GOV; o n the other hand, we focus on term-level to refine our search for  X  X r oad queries X  used on WebBase. Some intermediate form, such as a finer-grained categorical representation, might be suitab le for both scenarios, but is left for future work.

Since text vector space is sparse, it is no surprise that two r eputa-tion domains may not have significant overlap in text; on the o ther hand, even if two domains fall into the same broad category, t he de-gree of relevance varies from case to case. A possible compro mise is to combine the two relevance measurements so that we can de -cide whether the source and target are relevant based on  X  X at egory Level X  results and further find out how relevant they are by ex am-ining the textual similarity at  X  X erm level X . In the future, we plan to explore a variety of ways to combine the two measurements.
Intelligent surfer exhibits quite poor performance on GOV dataset. A possible explanation is to note that intelligent surfer only wanders in a term-specific subgraph consisting of pages cont aining the particular term. Given a small dataset like GOV, it X  X  har d to expect that such a graph will be well-connected and amenable to link analysis. Based on our statistics, the average density (links per node) of term-specific subgraphs in GOV (for terms in the 5 0 queries) is 3.11 versus 16.5 in WebBase.
In this paper we have proposed a novel ranking method that ex-amines link contexts to divide the normal web graph into a fine r-grained domain-based subnode graph. Our approach associat es hyperlinks with particular reputation domains/topics and weights them with importance, so that multiple topical authority flo ws are propagated through the web graph heterogeneously. Experim en-tal results on two real datasets indicate that this approach is con-sistently promising in improving search engines X  ranking p erfor-mance.
 This work was supported in part by a grant from the National Sc i-ence Foundation under CAREER award IIS-0545875. We also thank TREC and Stanford University for access to their colle ctions. [1] R. Andersen and K. J. Lang. Communities from seed sets. In [2] D. Cai, X. He, J.-R. Wen, and W.-Y. Ma. Block-level link [3] S. Chakrabarti, B. E. Dom, D. Gibson, J. M. Kleinberg, S. R . [4] J. Cho, H. Garcia-Molina, T. Haveliwala, W. Lam, [5] G. W. Flake, S. Lawrence, and C. L. Giles. Efficient [6] T. H. Haveliwala. Topic-sensitive PageRank. In Proc. of the [7] J. Hirai, S. Raghavan, H. Garcia-Molina, and A. Paepcke. [8] K. Jarvelin and J. Kekalainen. IR evaluation methods for [9] R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins. [10] A. K. McCallum. Bow: A toolkit for statistical language [11] L. Nie, B. D. Davison, and X. Qi. Topical link analysis fo r [12] L. Nie, B. D. Davison, and B. Wu. From whence does your [13] L. Nie, B. D. Davison, and B. Wu. Ranking by community [14] The dmoz Open Directory Project (ODP), 2008. [15] L. Page, S. Brin, R. Motwani, and T. Winograd. The [16] S. K. Pal and B. L. Narayan. A web surfer model [17] D. Rafiei and A. O. Mendelzon. What do the neighbours [18] D. Rafiei and A. O. Mendelzon. What is this page known [19] M. Richardson and P. Domingos. The Intelligent Surfer: [20] G. O. Roberts and J. S. Rosenthal. Downweighting tightl y [21] S. E. Robertson. Overview of the OKAPI projects. Journal of
