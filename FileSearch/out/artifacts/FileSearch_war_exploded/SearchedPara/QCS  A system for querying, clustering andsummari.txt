 1. Introduction
Information retrieval (IR) systems provide users with a vast amount of reference material. Along with this tremendous access comes the challenge of effectively presenting the user with relevant information in response to a query. When using an IR engine to search through electronic resources, simple queries often return too many documents, including many that are not relevant to the intended search. For instance, there are several million documents on the World Wide Web pertaining to  X  X  X ichael Jordan. X  X  Most of these concern the basketball star, so it is difficult to find information about the television personality, the jazz musician, the mathematician, or the many others who share that name. It would be useful to have a system that could over-come this limitation.

One approach is to cluster the documents after retrieval and present a synopsis of each cluster so that a user can choose clusters of interest. This is the motivation for our Query, Cluster, Summarize (QCS) system, which performs the following tasks in response to a query: retrieves relevant documents, separates the retrieved documents into clusters by topic, and creates a summary for each cluster.

Our implementation of the QCS system partitions the code into portable modules, making it easy to exper-iment with different methods for handling the three main tasks listed above. In our current implementation of the QCS system, we use existing software libraries for each task. Throughout this paper, we discuss our choices for each of the modules used, but note that it is possible to exchange individual modules with other methods.
Our goal in this work is to produce short summaries ( 100 words) for clusters of documents organized by topic and listed in descending order of relevance to a user X  X  query.

Many existing retrieval systems focus on organizing individual documents related to a query by using one or more ranking algorithms to order the retrieved documents. For example, Google uses its hyperlink analysis algorithm called PageRank to order documents retrieved from web-based collections ( Google, 2006 ). The combination of link analysis and text-based topic analysis employed by the ExpertRank (Teoma) algorithm extractions of the documents containing one or more of the query terms are displayed to users.
Examples of retrieval systems employing clustering algorithms for organizing sets of retrieved documents include Velocity/Clusty ( Vivisimo, 2006 ), Infonetware/RealTerm ( Infogistics, 2001 ), WiseNut ( LookSmart, 2006 ), Accumo ( Accumo, 2006 ), iBoogie ( CyberTavern, 2006 ), and the KartOO and Ujiko systems ( KartOO, 2006 ). These systems organize the documents into clusters and generate a list of keywords associated with each cluster. The latter two systems also present graphical representations of the resulting clusters. As with the retrieval systems above, these systems also present document extractions containing one or more query terms; the only summary presented is a list of keywords.

Much of the recent research on automatic text summarization systems is available in the proceedings of the 2001 X 2006 Document Understanding Conferences. 1 The focus of these conferences is the evaluation and dis-cussion of summarization algorithms and systems in performing sets of tasks on several types of document collections. Several tasks included in previous DUC evaluations focused on multidocument summarization for clusters of documents, and our participation in these evaluations led to the development of QCS. Previous work on using a combination of clustering and summarization to improve IR is summarized in
Radev, Fan, and Zhang (2001) . Of existing IR systems employing this combination, QCS most resembles the NewsInEssence system ( Radev, Blair-Goldensohn, Zhang, &amp; Raghavan, 2001 ) in that both systems can produce multidocument summaries from document sets clustered by topic. However, NewsInEssence is designed for IR from HTML-linked document sets while QCS has been designed for IR from generic docu-ment sets.

Another system that leverages clustering and summarization for information organization similarly to QCS is the Columbia Newsblaster system ( McKeown et al., 2002 ). Newsblaster, like NewsInEssence, is a web-based system which crawls news websites and then clusters and summarizes the news stories, but it does not currently
Beunaga, and Go  X  mez-Hidalgo (2004) , whose study showed increases in user recall of retrieved information when clustering and summarization were included in the output of the IR system.
 We have used QCS for information retrieval in two information domains: biomedical abstracts from the US National Library of Medicine X  X  MEDLINE database, discussed in Dunlavy, Conroy, O X  X eary, and O X  X eary (2003) , and newswire documents from the 2002 X 2004 DUC evaluations, discussed here.
In Section 2 , we discuss our choices for each of the components of the QCS system. An example of use of the QCS system is presented in Section 3 . Section 4 presents results of experiments evaluating some of the com-ponents of the implementation, and we conclude in Section 5 . 2. The QCS system
QCS is a collection of software modules developed in the languages C and C++ and tested under the oper-ating systems SunOS 5.8 (Solaris 8) and Linux (kernel v2.4). Preprocessing tools for all QCS data, including processing of the data passed from one module to another, were developed in the Perl language. QCS has been developed as a client-server application, and the implementation took approximately 6 person-months of full-time effort.

In this section we describe the components of our system: document preprocessing, the representation of documents and queries, and the querying, clustering, and summarization of documents. 2.1. Document preprocessing
In preprocessing a document set for use with QCS, we convert the documents to a standardized format, determine the parts of speech of all words, detect and mark the sentence boundaries, classify sentences by their content, and develop a compact representation for the document information.
 The resulting data is stored for use by any of the QCS modules.

If not already in SGML format, documents are converted into SGML-encoded documents, with start and end tags around each part of the text. For example, the tags  X  DOC  X  and  X  /DOC  X  are placed at the beginning and end of each document.

Determining the parts of speech for document terms and sentence boundary detection is performed primar-ily using a probabilistic part-of-speech tagger and sentence splitter based on a combination of hidden Markov and maximum entropy models ( Mikheev, 2000 ). The default models, trained on the Brown corpus ( Francis &amp;
Kucera, 1982 ), are used in the current implementation of QCS. This method was chosen due to its ability to handle the two most crucial preprocessing tasks required by the QCS system without modifications and for its proven performance in performing part-of-speech tagging and sentence boundary detection ( Mikheev, 2000 ).
An important part of preprocessing the data for use in the summarization module of QCS is assessing the value of the content of each sentence based on the role of that sentence in the document. Thus we tag each sentence as a candidate for extract summaries ( stype = 1), not a candidate but possibly containing useful key terms or phrases ( stype = 0), or containing no useful information ( stype = 1). For example, in DUC newswire documents, sentences within tags such as  X  TEXT  X  ,  X  LEADPARA  X  , etc., are assigned stype = 1, those stype = 1. Note that the choice for these mappings is heuristic X  X ased on our manual inspection of several documents of each type X  X nd may need to be amended for other document sets. The complete set of stype mappings used in the version of QCS reported here are listed in Dunlavy, O X  X eary, Conroy, and Schlesinger (2006) .

Choosing to embed information (i.e., the stype of each sentence) in the document itself instead of creating a processing module in the summarization algorithm allows for the flexibility of using the information through-out affecting the implementation of the summarization module.

Currently, QCS uses a vector space model ( Salton, 1989 ) for document representation in the querying, clus-tering, and summarization modules. In such a model, a set of m documents containing n distinct terms can be represented by an m  X  n term-document matrix A . Terms in QCS are all the (white space delimited) words in a document with the exception of a pre-designated list of stop words. The list of stop words currently used in
QCS is the one provided with the implementation of the query module. The value of an entry of the matrix A is a product of three scaling terms: are chosen so that the value a ij best represents the importance (or weight) of term i in document j for a par-the list of scaling options available in QCS. The standard tf.idf (term frequency local weighting, inverse doc-ument frequency global weighting) scheme, along with normalization, is used in the examples presented in this paper.
 The indexing of the terms and documents is performed in QCS using the General Text Parser ( GTP )( Giles,
Wo, &amp; Berry, 2003 ). GTP was chosen for use in QCS since it includes tools for parsing documents and repre-senting them in a vector space model along with a retrieval tool that is currently used in the querying module.
Minor changes were necessary to provide an interface to the term-document matrix consistent with that needed by the clustering module.

Since parsing and indexing are too computationally expensive to be done in real-time, they are performed once as a preprocessing step for a static document set or during system downtime for a dynamic set. 2.2. Querying documents
The method used for query-based document retrieval in QCS is Latent Semantic Indexing (LSI) ( Deer-wester, Dumais, Landauer, Furnas, &amp; Harshman, 1990 ). LSI attempts to reveal latent relationships caused by term ambiguity, while preserving the most characteristic features of each document. It does this by approxi-mating the matrix A by a rank-p matrix A p computed using a singular value decomposition (SVD) of A .
We represent a query using a query vector q , with m components, just as a document can be represented by terms than an average document) and does not necessarily use the same scaling scheme. For comparing query vectors and document vectors in LSI, the query vector is projected into the p -dimensional subspace spanned by the columns of A p , and we denote the projected vector as q
The relevance of a document to a query is measured by the cosine similarity score, s , between q column of A p corresponding to that document. For example, the relevance of document j to the query is com-puted as where  X  A p  X  j is the j th column of A p . Note that 0 6 s
The querying module in QCS is called GTPQUERY and is part of the GTP system. GTPQUERY parses the query (using the method used to parse the document set), normalizes the resulting vector, and calculates the cosine similarity scores. A very helpful feature implemented in GTPQUERY is the ability to use different low-rank approximations without having to recompute the SVD. Since we store the components of the
SVD rather than the reassembled low-rank approximation, a user is able to choose the rank of the approxi-mation to be used for each query up to the number of singular values computed during the SVD computation by GTP . If all of the singular values are stored, the user has the option of performing queries ranging from exact matches (using all of the singular values) to extremely conceptual matches (using just a few singular val-ues). In the current implementation of QCS, all of the singular values are computed and stored for each doc-ument collection. Note that for larger collections, though, the number of singular values computed may be limited by the computational resources available.

The documents matching a query can be chosen by specifying either the number of documents to be retrieved or a cutoff for the query score. In the current implementation of QCS, 100 documents are returned in order to have a large enough subset of documents to guarantee good clustering and summarization output.
The potential downside to this is that, depending on the specific query, many of the retrieved documents may have very low query scores. This may need to be adjusted based on the document set and/or distribution of query scores. 2.3. Clustering documents
In QCS, we use the information derived from the query processing phase to cluster documents into a variable number of clusters, each representing a single topic. Throughout this section, we assume that the querying module has identified a set of N documents for further processing.

Our clustering of the N documents is a partitioning into k disjoint subsets, p similarity of the N document feature vectors, f d 1 ; ... ; d where d i is assumed to be normalized (i.e., k d i k X  1) and c documents:
We want to choose the clusters p j to maximize the sum of the coherence functions. This is one of the classical approaches to k -means clustering and can be shown to be equivalent to minimizing the radii of the clusters.
To perform the clustering in QCS, we currently use the spherical k -means algorithm ( Dhillon &amp; Modha, 2001 ) employing first variation and splitting ( Dhillon, Guan, &amp; Kogan, 2002 ). This is an iterative method for maximizing the coherence functions of document feature vectors and includes efficient computation of fea-ture vector similarities (the main computational bottleneck in many implementations of k -means algorithms) and the ability to choose a range for the number of clusters into which the feature vectors will be partitioned.
Comparisons of this clustering algorithm to the classical k -means algorithm on large document sets indicate ( Dhillon, Fan, &amp; Guan, 2001 ).

Clustering can be a computational bottleneck unless a good initial guess is provided. In QCS, we use 5 ini- X  i  X  1 ; ... ; 5  X  , containing documents with scores satisfying where s max and s min are the maximum and minimum scores, respectively, of the documents returned from the query module. Although the complexity of the clustering algorithm is O  X  nz k j  X  for j iterations of finding k does not work well for all sets of documents, and the best use of the similarity scores in seeding the initial clusters remains an open question.
The clustering of documents in QCS is performed using GMEANS v1.0 ( Dhillon et al., 2002 ). Only slight modifications to the original code were necessary to insure that the interface to the data in the vector space model matched both the query and summarization modules. The GMEANS software includes several distance measures; only spherical k -means has been tested extensively in QCS. The other distance measures are Euclid-ean distance, Kullback X  X eibler divergence, and diametric distance. More testing on the use of these distance measures will help determine their usefulness in producing good clusters for use in summarization.
The list of documents in each cluster is passed to the summarization module. 2.4. Summarizing documents and clusters
The summarization module in QCS is based on the work of Conroy and O X  X eary (2001) and its implemen-tation for the DUC 2003 evaluation ( Dunlavy et al., 2003 ). The algorithm proceeds in two steps: trimming sentences and then choosing the sentences to include in a summary. The sentence trimming algorithms are the work of Schlesinger, first documented in Dunlavy et al. (2003) . 2.4.1. Choice of summary sentences
The choice of sentences to include in the summary is done in two phases; single document extract summa-ries are first produced for each document in the cluster, and then sentences from these summaries are consid-ered for inclusion in the summary of the document cluster.
 Single document summaries are produced using a hidden Markov model (HMM) ( Baum, Petrie, Soules, &amp;
Weiss, 1970; Rabiner, 1989 ) to compute the probability that each sentence is a good summary sentence. The highest probability sentences are chosen for the summary. The 13-state HMM shown in Fig. 1 , built to extract six primary sentences and an arbitrary number of additional supporting sentences, is used to compute these probabilities. Currently, this 13-state HMM and an additional 5-state HMM (3 primary sentence states and 2 supporting sentence states) are used in QCS for different document collections. The ability to use a dif-ferent extraction model for each document collection allows for the application of QCS to a wide range of document formats and genres. The number of states for the HMM was determined empirically using DUC 2003 Novelty data, with human sentence extracts chosen to match the NIST-provided extracts. One of two variants of the pre-processing was applied to the DUC data. These variants were in turn applied to the training data and a corresponding HMM was built.
 The HMMs in QCS use features based upon  X  X  X ignature X  X  and  X  X  X ubject X  X  terms occurring in the sentences.
The signature terms are the terms that are more likely to occur in the document (or document set) than in the corpus at large. To identify these terms, we use the log-likelihood statistic suggested by Dunning (1993) and first used in summarization by Lin and Hovy (2002) . The statistic is equivalent to a mutual information sta-tistic and is based on a 2-by-2 contingency table of counts for each term. The subject terms are those signature terms that occur in sentences with stype = 0, e.g., headline and subject leading sentences.
Both the query and cluster affect the signature and subject terms selected. The term counts used by the mutual information statistic are the number of times each term appears in the set of relevant documents and in background documents. For QCS we define relevant documents as those in the cluster to be summarized and background documents as those returned by the querying module but not appearing in the cluster to be summarized. Thus, the mutual information score will select signature (and subject) terms which are novel to the cluster relative to the set of documents returned by the query.

The HMM features are log  X  n sig  X  1  X  , where n sig is the number of signature terms in the sentence, log  X  n subj  X  1  X  , where n subj is the number of subject terms in the sentence, the position of the sentence in the document, built into the state-structure of the HMM.
The two term-based features are normalized component-wise to have mean zero and variance one. In addi-tion, the features for sentences with stype =0or 1 are coerced to be 1, which forces these sentences to have an extremely low probability of being selected as summary sentences.

Multidocument summaries are created for each cluster by choosing a subset of the sentences identified by the HMM. If we want a summary containing w words, we consider the highest probability sentences from doc-uments in that cluster, cutting off when the number of words exceeds 2 w . We form a term-sentence matrix, B , similar in structure to the term-document matrix A used in the querying and clustering modules, containing a column for each of these sentences. The columns of B are scaled so that the Euclidean norm equals the prob-ability assigned to the sentence by the HMM.

In order to remove redundant sentences, a pivoted QR algorithm is applied to the scaled term-sentence matrix. We first choose the sentence whose corresponding column in B has maximum norm. Then, within the matrix B , we subtract from each remaining column the component in the direction of the column for this chosen sentence. This process is iterated until the number of words in the collection of chosen sentences exceeds the desired length w . For more details, see Conroy and O X  X eary (2001) . 2.4.2. Sentence trimming
The HMM tends to select longer sentences due to the features currently used. Because of this, for a 100-word summary, the pivoted QR algorithm typically selects 2 or 3 sentences from all those first selected by the HMM.
We hypothesized that if we could shorten sentences, by removing the less important information, we could increase the number of sentences in a summary and, therefore, add additional information to the summary.
As an inexpensive alternative to full parsing and comprehension, we identified trimming patterns using  X  X  X hallow parsing X  X  techniques, keying off lexical cues based on part-of-speech (POS) tags in our preprocessed data.

The following eliminations were implemented: lead adverbs and conjunctions; gerund phrases; restricted relative-clause appositives; intra-sentential attribution.

We define a token to be a white-space delimited word with all punctuation removed and use the simple heu-the elimination is not performed.

Lead adverbs and conjunctions include POS-tagged adverbs that are comma-delimited from the remainder of the sentence along with conjunctions such as  X  X  X nd X  X  and  X  X  X ut X  X . They do not usually add substantial infor-mation and often hinder the flow of the summary unless the preceding sentence of the document is also selected.
 Gerund phrases often comment on, rather than advance, a narration and therefore tend to be incidental. Restricted relative-clause appositives usually provide background information which could be eliminated.
While attributions can be informative, we decided that they could be sacrificed in order to include other, hope-fully more important, information in the summary.
 An example of each of the three phrase/clause eliminations is given in Fig. 2 .

Our DUC 2003 submission, which used the same summarizer as in QCS, used these phrase/clause elimina-tions in a post -processing mode. Sentence selection was first made by the HMM and QR algorithms. These sentences were then trimmed, and one or more sentences were added if space was made available. Based on the DUC 2003 results, we hypothesized that we would see added benefit if we applied these transformations as a pre -processing step applied to all sentences in the documents, before summary sentence selection was per-formed. This was tested in DUC 2004 and results were superior to the submission using the post-processing version. See Conroy, Schlesinger, Goldstein, and O X  X eary (2004) for details.

We also experimented with removing two types of sentences. The first type of sentence is one that begins with an imperative. This type is not currently removed since a lead imperative so rarely occurred it was not worth looking for it. The second type of sentence is one containing a personal pronoun at or near the start.
While these sentences negatively impact a summary X  X  readability, eliminating them adversely affected the qual-ity of the summary X  X  information content. We are working on a solution to the anaphora problem to resolve this issue. 2.5. The QCS client-server architecture
A screen shot of the QCS user interface is presented in Fig. 3 . There are three main frames in the interface: the query form, the navigation bar, and the results frame.

The query form contains an input field for entering a query and a field for selecting the document set on which to perform the query. Currently, the document sets from the 2002 X 2004 DUC evaluations and a Med-line document set are available for online use.

The navigation bar contains links to the documents and is organized to reflect the output from the querying, clustering and summarization modules. For each cluster, query scores and document names are given, with hyperlinks to the text of the documents in the  X  X  Q  X  X  subsection. In the  X  X  C  X  X  subsection, links are given to the documents containing the sentences used in the multidocument summary, along with the index of the sen-tence within the original document. Lastly, in the  X  X  S  X  X  subsection, a link to the multidocument summary for the cluster is presented.

The results frame displays information requested through the navigation bar. The default output is multidoc-ument summaries (also chosen using the  X  X  S  X  X  links). Other options include the text of individual documents (chosen using the  X  X  Q  X  X  links) or individual documents with summary sentences highlighted (chosen using the
The client in QCS consists of dynamically-created HTML pages. These pages are generated by Java servlets that are deployed via an Apache Tomcat Java Server (v.4.1.12). The interface between the QCS server (con-sisting of all of the C/C++ code) is handled using the Java Native Interface. This allows the computationally intensive code to be developed in C and C++, which can be highly optimized on a given hardware platform, while still allowing for the greatest amount of portability for the user interface.

The current implementation of QCS can be found at http://stiefel.cs.umd.edu:8080/qcs/ . 3. Example of the QCS system
We present an example of the entire QCS system from the standpoint of the user. The example uses the query hurricane earthquake in finding documents in the DUC 2002 document collection. The DUC 2002 col-lection consists of 567 documents and the QCS preprocessing modules identified 7767 unique terms across the collection.
 with stype = 0) from each document. In this example, a rank-50 approximation of A (i.e., p = 50) was used in computing the similarity scores; the choice of p = 50 is ad hoc, reflecting about 90% dimensionality reduction in this case. Clearly, QCS has found several documents about hurricanes. Yet there are no clear examples of documents relating to earthquakes in these documents. Most importantly, some of the subject sentences are this kind of information (as is typically the case with query tools), a user would have many documents to read and no idea whether or not the high-ranking documents contained redundant information.

The results of clustering the 100 top scoring documents returned by the querying module using an upper limit of 10 clusters are presented in Table 2 . The clustering algorithm split the original 5 seed clusters into 10 clusters, and the table shows the number of documents and the mean query score for each cluster. For this example, a majority of the documents are in the 5 clusters with the highest mean query scores; this is repre-sentative of most of our tests and may be biased by our initial seeding scheme. However, it is unclear if and how this behavior would change if a different initial cluster seeding is used.

Table 3 presents the subject sentences of the top 3 scoring documents in each of the top 3 clusters, illustrat-
Hurricane Gilbert and those in the third cluster relate to insurance claims associated with hurricanes. How-ever, from the subject lines alone, is difficult to determine the focus of the documents in the second cluster; they could relate to forecasting or a specific hurricane which hit a historic city or something else. Fig. 4 shows the multidocument summaries for the top 5 scoring clusters. We see that the subject lines in trated by the summaries. From the summary for the second cluster, we see that the documents in that cluster focus on Hurricane Hugo. Note that the name Hugo did not appear in the subject lines of the top query results multidocument summary. Moreover, the name Hugo only appears in the subject line of the document in the second cluster which has the lowest query score (47).

Summaries for the top 5 clusters are shown in Fig. 4 to illustrate the ease of finding information about potential savings to the user in using QCS in this example is that only 3 summaries would need to be read before finding information about earthquakes (instead of 38 subject lines or even full documents). Further-more, the documents related to earthquakes are clustered to differentiate between those related to an earth-quake in California (cluster 4) and those related to one in Iran (cluster 5).

The flow of the summaries is representative of the output of QCS for the queries tested. They do not read like human-generated summaries, but the hope is that they are sufficient to inform a user of the content of the documents contained in each cluster. Note that in some cases, the summaries can be misleading, most notably for clusters containing documents covering two or more related but distinct topics.

This example illustrates the usefulness of providing document clusters and cluster summaries in presenting query results to a user. 4. Experiments
In this section, we describe the results of two sets of experiments performed to test QCS on various docu-ment collections. We first present the data used in the experiments, then describe our framework for evaluating the performance of QCS and finally present the results of several tests performed within this framework. Tests were performed on a Sun Ultra60 with a 450 MHz processor and 512 Mb of RAM running Solaris 8. Further results for testing QCS, including timings and additional retrieval results, can be found in Dunlavy et al. (2006) . 4.1. Data used in the experiments The document collections used in the experiments presented here are from the 2002 X 2004 DUC evaluations.
Specifically, the collections from the DUC evaluations used in the tasks focused on generating generic 100-word multidocument summaries were used. A summary of the collections is presented in Table 4 . 4.2. Experiments with QCS on small topic-related document collections
The first set of experiments focused on the interplay between the querying, clustering and summarization modules in QCS. We evaluated the system measuring the effect of replacing a machine generated component with the  X  X  X old-standard X  X  equivalent.

We evaluated both single and multidocument summaries. In each case we compared machine summaries with human model summaries using the Recall-Oriented Understudy for Gisting Evaluation ( ROUGE ) v1.5.5 summarization evaluation tool ( Lin, 2004 ). ROUGE scores range from 0 to 1 and reflect the similar-ity X  X ith higher score reflecting more similarity X  X etween two summaries. scores are based on the overlap of unigrams and bigrams (using words as tokens), respectively, between auto-matically-generated summaries and human-generated summaries. The ROUGE-1 score solely reflects the over-lap in vocabulary between two summaries, whereas the ROUGE-2 score also reflects overlap in phrase choice and to some extent word ordering. 4.2.1. Experiments with single document summaries
We designed an experiment to measure the effects of the clustering algorithm on single-document summa-ries. Recall that the summarization component uses signature terms X  X erms identified as representative of the document X  X nd the performance of the algorithm is greatly influenced by the quality of the signature terms.
The experiment was to compare the quality of the summary when signature terms are taken from  X  X  X round-truth X  X  clusters versus when the clustering information is withheld and the documents are treated in isolation.
For this test, we turned to the DUC02 data sets. These data contain 1112 human model summaries, with approximately 2 summaries per document. In Table 5 , we see that the ROUGE-1 and ROUGE-2 scores are sig-nificantly better when the summarization algorithm is provided the cluster information.
 4.2.2. Experiments with multidocument summaries
The goal of these experiments was to determine whether the best machine-generated summary produced for a given DUC cluster is one using all of the documents for that cluster or a subset of those documents. In the cases where a better summary could be produced with fewer documents, we also ran experiments to determine if QCS is able to generate such summaries by incorporating document querying and clustering into the sum-marization process.

In these experiments, a multidocument summary was produced using the summarization module of QCS for each possible subset of two or more documents from each cluster. Since each DUC collection contained 10 documents, there were a total of 1013 subsets generated for each. Next, several queries for each cluster were generated from the cluster topic descriptions included as part of the DUC evaluations and used to run QCS.
Finally, the output of QCS was compared to the human summaries and summaries generated by the variant of the summarization module in QCS for each year of DUC.

We used the topic descriptions for each cluster provided in the DUC 2003, Task 2 description to generate queries to be used in QCS. Three queries were generated for each cluster using the words from the (1) topic headline; (2) topic headline and seminal event description; and (3) topic headline, seminal event description, and topic explication. Our intent in using these different queries was to simulate a range of queries containing different amounts of information X  X rom an ambiguous query with a few key words to a query reflecting all known information on a particular subject of interest.

To study the effects of cluster size on the quality of summaries produced by QCS, we ran QCS using each of the three queries and allowing up to k =2, ... ,9 subclusters to be formed for each of the DUC clusters. Note that with such small document collections (10 documents for each cluster), the clustering module failed in sev-eral instances where too many singleton clusters were formed (i.e., when k &gt; 5 maximum number of clusters were allowed).

Figs. 5 and 6 present the results of these experiments for the d31033t document collections. In these figures, only the ROUGE-2 scores are reported; other ROUGE scores (along with results of experiments for other clusters) can be found in Dunlavy et al. (2006) .

Fig. 5 presents the ROUGE-2 recall scores for the human (  X  ), summarization module ( s ), and QCS ( d ) summaries (over all runs where k =2, ... ,9 subclusters were formed). The scores appear in descending order of average score from left to right, and include 95% confidence intervals for the machine-generated systems.
Note that there are no confidence intervals for the human summaries since each human summary is scored once per cluster against all other human summaries. To remain consistent with the DUC evaluations, the sum-mary labels appearing along the horizontal axes in the figures correspond to the summary labels used in the
DUC evaluations (A X  X  for the humans and S# for the system number assigned to the variant of the summa-rization module used in QCS that was submitted to DUC) These results suggest that an improvement in sum-mary quality can be made using QCS in place of the summarization module alone; at least one summary returned by QCS has a higher average score than those of the summaries produced using the summarization module. However, the results suggest only marginal improvement, as illustrated in the overlap of the confi-dence intervals for the scores.

Fig. 6 presents the best ROUGE-2 scores as a function of the number of clusters formed in the clustering module of QCS. The dotted lines denote the best score of the summaries generated by the different variants of the summarization module submitted to the DUC evaluations. These results suggest that the number of clusters formed in QCS affects the quality of the summary produced. Although the improved QCS summaries are not generated using the same number of clusters across all of the experiments, the appearance of trends in the scoring data between summary quality and the number (and thus size) of QCS clusters suggests a potential relationship that may be leveraged using QCS. Note that for two clusters of different documents on the same topic (d31033t from DUC03-2 and DUC04), a different number of subclusters leads to the best results: 6 X 7 for
DUC03-2 and 2 X 3 for DUC04. This illustrates the need for a clustering algorithm which allows for an adaptive number of clusters to be formed; the GMEANS clustering algorithm in QCS allows for such adaptivity.

We conclude from this experiment that the clustering of documents used for multidocument summarization can greatly affect the quality of the summary produced. Specifically, determining subclusters (i.e., subtopic detection) is critical for accurately reflecting the information conveyed by a set of documents through auto-matically generated summaries. Furthermore, we have demonstrated that the use of clustering as a preprocess-ing step used before performing automatic summarization can help improve summaries generated. 4.3. Experiments with QCS on a larger diverse document collection
In the second set of experiments, we focused on the effects of querying and clustering on summarization, both independently and in the full QCS system, on a larger collection of documents covering a wide variety of topics. The collection consisted of documents from all clusters in the DUC 2003 Task 4 evaluation data where (1) a topic description was provided, (2) a summary generated using the summarization module was submitted to the DUC 2003 Task 4 evaluation, and (3) four human-generated summaries were provided. There were 28 clusters which met this criteria, resulting in a collection of 625 files.

For each of the 28 clusters, we generated several summaries using four different methods, as well as the method we submitted to the DUC 2003 evaluation, denoted here as S . The first method is the full QCS system.
As in the experiments in the previous section, queries were derived from each topic description. The topic descriptions for the DUC03-4 data included a title, two short descriptions, and a topic narrative. Four queries (4) all topic information. Using the default QCS setup, up to 10 multidocument summaries were generated per query.

The second method, denoted QL , combines the QCS query module and lead sentence extraction to generate one multidocument summary per query. Given a query, a subset of documents is retrieved and ordered by query score. A multidocument summary is then produced using the lead sentence with stype = 1 from each of the top scoring documents until the total number of words in these sentences exceeds 100. As in the exper-the 625 documents. In many of the DUC evaluations, similar lead-sentence summaries have been used as base-lines, representing a summarization approach requiring minimal text and/or natural language processing.
However, since the DUC evaluation data consists of newswire documents, such baseline summaries have per-formed fairly well compared to many more sophisticated approaches in several of the DUC evaluations ( Dang, 2005; Over &amp; Yen, 2004 ).

The third method, denoted QS , is similar to the QL method, but uses the QCS summarization module instead of lead-sentence extraction to generate a summary. Again, given a query, a subset of documents is retrieved and ordered by query score. The top scoring document and those documents with query scores within 30% of the top score are collected into a cluster and a single multidocument summary is generated for this cluster using the QCS summarization module.

The final method, denoted CS , combines the clustering and summarization modules from QCS to generate several multidocument summaries. Given a cluster of n documents, the clustering module generates a maxi-mum of k =min{10, n /2} subclusters starting with 2 randomly seeded initial subclusters. Multidocument sum-maries for each of the resulting k subclusters are then generated using the QCS summarization module. Fig. 7 presents the ROUGE-2 recall scores for all of these systems for each of the 28 DUC clusters. Other
ROUGE scores for these experiments can be found in Dunlavy et al. (2006) . For QL ( h ) and QS (+), four sum-maries associated with each of the DUC clusters were produced (one for each query); for CS ( ), an average of 9.14 summaries were produced per DUC cluster (due to the varying number of subclusters generated); and for
QCS ( d ), an average of 33.5 summaries associated with each DUC cluster generated (using the four summa-ries and generating up to 10 clusters). The results presented in Fig. 7 only show the top scoring summary for each of the QCS, QL, QS, and CS methods.

Table 6 presents the results of pairwise comparisons of the top scoring summaries generated by the five methods. The entry in the row labeled QCS and the column labeled S , for instance, indicates that S had a better ROUGE-2 score on 57% of the 28 instances. There is much variability in scores across the different experiments, as shown in Fig. 7 . However, the pairwise comparisons of methods using ROUGE-1 and
ROUGE-2 suggest the following overall performance ordering: S , CS, QCS, QS, and QL. Although QCS is not the top-performing method throughout all of the experiments, we note that it outperforms S and CS at least 25% of the time using any of the ROUGE scores and it outperforms S 43% of the time evaluated with the ROUGE-2 score. Furthermore, both S and CS had human intervention to obtain the relevant documents.

We conclude from these experiments that QCS performs well in producing summaries for automatically generated clusters of documents, rivaling summaries generated using manual processing of data. The benefit of using QCS over such methods is that it is a fully automatic system for document retrieval, organization, and summarization. 5. Conclusions
QCS is a tool for document retrieval that presents results in a format so that a user can quickly identify a set of documents of interest. The results include a multidocument summary of each cluster of documents, a sum-mary of each individual document, a pointer to each document, and pointers to documents from which the multidocument extract summary was derived. Results of using QCS on the DUC document set illustrate ing the quality of the summaries.

The QCS system has been developed as a completely modular tool, enabling new methods to be integrated into the system as improvements are made in the areas of query, clustering, and summarizing documents. It has been developed as a client-server application in which the client can be run from any platform that can process HTML documents, which currently includes most major computing platforms.
 Acknowledgements
We thank the authors of LT TTT , GTP , and GMEANS for the use of their code, Timothy O X  X eary for his assistance with the MEDLINE data set used in QCS, and Tamara Kolda for her helpful suggestions during the preparation of this manuscript. Daniel Dunlavy was supported in part by the Applied Mathematics Research program of the Office of Advanced Scientific Computing Research of DOEs Office of Science and by
Sandia National Laboratories, a multiprogram laboratory operated by Sandia Corporation, a Lockheed Mar-tin Company, for the United States Department of Energy under contract DE-AC04-94AL85000. Dianne O X  X eary was partially supported by NSF Grant CCR-0204084 and CCF-0514213.
 References
