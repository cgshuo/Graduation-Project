 Similarity computation between pairs of objects is often a bottleneck in many applications that have to deal with mas-sive volumes of data. Motivated by applications such as collaborative filtering in large-scale recommender systems, and influence probabilities learning in social networks, we present new randomized algorithms for the estimation of weighted similarity in data streams.

Previous works have addressed the problem of learning binary similarity measures in a streaming setting. To the best of our knowledge, the algorithms proposed here are the first that specifically address the estimation of weighted similarity in data streams. The algorithms need only one pass over the data, making them ideally suited to handling massive data streams in real time.

We obtain precise theoretical bounds on the approxima-tion error and complexity of the algorithms. The results of evaluating our algorithms on two real-life datasets validate the theoretical findings and demonstrate the applicability of the proposed algorithms.
 H.2.8 [ Database Management ]: Database Applications X  Data mining Theory; Algorithms; Experiments Recommender systems; Viral marketing; Collaborative fil-tering; Sketching; Streaming algorithms
Similarity computation is a basic primitive in many data mining algorithms, ranging from association rule mining to c  X  clustering. However, for many Big data applications, sim-ilarity computation can be prohibitively expensive, hence necessitating the need for scalable methods for similarity es-timation in massive datasets. As such the streaming model of computation has become extremely popular over the last decade, because it can handle massive data through sequen-tially processing the input. Further, streaming algorithms that need only one pass over the input open the door to real-time stream processing which considerably extends the applications domain. For example, Locality-sensitive hash-ing [17 ] techniques have found application in numerous re-search areas, see [ 11 , 28 , 30 ] for concrete examples.
Somewhat surprisingly however, similarity estimation in streams has received less attention for cases when the un-derlying similarity measures are weighted. Motivated by applications in recommender systems and viral marketing, in this work we consider how to estimate the weighted simi-larity between real vectors revealed in a streaming setting.
Modern recommendation systems work with huge volumes of data which necessitates the scalable processing of the in-put. As such, traditional algorithms that work in an of-fline fashion are often not applicable since we cannot afford to load the full input in memory. Similarly, in online so-cial networks, users activity results in huge amounts of data that needs to be processed efficiently. In the following we review previous work on stream processing for applications in recommender systems and viral marketing.
 Collaborative filtering is widely applied in recommender systems. The basic idea is to recommend an item i to a user u if i has been highly ranked by users who have similar preferences to u . The similarity between users is defined based on their rating history, and is represented as a (sparse) vector whose dimensionality is the total number of items.
Several different similarity measures have been proposed in the literature, e.g. Jaccard and Cosine similarity, Eu-clidean distance and Pearson correlation. In a series of pa-pers [ 3, 4, 5, 6], Bachrach et al. proposed sketching al-gorithms to estimate the Jaccard similarity between users using min-wise independent hashing [7 ]. Since Jaccard sim-ilarity only applies to unweighted sets, this is a simplication of the problem, whereby we are only interested which items users have rated, not how they have rated them. It is argued that rating itself is an indication of interest in a given prod-uct. However since user ratings indicate the level of interest, i.e. two users may provide conflicting ratings on the same item, in practice, weighted similarity is commonly used [ 23 ]. The state-of-the-art result is [ 5], which details how to com-bine the compact bit-sketches from [24 ] with the method proposed in [ 16 ] that achieve an exponential speed-up of the evaluation time of a  X  X andom enough X  hash function. Fi-nally, in [4 ], an extension of Jaccard similarity to ranking estimation is proposed. (Note however that the extension does not achieve the improved time and space complexity of [5 ].) Influence propogation is widely studied in the context of viral marketing in social networks. Based on the observation that influence propagates through the network, the goal is to detect the influential users that are most likely to determine the overall behavior of the network. Such users may then be targeted during advertisement campaigns or used to pre-dict the success of a campaign before launch. In a seminal work Kempe et al. [21 ] introduced the independent cascade model and presented approximation algorithms for influence maximization in social networks. Under this model, a user u influences a neighbor of hers v with a certain probability p however, the authors assume that propagation probabilities p uv are known in advance.

Goyal et al. [18 ] present methods for inferring influence probabilities. Informally, they define different measures of influence probability, whereby a user u is said to have in-fluenced a user v if an action performed by u is later per-formed by v within  X  time units, for a suitably defined  X  . Note that this approach does not distinguish between how actions are performed. Building upon this work, Kutzkov et al. [ 22] have presented streaming algorithms capable of es-timating the influence probability between users using only a small amount of memory per user. In [ 22], the influence probability is defined as an extension of Jaccard similarity with time constraints, and the algorithm extends min-wise independent hashing to handle time constraints.

Finally, another thread of works has analyzed user behav-ior in online social networks, in terms of social and corre-al. [19 , 20 ] have studied the problem of influence propaga-tion in Social Rating Networks (SRN). The correlational in-fluence between users is computed as a Pearson correlation between the rating vectors.
 We obtain precise theoretical bounds on the complexity of the presented algorithms. An experimental evaluation on real datasets confirms the theoretical findings.
 In Section 2 we present necessary notation and formal def-initions. In Section 3 we present and analyze a weighted similarity estimation algorithm for collaborative filtering in data streams. We extend the considered similarity measures to model influence propagation and present streaming algo-rithms for their estimation in Section 4. Results from ex-perimental evaluation on real data are reported in Section 5. The paper is concluded in Section 6. The k -norm of a vector x  X  R n is defined as k x k k = ( P The 2-norm of x will be denoted as k x k , and the 1-norm as | x | . Let U be a set of m users and I a set of n items. A rating given by user u  X  U on an item i  X  I is denoted as r . A user is described by an n -dimensional real vector. For the i -th entry in u it holds u i = r ui , thus we will also use u i instead of r ui to denote u  X  X  rating on i . The set of items rated by user u is denoted by I u  X  I . We will denote by [ n ] the set { 0 , 1 ,...,n  X  1 } .

We consider following measures for the similarity between two vectors. 1. Jaccard similarity computes the probability that an 2. Cosine similarity. 3. Pearson correlation. We assume familiarity with basic probability theory nota-tion. In the analysis of the algorithm we use Chebyshev inequality defined as follows. Let X be a random variable with expectation E [ X ] and variance V [ X ]. Then A family F of functions from V to a finite set S is k -wise independent if for a function f : V  X  S chosen uniformly at random from F it holds for s = | S | , distinct v i  X  V and any c i  X  S and k  X  N
A family H of functions from V to a finite totally ordered set S is called (  X ,k ) -min-wise independent if for any X  X  V and Y  X  X , | Y | = k for a function h chosen uniformly at random from H it holds we will refer to a function chosen uniformly at random from a k -wise independent family as a k -wise independent function.
Finally, we will say that an algorithm computes an  X  -approximation of a given quantity q if it returns a value  X  q such that q  X   X   X   X  q  X  q +  X  . In the theoretical analysis of the algorithms, we will show results stating that certain approx-imation guarantee is obtained with probability 2/3. Using the Chernoff inequality this probability can be amplified to 1  X   X  for any  X   X  (0 , 1) by running O (log 1  X  ) independent copies of the algorithm in parallel, c.f. [26 ]. Note that 2/3 can be replaced by any constant c &gt; 1 / 2.
 Let S be a stream of triples ( u,r i ,t u ) where u is the user identifier, r i is the rating the user gives to item i , and t is the timestamp of the rating. We assume that each user rates an item at most once. Throughout the paper we will often represent the rating given by u on i as u i instead of ( u,r ui ).
 We will consider the following two problems: 1. Collaborative filtering: Given users u and v , what is 2. Influence propagation: Given users u,v  X  U , compute Before formally describing the proposed algorithms, we first provide an overview of our reasoning. We assume that we can store a sketch for each user X  X  activity. Sketches are com-pact representations of a user X  X  rating history. We observe that if we can estimate the inner product of uv , then we can also estimate cos ( u,v ). This is achieved by comput-ing k u k and k v k in a streaming setting by simply maintain-ing a counter to record the current value of the squared 2-norm of u and v . Utilising AMS sketches for the estima-tion of the inner product, as proposed in [ 12 ], we obtain an  X  -approximation of cos ( u,v ).

The Pearson correlation can be rewritten as cos ( u  X   X  u,v  X   X  v ), where u  X   X  u is the vector obtained from u by replacing u passes over the data are allowed, we can compute in the first pass the values  X  u and in a second pass we can run the cosine similarity estimation algorithm on the updated vectors u  X   X  u . We will show that the linearity of AMS sketches enables us to compute a sketch u  X   X  u in a single pass without knowing  X  u in advance. Before presenting our algorithm, we briefly present how AMS sketching works: AMS sketching AMS sketches were originally designed as an efficient tech-nique for estimating the second moment of a data stream X  X  frequency vectors [1 ]. For an (unweighted) stream S = i ,i 2 ,i 3 ,... over a set of items I , let f i denote the frequency of item i , i.e., the number of occurrences of i in S . Then the second moment of the stream S is defined as F 2 = P i  X  I
AMS sketching works as follows: Let s : I  X  { X  1 , 1 } be a 4-wise independent function. We maintain a variable X , and for each incoming item i , we update X as X += s ( i ). After processing the stream it holds that X = P i  X  I s ( i ) f i 6 = j and s ( i ) 2 = 1. Using that s is 4-wise independent, the variance of the estimator can be bounded by V [ X 2 ]  X  V ( X ) /c 2 , the average of O (1 / X  2 ) random variables X , is an unbiased estimator of F 2 and has variance  X  2 F 2 2 . A standard application of Chebyshev inequality yields that we obtain an  X  -approximation from the average of O (1 / X  2 ) AMS sketches. Finally, it was first observed in [ 12 ] that AMS sketching can be also used to estimate the inner product uv of vectors u,v  X  R m . To do so, we maintain random variables X = P i =1 s ( i ) u i and Y = P we see next, the variance is bounded by V [ XY ]  X  ( k u kk v k ) From this, an estimate of cos ( u,v ) easily follows.
A drawback of AMS sketching is that for each new item in the stream, we need to update all AMS sketches. The Count-Sketch algorithm helps to overcome this by im-proving the update time [ 10 ]. Instead of working with many different AMS sketches, Count-Sketch works with a single hash table. For a new entry u i , one updates the hash ta-hash function h : [ n ]  X  [ k ]. After processing the vectors u,v , their inner product can be estimated as P j  X  [ k ] H u [ j ] H Intuitively, the larger hash table we use, the better estimates we obtain because we have less collisions between items. Pseudocode A weighted similarity estimation algorithm is presented in Figure 1. We assume that the input is a stream S of pairs ( u,r i ) denoting that user u rated item i with a rating r each user, we keep a hash table H u and we run the Count-Sketch algorithm. We process an incoming pair ( u,r updating H u with r i . In addition to the sketches H keep the following arrays: C  X  counting the number of items rated by each user u , N  X  for computing the squared 2-norm of u and S  X  for computing P i  X  I sketch H u as follows: in the j -th cell we store an array Sign such that Sign u [ j ] = P i  X  I for the estimation of Pearson correlation.

After processing the stream, an inner product uv is esti-mated from the sketches H u and H v . The estimation of co-sine similarity follows directly from the respective definition. As we show in the theoretical analysis we compute Pear-son correlation  X  ( u,v ) as if we computed cos ( u  X   X  u,v  X   X  v ). We show in the proof of Theorem 2 that after processing the stream, using C [ u ] ,S [ u ] ,N [ u ] ,Sign u we can update the sketches H u such that we obtain an estimate of the Pearson correlation  X  ( u,v ). ProcessRatings Input: stream of user-rating pairs S , pairwise independent 1: for each ( u i )  X  X  do 2: Update ( u i ,h,s ) Update Input: rating u i , k -wise independent function h : N  X  [ k ], 1: H u [ h ( i )] += s ( i ) u i 2: N [ u ] += u i 2 3: S [ u ] += u i 4: C [ u ] += 1 5: Sign u [ h ( i )] += s ( i ) EstimateCosine Input: users u,v , sketches H , array N 1: inner product = 0 2: for i = 1 to k do 3: inner product + = H u [ i ] H v [ i ] 4: return inner product/ p N [ u ] N [ v ] EstimatePearson Input: users u,v , sketches H , arrays N , S , C , Sign 1: mean u = S [ u ] /C [ u ] 2: mean v = S [ v ] /C [ v ] 3: inner product = 0 4: for i = 1 to k do 5: H u [ i ]  X  = Sign u [ i ] mean u 6: H v [ i ]  X  = Sign v [ i ] mean v 7: inner product + = H u [ i ] H v [ i ] 8: norm u = N [ u ]  X  2 mean u S [ u ] + C [ u ] mean 2 u 9: norm v = N [ v ]  X  2 mean v S [ v ] + C [ v ] mean 2 v 10: return inner product/ Figure 1: Weighted similarity estimation through sketching.
Theorem 1. Let u,v  X  R n be revealed in a streaming fashion. There exists a one-pass algorithm that returns an  X  -approximation of cos ( u,v ) with probability 2/3 and needs O ( 1  X  2 ) space. Each vector entry u i and v i can be processed in O (1) time. The estimation can be computed in O ( 1  X  2 ) time. Proof. Assume for u and v we keep hash tables H u and H v recording k values, for k to be specified later. We also keep a counter N that will record the squared 2-norm of each vector u . Let h : R  X  [ k ] and s : N  X  { X  1 , 1 } . For a new arrival u i we then update H u [ h ( i )] += s ( i ) u i . Simultane-ously, we update N [ u ] += u 2 i . After processing the stream,
Clearly, after processing the stream k u k = p N [ u ] and k v k = p N [ v ]. Therefore, an  X  k u kk v k -approximation of uv will yield an  X  -approximation of cos ( u,v ). Let I indicator variable for the event that h ( i ) = h ( j ). For a pair-wise independent h it holds Pr[ I ij = 1] = 1 if i = j and Pr[ I ij = 1] = 1 /k for i 6 = j . Note that E ( I ij ) = E ( I ity of expectation it holds that E ( Z ) = uv . For a 4-wise independent s the variance of the estimator can be upper bounded as follows: = X = X
The above follows from linearity of expectation and from 1 if and only if there are two pairs of identical indices. By Chebyshev X  X  inequality we have
Therefore, for k = O (1 / X  2 ) we can show that Z is an  X  -approximation of uv with probability 2/3.

For Pearson correlation it holds  X  ( u,v ) = cos ( u  X   X  u,v  X  second pass update each component as u i  X   X  u and estimate the cosine similarity between the vectors. Next, we adjust AMS sketching to estimate cos ( u  X   X  u,v  X   X  v ) in a single pass, without knowing  X  u,  X  v in advance.

Theorem 2. Let u,v  X  R m be revealed in a streaming fashion. There exists a one-pass algorithm that returns an  X  -approximation of  X  ( u,v ) with probability 2/3. The algorithm needs O ( 1  X  2 ) space. Each vector entry u i ,v i can be processed in O (1) time.

Proof. Assume we know  X  u in advance. Let H u be the sketch for u  X   X  u . Then, H u [ k ] = P i  X  I P stream it holds that Sign u [ j ] = P i  X  I P thus update H u [ j ]  X  = Sign u [ j ] mean u . The squared norm Thus, in each H u we have exactly the same values if we had sketched the vector u  X   X  u . The complexity and approxi-mation guarantee of the algorithm follow directly from the proof of Theorem 1 and the pseudocode.
In a series of papers Bachrach et al. presented sketch-ing algorithms for Jaccard similarity estimation [3 , 4, 5, 6]. The state-of-the-art result is presented in [ 5]. The authors present a min-wise independent hashing algorithm that com-bines b -bit min-wise hashing [ 24], and yields optimal space complexity, with the min-wise independent sampling ap-proach [ 16 ] that achieves the best known processing time per update.
Inner product estimation using AMS sketching was pre-sented in [ 12 ] but, to the best of our knowledge, extensions to similarity estimation have not been considered in the lit-erature. This appears to be surprising since LSH based approaches might not be suitable when applied to high-speed streams. Consider for example cosine similarity. The LSH algorithm proposed in [9 ] needs space and update time  X ( u,v ), (the algorithm thus estimates arccos ( u,v ))). From Theorem 1, we need space O ( 1 cos 2 ( u,v )  X  2 ), but the processing time per update is constant. We consider the problem of similarity estimation in data streams where an additional time constraint is introduced. In particular, we consider i.e.,  X ( u i ,v i ) is the binary constraint that evaluates whether user v has rated items i within  X  time units after user u has rated item i . When clear from the context, we will write  X  instead of  X ( u i ,v i ). This allows to model propagation and influence as discussed in the introduction. Note that the approach presented in the previous section does not seem to apply here. AMS sketching is a linear transformation of the stream, i.e., the order in which we sketch the items does not affect the final result. However, when sketching the stream of ratings we have to also record information about the time when items were rated. We will thus present a different solution building upon the STRIP algorithm [22 ] that estimates Jaccard similarity with time constraints. In the following we extend the considered similarity measures to also include time constraints, briefly present the STRIP approach and discuss how to generalize it to weighted simi-larity measures estimation.

We extend the considered similarity measures as follows: Assume we are given a social graph and a stream of user-action pairs ( u,a i ) denoting that user u performed action a Actions can denote any activity like liking a photo, sharing a video or rating an item. For a possible set of n actions, user X  X  activity is represented by a (sparse) binary vector where the i -th entry denotes whether a user has performed the action a . In our context an action corresponds to rating an item, whereby we are not interested in the exact rating but only in the fact that the item was rated. We want to detect users that appear to have high influence on their neighbors. Since we are only interested which items a user has rated, we can assume that the items rated by user u correspond to an n -dimensional binary vector r u such that r u i = 1 iff user u has rated item i . Following the definition from [ 18 ], the influence probability is defined as where A  X  u 2 v is the set of actions that have propagated from u to v within  X  time units, i.e., u has done the action within  X  time units before v . A u | v is the set of actions performed by either u or v , independent of time. In our setting actions correspond to item ratings without distinguishing how the item is rated.

The STRIP algorithm works by extending min-wise inde-pendent hashing [7 ]. Let h : A  X  [0 , 1] be a random hash function that maps actions to values in the interval [0 , 1]. (As shown in [ 22 ], we can assume that h is injective with high probability.) A user-action-timestamp triple ( u,a i is then processed as follows: For each user u we keep a sam-ple H u that records the k action-timestamp pairs with the smallest hash values. If h ( a i ) is smaller than the largest en-try in H u , or H u contains less than k entries, we add ( a to H u and remove the ( k + 1)-th largest entry, if any. Im-plementing H u as a priority queue guarantees fast update. Once the stream has been processed, the influence probabil-ity p uv of user u on user v is estimated as where  X   X  ( H u ,H v ) denotes the set of actions in both H H v which satisfy the time constraint  X .
 Assume that ratings are small integer numbers. (Clearly, such an assumption is justified in practice, users usually rate items on a 5-or 10-scale.) We extend the STRIP approach to handle weighted similarity measures by treating each rat-ing as being composed by r max binary ratings, r max being the maximum rating. More precisely, a rating r u can be ex-of two ratings r u ,r v can thus be written as For example, let r u = 1, r v = 3 and r max = 3. Then, r = 1 + 0 + 0, r v = 1 + 1 + 1 and r u r v = (1  X  1 + 1  X  1 + 1  X  1) + (0  X  1 + 0  X  1 + 0  X  1) + (0  X  1 + 0  X  1 + 0  X  1).
Let c u k  X  X  0 , 1 } n be the binary vector that corresponds to the k -th position of u  X  X  ratings. For example, assume n = 5, r max = 5 and a user u has given following ratings: r u 1 = 3, r u 4 = 5 ,r u 5 = 1, items 2 and 3 have not been rated by u . We have c u 1 = (1 , 0 , 0 , 1 , 1), c u 2 = (1 , 0 , 0 , 1 , 0), c c = (0 , 0 , 0 , 1 , 0) and c u 5 = (0 , 0 , 0 , 1 , 0).
We can rewrite an inner product uv as the sum of r 2 max inner products of binary vectors: ProcessStream Input: stream of user-rating-timestamp triples S , pairwise 1: for each ( u,r ui ,t u )  X  X  do 2: Let c = ( c 1 ,...,c r max ) such that c ` = 1 for 1  X  `  X  3: for j = 1 to r ui do 4: STRIPUpdate( u,i,t u ,j,h ) 5: N [ u ] += r 2 ui 6: S [ u ] += r ui 7: C [ u ] += 1 STRIPUpdate Input: user u , item i , timestamp t u , sample number j , hash 1: if h ( i ) &lt; H j u .getMax () then 2: H j u .pop () 3: H j u .add ( h ( i ) ,t u ) MinWiseEstimate Input: sketches H k u ,H ` v , constraint  X  1: Let Min s be the s pairs ( h ( i ) ,t ) with the smallest hash EstimateSum Input: user u , user v , sketches H , constraint  X  1: sum u = 0 2: for k = 1 to r max do 3: sum u += MinWiseEstimate ( H k u ,H 1 v ,  X ) 4: return sum u EstimateInnerProduct Input: users u,v , sketches H , constraint  X  1: ip = 0 2: for k = 1 to r max do 3: for ` = 1 to r max do 4: ip += MinWiseEstimate ( H k u ,H ` v ,  X ) 5: return ip EstimateCosine Input: users u,v , sketches H constraint  X  1: ip = EstimateInnerProduct( u,v,H,  X ) 2: return ip/ p N [ u ] N [ v ] EstimatePearson Input: users u,v , sketches H , arrays N , S , C , constraint  X  1: ip = EstimateInnerProduct ( u,v,H,  X ) 2: nnz  X ( u,v ) = MinWiseEstimate ( H 1 u ,H 1 v ,  X ) 3: sum u  X  = EstimateSum ( H,u,v,  X ) 4: sum v  X  = EstimateSum ( H,v,u,  X ) 5: m u = S [ u ] /C [ u ], m v = S [ v ] /C [ v ] 6: norms u = N [ u ]  X  2 m u S [ u ] + C [ u ] m 2 u 7: norms v = N [ v ]  X  2 m v S [ v ] + C [ v ] m 2 v 9: return ip/ The above suggests the following algorithm. For each user we maintain r max sketches. Thus, for each user we can consider r max separate binary substreams. For each such stream we will run the STRIP algorithm and maintain a min-wise independent sample. Let ( u i ,t ui ) be an incoming rating-timestamp tuple. We have to update u  X  X  k -th min-wise sample, 1  X  k  X  r max , iff r ui  X  k . Once the stream has been processed, the inner product uv  X (  X  ) can be estimated timated constrained inner product of the binary vectors c and c v ` . A pseudocode based on the above discussion is pre-sented in Figure 2.
 Pearson correlation. Clearly, an estimation of uv  X  results in an estimation of cos  X ( u,v ) . However, for Pearson correla-tion we need to estimate ( u  X   X  u )( v  X   X  v )  X  . Let sum P entries in uv  X  . By rewriting the inner product we obtain ( u  X   X  u )( v  X   X  v )  X  = uv  X   X   X  usum u  X ( u,v )  X   X  vsum number of nonzero entries in uv  X  . We observe that we be easily verified: we consider exactly those u i for which  X ( u i ,v i ) and for each of them we add up exactly u The number of nonzero entries in uv  X  is exactly the number of indices i for which  X ( u i ,v i ) is true, i.e., c u 1
Lemma 1. Let z  X  0 , x  X  1 and 0 &lt;  X  &lt; 1 / 2 . Then  X   X z x = (1  X   X  ) z x .

Similarly, for  X  &lt; 1 / 2 we have z x  X   X   X  z (1  X   X  ) x We first show that an inner product can be approximated using min-wise independent hashing.

Theorem 3. Let S be a stream of vector entries u i , 1  X  i  X  m arriving in arbitrary order for different vectors u  X  N . Let u i  X  r max for all vector entries. There exists a one-pass algorithm that computes a sketch of the user activ-ity using O ( r max  X  2 log r max ) space per user and vector pair u,v  X  R n , after preprocessing the sketches of u and v in time O ( r max  X  2 log( r max  X  )) we obtain an  X r | v | ) -approximation of the inner product uv with probability Proof. Consider an incoming entry u i . We update all H u for which it holds u i  X  k . In H k u we keep the s pairs ( h ( i ) ,t u ) with smallest hash values. (Under the assump-tion that h is injective, the s pairs are well-defined.) We assume h is implemented as (1 /c,s )-min-wise independent evaluated in time O (log 2 s ). Implementing H k u as a priority queue, the total processing time is O ( r max log 2 s ).
We next show the quality of the estimate in terms of the sample size s . Let A,B  X  [ n ] be two subsets. Let  X  = J ( A,B ) denote the Jaccard similarity between A and B . We first show how to obtain an  X  -approximation of  X  . Let min h s ( A  X  B ) denote the s smallest elements in A  X  B under h . Let X be a random variable counting the number of attributes from A  X  B in min h s ( A  X  B ). Clearly, it holds E [ X ] =  X s . The number of attributes from A  X  B in size-s subsets follows hypergeometric distribution with s samples from a set of size n and  X n successes. Thus, for s &gt; 1 we have By Chebyshev X  X  inequality we thus have For s = O (1 / X  2 ) we thus can bound the probability to 1 /c for arbitrary fixed c . For h being (1 /c,k )-wise independent we will thus obtain  X  -approximation of  X  for s = O (1 / X  with probability 2 3 .

We obtain an approximation of | A  X  B | as follows. It holds
Consider an  X  -approximation of  X  . For the approximation error of | A  X  B | we obtain By Lemma 1 and using  X   X  [0 , 1] we can thus bound the approximation error by O (  X  ( | A | + | B | )).

The total approximation error for estimating an inner product uv is then bounded by uv  X   X  (
In order to guarantee the  X  -approximation error with prob-ability 2/3, we work with t = O (log r max ) independent hash functions and sketches. By taking the median of the esti-mates from the t sketches, by Chernoff bounds each binary inner product c u l c v ` is approximated with probability By the union bound we obtain an  X  -approximation of uv with probability 2/3.

The s smallest elements in the intersection of two sets with O ( s ) elements each can be found in O ( s ) time after pre-sorting the sets in time O ( s log s ). Thus, from the sketches H u and H v the inner product uv can be estimated in time O ( r 2 max  X  2 ). The time and space bounds follow directly from the above discussion.
 We next extend the above result to estimate time constrained inner product uv  X  .

Theorem 4. Let S be a stream of entry-timestamp pairs ( u i ,t ui ) arriving in arbitrary order for different vectors u  X  N n such that u i  X  r max . There exists a one-pass algorithm that computes a sketch of the user activity using O ( Let  X  be an appropriately defined time constraint. For any two users u,v , from the sketches of u and v we obtain an  X  ( | u | + | v | ) -approximation of the inner product uv ability 2/3 in time O ( r 2 max  X  2 ) .
 Table 1: Evaluation datasets. Both ratings sets are for movies, and in 5-scale with half-star increments.

Proof. Consider the estimation of the time constrained inner product of two binary vectors c k c `  X  . As in the proof of Theorem 3, we consider two sets A,B and apply minwise independent hashing in order to estimate  X   X  = | A  X  B  X  | B
 X  are the elements in A  X  B that satisfy  X ). Let  X  = | A  X  B | By the very same reasoning as in the proof of Theorem 3 we obtain an  X  -approximation of  X   X  using O (1 / X  2 ) space and O (log 2 ( 1  X  )) processing time.

Assume we have computed an  X  ( | A | + | B | )-approximation of | A  X  B | . (By Theorem 3 we need again O (1 / X  2 ) space and O (log 2 ( 1  X  )) processing time.) It holds With some simple algebra we can bound the approximation error to O (  X  ( | A | + | B | )). The claimed time and space bounds follow directly from the above and the discussion in the proof of Theorem 3.
 The above two theorems yield following approximation guar-antees for the considered similarity measures.

Corollary 1. Let u,v  X  N n be revealed in a streaming fashion. Let u i ,v i  X  r max . There exists a one-pass stream-ing algorithm processing each entry in time O ( r max log r and returning a sketch for each vector using O ( r max  X  2 After preprocessing each sketches in time O ( r max  X  2 log( we compute  X r max cos ( u,v ) -approximation of cos  X  ( u,v ) and  X 
Proof. Observe that for u,v  X  N n it holds | u | + | v | = (The last inequality follows from u i ,v i  X  1.) Thus, for q  X  uv  X   X r max ( | u | + | v | ) we have Similarly, for q  X  uv +  X r max ( | u | + | v | ) we obtain Rescaling  X  , we obtain the claimed bound for cosine similar-ity.

Consider now Pearson correlation. As shown in the proof the inner product c u 1 c v 1  X   X  uv  X  , and an  X r max the claimed time and space bounds. Since | c u 1 | X | u | for any u  X  N n , we obtain an O (  X r max ( | u | + | v | ))-approximation approximation bounds. Figure 3: Similarity distribution for the MovieLens dataset.
In this section, we present experimental evaluation of the algorithms presented in Sections 3 and 4. The main purpose of the evaluation is to validate the theoretical findings on real data. Note that the approximation quality does not depend on the dataset size. Therefore, for larger dataset the space saving becomes more pronounced. We use two publicly available datasets, MovieLens and Flixtser detailed in Table 1. The two datasets consist of movie ratings on a 5-star scale, with half-star increments. In Figure 3 we plot the distribution of cosine similarity and Pearson correlation for the MovieLens dataset. In addition, the Flixster dataset contains a who-trusts-whom social network containing links between users. For more details on the properties of the two datasets we refer to [20 , 29 ].
 Implementation: The algorithms are implemented in the Python programming language using its standard data struc-tures for the hash tables and priority queues. All experi-ments were performed on commodity hardware. We worked with a single hash function implemented using tabulation hashing [ 8]. Even if tabulation hashing is only 3-wise in-dependent, it is known to simulate the behavior of a truly random hash function, see P X atra  X scu and Thorup [27 ] for Chernoff-like concentration bounds for the randomness of tabulation hashing. (Note that working with several hash functions in parallel and returning the median of the esti-mates results in more accurate approximation. However, this comes at the price of slower processing time and in-creased space usage.)
Assume all keys come from a universe U of size n . With tabulation hashing, we view each key r  X  U as a vector consisting of c characters, r = ( r 1 ,r 2 ,...,r c ), where the i -th character is from a universe U i of size n 1 /c . (W.l.o.g. we assume that n 1 /c is an integer). For each universe U initialize a table T i and for each character r i  X  U i we store a random value v r i obtained from the Marsaglia Random Table 2: Quality of the similarity approximation for varying sketch sizes. Figure 4: MovieLens approximation for sketch size s = 200, for pairs with similarity at least 0.1.
 Number CDROM 3 . Then the hash value is computed as: where  X  denotes the bit-wise XOR operation. Thus, for a small constant c , the space needed is O ( n 1 /c log n ) bits and the evaluation time is O (1). In our setting keys are 32-bit integers (the item ids), and we set c = 4. Clearly, this yields a very compact representation of the hash function. http://www.stat.fsu.edu/pub/diehard/cdrom/ Evaluation: After sketching the activity for each user, we consider only users that have rated at least 1,000 movies. In MovieLens there are 840 such users and 1,204,445 ratings, and in Flixster there are 1,231 such users and 2,050,059 rat-ings. For the quality of approximation, we report i) the average approximation error ( aae ): P n i =1 | r i  X   X  r the approximated value of a rating r . ii) Given an approx-imation parameter  X  , we report the quality of approxima-tion in terms of the number of estimates  X  r i that are within [ r  X   X ,r i +  X  ] (denoted as 1-dev ), and within [ r i  X  2  X ,r ( 2-dev ). Finally, for all experiments we compute 1-dev and 2-dev w.r.t.  X  = 1 / of the approximation guarantee we showed in Theorem 3. Note that the approximation guarantees in Section 4 also depend on r max and the cosine similarity between users. Since the approximation quality scales with s , we present the approximation guarantees in terms of the sketch size per position, i.e., the total space usage is sr max .
For the scalability of the algorithms, we report the mem-ory requirements in terms of the sketch size ( s ) used, i.e., the number of entries stored in the sketch. Finally, we briefly report the run times of the algorithms on the datasets. The precise run times and actual space used highly depend on low-level technical details that are beyond the scope of the paper; we refer to [ 13 ] for a discussion on the evaluation of different implementations of streaming algorithms.
Having run the algorithms described in Section 3 on the datasets, we evaluated the impact of varying the sketch size (s) from 200 to 800 in increments of 100. The processing time varied between 90 -110 seconds for MovieLens and 80-100 seconds for Flixster when varying sketch sizes.
To evaluate the approximation error (aae) and the per-centage of good estimates (1-dev and 2-dev), we randomly sampled two separate groups of 300 users for each dataset and computed the similarity for every pair of users. In both MovieLens and Flixtser there are almost 90,000 pairs with cosine at least 0.1, while in MovieLens there are about 48,000 pairs with Pearson at least 0.1, and in Flixster  X  less than 10,000 pairs. Table 2 summarizes the results for MovieLens (Table 2a) and Flixster (Table 2b) respectively.

We observe as expected, the estimation error falls in re-sponse to increasing the sketch size (s). From the values 1-dev and 2-dev, we observe that the quality approximation is very high; the 2-dev ranges between 0.89 -0.97 across both measures and data sets. Furthermore, the average er-ror is always smaller than the  X  -approximation given in The-orem 1 and Theorem 2. Figure 4 plots the exact alongside the approximated cosine similarity (Figure 4a), and Pearson correlation (Figure 4b) for MovieLens, with s = 500. As we see, despite working with a single hash function there are no outliers in the estimates.
We evaluated the algorithm listed in Section 4 on the two data sets, and tracked the influence probability between users for a period of 6 months. In order to speed up the processing time, we discretized the rating to be in a 5-star For example, for a sketch size s = 400 we have  X  = 0 . 05 Table 3: Quality of approximation of the influence probabil-ity for varying sketch sizes. Figure 5: Approximation of the influence probability for Flixster dataset for sketch size s = 200. scale using ceiling r ui to d r ui e . However, we compute the exact similarity using the original 10-scale ratings. Because the social graph of the Flixster network is very sparse, to demonstrate statistical significance, we have increased the density of links. This is achieved by adding a new link be-tween a pair of users u and v if d ( u )  X  d ( v )  X  1 /r for a random number in (0 , 1], where d ( u ) is the number of neighbors of u in the network. Note that there is no social graph in the MovieLens dataset. As discussed in the introduction, we are interested in users whose behaviour is a good predictor for the behaviour of other users and we consider all user pairs.
Table 3 again reports approximation error when varying the sketch size (s). We observe very precise estimates for cosine similarity for both datasets; Table 3a for MovieLens and Table 3b for Flixster. In fact these numbers are con-siderably better than what the theoretical analysis suggests. This is not very surprising, in [ 3] the authors also report a gap between theoretical and empirical approximation.
Furthermore, from Table 3 we observe that unsurprisingly, the approximation error for the Pearson correlation is higher than the corresponding Cosine similarity error. This is due to the fact that we need to estimate four different quanti-ties which makes inaccurate estimates more likely. However, again the results show a lower error than the bounds we would expect from Theorem 3.

In Figure 5 we plot the approximation error for the Flixster dataset for sketch size 200. As evident, the approximation of the Pearson correlation is not so precise and there are a few significant outliers.

With respect to the datasets considered, space savings are made for smaller sketch sizes. For example, for a sketch size of 200 and ratings on a 5-scale, in the MovieLens dataset we need to store 840,000 samples, while for Flixster we need more than 1,2 million samples. (The pre-processed Movie-Lens and Flixster datasets contain 1,204,445 and 2,050,059 ratings respectively.) Finally, we observe higher running time compared to the AMS-sketch based algorithm, for Movie-Lens it varies between 170 and 190 seconds and for Flixster between 160 and 180. (However, this might be due to dif-ferent time formats in the two datasets.) We presented the first streaming algorithms for handling weighted similarity measures in data streams. The algo-rithms extend state-of-the-art techniques for scalable stream processing and are shown to be applicable to real-world do-mains. A few remarks are in place here. In [22 ] the algo-rithms were extended to min-wise independent hashing from sliding windows [15 ] where we are interested only in more recent user activity and to sublinear space usage where we are only interested in users whose activity is above certain threshold, i.e., have rated a certain amount of items. It is straightforward to extend the influence probability learning algorithm from Section 4 to also consider these extensions. However, it does not appear easy to extend the AMS sketch based algorithm from Section 3 to similarity estimation over sliding windows and active user mining. Finally, we note that it is easy to extend the here presented algorithms to the estimation of Euclidean distance, but due to lack of space we omit this result.
