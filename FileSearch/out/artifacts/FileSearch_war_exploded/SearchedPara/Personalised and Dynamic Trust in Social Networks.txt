 We propose a novel trust metric for social networks which is suitable for application to recommender systems. It is personalised and dynamic, and allows to compute the in-direct trust between two agents which are not neighbours based on the direct trust between agents that are neighbours. In analogy to some personalised versions of PageRank, this metric makes use of the concept of feedback centrality and overcomes some of the limitations of other trust metrics. In particular, it does not neglect cycles and other patterns characterising social networks, as some other algorithms do. In order to apply the metric to recommender systems, we propose a way to make trust dynamic over time. We show by means of analytical approximations and computer simu-lations that the metric has the desired properties. Finally, we carry out an empirical validation on a dataset crawled from an Internet community and compare the performance of a recommender system using our metric to one using col-laborative filtering.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering Algorithms, Design, Experimentation, Human Factors Information Overload, Personalisation, Recommender Systems, Social Networks, Trust
An increasing number of information technologies focuses on how web users can effectively share opinions about var-ious types of products, services or even other users. These technologies are the basis of several types of Web 2.0 appli-cations such as collaborative tagging, social bookmarking [5, 9] and, in particular, also recommender systems. Given the heterogeneity of web users , a major issue is how to appro-priately aggregate opinions in order to provide judgements that are useful for each individual user.

Most of these applications use collaborative filtering algo-rithms which compute an index of similarity between users or between items, based on the ratings that users have pro-vided on these items [8, 12, 19]. When a user belongs to a community with common, shared tastes, these algorithms work well in suggesting new items similar to the ones the users have already rated. There are several other bene-fits: except providing enough ratings, no further action is required of users; algorithms for collaborative filtering are scalable (when similarities are computed across items [22]); and, finally, they provide some level of personalisation. A shortcoming is that if users are looking for items which are seldomly rated by their community, the predictions are poor  X  e.g. people who have rated only travel books may not re-ceive very good recommendations on tools for gardening.
To cope with this, a line of research has focused on basing recommendations for users not on their similarity, but on their trust relations to other users. In this context, trust is meant to be the  X  X xpectancy of an agent to be able to rely on some other agent X  X  recommendations X  [15, 25]. There has been a body of work on  X  X rust webs X  [1, 10, 15, 21] and on their application to recommender systems [7, 16, 18]. The small-world property of social networks [20] allows to potentially reach a lot of information, while the trust allows to filter out the relevant pieces [25]. The benefits of these trust-based algorithms include strong personalisation, no need to have a long rating history in the system because recommendations are not based on similarity, and the ability to receive recommendations on items different from the ones already rated. Some limitations of the trust-based approach concern the scalability and the fact that, in addition to their ratings of items, users have to provide information about their level of trust to some other users.

In this paper, we introduce a novel metric for trust in so-cial networks. A trust metric allows to compute the indirect trust between two agents in a social network which are not neighbours, based on the direct trust between agents that are neighbours. While it is intuitive to do this on a chain, e.g. from user A via user B to user C , for instance by mul-tiplying the values of trust along the chain, it is not a priori trivial how to proceed when a graph contains multiple, re-dundant paths, cycles, or triangles (because of mathematical issues related to uniqueness and consistency). This is a cru-cial issue because these patterns all play an important role in social networks, in particular for the diffusion of information and the build-up of social capital [27, 24]. Some trust met-rics address these issues by reducing the direct trust graph to an acyclic graph before applying their computation of in-direct trust [7, 16]. Other metrics use only the path of the shortest distance or of the highest trust [25]. Our trust met-ric takes all the paths in the graph into account and it is well-defined on any given graph. It provides each user with personalised trust ratings about other users in the network. Our metric also is dynamic, i.e. it evolves in time depend-ing on how useful the information received by users is to them. This makes the metric suitable for application in rec-ommender systems, as we will illustrate in the remainder of the paper.
Consider a scenario in which there is a social network of agents which have trust relationships among each other. This can be described by a graph in which the nodes rep-resent the agents and the links represent the trust relation-ships. There also is a set of objects which can be rated by agents. Since each agent only knows a few objects, it may want to know other agent X  X  opinions on unknown objects. However, since there are potentially many opinions of other agents, it needs to be able to determine which of these are trustworthy. This implies that an agent needs to reason about the trustworthiness of other agents [25]. However, since its time and resources are constrained, an agent can only build and maintain trust relationships with a limited number of other agents. Thus, if T ij  X  [0 , 1] represents the level of direct trust of agent i towards j , how do we compute the indirect trust  X  T kl between two agents k and l that are not neighbours 1 ?
In the following, we will describe the TrustWebRank met-ric for computing indirect trust in a network with direct trust. This metric builds on the concept of feedback cen-trality which assigns a centrality score to the nodes of a net-work based on the centrality scores of the node X  X  neighbours. In other words, in feedback centrality, the higher (or lower) the centrality score of a node X  X  neighbours, the higher (or lower) this node X  X  own centrality is. These principles can be adapted to define a metric for the trustworthiness of agents in a social network with trust relationships.

We briefly review PageRank, one of the most widely known and studied feedback centrality algorithms [4, 3]. In our sce-nario this would assign a trustworthiness score c i to an agent i depending on the trustworthiness of its neighbours j : where N i is the set of neighbours of i , and  X   X  [0 , 1) is a damping factor which is chosen around 0 . 8 [4]. In vector notation: where 1 is the vector consisting of ones and P is a stochastic
Variables expressing indirect trust are as the corresponding ones expressing direct trust, but with a tilde: e.g. T and
We will always assume row-stochastic when we state  X  X tochastic X ; this does not imply that the matrix need (or not) to be column-stochastic. transition matrix defined as Eq. (1) can easily be extended to weighted graphs [3]. Solv-ing Eq. (2) for c we obtain: where I is the identity matrix. Since P is, by construc-tion, stochastic and thus, by the Perron-Frobenius theorem [23], the largest eigenvalue is  X  PF ( P ) = 1, it follows that  X 
PF (  X P ) =  X  &lt; 1. This ensures the existence of a unique solution of c . Usually, one uses Jacobi iteration to compute such a solution.

The result of applying this algorithm to a graph is a vector which gives a score of the trustworthiness c i for each node i in the graph. Note that this is a global metric, i.e. there is one score for each agent. It has been observed in the litera-ture that, for recommender systems, such metrics are often not appropriate and that local metrics, which are person-alised for each agent ( X  X ow trustworthy is agent i from the perspective of agent j  X ), are required [16]. EigenTrust, for example, is a PageRank-inspired, global trust metric [14].
Proceeding in analogy to PageRank and using the princi-ples of feedback centrality to construct a personalised metric for trust, one could define the indirect trust of agent i to j as the indirect trust of the neighbour agents k of agent i to agent j , weighted by the trust of agent i towards these neigh-bour agents k . Let T be the trust matrix, where T ij  X  [0 , 1] reflects the direct trust from agent i to agent j ( T if there is no link between agent i and agent j ). S is the stochastic matrix where N i is the set of neighbours of agent i . S is a normali-sation of T . We define  X  T ij to be the indirect trustworthiness score from i to j : This allows us to estimate the trust between any two agents i and j : if there is a link between i and j , T ij reflects the trust between them; if there is no link between i and j , reflects the trust between them. Notice that this definition is similar to to the approaches used in [7, 16]. In matrix notation, this is the recursive definition
Notice that this approach has several limitations: 1) Uniqueness of the solution : Let  X  v  X  j be one column of  X  T , i.e. the vector that expresses how much agent j is trusted by other agents. Then, Eq. (7) gives If S is acyclic [23] (i.e. the underlying graph is so), then there is a unique solution of Eq. (8). If S is not acyclic, it can be either primitive or non-primitive [13]. If S is primitive (and stochastic), there is a unique solution of Eq. (8), a vector with all components being identical [23]. This would imply that all agents i would trust agent j equally, which is obviously not desirable. If S is not primitive, there are multiple solutions for Eq. (8), which also is not desirable.
One way of dealing with this could be to make S acyclic, for example by constructing a tree with a breadth-first search (BFS) from a chosen node, as for example [7, 16] do. The BFS selects one node as a root, and from there on, explores the neighbours of the nodes, proceeding in levels 1 , 2 , 3 ,... and removing links within a level and links from level k to level l where l &lt; k at each step. However, this entails further limitations:
Social networks are characterised by a high clustering co-efficient [27, 20, 24]. By making the underlying graph of a social network acyclic, one removes the links within each level and the links from levels k to l where l  X  k , thus making the clustering coefficient 0. This implies that, subsequent to this procedure, the trust metric will not be able to differen-tiate well between regions of high clustering (thus, possibly high trust) and regions with lower clustering (thus, possibly lower trust) as on the original graph.

Further, depending on which node is chosen as the root of the BFS, the acyclic graph will be different. This is not a problem in a decentralised scenario, when the computation is spread over many nodes. In this case, each node computes its own set of  X  v  X  j by being root of its own breadth-first explo-ration. However, this is a problem in a centralised scenario, where such an approach is not scalable and also not mathe-matically tractable: as a result of a BFS rooting at each i , the computation uses a different matrix T for each node. 2) Combination of direct and indirect trust : The metric defined in Eq. (6) is not able to account properly for the following situation: consider an agent i that trusts a neigh-bour agent j with intermediate level of trust, e.g. T ij  X  0 . 5, because it does not yet know this agent well. If many of the other neighbours of agent i trust agent j , this should increase the trust between agent i and j . This does not happen with the current definition of trust. 3) Normalisation of trust : another property, resulting from Eq. (5), is that the normalisation removes knowledge from the system. If an agent i trusts n neighbours equally, it does not matter whether it trusts them a lot or a little  X  the normalisation would assign the same value of trust of 1 n each of the neighbours. Then, during propagation, only the relative trust compared to other neighbours is considered. Equally, suppose that an agent i has just one neighbour agent j  X  no matter whether i trusts j highly or lowly, in each case the normalisation would cause the trust from i to j to be 1. The normalisation is necessary, however, to have values of direct and indirect trust which are in the same range.
Thus, given these limitations, can we modify Eq. (6) in such a way that the following requirements are met?
Requirement 1 : The solution of the equation over graphs with cycles is unique, but not trivial.

Requirement 2 : The range of indirect trust is the same as for direct trust, i.e. [0 , 1], so that direct and indirect trust can be compared.

Requirement 3 : In the metric, direct trust  X  X dds on X  to indirect trust (capturing the fact that it complements it).
One possibility to address these issues is the following: we compute the indirect value of trust between two agents i and j based on the direct trust between them, if there is any, but also based on the trust that the neighbours of i have in j : where  X   X  [0 , 1). Now, in matrix form Eq. (9) is and, using elementary algebra, we can derive There exists a unique, non-trivial solution to Eq. (11) if  X 
PF (  X S ) &lt; 1, [13]. Since S is stochastic, i.e.  X  PF and  X   X  [0 , 1), it follows that  X  PF (  X S ) &lt; 1 (Requirement 1).
The parameter  X  has a similar role as the damping factor in PageRank in Eq. (1): given  X   X  [0 , 1), the impact of agents far away in the social network is discounted. This can be seen more clearly when expressing (1  X   X S )  X  1 as a geometric sum in Eq. (11) [13]:  X  T = (1  X   X S )  X  1 S = The k th power of the adjacency matrix of a graph gives the number of walks of length k between any two nodes in the graph. Similarly, the k th power of the matrix S gives the sum of the products of the weights along all walks of length k in the underlying graph of S . In Eq. (12), the higher the length of the walks, the stronger the discount (since  X  &lt; 1). As in PageRank, a reasonable value of  X  turns out to be around 0 . 75 to 0 . 85 (see Section 4.5). Note that  X  T We can normalise it to to ensure the comparability of values of direct and indirect trust (Requirement 2).

Furthermore, if agents i and j are not neighbours, the indirect trust of i to j is entirely based on how much the neighbours of i trust j . However, if agent i has agent j as a neighbour, the indirect trust of i to j will also incorporate how much the other neighbours of agent i trust or do not trust agent j (Requirement 3).

The definition of Eq. (9) naturally takes the real struc-ture of a social network into account without needing to prune any link. Unlike to what would happen during the conversion of the underlying graph to a tree using a BFS, the algorithm preserves the links which, in a social network, lead to a high clustering coefficient, and are not negligible when reasoning about the social network itself [27, 20, 24].
When dealing with huge graphs, however, inverting a ma-trix as required by Eq. (11) poses an issue of computation time and memory. Yet, instead of inverting a matrix or com-puting eigenvectors, it is possible to use an iterative method [3] as follows: At each step k , one only needs the neighbourhood N i of a given agent i , as well as access to the matrix of  X  T ( k  X  1) puted at the previous step k  X  1. Notice that now we are computing a matrix while, with the centrality, e.g. in PageR-ank, we were computing a vector . This is natural since the centrality is one value per agent (it is a global notion), while trust is a value per pair of agents (it is a local, personalised notion). Therefore computing trust (  X  O ( N 2 )) is inherently more expensive than computing centrality (  X  O ( N )). How-ever, do we really need to compute indirect trust among all agents? In fact, for a given agent i , computing the trust to a selected amount of other agents j , if well chosen, will be sufficient, as the trust to agents far away in the network will be damped out anyway. So, the scalability of the trust computation rather is (  X  O ( mN )), where m is the number of other agents j to consider for each agent i .
So far, we have described a trust metric which allows to compute a measure of trust between two agents which are not necessarily neighbours in a social network. We will now construct a simple model which applies this metric in the context of a recommender system . The purpose is to show how it is possible to compute predictions of how an agent i likes a particular object o (suppose a book, CD, or movie) based on how other agents j liked that item combined with how much i trusts j .
Suppose we have a system of agents embedded in a social network, defined by a graph and associated to an adjacency matrix A . Each agent i keeps track of its trust relationships to neighbours j . These are reflected in the matrix of direct trust T . Obviously, T ij &gt; 0 only if A ij = 1. For the moment, we take the network to be described by a random graph [6, 2] in which each agent roughly has the degree d .

Let each agent i be characterised by a profile  X  i . The pro-file expresses which ratings an agent would give to all pos-sible objects; however, agents only know a subset of their ratings on objects. Given an object o , r o i  X  { X  1 , 1 } is the rating of agent i on object o . If an agent is willing to share all its opinions with other agents, then the set of all of its rat-ings corresponds to its profile; however, there may be agents which are not willing (because they want to keep their se-crets) or able (because they simply do not know particular objects) to share ratings. This can be captured by a param-eter  X  which reflects the probability of an agent to share  X  i.e. signal  X  its rating with other agents. E.g., a value of  X  = 0 . 1 would imply that, on average, at each time step 10% of the agents are willing to share their ratings with other agents. At the moment,  X  is the same value for all agents, but it could also be set differently for each agent i or even for each pair of agents i and j .

If an agent i is not willing or able to share its rating for an object o , the system computes a prediction p o i as follows: so p o i  X  [  X  1 , 1], since P j  X  N vector notation, i.e. the prediction for an agent i is the sum of the ratings of all neighbours j weighted by the indirect, normalised trust that agent i has in these neighbours j .

Note that this bears resemblance to Collaborative Filter-ing (CF) [8, 12] in which the prediction for an agent i is also computed as a weighted sum of the ratings of all neighbours j (not neighbours in a graph-theoretic sense, but neighbours in terms of similarity of ratings). The more similar a neigh-bour, the more influential its rating will be for the predic-tion. In our case, making a prediction based on the ratings of the trusted neighbours implies that we make the assump-tion that agents who are connected by trust have similar mind-sets. Notice that this does not imply that they have rated the same items  X  for example, one user could appreci-ate the knowledge of another user in gardening, even though his own domain are travel books. Thus, unlike the similar-ity that could be computed e.g. by Pearson correlation, this notion of similarity extends not just across rated items, but rather is an  X  X xpected X  similarity reflecting a similar mind-set of two agents.
So far, we have a static model which, based on the trust web of a particular agent i and the ratings r o j of its neigh-bours j , is able to compute predictions p o i for that agent. We now would like to model the evolution of the trust net-work over time in the sense that, based on the quality of a particular recommendation, agent i can update its trust to its neighbours j . This adds a time dimension to the model and requires a mechanism to update the trust between neigh-bours. This can be done by adding a utility function: agents experience a utility by using the ratings or predictions of neighbours and then the trust update is coupled with the utility experienced. We define each agent i to experience a utility u ij ( t ) by following the recommendation from each neighbour j at time t as follows: Note that u ij ( t )  X  [  X  1 , 1]. If the neighbour j signals to agent i , it knows the rating r o j ( t ); otherwise, it only knows a prediction p o j ( t ). The closer the recommendation of agent j for agent i to the rating of agent i is, the greater the agents X  similarity is and thus the higher the utility u that agent i experiences from the recommendation of agent j at step t is. Note that because of the level of cooperation  X   X  which affects whether agent j signals to i  X  the utility takes into account not only similarity [28], but also cooperation between agents. Based on the utility, agent i can update the trust towards its neighbour agents j . We distinguish four cases, based on the sign and the magnitude of the utility:
This leads us to the following definition of how an agent i updates its trust to agent j from time t to t + 1:  X  T ( t + 1) = where we take u thr = 0 . 5 and  X   X  [0 , 1] is a parameter that controls the relative weights of the current history of trust between two agents, T ij ( t ), and of the current utility, u For  X  &gt; 0 . 5, this gives the history of trust more weight than the current utility. In the analysis and simulations (next section), we found that  X  = 0 . 75 is a reasonable value. Since  X  T ij  X  [  X  1 , 1], but we want T ij  X  [0 , 1], we cap it to [0 , 1]: As an example, the effects of these dynamics are illustrated in Figure 1: this is an example of a network of agents hav-ing two profiles (red and blue). Some nodes are signalling (squares), others are not (circles). The network contains cycles. At t = 1, the agents are just connected, the trust be-tween all agents is equal to zero. At t = 2, agent 3 and agent 4 have received recommendations from agents 5 and 6, and from agents 7 and 8, respectively. Since agent 3 (4) has the same profile as agents 5 and 6 (7 and 8), namely red (blue), it perceives a high positive utility from the recommendation and thus increases its trust to the recommending agents. At t = 3, the system can now provide a recommendation to agent 2, even though agents 3 and 4 are not signalling their own rating. Since agent 2 has the same profile as agent 3, trust between these two agents increases. Agent 2 perceives a high negative utility from the recommendation of agent 4, thus its trust remains zero. At the same time, the links from 3 to 5 and 6 reinforce. The same happens in the cy-cles. These mechanisms continue and we see that at t = 5, paths of trust have developed between agents of the same profile. Although agent 1 has no agents of its profile that are signalling in one or two levels of distance, it is still able to discover a path to two agents of its profile that are signalling and further away in the network.
In this section we derive a self-consistent equation for the matrix of trust which allows to investigate the dynamics of trust. We analyse the case of a population of agents with only two opposite profiles (see Section 4.1) which provide ratings on objects as +1 or  X  1, respectively.

We want to compute the expected value of trust at the equilibrium of the dynamics defined in Eqs. (18-19). We do so by a mean-field approximation in which we replace the utility u ij ( t ) in Eq. (18) with the expected utility over time, denoted by u ij := E ( u ij ( t )) (without the time dependency). We impose T ij ( t ) = T ij ( t + 1) at the equilibrium, obtaining which requires us to estimate u ij . Given the definition of u ( t ) in Eq. (17) and the fact that agents signal a rating with probability  X  and they do not with probability 1  X   X  , it follows that the expected utility u ij is u ij =  X  (1  X  X   X  i  X   X  j | ) + (1  X   X  )(1  X  X   X  i  X  X Since we are considering the simple case in which agents signal faithfully, the expected rating provided by an agent j coincides with its profile: E ( r o j ) =  X  j . We can thus express the expected prediction for agent j as E ( p o j ) = P k  X  In future work, we will also consider more complicated cases, e.g. including non-faithful (selfish or malicious) behaviour. Substituting into Eq. (20), we get:
T ij = max(0 , min(1 , X  (1  X  X   X  i  X   X  j | ) Since the profiles  X  are given, T is a function of  X  S . Notice that by combining Eqs. (12-13-5), we can express  X  terms of the components T jk , ( T 2 ) jk , ( T 3 ) jk ,... as well as T , ( T 2 ) jl , ( T 3 ) jl ,... where l are the other neighbours of j : It follows that we can express the value of trust T ij between any pair of agents in terms of the value of trust among the other pairs. This leads to a self-consistent equation for T , where the only parameters are the initial values of trust T (0), the probability to signal,  X  , the discount factor along the walks of the graphs,  X  , and the profiles of the agents,  X  : Notice that Eq. (24) is obtained without any assumption on the structure of the network that is reflected in T .
One is, of course, interested in the fixed points of Eq. (24), their stability and whether they are attained by the dynam-ics. On the one hand, it is trivial to check that the matrix T with T ij = 1 among agents with the same profile and T ij = 0 among agents with opposite profile is a fixed point of Eq. (24). Denote this configuration as { T + = 1 ,T  X  On the other hand, the configuration with trust equal zero among all pairs { T + ,  X  = 0 } is not a fixed point.
In the next section, we find, by means of computer simu-lations, that the system, starting from a configuration with no trust among the agents, { T + ,  X  = 0 } , always evolves to a configuration in which agents with similar profile trust each Figure 2: Trust between agents of the same profile over time, for a fixed average degree of agents but variable level of cooperation. other { T + = 1 ,T  X  = 0 } . This is true even if agents do not signal all the time (i.e.  X  &lt; 1). A formal investigation of the stability of all the fixed points of Eq. (24) will be performed in future work.
The simulations that we carried out were done on an agent population of 500 agents. We considered two opposite pro-files with ratings on objects as +1 or  X  1. The agents are connected in a random graph [6, 2]. Initially, T ij = 0  X  i,j , i.e. the agents have to learn who to trust. We varied the average degree d of each agent, as well as the level of co-operation  X  in the system. The following figures illustrate the system behaviour over 50 steps; all results were averaged over 100 runs.

Figure 2 illustrates the average trust between agents of the same profile over time: the average degree of agents is fixed, d = 7, and the level of cooperation  X  is variable, ranging from 0 . 01 to 0 . 25 in steps of 0 . 01. The average trust between agents of the same profile converges to 1 for almost all  X  . For larger  X  , this process takes place much faster than for smaller  X  . Given a sufficient level of cooperation in the system, the agents develop trust to the agents that have the same profile. Furthermore (not shown in the figure), agents of opposite profiles do not develop trust between each other.
Figure 3 illustrates the trust between agents of the same profile as a function of the level of cooperation and the av-erage degree of agents at t = 5, and t = 10. Initially, at t = 0, agents still have to learn who to trust (and the whole figure would be blue, corresponding to zero trust between everyone). At t = 5, trust is already developing; for larger Figure 3: Trust between agents of the same profile as a function of level of cooperation and average degree of agents at t = 5 (left), and t = 10 (right). Figure 4: Performance over time, for a variable aver-age degree of agents, but a fixed level of cooperation. average degrees of agents d as well as for larger levels of co-operation  X  , this happens faster. At t = 10, trust between agents of the same profile has developed for an average de-gree of agents d &gt; 5 and a level of cooperation  X  &gt; 0 . 05.
The obvious consequence of the evolution of trust is that predictions tend to match the profiles. We test this by mea-suring the performance of the system. Let the performance be defined as the sum of the products of the utility and the trust between all pairs of agents i and j : where n is the number of agents, e.g. in our case n = 500. Agents are exposed to ratings which lead to both positive or negative utility. By building trust, they give more weight to the positive utility and less weight to the negative utility. Therefore, this measures  X  X ow well agents use their trust X .
Figure 4 illustrates the performance over time: again, the average degree of agents is fixed, d = 7, and the level of cooperation  X  is variable, ranging from 0 . 01 to 0 . 25 in steps of 0 . 01. The performance converges to 1 for almost all d . The similarity to Figure 2 is due to the fact that agents who have developed trust to other agents of the same profile are provided with good recommendations; these agents perceive high utility, leading to high performance.

Finally, Figure 5 illustrates the performance as a function of the level of cooperation and the average degree of agents at t = 1 and at t = 5. Again, just as the trust between agents of the same profile increases in Figure 3, the perfor-mance increases with increasing average degree of agents and level of cooperation. One might wonder how, at t = 1, the performance can already be nonzero  X  this is due to the fact Figure 5: Performance as a function of level of co-operation and average degree of agents at t = 1 (left) and at t = 5 (right). that there are only two opposite profiles; this implies that half of the neighbours of an agent are of the same profile and, as soon as an agent has developed some trust to one of these neighbours, it will benefit from their recommendations which, again, drives the performance up.
To support the analytical approximations of the model and the results of the computer simulations, we empirically tested the performance of a recommender system using our TrustWebRank (TW) metric against one using a standard Collaborative Filtering (CF) approach, similarly to what has been done in [17]. We crawled Epinions.com, an on-line platform which allows consumers to read and write reviews about products. The unique feature of Epinions is that users can also form a  X  X eb-of-trust X  and specify other users that they trust with respect to their reviews. The crawling was performed in mid-2007 and led to a dataset of 60,918 users with 896,969 reviews on 223,687 products and with 518,505 relationships. We cleaned this dataset and removed users that either had not written any reviews or had no relation-ships to other users because no reasonable validation can be done with these users. Furthermore, we focus on the great-est strongly connected component (SCC) because a) there is only one large SCC and many small SCC (1-3 users) and b) membership in this SCC can be seen as a proxy for hav-ing a properly formed web of trust. Having applied this procedure, we are left with 29,478 users, 731,220 reviews on 201,674 products, and 471,888 relationships. The data sparsity is 99.9877%. Reviews have a rating which is on the scale of 1 (min) to 5 stars (max). There is a bias to review favourably, as 75% of the ratings are either 4 or 5 stars and only 25% of the ratings are 1, 2, or 3 stars  X  probably be-cause users are more likely to spend time to write a review when they like a product.

We split the reviews into a training set R Training test set R Test . We then compare the performance of TW and CF by training the algorithms on R Training and testing with R Test . TW, in general, has comparable performance to CF, and performs better in particular situations, as we will describe in the following. The complete empirical validation will, together with some statistical analyses of the Epinions community, be reported on in a separate paper [26].

Mean Absolute Error : the mean absolute error (MAE) is defined as Figure 6 shows the MAE of TW for changing  X  and CF.
 Depending on the value of  X  , TW performs (marginally) better than CF. There is an optimal  X  opt  X  0 . 8.

However, the fact that most ratings are 4 or 5 limits the meaning of the MAE as a measure of performance. Indeed, predictions based on the Simple Average (SA) of ratings on a product, a global algorithm which is not personalised for users, outperform both TW and CF: e MAE ( SA ) = 0 . 21. Similar results were found in [17] using a different dataset of Epinions (from 2003). An explanation for this is that re-views are very homogeneous and almost all ratings are posi-tive. Other datasets, such as the commonly used MovieLens dataset, are more heterogeneous and SA performs worse than CF on such datasets. Unfortunately, at the moment, Figure 6: Mean Absolute Error of TW (blue/circles) against  X  and CF (red/squares). The MAE is nor-malised to a scale in [0 , 1] , i.e. it reflects percentages. Epinions is the only available dataset which combines rating data and a social network  X  and which is thus suitable to test the performance of TW.

Coverage : coverage measures the percentage of elements that can be predicted from the training set. Both TW and CF cannot compute predictions for all elements in the test set. For example, if there is no similar or trusted user who has rated a particular product, CF or TW are not able to compute a prediction for that product. CF was able to com-pute 41.65% of the predictions and TW was able to compute 75.11% of the predictions. Thus, coverage with TW is much higher than with CF. The reason for this is that TW is able to reach a large neighbourhood even when the neighbour-hood based on co-ratings, as in CF, is small.
 Top-N Set Overlap : as noted, the value of ratings in Epinions does not seem to carry a lot of meaning  X  probably because people tend to rely more on the text of reviews than on the rating. Therefore, it makes sense to compare the performance based on the ability to predict the subset of products rated by a user. We define the following measures of overlap between sets: where P i is the set of products rated by a user i ; R N is the set of the N most rated products overall in the system; X denotes either CF or TW and thus R N CF,i and R N T W,i the sets of the N most rated products in the neighbourhood of a user i constructed by CF and TW. Note that R N is a global set which is the same for all users i . Thus, o N counterpart of e MAE ( SA ) in this context. R N CF,i and R are personalised sets which depend on the neighbourhood of user i and thus are different for any two users. We define the average overlap across all users as O N , O N CF , and O For N = 100, we obtain O N  X  0 . 0819, O N CF  X  0 . 2526 and O
T W  X  0 . 1724. Since a larger overlap signifies a better pre-diction, the larger the values, the better the performance. This implies that the global measure O N performs worse than both O N CF and O N T W . In addition, CF performs bet-ter than TW. However, it should be emphasised that this measure is obviously biased in favour of CF: by definition, P i  X  R N CF,i 6 =  X  . In contrast, P i  X  R N T W,i can be empty, as a user does not necessarily declare trust to people who have rated the same items. Still, TW performs significantly better than the global measure O N . This illustrates the dif-ficulty to compare the performance of TW with CF. In fact, the most appropriate way to measure performance would be based on user-provided feedback subsequent to having followed a recommendation.

In conclusion, we found that TW and CF have compara-ble performance. TW seems mostly useful for recommenda-tions of items different from those a user has already rated  X  e.g. recommendations on travel books for people usually interested in tools for gardening.
We introduced a novel metric for computing indirect trust in social networks. We derived this metric from feedback centrality measures in graphs and illustrated how it ad-dresses some limitations of other trust metrics; most im-portantly, that it takes cycles in the underlying graph into account. We constructed a simple model of a recommender system that makes use of our metric and showed how in-direct trust can be used to generate recommendations. We performed analytical approximations and computer simula-tions to characterise the system behaviour. Finally, we also tested the model by validating it with empirical data of an Internet community devoted to product reviews.

Some extensions to this model could involve changing the trust dynamics: Trust update as a slow-positive, fast-negative dynamics. It has been observed in the literature that trust follows a slow-positive, fast-negative dynamics [1, 10, 15, 21, 25]. This means that trust builds up slowly, but gets torn down quickly and this behaviour could be implemented by modi-fying Eq. (18).

Coupling the utility with the level of cooperation  X  . In real applications, if, initially, the utility for users is zero, then nobody will signal and this is a fixed point  X  and a social dilemma [11]. Thus, we could couple the probability of signalling to the utility and investigate how to make the system escape from this undesirable fixed point.

With this work, we have shown that incorporating this novel trust metric in recommender systems is a promising and viable approach. [1] Abdul-Rahman, A., and Hailes, S. Supporting [2] Bollob  X  as, B. Random Graphs . Academic Press, 1985. [3] Brandes, U., and Erlebach, T. , Eds. Network [4] Brin, S., and Page, L. The anatomy of a large-scale [5] Cattuto, C., Loreto, V., and Pietronero, L.
 [6] Erd  X  os, P., and R  X  enyi, A. On random graphs. Publ. [7] Golbeck, J. A. Computing and applying trust in [8] Goldberg, D., Nichols, D., Oki, B. M., and [9] Golder, S., and Huberman, B. A. The structure of [10] Grandison, T., and Sloman, M. A survey of trust [11] Hardin, G. The tragedy of the commons. Science 162 [12] Herlocker, J. L., Konstan, J. A., Borchers, A., [13] Horn, R. A., and Johnson, C. R. Matrix Analysis . [14] Kamvar, S. D., Schlosser, M. T., and [15] Marsh, S. Formalising Trust as a Computational [16] Massa, P. Trust-aware Decentralized Recommender [17] Massa, P., and Avesani, P. Trust-aware [18] Montaner, M., L  X  opez, B., and de la Rosa, J. L. [19] Montaner, M., L  X  opez, B., and de la Rosa, J. L. [20] Newman, M. E. J., Watts, D. J., and Strogatz, [21] Sabater, J., and Sierra, C. Review on [22] Sarwar, B., Karypis, G., Konstan, J., and Riedl, [23] Seneta, E. Non-Negative Matrices and Markov [24] Vega-Redondo, F. Complex Social Networks .
 [25] Walter, F. E., Battiston, S., and Schweitzer, [26] Walter, F. E., Battiston, S., and Schweitzer, [27] Wassermann, S., and Faust, K. Social Network [28] Ziegler, C.-N., and Golbeck, J. Investigating
