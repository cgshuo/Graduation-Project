 J.4 [ Computer Applications ]: Social and Behavioral Sci-ences; H.3.5 [ Online Information Services ]: Web-based services Human Factors, Measurement, Experimentation online user behavior, customer relationship management, growth and retention, hazard based methods
User attention is perceived as the most important resource in the internet era [12]. The web is described [23] as a  X  X irtual theme park where most rides are free such that revenue is generated through  X  X elling eyeballs X  to advertisers X  . The ad-supported economy of the web has the web-services vying for users X  time rather than their money. Having a large loyal and dedicated user base has several indirect benefits as well. Many services grow with their users, improving themselves based on their feedback and through the power of big data analytics on their activities logs. A common example is the Google search engine, which has perfected its query auto-correct feature primarily using user click-through data, as well as improved its search performance regularly using user search histories. Furthermore, an active community can be tapped to create new content that benefits the other users of the service and the service as a whole as seen for popular social networks such as Facebook and Twitter.

There is tremendous competition among the rapidly in-creasing number of web services for the finite and limited resource corresponding to user attention. Although, attract-ing new customers is crucial for any business, it is generally much easier and cheaper to retain existing customers [4, 29]. This directly results in a great deal of emphasis being placed on engaging one X  X  current user base. Customer re-tention efforts have been heavily researched in sectors such as telecommunication [29], financial services [22], internet services [13] and other utilities etc. which follow the sub-scription based model. The methods in these domains have focused on identifying potential churners in the user popula-tion, where churners are defined as those current subscribers who are not likely to renew their subscription in the coming months. Once detected, the churner population is targeted with retention strategies like offers, customer solutions and recommendations to win them back.

However, such methods cannot be directly applied to solv-ing the user retention problem for web services due to the following reasons: 1. Difficult to define churn for a non-contractual 2. Highly dynamic user visitation behavior. Web
To adapt to the unique incentive structures and dynamic user base, in this work we propose a novel retention metric which tracks the user return rate for addressing growth and retention in web services. The user return rate is defined as the fraction of the existing users returning to the service on a particular day. It is beneficial for a web service to improve its user return rate in order to increase its revenue. Predictive analysis of user return times can direct such improvements efforts. Return time prediction allows a service to identify indicators of earlier (longer) return times for their users. Identifying such indicators and quantifying their impact on user return times offers a service insights into its practices. It also enables a service to employ corrective measures and improve the experience to its users. Secondly, a service can identify sections of its user base that are not likely to return soon. Studies have shown that the longer the users stay away from a service, the less likely are they to return in the future [24]. Early identification of users who are not likely to return soon to the service allows the deployment of suitable marketing strategies to encourage those users to engage with the service again.

We propose a hazard model [8] from survival analysis to predict user return times. The hazard based models are preferred over the standard regression based methods for this problem due to their ability to model particular aspects of duration data such as censoring. More importantly, the Cox X  X  proportional hazard regression model is used as it can incorporate the effects of covariates 1 . We apply the model for return time prediction on real-world datasets from two popular online music services.

We now summarize the key contributions made by us in this paper: (a) We formally define an approach for targeting retention (b) We propose the Cox X  X  proportional hazard to model dy-(c) The Cox X  X  proportional hazard model outperforms state-
The rest of the paper is organized as follows. In Sec-tion 2 we provide a brief overview of the related research in the area of churn prediction and the use of hazard based methods. We then formally define our problem and lay out our contributions in Section 3 . In Section 4 we describe our hazard based predictive model and provide details of the covariates used and the model estimation procedure. In Section 5 and Section 6 we discuss the experimentation setup and the results. We summarize the conclusions from our experimental analysis and provide future directions for our work in Section 7 .
In this work, we propose a new approach for directing growth and retention solutions for web services through re-turn time prediction. Traditionally studies have focused on the problem of churn prediction defined as a binary classifi-cation problem where users are categorized based on several behavioral and demographic features into two categories: fu-ture churners or non-churners. The popular data mining techniques used for building classifiers for churn prediction include decision trees such as CART and C4.5 etc. [29], lo-gistic regression [4], support vector machines [7] and neural networks [28], though random forests [7, 30] are found to be better in performance. Ensemble methods have been used to combine multiple classifiers to construct powerful meta-classifiers and to handle the class imbalance problem typical to churn prediction [5]. Alternatively, approaches from sur-vival analysis have been used to model the dynamics in the churn event rate with user tenure [14]. The churn event rate for users is found to decline with tenure such that new users are more likely to churn than tenured users.

A major hurdle to applying these methods to free-to-use services discussed in this paper is to provide an appropri-ate definition of churn. Studies on user lifetime modeling and retention for online environments have used different criteria for defining the loss of a customer, such as the pe-riod of inactivity [31], decrease in activity [16] or indirectly, via a definition of loyal users [21], discussed earlier. Yang et al. [31] have further studied how user participation pat-terns affect the length of their lifetimes on online knowledge sharing communities. However, most of these methods focus on the length of user participation rather than the volume of their activities. Instead, online businesses are increasingly paying attention to their returning users rather than the count of their registered users. Further, the research com-munity has started concentrating on analyzing and modeling users activities on different types of websites [18, 33]. A ma-jor focus of these methods have been to understand how websites memberships, specifically measured in the number of active users, evolves with time and correlate such factors to the success or failure of the website [25]. Also, many stud-ies on the measurement and improvement of intra-site [2, 17, 10] and inter-site engagement have emerged [32]. Many of these studies identify return time as a robust metric for user engagement. All these factors suggest that continuous tracking and improvement of user engagement, measured in terms of their return time, is crucial for the performance goals of web services. Hence, in this work we directly focus on the return time metric for organizing retention efforts for web services. We use a Cox X  X  proportional hazard regres-sion model [8] from survival analysis for this problem as the model can quantify the impact of covariates on the target event rate. This unique property results in the Cox X  X  Model being a popular choice for several online user behavior stud-ies [10, 31]. Additionally, we also define different types of return time predictors suitable for this problem.

Several types of covariates have been used for churn pre-diction. RFM models [11] propose the use of three variables, Recency, Frequency and Monetary value of their previous in-teraction for identifying potential churners. Other covariates based on demographics, contractual details, service logs, use patterns, complaints and customer service responses [3, 29] have been found useful. We use some of these covariates in our model. In addition, we also incorporate user behavior related covariates in order to understand how user interac-tions while engaging with the service affect the rate of their return in the future. A special feature of our model is that it can handle the recency variable implicitly by computing the expected future time of return for the users given their length of absence from the service.
A user X  X  visitation behavior on a free web service tends to be quite flexible and arbitrary post registration partially due to the lack of financial investments and constraints. Instead, the length of the tenure of users of web service displays a power law distribution with most of the users never return-ing back to the service [9]. In this work, we adopt a unique methodology for analyzing the dynamic user visitation data by directly modeling the user return time.
We define users as belonging to either of the two activity states -the in and the out states. When users visit the service, they are said to be in the in state; while, when they do not visit the service, they are said to be in the out state.
We focus on the problem of predicting the return time of the users which is the time the user spends in the out state. The return time for a user can potentially extend to infinity (for users who never return back to the service). Therefore, a threshold, t d is defined on the return time and we predict the return time for the users up to time t d . The return time prediction problem may be formally defined as follows:
Definition 1 Given that the last time the user was in the in state was at time t 0 , the return time prediction problem is to predict the quantity min ( t r ,t d ), also called the trun-cated return time ( T rd ), where t r is the total time the user spends in the out state and ranges from 0 to  X  , t d is a fi-nite threshold on the return time and either of the following holds: (a) the user is expected to return to the in state at time (b) the user is expected to stay in the out state for at least
Figure 1 provides a diagrammatic representation of the user return time prediction problem.
The time duration between events has been studied exten-sively in queuing theory, for example to study the waiting time between customer arrival and customer service events. Such events are commonly modeled to generate from a Pois-son process such that the waiting times follow the exponen-tial distribution. An attractive property of the exponential distribution is the memoryless property which entails that the future rate of occurrence of the event is independent of the elapsed time. For a random variable T denoting the time of occurrence of the event, the following equation is said to hold if the memoryless property is satisfied:
However, several phenomena are seen to defy the simple memoryless property in interesting ways. For example, the rate of adoption of new products is found to increase with the elapsed time [26]. Alternatively, the rate of events like responses to surveys, promotions [15] etc. is seen to decline with the elapsed time. The decline in future event rate with the elapsed time, has been referred to as  X  X nertia X  . We sus-pect similar type of inertia in user return behavior. For duration data showing time dependence, it becomes mean-ingful to compute the expected future time of the event given the elapsed time, E ( T | T &gt; s ). We, now define the problem of predicting the expected future time of return of the users given their length of absence (LOA) from the service.
Definition 2 Given that the last time the user was in the in state was at time t 0 , and he has already been in the out state for time t s , the future return time prediction problem is to predict the quantity min ( t fr , ( t d  X  t called the truncated future return time T frd , where t fr additional time the user spends in the out state and ranges from 0 to  X  , t d is a finite threshold on the return time and either of the following holds: (a) the user is expected to return to the in state at time (b) the user is expected to stay in the out state for atleast
We consider a time window over which user return time observations are collected. Each return time observation can be associated with a set of covariates influencing its magni-tude. Hence, the data can be represented as a set of tuples: &lt; X,T &gt; where, T is the return time observation and X is the vector of covariates associated with that observation. Since a user can return to the service multiple times during the considered time window, we can have multiple tuples corresponding to a single user.

There are two aspects of the collected data that need spe-cial attention. 1. Censoring: Duration data which is collected over a 2. Recurrent observations: The collected data may con-
Survival analysis is a branch of statistics which deals with the time of occurrence of events, also called duration mod-eling. It offers a rich set of methods which allow us to easily address questions like what is the probability that an event is going to happen after t units of time or what is the future rate of occurrence of the event given it has not happened in t units of time. In this work we deal with discrete measures of time. Two functions are useful for analyzing duration information: The survival function at time t is defined as: where T is a random variable denoting the time of occur-rence of the event. The hazard function measures the instan-taneous rate of occurrence of the event at time t , conditioned on the elapsed time t .
The Cox X  X  proportional hazard model is commonly used to incorporate the effect of covariates on the hazard rate. The model is based on the simple assumption that the covariates affect the magnitude of individual hazard rates but not the shape of the hazard function. Expressed mathematically,  X  ( t ) =  X  0 ( t )  X  exp(  X  1  X  X 1 ( t ) +  X  2  X  X 2 ( t ) + ... ) (4) where,  X  0 is the baseline hazard function, X 1 ( t ), X 2 are the covariates which may be static or may vary with time and  X  1 ,  X  2 etc. are the regression coefficients. The ability of the Cox X  X  model to handle time-varying covariates is an important feature of the model.

One can obtain the survival function from the hazard func-tion using the following equations: where  X  is defined as the cumulative hazard function. The expected time of return can then be computed using the equation below: Furthermore, the expected future time of return given the time not returned for ( t s ) can be computed as follows: The survival function is truncated beyond a point of time or when the probability of survival drops below a threshold in order to prevent the return time estimate from diverging. For our prediction problem, we impose t d as an upper bound on the return time estimate. Hence, the expected return time and the expected future return time computations can be re-defined as:
The Cox X  X  proportional hazard model is a semi-parametric model as it does not assume a mathematical form for the baseline hazard function. Instead, the model can be broken down into two factors. The first factor represents the effect of the covariates on the hazard rate. The effect parameters (regression coefficients) are learnt by maximizing the par-tial likelihood which is independent of the baseline hazard function. Once the regression coefficients have been learnt, the non-parametric form of the baseline hazard function is estimated using the Breslow X  X  method. Cox X  X  seminal pa-per [8] is a good reference for the details of the estimation procedure.

We use the standard survival package in R for estimating the Cox X  X  model. The survival package can handle weighted data instances. We use days as the unit of time for our anal-ysis as most of the users in our datasets are found to return within the first week. A user is considered to have visited the service on a day if he performed at least one activity on the service on that day. One may define more stringent criteria on user activity for this purpose, such as minimum time spent, number of interactions etc. The threshold ( t for the return time prediction problem is set to 60 days, which is a reasonably large value and beyond which users are already the focus of retention efforts. Return time ob-servations larger than 60 days are hence assumed to be cen-sored. In our experiments, we also study the performance of the model for different choices of the threshold.
We now evaluate the performance of the Cox X  X  propor-tional hazard model for solving our proposed return time prediction problem. We consider both the performance of the model at predicting the return time of the user and at classifying users based on their expected return times. Such a categorization procedure is the logical next step for a ser-vice looking to target marketing strategies to users not likely to return soon. For both the problems, we also evaluate how well the Cox X  X  model can incorporate the LOA information by re-estimating the expected future return time given the LOA.
For our experiments we use a small public and a larger proprietary dataset. We briefly describe these two datasets:
We constructed the following covariates for the return time prediction problem.
Different baselines are used for evaluating the performance of the Cox X  X  model at the regression and the classification tasks. All baselines are implemented using the same covari-ates as used in the Cox X  X  model. For the regression problem we compared the Cox X  X  model against simple average (triv-ial baseline), linear regression, decision tree regression (Rep-Tree), Support Vector Machine (with linear kernel) and neu-ral networks (multilayer perceptron). Support Vector Ma-chine Regression took too long to run (more than a day) on our large scale dataset and was omitted in those results. The performance of the models were evaluated using Weighted Root Mean Square Error(WRMSE). The WRMSE is com-puted by weighting the error between the true return time and predicted return time with the weight of the test in-stance as follows: where, N is the number of test instances, T p rd ( i ) denotes the truncated return time predicted for the i-th observation and T rd ( i ) denotes the true truncated return time the i-th observation. We can replace T p rd ( i ) with T p frd ( i ) and T with T frd ( i ) for computing the WRMSE for the expected future return time predictions.

Our classification baselines included logistic regression, random forest, support vector machine (with a linear ker-nel) and neural networks (multilayer perceptron). We used weighted F-measure for the minority class for measuring per-formance at the classification task. The weighted f-measure is defined as the harmonic mean of the weighted precision and weighted recall scores which are defined as follows. The set A denotes the instances actually belonging to the minor-ity class and set P denotes the instances which were pre-dicted to belong to the minority class.
 Weighted Precision = sum of weights of instances in A  X  P
Weighted Recall = sum of weights of instances in A  X  P The experiments for the baselines were conducted using Weka, the open source data mining software available un-der the GNU General Public License. The baselines were suitably tuned using a hold out set. Also, Weka provides support for handling weighted data instances allowing us to easily incorporate the weight vector while training the models. Since a direct application of cross-validation would not maintain temporal ordering between observations of the same user, for our evaluation we took special care to ensure that all recurrent data corresponding to a user belonged to the same fold. This was done by first randomly dividing users into different folds and then placing all observation as-sociated with the user in that fold. As a result, the training and testing folds had observations from different users.
In this section we analyze the results of the experimental evaluation of the Cox X  X  model. We only discuss the parameters of model trained on the Last.fm dataset.

The importance of the covariates for the prediction prob-lem can be assessed using different importance indicators (Table 1). The regression coefficients and the significance score for the covariates can be obtained directly from the output of the R function for fitting the Cox X  X  model. The regression coefficient tells us how much a unit change in the value of the covariate impacts the user X  X  rate of re-turn. For example, with every song the users listened during their last visit, their hazard rate was found to multiply by exp(1 . 315 e  X  03) = 1 . 0013, decreasing their return times estimates. The value of the coefficient was statistically sig-nificant at a significance level of 0 . 01. We found most of the covariates associated with the typical patterns of visitation (Active Weeks, Density, Previous gap) to be highly signifi-cant for predicting the return time variable. Also, some of the engagement/satisfaction related covariates, namely du-ration and % artists had significant effects on the hazard rate. We also computed the mean of the product of the co-variate and its coefficient (MEAN( X  X   X  )) measured for all instances in the training set. This provided an average score for how much the covariate impacted the magnitude of the baseline hazard function. The density covariate impacted the rate of return the most on an average for our dataset. Table 1: Covariate Importance Indicators for the Last.fm Dataset. Signif. codes: 0  X *** X  0.001  X ** X  0.01  X * X  0.05  X . X 
Figure 2 displays the baseline hazard function and the survival function computed for the training dataset from Last.fm. The baseline hazard function has a sharply declin-ing shape typical of processes exhibiting inertia. Hence, the longer users stay away from the service the lesser likely they are of returning within sometime in the future. As a result, it is all the more important for a web service to ensure that its user are motivated to return to the service soon. The survival function has a value of 0.0009 at 60 days. This suggests that 0.09% of users for this dataset did not return within 60 days.
Table 2 and Table 3 display the weighted root mean square error scores obtained using the hazard based approach and the standard regression based approaches for the large-scale proprietary and the Last.fm datasets, respectively. We find that the hazard based approach is superior in predictive per-formance than the other baselines and the improvements are highly significant (p-value &lt; 10  X  10 , using two-tailed paired t-test). The hazard based approach also fares well in terms of run time. On a Intel(R) Xeon(R) CPU X5650 @ 2.67GHz 24GHz, the hazard based approach takes  X  8 minutes as compared to neural networks which take  X  16 minutes to finish one run of cross-validation. Decision tree regression (  X  4 minutes), linear regression (  X  26 seconds) and average (  X  20 seconds) are faster however, the lower run times come at the cost of performance.

As discussed earlier, the hazard based approach allows us to compute the expected future return time for a user given their length of absence (LOA) by incorporating the dynam-ics in the hazard function. We evaluate the performance of the hazard-based approach in updating its prediction given the LOA values. Since the standard regression approaches do not provide similar functionality, we re-learn those mod-els by incorporating the LOA values as a separate feature. The values for this feature is generated by replicating each return time observation T, T times for all values of LOA ranging from (0)  X  ( T  X  1). The future return time is appropriately reassigned to range from ( T )  X  (1). Doing so can significantly increase the size of the dataset. The data instances are re-weighted to ensure that each user still holds a unit weight in the test and the training sets. Due to space limitations we only show the comparisons between two of our baselines: decision tree regression (best performing baseline) and linear regression (because of its ease of use), with the hazard based approach for the large-scale propri-etary dataset. We find that the hazard based approach is superior than both these models in estimating the expected future return time (Fig.3).
 Table 2: WRMSE for User Return Time Prediction us-Table 3: WRMSE for User Return Time Prediction us-
The users are classified into different categories based on their predicted return times. For the Last.fm dataset we bucketed users based on their predicted return times be-ing larger or within 7 days, while for the larger proprietary dataset we classified them based on their predicted return times being larger or within 30 days. The shorter time pe-riod was used for the Last.fm dataset due to scarcity of users in the test set that returned after 7 days. Table 4 and Table 5 provide the performance scores for the hazard based approach and the other baselines for classifying in-stances into the minority class for the proprietary and the Last.fm datasets. The proprietary dataset had 15.4% sam-ples and the last.fm dataset had 12.2% samples belonging Figure 3: WRMSE for different values of LOA for the Table 4: Weighted precision, recall and f-measure scores for the minority class (expected return time &gt; 30 ) for the Large-scale Proprietary Dataset. to the minority class. A naive classifier would have a pre-cision of 0.154 and 0.122, respectively for these datasets. All the models perform better than a naive classifier. Al-though, the hazard based model is not learnt as a classifica-tion model, it still performs superior to the state-of-the-art baselines for our proprietary dataset (p-value &lt; 10  X  8 two-tailed paired t-test) and is comparable in performance to the best performing baselines for our Last.fm dataset. The hazard based approach has the highest recall of all the models which seems to be at the cost of precision. However, for a rare class problem like ours, recall at identifying most of the at-risk users is far more important to a business and the overheads from the lower precision are low. In terms of run time, on a Intel(R) Xeon(R) CPU X5650 @ 2.67GHz 24GHz, the hazard based approach takes  X  8 minutes to fin-ish one run of cross-validation, which is lower as compared to the other baselines: neural network classifier (  X  15 min-utes), logistic regression (  X  11 minutes) and support vector machine (  X  24 minutes). Random forest has the lowest run time of all the models (  X  6 minutes).

We also evaluate the performance of the hazard based ap-proach in classifying users into buckets given the LOA val-ues. Again, the classification baselines do not offer similar capabilities for updating their prediction scores given LOA values. Hence, we incorporate LOA values as an additional feature for classification and replicate instances to populate the values for the feature as done for the standard regression methods earlier. We provide comparison results against the best performing baseline classification approaches -logistic regression and neural networks. We find that the hazard-based approach can incorporate the LOA information and update its prediction much effectively as compared to both logistic regression and neural networks (Fig. 4). Figure 4: Figures (a), (b) and (c) are the plots of the
The threshold ( t d ) was set to 60 days in our experiments, which was a reasonably large value and beyond which users are already the focus of retention efforts. For completeness, we also evaluate our model for some smaller values of the threshold. Table 6 lists the performance of the models at predicting the return time for threshold values of 15, 30 and 45 days. We find that the Cox X  X  model still performs for the Last.fm Dataset. better than the other baselines at the prediction task in these experiments (p-value &lt; 10  X  8 , using two-tailed paired t-test). Table 6: WRMSE for User Return Time Prediction
We use a re-weighting scheme for handling recurrent ob-servations which allows us to retain all data instances for a user in the dataset without biasing the models towards ac-tive users. However, we now evaluate the sensitivity of our results to our weighting schemes by considering alternative approaches for handling recurrent observations. Four such approaches are defined: unweighted, using only the first ob-servation per user, using only the last observation per user and considering only users active on a particular date cho-sen randomly. The last three approaches eliminate recurrent observations by data selection. We use Root Mean Square Error (RMSE) for evaluation. Due to space constraints we only report RMSE results on our proprietary dataset. Table 7: RMSE for User Return Time Prediction with
We find that the Cox X  X  model outperforms the other base-lines when we use only the first or the last observation per user for training and testing the models (p-value &lt; 10 using two-tailed paired t-test). All the models have compa-rable performance when we use the un-weighted scheme or work with user observations recorded on a particular day. Both these scheme also record the lowest errors compared to the other schemes for all the models. We suspect this to happen because both these schemes are dominated by the active users and predicting the return time for such users is much easier. In order to investigate this further, we perform a pilot study in which we hold out a small sample of 1000 return time observations selectively chosen to be longer than 30 days from the proprietary dataset. The performance of different versions of the Cox model trained using the var-ious schemes for handling recurrent observations discussed earlier is then tested at predicting these longer return time observations. The RMSE results are reported in table 8 Table 8: RMSE for Long Return Time Prediction for
These results further show that both the un-weighted scheme and choosing observations from a single day, perform poorly at predicting longer return times. Since the focus of our methods is to find users which are not likely to return soon, these approaches may not be suitable for our application. Furthermore, the weighted scheme offers a good trade-off between using just the first events or just the last events per user in our model making it more suitable for our problem.
In this work, we have focused on the return time perfor-mance metric for free web services. We suggest that reten-tion solutions driven by the projected return time of users can directly address the heart of the problem for web ser-vices, which is to encourage their users to frequently engage with their service. To facilitate such efforts, we formulate the problem of user return time prediction and define several covariates relevant to the problem. The Cox X  X  proportional hazard model is proposed as the model of choice for this pre-diction problem due to several reasons including the ability to handle dynamics in user return rate with time and to incorporate the LOA information. A plot of the prediction performance scores against the LOA values allows a service to identify the right amount of gap since the user X  X  last visit needed to start retention efforts. The performance of the hazard based model is found to surpass all the state-of-the-art baselines considered by us. Finally, we find that the ability of the Cox model to quantify the impact of several important covariates, including those related to user usage patterns, on user return rates to provide important insights that can guide future decision making for the service.
The Cox X  X  model can further accommodate several com-plexities of the real-world quite well. For example, our anal-ysis till now has been limited to static covariates. How-ever, time-varying covariates including those pertaining to external factors such as holiday and weekend effects can be important for return time prediction and can be easily in-corporated in the Cox Model [27]. In our final model for the large scale music service, we incorporated the effect of the day of the month covariate (Fig. 5) on the user return rates. Another direction for future research is to account for heterogeneities among users. Several solutions exist for ei-ther controlling for such differences between users [20] or for extracting different users segments through clustering [19] can also be applied to the return time prediction problem. Figure 5: The regression coefficient for time-varying co-
