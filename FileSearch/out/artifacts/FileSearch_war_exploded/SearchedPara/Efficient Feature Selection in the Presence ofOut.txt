 Feature subset selection is a process of identifying a small subset of highly pre-dictive features out of a large set of candidate features which might be strongly irrelevant and redundant [2,3]. It plays a fundamental role in data mining, in-formation retrieval, and more generally machine learning tasks for a variety of reasons [3]. In the literature, many feature selection methods approach the task as a search problem [3,4], where each sta te in the search space is a possible fea-ture subset. Feature weighting simplify this problem by assigning to each feature a real valued number to indicate its usefulness, making possible to select a subset of features efficiently by searching in a c ontinuous space rather than a discrete state space.

Among the existing feature weighting methods, Relief [5,7,9] is considered one of the most successful ones due to its eff ectiveness, simplicity and efficiency. Suppose we are given a set of input vectors { x n } N n =1 along with corresponding targets { y n } N n =1 ,where x n  X  X  X  X  D is a training instance (e.g., the vector space model of a document) and y n  X  Y = { 0,1,. . . , C -1 } is its label (e.g., the category of the document), N , D , C denote the training set size, the input space dimensionality and the total number of categories respectively. The d -th feature weights w d  X  X  obtained from a convex optimization problem [9]: margin for the pattern x n , H ( x n )and M ( x n ) denote the nearest-hit (the near-est neighbor from the same class) and nea rest-miss (the nearest neighbor form different class) of x n respectively.

However, a crucial drawback [9] of the standard Relief algorithm is that it lacks mechanisms to tackling outliers and re dundant features, which heavily degrade its performance in practice.  X  The success of Relief hinges largely on its attempting to discriminate between  X  The objective function of the Relief algorithm, Eq.(1), is to maximize the The recently proposed Iterative-Relief al gorithm (I-Relief, [9]) addresses these two problems by introducing three latent variables for each pattern and em-ploying the Expectation-Maximization (EM) principal to optimize the objective function. Powerful as it is, this algorithm surfers two drawbacks: ( i )Itisvery time-consuming since there is no close-form solution. Therefore, iterative op-timization scheme must be employed. In p articular, within each iteration, the I-Relief algorithm involves at least O ( N 2 D ) times of computation, which is only tractable for very small data set; ( ii ) I-Relief requires storing and manipulating three N  X  N -sized matrix at each iteration, which is infeasible for large data set.
In this paper, we propose efficient alternative approaches to address the defi-ciencies of Relief in tackling outliers and noises. In particular, in order to handle outliers, we borrow the concept of margin-based loss function [1,6] from the supervise learning literature, and int egrate a loss function into the objective function of Relief, i.e.: instead of maximizing the average margin, this method minimizes the empirical sum of a specific loss function. Since the resulted prob-lem has a close-form solution, this method is much more efficient (in fact, it is of the same complexity as the standard Relief). In the meanwhile, when appropri-ate loss functions are chosen, this method can achieve comparable performance as I-Relief. In addition, to tackling noisy features, we propose a novel algorithm, named Exact-Relief, which is based on a new perspective of Relief as a greedy nonparametric Bayes error minimization feature selection approach. We finally conduct empirical evaluations on various benchmark information retrieval tasks. The results confirm the advantages of our proposed algorithms. 2.1 Against Outliers: Ramp-Relief Relief maximizes the empirical average margin on the training set (see Eq.(1)). An alternative (and equivalent) way to view this is to minimize the empirical sum of a margin-based loss function: where l (  X  ) is a margin-based loss function [1,6]. In this viewpoint, the standard Relief is a special case of the above formulation, i.e., it uses a simple linear loss function l ( z )=-z .

To minimize empirical sum of a specific margin-based loss function has been extensively studied in supervised learning literature both theoretically and em-pirically. This methodology offers various advantages. We refer the interested readers to [6,1] and the references th erein for more detailed discussions.
The new perspective of Relief allows us to extend Relief from using linear loss function to other more extensively studied loss functions. For computational simplicity, we solve an approximate problem in this paper, i.e.: and a variation of the Ramp loss function used in  X  -learning [8] is employed: where z 0 , z 1 and z 2 are three constants. By using the Lagrangian technique, a quite simple close-form solution to problem Eq.(4) can be easily derived, i.e.: tive part.
 We term this algorithm as Ramp-Relief ( R-Relief ). We will show that the R-Relief algorithm is able to deal with outliers as well as I-Relief but is much more efficient and simpler to compute. 2.2 Against Noisy Features: Exact-Relief Recently, we found that Relief greedily a ttempts to minimize the nonparamet-ric Bayes error estimated by k -nearest-neighbor ( k NN) methods with feature weighting as the search strategy [10]. One of the assumptions made by Re-lief is that the nearest neighbor of a pattern x locates close to x in any sin-gle dimensional space. For instance, suppose x a is the nearest neighbor of x : || x b with the computation complexity significantly, it also pays prices. In particular, if the feature set is strongly redundant such that a large proportion of features are x ( d ) , which can heavily degrade the performance of the solutions. Therefore, it may be preferable to eliminate this assumption. For this purpose, we propose an algorithm refereed as  X  Exact-Relief  X ( E-Relief ), which resemble the standard Relief algorithm except using a d ifferent margin definition: m n =( m ( d ) n ) D  X  1 , miss and nearest-hit of x n in the d -th dimension. 2.3 Against Both Outliers and Noisy Features In practice, it is quite possible that both outliers and noisy features are present in the data. For instance, in spam filtering, junk mails usually contain a large amount of noisy characters in order to cheat the filter. On the other hand, legitimate mails may only have very few words but contain many hyperlinks. Such mails not only contain many noisy features but can also be easily detected as outliers. To handle both factors, an obvious strategy is to combine the R-Relief and E-Relief algorithm, i.e.:
We term this algorithm as ER-Relief . It can be easily seen that ER-Relief and E-Relief are of the same complexity, i.e., O ( N 2 D ),whichismuchmore efficient compared to I-Relief, whose worst case complexity is O ( N 3 D ). In this section, we conduct extensive experiments to evaluate the effectiveness and efficiency of the proposed methods in comparison with state-of-art algo-rithms in Relief family. 3.1 Experiments on UCI Data Sets To demonstrate the performance of the proposed algorithms in different infor-mation retrieval tasks, we first perform experiments on three benchmark UCI data sets, namely, the spam filtering data set ( Spam ), the low-resolution satellite image recognition data set ( LRS ) and the speaker-independent speech recognition data set ( Vowel ). To conduct comparison in a controlled manner, fifty irrelevant features (known as  X  X robes X ) are added to each pattern, each of which is an inde-pendently Gaussian distributed random variable, i.e., N (0,20). The efficiency of a feature selection algorithm can be directly measured by its running time. To evaluate the effectiveness, two distinct m etrics are used. One is the classification accuracy estimated by k NN classifier, where k is determined by five-fold cross validation. The other metric is the Recei ver Operating Characteristic (ROC) curve [9], which is used to indicate the abilities of different feature selection al-gorithms in identifying relevant features and at the same time ruling out useless ones. To eliminate statistical deviations, all the experiments are repeated for 20 runs. In each run, the data set is randomly partitioned into training and testing data, and only the training data are used to learn the feature selector. Three groups of experiments have been done: 1. Against outliers. Relief, I-Relief and R-Relief are compared. A randomly 2. Against noisy features. E-Relief is compared with Relief and I-Relief. 50 3. Against outliers and noisy features. E-Relief, R-Relief and their com-3.2 Document Clustering and Categorization We then apply the algorithms to document clustering and categorization tasks. For this purpose, six benchmark text data sets from Trec (the Text REtrieval Contest, http://trec.nist.gov) collection that are frequently used in information retrieval research are selected. The in formation of each data set is also summa-rized in Table.1.

The Relief, I-Relief and ER-Relief algorithms are compared, with no probe or mislabelling. For text clustering, C -mean algorithm is employed to get the clustering result after dimensionality reduction. For simplicity, the number of cluster, C , is set to be the true number of classes. For document categorization, the nearest-neighbor classifier is applied for final classification. Each experiment is repeated for 20 runs, each of which is based on a random splitting of the data set. The Macro ave F 1 and Micro ave F 1 are used to assess the classification results, and ARI (Adjusted Rand Index) and NMI (Normalized Mutual Information) are used to evaluate the cluster ing results. Table.2 presents the best average result of each algorithm.

Again, we observe that ( i ) ER-Relief performs much better than Relief, and that ( ii ) ER-Relief has achieved comparable performances comparably to I-Relief in most cases, although its computation complexity and operating time are much less than I-Relief. Note that the results about the running time are not given due to space limitation.

In information retrieval, huge amount of data and extremely high dimension-ality are two core challenges (and are also becoming increasingly challenging). Therefore, the efficiency of ER-Relief as well as its effective ability to identify a small subset of predictive features (out of a huge amount of redundant ones) may make it a rather appealing and encouraging tool for both challenges, i.e., it is efficient with respect to data set size, and, it is able to effectively reduce the dimensionality. This confirms our attempting in applying ER-Relief to informa-tion retrieval tasks and encourages us to investigate its performance in extensive IR applications in the future.
 Fast growing internet data poses a big challenge for information retrieval. Feature selection, for the purpose of defying curse of dimensionality among others, plays a fundamental role in practice. Relief is an appealing feature selection algorithm. However, it lacks mechanisms to handle outliers and noisy features. In this paper, we have established two algorithms to address these two factor respectively. Compared with the recently proposed I-Re lief, our algorithms are able to achieve comparable performance, while operating much more efficiently, which is proved by extensive experiments on various ben chmark information retrieval tasks.
