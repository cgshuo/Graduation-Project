 Abstract During the last decades the Web has become the greatest repository of digital information. In order to organize all this information, several text categori-zation methods have been developed, achieving accurate results in most cases and in very different domains. Due to the recent usage of Internet as communication media, short texts such as news, tweets, blogs, and product reviews are more common every day. In this context, there are two main challenges; on the one hand, the length of these documents is short, and therefore, the word frequencies are not informative enough, making text categorization even more difficult than usual. On the other hand, topics are changing constantly at a fast rate, causing the lack of adequate amounts of training data. In order to deal with these two problems we consider a text classification method that is supported on the idea that similar documents may belong to the same category. Mainly, we propose a neighborhood consensus classification method that classifies documents by considering their own information as well as information about the category assigned to other similar documents from the same target collection. In particular, the short texts we used in our evaluation are news titles with an average of 8 words. Experimental results are encouraging; they indicate that leveraging information from similar documents helped to improve classification accuracy and that the proposed method is especially useful when labeled training resources are limited.
 Keywords Short text categorization Unlabeled information Prototype-based classification News titles 1 Introduction The tremendous amount of digital content available on the Web has motivated the research and development of different mechanisms that facilitate its search, organization, and analysis. One example of such mechanisms are document categorization methods, which consider the automatic assignment of category labels to free-text documents (Sebastiani 2002 ).

Several approaches have been proposed so far for the automatic categorization of documents. Among them, the leading approach considers the application of supervised learning algorithms, which infer a classification function from a given hand-labeled training set and then use this function to predict the category of new unlabeled documents (Feldman and Sanger 2006 ). In particular, Bayesian models (Lewis 1998 ), Support Vector Machines (Cortes and Vapnik 1995 ), K -Nearest Neighbors (Tan 2005 ) and Prototype-based classifiers (Cardoso-Cachopo and Oliveira 2007 ; Han and Karypis 2000 ; Tan 2008 ) have been successfully used for categorizing documents from different domains.

In recent years, the Internet has emerged not only as a huge data repository but also as an important communication and socialization tool (Makagonov et al. 2004 ; Perez-Tellez et al. 2010 ; Sharifi et al. 2010 ; Sriram et al. 2010 ). As part of this evolution several Web applications have appeared such as wikis, blogs, social networks, and news advertising services, causing an exponential growth of unstructured textual information and a pressing need for their automatic categorization (Go et al. 2009 ; Ostrowski 2010 ; Pinto et al. 2010 ). In particular, this new kind of information poses additional challenges to categorization methods since most of it is in the form of very short documents; consider for instance news titles, which have 8 words on average (Faguo et al. 2010 ; Wermter et al. 1999 ), or tweets that are limited to 140 characters. Short documents are difficult to categorize since they contain a small number of words whose absolute frequency is relatively low, causing the generation of very sparse representations and the inadequacy of frequency-based weighting schemes such as tf-idf (Pinto et al. 2010 ). Current solutions to this problem are mainly based on the idea of expanding the original documents with information extracted from other similar documents (Fan and Hu 2010 ; Tao and Xi-wei 2010 ; Wang et al. 2009 ; Zelikovitz and Hirsh 2000 ) or from WordNet (Perez-Tellez et al. 2010 ). Other works have also considered the application of Word Sense Induction to improve the clustering of short text fragments (Navigli and Crisafulli 2010 ).

In addition to the above mentioned problem, information from this kind of Web applications is very diverse and dynamic. This circumstance makes very difficult the creation of large training sets and, consequently, interferes with the learning of accurate classification models. One well-known solution considers the use of semi-supervised learning methods which take advantage of available unlabeled documents to iteratively generate a better classification model (Guzma  X  n-Cabrera et al. 2009 ; Ko and Seo 2009 ; Xu et al. 2008 ). Other common solutions include the application of crosslingual (Escobar-Acevedo et al. 2009 ; Rigutini et al. 2005 )or transductive (Ifrim and Weikum 2006 ; Kyriakopoulou and Kalamboukis 2006 ) classification methods.

The method proposed in this paper aims to simultaneously address the problems caused by having short-texts and small training sets. Inspired by the popular saying  X  X  X  man is known by the company he keeps X  X , it attempts to improve document classification by using more information to support the decision process. Different to the standard classification paradigm that applies the classification model to each document from the target collection individually (Sebastiani 2002 ), our method is based on the assumption that documents in a close neighborhood may help to reveal the class of a given target document. That is, we propose a neighborhood consensus classification method that assigns classes to documents by considering their own information as well as information about the category assigned to other similar documents from the same target collection.

The evaluation of the proposed method was carried out using the Reuters R8 news collection by considering training sets of different sizes. The results are encouraging; on the one hand, they indicate that neighborhood information can help to improve classification effectiveness by up to 9 %. On the other hand, they demonstrate the appropriateness of our method for categorizing short documents using very small training sets. In particular, our method reached an F -measure of 0.64 on the classification of news titles using only 10 % of the original R8 training set, significantly outperforming traditional classification methods such as Na X   X  ve Bayes and SVM.
 The rest of the paper is organized as follows. Section 2 presents the related work. It mainly describes previous work on short-text classification as well as on learning from small training sets. Section 3 introduces the proposed neighborhood-consensus classification approach while Sect. 4 describes our prototype-based implementation of this approach. Section 5 presents the evaluation of the proposed method on the task of news title classification. An analysis of results is discussed in Sect. 6 . Finally, Sect. 7 shows our conclusions and describes some future work directions. 2 Related work Current text-classification methods are very accurate at classifying large documents, such as scientific papers or news paper articles, but have problems when dealing with short texts. This drop in accuracy has been attributed mainly to the weak signature of the concept being modeled because of the short length of the documents (Healy et al. 2005 ). As a consequence, the majority of the work for short-text classification focuses on expanding the documents with a set of related words. Some methods consider the use of WordNet (Hu et al. 2009 ; Perez-Tellez et al. 2010 ); they expand the documents with synonyms and hyperonyms from their original words. Other methods perform the expansion by including related words extracted from the same (training) document collection by means of co-occurrence statistics (Fan and Hu 2010 ; Perez-Tellez et al. 2010 ; Pinto et al. 2010 ; Tao and Xi-wei 2010 ; Wang et al. 2009 ). This latter approach has achieved satisfactory results but it requires large training sets in order to extract meaningful associations. An alternative solution considers the use of Wikipedia as an external document collection (Banerjee et al. 2007 ).

A different approach focuses on enriching the document representation instead of trying to expand the documents. Methods following this idea consider the application of latent semantic indexing to capture some word relations (Zelikovitz 2004 ) as well as the use of some stylistic features such as the presence of shortening of words, slangs, emphasis on words, and currency and percentage signs among other things (Sriram et al. 2010 ). Similar to the methods discussed above, the successful outcome of approaches aiming at enriching document representations depends on the size of the training set, and some of these methods are only applicable to certain types of documents, such as blog posts.

Regarding the problem of small training sets, the main approach considers the application of semi-supervised learning techniques such as self-training and co-training (Abney 2008 ; Guzma  X  n-Cabrera et al. 2009 ). The key idea behind this approach is to take advantage of available unlabeled documents to iteratively generate a better classification model (Faguo et al. 2010 ; Guzma  X  n-Cabrera et al. 2009 ; Sriram et al. 2010 ). An alternative idea consists of applying a transductive learning strategy (Ifrim and Weikum 2006 ; Kyriakopoulou and Kalamboukis 2006 ). Methods following this strategy aim to build an accurate classifier for a given target collection by considering information of the relations between the target-collection words and the training-set words during the learning of the classification model. These methods have shown to effectively address the problems caused by having small training sets; however, they seem not to be appropriate for classifying short-texts from Web applications. On the one hand, semi-supervised methods tend to produce unstable results when initial accuracy is very low, and, unfortunately, this is typically the case when classifying short-texts. On the other hand, transductive learning methods build classifiers on-the-fly and, therefore, they are not a practical solution when dealing with very dynamic information such as those from news, blogs, tweets and online reviews.

In order to address both problems simultaneously, in this paper we propose a method that carries out the categorization of short documents by considering a neighborhood consensus classification approach. Classification of a document under our method takes into account the content of the document at hand and also the information about the assigned category to other similar documents from the same target collection. This approach differs considerably from previous work in short text classification in that it does not modify the training set or employs target-collection information to build the classification model; instead, it uses this information only to support the classification decision made by a given weak classifier. We consider this characteristic to be important for applications concerning the classification of short texts on the Web since content from news, blogs, tweets, and reviews tends to be very dynamic.
It is worth mentioning that using neighborhood information is not a new idea. In information retrieval this kind of information has been used in very different ways. For example, some approaches have proposed clustering the entire document collection in order to increase recall as well as efficiency searching and browsing on clusters rather than the individual documents, under the assumption that similar documents tend to be relevant to the same queries (Kang et al. 2007 ; Liu and Croft 2004 ).

Other works have proposed applying local smoothing techniques to expand documents with related terms in order to handle polysemy and synonymy phenomena (Huang et al. 2009 ; Kurland and Lee 2004 ; Mei et al. 2008 ; Tao et al. 2006 ). Neighborhood information is also at the center of most query expansion techniques. In particular, methods based on pseudo relevance feedback use terms in the top results of the first retrieval pass as expansion terms. Under this framework, Udapta et al. ( 2009 ) demonstrated that expansion terms by themselves are neither good nor bad, their behavior depends very much on other expansion terms. Based on the observation that  X  X  X  term is known by the company it keeps X  X , they proposed a spectral partitioning method that allows taking a collective decision on all expansion terms instead of independent decisions on individual terms.

In hypertext classification neighborhood information is also very important. Most methods determine the class of documents by considering the category assigned to their neighbors (Sen and Getoor 2007 ). In particular, our approach is very close to those proposed by Oh et al. ( 2000 ) and Angelova and Weikum ( 2006 ). The main difference is that our method does not require or assume any predefined  X  X  X ypertext X  X  structure on the training and target collections. This difference implies that our method does not have a priori information about the association between documents from the same or different classes, which gives flexibility to our approach since in many cases finding such associations could be difficult. Furthermore, our approach is conceptually simpler than previous approaches and can be easily combined with different classification algorithms. 3 Neighborhood-consensus text categorization The task of text categorization involves assigning documents into a set of predefined categories or topics (Sebastiani 2002 ). It can be modeled as the problem of learning a function that maps documents, represented by vectors in an n -dimensional space, to a finite set of categories C  X f c 1 ; c 2 ; ... ; c m g .

In the supervised approach for this task, the classification function is learned from a training set containing sample documents and their corresponding categories, and, in general, the assignment of the category to a new document from a given target collection ( d 2 D ) is carried out as indicated in Eq. 1 , where function c indicates the relationship between document d and categories c j 2 C .
One distinguishing characteristic of this standard approach is that the classifi-cation model is applied to each document individually, in a context-free manner, and therefore, the classification decision is based only on the document X  X  content, disregarding information from other documents in the same target collection. Based on the assumption that short documents do not have enough information for their accurate classification and that similar documents tend to belong to the same category (Driessens et al. 2006 ; Ning and Karypis 2008 ), in this paper we propose a Neighborhood-Consensus Categorization (NCC) approach. In this new approach, class assignments are determined by Eq. 2 , where the classification of document d 2 D considers not only the content of d , but also the category assigned to other similar documents from the same target collection. Here, N d k indicates the set of k nearest neighbors of d in D .
Figure 1 shows the general scheme of the proposed approach. It consists of two main phases. The first phase, called training , carries out the construction of the classifier using a set of labeled documents L . This phase can be accomplished by applying any supervised classification algorithm. The second phase, called classi-fication , involves two processes. First, the identification of the k nearest neighbors for each document from the target collection D , and second, the assignment of the category to each document using Eq. 2 . The following section describes in more detail the implementation of a classification method based on this new approach. Then, Sect. 5 presents some results about its application to the problem of news title classification.
 4 Neighborhood consensus using prototype-based classification As we previously mentioned, the NCC approach can be used in combination with any classification algorithm. In this section we present its implementation using a prototype-based classifier. We decided to use this classifier because it showed the best performance in the classification of news titles. Refer to Sect. 5.3 for a comparison of various classification algorithms in this task.

Prototype-based classification can be summarized as follows (Han and Karypis 2000 ). In the training phase, it considers the construction of one single representative instance, called prototype, for each category. Then, in the classification phase, each given unlabeled document is compared against all prototypes and it is assigned to the category with the greatest similarity score. Our proposed approach extends this traditional method by considering the category assigned to other similar documents from the same target collection. That is, in the classification phase we compute a similarity score for each unlabeled document and class prototype. Then we compute a X  X ombined X  X imilarity score for each unlabeled document that is a linear combination of the similarity score for the document and the class prototype, and the similarity score of the document X  X  neighbors with the same class prototype. The document is then assigned to the class with the largest combined similarity score. In this new approach, the contribution of each neighbor to the final decision is inversely proportional to its distance with the document of interest, as in a weighted k -nearest neighbor method (Tan 2005 ), with the main difference that in our approach the neighbors are other unlabeled documents and not documents from the training set.
The following sections describe the formal adaptation of prototype-based classification to the NCC approach, herein referred as NC-PBC. 4.1 Training phase Given a set of labeled documents L we compute a prototype for each category. There are numerous ways to build the prototypes, in this work we use the well-known normalized sum technique (Cardoso-Cachopo and Oliveira 2007 ; Tan 2008 ). Using this technique, each category c i 2 C is represented by a unitary vector that represents the sum of all documents from that category. Equation 3 defines the computation of the prototype P i corresponding to category c i . For this process, each indicates the frequency of occurrence of the term t i 2 V L in document d , and V L denotes the vocabulary in L .
 4.2 Classification phase The classification phase consists of two main processes: neighborhood identification and class assignment. The following is a description of these two processes. 4.2.1 Neighborhood identification This process focuses on identifying the k nearest neighbors for each document d from the target collection D . The set of k nearest neighbors of a document d ; N d k , is formally defined in Eq. 4 .

In order to identify the set N d k for each d 2 D , this process computes the similarity between each pair of documents from the target collection by means of the cosine similarity (Eq. 6 ), and then, based on the computed similarities, it selects the k nearest neighbors for each document. It is important to mention that for this element indicates the frequency of occurrence of term t i 2 V D in document d , and V D denotes the vocabulary in D .
 where: 4.2.2 Class assignment After the neighborhood identification, this process assigns a category to each document from the target collection by considering both, the class assigned to it by the classifier as well as the class assigned to each document from its neighborhood . For this process each document is represented by a vector defined in the training of occurrence of the term t i 2 V L in document d 2 D . Equation 7 shows our proposed implementation of the NC-PBC method. class  X  d  X  X  arg max
Equation 7 is a derivation of Eq. 2 . It defines the function c as the similarity between prototypes and documents. It also incorporates an influence function used to weight the contribution of each neighbor document, d i , to the classification of d . The purpose of this function is to give more relevance to the closest neighbors. In particular, we define this influence in direct proportion to the similarity between each neighbor d i and d , as computed using the cosine formula (refer to Eq. 8 ). In addition, Eq. 7 uses a constant k to determine the relative importance of both, the information from document d and the information from its neighbors. The lower the value of k is, the greater the contribution of the neighbors, and vice versa.
It is also important to point out that Eq. 7 is similar to that proposed in Tao et al. ( 2006 ) for computing the expanded representation of documents. Both formulas share the idea of weighting the contribution of neighbors, but differ in their purpose; while Eq. 7 uses neighborhood information to determine the category of a given document, the other formula employs that information to build an accurate esti-mation of the document models.

In order to clarify the training and classification phases, Fig. 2 presents the general algorithm of the proposed prototype-based method for neighborhood-consensus text classification. 5 Experimental evaluation 5.1 Datasets For the evaluation of the proposed method we considered the R8 news collection. This collection contains documents labeled with only one class from the eight largest categories of the Reuters-21578 dataset (Lewis 1991 ). It is worth mentioning that we have detected several inconsistencies in the number of documents, as well as in the vocabulary size reported by previous work using this collection. For instance, Pinto ( 2008 ), Pinto et al. ( 2010 ) used 5,839 and 2,319 documents for training and testing, respectively; Anguiano-Herna  X  ndez et al. ( 2010 ) reported 5,198 and 2,075 for training and testing, while Cardoso-Cachopo and Oliveira ( 2007 ) as well as Jiang ( 2010 ) used 5,485 documents for training and 2,189 for testing. We attribute these discrepancies to the application of different preprocessing procedures, especially to the set of documents that contain empty body text, or title and body having only  X  blah blah blah  X  like sentences. Table 1 shows the number of documents per category in the training and test sets from this collection as used in this work. Herein, we will refer to these collections as R8-train-docs and R8-test-docs respectively because they are formed by complete documents consisting of title and body.

Given that our main purpose was to evaluate the effectiveness of the proposed method on the classification of short documents, we assembled a test collection of news titles. We called this collection R8-test-titles. Table 2 shows some information about this new test collection, such as the size of its vocabulary and the average number of words per document. Note that the new test instances (news titles) are very short documents that contain only 8 words on average, and that the vocabulary of the new test collection shows an 80 % reduction in comparison to the original R8-test-docs set.

With the aim of evaluating the proposed method in a realistic scenario consisting of small training sets, we generated four smaller collections from the original R8 training set: R8-train-red50, R8-train-red20, R8-train-red10, and R8-train-red5, which include 50, 20, 10 and 5 % of the original training instances, respectively. Table 3 shows some statistics about these four collections, such as the number of documents in the training set and the vocabulary size. Given that the percentage reduction was applied in a stratified way, the new training sets maintain a very similar imbalanced distribution to that of the original R8-train-docs collection. We performed a random document selection for the construction of the four reduced collections, and we repeated this process five times, generating five different training sets for each reduction percentage. Table 3 indicates average numbers on these collections. 5.2 Evaluation measure The evaluation of the effectiveness of the proposed method was carried out by means of the macro F -measure. This measure is a linear combination of the precision and recall values from all classes c i 2 C and it is defined as follows: 5.3 Experiment 1: Comparison of classifiers in news title classification This experiment consisted in evaluating the effectiveness of several classification algorithms on the categorization of short documents. The objective for carrying out this experiment was to establish a baseline result for comparison purposes since, as far we know, there are no previous results on the R8 collection using only titles. For this experiment we selected the two classification methods that have achieved the best results in the R8 collection using complete documents: a Support Vector Machine (SVM) and a Prototype-Based Classifier (PBC) (Cardoso-Cachopo and Oliveira 2007 ). In addition, we also considered k -Nearest Neighbors (kNN) (Abney 2008 ; Tan 2005 ), C4.5 (Quinlan 1996 ), and naive Bayes (Lewis 1998 ) because they have been successfully applied to different text classification tasks.

For this experiment we used a bag-of-words representation of documents using boolean weights, and employed the Weka implementations of the classifiers when available (Witten and Frank 2005 ). In all cases we used default settings, except for SVM. We evaluated different SVM configurations using polynomial and RBF kernels with different degrees and kernel widths, respectively, and the results reported are those that gave us the highest F -measure. In the case of PBC, which is not implemented in Weka, we used the normalized sum (refer to Eq. 3 ) to construct each class prototype; and assigned the category of a new document d based on the formula class  X  d  X  X  arg max j sim  X  d ; P j  X  , where sim( d , P j ) indicates the cosine similarity between the given document and the prototype of category j . On the other hand, we trained the classifiers using the original R8 training set (R8-train-docs) and used the R8-test-docs and R8-test-titles collections for testing.
 Figure 3 shows the F -measure results for each classifier on the R8-test-docs and R8-test-titles collections. 1 The results are consistent in demonstrating the complex-ity of classifying short documents. In all cases the classification of complete news articles was more effective than the classification of news titles. Table 4 indicates the drop in F -measure for each one of the classifiers. These results clearly indicate that PBC is the most robust approach for news title classification. Based on these results, we considered PBC as the base classifier in our implementation of the NCC approach (refer to Sect. 4 ) We also used it as the main baseline result in the following experiments. 5.4 Experiment 2: Title categorization by neighborhood-consensus This second experiment aimed to evaluate the effectiveness of the NC-PBC method in the classification of short documents. The experiment considered two different scenarios: the classification of complete news articles, and the classification of news titles. In order to carry out this experiment, we trained our classifier using the R8-train-docs set and evaluated the classification effectiveness on the R8-test-docs and R8-test-titles collections. We performed several runs by selecting a different number of neighbors and several values of k . In particular, we used k  X  of k = 1 corresponds to the baseline, i.e., the traditional PBC approach, where information from neighbors is not considered and, therefore, the classification of documents exclusively relies on their own content.

Figure 4 shows the results from this experiment. For each number of neighbors, it plots the average F -measure and the standard deviation for nine different k values. Figure 4 a shows the results for news article classification, whereas Fig. 4 b shows the results for news title classification. These results indicate that our method outperformed the standard PBC approach in both scenarios. In order to evaluate the statistical significance of the improvements that NC-PBC had over PBC, we performed the z-test with a confidence of 95 %. The results of this analysis indicated that improvements on the classification of complete news articles (R8-test-docs set) were not statistically significant at that level, but demonstrated they were for k = 8). An interesting pattern from these results is that, independently from the parameter values, NC-PBC always outperformed PBC, clearly indicating that leveraging information from similar documents helped to improve the effectiveness of the classification of short documents. 5.5 Experiment 3: Neighborhood-consensus classification using small training The previous experiment demonstrated the appropriateness of our method for short document categorization. The purpose of this experiment was to evaluate the effectiveness of the NC-PBC method in a more realistic scenario consisting of small training sets. In order to carry out this experiment we used the reduced collections described in Table 3 . We considered several values of k and k and, because we generated five samples from each reduced collection, we performed five different runs for each experiment. Figure 5 shows the average results of the five runs.
Results from Fig. 5 reveal some interesting findings. First, the traditional PBC emerged as a very robust approach for learning from small training sets. Its effectiveness showed only a slight decrease when the training set was reduced from 50 to 5 %. Second, the proposed NC-PBC method consistently outperformed PBC results, independently from the parameter values. Particularly, the differences between the PBC results and the average results of our method were statistically significant for all datasets using 3 &lt; k &lt; 20 according to the z-test with a confidence of 95 %.

As a summary of these results, Table 5 compares the baseline F -measure and the global-average results obtained by our method for the different training sets. These results confirm that the NC-PBC method consistently outperformed PBC. Moreover, they show that the smaller the training set the greater the improvement, indicating that the proposed NC-PBC method can effectively address the problems caused by having short-texts and small training sets simultaneously. In addition to these conclusions, it is important to point out that NC-PBC greatly outperformed other traditional classification methods, such as SVM and Naive Bayes; for instance, using the R8-reduced-5 % training set, these methods obtained F -measures of 0.194 and 0.190 respectively, indicating an improvement of 200 % by NC-PBC. 5.6 Experiment 4: Neighborhood-consensus classification using news titles Our method has demonstrated good performance in news titles classification and using small training sets of complete documents. Our assumption is that in such a setting we have entire news available to train the model. We consider that situation as a realistic scenario, although we also assume that the complete news documents are few. However, we want to assess the performance of our method in a more drastic scenario, that of training and testing only with titles. Figure 6 shows the results on the R8-test-titles collection when using only titles for training. The behavior is very similar to that shown in Fig. 5 . Note that in this case the performance is, in general, lower than before, but the performance of NC-PBC in comparison with PBC had a larger improvement than in the previous setting. This was expected, because we do not have enough information in the training set to generate good models. 6 Analysis of results 6.1 How important are the neighbors? Results from previous experiments indicate that neighborhood information is relevant for the classification of short documents. In order to have a deeper understanding of this situation we analyzed the similarity of documents and their k nearest neighbors, as well as their (real) distribution across the different categories based on the ground-truth information. Figure 7 shows the average percentage of neighbors in the test set that have the same category than the target document. These percentages are high, confirming our initial hypothesis that documents in a close neighborhood may belong to the same category and, therefore, may help to reveal the class of a given target document.
Results from previous sections also indicate that the NC-PBC method is not very sensitive to the selection of the k value. Based on Fig. 7 , we can conjecture that this behavior is caused by the use of the influence function, which helps to strengthen the information derived only from the very close neighbors.

Regarding the selection of the k value, we analyzed its impact on the classification effectiveness by plotting, for each k value, the average F -measure and the standard deviation for the 20 different k values. Figure 8 a shows the classification results for complete news articles, whereas Fig. 8 b shows the results documents, the greater the relevance of the neighbors. Therefore, these results suggest using small k values for classifying short documents. 6.2 On the selection of parameter values Previous experiments showed that the efficacy of the proposed method varies depending on the values for the parameters k and k . In order to have a deep understanding of the impact of these parameters on the method X  X  performance, Figs. 9 and 10 plot the statistical significance of the improvements that NC-PBC had over PBC on the different collections, in accordance to the z-test with a confidence of 95 %. In these figures a white dot indicates that the achieved improvement was statistically significant, whereas a black dot indicates that our method did not improve the results obtained by PBC or that the performance improvement was not significant.

The interpretation of Fig. 9 is very clear; the proposed method does not show any advantage over the traditional PBC method on the classification of complete news documents. On the other hand, it is significantly better than PBC on the classification of news titles. Regarding the selection of the values for parameters k and k , this figure suggests the usage of several neighbors and low values of k , indicating that neighborhood information is very relevant for short text classification.

Figure 10 confirms the conclusions derived from Fig. 9 . In addition, it shows the robustness of the method with respect to the size of the training data set, indicating that the NC-PBC method can effectively address the problems caused by having short-texts and small training sets simultaneously. 6.3 Comparison with other approaches As far as we know, there are no previous results on the R8 collection using only titles, therefore we cannot compare the performance of our method with previous work. Nevertheless, there are several approaches that use this collection as evaluation corpus for text classification. All these works have considered the classification of complete news documents.

As mentioned earlier, the statistics reported on the R8 collection are inconsistent (refer to Sect. 5.1 for a complete description of this fact). Thus, it is not possible to directly compare our results against previously published work. For comparison purposes, we decided to employ the collection used by Cardoso-Cachopo and Oliveira ( 2007 ) and posted online already preprocessed. We evaluated our method with this preprocessed version. 2 We chose this version of the R8 because Cardoso-Cachopo and Oliveira have reported the best accuracy results on this collection.
Figure 11 compares the accuracy of the proposed NC-PBC method and the best result reported in Cardoso-Cachopo and Oliveira ( 2007 ). The results indicate that our method achieved slightly lower results than the baseline, confirming the conclusion from Sect. 6.2 : when documents are long enough, such as entire news, they contain sufficient information to perform classification, and using a consensus approach is no longer advantageous. 7 Conclusions and future work Inspired by the popular proverb  X  X  X  man is known by the company he keeps X  X , and motivated to address the challenges involved in the classification of short documents using small training sets, we have proposed a new text classification approach. This new method classifies a document by taking into account the content of the document and also the information about the assigned category to other similar documents. We called this approach neighborhood-consensus classification.

In particular, we implemented the proposed approach using the prototype-based classification algorithm. In our implementation, referred as NC-PBC, the decision about the category of each document is determined by the category whose prototype is more similar to it and to its nearest neighbors. This way, the proposed method determines the category of documents taking advantage of the information about the relationships between documents from the same target collection.
 The evaluation of the NC-PBC method was carried out using the well-known Reuters R8 news collection considering training sets of different sizes. The test instances in our setting are the news titles, not the entire documents. Our results revealed the following interesting facts:  X  The classification effectiveness of most learning algorithms was reduced when  X  The proposed NC-PBC method clearly outperformed the traditional PBC in the  X  The NC-PBC method outperformed PBC results for the different reduced
As future work we plan to carry out an extensive analysis of the method on different collections to establish a systematic approach for determining the appropriate values for parameters k and k . In particular, we plan to apply the NC-PBC method on the classification of tweets. The properties of tweets make them a very appealing challenge. They are very short, topically diverse, and tend to use colloquial language, but they are readily available in large quantities. Our approach can also be a good alternative in computer forensic tasks, such as spam and phishing url detection, since they typically consider a great number of related instances that tend to be very dynamic. On the other hand, we plan to evaluate the usage of the proposed method for the selection of documents that will be iteratively included in the training set within a bootstrapping approach. This kind of strategy may be useful for cross-domain and cross-language applications, where training and test distributions are considerably different. References
