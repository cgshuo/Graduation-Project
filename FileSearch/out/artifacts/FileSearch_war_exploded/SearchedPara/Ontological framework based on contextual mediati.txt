 Jason J. Jung
Abstract On the heterogeneous web information spaces, users have been suffering from efficiently searching for relevant information. This paper proposes a mediator agent system to estimate the semantics of unknown web spaces by learning the fragments gathered during the users X  focused crawling. This process is organized as the following three tasks; (i) gathering semantic information about web spaces from personal agents while focused crawling in unknown spaces, (ii) reorganizing the information by using ontology alignment algorithm, and (iii) providing relevant semantic information to personal agents right before focused crawling. It makes the personal agent possible to recognize the corresponding user X  X  behaviors in semantically heterogeneous spaces and predict his searching contexts. For the experiments, we implemented comparison-shopping system with heterogeneous web spaces. As a result, reduced.
 Keywords Collaborative information retrieval . Ontology .
 systems . Focused crawling . Feature manipulation 1. Introduction
The world-wide web is one of the richest online information space. Due to a large amount of information on the web, however, the user has been suffering from the problem to search for relevant information (Baeza-Yates and Ribeiro-Neto, 1999; Chang and Hsu, 1999; Gudivada et al., 1997; Maes, 1994). In order to overcome this problem, a variety of web information retrieval (IR) methodologies have been introduced. Such methods are probabilistic model (Wang and Ng, 2003), semantic analysis based approach (Graupmann et al., 2005), and link analysis model (Amitay et al., 2004; Henzinger, 2000; Plachouras and Ounis, 2005).
Especially, software agents have focused on analyzing relevance feedbacks during users X  searching tasks. This agent module embedded in information retrieval system can analyze a certain user X  X  behaviors, e.g., navigation (or browsing) and interactions for supporting them (Cothey, 2002; Hargittai, 2002). Such agent system can carry out manipulating the searching results by not only sorting (or ranking) them in order of relevance (Deters, 2001; Ruthven et al., 2003), but also filtering out noisy information from the list of them (Jung and Jo, 2003; Mukhopadhyay et al., 2005). Moreover, in multi-agent systems (MAS), a mediator agent plays an important role of coordinating the personal agents, in order to allow them to exchange meaningful information (or knowledge) between each other. Such manipulation tasks of the personal agents are obviously dependent on how efficiently and effectively the mediator agent works.

Now, we have to consider two main characteristics of web information spaces. Firstly, the web information spaces have been getting more dynamic and diverse. It means that the content and structure in a certain web space might be easily and quickly modified. We think that it is a  X  X atural X  phenomenon caused by the abundant information flowing into the web space. On the other hand, second characteristic is the fact that the semantic of each web space is static and unique. Each web space tends to include the information which is related to the specific and unique topics, represented as the consistent linguistic terminologies, organized by local database schema, and annotated with local metadata.

It means, in summary, a large amount of information in a web space is semantically encoded by the corresponding local ontologies. For example, a web space www.shirts.com is a shopping mall selling a variety of shirt products. A set of web pages in this web space include HTML code for representing the information about a certain shirt item, e.g.,  X 79-T05 X . In this code, we have to concentrate on the user-defined XML tag which is an instance of annotation problem is that semantic information is heterogeneous, so that the agents from other web spaces are not able to understand it. In case of the heterogeneity problem caused by linguistic terminology, for representing  X  X he price of product, X  the ontologies can be designed in many different ways like  X  item cost , X   X  product-price , X  and so on. Then, the agents dispatched from different spaces are not able to be successfully working on the particular inference tasks within the heterogeneous environments. In these heterogeneous web spaces, the existing agent systems have shown some drawbacks. Personal agents are semantically biased to only the corresponding web space and local domain knowledge. More seriously, the mediator agent is incapable of sharing information between these agents.

In this paper we propose a multi-agent platform to support focused crawling process on the heterogeneous web spaces. Personal agents on this platform are playing a role of focused crawlers in a non-standard manner. They can gather only the features that are related to a user X  X  interests implicitly expressed through his browsing behaviors on the heterogeneous web spaces. Thereby, we have two main challenges on this platform, as follows. 1. The personal agent has to estimate the corresponding user X  X  interests and formulate local ontologies of web spaces by tracking user X  X  navigation process. Each personal agent accompanies the corresponding user X  X  web accesses, and also, automatically explores the neighboring web space for attempting to find the candidate resources. It is depicted as Tasks (1) and (2) in Fig. 1.
 2. The mediator agent has to merge a set of local ontologies discovered by the personal agents and provide relevant semantics to other personal agents attempting to access to the corresponding web spaces like Task (3) shown in Fig. 1.

Especially, a web service-based framework has been investigated for ontological mediation or composition among heterogeneous web services (Bussler et al., 2002). In e-commerce applications, some studies (Curbera et al., 2004; Pires et al., 2003) have been focusing on enabling open business to business (B2B) interactions on this web service framework.
The outline of this paper is as follows. In the following Section 2, we describe basic ideas of focused crawling, and then the mechanism for estimating user interests and local ontologies from focused crawling on web spaces. Section 3 explains ontological mediation process such as merging and mining consensual knowledge. In Section 4, a simple example and experimental results are shown. Finally, we compare with some related studies and draw conclusions in Sections 5 and 6, respectively. 2. Focused crawling on heterogeneous space
Since the first idea of focused web crawlers was introduced in De Bra and Post (1994), this work has been regarded as a potential solution to the problem of indexing the exponentially growing web. Focused crawling is originally designed to only gather some relevant docu-ments on a specific topic. We can also expect to reduce the amount of network traffic and downloading cost. Basically, focused crawling has to be able to analyze the behavior informa-tion about the users. Examples of such data are social connections (Chakrabarti et al., 1999), a set of bookmarks (Jung, 2005), and a set of URL sequences visited by users (Liu et al., 2004). As depended on the characteristics of a given dataset, the proper methodologies such as heuristic searching, graph matching, time-series analysis, and semi-supervised learning should be chosen.

Now, we consider on the focused crawling process on semantic space in which resources are, in advance, annotated by referring to the local ontologies. The user behaviors taken during browsing these heterogeneous web spaces can be enriched with semantic information extracted from the corresponding resources. Thus, the crawler can estimate the particular intention (or contexts) of the users for assisting their information searching tasks. In fact, we have firstly introduced a preliminary study of focused crawling on semantic space with an example of the product image files assumed to be annotated with the local ontologies of e-commerce business sites (Jung et al., 2005). In order to measure the similarity between the estimated user intention and a set of candidate resources on semantic space, we represent the intention model of user u i as a set of semantic features by the resources selected by users over time. The candidates indicate a set of resources within a user-specified search space of the same web space. In this case, for retrieving the candidate resources, the subspace of which maximum radius of the search space experimentally deter-mined by system administrator has to be defined. Users can expect that in the limited scope of searching space there might be some resource potentially matched with the estimated his the weight value w  X  between [0 , 1], and initial value is defined by the user (in this paper, as 0.5). The temporal adaptability of user model FU ( t ) which is based on (i) adjusting feature weights with (ii) merging features.
 features which are applied to a certain resource together. If f  X  in , the corresponding weight value is reinforced (and the rest are discouraged) by where  X  is the coefficient for the learning rate. Variable number of different features related to the corresponding user U .
 On the other hand, even if feature f  X  is not existing in into related to user intentions and preferences. Meanwhile, we define two ratio measures, and  X   X  , for representing convergence degree of the estimation process, because we want to finish adjusting weighting values at a certain moment (e.g., in our case, when , respectively. The matching ratio  X  ( t ) + is defined as the unmatching ratio  X  ( t )  X  = 1  X   X  ( t ) + . Both ratios are assigned in [0 i is expanded to
The weight values of newly asserted features F ( t )  X  i should be initialized by , in the same way of the previous features X  initialization. As next step, all the weight values in be normalized again by and then, FU i is represented as { f  X  ,w  X  | f  X   X  FS j ,w the error reduction procedure caused by initialization of newly merged features.
Therefore, the personal agents need to measure the similarities between the estimated user intention and the candidate resources on a certain web space, so that they can recommend the most relevant resources among them. In order to evaluate this estimation method for user intentions, we simply formulate three heuristic functions where | F + i | is the number of matched features between the resources equations return the minimum ratio of matched features, the maximum probability among matched features, and the mean probability of matched features, respectively.
Now, we want to briefly describe the problem caused by semantic heterogeneity. As clicking the resources under heterogeneous spaces, the user intention model the similarities computed by Eqs. (6) X (8) are trivial and meaningless. This problem is very similar to the so-called  X  X urse of dimensionality X  in machine learning community. In order to maintain the discrimination power, each agent needs to keep the number of features constant ( turn out that there are some relationships between the features which were unknown before. 3. Ontological mediation
In this paper, we propose a novel method for feature mapping by using partial semantics (e.g., relationships between features) collaboratively discovered during focused crawling in a particular web space. It means that ontological mediation has to be able to recognize how to organize the conceptual structure O of unknown web space
MAS framework is deployed to share the semantic information about the web spaces. The cognitive model of each personal agent A k is encoded by the corresponding agent ontology O A k which is organized as a set of features and the operators relationships between features. Such relationships are subClass , superClass , sibling , and instance . The roles of personal agents can be defined as follows;
X , the partial semantic structure (e.g., the relationships between features) can be extracted.
After feature merging operation in Eq. (2), if the matching ratio Querying Q ( f k  X  , X f k  X   X  F  X  X ). Conversely, when the personal agent resources whose unmatching ratio  X   X  i is over the threshold value relevant information from the up-to-date semantic information about the web space ing agent X  X  performance. Especially,  X  P is related to the efficiency of consensual ontology learning process in mediator. In this paper, both thresholds
Next, we have to consider the mediator agent X  X  functions, which are mainly on the purpose of (i) learning (or uncovering) the local ontologies and (ii) merging ontologies for recon-ciling several ontologies into a single coherent ontology (Noy and Musen, 2003). Here, the mediation tasks can be classified; Knitting K ( O X , f k  X  , rel , f X  X  ). The information received from the agent evidence that can make the semantic structure of the corresponding local ontology more enriched.
 be merged.
 agent can find out the correspondences f  X  which is associated with f  X  by the relations rel intention model user u i and a web space X
Figure 1 illustrates that two users are browsing the web space (e.g., Shirt.Com ) annotated corresponding users to efficiently browse, because their own ontologies applied to model user X  X  interests are heterogeneous with the ontology O X .

From the resources R taken by a user X  X  activities, the partial evidence can be obtained by his agent X  X  operation P . By comparing this partial information with the intention model
FU heterogeneous space, so that it has to conduct picking up the semantic information (e.g.,  X  X olor X  PA 1 , subClass ,  X  X ed X  X ), instead of updating the user intention model. We assume that the user interest model should be time-invariant, and the activities taken by each user be fully consistent within any heterogeneous web spaces. In other words, users are assumed to tend to maintain their own interests while browsing heterogeneous (or unknown) spaces. 3.1. Learning local ontologies
Through focused crawling in heterogeneous spaces, personal agents can obtain partial evi-dence of local ontologies. Because a user continuously takes activities to particular resources relevant to his interest and context, the corresponding personal agent can hypothesize that the resources R from heterogeneous space are significantly matched with right before. It is the process to collect the ontological instances of web space. As shown in the arrows between personal agent and web space in Fig. 2, the personal agent the operation P for estimating the semantic structure and informing them to the mediator agent. This process is based on the matched feature set F + fact, F + is organized as where the first term is the features by simple keyword matching with the corresponding but these are trivial because the mediator have been already aware of be knitted into the local ontology.
 and support the other personal agents, as shown in the arrows between personal agent and mediator agent in Fig. 2. The objective of this process is to find the real local ontology by the best mapping of the ontological instances discovered by personal agents. Thereby, we exploit co-occurrence analysis method based on the extended term-frequency and inverse document-frequency analysis ( TF -IDF ) Salton and Buckley (1998) to discover the underlying correlations. In fact, it has been widely applied to term-document analysis in information retrieval systems. By measuring the correlation coefficient and IDF of each feature, we can extract the major pairs of features for the corresponding relation. In the opposite aspect, this is also regarded as filtering the noisy information out.
With selected the major pairs of features, we can estimate the most significant correlation between features. For the computation given a dataset sent from personal agents about web space X , we represent the aggregation of pairs of two features as the matrices to the relation between features. Each element s ij is assigned by counting the corresponding events invoked by personal agents X  operation P f i , rel , | f | where  X   X  is the user-defined threshold for defining the minimum degree of involvement to f
TF is weighed by w c . Then, the correlation coefficient  X  we have been concerning various relationships between them. From best relationship between f i and f j of which correlation coefficient is maximum should be retrieved by where rel indicates the best relationships between two features.

Finally, the local ontology O X is organized by collecting these results be feature mapping can be changed over time, because the ontological instances by agent X  X  P might be continuously delivered, and they make the mediator X  X 
But, eventually, for a certain web space, this knitting operation of the mediator might be ceased, when the personal agents are efficiently informed of the semantic structures of that local ontology, and the unmatching ratio for their focused crawling is too small to conduct picking operation. 3.2. Bridging local ontologies
After knitting the local ontologies of single web spaces, the mediator agent has to be able to merge them into the global ontology O G through the operation ability can be grounded in ontology reconciliation. The underlying problem, which we call the  X  X ntology alignment X  problem, can be described as follows: given two ontologies O derived from agents R k ), find the relationships (e.g., equivalence or subsumption) that hold between these entities. As a result of alignment tasks, we can find out the correspondences, transform one source into another, and most importantly, formulate the bridge axioms be-tween the ontologies. Figure 3 illustrates a simple example of merging the local ontologies from two web spaces X and Y . When two features (e.g., f 3 is uncovered as equivalent relationship, we can obtain additionally inferable relationships.
For instance, the relationship between f 0 and f 5 turns out rel
Moreover, we can expect to reconcile the conflicts of relationships. Somehow, a pair of fea-tures is possible to be connected with semantically opposite relationships (e.g., subClass and superClass ).

Thereby, in this paper, we want to exploit the similarity maximization method for relaxation of alignment process problem caused by a large number of personal agents. It is because we have to consider the conflicts caused by the heterogeneity problem between them. Especially,
Euzenat and Valtchev (2004) investigate similarity-based alignment by defining a universal properties, and so on. It is based on the principle that the more features of two entities are similar, the more these entities are similar. Hence, we focus on two entities and Sim R , respectively.

Given two knitted ontologies O X , O Y , therefore, the similarity measurement Sim H is formulated by Sim H ( O X , O Y ) = where N ( H ) is the set of all relationships in which H involves. The weights Here, G means a set of components which is able to be decomposed from matching maximizing the summed similarity between the classes. Given two sets of entities
S , S , function Sim G in Eq. (15) represents the maximum similarity value as
Eq. (16), feature similarity Sim F ( f i , f j ) where f i where L , SupC , and SubC are the operators for comparing class labels, superclasses, and subclasses, respectively. Label similarity is computed by several string matching algorithms between two labels. Next, in the similar way, relation similarity Sim R is formulated as Also, L indicates the relation labels.

Finally, with the most similar pair of nodes (i.e., concepts) from given two ontologies, we can construct the correspondences step-wise, at each step selecting the most similar pair and deleting its members from the table. Methods like the Hungarian method allow to find directly the pairing which maximizes similarity.

We want to show a simple example. In Fig. 4, two alignments are occurred. These are showing the best mappings between ontologies. We consider that the relations are only sub-class relation, and the weights are assumed as  X  F between ontology Onto1 and Onto2 , label similarity based on edit (or Levenshtein) dis-tance (Levenshtein, 1996) is measured by sim L ( f i , f j similarities between pairs of features are shown as follows. Then, we can find out that the most similar pair of features appears between  X  X ull Prof X  of
Onto1 and  X  X ull rofessor X  of Onto2 in Eq. (21). As shown in Eqs. (19) and (20), features  X  X erson X  and  X  X rof X  in Onto1 have shown the same label similarities with  X  X rofessor X  in Onto2 . We have to pay attention to the subclasses. Thus, these similarities are calculated by which means that  X  X rof X  in Onto2 has to be aligned to  X  X rofessor X  in Onto1 . Additionally, second alignment between Onto1 and Onto3 based on substring distance (Euzenat, 2004) is shown in the right side of Fig. 4.

The algorithm will then stop whenever no pair remains whose similarity is larger than the user-specific threshold. We consider that some alignment is able to be discarded. For example, the alignment between  X  X aculty X  and  X  X ssistant professor X  in Fig. 4 is useless, so that it can be removed if the threshold is set as 0.5. 3.3. Mining the merged ontologies
As another important issue for learning semantic structures, we exploit a mining process to the ontologies merged by M . This mining process is to discover the largest common semantic substructure of local ontologies. For efficiency, we can begin this process from the aligned features. By doing this, we can extract the consensual ontology useful to the comparison operation C for recommending the relevant web spaces to users.
In order to establish the consensual ontology, we focus on recognizing the most frequent and common subtree patterns from a set of local ontologies estimated. We regard them as sig-nificantly consensual substructures. Of cause, the features discovered by merging operation
M have high probability to be contained in this set. Not only these but also the features which are close to the concepts of common sense (or background knowledge in a certain domain application) might be included. Then, Fig. 5 briefly illustrates the expected area which can organize the elements of the  X  F CO .

Basically, graph (or tree) mining is to discover the maximal frequent substructures from a given graph-structured dataset. Particularly, in subtree mining, PatternMiner and TreeMiner propose a level-wise algorithm based on Apriori scheme (Agrawal and Srikant, 1994) for mining association rules and depth-first searching for using the novel scope-list, respectively (Zaki, 2005). In this paper, thereby, we propose a consensual ontology mining algorithm based on Apriori assumption. The algorithm follows the three steps; (i) initializing a set of candidate features F 1 CO , (ii) expanding a set of candidate features
F
CO based on evaluation. And, the second and third steps are iterated by T constraints such as minimum support are met. It mean that finally we can get the consensual ontology which is composed of T features.

For initialization, as the first step, we have to discover the features that are  X  X ost com-monly X  used in local ontologies for selecting the candidate feature in measured by Sim F , we can count the occurrence of each feature in the ontologies which are merged into the global ontology O G . Thus, a set of candidate features by where  X  SU P is the user-specified minimum support threshold for discriminating whether feature f i can be as a candidate or not (in this paper, we set occur is given by where  X  L is the threshold for removing the features of which label similarity is small. It can check and count the occurrence of the corresponding feature.

In second step, we have to expand the set of candidate features becomes a power set. Based on the Apriori assumption, the set elements are generated from in the previous step t  X  1. Thus,  X  F t CO is represented as where function expand can return all possible combinations of the given set. Finally, we can In third step, we have to evaluate the candidate set  X  F t features, because the relationships between two features f and sometimes, conflicted with each other, depending on the local ontologies merged into global ontology. Basically, on a given ontology O X = ( F (
F , R
R should be justified how suitable it is as a subtree of a local ontology by using the mapping function  X  . Function refine is defined as relationships, respectively. These two  X  functions are formulated by where function U is given by which is the Heaviside unit step function. Here, we design the device for strictly discarding the conflicted pairs of features, although they are frequently occurred in local ontologies. As a result by t iterations of functions expand and refine , we can be provided the consensual ontology F t CO . 3.4. Supporting to conceptualize user intention model
Personal agents have to support the corresponding users to search for the relevant resources by recommending the potential information. Thereby, they have to be able to estimate the user X  X  explained in Section 2, in this paper, we manipulate the set of features extracted from the resources accessed by users during their browsing. By interacting with the ontological medi-ator, the personal agent can be provided with semantic information about heterogeneous web spaces, and conceptualize the user intentions. We note two main tasks that personal agents should be automatically fulfilled; Generating semantic queries, and Fitting the results in user context.

In order to generate the semantic queries, personal agent can be aware of the significant heterogeneity between the estimated user context FU i and the current web space comparing the unmatching ratio  X   X  with the threshold value increasing this threshold value, his personal agent can generate and send the query messages to mediators more frequently and easily. Then, the personal agent semantically identical to the unmatched feature f  X  in web space query, the mediator can conduct the translation operation find out the features that the personal agent can understand the web space rel . Figure 6 illustrates the protocol for communicating between the mediator and personal agents. Hence, as a translation T , the features (e.g., f replaced to the others which are understandable to the agent X  X  ontology, by scanning the global ontologies O G merged by M . Such features  X  f k  X  are associated by semantic equivalence and semantic subsumption. The agent A k keeps asking the mediator, until realizing the semantic relationships between f k  X  and any feature in O A k .

Finally, we want to discuss recommendation task of the mediator agent. The mediator agent has to periodically broadcast the consensual ontology means of learning and enriching the background knowledge of the society of personal agents.
It can reduce the frequency of querying-replying processes between personal agents and the mediator. Additionally, comparing operation C is applied to measure the similarity between the estimated user context FU and the web spaces, so that the mediator can recommend the corresponding personal agent by sending the semantic information about the most related web space X .
 4. Performance evaluation and discussion In order to evaluate our proposed mediation method, we made up web-based testing beds.
It composed of heterogeneous web spaces, each of them containing the resources annotated by its own local ontology. Each web page in a web space can display only one resource (or item), and the web pages are linked with each other. For user study in 25 days, we invited 30 users divided into two groups G  X  (15 users) and G  X  (15 users). All users participated from the first day of experimentation. Each user searched for his ten predefined items during the course of a single day. In addition, they had to design the personal ontologies implying their own preferences, and then, navigate this heterogeneous web space for searching for the relevant information. Only the users in G  X  were supported by ontological mediation, while patterns of the web accesses of each user during browsing our heterogeneous web spaces.
In order to implement our multi-agent architecture, we used JADE platform side. For client-side of users, the web browser was developed by Borland Delphi. tended IRCS browser (Jung et al., 2005) to be capable of communicating with the mediator-side. Figure 7 illustrates the snapshot of IRCS browser. Users can be  X  X mplicitly and contin-uously X  recommended by IRCS browser embedding personal agent system.
 The graphic user interface (GUI) of this browser consists of four parts of frames. Frame
I presents simply HTML documents of requested URL X  X , just like normal browsers, and frame II is a space for only the annotated resources extracted from the current web pages (shown in frame I). As a main function of IRCS browser, frame III is containing a set of the items to recommend to uses, by comparing the estimated user intentions with the candidate hopefully finish the browsing tasks by choosing a particular item recommended in this frame.
Furthermore, users can adjust the amount of recommendations by selecting the particular web spaces they want to compare resources on the frame IV. Meanwhile, the threshold values in this system were determined by the administrator X  X  heuristics. The picking threshold personal agent is the only value the users can control in this paper. 4.1. Scenario and testing dataset
We want to describe a simple scenario for which we applied our multi-agent system, and the information about testing bed. The scenario for experimentation is to let the users conduct comparison shopping tasks in our testing bed. In electronic commerce sites, for the purpose of advertising a specific product, the corresponding multimedia data (e.g., mostly, image files) should be shown up in the web page. System administrators tend to annotate these files by referring to their domain database schema or policies. Most of online shopping malls have been strategically conducting advertisements by using various multimedia data formats that are more sensuous and intuitively identifiable than textual data (Storey et al., 2003).
Especially, image data is the most general and appropriate way to publish the web pages in shopping mall sites like clothes and shoes.

It consists of 17 shopping malls that are semantically heterogeneous. As shown in Table 1, we used three different ontology languages, which are DAML design the local ontologies of web spaces. The size means the number of features in each constructed as generic tree structure of which characteristics are represented as the number of nodes (pages) and maximum depth. Here, we marked six spaces of which branchness (2  X  Page number (Eqs. (6) X (8)) against basic breadth-first search algorithm. Additionally, measurement density of each web space is calculated by Item number Page number
In order to participate in this experiment, each user had to login the IRCS system with his account and password, so that a new session was begun. Right after this, we asked him to select ten items from the whole list of sources in the testing bed. At the moment, he could not be aware of any information about the location of resources (e.g., URLs of web spaces and resources). We allowed the duplication of resources in difference sessions. After the user submits the selected items, he is automatically located to the home web page showing only list of all web spaces. Every time the users finish searching one web space, they have to be back to this home web page. During each session for navigating the testing bed for searching for the selected items, the personal agents had to keep communicating with the mediator agent from the very beginning. Not only the kitting operation for ontology learning but also the recommendation operation had been started together to support personal agents. Of cause, in the early stage, the consensual ontology were not merged enough to recommend relevant resources. Moreover, because our ontology learning approach is based on on-line incremental learning process, consensual ontology is kept merging until the end of experiment. But, after the picking operations of personal agents was decreased by mediator X  X  recommendations, the consensual ontology X  X  merging eventually became finished. 4.2. Experimental results
In order to prove the performance of this system, we acquired and analyzed three kinds of gies and constructing consensual ontology, by interacting personal ontologies and mediator agent. Second is the comparative evaluation between the mediator agent X  X  recommendation process and item-based recommendation algorithms. We compared total number of HTTP requests by each user in both groups to justify how efficiently this mediation system assisted users. 4.2.1. Evaluation of estimating local ontologies by the consecutive steps of personal agent X  X  picking operation knitting operation K . We compared the estimated results with the real local ontologies, and
We found out that most of local ontologies has been very quickly estimated in the early stage. Then, after ending the users X  focused crawling, in the converged state, totally 81.38% estimate over 80% of semantic structures of local ontologies. We also computed the error rate of the estimation process. The largest error value among these results was 6.25% shown uncovering new semantic relationships. It means the duplicated features are detected and filtered out.

Here, we want to discuss relationships between the converging rate of matching ratio and the characteristics of web space. We discover that converging rate is dependent on the density of web space. The densest spaces (e.g., 8th and 10th) needed only 5.8 and 6.5 days for constructing 80% of the local ontologies, respectively. Conversely, the sparest spaces (e.g., 4th, 13th, and 5th) took 17.9, 17.3, and 16.4 days, respectively. 4.2.2. Evaluation of constructing consensual ontology Next, we evaluated the mediator X  X  facilities to merge ontologies ontology CO . During seven days (19th to 25th day), since converged, we applied two mea-surements precision ( P ) and recall ( R ), which are well-known in information retrieval com-munity, to compare the merged ontology of a pair of estimated local ontologies with the reference ontologies ( O H ) manually merged by human experts. These are calculated by
Table 2 shows the average results, where upper and lower diagonal elements mean recall and precision values, respectively.
 Bold values represent the maximum merging pairs, with respects to recall and precision.
More importantly, most of them are located near the diagonals. It means the ontology lan-guages have significant influence on the performance of merging operation. We analyze that the relation similarity function Sim R between ontologies by different languages may cause this problem.

Meanwhile, we found out that precision (76.6%) measurement of merged ontologies is about 29.5% higher than recall, as a whole. This ontology merging process has shown to be reliable with less false-positive error.
 Additionally, we evaluated how efficiently the consensual ontology The size of CO , which is equal to the number of features, was measured over time, as shown in in Fig. 8. It means the mining algorithm for consensus ontology needs relatively adequate semantic structure from local ontologies. At last (since 11th day), the size of consensual ontology was converged to eight. It is only 19.2% of average number of features of local ontologies.
 ontology. A half of features of the consensus ontology are identical with features used in 6th space. We infer that it has shown the highest precision of merging with four other ontologies (in 8th, 10th, 11th, and 12th spaces). 4.2.3. Evaluation of recommending operation
Recommendation process of the mediator agent can be done by combination of two oper-ations, translating T and comparing C . We evaluated this process by using three heuristic functions in Eqs. (6) X (8) proposed to measure the relevance between the annotated resources (e.g., images) and the up-to-date estimated user intentions, as mentioned in previous section. By using each heuristic function, the users have tried to search for the ten predefined items, according to their preferences and intentions. Users were divided into two groups, G  X  (user 1 to user 15) and G  X  (user 16 to user 30). Only users in G  X  were allowed to use IRCS browser.
We want to compare the navigation behaviors done by two user groups. During navigating our testing bed, we counted their HTTP requests including backtracking, and recorded the browsing paths. The coefficient of learning rate  X  was 0.05. Three heuristic functions are named as H1, H2, and H3, respectively.
 Table 3 shows the total number of HTTP requests and browsing paths. The number of were also included. On the other hand, the browsing path means the size of browsing space accessed by the user. The ratio, computed by ( Browsing paths efficiency of information searching tasks, because the less number of HTTP requests implies the higher performance of recommendation of mediation. With respect to the number of
HTTP requests, G  X  had needed only 66.1% (H1), 54.5% (H2), and 58.7% (H3) of HTTP requests by G  X  . Secondly, the size of browsing space by G  X  was also 70.6% (H1), 59.9% (H2), and 64.2% (H3) by G  X  . Based on these facts, we can say that ontological mediation efficiently supported the users via IRCS browser. More precisely, G  X  using IRCS browsing has shown approximately 6.5% (H1), 11.5% (H2) and 10.2% (H3) improvement with respect to the ratio of browsing path to HTTP requests. Given these empirical results, we were able to evaluate the heuristic functions. Then, we found out that H3 function outperforms the other two heuristic functions in the sessions of 21 users.

As another evaluation indicator for the effectiveness of recommendation process, we measured the total browsing time taken for searching for the target items. Every day on the experimentation, the average time of each group has been calculated in Table 4. We found browser were supporting the users with effective recommendation. Particularly, amongst the heuristic functions, heuristic function H1 in both groups has shown better performance, compared to the others. We believe that the user X  X  item selection is only based on the maximum number of features matched with his context. 4.3. Discussion
Overall, we want to discuss our achievements about the challenges described in Section 1, by using the experimental results about the performance and scalability of ontological mediation. 4.3.1. Supporting focused crawling process
As first issue, we have aimed at maximizing user supportability of the personal agents. They have to estimate the corresponding user X  X  context, and compare with arbitrary resources on heterogeneous web information space for meaningful recommendation. From Table 3, we proved that the personal agents have recommended the resources related to the up-to-date context of users, and they have shown about 35.1% improvement (maximum 40.1% in case of H2 function). These recommendations have made irrelevant search space pruned to save searching time without backtracking. More importantly, it had an effect on reducing network traffic. In average, we observed that 40.2% of HTTP requests (maximum 45.5% in case of
H2 function) was diminished. 4.3.2. Construction of global semantic space
The other important challenge is to construct global semantic space. Thereby, the mediator
Eventually, we found out associations between characteristics of web spaces and the per-formance of ontology learning. We realized that, in the early stage, mediator agent has to be designed to be more focused on the web spaces in where resources are more densely located.

As shown in Fig. 10, we are able to compare the ratio of estimation rate between dense spaces and sparse spaces. During first seven days, for sending semantic instances to mediator agents the personal agents X  picking operations in the dense spaces have significantly (over 200%) outperformed those in the sparse ones. Especially, the ratio was about five times larger at the earlier stage (2th X 3th days). Thus, we realized that for optimizing the performance of this system the personal agents should to be exposed of as dense web pages as possible (especially, at the early stage). It means the the recommendation in the premature stage should be generated by considering how many the annotated resources are in the chosen hyperpaths of web pages.

Next issue is ontology languages for global ontologies. We empirically found out that the interoperability between ontologies described by difference languages (RDFS, OWL, and DAML) has still drawbacks. Thereby, we have to consider to develop more powerful translator between different ontology languages in the near future.
 5. Related work
There have been various studies to model and infer the context on user searching tasks in information retrieval (IR) systems. Especially, like our user intention modeling, some studies have proposed methods for modeling contexts from implicit relevance feedback (Implicit
RF). Kelly and Teevan (2003) classified the IR systems with respect to the kinds of observed user behaviors. The interested behaviors are depended upon the goal of target applications.
Internet surfing (Claypool et al., 2001) is related to scrolling and clicking, while bookmark sharing system (Jung, 2004) has to focus on bookmarking, deleting, and reusing. Moreover, some studies have investigated to model the contents accessed by users like binary voting model (White et al., 2006) and this study. In this paper, simply user X  X  browsing pattern is the only behavior that we are interested. But the annotated semantic information, instead of studies for building consensual knowledge base acquired from end-users. Such methods are
CO (Euzenat, 1995) and meta-level patterns (Kim, 2005). In our work, the consensus are represented as a set of features most frequently applied to local ontologies.
With the emergence of semantic web (Berners-Lee et al., 2001), many studies have been focusing on manipulation of ontologies on the web. Especially, we want to mention ontology mapping on the purpose of ontology enrichment. Recently, Shvaiko and Euzenat explained the classification method of ontology alignment (or matching) Kalfoglou and Schorlemmer (2003) and ontology mapping algorithms (Shvaiko and Euzenat, 2005). Several alignment methodologies have been introduced. Since Dieng and Hug proposed an algorithm for match-ing conceptual graphs using terminological linguistic techniques and comparing superclasses and subclasses (Dieng and Hug, 1998), Euzenat developed T-tree to infer the dependencies between classes (bridges) of different ontologies sharing the same set of instances based only on the  X  X xtensions of classes X  (Euzenat, 1994). Additionally, FCA-merge uses formal concept analysis techniques to merge two ontologies sharing the same set of instances while properties of classes are ignored (Stumme and Maedche, 2001). Meanwhile, Cupid is a first approach combining many of the other techniques. It aligns acyclic structures taking into account terminology and data types (internal structure) and giving more importance to leaves (Madhavan et al., 2001). Giunchiglia and Shvaiko developed the algorithm using a complete prover to decide subsumption or equivalence between classes given initial equivalence of some classes and analysis of the relationships in the taxonomy (Giunchiglia and Shvaiko, 2003). Also, Doan et al. studies a machine learning approach based on probabilistic measure-ment and introduces GLUE for finding class similarity from instances (Doan et al., 2004).
For ontology enrichment and population, CROSSMARC (Valarakos et al., 2004) proposed incremental ontology maintenance methodology which exploits ontology population and en-richment methods to enhance the knowledge captured by the instances of the ontology and their various lexicalizations. Reversely, we also should be under consideration about ontol-ogy searching. As a representative example, Swoogle 6 can search for the relevant ontologies for users X  particular queries, and rank them for high usability (Ding et al., 2004). 6. Concluding remarks and future work
With exponentially increasing number of semantically heterogeneous web spaces, we were motivated to overcome the information searching problem based on multi-agent framework. The aim of this paper is seamless interoperability for focused crawling of personal agents. agent) to efficient perform particular collative works between each other. More specifically, our study is basically conducted in 2-level learning process; for local ontologies, learning by instances , and then, for global ontology, learning by federation . While the first level is based on the messages by communications between personal agents and mediator agent, the second level is executed by the mediator X  X  inference engine. We also propose a subtree mining algorithm to find out the consensual ontology from the estimated local ontologies.
It has a power to recognize the upper-level ontologies by socializing the ontologies. Most importantly, we are expecting that this study is very appropriate to many applications (e.g., business process, information integration, and so on) in era of semantic web.
As future work, we, above all, plan to develop the semantic transcoder for translating different ontology languages, because it was the main problem of merging operation. Also, after building consensus ontology, we want local ontologies to be clearly socialized based on extracting social features from relationships between ontologies (Kleinberg, 1999), in order to construct society of ontologies.

Meanwhile, during experimentation, we collected only the user navigational patterns as the only evaluation indicator, since we have been more interested in the implicit relevance feedback of users. But, we have to consider to gather additional behavioral evidence like viewing time, and implement various functions (e.g., keyword-based searching) on IRCS browser system by questionnaires and interviews with users.
 References
