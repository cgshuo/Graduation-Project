 Retrieving images from a large and highly varied image data set based on their visual contents is a very challenging problem. The content-based image retrieval (CBIR) has been studied for decades and many good approaches have been proposed [2, 3] . However, as Arnold mentioned, there is a  X  X emantic gap X  between image features and the user [2] , in another word, similar visual features (such as similar color and shape) do not guarantee similar semantic meanings. The semantic gap between low-level image features and high level semantic concepts has been a major problem in CBIR. 
Recently, the  X  X ag-of-visual-words X  [6] approach exhibits very good performance in image categorization and semantic image retrieval across several well-known different image categories can be represented by different distributions of microstructures (key-points). As an image document can be constantly represented as an unordered collection of key-points which contain rich local information, it can to some extent be regarded as a  X  X ag X  of  X  X isual words X . Image patches containing key-points can be quantified based on affine invariant local descriptors [9, 11-13] . Sivic et al. further proposed the idea of assigning all the patch descriptors into clusters to build a  X  X ocabulary X  of  X  X isual words X  for a specific image set [6] . 
Inspired by the success of vector-space model of the text document representation, the  X  X ag-of-visual-words X  approach usually converts images into vectors of visual words based on their frequency [4, 6] . Many effective text mining and information retrieval algorithms like tf-idf weighting, stop word removal and feature selection have been applied to the vector-space model of visual-words. Problems such as how vocabulary size and term weighting schemes affect the performance of  X  X ag-of-visual-words X  representation are also studied in recent research works [4, 15] . 
Despite the success of  X  X ag-of-visual-words X  in recent studies, however, there are two problems to be concerned. Firstly, since the 'bag of visual words' approach represents an image as an unordered collection of local descriptors, the resulting vector-space model provides little insight about the spatial constitution of the image. Secondly, as most local descriptors are based on the intensity information of images, no color information is used. 
We have seen some works aiming at inco rporating spatial information and color information in the  X  X ag-of-visual-words X  model, such as dividing an image into equal-sized rectangular regions and computing visual word frequency from each region [4] , using multi-scale spatial grids for locally order-less description of visual words [16] and using color SIFT descriptors [17] . However, to the best of our knowledge, there hasn X  X  been any study combining visual-word features with the real spatial constitution of image content. In this paper, we develop a novel image representation method which uses Gaussian mixture model to provide spatial weighting for visual words. The method has been applied to CBIR. As illustrated in Fig. 1, we firstly extract visual tokens from the images data set and cluster them into a lexicon of visual words. Then, we represent the spatial constitution of an image as a mixture of n Gaussians in the feature space and decompose the image into n regions. The spatial weighting scheme is achieved by weighting visual words according to the probability of each visual word belonging to each of the n regions in the image. The cosine similarity between spatial weighted visual word vector pairs is used as distance measurement between regions. The image-level distance is obtained by averaging the pair-wise distances between regions. We compare the performance of our method with the traditional 'bag of visual words' and the 'blobworld' approaches under the same image retrieval scenario. Experimental results demonstrate that our method is able to tell images apart in the semantic level and improve the performance of content-based image retrieval. 
The remainder of this paper is organized as follows. In Section 2, we describe the procedure to generate a lexicon of the visual words for an image data set. In Section 3, we briefly review the process of using Expectation-Maximization (EM) algorithm to iteratively model the parameters of a mixture of Gaussians in the feature space and introduce our spatial weighting schemes. Section 4 reports the experimental results of the proposed method and compares our approach to the traditional 'bag of visual words' approach and the 'blobworld' approach. We conclude the paper in Section 5. In our approach, we adopt the Difference-of-Gaussian (DoG) salient point detector [13] to detect salient points from images. The detection is achieved by locating scale-space extreme points in the difference-of-Gaussian images. The main orientations of salient points are determined by image gradient. After that, image patches containing the salient points are rotated to a canonical orientation and divided into 4 X 4 cells. In each cell, the gradient magnitudes at 8 different orientations are calculated. Finally, each salient point is described by a 128-dimensional SIFT descriptor. Compared to other local descriptors, the SIFT descriptor is more robust and invariable to rotation and scale/luminance changes [11] . In this paper, we name the extracted 128-dimension SIFT descriptors as  X  X isual tokens X . 
At this stage, although we have extracted visual tokens from images, these visual tokens are still too diverse to be analyzed. It is necessary to convert those variant visual tokens into uniform forms (visual words) to facilitate our analysis. Thus, we adopt the K-Means algorithm to cluster visual tokens into groups; the Euclidian distance is used as the distance measurement. 
We use the R-Square (RS) as an indicator of appropriate cluster number. The RS is defined as the ratio of between-group sum-of-square over total sum-of-square. Generally, the RS value increases as the cluster number increases. On one hand, the larger RS value we have, the more patterns we are able to distinguish. On the other hand, the cluster number can not increase without limit due to the computational efficiency. After extensive experimental study, we take RS=0.7 as threshold and set the cluster number to be 1000, which is large enough to represent various images while still computationally efficient. In this way, we establish a lexicon of  X  X isual words X  whose vocabulary size is 1000, with each cluster center as a  X  X isual word X . The Gaussian mixture model (GMM) [12] has been used to model the feature space and the spatial distribution of images [5] . In this paper, based on GMM, we present a novel spatial weighting scheme for visual words as follows. 
Firstly, each pixel is represented by 5-dimensional normalized feature vector, including 3-dimensional LUV color features plus 2-dimensional (x,y) position. In an obtained. Then, each image is assumed to be a mixture of n Gaussians in the 5-dimensional feature space and the Expectation-Maximization (EM) algorithm is used to iteratively estimate the parameter set of the Gaussians. The parameter set of Gaussian; i  X  denotes the d X d covariance matrix; while i  X  represents the prior probability of the i th Gaussian. 
At each E-step of the EM algorithm, we estimate the probability of a particular last maximization step (eq. 1) 
In which j z denotes which Gaussian j y comes from and t  X  is the parameter set toward maximizing the log-likelihood, which is: 
When the algorithm converges, the parameter sets of n Gaussians as well as the probability (| ) pi j are obtained. Based on the estimated GMM model, an image can be decomposed into n regions and max arg max ( | ) j region the given pixel point most likely belongs to (Fig. 2c). 
The spatial weighting scheme is achieved by weighting visual words with regard to each region according to the location of th e detected salient points. Supposing that visual word V , then the summation of ( | ), 1,..., k pi j k M = will indicate the contribution of visual word V to region i . Therefore, the weighted term frequency of V with regard to region i can be defined as: 
Supposing that d i and d j are two D-dimensional (D equals the vocabulary size of visual words) vectors of spatial weighted visual word frequencies, which come from region i and region j , respectively. Then the most natural way to measure the similarity between vectors d i and d j is using the cosine similarity (eq. 3). that, the image-level similarity is obtained by taking the average of the pair-wise similarity between regions in I q and their closest regions in I r . In this section, we conduct a content-based image retrieval experiment based on the proposed image representation method and compare the experiment results with the  X  X ag-of-visual-words X  and the  X  X lobworld X  approaches. 
The image dataset we used is composed of 8 categories, a total of 2,689 outdoor images from the LabelMe dataset [1] . In our experiment, we randomly select 1/6 images from each image category to build the lexicon of visual words. In total, we extract 175,535 visual tokens. At the retrieval stage, we use the selected 1/6 images as query images to retrieve images from the remaining 5/6 images in the data set. The retrieval results are ranked according to the similarity between the query image and all the images in the retrieval set following the similarity measurement in section 3. 
For comparative study, we implement the basic 'bag of visual words' approach and the 'blobworld' approach. In recent study, th ere has been an intense focus on applying term weighting schemes (like tf , idf ) to the  X  X ag-of-visual-words X  feature vectors [4, 6] . Extensive study in [15] suggests that when the vocabulary size of visual words is around 1000, the tf-idf weighting performs best. Recall that in our approach, the vocabulary size of visual word s for the data collection is D =1000, to make the experimental results comparable, we chose to compare our spatial weighting approach with the tf-idf weighted  X  X ag-of-visual-words X  approach [4] . 
The  X  X lobworld X  approach is another well-known image representation method, which simply represents images by the parameter sets of Gaussian mixture models. Following the method in [5], each coherent region is modeled as a multivariate Gaussian. After learning the parameters sets (that is, the mean vector  X  i and the covariance matrix  X  i .) of Gaussians, the KL-divergence for multivariate Normal densities is used as the similarity measurement. 
Fig. 3 represents the over-all precision-recall of our approach, the  X  X ag-of-visual-words X  approach and the  X  X lobworld X  appr oach. As expected, our approach achieves high semantic consistency in CBIR and outperforms both two comparative approaches. Since the contents of different image categories are widely different, it is helpful to compare the performances in individual categories (a briefly description of the three selected categories is represented in Table 1.) 
The precision-recalls of our approach (spatial weighting, short for S), the  X  X ag-of-visual-words X  approach (short for V) and the  X  X lobworld X  approach (short for B) in selected categories are shown in Fig. 4. In categories whose image compositions are highly varied and thus more complicated (such as  X  X oast X  and  X  X all-building X ), our approach is about 10-20 percentage points better than the  X  X ag-of-visual-word X  approach, while in the category whose image compositions are relatively uniform (like  X  X orest X ), the  X  X ag-of-visual-word X  approach performs as well as our approach. Compared to the other two approaches, the  X  X lobworld X  approach works well only when the colors and image compositions are uniform. 
The experiment results suggest that, visual words from different kinds of regions may make the  X  X ag of visual words X  noisy and thus less differentiable. Take the  X  X oast X  images for example, the  X  X rimary X  information about sea and sand beach may be  X  X ontaminated X  by visual words from other  X  X nessential X  parts like boats, buildings and coconut trees. Therefore, the significant improvement in our approach can be explained by the introducing of spatial weighting, which weights visual words according to actual spatial constitution of regions in images. Moreover, the experiment results also suggest that the Gaussian mixture model alone is insufficient to distinguish images which are highly varied in colors and compositions. However, the Gaussian mixture model is still able to provide enough information about the spatial constitutions of images. Although recent studies about the  X  X ag-of-visual-words X  have achieved many good results, the performance of visual words based image representation can be further improved if we properly weight the visual words according to th e spatial constitution of image content. In this paper, the proposed spatial weighting method achieves high semantic consistency in content based image retrieval and outperforms the basic  X  X ag-of-visual-words X  approach in image categories with high inner-variation. The experiment results demonstrate that the GMM model is suitable to model the spatial constitution of regions in images and the spatial weighting method can help us more accurately represent the content of images. Acknowledgments. This work is supported in part by NSF Career grant (NSF IIS 0448023), NSF CCF 0514679. 
