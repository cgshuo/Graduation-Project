 With the prevalence of the Web and social media, users increasingly express their preferences online. In learning these preferences, recommender systems need to balance the trade-off between exploitation , by providing users with more of the  X  X ame X , and exploration , by providing users with some-thing  X  X ew X  so as to expand the systems X  knowledge. Multi-armed bandit (MAB) is a framework to balance this trade-off. Most of the previous work in MAB either models a single bandit for the whole population, or one bandit for each user. We propose an algorithm to divide the population of users into multiple clusters, and to customize the bandits to each cluster. This clustering is dynamic, i.e., users can switch from one cluster to another, as their preferences change. We evaluate the proposed algorithm on two real-life datasets. H.4 [ Information Systems Applications ]: Miscellaneous; H.2.8 [ Database Applications ]: Data Mining multi-armed bandit; clustering; exploitation and exploration
With the rapid growth of the Web and the social media, users have to navigate a huge number of options in their daily lives. To help users in making these choices, content providers rely on recommender systems [1, 10] that learn user preferences based on their historical activities. In a rapidly changing environment [2], where new items appear all the time, relying on historical data alone ( exploitation ) may not work as well. Instead, what is needed is another paradigm that can continually explore the space of user pref-erences ( exploration ) as new items appear, or as users change their preferences. The exploitation vs. exploration trade-off refers to balancing the short term interest of making the next recommendation as accurate as possible, with the long term interest in learning about the users as much as possible (perhaps at the cost of lower accuracy in the short term).
One such paradigm is the multi-armed bandit [14]. A bandit (a recommender system) has multiple arms (items to recommend). Pulling an arm (recommending an item) generates some amount of reward (such as user liking the recommendation). This reward is not known in advance. Because the bandit has multiple chances, the main objective is to maximize the accumulated rewards (or to minimize the regret of not pulling the best arms) over time. For this, the bandit should not just pull the arms that produce the highest rewards in the past, but also explore other arms that could potentially generate even higher rewards in the future.
Multi-armed bandits have been shown to work well in var-ious Web recommendation scenarios, such as advertisements [2], news articles [11], and comments [12]. In many cases, it is advantageous to contextualize the bandit, such that the reward of an arm also depends on the  X  X ontext X  of a recom-mendation, e.g., the content of a Web page (see Section 4).
In this paper, we study the research question of whether there is an appropriate number of bandits to serve a pop-ulation of users. Most current approaches fall into two ex-tremes. One option is to build a single bandit for all users, which has the advantage of scale, in learning from the ob-served rewards of many users. However, a global recommen-dation may not be appropriate for all users. Another option is to personalize it completely, by building one bandit for ev-ery user, which is fully customized to every individual, but may suffer from the sparsity of learning instances.
To address the above disadvantages of the two extremes, we advocate a dynamic clustering approach. In this ap-proach, the population of users are partitioned into K clus-ters. The bandits of individual users in the same cluster can  X  X ollaborate X  in estimating the expected reward of an arm for any one user in the same cluster. That way, we can keep a bandit customized to an individual user, and yet allow users to benefit from the collective set of learning instances in their cluster. Moreover, as a user changes preferences, or as we learn the user X  X  preference better, the user may switch from one cluster to another more  X  X uitable X  cluster.
Contributions. First , we propose a clustering-based contextual bandit algorithm called DynUCB in Section 2, building on the contextual bandit LinUCB [11]. Its novelty arises from dynamic clustering, which we relate to existing work in Section 4. Second , in Section 3, we verify the effi-cacy of this approach through experiments on two real-life datasets, studying the appropriate number of clusters for each dataset, and comparing against the baselines.
We first review the framework of contextual bandit, before introducing our proposed algorithm, which we call DynUCB .
Contextual Bandit. A contextual bandit algorithm proceeds in discrete iterations. At any iteration t , the ban-dit observes a particular user u t . There are also a set of available arms A t that the bandit may choose to pull for this user. For each arm a  X  X  t , the bandit observes its con-text, in the form of a d -dimensional feature vector x t,a Based on the reward experience in previous iterations, the bandit may choose to pull an arm a t . Upon pulling a t , the bandit observes a reward r t,a t . There is no reward observa-tion for a 6 = a t . The bandit therefore needs to learn from the observations of  X  x t,a t ,a t ,r t,a t  X  for ongoing iterations t  X  X  to improve its strategy for choosing arms in future iterations.
After T iterations, the bandit would observe a cumulative reward of P T t =1 r t,a t . The objective is to design an intelli-gent way to choose arms so as to maximize this cumulative reward over time. Equivalently, we can express the objective in terms of minimizing total regret, where regret is defined as the difference between the observed reward r t,a t and the reward of the  X  X ptimal X  arm in each iteration.

A popular framework for contextual bandit is LinUCB [11], which estimates the expected reward of each arm a as a linear regression on the context vector w T x t,a , where w  X  R d is the regression coefficient to be learned. However, maximizing the expected reward alone may result in a long term regret from not discovering a better arm through ex-ploration. Therefore, it also considers the confidence bound  X  of exploration. It is expressed as  X  = 1 + p ln(2 / X  ) / 2, where 1  X   X  is the confidence interval. We set  X  = 0 . 05 for 0.95 con-fidence interval. M  X  1  X  R d  X  d is the update weights, which can be interpreted as the covariance of the coefficient w . The arm a t selected is the one maximizing the upper confidence bound: a t = arg max a  X  X 
Clustering of Contextual Bandits. To build a recom-mender system that serves N users, one option is to build a SINgle instance of LinUCB for all users, which we call LinUCB-SIN. Another option is to train a bandit for every INDividual user, which we call LinUCB-IND. The former benefits more from the wealth of training instances, while the latter benefits from a more customized bandit. However, we hypothesize that a large population of users are neither as monolithic as in LinUCB-SIN, nor as heavily splintered as in LinUCB-IND. Rather, there may be several communities or clusters in the population, where users within the same cluster may share preferences. By grouping together like-minded users, we can benefit from having a larger number of training instances, while still customizing the bandits.
We therefore propose an algorithm, called DynUCB , as described in Algorithm 1. Since the appropriate number of clusters may vary in different domains and populations, the algorithm takes as its input the desired number of clusters K . Initially, we start out with K random clusters, denoted C k for k = 1 ,...,K , and refine the clustering over iterations. In a way, DynUCB still maintains N bandits for N users. For each user u , its coefficient w u is learned from its own bandit parameters b u and M u (initialized to 0 and identity matrix I respectively). However, unlike LinUCB-IND X  X  N independent bandits, in DynUCB the bandits in the same Algorithm 1: DynUCB cluster C k  X  X ollaborate X  with one another. For instance, at iteration t , when generating a recommendation for u t  X  C the estimation of expected reward for each arm a  X  X  t , i.e., level coefficient  X  w k , learned from cluster-level parameters and  X  M k derived from the bandit parameters b u and M u each user u  X  C k . The confidence bound is a simplified version of the theoretical confidence bound shown in [8].
Consequently, each user benefits from the reward experi-ences of other users in the same cluster. The observed re-ward r t,a t from recommending the arm a t to u t is then used to update u t  X  X  own coefficient w u t , which reflects u experience over iterations. Due to the clustering hypothe-sis, u t benefits more from belonging to the  X  X ight X  cluster of like-minded users that complement one another. Therefore, at each iteration, we re-assign u t to the cluster C k 0 whose coefficient  X  w k 0 is closest to w u t , a practice reminiscent of the K-means clustering algorithm but conducted within the con-textual bandit framework. This dynamic re-assignment of clusters is a key feature of DynUCB , allowing it to be adap-tive to changing contexts and user preferences over time.
The objective of experiments is to investigate the effec-tiveness of our proposed method DynUCB . First, we de-scribe the two real-life datasets for experiments. Then, we investigate the effects of different number of clusters, before presenting a comparison against state-of-the-art baselines.
Datasets. We use two publicly-available 1 datasets that have previously been used for contextual bandits evaluation [6]. The first dataset is on the social bookmarking site Deli-http://grouplens.org/datasets/hetrec-2011 cious , where a set of users assign tags to a set of bookmarked URLs. The second dataset is on the online radio LastFM , where a set of users assign tags to a set of music artists. We follow similar processing steps as in [6]. The statistics for these datasets after processing are shown in Table 1.
The task of interest is to recommend a new item to a user, where an item refers to a bookmark URL for Delicious, and a music artist for LastFM. Importantly, for both datasets, the tags are used to generate the contexts for items, as fol-lows. First, we treat each item as a  X  X ocument X  consisting of tags (and their frequencies) assigned by all users. Then, we compute a TFIDF vector for each item from the  X  X ocu-ment X  representations. We further reduce this vector into a 25-dimensional context vector using PCA [9].

Metric. The prediction task is as follows. For every round t of the bandit algorithm, for the user u t , we pick one of her items i randomly. We then present the context vector of the item i , together with 24 other randomly gener-ated context vectors, to a bandit algorithm. If it makes the correct recommendation, i.e., it picks the item i out of the 25 options, the reward is 1. Otherwise, the reward is  X  1 Random guesses are expected to have a cumulative reward of 0. A better algorithm is expected to have a higher positive cumulative reward over iterations. We consider T = 50000 iterations, which is considered large. For all algorithms, we average the cumulative rewards across ten different runs.
Here, we study the relationship between the number of clusters in DynUCB with the cumulative rewards.

Delicious. First, we consider the case of Delicious. Fig-ure 1(a) shows the cumulative rewards of DynUCB after 50000 iterations, for different number of clusters K  X  X . As we increase K from 1 to 256, there is a trend whereby the re-wards at first increase, reaching the peak at around K = 16, and then eventually begin to decrease. This trend helps to validate that the clustering hypothesis indeed applies to the Delicious dataset. We hypothesize that being a social book-marking website, Delicious may support a number of user communities, e.g., technology, music, sports. By clustering the bandits, we can customize the bandits to cater to dif-ferent communities, while still benefiting from the collection of training instances from users of that community. Having too few or too many clusters may be counter-productive, as we begin to clump unrelated users, or to split related users.
It is also interesting to look into the distribution of clus-ter sizes. For K = 16, we get one large cluster containing 58% of the users, and the other 15 small clusters are roughly even-sized, containing between 2% to 5% of the users. These numbers are based on one specific run, but we observe virtu-ally similar distributions across all the runs. This suggests the presence of one main group, and several smaller commu-nities that benefit from having more customized bandits.
LastFM. Figure 1(b) shows a very different picture for the LastFM dataset. It shows that cumulative reward of DynUCB after 50000 iterations is highest for K = 1, and goes downhill for larger K  X  X . This result is very revelatory, suggesting that there are populations, such as in LastFM, where most of the general users pretty much agree in their preferences. This potentially arises from the phenomenon where there are a few artists that practically everyone lis-tens to, unlike social bookmarking (Delicious) where differ-ent users may have different bookmark preferences.
We now compare DynUCB (with the optimal number of clusters found in the previous section, i.e., K = 16 for Deli-cious, and K = 1 for LastFM) against baselines.

Baselines. Since we propose a dynamic clustering ap-proach, we compare to two types of baselines. The first are the non-clustering baselines LinUCB-IND and LinUCB-SIN. The second is a clustering baseline CLUB [8], which is hi-erarchical and does not model dynamic movements between clusters (see Section 4). Because CLUB 2 assumes an input graph, we use a complete graph of users so as not to restrict the clustering that it could discover. We have also tried randomly-generated input graphs as described in [8], but find the results to be worse than a complete graph. We tune its parameter  X  2 in the range 0 to 1, and use the best pa-rameter at 5000 iterations (  X  2 = 0 . 55 for Delicious,  X  for LastFM) to obtain the rewards for 50000 iterations.
Delicious. For Delicious, Figure 2 shows the cumula-tive rewards over iterations up to T = 50000. Evidently, DynUCB at K = 16 has higher cumulative rewards than the baselines over the long run. In the short run (for t &lt; 20000), LinUCB-SIN tends to have a higher cumulative reward, be-cause it benefits from  X  X aster X  learning from the large number of training instances of all users. Since DynUCB partitions the users into different clusters, and begins with random clusters, it learns more slowly in the early stages as it fig-ures out the clustering. In the long run, DynUCB more than catches up, benefiting from more customized bandits in each cluster. LinUCB-IND performs at a similar level, if slightly lower than LinUCB-SIN. Unexpectedly, the clus-tering baseline CLUB does not perform well. Upon further investigation, we observe that 90% of users belong to one cluster, while the other users are splintered into 6 clusters of 2 users each, and 175 independent users. It is unclear if this is an artefact of parameter tuning or the algorithm itself.
LastFM. Figure 3 shows the comparison for LastFM. As previously mentioned, LastFM is not conducive for cluster-
We implement CLUB as there is no publicly available im-plementation at the point of writing. ing, because of the bias for the most popular items. As expected, LinUCB-SIN has the highest reward, for the same reason why DynUCB is optimal for K = 1. DynUCB is second, followed by CLUB with a very similar performance. CLUB keeps virtually all the users (  X  96%) within a sin-gle cluster. LinUCB-IND is the worst, because of the lack of training instances for the independent bandits. Notably, even for a non-conducive dataset, DynUCB does not degen-erate completely (unlike LinUCB-IND), and still manages to get a reasonable performance. We interpret this as the need to fit the right algorithm for the right dataset, based on how well the underlying hypothesis holds for the dataset.
The principle behind bandits is to balance the trade-off be-tween exploration and exploitation. For instance, -greedy [14] picks a random arm (exploration) with probability , and picks the arm with the highest expected reward (ex-ploitation) with probability (1  X  ). Instead of a  X  X andom X  exploration, the Upper Confidence Bound or UCB approach [3, 4] estimates not just the expected reward, but also the confidence interval, of every arm. It then picks the arm with the highest sum of reward and confidence interval, which is the upper confidence bound. Thompson Sampling [7] picks an arm that has the largest success probability.

Contextual bandits make bandit algorithms more adap-tive to the changing  X  X ontexts X . This context is usually ex-pressed as a feature vector. Similar contexts would have cor-related rewards. For instance, LinUCB [11], used as a foun-dation for our method, models the expected reward through a linear regression on context vectors. LogUCB [12] models it through logistic regression.

One related bandit clustering work is CLUB [8]. It mod-els a cluster as a connected component in a graph of users. From the input graph, it slowly removes edges over itera-tions, splintering the graph into multiple clusters. CLUB and our method seek a partitioning of the user population. There are a couple of crucial differences between the two. Firstly, CLUB does hierarchical clustering, whereas we pur-sue a flat clustering. Secondly, and more importantly, our clustering is dynamic, allowing users to move between clus-ters, whereas CLUB only models the splintering of clusters, but not the movement of users across clusters. Other cluster-ing works are based on standard bandits [13, 5]. Previously [11], contextual bandit (i.e., LinUCB) has been shown to outperform standard bandit (i.e., UCB). Social bandits [6] correlate bandits of different users based on a social network graph (which we do not consider).
We investigate the problem of dynamic clustering of con-textual bandits, and propose an algorithm DynUCB . In the case where the clustering hypothesis applies (Delicious), DynUCB achieves a significant gain in rewards when com-pared to non-clustering bandit baselines. This result points to a promising direction of customizing bandits to specific segments of users who may have distinct preferences. As future work, we plan to investigate the clustering hypothe-sis further, to analysize the confidence bound the algorithm, and to consider extensions such as overlapping clusters. This research is supported by the Singapore National Re-search Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office, Media Development Authority (MDA).
