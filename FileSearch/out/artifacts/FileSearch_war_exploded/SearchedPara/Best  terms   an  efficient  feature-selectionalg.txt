 become an important component in many information organization and management language text documents, based on their contents.

There are numerous statistical classification methods and machine-learning tech-the-art text categorization algorithms include support vector machines (SVM) (Dumais 1999), ridge regression (RR) (Zhang and Oles 2001), linear least squares fit (LLSF) (Yang 1999; Yang and Liu 1999) and logistic regression (LR) (Zhang and Oles 2001). Another very well-studied and simple algorithm is naive Bayes (NB) (McCallum and Nigam 1998).

Pedersen 1997; Lewis 1992). This raises big hurdles in applying many sophisticated features used for the representation of documents is an absolute requirement for using many machine learning algorithms in text categorization (Blum et al. 1997). the dataset by removing features that are cons idered irrelevant for the classification. the curse of dimensionality to yield i mproved classification accuracy. therefore, to increase generalization (Sebastiani 2002).
 first searches all documents that belong to c and selects a set of features that predict consists of the union of the two sets of features that are selected during the first and the second step of BT, respectively.
 tiveness of two well-known classifiers, namely NB and SVM.

Sect. 5 provides a brief overview of the document collections used in the experimen-and finally Sect. 7 summarizes the conclusions.
Extensive research work exists on feature selection for text categorization and this is cabularies) that can reach up to tens or even hundreds of thousands depending on the text representation. Such feature set sizes make the application of many classification algorithms infeasible becau se most algorithms do not scale up well with the feature set size. Therefore, reducing the feature s et size enables the application of more so-phisticated algorithms, hopefully grasping more complex relationships inherent in the and Pedersen 1997; Rogati and Yang 2002).

In Blum et al. (1997), the feature-selection methods are grouped into three classes: [Quinlan 1983] and C4.5 [Quinlan 1993]). 3. Those that treat feature selection as a wrapper around the induction process. The the induction algorithm on a separate validation set by employing various iterative strategies, like forward selection or backward elimination (John et al. 1994).
The most popular and a computationally fast approach to the feature-selection problem is the filter approach (John et al. 1994), i.e. keeping the or features that receive the highest scor es according to a function that measures the importance of the term for the text categorization task ( the term space).

Many information-theoretic functions have been used in the literature. A very functions of Table 1 are the Darmstadt Indexing Approach association factor (Fuhr et al. 1991), chi-square ( X  2 ) (Yang and Pedersen 1997; Yang and Liu 1999), the Ng X  X oh X  X ow coefficient (NGL) (Ng et al. 1997; Ruiz and Srinivasan 1999), information gain (IG) (Dumais et al. 1998; Joachims 1998; Yang and Pedersen 1997;
Yang and Liu 1999), mutual information (MI) (Yang and Pedersen 1997), odds ratio (OR) (Ruiz and Srinivasan 1999), relevancy score (RS) (Wiener et al. 1995), and the Galavotti X  X ebastiani X  X imi coefficient (G SS) (Galavotti et al. 2000). For the NGL and GSS coefficients, we use the same naming conventions as in Sebastiani (2002). are the ones distributed most differently in the sets of positive and negative examples of c .
 words and report increased classification accuracy. The same holds for latent semantic occurrence, low-level terms (words) are combined into high-level ones.
Assume a set of documents D = { d 1 , d 2 ,... , d N } { c nary attributes, indicating which words occur and which do not occur. Although there word frequency is captured (McCallum and Nigam 1998; Yang and Liu 1999), our to determine in-class vs. out-of-c lass membership for each category c al. 1998; Joachims 1998; Yang and Liu 1999; Zhang and Oles 2001). which will be in turn used to define the BT algorithm.
 relation holds:
P ( c ) .
 lowing relation holds:
In this subsection, we are providing a general description of the BT algorithm. Given atargetclass c and a score function f (any one of the functions listed in Table 1), the BT algorithm operates, in general, in two steps, as given in Fig. 1.
The first step of the BT algorithm, described on Fig. 1, aims for the selection of a set of positive features that are present in most of the in-class documents. Because effect of the first step of the BT algorithm is the creation of a new, lower dimensional document space that contains the majority of the in-class documents and only a small portion of the out-of-class ones.
 are considered as not carrying any useful in formation and thus they are neither used are immediately classified as negative during classification. The intuition behind this document space, it will provide a better e stimation of cla ss membership. features that are present mostly in the out-of-class documents. These features should facilitate the induction process by maximizin g the distance between the in-class and the out-of-class documents because the for mer will mostly contain positive features while the latter will mostly contain negative features.

In Fig. 2, we provide a more detailed presentation of the BT algorithm.
The variables used in the following complexity analysis of the BT algorithm and the filter approach are  X 
N  X  X he number of training documents  X 
V  X  X he number of features  X 
M  X  X he number of categories ( M &lt; N , in general)  X 
L u  X  X he average number of unique words in a document  X 
L d  X  X he average number of words in a document  X 
L c  X  X he average number of classes with which a document is labeled ( L preprocessing. A complexity analysis of preprocessing indicates that  X  The calculation of word counts costs O ( N  X  L  X  The estimation of D
A complexity analysis of the BT algorithm indicates that:  X  The estimation of D  X  Finding the top-scoring positive feature of a document, in lines 4 X 8, costs O  X  Determining which documents contain at least one feature from F containing each indi vidual term, during preprocessing.  X  Finding the top-scoring negative feature of a document, in lines 14 X 18, costs
Therefore, the execution of lines 13 X 19 costs O ( | D F  X  algorithm itself, but it is required i n order to create the training set. Combining the above costs, we can easily calculate the cost per category as follows: as follows: obtained from the Reuters and the Newsgroups collections (see the experimental results section) have shown that:  X  it holds | D F | X  X  D C | ,if p  X  0 . 25.  X 
For the rest of the functions listed on Table 1, it holds Under the above restrictions, the cost per category is reduced to Summing over all the M categories, we take
However, it holds that total cost of the BT algorithm as
Therefore, the cost of the BT algorithm  X  is linear with respect to the number of training documents N  X  depends on document length rather than vocabulary size (although there is often a strong relationship between these two)
On the other hand, a complexity analysis of the filter approach indicates that  X  Score calculation costs O ( V ) for local filtering and O  X  Sorting costs O ( V  X  logV )  X  Projecting the N training documents onto the new feature space costs O filtering as follows: the total cost of the local filter approach as On the other hand, the cost of global filtering is computed as follows:
One can easily deduce that global filtering costs much less than local filtering; how-features.
 these are expressed in (8) and (11), respectively, we should point out that  X  In large document collections, where N  X  L  X  In small document collections, where N  X  L
In both cases, we assume that L c is close to 1, which is the case for many real-world datasets. of BT.
The basic naive Bayes model (Duda and Hart 1973) assumes that the probability of each word w j occurring in a document d i is independent from the occurrence of other the easy computation of the conditional probability of the class c d as follows:
For the sake of simplicity, we have used the multivariate Bernoulli model, where the word w t occurs at least once in d i (McCallum and Nigam 1998).
The support vector machines algorithm is a relatively new approach to the text clas-
The SVM algorithm treats examples as points in a high-dimensional space and tries
The task is treated as a quadratic optimi zation problem and the resulting hyperplane is used to classify new examples. In our experiments, we have used the linear model that is offered by the SVMlight package (Joachims 1999), due to its time efficiency and its high classification performance (Yang and Liu 1999). 21578 and the 20-Newsgroups collections.
The Reuters-21578 collection consists of 21,578 stories from the 1987 Reuters news-example. All words were converted to lower case, punctuation marks were removed, numbers were ignored and words from a common list of stop-words were removed.
No stemming was used. We have also removed words occurring in at most four training documents. Rogati and Yang (2002) have reported boosted performance with even more aggressive rare word elimination. The resulting dictionary contained 8,067 unique words.
The 20 newsgroups were collected by Ken Lang (Lang 1995) and has become one tains 19,949 nonempty newsgroup postings from 20 different UseNet groups. The 1,000 documents. All headers, except from t he  X  X ubject X  header, were removed, which all words were converted to lower case, punctuation marks were removed, numbers were ignored and words from a common list of stop-words were removed. Again, no stemming was used and words occurring in at most four training documents were re-moved. The resulting vocabulary contained 26,983 words. Furthermore, we combined contained 19,420 unique documents, 527 of which were posted from 2 to 4 different groups. Because there is no training-test split defined for this collection, we decided to sort the documents by post time and keep the last 250 documents of each group as test documents, which resulted in 14,482 training and 4,838 test documents. This to another as time passes. the standard recall, precision and F 1 measures. Given a category c (Van Rijsbergen 1979) is the harmonic mean of recall
These scores can be computed for the bina ry decisions on each individual category globally over all N T  X  M binary decisions (microaveraging), where N number of test documents. evaluation of the BT algorithm versus the filter approach, on the Reuters-21578 and 20-Newsgroups datasets, using the NB and SVM classification algorithms.
Although there are several studies on the popular filter approach (Yang and Pedersen
Table 1. Hence, we have performed first a thorough evaluation of the filter approach the case of no feature selection and the results are presented on Figs. 3 X 4. Figure 3 shows the classification performance of NB and SVM with respect to the vocabulary size for each one of the functions listed in Tab le 1, while Fig. 4 shows the respective more expected:  X 
The performance of the DIA and MI functi ons has been identical, on both datasets, for both NB and SVM. This was actually expected because the MI formula de-rives from the DIA formula after dividin g with the class prior and logging. There-fore, in Fig. 3, we present statistics only for the MI function.  X 
The performance of the DF and RS functi ons has been almost identical, on both datasets, for both NB and SVM, regardless of the value of the damping factor d in the RS formula (we have tested various values of d , from 10 in Fig. 3, we present statistics only for the DF function.  X 
The behavior of SVM has been monotonous on both datasets, regardless of the score function used. Consistent with the results reported in Joachims (1998), the best classification performance of SVM has been obtained when using the maximum number of features, i.e. the cas e of no feature selection. Therefore, in
Fig. 3, we present the classification pe rformance of SVM for that case only (the straight line labeled as svm).  X 
Although the behavior of NB has been monotonous for most of the score func-tions on the Reuters dataset, the same does not hold for the Newsgroups dataset.  X  The 20-Newsgroups dataset has been a much tougher domain to learn than the can achieve a significant reduction to the si ze of the dataset, as one may observe in
Fig. 4. appropriate number of features.

Therefore, in order to perform a thorough evaluation of the BT algorithm, we have run a series of experiments for various values of p . More specifically, we have started with p = 0to p = 1, with stepsize equal to 0.25. This series of experiments has been executed for each one of the functions listed on Table 1 and the results are presented in Figs. 5 X 8. Figure 5 shows the average number of features/class that have been selected from each dataset. Figures 6 X 7 show the classification performance of
NB and SVM on the Reuters-21578 and the 20-Newsgroups datasets respectively, while Fig. 8 shows the respective dataset size reduction. In addition, Table 2 provides the popular filter approach.

Several findings stem from the above experiments:  X 
There is a direct relation between the number of in-class documents and the num-ber of features selected, as one may observe in Fig. 5, which is consistent with the nature of the BT algorithm. Hence, the almost an order of magnitude difference in the average number of features selected on the two datasets is clearly justified, considering that each class of the Newsgroups dataset contains 743 documents on the average, while the respective number on the Reuters collection is just 106.  X 
The experimental results have shown that there is also a direct relation between the number of features selected and the score function used. Functions that favor high-precision features, such as MI or O R, have collected many more features ments.  X 
The use of the BT algorithm has resulted in a significant increase to the classifi-cation performance of NB, in both datasets, as one may observe in Figs. 6 X 7. The increase has been more apparent when using functions that favor high-precision features, such as MI or OR, although the rest of the functions have also provided a considerable improvement over the filter approach. An interesting observation is  X  The use of the BT algorithm has boosted the classification performance of SVM  X  As has been already discussed in Sect. 3, t he use of the BT algorithm has resulted 512 MB RAM. I/O is included in all time statistics. new feature-selection methods. In this paper, we have proposed a new algorithm for feature selection for text categorization, named nest terms (BT). The benefits of BT compared with the various filtering approaches are as follows:  X  BT is faster than the filter approach becau se it is linear to the number of training- X  BT leads to considerable improvement in the running times of the subsequently
