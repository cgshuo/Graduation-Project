 Search is an inherently interactive, non-deterministic and user-dependent process. This means that there are many different possible sequences of interactions which could be taken (some ending in success and others ending in failure). Simulation provides a low cost, repeatable and reproducible way to explore a large range of different possibilities. This makes simulation very appealing, but it also requires care and consideration in developing, implementing and instan-tiating models of user behaviour for the purposes of experi-mentation.

In this tutorial, we aim to provide researchers with an overview of simulation, detailing the various types of simula-tion, models of search behavior used to simulate interaction, along with an overview of the various models of querying, stopping, selecting and marking. Through the course of the tutorial we will describe various studies and how they have used simulation to explore different behaviours and aspects of the search process. The final section of the tutorial will be dedicated to  X  X est practice X  and how to build, ground and validate simulations. The tutorial will conclude with a demonstration of an open source simulation framework that can be used develop various kinds of simulations.  X  Information systems  X  Task models; Search inter-faces; Test collections;  X  Human-centered computing  X  User models; Web-based interaction; Simulation, Evaluation, Performance, Information Retrieval
Simulation provides a complementary approach to tradi-tional forms of evaluation i.e. TREC style batch evaluations and user studies. Recently, there has been a growing inter-est in developing user models and developing simulations to augment these traditional forms of evaluation [31] in order to explore more possibilities and draw deeper insights into search and search behaviour [12, 25]. This is because sim-ulation provides a low cost means of experimentation that enables repeatable and reproducible evaluations to be con-ducted that include interaction. However for the results of such simulations to be credible the models used need to be validated. And so it is important and timely to discuss the state of the art in simulation and how to build credible and what, why, when of simulation, (ii) the high level interac-tion models used for simulation, (iii) simulating the different components: (a) query generation, (b) stopping strategy, (c) decision making, etc, and (iv) how to build and evaluate valid simulations and what-if simulations.
First we will provide an overview of simulation in IR and describe the different ways in which simulation has been used (e.g. simulated work tasks and tracks [22, 47], simulated and synthetic data collections [1, 10, 29, 44], and simulated interaction [12, 24, 25, 34, 48].) The focus of this tutorial will be on how to use simulation to undertake interactive-based evaluations and explorations to determine: 1. how well an IR system performs, and 2. how performance changes under different conditions and behaviours.

We will describe and explain the benefits and drawbacks of simulation and how it fits with other evaluations type (i.e. TREC style, user studies, etc).
 In the next part of the tutorial, we will focus on explaining the different high level models used to simulate the interac-tion between the user and the search system [21, 37, 38, 39, 45, 24].

Here we will cover the state-based stochastic model of the user [21] and the process oriented: Searcher Model [39, 45] and the Complex Searcher Model [37, 38] as shown in Fig-ure 1,. This will provide the necessary background for un-derstanding how the search process is modelled, and what components need to be instantiated when developing a sim-ulation. We will point out the types of tasks that are cur-rently modelled and what we can evaluate with them along with pointing out their limitations and describing avenues for further development. The core actions/components in most high level interaction models are: 1. the application of query (re)formulation strategies; 2. snippet scanning and assessment; 3. snippet clicking; 4. document reading; 5. document assessment; and 6. session stopping.
 So in this part of the tutorial, we will explore in detail each of the components within the high level interaction models and describe and explain the different methods and models used to: (i) generate/formulate queries and query suggestions [1, 10, 21, 23, 29, 33, 46], (ii) determine when users stop and how they browse [24, 23, 37, 38, 45], (iii) interact and provide feedback [19, 27, 28, 32, 33, 35, 41] and (iv) encode the cost of actions [13, 20, 40, 42, 43]. Here we will explain a number of these different studies, and how they have examined the influence of the different components on search performance and/or search behaviors.
 The final part of the tutorial is how to put all the compo-nents together to build a credible and valid simulation, or to create reasonable  X  X hat-if X  simulations. To build a sim-ulation, we will explain the typical method undertaken: This will be followed up by discussion on how to ground and validate simulations -and how simulations can guide empir-ical research and illuminate theory. We will conclude the day by providing a demonstration of a toolkit for simulating interaction [36].
This tutorial will be focused at an introductory to in-termediate level. We assume the participant has a good knowledge of Information Retrieval i.e. knowledge about the TREC style evaluation and the IR process. The tutorial is aimed at masters, graduate students and researchers want-ing to know more about the variety of simulation methods, techniques and tools and how they can use them to enhance their research.
By the end of the tutorial, participants should be able to:  X  Explain the benefits and drawbacks of simulations  X  Describe the different high level interaction models  X  Discuss and compare the different querying, stopping and decision making components  X  Evaluate and measure the performance of systems and behaviours using simulation  X  Design and construct grounded simulations and what-if simulations
Leif Azzopardi is a Senior Lecturer within the School of Computing Science at the University of Glasgow, within the Glasgow Information Retrieval Group. His research fo-cuses on building formal models for Information Retrieval -usually drawing upon different disciplines for inspiration, such as Quantum Mechanics, Operations Research, Microe-conomics, Transportation Planning and Gamification. Cen-tral to his research is the theoretical development models for Information Seeking and Retrieval, where his research interests include:  X  Models for the retrieval of documents, sentences, ex-perts and other information objects [18, 26];  X  Probabilistic models of user interaction and the simu-lation of users for evaluation [1, 9, 11];  X  Economic and optimization of models of interaction [3, 5, 6, 16], specifically how cost and effort affect inter-action and performance with search systems [13, 30];
Over the past ten years, he has developed a number of techniques, tools and methods for simulated evaluations, in-cluding: (i) query generation methods [9, 11, 1], (ii) stopping strategies [37, 38] (iii) interaction styles and search strate-gies [3, 6, 2]. In 2010, he co-organized an ACM SIGIR Workshop on the Simulation of Interaction and delivered a keynote on the  X  X ssimilation of Users X  at the ACM SIGIR 2013 Workshop Modeling User Behaviour for Evaluation. He has also given numerous lectures and invited talks on simula-tion at various universities and at the Information Foraging Summer School (2010-2012). He has also given a series of tutorials on Retrievability [7, 8] (ACM SIGIR 2014, ECIR 2015, ACM ICTIR 2015) and on Formal Models of Informa-tion Seeking, Search and Retrieval [17] (ACM SIGIR 2015, ACM CIKM 2015). [1] L. Azzopardi. Query side evaluation: An empirical [2] L. Azzopardi. Usage based effectiveness measures: [3] L. Azzopardi. The economics in interactive [4] L. Azzopardi. Searching for unlawful carnal [5] L. Azzopardi. Economic models of search. In [6] L. Azzopardi. Modelling interaction with economic [7] L. Azzopardi. The retrievability of documents. In [8] L. Azzopardi. Theory of retrieval: The retrievability of [9] L. Azzopardi and M. de Rijke. Automatic construction [10] L. Azzopardi, M. de Rijke, and K. Balog. Building [11] L. Azzopardi, M. de Rijke, and K. Balog. Building [12] L. Azzopardi, K. J  X  arvelin, J. Kamps, and M. Smucker. [13] L. Azzopardi, D. Kelly, and K. Brennan. How query cost affects search behavior. In Proceedings of 36 th ACM SIGIR Conference , pages 23 X 32, 2013. [14] L. Azzopardi and V. Vinay. Accessibility in information retrieval. In Proceedings of the 30th European Conference on Advances in Information Retrieval , pages 482 X 489, 2008. [15] L. Azzopardi and V. Vinay. Retrievability: An evaluation measure for higher order information access tasks. In Proc. of the 17th ACM CIKM Conference , pages 561 X 570, 2008. [16] L. Azzopardi and G. Zuccon. An analysis of theories of search and search behavior. In Proceedings of the 2015 International Conference on The Theory of Information Retrieval , ICTIR  X 15, pages 81 X 90, New York, NY, USA, 2015. ACM. [17] L. Azzopardi and G. Zuccon. Building and using models of information seeking, search and retrieval: Full day tutorial. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR  X 15, pages 1107 X 1110, 2015. [18] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR  X 06, pages 43 X 50, 2006. [19] F. Baskaya, H. Keskustalo, and K. J  X  arvelin. Simulating simple and fallible relevance feedback. In Advances in Information Retrieval , pages 593 X 604. Springer Berlin Heidelberg, 2011. [20] F. Baskaya, H. Keskustalo, and K. J  X  arvelin. Time drives interaction: Simulating sessions in diverse searching environments. In Proc. 35 th ACM SIGIR , pages 105 X 114, 2012. [21] F. Baskaya, H. Keskustalo, and K. J  X  arvelin. Modeling behavioral factors in interactive information retrieval. In Proc. 22 nd ACM CIKM , pages 2297 X 2302, 2013. [22] P. Borlund. The iir evaluation model: a framework for evaluation of iir systems. Info. research , 8(3), 2003. [23] B. Carterette, A. Bah, and M. Zengin. Dynamic test collections for retrieval evaluation. In Proc. 5 th ACM ICTIR , pages 91 X 100, 2015. [24] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In Proc. 20 th ACM CIKM , pages 611 X 620, 2011. [25] C. Clarke, L. Freund, M. Smucker, and E. Yilmaz. Report on the sigir 2013 mube workshop. SIGIR Forum , 47(2):84 X 95, 2013. [26] R. T. Fern  X andez, D. E. Losada, and L. A. Azzopardi. Extending the language modeling framework for sentence retrieval to include local context. Information Retrieval , 14(4):355 X 389, 2011. [27] D. Harman. Relevance feedback revisited. In Proc. 15 th ACM SIGIR Conference , pages 1 X 10, 1992. [28] K. J  X  arvelin. Interactive relevance feedback with graded relevance and sentence extraction: Simulated user experiments. In Proc. 18 th ACM CIKM Conference , pages 2053 X 2056, 2009. [29] C. Jordan, C. Watters, and Q. Gao. Using controlled [30] D. Kelly and L. Azzopardi. How many results per [31] H. Keskustalo and K. J  X  arvelin. Simulations as a means [32] H. Keskustalo, K. J  X  arvelin, and A. Pirkola. The effects [33] H. Keskustalo, K. J  X  arvelin, and A. Pirkola. Evaluating [34] A. Leuski. Relevance and reinforcement in interactive [35] J. Lin and M. D. Smucker. How do users find things [36] D. Maxwell and L. Azzopardi. Simulating interactive [37] D. Maxwell, L. Azzopardi, K. J  X  arvelin, and [38] D. Maxwell, L. Azzopardi, K. J  X  arvelin, and H. Keskustalo. Searching and stopping: An analysis of stopping rules and strategies. In Proc. 24 th ACM CIKM , pages 313 X 322, 2015. [39] A. Moffat, P. Thomas, and F. Scholer. Users versus models: What observation tells us about effectiveness metrics. In Proc. 22 nd ACM CIKM , pages 659 X 668, 2013. H. Keskustalo, F. Baskaya, D. Maxwell, and L. Azzopardi. Exploring behavioral dimensions in session effectiveness. In Proc. 6 th CLEF , pages 178 X 189, 2015. [41] I. Ruthven. Re-examining the potential effectiveness of interactive query expansion. In Proc. 26 th ACM SIGIR Conference , pages 213 X 220, 2003. [42] M. Smucker. An analysis of user strategies for examining and processing ranked lists of documents. In Proc. of 5 th HCIR , 2011. [43] M. D. Smucker and C. L. Clarke. Modeling optimal switching behavior. In Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval , pages 317 X 320. ACM, 2016. [44] J. Tague, M. Nelson, and H. Wu. Problems in the simulation of bibliographic retrieval systems. In Proc. 3 rd ACM SIGIR , pages 236 X 255, 1980. [45] P. Thomas, A. Moffat, P. Bailey, and F. Scholer. Modeling decision points in user search behavior. In Proc. 5 th IIiX , pages 239 X 242, 2014. [46] S. Verberne, M. Sappelli, K. J  X  arvelin, and W. Kraaij. User simulations for interactive search: Evaluating personalized query suggestion. In Advances in Information Retrieval , volume 9022 of LNCS . 2015. [47] E. Voorhees and D. Harman. TREC: Experiment and Evaluation in Information Retrieval . The MIT press, 2005. [48] R. White, J. Jose, C. van Rijsbergen, and I. Ruthven. A simulated study of implicit feedback models. In Advances in Information Retrieval , volume 2997 of LNCS , pages 311 X 326. 2004.
