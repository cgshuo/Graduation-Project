 Eugenio Cesario  X  Francesco Folino  X  Antonio Locane  X  Giuseppe Manco  X  Riccardo Ortale Abstract Anovelapproachforreconcilingtuplesstoredasfreetextintoanexistingattribute schema is proposed. The basic idea is to subject the available text to progressive classification , i.e., a multi-stage classification scheme where, at each intermediate stage, a classifier is learnt that analyzes the textual fragments not reconciled at the end of the previous steps. Classifica-tion is accomplished by an ad hoc exploitation of traditional association mining algorithms, and is supported by a data transformation scheme which takes advantage of domain-specific dictionaries/ontologies. A key feature is the capability of progressively enriching the avail-able ontology with the results of the previous stages of classification, thus significantly improving the overall classification accuracy. An extensive experimental evaluation shows the effectiveness of our approach.
 Keywords Schema reconciliation  X  Text segmentation  X  Classification 1 Introduction The wide exploitation of new techniques and systems for generating, collecting and storing data has made available a huge amount of information. Large quantities of such data are stored as continuous text. In many cases, this information has a latent schema consisting of a set of attributes, that would in principle allow to fit such textual data into some field structure, so that to exploit the mature relational technology for more effective information management. For instance, personal demographic information typically comprises names, addresses, zip codes and place names, which indicate a convenient organization for the these kind of data. However, the extraction of structure from textual data poses several challenging issues, since free text does not necessarily exhibit a uniform representation.

Foremost, the order of appearance of the attributes across the individual lines of text may not be fixed. In addition, their recognition is further complicated by the absence of both suitable field separators and a canonical encoding format, which is mainly due to erroneous data-entry, misspelled terms, transposition oversights, inconsistent data collection and so forth [ 11 ]. As a concrete example, common issues in personal demographic data are the adoption of abbreviations for both proper names and common terms and the availability of multiple schemes for formatting addresses, phone numbers and birth dates. Also, dis-tinct records may lack different attribute values, which makes them appear with a variable structure. Yet, the same data may be fragmented over disparate data sources, which further exacerbates the aforementioned difficulties.

The notion of Entity Resolution [ 7 , 10 , 23 ], denotes a complex process for database manip-ulation that embraces three primary tasks. Schema reconciliation consists in the identification of a common field structure for the information in a data source. Data reconciliation is the act of discovering synonymies in the data, i.e., apparently different records that, as a matter of fact, refer to a same real-world entity. Identity definition groups tuples previously discovered as synonymies, and extracts a representative tuple for each discovered group.

In this paper we propose RecBoost, a novel approach to schema reconciliation, that adopts classification as an effective mechanism for fragmenting free text into tuples with a common attributes structure. RecBoost works by performing two macro-steps, namely preprocessing and reconciliation. The former step is primarily thought for formatting the individual lines of text, with potentially-different encoding format, into a uniform representation. Domain-specific ontologies and dictionaries are then exploited to associate each such a token with a label denoting its ontological or syntactic category. Reconciliation is eventually accom-plished in terms of progressive classification , i.e., a multi-stage classification scheme where, at each intermediate stage, a classifier is learnt from the previous classification outcome, thus being specifically targeted at handling with those textual fragments not reconciled yet.
The main contribution of this paper is thus a methodological approach in which a strict cooperation between ontology-based generalization and rule-based classification is envis-aged, which allows to reliably associate terms in a free text with a corresponding semantic category. A key feature is the introduction of progressive classification , which iteratively enriches the available ontology, thus allowing to incrementally achieve accurate schema rec-onciliation. This ultimately differentiates our approach from previous works in the current literature, which adopt schemes with fixed background knowledge, and hence hardly adapt to capture the multi-faceted peculiarities of the data under investigation. Moreover, due to the variable number of classification stages, RecBoost gives the user finer control over the trade-off between accuracy (i.e., the proportion of correctly classified tokens w.r.t. the classi-fication behavior of the overall RecBoost system) and recall (i.e., the proportion of correctly classified tokens w.r.t. the actual tokens to reconcile). In practice, the user can choose a classifier with a trade-off satisfying the requirements of the specific application.
Still, the approach is further strengthened by the adoption of local rule-based classifica-tion models , i.e., patterns of term co-occurrence associated with specific class labels. Local models work practically well in combination with progressive classification, since they only handle the local specificities they are able to cope with, and postpone the unknown cases to subsequent classification stages. By contrast, traditional approaches from the literature exploit global classification models, which are more prone to overfitting when dealing with the several contrasting specificities occurring across individual sequences. In addition, the combination of rule-based classification models with domain-specific ontologies makes the generic RecBoost classifier very intuitive, i.e., easier to interpret than state-of-the-art proba-bilistic methods, such as DATAMOLD [ 3 ] and Mallet [ 18 ].

The outline of the paper is as follows. Section 2 introduces the basic notation for the problem we face; next, it continues by covering details on the process adopted to learn a generic rule-based classifier and, then, proceeds to examine the RecBoost methodology. The architecture of RecBoost is discussed in Sect. 3 . The overall RecBoost methodology is then elucidated in Section 4 . Sect. 5 presents the results of an intensive experimental evaluation. Section 6 overviews and evaluates works from the literature, that are most closely related to our study. Finally, Sect. 7 draws some conclusions and highlights a number of interesting directions, that are worth further research. 2 Text segmentation with RecBoost To formalize the Schema Reconciliation problem, we assume the basic definitions in the following section. 2.1 Notation and preliminaries An item domain M ={ a 1 , a 2 ,..., a M } is a collection of items .Let s be a sequence a ,..., a m where a i  X  M . The set of all possible sequences is denoted by M  X  .Ingen-Moreover, we denote the subsequence a 1 , a 2 ,..., a k  X  1 as pre s ( a k ) , and the subsequence a + 1 , a k + 2 ,..., a n as post s ( a k ) .

A descriptor R ={ A 1 ,..., A n } is a set of attribute labels. A descriptor corresponds to a database schema, with the simplification that, for each attribute label A i , domain information is omitted. Thus, our specific problem can be viewed as follows: given a descriptor (database we want to segment each sequence s i into subsequences s 1 i ,..., s n i , such that each token a  X  s h i is associated with the proper attribute A j .

For example we may want to fit an unstructured collection of personal demographic infor-mation representing names, addresses, zip codes and cities, in a proper schema with specific fields for each category, as exemplified in Fig. 1 .

Text reconciliation can be profitably employed in several contexts. We briefly mention a wide range of major applications, to provide a taste of the generality of our approach. The harmonization of postal addresses affects large organizations like banks, telephone compa-nies and universities, which typically collect millions of unformatted address records. Since each address-record can, in principle, be retrieved from a different data source (designed with different purposes), variations in the way such records are stored are far from unusual. Further applicative scenarios requiring to deal with the schema reconciliation problem include pro-cessing bibliographic records, collections of information about products, medical sheets, and so forth.

A typical applicative setting which motivates our work is the mentioned entity resolution problem, which roughly consists in discovering/disambiguating duplicate tuples [ 7 , 10 , 21 ]. When tuples consist of free text, which however contains a hidden structure, tuple disambig-uation can be accomplished by exploiting either exact matching techniques, based on specific segments of the strings, or simpler (fuzzy) techniques, that ignore segmentation. Clearly, ex-act matching is more reliable, provided that the original text is correctly segmented. Consider, e.g., the strings that clearly represent the same entity. A correct segmentation of the strings would eventually ease the task of recognizing the similarity.

In principle, various schemes may be followed to catch string resemblance. Without loss of generality, we here focus on two basic approaches, namely Jaccard similarity and weighted Jaccard similarity. The former measure catches resemblance between any two strings by mea-suring their degree of overlap, i.e., the proportion of common tokens. Formally, the Jaccard similarity between the above strings s 1 and s 2 is defined as The latter measure weights segment matches by the relevance of the corresponding attributes. Let w 1 ,w 2 ,w 3 ,w 4 be four weights respectively denoting the relevance of segment matches in correspondence of the attributes NAME , ADDRESS , CITY and STATE . The definition of the weighted Jaccard similarity between the foresaid strings s 1 and s 2 is where entities a i and b i denote two corresponding segments, respectively within s 1 and s 2 .
It is easy to see that d J ( s 1 , s 2 ) = 0 . 44. Such a result does not fully reflect the evident similarity of s 1 and s 2 , which may generally prevent their disambiguation. To overcome such a limitation, it is sufficient to reasonably assume that a low degree of overlap between string segments corresponding to the ADDRESS and STATE attributes should not heav-ily penalize the overall similarity. Indeed, multiple addresses can be associated to an indi-vidual in a same city, whereas the latter resides in only one state. By accordingly fixing w more appropriately reflects the actual resemblance between the two strings under comparison. This confirms that exact matching techniques enable more effective string de-duplication, whenever the original strings can be accurately segmented.

Thus, a deduplication system adopting a text segmentation methodology, as described in the figure below, would effectively leverage its performance, provided that the embedded segmentation methodology is reliable.

Reliability here has a strict meaning: strings should be correctly segmented, and errors in segmenting should be absolutely avoided. Indeed, a wrong segmentation would likely result in a worsening of the deduplication effectiveness, even when compared to simpler schemes. As an example, let us consider the following wrong segmentation of the above strings: words, though previously shown more effective, weighted Jaccard similarity now reveals unreliable. Instead, simpler schemes disregarding segmentation, such as basic Jaccard simi-larity, could still somehow enable string disambiguation in an acceptable way.

The point is the trade-off between precision (i.e., the capability of correctly segmenting a tuple) and recall (i.e., the capability of segmenting a whole). It is clear that classification systems exhibiting high precision, even at the cost of low recall, can be safely embedded into a deduplication system, according to the scenario described in Fig. 2 .

In this scenario, a text segmentation system has two choices: either a sequence can be safely segmented, and the result of the segmentation is reliable, or the segmentation result is not affordable. In the former case, exact matching techniques based e.g., on weighted Jaccard similarity can be applied, whereas the latter case can still provide a reconciliation, which is however based on fuzzier schemes. Unfortunately, the state-of-the-art approaches from cur-rent literature, based on probabilistic modeling, do not properly fit the above scheme, since they foresee a segmentation even in presence of high uncertainty.

In this paper we discuss RecBoost, an approach for contextualized reconciliation, that moves away from probabilistic modeling. The idea is to first foresee a segmentation of textual sequences into tokens and, then, to perform a token-by-token classification, that involves the i.e., a strategy for text reconciliation, consisting in the exploitation of multiple, consecutive stages of classification. At each intermediate stage, a classifier learns from the outcome of its antecedent how to deal with those tokens, that were not reconciled at the end of the previous stage. This ensures reconciliation effectiveness even on unknown terms. 2.2 The RecBoost methodology The reconciliation of a set S ={ s 1 ,..., s m } of sequences with an attribute schema R = {
A 1 ,..., A n } consists in the association of each token a within the generic sequence s  X  S with an appropriate attribute of R .

RecBoostpursuestextreconciliationviatermgeneralization.Precisely,twotypesofgener-alizations are involved, namely syntactic and ontological analysis, and contextual generalization. The former aims at labeling textual tokens with their syntactic or ontolog-ical categories. The latter employs knowledge of the relationships among textual tokens, ontological categories and schema attributes, for assigning each token to a proper schema attribute.

As an example, a token a composed by multiple consecutive digits may be ontologically denoted as a number . Subsequently, the contextual presence on the same sequence contain-ing a of two further ontological labels, such as city and street (respectively following and preceding a ), may determine the reconciliation of a with an attribute address of the schema descriptor. 2.2.1 Syntactic and ontological analysis RecBoost exploits a user-defined domain ontology, in the style of the ones employed in [ 2 , 3 ], to preprocess sequences within S . In practice, a domain ontology is specified as G = L , , A ,where L is a set of categories, is a precedence relation defined on L , and A is a set of rules whose structure is sketched below: L represents a set of ontological concepts, which can be exploited in order to generalize tokens within a sequence. Such concepts are structured in a concept hierarchy, specified by the relation. Figure 3 shows an exhaustive set of concepts and their hierarchical relationships. A ={ r 1 ,..., r h } is a set of rules, that are useful to specify background knowledge about the domain under consideration, and are meant to provide a transformation of a set of tokens appearing in a sequence. Formally, the generic rule r i  X  A relates a basic textual token with some corresponding ontological concept in L , i.e., r i : M  X   X  L . Generally, the prior definition of a number of rules allows to properly deal with several tokens in a wide variety of applicative settings, with no substantial human effort.

As to the interpretation of ontological rules, Condition specifies a pattern-matching expres-sion defined over the tokens of a sequence, and Action specifies some actions to take on such tokens. We here focus on two main actions, exemplified in the context of the ontological illustration, rules r 1 and r 2 involve both type of actions: In general, relabeling actions, such as r 1 , substitute a token (or a set of tokens) with a con-cept in L . Restructuring actions, such as r 2 , operate on a set of tokens, by applying basic transformation operations (such as deleting, merging or segmenting).

Rules can also exploit user-defined dictionaries. As an example, the below rule r 3 specifies that each token appearing in the set Dictionary of all known toponyms (which comprises, e.g., street , road , blvd , etc.) can be generalized by the category TOPONYM in L .
By exploiting G , syntactic generalization performs two steps. First, it transforms the orig-is obtained from s i by applying the rules in A . Notice that multiple matching preconditions can hold for the same set of tokens. This potential ambiguity is solved via a user-defined order over the rules in A : when multiple rules can be applied, the first rule is chosen, and the others are ignored. In the above example, both rules r 1 and r 2 can be potentially applied to a sequence of digits. However, a token containing four digits can be interpreted as a zip code if and only if it is not followed by a new number (in which case, the former token has to be interpreted as an area code within a phone number). Thus, in order to disambiguate rule longer digit sequences. Second, the available tokens in each sequence are further generalized by an ad-hoc exploitation of the hierarchy described by the relation. The exploitation is a direct result of a cooperation with contextual analysis, which reconciles tokens in S as described in the next subsection. 2.2.2 Contextual analysis This step is meant to associate tokens in S with their corresponding attributes in R .We approach the problem from a supervised learning perspective. Formally, we assume that there exists a partial function  X  : M  X   X  M  X  R that, for each sequence s  X  M  X  , labels atoken a into a schema attribute A j , namely  X  s ( a ) = A j  X  R . Hence, the problem can be stated as learning  X  from a training set T such that, for each sequence s  X  T and for each token a i  X  s , the label  X  s ( a i ) is known.
 In order to correctly classify each token a i  X  s , we exploit information about its context . preceding and following a i in s and is formally defined as follows: post s ( a i ) indicates the context segment that follows a i .
 fication problem.

The idea beyond contextual analysis is to examine the context feature s ( a ) of each token a within any sequence s , in order to learn meaningful associations among groups of tokens of S . These associations can be then exploited to learn a rule-based classifier, that associ-ates each individual token in S with an attribute in R . In practice, our objective is to build a classifier C : ( M  X  L  X  R )  X   X  M  X  R , specified by rules such as the one sketched below:
Here, a and s represent, respectively, token and sequence variables. Moreover, Condition represents a conjunction of terms, and Class represents an attribute in R . Terms in Condition v is any constant in M  X  L  X  R . The latter two conditions strictly relate token reconciliation analysis to what actually precedes (risp. follows) token a .

In the process of distilling a rule-based classifier from a training set T , a holdout approach is adopted to partition T into a validation set V and an actual training set D = T  X  V . The goal is learning a classifier from D that has highest accuracy on V . In principle, any rule-based classifier could be used here. However, we found that classification based on association rules is more effective in this setting than, e.g., traditional algorithms based on decision-tree learning. The intuition behind the above statement is that association rules are better suited to detect local patterns which hold locally on small subsets of D . This is especially true when D is large, and contains many contrasting specificities across individual sequences. By contrast, decision trees represent global models, which are hardly able to capture such specificities without incurring into the overfitting phenomenon. In addition, the intrinsic unstructured nature of the feature space to analyze does not allow an immediate application of decision-tree learning techniques, whereas association rule mining techniques naturally fit the domains under consideration.

A variant of the Apriori algorithm [ 22 ] is exploited in order to extract from the explicit representation of token contexts, D ={ feature s ( a ), A | s  X  D , a  X  s , A  X  R } ,asetof association rules that meet pre-specified requirements on their support and confidence values and whose consequents are narrowed to individual schema attributes. A classifier can hence be built on the basis of such discovered rules, by selecting the most promising subset, i.e., the subset of rules which guarantees the maximal accuracy. To this purpose, we adopted the CBA-CB method [ 15 ], which allows an effective heuristic search for the most accurate association rules. Succinctly, its basic idea is to sort the extracted associations by exploiting a precedence operator  X  . Given any two rules r i and r j , r i is said to have a higher precedence confidences and supports are the same, but r i is shorter than r j . Hence, a classifier can be formed by choosing a set of high precedence rules such that 1. each case in the training set D is covered by the rule with the highest precedence among 2. every rule in the classifier correctly classifies at least one case in D , when it is chosen. While considering an unseen case of D , the first rule that covers the case also classifies it. Clearly, if no rule applies to a given case, the case is unclassified.

We revised the scheme of [ 15 ] by implementing a post-processing strategy, which aims at (1) further improving the classification accuracy of the discovered rules, and at (2) reducing the complexity of the discovered rules. The postprocessing is mainly composed by attribute and rule pruning. The idea behind attribute pruning consists in removing items from clas-sification rules, whenever this does not worsen the error rate of the resulting classifier. The validation set V is exploited to assess classification accuracy.

Precisely, let r be a generic classification rule containing at least two terms in the ante-cedent. Also, assume that s denotes a generic sequence in V and that x represents a token within s . The error r x of rule r on x is a random variable Hence, the overall error of r on V can be defined as follows, where n V indicates the overall number of tokens within V . A new rule r can now be gen-erated by removing from the antecedent of r any of its terms. We replace r by r if two conditions hold, namely E ( r )&lt; E ( r ) and the discrepancy E ( r )  X  E ( r ) is statistically relevant. To verify this latter condition, we exploit the fact that for n V large, the distribution of E ( r ) approaches the normal distribution. Hence, we compute a  X  % confidence interval [  X ,  X  ] , whose lower and upper bounds are, respectively, given by and where, constant c  X  depends on the confidence threshold  X  . The above interval represents an estimate for the actual error of rule r . Finally, we retain r instead of r , if it holds that E ( r )&lt; X  . In such a case, we analogously proceed to attempt at pruning further items from the antecedent of r . Otherwise, we reject r .

Rule pruning instead aims at reducing the number of rules in a classifier. As in the case of attribute pruning, the idea consists in removing rules from a classifier, whenever this does not worsen the accuracy of the resulting classifier.

To this purpose, all rules in a classifier are individually evaluated on the basis of their precedence order. A generic rule r is removed, if one of the following conditions holds:  X  r does not cover a minimum number of cases in V ;  X  the accuracy of r on V is below a minimum threshold;  X  the removal of r from the classifier increases its overall accuracy on V . 3 RecBoost anatomy Association rules for classification allow to tune the underlying classification model to a local sensitivity. However, in principle their adoption can yield a high number of unclassified tokens , i.e., tokens for which no rule precondition holds. In a reconciliation scenario, this is due to the presence of unknown or rare tokens, as well as errors in the text to segment. The adoption of a concept hierarchy mitigates such a drawback and, indeed, it has already been adopted in traditional approaches based on HMM [ 2 , 3 ]. The novelty in the RecBoost recon-ciliation methodology relies on a finer cooperation between synthactic/ontological analysis and contextual analysis. The reiteration of the process of transforming tokens and learning a rule-based classifier allows progressive classification , i.e., the adoption of multiple stages of classification for more effective text reconciliation. Precisely, a pipeline P ={ C 1 ,..., C k } sifier C i is specifically learnt to classify all those tokens, that were not reconciled at the end of step i  X  1. The length k of the classification pipeline is chosen so that to achieve accurate and exhaustive classification. Conceptually, this requires to minimize the overall number of both misclassified and unclassified tokens. In practice, a further classification stage is added to P whenever such values do not meet application-specific requirements, such as in the case where the misclassification rate is acceptable, but the unclassification rate is not satisfactory.

Thegenericclassifier C i canbeformallydescribedasapartialmapping C i : ( M  X  L  X  R )  X   X  M  X  R , and its construction relies on a specific training set T i , that is obtained from T  X  1 by adding domain information provided by C i  X  1 . Given any sequence s  X  T i  X  1 , C i is learnt from the evaluation of the set X s of unknown tokens, i.e., the set of those tokens in s , that are not covered by any rule of C i  X  1 . This is accomplished by enriching the domain information in G with a new set of rules directly extracted from the set of classification rules in C i  X  1 . Specifically, each classification rule r  X  C i  X  1 such as the one below: is transformed into a labeling rule r , having the following structure: The new rule r is then added to the set A of rules available for syntactic analysis. Then, syntactic analysis is applied to each sequence s in T i  X  1 , and the resulting transformed se-quences are collected in T i . A new training set T i is then generated by collecting, for each sequence s  X  T i and each token a  X  X s , the tuples feature s ( a ),  X  s ( a ) . Notice that there is a direct correspondence between the context feature s ( a ) computed at step i and the con-text computed at step i  X  1. Indeed, the new context feature s ( a ) follows from the context of a within T i  X  1 by replacing each token b  X  X s of s with its corresponding attribute C
The above detailed methodology is supported by three main components, namely a pre-processor ( tokenizer ), a classifier learner and a postprocessor . The components cooperate both in the training and in the classification phases, as detailed in Fig. 4 . In the following, we explain the role played by each of the aforementioned modules. 3.1 Preprocessor A cleaning step is initially performed by this component, to the purpose of encoding the ini-tial data sequences of a free text S into a uniform representation. This phase involves typical text-processing operations, such as the removal of stop-words, extra blank spaces, superflu-ous hyphens and so forth. The preprocessor then proceeds to split free text into tokens. The main goal of this phase is to recognize domain-dependent symbol-aggregates (e.g., acro-nyms, telephone numbers, phrasal construction, and so on) as single tokens. As an example, aggregates such as  X  IBM  X ,  X  G. m. b. H.  X  X r X  as well as  X  are more significant as a whole, rather than as sequences of characters in the text. The identification of symbol aggregates as well as domain/specific cleaning steps are accomplished by using domain-specific transformation rules suitably defined in G . 3.2 Classifier learner The classifier learner is responsible for producing an optimal set of classification rules, as showninFig. 4 a. It consists of four main elements: a generalizer ,an association rule miner , a filter for classification rules and a classifier pruner . In particular, the generalizer performs ontological generalization, by exploiting the labeling rules and the relationship defined in G . Its role is mainly to enable the discovery of accurate association/classification rules, by providing an adequate degree of generalization among the data. To accomplish this task, the generalizer employs the labeling rules in A . Next, for each label replacing a token somewhere in a textual sequence, the related concept hierarchy is inspected and the textual sequence is extended to also include the ancestors of the specific label. The latter operation is performed by the association rule miner , that extracts generalized association rules from the above extended sequences. The classification rules filtered by the classification rules filter ,which in principle could contain several redundancies (due to the exploitation of the hierarchy in the association mining step), are further postprocessed by the classifier pruner . The latter attempts to reduce the overall size of the discovered rules by exploiting the aforementioned attribute and rule pruning techniques. 3.3 Postprocessor The postprocessor rebuilds the sequences reconciled by a rule-based classifier, at any stage of progressive classification, by fitting them into a relational structure with schema R ,as shown in Fig. 4 b. This is accomplished by interpreting each (partially) reconciled sequence as a structured tuple, and organizing the tokens that have been so far reconciled as values of corresponding schema attributes.

Postprocessing enables progressive reconciliation: at any stage, a classifier is specifically learnt for dealing with those sequence tokens, that were not reconciliated at the end of the previous stage. The postprocessor is also exploited during the training phase, as shown in via the application of the rules in C i  X  1 . 4 An illustrative example We elucidate the overall RecBoost methodology, by exemplifying the reconciliation of a col-lection of personal demographic information, shown below, in compliance with the attribute descriptor R ={ NAME , ADDRESS , ZIP , CITY } .
 In particular, we assume to exploit a dictionary containing all known toponyms, and a domain-specific ontology G = L , , A ,where L = { PHONE-NUMBER , SSN , TOPONYM , ZIP-CODE } and A consists of the following ontological rules: The example data collection is corrupted by noise, i.e., by the absence of a uniform represen-tation for all of its constituting sequences. Indeed, a comparative analysis of their formatting encodings reveals that:  X  there is a telephone number in sequence s 1 that has to be discarded, since it is not expected  X  character  X  - X  is employed in sequence s 1 as a separator between the words Northern and  X  brackets are exploited to separate the zip-code information in sequence s 1 ;  X  three non-relevant dots precede the address information in sequence s 2 ;  X  two hyphens in sequence s 3 demarcate the word Brooklyn ;  X  there is a social security number ( SSN ) in sequence s 3 that has to be discarded, since it
The identification of a uniform representation format for all of the individual sequences in the textual database enables an effective segmentation of such sequences into tokens and, hence, a reliable reconciliation. A preprocessing step is performed to this purpose. 4.1 Preprocessing The input textual sequences are suitably tokenized. This is accomplished by exploiting the presence in the original text of domain-specific delimiters such as single or double quotes, hyphens, dots, brackets and blanks. After segmentation, such delimiters become spurious characters, i.e., play no further role in the reconciliation process, and are hence ignored. The output of this step, with respect to the hypothesized data, is represented below: The fragmented text is now subjected to a pipeline of rule-based classifiers, that reconciliate groups of tokens across the individual sequences s 1 , s 2 , s 3 with the attributes in R .
For the sake of convenience, we assume that two stages of classification allow the accom-plishment of an actual reconciliation. Furthermore, since progressive classification involves a similar processing for each sequence in the tokenized text, we proceed to exemplify the sole reconciliation of s 1 . 4.2 Progressive classification Progressive classification divides into syntactic and contextual analysis. 4.2.1 Syntactic analysis This step performs token generalization. Here, the exploitation of the above ontological rules allow the generalization of a number of tokens in s 1 as shown below, where labels denoting ontological categories are enclosed between stars. To this point, s 1 undergoes two levels of contextual analysis, where at each level a suitable set of rules is applied. 4.2.2 First-level classifier A classifier is generally distilled from the analysis of the relationships among textual tokens, ontological categories and, also, attributes in the context of each token within the generalized sequences at hand. In particular, we suppose that the classifier resulting from the learning phase includes the classification rules listed below:
The first-level classification hence starts by classifying the tokens of s 1 , according to their features. In particular, being s 1 composed of six tokens, a first-level classifier is applied against the six context representations feature s low, where a is any token of s 1 .

Notice that, at this stage of contextual analysis, s 1 does not include attribute labels. Hence, reconciliation takes into account relationships among ontological labels and textual tokens. These enable the reconciliation of the entities *TOPONYM* , Boulevard , London and *ZIP* ,butfail in dealing with *PHONE* and Hacker . In particular, this latter token is not covered by the rule that classified Harry ,since pre s sequence s 1 assumes the following form, where reconciliated tokens are replaced by their corresponding attribute labels, enclosed between square brackets. 4.2.3 Second-level classifier To this point, as a consequence of progressive classification, the original domain information in
G is updated to yield an enriched ontology G = L , , A .Theset A of ontological rules is obtained by augmenting the original one, A , with the classification rules learnt at the first-level classification rules are transformed into labeling rules before being added to A .
Contextual analysis is then reiterated to reconciliate those tokens that were not associated with a schema attribute at the end of the previous step. Again, we assume that a second-level classifier is learnt from the training data and, and it is composed by the following individual rule: There are only two tokens in s 1 that were not associated with a schema attribute and, hence, the above classifier is applied against two context representations: As a result, the classifier further generalizes s 1 into the following sequence:
Notice that *PHONE* is still not reconciliated, since no classification rule applies to it. 4.3 Postprocessor The postprocessor rebuilds the original sequence s 1 , by fitting its corresponding tokens in a suitable structure defined by the descriptor R ={ NAME , ADDRESS , ZIP , CITY } :
Notice that the structure above exactly complies with R . However, in some cases, it may be useful to add an extra column NOISE , to the purpose of tracing all the original tokens. This would correspond to the following tuple: 5 Experimental evaluation In this section, we describe the experimental evaluation we performed on the proposed meth-odology. Experiments were mainly aimed at evaluating the effectiveness of the proposed methodology in segmenting strings. To this purpose, we accomplish the following tasks: 1. We evaluate the effectiveness of the basic rule-based classifier systems proposed in 2. Next,weevaluateclassificationaccuracyobtainedbytheprogressiveclassificationmeth-5.1 Experimental setup In order to accomplish the above tasks, we considered the following datasets:  X  Addresses , a real-life demographic database consisting of information about the issue- X  BigBook , a publicy-available dataset 1 consisting of a set of business addresses. Each  X  dblp , a collection of articles extracted from the DBLP website 2 . Each entry refers to an
The evaluation of Recboost effectiveness requires the design of a domain-specific ontol-ogy for each of the aforementioned datasets. Specifically, the concept hierarchy devised for the Addresses dataset is shown in Fig. 3 . This consists of 11 concepts for token gen-eralization, suitably organized into a compact hierarchical structure. The ontological rules restructuring rule).

The ontology employed for the BigBook dataset, shown in Fig. 5 , embraces nine con-cepts.
In such a context, no use is made of restructuring actions, so that background knowledge reduces to the relabeling rules shown next: Finally, the ontology for the dblp dataset is shown in Fig. 6 .
 Again, background knowledge only involves relabeling rules, that are reported below:
Noticethatthedefinitionoftheaboverulesreliesonanumberofdomain-specificdictionar-ies. In particular, JOURNAL DICTIONARY includes several alternative ways of denoting a journal article, such as j. , journal , trans. and transaction . GRAMMATICAL-ARTICLE DICTIONARY groups English-language articles a , an and the . Similarly, PREPOSITION DICTIONARY collects commonly used prepositions, such as by , to , with and via . DELIM-ITER DICTIONARY is a set of token delimiters, that comprises  X  ,  X  , ; , , , -, . ,and * .
It is worth noticing that the analysis of the above domain-specific ontologies reveals a key feature of RecBoost methodology. Roughly speaking, it can be easily employed for pursuing text reconciliation in a wide variety of applicative settings, by simply providing a domain-specific concept hierarchy along with a corresponding compact set of ontological rules. The overall process of ontology design is rather intuitive and does not require substantial effort by the end user.

The evaluation of the results relies on the following standard measures which are custom-ized to our scenario. Given a set N of tokens to classify, we define:  X  the number of tokens, which were classified correctly, TP ;  X  the number of tokens, which were misclassified, FP ;  X  the number of tokens, which were not classified, FN  X  notice that this is a different In the following, we shall report and illustrate the above measures over the mentioned data-sets. Two further important measures, however, can give an immediate and summarizing perception of the capabilities of our classification system. In particular, Precision (or Accu-racy ) can be defined as the number of correctly classified tokens, w.r.t. the classification behavior of the system: Analogously, Recall can be defined as the number of correctly classified tokens, w.r.t. the tokens to classify: Intuitively, Recall describes the locality issues, that affect the system: if a classifier contains rules which can cover all the examples, then it has 100% recall (i.e., no locality effect). Precision, by the converse, describes the accuracy of the rules contained: the higher is the error rate of a rule, the lower is its precision.
 A measure which summarizes both precision and recall is the F measure, defined as The F measure represents the harmonic mean between Precision and Recall. The  X  term in the formula assigns different weights to the components: when  X  = 1 both the components have the same importance. The tuning of the  X  parameter is application-dependent. Here, we are interested in the cases where  X &gt; 1 (which assigns higher importance to Precision than to Recall). This is a crucial requirement of many application domains, such as the one described in Sect. 2 . Hence, in the following we shall study the situations where  X &gt; 1, and in particular we are interested in the cases where  X  ranges into the interval ( 1 , 10 ] . 5.2 Evaluating the basic classifier system In an initial set of experiments, we classified the data without exploiting ontologies and multiple classification stages. In these trials, support was fixed to 0 . 5%, with ranging values of confidence. Figure 7 shows the outcome of classification for the three datasets. Each bar in the graph describes the percentage of correctly classified tokens, together with the per-centages of misclassified and unclassified tokens. As we can see, the effectiveness of the classifiers strongly relies on the confidence value. In particular, low confidence values (up to 40% in both Addresses and dblp , and 60% in BigBook ) to classify all the tokens, but the percentage of misclassified is considerably high. This is somehow expected, since low confidence values induce rules exhibiting a weak correlation between the antecedent and the consequent.

By contrast, higher confidence levels lower the misclassification rate, but the degree of unclassified tokens raises considerably. It is worth noticing that, in all the examined cases a confidence rate of 100% guarantees a percentage of misclassified data which is nearly zero. This is the locality effect : high confidence values produce extremely accurate rules that, as a side effect, apply only to a limited number of tokens. By lowering the confidence, we relax the locality effect (the resulting rules apply to a larger number of tokens), but the resulting rules are less accurate.

The dblp dataset is particularly interesting to investigate in this context, since it exhibits the worst performances. The best we can obtain in this dataset is with confidence set to 40%, which guarantees a significantly high percentage (30 . 52%) of misclassified tokens. A  X  X afer X  confidence value leverages the number of unclassified tokens considerably.

Figure 8 describes the accuracy of the classifier with the adoption of domain-specific concept hierarchies. We exploited the hierarchies described in Figs. 3 , 5 and 6 , respectively. The benefits connected with the exploitation of such simple ontologies are evident: the gen-eralization capabilities of the classification rules are higher, thus lowering the number of unclassified tokens. Notice how the dblp dataset still exhibits unacceptable performances. ( a ) (c) Results in the above figures were obtained by exploiting the pruning steps detailed in Sect. 2.2.2 . Indeed, the contribution of the classifier pruner to the misclassification rate is investigated in Table 1 , which describes how the error rate changes if pruning is not applied. The effectiveness of the classifier pruner can be appreciated at lower confidence values: there, the classifier produces weaker rules, which clearly benefit of a re-examination. 5.3 Evaluating multiple classification stages The above analysis allows us to test the effectiveness of the progressive classification meth-odology. We recall the underlying philosophy: starting from the following observations, ( a ) (c)  X  ontological analysis eases the classification task (as testified by the comparison between  X  a richer set of relabeling rules should in principle boost the results of classification; the adoption of multiple classification stages, where at each stage the relabeling rules of the previous stage are enriched by exploiting the results of classification at earlier stages, should boost the performance of the overall classification process.

And indeed, Fig. 9 describes the results obtained by applying a second-level classifier to the unclassified cases of the first stage of classification. In detail, the input to the second-confidence to 100% (described by the first bar of each graph in Fig. 8 ). Again, support was set to 0 . 5% and confidence was ranged between 100 and 80%.

As shown in the figure, the second-level classifier is in general able to correctly classify a portion of the data, that were unlabeled at the end of the previous stage. For example, in the Addresses dataset, a 95% threshold allows to classify a further 62% of the (originally unclassified) data. By combining such a result with the outcome of the first-level classifier, we obtain nearly 91% of correctly classified data, less than 1% of misclassified data and nearly 8% of unclassified data. Table 2 summarizes the the cumulative results achieved by two levels of classification over the employed datasets.

The effectiveness of the second stage of classification is even more evident in the graphs of Fig. 10 . The graphs depict the trend of F for different values of  X  . The graphs compare a selection of 2-level classifiers with the single-level classifier (among those shown in Fig. 8 ) ( a ) (c) exhibiting the best performance in terms of TP . In all the cases shown, the 2-level classifiers exhibit better performances for  X &gt; 2.

Since each classification level boosts the performance of the system, two important ques-tions raise, that are worth further investigation in the following: 1. how many levels allow to achieve an adequate performance? 2. how should the parameters at each level be tuned? The dblp dataset is particularly interesting in this context, since the accuracy of RecBoost is still low after two classification levels. We start our study by investigating the number of needed classifiers. Figure 11 a, b describe an experiment performed by allowing a hypo-thetical infinite number of levels, where at each level support was set to 1% and confidence to 100%. Roughly, the strategy implemented is the following: since high confidence values bound the number of misclassified tokens, and further levels allow to recover unclassified tokens, just allow any number of levels, until the number of unclassified tokens is nearly 0.
As we can see from Fig. 11 b, however, this strategy does not necessarily work: although the number of misclassified tokens is kept low, the capability of each classifier to recover tokens unclassified in the previous stages decreases. The 5th level looses the capability to further classify tokens, thus ending de-facto the classification procedure. Figure 11 ashows the cumulative results at each level. ( a )
Thus, an upper bound in the number of stages can be set by the classification capability of the stages themselves. A smarter tuning of the parameters which rule the performance of each single stage, allows to achieve best classification accuracy. Figure 11 c, d report a different classifier, generated by fixing the following constraints: each classification stage should classify at least 30% of the available tokens, and should misclassify at most 10% (if possible). The methodology adopted for achieving this was to perform several tuning trials at each stage, by starting from the value 100% of confidence and progressively lowering it until the criterion is met. Figure 11 d describes the tuning occurred at each classification stage. The constraint over the classification percentage clearly boosts the performance of each single classification stage: as a result, the overall number of classified tokens is 86 . 7%, with a misclassification rate of 10 . 2and3 . 1% unclassified tokens.

Notice that further effective strategies can be employed, by fixing e.g., different con-straints: in Fig. 11 e, for example, each classification stage should classify at least 20% of the available tokens, and should misclassify at most 5% of them. Figure 11 g, reports a differ-ent experiment, where the number of stages is fixed to 4: here, confidence is progressively lowered, and the last stage is tuned to minimize the number of unclassified. Again, Fig. 11 h describes the tuning occurred at each stage.

Similar conclusions can be drawn with the other datasets: Fig. 12 , e.g., describes the results on both BigBook and Addresses . In particular, we adopted three levels (with confidence fixed to 100% in the first two levels) for BigBook and four levels (with thresholds 100, 100, 85% in the first three levels) for Addresses . The bars report the cumulative classification results when different confidence levels are applied in the last classification level.
The adoption of multiple classification stages over BigBook deserves further discussion about the relation between the size of the labeled data and the number of classification levels which can be defined. Each classification level should build on a separate training set (pre-processed by the preceding levels). Clearly, given a dataset D , the amount of unclassified tokens of D diminishes at subsequent levels. Hence, the size of the training set T i required for learning rules at level i should be large enough to guarantee that an adequate number of unclassified tokens are available at that level.

Thus, the size of the training has an influence over the number of classification levels which can be defined: the larger the training set, the higher the number of significant levels. In other words, a small dataset saturates the potential of progressive classification within few levels, and adding further levels does not yield any improvements. This is what happens in the case of the BigBook dataset. As already mentioned, the available training set here is quite small. Thus, a classifier exhibiting 100% confidence in the last level, would produce at ( a ) (c) ( a ) most 2,500 unclassified tokens. This amount would not allow to learn a further meaningful set of rules, since such tokens distribute over different sequences and different attributes.
The conclusion we can draw is that the adoption of multi-stage classification allows to increase recall, by contemporarily controlling the decrease in the overall classification accu-racy. The above figures show how a proper manipulation of the confidence threshold value over each classification stage allows to achieve this. The contribution of the support threshold is less restrictive for two main reasons: first, it should anyway be kept at very low levels, in order to enable a significant amount of rules; second, small variations are of little signifi-cance, and at most at the first level. We here provide details on a set of tests performed on a pipeline of three classifiers, over the Addresses dataset. In particular, for brevity sake, we investigate the effects of varying support and confidence for the first-level classifier, whereas the remaining two stages have instead both parameters fixed to respectively 0 . 5 and 98%. Specifically, in Fig. 13 a, c confidence is fixed to 98% and support varies, whereas in Fig. 13 b, d support is set to 0 . 5% and confidence varies. Figure 13 a, c shows that classification accu-racy and recall do not significantly change, especially at higher levels. By the converse, even small variations in the confidence cause significant changes, as testified by Fig. 13 b, d.
It is interesting to see that, in the above described experiments, the average number of rules which are exploited is nearly stable even on different values of support and confidence (which instead affect the number of discovered rules). Fig. 14 a,b depict such a situation. In general, a decrease in support or confidence causes an increase in the overall number of discovered classification rules. However, from experimental evaluations, it emerges that the average number of rules actually applied in the classification process does not significantly vary. This is testified by the bold hatched line in both subfigures, which represents such an average value. As we can see, the number of rules applied fluctuates around 50% of the total number of rules obtained in correspondence of the maximum values of support and confidence. 5.4 Comparative analysis The exploitation of the  X  X ecursive boosting X  strategy proposed in this paper is quite new, as it relies on the capability of recovering unclassified tokens in the next stages. To this pur-pose, the former experiments aimed essentially at checking whether this strategy is effective. In order to asses the practical effectiveness of RecBoost, we here compare the behavior of the RecBoost methodology with consolidated approaches from the literature. To this end we preliminarily observe that, although many results are available in the literature, a direct comparison is often difficult, as different data collections and/or different ways of tuning the algorithm parameters have been used. For example, although bibliographic citations extracted from the DBLP database have been extensively used in the literature, the datasets used for the analysis were not made publicly available.

In the following we provide a comparison by exploiting the datasets described in the previous sections. We compare our system with the Mallet system [ 18 ], which provides the implementation of Conditional Random Fields [ 14 ] and with the DataMold system [ 3 ]. We refer the reader to Sect. 6 for a detailed description of the techniques underlying such sys-tems. Both Mallet and DataMold are equipped with the same ontology and preprocessing used in RecBoost. In addition, contextual information in the CRF implemented by Mallet was provided by resorting to the Pre/Post information.

An overall comparison is shown in the graphs of Fig. 15 , which plot the F values obtained by Mallet, DataMold, and several different instantiations of the RecBoost system. In partic-ular, we consider the classifiers of Figs. 11 and 12 , and choose, for each dataset, the three instantiations which guarantee the lowest (constrained) value of FN , the lowest (constrained) value of FP , and a  X  X iddle X  value. The constraint refers to the possibility of maintaining an acceptable value of TP .Forthe dblp dataset, we also show an instantiation
As we can see from the figure, the gain in the F value is evident for  X &gt; 2. Table 3 details the results. Here we compare with Mallet, DataMold and the version of RecBoost (RecBoost 1 in the tables), relative to a single stage of classification which achieves the highest value of TP in Fig. 8 . Mallet (and in some cases even Datamold) typically achieves a high rate of correctly classified tokens at the expense of a higher misclassification rate. Also, RecBoost 1 may achieve a higher TP than the approaches with multiple classification stages. However, the latter exhibit a higher affordability (which is even higher than that of Mallet and Data-Mold). In practice, the adoption of multiple stages allows to achieve a higher precision, at the expense of a lower recall. Clearly, a proper tuning at the higher levels makes the Rec-Boost system highly competitive: in Addresses , for example, the performance of the more conservative classifier (the one which tries to minimize FN ) is even better than Mallet.
In practice, the recursive boosting offered by progressive classification allows to maintain a higher control over the overall misclassification rate, by forcing stronger rules which, as a side effect, exhibit a higher locality . Thus, RecBoost is more reliable in scenarios where misclassifying is worst than avoiding to classify.

Finally, two major arguments emerge in favor of Recboost as a further result of our com-parative analysis.  X  Due to the variable number of classification stages, RecBoost gives the user better control  X  The generic RecBoost classifier is easier to interpret than existing methods such as DA-6 Related work Text reconciliation is clearly related with Part Of Speech (POS) Tagging and Shallow Pa rs i n g , Wrapping and, in general, with the problem of extracting structure from free text. The aim of POS Tagging is to assign labels to speech words that reflect their syntactic cat-egory. To this purpose, both statistical and rule-based techniques [ 4 , 13 , 16 , 17 ] have been proposed in the literature. In practice, the basic idea behind POS tagging consists in disam-biguating phrases by exploiting dictionaries and analyzing the textual context surrounding each candidate entity. However, the approach fails at treating exceptions , i.e., words that are not included in a dictionary, such as proper names, cities, or addresses. By contrast, these are exactly the features which characterize our scenario.

As far as wrapping is concerned, most algorithms considerably rely on HTML separator tags, and on the fact that data represent a regular multi-attribute list [ 8 ]. Such approaches are not effective in domains where data do not necessarily adhere to a fixed schema. Indeed, instances in our problem are more irregular, since the order of fields is not fixed, not all attributes are present, etc. The classification of an item is better performed according to its neighboring words, absolute/relative position in the string, numeric/alphanumeric charac-ters, and so on. To our knowledge, few exception are capable of effectively dealing with such features [ 1 , 5 , 21 ]. For example, WHISK [ 21 ] can deal with missing values and permutations of fields, but it requires a  X  X omplete X  training set, i.e., a set of examples including all the possible occurrences of values.

ILP provides a solid framework for designing rule-based systems aimed at text categori-zation and information extraction [ 6 , 9 ]. In particular, a divide-and-conquer approach to the problem of learning rules for accomplishing both these latter tasks is proposed in [ 12 ]. In principle, such a technique can be employed for text reconciliation, since it allows the extrac-tion of focussed textual fragments and their subsequent labeling. However, its exploitation for practical applications is problematic, due to the fact that rules have to explicitly locate fragment boundaries. This imposes a non-trivial learning phase, that requires two distinct sets of training examples, respectively necessary for denoting what specific (aggregates of) words can be taken into account as possible boundaries within the underlying textual data and for specifying how to label their intermediate fragments. Also, fragment extraction relies on tests on the occurrences of domain-specific words, that are either learnt from the train-ing examples, or exhibit some degree of positive correlation to such examples. However, locating all relevant fragments determined by meaningful combinations of these words is computationally unfeasible. This imposes the exploitation of various indexing structures to accelerate the evaluation of rule predicates on the underlying text. By contrast, RecBoost pursues token-by-token reconciliation, thus overcoming all of the above issues related to fragment boundaries.

Several recent approaches to schema reconciliation rely on Hidden Markov Models (HMM) [ 2 , 3 , 14 , 19 , 20 ]. Schema reconciliation with HMM can be accomplished by learning the structure of a HMM, and applying the discovered structure to unknown examples. As an example, DATAMOLD [ 3 ] employs a training phase to learn a HMM, that consists of a set of states and directed edges among such states. Two particular states are the initial and the final states. The former has no incoming edges, whereas the latter has no outgoing edges. Every state of the HMM, except from the initial and final ones, represents a class label and is associated with a dictionary, grouping all the terms in the training set that belong to the class. Edges among states are associated with transition probabilities. A textual sequence can be classified if its constituting terms can be associated to states of the HMM, that form a path between the initial and final states. Precisely, DATAMOLD pursuits classification by associating a single term to all those states, whose corresponding dictionaries include the term. Hence, a sequence of textual terms is mapped to multiple paths throughout the HMM. Transition probabilities are then exploited to identify the most probable path and, hence, to accordingly classify the terms in the sequence at hand. Clearly, those sequence, whose tokens do not form any path between the initial and final states, cannot be classified.
The effectiveness of the approaches based on HMMs strongly depends on the number of distinct terms occurring in the training set. Indeed, in order to associate terms that do not appear in the training set with a corresponding state, DATAMOLD relies on smoothing , i.e., on the exploitation of ad hoc probabilistic strategies.

Furthermore, the classification of individual term sequences in one step, i.e., subjected to the existence of corresponding paths throughout the automaton, is a major limitation of HMMs. Indeed, depending on the outcome of the training phase, these cannot undertake the reconciliation process, whenever a path for the sequence at hand does not exist. Also, the existence of one or more paths for a given input sequence may not determine a proper recon-ciliation. This latter aspect is clarified in Fig. 16 , where it is shown that the HMM topology prevents the correct reconciliation of the input sequence Harry Hacker London .Noticethat node labels indicate input tokens for the automaton. Moreover, the internal shape surrounding node labels corresponds to actual token classes (i.e., name/rhombus, surname/rectangle and city/circle), whereas the external shape of a node denotes the class assigned by that node to its label. Clearly, discrepancies between internal and external shapes represent reconcil-iation errors. The illustration shows that four paths exist in the automaton and, hence, as many alternatives to reconcile the input sequence. However, all such paths lead to erroneous reconciliations. The only sequence of states in the automaton, that would lead to a correct reconciliation, does not form a path between the ending states I . S . and F . S . , thus being unemployable to reconciliation purposes.

Worst, HMMs represent  X  X lobal classification models X , since they tend to classify each term of the sequence under consideration, and hence are quite sensitive to unknown tokens. Consider the sequence Harry 348.2598781 London , where the second token represents a phone number. Although the correct label is unknown to the model shown in Fig. 16 ,the latter will still try to assign a known label to the token. As a consequence, the whole sequence (e.g., the high probability that a sequence starts with a name) would allow to correctly classify the  X  X nown part X  of the sequence, by avoiding to classify the  X  X nlikely X  term.
Recently, emphasis has been paid to the analysis of token context (i.e., of the tokens following and preceding the one at hand) for more accurate reconciliation. In particular, Maximum Entropy Markov Models (MEMMs) [ 19 ], i.e., conditional models that represent the probability of reaching a state given an observation and the previous state, can be seen as an attempt at contextualizing token reconciliation. However, MEMMs suffer from the well known label-bias problem [ 14 ].

Conditional random fields (CRFs) [ 14 , 18 ] are a probabilistic framework, that can be employed for text labeling and segmentation. The underlying idea is to define a condi-tional probability distribution over label sequences, given a particular observation sequence, rather than a joint distribution over both label and observation sequences. CRFs provide two major advantages. First, their conditional nature relaxes the strict independence assumptions required by HMMs to guarantee tractable inference. Second, CRFs avoid the label bias prob-lem. Still, such improvements in the HMM technology represent global classification models, since they tend to classify each term into the sequence under consideration, and hence do not prevent the problem of misclassifying unknown tokens.

An unsupervised approach to text reconciliation is introduced in [ 2 ]. The basic idea here is to exploit reference relations for building segmentation models. The notion of reference relation denotes a collection of structured tuples, that are specific to a domain of interest and exemplify clean records for that domain. The approach consists of a two-step process. Assume that R is a reference relation with an attribute schema A 1 ,..., A n . Each column of R is considered as a dictionary of basic values for the corresponding attribute. Initially, a preprocessing step is performed for building an attribute recognition model (ARM) for each attribute of the reference relation schema. The generic ARM i is a HMM that allows the evaluation of the probability with which a subsequence of tokens in an input string belongs to the domain of the corresponding schema attribute A i . The ARMs of all attributes can be exploited to determine the best segmentation of an input string at the second (run-time) step. This involves to first learn the total order of attributes from a batch of input strings and to subsequently segment the individual input strings with respect to the detected attribute order. More specifically, the identification of a total attribute order requires the previous computa-tion of pairwise precedence probabilities. These are probabilistic estimates of precedences between all pairs of attributes, that are provided by their corresponding ARMs . A total order-ing among all of the attributes is hence discovered by choosing the best sequence of attributes, i.e., the sequence that maximizes the product of precedence probabilities of consecutive attri-butes with respect to the given order. Finally, an exhaustive search is employed to determine the best segmentation of an input string s into n token subsequences s 1 ,..., s n , such that the reconciliation of each s i with the corresponding schema attribute A s i maximizes the overall reconciliation quality n i = 1 ARM s i ( s i ) among all possible segmentations.
Notice that the exploitation of reference tables is a natural way of automatically building training sets for the text reconciliation problem described beforehand. And indeed, although declared as an unsupervised approach, this technique suffers from two general weaknesses, that are inherent of supervised methods. Foremost, a reference relation may not exist for a particular applicative scenario. Also, whenever the overall number of tuples involved is not sufficiently large, the columns of the employed relations may not be adequately rich dictio-naries of basic domain tokens. This would affect the construction of ARMs and, hence, the overall segmentation effectiveness.

As to a more specific comparison with our contribution, the reference table approach [ 2 ] requires to initially learn the order with which attributes appear within the input data. By contrast, though being a supervised approach, RecBoost does not rely on learning attribute order from training data. This is due to the adoption of classification rules, that allow the reconciliation of a given token on the sole basis of the relationships among the entities (i.e., further textual tokens, ontological categories and attributes) in the context surrounding the token at hand. Moreover, segmentation with respect to a given attribute order relies on the underlying assumption that such an ordering is fixed across input sequences. This may make reconciliation problematic when, instead, the tokens of two or more attribute values are interleaved (rather than being concatenated) in the data to segment.

Furthermore, the reference table approach adopts ARMs for reconciliating individual attribute values. However, ARMs are basically HMMs and, hence, suffer from the aforemen-tioned limitations. Roughly speaking, ARMs are global classification models and, hence, overly specific in attribute recognition as far as three aspects are concerned, namely posi-tional, sequential and token specificity. These aspects impose suitable generalizations for the ARMs: the adoption of a fixed three-layered topology capable of dealing with positional and sequential specificities and the exploitation of token hierarchies for mitigating token specific-ity. On the contrary, RecBoost relies on association rules for attribute value reconciliations. Association rules are better suited at detecting local patterns, especially when the underlying data to segment contain many contrasting specificities. Moreover, a natural generalization of classifiers, i.e., the improvement of their classification accuracy, is trivially obtained by attempting to reduce classifier complexity, via attribute and rule pruning. 7 Discussion and future works The contribution of this paper was RecBoost, a novel approach to schema reconciliation, that fragments free text into tuples of a relational structure with a specified attribute schema. Within RecBoost, the most salient features are the combination of ontology-based general-ization with rule-based classification for more accurate reconciliation, and the adoption of progressive classification , as a major avenue towards exhaustive text reconciliation. An inten-sive experimental evaluation on real-world data confirms the effectiveness of our approach. Also, from a comparative analysis with state-of-the-art alternative approaches reveals the following two main arguments in favor of Recboost.  X  Due to the variable number of classification stages, RecBoost gives the user better control  X  The generic RecBoost classifier is easier to interpret than existing methods such as
There are some directions that are worth further research. First, notice that the proposed methodology is, in some sense, independent from the underlying rule-generation strategy. In this respect, it is interesting to investigate the adoption of alternative strategies for learning local classification models. This line is also , is a correlated with the effort for identifying a fully-automated technique for setting the parameters of progressive classification , in terms of required classification stages. Since parameters are model-dependent, two alternate strategies can be either to investigate different, parameter-free models, or to detect ways to enable a natural way of fixing the parameters of the system, on the basis of the inherent features of the text at hand, rather than relying on pre-specified estimates. The experimental section already contains some pointers in the latter direction: however, more robust methods need in-depth investigation.

In addition, we plan to investigate the development of an unsupervised approach to the induction of an attribute descriptor from a free text. This would still allow reconciliation, even in the absence of any actual knowledge about the textual information at hand. Finally, we intend to examine the exploitation of RecBoost in the context of the Entity Resolution process, to the purpose of properly filling in missing fields and rectifying both erroneous data-entry and transpositions oversights.
 References Authors Biography
