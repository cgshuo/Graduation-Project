 Recent years have seen widespread application of reproducing kernel Hilbert space (r.k.h.s.) based methods to machine learning problems (Sch  X  olkopf &amp; Smola, 2002). As a result, kernel methods seem to have received insufficient attention, at least within the machine learning community. Note we do not mean by this the local invariance (or insensitivity) of an algorithm to application translations. In Sections 2 and 3 we introduce the more general concept of transformation scaled-there exist no non-trivial p.d. kernel functions which are radial and dilation scaled. There do exist non-trivial c.p.d. kernels with the stated invariances however. Motivated by this, we analyse the c.p.d. case in Section 4, giving novel elementary derivations of some key results, most notably a c.p.d. representer theorem. We then give in Section 6.1 an algorithm for applying for the following reason. Due to its invariances, the c.p.d. thin-plate kernel which we discuss in Section 5, is not only richly non-linear, but enjoys a duality between the length-scale parameter and the regularisation parameter of Tikhonov regularised solutions such as the s.v.m. In Section 7 we compare the resulting classifier (which has only a regularisation parameter), to that of the s.v.m. with Gaussian kernel (which has an additional length scale parameter). The results show that the two algorithms perform roughly as well as one another on a wide range of standard machine learning problems, notwithstanding the new method X  X  advantage in having only one free parameter. In Section 8 we make some concluding remarks. Definition 2.1. Let T be a bijection on X and F a Hilbert space of functions on some non-empty set X such that f 7 X  f  X T is a bijection on F . F is T -scaled if for all f  X  F , where g T ( F )  X  R + is the norm scaling function associated with the operation of T on F . If g T ( F ) = 1 we say that F is T -invariant .
 The following clarifies the behaviour of Tikhonov regularised solutions in such spaces. Lemma 2.2. For any  X  : F  X  X  X  X  X  X  R and T such that f 7 X  f  X T is a bijection of F , if the left hand side is unique then Proof. Let f  X  = arg min f  X  X   X ( f ) and f  X  T = arg min f T  X  X   X ( f T  X T ) . By definition we have that  X  g  X  F ,  X ( f  X  T  X T )  X   X ( g  X T ) . But since f 7 X  f  X T is a bijection on F , we also have  X  g  X  X  ,  X ( f  X  T  X T )  X   X ( g ) . Hence, given the uniqueness, this implies f  X  = f  X  T  X T . The following Corollary follows immediately from Lemma 2.2 and Definition 2.1.
 Corollary 2.3. Let L i be any loss function. If F is T -scaled and the left hand side is unique then arg min Corollary 2.3 includes various learning algorithms for various choices of L i  X  for example the ( y i  X  t ) Definition 2.4. Let W s , T a and O A be the dilation, translation and orthonormal transformations R d  X  R d defined for s  X  R \ { 0 } , a  X  R d and orthonormal A : R d  X  R d by W s x = s x , T x = x + a and O A x = A x respectively.
 resultant decision function by some amount is equivalent training the s.v.m. on similarly dilated input patterns but with a regularisation parameter adjusted according to Corollary 2.3. as we have just seen it is easy to demonstrate for the more general Tikhonov regularisation setting with any function norm satisfying our definition of transformation scaledness. We now derive the necessary and sufficient conditions for a reproducing kernel (r.k.) to correspond to derive given the uniqueness of the r.k. (Wendland, 2004). It is given by the following novel Lemma 3.1 (Transformation scaled r.k.h.s.) . The r.k.h.s. H with r.k. k : X  X X  X  R , i.e. with k satisfying is T -scaled iff Which we prove in the accompanying technical report (Walder &amp; Chapelle, 2007) . It is now easy W s -scaled r.k.h.s. H with g W s ( H ) =  X  x , y  X  holds only under appropriate scaling (as per Corollary 2.3) of the margin softness parameter ( i.e.  X  of the later equation (14)).
 for all s 6 = 0 . First however we need the following standard result on homogeneous functions: Which we prove in the accompanying technical report (Walder &amp; Chapelle, 2007). Now, suppose that H is an r.k.h.s. with r.k. k on R d  X  R d . If H is T a -invariant for all a  X  R d then k x  X  y k  X  e where  X  e is an arbitrary unit vector in R d we have i.e. k is radial. All of this is straightforward, and a similar analysis can be found in (Wendland, 2004). Indeed the widely used Gaussian kernel satisfies both of the above invariances. But if we now also assume that H is W s -scaled for all s 6 = 0  X  this time with arbitrary g W s ( H )  X  then p = 0 , but there are various ways of showing this cannot be non-trivially positive semi-definite for For the corresponding Gram matrix In the last Section we alluded to c.p.d. kernel functions  X  these are given by the following Definition 4.1. A continuous function  X  : X  X X  X  R is conditionally positive definite with  X   X  R m \{ 0 } satisfying P m j =1  X  j p ( x j ) = 0 for all p  X  X  , the following holds Due to the positivity condition (4)  X  as opposed one of non negativity  X  we are referring to c.p.d. rather than conditionally positive semi-definite kernels. The c.p.d. case is more technical than the p.d. case. We provide a minimalistic discussion here  X  for more details we recommend e.g. (Wend-dard (see e.g. (Wendland, 2004; Wahba, 1990)), many authors in the machine learning community &amp; Smola, 2002)) or when P is taken to be the space of polynomials of some fixed maximum degree ( e.g. (Smola et al., 1998)). Let us now adopt the notation P  X  ( x 1 ,..., x m ) for the set The c.p.d. kernels of Definition 4.1 naturally define a Hilbert space of functions as per Definition 4.2. Let  X  : X  X X  X  R be a c.p.d. kernel w.r.t. P . We define F  X  ( X ) to be the Hilbert space of functions which is the completion of the set which due to the definition of  X  we may endow with the inner product remainder of this Section we develop a c.p.d. analog of the representer theorem. We begin with Lemma 4.3. Let  X  : X  X  X  X  R be a c.p.d. kernel w.r.t. P and p 1 ,...p r a basis for P . For any { ( x 1 ,y 1 ) ,... ( x m ,y m ) }  X  X  X  R , there exists an s = s F P j =1  X  j  X  (  X  , x j )  X  F  X  ( X ) and s P = P A simple and elementary proof (which shows (17) is solvable when  X  = 0 ), is given in (Wendland, 2004) and reproduced in the accompanying technical report (Walder &amp; Chapelle, 2007). Note that although such an interpolating function s always exists, it need not be unique. The distinguishing Definition 4.4. Let  X  : X X X  X  R be a c.p.d. kernel w.r.t. P . We use the notation P  X  ( P ) to denote the projection F  X  ( X )  X  X   X  F  X  ( X ) .
 Hence, returning to the main thread, we have the following lemma  X  our proof of which seems to be novel and particularly elementary.
 Lemma 4.5. Denote by  X  : X  X  X  X  R a c.p.d. kernel w.r.t. P and by p 1 ,...p r a basis for P . Consider an arbitrary function s = s F and s P = P r k =1  X  k p k  X  P . k P  X  ( P ) s k F satisfying Proof. Let f be an arbitrary element of F  X  ( X )  X  X  . We can always write f as [ X  zx ] i,j =  X  ( z i , x j ) , then the condition (6) can hence be written The inequality to be demonstrated is then By expanding (7) and (8) imply that L  X  R , since Using these results it is now easy to prove an analog of the representer theorem for the p.d. case. Theorem 4.6 (Representer theorem for the c.p.d. case) . Denote by  X  : X  X X  X  R a c.p.d. kernel w.r.t. P , by  X  a strictly monotonic increasing real-valued function on [0 ,  X  ) , and by c : R m  X  R  X  X  X  X  an arbitrary cost function. There exists a minimiser over F  X  ( X )  X  X  of which admits the form P m i =1  X  i  X  (  X  , x i ) + p , where p  X  X  .
 1 ...m . By Lemma 4.3 we know that such an s exists. But by Lemma 4.5 k P  X  ( P ) s k 2 F k P  X  ( P ) f k Definition 5.1. The m -th order thin-plate kernel  X  m : R d  X  R d  X  R is given by mials of degree at most m  X  1 . The kernel induces the following norm on the space F  X  m R d of Definition 4.2 (this is not obvious  X  see e.g. (Wendland, 2004; Wahba, 1990))  X  f,g  X  F where  X  : F  X  m R d  X  L 2 ( R d ) is a regularisation operator , implicitly defined above. Clearly g O A ( F  X  m R d ) = g T a ( F  X  m R d ) = 1 . Moreover, from the chain rule we have and therefore since  X  f,g  X  L  X   X  ( f  X  W s ) , X  ( g  X  W s )  X  L easily using (11) and an argument similar to Lemma 3.1, the process is actually more involved due w.r.t.  X  m  X  1 ( R d ) . Since this is redundant and not central to the paper we omit the details. In the Section 3 we showed that non-trivial kernels which are both radial and dilation scaled cannot be p.d. but rather only c.p.d. It is therefore somewhat surprising that the s.v.m.  X  one of the most widely used kernel algorithms  X  has been applied only with p.d. kernels, or kernels which are their absolute multiplicative scale.
 Hence we propose using the thin-plate kernel with the s.v.m. by minimising the s.v.m. objective over we require somewhat non-standard s.v.m. optimisation software. The method we propose seems simpler and more robust than previously mentioned solutions. For example, (Smola et al., 1998) mentions the numerical instabilities which may arise with the direct application of standard solvers. Table 1: Comparison of Gaussian and thin-plate kernel with the s.v.m. on the UCI data sets. Results are reported as  X  X ean % classification error (standard error) X . dim is the input dimension and n the total number of data points. A star in the n column means that more examples were available but we kept only a maximum of 2000 per class in order to reduce the computational burden of the extensive number of cross validation and model selection training runs (see Section 7). None of the data sets were linearly separable so we always used used the normal (  X  unconstrained) version of the optimisation described in Section 6.1. 6.1 Optimising an s.v.m. with c.p.d. Kernel space of functions P by extending the primal optimisation approach of (Chapelle, 2007) to the c.p.d. case. The quadratic loss s.v.m. solution can be formulated as arg min f  X  F Note that for the second order thin-plate case we have X = R d and P =  X  1 ( R d ) (the space of constant and first order polynomials). Hence dim ( P ) = d + 1 and we can take the basis to be p ( x ) = [ x ] j for j = 1 ...d along with p d +1 = 1 .
 approximation. Hence by repeatedly solving in this way while updating the set of margin violators, we will have implemented a so-called Newton optimisation. Now, since the local approximation of the problem is, in  X  and  X  the margin. The solution in this case is given by (Wahba, 1990) In practice it is essential that one makes a change of variable for  X  in order to avoid the numerical problems which arise when P is rank deficient or numerically close to it. In particular we make the solve for  X  and  X  = R  X  . As a final step at the end of the optimisation process, we take the minimum norm solution of the system  X  = R  X  ,  X  = R #  X  where R # is the pseudo inverse of R . Note that although (17) is standard for squared loss regression models with c.p.d. kernels, our use of it in optimising the s.v.m. is new. The precise algorithm is given in (Walder &amp; Chapelle, 2007), where method we present in Section 6.2 deviates considerably further from the existing literature. 6.2 Constraining  X  = 0 Previously, if the data can be separated with only the P part of the function space  X  i.e. with  X  = 0 space of the regulariser k P  X  ( P )  X k 2 F P =  X  1 ( R d ) and the solutions are simple linear separating hyperplanes. Finally, there may exist infinitely many solutions to (14). It is unclear how to deal with this problem  X  after all it implies that the regulariser is simply inappropriate for the problem at hand. Nonetheless we still wish to apply a (non-linear) algorithm with the previously discussed invariances of the thin-plate. is important to note that by doing so we can no longer invoke Theorem 4.6, the representer theorem for the c.p.d. case. This is because the solvability argument of Lemma 4.3 no longer holds. Hence The way we deal with this is simple  X  instead of minimising over F  X  ( X ) we consider only the finite dimensional subspace given by where x 1 ,... x n are those of the original problem (14). The required update equation can be ac-quired in a similar manner as before. The closed form solution to the constrained quadratic pro-gramme is in this case given by (see (Walder &amp; Chapelle, 2007)) PP  X  = 0 . The precise algorithm we use to optimise in this manner is given in the accompanying We now investigate the behaviour of the algorithms which we have just discussed, namely the thin-plate based s.v.m. with 1) the optimisation over F  X  ( X )  X  X  as per Section 6.1, and 2) the optimi-data is linearly separable, otherwise we use the first. For a baseline we take the Gaussian kernel k ( x , y ) = exp  X  X  x  X  y k 2 / (2  X  2 ) , and compare on real world classification problems. Binary classification (UCI data sets). Table 1 provides numerical evidence supporting our claim that the thin-plate method is competitive with the Gaussian, in spite of it X  X  having one less hyper parameter. The data sets are standard ones from the UCI machine learning repository. The experi-ments are extensive  X  the experiments on binary problems alone includes all of the data sets used in (Mika et al., 2003) plus two additional ones ( twonorm and splice ). To compute each error measure, each split, over an exhaustive search of the algorithm parameter(s) (  X  and  X  for the Gaussian and happily just  X  for the thin-plate). We then take the parameter(s) with lowest mean error and retrain on the entire four fifths. We ensured that the chosen parameters were well within the searched range by visually inspecting the cross validation error as a function of the parameters. Happily, for the Gaussian we had to choose both  X  and the scale parameter  X  . The discovery of an equally effec-tive algorithm which has only one parameter is important, since the Gaussian is probably the most popular and effective kernel used with the s.v.m. (Hsu et al., 2003).
 Multi class classification (USPS data set). We also experimented with the 256 dimensional, ten class USPS digit recognition problem. For each of the ten one vs. the rest models we used five fold cross validation on the 7291 training examples to find the parameters, retrained on the full training set, and labeled the 2007 test examples according to the binary classifier with maximum output. The Gaussian misclassified 88 digits (4.38%), and the thin-plate 85 (4.25%). Hence the Gaussian did not perform significantly better, in spite of the extra parameter. Computational complexity. The normal computational complexity of the c.p.d. s.v.m. algo-rithm is the usual O ( n sv 3 )  X  cubic in the number of margin violators. For the  X  = 0 variant (necessary only on linearly separable problems  X  presently only the USPS set) however, the cost experiments we expanded on all m training points, but if n sv m this is inefficient and proba-bly unnecessary. For example the final ten models (those with optimal parameters) of the USPS problem had around 5% margin violators, and so training each Gaussian s.v.m. took only  X  40 s in comparison to  X  17 minutes (with the use of various efficient factorisation techniques as detailed in the accompanying (Walder &amp; Chapelle, 2007) ) for the thin-plate. By expanding on only 1500 randomly chosen points however, the training time was reduced to  X  4 minutes while incurring only 88 errors  X  the same as the Gaussian. Given that for the thin-plate cross validation needs to be performed over one less parameter, even in this most unfavourable scenario of n sv m , the overall times of the algorithms are comparable. Moreover, during cross validation one typically encoun-ters larger numbers of violators for some suboptimal parameter configurations, in which cases the Gaussian and thin-plate training times are comparable. We have proven that there exist no non-trivial radial p.d. kernels which are dilation invariant (or has the same effect as changing the regularisation parameter  X  hence one needs model selection to chose only one of these, in contrast to the widely used Gaussian kernel for example. Motivated by this advantage we provide a new, efficient and stable algorithm for the s.v.m. with arbitrary c.p.d. kernels. Importantly, our experiments show that the performance of the algorithm nonetheless matches that of the Gaussian on real world problems.
 it is time to redress the balance. Accordingly we provided a compact introduction to the topic, including some novel analysis which includes an new, elementary and self contained derivation of one particularly important result for the machine learning community, the representer theorem.
