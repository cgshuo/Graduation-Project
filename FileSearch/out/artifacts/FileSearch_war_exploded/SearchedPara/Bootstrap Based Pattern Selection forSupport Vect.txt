 Support Vector Machines (SVM), developed by Vapnik based on the Structural Risk Minimization (SRM) principle [1], has performed with a great generalization accuracy [2]. Support Vector Regression (SVR), a regression version of SVM, was developed to estimate regression functions [3]. SVM is capable of solving non X  linear problems, but, has relatively high training time complexity O ( n 3 )and training memory span O ( n 2 )where n is the number of training patterns.
Such algorithms as Chunking, SMO, SVM light and SOR have been proposed to reduce the training time with time complexity T  X  O ( nq + q )where T is the number of iterations and q is the size of working set [4]. However, their training time complexities are still strongly related to the number of training patterns. Another direction of research focused on selecting important patterns from the training dataset directly with minimum accuracy loss. SVM X  X M [5], NPPS [6], Cross X  X raining based method [7] and Linear SVM based method [8] have been proposed. However, the binary class label s of training data are used to implement those approaches, which makes those approaches suitable for the classification problems, but not for the regression problems.

Recently, pattern selection methods de signed especially for SVR such as HSVM [9] and Sun X  X  method [10] have been proposed. However, both HSVM and Sun X  X  method have a couple of critical parameters which are to be empirically de-termined. A stochastic pattern selection method was proposed which estimates the margin space of  X   X  X nsensitive tube (  X   X  X ube) [11] and was successfully ap-plied to a response modeling problem [12]. However, those three methods tend to degrade accuracy when they train hig h dimensional datasets. Moreover the number of patterns selected was manipul ated with parameters or thresholds to be determined by users without any guidelines.

In this paper, we propose a bootstrap based pattern selection method. For better performances, we have focused on selecting support vectors. SVR can construct the same regression model with only support vectors. The proposed method tries to identify patterns whi ch are likely to become support vectors. Since support vectors are always located outside the  X   X  X ube under the  X   X  X oss function. by multiple bootstrap samples, the number of times a pattern located outside the  X   X  X ube are calculated, which is used as the likelihood of a pattern to become a support vector. On the other ha nd, the number of patterns selected is the most critical parameter. The prop osed method does not leave the number of pattern selected as a parameter, but determines by itself. We estimate the expected number of support vectors, whic h is used as the appropriate number of patterns selected. In our experiments, twenty datasets were used while HSVM, Sun X  X  method and random sampling were adopted as benchmark methods. We compared their results in terms of the training time and Root Mean Squared Error (RMSE).

The remaining of this paper is organized as follows. In Section 2, we provide the main idea of the proposed method and state the algorithm. In Section 3, we present details of datasets and parameters for experiments as well as the result. In Section 4, we summarize the results and conclude the paper with a remark on limitations. SVR can train the same regression model even if input patterns were only sup-port vectors. Hence, in some sense, s upport vectors are an ideal subset to be selected. However, before training, th ere is no way to identify support vectors from all training patterns. In this paper, we propose a bootstrap approach which estimates a likelihood of each pa ttern to become a support vector.

We construct k bootstrap samples D j = { ( x j i ,y j i ) } l i =1 ( j =1 ,  X  X  X  ,k )of size l ( l n ) from the original dataset D = { ( x i ,y i ) } n i =1 .Wethentrainan SVR with each bootstrap sample D j and obtain k SVR regression functions f j ( j =1 ,  X  X  X  ,k ). Every pattern x i in the original dataset D is evaluated by each regression function f j , whether it is located inside or outside the  X   X  X ube. If pat-tern x i is located outside the  X   X  X ube of f j , the pattern is marked, i.e. m ij =1. We then calculate L i = k j =1 m ij , the number of total markings of the pattern x , which is used as the estimated likelihood of x i to become a support vec-tor. At the same time, we calculated th e expected number of support vectors S marked by f j . After calculating L i and S , we select S patterns deterministically with largest L i . Or, we select S patterns stochastically based on the probability an example of the proposed method with a toy dataset while Fig. 2 presents the algorithm.

The number of patterns selected is a key factor of the proposed method. In the proposed method, a pattern located farther from the regression function is more likely to be selected. If the number of patterns selected is much smaller than the necessary number, only noisy patterns can be selected. On the other hand, if the number of patterns selected is much larg er than the necessary number, pattern selection falls into a meaningless effort. By introducing S , we get a guideline of the number of patterns selected. Our experiments were conducted on twenty datasets including four artificial datasets and sixteen real world benchmark datasets. Real world benchmark datasets including time series datasets were gathered from Delve datasets 1 ,Time Series Data Library (TSDL) 2 , Statlib 3 and Korean Stock Market 4 . All datasets are summarized in Table. 1.

Artificial dataset 1 was originally introduced from [13] based on a mathemat-ical function, y = sin  X x  X x +  X  where x  X  [  X  3 , 3] and  X   X  N (0 , 0 . 5 2 ). Artificial dataset 2 introduced from [1 1] was generated based on y =2cos(15 x )+(  X  1 +  X  2 ) dataset 3 is newly generated from a mathematical function, y =sin(2 x )+  X  where x  X  [0 , 5] and  X   X  N (0 , 0 . 5 2 ). Add10 dataset is another artificial dataset gathered from the Delve datasets. We used only five relevant input features ex-cluding five noise terms. Time series datasets were re X  X ormulated as regression problems by using the previous 10 values to estimate the following one value, which is a typical way to solve time series problems. The Foreign Exchange dataset was re X  X ormulated as a regression problem to estimate British/US ex-change rate by using other 6 nations X  exchange rates while the wind dataset was re X  X ormulated to estimate the wind speed of Dublin station using observed other 11 stations X  wind speed. For evaluating performances, the original dataset was randomly split into training and test data. The hyper X  X arameters of SVR were { 0 function and the kernel parameter  X  was fixed to 1.0 for all datasets. All datasets were normalized.

HSVM, Sun X  X  method and random sampling were implemented to be com-pared. For HSVM, the partitioning parameter was set to 10 and similarity threshold was set to be the average value of similarities of all patterns. For Sun X  X  method, the k of k  X  X N was fixed to 5 and the number of patterns selected was set to be similar to S from the proposed method. The parameters of the proposed method, k and l , were fixed to 10 and 10% of n , respectively. We eval-uated the performances of each method by RMSE and training time (sec.). All experimental results were ave raged over 30 repetitions.

Fig. 3 shows the experimental results of artificial datasets including three ar-tificial datasets and Add10 dataset from the Delve datasets. The pairs of RMSE and training time in seconds are plotted corresponding to each method. The closer a result is plotted to the origin, the better the method performs. The solid line indicates the results of the random sampling from 10% to 100% of n .Marked squares and marked circles are experimental results of the proposed method with the deterministic selection (DET) and the stochastic selection (STO), re-spectively. The results of random sampl es were polynomially decreased as the number of patterns selected goes smalle r. As Fig. 3 shows, the propose method shows competitive results. The propose d method resulted better performances than benchmark methods in terms of pairs of RMSE and training time. Random sampling resulted the best for artificial dataset 1. In this case, artificial dataset 1 was so easy that random sampling can handle it sufficiently.

Table 2 shows the experimental results o f sixteen real world datasets in terms of RMSE.  X  Dataset X  represents the index number of datasets given in Ta-ble 1 while  X  X 30 X ,  X  X 50 X  and  X  X 70 X  represent the random sampling with 30%, 50% and 70% of original patterns, respectively. The deterministic selection of the proposed method resulted the best accuracy among the benchmark meth-ods for fifteen datasets. Table 3 shows th e observed training time. Each cell of Table 3 represents the percentage of training time of a method compared to orig-inal training time. HSVM and the stochastic selection of the proposed method ranked 1 st for eight datasets and for five dataset s, respectively. The deterministic selection of the proposed method used much training time than HSVM, but, still used only around 15  X  50% of original training time.
All the experimental results are summarized in Table. 4. Compared to bench-mark methods, the deterministic select ion of the proposed method, which showed the smallest standard deviation, ranked 1 st for eighteen datasets over twenty datasets in terms of RMSE. Averaging ov er all experimental results, the pro-posed method with the deterministic selection can train SVR using 30.35% of original training time with only 2.58% of increased error. The proposed method with the stochastic selection increases RMSE about 2.5% than the proposed method with the deterministic selectio n, but it was still competitive. The sen-sitivity analysis shows that the proposed method selected on average 87 . 78% deterministically and 73 . 44% stochastically of the actual support vectors. This paper provides a new pattern selection method to reduce training time of SVR. Only those patterns that were likely to become support vectors were selected and used for training. The proposed method automatically determined the number of patterns selected, which was a key factor of obtaining good results. Twenty datasets including sixteen real wo rld datasets were ana lyzed. The results showed that the generalization perform ance of the proposed method was better than other benchmark methods. It performed well for diverse datasets.
Another strong point of the proposed method is that it has fewer critical pa-rameters than benchmark methods. Several parameters such as similarity thresh-old and the number of patterns selected affect the accuracy but are ambiguous to users. Not only is there no guideline for those parameters, but also a parameter set determined best to a certa in dataset is rarely best to o ther datasets. However, the proposed method only needs to set k and l which do not affect the accuracy directly. In this paper, the proposed method showed good performances even though we used only one fixed parameter set.

There are limitations of the current work. The result of the proposed method can be largely affected by the number of support vectors. This method may select too many patterns for some applications that have a lot of support vectors. This work was supported by the Korea Science and Engineering Foundation (KOSEF) grant funded by the Kore a government (R01 X 2005 X 000 X 103900 X 0), the Brain Korea 21 program in 2007, and partially supported by Engineering Research Institute of SNU.

