 Sunil Kumar Gupta sunil.gupta@deakin.edu.au Dinh Phung dinh.phung@deakin.edu.au Svetha Venkatesh svetha.venkatesh@deakin.edu.au Multi-task learning aims to improve generalization performance of related tasks by joint learning (Caru-ana, 1997). Empirical and theoretical evidence sup-port this claim (Ando &amp; Zhang, 2005; Baxter, 2000; Argyriou et al., 2008; Xue et al., 2007), with appli-cations in medical diagnosis (Bi et al., 2008), hand-written digit recognition (Kang et al., 2011) and im-age/video search (Wang et al., 2009). In real-world situations, where there is limited data, it is useful to combine related tasks and exploit common statisti-cal strengths for improved prediction. A common ap-proach is to assume similarity across tasks -the task predictors may belong to a low dimensional subspace or manifold (Argyriou et al., 2008; Rai &amp; Daum X  III, 2010; Agarwal et al., 2010), form clusters (Xue et al., 2007; Kang et al., 2011; Passos et al., 2012) or share a generative process (Daum X  III, 2009). However, given a pool of so called  X  X elated X  tasks, it is not easy to as-sess the degree of similarity/relatedness. If tasks are not related or related minimally, joint learning may degrade performance  X  a phenomenon widely known as negative transfer learning (Rosenstein et al., 2005). Therefore, automatically inferring task relatedness is crucial to the success of multi-task learning. Although a critical problem, there had been few works on separating the unrelated tasks from the set of re-lated tasks. The problem is hard since the tasks usu-ally have varying degree of relatedness. One approach represents task predictors (or parameters) in a low dimensional subspace (Argyriou et al., 2008; Rai &amp; Daum X  III, 2010; Agarwal et al., 2010). However, a single subspace assumes all the tasks are related, and this may cause performance degradation when tasks are unrelated. Dealing with this problem, some works divide tasks into groups and learn one subspace for each group (Kang et al., 2011; Passos et al., 2012). Modeling tasks in this way is extreme in that the tasks in different groups can not influence each other. This is because real world tasks can rarely be catego-rized as totally  X  X elated X  or  X  X nrelated X . Instead, there exists a varying degree of relatedness. This problem was partially tackled by (Kumar &amp; Daum X  III, 2012), who propose a model that learns a subspace whose bases are shared across tasks and the relatedness be-tween tasks is determined by the number of shared bases. However, being a parametric model, this model needs a priori specification of parameters: the num-ber of task groups and the dimensionality of the sub-space. The performance of this method crucially de-pends on these parameters and it is hard to estimate them for the real data. Therefore, a nonparametric extension of this model is required. The work in (Pas-sos et al., 2012) is a related Bayesian nonparametric MTL model -however, it does not allow sharing be-tween tasks across groups, and thus is not flexible to capture varying degree of relatedness. (Gupta et al., 2012a;b) model data from multiple groups through a subspace allowing sharing of bases. However, their model is suitable only for unsupervised learning and requires the number of groups and group membership of data points to be specified a priori .
 Addressing this gap, we propose a Bayesian nonpara-metric MTL framework that groups tasks based on their relatedness in a low dimensional subspace. The assumption is that the when tasks are related, task predictors lie close in subspace. To model varying degrees of sharing across tasks, we use a joint factor modeling approach that allows task predictors to have both shared and individual subspace bases. We refer to the approach as factorial multi-task learning (F-MTL). Since our goal is to model a set of T task pre-dictors jointly, we employ hierarchical factor analysis  X  a modeling paradigm that can jointly model the data from multiple groups through a subspace such that some of the subspace bases are shared across groups while the others are individual to a group. However, using hierarchical factor analysis in its standard form requires the tasks to be grouped in advance  X  some-thing that is unknown for multi-task learning problem. To address this, we extend the hierarchical factor anal-ysis for modeling tasks whose grouping is unknown. The task membership to the groups and the number of groups are inferred by clustering the tasks in the sub-space through a Dirichlet process (DP), while the sub-space dimensionality and the sharing configurations are inferred using a hierarchical beta process (HBP). For an optimal solution, the two processes are unified by coupling the DP prior with the HBP prior, lead-ing to a novel Bayesian nonparametric prior termed as generalized hierarchical beta process (G-HBP). The proposed model is applied in two settings : multi-task regression and classification. The model inference is done using Gibbs sampling. Experimental results using several synthetic and real-world datasets show the superiority of the proposed model to recent state-of-the-art multi-task learning methods. Our main con-tributions are  X  A novel Bayesian nonparametric prior extending  X  A Bayesian nonparametric, multi-task learning  X  Inference using a novel combination of Gibbs sam-The significance of our approach is that the number of task-groups, the subspace dimensionality and the usage of bases by different groups are automatically inferred from data. In addition, the Bayesian nonpara-metric priors keep the model flexible to allow each task group to also have its own set of factors and therefore every group need not necessarily share factors. This feature is the key to overcome the problem of any neg-ative inductive bias due to unrelated tasks. This leads to a flexible model that can be applied freely on a pool of related/unrelated tasks without any performance degradation, exploiting statistical strengths from even marginally related tasks. 2.1. Dirichlet Process Mixture (DPM) Model Dirichlet process (DP) (Ferguson, 1973) has been widely used in Bayesian mixture models for cluster-ing applications. It provides a Bayesian nonparamet-ric prior over clustering partitions enabling a mixture model to accommodate infinitely many components. For a given set of observations, the active set of com-ponents are finite and can be inferred from the pos-terior distribution. Assuming that we have a set of observations { c t } T t =1 with the corresponding mixture component parameters as {  X  t } T t =1 where  X  t  X  X  are real-izations from a Dirichlet process G with concentration parameter  X  0 and base measure G 0 . Using a paramet-ric distribution F c (  X  ) for data, we can write Using a constructive definition of DP (Sethuraman, 1994), the measure G can also be written as G = P j =1  X  j  X   X  j . To relate {  X  1 ,..., X  T } with {  X  1 ,..., X  we can use an indicator u t for each  X  t such that u t = j if  X  t =  X  j . In applications of DPM to clustering, u t represents the cluster indicator for data c t and we have 2.2. Beta Process In factor analysis, inferring the number of factors requires us defining a prior on the usage probabili-ties of factors. Under Bayesian framework, this can be realized using a Bernoulli process prior that is parametrized by a beta process (Thibaux &amp; Jordan, 2007). Formally, a beta process B is a completely random measure implying that for any r disjoint sets S ,...,S r  X   X  (where  X  is a measurable space with sigma field F ), B ( S 1 ) ,...,B ( S r ) are independent and the draws from the beta process B are discrete with probability one (Kingman, 1967). We denote the beta process as B  X  BP (  X  0 ,B 0 ) , where  X  0 &gt; 0 is referred to as a concentration parameter and B 0 is a base measure with total mass B 0 ( X ) =  X  0 . In set function notation, we can write B = P k  X  k  X   X  k where {  X  k , X  k } are drawn from a non-homogeneous Poisson process defined on the product space  X   X  [0 , 1] . The distribution over weight  X  k follows For a discrete B 0 , i.e. B 0 = P k p k  X   X  k , we have B 0 (  X  k ) = p k . For a continuous B 0 ,  X  k are drawn from a Poisson process with the following rate measure Fixing  X  0 to one,  X  k can be sampled using the stick-breaking construction (Teh et al., 2007), i.e. The beta process defined above can be used to parametrize a Bernoulli process, which then can be used to infer the number of factors in the factor analy-sis. Formally, let z t be a draw from a Bernoulli process, i.e. z t  X  BeP ( B ) , then we have z t,k  X  Bernoulli (  X  If Z is defined to be a collection of { z t } T t =1 , the pos-terior samples of Z given data gives us an estimate of which factors out of an infinite set are required to explain the data  X  indirectly inferring the number of factors automatically from the data. 2.3. Hierarchical Beta Process and Factor A major strength of probabilistic modeling is to be able to express the dependencies through hierarchies. Building a hierarchy over Dirichlet processes, (Teh et al., 2006) proposed hierarchical Dirichlet process (HDP) that allows data form multiple groups to share a common set of parameters. Motivated by the con-struction of HDP, (Thibaux &amp; Jordan, 2007) devel-oped a similar hierarchy over beta processes called as hierarchical beta process (HBP). Formally,
B  X  BP (  X  0 ,B 0 ) ,A j  X  BP (  X  j ,B ) , Z t, : j  X  BeP ( A where Z t, : j denotes t -th data point from j -th group. Similar to the use of beta process in nonparametric factor analysis, hierarchical beta process is used for a nonparametric hierarchical factor analysis (NHFA) (Gupta et al., 2012a)  X  a model that can jointly model data from multiple groups (or sources). Given data { X j } J j =1 from J -groups, hierarchical factor analysis (also encountered in shared subspace learning (Gupta et al., 2011; 2013)) is carried out as
X j =  X  H T j + E j , j = 1 ,...,J (known grouping) (7) where  X  = [  X  1 ,..., X  K ] contains the subspace bases, and H j denotes the subspace representations for X j . Carrying out a joint factor analysis in this manner al-lows some of the bases to be shared across the groups while keeping others individual to a group. Being a Bayesian nonparametric model, NHFA can infer the number of shared and individual bases using a hierar-chical beta-Bernoulli process prior (Thibaux &amp; Jordan, 2007). For this, H j is decomposed as H j = Z j W j where W j contains the actual subspace representa-tions and Z j is a binary matrix with its ( t,k ) -th el-ement indicating the presence or absence of the basis  X  k for the t -th data point of j -th group. We note that (6) can be used as a prior over Z 1 ,..., Z J and given the data, posterior distribution can be used to get an estimate of the number of shared / individual bases. In this section, we describe a our framework for multi-task learning. Our goal is to jointly model the tasks with varying degree of relatedness in such a way that related tasks strengthen each other while unrelated tasks do not affect themselves. The underlying as-sumption is that the when tasks are related, the tasks predictors closely lie in a subspace. For this, we use a joint factor modeling approach that allows task pre-dictors to have both shared and individual subspace bases. We refer to our approach as factorial multi-task learning (F-MTL). Let us assume we have T tasks, in-dexed as t = 1 ,...,T and t -th task has labeled training examples denoted as D t = { ( x ti ,y ti ) | i = 1 ,...,N where x ti  X  R M  X  1 and we define X t = [ x t 1 ,..., x tN and y t = [ y t 1 ,...,y tN t ] T . Let the task predictor (the regression or classification weights) of t -th task be de-noted as  X  t where  X  t  X  R M  X  1 . We use  X  to collectively define all the task predictors, i.e.  X  = [  X  1 ,..., X  T ] . Since our goal is to model a set of T task predictors jointly, we employ hierarchical factor analysis  X  a mod-eling paradigm that can jointly model the data from multiple groups through a subspace such that some of the subspace bases are shared across groups while the others are individual to a group. However, using hi-erarchical factor analysis in its standard form requires the tasks to be grouped in advance  X  something that is unknown for multi-task learning problem. To address Figure 1: Directed graphical representation for the pro-this problem, we shall extend the hierarchical factor analysis for modeling a set of tasks whose grouping is unknown. If the task groupings are known (i.e. a partition of  X  having J groups as {  X  1 ,...,  X  J } ), fol-lowing (7) we can model the task predictors as where  X  = [  X  1 ,..., X  K ] contains basis vectors of a sub-space (spanned by the column of  X  ) and H j is the representation of the task predictors of j -th group (i.e.  X  ) in the subspace. The matrix E j represent model-ing errors. We note that K is overall subspace dimen-sionality. For j -th task group, some of the basis vectors may not be used and thus, its effective dimensionality K j  X  K . For real world applications, the values of K and K j are hard to guess and require model selection . Conventional model selection approaches are compu-tationally expensive and wasteful of data (Corduneanu &amp; Bishop, 2001). The subspace dimensionalities can be automatically inferred using a hierarchical beta pro-cess prior over  X  and H j (Gupta et al., 2012a). In par-ticular, H j can be written as an element-wise product H j = W j Z j where W j denotes the subspace rep-resentations and Z j is a binary matrix denoting the presence or absence of a basis vector. Using Bernoulli process priors over matrices Z 1: J in combination with the task-predictor likelihood, an estimate of K and K j is obtained from the posterior distribution. The g -th vector of j -th group, z g,j is drawn as
B  X  BP (  X  0 ,B 0 ) ,A j  X  BP (  X  j ,B ) , z g,j  X  BeP ( A where B = P k  X  k  X   X  k , A j = P k  X  jk  X   X  k and A j are Figure 2: A summary of the G-HBP prior, and the F-the beta distributions respectively;  X  = [  X  1 , X  2 ,... ] and i.i.d. draw from a beta process. When the task group-ings are unknown , we can use a group indicator vari-able u t such that u t  X  { 1 ,...,J } and u t is to be in-ferred from data . For this, we use a Dirichlet process (DP) as a prior to induce clusters over z t , creating an infinite mixture of Bernoulli processes . Formally where GEM (  X  0 ) denotes the well-known stick-breaking construction for DP. Since t -th task pre-dictor belongs to the group indexed by u t , we fur-ther use g u t to index its position in u t -th group. To keep the notation simple, we write g u t just as g t . As an example, consider 4 tasks in-dexed as t = 1 , 2 , 3 , 4 ; assuming a partition (1 , 4) and (2 , 3) , we have ( u 1 = 1 ,g 1 = 1) , ( u 2 = 2 ,g 2 ( u 3 = 2 ,g 3 = 2) , ( u 4 = 1 ,g 4 = 2) . Using this notation We call the prior described by (13-15) as generalized hierarchical beta process (G-HBP) prior. The closed form inference for the proposed model is intractable. Therefore, we use Markov chain Monte Carlo (MCMC) (Gilks et al., 1995), which is widely used for performing inference with such kind of hi-erarchical Bayesian models. The MCMC state space comprises of the variables {  X  ,  X  , W , Z , X , X , u , X , X  } where u , { u 1 ,...,u T } . However, some of these variables {  X , X , X  } are nuisance variables. Due to us-ing conjugate priors, we can integrate out {  X , X  } and sample the remaining variables. For inference, we combine Gibbs sampling with adaptive rejection sam-pling/Laplace approximation (Bishop et al., 2006). To sample  X  , we use stick-breaking process construc-tion of the Indian buffet process (Teh et al., 2007). This is made possible by fixing the concentration pa-rameter of the parent beta process to one. How-ever, the stick-breaking process construction requires us maintaining an infinite set of atoms. This is dealt with by using the slice sampler (Gupta et al., 2012b), which turns the infinite representation into a finite one. The slice sampler employs an auxiliary variable  X  that can be sampled from a uniform distribution. In par-ticular,  X  | Z , X   X  U (0 , X   X  ) where  X   X  = min and  X  ( k ) is a decreasing order representation of  X  k . need to update Z t,k for only those k such that  X  ( k )  X   X  . Assuming K  X  to be an index so that all active features have index k &lt; K  X  , if  X  ( K  X  )  X   X  , we extend our stick-breaking representation until  X  ( K  X  ) &lt;  X  (see (Gupta et al., 2012b) for details). 4.1. Sampling  X  Sampling of  X  t can be done independently for each t . Under the Gibbs sampling framework, we can sample  X  from the following conditional posterior distribution Since our data generative process differs for regression and classification, we separately describe them below. 4.1.1. Regression Using the prior distributions of (11-12) for regression model, the posterior p (  X  t | ... ) can be derived to be a multi-variate Gaussian with mean and covariance as where we have h t , z t w t . 4.1.2. Classification Using the generative distributions of (11-12), the con-ditional posterior p (  X  t | ... ) of (16) can be written as where we define s ti ,  X   X  T t x ti and R t , ||  X  t  X   X  h The above expression can not be simplified to standard parametric distributions. Therefore, exact inference is intractable. From this point, we found two ways to proceed (1) adaptive rejection sampling (ARS) (2) Laplace approximation. The first method fits well un-der the Gibbs framework while ensuring sampling from the exact distribution (Gilks &amp; Wild, 1992). To mo-tivate the application of adaptive rejection sampling, we note that the second derivative of log of the above posterior is given as where D s (  X  t ) , diag ([ s t 1 (1  X  s t 1 ) ,...,s tN t is a diagonal matrix with entries between 0 and 1 . We note that the Hessian given by (19) is a negative semi-definite implying that p (  X  t | ... ) is log-concave . This allows efficient sampling of  X  t through ARS. The second method to obtain the samples of  X  t is to use Laplace approximation, which is obtained by finding the mode of the posterior distribution and then fit-ting a Gaussian having mean at the computed mode. We can find the mode of the posterior by maximizing the log of the posterior, which can be done by find-can be done using either gradient-descent or Newton X  X  method. The co-variance of the Gaussian is given by the negative of the inverse of Hessian of log posterior, in (18) takes the form In our implementation, we found that both gradient-descent and Newton X  X  method methods worked fairly well and the later was clearly faster. 4.2. Sampling u t Gibbs conditional posterior of u t can be written as where we define u  X  t = { u t 0 | t 0 6 = t } and Z  X  g t { z g 0 ,u t | g 0 6 = g t } . In the above expression, the first term is the predictive prior distribution of u t and widely known as Chinese Restaurant process (CRP). The second term is the likelihood, which measures how well t -th task predictor goes along with the group in-dexed by u t in latent space. The predictive distribu-the total number of tasks in the group indexed by u t . Plugging (22), the CRP prior over u t and the normal prior over w g t ,u t into (21), we can sample u t from a categorical distribution. 4.3. Sampling Z Gibbs conditional posterior for Z can be written as { z tk 0 | k 0 6 = k } . The predictive prior simplifies to Thus the variable z tk can be drawn from the posterior distribution of (23), which is a Bernoulli distribution. 4.4. Sampling W ,  X  and  X  Once we sample the group indicator of the t -th task (i.e. the variable u t ), we have a partition of the task predictors  X  into J groups. In other words, we have a partition of the tasks as  X  = [ X  1 ,...,  X  J ] where  X  j is a matrix formed by stacking (column-wise) each task predictor t such that u t = j . Using u t  X  X , we can parti-tion the matrix W similarly. Given this partition, we can utilize the sampling steps in (Gupta et al., 2012b) to sample  X  , W ,  X  and the hyperparameters. We evaluate our proposed F-MTL on multi-task re-gression and classification applications using both syn-thetic and real datasets. Experiments with synthetic data do a sanity check of the model while also il-lustrating the model behavior. Experiments with real datasets demonstrate the true effectiveness of our model for multi-task learning. 5.1. Synthetic Data Experiments For synthetic data experiments, we use two different datasets. The first dataset is identical to the synthetic data used in (Kang et al., 2011; Kumar &amp; Daum X  III, 2012). This dataset has 3 groups of tasks. Within each task-group, there are 10 tasks whose predictors (i.e.  X  t ) are identical up to a scaling factor. Each task has 15 examples lying in a 20 -dim space. The task predictors are used in a linear regression model to generate target values of the training data. We note that only tasks within a group are related.
 The second dataset is generated to simulate the vary-ing degree of relatedness among task predictors. The degree of relatedness is controlled by varying the num-ber of subspace bases shared by the tasks. For this dataset, we have 4 groups of tasks, 10 tasks per group and 4 bases. The task predictors of the first group are generated as random (Gaussian with zero mean and one standard deviation) linear combination of the basis-1 and basis-2. The task predictors of the second group are generated similarly using basis-2 and basis-3 causing basis-2 to be shared between groups 1 and 2. Tasks of the third group are generated using basis-1 and basis-3 to simulate relatedness with the tasks in the first group. Finally, a fourth group is created with task predictors using only basis-4 and thus not share anything from the tasks of other groups. For each task, we randomly generate 15 examples in a 20 -dim space. These task predictors are used in a linear regression model to generate the target values where we intro-duce an additive Gaussian noise with zero mean and standard deviation 0 . 1 .
 For our model, we initialized all the hyperparameters ( i.e.  X  y , X  n , X  w , X , X , X  j ) to 1 while both the number of bases ( K ) and the number of groups ( J ) to 10. We run the Gibbs sampler for 500 iterations and the results (after rejecting first 200 samples as burn-in) are shown in Figure 3. The first row of figures depict the results for the first synthetic dataset. The joint posterior over ( K,J ) in Figure 3(a) clearly shows the mode of the dis-tribution at (3 , 3)  X  X ccurately recovering the subspace dimension and the number of task groups. Figure 3(b) depicts the basis usages of different tasks clearly show-ing that the three groups are nearly disjoint in using the bases. The 3(c) shows the root-mean-square-error (RMSE) for all the tasks, which is roughly at the true noise level. The second row of figures depict the simi-lar results for the second synthetic dataset. We can see that model infers the true number of bases and groups (see the mode at (4 , 4) in Figure 3(d)) along with the correct basis usages including the sharing/non-sharing patterns. The model captures the fact that the first three task groups are related with one another while the fourth is disjoint. This ensures that there is no inductive bias transferred from the first three groups to the fourth group and vice versa . 5.2. Real Data Experiments We perform experiments on four real datasets: two of them have regression tasks and the other two have classification tasks. 5.2.1. Datasets  X  Computer Survey : This dataset contains a sur- X  School : This dataset consists of examination  X  MNIST and USPS : Both of these datasets con-To have a fair comparison, we keep the training and test sets for all the above datasets identical to the ones used in (Kumar &amp; Daum X  III, 2012). Further, all our comparisons are based on the RMSE for regression and multi-class classification error for classification. 5.2.2. Baseline Methods  X  STL : This baseline learns each task separately  X  NG-MTL (Argyriou et al., 2008): This model  X  DG-MTL (Kang et al., 2011): This model par- X  GO-MTL (Kumar &amp; Daum X  III, 2012): This  X  MFA-MTL (Passos et al., 2012): This is a 5.2.3. Experimental Results Table 1 presents a comparison of our proposed F-MTL with the first four baselines for both regression and classification tasks. The F-MTL clearly outper-forms all the baselines for both tasks irrespective of the datasets. The closest contender is the GO-MTL, whose performance on regression tasks is somewhat close, however, the difference is still statistically signifi-cant (details omitted). On classification tasks, F-MTL gets significantly better results (6.4% improvement on MNIST and 3.6% improvement on USPS) than GO-MTL. This is important especially under the view that both methods use logistic regression for mapping in-puts to target outputs. We attribute this improve-ment in performance to the following (1) F-MTL being a Bayesian nonparametric model automatically infers the number of groups and subspace bases from data and thus uses optimal sharing (2) F-MTL shares the latent bases across different task groups (enables over-lapping/grouping) while allowing a separate distribu-tion for each group respecting the idiosyncrasies. We also compare our method with MFA-MTL (Passos et al., 2012). This comparison is presented separately as this baseline uses different datasets (Landmine and 20 Newsgroup) for classification and the computer and school datasets for regression but with different train-ing/test settings. We use the same training and test sets as in (Passos et al., 2012). Table 2 shows the comparison between the two models where we can see that F-MTL clearly outperforms MFA-MTL on school dataset for regression and both Landmine and 20-Newsgroup datasets for classification.
 Finally we show the performance of our method for varying fractions of training data and compare it with those of STL and NG-MTL. For this, we used the same training and test sets as used for generating Table 1. For all the datasets, we increased the training exam-ples from 10% to 100% with a step of 10% and mea-sured the performances. The results are shown in Fig-ure 4. We can see from the figure that the performance of the proposed method is consistently better irrespec-tive of the size of the training set. We present a novel Bayesian nonparametric, factorial multi-task learning (F-MTL) framework that groups the similar task predictors by representing them in a low dimensional subspace. A key feature of the pro-posed F-MTL is that it automatically infers the num-ber of task groups and allows the groups to share the subspace bases. This feature enables the framework to jointly model the tasks with varying degree of related-ness. For this, we propose a generalized hierarchical beta process (G-HBP) prior that permits a hierarchy of potentially infinite number of child beta processes controlled via a Dirichlet process. Another key feature of the proposed model is that it uses a different dis-tribution of basis usages for each group allowing each group to vary when necessary. This has implications in overcoming the negative inductive biases from un-related tasks. Using synthetic and real datasets, we demonstrate the utility of the model for regression and classification tasks while outperforming many recent state-of-the-art multi-task learning techniques.
