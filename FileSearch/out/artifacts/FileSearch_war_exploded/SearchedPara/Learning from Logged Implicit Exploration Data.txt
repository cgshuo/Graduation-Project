 Consider the advertisement display problem, where a search engine company chooses an ad to dis-Google, Yahoo!, and Facebook.
 Before discussing the proposed approach, we formalize the p roblem and then explain why more conventional approaches can fail.
 The warm-start problem for contextual exploration: Let X be an arbitrary input space, and Events occur on a round-by-round basis where on each round t : of other actions a  X  6 = a is not revealed.
 The standard goal in this setting is to maximize the sum of rew ards r a policy h maximizing (either exactly or approximately) Approaches that fail: There are several approaches that may appear to solve this pr oblem, but turn out to be inadequate: Our Approach: The approach proposed in the paper naturally breaks down int o three steps. is completely deterministic.
 Three critical questions arise when considering this appro ach. bound in section 3.2 and then experimental results in sectio n 4 using real data. Let  X  each of the form ( x, a, r and the action a  X   X  samples, which we denote S  X  ( D,  X  Offline policy estimator: Given a dataset of the form where  X  t, x with a threshold  X   X  [0 , 1] to form an offline estimator for the value of a policy h . Formally, given a new policy h : X  X  A and a dataset S , define the estimator: where I ( ) denotes the indicator function. The shorthand  X  V h like robust importance sampling [10].
 like robust importance sampling [10]. Our main result, Theorem 3.2, provides a generalization bou nd X  X ddressing the issue of how both the estimation and optimization error contribute to the tot al error. The logging policy  X  randomization in the algorithm for randomization in the wor ld.
 where UNIF( ) denotes the uniform distribution. The stochastic policy  X  chooses an action uni-formly at random over the T policies  X  our paper. Note that the policies  X  relaxed in practice, as we show in Section 4.1.
 Theorem 3.1. For any contextual bandit problem D with identical draws over T rounds, for any sequence of possibly stochastic policies  X  simpler and more standard setting where a single fixed stocha stic policy is used. 3.1 Policy Estimation shows that we can think of the data as generated by a fixed stoch astic policy  X  , i.e.  X  For a given estimate  X   X  of  X  define the  X  X egret X  to be a function reg: X  X  [0 , 1] by We do not use  X  estimator as defined by Equation 2 based on parameter  X  .
 Lemma 3.1. Let  X   X  be any function from X to distributions over actions A . Let h : X  X  A be any deterministic policy. Let V h ( x ) = E on input x . We have that
In the above, the expectation E [  X  V h D and a  X   X  ( | x ) . 1 that we analyze the result in terms of the squared loss rather than (say)  X  Lemma 3.1 shows that the expected value of our estimate  X  V h  X   X  and the lower bound is due to the threshold  X  . When  X   X  =  X  , then the statement of Lemma 3.1 simplifies to Thus, with a perfect predictor of  X  , the expected value of the estimator  X  V h chosen by  X  .
 instance of the bandit problem with a single input x and two actions a pose further that the rewards are always 1 and that  X   X  ( a isfies E [  X  V h E [  X  3.2 Policy Optimization  X  generated from T fixed, possibly deterministic, policies  X  now assumed to be drawn from the execution of a sequence of T policies  X  T draws from  X  .
 support under  X  (even though the data are not generated from  X  ).
 Let  X  h = argmax in Equation 2. Then, with probability at least 1  X   X  , where reg( x ) is defined, with respect to  X  , in Equation 5.
 We evaluated our method on two real-world datasets obtained from Yahoo!. The first dataset con-then demonstrates how policy optimization can be done from n onrandom offline data. 4.1 Experiment I The first experiment involves news article recommendation i n the  X  X oday Module X , on the Yahoo! which is defined as the ratio of the true CTR and the CTR of a rand om policy. Our dataset, denoted D Lemma 3.1 implies E [  X  V h of Hoeffding X  X  inequality guarantees that  X  V h dataset to calculate  X  V data are used instead.
 To obtain non-random log data, we ran the LinUCB algorithm us ing the offline bandit simulation procedure, both from [8], on our random log data D user visits to Yahoo! front page. We used D To define the policy h for evaluation, we used D users, and then h was defined as selecting the article with highest estimated C TR. We then evaluated h on D we split the dataset D more advanced conditional probability estimation techniq ues can be used. Figure 1 plots  X  V h our estimate can become more (downward) biased. For a large r ange of  X  values, our estimates approach, which assumes  X  ( a | x ) = 1 / | A | , gives a very poor estimate of 2 . 4 . often larger than the corresponding moments of its expectat ion [7]. 4.2 Experiment II In the second experiment, we investigate our approach to the warm-start problem. The dataset was provided by Yahoo!, covering a period of one month in 2008. Th e data are comprised of logs of
Figure 1: Accuracy of offline evaluator with varying  X  values.
 a The total number of distinct web pages is approximately 3 . 4 million. input ( x, a ) 4 The particular policy that was optimized, had an argmax form : h ( x ) = argmax A |  X   X  ( a | x ) &gt; 0 } is a set of feasible ads.
 shown on page x or y = 0 otherwise. The regressor f was chosen to approximately minimize squared loss on the training data.
 During the evaluation, we computed the estimator on the test data ( x summarized in the Table 2.
 rescaling by  X  , applying the bound, and then rescaling the results by 1 / X  . Random( x ) = a  X  UNIF( C ( X )) , where UNIF( ) denotes the uniform distribution. not significant, although the reward obviously remains 0 .
 when we decrease  X  from 0 . 05 to 0 . 01 .
 than the set of all ads which might have been displayable at ot her points in time.  X  proaches which choose amongst the the explored space of ads. It would be interesting future work to compare our approach to a full-fledged production online a dvertising system. search results, news stories, etc...) to users.
 RL agent to learn from external observations of other agents .

