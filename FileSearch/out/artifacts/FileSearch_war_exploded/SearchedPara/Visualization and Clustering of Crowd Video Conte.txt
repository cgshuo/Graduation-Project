 This paper presents a novel approach for the visualization and clustering of crowd video contents by using multilin-ear principal component analysis (MPCA). In contrast to feature-point-based approach and frame-based dimension-ality reduction approach, the proposed method maps each short video segment to a point in MPCA subspace to take temporal information into account naturally through tenso-rial representations. Specifically, MPCA projects each short segment of a video to a low-dimensional tensor first. A few MPCA features are then selected according to the variance captured as the final representation. Thus, a video is vi-sualized as a trajectory in MPCA subspace. The trajec-tory generated enables visual interpretation of video con-tent in a compact space as well as visual clustering of video events. The proposed method is evaluated on the PETS 2009 datasets through comparison with three existing meth-ods for video visualization. The MPCA visualization shows superior performance in clustering segments of the same event as well as identifying the transitions between events. G.3 [ Probability and Statistics ]: Statistical computing; I.2.10 [ Vision and Scene Understanding ]: Video analy-sis Algorithms,Experimentation, Human Factors, Management Visualization, clustering, crowd, video analysis, MPCA
Recent proliferation of surveillance cameras has led to a strong demand for automatic data processing tools for the enormous amount of video data generated in applications such as surveillance, healthcare, and ambient intelligence [8, 10, 4]. While earlier works focused on videos with single or just a few subjects [8, 7], the analysis of crowd video content starts to attract more and more attentions recently [4].
A popular approach to video analysis is to detect and track a set of feature points [7], which has been quite successful for videos with single or just a few subjects. However, this approach is sensitive to noise and the performance is heavily dependent on the detection and tracking modules. Further-more, when there is a crowd in the scene, it will be extremely difficult to detect and track individuals. On the other hand, it might not be necessary to know the movement of each in-dividual, instead, a characterization of the crowd movement as a whole would provide insights on the scene.

To avoid object detection and tracking, recent approach considers frames of a video as a set of images with each frame (image) as a basic video element. In [8], video is considered as a collection of unordered images and an image space is defined through Isomap [9]. Thus, video sequences specify a trajectory through that image space. Similarly in [10], input frames are represented in a low-dimensional space using Laplacian Eigenmaps [1], where a graph is defined based on similarity of frames. Since the video sequences used in [10] consist of video events well separated by the so-called no event, a rule-based temporal graph is further introduced to incorporate temporal information.

Instead of embedding the raw frames, another approach is to extract motion patterns through the calculation of op-tical flow. In [11], a video sequence is divided into video segments (or clips) and then each segment is split into sev-eral cuboids. A  X  X ideo word X  representation is obtained for each video segment by concatenating histograms of optical flow fields for all the cuboids in the segment. Diffusion map [3] is employed to embed the motion pattern information.
In this work, we consider a short video segment as a basic video element represented as a tensor (a multidimensional array) and propose to use multilinear principal component analysis (MPCA) [5] to embed each video segment into more compact manifolds for analysis. The proposed approach is less sensitive to noise and it operates on raw video sequences, without background subtraction, foreground segmentation, or silhouette extraction. Video content can be visualized as a trajectory in MPCA subspace and the visual cluster rendering scheme in [2] can further be adopted for inter-pretation. The proposed solution is evaluated on sequences from the Performance Evaluation of Tracking and Surveil-lance (PETS) 2009 [4] against three other methods to show its superior performance. Figure 1: Illustration of the proposed MPCA-based approach for crowd video content analysis.
Figure 1 illustrates the proposed MPCA-based approach for crowd video content analysis. Each segment is repre-sented naturally as a third-order tensor with three modes (row, column and time). Input video segments are first mapped to low-dimensional tensors by MPCA. Then, a few MPCA features are selected to obtain low-dimensional vec-tors for final representation. Since we treat a video seg-ment as a basic video element, we analyze the input video in overlapping segments by using an observation window of L frames. This window is shifted by S frames in each step. Thus, for a video with H frames in total, the number of overlapping segments will be where b X c denotes the floor operation.
In this paper, vectors are denoted by lowercase boldface letters, e.g., x ; matrices by uppercase boldface, e.g., U ; and tensors by calligraphic letters, e.g., A . Their elements are denoted with indices in brackets. Indices are denoted by lowercase letters and span the range from 1 to the uppercase letter of the index, e.g., n = 1 ,...,N . An N th -order tensor is n = 1 ,...,N , and each i n addresses the n -mode of A . The n -mode product of a tensor A by a matrix U  X  R J n denoted by A X  n U , is a tensor with entries [5]: A rank-1 tensor A equals to the outer product of N vectors: which means that A ( i 1 ,...,i N ) = u (1) ( i 1 )  X  ...  X  u
MPCA [5] is a multilinear subspace learning method that extracts features directly from tensorial representation of multi-dimensional objects. In [5, 6], MPCA is proposed for gait recognition by representing each half cycle of gait sil-houette sequences as a third-order tensor. In this paper, we apply MPCA to crowd video analysis by extracting MPCA features from raw video sequences.

As described in the beginning of this section, M overlap-ping video segments are obtained first from the video to be analyzed. They are represented as M third-order tensors {X 1 , ..., X M  X  R I 1  X  I 2  X  I 3 } ( I 3 = L ), as the input to MPCA. The MPCA algorithm solves for a multilinear projection where P n &lt; I n for n = 1 , 2 , 3, that maps the original video sor subspace R P 1 N R P 2 N R P 3 : such that the total tensor scatter  X  Y = P M m =1 kY m  X   X  is maximized, where  X  Y = 1 M P M m =1 Y m is the projection of the mean sample (i.e., the average of the M tensorial sam-ples). This MPCA problem is solved through an iterative alternating projection method in [5].

The MPCA projection matrices {  X  U ( n ) ,n = 1 , 2 , 3 } can be viewed as Q 3 n =1 P n EigenTensors [5]: to perform unsupervised analysis on crowd video content, in particular visualization, only the first a few most important EigenTensors are needed. Therefore, we further perform a feature selection based on an importance score  X  calculated from the variation captured in each EigenTensor.  X  where Y m is the projection of X m in MPCA subspace, and  X  Y is the mean feature tensor defined above.

For the EigenTensor selection, the entries in Y m are ar-ranged into a feature vector y m according to  X  p 1 p 2 p scending order. Only the first D entries of y m are kept for subsequent analysis.
The selected D features allow us to embed each video seg-ment to a D -dimensional MPCA subspace, resulting a tra-jectory for the video. Since the raw video segments are pro-jected, the relative positions between data points in MPCA subspace will be closely related to the visual changes be-tween the corresponding video segments. Small changes will result in closely spaced points while abrupt changes will lead to points far apart. In the case of crowd video content, the spacing between points in MPCA subspace tends to be af-fected mainly by two factors: crowd size and activity speed. Generally, small crowd with slow activity results in very closely spaced data points while big crowd with fast activity results in large spacing between data points. Therefore, we expect the MPCA subspace embedding to provide a good visualization of the video content. Two key benefits of the proposed approach are summarized below: 1. The raw video segments are taken as the input so there 2. Through representing video segments in their inher-
Benefiting from human cognitive abilities, visualization of patterns is a practical way to gain insight into large databases [2]. As MPCA-based visualization can provide us cues on the characteristics of the video content in a compact form, human subjects can perform various tasks by observing and interpreting the visualization of a video rather than the video itself, such as anomaly detection, video summarization and clustering of video events. In the next section, experi-ments on real-world crowd video sequences are carried out to demonstrate the superiority of the MPCA-based approach in visualization of crowd video content for summarization and visual clustering of events.
In this section, several experiments on real-world sequences from surveillance cameras are performed to show the effec-tiveness of our proposed method in video content visualiza-tion and event clustering. Different from commercial movies, video sequences from surveillance cameras usually contain a smooth change of motion in the frames rather than frequent scene cuts and rapid changes [10]. Therefore, we expect a smooth manifold of motion over time for these sequences. The proposed method is evaluated on the dataset S3  X  X vent Recognition X  of the PETS 2009 database 1 with dense crowd and subjective difficulty of L3 (the most difficult) [4]. There are four sequences with time stamps 14-16, 14-27, 14-31 and 14-33. Due to space limitation, only results on sequence 14-27 from view 001 are reported in this paper. There are 334 frames in this sequence ( H = 334). In simulations, the original sequence is down-sampled for lower computational cost. The original color sequences are converted to gray-level and each frame is resized to 192  X  144 pixels. The sequences are recorded at 7 frames per second. Figure 2(d) shows four frames from this sequence. For performance eval-uation, we compare the visualization of sequence Time1427 from PETS 2009 in a two-dimensional (2D) space using four methods with the following settings.

Isomap-based method [8] and Laplacian-Eigenmap-based method [10]: the distances between each frame pair of a sequence are calculated and for each frame, the distance for the k nearest neighbors are kept. Four values of k = 4 , 6 , 8 , 10 are tested and the best results are reported.
Diffusion-map-based method [11]: a video is divided into overlapping segments, each with L = 5 frames. Two successive segments are overlapped by 4 frames ( S = 1) so that almost every frame will have a corresponding segment for easy comparison with the first two methods. 8  X  8  X  5 cuboids are obtained for each segment and 4-bin histograms of optical flow are computed for each cuboid, which are con-catenated to form  X  X ideo words X . The frequency of each video word in different segments is normalized to obtain probability, with 10% of the video words with the highest and lowest conditional entropy are discarded. Four sets of diffusion map parameters ( t, X  ) are tested: (2 , 8), (2 , 10), (4 , 6) and (8 , 5). The best results are reported.
The proposed MPCA-based method : overlapping seg-ments are first obtained with L = 5 and S = 1 as in the http : // www . cvg . rdg . ac . uk / PETS2009 / a . html Diffusion-map-based method for easy comparison. They are then embedded in a 2D MPCA subspace ( D = 2) following the process described in Section 2 with the same setting for MPCA in [5].
The visualizations obtained from the four methods enable visual cluster rendering of the video sequences [2]. For the convenience of evaluation, we use different colors to code different events. The color code for each event is depicted in Fig. 2(a). The labeling of events is produced by human subjects. The visualizations of the Time1427 sequence are reported in Figs. 2(b), 2(c), 2(e) and 2(f), each showing the 2D embedding by a method with each point corresponding to a frame or a video segment. There is a scene change in the sequence so Isomap-based method and Laplacian-Eigenmap-based method detect two separately connected components. Thus, the respective visualizations are shown separately for each connected component detected for fair comparison. For each visualization, a red asterisk marks the beginning of the trajectory and a red pentagram marks the end of the trajec-tory. For evaluation, we examine whether the visualizations provide clear clues about the events in the sequences and whether we can visually identify the start and end of the events as well as the transitions between them from the vi-sualizations.

There are six events (with two transitions) for the se-quence Time1427, with a scene change in the middle. The Isomap-based method performs poorly for most of the tra-jectory, mapping points of the same event to points far apart, except for events 3 (green) and 6 (red). The Laplacian-Eigenmap-based method is able to map two different crowd patterns to different parts of the trajectory in Fig. 2(c). However, the trajectory does not truly reflect the actual characteristics of the event. E.g., the event crowd pattern 1 (blue) consists of small local movement of the same pat-tern, but it is mapped to a long trajectory in Fig. 2(c). The Diffusion-map-based method fails in visualizing this se-quence, with points of different events heavily mixed. The MPCA-based method has visualized this sequence particu-larly well. Events 1 (blue), 3 (green), 4 (yellow) and 6 (red) all have fixed crowd patterns and their MPCA visualiza-tions form small clusters in Fig. 2(f), except a few green points due to the scene change. The transition (cyan) be-tween patterns 1 (blue) and 2 (green) is more significant and it is clearly observed in Fig. 2(f). The transition (orange) between patterns 3 (yellow) and 4 (red) is more subtle so it is less distinguishable from pattern 4 (red) in Fig. 2(f).
In summary, the Isomap visualization tends to be noisy while the Laplacian Eigenmap visualization produces smooth curves for events of very different characteristics so it is dif-ficult to interpret events. The Diffusion map visualization cannot provide insights to the corresponding video content at all. In contrast, the proposed MPCA visualization is more meaningful and gives a better interpretation of the video content. In particular, small local movements for a partic-ular crowd pattern nicely form clusters in MPCA subspace. Furthermore, its performance is consistent over the other PETS2009 sequences with various characteristics, which are not reported here due to space constraint. Therefore, the proposed MPCA-based method for video visualization and clustering provides a powerful tool to video analysis.
Analysis of crowd video content is becoming an important topic, where detection and tracking of individual subjects have become extremely difficult due to the large number of subjects in the scene. In this paper, we use MPCA, a recent multilinear statistical method, to analyze crowd activities and events, with no need for object detection and tracking. We consider a video segment as a basic video element and map it to a point in MPCA subspace. The MPCA visualiza-tion characterizes the entire video sequence with an abstract description of the events and it provides a valuable tool in analyzing video content for video summarization, anomaly detection, and behavior understanding. In particular, the vi-sualization enables visual clustering of events. Experiments show that the proposed MPCA-based method gives much better visualization of challenging PETS 2009 crowd video sequences and produces more visible clusters of events than three existing methods. [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for [2] K. Chen and L. Liu. Clustermap: labeling clusters in [3] R. R. Coifman and S. Lafon. Diffusion maps. Applied [4] J. Ferryman and A. Shahrokni. Pets2009: Dataset and [5] H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos. [6] H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos. [7] J. C. Niebles, H. Wang, and L. Fei-Fei. Unsupervised [8] R. Pless. Image spaces and video trajectories: Using [9] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A [10] I. Tziakos, A. Cavallaro, and L.-Q. Xu. Video event [11] Y. Yang, J. Liu, and M. Shah. Video scene
