 The goal of computer tailored health communications (CTHC) is to elicit healthy behavior changes by sending motivational messages personalized to individual patients. One promi-nent weakness of many existing CTHC systems is that they are based on expert-written rules and thus have no ability to learn from their users over time. One solution to this problem is to develop CTHC systems based on the princi-ples of collaborative filtering, but this approach has not been widely studied. In this paper, we present a case study eval-uating nine rating prediction methods for use in the Patient Experience Recommender System for Persuasive Communi-cation Tailoring , a system developed for use in a clinical trial of CTHC-based smoking cessation support interventions. H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval Recommender systems; tailored health communications
Computer tailored health communications (CTHC) sys-tems aim to elicit healthy behavior changes by sending moti-vational messages personalized to individual patients. Cur-rent CTHC systems start by creating a pool of messages and a fixed set of theory-driven, expert-written rules that relate the characteristics of messages to the characteristics of the patients [4]. When a new patient is enrolled in the system, a baseline patient profile is collected that includes demographic information and assessments of the relevant tailoring characteristics. Messages are then selected for the patient on the basis of the patient X  X  characteristics and the expert-derived message selection rules.

CTHC systems have been deployed to address a range of high-profile health problems including supporting smoking cessation [12]. One prominent weakness of many existing CTHC systems is that they do not solicit feedback from their users and thus have no ability to learn over time. The fixed rules these systems are based on may fail to account for socio-cultural concepts that have intrinsic importance to the targeted population, thus limiting their success [1].
One solution to this problem is to develop CTHC sys-tems based on the principles of collaborative filtering. In electronic commerce settings, collaborative filtering recom-mender systems have been successfully used to derive per-sonalized recommendations for items like books or movies based on implicit or explicit feedback about the items col-lected from an online community of users. The promise of collaborative filtering-based recommendation in the CTHC setting derives from its ability to learn associations between types of patients and types of messages directly from data, allowing the CTHC system to adapt to individual users over time [9].

Collaborative filtering in the CTHC setting presents a number of challenges. Our prior work has addressed the problem of defining semantics for explicit feedback ratings in the CTHC setting as a precursor to developing the Pa-tient Experience Recommender System for Persuasive Com-munication Tailoring (PERSPeCT) [5]. The PERSPeCT system will use collaborative filtering-based rating predic-tion to drive the personalized selection of smoking cessation support messages that are currently in use in a deployed ex-pert system operated through our online smoking cessation support portal, decide2quit.org.

The primary challenge faced by PERSPeCT is the need to construct a system with maximal accuracy in the small data regime. In this case study, we present an empirical evaluation of nine classical and state of the art rating pre-diction methods applied to the novel problem of predicting smoking cessation support message ratings. We begin by reviewing the problem of rating semantics in the CTHC set-ting. We then present the details of a new data set collected to bootstrap collaborative filtering model learning and se-lection for PERSPeCT and provide a brief overview of the models evaluated in our study. Finally, we describe our re-sults, which underscore the importance of Bayesian methods in small data regimes and surprisingly show that the inclu-sion of patient and message features has no marginal benefit in addition to the available rating information.
To date, two studies have been performed to guide the design of PERSPeCT. The first study was designed to in-form the choice of rating semantics, which was a signifi-cant open question resulting from the differing goals of e-commerce recommender systems (recommending items that users will like) and CTHC systems (selecting messages to promote healthy behavior changes for individual patients).
To address this issue, four candidate factors including how much the message influenced the user not to smoke, how much the message effected the subject emotionally, how rel-evant the message was to the subject, and how much the user liked the message were identified. To assess the differ-ences between these factors, we collected a novel multi-factor rating data set consisting of ratings for all four message fac-tors collected from 100 study participants. Each participant provided ratings on a 5-point scale for each of five unique messages drawn randomly from a 50 message subset of all messages available on decide2quit.org. The results showed that the ratings for each of the four factors were highly cor-related, rendering the collection of all four ratings for each message unnecessary. These results are presented in detail in our prior work [5], which also includes ratings analysis and a small-scale rating prediction feasibility study.
Following the completion of the rating semantics study, a second study was performed to collect a larger rating data set to bootstrap the learning and evaluation of collaborative filtering models for PERSPeCT. We collected ratings of both influence and relevance from a total of 846 users with respect to all 261 actively used messages from decide2quit.org. Each study participant was asked to rate 20 messages. This re-sulted in a total of 16920 ratings, yielding a data density of 7.7%. Subjects also supplied demographic data and values for a variety of tailoring variables related to smoking be-havior including number of cigarettes smoked per day and readiness to quit. Study subjects were recruited through a mix of local and online sources and received a $50 incentive.
The marginal distribution of influence ratings for this data set is shown in Figure 1. In the remainder of this paper, we present the first empirical analysis of influence rating prediction using the data collected in this second study.
Baselines: Our evaluation includes a number of com-mon baselines including predicting the global mean rating (Global Mean) and the item mean rating (Item Mean). The item mean model includes smoothing hyperparameters that pull the means toward the global mean. We also include a simple additive model that learns an optimal additive com-bination of ` 2 regularized user and item bias terms.
KNN: User and item-based K-Nearest Neighbors are clas-sical models in collaborative filtering. A user-based KNN model works by predicting user u  X  X  rating for item i as a weighted sum of other users X  ratings of i , where the weights are a similarity measure. Following [2], we used Pearson Correlation between ratings on co-rated items, adjusted to account for the number of co-rated items. KNN models have two hyperparameters: the neighborhood size, K , and the weight for pairs with few shared ratings.

PMF: Probabilistic Matrix Factorization represents each user and item as a K -dimensional vector with a multivari-ate normal prior. User u  X  X  rating for item i is then a normal random variable with mean equal to the inner product of u  X  X  and i  X  X  factors. The user and item factors can be learned using standard numerical optimization techniques [8]. Ad-ditionally, global, user, and item biases can be added to the model with little additional complexity. The hyperparame-ters in PMF include the ` 2 regularization parameters for the user/item biases and factor matrices, as well as the dimen-sionality of the model K .

BPMF: Bayesian Probabilistic Matrix Factorization ex-tends PMF by adding Normal-Wishart hyper-priors to the mean and covariance matrices of the user and item factors and replaces learning the primary parameters with optimiza-tion by full Bayesian inference using Gibbs sampling [7].The hyperparameters in BPMF include the parameters for the Normal-Wishart hyper-priors, the rating noise variance, and the dimensionality of the factors.

PMF-F/BPMF-F: A simple way to incorporate user and item side-information into both of the previous models is to add linear regression terms to the models. In PMF, the feature weights can be ` 2 regularized and estimated along with all of the other parameters. In BPMF, the individual feature weights are given normal priors with zero mean and tunable variance and added to the Gibbs sampling routine.
CMF: Collective Matrix Factorization jointly factorizes the rating matrix and a related feature matrix into a set of user, item, and feature factors [10]. In our case, the item factors are shared between the two factorizations and allow information to flow between the feature and ratings matri-ces. A hyperparameter,  X  , balances the importance of the reconstruction error on the ratings against the reconstruc-tion error on the features. Parameters in this model are learned via numerical optimization as in PMF. The hyper-parameters in CMF are the same as those in PMF plus  X  and an ` 2 regularization parameter for the feature factors. BCMF: Bayesian Collective Matrix Factorization extends CMF in the same way that BPMF extends PMF, with Normal-Wishart hyper-priors on the user, item, and feature hyper-parameters [11]. The same distributions as in BPMF are in-volved, so we used a similar Gibbs sampler to run inference in this model. We used the same Normal-Wishart parame-ters as in BPMF and the the only additional hyperparameter is a noise variance parameter for the features.

HFT: In addition to the user demographics and theory based message codings, the text of the messages themselves can be used as features. Similar to CMF, the recently in-troduced Hidden Factors as Topics model incorporates text by jointly fitting a PMF model to the ratings data and a latent Dirichlet allocation (LDA) model to the message text where a softmax transformation of the item factor is used as the distribution over topics [6]. The model is trained by alternating numerical optimization of the user/item factors and topic specific distributions over words and sampling the topic assignments for individual words. The only additional hyperparameter is similar to  X  in CMF and balances the importance of the LDA portion of the model.
The fielded PERSPeCT system will select messages for users based on their predicted influence ratings. One pre-viously unseen message will be selected and sent per day for each user. The system will initially be evaluated on its ability to select messages that are rated higher on average than messages selected by an existing expert system. These design factors are reflected in our evaluation methodology as described below. All methods were implemented by the authors using Python with the exception of HFT, for which we use the originally published code [6].

Empirical Protocol: We use a strong-generalization protocol that evaluates the ability of the system to gener-alize to non-training users, which better matches our appli-cation. This involves completely separating test users from train users, learning a model using all of the train users X  ratings, freezing all non-user-specific parameters, and finally training the user-specific parameters (e.g. user factors and biases in PMF) on a subset of each test user X  X  observed rat-ings. To implement this protocol, we first divided the users randomly into five folds and then generated three random train and validation sets for each test fold. We further di-vided each test user X  X  ratings into five folds. To evaluate each method X  X  performance given varying levels of informa-tion about a test user, we evaluated all methods with 5, 10 and 16 of each test user X  X  ratings available for inference and learning of user-specific parameters. Each test user has a constant set of 4 test ratings per test fold. The validation sets were used to set the hyper-parameters of each method (e.g. K in KNN). Exhaustive grid search was used and the hyper-parameter ranges were iteratively extended to ensure that no selected hyperparameter values occurred at the end-points of the search intervals.

Performance Metrics: In evaluating rating prediction methods, we considered a range of standard performance metrics including root mean squared error (RMSE), Kendal X  X  Tau-b (KTAU-b), and normalized discounted cumulative gain (NDCG) to analyze rating prediction and ranking perfor-mance. We also evaluate some less common metrics includ-ing the fraction of times a test item with the maximal rating is ranked first (Hit Rate), the absolute difference between the maximum test item rating and the rating of the top ranked test item (Top Loss), and the average rating of the top-ranked test items (Avg. Top). We report averages for these metrics over all test folds. Figure 2: Average RMSEs for all models without features using 5, 10, and 16 training examples.
In this section, we present an empirical evaluation of the methods described in the previous section applied to the prediction of the influence ratings collected in our second PERSPeCT study.

Pure Methods: The average test performance for all collaborative filtering models based on 10 test user observa-tions is reported in Table 1 along with standard errors. In terms of RMSE, the best performing pure (Type P) collab-orative filtering method is BPMF, with an RMSE of 1.001. The absolute performance gap between BPMF and the other base methods is small, ranging between 1 and 10 percent improvement. However, the RMSE gap between BPMF and all the other pure methods is statistically significant at the p = 0 . 01 level as determined by a paired t-test with Bon-ferroni correction. Figure 2 shows the performance of each of the pure collaborative filtering methods across 5, 10 and 16 observed ratings per test user. We can see that the per-formance gap is larger with fewer observed ratings, which highlights the superiority of Bayesian models in small data settings. Further investigation showed that BPMF was able to effectively use more factors than PMF. Across all runs, BPMF selected between 4 and 7 factors, while PMF selected only 1 or 2 factors. This highlights the ability of Bayesian methods to learn richer models while avoiding over fitting problems that effect optimization-based approaches in small data settings. A surprising finding is that many methods, including PMF, perform worse or no better than the Biases baseline approach in terms of RMSE. This indicates the diffi-culty of extracting useful structure in the small data setting. In general, the performance gap is much smaller with respect to the ranking metrics; however, BPMF is always either the best model or statistically indistinguishable from the best model.

Ensemble Methods: The Netflix competition demon-strated the success of ensemble methods which aggregate the predictions of a number of distinct models [3]. To evaluate ensemble methods in our context, we considered combing the results of the best neighborhood-based method (user KNN) and the best matrix factorization method (BPMF) using an unweighted average of their predictions. The aver-age RMSE at each training data size is shown in Figure 2. With 10 test user observations, the ensemble method has an average RMSE of 0.986, approximately 3% better than the best pure method, BPMF. This confirms the utility of ensemble methods in the small data regime.
 Hybrid Methods: The models indicated as hybrid (Type H) in Table 1 include features in various ways. The best per-forming of these models in terms of RMSE is BCMF, how-ever, its performance is statistically indistinguishable from BPMF. In fact, PMF with linear features, CMF, BPMF with linear features, and BCMF are all very close in performance to their counterparts that do not include features. Finally, the addition of text features through HFT also results in per-formance that is slightly worse than PMF, the correspond-ing base model. Interestingly, these results indicate that the demographic and behavioral theory-based features, which are currently the basis for assigning messages to users in an existing expert system, do not add any information over rat-ings alone within the state-of-the-art modeling frameworks we have investigated. To further investigate this finding, we evaluated the mutual information between the rating values and each of the patient and message features. The results showed a maximum absolute mutual information value of 0 . 027 for patient features and 0 . 002 for message features. These values are remarkably low and explain why the fea-tures do not result in a significant boost in performance.
In this paper we have described PERSPeCT, a novel col-laborative filtering-based computer tailored health commu-nications (CTHC) system targeted at the smoking cessation support domain. Our primary contribution is an evaluation of a wide range of classic and state-of-the art collaborative filtering methods including pure methods, ensemble meth-ods, and hybrid methods in a new and important small-data domain. Due to the success of Bayesian Probabilistic Matrix Factorization (BPMF) as the best single model identified in our evaluation, a pure BPMF-based instance of PERSPeCT is currently being deployed and evaluated relative to our existing expert system through a randomized control trial to determine its effectiveness in an online message selection context. The online evaluation of ensemble methods is re-served for future work and future versions of PERSPeCT will be evaluated based on patient outcomes.
 This work is supported by the Patient Centered Outcomes Research Institute (1IP2P1000582) and the University of Massachusetts Center for Clinical and Translational Sciences (UL1TR000161). Dr Sadasivam is also funded by a National Cancer Institute Career Development Award (K07CA172677).
