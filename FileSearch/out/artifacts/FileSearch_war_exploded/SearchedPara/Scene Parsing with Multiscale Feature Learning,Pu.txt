 Laurent Najman 2 L . NAJMAN @ ESIEE . FR Scene parsing, or full scene labeling (FSL), is the task of labeling each pixel in a scene with the category of the ob-ject to which it belongs. FSL requires to solve the detec-tion, segmentation, recognition and contextual integration problems simultaneously, so as to produce a globally con-sistent labeling. One of the obstacles to FSL is that the information necessary for the labeling of a given pixel may come from very distant pixels as well as their labels. The category of a pixel may depend on relatively short-range information ( e.g. the presence of a human face generally indicates the presence of a human body nearby), as well as on very long-range dependencies (is this grey pixel part of a road, a building, or a cloud?).
 This paper proposes a new method for FSL, depicted on Figure 1 that relies on five main ingredients: 1) Trainable, dense, multi-scale feature extraction : a multi-scale, dense feature extractor produces a series of feature vectors for regions of multiple sizes centered around every pixel in the image, covering a large context. The feature extractor is a three-stage convolutional network applied to a multi-scale contrast-normalized laplacian pyra-mid computed from the image. The convolutional network is fed with raw pixels and trained end to end, thereby alle-viating the need for hand-engineered features. 2) Segmentation Tree : A graph over pixels is computed in which each pixel is connected to its 4 nearest neighbors through an edge whose weight is a measure of dissimilarity between the colors of the two pixels. A segmentation tree is then constructed using a classical region merging method, based on the minimum spanning tree of the graph. Each node in the tree corresponds to a potential image segment. The final image segmentation will be a judiciously chosen subset of nodes of the tree whose corresponding regions cover the entire image. 3) Region-wise feature aggregation : for each node in the tree, the corresponding image segment is encoded by a 3  X  3 spatial grid of aggregated feature vectors. The ag-gregated feature vector of each grid cell is computed by a component-wise max pooling of the feature vectors cen-tered on all the pixels that fall into the grid cell; This pro-duces a scale-invariant representation of the segment and its surrounding. 4) Class histogram estimation : a classifier is then applied to the aggregated feature grid of each node. The classifier is trained to estimate the histogram of all object categories present in its input segments. 5) Optimal purity cover : a subset of tree nodes is selected whose corresponding segments cover the entire image. The nodes are selected so as to maximize the average purity of the class distribution. We define the class purity as a quan-tity that is inversely proportional to the entropy of the class distribution. The choice of the cover thus attempts to find a consistent overall segmentation in which each segment contains pixels belonging to only one of the learned cate-gories.
 All the steps in the process have a complexity linear (or almost linear) in the number of pixels. The bulk of the computation resides in the convolutional network feature extractor. The resulting system is very fast, producing a full parse of a 320  X  240 image in less than 1 second on a conventional CPU, and in less than 100ms using dedicated hardware, opening the door to real-time applications. Once trained, the system is parameter free, and requires no ad-justment of thresholds or other knobs.
 There are three key contributions in this paper 1) using a multi-scale convolutional net to learn good features for re-gion classification; While using a multiscale representation seems natural for FSL, it has rarely been used in the con-text of feature learning systems. 2) using a class purity criterion to decide if a segment contains a single objet, as opposed to several objects, or part of an object; 3) an effi-cient procedure to obtain a cover that optimizes the overall class purity of a segmentation. The FSL problem has been approached with a wide variety of methods in recent years. Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the con-sistency of the labeling and to account for context (He &amp; Zemel, 2008; Russell et al., 2009; Gould et al., 2009; Ku-mar &amp; Koller, 2010; Munoz et al., 2010; Tighe &amp; Lazeb-nik, 2010; Lempitsky et al., 2011). Most methods rely on a pre-segmentation into super-pixels or other segment can-didates, and extract features and categories from individ-ual segments and from various combinations of neighbor-ing segments. The graphical model inference pulls out the most consistent set of segments which covers the image. Socher et al. (2011) propose a method to aggregate seg-ments in a greedy fashion using a trained scoring function. The originality of the approach is that the feature vector of the combination of two segments is computed from the feature vectors of the individual segments through a train-able function. Like us, they use  X  X eep learning X  methods to train their feature extractor. But unlike us, their feature extractor operates on hand-engineered features.
 One of the main question in scene parsing is how to take a wide context into account to make a local decision. Munoz et al. (2010) proposed to use the histogram of labels ex-tracted from a coarse scale as input to the labeler that looks at finer scales. Our approach is somewhat simpler: our feature extractor is applied densely to an image pyramid. The coarse feature maps thereby generated are upsampled to match that of the finest scale. Hence with three scales, each feature vector has multiple fields which encode multi-ple regions of increasing sizes and decreasing resolutions, centered on the same pixel location.
 Like us, a number of authors have used trees to generate candidate segments by aggregating elementary segments, as in Russell et al. (2009); Lempitsky et al. (2011). These approaches rely on inference algorithms based on Graph Cuts or other methods. In this paper, we use an innova-tive method based on finding a set of tree nodes that covers the image while minimizing a class purity criterion, that al-lows us to label scenes in less that one second whereas the previously mentionned approaches require minutes.
 Our system extracts features densely from a multiscale pyramid of images using a convolutional network (Conv-Net) (LeCun et al., 1998a). ConvNets can be fed with raw pixels and can automatically learn low-level and mid-level features, alleviating the need for hand-engineered features. One big advantage of ConvNets is the ability to compute dense features efficiently over large images. ConvNets are best known for their applications to detection and recog-nition (Osadchy et al., 2007; Jarrett et al., 2009), but they have also been used for image segmentation, particularly for biological image segmentation (Ning et al., 2005; Jain et al., 2007; Turaga et al., 2009).
 The only published work on using ConvNets for scene parsing is that of Grangier et al. (2009). While some-what preliminary, their work showed that convolutional networks fed with raw pixels could be trained to perform scene parsing with decent accuracy. Unlike Grangier et al. (2009) however, our system uses a boundary-based hierar-chy of segmentations to align the labels produced by the ConvNet to the boundaries in the image and thus produces representations that are independent of the size of the seg-ments through feature pooling. The model proposed in this paper, depicted on Figure 1, re-lies on two complementary image representations. In the first representation, an image patch of size P is seen as a point in R P , and we seek to find a transform f : R P  X  R that maps each patch into R Q , a space where it can be clas-sified linearly. This first representation typically suffers from two main problems when using a classical ConvNet, where the image is divided following a grid pattern: (1) the window considered rarely contains an object that is prop-erly centered and scaled, and therefore offers a poor obser-vation basis to predict the class of the underlying object, (2) integrating a large context involves increasing the grid size, and therefore the dimensionality P of the input; given a finite amount of training data, it is then necessary to en-force some invariance in the function f itself. This is usu-ally achieved by using pooling/subsampling layers, which in turn degrades the ability of the model to precisely locate and delineate objects. In this paper, f is implemented by a multiscale convolutional network, which allows integrating large contexts (as large as the complete scene) into local decisions, yet still remaining manageable in terms of pa-rameters/dimensionality. This multiscale model, in which weights are shared across scales, allows the model to cap-ture long-range interactions, without the penalty of extra parameters to train. This model is described in Section 3.1. In the second representation, the image is seen as an edge-weighted graph, on which a hierarchy of segmenta-tions/clusterings can be constructed. This representation yields a natural abstraction of the original pixel grid, and provides a hierarchy of observation levels for all the ob-jects in the image. It can be used as a solution to the first problem exposed above: assuming the capability of assess-ing the quality of all the components of this hierarchy, a system can automatically choose its components so as to produce the best set of predictions. Moreover, these com-ponents are spatially accurate, and naturally delineate the underlying objects, as this representation conserves pixel-level precision. Section 3.2 describes our methodology. 3.1. Scale-invariant, scene-level feature extraction The feature extractor is a three-stage ConvNet. Each of the first two stages contains a bank of filters producing multi-ple feature maps, a point-wise non-linear mapping, and a spatial pooling and subsampling of each feature map; the third stage only contains a bank of filters and a point-wise non-linear mapping. The filters (convolution kernels) are subject to training. Each filter is applied to the input fea-ture maps through a 2D convolution operation, which de-tects local features at all locations on the input. Each filter bank of a ConvNet produces features that are equivariant under shifts, i.e. if the input is shifted, the output is also shifted but otherwise unchanged.
 While ConvNets have been successfully applied to a num-ber of image labeling problems, image-level tasks such as full-scene understanding (pixel-wise labeling, or any dense feature estimation) require the system to model complex interactions at the scale of complete images, not simply within a patch. To view a large contextual window at full resolution, a ConvNet would have to be unmanageably large.
 The solution is to use a multiscale approach. Our multi-scale convolutional network overcomes these limitations by extending the concept of spatial weight replication to the scale space. Given an input image I , a multiscale pyramid of images X s ,  X  s  X  X  1 ,...,N } is constructed, where X has the size of I . The multiscale pyramid can be a Lapla-cian pyramid, and is typically pre-processed, so that local neighborhoods have zero mean and unit standard deviation. Given a classical convolutional network f s with parame-ters  X  s , the multiscale network is obtained by instantiating one network per scale s , and sharing all parameters across scales:  X  s =  X  1 ,  X  s  X  X  1 ,...,N } .
 The maps in the pyramid are computed using a scal-ing/normalizing function g s as X s = g s ( I ) , for all s  X  { 1 ,...,N } .
 For each scale s , the convolutional network f s be described as a sequence of linear transforms, inter-spersed with non-linear symmetric squashing units (typi-cally the tanh function (LeCun et al., 1998b)), and pool-ing/subsampling operators. For a network f s with L lay-ers, we have: f s ( X s ;  X  s ) = W L H L  X  1 , where the vector of hidden units at layer l is H l = pool(tanh( W l H l  X  1 + b for all l  X  { 1 ,...,L  X  1 } , with b l a vector of bias pa-rameters, and H 0 = X s . The matrices W l are Toeplitz matrices, therefore each hidden unit vector H l can be ex-pressed as a regular convolution between kernels from W l and the previous hidden unit vector H l  X  1 , squashed through a tanh , and pooled spatially. More specifically, H The filters W l and the biases b l constitute the trainable parameters of our model, and are collectively denoted  X  s The function tanh is a point-wise non-linearity, while pool is a function that considers a neighborhood of activations, and produces one activation per neighborhood. In all our experiments, we use a max-pooling operator, which takes the maximum activation within the neighborhood. Pooling over a small neighborhood provides built-in invariance to small translations.
 Finally, the outputs of the N networks are upsampled and concatenated so as to produce F , a map of feature vec-tors of size N times the size of F 1 , which can be seen as local patch descriptors and scene-level descriptors: F = [ F 1 ,u ( F 2 ) ,...,u ( F N )] , where u is an upsampling func-tion.
 As mentioned above, weights are shared between networks f . Intuitively, imposing complete weight sharing across scales is a natural way of forcing the network to learn scale invariant features, and at the same time reduce the chances of over-fitting. The more scales used to jointly train the models f s (  X  s ) the better the representation becomes for all scales. Because image content is, in principle, scale invari-ant, using the same function to extract features at each scale is justified. In fact, we observed a performance decrease when removing the weight-sharing. 3.2. Parameter-free hierarchical parsing Predicting the class of a given pixel from its own feature vector is difficult, and not sufficient in practice. The task is easier if we consider a spatial grouping of feature vectors around the pixel, i.e. a neighborhood. Among all possible neighborhoods, one is the most suited to predict the pixel X  X  class. In Section 3.2.1 we formulate the search for the most adapted neighborhood as an optimization problem. The construction of the cost function that is minimized is then described in Section 3.2.2. We define the neighborhood of a pixel as a connected com-ponent that contains this pixel. Let C k ,  X  k  X  { 1 ,...,K } be the set of all possible connected components of the lat-tice defined on image I , and let S k be a cost associated to each of these components. For each pixel i , we wish to find the index k  X  ( i ) of the component that best explains the class of the pixel, that is, the component with the minimal Note that components C k  X  ( i ) are non-disjoint sets that form a cover of the pixels (lattice). Note also that the overall cost S  X  = P In practice, the set of components C k is too large, and only a subset of it can be considered. A classical technique to reduce the set of components is to consider a hierarchy of segmentations (Najman &amp; Schmitt, 1996; Arbel X ez et al., 2011), that can be represented as a tree T . Solving Eq 1 on T can be done simply by exploring the tree in a depth-first search manner, and finding the component with minimal weight along each branch. Figure 2 illustrates the proce-dure. Given a set of components C k , we explain how to produce all the confidence costs S k . These costs represent the class purity of the associated components. Given the groundtruth segmentation, we can compute the cost as being the entropy of the distribution of classes present in the component. At test time, when no groundtruth is available, we need to de-fine a function that can predict this cost by simply looking at the component. We now describe a way of achieving this, as illustrated in Figure 3.
 Given the feature vectors in F , we define a compact repre-sentation to describe objects as an adaptive spatial arrange-ment of such features. In other terms, an object, or cate-gory in general, can be best described as a spatial arrange-ment of features, or parts. We define a simple attention function a used to mask the feature vector map with each component C k , producing a set of K masked feature vector patterns { F T C k } ,  X  k  X  { 1 ,...,K } . The function a is called an attention function because it suppresses the back-ground around the component being analyzed. The patterns { F T C k } are resampled to produce fixed-size representa-tions. In our model the sampling is done using an adaptive max-pooling function, which remaps input patterns of ar-bitrary size into a fixed G  X  G grid. This grid can be seen as a highly invariant representation that encodes spatial re-lations between an object X  X  attributes/parts. This represen-tation is denoted O k . Some nice properties of this encod-ing are: (1) elongated, or in general ill-shaped objects, are nicely handled, (2) the dominant features are used to repre-sent the object, combined with background subtraction, the features pooled represent solid basis functions to recognize the underlying object.
 Once we have the set of object descriptors O k , we define a classifier function c : O k  X  [0 , 1] N c (where N c is the number of classes) as predicting the distribution of classes present in component C k . We associate a cost S k to this distribution. In this paper, c is implemented as a simple 2-layer neural network, and S k is the entropy of the predicted distribution. More formally, let x k be the feature vector associated with component C k ,  X  d k the predicted class dis-tribution, and S k the cost associated to this distribution. We have Matrices W 1 and W 2 are noted  X  c , and represent the train-able parameters of c . These parameters need to be learned over the complete set of hierarchies, computed on the en-tire training set available. The exact training procedure is described in Section 4. Let F be the set of all feature maps in the training set, and T the set of all hierarchies. Training the model described in Section 3 can be done in two steps. First, we train the low-level feature extractor f in complete independence of the rest of the model. The goal of that first step is to pro-duce features ( F ) F  X  X  that are maximally discriminative for pixelwise classification. Next, we construct the hierar-chies ( T ) T  X  X  on the entire training set, and, for all T  X  X  train the classifier c to predict the distribution of classes in component C k  X  T , as well as the costs S k . Once this sec-ond part is done, all the functions in Figure 1 are defined, and inference can be performed on arbitrary images. In the next two sections we describe these two steps. 4.1. Learning discriminative scale-invariant features As described in Section 3.1, feature vectors in F are ob-tained by concatenating the outputs of multiple networks f , each taking as input a different image in a multiscale pyramid. Ideally a linear classifier should produce the cor-rect categorization for all pixel locations i , from the feature vectors F i . We train the parameters  X  s to achieve this goal, using the multiclass cross entropy loss function. Let  X  c the normalized prediction vector from the linear classifier for pixel i . We compute normalized predicted probability distributions over classes  X  c i,a using the softmax function, i.e. where w is a temporary weight matrix only used to learn the features. The cross entropy between the predicted class distribution  X  c and the target class distribution c penalizes their deviation and is measured by The true target probability c i,a of class a to be present at location i can either be a distribution of classes at location i , in a given neighborhood or a hard target vector: c i,a if pixel i is labeled a , and 0 otherwise. For training max-imally discriminative features, we use hard target vectors in this first stage. Once the parameters  X  s are trained, we discard the classifier in Eq 5. 4.2. Teaching a classifier to find its best observation Given the trained parameters  X  s , we build F and T , i.e. we compute all vector maps F and hierarchies T on all the training data available, so as to produce a new training set of descriptors O k . This time, the parameters  X  c of the clas-sifier c are trained to minimize the KL-divergence between the true (known) distributions of labels d k in each compo-nent, and the prediction from the classifier  X  d k (Eq 3): In this setting, the groundtruth distributions d k are not hard target vectors, but normalized histograms of the la-bels present in component C k . Once the parameters  X  c are Tighe &amp; Lazebnik (2010) 77.5% / -10 to 300s Kumar &amp; Koller (2010) 79.4% / -&lt; 600s
Lempitsky et al. (2011) 81.9% / 72.4% 1 &gt; 60s trained,  X  d k accurately predicts the distribution of labels, and Eq 4 is used to assign a purity cost to the component. Note that the parameters of the feature extractor are not updated while training this classifier. This has been tried, and, interestingly, gave results that were slightly worse. We believe this is caused by the fact that learning all the param-eters of the complete system jointly is more prone to over-fitting. Moreover, the feature extractor and the classifier are trained on the same data, which can also be suboptimal in terms of generalization, as mentioned in (Munoz et al., 2010). We report our semantic scene understanding results on three different datasets:  X  X tanford Background X  on which related state-of-the-art methods report classification errors, and two more challenging datasets with a larger number of classes:  X  X IFT Flow X  and  X  X arcelona X . The Stanford Background dataset Gould et al. (2009) contains 715 im-ages of outdoor scenes composed of 8 classes, where each image contains at least one foreground object. We use the evaluation procedure introduced in Gould et al. (2009), 5 -fold cross validation: 572 images used for training, and 143 for testing. The SIFT Flow dataset Liu et al. (2009) is com-posed of 2 , 688 images, 2 , 488 training images and 200 test images containing 33 semantic labels. The Barcelona dataset, as described in Tighe &amp; Lazebnik (2010), is de-rived from the LabelMe subset and contains 170 unique labels. It has 14 , 871 training and 279 test images. The test set consists of street scenes from Barcelona, while the training set ranges in scene types but has no street scenes from Barcelona.
 For all experiments, we use a 3 -stage convolutional net-work. Each layer of the network is composed of a bank of filters of size 7  X  7 followed by tanh units and 2  X  2 max-pooling operations. The input image I is transformed into a 16 -dimension feature map, using a bank of 16 fil-ters, the second layer transforms the 16 -dimension fea-ture map into a 64 -dimension feature map, each compo-nent being produced by a combination of 8 filters, finally the 64 -dimension feature map is transformed into a 256 -dimension feature map, using a combination of 16 filters. The network is applied to a locally normalized Laplacian pyramid constructed on the input image. For these exper-iments, the pyramid consists of 3 rescaled versions of the input. All inputs are properly padded, and outputs of each of the 3 networks upsampled and concatenated, so as to produce a 256  X  3 = 768 -dimension feature vector map F . The field-of-view of the network at each scale is 46  X  46 , such that the complete quarter-resolution field-of-view is 184  X  184 . The network is trained on all 3 scales in parallel. Simple grid-search was performed to find the best learning rate and regularization parameters , using a holdout of 10% of the training dataset for validation. To ensure that the features do not overfit some irrelevant biases present in the data, jitter  X  horizontal flipping of all images, and rotations between  X  8 and 8 degrees  X  was used to artificially expand the size of the training data.
 The segmentation tree used to find the optimal cover is computed from a 4-connexity graph built on the raw (un-normalized) RGB pixels. The edge weights are set to the Euclidean distance between two pixels. A minimum span-ning tree is then computed, and the dendrogram of that spanning tree is our segmentation tree. Each edge in that tree is the weakest edge between the two components. Fi-nally, the tree is filtered according to a morphologic vol-umetric criterion (Meyer &amp; Najman, 2010; Cousty &amp; Na-jman, 2011), completed by a removal of non-informative small components (less than 100 pixels). These two opera-tions help produce larger, more stable components. Classically segmentation methods find a partition of the segments rather than a cover. Partitioning the segments consists in finding an optimal cut in the tree (so that each terminal node in the pruned tree corresponds to a segment). We experimented with a number of graph based methods to do so, including graph-cuts (Ford &amp; Fulkerson, 1955; Boykov &amp; Jolly, 2001), but the results were less accurate than with our optimal cover method.
 The 2  X  layer neural network c (Eq 2) has 3  X  3  X  3  X  256 in-put units (using a 3  X  3 grid of feature vectors from F ), 512 hidden units; and the size of the output unit corresponds to the number of different classes in the dataset.
 To evaluate the gain of each specific components of our pu-rity tree approach, we report on the Stanford dataset three experiments: a system based on a convolutional network alone; the multiscale convolutional network, and the full model as described in Section 3. Results are reported in Table 1, and compared with related works. Our model achieves very good results in comparison with previous approaches. Methods of (Kumar &amp; Koller, 2010; Lempit-sky et al., 2011) achieve similar or better performances on this particular dataset but to the price of several minutes to parse one image. We demonstrate that our system scales nicely when augmenting the number of classes on two other datasets, in Tables 2 and 3. Example parses on the SIFT Flow dataset are shown on Figure 4.
 Network and multiscale network: for our baseline, we trained a single-scale network and a three-scale network as raw site predictors, for each location i , using the classi-fication loss L cat defined in Eq 6. Table 1 shows the clear advantage of the multi-scale representation, which captures scene-level dependencies, and can classify more pixels ac-curately. Without an explicit segmentation model, the vi-sual aspect of the predictions still suffers from poor spatial consistency, and poor object delineation.
 Complete system, network and hierarchy: in this exper-iment, we use the complete model, as described in Section 3. Results are significantly better than the baseline method, in particular, much better delineation is achieved. For the SIFT Flow dataset, we experimented with two sam-pling methods when learning the multiscale features: re-specting natural frequencies of classes, and balancing them so that an equal amount of each class is shown to the network. Both results are reported in Table 2. Training with balanced frequencies allows better discrimination of small objects, and although it decreases the overall pixel-wise accuracy, it is more correct from a recognition point of view. Frequency balancing is used on the Stanford Back-ground dataset, as it consistently gives better results. For the Barcelona dataset, both sampling methods are used as well, but frequency balancing worked rather poorly in that case. This could be explained by the fact that this dataset has a large amount of classes with very few training exam-ples. These classes are therefore extremely hard to model, and overfitting occurs much faster than for the SIFT Flow dataset. Results are shown on Table 3.
 Results in Table 1 demonstrate the impressive computa-tional advantage of convolutional networks over competing algorithms. Exploiting the parallel structure of this special network, by computing convolutions in parallel, allows us to parse an image of size 320  X  240 in less than one sec-ond on a 4 -core Intel i7 laptop. Our scene parsing method shows promise of real time applications, which would con-stitute a breakthrough in the fields of machine learning and computer vision. Training time is also remarkably fast: re-sults on the Stanford dataset were typically obtained in 48 h on a regular server.
 We introduced a discriminative framework for learning to identify and delineate objects in a scene. Our model does not rely on engineered features, and uses a multi-scale con-volutional network operating on raw pixels to learn appro-priate low-level and mid-level features. The convolutional network is trained in supervised mode to directly produce labels. Unlike many other scene parsing systems that rely on expensive graphical models to ensure consistent label-ings, our system relies on a multiscale feature representa-tion to consider large a large context for each local deci-sion. It also relies on on a segmentation tree in which the nodes (corresponding to image segments) are labeled with the entropy of the distribution of classes contained in the corresponding segment. Instead of graph cuts or other in-ference methods, we use the new concept of optimal cover to extract the most consistent segmentation from the tree. The complexity of each operation is linear in the num-ber of pixels, except for the production of the tree, which is quasi-linear (meaning cheap in practice). The system produces state-of-the-art accuracy on the SIFT Flow and Barcelona datasets (both measured per pixel, or averaged per class) and state-of-the-art averaged per class accuracy on the Stanford Background dataset, while dramatically outperforming competing models in inference time.
 Our current system relies on a single segmentation tree constructed from image gradients, and implicitly assumes that the correct segmentation is contained in the tree. Fu-ture work will involve searches over multiple segmentation trees, or will use other graphs than simple trees to encode the possible segmentations (since our optimal cover algo-rithm can work from other graphs than trees). Other direc-tions for improvements include the use of structured learn-ing criteria such as Maximin Learning (Turaga et al., 2009) to learn low-level feature vectors from which better seg-mentation trees can be produced.

