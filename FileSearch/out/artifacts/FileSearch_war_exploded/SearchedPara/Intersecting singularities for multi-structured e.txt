 Emile Richard emile.richard@mines-paristech.fr CBIO Mines ParisTech, INSERM U900, Institut Curie Francis Bach francis.bach@inria.fr Jean-Philippe Vert jean-philippe.vert@mines-paristech.fr CBIO Mines ParisTech, INSERM U900, Institut Curie Estimating high-dimensional simple objects from a few measurements has been widely investigated in statis-tics and optimization due to the large body of potential applications. In biology, where measurements are ex-pensive to obtain and the data very complex, building reliable predictors from the limited amount of avail-able data plays an increasing role in medicine. In web applications building recommender systems presents the same kind of challenges with huge economic im-pacts. In some applications such as breakpoint detec-tion (Vert &amp; Bleakley, 2010), clique detection (Alon et al., 1998; Doan &amp; Vavasis, 2010) that is closely related to recommender systems, compressed sensing (Golbabaee &amp; Vandergheynst, 2012) and sparse prin-cipal component analysis (d X  X spremont et al., 2007), the goal is to capture rich objects on which multiple structural informations are available. In such cases, at the first glance, one would expect the inference to be simpler when more structural information is known, as we search for the solution in a smaller space: the inter-sections of the multiple low-complexity sets. However, to the best of our knowledge, no general methodology exists for combining multiple priors to recover objects having simultaneously the different structures. A popular methodology for incorporating particular effects in the solution is to use convex regularizers R ( w ) that are nondifferentiable at points w that ful-fill the structural constraints, such as sparse vectors or low-rank matrices (Bach et al., 2011; Chandrasekaran et al., 2012). Assume we are given nonsmooth regu-larizers R 1 and R 2 , each inducing a particular desired behavior, and we want to build a new regularizer in-ducing both properties. A natural approach is to add together both regularizers to form a joint regularizer R + = R 1 + R 2 , to enforce both constraints (Richard et al., 2012). However adding regularizers encourages objects having one or the other property, not neces-sarily both at the same time. The infimal convolution R modelling objects as the sum of two terms, each of them respectively penalized by one of the original reg-ularizer (Cand`es et al., 2009; Chandrasekaran et al., 2011). This is again different from finding objects pre-senting the two properties simultaneously. Taking the maximum R  X  = max( R 1 , R 2 ) as suggested by Oymak et al. (2012) has the obvious drawback of promoting points on which regularizers take equal value, which is not the goal.
 In this paper, we propose a new approach to combine structure-inducing penalties, focusing on the problem of inferring sparse and low-rank matrices. Our ap-proach is based on analyzing the geometry of the space around singular points. To combine two regularizers, we suggest to intersect singularities and relax the new index measuring simultaneously both properties rather than adding regularizers. The subdifferential of a con-vex function at a singular point is a convex set not reduced to a singleton. We consider the cases where this convex set lies in a well characterized subspace, and by intersecting singularities we mean intersecting the subspaces corresponding to each penalty term. We aim at expanding the intersection or equivalently re-ducing the complement. We study the case of sparse low-rank matrices where by intersecting the subspaces in which subdifferentials of each of the regularizers lie we build a new measure for matrices which we call ranksity : the dimension of the complement to the in-tersection space (Section 2).
 We consider a convex relaxation of ranksity as the trace norm of a linear function of the unknown, and show that the standard sum of regularizers can be writ-ten in a similar way. In Section 3 we provide a theoret-ical analysis of this family of regularizers from statisti-cal and compressed sensing point of views. In Section 4 we provide algorithmic schemes to solve problems of interest and finally show in Section 5 numerical exper-iments showing the applicability of our regularizer and comparison with baselines.
 In the sequel, n and m are integers and w.l.o.g. n  X  m . For any matrix X  X  R n  X  m the notations k X k F , k X k 1 Frobenius norm, the entry-wise ` 1 and `  X  norms, the number of nonzero elements, the trace-norm (or nu-clear norm, the sum of the singular values), the op-erator norm (the largest singular value) and the rank of X . The letters r and s denote the rank and the sparsity index of X . Given matrices A and B , we de-note by  X  A,B  X  = tr( A &gt; B ), A  X  B and A  X  B the inner product, the Hadamard and the Kronecker products of matrices. A vector in R d is always understood as a d  X  1 matrix and vec ( X ) denotes the vectorized version of X , Diag( x ) and diag( X ) are respectively the matrix having the vector x at its diagonal and 0s elsewhere and the vector formed by X i,i s. The matrix | X | con-tains the absolute values of entries of X and sgn( X ) is the sign matrix associated with X with the conven-tion sgn(0) = 0. We denote by U X  X  X V X the singu-lar value decomposition of X , and we define the sub-spaces of R n  X  m , span ( X ) and supp ( X ) as the ranges of linear applications ( A,B ) 7 X  AX + XB  X  R n  X  m and C 7 X  C  X  X respectively. We denote by P X , P  X  X , Q
X and Q  X  X the orthogonal projectors onto span ( X ), span  X  ( X ), supp ( X ) and supp  X  ( X ) respectively. Nonsmooth convex regularizers (Bach et al., 2011; Chandrasekaran et al., 2012) have recently received tremendous interest for estimating objects having par-ticular structural properties. Indeed, their convexity makes them computationally attractive: they lead to polynomially converging algorithmic schemes that are easy to implement. From a statistical point of view their analysis benefits from a relatively good under-standing of the behaviors and a series of theoretical results ensure the quality of the provided estimators. The nondifferentiable points of such penalties attract the minimizers of optimization procedures and this is the key to their success. The location and the strength of these promoted points can be read in the penalty X  X  subgradients expressions. The subgradients of the trace-norm and the ` 1 norm, which are widely used to infer respectively low-rank and sparse matri-ces, are the sets B op and B  X  being the unit balls of the operator and `  X  norms respectively. A point is nondifferentiable when the subgradient at this point is not reduced to a singleton. To understand the strength of the nondiffer-entiability we recall that the normal cone of a convex function at a given point is the set of points obtained by multiplying an element of the subgradient by a non-negative real number. Let us define the dimension of a cone as the dimension of its affine hull. The dimension of the normal cone is a fair measure of the singular-ity at a given point. From the subgradient expressions one can see that the singularity of the ` 1 norm at X is reflected onto the dimension of supp ( X )  X  through the range of Q  X  X , and similarly, the nondifferentiability of the trace norm at X is reflected onto dim( span ( X )  X  ) through P  X  X . This makes the subspaces span ( X ) and supp ( X ) privileged subspaces for trace-norm and ` 1 norm penalized estimation procedures. In fact they are respectively the tangent spaces to the manifolds of rank r = rank( X ) matrices and s = k X k 0 sparse matrices at X . In the following we discuss two alter-native possibilities for building a regularizer for sparse low-rank estimation. 2.1. Summing: the  X  X race + 1 X  penalty For estimating sparse low-rank matrices, previous approaches (Richard et al., 2012; Oymak et al., 2012; Doan &amp; Vavasis, 2010) have suggested to add regularizers resulting in the  X  trace + 1  X  penalty X 7 X  (1  X   X  ) k X k  X  +  X  k X k 1 . The subgradient of this penalty is not reduced to a singleton as soon as the subgradient of the ` 1 or the trace norm component is not a singleton, i.e. , when the matrix is either sparse or low-rank. By letting the trace + 1 penalty can be reformulated as k  X ( X ) k  X  , the trace norm of  X ( X ). It can be thought of as a convex relaxation of the index rank  X ( X ) = rank( X ) + k X k 0 . This index measures the sparsity and the rank by taking its maximal value nm + n  X  m when X is full rank and dense. Note that this index does not penalize density and high-rank in a disjunctive manner, in the sense that it does not have large values when X is dense or full-rank. 2.2. Intersecting: the ranksity index A drawback of adding nonsmooth penalties is that the singularities of the sum are located on the union of singularities of each of the components and not at the intersection of them. We argue that if the goal is to measure matrices having both the sparse and low-rank properties, the penalty to use should be nondifferen-tiable at points which are both sparse and low-rank, namely at the intersection of the singularities of the ` 1 and singularities of the trace norm. For building such a penalty we will build a norm such that the dimension of its normal cone at a given point X is given by the di-mension of span ( X )  X   X  supp ( X )  X  , the intersection of the normal cones of both individual penalties. To this end let us first define the ranksity index as the dimen-sion of the orthogonal space to span ( X )  X   X  supp ( X ) that is precisely span ( X ) + supp ( X ): ranksity ( X ) takes its maximum value nm on matri-ces X that are either dense (possibly low-rank) or full rank (possibly sparse). In fact if X is dense, then supp ( X ) = R n  X  m , and if X is full rank, then span ( X ) = R n  X  m . Providing a closed form expres-sion for ranksity is not straightforward in general, but for instance in the case of block-diagonal adja-cency (binary-valued) matrices X having r nonzero non-overlapping blocks of size k i  X  l i we can obtain by recursion that ranksity ( X ) = r ( m + n  X  r ) + It is convenient to have in mind the relationships with the rank and the sparsity index: dim( supp ( X )) = s , dim( span ( X )) = ( n + m  X  r ) r based on which we can easily derive the bounds s  X  ( m + n  X  r ) r  X  ranksity  X  s + ( m + n  X  r ) r which show that this nonconvex discontinuous func-tion is sandwiched by two non-decreasing functions of the rank and the sparsity index. 2.3. A convex regularizer for low ranksity vec ( XB ) = ( I m  X  X ) vec ( B ). It follows that for any vec ( AX + XB + X  X  C ) = [ X &gt;  X  I n , I m  X  X , Diag( vec ( X ))] the term inside the vec on the left hand side de-scribes precisely the sum of the subspaces span ( X ) and supp ( X ) used to define ranksity in (1). This is why, after weighting the terms to control the tradeoffs, we define  X ( X ) = This lifting is built so that for any  X   X  ]0 , 1[, the range of the matrix  X ( X ) is isomorphic to span ( X ) + supp ( X ). Using this fundamental prop-erty we can state the closed-form expression (valid for  X   X  ]0 , 1[): This property suggests in turn to consider k  X ( X ) k as a convex surrogate of ranksity ( X ), which we call block norm and which can be used as a regular-izer to infer low-ranksity matrices. Notice that for  X  = 0, k  X ( X ) k  X  = k  X ( X ) k  X  = k X k 1 and for  X  = 1, k  X ( X ) k  X   X  ( n + m ) k X k  X  and k  X ( X ) k  X  = k X k  X   X  ]0 , 1[, k  X (  X  ) k  X  has no singularities on matrices that are full-rank and sparse or low-rank and dense as op-posed to k  X (  X  ) k  X  which has this undesirable property. In Figure 1 one can clearly see that in the case of the  X  X race + 1 X  penalty on X = mimic the shapes of a cylinder that represents the unit ball of the trace norm and that of the unit ball of the ` 1 norm. As opposed, the block norm ball has only 4 nonsmooth points located at that exactly correspond to the intersections of the sin-gularities of the ` 1 and trace norm balls. We reformulated the  X  X race + 1 X  penalty using a linear mapping  X  and introduced a new penalty, the block norm, using  X . Using the general formalism of lifted trace norms we state theoretical results that help us better understand the behaviour of each of the two norms and compare them more easily. Due to space constraints, all proofs are postponed to appendices available as supplementary materials. 3.1. Lifted trace norms We call lifting a linear mapping  X  : R n  X  m  X  R n 0  X  m 0 and call the penalty induced by k  X ( X ) k  X  on the ma-trix X the  X -trace or lifted trace norm . Such penalties have been used in compressed sensing (Hosseini Ka-mal &amp; Vandergheynst, 2013), in statistics (Grave et al., 2011), and have similarities with fused spar-sity inducing type of penalties k  X ( X ) k 1 studied for ley (2010); Vaiter et al. (2012). Note that a lifted trace norm is not necessarily a norm. It verifies tri-angle inequality and positive homogeneity, but only separates points so becomes a norm if  X  is injec-tive ( i.e. ,  X ( X ) = 0  X  X = 0). We denote by linear map  X . The mapping  X   X  denotes the adjoint gular value decomposition of  X ( X ), the subgradient of the  X -trace at X is given by From this expression one can see that when  X ( X ) is rank deficient then k  X ( X ) k  X  is nondifferentiable, in cases where the image of  X   X  is the whole space R n  X  m . This makes the rank of  X ( X ) a particularly interesting quantity in this context.
 In the following X ? denotes the target matrix to be estimated and  X  : R n  X  m  X  R d a set of linear mea-surements: We call the  X  i s design matrices and we will be inter-ested in the estimation procedures (i) minimizing the lifted trace norm and (ii) minimizing the  X -trace sub-ject to  X  ( X ) =  X  ( X ? ). 3.2. Least squares regression with lifted We consider linear regression and prove oracle inequal-ities for the estimation procedure using techniques in-troduced by Koltchinskii et al. (2011). That is, we consider the model where  X  R d having i.i.d zero mean entries.
 Assumption 1 We assume that the lifting  X  is or-thogonal, that is  X   X   X  = k  X  k 2 Id , which is for instance the case of  X  and  X  .
 For the two orthogonal liftings of interest  X  and  X , the operator norms respectively are given by Definition 1 The cone of restriction C ( X, X ,  X ) is the set of matrices B  X  R n  X  m satisfying The restricted eigenvalue of  X  at X is  X   X ,  X  ( X ) = inf  X  &gt; 0 such that kP Define the objective and consider the following estimation procedure where S  X  R n  X  m is the convex cone of admissible solutions. We can state the following oracle inequality on the estimate  X  X .
 Proposition 1 Under Assumption 1, for  X   X  3 d k  X ( M ) k op / k  X  k 2 , where M = P d i =1 i  X  the following holds: k  X  ( b X  X  X ? ) k 2 2  X  inf Note that as (see the proof) b X  X  X ?  X  C ( X ? , 5 ,  X ) and by orthogonality of  X , we bound the estimation error by the prediction error k b hence the oracle inequality of Proposition 1 provides a bound on the estimation error.
 We point out that using similar techniques, and un-der the stronger assumption called Restricted Isometry Property that assumes there exists  X  &gt; 0 such that for any X 1 ,X 2  X  X  one can state that for  X   X  2 d k  X ( M ) k op / k  X  k 2 , we have where c 0 = if X ?  X  S . In particular in the case of denoising  X  = id,y = X ? + M considered for instance by Chandrasekaran &amp; Jordan (2012), this proves that if  X   X  2 nm k  X ( M ) k op / k  X  k 2 3.3. Probabilistic results The theoretical analysis of penalized estimation pro-cedures by a norm highlights that when the dual norm of the noise is low the result is more attractive. This motivates us to understand the behavior of k  X ( G ) k op where G denotes the noise which we assume to Gaus-variance of a lifting using canonical matrices E i,j hav-ing 1 at the ( i,j ) entry and 0 everywhere else as v  X  = k Using results stated in (Tropp, 2010), we know that for a matrix G having i.i.d. centered normal entries and we can control the deviation for t &gt; 0 as We can bound the  X s variance v 2  X  (  X  )  X   X  2  X  n (1  X   X  ) and observe that by setting  X  = per bound on the expectation over standard normal matrices G The variance of  X  can be controlled by v (  X  )  X  (1 + n )(1  X   X  ) 2 +  X  2 , which suggests to set  X  = n +1 n +2 in order to obtain We also define the observable variance under the linear map  X  as v which is a function of  X  for  X  and  X  and equal to the noise vector elements i are independently drawn from N (0 , X  2 ).
 Corollary 1 (Block norm) Consider the  X  -trace penalty and calibrate for t &gt; 0 then with probability at least 1  X  e  X  t , k  X  ( b X  X  X ? ) k 2 2  X  inf where c = 6  X v  X  , X   X  5 ,  X  ( X ) / [  X  2 + ( n + m )(1  X   X  ) pends on  X  .
 Corollary 2 (Trace + 1) Consider the  X  -trace penalty and calibrate for t &gt; 0 then with probability at least 1  X  e  X  t , k  X  ( b X  X  X ? ) k 2 2  X  inf + c 2 where c = 3 In both cases it is the minimizer of respectively The two corollaries are interesting because they show that after a natural calibration of the tuning parameter  X  , the convex estimation procedure (4) outputs the op-timal estimators for the nonconvex penalties rank + ` 0 and ranksity , respectively. In addition the multiplica-tive factor behind these estimators sharply reminds us of known optimal rates, such as (log n ) /p for the Lasso. 3.4. Compressed sensing and exact recovery Consider the constrained convex optimization problem where the design matrices  X  i are i.i.d. Gaussians. We have the following bound on the minimum required such observations for perfect recovery of X ? . Proposition 2 The minimum required number of Gaussian i.i.d. observations for achieving perfect re-covery of X ? with overwhelming probability by solving (5) where  X  is an orthogonal lifting is at most the expectation being taken over the set of i.i.d. stan-dard normal matrices G .
 In the case of the orthogonal lifting  X , the quan-tity kP  X   X ( X ? ) ( X ( G )) k op can be naively bounded by k  X ( G ) k 2 op for which we already have an upper bound. Corollary 3 (Block norm) For the  X  -trace penalty, by taking  X  = ( n + 1) / ( n + 2) , d
 X   X  1 + 4 ranksity ( X ? ) log( n + m ) i.i.d. Gaussian observations are enough to achieve with overwhelming probability perfect recovery of X ? by solving (5). For  X  the situation is simpler as we have a better un-derstanding of the behavior of P  X   X ( X ? ) ( X ( G )). In fact allows us to analyze the terms separately and state Corollary 4 (Trace + 1) In the case of  X  -trace penalty, take  X  = 1  X  1  X  we have On a bi-clique of size ( k,l ) we get d  X   X  c 1 kl log( nm  X  s ) and d  X   X  4 { ( n + m  X  1) + ( k  X  1)( l  X  1) } log( n + m ). 4.1. A Chambolle-Pock algorithm for general The  X -trace penalties have similarities with total vari-ation minimization as in both cases a simple (having an easy to compute proximal operator) norm of a lin-ear function of the variable is being minimized. In our case the unconstrained optimization problem can be re-written as a primal-dual problem where  X  B op is the indicator of the unit ball of the oper-ator norm. The Chambolle-Pock framework (Cham-bolle &amp; Pock, 2011) applies and we can derive the Algorithm 1 for cases where ` is convex and simple. The accelerated algorithm applies for cases where ` in addition to being convex has a Lipschitz continu-ous gradient, for instance in least squares regression. In these settings, the second prox step is replaced by a gradient descent step and the tuning parameters  X , X , X  are updated at each step. We refer to Chambolle &amp; Pock (2011) for technical details such as the choices of tuning parameters that were set according to the paper X  X  remarks in our experiments. In numerical ex-periments following the lifting  X  it can be convenient to use other algorithms, for instance in case of  X  we used ADMM (see Boyd et al., 2011, for a survey) in our experiments.
 Algorithm 1 Chambolle-Pock algorithm for  X -trace penalized optimization Initialize X,Z,  X  X, X , X , X  Repeat until convergence 4.2. A Frank-Wolfe algorithm for smoothly In cases where the loss function ` to minimize has a Lipschitz continuous gradient (least squares regression is a standard example) rather than penalizing the loss by the lifted trace norm one can solve the following constrained optimization problem: where C is a constant replacing the tuning parame-ter  X  in this setting. The advantage of this equiva-lent formulation is in the possibility to use algorithmic schemes offered by Frank-Wolfe or conditional gradient algorithm (see Jaggi, 2013, for a recent survey). We argue that these algorithms allow to save both com-putational and memory resources as they require only the top singular vectors of a n 0  X  m 0 matrix rather than the full SVD of it at each iteration. Frank-Wolfe algo-rithm in this situation requires at each iteration k to solve the following linear subproblem which in the case of orthogonal liftings ( i.e.  X 
 X  = k  X  k 2 id such as  X  and  X ) can be written using the variable  X  =  X ( S ) as and therefore reduces to The latter optimization problem is a linear problem to solve over an atomic set. We know that the top approximate solution to the problem and in addition they present the advantage of being extremely fast to obtain thanks to the Lanczos method. The pseudo code can be found in Algorithm 2 and we refer to Jaggi (2013) for further technical details such as variants us-ing refined step-sizes and the stopping criterion. Algorithm 2 Frank-Wolfe algorithm for  X -trace pe-nalized optimization
Initialize X 0 = 0 for k = 0  X  X  X  K do end for 5.1. Nonsmooth ` : decomposing simply We know from Chandrasekaran et al. (2011) that a matrix that is built by adding a sparse matrix to a low-rank matrix can be decomposed onto its compo-nents by solving the so-called robust PCA problem. It is known however that robust PCA fails in recovering the additive components when the two types of struc-ture are present in one of the components e.g. when the low rank component is itself sparse. In this work we specifically focus on the two following challenging tasks: 1. [SL +S] The observation is the sum of a sparse 2. [SL + L] In this setup the observation is the sum In each case we compared our algorithms to element-wise thresholding of the observation and to the singu-lar value thresholding of the matrix sometimes called shrinkage. For simulations we took n = m = 10 and the ground truth X ? was a matrix of rank two hav-ing 9 nonzero elements. In the first case the noise is generated as i.i.d. gaussian entries at sparse (15%) po-sitions with various variances given by the noise level, and in the second case [SL + L] the noise is a rank 2 dense matrix generated proportional to the noise level times the highest singular value of X ? . We selected the tuning parameters using cross-validation and em-phasize that 0 and 1 where included in the potential values for  X  but were not the favorite values following the cross-validation step. The results over 10 runs of the experiment can be found in Figure 2. The two al-gorithms penalizing the trace + 1 and the block norm are superior to the competitors. 5.2. Smooth ` : dense denoising and multitask We performed numerical tests using the Franck-Wolfe algorithms on two different problems. In these exper-iments n = 15 ,m = 10. The matrix X ? is generated using sparse factors having r = 3 columns. 1. Denoising. In this case the observation 2. Multitask learning. The observation In all the simulation the parameters  X  and  X  are chosen using a cross-validation step and again we noticed the algorithms choose values of  X  6 = 0 , 1 that correspond to basic regularizers. So without explicitely testing them, superiority to the Lasso and trace norm penal-ized regression is empirically observed. See Table 1 for relative estimation errors k X ?  X   X  X k F / k X ? k F in these experiments over 100 runs. Our main methodological point can be applied to a complexity index that can be written as the dimen-sion of a linear subspace, as it is the case of the ` 0 and the rank (recall rank( n + m  X  rank) = dim( span ) is an increasing function of the rank). Other penalties have the same flavor. By letting k X k 2 , 1 = P n i =1 k X we can concatenate a matrix corresponding to the linear space spanned by the columns, or namely the range of ( c 1 ,  X  X  X  ,c m ) 7 X  P m i =1 X .,i c &gt;
I m  X  X ., 1  X  X  X  I m  X  X .,m to other blocks. For in-stance, our rational on the block norm would suggest, instead of using  X  k X k 2 , 1 + (1  X   X  ) k X k  X  (Golbabaee &amp; Vandergheynst, 2012) to use the lifted trace norm defined through the lifting and a similar lifting can be written for estimat-ing sparse and row-sparse matrices, instead of using (1  X   X  ) k X k 2 , 1 +  X  k X k 1 .
 This work was supported by the European Research Council (SIERRA-ERC-239993 and SMAC-ERC-280032). E.R. benefited from the support of the FMJH Program Gaspard Monge in optimization and opera-tion research (EDF).

