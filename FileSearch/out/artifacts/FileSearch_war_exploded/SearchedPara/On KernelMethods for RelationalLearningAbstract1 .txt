 Chad Cum by cumby@uiuc.edu Dan Roth danr@uiuc.edu Recen tly, much interest has been generated in the mac hine learning comm unit y on the sub ject of learn-ing from relational and structured data via prop osi-tional learners (Kramer et al., 2001; Cum by &amp; Roth, 2003a). Examples of relational learning problems include learning to iden tify functional phrases and named entities from structured parse trees in natu-ral language pro cessing (NLP), learning to classify molecules for mutagenicit y from atom-b ond data in drug design, and learning a policy to map goals to ac-tions in planning domains. At the same time, work on SVMs and Perceptron type algorithms has gener-ated interest in kernel metho ds to sim ulate learning in high-dimensional feature spaces while working with the original low-dimensional input data.
 Haussler's work on con volution kernels (Haussler, 1999) introduced the idea that kernels could be built to work with discrete data structures iterativ ely from kernels for smaller comp osite parts. These kernels fol-lowed the form of a generalized sum over pro ducts { a generalized con volution. Kernels were sho wn for sev-eral discrete datat ypes including strings and rooted trees, and more recen tly (Collins &amp; Du y , 2002) dev el-oped kernels for datat ypes useful in man y NLP tasks, demonstrating their usefulness with the Voted Percep-tron algorithm (Freund &amp; Schapire, 1998). While these past examples of relational kernels are for-mulated separately to meet eac h problem at hand, we seek to dev elop a exible mec hanism for building ker-nel functions for man y structured learning problems -based on a uni ed kno wledge represen tation. At the heart of our approac h is a de nition of a relational kernel that is speci ed in a \syn tax-driv en" manner through the use of a description language. (Cum by &amp; Roth, 2002) introduced a feature description lan-guage and have sho wn how to use prop ositional clas-si ers to successfully learn over structured data, and pro duce relational represen tation, in the sense that dif-feren t data instan tiations yield the same features and have the same weigh ts in the linear classi er learned. There, as in (Roth &amp; Yih, 2001), this was done by signi can tly blowing up the relational feature-space. Building on the abovemen tioned description language based approac h, this pap er dev elops a corresp onding family of parameterized kernel functions for structured data. In conjunction with an SVM or a Perceptron-like learning algorithm, our parameterized kernels can sim ulate the exact features generated in the blown up space to learn a classi er, directly from the original structured data. From among sev eral ways to de ne the distance between structured domain elemen ts we follo w (Khardon et al., 2001) in choosing a de nition that pro vides exactly the same classi ers pro duced by Perceptron, if we had run it on the blown up discrete feature space, rather than directly on the structured data. The parameterized kernel allo ws us to exibly de ne features over structures (or, equiv alen tly, a met-ric over structures). At the same time, it allo ws us to choose the degree to whic h we want to restrict the size of the expanded space, whic h a ects the degree of ef-ciency gained or lost -as well as the generalization performance of the classi er -as a result of using it. Along these lines, we then study time complexit y and generalization tradeo s between working in the ex-panded feature space and using the corresp onding ker-nels, and between di eren t kernels, in terms of the expansions they corresp ond to. We sho w that, while kernel metho ds pro vide an interesting and often more comprehensible way to view the feature space, compu-tationally , it is often not recommended to use kernels over structured data. This is esp ecially clear when the num ber of examples is fairly large relativ e to the data dimensionalit y.
 The remainder of the pap er is organized as follo ws: We rst introduce the use of kernel functions in linear classi cation and the kernel Per ceptr on algo-rithm (Khardon et al., 2001). Sec. 3 presen ts our ap-proac h to relational features, whic h form the higher-dimensional space we seek to sim ulate. Sec. 4 intro-duces our kernel function for relational data. Sec. 5 discusses complexit y tradeo s surrounding the ker-nel Perceptron and standard feature-based Perceptron and the issue of generalization. In Sec. 6 we validate our claims by applying the kernel Perceptron algo-rithm with our enhanced kernel to two problems where a structured feature space is essen tial. Most mac hine learning algorithms mak e use of feature based represen tations. A domain elemen t is trans-formed into a collection of Boolean features, and a la-beled example is given by &lt; x; l &gt; 2 f 0 ; 1 g n f 1 ; 1 g . In recen t years, it has become very common, esp e-cially in large scale applications in NLP and computer vision, to use learning algorithms suc h as variations of Perceptron and Winno w (No viko , 1963; Little-stone, 1988) that use represen tations that are linear over their feature space (Roth, 1998). In these cases, working with features that directly represen t domain elemen ts may not be expressiv e enough and there are standard ways of enhancing the capabilities of suc h al-gorithms. A typical way whic h has been used in the NLP domain (Roth, 1998; Roth, 1999) is to expand the set of basic features x 1 ; : : : ; x n using featur e func-tions i : f x 1 ; : : : x n g ! f 0 ; 1 g , i 2 I . These features functions could be, for example, conjunctions suc h as x x 3 x 4 (that is, the feature is evaluated to 1 when the conjunction is satis ed on the example, and to 0 other-wise). Once this expansion is done, one can use these expanded higher-dimensional examples, in whic h eac h feature function plays the role of a basic feature, for learning. This approac h clearly leads to an increase in expressiv eness and thus may impro ve performance. However, it also dramatically increases the num ber of features (from n to j I j ; e.g., to 3 n if all conjunctions are used, O ( n k ) if conjunctions of size k are used) and thus may adv ersely a ect both the computation time and con vergence rate of learning.
 Perceptron is a well kno wn on-line learning algorithm that mak es use of the aforemen tioned feature based represen tation of examples. Throughout its execution Perceptron main tains a weigh t vector w 2 &lt; n whic h is initially (0 ; : : : ; 0) : Up on receiving an example x 2 f 0 ; 1 g n ; the algorithm predicts according to the linear threshold function w x 0 : If the prediction is 1 and the lab el is 1 then the vector w is set to w x , while if the prediction is 1 and the lab el is 1 then w is set to w + x: No change is made if the prediction is correct. It is easily observ ed, and well kno wn, that the hy-pothesis w of the Perceptron is a sum of the pre-vious examples on whic h prediction mistak es were made. Let L ( x ) 2 f 1 ; 1 g denote the lab el of ex-ample x ; then w = P v 2 M L ( v ) v where M is the set of examples on whic h the algorithm made a mis-tak e. Thus the prediction of Perceptron on x is 1 i w x = ( P v 2 M L ( v ) v ) x = P v 2 M L ( v )( v x ) 0. For an example x 2 f 0 ; 1 g n let ( x ) denote its trans-formation into an enhanced feature space, ( x ) = f To run the Perceptron algorithm over the enhanced space we must predict 1 i w ( x ) 0 where w is the weigh t vector in the enhanced space; from the above discussion this holds i P v 2 M L ( v )( ( v ) ( x )) 0. Denoting this holds i P v 2 M L ( v ) K ( v; x ) 0. We call this ver-sion of Perceptron, whic h uses a kernel function to ex-pand the feature space and thus enhances the learning abilities of Perceptron, a kernel Per ceptr on (Khardon et al., 2001).
 We have sho wn a bottom up construction of the \ker-nel tric k". This way, we nev er need to construct the enhanced feature space explicitly; we need only be able to compute the kernel function K ( v; x ) ecien tly. This tric k can be applied to any algorithm whose prediction is a function of inner pro ducts of examples; see e.g. (Cristianini &amp; Sha we-T aylor, 2000) for a discussion. In principle, one can de ne the kernel K ( x; v ) in di eren t ways, as long as it satis es some conditions. In this pap er we will concen trate on a de nition that follo ws the discussion above. We will rst de ne a feature space. Then de ne a collection of features functions and a mapping to an enhanced feature space. We will then sho w that our kernel de nition satis es Eq.1. The discussion above implies that running kernel Per-ceptron yields iden tical results to running Perceptron on the enhanced features space. In order to learn in structured domains suc h as natu-ral language, object recognition, and computational biology , one must decide what represen tation is to be used for both the concept to be learned, and for the instances that will be learned from. Giv en the intractabilit y of traditional Inductiv e Logic Program-ming (ILP) approac hes under man y conditions, recen t approac hes attempt to dev elop ways that use ecien t prop ositional algorithms over relational data. One tac-tic that has sho wn itself to be particularly useful is the metho d of \prop ositionalization" (Kramer et al., 2001; Cum by &amp; Roth, 2003a). This metho d suggests to rep-resen t relational data in the form of quan ti ed prop o-sitions (Khardon et al., 1999), whic h can be used with standard prop ositional and probabilistic learners. This technique allo ws the expansion of the space of possible prop ositional features to include man y structured fea-tures de ned over basic features abstracted from the input data instances. One metho d of performing this expansion of relational features is through the notion of a Feature Description Logic.
 By the metho d detailed in (Cum by &amp; Roth, 2002), this Description Logic allo ws one to de ne \types" of fea-tures, whic h through an ecien t generation function, are instan tiated by comparing the feature de nition against data instances. These data instances are in general any type of structured data, represen ted by a graph-based data structure kno wn as a conc ept graph . For an example of a concept graph, consider the sen-tence represen ted by the dep endency graph in Fig. 1. This digraph con tains nodes to represen t eac h word, along with two nodes to represen t the noun and verb phrase presen t in the sen tence. In general, eac h node is lab eled with attribute information about some indi-vidual, and eac h edge is lab eled with relational infor-mation about two individuals. The description logic, as de ned below, pro vides a framew ork for represen t-ing the same seman tic individuals as are represen ted by nodes in a concept graph.
 De nition 1 An FDL description over the attribute alphab et Attr = f a 1 ; :::; a n g , the value alphab et V al = v ; :::; v n , and the role alphab et Role = f r 1 ; :::; r de ne d inductively as follows: 1. For an attribute symb ol a i and a value symb ol v j , 2. If D is a description and r i is a role symb ol, then 3. If D 1 ; :::; D n are descriptions, then ( AND In order to generate useful features from the struc-tured instance above we could, for example, de ne a description suc h as: ( AND phrase (con tains word)). Eac h feature to be generated from it is a Boolean val-ued function indexed by a syn tactic description. Belo w are features generated through the Feature Generat-ing Function (FGF) mec hanism describ ed in (Cum by &amp; Roth, 2002). To do so, the generating descrip-tion is matc hed against eac h node of the given in-stance graph. Feat. (1) below means essen tially , \In the given data instance, output 1 if 9 x; y 2 X suc h that phr ase ( x; N P ) ^ contains ( x; y ) ^ word ( y; T he )". We note that it is an easy matter to move from a pure Boolean represen tation of features to a positiv e integer valued represen tation, e.g., by asso ciating the num ber of times eac h feature substructure occurs in a given instance. This change of feature represen tation does not change the operation of the Perceptron algorithm and is related to our kernel construction in Sec. (4). The generation of features in suc h a manner pro duces a feature space polynomial in the size of the original input space with the degree being the generating de-scription size. This is attractiv e in terms of complexit y of generation and also poten tially in terms of the abil-ity to learn ecien tly in suc h spaces. We return to this topic in Sec. (5). We now de ne a family of kernel functions based on the Feature Description Language introduced. We utilize the description language introduced in Sec. (3) with a di eren t purp ose -to de ne the operation of a ker-nel function -but still with the aim of exploiting the syn tax of the language to determine the shap e of the feature-space. The de nition of our kernel functions uses the formal notion of the FDL concept graphs in-troduced earlier, whose de nition we summarize here: An FDL concept graph is a lab eled directed acyclic graph G = G ( N; E; l N ( )), where N is a set of nodes, E ( N N Role ) a set of lab eled edges (with role sym bols as lab els) and l N ( ) a function that maps eac h node in N to a set of sensor descriptions asso ciated with it. A rooted concept graph is speci ed by desig-nating a node n 0 2 N as the root of the graph. With these two de nitions we de ne the operation of a family of kernels parameterized by an FDL descrip-tion. We call this parameter the gener ating descrip-tion. Eac h kernel tak es as input two directed acyclic concept graphs, whic h may represen t man y types of structured data. Eac h outputs a real valued num ber represen ting the similarit y of the two concept graphs with resp ect to the generating description. The fam-ily of kernels is \syn tax-driv en" in the sense that the input description speci es the kernel's operation. De nition 2 Let D be the space of FDL descriptions, and G the space of all conc ept graphs. The family of functions K = f k D j D 2 D g is de ne d inductively as: 1. If D is a sensor description of the form s ( v ) and 2. If D is an existential sensor description of the 3. If D is a role description of the form ( r 4. If D is a description of the form ( AND D 1 Theorem 3 For any FDL description D and for any two conc ept graphs G 1 ; G 2 , the quantity output by k
D ( G 1 ; G 2 ) is equivalent to the dot product of the two featur e vectors D ( G 2 ) , D ( G 2 ) output by the fea-ture gener ating function describ ed in (Cumby &amp; Roth, 2002). Thus for all D k D necessarily de nes a kernel function over the space of conc ept graphs G . For a pro of of Theorem 3 see (Cum by &amp; Roth, 2003b). We examplify the claim that k de nes a kernel function by demonstrating that we can sim ulate two expanded feature spaces directly with our kernel construction. First of all, the technique can generalize the simple prop ositional case as considered in (Khardon et al., 2001) and also generalize learning with word or POS-tag n-grams of the type often considered in NLP ap-plications, given that the words and tags are enco ded as sensor descriptions appropriately .
 In order to mimic the case of learning from simple Boolean bit vectors suc h as [ x 1 x 2 : : : x n ] 2 f 0 ; 1 g n , we perform the follo wing translation. For eac h suc h vector, de ne a mapping from eac h comp onen t x i in x 2 f 0 ; 1 g n to a relation s ( x; v i ) where v i corresp onds to the truth value of x i on x . We can then use an FDL concept graph consisting of a single node lab eled with sensor descriptions corresp onding to eac h s ( x; v i ) to represen t x . At this point it becomes a simple matter to mimic the parameterized kernel of (Khardon et al., 2001) using our feature description kernel. De ne the generating descriptions: If we evaluate eac h k D stances n 1 , n 2 as describ ed above, we exp ect to cap-ture eac h conjunction of literals up to size three. Let same ( n 1 ; n 2 ) denote the num ber of sensor descrip-tions presen t as lab els of both n 1 and n 2 . Then we rors the parameterized kernel detailed in (Khardon et al., 2001). The expanded space feature vectors ( n 1 ) ( n 2 ) for nodes n 1 n 2 consist of all conjunctions of sensors up to size 3, and the dot pro duct ( n 1 ) ( n is equal to the num ber of conjunctions activ e in both vectors. This quan tity is equal to P i k D The example of extracting k -conjunctions from bit vec-tors is mark edly di eren t from the example of gener-ating n -gram type features as seen in the earlier exam-ple. In the case of n -grams, relational information is implicit in the manner in whic h com binations of words or other objects are chosen to be output. For example in the sen tence: The boy ran quickly , the com bination The-b oy is a valid bigram whereas boy-The is not. Via our kernel de nition, we can sim ulate the operation of a feature-space based algorithm on bigrams as follo ws. Giv en the generating description along with two data instances represen ted as concept graphs G 1 ; G 2 ; where G 1 corresp onds to the instance in Fig. 1 and G 2 to a similar instance for the sen tence The boy ran quickly , we can observ e the output of the function k D ( G 1 ; G 2 ) = P n For four pairings of n 1 and n 2 , k word ( n 1 ; n 2 ) will be non-zero, corresp onding to the pair of the nodes lab eled with wor d(The) , wor d(boy) , wor d(ran) , and with wor d(quickly) . The output of the rst k 1 1 = 1. The output for the second pairing of k
D ( N 1 boy ; N 2 boy ) is also 1 by a similar evaluation. The output of the third pairing is k D ( N 1 ran ; N 2 ran ) = k k k k D ( G 1 ; G 2 ) = 1 + 1 + 0 = 2.
 If we were to expand the feature-space for G 1 using the description D and the metho d referred to in Sec-tion (3), we would have a feature vector ( G 1 ) with the follo wing features outputting 1 and all else 0: Computing ( G 2 ) in a similar manner pro duces a vec-tor with these activ e features, and all else 0: Computing the dot-pro duct: ( G 1 ) ( G 2 ) = 2. Thus k D ( G 1 ; G 2 ) = ( G 1 ) ( G 2 ), exemplifying Thm 3. Before con tinuing, we presen t an extension to Def. 2 with its corresp onding e ects on the types of kernels included in the family de ned in Def. 2, intended to incorp orate the notion of locality along with focus of inter est . The focus of interest for a given input concept graph is de ned as a single distinguished node f . De nition 4 We extend the language of Def. 2 with the following rule and de ne it as FDL loc : If D is a description, then ( IN [ n , r ] D) is also a description, wher e n 2 Z + and R 2 Role . This description de-notes the set of individuals x 2 X describ ed by D and represente d by a node in a given input conc ept graph within n R -lab eled edges from the given focus f . When R = Role , we write IN [ n ] D).
 The family of kernels K is extended accordingly to include kernels k D form ( IN [n,r] D) , and D 2 F DL . In this case G 1 ; G 2 are concept graphs eac h with a given fo-cus of interest f 1 ; f 2 . As before k K P k n r -lab eled edges from f 1 and f 2 resp ectiv ely, or else k Our treatmen t of kernels for relational data aims at dev eloping a clear understanding of when it be-comes adv antageous to use kernel metho ds over stan-dard learning algorithms operating over some feature space. The common wisdom in prop ositional learning is that kernel metho ds will alw ays yield better perfor-mance because they cast the data in an implicit high-dimensional space (but also see (Khardon et al., 2001; Ben-Da vid &amp; Simon, 2002)).
 Prop ositionalization metho ds in relational learning have demonstrated separately that it is possible to learn relational concepts using a transformation of the input data. There, however, the transformation into a high-dimensional feature space is done explicitly through the construction of prop ositional features, as done in Sec. (3) using the Feature Generating Func-tion. The discussion of the relational kernel sho wed that we can form ulate a kernel function that sim ulates the updates performed by running Perceptron over the features generated this way. Conceiv ably then, we should be able to perform a learning exp erimen t com-paring the kernel metho d and the explicitly generated feature space, and exp ect to achiev e the same results in testing. Giv en suc h a situation, the main di erence between the two metho ds then becomes the exp ected running time involved. Thus a general analysis of the complexit y of running the kernel metho d and of gener-ating features and running over them seems warran ted. Giv en some description in our FDL D , and two concept graphs G 1 ; G 2 , let t 1 be the time to eval-uate k D ( n 1 ; n 2 ) if n 1 ; n 2 are two nodes of G 1 resp ectiv ely. Assuming all input concept graphs are equally likely, let g be the average num ber of nodes in a given graph. Since k D ( G 1 ; G 2 ) = P ating k D ( G 1 ; G 2 ) is prop ortional to g 2 t 1 . Since for an arbitrary sequence x 1 : : : x m of input graphs the kernel Perceptron algorithm could, in the worst case, mak e i 1 mistak es on x i , the overall running time for the algorithm given this kernel is O( m 2 g 2 t 1 ). To analyze the normal version of Perceptron, run over an equiv alen t feature-space generated by the FGF al-gorithm given in (Cum by &amp; Roth, 2002) with the same description D , we rst assume that the time to evalu-ate D for a node of a given concept graph is prop or-tional to t 2 . The total time to generate features from m arbitrary concept graphs is then O( mg t 2 ) with g as de ned above. We assume that the feature vectors we work with are variable length vectors of only posi-tive features (this is equiv alen t learning in the in nite attribute space (Blum, 1992) and is common in ap-plications (Roth, 1998)). To run Perceptron over the resulting feature vectors, eac h time a mistak e is made, we update eac h weigh t corresp onding to an activ e fea-ture in the curren t example. We could abstract the num ber of activ e features per example with an aver-age, but in realit y these updates tak e time prop ortional to the time spent in generating the features for eac h input graph. Thus the total running time is O( mg t 2 ). It is interesting to note that this result also applies to the speci c situation of kernel learning from Boolean bit vectors. Consider the example given earlier of ex-tracting conjunctions up to size k from bit vectors of case, as the represen tation of eac h vector is mapp ed to a single node concept graph, g = 1. t 1 is prop ortional examples the total time of learning with kernel Percep-tron is then O( m 2 n ). To compute the feature space of conjunctions up to size k explicitly tak es O( n k ), thus the total time for running standard Perceptron over the blown up feature space is O( mn k ). If m &gt; n k 1 the complexit y disadv antage of the kernel approac h becomes apparen t.
 A similar tradeo can be seen in terms of generaliza-tion abilit y. Our discussion sho ws that learning with kernel metho ds sim ulates learning with a highly ex-panded feature space. It is not surprising then, that recen t work (W eston et al., 2000; Ben-Da vid &amp; Si-mon, 2002; Khardon et al., 2001), has sho wn that even margin-maximizing kernel learners may su er from the curse of dimensionalit y; this happ ens in cases where the use of kernels is equiv alen t to introducing a large num ber of irrelev ant features. Assuming that a sub-set of the features generated is expressiv e enough for a given problem, it is not hard to sho w that embedding the problem in an even higher dimensional space can only be harmful to generalization.
 Previous metho ds that use structured kernels (e.g. (Collins &amp; Du y , 2002)) mak e use of an implicit fea-ture space that is exp onen tial in the size of the input represen tation, by de ning a metric that dep ends on all \substructures" (as do standard polynomial ker-nels). Our kernel approac h allo ws a transformations of the input represen tation to an implicitly larger prop o-sitional space; it does so, however, using a parameter-ized kernel, thus allo wing con trol of this transforma-tion so that it can be equiv alen t to a smaller prop osi-tional feature space. By specifying a kernel through a syn tax-driv en mec hanism based on the relational structure of the input, we can activ ely attempt to de-crease the num ber of irrelev ant features introduced. The bene t to generalization will be sho wn exp erimen-tally in the next section.
 Note that the focus of this analysis is on kernel Percep-tron; the exact complexit y implications for other ker-nel learners suc h as SVM, beyond the quadratic dep en-dence on the num ber of examples, is not so clear. The implications for generalization, however, apply across all kernel learners. We presen t two exp erimen ts in the domains of bioin-formatics and NLP . Our goal is to demonstrate that by restricting the expanded feature space that learn-ing tak es place in -using the standard feature based learner, or kernel Perceptron -we bene t in terms of generalization, and thus accuracy . Our main compar-ison is performed against a kernel learner that implic-itly constructs an exp onen tially larger feature space using a kernel based on (Collins &amp; Du y , 2002). As we can explicitly generate the smaller feature space used in the FDL approac h, it is possible to verify our equiv alence claim and compare with the performance of other prop ositional learners that do not admit ker-nels. The latter comparison is done using a variation of Winno w (Littlestone, 1988) implemen ted in the SNoW system (Carlson et al., 1999).
 The rst exp erimen t addresses the problem of predict-ing mutagenicit y in organic comp ounds, whic h has be-come a standard benc hmark in ILP for `prop osition-alization' metho ds (Sriniv asan et al., 1996). In this problem, a set of 188 comp ounds is given in the form of atom and bond tuples corresp onding to the atoms in eac h molecule with their prop erties and links to other atoms. Additionally , a set of lab el tuples is given to indicate whether eac h molecule is mutagenic. We map eac h comp ound to a concept graph, construct-ing nodes for eac h atom tuple lab eled with sensor de-scriptions of the form atom -elt ( v ) for the elemen t type, atom -type ( v ) for the atom type and atom -chr g ( v ) for the partial charge. We construct edges for eac h bond tuple and construct a single node for the comp ound overall -connected to eac h atom and lab eled with the comp ound's mutagenicit y, lumo , and logP values. We rst perform classi cation using an expanded fea-ture space generated by taking the sum of the ker-nels k D D 1 ; D 2 ; D 3 as describ ed below: Since kernels are comp osable under addition, the over-all kernel is valid. We introduce the second exp erimen-tal setup before con tinuing with results.
 The next exp erimen t deals with the natural language pro cessing task of tagging Named Entities. Giv en a natural language sen tence, we wish to determine whic h phrases in it corresp ond entities suc h as locations, or-ganizations or persons . In this exp erimen t, the MUC-7 dataset of sen tences with lab eled entities was used. Rather than focusing on the tagging problem of deter-mining whether a word is in some entity or not, we instead attempt to classify what entity a given phrase corresp onds to out of these three. we rst con vert the raw MUC data into a chunk ed form (with subsets of words mark ed as noun phrases, verb phrases, etc ) and map this form to concept graphs as in Fig. 1. We extracted a set of of 4715 training phrases and 1517 test phrases, and once again trained a classi er based on a restrictiv ely expanded feature space as detailed in Sec. (3) and Sec. (4). The generating descriptions used to determine the types of features include: For eac h input graph, sev eral noun phrase nodes may be presen t. Thus eac h instance is presen ted to the fea-ture generating pro cess/k ernel learner sev eral times, eac h time with a di eren t phrase node designated as the focus of inter est . The above descriptions reference this focus as men tioned in Sec. (4).
 As we are trying to predict from a set of k category lab els for eac h phrase, we are faced with a multi-class classi cation problem. We used a standard 1-vs-all metho d, with a winner tak e all gate for testing. In eac h exp erimen t we also trained a classi er using kernel Perceptron with a mo di ed kernel based on the parse tree kernel given in (Collins &amp; Du y , 2002). This kernel trac ks the num ber of common subtrees descend-ing from the root node, whic h is designated to be the `molecule' node in eac h example comp ound in the mu-tagenesis exp erimen t, and the curren t phrase focus of interest node in the named entity exp erimen t. Down-weigh ting of larger subtrees is performed also as in (Collins &amp; Du y , 2002). As our concept graphs may be cyclic, a termination rule is added to stop traversal of eac h graph if a node previously encoun tered on a traversal path is touc hed again. We sho w below the results of training and evaluating eac h type of classi-er. For mutagenesis we train with 10-fold cross vali-dation on the set of 188 comp ounds with 12 rounds of training for eac h fold. For the Named Entity task we train using 5 rounds of training over the entire training set. Results in terms of accuracy 1 are sho wn. The main comparison in the above table is between the feature space represen ted in both col. 1 and 2, and the one represen ted in col. 3. In both exp erimen ts the classi ers learned over the FDL induced feature space perform mark edly better than the the one learned over the `all-subtrees' feature space. Col. 1 is the outcome of running a di eren t classi er on an explicit feature space equiv alen t to the FDL of col. 2. It sho ws that using the explicitly prop ositionalized feature space, we can use other learning algorithms, not amenable to kernels, whic h migh t perform sligh tly better. For the mutagenesis task the FDL approac h enco des the typical features used in other ILP evaluations, in-dicating that man y ILP tasks, where the relational data can be translated into our graph-based struc-ture (Cum by &amp; Roth, 2002), can be addressed this way. The named entity example abstracts away the imp ortan t task of determining whic h phrases corre-spond to valid entities, and performs sub-optimally in terms of classifying the entities correctly due to not ex-ploiting richer features that can be used in this task. However, our goal here in the classi cation tasks is to exhibit the bene t of the FDL approac h over the \all-subtrees" kernel. Namely , that by dev eloping a param-eterized kernel whic h restricts the range of structural features pro duced, we avoid over-tting due to a large num ber of irrelev ant features. We have presen ted a new approac h to constructing ker-nels for learning from structured data, through a de-scription language of limited expressivit y. This family of kernels is generated in a syn tax-driv en way parame-terized by descriptions in the language. Thus, it high-ligh ts the relationship between an explicitly expanded feature space constructed using this language, and the implicitly sim ulated feature space that learning tak es place in when using kernel based algorithms suc h as kernel Perceptron or SVM. In some cases, it is more ef-cien t to learn in the implicitly expanded space, when this space may be exp onen tial in the size of the input example, or in nite. However, we have sho wn that an expanded space of much higher dimensionalit y can de-grade generalization relativ e to one restricted by the use of our syn tactically determined kernel.
 By studying the relationship between these explicit and implicitly sim ulated feature spaces, and the com-putational demands of kernel based algorithms suc h as kernel Perceptron, we have highligh ted the cases in whic h, con trary to popular belief, working in the explicit feature space with the standard learning algo-rithms may be bene cial over kernels.
 Ackno wledgmen ts: This researc h is supp orted by NSF gran ts ITR-I IS-0085836, ITR-I IS-0085980 and IIS-9984168 and an ONR MURI Aw ard.

