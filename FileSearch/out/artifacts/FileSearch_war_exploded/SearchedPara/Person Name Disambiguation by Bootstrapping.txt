 In this paper, we report our system that disambiguates per-son names in Web search results. The system uses named entities, compound key words, and URLs as features for document similarity calculation, which typically show high precision but low recall clustering results. We propose to use a two-stage clustering algorithm by bootstrapping to improve the low recall values, in which clustering results of the first stage are used to extract features used in the second stage clustering. Experimental results revealed that our algorithm yields better score than the best systems at the latest WePS workshop.
 H.3.3 [ Information Storage Retrieval ]: Information Search and Retrieval X  Clustering ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text analysis Languages person name disambiguation, Web people search, clustering
World Wide Web (WWW) search engines are commonly used for learning about real-world entities, such as people. In such cases, users key the name of the target entity in search engines to obtain a set of Web pages that contain that name. However, ambiguity in names ( i.e., many entities having the same name) typically causes the search results to contain Web pages about several different entities.
For example, if we want to know about a  X  X eorge Bush X  other than the former U.S. president, many pages about the former president are returned in the search results, which may be problematic. Depending on the circumstances, we may have to search once more to find Web pages about the target person buried in the numerous unrelated ones.
Hereon, we will use the term  X  X erson name X  to mean a string indicating the name of a person. Many studies have recently been carried out on disambiguating people X  X  names, as was reported at the recent WePS (Web People Search) workshops [2, 3]. In this disambiguation task, the typical approach is to define similarities between documents based on features extracted from the documents, and cluster Web pages returned by search engines for person name queries by using the similarity. In terms of performance, named entities (NEs) have been reported as one of the most ef-fective features for this task [9]. NEs are good features for distinguishing people because they concisely represent real-world concepts related to the people. For example, the person X  X  name Paul Allen or the organization X  X  name Mi-crosoft indicate real-world entities that are related to the person Bill Gates . In addition, we also focus on Compound key words (CKWs) and URLs as additional useful features. These features show similar properties to NEs. For exam-ple, the compound noun chief software architect indicates a concept strongly related to Bill Gates .(Theseexamples can be found in the Wikipedia article on Bill Gates .) Links to relevant pages such as the Microsoft homepage are also good indicators for distinction.

The problem we observed with such features is that they show high precision values but low recall values ( i.e., they are not observed frequently but work as strong evidence for entity identification). One typical approach to improve the recall value is to reduce the threshold value for document similarities, which involves the merging of non-similar doc-uments and typically worsens the precision.

Another typical approach is to use weak features for calcu-lation of document similarities. Features to represent docu-ments for clustering are mainly categorized into two types: strong features and weak features . Strong features have the ability to clearly distinguish between clusters and weak fea-tures do not. We categorized named entities (NEs), com-pound key words (CKWs), and URLs, as mentioned above, as strong features, and single words as weak features. Al-though we can improve the recall value by using weak fea-tures, it typically worsens the precision in the same way as reducing the threshold value discussed above.

We solve this problem by distinguishing reliable weak fea-tures from others by using two-stage clustering algorithm . The two-stage clustering algorithm clusters documents by using only strong features in the first stage, and revises them by using weak features in the second stage. The algorithm gives weights to weak features by using bootstrapping tech-niques, which is popular in the natural language processing community. For example, if a computer scientist and a base-ball player share the same name, words like memory , algo-rithm are reliable weak features for the former, and words like bal l and batting are reliable weak features for the lat-ter. We report that we can use word features effectively by our feature weighting algorithm. We call the clusters pro-duced in the first stage first-stage clusters and the clusters produced in the second stage second-stage clusters .
Bootstrapping is a category of algorithms for instance ex-traction , which start with some seed instances and iterate some process to repeatedly improve the instances (i.e., ex-tracted collections such as a set of movie names )and pat-terns (i.e., linguistic rules that co-occur with movie names, in this case). Typical bootstrapping algorithm selects in-stances and patterns according to some reliability scores .We apply Espresso [20], one of such bootstrapping algorithms, to the person name disambiguation problem. In our case, patterns correspond to weak fea tures and instances corre-spond to documents newly added to clusters, while seed in-stances correspond to documents in each first-stage cluster. Reliability scores give high weights to useful weak features and low weights to useless weak features. The experimen-tal evaluation showed that using this algorithm dramatically improved the performance.

We compared the bootstrapping approach with two base-line methods for the second-stage clustering: compound key words and latent topics. The former uses strong features only, and the latter uses weak features. We observed that the bootstrapping algorithm showed the best performance, which suggests that bootstrapping approaches can get the most out of the ability of weak features.

Two-stage clustering has the same aim as pseudo-relevance-feedback for document retrieval in that both extract new fea-tures from a set of documents in the first stage. The main difference is that our purpose is not to produce words used as queries, but to refine clustering results. We therefore can choose other types of features to be extracted from docu-ments than the features typically used for pseudo-relevance-feedback. Another difference is that we applied feature ex-traction to all the resulting clusters while pseudo-relevance-feedback does not focus on the ambiguity of a query itself. Because our purpose is to distinguish documents related to the same (person name) query, the differences between documents to be separated are often small. We therefore need more careful treatment of features to make distinctions among different clusters.

The word sense disambiguation (WSD) problem, exten-sively studied by the natural-language-processing commu-nity, is a task usually compared to the person name disam-biguation problem [3]. Although some of the results can be used to solve our name disambiguation problem, there are also several important differences between these two disam-biguation tasks. For example, the number of true entities is not known in advance in name-disambiguation problems while in WSD the task is to categorize each word into one of several predefined senses. Another difference is the knowl-edge source. Dictionaries or thesauruses that describe each sense are available in most word disambiguation cases, and are used in the algorithm as sources to discriminate the sense.

The remainder of this paper is organized as follows. Sec-tions 2 and 3 explain our task and describe related work, re-spectively. Sections 4 and 5 explain our framework. Section 6 evaluates our framework with an actual Web document dataset. Section 7 summarizes our work.
Our task, the disambiguation of person names appearing on Web pages, is formalized as follows. The query (target person name) is referred to as q . The set of Web pages obtained by inputting query q to a search engine is denoted by P = { d 1 ,d 2 ,  X  X  X  ,d k } . Each Web-page d i has at least one string q . We assume that q on the same page refers to the same entity. Therefore, person name disambiguation is achieved by document clustering where each cluster refers to a single entity. The input of the algorithm is query q . The output of the algorithm is a set of page clusters.
In this paper, we use the term features to indicate strings extracted from documents. The features include NEs, CKWs, URLs, and words.
Several important studies have tried to solve the task de-scribed in the previous section. Bagga and Baldwin [4] ap-plied the vector space model to calculating similarities be-tween names only using co-occurring words. Based on this, Niu et al. [17] presented an algorithm that uses information-extraction results in addition to co-occurring words. How-ever, these methods had only been tested on small artificial test data, raising doubts as to their suitability in practical use. Mann and Yarowsky [15] employed a clustering algo-rithm to generate person clusters based on extracted bio-graphic data. However, this method was also only tested on artificial test data. Wan et al. [24] proposed a system that rebuilt search results for person names. Their system, called WebHawk, was aimed at practical use like our sys-tems, but their task was somewhat different. Their system was designed for actual queries that occurred frequently. The algorithm in their system was specialized for English person-name queries that consisted of three words: family name, first name, and middle name. They mainly assumed name &gt;  X , and took middle names into consideration, which may have improved accuracy. So the problem setting of them is different from ours.
 In another approach to this task, Bekkerman and Mc-Callum [6] proposed two methods of finding Web pages that refer to a particular person. Their work consisted of two dis-tinct mechanisms. The first was based on a link structure and the second used agglomerative/conglomerative double clustering. However, they focused on disambiguating an ex-isting social network of people, which is not the case when searching for people in real situations. In addition, as our experience is that the number of direct links between pages that contain the same name are fewer than expected, infor-mation on link structures would be difficult to use to resolve our task. Although there may be indirect links ( i.e. ,one page can be found from another page via other pages), it is far too time consuming to find these.

The method proposed by Bollegala et al.[7] used extracted keywords to calculate similarities between documents. They further extracted keywords fr om resulting clusters. How-ever, their research was aimed at keyword extraction itself.
Bunescu et al.[8] reported using Wikipedia knowledge to disambiguate named entities. They used Wikipedia to ex-tract features for supervised learning. Using knowledge sources like Wikipedia is an interesting direction for named entity disambiguation in general, but in our case it is difficult to use them because our targets contain many minor person names that are not defined in Wikipedia. A large workshop for disambiguating person names, called WePS , was held in 2007[2]. The workshop provided a com-mon dataset for research on person-name disambiguation. Sixteen systems were introduced for participation at the workshop. Most of their methods can be categorized into one of the approaches described in the previous section. Their methods typically involved some preprocessing such as POS tagging and NE extraction, calculating similarities be-tween documents, and creating clusters according to the cal-culated similarities. In 2009, the second WePS workshop[3] was also held.

Named entities were one of the most effective features for the task[2][9]. Preprocessing such as filtering out feature values from some  X  X alueless X  pages was also important. The best system used both  X  X leaning X  of documents and selection of useful features such as named entities.

Several studies that have used the WePS corpus have been reported[12][5]. [12] extensively used named entities. They extracted per-son names and organization names from the documents, and calculated the similarities between documents using web counts of their co-occurrence. Although their method per-formed extremely well, counting from the Web required mas-sive amounts of time (because they needed to query the Web search engines about 40,000 times for each cluster), making it difficult to use in real-time systems (they stated their sys-tem was better suited for servers.) [5] proposed a simple algorithm that used Single-Pass Clus-tering with a bag-of-words model, which attained perfor-mance that was comparable to the best system at WePS. Their idea of making use of positions (ranks) in search re-sults and HTML trees to extract relevant blocks for the query was complementary to ours, which can improve the performance of our algorithm. We plan to incorporate their method in our framework in the future.
Previous research has tackled clustering problems with two-stage clustering approaches. Tishby et al. [23] proposed the information-bottleneck method, which finds the optimal clusters according to an information-theoretic measure for cluster goodness. Slonim et al. [22] applied this method to document clustering by a double-clustering approach, which extracts word clusters that preserve information about the document clusters, and used the extracted word clusters in turn to cluster documents. These previous methods were for document clustering in general, which are difficult to be directly applied to person name disambiguation prob-lem because, as discussed in the introduction, general term (word) frequencies or TF-IDF scores are not so effective for this problem compared to general document clustering prob-lems. The two-stage clustering approach was not used in any WePS-1 system. Two systems at WePS-2 used two-stage clustering approaches[18][10], but both of them used no weak (term frequency) features.

Liu et al. [14] proposed extracting effective features from the first-stage clustering results and using the features for the second-stage clustering by feature voting. They observed that named entities and term pairs were salient features for identifying documents in the same clusters, and proposed to use them as well as ordinary term frequency features. One contribution by ours is applying a framework of bootstrap-ping algorithms to model such feature weighting methods via two-stage clustering in an elegant way. Moreover, they used all the features simultaneously. However, our preliminary in-vestigation and previous reports suggest that, as mentioned above, simply blending term frequency features with NE features generally little contribution to person name disam-biguation. This means that the strong features and term frequency features should be used separately. Our two-stage algorithm models this hierarchy of document features. Weak features are used only in the second-stage clustering while the first-stage clustering is pe rformed by using strong fea-tures only.
In this and the next section, we describe our two-stage clustering algorithm. As mentioned in the introduction, we categorize the features for representing documents into strong features , including NEs, CKWs, and URLs, and weak features , including single words, where the former have strong discriminative power and the latter do not. Our method uses only strong features to make the first-stage clusters and uses weak features to supplement them, where this use is guided by the first-stage clusters.

The algorithm proceeds as follows: (1) Make clusters on the basis of the similarities calculated by strong features, and (2) Find documents highly related to each cluster through weak features and add them to the cluster.

This section describes the first-stage clustering in detail, and the next section provides the second-stage clustering. We used lxml 1 and a sentence segmenter 2 to convert HTML files to text files that consist of sentences. We next extracted local text around each query string by using the window sizes set as parameters 3 . Tree Tagger 4 was used http://codespeak.net/lxml/ http://www.answerbus.com/sentence/
We tested four different window-size parameters (50, 100, 200, and all words) in preliminary experiments and chose the best setting for each feature (100 for CKW, all for NE). http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ to add part-of-speech tags to each word. We used Stanford NER 5 for identifying person, place, and organization names. In addition, URL strings were extracted from the original HTML files.
In the first stage, three types of features (strings) are extracted from documents: named entities, compound key words, and URLs. In this subsection, we explain these fea-tures in detail.
We used person names, organization names, and place names as the named entities used as features. They typically represent real-world entities related to the person.
While person names were used as is, some location names and organization names were filtered out by using stop-word lists that list the location/organization names that have high frequencies.
We also use compound key words as features. We describe how to extract them here.

First, we calculate the importance score for compound words in a document with the method proposed by Naka-gawa et al. [16].

The importance score for the compound words is calcu-lated as follows: Let CW (= W 1 W 2  X  X  X  W L ) be a compound word, where W i ( i =1 , 2 ,  X  X  X  ,L ) is a simple noun. f ( CW )is the number of independent occurrences of compound word CW in a document where  X  X ndependent X  occurrence of CW means that CW is not a part of any longer compound nouns. The importance score of compound word CW is LR ( CW )isdefinedasfollows: LR ( CW )= LN ( W i )and RN ( W i ) are the frequencies of nouns that di-rectly precede or succeed simple noun W i .Weextractedthe compound words that have a score higher than the threshold value  X  CKW as CKW features.

This score is defined based on the intuition that some words are used as term units more frequently than oth-ers, and a phrase that contains such  X  X ood X  term units is likely to be important. Figure 1 outlines example statis-tics for the appearance of compound words in a corpus, which include  X  X isaster information X , and  X  X nformation se-curity X  three times each,  X  X nformation system X  once, and  X  X nformation ethics X  two times. In this case, for example, LN ( Information )=3and RN ( Information ) = 3+1+2 = 6.
We extract URLs (in &lt;a&gt; tags) from each document and use them as link features . Link features also include the URL of the document itself. URLs with high frequencies were discarded in the same way as in the location/organization name filtering described in section 4.2.1. http://nlp.stanford.edu/software/CRF-NER.shtml
In this subsection, we describe how to calculate document similarities using the extracted features. We used the over-lap coefficient [21] defined below to calculate similarities be-tween documents.
 where f x and f y are sets of features extracted from docu-ments d x and d y , respectively.  X  overlap is a threshold value to avoid too small denominator values in the equation, which is currently set to  X  overlap = 4 determined on the training data. Similarities by NEs ( sim NE )andbyCKWs( sim CKW ) are defined by this overlap coefficient, where f x is a set of NEs in d x for sim NE ,andasetofCKWsin d x for sim CKW .
The definition of similarities by URLs ( sim URL ) is slightly different. Link similarity sim URL is defined as follows.
In this subsection, we describe how to merge different sim-ilarity scores. First, we define the merged similarity of dif-ferent types of NEs. Next, we define the merged similarity of NEs, CKWs, and URLs.
Different similarity scores are calculated for different types of named entities, namely, person names, location names, and organization names. We take the linear interpolation of these different scores: where  X  P +  X  L +  X  O = 1. We set these values to be  X  P  X 
O &gt; X  L
We define the merged similarity score given the set of these different similarity values. The new similarity score is
Currently, we use the parameters  X  P =0 . 78,  X  O =0 . 16, and  X  L =0 . 06 tuned by the training data.
 Figure 1: Example of statistics for term unit con-nections calculated by taking the maximum of the given similarity values of NE, CKW, and LINK as follows. sim max ( d x ,d y )=max( sim NE ( d x ,d y ) ,
With the document similarities calculated by the methods described above, the algorithm makes clusters of documents.
We used the standard hierarchical agglomerative cluster-ing (HAC) algorithm for clustering. This algorithm starts from one-in-one clustering (each document is a size-one clus-ter) and iteratively merges the most-similar cluster pairs. The parameter of HAC is the similarity threshold value and does not need the number of clusters. We used the average-distance approach for defining inter-cluster distances. In this approach, similarity between clusters C i ,C j is defined as equation (5). sim max ( d x ,d y ) is the above-mentioned sim-ilarity scores.
This section describes the second-stage clustering. It uses the bootstrapping approach which is achieved by matrix multiplication.
Our approach is to apply bootstrapping algorithm to the person name disambiguation. Bootstrapping is a method used originally to extract a set of instances (e.g., country names ) and patterns iteratively. It starts with some seed instances and finds patterns that are useful to extract such seed instances (e.g.,  X * X  X  prime minister X ). These patterns are in turn used to harvest new instances, and from the harvested new instances new patterns are induced. The al-gorithm repeats these steps until convergence criteria are fulfilled.

Espresso [20] is a well-known algorithm for information ex-traction that was proposed for harvesting semantic relations. It finds in documents word pairs that have relations simi-lar to the given seed pairs, e.g., finds a pair (Ford, Nixon) given the pair (Bush, Regan). 7 It extracts instances and ex-traction patterns iteratively, and selects the instances and patterns according to the reliability function defined based on self-mutual-information values. Komachi et al. [13] pro-vided a theoretical analysis for Espresso by representing it as HITS-like matrix multiplication. We hereinafter borrow this matrix representation for the Espresso algorithm from [13]. They represent Espresso algorithm 8 by the matrix multipli-resents strength of connections between instances and pat-terns, and vector i represents a cluster of instances where i is the weights (called reliability )tothe j -th instance. Start-ing with the initial vector i (0) which represents a cluster
Both have the relation  X  X uccession X . This representation is for the algorithm they call simiplfied Espresso in which some filtering steps after each iteration are omitted from the original Espresso. The bootstrapping algorithm we used is also this simplified Espresso algorithm. of seed instances, we can calculate the weights of instances obtained by bootstrapping by multiplying M, M T ,andthe normalizing factor 1 | I || T | to the initial vector repeatedly.
We try to apply it to the person name disambiguation problem. In our problem settings, instances and patterns discussed above correspond to documents and (weak) fea-tures . Given the first-stage clusters, the algorithm regards them as seed instances, finds weak features related to them, and finds new instances (new documents, in our case) by using the weak features (as extraction patterns). One dif-ference between our representation and the above one is that we use matrix R instead of vector i becausewehavemore than one cluster. In our matrix representation, each column vector in matrix R represents each cluster. We can represent updating of clusters simultaneously by using R.

If we define a feature-document matrix P, which has strength of relations between the i th feature and j th document in its (i,j) element and denote a document-cluster matrix by R the results in practice.) Figure 2: Second-Stage Clustering by Bootstrap-ping
Figure 2 illustrates the meaning of this matrix multipli-cation. Here, feature f i is connected to document d j if f is contained in d j (i.e., the element p i,j in feature-document matrix P is not zero). Documents d 1 , d 2 , d 3 ,and d 4 in the same initial cluster and this is represented in the document-cluster matrix by setting r (0) 1 ,k = r (0) 2 ,k r ,k = 1 if the cluster ID is k . Such clustering information is propagated to the feature-cluster matrix R (0) f,C through the document-feature matrix P . That is, feature-cluster re-lation weights are obtained by multiplying R (0) d,C by P. In resulting weight matrix R f,C , features strongly related to the k -th cluster are given high weights in the k -th column. After that, new document-cluster matrix R (1) d,C is obtained by multiplying R (0) f,C by P T , which propagates the feature-cluster relation weights to the new document-cluster relation weights.
This section describes our bootstrapping algorithm. Here, we assume that first-stage clusters of size 2 or more are seed instances . The remaining documents (i.e., documents Algorithm 1 Bootstrapping Algorithm for Person Name Disambiguation Procedure: D, F, R (0) D Step-1: // Calculation of Feature-Document Matrix P P[ f, d ]= where max pmi =max(P[ f ,d ]) ( f  X  F, d  X  D ) for t  X  0 ,  X  X  X  ,T  X  1// T : Number of Iterations endfor Step-4: where { C | ( C  X  X  X | C | &gt; 1)  X  C (0) d } endfor Define C ( T ) based on C d . not connected to any other document) are the sources from which the algorithm extracts new instances for each cluster.
Algorithm 1 shows our bootstrapping algorithm. Here, P is a feature-document matrix, R ( t ) D = { r d,C } is a document-cluster matrix, and R ( t ) F = { r f,C } is a feature-cluster matrix. Our definition of document-feature matrix P is the same as in the Espresso [20] algorithm, which uses the self-mutual information. The algorithm updates R ( t ) D and R ( t tively by multiplying P by P T , resulting in refinement of the document-cluster matrix. The initial document-cluster matrix R (0) D is generated by setting r (0) d,C =1when d the first-stage clustering result C and r (0) d,C = 0 if otherwise.
The algorithm is explained in detail below. 1. In step 1: A feature-document matrix P is generated. 2. In step 2: A feature-cluster matrix R ( t ) F is calculated 3. In step 3: A document-cluster matrix R ( t +1) D is cal-4. In step 4: Find cluster C  X  X  for each document d
The algorithm iterates steps (2) and (3), and the final result C is generated from document-cluster matrix. In this section, we report our experimental results on the WePS-2 data set.
We used the latest dataset for person name disambigua-tion: WePS-2 clustering task data set. The WePS-2 test set 9 consists of 30 names, each of which has 150 pages. The all-in-one baseline is the result when all documents belong to one cluster. The one-in-one baseline is the result when each document is a size-one cluster. The combined baseline is a mixture of these two baselines, where each document belongs to one cluster from the all-in-one baseline and an-other cluster from the one-in-one baseline. Note that the same document can refer to two or more entities in these data sets.

We used two baselines for the evaluation of the second-stage clustering. The first baseline is the TOPIC algorithm proposed by Ono et al.[19]. This algorithm estimate the latent topic (e.g., sports, computer science, arts, etc.) for each document by using the probabilistic generative model called the Dirichlet process unigram mixture where param-eters are initialized by the first-stage clustering results. If two clusters share the same topic 10 , they are merged into one cluster.
 The second baseline is the CKW algorithm proposed by Ikeda et al.[11]. This second-stage clustering algorithm re-extracts CKWs from the resulting clusters of the first stage in contrast with the fact that CKWs for the first stage clus-tering are extracted from each document . The concept be-hind the algorithm is that CKWs extracted from clusters are more reliable than the ones extracted from documents because clusters contain more words than documents, and therefore the former provides more reliable statistics of term frequency for the keyword extraction algorithm than the lat-ter. If documents in small-size clusters share the same re-extracted CKWs with large clusters, these documents are moved to the large cluster.

We used 1-gram and 2-gram features, excluding some stop words, to represent the documents for the second-stage clus-tering. The weight for each feature was defined by using TF-IDF scores, where the IDF values were estimated by using Web 1T 5-gram 11 .
The extended B-Cubed measure[1] was the measure adopted at the second WePS workshop held in 2008 X 2009[3].
Assume that L ( e )and C ( e ) correspond to the collect class and machine-predicted class of e . Multiplicity precision and recall between e and e are calculated as
MPrec ( e, e )= Min (
Extended B-Cubed precision (BEP) and recall (BER) are calculated as http://nlp.uned.es/weps/weps-2-data/
The topic of each cluster is defined as the most frequent topic for documents in the cluster. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?catalogId=LDC2006T13
Here, Avg the harmonic mean of the two, F = 1 1 We tested our algorithm on the WePS-2 data set with BEP-BER measures.

WeshowtheresultsinTable1. Wehaveaparameter  X  f that determines the threshold value in the HAC algo-rithm used in the first-stage clustering. The value of  X  f determined by using the training data.  X  X RIGINAL X  represents the results when we used the first-stage clustering only.  X  X OPIC X  and  X  X KW X  are base-lines for the second-stage clustering.  X  X OOTSTRAP X  rep-resents our bootstrapping-based algorithm. The features used in the second-stage clustering are represented by  X 1-gram X  and  X 2-gram X , and the number of iterations in the second-stage clustering is T .

The two-stage clustering algorithm with CKW improved the results from ORIGINAL. Using topic features also im-proved the performance but the improvement was small. We believe the reason the TOPIC features did not work well is that TOPIC treats all single words equally without giving any weight to each word, while the bootstrapping approach can give weight to each weak feature through the calculation of reliability scores. BOOTSTRAP with 1-gram features and T = 1 performed the best (0 . 85 in B-Cubed f-measure.) among all the systems. We believe that one of the reasons our method performed well was that the first-stage cluster-ing had already shown good precision values for the WePS-2 data set. This was good for our algorithm, which assumed high-precision clusters at the first stage.

The score of  X  X OOTSTRAP X  was better than that of  X  X KW X  by 0.04 points, which was even higher than the score of top-ranked system at WePS-2.

The use of 2-gram features did not contribute to improv-ing the performance. (We observed no changes in clusters at the first iteration, so the second and third iterations were omitted.) We believe the reason was that the 2-gram fea-tures were  X  X parse X  and our bootstrapping algorithm re-quires a large number of appearances of weak features. In-creasing the iteration number for  X 1-gram X  makes the pre-cision values severely worse, and the improvement of recall values could not cover this performance drop. We believe the reason was that, in T =2 , 3 cases, weak features were overestimated and documents having only weak relations to each cluster were merged into the cluster.

We also tested the Von Neumann Kernel and Graph Lapla-cian reported in [13], which models the infinite number of it-erations in efficient ways, but no improvement was observed. This result and the previous result that T = 1 was better than T =2 , 3 suggest that it is sufficient to do the iteration only once, which is the weight propagation from documents to features followed by the weight propagation in the oppo-site direction (i.e., from features to documents.) Iterating this procedure more than once causes too much overspread-ing of features, which results in low precision. However, it is possible that there is the optimum solution is in between T =0and T = 1, or between T =1and T =2. Search-ing for these  X  X ntermediate X  states by introducing a method to slow the propagation of weights is an interesting future direction.
We proposed a new algorithm for person name disam-biguation. It consists of two stages where the results of the first-stage clustering are used to extract features for the second-stage clustering by applying the Espresso boot-strapping algorithm. We compared our method with various methods including two baseline two-stage clustering meth-ods and WePS top systems and found that our method out-performed all of them. Future work includes testing our method on a greater variety of domains and improving the preprocessing algorithm to filter out useless documents. Acknowledgments This work was supported in part by MEXT Grant-in-Aid for Scientific Research on Priority Ar-eas:  X  X yber Infrastructure for the Information-explosion Era X  and MEXT Grant-in-Aid for Scientific Research (A). [1] E. Amigo, J. Gonzalo, J. Artiles, and F. Verdejo. A [2] J. Artiles, J. Gonzalo, and S. Sekine. The [3] J. Artiles, J. Gonzalo, and S. Sekine. WePS 2 [4] A. Bagga and B. Baldwin. Entity-based [5] K. Balog, L. Azzopardi, and M. de Rijke. Personal [6] R. Bekkerman and A. McCallum. Disambiguating web [7] D. Bollegala, Y. Matsuo, and M. Ishizuka. Extracting [8] R. Bunescu and M. Pasca. Using encyclopedic [9] E. Elmacioglu, Y. F. Tan, S. Yan, M.-Y. Kan, and [10] M. Ikeda, S. Ono, I. Sato, M. Yoshida, and [11] M. Ikeda, S. Ono, I. Sato, M. Yoshida, and [12] D. Kalashnikov, R. Nuray-Turan, and S. Mehrotra. [13] M. Komachi, T. Kudo, M. Shimbo, and [14] X. Liu, Y. Gong, W. Xu, and S. Zhu. Document [15] G. S. Mann and D. Yarowsky. Unsupervised personal [16] H. Nakagawa and T. Mori. Automatic term [17] C. Niu, W. Li, and R. K. Srihari. Weakly supervised [18] R. Nuray-Turan, Z. Chen, D. Kalashnikov, and [19] S. Ono, I. Sato, M. Yoshida, and H. Nakagawa. Person [20] P. Pantel and M. Pennacchiotti. Espresso: Leveraging [21] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [22] N. Slonim and N. Tishby. Document clustering using [23] N.Tishby,F.C.Pereira,andW.Bialek.The [24] X. Wan, M. L. J. Gao, and B. Ding. Person resolution
