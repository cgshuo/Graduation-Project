 People X  X  beliefs, and unconscious biases that arise from those be-liefs, influence their judgment, decision making, and actions, as is commonly accepted among psychologists. Biases can be observed in information retrieval in situations where searchers seek or are presented with information that significantly deviates from the truth. There is little understanding of the impact of such biases in search. In this paper we study search-related biases via multiple probes: an exploratory retrospective survey, human labeling of the captions and results returned by a Web search engine, and a large-scale log analysis of search behavior on that engine. Targeting yes-no questions in the critical domain of health search, we show that Web searchers exhibit their own biases and are also subject to bias from the search engine. We clearly observe searchers favoring pos-itive information over negative and more than expected given base rates based on consensus answers from physicians. We also show that search engines strongly favor a particular, usually positive, per-spective, irrespective of the truth. Importantly, we show that these biases can be counterproductive and affect search outcomes; in our study, around half of the answers that searchers settled on were ac-tually incorrect. Our findings have implications for search engine design, including the development of ranking algorithms that con-sider the desire to satisfy searchers (by validating their beliefs) and providing accurate answers and properly considering base rates. In-corporating likelihood information into search is particularly im-portant for consequential tasks, such as those with a medical focus. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Search process, Selection process. Beliefs; Biases; Search interaction; Health search. Information scientists have analyzed the cognitive mechanisms be-hind the search for information, including the development of mod-els for how information needs emerge [4][37] and how they evolve during search [23][24]. The models developed typically focus on a cognitive actor, and that actor X  X  interactions with information ob-jects and systems, within a context [17][30]. Although they may model historic search interests, these models do not consider prior beliefs about outcomes and associated likelihoods, or biases arising from those beliefs. Biases in cognition may lead people to create beliefs based on false premises and behave in a seemingly irrational manner [21], e.g., exhibiting preference for information in support of their position over information refuting it, irrespective of factual correctness [2][3][21]. For example, although there is a definite an-swer to the yes-no question Can tea tree oil treat canker sores? , and that answer is yes , a health seeker may favor a particular out-come in light of their beliefs about the value of the oil, and seek or unconsciously prefer disaffirming information. Biases can be observed in information retrieval (IR) in situations where searchers seek or are presented with information that signif-icantly deviates from true likelihoods . In IR, the term  X  X ias X  has been associated with search engine functionality (e.g., caption gen-eration [47]) and user preferences (for higher ranked results [20] or particular domains [16]), but not seemingly irrational behaviors or skewed result lists as we focus on here. Although the motivation between previous studies and ours differs, the impact of the biases can be similar: both types cause searchers to be interested in partic-ular results for reasons beyond relevance. As such, biased beliefs can also affect aggregated behavioral signals (e.g., result click-through) used by search engines for ranking [1][19]. As we show here, when an answer to a yes-no medical question is provided in the top result from a search engine, more than half of the time that answer is incorrect. Search engines need to consider both satisfying users (by surfacing results that reinforce biases, but could also be factually incorrect) and providing correct answers (e.g., performing bias mitigation to provide more rational result sets that accurately reflect background probabilities). For important searches such as those in the health domain X  X here people have been shown to in-terpret the result ranking as an ordering of condition likelihoods [44] X  X earch engines may wish to favor accuracy over satisfaction given the potentially-serious ramifications of an incorrect answer. In this paper we present the first comprehensive study of beliefs and biases in search, highlighting the potentially counterproductive effects of biases on answer accuracy. Focused on the health do-main, we demonstrate the large extent to which bias is evident in search behaviors and results, and we argue that biases need to be considered by the IR community in ranking and interaction support. To study these biases we employ a range of methods, including an initial exploratory survey to gain insight into prior beliefs and how they change as a result of search, human labeling (by crowd-sourced judges as well as physicians), and a log analysis of Web search behavior. We study biases in the context of yes-no questions because we can attain direct ground truth answers via expert con-sensus, allowing us to study answer accuracy, central to our defini-tion of bias. As we show, yes-no questions are also popular (2% of the queries in our search engine log sample were yes-no questions) and they offer an excellent opportunity for search engines to help users since there is often a likely answer. This study answers the following research questions: (i) Do people X  X  beliefs in different outcomes ( yes versus no in this case) change (and how) as a result of searching? (ii) To what extent are search engine results biased in favor of particular outcomes? (iii) To what extent do biases appear to affect search behaviors? and (iv) What is the impact of these fac-tors on search outcomes, specifically answer accuracy? Answers to these questions help us better understand the role of biases in search and can inform the design of IR methods to consider biases, e.g., in deciding when accuracy should feature in result ranking, or when systems should diversify the result set to expose new perspectives. The remainder of the paper is structured as follows. Section 2 de-scribes related work in the psychology and IR research communi-ties. Section 3 presents the results of a large exploratory survey of biases in beliefs during the search process. Section 4 describes the identification of yes-no question content labels and definitive an-swers necessary for our analysis. Section 5 describes the extent to which search engine result pages (SERPs) and the result ranking is skewed toward a particular answer for the yes-no questions we se-lect. Section 6 focuses on search behavior, including the accuracy of the answers that searchers appeared to find. We discuss the find-ings and their implications in Section 7 and conclude in Section 8. Research in a number of areas is relevant to the work described here, namely: (i) cognitive biases and decision making, (ii) utilizing search behavior to rank results, (iii) biases in search interaction, and (iv) personalization and diversity. We consider each area in turn. Numerous models of the search process have been developed over the past few decades [4][17][23][24][30]. These models primarily focus on information need formulation, search interaction, resolv-ing uncertainty, and contextual influences on the search process. However, these models largely ignore situations when seemingly irrational search behavior is observed, e.g., people accepting factu-ally incorrect or unsupported information because it reinforces a particular belief they hold. Cognitive biases are defined as a pattern of deviation in judgment occurring in particular situations, where a deviation may refer to a difference from what is normatively ex-pected, either by the judgment of people outside of the situation or by independently verifiable facts [21]. Biases play a central role in human judgment and decision making and span a number of differ-ent dimensions (see [2][3][21] for summaries). Other models of ir-rational human behavior are plentiful, including related concepts such as bounded rationality [33]. Biases can be difficult to distin-guish and are not necessarily negative. For example, they can form information-processing shortcuts leading to more effective actions in a given context or enable faster decisions when timeliness is more valuable than accuracy, e.g., the availability heuristic allows rapid assessments of likelihoods based on ease of recall [15][21]. Confirmation bias describes people X  X  unconscious tendency to pre-fer confirmatory information [26][41]. This can make searchers more likely to employ positive test strategies, where people seek evidence that supports their hypothesis and disregard evidence that refutes it [22]. Moving beyond behavior, biases in the results re-turned toward one perspective can reinforce existing beliefs and leave searchers susceptible to the effects of information availability [33][39], where people X  X  ability to make rational decisions is lim-ited by the information that they have access to [33] and the ease with which information is recalled (via the engine in this case) maps to likelihood estimates [39]. Although there has been extensive re-search on cognitive processes in search, we are the first to present a detailed investigation of beliefs and biases in search. The role of biases in search were discussed in previous studies on  X  X yberchon-dria X  [44], but were not studied as directly as we do in this paper. Over the past decade, a number of authors have proposed methods for using behavioral data of various forms X  X ncluding queries, re-sult clicks, and post-click navigation behavior X  X o improve result relevance [1][4][19]. Despite its utility for ranking, search behavior can still be affected by biases related to the ordering or presentation of results on the SERP, or user preferences for particular resources. Joachims et al. [20] analyzed searchers X  decision processes via gaze tracking and compared implicit feedback from search-result clicks against manual relevance judgments. They found that clicks are in-formative but biased (favoring results at higher rank positions), yet relative result preferences derived from clicks mirror searchers X  true preferences. Searcher models can capitalize on this consistent behavior to infer search result attractiveness and document rele-vance (e.g., [10]). Over time, searchers X  focus of attention on top-ranked content can create a vicious cycle whereby clicks reinforce popular results [8], although this popularity bias may be offset somewhat by the heterogeneity of searchers X  topical interests [14]. Other factors beyond rank position can introduce bias into search. Clarke and colleagues [9] introduced click inversions to study fea-tures of the captions that increase caption attractiveness. Yue and colleagues [47] studied the effect of caption attractiveness, defined for their study as the presence and absence of bolded terms in the titles and snippets of the caption. They show via experiments con-ducted on the Google Web search engine substantial evidence of presentation bias in clicks towards results with more attractive ti-tles. Beyond captions, Ieong and colleagues [16] studied the effect of domain biases, whereby a result is believed to be more relevant because of its source domain. They show that this bias exists in click behaviors as well as human judgments, and that domain can flip caption preferences around a quarter of the time, independent of rank or relevance. However, these biases are unrelated to biased beliefs about task outcomes and the role of search engines in rein-forcing those beliefs, as we target in this study. Beyond aggregating search behavior across all users, search en-gines can also cater to the individual needs of their users via per-sonalization, presenting the opportunity to model their preferences within the current session [42][46] and across multiple sessions [34][38][45]. However, these models focus on term or topic level interests rather than degrees of belief in particular outcomes. This provides a limited view on factors that could affect preferences, and considering beliefs and biases could improve personalization. Re-search on result diversity (e.g., [25][40][48]) is also relevant but typically focuses on aspects such as topical variance, rather than results skewed toward one perspective or biased search behaviors. The research presented in this paper extends previous work in a number of ways. First, we are the first, to our knowledge, to exam-ine biased beliefs and skewed search engine result lists in IR. Sec-ond, through a detailed study of yes-no questions (involving a sur-vey, human labeling, and large-scale log analysis  X  the latter two focused on the important domain of health search) we empirically demonstrate and quantify the effect of biases in results and search behaviors. Finally, we establish the effects of biases on search out-comes, specifically the accuracy of answers that people find. We begin by describing the retrospective survey that we performed to better understand the role of biased beliefs in Web search. This provided us with insight into people X  X  beliefs before searching as well as afterwards, their perceptions of the process, and their ra-tionales for their actions. An invitation to complete an online sur-vey was distributed via email to a sample of employees within Mi-crosoft Corp. The sample comprised employees in a range of tech-nical and non-technical roles. We were particularly interested in those who had recently pursued yes-no answers using search en-gines. Since there are only two opposing outcomes, such questions provide a means of quickly and easily measuring degrees of belief. A total of 198 respondents (23.1% of all respondents) reported is-suing such a question to any Web search engine during two weeks immediately prior to survey distribution. We asked participants to recall that particular searching episode, and provide the yes-no query issued, as well as the motivation behind the search. Question queries reported by respondents included  X  Does chocolate contain caffeine?  X  and  X  Are shingles contagious?  X . We also asked respond-ents questions about their experience during their recalled search. We believe that there is value in using retrospective analysis to ex-plore search beliefs, even though our method depends on partici-pant recollections and they already knew the outcome of the re-called search episode. Alternative methods such as in-situ judging are intrusive and may draw undue attention to beliefs at query time, asking respondents to create yes-no questions at survey time lacks realism, and third-party judges issuing these questions as queries could not accurately consider searchers X  true beliefs or motivations. The focus of our study was on how participant beliefs changed as a result of performing the search that they recalled. To do this, we asked participants to quantify their level of belief in each of the outcomes ( yes and no ) before and after the search. Before searching: Given that they recalled a recent yes-no ques-tion searching episode, survey respondents were then asked to  X  Rate your relative prior belief about the likelihood of each out-come before you used the search engine  X  on a nine-point scale, where extreme values corresponded to the response options yes and no , and the mid-point corresponded to an equal belief in yes and no . This allowed us to obtain a precise distribution of beliefs across the range of response options. Given small counts in some cells, and to more easily identify trends, we created five response groups: yes , lean yes , equal , lean no , and no . The lean yes and lean no groups comprise the three ratings between equal and yes and no respec-tively. Figure 1a has the distribution of reported recalled beliefs. Figure 1a provides evidence of a positive skew in respondents X  be-liefs before they search. Specifically, 58% of respondents leaned toward yes and only 21% leaned toward no (the remaining 21% re-ported an equal belief in both outcomes). This corroborates prior research on confirmation discussed earlier in the related work sec-tion (e.g., [26][41]). In addition, analysis of the explanations of-fered for leaning yes or no revealed that 47% of respondents explic-itly cited confirmation of their beliefs as the primary search activity they were performing (e.g.,  X  X  was verifying X ,  X  X  wanted to check X ). After searching: We also wanted to understand the impact of search on respondents X  beliefs. To do so we asked respondents to  X  Rate your relative posterior belief about the likelihood of each outcome once you finished searching  X  The distribution of responses appears in Figure 1b. This distribution shows less uncertainty, with more responses at yes and no . One possible explanation for this is that people were more informed after they examined results. There is more of a split between yes and no , with less than half of respond-ents still exhibiting some uncertainty (i.e., 48% believed lean yes , equal , or lean no ) compared to more than three quarters of respond-ents (77%) before searching. The fraction of searchers who be-lieved yes after the search is more than double that of any other outcome, suggesting that respondents mostly shifted their prior be-liefs from lean yes to yes . However, this analysis is insufficient to understand the belief dynamics in individual respondents. Table 1 shows the fraction of users transitioning to a particular be-lief after they search (columns) conditioned on their beliefs with respect to yes and no before they searched (rows). For example, 77% of searchers who strongly believed yes beforehand, still be-lieve this afterwards, with a similar value (78%) for no . These find-ings suggest that if searchers are certain about their beliefs initially then they are likely to remain unchanged following the search. Re-search on anchoring-and-adjustment has shown that people typi-cally perform little revision to their beliefs, especially if those be-liefs are strongly held [21]. To our knowledge, our work is the first demonstration of this heuristic being applied in a search setting. There are other noteworthy findings from Table 1: 1. Respondents leaning yes or no before searching either retain 2. Respondents with a belief in yes or no (definite or lean) at the 3. Respondents who are uncertain at the outset ( equal ) are There are a few possible explanations for these findings: (i) search-ers are drawn to information supporting or confirming their prior beliefs (as suggested in survey remarks) and are therefore unlikely to change their opinion as they are not exposed to contradictory ev-idence, (ii) search engines rank results with yes higher in the list leading searchers to be more likely to view those results, or (iii) the prior distribution of correct answers to the yes-no questions sub-mitted to search engines is skewed positive. We answer the first question using survey responses and later log analysis. The others require an assessment of the content of the SERPs and landing pages, and a correct answer to the questions posed. We do not ad-dress these directly in the survey, since respondents may not be able to make the assessments objectively. However, later in the paper we describe how we obtained direct labels for SERPs and results and answers for a subset of yes-no question queries. 
Table 1. Fraction of respondents reporting different beliefs following search (columns) given the belief before (rows). Belief before search (a) (b)
Figure 1. Distribution of (a) prior and (b) posterio r reported recalled beliefs about the answer (ranging from Yes to No). (a) (b) Answers found using search engines can affect action in the world. It is therefore important to understand whether people report find-ing answers and how confident they are in the accuracy of those answers. Overall, 85% of respondents reported finding an answer to their yes-no question by searching, with 92% of those who found an answer reporting that they were confident or extremely confident in answer accuracy. These high percentages are encouraging if the search engine is correct, but as we show this is not always the case. Since searchers frequently examine multiple results [11], we were also interested in the motivation behind additional seeking once an initial answer was found. We asked respondents:  X  If you found an answer early in your search, did you still consider multiple results before settling on your final answer?  X  In total, 49% of our respond-ents reported viewing multiple answers. We then asked them to ex-plain the motivation behind that additional searching (Table 2). The findings in the table demonstrate that once a searcher finds an initial answer their follow-up searching is likely to be related to seeking confirmatory information. Overall, 84% of respondents reported that confirmation of the initial answer was their motivation, com-pared to the 43% and 17% who provided reasons linked to testing the robustness of the initial answer with contradictory information. The survey was part of an initial exploration of belief dynamics and their impact on search behavior. There are two main takeaways from the findings that are relevant to our focus in this paper: 1. Respondents reported confirmatory search behaviors by 2. If people are unsure (i.e., have equal belief in yes and no ) Search engines may be supporting these biased processes and we need to explore this further as part of our study. While the survey findings provide motivation to study search-related biases, to study them in detail we need ground truth answers for the questions that people pose and judgments reflecting the results surfaced by search engines. We performed an extensive follow-up study using human labeling of SERPs and results, and a log analysis of search behavior for yes-no questions. Focusing on the medical domain, given its importance and since we could gather ground truth answers, we ob-tained answers from physicians and judgments from crowdworkers. We first describe the automatic identification of yes-no questions from logs, the process of obtaining answers to a subset of those questions from physicians, and the process of gathering judgments about answers in SERP captions and landing pages for that subset. We automatically extracted yes-no questions from a random sam-ple of the logs of queries issued by 2.3M users of Microsoft Bing during a two-week period from September 2012. The data includes user identifiers, timestamps, queries, result clicks, and the captions (titles, snippets, URLs) of each of the top 10 results. To remove variability from cultural and linguistic variation in search behavior, we only include log entries from searchers in the English-speaking United States locale. We also obtained the HTML content of each of the top 10 results from the engine index during the same period. Yes-no questions are an interrogative construction where an answer of yes or no is required. We automatically analyze the logs search-ing for questions asked as queries using variants of  X  X e, X   X  X ave, X   X  X o X , or a modal verb (e.g.,  X  X an, X   X  X ill X ). We also created a list of stop phrases (e.g.,  X  X o not call, X   X  X o it yourself, X   X  X ill smith X ) to remove frequent non-yes-no questions from our data. During ex-traction, we normalized queries via lowercasing, whitespace trim-ming, and punctuation removal. We identified 3.4M yes-no ques-tion queries using this method. The yes-no question queries in our set comprised around 2% of the total query volume in our sample and covered a wide range of topics. Since we are interested in the assessment of answer accuracy, we needed to filter the yes-no ques-tions to a topic where we could obtain reliable answers. To this end, we restricted the questions to those with a medical intent and sought answers to them from trained medical professionals (physicians). To help ensure data quality, we did the following additional filter-ing: (i) selected SERPs with same 10 results and same result order-ing across all instances of the query in the two weeks, and; (ii) fo-cused on query instances that were either the only query in the ses-sion or the terminal query in the session with no preceding queries with query-term overlap. Filter #2 gave us more certainty that users had terminated their search with that query. This is important in later analysis when we infer answer attainment from clicked results. Given the large number of questions selected as described in the previous section, we needed a way to automatically label whether questions had medical intent. To do this we used a proprietary clas-sifier from Bing. The classifier labeled 2.5% of yes-no questions as having strong medical intent (threshold &gt; 0.8). This aligns with prior analysis of query topics, which showed that approximately 3% of search queries are medical [45]. From this set, we randomly selected 1000 medical yes-no questions. To remove noisy queries and provide sufficient data from which to analyze search behavior, selected questions had to be issued by at least 10 users. Examples of the questions selected include  X  Do food allergies make you tired?  X ,  X  Is congestive heart failure a heart attack?  X , and  X  Can as-pirin cause blood in urine?  X . We restricted the size of the question set to 1000 questions since we wanted to obtain answer labels from medical professionals, who were time constrained. As stated earlier, to measure biases we needed ground truth answers to the yes-no questions in our set. We employed two practicing phy-sicians as judges. Each judge reviewed the same set of 1000 medi-cally-focused yes-no questions and provided an answer on a three-point scale: yes , 50/50 , no . The judges worked independently and there was no opportunity for discussion between them to resolve disagreements. Judges were encouraged to apply their knowledge and think of the most common scenario/circumstances that could apply when a user types such a question on the Internet. The middle 
Table 2. Reported motivations for considering other answers once candidate answer was found. Multiple reasons permitted. rating (50/50) was only to be used if: (i) there really was an equal split between yes and no in the most common scenario or circum-stances, and/or (ii) more information would certainly be needed to provide an answer. Two other response options were provided: (i) don X  X  know : if the judge did not know the answer, and (ii) n/a : if the query was not medical or was not a yes-no question (possible given the automated identification of yes-no questions from logs). Overall, 4% of the questions were labeled by at least one of the two judges as not being a yes-no question. These questions were ex-cluded from further analysis. Of the remaining 960 questions, the judges agreed on either yes or no as the answer to the question for 674 (70.2%) of them. Seeking a second opinion is common practice in medicine and the 30% disagreement between physicians is sim-ilar to that reported in medical literature [18]. However, if we con-sider the disagreement in more detail, we see that for 14.0 % of the questions (roughly half of the disagreement), judges completely disagreed on the answer (i.e., one labeled yes and the other chose no ). For 15.8% of the questions, one judge was unsure (13.6%) or both were unsure (2.2%). The agreement matrix is in Table 3. The percent of overall agreement across all of the four answer op-tions was 72.2%. The Cohen X  X  free-marginal kappa (  X  ) inter-rater agreement, considering chance agreement between raters is 0.630, signifying substantial agreement. Note that we use the free-mar-ginal kappa because our raters were not forced to assign a certain number of cases to each category [7]. If we only focus on the ques-tions where both judges provided a yes or no response, the percent of overall agreement rises to 83.4%, with  X  =0.668, signifying even more substantial agreement. The only discernible difference be-tween the questions where the physicians disagreed directly on yes and no , was that those with disagreement were much more likely to start with  X  X an X  (e.g.,  X  Can a pinched nerve in neck cause throat pain?  X ). Overall, 49.3% of questions with disagreement started with  X  X an, X  vs. 34.0% of questions with agreement. Since the usage of  X  X an X  denotes possibility , these questions may be more subjec-tive and dependent on the knowledge and experience of the judges. Since there is a high amount of uncertainty for the 50/50 and the don X  X  know categories, and they occur infrequently (e.g., only 12 cases where the experts both believed that more information was definitely required), we focus on the cases where both judges were sufficiently confident to assign a rating of yes or no . Because the questions with yes-no disagreement might highlight contentious, subjective, or difficult questions, we focused on the 674 questions where both judges agreed that the answer was yes or no . Within this set, 55.2% of answers were yes and 44.8% no . These particular per-centages are important because they provide the background prob-abilities, or base rates , of each answer across our data set. Returning to our earlier discussion of why post-search beliefs tend positive (Section 3), this provides supporting evidence that the questions being posed are more likely to be answered affirmatively, although the difference is not as large as the 2:1 ratio between yes and no shown in Figure 1b, perhaps because the question sets differ. Using search logs, we have the ability to analyze results and behav-iors in detail. If search engines or users lean toward yes / no signifi-cantly more than the 55/45 split, this offers direct evidence of bias. So that we could assess the level of skew in SERPs and results, and also study which captions and results people selected, we required judgments on the perspectives offered by captions and results. For this task we used crowd-sourced judges from a pool provided under contract to our organization by Clickworker.com. To suit the geo-graphic and linguistic filtering performed on the question queries, all judges resided in the United States and were fluent in English. The judges were required to read task guidelines and successfully complete a qualification test similar before they could start judging. The task required judges to assess whether a caption (title, snippet, and URL) presented on a SERP suggested an answer to the current yes-no question. Judges were provided with a yes-no query such as  X  Can Flonase make you tired?  X  and a single caption. The caption may answer the question with yes or no directly (e.g.,  X  ... can Flo-nase make you tired? Yes ...  X ) or suggest an answer somewhat in-directly (e.g.,  X  ... Flonase is unrelated to tiredness ...  X ). Captions can also contain contradictory answers, and no answer. Judges were required to review the caption with respect to the question and pro-vide one of the following ratings about answers to the question: 1. Yes only (affirmative): Caption only contains content an-2. No only (negative): Caption only contains content answering 3. Both (affirmative and negative): Caption contains content 4. Neither: Caption contains neither affirmative nor negative Figure 2 shows examples of each of the caption labels for a variety of queries. These examples are taken from the guidelines provided to crowd-sourced judges to help them understand the task. Between three and five judges rated each of the 6,740 captions un-der consideration to achieve a consensus comprising at least three judges with the same rating. In total, consensus was reached for 96% of captions, with 85% of captions attaining agreement with only three judges. We only use those captions with agreement. In addition to judging the captions that the search engine presented on the SERP, we employed a similar methodology to judge the full text of the results returned by the search engine. As mentioned ear-lier, the results were retrieved from the search engine X  X  index at the time that the query logs were recorded. We once again used crowd-sourced judges from the same pool, between three and five judges (three needed to attain consensus), as with the captions. Judges la-beled each page based on whether it contained an answer and the type of answer it contained. The judges provided one of the same four ratings as they provided for the captions. Judges could provide an error label if the page could not be loaded, although this was seldom used (for &lt; 2% pages). In total, consensus was achieved for 92% of pages (excluding errors); 81% of results had agreement among three judges and did not need further judgments. This label-ing task was slightly more difficult than caption labeling because the full page had to be inspected and some answers may be missed. We suspected that these tasks (which were recognition oriented) would not require the specialized medical training. To verify the correctness of this decision we asked our medical experts and our 
Table 3. Agreement matrix in the responses from the two physicians. Highlighted = yes-no agreement between judges.
 Physician 1 crowd-sourced judges to label the same sample of 100 captions and 100 landing pages with the labels described above, and we assessed the agreement between them. We found a high level of agreement between the physicians (both  X   X  0.886), as well as between the two physicians and the consensus labels provided by the crowd-sourced judges (both Fleiss X  multi-rater  X   X  0.853 [13]). The find-ings suggest that for these tasks, the crowd-sourced judges provide similar rating quality to trained physicians (mirroring [34]). There were only 6% labeling discrepancies between captions and their as-sociated landing pages (e.g., caption label = both , page label = yes -only). Explanations for these differences include caption generation effects or differences in judges used for caption and result labeling. The data described in this section allows us to study the distribution of answers on SERPs and in the results themselves, as well as search interaction. Given the physician answers, we can also ana-lyze the accuracy of the answer pages found and condition analyses of SERPs, results, and behaviors on the ground truth answers. Given the data described in the previous section we can analyze the SERPs generated by the search engine (using the caption judg-ments) and the top-ranked results returned by the engine (using the landing-page judgments). In this analysis we focus on the presence and distribution of yes and no answers across the 674 queries in the agreement set. Recall from earlier that we focused on queries where the SERP was unchanged over the two weeks of logs, giving us exactly one SERP per query. Studying the role of captions is par-ticularly important as they play a critical role in searchers X  decision making [9][47]. We used the physician answer to study significance in two ways: (i) changes in the distributions across answer types when conditioned on the answer types (e.g., does fraction of yes -only top-ranked results increase when the ground truth (from expert consensus) is yes ?), and (ii) how closely does the distribution of answer types model the truth? For (ii), if there was no bias, the dis-tribution of yes and no would resemble the 55/45 split in our data. We first wanted to determine what fraction of the results (SERP captions and landing pages) contained an answer of yes , no , both , and neither to the yes-no question posed by the user. Table 4 shows the fraction of SERPs that contain each type of answer, in terms of occurrences in captions and occurrences in the full-text of the re-sults. The table clearly shows a bias toward yes . Since these are per-SERP statistics they do not reveal much about the concentration of the answers in the SERP or their relative ordering by the search engine. Table 5 presents the fraction of all captions and results for the yes-no question containing an answer. The findings show that results and captions containing yes are much more likely to appear in the results than those with no; around 3-4 times more likely across all captions and all SERPs. There were no significant varia-tions in those percentages when we conditioned using the physician answer, suggesting that for these questions the search engine is in-sensitive to the truth (very little change in the values as we varied the physician answer). Focusing on yes-only and no-only captions and results, McNemar X  X  chi-squared tests and Z -tests of proportions showed differences between yes-only and no-only in all cases (all p &lt; 0.01) and differences from the base rates (i.e., 55/45) at p &lt; 0.01 in all cases other than no -only in Table 5 ( p = 0.54), which was similar to the no answer prior in our dataset (i.e., 45%). The previous section examined answer presence on SERPs and in results. In this section we examine the distribution of answers in captions and results as a function of rank position within the top 10 results. Table 6 presents the average rank position of the first an-swer of each type in the result list. We focus on the first answer to help ameliorate the effects from different volumes of answer types in the top 10 results. Since people inspect the results from top to bottom [20], the first occurrence of each answer in the captions is particularly important anyway. The table shows that the topmost captions and the results with yes are ranked above those with no and other answer labels. We performed one-way analyses of vari-ance (ANOVA) for each of the two sources (captions and results), and the findings revealed significant differences between the rank of yes and no and all of the other answers for both captions and the full text of the results (both F (1,4665)  X  6.85, both p &lt; 0.01). The analysis also shows that that results with yes -only appear higher in the ranking (rank=2.15) than captions (rank=2.66) ( p &lt; 0.001). The answer text is highlighted in the first three captions.
Table 4. Percentage of SERPs with at least once instance of each type of answer in captions or results. N =674. SERPs can Table 5. Percentage of captions/results with answer. N =6491.
Table 6. Average rank of the highest-ranked result with each Source Yes only No only Both Neither Caption 2.66  X 1.72 2.98  X 2.01 4.02  X 2.35 2.15  X 2.00 Result 2.17  X 1.65 2.98  X 1.98 3.93  X 2.35 3.11  X 2.17
Table 7. Percentage of SERPs where top yes caption or result appears above (nearer the top of the ranking than) the top no . We also computed the distribution of ranks for highest-ranked an-swers and report this in Figure 3 for answers of each type and for captions and results. The presence of an answer at ranks 2-10 is conditioned on the absence of the same type of answer at higher ranks. Figure 3a shows that the distributions of highest-ranked yes and no captions was similar across all ranks, with 40-45% occur-ring at the top of the list. The distribution of top-ranked answers in results , shown in Figure 3b, was more skewed toward the top of the ranking for yes than for other types, and yes was significantly more likely to appear in the top position than no (59.1% yes vs. 40.2% no , Z = 5.00, p &lt; 0.001), once again illustrating positive rank bias. Despite the evidence of bias toward yes answers presented in the previous section, a more direct comparison of yes and no comes from their relative ordering when both are present in the results. Our analysis also shows that around 35% of SERPs contained both a yes and a no answer. Table 7 presents the fraction of captions and results that have the top yes above the top no , and vice versa. The results show that the captions with yes are ranked well above those with no for both captions and results. The results also show that captions exhibit this bias slightly more than the results themselves (65% for captions vs. 62% for results), highlighting possible skew in the caption generation methods or rankings associated with cap-tion clickthrough. There were no significant effects on the distribu-tion from physician answers. However, yes was ranked above no more often than the 55% that would be expected given the base rates (both Z  X  2.00, both p  X  0.02), further confirming a positive bias in SERPs and results for the queries in our dataset. Overall, we have shown that search engines are more likely to pre-sent captions/results answering a yes-no question positively ( yes ) in the top results, and they are more likely to rank results with pos-itive answers at the top of the list, and above no (when both are shown on the SERP). We also showed that the distribution of an-swer content was insensitive to the ground truth answers provided by medical professionals; the positive bias persists irrespective of the ground truth. Finally, we showed that distributions were skewed more positive than expected from base rates. It might be argued that this bias is reasonable if searchers are satisfied. However, if search engines lead users to incorrect answers, then this requires corrective action. We examine answer accuracy at the end of the next section. We now study biases in behavior, linking SERP caption judgments with clicks, and using page judgments to study answer accuracy. From our search logs, we extracted all instances of the 674 yes-no queries that resulted in at least one click on a result: 496 (83%) of our yes-no question queries met this criterion. In this analysis, we focus on the first click that users perform and the content of the captions that they click. We do this to help reduce the effects of learning during search. We consider a number of aspects of click behavior, namely all clicks, clicks controlled for position and the distribution of yes-no results, and we also examine caption skips. Across all observed clicks in our set, 41.1% are on a caption labeled yes-only and 16.3% are on a caption labeled no-only ( note: the other clicks are on captions with both or neither ). Focusing solely on the clicks on yes and no that means 71.6% of clicks were on yes , whereas only 28.4% of clicks were on no . This strongly suggests that people are more likely to click on captions with positive out-comes. Table 8 provides a series of click likelihoods computed em-pirically from the data depicting the relationship between properties of the SERP and SERP captions, and click likelihoods (in the All column). The presence of yes or no in the caption is denoted as or respectively, and the rank of the click is denoted . means that captions with yes-only and no-only answers appear on the SERP. The table also breaks down the clicks by physician an-swer, to help us understand the effect of the truth on click behavior as we did in previous sections. A number of conclusions can be drawn from the findings. The first is that people are more likely to click when the SERP contains yes , and also are more likely to click when the physician answer is yes , irrespective of the SERP content (top two rows of Table 8), although this difference was not signifi-cant ( p = 0.63). Focusing on captions, people appear 2-4 times as likely to click on captions with positive content (rows 3-6), even though yes is only marginally more likely in our ground truth data. Considering physician answers we see that likelihoods shift as ex-pected (e.g., clicking no becomes more likely when answer is no ), but clicking yes is still around twice as likely (all Z  X  3.16, all p  X  0.001) and far exceeds what is expected given our base rates. 
Table 8. SERP click likelihoods for different captions given variations in answer presence in SERPs/captions, and rank. Table 9. Distribution of clicks and skips by answer. N =245. (a) (b) One major factor that could explain the strong preference for posi-tive information is the tendency of the search engine to return pos-itive search results above negative results (as shown in Section 5). Given how searchers typically examine result lists (top-to-bottom), this could lead to an apparent preference for positive information caused by the search engine, even if one did not exist for searchers. Since we were performing our analysis retrospectively, we could not use methods such as FairPairs [29] to counteract positional bias. To do this in our study, we focused only on the result at the top position in the list ( =1) and assume that it is always examined (a hypothesis supported by gaze tracking studies [20]). Although the distribution of yes and no answers in these captions was very simi-lar there were still more yes than no captions at the first position (see Figure 3a for the distribution). To ensure that the distribution was equal, we randomly down-sampled the yes instances. This gave us the same number of yes and no captions on which to study clicks ( N =357). The findings appear in the last two rows of Table 8. The trends are similar to the original analysis, and the differences are still significant (all p &lt; 0.001), although the effects of the physician answer are amplified when removing position effects (e.g., click yes is five times as likely as click no when physician answer is yes , and still more than twice as likely when the answer is no ). Beyond clickthrough behavior, it is also worth considering the na-ture of the results that users skipped over prior to clicking on a par-ticular caption. This provides additional insight into searcher pref-erences that is not available in clicks alone. To study this in our context, we targeted SERPs with both yes-only and no-only an-swers in captions, and identified clicks on a caption with a yes or no answer where the user had skipped over another caption prior to clicking (e.g., skip yes , click no ). Table 9 shows the percentage of all skip-click pairs in each combination. A McNemar X  X  chi-squared 16.99, p &lt; 0.001). The table shows that it is common to skip over a caption and click on another caption with the same answer. How-ever, what is most interesting is that on 42% of observed skip events, people skip over a caption with no to click on a caption with yes . This happens more frequently (five times) than it occurs in the opposite direction (skip yes , click no ) and offers more evidence that people are drawn to information related to positive ( yes ) answers. Moving beyond search behavior, we now focus on whether search-ers found the correct answer to their yes-no question and try to bet-ter understand the role of the engine in getting them to that answer. To study accuracy in our logs, we first had to identify the particular answer that searchers found. Since we did not have direct judg-ments about if and where an answer was located, we inferred that from results and behavior. We devised three answer definitions: 1. Top result provided by the search engine. This allows us to 2. First satisfied result click for the query instance. A satisfied 3. Last satisfied result click for the query instance. This helps Note that since most of the impressions had only one click, then the first and the last clicks are often the same. For this analysis, we only focused on clicked pages assigned a yes -only or no -only judgment. Given that we know the physician answer to each of the questions in the set (which we regard as our ground truth), and we know the answer rating for the page that searchers clicked on, or the top-ranked search result provided by the engine, we can calculate the correctness of the answer that the user found. Table 10 presents the breakdown of correctness by our three answer types. The findings summarized in the table show that if searchers trust the top-ranked result of the search engine, they will obtain the correct answer less than half of the time (45%). Analyzing results by physician answer, we can better understand its effect on answer correctness. In all cases, the answer was more likely be correct if the physician answer was yes (all Z  X  4.23, all p  X  0.001). If the physician answer is yes , then searchers settled on yes 66% of the time, perhaps because re-sults with yes are more likely to be ranked higher in the list making them more likely to be chosen. Importantly, if the physician answer is no , then users attain the correct answer 23-29% of the time, showing that the focus is still on positive information. In analyzing the results that searchers select rather than the top re-sult that the search engine returns, we see that there is a small in-crease in correctness (45% to 50-52%) that can be attributed to searchers overriding the search engine ranking. The gains are small, perhaps because choices are limited to the results available in the top 10. Further analyzing the questions with high and low accuracy in the answer provided by the top result, we see that those questions starting with the terms  X  X s X  and  X  X oes X  have the highest answer ac-curacy (61% and 59% respectively). However, questions starting with  X  X an X  (e.g.,  X  Can acid reflux cause back pain?  X ) had the low-est accuracy (38%). As mentioned earlier in the analysis of the ex-pert labels,  X  X an X  denotes possibility and as such, for a question of this type base rates may need to factor in result ranking. The findings in the survey suggested that there may be some inter-esting dynamics between the first and last clicks for a query (e.g., people reported being much more likely to search for confirmatory information than for information that challenged their hypothesis). We studied the changes in outcomes between queries where there were multiple clicks and different URLs at first and last click. Table 11 shows that people focus on a particular answer and stick with it as they review other resources ( 2 (1) = 70.01, p &lt; 0.001). The table also shows that no one transitioned from yes to no . These findings Table 10. Breakdown in correctness by answer definition. Table 11. Distribution of changes in the answer rating for first align with our survey, which showed that confirmation was the pri-mary motivation for pursuing information after the initial answer. Our findings have shown that people are more likely to target re-sults with positive content than expected given base rates. They also show that users are much more likely to skip over captions contain-ing no-only answers to click on those with yes-only (42%) than vice versa (9%). We studied answer accuracy and showed that around half of the time people found the correct answer to our questions. Although the search engine is not trying to answer directly, biases limit selection options and previous studies have shown that people can interpret result order as a likelihood ordering [44]. In addition, we also see strong evidence to support our survey findings that peo-ple seek confirmatory information and rarely change answer focus. We have shown evidence of biases in how people seek information and in search engine rankings toward particular outcomes. Our mixed methods approach used a survey, log analysis, and analysis of labeled data from crowdworkers and physicians. The survey pro-vided insight into people X  X  belief dynamics during search, and the role of search engines and their search behavior in finding infor-mation that supports those beliefs. As we have shown, biases in the results provided by the search engine may lead users to incorrect answers; on average people were likely to find the wrong answer half of the time. However, there were also marked differences in accuracy related to the nature of the queries, with questions denot-ing answer possibility resulting in lower answer accuracy. There are limitations of the research that we should acknowledge. First, we focused on a small set of carefully-selected queries of a particular question type. Such focus was necessary given the scale of the human labeling effort (over 2000 person-hours of judgment time for the crowd-sourced caption/content judging alone) and to simplify our analysis. Different types of questions, such as those with a subjective or exploratory focus, are also worth examining. It is also worth considering situations where biases in beliefs could be significant (e.g., diagnostic scenarios, controversial topics). Sec-ond, although there are similarities between the survey and our later analysis, the survey did not specifically focus on health. Finally, we focused on the questions with physician agreement. The questions with diverging opinions may represent challenging or difficult top-ics, or cases where more information is needed prior to answering. The research has implications for improving the design of search systems and raises important points for discussion on the role of search engines. Overall, a better understanding of how search en-gines rank answer pages is needed. Our findings show that the re-sults that search engines offer are heavily skewed toward particular answers (usually positive), irrespective of the truth. As suggested earlier, this may be a consequence of search engines learning from biased, aggregated user behavior [1]. It may also relate to how users frame their queries. Studies have shown that people are more likely to frame questions positively when testing hypotheses [41]. Indeed, almost all of the yes-no queries in our dataset were framed in this way (e.g.,  X  Can acid reflux cause back pain?  X  and not  X  Is acid re-flux unrelated to back pain?  X ). As a result, ranking methods such as vector-space similarity [30] may consider a result with a yes -oriented answer (e.g.,  X  X cid reflux can cause back pain X ) to be more similar to the query than a no -oriented answer (e.g.,  X  X cid reflux cannot cause back pain X ). Although text similarity is only one component of sophisticated search engine ranking algorithms, the higher query similarity with confirmatory content may be evi-dent in other sources such as anchor text and document titles, and needs further investigation. Although search results may not reflect the truth, they may reflect the dominant opinions of page authors or searchers, in which case on average people may well be satisfied with the answers provided, even if they are strictly inaccurate. Research on the  X  X ilter bubble X  [27] suggests that search engines have a duty of care and that per-sonalization filters out information that disagrees with user view-points. In this work, we show that related effects are observed be-yond personalization; search engines exhibit positive biases in re-sult ordering irrespective of the truth. Rank ordering is important given that people may interpret ranked lists of results as a ranking of likelihoods [44]. We also show that users are more likely to en-gage with positively-leaning content, important because it confirms their beliefs and biases (as our survey shows). The tradeoff between helping users validate beliefs and providing accurate information, and the incorporation of likelihood information should be explored at a query, domain, user, or cohort level. Other effects such as source reliability, the ease with which people can confirm versus refute their beliefs (conclusively falsifying hypotheses can be chal-lenging [28]), and the distribution of positive and negative answers in online content, also need to be considered. Although the general online availability of yes and no content needs to be studied, we did show in Table 7 that even when both answers are available in the top 10 results, yes is ranked above no much more frequently. Exploring accuracy depends critically on the availability of reliable truth data. Technology developed for Web-scale question answer-ing (e.g., [12]) may be applied in the ranking of search results to help search engines consider base rates, especially for particular queries or query classes (e.g., as we demonstrate in our analysis with yes-no questions beginning with  X  X an X ). Search engines using learning-to-rank algorithms could consider the nature of the termi-nology in captions associated with clicks and downweight clicks that appear to be driven by known biases, e.g., those associated with health anxiety. Focused crawling strategies, proposed in the medi-cal domain [36], could improve the reliability of the content in the engine index, since that may affect answer correctness. Long-term models to better represent beliefs, preferences, and perspectives of users beyond topical or term-level interests may also be useful. Tools and other search support could also be provided to introduce diversity into the results, highlight more trustworthy content [32], and prominently display base rates (e.g., in answers on SERPs). Biases affect judgment and decision making. Although these biases can impact search behavior and result ranking, they have largely been ignored in IR. We have described a detailed study of biases in search, in the context of yes-no questions in the medical domain. We employed a retrospective survey, log analysis, content labeling by human judges, and answers from physicians. We showed that people seek to confirm their beliefs with their searches and that search engines provide positively-skewed search results, irrespec-tive of the truth. We also showed that people are more likely to select positive ( yes ) information on SERPs, even when we control for rank, and are likely to skip negative results to reach positive ones (when the opposite is not true). Perhaps the most concerning insight from our analysis is that the combination of system and searcher biases lead people to settle on incorrect answers around half of the time (and that this inaccuracy is amplified when the phy-sician answer, used as our ground truth, is no ). The findings high-light a tradeoff between bias and accuracy that search engines need to consider. In future work we will study biases in domains beyond health search, and on different question types. We will also inves-tigate bias-sensitive ranking methods to tackle challenges such as when accuracy should factor in ranking, and how base rates should be accurately determined, represented, and used by search engines. The author is grateful to Peter Bailey, Susan Dumais, Eric Horvitz, and Resa Roth for feedback and discussions around related ideas. [1] Agichtein, E., Brill, E., and Dumais, S. (2006). Improving [2] Ariely, D. (2008). Predictably Irrational: The Hidden Forces [3] Baron, J. (2007). Thinking and Deciding . Cambridge Press. [4] Belkin, N.J., Oddy, R.N., and Brooks, H.M. (1982). ASK for [5] Bennett, P.N. et al. (2012). Modeling the impact of short-[6] Bilenko, M. and White, R.W. (2008). Mining the search [7] Brennan, R.L. and Prediger, D.J. (1981). Coefficient Kappa: [8] Cho, J. and Roy, S. (2004). Impact of search engines on page [9] Clarke, C., Agichtein, E., Dumais, S., and White R.W. [10] Craswell, N., Zoeter, O., Taylor, M., and Ramsey, B. (2008). [11] Dou, Z., Song, R., and Wen, J.R. (2007). A large-scale evalu-[12] Dumais, S. et al. (2002). Web question answering: Is more [13] Fleiss, J.L. (1971). Measuring nominal scale agreement [14] Fortunato, S., Flammini, A., Menczer, F., and Vespignani, A. [15] Gigerenzer, G. and Todd, P.M. (2000). Simple Heuristics [16] Ieong, S., Mishra, N., Sadikov, E., and Zhang, I. (2012). Do-[17] Ingwersen, P. (1994). Polyrepresentation of information [18] Inlander, C.B. (1993). Good operations, Bad operations: The [19] Joachims, T. (2002). Optimizing search engines using click-[20] Joachims, T. et al. (2007). Evaluating the accuracy of im-[21] Kahneman, D. and Tversky, A. (1974). Judgment under un-[22] Klayman, J. and Ha, Y. (1987). Confirmation, disconfirma-[23] Kuhlthau, C. (1991). Inside the search process: Information [24] Marchionini, G. (1995). Information Seeking in Electronic [25] Mowshowitz, A. and Kawaguchi, A. (2002). Bias on the [26] Nickerson, R.S. (1998). Confirmation bias: a ubiquitous phe-[27] Pariser, E. (2011). The Filter Bubble: What is the Internet [28] Popper, K. (1959). The Logic of Scientific Discovery . Basic [29] Radlinski, F. and Joachims, T. (2006). Minimally invasive [30] Salton, G., Wong, A., and Yang, C.S. (1975). A vector space [31] Saracevic, T. (1997). The stratified model of information re-[32] Schwarz, J. and Morris, M.R. (2011). Augmenting Web [33] Simon, H. (1991). Bounded rationality and organizational [34] Snow, R., O X  X onnor, B., Jurafsky, D., and Ng, A.Y. (2008). [35] Sontag, D. et al. (2012). Probabilistic models for personaliz-[36] Tang, T.T., Hawking, D., Craswell, N., and Griffiths, K. [37] Taylor, R.S. (1968). Question-negotiation and information [38] Teevan, J., Dumais, S.T., and Horvitz, E. (2005). Personaliz-[39] Tversky, A. and Kahneman, D. (1973). Availability: A heu-[40] Vaughn, L. and Thelwall, M. (2004). Search engine coverage [41] Wason, P.C. (1960). On the failure to eliminate hypotheses [42] White, R.W., Bennett, P.N., and Dumais, S.T. (2010). Pre-[43] White, R.W. and Drucker, S.M. (2007). Investigating behav-[44] White, R.W. and Horvitz, E. (2009). Cyberchondria: Studies [45] White, R.W. and Horvitz, E. (2012). Studies on the onset and [46] Xiang, B. et al. (2010). Context-aware ranking in web search. [47] Yue, Y., Patel, R., and Roehrig, H. (2010). Beyond position [48] Zhai, C., Cohen, W.W., and Lafferty, J. (2003). Beyond in-
