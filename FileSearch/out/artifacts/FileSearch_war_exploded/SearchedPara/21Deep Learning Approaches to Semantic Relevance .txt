 BAOXUN WANG, BINGQUAN LIU, XIAOLONG WANG, CHENGJIE SUN, and In natural language processing (NLP) and information retrieval (IR) fields, the ques-tion answering (QA) problem has attracted much attention over the past few years. Nevertheless, most of the QA researchers mainly focus on locating the exact answer to a given factoid question in the related documents. The most well-known international evaluation on the factoid QA task is the Text REtrieval Conference (TREC) 1 ,andthe annotated questions and answers released by TREC have become important resources for the researchers. However, when facing a non-factoid question such as why , how ,or what about , almost no automatic QA systems work very well.

The user-generated question-answer pairs are definitely of great importance to solve the non-factoid questions, thus there is a growing need to collect such natural QA pairs recently. Obviously, these natural QA pairs are usually created during peo-ple X  X  communication via Internet social media, among which we are interested in the community-driven question-answering (cQA) sites and the online forums. The cQA sites (or systems) provide platforms where users can either ask questions or deliver answers, and the best answers are usually selected manually (e.g., Baidu Zhidao 2 and Yahoo! Answers 3 ). Compared with cQA sites, online forums have more virtual society characteristics where people hold discussions in certain domains, such as techniques, travel, sports, etc. Online forums contain a huge number of QA pairs and much noise information is involved.

The accumulation of the natural generated QA pairs will promote the research on automatic question answering technique and influence the performance of the com-mercial cQA systems. To make use of the QA pairs in cQA sites and online forums, one has to face the challenging problem of distinguishing the questions and their answers from the noise. According to our investigation, the data in the community-based sites, especially for the Chinese cQA services and online forums, have two obvious character-istics: (a) a post usually includes a very short content, and when a person is initializing or replying a post, an informal tone tends to be used; (b) most of the posts are useless, which makes the community become a noisy environment for question-answer detec-tion. In addition, the quality of the different community corpora varies significantly. Thus it is important to enhance the generalization of the community-oriented QA pair mining approaches.

Since both the cQA systems and the online forums are open platforms for people to communicate, the QA pairs in the cQA systems have similarity with those in the forums. According to our investigation, the similarity between the cQA and the forum contents mainly lies in the textual characteristics, ranging from the text styles and the word distributions. So far, most QA researches have been done on the cQA and the forum datasets separately [Ding et al. 2008; Surdeanu et al. 2008], and few have noticed the relationship between the two kinds of data. However, the textual similarity indicates that it is possible and desirable to consider the cQA dataset and the forum dataset to be homogenous and take advantage of their potential common properties to facilitate the studies on Web social community-based question answering.

In Web social communities, there exist various social-based features which have been proved to be helpful to the QA pair detecting task [Agichtein et al. 2008; Hong and Davison 2009; Jeon et al. 2006]. The social features, also called structural features or non-textual features, can be obtained from the structure of the communities X  Web pages without any text analysis. The effect of the non-textual features have been realized by many researchers in this field, but introducing such features is not always the best solution: the non-textual features in the cQA datasets do not match those in the forum datasets. For the answer-ranking task in the cQA services, the information such as the author X  X  authority and the users X  rating is very important. In the forum corpora, however, to detect the answer posts one has to pay attention to the features such as the number of quotes, the positions of the posts, etc. This is not good news for the studies which attempt to generalize the answer detecting models for both cQA and forum corpora, because the model trained with the non-textual features in the cQA corpus will not fit the forum corpus (and vice versa).

In this article, a novel approach is proposed to model the semantic relevance for the QA pairs in the Web social communities, and the methodology is applied in the answer-ranking task.

Our study concentrates on modeling the semantic relationship between questions and their answers using simple textual features. Taking pure word-based features is meaningful for the generalization of the model. As mentioned above, the user generated questions and their answers via social media are always short texts. The limitation of length leads to the sparsity of the word features. In addition, the word frequency is usually either 0 or 1, that is, the frequency offers little information except the occurrence of a word. Because of this situation, the traditional relevance computing methods based on word co-occurrence, such as Cosine similarity and KL-divergence, are not effective for question-answer semantic modeling. Most re-searchers try to introduce structural features or users X  behavior to improve the models performance, by contrast, the effect of textual features is not obvious.

To solve this problem, we present a deep belief network (DBN) to model the semantic relevance between questions and their answers. Different training principles have been taken for the deep network to establish the semantic relationship for the QA pairs from the social communities, and the results have shown the power of the DBN based methodology. Using only word features, our model outperforms the traditional methods on question-answer relevance calculating.

In addition, rather than mine the structure-based features from cQA pages and forum threads individually, our study focuses on the textual similarity between the two kinds of data. The semantic information learned from cQA corpus is helpful to detect answers in forums, which makes our model show good performance on both the corpora. Thanks to the labels for the best answers existing in the cQA pages, no manual work is needed in our work.
 The rest of this article is organized as follows: Section 2 surveys the related work. The textual similarity between cQA and online forum contents is discussed in Sec-tion 3. Section 4 details the deep belief networks for answer detection. Experimental results are presented and analyzed in Section 5. Finally, conclusions and future direc-tions are drawn in Section 6. The value of the naturally generated question-answer pairs was not recognized until recent years. Early studies mainly focus on extracting QA pairs from frequently asked questions (FAQ) pages [Jijkoun and de Rijke 2005; Riezler et al. 2007] or service call-center dialogues [Berger et al. 2000]. Since the contents of such pages are normalized, the early researchers do not focus on the QA pair detecting methods.

Judging whether a candidate answer is semantically related to the question in the cQA page automatically is a challenging task. A framework for predicting the quality of answers was presented in Jeon et al. [2006]. Bernhard and Gurevych [2009] devel-oped a translation-based method to find answers on WikiAnswers 4 . Surdeanu et al. [2008] propose an approach to rank the answers retrieved by Yahoo! Answers. Our work is partly similar to Surdeanu et al. [2008], for we also aim to rank the candidate answers reasonably, but our ranking algorithm needs only word-occurrence informa-tion, instead of the combination of different kinds of features.

Because people have considerable freedom to post on forums, there are a great num-ber of irrelevant posts for answering questions, which makes it more difficult to detect answers in the forums. In this field, exploratory studies were done by Feng et al. [2006] and Huang et al. [2007], who extract input-reply pairs for the discussion-bot. Ding et al. [2008] and Cong et al. [2008] also presented outstanding research works on forum QA extraction. Ding et al. [2008] detect question contexts and answers us-ing the conditional random fields, and a ranking algorithm based on the authority of forum users is proposed by Cong et al. [2008]. Treating answer detection as a binary classification problem is an intuitive idea, thus there are some studies trying to solve it from this view [Hong and Davison 2009; Wang et al. 2009]. Especially Hong and Davison [2009] achieved a rather high precision on the corpora with less noise, which also shows the importance of social features.

In order to select the answers for a given question, one has to face the problem of lexical gap. One of the problems with lexical gap embedding is to find similar questions in QA archives [Jeon et al. 2005]. Recently, the statistical machine translation (SMT) strategy has become popular. Lee et al. [2008] use translation models to bridge the lexical gap between queries and questions in QA collections. The SMT-based methods are effective on modeling the semantic relationship between questions and answers and expending users X  queries in answer retrieval [Berger et al. 2000; Bernhard and Gurevych 2009; Riezler et al. 2007]. In Surdeanu et al. [2008], the translation model is used to provide features for answer ranking.

The non-textual features (e.g., authorship, acknowledgement, post position, etc.) play an important role in answer extraction. Such features are used in Ding et al. [2008], Cong et al. [2008], and Agichtein et al. [2008], and have significantly improved the performance. The studies of Jeon et al. [2006] and Hong and Davison [2009] show that the structural features have even more contribution than the textual features. In this case, the mining of textual features tends to be ignored.

There are also some other research topics in this field. Although such studies are not directly related to the research in this article, they share some essential problems with our work and inspire our research, for example, the semantic relevance for short texts, etc. Cong et al. [2008] and Wang et al. [2009] both propose the strategies to de-tect questions in the social media corpus, which is proved to be a non-trivial task. The deep research on question detection has been taken by Duan et al. [2008]. A graph-based algorithm is presented to answer opinion questions [Li et al. 2009]. Summariza-tion on the short texts such as user-generated answers and e-mail contents is a new topic, Tomasoni and Huang [2010] exploit metadata in the answers to bias automatic multi-document summarization techniques toward high-quality information. Besides, the QA pairs are also extracted from e-mail contents as the main elements of e-mail summarization [Shrestha and McKeown 2004]. In this section, the textual similarity between the cQA and the forum contents is dis-cussed. The similarity supplies us with a strategy to make our DBN model to detect an-swers in both cQA and forum datasets. To our knowledge, the existing studies mainly focus on one single dataset. Our motivation of finding the similar question-answer cor-pora from different kinds of social media is to guarantee the model X  X  performance and avoid hand-annotating work.

In this article, we get the  X  X olved question X  pages in the computer technology domain from Baidu Zhidao as the cQA corpus, and the threads of ComputerFansClub Forum 5 as the online forum corpus. The domains of the corpora are the same. To further explain that the two corpora are similar in terms of textual characteristics, we will give the detail comparison on text style and word distribution.
As shown in Figure 1, we compared the post content lengths of the cQA and the forum in our corpora. For the comparison, 5,000 posts from the cQA corpus and 5,000 posts from the forum corpus are randomly selected. The left panel shows the statistical result on the Baidu Zhidao data, and the right panel shows the one on the forum data. The number i on the horizontal axis denotes the post contents whose lengths range from 10( i  X  1) + 1 to 10 i bytes, and the vertical axis represents the counts of the post contents. From Figure 1 we observe that the contents of most posts in both the cQA corpus and the forum corpus are short, with the lengths not exceeding 400 bytes.
The content length reflects the text style of the posts in cQA systems and online forums. From Figure 1 it can be also seen that the distributions of the content lengths in the two figures are very similar. It shows that the contents in the two corpora are both mainly short texts.

Figure 2 shows the percentage of the concurrent words in the top-ranked content words with high frequencies. In detail, we first rank the words by frequency in the two corpora. The words are chosen based on a professional dictionary to guarantee that they are meaningful in the computer knowledge field. The number k on the horizontal axis in Figure 2 represents the top k content words in the corpora, and the vertical axis stands for the percentage of the words shared by the two corpora in the top k words.
Figure 2 shows that a large number of meaningful words appear in both of the two corpora with high frequencies. The percentage of the concurrent words maintains above 64% in the top 1,400 words. It indicates that the word distributions of the two corpora are quite similar, although they come from different social media sites.
Because the cQA corpus and the forum corpus used in this study have homogenous characteristics for answer detecting task, a simple strategy may be used to avoid the hand-annotating work. Apparently, in every  X  X olved question X  page of Baidu Zhidao, the best answer is selected by the user who asks this question. We can easily extract the QA pairs from the cQA corpus as the training set. Because the two corpora are similar, we can apply the deep belief network trained by the cQA corpus to detect answers on both the cQA data and the forum data. For a non-factoid question, the answer is always descriptive and has a semantic rela-tionship with the corresponding question. Due to the feature sparsity and the low word frequency of the social media corpus, it is difficult to model the semantic relevance be-tween questions and answers using only the co-occurrence information. Nevertheless, there are two facts that attract our attention: a) the QA pair provides complete descrip-tion of a certain topic, thus the real answer semantically matches the corresponding question. In this case, it is possible to obtain semantic information from the joint dis-tributions of the questions and their real answers. b) An answer usually involves more information than the question, the evidence of which is that given the real answer, the people with enough knowledge can easily guess key words of the question. Hence, the semantic association can be learned according to the procedure of reconstructing questions using their answers. Basically, the essence of the above phenomena is that the semantic link exists between the question and its answers, even though they have totally different lexical representations. In this section, based on the two facts above respectively, we propose the deep belief networks for modeling the semantic relation-ship between questions and their answers. Our models are able to map the QA data into a low-dimensional semantic-feature space, where a question is close to its answers. An ensemble of binary vectors can be modeled using a two-layer network called a  X  X estricted Boltzmann machine X  (RBM) [Hinton 2002]. The dimension reducing ap-proach based on RBM initially shows good performance on image processing [Hinton and Salakhutdinov 2006]. Salakhutdinov and Hinton [2009] propose a deep graphical model composed of RBMs into the information retrieval field, which shows that this model is able to obtain semantic information hidden in the word-count vectors.
As shown in Figure 3, the RBM is a two-layer network. The bottom layer represents a visible vector v and the top layer represents a latent feature h . The matrix W con-tains the symmetric interaction terms between the visible units and the hidden units. Given an input vector v , the trained RBM model provides a hidden feature h ,which can be used to reconstruct v with a minimum error. The ability of the RBM suggests us to build a deep belief network based on RBM so that the semantic relevance between questions and answers can be modeled. In this section, we present a deep belief network designed based on the joint distri-butions of the QA pairs. As mentioned above, the semantic relationship between the questions and their answers can be modeled by simulating some linguistic phenomena of the QA pairs, among which the joint distribution is typical.
 The DBN used to model the QA pairs X  joint distribution is inspired by the work of Hinton and Osindero [2006], who have proposed to learn to label the images automat-ically. In their study, the relationship between the images and their digital labels is achieved by training a deep network composed of RBMs. Taking the joint distribution of the image and the digital label as the basis, their model succeeds in getting the semantics hidden in the image.

The illustration of this DBN is given in Figure 4. The model first handles the binary word-occurrence feature vectors of the question and its answers separately through two independent paths. Each path is composed of two layers, and here each layer stands for the RBM shown in Figure 3. In the bottom layers of the two branches, vectors of the question and the answer are input into the visible units of the RBMs. The visible feature vectors can be used to compute the  X  X idden features X  in the hidden units. The RBM is able to reconstruct the inputs using the hidden features.
After training the bottom layers, the generated hidden feature vectors are then sent to the higher-level layers as the  X  X raining data X . Noticing that in each branch the layers X  function is to reduce the dimension of the question/answer vector, the structures of the layers are quite similar, with little difference on the number of the hidden units. The characteristic of the RBM guarantees that the hidden features include enough semantic information of the questions and the answers.

When arriving at the top layer, both the question and the answer are compressed to low-dimensional hidden feature vectors. At the top layer of the network, the two hid-den feature vectors are joined together as the training data. In this case, the top layer models the joint distribution of the QA pairs, and the hidden vectors finally generated by the DBN model involve semantic information of the questions and their answers.
Taking the classic RBMs as the basic components, the architecture of the DBN in this subsection is similar with the one in Hinton and Osindero [2006], thus the train-ing algorithm of each layer is the same with Hinton and Osindero [2006], in which the method of fine-tuning the network is also presented, and we perform the same procedure as well.

With a trained DBN, the answer detecting task can be conducted simply: the vector of the question is firstly sent into the question (left) branch in Figure 4 and perform a level-by-level calculation to get the corresponding hidden feature vector at the top level. Then the vectors of the candidate answers are sent to the answer (right) branch and the model take the same procedure to obtain a group of feature vectors. Finally the distance between the mapped question vector and each candidate answer vector is calculated and the candidate answer which is closest to the question in the mapped space is considered as the best answer. Reconstructing the questions using the information of the answers is an intuitive se-mantic relevance based procedure. In this subsection, a specially designed deep belief network is proposed to learn the semantic knowledge by reconstructing a great num-ber of questions using the corresponding answers. 4.3.1 Pretraining a Deep Belief Network. In the social media corpora, the answers are always descriptive, containing one or several sentences. Noticing that an answer has strong semantic association with the question and involves more information than the question, we propose to train a deep belief network by reconstructing the question using its answers. The training object is to minimize the error of reconstruction, and after the pretraining process, a point that lies in a good region of parameter space can be achieved.

First, the illustration of the DBN model is given in Figure 5. This model is composed of three layers and here each layer stands for the RBM or its variant. The bottom layer is a variant form of RBM X  X  designed for the QA pairs. This layer we design is a little different from the classic RBMs, so that the bottom layer can generate the hidden fea-tures according to the visible answer vector and reconstruct the question vector using the hidden features. The pre-training procedure of this architecture is practically con-vergent. In the bottom layer, the binary feature vectors based on the statistics of the word occurrence in the answers are used to compute the  X  X idden features X  in the hid-den units. The model can reconstruct the questions using the hidden features. The processes can be modeled as follows: where  X  ( x )=1 / (1 + e  X  x ), a denotes the visible feature vector of the answer, q i is the ith element of the question vector, and h stands for the hidden feature vector for re-constructing the questions. w ij is a symmetric interaction term between word i and hidden feature j , b i stands for the bias of the model for word i ,and b j denotes the bias of hidden feature j .

Given the training set of answer vectors, the bottom layer generates the corre-sponding hidden features using Equation (1). Equation (2) is used to reconstruct the Bernoulli rates for each word in the question vectors after stochastically activating the hidden features. Then Equation (1) is taken again to make the hidden features active. We use one-step Contrastive Divergence [Hinton 2002] to update the parameters by performing gradient ascent: where &lt; q i h j &gt; qData denotes the expectation of the frequency with which the word i in a question and the feature j are on together when the hidden features are driven by the question data. &lt; q i h j &gt; qRecon defines the corresponding expectation when the hidden features are driven by the reconstructed question data. is the learning rate.
The classic RBM structure is taken to build the middle layer and the top layer of the network. The training method for the higher two layer is the same with that in Subsection 4.2, and we only have to make each RBM to reconstruct the input data using its hidden features. After training one layer, the h vectors are then sent to the higher-level layer as its  X  X raining data X . 4.3.2 Fine-Tuning the Weights. Notice that a greedy strategy is taken to train each layer individually during the pre-training procedure, it is necessary to fine-tune the weights of the entire network for optimal reconstruction. To fine-tune the weights, the network is unrolled, taking the answers as the input data to generate the corresponding ques-tions at the output units. Using the cross-entropy error function, we can then tune the network by performing backpropagation through it. 4.3.3 Best Answer Detection. After pre-training and fine-tuning, a deep belief network for QA pairs is established. The operations for detecting the best answer to a given question are quite similar with the ones in Subsection 4.2: we just have to send the vectors of the question and its candidate answers into the input units of the network and perform a level-by-level calculation to obtain the corresponding feature vectors. Notice that the bottom layer in Figure 5 just aims to illustrate the reconstructing objective in pretraining, the vectors of both the question and the candidate answers are sent to the same input units in the best answer detecting period. Then we calculate the distance between the mapped question vector and each candidate answer vector. We consider the candidate answer with the smallest distance as the best one. The task of detecting answers in social media corpora suffers from the problem of feature sparsity seriously. High-dimensional feature vectors with only several non-zero dimensions bring large time consumption to our model. Thus it is necessary to reduce the dimension of the feature vectors.

In this article, we adopt two kinds of word features. First, we consider the 1,300 most frequent words in the training set as Salakhutdinov and Hinton [2009] did. As mentioned in Section 3, the content words come from a professional dictionary in com-puter technology which introduces little segment ambiguity, thus the methodology of the Chinese word segmentation is not the major fact that influences the final results. In fact, the basic word segmentation strategy is able to bring good performance to our experiments. According to our statistics, the frequencies of the rest words are all less then 10, which are not statistically significant and may introduce much noise.
We take the occurrence of some function words as another kind of features. The function words are quite meaningful for judging whether a short text is an answer or not, especially for the non-factoid questions. For example, in the answers to the causation questions, the words such as because and so are more likely to appear; and the words such as firstly , then ,and should may suggest the answers to the manner questions. We give an example for function word selection in Figure 6.

For this reason, we collect 200 most frequent function words in the answers of the training set. Then for every short text, either a question or an answer, a 1,500-dimensional vector can be generated. Specifically, all the features we have adopted are binary, for they only have to denote whether the corresponding word appears in the text or not. To evaluate our question-answer semantic relevance computing methods, we com-pare our DBN based approaches with the popular techniques on the answer detecting task. 5.1.1 Architecture of the Networks. Basically, the first DBN illustrated by Figure 4 fol-lows a classic architecture, which is similar with the deep net in Hinton and Osindero [2006]. The 1500-1500-1000 architecture is taken to form each of the branch, that is, the bottom layer of the branch contains 1,500  X  1,500 units and the upper one contains 1,500  X  1,000 units. On the top layer of the network, we use a 2000-1000 RBM to join the compressed 1,000 dimensional vectors of the question and the answer and map them into a 1,000 dimensional space. During the pretraining period, the bottom layers of the two branches are greedily pretrained for 100 passes through the entire training set, and the rest layers (the middle two layers and the top layer) are trained for 200 passes.

To build the DBN in Figure 5, we use a 1500-1500-1000-600 architecture, which means the three layers of the network have individually 1,500  X  1,500, 1,500  X  1,000 and 1,000  X  600 units. Using the network, a 1,500-dimensional binary vector is finally mapped to a 600-dimensional real-value vector. In the pretraining stage, the bottom layer is greedily pretrained for 200 passes through the entire training set, and each of the rest two layers is greedily pretrained for 50 passes.

In addition, both the networks are fine-tuned for optimal reconstruction. For fine-tuning we apply the method of conjugate gradients 6 , with three line searches performed in each pass. This algorithm is performed for 50 passes respectively to fine-tune the networks. 5.1.2 Dataset. We have crawled 20,000 pages of  X  X olved question X  from the computer and network category of Baidu Zhidao as the cQA corpus. Correspondingly we ob-tain 90,000 threads from ComputerFansClub, which is an online forum on computer knowledge. We take the forum threads as our forum corpus.

From the cQA corpus, we extract 12,600 human generated QA pairs as the training set without any manual work to label the best answers. We get the contents from another 2,000 cQA pages to form a testing set, each content of which includes one question and 4.5 candidate answers on average, with one best answer among them. To get another testing dataset, we randomly select 2,000 threads from the forum corpus. For this testing set, human work are necessary to label the best answers in the posts of the threads. There are seven posts included in each thread on average, among which one question and at least one answer exist. 5.1.3 Baseline. In this article, our DBN models actually provides a way to quantify the semantic relevance for the QA pairs, thus three classic relevance computing meth-ods for ranking candidate answers are considered as the baselines to show the per-formance of our approaches. Besides, in order to show the importance of mining the semantic relevance features, a popular classification-based approach using the non-textual features in Hong and Davison [2009] is also taken as the baseline. We will briefly introduce them.

Cosine Similarity . Given a question q and its candidate answer a , their cosine simi-larity can be computed as follows: where w q respectively. The weights can be get by computing the product of term frequency ( tf ) and inverse document frequency ( idf ).

HowNet-Based Similarity . HowNet 7 is an electronic world knowledge system, which serves as a powerful tool for meaning computation in human language technology. Nor-mally the similarity between two passages can be calculated by two steps: (1) matching the most semantic-similar words in each passages greedily using the API X  X  provided by HowNet; (2) computing the weighted average similarities of the word pairs. This strategy is taken as a baseline method for computing the relevance between questions and answers.
KL-Divergence Language Model . Given a question q and its candidate answer a ,we can construct unigram language model M q and unigram language model M a . Then we compute KL-divergence between M q and M a as below:
Classification-Based Approach . Hong and Davison [2009] have presented a classifica-tion-based approach to answer detecting and answer ranking. According to their study, the method adopting only non-textual features performs well on the given corpora. The non-textual features used by them includes the position of the answer post and the authorship, which is introduced based on the fact that the users who usually answer others X  questions may provide good answers. In our study, this method is taken as a baseline and repeated on our corpora. From our forum corpus, 500 threads involving questions are randomly selected and well labeled for training, and another 500 labeled threads are taken as the testing set. On the cQA corpus, the same scale of dataset is obtained for the classification method. We evaluate the performance of our approach for answer detection using two metrics: Precision@1 (P@1) and Mean Reciprocal Rank (MRR). Applying the two metrics, we perform the baseline methods and our DBN based methods on the two testing set above.

Table I lists the results achieved on the forum data using the baseline methods and ours. The additional  X  X earest Answer X  stands for the method without any ranking strategies, which returns the nearest candidate answer from the question by posi-tion. As mentioned above,  X  X ong X  X  method X  is a classification-based ranking scheme, which has achieved good results in Hong and Davison [2009]. In this article, two deep networks with different architectures are proposed to model the QA pairs X  se-mantic relevance. In this part,  X  X BN-JD X  represents the DBN model in Figure 4, and  X  X BN X  stands for the deep network based on the reconstructing strategy illustrated by Figure 5. To show the effect of the fine-tuning for our models, we list the results of our methods without fine-tuning and the results with fine-tuning.

As shown in Table I, our deep belief network-based methods outperform the baseline methods as expected. The main reason for the improvements is that the DBN based approach is able to learn semantic relationship between the words in QA pairs from the training set. Although the training set we offer to the network comes from a different source (the cQA corpus), it still provides enough knowledge to the network to perform better than the baseline methods. This phenomena indicates that the homogenous corpora for training is effective and meaningful.

We have investigated the reasons for the unsatisfying performance of the tradi-tional semantic modeling approaches. Basically, the low precision is ascribable to the forum corpus we have obtained. As mentioned in Section 1, the contents of the fo-rum posts are short, which leads to the sparsity of the features. Besides, when users post messages in the online forums, they are accustomed to be casual and use some synonymous words interchangeably in the posts, which is believed to be a significant situation in Chinese forums especially. Because the features for QA pairs are quite sparse and the content words in the questions are usually morphologically different from the ones with the same meaning in the answers, the Cosine Similarity method become less powerful. For HowNet based approaches, there are a large number of words not included by HowNet, thus it fails to compute the similarity between ques-tions and answers. KL-divergence suffers from the same problems with the Cosine Similarity method. Compared with the Cosine Similarity method, this approach has achieved the improvement of 9.3% in P@1, but it performs much better than the other baseline methods in MRR.

The results of the classic approaches indicate that the online forum is a complex environment with large amount of noise for answer detection. Traditional IR methods using pure textual features can hardly achieve good results. We also notice that, how-ever, such baseline methods have obtained better results on forum corpus [Cong et al. 2008]. One possible reason is that the approaches are suitable for their data, since we observe that the  X  X earest answer X  strategy has obtained a 73.5% precision in their work.

As a popular strategy for answer ranking and detecting, Hong and Davison X  X  [2009] classification-based method considers pure non-textual information as its features and achieves good results with the precision of more than 90% on their corpora. This work indicates a trend in the community based QA mining field. When this approach is repeated on our datasets, however, its performance is not quite satisfying. According to the comparison of Hong and Davison X  X  [2009] results and ours, we can infer that the performance of the non-textual (or social) feature-based answer ranking approach depends on the quality of the corpus heavily. For our Chinese forums, the feature set composed of position and authorship provides less useful information for the answer ranking task. Especially, it can be found that the best answers do not always appear next to the corresponding questions in our Chinese corpora. In this case, the effect of the post position feature has become less obvious. The problem of corpus sensitivity can be seen in Hong and Davison [2009] as well, in which the authors have repeated the answer detecting algorithm of Cong et al. [2008] on their corpora and the precision becomes much lower.

Taking only word-occurrence information as the original features, our DBN based methods concern on the mining of the semantic relevance, and the answer-to-question strategy based model has achieved the precision of 45.00% in P@1 and 62.03% in MRR for answer detecting on forum data after fine-tuning. Noticing that both Cong et al. [2008] and Hong and Davison [2009] have reported the results with higher precisions, we attribute the results X  difference to the quality change of the corpora. For the exper-iments of this article, a large amount of noise is involved in the forum corpus and we have done nothing extra to filter it.

Table II shows the experimental results on the cQA dataset. In this experiment, each sample is composed of one question and its following several candidate answers. We delete the ones with only one answer to confirm there are at least two candidate answers for each question. The candidate answers are rearranged by post time so that the real answers do not always appear next to the questions. In this group of experiments, no hand-annotating work is needed because the real answers have been labeled by cQA users.

From Table II we observe that all the approaches perform much better on this dataset. We attribute the improvements to the high-quality QA corpus Baidu Zhidao offers: the candidate answers tend to be more formal than the ones in the forums, with less noise information included. In addition, the  X  X earest Answer X  strategy has reached 36.05% in P@1 on this dataset, which indicates quite a number of askers re-ceive the real answers delivered by the professional users at the first answer post. This result has supported the idea of introducing position features. What X  X  more, if the best answer appear immediately, the asker tends to lock down the question thread, which helps to reduce the noise information in the cQA corpus. Thus it can be seen that the  X  X ong X  X  method X  has performed better than the rest baseline methods and approached our DBN based models.

Despite the baseline methods X  performances have been improved, our approaches still outperform them. Our models with two different architectures have obtained good results in both P@1 and MRR, which further indicates the ability of the deep belief network on the semantic modeling. On the cQA dataset, our model shows better per-formance than the previous experiment, which is expected because the training set and the testing set come from the same corpus, and the DBN model is more adaptive to the cQA data.

According to the two groups of experiments above, the network based on the answer-to-question reconstructing strategy performs better than the joint distribution based one. Besides, we have also observed that, from the results of both the two kinds of DBNs, fine-tuning is effective for enhancing the performance of our models. In this article, we proposed the deep learning approaches for modeling the semantic relevance of the Chinese question-answer pairs in the web social communities.
The contributions of this article can be summarized as follows: (1) two deep be-lief networks with different architectures have been presented based on the QA joint distribution and the answer-to-question reconstruction principles respectively. Both the models show good performance on modeling the semantic relevance for the QA pairs, using only word occurrence features. Taking the data driven strategy, our DBN models learn semantic knowledge from large amount of QA pairs to quantify the se-mantic relevance between questions and their answers. (2) We have investigated the textual similarity between the cQA and the forum datasets for QA pair extraction, which provides the basis to our approaches to avoid hand-annotating work and show good performance on both the cQA and the forum corpora.

Our future work will be carried out along two directions. First, some research will be put forward to improve the performance of the DBN based methods by adopting the specific non-textual features on the given social community based corpus. Second, we will extend the application of the deep learning approach to the solution of other problems in the automatic QA field.

