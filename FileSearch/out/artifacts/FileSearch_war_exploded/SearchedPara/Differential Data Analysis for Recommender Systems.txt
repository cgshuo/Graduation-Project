 We present techniques to characterize which data contributes most to the accuracy of a recommendation algorithm. Our main technique is called differential data analysis. The name is inspired by other sorts of differential analysis, such as dif-ferential power analysis and differential cryptanalysis, where insight comes through analysis of slightly differing inputs. In differential data analysis we chunk the data and compare results in the presence or absence of each chunk. We apply differential data analysis to two datasets and three differ-ent attributes. The first attribute is called user hardship. This is a novel attribute, particularly relevant to location datasets, that indicates how burdensome a data point was to achieve. The second and third attributes are more stan-dard: timestamp and user rating. For user rating, we con-firm previous work concerning the increased importance to the recommender of high and low user ratings.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information Filtering Experimentation, Human Factors Recommender Systems
Services that use recommender systems have become in-creasingly common and most of these systems exist by virtue of  X  X ig data X  stored and used for recommendation and per-sonalization purposes. One example is location recommender systems (e.g., recommending nearby points-of-interest), a  X  Work performed while at Samsung Electronics R&amp;D. large and growing area because of the connection with mo-bile devices. What part of all this data is really necessary for making good recommendations?
This question is our main motivation for the paper. Our approach starts with techniques to rank data according to usefulness to the recommender system. In essence, we iden-tify attribute values which are associated with usefulness to the recommender. We test the association through what we call differential data analysis : we select data according to attribute values and observe the effect of the presence and absence of this data on recommendation accuracy.

Our work has applications to user privacy and data re-duction, but due to space considerations we only mention these applications in passing here and defer detailed discus-sion to the extended version of this paper [1]. One of the main challenges of recommender systems is user privacy, as the data to build a user profile can be potentially sensitive or embarrassing. Adding fake data and suppressing actual data are standard tactics to enhance user privacy, and one consequence of our work is to make these tactics more effi-cient.
Our main experimental technique involves dividing the data into chunks and comparing results in the presence or absence of each chunk. More specifically, suppose a data attribute ranks each user X  X  data in some way, for instance by time or user rating. One can then divide the data into chunks based on this ranking. For convenience, we often divide into 10 chunks, or deciles . We can then examine the relative effect of a particular decile on recommendation ac-curacy as follows. We form a training and test set as usual. We rank each user X  X  training data by the data attribute un-der examination. We remove the first decile of each user X  X  data from the training set and calculate the recommenda-tion accuracy (using a fixed algorithm). We continue by removing the second decile of each user X  X  data, etc., and we end up with 10 readings for the accuracy, which can then be compared. A relatively high accuracy reading implies the corresponding decile is less important to the recommender; a low accuracy reading implies the corresponding decile is more important.

Note that there are other ways to do the differential data comparison. Rather than removing data deciles from all users simultaneously, an alternative approach would con-sider each user in isolation. Deciles would be removed from one user at a time, the effect on recommendation accuracy would be measured for that user, and accuracy measure-ments would be aggregated over all users at the end. For the experiments described in this paper, we used the former approach since it is less computationally intensive.
We study three data attributes in this work. The first at-tribute we examine is a  X  X ser hardship X  for each data point, the effort it takes the user to attain the data point. The idea is that data points which are more or less difficult to achieve may be more or less indicative of a user X  X  preferences. This attribute is most intuitively associated with location datasets. To compute the user hardship on our location dataset, a user X  X  location traces can be clustered according to surface-of-the-earth distance, and points are ranked ac-cording to their distance to the set of cluster centroids. We call this the KMeans user hardship measure. Points fur-thest away from the cluster centroids have a higher user hardship score, as these locations are further from a user X  X  usual haunts and require more work to get to. Another way to compute the user hardship is to measure the distance to any other point (rather than the cluster centroid). We call this the Density user hardship measure. We present experi-mental results in Section 4.1 on user hardship for a location dataset.

The second attribute we investigate is the timestamp of the data, for instance the time of a location checkin or the time a rating is given. We investigate whether certain time periods of the day might correspond to more important data. In Sections 4.1 and 4.3 we present our results on filtering by timestamps for a location dataset and a rating dataset.
Finally, our third attribute has been discussed in previous work: the actual user rating, for instance the number of stars given by a user for a product. Previous work has recognized that data with high and low ratings are more important to the user [5] and to the recommender [3].
We experimented with our techniques using two actual datasets, a location dataset and a movie rating dataset. To measure the effect of various filters on recommendation ac-curacy, we adopt an application-specific definition of rec-ommendation accuracy, as in [2]. For each dataset, we use standard recommender algorithms and measure the effect of the filters on the output of these algorithms. For example, for movie ratings prediction, we measure the RMSE with and without the filter for the same algorithm.
We modeled a location recommender service with a dataset of Gowalla check-ins collected from June to October 2010 1 We studied mainly Austin, Texas, which was Gowalla head-quarters and had the highest amount of activity. We also studied three other cities: Los Angeles, New York, and Dal-las (see Table 1).

We considered a simple scheme of each user giving a bi-nary positive rating to each location visited. Locations not visited were considered unrated. We ignored the number of visits. In fact, in our dataset, over 80% of the ratings were the result of a single visit by a user, i.e. checking in multiple times to the same location was less common than checking We thank Betim Berjani and Thorsten Strufe for this data. in once. To give users a realistic chance of checking into any location, we confined ourselves to one city at a time.
We randomly chose 20% of each user X  X  ratings as a held-out test set. We trained a Top-N recommender based on the remaining ratings. The recommender X  X  task is to predict the likelihood of the remaining (user, location)-pairs.
We used the following standard algorithm for computing recommendations, following [7]. We did not try to optimize the algorithm; our goal was not to achieve the best possible accuracy, but to choose a representative algorithm and see the effect on accuracy of various data attribute values. For each user, we form a binary vector u where u [ i ] = 1 if the user has visited location i and u [ i ] = 0 otherwise. From [7], we calculate the cosine similarity between any two users u and v by
The normalized similarity measure c u ,i of u to location i is given by the fraction of users who checked into location i weighted by similarity: where L i are the users who have visited location i .
We took the user X  X  top N predicted locations, as given by our similarity measure, and calculated the number of hits, i.e. the number of locations in the test set in this set of N locations. We measured performance using precision and recall. Precision is the fraction of recommendations that are hits; precision@5 means the precision with 5 recommenda-tions. The recall is the number of hits out of the number of possible hits (i.e., the size of the test set for the user). We use the macro-averaged recall, the average over all users of each user X  X  recall. As explained in [7], low precision and re-call numbers are expected with such a Top-N recommender. For Austin, our recommendations are an order of magnitude better than random recommendations: five random recom-mendations would have a precision of approximately 0 . 005. User Hardship. We considered two measures of user hard-ship. In the first, we used KMeans (with two centroids, mod-eling home and work/school) to cluster each user X  X  training points. We then ranked each user X  X  training points accord-ing to their minimum distance to the centroids. In the sec-ond, for every point, we calculated the minimum distance to any of the user X  X  other training points. We call the two user hardship measures  X  X Means X  and  X  X ensity X . For each measure, we used differential data analysis to rank a user X  X  training points according to the measure: we divided into deciles, and omitted each decile in turn to see the effect on accuracy.

Figure 1 displays our results. We take precision@5 as our proxy for accuracy. We found that recall and precision be-haved very similarly in our experiments. Also similar were precision and recall with N = 10 and N = 20 recommenda-tions.

Observe that user hardship segregates the data well with respect to effect on recommendation accuracy. With the Austin data, we did 20 trials of randomly removing 10% of the training data and computed a mean and standard de-viation. The ten sample values of our precision@5 statistic ranged from  X  3 . 76 to 2 . 60 standard deviations away from the mean of the random removals, and only 3 of the 10 sam-ple values were within one standard deviation. In fact, by removing some deciles the accuracy actually became signif-icantly better than with all the data, implying these deciles have the effect of noise.

The general trend in the four cities we tried was that the lower hardship deciles (i.e., deciles 2, 3, and 4) were most im-portant for accuracy and higher hardship deciles (i.e., deciles 8, 9, 10) were least important and even could be considered noise. One intuitive explanation is that low-to mid-user hardship is the optimum zone for discovering user prefer-ences. A user would not usually endure high user hardship without other reasons besides just his preferences. Austin is a notable exception for the last decile. The Density user hardship measure generally spreads data points better than KMeans, perhaps owing to the fact that location traces are not defined by only two centroids for many users. We ex-pect the two measures to become closer as the number of KMeans centroids increases.
 Timestamp. The Gowalla dataset consists of timestamped checkins, so we also tested whether the timestamp attribute could be used to predict the importance of data for the rec-ommender. These timestamps are in local time of the user. We used our technique of differential data analysis and di-vided up the day into time intervals so that removal of check-ins for each interval corresponds to removing about 10-15% of the data from the training set. We chose this granularity of time interval so one can easily compare with removing 10% of the data randomly or with removing a decile of data using some other attribute. Note that in our experiments we used a binary measure of the user X  X  preference, so removal of a checkin from one time interval does not affect the train-ing set in the case that the user has checked into the same location in a different time interval.

Our results are shown in Figure 2. Overall, timestamp seems less predictive of accuracy than user hardship. The most useful data was around 8 p.m.  X  midnight, and post-2 a.m. data was least useful. In three out of the four cities, the data from 2 a.m.  X  4 a.m. seemed to even confuse the rec-ommender and decreased accuracy. There are also notable differences in the cities, perhaps related to culture and ge-ography (at least for Gowalla users). For instance, the more important and less important data for Los Angeles came a few hours later than for the other cities.
In this section we examine the stability of our findings, i.e. whether the relative ranking of deciles will change with new data. We used the Austin data (the city with the most data) and the Density user hardship measure. We performed two experiments, one indicating stability with respect to differ-ent test and training sets and one indicating stability with respect to different sets of users. We found that using differ-ent sets of users was slightly less stable than using different data sets (from the same set of users), but in either case there was consistency in the less important and more important deciles. We did not study stability over time, although that would be another interesting dimension to study.

In our first experiment, we divided the users into four dis-joint sets and with any of the user sets, we get a similar qualitative ranking of the deciles: the last and the closer deciles are important, and the middle deciles through Decile 9 are less important (see Figure 3a). In our second experi-ment, we examined stability with different sets of data. The resulting plots of accuracy versus decile removed all have similar shapes, but appear to be shifted relative to one an-other due to the differing test sets (see Figure 3b). We note that for all trials, the four most important deciles were al-ways deciles 1 through 3 and 10. The five least important deciles were always contained in deciles 4 through 9. We consider movie recommendation as another case study. We used the well-studied MovieLens 1M dataset [6] which contains 1,000,209 anonymous ratings of 3,952 movies made by 6,040 users. Ratings in MovieLens range from one star to five stars. We divided the dataset into a training set and test set by randomly putting 20% of the ratings for each user in a held-out test set. The remainder is the training set.
We used the Biased Stochastic Gradient Descent (SGD) and Alternating Least Squares (ALS) collaborative filtering algorithms to predict user ratings for movies in our dataset, and Root Mean Squared Error (RMSE) and Mean Average Error (MAE) metrics to measure accuracy. For clarity and space, we present only results with the RMSE metric and the Biased SGD algorithm. Results with MAE and ALS were very similar. Biased SGD and ALS are based on a latent fac-tor model found through matrix factorization. We used 20 factors, so users and movies are each summarized by factor ( b) Different test and training sets vectors of length 20. We did not search for optimal algo-rithms or do extensive parameter selection. Again, our goal was not to reduce error but to see which parts of the data had the most effect on error. We used the implementations from [4].

Berkovsky et al. [3] showed previously that high/low rat-ings were most important to the recommender. We vali-dated their results using our technique of differential data analysis. We divided each user X  X  ratings into 10 equal deciles according to value, so that Decile 1 contains the user X  X  low-est ratings and Decile 10 contains the users high ratings. Figure 4 shows our results when we remove each decile in turn. We ordered each user X  X  ratings in the training set and examined the effect of removing successive rating deciles. For example, when we removed the 10% lowest ratings from each user, we obtained an RMSE of about 0 . 89. The figure indicates that high and low ratings are most important for recommendation accuracy and that removing deciles 3 or 4 affect accuracy the least.
We presented a simple technique called differential data analysis. Using a Gowalla checkin dataset and a novel at-tribute called user hardship, we found that locations closer to a user X  X  usual haunts were most important for recommen-dation accuracy. It would be interesting to apply the concept of user hardship to other types of data besides location data. For instance, activity data can also be classified according to difficulty or resources required. Using the timestamp at-tribute, we found that in general very late-night data is least useful and may even confuse the recommender. Interest-ingly, our results differ from city to city. The root causes for these differences deserve further study. We also applied our techniques to the MovieLens dataset and confirmed pre-vious work that high and low user ratings are most impor-tant to the recommender. It would be interesting to explore other data attributes using our technique, for example, to determine which part of the graph for social network data is actually important to the recommender. Finally, our work has applications in user privacy and data reduction; [1] has experiments in this direction, but further study is needed.
