
Office of Research, University of the Sunshine Coast, QLD, Maroochydore, Australia School of Science, Information Technology and Engineering, University of Ballarat, Ballarat, VIC, Australia National ICT Australia, VRL, Melbourne, VIC, Australia 1. Introduction
A loss function measures the disagreement of the prediction values with the actual labels. In clas-sification algorithms, loss functions are often used to construct predictors that achieve high prediction performance. Therefore, the design of loss functions is of a very important issue in the learning theory.
The main goal of a learning process in classification is to construct a function f : x  X  y ,which predicts label y given unlabeled sample x . We then need a criterion for finding the  X  X est X  function f . is defined as risk of f 0-1 loss function.

Since P is unknown, we can only measure the agreement of a candidate function with the data. This is called the empirical risk :
However, even though 0-1 loss function is an  X  X deal X  loss function, making as few mistakes as possible, trying to optimize the 0-1 loss function directly leads to a nonconvex, NP-hard optimization problem [5]; it is insensitive to the magnitude of f and regularization of f is meaningless. Therefore, a number of surrogate convex and nonconvex loss functions have been proposed in the literature. Some examples for convex loss functions include the square loss function [21], the hinge loss function [37], the smoothed hinge loss function [36], the modified square loss function [44], the exponential loss function [15], and are the examples for nonconvex loss functions.

In this paper, we propose a new nonconvex loss function that will be called the smoothed 0-1 loss a minimal condition for a loss function to be appropriate for approximation of the 0-1 loss function or the Bayes decision rule. A classification algorithm is developed based on the proposed loss function. To compare the performance of different loss function based classification algorithms, a set of experiments that the proposed smoothed 0-1 loss function is robust, especially for those noisy data sets with many outliers.

This article is organized as follows. We review some existing loss functions in Section 2 and 3. We describe the formulation and properties of the smoothed 0-1 loss function in Section 4. In Section 5 we propose a binary classification algorithm. Numerical experiments are presented in Section 6. Finally, we give concluding remarks in Section 7. 2. Convex loss functions
In the literature, loss functions are commonly assumed to be convex [38]. The main advantage of such a loss function is the computational simplicity that helps to avoid complex global optimization approaches. Square loss and hinge loss are the most commonly adopted loss functions. There are also some other convex loss functions, including the smoothed hinge loss function [36], the modified square loss function [44], the exponential loss function [15], and the log loss function [16].
Among the convex loss functions, the square loss function V ( yf ( x )) = ( y  X  f ( x )) 2 is the most inexpensive one; solutions in this case can be obtained merely through solving linear equations. This outstanding feature in computational efficiency makes the square loss an appealing tool for large data sets [21].

By adopting the square loss function, we can have a Regularized Least Squares Classification (RLSC) model [37]. It is also called Regularization networks by Evgeniou et al. [13], and Proximal Support Vector Machine by Mangasarian and Wild [31]. To put it in the same framework as SVMs, a simple transformation can be made for the square loss for binary classification ( y  X  X  X  1 , 1 } ):
Considering the class of linear functions f ( x )= x T w + b, we have the regularized least squares classification model
We can see that high penalties are given to those misclassified examples far from the origin, which make the corresponding model liable to be dominated by the outliers. Even worse, unlike other loss functions, the square loss function is not monotonically decreasing, which also penalizes those correctly classified examples with large positive value of yf ( x ) .
 The classical SVM arises by considering the hinge loss defined as the constraint is mounted [36].

Using the hinge loss, we have the following regularization problem:
If we consider the space of linear functions and use slack variables  X  i , corresponding to the penalty we pay at data point x i , the problem becomes: where w and b is from the linear function f ( x )= x T w + b .

In SVMs, the support vectors represent the most informative data points and compress the information training examples can be discarded. This, along with some geometric properties of SVMs such as the interpretation of the Reproducing Kernel Hilbert Space (RKHS) norm of their solution as the inverse of the margin [41], is a key property of SVMs and might explain why this technique works well in many practical applications [14]. However, the foundation of SVM X  X  margin theory becomes much less solid in non-separable cases [39].
 derivative at z =1 . Rennie and Srebro [36] proposed a smooth version of the hinge loss called smoothed hinge loss [36].

Smoothed hinge loss preserves important features of the hinge loss, and is easier to minimize by using the direct derivative.

Similar to hinge loss, Zhang and Oles propose a modified square loss Eq. (7) with smooth deriva-tive [44]
Compared with the hinge loss and the modified hinge loss, the modified square loss is much more sensitive to outliers and large errors.

There are some other popular loss functions, for example, the exponential loss function Eq. (8) used by Adaboost [15], and the log loss function Eq. (9) employed by Logistic Regression [16] (see Fig. 1).
Unfortunately, in real application, data sets tend to be non-linearly separable. The drawback of convex loss function is that outliers are guaranteed to play a maximal role in determining the decision hyper-plane, or in other words, the decision hyperplane tends to be dominated by outliers, even when the ratio of the outliers is small.

Based on robust statistics, Kanamori pointed out that the derivative of the loss function directly af-fects the sensitivity of the boosting algorithm [18]. It is obvious that when the derivative of the loss function is unbounded, an extremely large weight can be put on outliers and therefore affect the gener-alization performance of the estimation. Kanamorie et al. provided detailed analysis of the robustness of estimators [19]. A class of robust loss functions with a bounded derivative are proposed. These loss of the parameter space of the estimator.

Madaboost: A modification of AdaBoost [12] is an example of this type loss function defined as follows: A comparison of Exponential Loss, Logistic Regression Loss, and Madaboost Loss are plotted in Fig. 2.

The property of bounded derivation of loss functions to some extend make the classifier less affected by outliers. However, the negative impact of outliers to classifier has not been removed. The weight of outliers are still unbounded under this situation.

There are some other attempts to improve the robustness of training to outliers. One attempt is a direct approach by formulating outlier detection and removal directly from the data sets. Most of these works focus on unsupervised learning [1,8], while Xu et al. [42] focuses on supervised case. The other attempt defines an upper bound and makes the loss stop increase after a certain point, which is also called a nonconvex loss function [10,11,33,35]. 3. Nonconvex loss functions
It is quite desirable that a loss function does not assign an  X  X nfinite X  cost for just one misclassified example. This means that a  X  X ood X  loss function should be bounded. This naturally leads to the study of nonconvex loss functions, although some prior research [10,11] show that it is possible to achieve an SVM classification algorithm where training errors are no longer support vectors. Different nonconvex loss functions have been introduced. They clearly associated with the potentially high cost of solving related nonconvex optimization problems. 3.1. Non-smooth nonconvex loss functions First we mention some non-smooth nonconvex loss functions. Mason et al. proposed heuristic Direct Optimization Of Margins ( DOOM) algorithm [33] based on sigmoid loss function (see Fig. 3) defined on the interval yf ( x )  X  [  X  1 , 1] as follows where  X   X  (0 , 1) ,and  X  is fixed at 0 . 1 . Parameter  X  plays the role of a complexity parameter. Shen et al. [39] proposed a nonconvex loss function (see Fig. 3).

The corresponding algorithm is called  X   X  -learning X , which uses the initial guess obtained from either an SVM or a stochastic search. Moreover, multiple starting values are adopted to prevent the algorithm from being trapped with a local optimizer.
 Ramp Loss (see Fig. 4) is proposed by Collobert et al. [10,11].
 where  X  1 &lt;s 0 is a hyper-parameter to be chosen. Concave-Convex Procedure (CCCP) [43] is adopted as the global optimization method to solve the corresponding nonconvex optimization problem.
Perez-Cruz et al. propose an Iterative Re-Weighted Least Squares (IRWLS) procedure [35], through which the Support Vector Classification (SVC) solution can be obtained for any convex or nonconvex loss function. However, the IRWLS procedure does not guarantee that the global minimum is found, In their experiment, nonconvex Sigmoid loss function shows better performance than the conventional SVC convex loss functions. 3.2. Smooth nonconvex loss functions The loss functions mentioned above are not differentiable, some are even not continuous (  X  -loss). This makes the corresponding optimization problems very difficult to solve by some efficient optimiza-tion methods, for example Quasi-Newton Methods. Taking into account this drawback, several smooth nonconvex loss functions have been introduced in the literature. Below we mention some of them. Mason et al. introduce a nonconvex normalized sigmoid lost function [34] (see Fig. 5) An algorithm DOOM II is proposed corresponding to the normalized sigmoid loss function. A similar loss function is defined in 2-layer Neural Newworks [21] (see Fig. 5):
Krause and Singer propose a quite similar loss function, called the Logistic difference loss func-tion [20]: The upper bound of logistic difference loss function is controlled by parameter  X  .

Loss functions like Eqs (14) X (16) are all differentiable. A possible drawback of these loss functions is or not. In other words, the value of these loss functions is always positive and therefore  X  X ero loss X  cannot be achieved in any case.

The above mentioned functions are just some examples for nonconvex loss functions for binary clas-sification where yf ( x ) is scalar. For multi-label classification there are many other measures/metrics projections of f ( x ) on a simplex generated by vector y with elements 0 and 1. This approach, initially developed for Adverse Drug Reaction Problems (see also [26] for regularized loss functions), has led to a special class of data called Shorter Featured Multi-Label (SFML) datasets [30]. It is demonstrated that in classification on SFML datasets this loss function works much better than some benchmark ap-proaches like SVM and BoosTexter, in spite of the fact that the square loss function fails in this class of classification problems. Note that this approach uses quite sophisticated global optimization proce-dure based on the global optimization method introduced in [27,28]. A similar loss function to what is suggested in [29] would the Cosine similarity between f ( x ) and y, however in this case corresponding global optimization problems would be too difficult to tackle.

We summarize some properties of reviewed loss functions in Table 1. 4. The smoothed 0-1 loss function and its properties
According to the analysis of different convex and nonconvex loss functions, it is worthwhile to discuss some useful properties that would be desirable from a loss function V .
 Zero Penalty : V ( z )=0 ,forall z 1 .
 Upper Boundedness : The inequality sup | z | b | V ( z ) | &lt;  X  holds for any b&gt; 0 . Differentiability : V is continuously differentiable.
 any A&gt; 0 .

Zero Penalty property is extracted from the hinge loss function, this property makes sure that those data points that have been correctly classified by the classifier and located beyond the margin will not be penalized. Upper Boundedness property is derived from nonconvex loss functions, which makes sure optimization algorithms can be applied. Based on robust statistics, the derivative of the loss function B-robust loss functions [19], which makes sure that the loss function satisfies the Lipschitz condition |
V Based on these properties, a new class of loss functions is constructed by the following manner: The first and third pieces of Eq. (17) satisfy Zero Penalty property and Upper Boundedness property. can not make this new loss function differentiable. We therefore define this polynomial function as a cubic function (the simplest case) ties, we have the following equations: By solving these simple linear Eq. (18), we obtain Definition 1. Given a&gt; 0 ,b&gt;  X  1 , a smoothed 0-1 loss function V ( t ) is defined as The derivative of the smoothed 0-1 loss function is:
The smoothed 0-1 loss function and its derivative are plotted in Fig. 6. When b  X  X  X  1 , the correspond-ing optimization problem will become intractable, because gradient will be 0 almost everywhere and no descent direction could be found to optimize the objective function. When b  X  X  X  , the loss function loses its justification because at all data points we have 0 or almost 0 loss.

By incorporating the smoothed 0-1 loss function into Tikhonov regularization framework, we can have a new linear binary classification model involving the class of linear functions f ( x )= xw + b where V ( t ) is defined by Eq. (19).

This is an unconstrained global optimization problem. To solve Eq. (21), global optimization method is required. Considering the loss function is strictly bounded between 0 and 1, the corresponding algorithm will be less sensitive to the choice of  X  . Furthermore, the degree of improvement will become dramatic as the sample size increases, particular for noisy data sets with many outliers.

It is obvious that the computational complexity of the nonconvex loss based algorithms will be sub-stantially higher than existing convex loss function based models, for example SVMs. We believe that the significant theoretical advantages will make it worthwhile to pursue further computational develop-ments.

However, it should be noted that the replaced new loss function in Eq. (21) may lead to inconsistency, that is, the optimum of Eq. (21) may be different from the optimum of the 0-1 loss function based model. A consistency analysis is necessary for the new loss function. Definition 2. Let p ( x )= P ( Y =1 | X = x ) be the conditional probability of the positive class. Then the decision (theoretically optimal classification rule) with the smallest generalization error is f
Before we study the smoothed 0-1 loss function, it is worthwhile to briefly review the theoretical research of the convexity-based loss functions.
 Definition 3. A loss function V is Fisher consistent [22 X 24] or classification-calibrated [4] if the pop-ulation minimizer of the risk E [ V ( Yf ( X ))] for all measurable functions leads to the Bayes optimal decision rule.

Consistency results provide reassurance that optimizing a surrogate loss function does not ultimately hinder the search for a function that achieves the Bayes optimal risk, and thus allow such a search to proceed within the scope of computationally efficient algorithms. Under sone assumptions it can be shown [25] the Bayes-risk consistency of methods based on minimizing convex surrogates of the 0-1 loss function. The consistency for the SVM has been demonstrated by Steinwart [40], some other results have been presented by Breiman [7], Jiang [17], and Mannor and Meir [32].

In machine learning theory, it is necessary to find general quantitative relationships between the ap-proximation and estimation errors associated with loss function V and those associated with the 0-1 loss to the Bayes optimal rule of classification.
 Theorem 1. Let P be a probability distribution on X X { X  1 } of the random pair ( X ,Y ) ,p ( x )= P ( Y =1 | X = x ) be the conditional probability of the positive class and V be a smoothed 0-1 loss minimizer of E V [ Yf ( X ) | X = x ] , then f  X  minimizes E V ( Yf ( X )) and E (1  X  Sign ( Yf ( X ))) . Proof. For any fixed x , we define a function Clearly to prove the theorem, we need to show that Eq. (22) is true.
 The derivative of A ( z ) is calculated as follows where V ( z ) is defined in Eq. (20). Denote 1. Let  X  1 &lt;b 0 .Then 2. Let 0 &lt;b&lt; 1 . We have 3. Let b =1 . Then V (  X  z )= a  X  V ( z ) and 4. Let b&gt; 1 . Then, we have
Theorem 1 says that the smoothed 0-1 loss function based model Eq. (21) estimates the Bayes classifier  X  f = Sign ( f  X  ) rather than the Bayes decision function f  X  . The Bayes classifier is the ideal optimal of the smoothed 0-1 loss function is essential because the optimal performance of realized by using the smoothed 0-1 loss function although it differs from the 0-1 loss function. We should note that unique minimizer is not required in Theorem 1.
 Remark 1. Fisher consistency is a desirable condition of a loss function, even through this property alone does not guarantee a good performance in classification. We should note that there is no constraint on the measurable function space H in Theorem 1. For some special function spaces, for example linear function space, Fisher consistency may hold only in linear separability conditions.

Given loss function V , the prediction accuracy of a classifier f is measured by the risk that is defined as the expectation of the loss function: where P is a probability measure. In applications the pr obability distribution is in order unknown. In to considering the empirical risk defined by
In the next section we consider Problem Eq. (21) that uses the above empirical risk on the class of linear functions f and discuss some optimization methods for solving this problem. 5. Solving problem Eq. (21)
Because to the nonconvexity of the smoothed 0-1 loss function, problem Eq. (21) becomes a quite we discuss several optimization methods for solving this problem and propose a new efficient scheme for this purpose by incorporating the Quasisecant method [3] as a local optimization method and the hinge loss based convex model for a good starting point.
 The Quasisecant Method is developed for solving the following unconstrained minimization problem: where objective function f is assumed to be locally Lipschitz, but not necessarily differentiable or con-vex. Therefore, this method can be applied to both smooth and non-smooth functions, which make it possible to compare the performance of different (smooth and non-smooth) loss functions under the framework of Tikhonov regularization model.

In numerical experiments we use five data sets (see Table 2) from UCI repository for the construction as a = b =1 .

First we examine the performance of three local optimization methods in solving problem Eq. (21) starting from 100 randomly selected points in [  X  10,10]. These methods are the Conjugate Gradient Method, the Quasi-Newton Method, and the Quasisecand Method.

Computational results are listed in Table 3, where the best, the worst and the average solutions out of 100 runs from 100 random starting points are listed. Table 3 shows that the Quasisecant method performs better on all data sets.

In order to demonstrate the main difficulties in the solution process we provide some calculations related to the liver-disorders data set where the best and worst function values obtained by the Quasi-Newton and Conjugate Gradient methods (0.4212 and 0.5788) are quite worse compared with the Qua-sisecant method (0.2503 and 0.4212). We plot the graph of the objective function where one feature is optimization purposes; that is, the objective function has a sharp decrease at the solution and is almost many local minima that naturally demands efficient global optimization techniques.

In Table 4, we also list the average CPU times over 100 runs. As it could be expected, the CPU time for Quasisecant method is much longer than the other two traditional local methods. However, taking into account the much better solutions obtained, these CPU times are quite acceptable. Therefore, in our future numerical experiments, we use the Quasisecant Method as the local optimization method for the smoothed 0-1 loss function based binary classification algorithms. 5.1. Choice of good starting points
In nonconvex optimization problems, even though random starting points may help local optimization methods find good solutions, it might be quite time consuming. Moreover such a solution might not be we propose an alternative way for choosing a  X  X ood X  starting point. It would be quite natural to use some for the nonconvex optimization problem. In this case the question arises: which convex loss function is more suitable for this purpose? Below we compare several popular convex loss functions: Square Loss, Hinge Loss, Modified Square Loss, and Exponential Loss.

Table 5 shows the optimization results by using different starting points calculated by several popular convex loss functions. The top 2 solutions are shown in bold font. From this table we can see that the hinge loss is more reliable compared the other convex loss functions. As we have discussed, the hinge loss has some good properties which make it an efficient loss function. For example, unlike the square loss and the exponential loss, it does not penalize those data points that have been correctly predicted, and it does not aggressively penalize those misclassified data points that are far away from their own class.
 Therefore, we propose the following algorithm for binary data classification, where QSM is used as an optimization solver. Firstly, a convex optimization problem is solved by adopting the hinge loss function to generate a good starting point. Then a nonconvex optimization problem is generated by adopting the smoothed 0-1 loss function to  X  X efine X  the solution obtained from previous convex optimization problem. ization term  X  w 2 . 6. Numerical experiments
In this section, we investigate the classification performance of different loss functions in the frame-work of Tikhonov regularization. These loss functions are: the smoothed 0-1 loss function Eq. (19), the hinge loss function Eq. (3), the square loss function Eq. (1), the ramp loss function Eq. (13), and the normalized sigmoid loss function Eq. (14).

To observe the influence of outliers or mislabels on different loss function based classifiers, outliers have been conducted on both synthetic and real data sets [6,19,20,34,42]. Mason et al. compare normal-ized sigmoid loss function based DOOM II algorithm with the exponential loss based AdaBoost [34]. 0%, 5%, and 15% label noise are applied to nine UCI data sets. Kanamori et al. flip labels of training set randomly as mislabels (mislabels are mixed uniformly) [19]. New training samples with low condi-robustness properties of the Optimal Dyadic Decision Tree on two dimensional artificial classification data sets [6]. Two kinds of data degradation are applied: 1) adding nuisance dimensions of pure noise (2, 4, 6, and 8 dimensions are used) and 2) flipping an increasing number of training example labels (0%, 2%, 10%, and 20% labels are flipped). By combining these two types of degradation, 16 different data sets are setup for each classification problem.

Similar to the experiment conducted by Kanamori et al. [19], we compare the performance of different loss functions in two different experiments. First, the robustness of the proposed method is investigated for a toy problem  X  a synthetic two demential binary classification problem. Second, we conduct the comparison experiment on UCI benchmark repository. 6.1. Experiments on synthetic data
Synthetic data has two key advantages over real data: 1) the optimal classifier of the problem is known and 2) outliers and mislabels can be injected into a synthetic data set following some strategies.
In this setup, labeled data points are generated from a fixed probability, and class labels of a few data points are flipped as outliers. The input vector x = { x 1 ,x 2 } is uniformly distributed on a two where f ( x )= x 2  X  3 x 1 ,  X  is the degree of mislabels.

We randomly select a % data points as outliers from the top 10 a % data points sorted in descending order of | f ( x ) | , and the label of these data points are flipped.

In this experiment, we generate a training set with 200 data points. This training set has mislabels (  X  =0 . 5 ) and outliers ( a =2 ). The training samples are shown in Fig. 8. We also generate 5000 data observe the robustness of classifiers to the training set.
 Before training a classifier, the penalization multiplier  X  introduced in Eq. (21) should be decided. Unfortunately, a good value of this constant cannot be directly derived. The most commonly adopted approach to estimate  X  is via cross-validation techniques. In this setup, we use 10-fold cross validation to select the  X  X est X   X  from a specified list.

Next, the training results by different loss function based classification algorithms are compared from that the hinge loss works better on the data sets without outliers. However, nonconvex loss functions outperform convex loss functions when outliers are inserted into the data set. In addition, comparing the nonconvex loss functions, we found that the smoothed 0-1 loss function is more resilient to outliers. Based on the above observation, we conclude that the smoothed 0-1 loss function is effective for data classification and, in particular, is robust to outliers in the training set. 6.2. Experiments on benchmark data
The following benchmark data sets are used in the experiments: the liver-disorders, diabetes, heart disease, Australian, and vote (see Table 2) from UCI repository [2]. Considering that there are only a small number of data points provided in these binary data sets, 10 fold cross validation are adopted. To validate the smoothed 0-1 loss function based algorithm, we first make a comparison with classical SVM. In the experiment, we use LIBSVM [9]. LIBSVM is an integrated software for support vector classification, which implements an SMO-type algorithm. Because the data sets are relatively small, we use cross validation to find the best parameters for the LIBSVM. After the best parameters are found, the whole training set is trained again to generate the final classifier.

Table 7 shows that in the training section, LIBSVM has better performance than our algorithm, how-ever, in the test part, our algorithm outperforms LIBSVM. The reasons behind this phenomenon are variable, however the robustness of our algorithm can be the most significant one.

For comparison of the performance of difference loss functions, we also applied the Hinge loss func-tion, Square loss function, Ramp loss function, and Normalized sigmoid loss function. The average results of the 10 fold train-test splits are reported in Table 8. From this comparison, we can see that the nonconvex loss function based algorithms can achieve better generalization accuracy compared to convex loss function based algorithms. The overall training accuracy of nonconvex loss function based algorithms is better than convex loss function based algorithms.

In particular, from Tables 7 and 8 we observe that the smoothed 0-1 loss function can achieve better generalization accuracy on data sets with low testing accuracy, like the liver-disorders and diabetes.
The above comparison demonstrates that the smoothed 0-1 loss function performs better on noisy data sets, in particular data sets with many outliers.

Table 9 presents the average CPU times (in seconds) over 20 runs for one overall training phase (e.g. data loading, error computing and optimization). As expected, among five classification algorithms, function based algorithm is the quickest among the nonconvex loss function (e.g. ramp and normalized sigmoid loss functions) based algorithms. 7. Conclusion
In this paper we propose a new loss function called the smoothed 0-1 loss function for binary classifi-ing non-convex optimization problems. We examine the performance of different local optimization methods on the smoothed 0-1 loss function based classification problems and then propose an efficient scheme, based on the Quasisecant method, that can be used to find  X  X ood X  solutions to these problems.
Different classification algorithms are considered based on several convex and nonconvex loss func-tions. The performance of different loss functions based classification algorithms are compared. Nu-merical experiments are conducted on both synthetic data set and several binary data sets from UCI repository. The results show that the proposed smoothed 0-1 loss function works better on noisy data sets compared with the other nonconvex and convex loss functions considered in the paper.
In our future studies, we plan to extend the smoothed 0-1 loss function based binary classification algorithm to multi-class and multi-label data classification, and perform experiments on large data sets. References
