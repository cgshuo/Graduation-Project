 All-pairs similarity search can be implemented in two stages. The first stage is to partition the data and group poten-tially similar vectors. The second stage is to run a set of tasks where each task compares a partition of vectors with other candidate partitions. Because of data sparsity, access-ing feature vectors in memory for runtime comparison in the second stage, incurs significant overhead due to the presence of memory hierarchy. This paper proposes a cache-conscious data layout and traversal optimization to reduce the execu-tion time through size-controlled data splitting and vector coalescing. It also provides an analysis to guide the opti-mal choice for the parameter setting. Our evaluation with several application datasets verifies the performance gains obtained by the optimization and shows that the proposed scheme is upto 2.74x as fast as the cache-oblivious baseline. H.3.3 [ Information Search and Retrieval ]: Search Pro-cess, Clustering; H.3.4 [ Systems and Software ]: Perfor-mance evaluation Similarity search, data traversal, memory hierarchy
All Pairs Similarity Search (APSS) [6], which identifies similar objects among a given dataset, has many important applications. For example, collaborative filtering provides recommendations by determining which users have similar tastes [29, 7], search query suggestions identifies queries with similar search results [22], web mirrors and plagiarism recog-nition [25], coalition detection for advertisement frauds [20], query suggestions [22], spam detection [8, 16, 14], cluster-ing [5], and finally near duplicate detection [12, 30].
The complexity of a na  X   X ve APSS can be quadratic to the dataset size. Previous research on expediting similarity com-puting, developed filtering methods to eliminate unnecessary computations [6, 28, 2], applied inverted indexing to com-pare vectors only when they share features [18, 21], and used partitioning and parallelization techniques [1]. The LSH based mapping can approximately map vectors to the same bucket when they are potentially similar. However, none of the previously developed methods have considered the impact of memory hierarchy on execution time. The main memory access latency can be 10 to 100 times slower than the L1 cache latency. Thus, the unorchestrated slow memory access can significantly impact performance.
In this paper, we exploit memory hierarchy and develop orthogonal techniques to improve the e ffi ciency of APSS by optimizing data layout and traversal methods. Specifically, we investigate how data traversal a ff ects the use of mem-ory layers. This method is also motivated by the work in query processing [23] and sparse matrix computation that considers cache optimization in computation arrangement.
Similarity comparisons can be performed through a num-ber of tasks where each of them compares a partition of vectors with other candidate vectors [1].

We propose two algorithms PSS1 and PSS2 to exploit the memory hierarchy explicitly. PSS1 splits the data hosted in the memory of each task to fit into the processor X  X  cache and PSS2 coalesces data traversal based on a length-restricted in-verted index. We provide an analytic cost model and iden-tify the parameter values that optimize the performance. Hence, the contribution of this paper is a memory-hierarchy aware framework for fast similarity comparison with opti-mized data layout and traversal.

The rest of this paper is organized as follows. Section 2 reviews background and related work. Section 3 discusses the design framework and PSS1 algorithm for cache-aware data splitting. Section 4 analyzes cost model of PSS1 and demonstrates the impact of the parameters on memory ac-cess performance. Section 5 presents the optimization using vector coalescing named PSS2. Section 6 is our experimental evaluation that assess PSS1 and PSS2. Section 7 concludes this paper.
Following the definition in [6], the APSS problem is de-fined as follows. Given a set of vectors d i = { w i, 1 ,w  X  X  X  ,w i,m } , where each vector contains at most m non-negative features and is normalized to a unit length, then the cosine-based similarity between two vectors is computed as:
Two vectors x, y are considered similar if their similar-ity score exceeds a threshold  X  , namely Sim ( x, y )  X   X  . The time complexity of APSS is high, especially for a big dataset. There are application-specific methods applied to reduce the complexity. For example, text mining removes stop-words or extremely high frequent features [18]. We use such prepro-cessing throughout the experiments in Section 6. Generally, there are two groups of optimization techniques developed in the previous work to accelerate APSS.
Cache optimization for computationally intensive applica-tions is studied in the context of general database query pro-cessing [23, 19] and matrix-based scientific computing [10, 9, 27, 4]. Motivated by these studies, we investigate the op-portunities of cache-conscious optimization targeting APSS.
In this section, we give an overview of the partition-based comparison framework [1] and then present the caching op-timization strategies as our contributions.
The framework for partition-based similarity search (PSS) consists of two steps. The first step is to divide a dataset into a set of partitions. During this process, the dissimilarity relationship among partitions is identified so that unneces-sary comparisons among them are avoided. The second step is to assign a partition to a task and each task compares this partition with other potentially similar partitions.
Dissimilarity-based partitioning identifies dissimilar vec-tors as much as possible without explicitly computing the product of their features. One approach is to use the fol-lowing inequality that uses the 1-norm and  X  -norm of each vector.

The partitioning algorithm sorts the vectors based on their 1-norm values and uses the sorted list to identify dissimi-the outcome of the dissimilarity detection in the above par-titioning. The details for the above static partitioning is discussed in [1]. This paper focuses on optimizing the task execution after the static partitioning is applied. Note that this scheme can also be applied with LSH when approxima-tions are allowed. When vectors are mapped into a set of dissimilar buckets using LSH, some buckets can still be big and the static partitioning can be further applied to divide such buckets.
 Figure 1: A PSS task compares the assigned partition A with other partitions O .

Figure 1 depicts a task for partition-based similarity search interacting with a CPU core with multiple levels of cache. Two or three cache levels are typical in today X  X  Intel or AMD architecture [17, 15]. We assume that the assigned partition A fits the memory of one machine as the data partitioning can be adjusted to satisfy such an assumption. But vectors of O can exceed memory and need to be fetched gradually from a local or remote storage. In a computer cluster with the distributed file system such as Hadoop, a task can seam-lessly fetch data from the file system without worrying about the machine location of data.

The memory used by each task has three areas, as illus-trated in Figure 1. 1) Area S : hosts the assigned partition A . 2) Area B : stores a block of vectors fetched from other candidate partitions O at each comparison step. 3) Area C : stores intermediate results temporarily.

Figure 2 describes the function of a PSS task. Each task loads the assigned vectors, whose data structure is in forward index format, into area S . Namely, each vector consists of an ID along with a list of feature IDs and their corresponding weights, stored in a compact manner. After loading the assigned vectors, the task inverts them locally within area S . It then fetches a number of vectors from O , in forward index format, and place them into area B .

Let d j be the vector fetched from O to be processed (Line 5). For each feature t in d j , PSS uses the inverted index in area S to find the localized t  X  X  posting (Line 10). Then weights of vector d i from t  X  X  posting and d j contribute a par-tial score towards the final similarity score between d j d . After all the features of d j are processed, the similarity scores between d j and the vectors in S are validated (Line 17) and only those that exceed the threshold are written to disk. In Compare(S, d j ) , the dissimilarity of vector d with d j can be marked (Line 14) by using a negative value Task ( A , O ) 1: Read all vectors from assigned partition A into S 2: Build inverted index of these vectors and store in S 3: repeat 4: Fetch a set of vectors from O into B 5: for d j  X  B do 6: Compare ( S , d j ) 7: until all vectors in O are fetched Compare (S, d j ) 8: Initialize array score of size | S | with zeros 9: r j  X  || d j || 1 10: for t  X  d j And Posting ( t )  X  S do 11: for d i  X  Posting ( t ) and d i is a candidate do 12: score [ i ]= score [ i ]+ w i,t  X  w j,t 13: if ( score [ i ]+ maxw [ d i ]  X  r j &lt;  X  ) then 14: Mark d i as non-candidate 15: r j = r j  X  w j,t 16: for i =1 to | S | do 17: if score [ i ] &gt;  X  then 18: Write ( d i ,d j ,score [ i ]) for score [ i ]. Array maxw [ ] contains the  X  -norm value of vector d i .
When dealing with a large dataset, the number of vectors in each partition is high. Having a large number of vectors increase the benefits of using inverted indexing as shown in Figure 2. But it has a problem that the accessed areas S or C may not fit in the fast cache. In that case, temporal locality is not exploited, meaning the second access of the same element during any computation will be a cache miss. As we show in the next section, this leads to frequent slow memory access and a significant increase in execution time. Since fast access of each area S , B or C is equally important in the core computation (Lines 12 and 13), one idea is to let area C fit in L1 cache by explicitly dividing vectors of the assigned partition in S into a set of splits and have the task focus on one split at a time.
 Figure 3: A partition in area S is further divided into mul-tiple splits for each PSS1 task. Four data items are involved in the core computation. The striped area indicates cache coverage.
 Figure 3 illustrates this cache-conscious data splitting idea. The corresponding algorithm called PSS1 is shown in Fig-ure 4. First, it divides the hosted vectors in S into q splits. Each split S i is of size s . PSS1 then executes q comparison sub-tasks. Each sub-task compares vectors from S i with a vector b j in B . The access in area C is localized such that array score [ ] and maxw [ ] can fully fit in L1 cache. This improves temporal locality of data elements for area C and Task ( A , O ) 1: Read and divide A into q splits 2: Build an inverted index for each split S i and store in S 3: repeat 4: Fetch a set of vectors from O into B 5: for d j  X  B do 6: for S i  X  S do 7: Compare ( S i , d j ) 8: until all vectors in O are fetched reduces the access time by an order of magnitude. The core computation speeds up as a result.

The data splitting also introduces potential benefits from exploiting the multi-core CPU architecture via threads. Ev-ery time a data block from O is fetched into B , there can be multiple threads running in parallel to execute Compare ( S d ) where d j is a vector in B .

The question is, how to determine the s value of each split so that the caches are best utilized? This is discussed next.
We model the total execution time of each PSS1 task and analyze how memory hierarchy a ff ects the running time. This analysis facilitates the identification of optimized pa-rameter setting. Table 1 describes the parameters used in our analysis. They represent the characteristics of the given dataset, algorithm variables, and the system setting. w d,t Weight of feature t in vector d  X  Similarity threshold k Average number of non-zero features in d
S, B, C Memory usage for each task n Number of vectors to compare per task ( | O | ) s Avg. number of vectors for each split in S b Number of vectors fetched and stored in B
S i A split in area S divided by PSS1 q Number of splits in S h Cost for t -posting lookup in table  X  total Cost of accessing the hierarchical memory m j ( X ) Miss ratio in level j cache for area X D j ( X ) Number of misses in level j cache for area X
D j Total number of access misses in level j cache p s Average posting length in the inverted index p b Average posting length in the inverted index l Cache line size e s , e b , e c Element size in S , B , C respectively f s , f b , f c E ff ective prefetch factor for elements in S , B  X  1 ,  X  2 ,  X  3 Latency when accessing L1, L2, and L3 cache  X  mem Latency when accessing main memory  X  Cost of addition and multiplication The total execution time for each task contains two parts: I/O and computation. I/O cost occurs for loading the as-signed vectors A , fetching other potentially similar vectors, and writing similarity pairs to disk storage. Notice that in fetching other vectors for comparison, the algorithm always fetches a block of vectors to amortize the startup cost of I/O. For the datasets we have used, read I/O takes about 2% of total cost while write I/O takes about 10-15%. Since I/O cost is the same for the baseline PSS and our proposed schemes, we do not model it in this paper.

For each split, the computation time contains a small over-head for the index inversion of its s vectors. Because the inverted index is built once and reused every time a parti-tion is loaded, this part of computation becomes negligible and the comparison time with other vectors dominates. The core part (Lines 12, 13 in Figure 2) is computationally in-tensive. Following Table 1, h is the cost of looking up the posting of a feature appeared in a vector in B . Symbol p is the average length of postings visited in S i (only when a common feature exists), so it estimates the number of itera-tions for Line 10. Furthermore, there are 4 memory accesses in Line 12 and 13, regarding data items score [ i ], w i,t and maxw [ d i ]. Other items, such as r j , and  X  , are constants within this loop and can be pre-loaded into registers. There are 2 pairs of multiplication and addition involved (one in Line 12 and one in Line 13) bringing in a cost of 2  X  .For simplicity of the formula, we model the worst case where none of the computations are dynamically filtered.
For a large dataset, the cost of self-comparison within the same partition for each task is negligible compared to the cost of comparisons with other vectors in O . The execution time of PSS1 task (Figure 4) can be approximately modeled as follows.

Time = q nk (
As s increases, q decreases and the cost of inverted index lookup may be amortized. In the core computation, p s in-creases as s increases. More importantly, the running time can be dominated by  X  total which is the data access cost due to cache or memory latency. The data access cost is a ff ected by s because of the presence of memory hierarchy. We in-vestigate how to determine the optimal s value to minimize the overall cost in the following subsection. Here, we estimate the cost of accessing data in S , B , and C . Define D 0 as the total number of data accesses in per-forming Compare( S i ,d j ) in Figure 4. Define D j as the total number of data access misses in cache level j .  X  i is the access time at cache level i .  X  mem is the memory access time.
To conduct the computation in Lines 12 and 13 of Fig-ure 2, the program needs to access weights of S i , B , score [] and maxw []in C . We model these accesses separately then add them together as follows: D 0 = D 0 ( S i )+ D 0 ( B )+ D 0 ( C )= Table 2: Cases of cache miss ratios for split S i and area C in PSS1 at di ff erent cache levels. Column 3 is the cache miss ratio m j ( S i ) for accessing data in S i . Column 4 is the cache miss ratio m j ( C ) for accessing data in C . Column 5 describes the condition of each case.
 Define D j ( X ) as the total number of data accesses missed in cache level j for accessing area X . m j ( X ) is the cache miss ratio to access data for area X in cache level j .
Table 2 lists six cases of miss ratio values m j ( S i ) and m j ( C ) at di ff erent cache levels j . The miss ratio for B is not listed and is considered close to 0 assuming it is small enough to fit in L1 cache after warm-up. That is true for our tested datasets. For a dataset with long vectors and B cannot fit in L1, there is a small overhead to fetch it partially from L2 to L1. Such overhead is negligible due to the relative small size of B , compared to S i and C . We explain this table in more details from the following aspects. Time = q nk ( h + p s 2  X  )+ nkp s 4  X  1 +( e s We illustrate s value for the optimal case. For the AMD Bulldozer 8-core CPU architecture (FX-8120) tested in our experiments, L1 cache is of size 16KB for each core. L2 cache is of size 2MB shared by 2 cores and L3 cache is of size 8MB shared by 8 cores. Thus 1MB on average for each core. Other parameters are:  X  m =64 . 52 ns ,  X  3 =24 . 19 ns ,  X  3 . 23 ns ,  X  1 =0 . 65 ns , l =64bytes. Weestimate  X  =0 . 16 ns , h =10 ns , p s =10% s , f c = f s = 4 based on the results from our micro benchmark. The minimum task time occurs in Case (2) when S i and C can fit in L2 cache, but not L1. Thus the constraint based on the L2 cache size can be expressed as
While satisfying the above condition, split size s is chosen as large as possible to reduce q value. For Twitter data, k is 18, e s is 28 bytes, and e c is 4 bytes. Thus the optimal s is around 2K.

To show how the choice of s a ff ects the task execution time in Formula (1), we measure the ratio of the data access time (including the inverted index lookup) over the computation time:
This ratio captures the data access overhead paid to per-form comparison computation and the smaller the value is, the better. For Twitter benchmark, the above ratio is 8 for optimum case, while it increases to over 25 for Case (3) and Case (4) where more frequent access to L3 cache is re-quired. The data-access-to-computation ratio deduction is supported by experiment results shown in Figure 5. It shows that by selecting s based on our cost function, we are able to reduce the data-access-to-computation ratio from 25 to 8. When an optimum s is chosen, we manage to dramatically reduce the slow cache/memory access out of the whole task execution time. Figure 5: Y axis is the ratio of actual data access time to computation time for Twitter data observed in our experi-ments.
In PSS1, every time a feature weight from area S i is loaded to L1 cache, its value is multiplied by a weight from a vec-tor in B .As S i does not fit in L1 cache, the utilization of L1 for S i is low. L1 cache usage for S i is mainly for spa-tial locality. Namely fetching one or few cache lines for S to avoid future L1 cache miss when consecutive data is ac-cessed. The benifit of temporal locality is low, because the same element is unlikely to be accessed again before being evicted, especially for L1 cache due to its small size.
Another way to understand this weakness is that the num-ber of times that an element in L1 loaded for S i can be used to multiply a weight in B is low before this element of S evicted out from L1 cache. PSS2 is proposed to adjust the data layout and access structure in B in order to increase L1 cache reuse ratio for S i . The key idea of PSS2 is listed as follows.
Figure 7 illustrates the data traversal pattern of PSS2 with b = 3. There is one common feature t 3 that appears in both S i and B . The posting of t 3 in S i is { w 1 , 3 PSS2Task( A, O ) 1: Read A and divide it into q splits of s vectors each 2: Build an inverted index for each split S i . 3: repeat 4: Fetch b vectors from O and build inverted index in B 5: for S i  X  S do 6: Compare ( S i , B ) 7: until all vectors in O are compared Compare ( S , B ) 8: Initialize array score of size s  X  b with zeros 9: for j =1 to b do 10: r j  X  || d j || 1 11: for Feature t appears in B and S do 12: for d i  X  Posting ( t )in S do 13: for d j  X  Posting ( t )in B and d i is a candidate do 14: score [ i ][ j ]= score [ i ][ j ]+ w i,t  X  w j,t 15: if ( score [ i ][ j ]+ maxw [ d i ]  X  r j &lt;  X  ) then 16: Mark pair d i and d j as non-candidate 17: for d j  X  Posting ( t )in B do 18: r j = r j  X  w j,t 19: for i =1 to s do 20: for j =1 to b do 21: if score [ i ][ j ] &gt;  X  then 22: Write ( d i ,d j ,score [ i ][ j ]) and each iteration of PPS2 uses one element from this list, and multiplies it with elements in the corresponding posting of B which is { w 4 , 3 , w 6 , 3 } . Thus every L1 cache loading for S i can benefit 2 multiplications with weights in B . In comparison, every L1 loading of weights for S i in PSS1 can only benefit one multiplication.

Increasing b values expands the size of areas B and C to store b vectors and a 2D array score [][]. B and C may not fit in L1, or even L2 cache anymore. Since L2/L3 cache has higher latency, cache capacity restricts the value of b from being too large. On the other hand, vectors in B are sparse and b cannot be too small so that there is a su ffi cient number of vectors sharing a feature after coalescing. Our experiment in Figure 11 discusses this issue in more details.
Similar to PSS1, we can conduct a case-by-case analysis for cache miss ratios of PSS2 based on how S i , B and C fit in the di ff erent levels of cache. Then we can derive the s and b ranges in each case.
We have implemented PSS, PSS1 and PSS2 in Java. Dur-ing the evaluation, PSS, PSS1 and PSS2 are applied after data preprocessing. In the default setting, static partition-ing [1] is adopted to partition the dataset then, a set of parallel tasks is executed following either PSS2 or PSS1. In another setting (Table 3), LSH [11, 26] is applied first before static partitioning.
 We also evaluated another design option we refer to as PSS3. PSS3 follows the previous scientific computing re-search that views a sparse matrix as a collection of dense small submatrices and employs BLAS3 to perform subma-trix multiplication [10, 24, 27]. In this case, we represent the feature vectors in S and B as a set of small submatrices and use a highly optimized BLAS3 library called MTJ [13] for the submatrix multiplication.

The evaluation has the following objectives: 1. Compare PSS1 and PSS2 with the baseline PSS us-2. Evaluate the choice and impact of s value for PSS1 3. Illustrate the predicted and observed cache hit ratio. 4. Report the overall parallel performance. 5. Evaluate PSS3 to understand the issues of submatrix
Metrics. We report the running time for di ff erent al-gorithms when the static partitioning is given. Since the number of tasks is fixed, the overall parallel time is propor-tional to the average task running time. Hence we mainly report the average task running time to evaluate the perfor-mance impact of adjusting split size and fetched block size. The cost of self-comparison among vectors within a parti-tion is included when reporting the actual cost. To assess the scalability, we report the overall speedup for the paral-lel performance, and measure the megaflops number as an additional metric. The experiments are mainly conducted on a cluster of AMD nodes where each node has 8 cores with 3.1GHz AMD Bulldozer FX8120 and 16GB memory. They run the Hadoop MapReduce environment. In reporting parallel speedup, we have used a bigger cluster of Intel 12-core nodes and each node has dual Intel X5650 six-core processors and 24GB memory. The following five datasets are used.
The datasets are preprocessed to follow the TF-IDF weight-ing after cleaning and stopword filtering [18]. average task running time includes I/O.

Figure 8 shows the improvement ratio on the average task time after applying PSS1 and PSS2 over the baseline. and each task handles a very large partition that fits into the main memory (but not fast cache). For example, each parti-tion for Clueweb can have around 500,000 web pages. Result shows PSS2 contributes significant improvement compared to PSS1. For example, under Clueweb dataset, PSS1 is 1.2x faster than the baseline PSS while PSS2 is 2.74x faster than PSS. The split size s for PSS1 and s and b for PSS2 are optimally chosen. Figure 9: The average running time in log scale per PSS1 task under di ff erent values for split size s . The partition size S for each task is fixed, S = s  X  q .

The gain from PSS to PSS1 is achieved by the splitting of the hosted partition data. Figure 9 shows the average running time of a PSS1 task including I/O in log-scale with di ff erent values of s . Notice that the partition size ( S = s  X  q ) handled by each task is fixed. The choice of split size s makes an impact on data access cost. Increasing s does not change the total number of basic multiplications and additions needed for comparison, but it does change the traversal pattern of memory hierarchy and thus a ff ects data access cost. For all the datasets shown, the lowest value of the running time is achieved when s value is ranged between 0.5K and 2K, consistent with our analytic results. Figure 10: Each square is an s  X  b PSS2 implementation (where s = S ) shaded by its average task time for Twitter dataset. The lowest time is the lightest shade.

The gain of PSS2 over PSS1 is made by coalescing visits of vectors in B with a control. Figure 10 depicts the average time of the Twitter tasks with di ff erent s and b , including I/O. The darker each square is, the longer the execution time is. The shortest running time is achieved when b = 32 and s is between 5K to 10K. When b is too small, the number of features shared among b vectors is too small to amortize the cost of coalescing. When b is too big, the footprint of area C and B becomes too big to fit into L2 cache.

While PSS1 outperforms PSS in all 5 datasets, there is an exception for Yahoo music dataset. The benefits of PSS2 over PSS1 depend on how many features are shared in area B . The top and bottom parts of Figure 11 show the average and maximum number of features shared among b vectors in area B , respectively. Sharing pattern is highly skewed and the maximum sharing is fairly high. On the other hand, the average sharing value captures better on the benefits of coalescing. The average number shared exceeds 2 or more for all data when b is above 32 (the optimal b value for PSS2) except Yahoo music. In the Yahoo music data, each vector represents a song and features are the users rating this song. PSS2 slows down the execution due to the low intersection of the interest among users. We demonstrate the cache behavior of PSS1 modeled in Section 4.2 with the Twitter dataset. The Linux perf tool is used to collect the cache miss ratio of L1 and L3.
Figure 12 depicts the real cache miss ratios for L1 and L3 reported by perf tool, the estimated L1 miss ratio which is D /D 0 , and the estimated L3 miss ratio which is D 3 /D 2 .L1 cache miss ratio grows from 3.5%, peaks when s =8 K , and gradually drops to around 9% afterwards when s value in-Figure 11: The top is the average number of shared features among b vectors. The bottom is the maximum number of features shared among b vectors. creases. L3 cache miss ratio starts from 3.65% when s =100, reaches the bottom at 1.04% when s = 5K, and rises to al-most 25% when s = 500K. The figure shows that the esti-mated cache miss ratio approximates the trend of the actual cache miss ratio well.

To validate our cost model, we compare the estimated cost with experimental results in Figure 13. Our estimation of cache miss ratios fits the real ratios quite well, reason-ably predicts the trend of ratio change as split size changes. When s is very small, the overhead of building and search-ing the inverted indexes are too high and thus the actual performance is poor. When s ranges from 50K to 80K, the actual running time drops. This is because as s increases, there is some benefit for amortizing the cost of inverted in-dex lookup. Both the estimated and real time results suggest that the optimum s value is around 2K. Given the optimum s , PSS1 is twice faster than when s is 10K.
We also compare the performance of PSS1 and PSS2 with the baseline PSS when the dataset size changes.

Figure 14 shows the average running time of tasks under the three algorithms for four benchmarks with varying input size. We still observe the same trend that PSS1 outperforms the baseline. PSS2 also outperforms PSS1 in all cases except for Yahoo music benchmark. In that case, PSS1 is better than baseline, which is better than PSS2 due to low sharing pattern among the b vectors discussed in Section 6.2.
To reduce the required comparisons with an approxima-tion, we tested the algorithms over an LSH implementation Figure 12: Estimated and real cache miss ratios for PSS1 tasks. from [26]. LSH is applied first then the static partitioning. Table 3 compares the baseline with PSS2 after applying LSH partitioning over Clueweb dataset with varying sizes. PSS2 is upto 2.55x as fast as PSS for the 50M dataset. Table 3: PSS and PSS2 task time after LSH mapping for Clueweb. The average number of partitions per bucket is about 6.
We assess the overall performance in terms of speedup in processing the entire dataset when varying the number of cores. Figure 15 reports the speedup (parallel time divided by sequential time) for processing Twitter dataset with dif-ferent numbers of cores in the Intel cluster aforementioned. PSS2 scales well with more computing resources.

We also assess the individual task performance in utilizing the CPU resource by collecting its megaflops rate and com-pare it with the peak megaflops rate when vectors are dense. Similarity computation can be viewed approximately as a Figure 13: Actual vs estimated average task time for PSS1 in 3M Twitter dataset while split size varies. Figure 15: Speedup of PSS2 for processing 20M Twitter and 40M Clueweb with varying numbers of cores. sparse matrix multiplication together with dynamic compu-tation filtering. We assess the gap between how fast each CPU core can do in terms of peak application performance with a dense matrix and what our scheme has accomplished. First we compare the megaflops performance of our Java code with MTJ [13] from Netlib, which is highly optimized for dense matrix multiplication. The megaflops numbers achieved by a dense matrix multiplication routine (called dgemm) in MTJ achieves 1500 megaflops for matrix dimen-sion 1000 on a single core and achieves 500 megaflops for a small dense matrix. Our scheme achieves 280 megaflops for Twitter benchmark. That is fairly high considering we are dealing with extremely sparse matrices.

In PSS3 design, we represent feature vectors in S and B as a set of small dense submatrices and employ a built-in MTJ BLAS3 dense matrix routine to multiply these submatirces. The advantage of PSS3 is that we leverage MTJ, a highly op-timized library for cache performance. The disadvantage is that these small dense matrices still contain many zeros and a BLAS3 routine does not remove the unnecessary computa-tion operations as well as an inverted index does. Figure 16 lists the comparison between PSS3 and PSS2 performance, is unfortunately much slower than PSS2. The reason is that vector-feature matrices in the tested similarity applications are extremely sparse and the PSS3 strategy with BLAS3 does not contribute enough benefits to counteract the intro-duced overhead. PSS2 in general under di ff erent blocking sizes.
Table 4 provides another angle to explain why PSS3 slows down the task. We list the average fill-in ratio of those nonzero submatrices handled by PSS3. Fill-in ratio is the number of stored values which are in fact zero divided by the number of true nonzeros. The fill-in ratio is high and the number of true nonzeros for each block is too low to gain enough benefits with this blocked approach.
 Block size 4  X  4 4  X  8 4  X  16 16  X  16 32  X  8 32  X  16 Twitter 2.5 3.7 3.9 6.2 5.3 7.7 Clueweb 2.6 8.2 4.8 5.6 4.4 6.2
Table 4: Average fill-in ratio with di ff erent block sizes.
The main contribution of this paper is the development and analysis of cache-conscious data layout and traversal schemes for partition-based similarity search. The key tech-niques are to 1) split data traversal in the hosted partition so that the size of temporary vectors accessed can be con-trolled and fit in the fast cache; 2) coalesce vectors with size-controlled inverted indexing so that the temporal local-ity of data elements visited can be exploited. Our analy-sis provides a guidance for optimal parameter setting. The evaluation result shows that the optimized code can be upto 2.74x as fast as the original cache-obvious design. Vector coalescing is e ff ective if there is a decent number of features shared among the coalesced vectors.
 We thank Xifeng Yan, Alexandra Potapova, and Paul Weak-liem for their support and feedback, and the anonymous ref-erees for their thorough comments. This work is supported in part by NSF IIS-1118106/0905084 and Kuwait University Scholarship. Equipment access is supported by the Center for Scientific Computing at CNSI/MRL under NSF DMR-1121053 and CNS-0960316. Any opinions, findings, con-clusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
