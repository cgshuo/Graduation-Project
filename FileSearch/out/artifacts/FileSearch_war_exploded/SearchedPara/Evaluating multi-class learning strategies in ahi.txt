 Object class detection has been one of the mainstream research areas in computer vision. In recent years we have seen a significant trend towards larger recognition datasets with an increasing number of object classes [1]. This necessitates representing, learning and detecting multiple object classes, which is a challenging problem due to the large number and the high visual variability of objects. To learn and represent multiple object classes there have mainly been two strategies: the detectors for each class have either been trained in isolation, or trained on all classes simultaneously. Both exert certain advantages and disadvantages. Training independently allows us to apply complex probabilistic models that use a significant amount of class specific features and allows us to tune the parameters for each class separately. For object class detection, these approaches had notable success [2]. However, representing multiple classes in this way, means stacking together specific class representations. This, on the one hand, implies that each novel class can be added in constant time, however, the representation grows clearly linearly with the number of classes and is thus also linear in inference. On the other hand, joint representations enlarge sublinearly by virtue of sharing the features among several object classes [3, 4]. This means sharing common computations and increasing the speed of the joint detector. Training, however, is usually quadratic in the number of classes. Furthermore, adding just one more class forces us to re-train the representation altogether. Receiving somewhat less attention, the strategy to learn the classes sequentially (but not indepen-dently) potentially enjoys the traits of both learning types [4, 5, 6]. By learning one class after another, we can transfer the knowledge acquired so far to novel classes and thus likely achieve both, sublinearity in inference and cut down training time. In order to scale to a higher number of object classes, learning them sequentially lends itself as the best choice.
 In literature, the approaches have mainly used one of these three learning strategies in isolation. To the best of our knowledge, little research has been done on analyzing and comparing them with respect to one another. This is important because it allows us to point to losses and gains of each particular learning setting, which could focus further research and improve the performance. This is exactly what this paper is set to do  X  we present a hierarchical framework within which all of the aforementioned learning strategies can be unbiasedly evaluated and put into perspective.
 Prominent work on these issues has been done in the domain of flat representations [4, 3], where each class is modeled as an immediate aggregate of local features. However, there is an increasing literature consensus, that hierarchies provide a more suitable form of multi-class representation [7, 8, 9, 10, 11, 12]. Hierarchies not only share complex object parts among similar classes, but can re-use features at several levels of granularity also for dissimilar objects.
 In this paper, we provide a rigorous experimental evaluation of several important multi-class learning strategies for object detection within a generative hierarchical framework . We make use of the hierarchical learning approach by [13]. Here we propose and evaluate three types of multi-class learning: 1.) independent training of individual categories, 2.) joint training, 3.) sequential training of classes. Several issues were evaluated on multiple object classes: 1.) growth of representation, 2.) training and 3.) inference time, 4.) degree of feature sharing and re-use at each level of the hierarchy, 5.) influence of class ordering in sequential learning, and 6.) detection performance, all as a function of the number of classes learned. We show that sequential training achieves the best trade-off between inference and training times at a comparable detection performance and could thus be used to learn the classes on a larger scale.
 Related work. Prior work on multi-class learning in generative hierarchies either learns separate hierarchies for each class [14, 15, 16, 10, 17], trains jointly [7, 18, 9, 19, 20, 11], whereas work on sequential learning of classes has been particularly scarce [6, 13]. However, to the best of our knowledge, no work has dealt with, evaluated and compared multiple important learning concepts under one hierarchical framework. The hierarchical model . We use the hierarchical model of [13, 21], which we summarize here. Ob-jects are represented with a recursive compositional shape vocabulary which is learned from images. The vocabulary contains a set of shape models or compositions at each layer. Each shape model in the hierarchy is modeled as a conjunction of a small number of parts (shapes from the previous layer). Each part is spatially constrained on the parent shape model via a spatial relation which is modeled with a two-dimensional Gaussian distribution. The number and the type of parts can differ across the shape models and is learned from the data without supervision. At the lowest layer, the vocabulary consists of a small number of short oriented contour fragments, while the vocabulary at the top-most layer contains models that code the shapes of the whole objects. For training, we need a positive and a validation set of class images, while the structure of the representation is learned in an unsupervised way (no labels on object parts or smaller subparts need to be given).
 The hierarchical vocabulary V = ( V,E ) is represented with a directed graph, where multiple edges between two vertices are allowed. The vertices V of the graph represent the shape models and the edges E represent the composition relations between them. The graph V has a hierarchical structure, where the set of vertices V is partitioned into subsets V 1 ,...,V O , each containing the shapes at a particular layer. The vertices { v 1 i } 6 i =1 at the lowest layer V 1 represent 6 oriented contour fragments. The vertices at the top-most layer V O , referred to as the object layer represent the whole shapes of the objects. Each object class C is assigned a subset of vertices V O C  X  V O that code the object layer shapes of that particular class. We denote the set of edges between the vertex layers V ` and V `  X  1 with E ` . Each edge e `  X 
Ri :=  X  ( e We will use  X  ` R = (  X  ` Ri ) i to denote the vector of all the parameters of a shape model v ` R . The pair V ` := ( V ` ,E ` ) will be referred to as the vocabulary at layer ` . Inference. We infer object class instances in a query image I in the following way. We follow the contour extraction of [13], which finds local maxima in oriented Gabor energy. This gives us the contour fragments F and their positions X . In the process of inference we build a (directed acyclic) inference graph G = ( Z,Q ) . The vertices Z are partitioned into vertex layers 1 to O (object layer), Z = Z 1  X  X  X  X  X  Z O , and similarly also the edges, Q = Q 1  X  X  X  X  X  Q O . Each vertex z ` = ( v ` ,x ` )  X  Z ` represents a hypothesis that a particular shape v `  X  V ` from the vocabulary is present at location x ` . The edges in Q ` connect each parent hypothesis z ` R to all of its part hypotheses z `  X  1 i . The edges in the bottom layer Q 1 connect the hypotheses in the first layer Z 1 with the observations. With S ( z ) we denote the subgraph of G that contains the vertices and edges of all descendants of vertex z . Since our definition of each vocabulary shape model assumes that its parts are conditionally inde-hypothesis z ` R = ( v ` R ,x ` R ) by taking a product over the individual likelihoods of the parts: ulary edge e ` Ri between a parent hypothesis z ` R and its part hypothesis z `  X  1 i . It is modeled by a above a threshold, we add edges between z ` R and its most likely part hypotheses. The log-likelihood of the observations under a hypothesis z ` R is then calculated recursively over the subgraph S ( z ` R ) : The last term is the likelihood of the Gabor features under a particular contour fragment hypothesis. We first define the objective function for multi-class learning and show how different learning strate-gies can be used with it in the following subsections. Our goal is to find a hierarchical vocabulary V that well represents the distribution p ( I | C )  X  p ( F , X | C ; V ) at minimal complexity of the representation ( C denotes the class variable). Specifically, we seek for a vocabulary V =  X  ` V ` that optimizes the function f over the data D = { ( F n , X n ,C n ) } N n =1 ( N training images): The first term in (3) represents the log-likelihood: while T ( V ) penalizes the complexity of the model [21] and  X  controls the amount of penalization. Several approximations are made to learn the vocabulary; namely, the vocabulary is learned layer by layer (in a bottom-up way) by finding frequent spatial layouts of parts from the previous layer [13] and then using f to select a minimal set of models at each layer that still produce a good whole-object shape representation at the final, object layer [21]. The top layer models are validated on a set of validation images and those yielding a high rate of false-positives are removed from V . We next show how different training strategies are performed to learn a joint multi-class vocabulary. 3.1 Independent training of individual classes In independent training, a class specific vocabulary V c is learned using the training images of each particular class C = c . We learn V c by maximizing f over the data D = { ( F n , X n ,C = c ) } . For the negative images in the validation step, we randomly sample images from other classes. The joint multi-class representation V is then obtained by stacking the class specific vocabularies V c together, V classes ( 6 oriented contour fragments), thus V 1 = V 1 c . 3.2 Joint training of classes the classes is presented to the algorithm simultaneously, and is treated as unlabeled. The spatial parameters  X  of the models at each layer are then inferred from images of all classes, and will code  X  X verage X  spatial part dispositions. The joint statistics also influences the structure of the models by preferring those that are most repeatable over the classes. This way, the jointly learned vocabulary V will be the best trade-off between the likelihood L and the complexity T over all the classes in the dataset. However, the final, top-level likelihood for each particular class could be low because the more discriminative class-specific information has been lost. Thus, we employ a second step which revisits each class separately. Here, we use the joint vocabulary V and add new models v ` R to each layer ` if they further increase the score f for each particular class. This procedure is similar to that used in sequential training and will be explained in more detail in the following subsection. Object layer V O is consequently learned and added to V for each class. We validate the object models after all classes have been trained. A similarity measure is used to compare every two classes based on the degree of feature sharing between them. In validation, we choose the negative images by sampling the images of the classes according to the distribution defined by the similarity measure. This way, we discard the models that poorly discriminate between the similar classes. 3.3 Sequential training of classes When training the classes sequentially, we train on each class separately, however, our aim is to 1.) maximize the re-use of compositions learned for the previous classes, and 2.) add those missing (class-specific) compositions that are needed to represent class k sufficiently well. Let V 1: k  X  1 denote the vocabulary learned for classes 1 to k  X  1 . To learn a novel class k , for each layer ` we seek a new set of shape models that maximizes f over the data D = { ( F n , X n ,C = k ) } conditionally on the already learned vocabulary V ` 1: k  X  1 . This is done by treating the hypotheses inferred with respect to V ` 1: k  X  1 as fixed, which gives us a starting value of the score function f . Each new model v ` R is then evaluated and selected conditionally on this value, i.e such that the difference f ( V ` 1: k  X  1  X  v )  X  f ( V ` 1: k  X  1 ) is maximized. Since according to the definition in (4) the likelihood L increases the most when the hypotheses have largely disjoint supports, we can greatly speed up the learning process: the models need to be learned only with respect to those ( F , X ) in an image that have a low likelihood under the vocabulary V ` 1: k  X  1 , which can be determined prior to training. We have evaluated the hierarchical multi-class learning strategies on several object classes. Specif-ically, we used: UIUC multi-scale cars [22], GRAZ [4] cows and persons, Weizmann multi-scale horses (adapted by Shotton et al. [23]), all five classes from the ETH dataset [24], and all ten classes from TUD shape 2 [25]. Basic information is given in Table 1. A 6 -layer vocabulary was learned. 1 The bounding box information was used during training.
 When evaluating detection performance, a detection will be counted as correct, if the predicted bounding box coincides with groundtruth more than 50% . On the ETH dataset alone, this threshold is lowered to 0 . 3 to enable a fair comparison with the related work [24]. The performance will be given either with recall at equal error rate (EER), positive detection rate at low FPPI, or as classif.-by-detection (on TUD shape 2 ), depending on the type of results reported on that dataset thus-far. To evaluate the shareability of compositions between the classes, we will use the following measure: defined for each layer ` separately. By  X  v ` R used by class C  X  it is meant that there is a path of edges connecting any of the class specific shapes V O C and v ` R . To give some intuition behind the measure: deg share = 0 if no shape from layer ` is shared (each class uses its own set of shapes), and it is 1 if each shape is used by all the classes. Beside the mean (which defines deg share ), the plots will also show the standard deviation. In sequential training, we can additionally evaluate the degree of re-use when learning each novel class. Higher re-use means lower training time and a more compact representation. We expect a tendency of higher re-use as the number k of classes grows, thus we define it with respect to the number of learned classes: Evaluation was performed by progressively increasing the number of object classes (from 2 to 10 ). The individual training will be denoted by I , joint by J , and sequential by S .
 Table 2 relates the detection performances of I to those of the related work. On the left side, we report detection accuracy at low FPPI rate for the ETH dataset, averaged over 5 random splits of training/test images as in [24]. On the right side, recall at EER is given for a number of classes. Two classes. We performed evaluation on two visually very similar classes (cow, horse), and two dissimilar classes (person, car). Table 3 gives information on 1.) size (the number of compositions at each layer), 2.) training and 3.) inference times, 4.) recall at EER. In sequential training, both possible orders were used (denoted with S 1 and S 2 ) to see whether different learning orders (of classes) affect the performance. The first two rows show the results for each class individually, while the last row contains information with respect to the conjoined representations. Already for two classes, the cumulative training time is slightly lower for S than I , while both being much smaller than that of J .
 Degree of sharing. The hierarchies learned in I , J , and S on cows and horses, and J for car-person are shown in Fig. 2 in a respective order from left to right. The red nodes depict cow/car and blue horse/person compositions. The green nodes depict the shared compositions. We can observe a slightly lower number of shareable nodes for S compared to J , yet still the lower layers for cow-horse are almost completely re-used. Even for the visually dissimilar classes (car-person) sharing is present at lower layers. Numerically, the degrees of sharing and transfer are plotted in Fig. 1. Detection rate. The recall values for each class are reported in Table 3. Interestingly,  X  X nowing X  horses improved the performance for cows. For car-person, individual training produced the best result, while training person before car turned out to be a better strategy for S . Fig. 1 shows the detection rates for cows and horses on the joint test set (the strongest class hypothesis is evaluated), which allows for a much higher false-positive rate. We evaluate it with F-measure (to account for FP). A higher performance for all joint representations over the independent one can be observed. This is due to the high degree of sharing in J and S , which puts similar hypotheses in perspective and thus discriminates between them better.
 Five classes. The results for ETH-5 are reported in Table 4. We used half of the images for training, and the other half for testing. The split was random, but the same for I , J , and S . We also test whether different orders in S affect performance (we report an average over 3 random S runs). Ordering does slightly affect performance, which means we may try to find an optimal order of classes in training. We can also observe that the number of compositions at each layer is higher for S as for J (both being much smaller than I ), but this only slightly showed in inference times. Ten classes. The results on TUD-10 are presented in Table 5. A few examples of the learned shapes for S are shown in Fig. 3. Due to the high training complexity of J , we have only ran J for 2 , 5 and 10 classes. We report classif.-by-detection (the strongest class hypothesis in an image must overlap with groundtruth more than 50% ). To demonstrate the strength of our representation, we have also ran (linear) SVM on top of hypotheses from Layers 1  X  3 , and compared the performances. Already here, Layer 3 + SVM outperforms prior work [25] by 10% . Fig. 4-(11.) shows classification as a number of learned classes. Our approach consistently outperforms SVM, which is likely due to the high scale-and rotation-variability of images with which our approach copes well. Fig. 4 shows: inference time, cumulative training time, degree of sharing (for the final 10 -class repr.), transfer, and classification rates as a function of the number of learned classes.
 Vocabulary size. The top row in Fig 4 shows representation size for I , J and S as a function of learned classes. With respect to worst case ( I ), both J and S have a highly sublinear growth. Moreover, in layers 2 and 3 , where the burden on inference is the highest (the highest number of inferred hypotheses), an almost constant tendency can be seen. We also compare the curves with those reported for a flat approach by Opelt et al. [4] in Fig 4-(5). We plot the number of models at Layer 5 which are approximately of the same granularity as the learned boundary parts in [4]. Both, J and S hierarchical learning types show a significantly better logarithmic tendency as in [4]. Fig 4-( 6 ) shows the size of the hierarchy file stored on disk. It is worth emphasizing that the hierarchy subsuming 10 classes uses only 0 . 5 Mb on disk and could fit on an average mobile device. 50 classes. To increase the scale of the experiments we show the performance of sequential training on 50 classes from LabelMe [1]. The results are presented in Fig. 5. For I in the inference time plot we used the inference time for the first class linearly extrapolated with the number of classes. We can observe that S achieves much lower inference times than I , although it is clear that for a higher number of classes more research is needed to cut down the inference times to a practical value. Figure 1: From left to right: 1.) detection rate (F measure) on the joint cow-horse test set. 2.) degree of We evaluated three types of multi-class learning strategies in a hierarchical compositional frame-work, namely 1.) independent, 2). joint, and 3.) sequential training. A comparison was made through several important computational aspects as well as by detection performance. We conclude that: 1.) Both joint and sequential training strategies exert sublinear growth in vocabulary size (more evidently so in the lower layers) and, consequently, sublinear inference time. This is due to a high degree of sharing and transfer within the resulting vocabularies. The hierarchy obtained by sequen-tial training grows somewhat faster, but not significantly so. 2.) Training time was expectedly worst for joint training, while training time even reduced with each additional class during sequential training. 3.) Different training orders of classes did perform somewhat differently  X  this means we might try to find an  X  X ptimal X  order of learning. 4.) Training independently has mostly yielded the best detection rates, but the discrepancy with the other two strategies was low. For similar classes (cow-horse), sequential learning even improved the detection performance, and was in most cases above the joint X  X  performance. By training sequentially, we can learn class specific features (yet still have a high degree of sharing) which boost performance. Most importantly, sequential training has achieved the best trade-off between detection performance, re-usability, inference and training time. The observed computational properties of all the strategies in general, and sequential learning in particular, go well beyond the reported behavior of flat approaches [4]. This makes sequential learning of compositional hierarchies suitable for representing the classes on a larger scale. Acknowledgments Figure 3: A few examples from the learned hierarchical shape vocabulary for S on TUD-10. Each shape in Figure 4: Results on TUD-10 . Top: (1-4) repr. size as a function of the number of learned classes. Middle: Figure 5: Results on 50 object classes from LabelMe [1]. From left to right: Size of representation (number of
