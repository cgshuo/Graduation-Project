 A major goal of current information retrieval research is to develop algorithms that can improve and goals of a particular information need. For example, in t he language modeling approach to retrieval [9], a simple query model may be a unigram language model, with higher probability given to terms related to the query text. Once estimated, a query mo del may be used for such tasks as documents obtained from an initial query. This task is known as pseudo-relevance feedback or blind worst-case performance for individual queries. This is one significant reason that Web search en-adequately capture the relationships or tradeoffs between competing objectives, such as maximizing to several problems.
 show in Section 3. Second, selection of expansion terms is ty pically done in a greedy fashion by imbalance, a major source of retrieval failures [2]. Third, few existing expansion algorithms can are not especially effective, and rely on sometimes complex heuristics that are integrated in a way factors that must be constrained, such as the computational cost of sending many expansion terms to the search engine. To our knowledge such situations are no t handled by any current query model estimation methods in a principled way.
 To remedy these problems, we need a better theoretical frame work for query model estimation: one that incorporates both risk and reward data about terms, tha t detect risky situations and expands budget, and has fast practical implementations.
 Our solution is to develop a novel formulation of query model estimation as a convex optimization problem [1], by casting the problem in terms of constrained g raph labeling. Informally, we seek has close connections with models of risk in portfolio optim ization [7]. An optimization approach frees us from the need to provide a closed-form formula for te rm weighting. Instead, we specify a integrating different criteria for expansion as optimizat ion constraints or objectives. the individual risk of a term, and the conditional risk of choosing one term given we have already a simple convex program for solving for the relative term wei ghts in a query model. Our aim in this section is to develop a constrained optimizat ion program to find stable, effective query models. Typically, our optimization will embody a bas ic tradeoff between wanting to use evidence that has strong expected relevance, such as expans ion terms with high relevance model weights, and the risk or confidence in using that evidence. We begin by describing the objectives 2.1 Query model estimation as graph labeling We can gain some insight into the problem of query model estim ation by viewing the process of building a query as a two-class labeling problem over terms. Given a vocabulary V , for each term terms w Figure 1: Query model estimation as a constrained graph labe ling problem using two labels (rele-vant, non-relevant) on a graph of pairwise term relations. T he square nodes X, Y, and Z represent query terms, and circular nodes represent potential expans ion terms. Dark nodes represent terms toward relevant labels related to multiple query terms (rig ht). a probabilistic setting, finding the most probable labeling can be viewed as a form of maximum a posteriori (MAP) estimation over the Markov random field defi ned by the term graph. Although this problem is NP-hard for arbitrary configuratio ns, various approximation algorithms general metric labeling problem is given by Ravikumar and La fferty [10]. The basic relaxation we use is The variable x assignment costs c set of query-ranked documents. For our baseline expansion m ethod, we use the strong default feed-back algorithm included in Indri 2.2 based on Lavrenko X  X  Rel evance Model [5]. Further details are available in [4].
 In the next section, we discuss how to specify values for c model estimation. For a two-label problem where j  X  X  0 , 1 } , the values of x only the x Our goal is to find a set of weights x = ( x in the final query model of term w risk associated with the selection. We now describe each of t hese in more detail, followed by a description of additional set-based constraints that are u seful for query expansion. 2.2 Relevance objectives Given an initial set of term weights from a baseline expansio n method c = ( c relevance over the vocabulary V of a solution x is given by the weighted sum c x = P highest c For example, if c and x represent probability distributions over terms, then we co uld replace c x with KL ( c || x ) as an objective since KL-divergence is also convex in c and x .
 The initial assignment costs (label values) c can be set using a number of methods depending on model-based expansion, we are given estimates of the Releva nce Model p ( w | R ) over the highest-approximate non-relevant documents, or using the lowest-ranked k documents out of the top 1000 retrieved by the initial query Q . To set c Theorem, is then to one, for binary labels we have c 2.3 Risk objectives Optimizing for expected term relevance only considers one d imension of the problem. A second critical objective is minimizing the risk associated with a particular term labeling. We adapt an certainty, encoded in the matrix  X  with entries  X  single term with highest relevance score. A lower-risk stra tegy would distribute bets among terms that had both a large estimated relevance and low redundancy , to cover all aspects of the query. Conditional term risk. First, we consider the conditional risk  X  w . To quantify conditional risk, we measure the redundancy of choosing word w has already been selected. This relation is expressed by cho osing a symmetric similarity measure formula The quantities  X  and  X  are scaling constants that depend on the output scale of  X  , and the choice of counts. For this experiment we used the Jaccard coefficient: future work will examine others. define it for a term w covariance matrix  X  then has diagonal entries Figure 2: Three complementary criteria for expansion term w eighting on a graph of candidate terms, by allowing more expansion candidates within a distance thr eshold of each term. Term centering with minimum variation in the distances to X and Y .
 term distributions.
 parameter  X  , by minimizing the function If
 X  is estimated from term co-occurrence data in the top-retrie ved documents, then the condition the same co-occurrence cluster. Rather, we prefer a set of ex pansion terms that are more diverse, covering a larger range of potential topics. 2.4 Set-based constraints One limitation of current query model estimation methods is that they typically make greedy term-A one-dimensional greedy selection by term score, especial ly for a small number of terms, has the query drift after expansion. We now define several useful con straints on query model terms: aspect balance , aspect coverage , and query term support . Figure 2 gives graphical examples of aspect balance, aspect coverage, and the term centrality objectiv e.
 separate and unique aspect of the user X  X  information need. W e create the matrix A from the vectors  X  of the solution model x on each query term X  X  feature vector  X  in balance to be that the vector Ax be element-wise close to the mean vector  X  of the  X  tolerance  X  To demand an exact solution, we set  X  results and so we use a small positive value for  X  for the  X  X elevant X  label on the query terms x the threshold l for all other terms. u reflect the rarity or ambiguity of individual query terms. Aspect coverage. One of the strengths of query expansion is its potential for s olving the vocabu-lary mismatch problem by finding different words to express t he same information need. Therefore, terms are balanced evenly among all query terms: we may care a bout the absolute level of support that exists. For example, suppose our information sources a re feedback terms, and we have two terms selected to give a minimal non-zero but even covering t o all aspects. The second weighting scheme has three times as many terms, but also gives an even co vering. Assuming no conflicting constraints such as maximum query length, we may prefer the s econd weighting because it increases the chance we find the right alternate words for the query, pot entially improving recall. We denote the set of distances to neighboring words of query t erm q g
T x gives us the aspect coverage, or how well the words selected b y the solution x  X  X over X  term q . The more expansion terms near q When only the query term is covered, the value of g each of the vectors g the following complete quadratic program for query model es timation, which we call QMOD and is shown in Figure 3. The role of each constraint is given in ital ics. In this section we summarize the effectiveness of using the Q MOD convex programs to estimate query models and examine how well the QMOD feasible set is cal ibrated to the empirical risk of support constraint ( l We used the following default values for the control paramet ers:  X  = 1 . 0 ,  X  = 0 . 75 ,  X   X  = 0 . 1 , u i = 1 . 0 , and l i = 0 . 95 for query terms and l i = 0 for non-query terms. 3.1 Robustness of Model Estimation In this section we evaluate the robustness of the query model s estimated using the convex program in Fig. 3 over several TREC collections. We created a histogr am of MAP improvement across sets feedback method. Using these histograms we can distinguish between two systems that might have precision by using feedback. The baseline feedback here was Indri 2.2 (Modified Relevance Model considering the expansion failures whose loss in average pr ecision is more than 10%, the robust version hurts more than 60% fewer queries. Figure 4: Comparison of expansion robustness for four TREC c ollections combined (TREC 1&amp;2, in average precision. The dark bars show robust expansion pe rformance using the QMOD convex program with default control parameters. The light bars sho w baseline expansion performance using term relevance weights only. Both methods improve average p recision by an average of 15%, but histogram (queries hurt). 3.2 Calibration of Feasible Set If the constraints of a convex program are well-designed for stable query expansion, the odds of an algorithm will not attempt to enhance the query. Conversely , the odds of finding a feasible query model should ideally increase for thoese queries that are mo re amenable to expansion. Overall, 17% that would have been achieved with the baseline expansion, n ormalized by the original number of queries appearing in each bin when the (non-selective) base line expansion is used. This gives the log-odds of reverting to the original query for any given gai n/loss level.
 The results are shown in in Figure 5. As predicted, the QMOD al gorithm is more likely to decide point where we reach an average precision gain of 75% and high er. At this point, such high-reward of the convex algorithm is well-calibrated to the true expan sion benefit. We have presented a new research approach to query model esti mation, showing how to adapt convex optimization methods to the problem by casting it as constra ined graph labeling. By integrating algorithm while retaining its strong gains in average preci sion.
 Our expansion framework is quite general and easily accomod ates further extensions and refine-ments. For example, similar to methods used for portfolio op timization [6] we can assign a compu-additional observations such as relevance feedback from th e user. Finally, there are a number of are binned by the percent change in average precision if base line expansion were used. Columns above the line indicate greater-than-even odds that we reve rt to the original query. values we use have not been extensively tuned, so that furthe r performance gains may be possible. Acknowledgments We thank Jamie Callan, John Lafferty, William Cohen, and Sus an Dumais for their valuable feed-back on many aspects of this work.

