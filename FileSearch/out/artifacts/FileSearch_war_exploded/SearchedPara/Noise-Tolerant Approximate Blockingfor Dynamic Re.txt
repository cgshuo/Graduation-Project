 The purpose of entity resolution is to find records in one or several databases that belong to the same real-world entity. Such an entity can for example be a person (e.g. customer, patient, or student), a consumer product, a business, or any other object that exists in the real world. Entity resolution is widely used in various applications such as identity crime detection (e.g. credit card fraud detection) and estimation of census population statistics [1].

Currently, most available entity resolution techniques conduct the resolution process in offline or batch mode with stati c databases. However, in real-world scenarios, many applications require real-time responses. This requires entity resolution on query records that need to be matched within sub-seconds with databases that contain (a possibly large number of) known entities [1]. For ex-ample, online entity resolution based on personal identifying details can help a bank to identify fraudulent credit card applications [2], while law enforcement officers need to identify suspect individuals within seconds when they conduct an identity check [1]. Moreover, real-world databases are often dynamic. The re-quirement of dealing with large-scale dynamic data with quick responses brings challenges to current entity resolution techniques. Only limited research has so far focused on using entity resolution at query time [3,4] or in real-time [5,6].
Typically, pair-wise comparisons bet ween records are used to identify the records that belong to the same entity . The number of record comparisons in-creases dramatically as the size of a database grows. Indexing techniques such as blocking or canopy formation can help to significantly decrease the number of comparisons [1]. Often phonetic encoding functions, such as Soundex or Double Metaphone, are used to overcome differences in attribute values.

Locality sensitive hashing (LSH) [7] is an approximate blocking approach that uses l length k hash functions to map records within a certain distance range into the same block with a given probability. This approach [8] can filter out records with low similarities, thus decreasing th e number of comparisons. However, the tuning of the required parameters k and l is not easy [9]. This is especially true for large-scale dynamic datasets. For some query records, one may need to investigate records with low similarities, while for other query records one only needs to investigate those records with h igh similarities with the query record. Moreover, entity resolution needs to deal with noise such as pronunciation or spelling errors. Although some LSH approaches such as multi-probe [10] are to decrease the number of hash functions needed, the question of how to make blocking approaches become more noise-tolerant and scalable remains open.
In this paper, we propose a noise-tolerant approximate blocking approach to conduct real-time entity resolution. To deal with noise, an n -gram based approach [1] is employed where attribut e values are converted into sets of n -grams (i.e, substring sets of length n ). Then, LSH is used to group records into blocks with various distance ranges based on the Jaccard similarity of their n -grams. To be scalable, for blocks that are large (i.e., contains more than a certain number of records), we propose to build dynamic sorting trees inside these blocks and return a small set of nearest neigh bor records for a given query record. Indexing techniques can help to scale-up t he entity resolution process [1]. Com-monly used indexing approaches include standard blocking based on inverted indexing and phonetic encoding, n -gram indexing, suffix array based indexing, sorted neighborhood, multi-dimensional mapping, and canopy clustering. Some recent work has proposed automatic blo cking mechanisms [11]. Only a small number of approaches have addressed real-time entity resolution. Christen et al. [5] and Ramadan et al. [6] proposed a similarity-aware indexing approach for real-time entity resolution. However, this approach fails to work well for large datasets, as the number of similarity comparisons for new attribute values in-creases significantly when the size of each encoding block grows.

Approximate blocking techniques such as LSH and tree based indexing [9] are widely used in nearest neighbour similarity search in applications such as recom-mender systems [12] and entity resolution [8]. In LSH techniques, the collision probability of a LSH family is used as a proxy for a given distance or similarity measure function. Popularly used LSH families include the minHash family for Jaccard distance, the random hyperplan e family for Cosine Distance, and the p -stable distribution family for Euclidean Distance [13].

Recently, Gan et al. [14] proposed to use a hash function base with n basic length 1 signatures rather than using fixed l length k signatures or a forest [9] to represent a data point. The data points that are frequently colliding with the query record across all the signatures are selected as the approximate sim-ilarity search results. However, as k =1 usually leads to large sized blocks, this approach needs to scan all the data points in the blocks to get the frequently colliding records each time. This makes it difficulty to retrieve results quickly for large-scale datasets. Some approaches such as multi-probe [10] have been used to decrease the number of hash func tions needed. However, how to explore LSH blocks in a fast way and decrease the retrieval time to facilitate real-time approximate similarity search for large scale datasets still needs to be explored. To describe the proposed approach, we first define some key concepts.  X  Record Set: R = { r 1 ,r 2 ,...,r | R | } contains all existing records in a dataset.  X  X lement: An element is an n -gram of an attribute value. Elements may  X  Query Record: A query record q i  X  U is a record that has the same  X  Query Stream: Q = { q 1 ,q 2 ,...,q | Q | } is a set of query records.  X  X ntitySet: E = { e 1 ,e 2 ,...,e | E | } contains all unique entities in U .
For a given record r i  X  R , the decision process of linking r i with the corre-the function of finding the entity of record r i . The problem of real-time entity resolution is defined as: for each query record q i in a query stream Q ,let e j be the entity of q i , e j = l ( q i ), find all the records in R that belong to the same entity as the query record in sub-second time. Let L q i denote the records in R that belong to e j , L q i = { r k | r k  X  l ( q i ) ,r k  X  R } ,L q i  X  R,q i  X  Q . [Example 1] Figure 1(a) shows example records r 1 , r 2 , r 3 ,and r 4 .They belong to three entities e 1 , e 2 ,and e 3 . Assume r 4 is a query record, the entity resolution process for r 4 is to find L r 4 = { r 1 } based on the four attribute values. A blocking schema [15] is an approach to map a set of records into a set of blocks. Blocking schemes generate sign atures for records. A blocking scheme can be a LSH function, a canopy clustering function, or a phonetic encoding function [1]. Those records with the same signature will be allocated together in the same block. To make a LSH blocking scheme scalable for large-scale dynamic datasets, we propose to build a dynamic sorting tree to sort the records of large-sized LSH blocks and return a window of nearest neighbors for a query record. Through controlling the window size, we can select nearest neighbors with various approximate similarity ranges for the purpose of being noise-tolerant. Thus, the proposed approach includes two parts: LSH and dynamic sorting tree, which will be discussed in Section 4.1 and 4.2. Then, the discussion of conducting entity resolution based on the proposed blocking approach will be in Section 4.3. 4.1 Locality Sensitive Hashing LSH can help to find approximate results for a query record for high dimensional data. Let h denote a basic approximate blocking scheme for a given distance measure D , Pr ( i ) denote the probability of an event i ,and p 1 and p 2 are two D , if for any records r x ,r y  X  R , the following conditions hold: 1. if D ( r x ,r y )  X  d 1 then Pr ( h ( r x )= h ( r y ))  X  p 1 2. if D ( r x ,r y ) &gt;d 2 then Pr ( h ( r x )= h ( r y ))  X  p 2 Minwise hashing (minHash) is a popular LSH approach that estimates the Jaccard similarity [7]. Let J ( r x ,r y ) denote the Jaccard similarity for any two records r x and r y . This hashing method applies a random permutation  X  on the elements of any two records r x and r y and utilizes to estimate the Jaccard similarity of r x and r y ,where min (  X  ( r x )) denotes the minimum value of the random permutation of the elements of record r x . p denotes the hash collision probability Pr ( min (  X  ( r x )) = min (  X  ( r y ))). It represents the ratio of the size of the intersection of t he elements of the two records to that of the union of the elements of the two records. A minHash function can generate a basic signature for a given record. The basic signature is called a length 1 signature and the hash function is called a length 1 hash function.

In order to allocate records that have higher similarities with each other into thesameblock, k length 1 hash functions can be combined to form a length k ( k&gt; 1) compound blocking scheme to get the intersection records of the basic length 1 blocking schemes. Let h c denote a compound LSH blocking scheme that is the AND -construction (conjunction) of k basic LSH blocking schemes h i , h c =  X  i =1 h i .Let p c denote the collision probability of a length k compound blocking scheme. It can be calculated based on the product of the collision probabilities of its basic length 1 blocking schemes, denoted as p c = p k . To increase the collision probability, each record is hashed l times to conduct OR -construction (disjunction) and form l hash tables (i.e., l length k signatures), n = k  X  l .Let H k,l denote a LSH family that has l length k hashing blocking schemes, the collision probability of H k,l can be estimated with p k,l =1  X  (1  X  p k ) l . [ Example 2 ] (LSH blocking scheme and blocks) Figure 2 (a) shows the ex-ample length 1 minHash signatures of the example records in Figure 1. Figure 2 (b) shows an example LSH blocking scheme H 2 , 2 and the generated LSH blocks. H different blocks based on the given blocking scheme, shown in Figure 2 (b). For example, the length 2 blocking scheme h 1  X  h 2 generated block signatures 3 2 and 3 5. Records r 1 , r 3 and r 4 are allocated in block 3 2 while r 2 is in block 3 5. 4.2 Dynamic Sorting Tree BasedonagivenLSHFamily H k,l , we can allocate records with certain similarity into the same LSH block and filter out those records that have lower similarities. Using a large value of k will result in smaller sized blocks and decrease the number of pair-wise comparisons, but it may result in low recall because a large value for k may filter out some true matched reco rds that have low similarities. A small value for k usually can get higher recall va lues but may result in large-sized blocks. As scanning the records in large-sized blocks to conduct pair-wise comparisons is usually time-consuming, how to quickly identify a small set of nearest neighbors in a large sized LSH block is very important.

If we sort the records in a LSH block an d only return a small set of similar records for each query record, then the expl oration of the whole large-sized block can be avoided. B + trees are commonly used to sort the data points for dynamic data in one dimension [9]. We build a B + tree to sort the records inside each large sized LSH block. As forming sub-blocks for those small sized blocks is not necessary, we set up a threshold for building sorting trees in a block. Let  X  (  X &gt; 1)denote this threshold, if the size of a block B i is greater than  X  ,then we build a B +treefor B i , otherwise, no sorting tree will be formed. To build a sorting tree, we firstly discuss how to select a sorting key.
 Selecting a Sorting Key Adaptively. Typically, the sorting key can be as-signed by a domain expert [16,17]. For example, for the example dataset, we can select  X  X irst Name X  as the sorting key. As the LSH blocks are formed by the random permutation of elements, the co mmon elements of the records in block B i may be different from those in another block B j . Using a predefined fixed sorting key may result in a whole block being returned as query results, which will fail to return a sub-set of records in a block. On the other hand, we can select the sorting key adaptively for each LSH block B i individually.
One or several attributes can be s elected as sorting key. For a block B i ,a good sorting key should divide the records into small sub-blocks. Thus, we can select those attributes that have a greater number of unique values in  X  records. Moreover, if the attribute value occurrences are uniformly distributed, we can get sub-blocks with the same or similar sizes. Thus, for an attribute a j ,wecalculate a sorting key selection weight w j , which consists of the linear combination of two components: attribute cardinality weight and distribution weight:
Where 0  X   X   X  1, and n j is the number of unique values of attribute a j in block B i . n j  X  measures the attribute cardinality weight, 0  X  n j  X   X  1.  X  j measures the distribution of occurrences of each unique value of a j , calculated as the standard deviation of the occurrences of the value of a j .Let o jc denote the occurrence of attribute value v jc in  X  records, m a denote the maximum occurrence, m a =max b  X  [1 ,K ] ( o jb ),where K is the number of unique attribute values of attribute j in  X  records, m i denotes the minimum occurrence, m i = min b  X  [1 ,K ] . To get a normalized weight between 0 and 1, we set n jc = o jc  X  m i m thus,  X  j = 1 K  X  K c =1 ( n jc  X   X  ) 2 ), where  X  is the average value of n jc for a j , The attribute a j that has the largest w j will be selected as the sorting key. Let sk denote a selected sorting key for B i , then the B + tree is built by sorting the records in B i on key sk . For text records, we can sort them lexically by the alphabetic order of the sorting key. One advantage of sorting by alphabetic order is that such an ordering is a global unique order for all the attribute values of the sorting key in B i . The distance of any two attribute values of the sorting key can be measured by the distance of their alphabetic order. Thus, for a query record, a set of nearest neighbors can be obtained through measuring their alphabetic order distance. Each unique sorting key value denoted as v sk is one node of the B + tree. Each node is an inverted index that points to the records that have the same v sk , denoted as dt i =( v sk ,I ( v sk )), where v sk is an attribute value of sk ,and I ( v sk ) is the set of records that have the same v sk .Let | B i | denote the number of records in a block B i , the time complexity of searching, insertion and deletion of B +treeis O (log | B i | ), which is quicker than scanning all the records in a block, O (log | B i | ) &lt;O ( | B i | ), for large blocks.
 Selecting Nearest Neighbors. Every record in a block B i can be selected as a candidate record for query record q i . However, for those blocks that have a large number of records, we can select a s et of nearest neighbors as candidate records to reduce the query time. For a given query record q i ,wefirstlyinsert it into the B + tree of a LSH block based on the alphabetic order of the block X  X  sorting key sk . Then the nearest neighbor nodes of the B + tree will be selected as the candidate records f or this query record. Let v i,sk denote the sorting key value of query record q i ,wechoose z r nodes that are greater than v i,sk and z l nodes that are smaller than v i,sk to form the nearest neighbor nodes of v i,sk , 0  X  z
If we set z l and z r =0, then only those records with the exact same value as the query record will be selected. If z l or z r = | B i | , then all the records of B i will be selected. As the distance of two nodes in a sorting B + tree reflects the distance of two records, we set a distance threshold  X  of two nodes. Let v i,sk denote the attribute value of sorting key sk for query record q i .Foranode v j,sk given distance measure D (e.g., edit distance [1]). If the distance of two nodes is less than  X  , then we increase the window size to include node v j,sk inside the window. Thus, we firstly set z l and z r = 0. The window size expansion is along both directions [16] (i.e., greater than or smaller than the query record node i ) of the B + tree. For each direction, we expand the window size (i.e., z l or z r ) and include neighbor node j ,if D ( v i,sk ,v j,sk ) &lt; X  ,with0  X   X   X  1.
To further decrease the number of candi date records and select a smaller set of nearest neighbors, we count the collision number of each record of the neigh-boring nodes inside the window of the sorting tree in all l LSH blocks to rank the records that are attached to the selected neighborin g nodes. This is because the co-occurrence of a record r x that appears together with q i in the LSH blocks re-flects the similarity of the two records [ 14]. The higher the co-occurrence is, the more similar the two record s are. We set a threshold  X  to select those records that appear at least  X  times with the query record q i together in LSH blocks. Let g ix denote the co-occurrence of record r x and query record q i .Let N v i,sk denote the nearest neighbor record set of query record q i in block B i . For each record r x of the neighbor node j inside the window of the sorting tree DST i (i.e., r x  X  I ( v j,sk )), we add r x to N v i,sk if g ix &gt; X  ,0  X   X   X  l . [ Example 3 ] (Dynamic sorting tree) Figure 2(c) shows the dynamic sorting tree DST 3 2 for block 3 2. DST 3 2 is sorted by attribute  X  X irst Name X . Through setting window size parameters, we can g et a set of nearest neighbor records for a query record. For example, if we set z r = 0, no records will be selected as candidate records, and only the records w ith node value  X  X onny X  will be selected. Assume  X  =0 . 6,  X  = 1, the Jaccard similarity of the bi-grams of  X  X ony X  and  X  X onny X  is 0.75 and that of the bi-grams of  X  X onny X  and  X  X ong X  is 0.167. Thus, we can set z l =0and z r =1for DST 3 2 . Record r 1 appears twice together with query record r 4 in blocks 3 2and1 6, thus r 1 is selected. 4.3 Real-Time Entity Resolution For a query record q i , we can obtain the nearest neighbor records that are being allocated in the same block with q i as the candidate records. Then, we can conduct pair-wise comparisons for all candidate records with the query record q i . We use the Jaccard similarity of the n -grams of a candidate record and the query record, or other appropriate approximate distance/similarity measures to rank their similarity [1]. Let C q i denote the candidate record set, for each candidate record r j  X  C q i and query record q i  X  Q , the similarity can be calculated with sim ( q i ,r j )= J ( q i ,r j ). The top N candidate records will be returned as the query results L q i . The algorithm is shown as Algorithm 1. 5.1 Data Preparation To evaluate the proposed approach, we conducted experiments on the following two datasets. 1) Australian Telephone Directory. [5] (named OZ dataset). It contains first name, last name, suburb, and postcode, and is sourced from an Australian telephone directory from 2002 (Australia On Disc). This dataset was modified by introducing various typographical and other variations to simulate real  X  X oisy X  data. To allow us to evaluate scalability, we generated three sub-sets of this dataset. The smallest dataset (named OZ-Small) has 34,596 records, the medium sized dataset (OZ-Median) has 345,996 records, and the largest dataset (OZ-Large) has 3,458,758 records. All three datasets have the same features including similarity distribution, duplicate percentages (i.e., 25%) and modification types. 2) North Carolina Voter Registration Dataset. (i.e., NC dataset). This dataset is a large real-world voter registration database from North Carolina (NC) in the USA [18]. We downloaded this database every two months since Oc-tober 2011. The attributes used in our experiments are: first name, last name, city, and zip code. The entity identificati on is the unique voter registration num-ber. This data set contains 2,567,642 records. There are 263,974 individuals (identified by their voter registration numbers) with two records, 15,093 with three records, and 662 with four records. 5.2 Evaluation Approaches In the experiments, we employ the co mmonly used Recall, Memory Cost and Query Time to measure the effectiven ess and efficiency of real-time top N entity resolution approach [1]. We divided each dataset into a training (i.e., building) and a test (i.e., query) set. Each test dataset contains 50% of the whole dataset. For each test query record, the entity resolution approach will generate a list of ordered result records. The top N records (with the highest rank scores) will be selected as the query results. If a record in the results list has the same entity identification as the test query record, t hen this record is counted as a hit (i.e., an estimated true match). The Recall value is calculated as the ratio of the total number of hits of all the test queries to the total number of true matches in the test query set. We conducted comparison experiments with three other state-of-the-art methods as described below. All methods were implemented in Python, and the experiments were conducted on a server with 128 GBytes of main memory and two 6-core Intel Xeon CPUs running at 2.4 GHz.  X  NAB : This is the proposed noise-tolerant approximate blocking approach  X  DCC : This is a locality sensitive hashing approach that uses dynamic colli- X  LSH : This is the basic LSH approach. It scans the data points in each block  X  SAI : This approach pre-calculates the similarity of attribute values to de-5.3 Parameter Setting We firstly discuss the parameter setting for the OZ datasets. To set the parame-ters k and l , we calculated the Jaccard similarity distribution of the exact values and n -grams with n =2, 3, 4 of the true matched records of the training sets. The similarity distribution is shown in Figure 3. The Jaccard similarity of 90% of the exact values of the true matched r ecords is zero. This means that it would be very difficult to find true matched records if we use the exact value of the records. Also, the similarity range of the majority (i.e., 95%) of the 2-grams of the true matched records is between 0.3 and 0.7. Thus, using an n -gram based approach can help to find those true matched records that contain small varia-tions or errors. n is therefore set to 2 in the experiments. Figure 3 also shows the collision probability of various k values with l =30. We set k =4 and l =30 for the NAB approach to let most true matched records have a higher collision probability. To get a similar collision probability, we set k =4 and l =30 for the LSH approach, and k =1 and l =20 for the DCC approach. The settings of the other parameters of the NAB approach are  X  =0.5,  X  =2,  X  =0.1.

Figure 4 shows the top N =10 evaluation of NAB for the OZ-Large dataset with various  X  value. With various  X  value, recall remains stable while average query time is increasing, with the increase of  X  . This shows that building dynamic sorting trees inside LSH blocks can help to decrease the query time through selecting a small number of nearest neig hborrecordsasca ndidate records. A small  X  value (e.g.,  X  =20) will not necessarily decrea se the average query time, as the building of trees also takes time and space. When  X  is set to a large value (e.g.,  X  =500), the average query time increases because scanning and comparing a large number of candidate reco rds is time consuming. We set  X  =200. For the NC dataset, we set n =3, k =2, l =20 for LSH and NAB, and k =1, l =10 for DCC. The other parameters for NAB are the same as with the OZ datasets. 5.4 Comparison with Baseline Models The performances of the compared approaches are shown in Figure 5. To elimi-nate the influence of random permutation, the LSH based approaches LSH, DCC and NAB were run three times. The average query time and memory usage are shown on a logarithmic scale. From Figure 5 we can see that SAI achieved good recall value for OZ-Small and NC data but low recall for OZ-Large. The LSH based approaches (LSH, DCC and NAB) had higher recall than SAI. This can be explained that these approaches can capture the common elements (i.e., n -grams) of the attribute values to deal with the noise of the data. Moreover, through controlling the k and l value, these approaches can filter out the records that have lower similarities with the query record. DCC had very low memory usage but high average query time. NAB had slightly lower recall and higher memory usage than that of LSH, but with the help of dynamic sorting trees, the average query time (e.g., 8 msec for OZ-Large) is much lower than LSH (e.g., 0.1 sec for OZ-Large). Thus, NAB can be eff ectively and efficiently used for large scale real-time entity resolution, especially for noisy data. We discussed a noise-tolerant approximate blocking approach to facilitate real-time large scaled entity resolution. To deal with noise, we use an LSH approach to group records into blocks with various distance ranges based on the Jaccard similarity of their n -grams. Moreover, we propose to build dynamic sorting trees inside large-sized LSH blocks. Through controlling the window size, we select a small set of nearest neighbors with various approximate similarity ranges to be noise-tolerant. Experiments conducted on both synthetic and real-world large scaled datasets demonstrates the effectiveness of the proposed approach. Our future work will focus on how to conduct adaptive real-time entity resolution. Acknowledgements. The authors would like to thank the great help of Pro-fessor David Hawking.

