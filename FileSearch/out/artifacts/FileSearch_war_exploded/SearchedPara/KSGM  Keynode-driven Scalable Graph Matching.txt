 Understanding how a given pair of graphs align with each other (also known as the graph matching problem) is a critical task in many search, classification, and analysis applications. Unfortu-nately, the problem of maximum common subgraph isomorphism between two graphs is a well known NP-hard problem, rendering it impractical to search for exact graph alignments. While there are several heuristics, most of these analyze and encode global and local structural information for every node of the graph and then rank pairs of nodes across the two graphs based on their structural similarities. Moreover, many algorithms involve a post-processing (or refinement) step which aims to improve the initial matching accuracy. In this paper 1 we note that the expensive refinement phase of graph matching algorithms is not practical in any appli-cation where scalability is critical. It is also impractical to seek structural similarity between all pairs of nodes. We argue that a more practical and scalable solution is to seek structural keynodes of the input graphs that can be used to limit the amount of time needed to search for alignments. Naturally, these keynodes need to be selected carefully to prevent any degradations in accuracy dur-ing the alignment process. Given this motivation, in this paper, we first present a structural keynode extraction ( SKE ) algorithm and then use structural keynodes obtained during off-line processing for keynode-driven scalable graph matching ( KSGM ). Experiments show that the proposed keynode-driven scalable graph matching al-gorithms produce alignments that are as accurate as (or better than) the state-of-the-art algorithms, with significantly faster online exe-cutions.
Graphs have been used to represent a large variety of complex data, from multimedia objects, social networks, hypertext/Web, knowledge graphs (RDF), mobility graphs,to protein interactions. Let D be a set of entities of interest, a graph, G ( V,E ) , defined over V = D describes the relationships between pairs of objects in D . The elements in the set V are referred to as the nodes or vertices of the graph. The elements of the set E are referred to as the edges and This work is supported by NSF Grants # 1339835 and # 1318788. This work is also supported in part by NSF grant # 0856090. c  X  2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00.
 Figure 1: Graph matching/alignment problem seeks a maxi-mum common subgraph isomorphism between two input graphs they represent the pairwise relationships between the nodes of the graph. Edges can be directed or undirected, meaning that the rela-tionship can be non-symmetric or symmetric, respectively. Nodes and edges of the graph can also be labeled or non-labeled. The label of an edge, for example, may denote the name of the rela-tionship between the corresponding pair of nodes or may represent other meta-data, such as the certainty of the relationship or the cost of leveraging that relationship within an application.

Due to the success of the graph model as a powerful and flex-ible data representation, graph analysis and search tasks are also increasingly critical in many application domains. In particular, understanding how a given set of graphs align with each other (also known as the graph matching/alignment problem, Figure 1) forms the core task in many search, classification, and analysis applica-tions. Unfortunately, the problem of maximum common subgraph isomorphism between two graphs is a well known NP-hard prob-lem [24], making it impractical to search for exact or maximal graph alignments. As a result, while there are some attempts to improve the performance of exact maximum common subgraph matching solutions [23], most of the recent efforts in the area have focused on seeking approximate/inexact graph alignments [3, 18, 22, 19, 29].

While these algorithms differ in their specific techniques, most of them rely on a four phase process: 1. First, the matching algorithm analyzes and encodes the 2. Secondly, the algorithm analyzes and encodes the local struc-3. Once these global and local signatures are encoded, the Figure 2: Keynode selection problem for scalable graph match-ing: the nodes marked with "*" are keynodes of the input graphs that can be used to reduce the amount of time needed to search for alignments 4. Finally, a post-processing, or refinement, step (involving, for Unfortunately, many of these steps result in significant scalability challenges in terms of the matching time needed to compare the pairs of nodes:
Based on these observations, in this paper, we argue that a more practical and scalable solution would be to seek structural keyn-odes of the input graphs that can be used to reduce the amount of time needed to search for alignments (Figure 2). Of course, these keynodes must be selected carefully to prevent any degradations in accuracy during the alignment process, especially because, as men-tioned above, refinement post-processes are detrimental to scalabil-ity of matching algorithms.

Given this motivation, in this paper, we first present a highly ef-ficient and effective structural keynode extraction ( SKE ) algorithm. The SKE algorithm, which is executed off-line, relies on a 3-step process: 1. In the first step, a PageRank algorithm [7] is ran to associate 2. In the second step, a scale-space (based on a difference-of-3. In the third step, keynode candidates are extracted by ana-We then propose a graph matching algorithm that uses these struc-tural keynodes (obtained during off-line processing) for keynode-driven scalable graph matching ( KSGM ). In particular, KSGM ex-tracts only local signatures and relies on the structural keynodes for fast node-to-node similarity searching. In addition, we also show that this keynode-driven approach not only reduces the number of comparisons that need to be performed online, but it also enables effective matching, even without having to rely on an expensive as-signment algorithm, like the Hungarian algorithm (with O ( | V | complexity). Experiment results show that the proposed structural keynode extraction and keynode-driven scalable graph matching algorithms produce alignments that are as accurate as (or better than) the state-of-the-art algorithms, while requiring significantly less online execution time without refinement.
The paper is organized as follows: in the next section, we first introduce basic concepts and review existing graph matching algo-rithms. In Section 3, we provide overviews of the general graph matching process as well as the proposed keynode-driven scalable graph matching ( KSGM ) algorithm. Then, in Section 4, we present our structural keynode extraction ( SKE ) algorithm. In Sections 5 and 6, we discuss how to use these structural keynodes for ob-taining graph alignments. We discuss the complexity of the pro-posed algorithms and parallelization opportunities in Section 7. We present experimental evaluations with various real and synthetic data sets in Section 8. These confirm that the proposed approx-imate graph matching algorithm is highly effective and efficient. Finally, we conclude the paper in Section 9.
In this section, we review key concepts related to the graph matching problem and discuss the existing algorithms.
 Graph Isomorphism : Given two graphs G and H , G is isomor-phic to H if there exists a bijective mapping from the nodes of G to the nodes H that preserves the edge structure [13]: for any two vertices that are adjacent on G , the vertices they are mapped to are also adjacent on H , and vice versa.
 Subgraph Isomorphism : Subgraph isomorphism seeks a bijective function, f , such that there is a subgraph G 0 of G and a subgraph H 0 of H , such that G 0 is isomorphic to H 0 , with respect to f . Maximum Common Subgraph Isomorphism : Maximum com-mon subgraph isomorphism seeks the largest subgraph of G iso-morphic to a subgraph of H [24]. Intuitively, the larger the max-imum common subgraph of two graphs is, the more similar the graphs are to each other.

One of the first exact graph matching algorithms was proposed by Ullman [24]. An alternative way to search for a matching be-tween two graphs is to rely on graph edit distance algorithms: given two graphs the corresponding graph edit distance is the least cost sequence of edit operations that transforms G 1 into G monly used graph edit operations include substitution , deletion , and insertion of graph nodes and edges. Unfortunately, the graph edit distance problem is also known to be NP-complete [24]. In fact, even approximating graph-edit distance is very costly; the edit dis-tance problem is known to be APX-hard [8]. [8] shows that graph isomorphism, subgraph isomorphism, and maximum common sub-graph problem are special instances of the graph edit distance com-putation problem. Many subgraph isomorphism search algorithms have been developed, such as [15, 29, 14].
 Approximate Graph Matching : In order to be applicable to large graphs, many heuristic and approximate graph matching algorithms have been proposed.

While, as we discussed above, graph matching through edit dis-tance computation is an expensive task, there are various heuristics that have been developed to perform this operation more efficiently. GraphGrep [14] is one such technique, relying on a path-based representation of graphs. GraphGrep takes an undirected, node-labeled graph and for each node in the graph, it finds all paths that start at this node and have length up to a given, small upper bound, l . Given a path in the graph, the corresponding id-path is the list of the ids of the nodes on the path. The corresponding label-path is the list of the labels of the nodes on the path. The fingerprint of the graph, then, is a hash table, where each row contains the hash of the label-path and the corresponding number of id-paths in the graph. Irrelevant graphs are filtered out by comparing the numbers of id-paths for each matching hash key and by discarding those graphs which have at least one value in its fingerprint less than the corre-sponding value in the fingerprint of the query. Matching sub-graphs are found by focusing on the parts of the graph which correspond to the label-paths in the query. After, the relevant id-path sets are selected and overlapping id-paths are found and concatenated to build matching sub-graphs.

A common method to obtain an approximate graph matching is to use the eigenvectors derived from the adjacency matrix of the graph [23]: intuitively, two similar graphs should have simi-lar eigenvectors; moreover, if we construct a | V | X | V | matrix (for example the Laplacian of the graph or a matrix encoding node dis-tances) and decompose it into three matrices of | V |  X  c , c  X  c , and c  X | V | elements using an eigen-decomposition technique like SVD, the c -length vector corresponding the node v  X  V can be used as a global-signature corresponding to node v . Once node-to-node similarities are computed, an assignment is usually found us-ing an assignment algorithm, such as the Hungarian algorithm [16], which uses a primal-dual strategy to solve this problem in O ( | V | time. This simple observation, led to several works leveraging dif-ferent global-signatures for identifying node matches across differ-ent graphs [3, 18, 22, 19, 29]. [28] formulates the labeled weighted graph matching problem in the form of a convex-concave program, which searches for appropriate permutation matrices by solving a least-square problem. In addition, feature selection techniques are used for more accurate calculation [11, 12, 20]. In order to improve matching accuracy, [29] proposes to enrich the global-signatures associated to the graph nodes with local-signatures, encoding the properties of the immediate neighborhood of each node. Given a set G = { G 1 ,G 2 ,...,G g } of graphs and a query graph G q  X  G , in this paper, we seek the maximum graph matching be-tween G q and all G i  X  G ( i 6 = q ). Note that the exact solution for this problem is NP-hard [24]. Since we treat scalability as a key constraint, we consider inexact solutions and rely on the match-ing quality measure proposed in [29] to evaluate the accuracies of the resulting alignments: Let G q ( V q ,E q ) be a query graph and let G ( V i ,E i ) be a graph in G . Let M q,i ( V q,i ,E q,i ) be a subgraph of both G q and G i , returned by an inexact subgraph search algorithm. [29] defines the matching quality function as follows: Intuitively, the quality function describes how similar the given query graph G q and known graph G i are by using the ratio of matched edges and the maximum number of edges that can be pos-sibly matched, which is equal to the minimum number of edges be-tween two graphs. In other words, the larger the number of edges in the graph M q,i , the better is the quality of the matching (or the more similar the two graphs are).
Given a query graph G q , our goal is to rank the graphs in G ac-cording to their matching similarities against G q (and eventually return the top few matches to the user). [29] solves this problem by relying on a 6-step process, common to many graph search algo-rithms: 1. [29], first, analyzes the global structure of each graph 2. Secondly, [29] encodes the structural information local to 3. Given the global and local signatures of all nodes in G Algorithm 1 Overview of keynodes based graph matching Input: Output: 1: for all G i  X  X  (including G q ) do 2: Perform structural keynode extraction ( SKE ) for G i 3: Extract local-signatures for all nodes in G i 4: (Optional) Extract global-signatures for all nodes in G 5: end for 6: for all G i  X  X  do 7: Compute local similarities for keynode pairs from G i and 8: (Optional) Compute global similarities for keynode pairs 9: Select anchors to obtain a base matching 10: Expand the base matching to obtain M q,i 11: Compute matching quality, quality ( M q,i ) 12: end for 13: Rank G i  X  X  in terms of quality ( M q,i ) 4. Once the overall similarities for | V q | X | V i | pairs of nodes 5. When no more pairs can be added to the anchor set, [29] uses 6. Finally, as a post-processing step, [29] applies a vertex cover
This process includes a number of very expensive steps: The first two steps, involving global and local analysis are expensive, but can be performed off-line and indexed for later reuse assuming that the graphs are available ahead of time. The last four steps, however, need to be performed on-line, yet they consist of operations that are quadratic or higher. In particular, the last refinement step, with O ( m  X  n 3 ) time cost is impractical for most large data graphs.
In this paper, we note that Step 3 can be significantly sped up if the similarity computations are limited to only a small subset of the vertices in V q and V i (which we refer to as keynodes of V V ). However, the use of keynodes for node similarity computation is not sufficient to reduce the overall complexity as, once the keyn-odes are identified and the keynode pairs set is expanded, solving the assignment problem needed to return the matching would still take O ( max {| V q | , | V i |} 3 ) time, if we were to apply the Hungarian algorithm on the extracted keynodes. Therefore, we also need to re-duce the time complexity of this step significantly. It is especially important that the initial keynode based similarity computation is accurate as we cannot afford a cubic algorithm like Hungarian al-gorithm to return a high-quality matching.
Algorithm 1 illustrates an overview of the keynode-driven scal-able graph matching ( KSGM ) process. In the rest of the paper, we study each step in detail. First, in the next two sections, we focus on the offline steps of KSGM , which involve identifying keynodes and extracting local-signatures. The online steps of the KSGM algorithm are discussed in Section 6.
In this section, we propose an off-line structural keynode ex-traction ( SKE ) algorithm which identifies  X % (where  X  is a user provided parameter) of the nodes in V as the keynode set, K , of a given graph, G ( V,E ) to support scalable graph matching. The proposed SKE algorithm has a number of advantages: (a) First of all, the identified keynodes are robust against noise, such as ran-dom edge insertion/removal; and (b) the identified nodes represent structural features of the graph of different sizes and complexities (i.e., correspond to neighborhoods of different sizes).
As described above, the keynodes of the graph need to represent the structural properties of the graph well (i.e., extracted keynodes need to be structurally significant in the graph) to support effective matching. Therefore, the first alternative is to rely on traditional node significance measures.

Measures like betweenness [26] and the centrality/cohesion [5], help quantify how significant any node is on a given graph based on the underlying graph topology. The betweenness measure [26], for example, quantifies the number of shortest paths that pass through a given node. The centrality/cohesion [5] measures quantify how close to a clique the given node and its neighbors are. Other au-thority , prestige , and prominence measures [4, 7, 5] quantify the significance of the node in the graph through eigen-analysis or ran-dom walks, which help measure how reachable a node is in the graph. PageRank [7] is one of the most widely-used random-walk based methods for measuring node significance and has been used in a variety of application domains, including web search, biology, and social networks. The basic thesis of PageRank is that a node is important if it is pointed to by other important nodes  X  it takes into account the connectivity of nodes in the graph by defining the score of the node v i  X  V as the amount of time spent on v ficiently long random walk on the graph. Given a graph G ( V,E ) , the PageRank scores are represented as ~r , where where T G is a transition matrix corresponding to the graph G , probability (or equivalently, (1  X   X  ) is the so-called teleportation probability). Unless the graph is weighted, the transition matrix, T
G , is constructed such that for a node v with k (outgoing) neigh-bors, the transition probability from v to each of its (outgoing) neighbors will be 1 /k . If the graph is weighted, then the transi-tion probabilities are adjusted in a way to account for the relative weights of the edges.

Therefore, as the first alternative, we consider a PageRank based keynode selection scheme: in this scheme, given a graph G ( V,E ) , we would (a) first identify the PageRank scores, p ( v i ) , of all v V , then (b) we would rank the nodes in non-increasing order of PageRank scores, and finally, (c) we would return the top  X % of the nodes in V as the keynode set, K .
We note that the above alternative has a number of disadvan-tages: We therefore argue that we need a better alternative, which is both robust and multi-scale . We build the proposed SKE algorithm based on three key insights: It is important to note that similar requirements also exist in other application domains. For example, algorithms for extract-ing such robust, local features have been developed for 2D images (SIFT [21]), uni-variate time series [9], and multi-variate time se-ries [25]. In this paper, we argue that a similar process can be used to identify keynodes (corresponding to robust, multi-scale struc-tural features) of a graph, if the nodes are annotated with PageRank scores ahead of the time. Let G ( V,E,p ) be a PageRank-labeled graph, where p () is a mapping from the nodes to the correspond-ing PageRank scores. What makes the problem of extracting lo-cal features from PageRank -labeled graphs challenging is that the concepts of neighborhood , gradient , and smoothing are not well-defined for graphs.

Therefore, before we describe the keynode extraction process, we describe how to smooth a PageRank-labeled graph. Intuitively, smoothing the graph with respect to the scores associated to the graph nodes creates versions of the given graph at different reso-lutions and, thus, helps identify features with different amounts of details. 1D or 2D data are commonly smoothed by applying a convo-lution operation with a Gaussian window. For example, if Y =  X  t ,t 1 ,...,t l  X  is a time series data and  X  is a smoothing parameter, its smoothed version,  X  Y ( t, X  ) , is obtained through G ( t, X  )  X  Y ( t ) where  X  is the convolution operation in t and G ( t, X  ) is the Gaus-sian function
Essentially, the Gaussian smoothing process takes a weighted average of values of the points in the vicinity of a given point, t . The closer a point to t , the higher is the weight. Therefore, in order to implement a similar Gaussian smoothing of the given graph, we first need to define a distance function to measure how close dif-ferent nodes are to each other. Common applicable definitions of node distance include the hop distance (determined by the shortest edge distance between the nodes on the given graph) or hitting dis-tance [10]. In this paper, we use hop distance to measure how far nodes are to each other: a graph G ( V,E ) with n nodes. The ordering among the nodes is described through a set of node distance matrices, N j , where Intuitively, the cell N j [ v 1 ,v 2 ] = 1 if the node v 2 from v 1 . When j is positive the hop-distance is measured following outgoing edges, whereas when j is negative, incoming edges are followed. Given this, we construct multiple scales of the given graph G by using a Gaussian graph smoothing function defined as follows.
 Let us be given a labeled graph G ( V,E,x ) and let Then, if G is a directed graph, the non-normalized Gaussian graph smoothing function , S G, X  () is defined as where G (0 , X  ) is a Gaussian function with zero mean and  X  stan-dard deviation. If, on the other hand, G is an undirected graph, then the non-normalized Gaussian graph smoothing function is Intuitively, S applies Gaussian-based weighted averaging to the entries of vector X based on the hop-distances 2 . However, unlike the basic Gaussian smoothing, during (non-normalized) relation-ship smoothing, there may be more than one node at the same dis-tance and all such nodes have the same degree of contribution. As a consequence, the sum of all contributions may exceed 1.0. There-fore, the normalized Gaussian graph smoothing function , S ( G, X  ) , discounts weights based on the number of nodes at a given distance: where 1 ( n ) is an n -vector such that all values are 1 and  X   X   X  is a pairwise division operation.
In practice, since the Gaussian function drops fast as we move away from the mean, we need to consider only a small window, w , of hops Figure 3: Computing the Difference-of-Gaussian (DoG) series of the PageRank values of a graph Intuitively, the smoothing function S applies Gaussian smoothing on the X values (associated with the nodes, v i  X  V ) based on the hop-distances between nodes and returns a vector encoding the smoothed X values associated with the graph nodes. Note that, since at a given hop distance there may be more than one node, all the nodes at the same distance have the same degree of contribution and the degree of contribution gets progressively smaller as we get further away from the node for which the smooth-ing is performed.

Therefore, given a PageRank-labeled graph, G ( V,E,p ) , and a corresponding PageRank vector, P =  X  p ( v 1 ) ,p ( v 2 encoding PageRank scores associated with the nodes, v i  X  V , the vector encodes the  X  -smoothing of the PageRank-annotated graph, G ( V,E,p ) . We also say that S G, X  ( P ) encodes the PageRank scores of G at scale  X  .

We next describe how to construct a scale-space for the given graph through an iterative smoothing process leveraging the PageR-ank vector and the structure of the graph.
The first step in identifying robust graph features is to generate a scale-space representing versions of the given graph with different amounts of details. In particular, building on the observation that features are often located where the differences between neighbor-ing regions (also in different scales) are large, we seek structural features of the given graph at the extrema of the scale space defined by the difference-of-the-Gaussian (DoG) series. More specifically, given then, we compute a difference-of-Gaussians (DoG) series, D ( G,P, X  min , X  max ,l ) = { D 1 ,D 2 ,...,D l } , where each D codes the differences of two nearby scales separated by a multi-plicative factor k : Note that, intuitively, D i [ j ] measures how different the PageRank values of the neighborhood around v j at scale  X  i  X  1 (= k are from the PageRank values of the neighborhood around v scale  X  i (= k i  X  min ) .

Therefore, a large D i [ j ] value would indicate a major structural change when neighborhoods of different size around v j are con-sidered (e.g., a node with a high PageRank score is included when considering a neighborhood of larger scale). In contrast, a small D [ j ] indicates that there is minimal structural change when con-sidering neighborhoods of different scales.
As we mentioned earlier, our intuition is that a graph node is structurally distinctive in its neighborhood, if "the relationship be-tween its PageRank score to the PageRank scores of its neighbors" is different from the "relationships between the node X  X  neighbors X  PageRank scores and the PageRank scores of their own neighbors, at multiple scales". Therefore, to locate the keynode candidates, we focus on the local extrema of the difference-of-Gaussian (DoG) series D . More specifically, we identify  X  v j , X  i  X  pairs where the DoG value for node v j at scale,  X  i = k i  X  min , is an extremium (maximum and/or minimum) with respect to the neighbors of v the same scale as well as neighbors in the previous and next levels of the scale space.

In order to verify if the pair  X  v j , X  i  X  is an extremium or not, we compare D i [ j ] with the values corresponding to eight DoG-neighbors in the scale-space, as visualized in Figure 4: Given these, the pair  X  v j , X  i  X  is an extremum (i.e., v candidate at scale  X  i ), iff D i [ j ] is a local maximum or it is a local minimum Intuitively, since D i [ j ] measures how different the PageRank val-ues of the neighborhood around v j at scale  X  i are from the PageR-ank values of the neighborhood around v j at scale  X  i  X  1 maximum corresponds to a highly scale-sensitive region (amidst relatively scale-insensitive regions), whereas a local minimum cor-responds to a scale-insentive region (amidst more scale-sensitive regions), of the graph.
In the final step, we need to rank the keynode candidates and return the top  X  100  X | V | of them, where  X  is a user provided pa-rameter, as the keynode set, K . We propose Extremum Ranking to select the best keynodes.

Since keynodes are located at the local extrema of the DoG se-ries, we can rank the keynode candidates based on their extremum score defined as follows: Let the pair  X  v j , X  i  X  be a local extremum. The corresponding extremum score,  X  (  X  v j , X  i  X  ) , is defined as
Intuitively, the higher the extremum score is, the better local ex-tremum (and, thus, a better keynode) is  X  v j , X  i  X  .
The next step in the process is to extract the local signatures (to be used to compute local node similarities) for the nodes in the graph. Note that this process is also offline.

While there are different local signatures proposed in the liter-ature, in our work we build on the k -neighborhood degree distri-bution based local signature proposed in [29] (both because it is simple and effective and also because this helps us compare our keynode-driven approach to the approach proposed in [29] more directly). Briefly, for each node v j  X  V and for a user provided k , [29] first identifies the set, N k ( v j )  X  V , of nodes that are at most k hops from v j and extracts a subgraph, G k ( v j )  X  G , induced by v and its k -hop neighbors. Then the degree sequence, consisting of the degrees of nodes in G k ( v j ) (excluding v in non-increasing order, along with the degree of the node v the numbers of vertices and edges in its k -hop neighborhood, form the local signature of node v j : where  X  j = | N k ( v j )  X  X  v j }| is the number of nodes in G  X  = | E k ( v j ) | is the number of edges.

Note that, while we use a local signature similar to that proposed in [29], we extend the node pair ranking function to better account for the node degrees as discussed later in Section 6.1.1. As we see in Section 8, this extension provides a significant boost in accuracy.
As discussed in Section 3 and outlined in Algorithm 1, once the keynodes are extracted and local signatures are computed offline, the next steps of the algorithm are to We now describe how these steps are implemented in the keynode-driven scalable graph matching ( KSGM ) algorithm.
Let G 1 ( V 1 ,E 1 ) and G 2 ( V 2 ,E 2 ) be two graphs and let K and K 2  X  V 2 be the corresponding keynodes identified by the SKE algorithm proposed in Section 4. The next step is to select a subset, A , of the pairs of nodes in K 1  X  K 2 as the anchor set of alignments based on a ranking function (a) evaluating how structurally similar a pair of nodes are and (b) how likely they are to lead to an effective expansion process to discover other alignments.
As we discussed in Section 5, KSGM uses a local node signa-ture similar to the one proposed by [29]:  X   X  j ,degree ( v where  X  j is the number of nodes in the neighborhood of v ,  X  j is the number of neighborhood edges, and  X  [ d k -neighborhood of v j (excluding v j ), sorted in non-increasing or-der.

Let v i and v j be two nodes (from two different graphs) and let G k ( v i ) and G 0 k ( v j ) be the corresponding induced k  X  neighborhood graphs. Then, local similarity function nbhd _ sim ( v i ,v j ) proposed by [29] accounts for the alignment be-tween the degree distributions in these neighborhood graphs where
While the local neighborhood similarity computation we use is similar to the one proposed in [29], we rank pairs of nodes differ-ently. Let v i and v j be two nodes (from two different graphs). In particular, [29] ranks the pair  X  v i ,v j  X  of nodes based on their neigh-borhood similarities, nbhd _ sim ( v i ,v j ) . We, however, argue that neighborhood similarity is not sufficient for accounting for how ef-fective the node pair is in supporting expansion. More specifically, we observe that a pair,  X  v i ,v j  X  , is likely to be a better anchor for expansion than the pair  X  v a ,v b  X  if not only (a) the neighborhoods of v i and v j are more similar to each other than the pair, v v , but also (b) if v i and v j have degrees that are more aligned with each other than v a and v b . Based on this observation, instead of ap-plying a degree threshold, we propose that the pair  X  v i be ranked based on the ranking function  X  ( v i ,v j ) = nbhd _ sim ( v i ,v j )  X  min { degree ( v Note that [29] simply drops node pairs where the minumum of the two node degrees is smaller than the larger average degree of the two input graphs. We, however, argue that such node pairs may be useful, especially if the degrees in the graph are not uniformly distributed and the maximum matching occurs at the sparse por-tions of the graph. Therefore, we keep such pairs as long as they rank highly based on  X  () . We evaluate this ranking function in Sec-tion 8. [29] uses the Hungarian algorithm to identify an initial node matching in O ( n 3 ) time, where n = max {| V 1 | , | V 2 the execution time, [29] prunes those node pairs for which the sim-ilarity is  X  0 . 5 . Since, instead of considering the node pairs in V  X  V 2 , we only need to consider pairs of nodes in K 1  X  K since | K 1 | | V 1 | and | K 2 | | V 2 | , keynode-driven processing is likely to be faster even without using the threshold. However, the cubic time of the Hungarian algorithm is still prohibitive and impractical for scalable graph matching. Therefore, we propose a greedy anchor selection algorithm, which (as we see in Section 8) performs very well when used along with keynodes selected in Sec-tion 4 and the proposed ranking function,  X  () . In particular, we first include all keynode pairs in K 1  X  K 2 into a queue in the order of their ranks based on  X  () , then, until the queue is empty, we remove and consider the keynode pair,  X  v,u  X  at the head of the queue. If neither v nor u has been marked anchored , we include  X  v,u  X  as an anchor and we mark v and u as anchored , otherwise, we drop the pair,  X  v,u  X  .

Note that this process has O (( | K 1 | X | K 2 | )  X  log ( | K time cost (instead of the cubic cost of the Hungarian algorithm) and,
In addition to using local similarities, [29] also extracts global signatures along with the local-signatures to compute node similar-ities. As we see in Section 8, the proposed keynode-driven graph matching algorithm achieves good results without having to rely on such global-signatures. as we see in Section 8, performs very well in practice. Furthermore, the nature of Hungarian algorithm, which forces to pair all possible nodes to produce the optimal bipartite matching for the given two sets of nodes, is not guaranteed to provide a better matching in this case. Since the extracted keynodes are not all necessarily perfectly paired with each other, some keynodes can be a unique feature of the given graph, which does not align with other graphs, by forcing them to pair with other keynodes, it in fact introduces a bad initial base matching, and thus expand into an even worse matching. The proposed greedy matching algorithm, however, only consider the highly aligned keynodes, which in practice provides better results than the optimal bipartite matching.
Because keynodes are inherently sparsely localized, the anchor set, A , is not necessarily a good final matching for graphs G G . We therefore need to expand this anchor list. Here, we fol-low [29] X  X  recommendation and expand the list incrementally by considering the neighbors (and their neighbors) until no effective expansion is possible (but we use the ranking function  X  () instead of the node similarity function): 1. we first include all node pairs in A into a ranked queue (i.e., 2. then, for each node pair  X  v,u  X  X  X  A , we also include the node 3. then, until the ranked queue is empty, we remove and con-
Once the anchor list is expanded, [29] relies on a post-process, with time complexity, O ( m  X  n 3 ) , where m = min {| E 1 and n = max {| V 1 | , | V 2 |} . This step is not scalable due to its pro-hibitive time complexity. Therefore, the proposed keynode-driven scalable graph matching ( KSGM ) algorithm omits this refinement post-process, due to its high time complexity 4 . Instead, the set, L , of node pairs remaining after the expansion process is directly returned as the aligned nodes of the matching, M 1 , 2 , for the input graphs, G 1 and G 2 .
Let G ( V,E ) be a graph to be indexed in the database. The first step in structural keynode extraction is to obtain the PageRank scores for the nodes of the two graphs. While, this is an expensive operation (involving a matrix inversion with O ( | V | complexity for a graph with | V | nodes), there are many efficient, approximate implementations of PageRank, including sublinear ap-proximations [6].
 The second step is the creation of an l -layer scale-space for G . To construct the scale space, we first construct a node dis-tance matrix, which requires an all-pairs shortest path computa-tion, with complexity O ( | V | 3 ) for a graph with | V | nodes, but
Though, in cases where scalability is not critical, this refinement can be implemented without any change in the rest of the algorithm. more efficient randomized algorithms exist [27]. Once the node distances have been computed, we construct the scale-space in O ( l  X | V | X  max _ w _ nbhd _ size ) , as for each of the l scales, the score of each node needs to be smoothed considering the scores of the vertices in its w -hop neighborhood ( w is the Gaussian win-dow size and max _ w _ nbhd _ size is the size of the largest w -hop neighborhood in G ).

Once the scale-space is constructed, next, we identify the keyn-ode candidates. This involves O ( l  X | V | ) time, because for each of the l scales, each node needs to be compared with a constant number ( 8 ) of DoG-neighbors in the scale-space.
 Finally, we rank the keynode candidates to select the top K = 100 V many as the keynodes to bootstrap the online matching pro-cess. Let there be C many keynode candidates. Computing the ranking scores for these takes O ( C ) time, because each keynode candidate needs to be compared with a constant number of DoG-neighbors and obtaining the top K takes O ( C  X  log ( K )) time.
Since the local signature extraction process needs to extract the k -hop neighborhoods around the nodes, the complexity of this step is O ( | V | X  max _ k _ nbhd _ size ) , where max _ k _ nbhd _ size is the size of the largest k -hop neighborhood in G . Note that this step can also leverage the node distance matrix constructed during the offline keynode extraction process.
Let G 1 ( V 1 ,E 1 ) and G 2 ( V 2 ,E 2 ) be two graphs. The online pro-cess includes the following operations.
This process has O ( | K 1 | X | K 2 | X  compare _ length ) complex-ity, where compare _ length = min { max _ k _ nbhd _ size 1 ,max _ k _ nbhd _ size since signatures (of length are compared for each pair of nodes in the keynode sets K 1 and K 2 .
This greedy process has O (( | K 1 | X | K 2 | )  X  log ( | K time cost as each pair off nodes among the keynode sets need to be considered only once in ranked order.
This has O (( | V 1 | X | V 2 | )  X  log ( | V 1 | X | V 2 | )) worst case time cost, as in the worst case, all pairs of vertices across the two graphs may need to be considered for expansion in ranked order.
In this section, we present experimental evaluations of the pro-posed keynode-driven scalable graph matching ( KSGM ) algorithm. In particular, we compare KSGM to the graph matching algorithm presented in [29] in terms of efficiency and accuracy.
The first data set we used is the Facebook social circles data graph obtained from the Stanford Large Network Dataset Collec-tion [2]. This is a connected graph with 4039 nodes and 88234 edges. The graph has a diameter of 8 and a 90-percentile effective diameter of 4 . 7 . For the experiments, we constructed 10 subgraphs by uniformly sampling connected subsets, containing 60  X  70% of the original graph nodes. Once the subgraphs are obtained, each of the subgraphs is used as a query against the rest. We report the averages of execution time and accuracy.
In addition to the Facebook graph, we also used synthetic graphs, where we controlled the topology, size, and node degree to explore the advantages and disadvantages of the algorithms under different scenarios.

We generated the synthetic graphs using the well known random graph generating tool, NetworkX [1]. We consider two common graph topologies: the Erdos-Renyi (ER) model and the power law topology (PL) under the Barabasi-Albert model. Table 1 lists the number of nodes and average degree settings that we used for as-sessing our algorithms. For each configuration, we generated 10 graphs. Note that, in addition to the base sizes (of 5000 and 7500), we randomly created an additional 1 to 10% more nodes to ensure that the different graphs in the data set have slightly different num-bers of nodes. As before, once the 10 graphs are obtained for each configuration, each of the subgraphs is used as query against the rest. We report the averages of execution time and accuracy.
All experiments were conducted using a 4-core Intel Core i5-2400, 3.10GHz, machine with 8GB memory, running 64-bit Win-dows 7 Enterprise. The codes were executed using Matlab 2013b and Visual Studio 2012. To evaluate accuracy, we use the matching quality defined in Section 3. We report both offline and online execution times. As shown in Algorithm 1 in Section 3, for KSGM , the offline execution includes structural keynode and local-signature extraction steps. Online ex-ecution includes similarity computation, anchor selection, and ex-pansion steps. [29] does not perform structural keynode extraction; instead, offline execution includes eigen-decomposition for global signatures.

For both KSGM and [29], we omit the refinement step as its com-plexity is prohibitive for scalable graph matching. For instance, for the Facebook graph for which KSGM takes  X  6 seconds for online processing for a pair of graphs, refinement takes  X  30 minutes  X  i.e., it causes a  X  260  X  slowdown 5 .
The default parameters for the structural keynode extraction ( SKE ) algorithm are as follows: In addition, for KSGM , the default percentage (  X  ) of keynodes se-lected from the graph was set to 3% . Also, for all algorithms, local signatures were extracted from 2 -hop neighborhoods (i.e., k = 2 ), as recommended by the authors of [29].
Table 2 lists the online and offline processing times and accu-racy for the Facebook graph under the default parameter settings. As we see here, while KSGM spends more time in one-time, offline
For this data configuration, when using expensive refinement post-processing, KSGM and [29] X  X  accuracies are 0.72 and 0.696, respec-tively.
 Table 2: Experiment results for the Facebook Graph (default parameters) Table 3: Impact of the keynode percentage,  X  , for the Facebook Graph Table 4: Impact of the node-pair ranking function,  X  () , for the Facebook Graph Table 5: Impact of the local-signature neighborhood size, k , for the Facebook Graph processing, its online matching time is 3  X  faster than that of [29]. Moreover, the matching accuracy of KSGM is 1 . 2  X  better than that of the competitor, through it does not use global signatures, nor it relies on the optimal Hungarian algorithm for anchor selection. The table also lists the performance of KSGM when using top PageRank (PR) scored nodes instead of those returned by the SKE algorithm. As we see here, while the offline process is faster when using PageRank scoring nodes, the runtime performance (both in terms of execution time and accuracy) is worse when using SKE keynodes. In addition, to see whether it is possible to achieve a competitive accuracy if we were to select a similar percentage of node randomly, in Table 2, we also include results where random keynodes are used in the matching online phase. As we can see, the accuracy drops significantly when we use random keynodes instead of using robust structural keynodes extracted by the proposed SKE algorithm 6 . These indicate that SKE is indeed effective in extracting structurally distinct and useful keynodes.
Table 3 studies the impact of the percentage,  X  , of the nodes used as keynodes. As we see, up to a point, the more keynodes we use, the more accurate and faster the matching becomes. Beyond that point, however, additional keynodes become disadvantageous. This indicates that top keynodes are the most effective in serving as good starting points and, as expected, below a certain rank they loose distinctiveness, resulting in increased cost and loss in accuracy.
Table 4 studies the impact of the node-pair ranking function,  X  () . In particular, we compare the performance of the ranking function proposed in Section 6.1.1, to the ranking function without degree extension and ranking function including additional global-signature similarity as proposed in [29]. As we see here, the pro-posed node-pair ranking function provides the best expansion op-portunities (and thus provides the highest accuracy, with slight ex-pansion time overhead). Also, the "with Global" optional provides a much worse matching. Thus, while the algorithm allows, we en-courage the users not to use "with Global" option.
The slight time gain when using random keynodes is due to the fact that random keynodes are not good starting points for expan-sion and, thus, the expansion process ends earlier. Table 6: Experiment results for the synthetic data sets (avg. degree=4, varying models and number of nodes) Table 7: Impact of the average node degree (number of nodes=5000, power law model)
Table 5 studies the impact of the neighborhood size, k , for local-signature extraction. As we see in the table, the highest accuracy is at 2 hops 7 , increasing the neighborhood size negatively affects the accuracy, indicating that unless locally meaningful signatures are used, the resulting node-pair ranking is not effective for expansion. This shows the keynode matching process is more accurate when keynodes are easy to localize and this requires them to be distinct and locally representative. Large neighborhoods potentially violate both. Note that this is in line with the observation in Table 4.
In this subsection, we consider the impacts of graph topology, size, and node degree using ER and PL topologies. We omit dis-cussions of the impacts of the other parameters, as they mirror those presented in Tables 3 through 5.
Table 6 lists the performances of KSGM and [29] for synthetic graphs for different topologies and numbers of nodes under the de-fault parameter settings. As we see here, the online execution time of KSGM is significantly ( 10  X  to 115  X  ) faster than that of [29], es-pecially for the ER topology. Moreover, on both Erdos-Renyi (ER) and power law (PL) topologies, the accuracy is highly competi-tive, with KSGM providing non-negligible accuracy gains for the PL model (where it is relatively easier to identify effective keynodes).
Table 6 studies the impact of average node degree on matching accuracies for the power law graph. As we see in the table, both algorithms see a drop in the matching accuracy with larger node degrees. However, KSGM stays competitive in terms of accuracy, whereas it provides more gains in terms of online execution time.
Noticing that existing solutions to the graph matching problem face major scalability challenges, we argue that it is impractical to seek alignment among all pairs of nodes. Given these observations, in this paper, we first presented an offline structural keynode ex-traction ( SKE ) algorithm and then discussed how to use these struc-tural keynodes in a novel keynode-driven scalable graph matching ( KSGM ) algorithm. Keynodes are selected carefully especially be-cause a post refinement step is not feasible due to scalability re-quirements. Experiment results show that the proposed KSGM al-
Coincidentally, this also is the scale at which the SKE algorithm located an overwhelming majority of the keynodes for this graph. gorithm works faster than the state-of-the-art algorithms without refinement, yet produces alignments that are as good or better. We thank the authors of [29] for sharing their source code and data.
