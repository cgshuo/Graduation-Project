 As question-answering systems adv ance from han-dling factoid questions to more comple x requests, the y must be able to determine how much informa-tion to include while making sure that the informa-tion selected is indeed rele vant. Unlik e factoid ques-tions, there is no clear criterion that denes the kind of phrase that answers the question; instead, there may be man y phrases that could mak e up an answer and it is often unclear in adv ance, how man y. As system developers, our goal is to yield high recall without sacricing precision.

In response to questions about particular events of interest that can be enumerated in adv ance, it is pos-sible to perform a deeper semantic analysis focusing on the entities, relations, and sub-e vents of interest. On the other hand, the deeper analysis may be error -ful and will also not always pro vide complete cov-erage of the information rele vant to the query . The challenge, therefore, is to blend a shallo wer , rob ust approach with the deeper approach in an effecti ve way.

In this paper , we sho w how this can be achie ved through a syner gistic combination of information re-trie val and information extraction. We interlea ve in-formation retrie val (IR) and response generation, us-ing IR in high precision mode in the rst stage to return a small number of documents that are highly lik ely to be rele vant. Information extraction of enti-ties and events within these documents is then used to pinpoint highly rele vant sentences and associated words are selected to revise the query for a sec-ond pass of retrie val, impro ving recall. As part of this process, we approximate the rele vant conte xt by measuring the proximity of the tar get name in the query and extracted events.

Our approach has been evaluated in the frame-work of the DARP A GALE 1 program. One of the GALE evaluations involv es responding to questions based on a set of question templates, ranging from broad questions lik e  X Pro vide information on X  X , where X is an organization, to questions focused on particular classes of events. For the experiments pre-sented here, we used the GALE program' s prosecu-tion class of questions. These are given in the fol-lowing form:  X Describe the prosecution of X for Y , X  where X is a person and Y is a crime or char ge. Our results sho w that we are able to achie ve higher accu-rac y with a system that exploits the justice events identied by IE than with an approach based on query-focused summarization alone.

In the follo wing sections, we rst describe the task and then revie w related work in question-answering. Section 3 details our procedure for nd-ing answers as well as performing the information retrie val and information extraction tasks. Section 4 compares the results of the two approaches. Finally , we present our conclusion and plans for future work. 1.1 The Task The language of the question immediately raises the question of what is meant by prosecution. Unlik e a question such as  X When was X born? X , which is ex-pected to be answered by a clear , concrete phrase, the prosecution question asks for a much greater range of material. The answer is in no way limited to the statements and acti vities of the prosecuting at-torne y, although these would certainly be part of a comprehensi ve answer .

In the GALE rele vance guidelines 2 , the answer can include man y facets of the case:  X   X   X   X   X   X   X   X 
The guidelines also pro vide a catchall instruction to  X include reported information belie ved to be rele-vant to the case, but deemed inadmissible in a court of law. X 
It is easy to see that the use of a few search terms alone will be insuf cient to locate a comprehensi ve answer .

We took a broad vie w of the question type and consider that any information about the investiga-tion, accusation, pursuit, capture, trial and punish-ment of the indi vidual, whether a person or organi-zation, would be desireable in the answer . 1.2 Ov erview The rst step in our procedure sends a query tai-lored to this question type to the IR system to ob-tain a small number of high-quality documents with which we can determine what name variations are used in the corpus and estimate how man y docu-ments contain references to the indi vidual. In the future we will expand the type of information we want to glean from this small set of documents. A secondary search is issued to nd additional docu-ments that refer to the indi vidual, or indi viduals.
Once we have the complete document retrie val, the foundation for nding these types of events lies in the Proteus information extraction compo-nent (Grishman et al., 2005). We emplo y an IE sys-tem trained for the tasks of the 2005 Automatic Con-tent Extraction evaluation, which include entity and event extraction. ACE denes a number of general event types, including justice events, which cover in-dictments, accusations, arrests, trials, and sentenc-ings. The union of all these specic cate gories gives us man y of the salient events in a criminal justice case from beginning to end. The program uses the events, as well as the entities, to help identify the passages that respond to the question.

The selection of sentences is based on the as-sumption that the co-occurrence of the tar get indi-vidual and a judicial event indicates that the tar get is indeed involv ed in the event, but these two do not necesssarily occur in the same sentence. A lar ge body of work in question-answering has fol-lowed from the opening of the Text Retrie val Con-ference' s Q&amp;A track in 1999. The task started as a group of factoid questions and expanded from there into more sophisticated problems. TREC pro vides a unique testbed of question-answer pairs for re-searchers and this data has been inuential in fur -thering progress.

In TREC 2006, there was a new secondary task called  X comple x, interacti ve Question Answering,  X  (Dang et al., 2006) which is quite close to the GALE problem, though it incorporated interaction to im-pro ve results. Questions are posed in a canonical form plus a narrati ve elaborating on the kind of in-formation requested. An example question (from the TREC guidelines) asks,  X What evidence is there for transport of [drugs] from [Bonaire] to the [United States]? X  Our task is most similar to the fully-automatic baseline runs of the track, which typically took the form of passage retrie val with query ex-pansion (Oard et al., 2006) or synon ym processing (Katz et al., 2006), and not the deeper processing emplo yed in this work.

Within the broader QA task, the other question type is closest to the requirements in GALE, but it is too open ended. In TREC, the input for other questions is the name or description of the tar get, and the response is supposed to be all information that did not t in the answers to the pre vious ques-tions. While a few GALE questions have similar in-put, most, including the prosecution questions, pro-vide more detail about the topic in question.
A number of systems have used techniques in-spired by information extraction. One of the top sys-tems in the other questions cate gory at the 2004 and 2005 evaluations generated lexical-syntactic pat-terns and semantic patterns (Schone et al., 2004). But the y build these patterns from the question. In our task, we took adv antage of the structured ques-tion format to mak e use of extensi ve work on the semantics of selected domains. In this way we hope to determine whether we can obtain better per -formance by adding more sophisticated kno wledge about these domains. The Language Computer Cor -poration (LCC) has long experimented with incorpo-rating information extraction techniques. Recently , in its system for the other type questions at TREC 2005, LCC developed search patterns for 33 tar get classes (Harabagiu et al., 2005). These patterns were learned with features from WordNet, stemming and named entity recognition.

More and more systems are exploiting the size and redundanc y of the Web to help nd answers. Some obtain answers from the Web and then project the answer back to the test corpus to nd a supporting document (Voorhees and Dang, 2005). LCC used  X web boosting features X  to add to key words (Harabagiu et al., 2005). Rather than go to the Web and enhance the question terms, we made a beginning at examining the corpus for specic bits of information, in this prototype, to determine alter -nati ve realizations of names. As stated abo ve, the system tak es a query in the XML format required by the GALE program. The query templates allo w users to amplify their requests by specifying a timeframe for the information and/or a locale. In addition, there are pro visions for en-tering synon yms or alternate terms for either of the main arguments, i.e. the accused and the crime, and for related but less important terms.

Since this system is a prototype written especially for the GALE evaluation in July 2006, we paid close attention to the way example questions were given, as well as to the evaluation corpus, which consisted of more than 600,000 short news articles. The goal in GALE was to offer comprehensi ve results to the user , pro viding all snippets, or segments of texts, that responded to the information request. This re-quired us to develop a strate gy that balanced pre-cision against recall. A system that reported only high-condence answers was in danger of having no answers or far fewer answers than other systems, while a system that allo wed lower condence an-swers risk ed producing answers with a great deal of irrele vant material. Another way to look at this bal-ancing act was that it was necessary for a system to kno w when to quit. For this reason, we sought to obtain a good estimate of the number of documents we wanted to scan for answers.

Answer selection focused rst on the name of the suspect, which was always given in the query tem-plate. In man y of the training cases, the suspect was in the news only because of a criminal char ge against him; and in most, the char ge specied was the only accusation reported in the news. Both location and date constraints seemed to be lar gely superuous, and so we ignored these. But we did have a mecha-nism for obtaining supplementary answers keyed to the brief description of the crime and other related words
The rst step in the process is to request a seed collection of 10 documents from the IR system. This number was established experimentally . The IR query combines terms tailored to the prosecution template and the specic template parameters for a particular question. The 10 documents returned are then examined to produce a list of name variations that substantially match the name as rendered in the query template. The IR system is then ask ed for the number of times that the name appears in the cor -pus. This gure is adjusted by the frequenc y per document in the seed collection and a new query is submitted, set to obtain the N documents in which we expect to nd the tar get' s name. 3.1 Inf ormation Retrie val The goal of the information retrie val component of the system was to locate rele vant documents that the summarization system could then use to construct an answer . All search, whether high-precision or high-recall, was performed using the Indri retrie val sys-tem 3 (Strohman et al., 2005).

Indri pro vides a powerful query language that is used here to combine numerous aspects of the query . The Indri query regarding Saddam Hus-sein' s prosecution for crimes against humanity in-cludes the follo wing components: source restric-tions, prosecution-related words, mentions of Sad-dam Hussein, justice events, dependence model phrases (Metzler and Croft, 2005) regarding the crime, and a location constraint.

The rst part of the query located references to prosecutions by looking for the keyw ords prosecu-tion , defense , trial , sentence , crime , guilty , or ac-cuse , all of which were determined on training data to occur in descriptions of prosecutions. These words were important to have in documents for them to be considered rele vant, but the indi vidual' s name and the description of the crime were far more im-portant (by a factor of almost 19 to 1).

The more hea vily weighted part of the query , then, was a  X justice event X  mark er found using in-formation extraction (Section 3.2) and the more de-tailed description of that event based on phrases ex-tracted from the crime (here crimes against human-ity ). Those phrases give more probability of rele-vance to documents that use more terms from the crime. It also included a location constraint (here, Iraq ) that boosted documents referring to that lo-cation. And it captured user -pro vided equi valent words such as Saddam Hussein being a synon ym for former President of Iraq .

The most comple x part of the query handled ref-erences to the indi vidual. The extraction system had annotated all person names throughout the corpus. We used the IR system to inde x all names across all documents and used Indri to retrie ve any name forms that matched the indi vidual. As a result, we were able to nd references to Saddam , Hussein , and so on. This task could have also been accom-plished with cross-document coreference technol-ogy but our approach appeared to compensate for incorrectly translated names slightly better than the coreference system we had available at the time. For example, Present rust Hussein was one odd form that was matched by our simple approach.

The nal query look ed lik e the follo wing: The actual query is much longer because it con-tains 100 possible entities and numerous sources. The processing is described in more detail else-where (K umaran and Allan, 2007). 3.2 Inf ormation Extraction The Proteus system produces the full range of anno-tations as specied for the ACE 2005 evaluation, in-cluding entities, values, time expressions, relations, and events. We focus here on the two annotations, entities and events, most rele vant to our question-answering task. The general performance on entity and event detection in news articles is within a few percentage points of the top-ranking systems from the evaluation.

The extraction engine identies seven semantic classes of entities mentioned in a document, of which the most frequent are persons, organizations, and GPE' s (geo-political entities  X  roughly , regions with a government). Each entity will have one or more mentions in the document; these mentions in-clude names, nouns and noun phrases, and pro-nouns. Text processing begins with an HMM-based named entity tagger , which identies and classies the names in the document. Nominal and pronomi-nal mentions are identied either with a chunk er or a full Penn-T reebank parser . A rule-based coref-erence component identies coreference relations, forming entities from the mentions. Finally , a se-mantic classier assigns a class to each entity based on the type of the rst named mention (if the entity includes a named mention) or the head of the rst nominal mention (using statistics gathered from the ACE training corpus).

The ACE annotation guidelines specify 33 dif fer -ent event subtypes, organized into 8 major types. One of the major types is justice events, which in-clude arrest, char ge, trial, appeal, acquit, con vict, sentence, ne, execute, release, pardon, sue, and ex-tradite subtypes. In parallel to entities, the event tagger rst identies indi vidual event mentions and then uses event coreference to form events. For the ACE evaluation, an annotated corpus of approxi-mately 300,000 words is used to train the event tag-ger .

For each event mention in the corpus, we collect the trigger word (the main word indicating the event) and a pattern recording the path from the trigger to each event argument. These paths are recorded in two forms: as the sequence of heads of maxi-mal constituents between the trigger and the argu-ment, and as the sequence of predicate-ar gument re-lations connecting the trigger to the argument 4 . In addition, a set of maximum-entrop y classiers are trained: to distinguish events from non-e vents, to classify events by type and subtype, to distinguish arguments from non-ar guments, and to classify ar-guments by argument role. In tagging new data, we rst match the conte xt of each instance of a trig-ger word against the collected patterns, thus iden-tifying some arguments. The argument classier is then used to collect additional arguments within the sentence. Finally , the event classier (which uses the proposed arguments as features) is used to re-ject unlik ely events. The patterns pro vide some what more precise matching, while the argument classi-ers impro ve recall, yielding a tagger with better performance than either strate gy separately . 3.3 Answer Generation Once the nal batch of documents is recei ved, the answer generator module selects candidate pas-sages. The names, with alternate renderings, are lo-cated through the entity mentions by the IE system. All sentences that contain a justice event and that fall within a mention of a tar get by no more than n sentences, where n is a settable parameter , which was put at 5 for this evaluation, form the core of the system' s answer .

The tactic tak es the place of topic segmentation, which we used for other question types in GALE that did not have the benet of the sophisticated event recognition offered by the IE system. Segmen-tation is used to give users suf cient conte xt in the answer without needing a means of identifying dif-cult denite nominal resolution cases that are not handled by extraction.

In order to increase recall, in keeping with the need for a comprehensi ve answer in the GALE eval-uation, we added sentences that contain the name of the tar get in documents that have justice events and sentences that contain words describing the crime. Ho we ver, we imposed a limitation on the gro wth of the answer size. When the tar get indi vidual is well-kno wn, he or she will be mentioned in numerous conte xts, reducing the lik elihood that this additional mention will be rele vant. Thus, when the size of the answer gre w too rapidly , we stopped including these additional sentences, and produced sentences only from the justice events. The threshold for triggering this shift was 200 sentences. 3.4 Summarization As a state-of-the-art baseline, we used a generic multidocument summarization system that has been tested in numerous conte xts. It is, indeed, the backup answer generator for several question types, including the prosecution questions, in our GALE system, and has been been tested in the topic-based tasks of the 2005 and 2006 Document Understand-ing Conferences.

A topic statement is formed by collapsing the template arguments into one list, e.g.,  X saddam hus-sein crimes against humanity prosecution X , and the answer generation module proceeds by using a hy-brid approach that combines top-do wn strate gies based on syntactic patterns, alongside a suite of summarization methods which guide content in a bottom-up manner that clusters and combines the candidate sentences (Blair -Goldensohn and McK e-own, 2006). The results of our evaluation are sho wn in Table 1. We increased the number of test questions over the number used in the ofcial GALE evaluation and we used only pre viously unseen questions. Documents for the baseline system were selected without use of the event annotations from Proteus.

We paired the 25 questions for judges, so that both the system' s answer and the baseline answer were assigned to the same person. We pro vided explicit instructions on the handling on implicit references, allo wing the judges to use the conte xt of the ques-tion and other answer sentences to determine if a sentence was rele vant  X  follo wing the practice of the GALE evaluation.

Our judges were randomly assigned questions and ask ed whether the snippets, which in our case were indi vidual sentences, were rele vant or not; the y could respond Rele vant , Not Rele vant or Don X  t Know . In cases where references were unclear , the judges were ask ed to choose Don X  t Know and these were remo ved from the scoring. 5
Our system using IE event detection and en-tity tracking outperformed the summarization-based baseline, with average precision of 68% compared with 57%. Moreo ver, the specialized system sus-tained that level of precision although it returned a much lar ger number of snippets, totaling 2,086 over the 25 questions, compared with 363 for the base-line system. We computed a relati ve recall score, us-ing the union of the sentences found by the systems and judged rele vant as the ground truth. For recall, the specialized system scored an average 89% ver-sus 17% for the baseline system. Computing an F-measure weighting precision and recall equally , the specialized system outperformed the baseline sys-tem 75% to 23%. The dif ference in relati ve recall and F-measure are both statisticaly signicant under a two-tailed, paired t-test, with Our results sho w that the specialized system statis-tically outperforms the baseline, a well-tested query focused summarization approach, on precision. The specialized system produced a much lar ger answer on average (Table 1). Moreo ver, our answer gener -ator seemed to adapt well to information in the cor -pus. Of the six cases where it returned fewer than 10 sentences, the baseline found no additional sen-tences four times (Questions B006, B011, B015 and B022). We regard this as an important property in the question-answering task.

A major challenge is to ascertain whether the mention of the tar get is indeed involv ed in the rec-ognized justice event. Our event recognition system was developed within the ACE program and only seeks to assigns roles within the local conte xt of a single sentence. We currently use a threshold to con-sider whether an entity mention is reliable, but we will experiment with ways to measure the lik elihood that a particular sentence is about the prosecution or some other issue. We are planning to obtain vari-ous pieces of information from additional secondary queries to the search engine. Within the GALE pro-gram, we are limited to the dened corpus, but in the general case, we could add more varied resources.
In addition, we are working to produce answers using text generation, to bring more sophisticated summarization techniques to mak e a better presen-tation than an unordered list of sentences.
Finally , we will look into applying the techniques used here on other topics. The rst test would rea-sonably be Conflict events, for which the ACE pro-gram has training data. But ultimately , we would lik e to adapt our system to arbitrary topic areas. This material is based in part upon work supported by the Defense Adv anced Research Projects Agenc y (D ARP A) under Contract No. HR0011-06-C-0023.
An y opinions, ndings and conclusions or recom-mendations expressed in this material are those of the authors and do not necessarily reect the vie ws of the Defense Adv anced Research Projects Agenc y (D ARP A).

