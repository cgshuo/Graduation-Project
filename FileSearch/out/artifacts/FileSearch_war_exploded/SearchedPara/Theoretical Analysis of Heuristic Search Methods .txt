 sross12@cs.mcgill.ca Partially Observable Markov Decision Processes (POMDPs) p rovide a powerful model for sequen-is too large to enumerate. A number of compression technique s have been proposed, which han-finite planning horizon. However despite good empirical per formance, both classes of approaches lack theoretical guarantees on the approximation. So it wou ld seem we are constrained to either solving small to mid-size problems (near-)optimally, or so lving large problems possibly badly. This paper suggests otherwise, arguing that by combining of fline and online techniques, we can previous work [11], we introduced an anytime algorithm for P OMDPs which aims to reduce the near-optimal solutions within a smaller overall time than p revious online methods. R : S  X  A  X  R is the reward function, O :  X   X  A  X  S  X  [0 , 1] is the observation function, belief update function b  X  =  X  ( b, a, o ) , defined as b  X  ( s  X  ) =  X O ( o, a, s  X  ) P  X  is a normalization constant ensuring P Solving a POMDP consists in finding an optimal policy,  X   X  :  X  S  X  A , which specifies the best optimal value function is defined as V  X  ( b ) = max be computed according to P ( o | b, a ) = P b and then follow the optimal policy Q  X  ( b, a ) = R ( b, a ) +  X  P this, we can define the optimal policy  X   X  ( b ) = argmax While any POMDP problem has infinitely many belief states, it h as been shown that the optimal value function of a finite-horizon POMDP is piecewise linear and convex. Thus we can define the their location in the space of beliefs. Contrary to offline approaches, which compute a complete pol icy determining an action for every lookahead search algorithm can compute this value in two sim ple steps.
 in general the belief MDP could have a graph structure with cy cles. Our algorithm simply handle bounds can be propagated to parent nodes according to: where U b in the tree T , U U ( b ) are the bounds on fringe nodes, typically computed offline.
 as a criteria to guide the search over the reachable beliefs. In this section, we review the Anytime Error Minimization Se arch (AEMS) algorithm we had first heuristics are guaranteed to yield  X  -optimal solutions.
 e error at a fringe node b  X  X  ( T ) ; (iv) h b 0 ,b the root b actions); and (vi) P ( h | b the action/observation sequence h if we follow the optimal policy  X   X  from the root node b optimal policy chooses action a in belief b ).
 Theorem 1. In any tree T , e Proof. Consider an arbitrary parent node b in tree T and let X  X  denote  X  a T 4.1 Search Heuristics possible, we should expand fringe nodes reached by the optim al policy  X   X  that maximize the term fringe node b . Clearly,  X  e ( b )  X  e ( b ) since U ( b )  X  V  X  ( b ) .
 To approximate P ( h b 0 ,b is optimal in belief b . Thus, we consider an approximate policy  X   X  bility that action a is optimal in belief state b given the bounds L have on Q  X  ( b, a ) in tree T . More precisely, to compute  X   X  random variable and make some assumptions about its underly ing probability distribution. Once cumulative distribution functions F b,a density functions f b,a  X   X  ( b, a ) = P ( Q  X  ( b, a  X  )  X  Q  X  ( b, a )  X  a  X  6 = a ) = integral may not be computationally efficient depending on h ow we define the functions f b,a consider two approximations.
  X   X  ( b, a ) = given bounds L variables with uniform distributions between their respec tive lower and upper bounds, we get: where  X  is a normalization constant such that P is 0 outside the interval between the lower and upper bound, t hen  X   X  actions, thus they are implicitly pruned from the search tre e by this method. A second practical approximation is: bound in b Using either of these two approximations for  X   X  a fringe node b on the value of root belief b as defined in Equation 5, and AEMS2 4 to denote the heuristic that uses  X   X  4.2 Algorithm near-optimal action within a finite allowed online planning time, the algorithm accepts two input function.
 Algorithm 1 AEMS: Anytime Error Minimization Search The E XPAND function expands the tree one level under the node b  X  by adding the next action and belief nodes to the tree T and computing their lower and upper bounds according to Equa tions 1-4. After a node is expanded, the U PDATE A NCESTORS function simply recomputes the bounds of in Section 2. It also recomputes the probabilities  X   X  the U PDATE A NCESTORS function without adding more complexity, such that when thi s function this new root can be reused at the next time step. 4.3 Completeness and Optimality We now provide some sufficient conditions under which our heu ristic search is guaranteed to con-main theorems, we provide some useful preliminary lemmas.
 Lemma 1. In any tree T , the approximate error contribution  X  e d is bounded by  X  e For the following lemma and theorem, we will denote P ( h probability of observing the sequence of observations h given that the sequence of actions h the set of all fringe nodes in T such that P ( h b 0 ,b the set of fringe nodes reached by a sequence of actions in whi ch each action maximizes U in its respective belief state.) Lemma 2. For any tree T ,  X  &gt; 0 , and D such that  X  D sup Proof. Let X  X  denote  X  a T Theorem 2. For any tree T and  X  &gt; 0 , if  X   X  b = argmax a  X  A U T ( b, a ) heuristics for Algorithm 1; the main sufficient condition be ing that  X   X  AEMS1 and AEMS2, are admissible. The following corollaries prove this: Corollary 1. Algorithm 1, using e b ( T ) , with  X   X  Proof. Immediate by Theorem 2 and the fact that  X   X  Corollary 2. Algorithm 1, using e b ( T ) , with  X   X  Proof. We first notice that ( U L
T ( b, a ) fore U ( | A | (sup b  X  e ( b )) 2 )  X  1  X  2 &gt; 0 . Hence, corrolary follows from Theorem 2. of other online approaches. The algorithm is evaluated in th ree large POMDP environments: Tag [1], RockSample [3] and FieldVisionRockSample (FVRS) [11] ; all are implemented using a factored and the FIB algorithm [15] to get an upper bound. We then compa re performance of Algorithm 1 with both heuristics (AEMS1 and AEMS2) to the performance ac hieved by other online approaches (Satia [7], BI-POMDP [8], RTBSS [10]). For all approaches we impose a real-time constraint of ( &lt; 1 s means the algorithm found an  X  -optimal action) 8 . Satia, BI-POMDP, AEMS1 and AEMS2 RAM; but the processes were limited to use a max of 1Gb of RAM.
 these results, AEMS2 provides the best average return, aver age error bound reduction and average lower bound improvement in all considered environments. Th e higher error bound reduction and lower bound improvement obtained by AEMS2 indicates that it can guarantee performance closer to the optimal. We can also observe that AEMS2 has the best ave rage reuse percentage, which indicates that AEMS2 is able to guide the search toward the mo st probable nodes and allows it to the considered environments.
 in Figure 2, showing that the bounds converge much more quick ly for the AEMS2 heuristic. To this end, we described a general online search framework, and examined two admissible heuris-tween the bounds (Heuristic AEMS1), the second favors an opt imistic point of view, and assume shows that AEMS1 and AEMS2 are admissible and lead to complet e and  X  -optimal algorithms. Our heuristics are most appropriate. Figure 1: Comparison of different online search al-This research was supported by the Natural Sciences and Engi neering Research Council of Canada
