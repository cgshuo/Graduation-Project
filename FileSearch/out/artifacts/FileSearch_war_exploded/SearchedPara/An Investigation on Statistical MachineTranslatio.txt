 Language model is the key component of SMT system as it ensures the fluency of the translation output. For a long period, n -gram LMs, which model over discrete representations of words have been the dominant form. However data sparsity always be an obstacle of such model as the size of training data cannot meet the increasing of parameters under the one-hot representation.
 tions, in which each word is represented as a dense vector in a low-dimensional feature space(compared to conventional one-hot representation), modelling over which they propose a feed-forward NPLM. During training, the NPLM learns both a distributed representation for each word in the vocabulary and a proba-bility distribution for n -grams over words representations. Furthermore, Mikolov et al. [12] introduced recurrent structure into neural network that can capture all previous words as context. Further experiments show that NPLMs can rival or even surpass traditional n -gram LMs [14, 11] evaluated in terms of perplexity. porated into SMT system for direct decoding and outperform both baseline and simply re-ranking. There are some questions remained. Firstly, in both works only un-normalized NPLM score are used during decoding. Although such usage could bring improvements, there is no detailed comparison on normalized and un-normalized NPLM for SMT decoding; secondly, current NPLM only train on part of words that occur in corpus and the rest words are simply truncated as &lt; unk &gt; ; finally, although NPLMs can improve SMT as supplement, it would be interesting to see whether NPLMs can replace conventional n -gram LMs for SMT or other NLP tasks.
 normalized NPLM performs same as normalized for SMT, as context score turns to be constant, which means the cost can be heavily reduced during decoding. Then we extend current NPLMs with a simple strategy, i.e., we use word cluster for truncated words during NPLM training, which could further improve MT performance by another 0.2 Bleu on average. Finally, we compare our mod-el with conventional n-gram LM and recurrent neural network(RNNLM) [10]. Results show that traditional n -gram LMs outperform current NPLMs, which means that they still cannot be simply replaced.
 state-of-art NPLMs with feedforward and RNN as examples. Then we present the implementation details on incorporating NPLMs to SMT systems in section 3. Experimental settings are reported in section 4. Then from 5 to 7 we discuss three questions separately. We conclude our work and present future work in section 9. In this section, we briefly review current NPLM models. The basic idea behind NPLMs is to predict probability of word w given context u . There are many types of NPLMs, e.g., feedforward [2], log-bilinear [13], recurrent neural net-work(RNN for short) [12]. One nice property of NPLMs is that for whatever n -gram input, the model can assign a non-zero score so that no smoothing or back-off is required. In this work, we mainly focus on two typical NPLMs, i.e., feedforward and recurrent.
 whose architecture could be seen on figure 2. Let n be the order of the language model; let u range over contexts, i.e., strings of length ( n  X  1), and w range over words. The embeddings of u is concatenated at input layer and then mapped to output layer through project layer. Two hidden layers can learn non-linear knowledge. Meanwhile, the parameter complexity is O ( | V | + | H | X  n ), while it is O ( | V | n ) for n -gram LM.
 calculation caused by repeated summations throughout whole vocabulary under standard maximum likelihood estimation (MLE) in NPLM training. Vaswani et al. [24] combined strategies including rectifier linear units [16], noise contrastive estimation [8] and mini-batch learning for fast training.
 layer is composed of a vector w ( t ) that represents current word w t and of vector s ( t  X  1) that represents hidden layer from previous step. After training, the output layer represents P ( w t +1 | w t ,s ( t  X  1)). As RNNLM also faces heavy cost of training as feedforward. Mikolov et al. [12] decomposed P ( w t +1 | w t ,s ( t  X  1)) O ( | V | ) to O ( p | V | ). In order to incorporate neural LMs into MT system, we first rerank k -best lists with NPLMs following previous work. As feedforward NPLM scores n -grams, it can also be integrated into decoder just as conventional n -gram models. Different from conventional n -gram LM, for which only performs search for each input query, each time matrix operation is done for NPLMs, which makes caching a necessary. During decoding, we maintain a dictionary for each thread to store the n-grams that have been calculated. In this section, we first list the detailed setting for whole experiment. The training data came from the NIST 2012 Chinese-English evaluation task with sentences shorter than 60 words. Phrases were extracted from all training data, while rules with nonterminals were extracted from only the FBIS corpus (LDC2003E14). We ran MERT on the NIST 2003 test data(MT03), while performance is tested from the NIST 2004 to 2008(referred as MT04, MT05, MT06, MT08). Align-ments were extract with GIZA++ [19] with refinement from both directions. An in-house hierarchical phrase-based SMT system [5] was used to report main experimental results.
 ified Kneser-Ney smoothing [4] on the English side of the bitext and the 329M-word Xinhua portion of English Gigaword (LDC2011T07). Against these base-lines, we tested systems that included the two conventional LMs as well as two 5-gram NPLMs trained on the same corpus. We trained both the log-linear mod-els and the discriminative rerankers on 1000-best lists with MERT [18]. In order to make the results stable[6], we ran MERT three times and reported the average score. The results were evaluated under case-insensitive NIST Bleu .
 ting in [24], all ff NPLMs had a vocabulary size of 100k by default while all digits are converted to 0 for training of NPLMs. The NPLMs used dimension size 150 for input and output embeddings, 750 units in hidden layer and 150 units in hidden layer. Besides, model was optimized by 10 epochs of stochastic gradient ascent(SGD) with mini-batches of size 1000 and an initial learning rate of 1. The number of noise samples was set 100 per training example. Meanwhile the RNNLMs 2 followed same setting for data as feedforward, with 150 dimensions for both word embeddings and hidden layer. Besides, 2 million direct connections under 4-gram was set for RNNLM. NPLMs, we run SMT experiments under both types of NPLMs, together with baseline setting that only uses two n -gram LMs. The detailed results could be found in table 1. We see that both re-ranking improves around 0.4 Bleu , while direct decoding can get better results up to over 0.85 Bleu . Besides, normalized NPLMs can achieve only slightly better performance(less than 0.05 Bleu on average) on re-ranking task, but un-normalized NLPMs outperform on decod-ing(also less than 0.05 Bleu on average). Obviously, the difference is so tiny that we believe we can use unnormalized NPLM score. Besides, we also collected the decoding time on different datasets presented on table 2. We observed that decoding with normalized NPLM is on average almost 155 times slower than baseline, and 17.1 times slower than with un-normalized NPLMs. The perfor-mance difference between normalized and unnormalized NPLMs for SMT may be the context score is a constant. In order to further verify this conclusion, we check the distribution of the context 4-grams for 1000-best output of MT03 using NPLM trained on Gigaword Xinhua portion, with the histogram plotted in figure 3. We could find that it obeys sharp normal distribution with  X  -0.0717 and  X  0.3106, which means that the unnormalized NPLM score could be ap-proximately equal to normalized score. Finally we could draw conclusion based on our experiments that unnormalized feedforward NPLM is sufficient for SMT, which is also consistent with the finding of [15].
 with decomposition, we want to see whether normalization term can be simply thrown away. As RNNLM cannot be incorporated for direct decoding for hierar-chical phrase-based system, we check this idea with reranking only. Similarly, in table 2 reranking with unormalized RNNLMs is three times faster than normal-ized, but with a heavy loss on Bleu performance(see in table 1). For feedforward NPLM, the normalized term is summed over whole vocabulary, while RNNLM sums over only subset of vocabulary for RNNLM. The histogram of normalized term can be found in figure 4. As we know the vocabulary size on large training set is usually quite big, e.g., the vocabulary size is 823.6k for Xinhua portion of Gigaword and 286.3k for target side of training data 3 . As a result, it is impractical to train NPLMs on whole vocabulary. Mentioned in section 4, the size was set 100k by default. We want to verify the effect of vocabulary size on Bleu performance. Without loss of generality, we trained both NPLMs on vocabulary size under 12.5k, 25k, 50k, 100k and 200k on both Gigaword Xinhua and target side of training data. The detailed Bleu results are shown in table 4. We can see that performance steadily improves as vocabulary size increases, while it drops when size turns from 100k to 200k. This means that larger vocabulary size does not mean better MT performance, as NPLMs cannot get precise estimation for low-frequency words. In contrast, things are different on n -gram LMs. In details, we trained two n -gram LMs with vocabulary 100k and got Bleu results of both tuning and test sets, which is shown on second row of table 4. We find that what is opposite to NPLMs, full-vocabulary n -gram LMs brings better performance than restricted vocabulary for SMT. Such results show that NPLM cannot obtain good estimate for low-frequency words.
 by some special like &lt; unk &gt; . Therefore there is no difference between two words A and B given same context if they both belong to OOVs, which means the missing of lot of information. Inspired by [26], who improved SMT with word clusters for both n -gram LMs and TMs, we also adopted word classes trained with mkcls 4 for OOVs. In details, each word not in word list is replaced with unk+word-class-label, while no changes for word within the list. With Fixing the vocabulary size as 100k, we trained different clusters-augmented NPLMs with numbers ranged from 250, 500, 1000 to 2000. The final MT results which are illustrated in table 5 show of NPLM augmented with word clusters can further achieve improvement than decoding with normal NPLMs by nearly 0.3 Bleu when cluster number is larger than 500. The biggest improvement on test dataset(MT05) could reach as high as 1.5 Bleu , which demonstrates that direct decoding with NPLMs plus word clusters achieves higher performance. As direct decoding with NPLMs can bring significant improvements, one impor-tant issue is that whether the NPLMs can replace conventional n -gram language model, which was concerned in[20]. Actually, such comparison has been conduct-ed in terms of perplexity, one latest results could be found in [24]. In following sections, we will discuss this question throughout detailed experiments: tioned in section 3, we only compare feedforward NPLM and n -gram. Table 6 shows the performance of decoding only with n -gram LM and NPLM respec-tively. Overall speaking, decoding with nplm only is averaged 1.3 Bleu lower than n -gram lms, while feedforward NPLMs augmented with word clusters can achieve better performance, with only about 1.0 Bleu point lower on average. Besides, compared with decoding under n -gram LMs vocabulary size 100k, such gap narrows to 0.9 Bleu and 0.6 Bleu separately. Here we can see that feed-forward NPLMs still cannot replace n -gram LMs. feedforward and RNN LMs on a re-ranking task. We first removed language model features of 1000-best outputs from the baseline system. Then different combinations of LMs were appended to output results for re-ranking. Detailed results can be seen on table 7. We may see that when adding one type LM features, similar performance was achieved on feedforward and ngram LMs, but much lower on RNNLM. Similar result can be observed when combing two LMs. Finally the best results can be achieved using all LM features, with a 0.5 Bleu improvement against baseline. 7.1 Comparison of LMs In this section, we investigate the possible reason for the difference of perfor-mance three types LMs. We focus on the power of models in terms of parameter space, which is listed in table 8 given same training corpus and vocabulary size. All probability information is hard-encoded in n -gram LMs 5 . In contrast, words are mapped to low-dimensional representations via both input and out-put embeddings. For RNNLM, the context is composed only one previous word and one hidden vector that contains all previous information(represented as one 150  X  150 matrix). Meanwhile, in addition to both input and output embeddings, feedforward NPLM also comprises two hidden layers(one 600  X  750 matrix and one 750  X  150 matrix). We believe the difference of parameters may cause the low performance of NPLMs on SMT. Neural network language models have attracted widespread attentions in recent years due to its property to overcome curse of dimension through learning a low-dimensional distributed word representation. Currently there are two types of NPLMs, i.e., feedforward [2] and RNN [12].
 Schwenk et al. [21, 22] applied feedforward and while Auli et al. [1] and Mikolov [10] investigated RNNLMs. Furthermore Le et al. [9] compared both NPLMs for MT. However, their usage in MT has largely been limited to reranking k -best lists for MT tasks with restricted vocabularies, which could not investigate the role of NPLM for MT in details. Niehues et al. [17] integrated a RBM-based LM directly into a decoder for the first time. The work of Vaswani et al [24] is most related to our work, as they adopted NPLMs for direct decoding on a large-scale MT task, while we try to cover questions that are not covered in their work and further improve the performance.
 model [15]. Later on both Niehues et al. [17] and Vaswani et al. [24] adopted unnormalized score for direct decoding, however they did not further investigate normalized score for MT and for other NPLMs(e.g., RNNLM), which were both discussed in this paper.
 Weubker et al. [26] improved MT with class-based n-gram LMs. Besides, Wu et al. [25] demonstrated that factored RNNLM with knowledge like POS and stem could outperform conventional RNNLM. However, as words can have more than one POS tag, which need to be inferred for the words in translation rules. Instead we used word cluster for truncated words to train NPLMs and also get further improvements.
 Sundermeyer et al. [23] compared feedforward with RNN LMs on speech recog-nition task. However there is no systematic comparison of neural models against n -gram LM, which we believe is an important question for MT research. In this work, we discuss several questions that exist in the application of N-PLMs for SMT system. First of all, through detailed experiments we show that un-normalized feedforward NPLM is equivalent as normalized for decoding and reranking, while removing softmax can reduce the decoding time to 1/10. How-ever, unnormalized RNNLM cannot replace normalized ones as it uses decom-position of probability. We also show that simply increasing vocabulary size of feedforward NPLMs cannot improve MT performance. Instead, replacing OOVs with cluster of original word can make better estimation of NPLMs and bring another 0.2 Bleu improvement for direct MT decoding. Finally, experiments also show that for MT both types of NPLMs might not simply replace conven-tional n -gram LM. We hope our findings can benefit the research on NPLMs in SMT.
 more experiments with direct decoding with RNNLMs for phrase-based MT system [1], in which way the power of RNNLM can be directly tested; secondly, we will compare the NCE training used in NPLM with self-normalization in neural network joint model(NNJM) [7]; we also plan to incorporate linguistic and domain information into the neural models.
 This work is supported by the National Natural Science Foundation of China (No. 61223003), Specialized Research Fund for the Doctoral Program of Higher Education of China(No. 20110091110003) and Graduate Research and Inno-vation Projects in Jiangsu Province(No. CXZZ12 0058). Shujian Huang is the corresponding author.

