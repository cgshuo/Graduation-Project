 Can we effectively influence aggregate user behavior in a cluster based retrieval (CBR) system by tuning its parameters? This ques-tion combines parameter tuning with models of user behavior. To address this question, we propose an approach based on three com-ponents: user model, criterion metric, and sensitivity analysis. We then demonstrate this approach on one of the most frequently asked questions to designers and operators of CBR systems in enterprises: namely,  X  X uggest a value for k ." Both the users and the system de-sire a value that is likely to maximize user satisfaction, and sway them towards a cluster based examination of their retrieved result set (rather than prefer the original ranked retrieved list). Based on observed user behavior in CBR systems, we posit a two-stage user model. We isolate its core element, which is a  X  X uery coverage metric." We then perform an empirical sensitivity analysis of this metric. Our analysis reveals that this metric is, surprisingly, robust to changes in k (i.e., insensitive to k ) in a wide range around its de-facto value. We conclude that in cases where our model approx-imates user behavior, the system cannot substantially increase the chances of the user resorting to CBR by tuning k . This has prac-tical implications on the design and day-to-day operation of CBR systems. Similar analyses can be carried out for other parameters. Cluster Based Retrieval; Parameter Tuning; User Models
Consider an enterprise system offering a cluster based retrieval (CBR) interface. The user poses a query to the system. The sys-tem first retrieves a ranked list of results. It then also produces a clustered and reranked version of this retrieved list. Once the user begins interaction with the CBR system, they make choices at mul-tiple stages. The first choice is, of course, whether to proceed on their information gathering by inspecting the clustered list, or sim-ply return to the original ranked list. More choices await the user at later stages of the inspection.
 This leads us to the following problem statement:
The motivation for this question comes from the immense com-mercial potential 1 for such technologies. Designers and operators would like to know whether such systems can be  X  X uned" to yield more user satisfaction 2 .

Questions of parameter tuning arise at various stages, and with respect to various parameters, in the working of an interactive CBR system. Yet to our knowledge, they have not received research at-tention. This is in contrast to the enormous amount of literature on parameter tuning questions in other areas of IR such as document representation, classification, language modeling, etc. The funda-mental difference between these two types of questions is that the first incorporates a user model . Understanding the interaction of the user model with the CBR parameters is key to answering such a question.

In this work, we propose a broad framework for tackling such questions of parameter tuning for influencing user interactive be-havior. Our framework uses three components: an abstract user model for the choice between CBR and ranked list, a criterion met-ric that captures roughly what the user bases his decision upon, and a sensitivity analysis of the distribution of the criterion metric to the parameter in question. We then demonstrate our framework for a central question: the choice of the number of clusters k in the CBR system. We posit a two-stage model for observed user behavior in enterprise CBR. We analyze the sensitivity of the distribution of the criterion metric to k in a wide range around its rule of thumb value of  X  p n / 2 [7]. Our sensitivity analysis shows a surprising result: that the system is unlikely to be able to tune k to sway more users towards cluster based inspection of retrieved lists.

The contributions of this work are twofold. We (i) introduce the question of tuning of parameters to influence aggregate user behav-ior in CBR systems, and formulate a broad framework to tackle them, and (ii) instantiate this framework on a central question for CBR systems: the choice of k to maximize user satisfaction at the initial inspection of CBR list.
To our knowledge, there is no work that squarely focuses on the influence of CBR parameters on user behavior. However, our ques-tion is related to a vast body of work on users interacting with CBR. We first survey the work on clustering in search X  X hat being our setting X  X nd then clustering in browsing.

Earlier systems that used clustering for information retrieval did so to augment (nearest-neighbor) search in response to a query. These systems typically resorted to a static one-time clustering of CBR is part of ECM, projected to grow to $8B by 2017 (Ford and Sullivan).
More so, since, in our experience, deployment of such systems is relatively meager compared to their potential. the entire corpus. The actual clustering could be either flat or hi-erarchical. The results were mixed, and in some cases, studies showed that strategies such as those described above actually re-duced the performance of the search on standard metrics such as precision and recall. See [13] for a survey of early work on cluster-ing on search.

Search can also be carried out in an interactive setting: [4] shows that a user can quickly navigate to a set of relevant documents in this setting.

The use of clustering in information retrieval as a form of brows-ing , as opposed to search, was investigated by [2]. They called their method of browsing  X  X catter-gather." [10] discuss scatter-gather on a large text corpus. They conclude that scatter-gather is capable of providing a coherent conceptual image of such large corpora.
We now return to the context of search. [3] provide the first dis-cussion of scatter-gather on the results of a query; namely, they cluster the documents returned by a search engine as the results of a query, and then perform scatter-gather on it. The paper pro-vides anecdotal evidence of the efficacy of scatter-gather using a few examples. They also demonstrate that the distribution of rele-vant documents indeed tends to form clusters rather than follow a uniform distribution, thereby providing empirical evidence for the cluster hypothesis. See [11] for recent work on evaluating the clus-ter hypothesis on web documents.

Recently, CBR using language models [5] has gained much at-tention since it seems to affirm the benefits of CBR over document-based retrieval. This provides an avenue for extension of this work.
Lastly, we mention that cluster labelling itself is a vibrant re-search area: recent works in this field include [1, 9].

There is a vast body of statistical work on determining the num-ber of clusters for a dataset: this does not involve the user, and is not therefore, directly related to this work. A comprehensive sur-vey that captures the diverse methods is [8]. As stated in  X 1, there is an abundance of work on parameter tuning on other issues of rel-evance to IR. This is peripherally related to our work, but cannot be surveyed here due to space constraints.
Our approach to the question of parameter tuning to influence aggregate user behavior is as follows. A user interacts with the CBR system at various stages in the IR process. Fix a stage of interaction that is of interest. For this stage, we go through two steps.
 First step . We study the user interaction with the CBR system, and extract out two components. 1. A user model of the interaction. 2. A (quantitative) criterion metric upon which the user bases their decision-making in accordance with the user model. This met-ric will serve as a proxy for user satisfaction during the stage of the interaction under study.
 Second step . Here, we perform a sensitivity analysis of the metric with respect to the parameter in question. The goal of this sen-sitivity analysis is to inform us whether varying the value of the parameter is likely to increase or decrease the metric, and therefore the user satisfaction.

In  X 4 and  X 5 we describe the user model and criterion metrics for our task. In  X 6, we conduct an empirical sensitivity analysis.
The impetus for our work came from observations of how users interact with a CBR system in some commercially important ap-plications of clustering in IR, including eDiscovery, IT manage-ment, records management, compliance, and others. The majority of users of such systems are not technical experts in data mining or IR. They are generally application experts.

Now consider the situation where a user has posed a query to the CBR system. The system first retrieves a list of documents ranked using some standard ranking function. It also performs a clustering of this set of documents, and prepares a presentation of this clustering for the user. Clusters are frequently presented to the user by means of a digest of terms. The digest serves as a proxy for the topics that the cluster speaks about. They give the user a high-level view of the contents of the cluster, from which they may determine importance and relevance of the cluster to their query. Figure 1: User model for preliminary inspection of the clustering.
The user now has to decide whether to proceed by inspecting the original ranked list, or the cluster based presentation.

We observed that users, faced with the choice of whether to pro-ceed with CBR, or return to their original ranked retrieval list, fre-quently perform a  X  X reliminary inspection" of the clustering. This inspection itself can be roughly broken into two stages. 1. The user inspects the cluster digests and tries to understand the contents of the cluster through them. 2. The user then evaluates a criterion of the form  X  X re the terms of my query adequately represented in the cluster digests to warrant detailed examination of this clustering?" If the clustering passes the criterion, the user proceeds along CBR. They may then open each cluster (either by the ranking the system has assigned to the clusters, or a heuristic ranking they may have given based on query coverage of each cluster). On the other hand, if the clustering fails the criterion, the user returns to the ranked list, and proceeds with standard (non-clustered) retrieval. See Fig. 1. Figure 2: The coverage of a query by digests. In this toy example, the coverage of this particular query is 2 / 3.
 At this stage, we may state our research question more formally. Please also see Fig. 2.
 Research Question. Let Q be a set of queries. For each query Q  X  Q (where Q = ( q 1 ,..., q | Q | ) ), let the following be done. Let Q be issued to an IR system. Let the set of results R ( Q ) be subjected to a k -way clustering C = ( C 1 ,..., C k ) . Compute digests D of length ` for each cluster by taking the top-` terms, ranked by information gain with respect to cluster membership 3 . Let a criterion metric that models the second stage of the preliminary inspection described previously. How does the distribution of vary as we vary k in a range around its  X  X ule of thumb" value of  X  p n / 2?
In addition to modeling user behavior, the above seems to be an interesting structural question. Note that k is a parameter in this question. Therefore, we would expect the answer to this question to depend (perhaps strongly) upon the value of k .
We compute cluster digests in the standard manner as described in [6]. Namely, rank the terms in the cluster with respect to their information gain with respect to cluster membership, and present the user with the top-` terms as a digest.
In order to address our research question, we need to specify the criterion metric(s) P ` k . We do so now. The setting and notation will be as in the research question in  X 4. In this section, we define two measures of the coverage of query terms in a clustering of the retrieved results of a query. The first measures what proportion of query terms occur in digests. The second measures how many times query terms occur in digests.
Our user model posits that the coverage of the query by the clus-ter digests combined is a criterion metric.
 Definition 1. N ` k ( Q ) is defined as the number of terms in Q that appear in at least one of the digests { D ` ( C i ) : 1  X  i  X  k } . Definition 2. We define the query term coverage of Q , denoted by P ( Q ) , as P ` k ( Q ) : = N ` k ( Q ) | Q | .

P ` k ( Q ) measures how much of a query is covered by the digests of its clustered results. Clearly, 0  X  P ` k ( Q )  X  1.

Next, we need to ascertain the distribution of P ` k ( Q ) as Q ranges over the entire query set Q .
 Definition 3. The query term coverage profile of Q , denoted by k , is defined as the distribution We will also examine the effect of query size on the distribution k . To this end, let us define the average coverage for a specific query size.
The measure P ` k ( Q ) did not take into account the multiplicities of the occurrences of query terms in cluster digests. We now define a measure that does so.
 Definition 4. We define M ` k ( Q ) as the total number of times a term in Q appears in any of the digests D ` ( C i ) . Namely, where I [  X  ] is the indicator function.

Once more, we are interested in the distribution of M ` k ranges through Q .
 Definition 5. The query term multiplicity profile of Q , denoted by k , is defined as
Note that P ` k ( Q ) and M ` k ( Q ) are independent. For example, if only one query term in a query having many terms appears in mul-tiple digests, and no other term appears in any digest, then the query would score relatively high on M ` k ( Q ) but relatively low on P
We now conduct an empirical sensitivity analysis of the two cov-erage metrics P ` k and M ` k to the number of clusters k .
Datasets. For our study, we used datasets obtained from the topic distillation category of TREC 2003 and 2004. We used 125 queries X 50 queries from TREC 2003 (TD2003), and 75 from TREC 2004 (TD2004). The number of terms in these queries range from one to four. There were 21 queries with one term, 76 with two terms, 20 with three terms, and 8 with four terms. Thus, they broadly reflect query sizes in KDDCUP 2005 4 on query classifica-tion, where roughly 80% of the queries had size four or less. Corre-sponding to these 125 queries, there were 110,229 documents total http://www.sigkdd.org/kddcup (about 1.85G) in the top-1000 BM 25 lists. The top-1000 lists in-clude all the documents deemed relevant to the queries by experts, except in a few cases, where more documents are retrieved. This scale is fairly representative of the real-world enterprise informa-tion management applications that we work on.

Design. For each query Q in the set of queries Q defined in the datasets above, we clustered R ( Q ) into k clusters using the bisect-ing k -means algorithm. We chose this algorithm since it is known to produce among the highest quality of clusters of any document clustering algorithm [12]. The  X  X ule of thumb" value (  X  p for the number of clusters for such list lengths would be approx-imately k = 20, and so we experimented with k = 10 to k = 30; larger values would yield clusters that were too small for typical en-terprise applications. We also chose the upper value of ` to reflect real-world cluster digest sizes in enterprise-class CBR interfaces Therefore our parameter ranges were We computed P ` k ( Q ) and M ` k ( Q ) , averaging over three runs of each experiment. As Q ranges through Q , we get a clear picture of the resulting distribution of the coverage metrics over the range of the parameters above.
Fig. 3 shows the query term informativeness profiles. For space considerations, k = 5 and ` = 5 are not shown.

We have represented the distribution P ` k using histograms. The number of queries sums to 125 in each histogram. We have chosen five equal-width bins for our histogram, and the queries have terms varying from one to four. Therefore, we have: 1. The size of the [ 0 , 0 . 2 ] bar is equal to the number of queries 2. The size of the [ 0 . 4 , 1 ] bar is equal to the number of queries
Let us first understand how the distribution P ` k varies with ` using Fig. 3. Notice that, for all values of k , the size of the right-most bar (which indicates that all the query terms are in the digest) falls monotonically with increasing ` . Let us now focus on k = 10. We find that roughly half the queries have all their terms in digests of ` = 10. Naturally, this is also the most filled bin in the histogram. As we raise the bar to ` = 3, we find that the most filled bin shifts to [ 0 . 4 , 0 . 6 ] , which has about 40% of the queries. This means that a majority of queries now no longer have all their terms appearing in digests. Finally, at ` = 1, the largest bin consists of queries none of whose terms is the most informative term of a digest. Namely, with such short digests, most queries have no coverage at all.
We should note here that in most enterprise applications that we have encountered, ` = 10 is a good baseline for the size of digests.
Table 1 shows avg | Q | = t [ P ` k ] for various values of k and ` .
Fig. 4 shows the behavior of M ` k as a function of k and ` .
Now we come to our sensitivity analysis of the distribution to the parameter in question X  X he number of clusters k . Once again, we draw the reader X  X  attention to Fig. 3. This time, we inspect differences in the sets of four profiles (for four values of ` ) taken
We work on a market leading enterprise CBR platform supporting a variety of enterprise applications from a Fortune-10 IT company. Table 1: Effect of query size on the coverage profile. Values of avg | Q | = t [ P ` k ] are shown. together, as k is varied from k = 10 to k = 30. We find a stable pattern. The query term coverage profiles (for each value of ` ) are almost unaffected by the parameter k . The distribution of query term coverages does not display sensitivity to variations in k .
Next, we investigate the sensitivity of the query term coverage profile to k , for either of the four specific values of query size. Namely, we investigate whether avg | Q | = t [ P ` k ] is sensitive to k for t = 1 , 2 , 3 , 4 separately. We see that the insensitivity to k in Table 1 is independent of query size. For each query size, we see stabil-ity for higher values of ` , and small variations between k = 10 and k = 30 only at ` = 1. However, this last feature must take noise into account, since the actual numbers at ` = 1 are small.

Finally, we inspect Fig. 4. We find that, surprisingly, the multi-plicity profiles for lower values of ` = 1 , 2 , 3 , 5 are almost flat. Note that lower values of ` correspond to the most significant and infor-mative terms in the cluster digests. This tells us that as we increase the number of clusters k , the query terms do not occur more often in the most significant and informative terms in the digests.
Summary. We analyze three different views into query cover-age: frequency view using histograms, averages for query sizes, and multiplicities. With the empirical evidence at our disposal, we may now answer our research question of  X 4. We observe a sta-bility (i.e., lack of sensitivity) to variations in k displayed by the various criterion metrics P ` k that model user behavior. This stabil-ity also holds for each specific query size, and for multiplicities at significant positions in cluster digests.
We have initiated research on tuning parameters using user mod-els in a CBR system in order to influence aggregate user behavior. We formulated a generic method for tackling such questions. We then demonstrated a specific important and frequent instantiation of our research question: namely, how can we vary k so that more users, after a preliminary inspection, choose cluster based inspec-tion of a retrieved list. Our analysis leads to the conclusion that the system may not be able to satisfy more users/queries and compel them to resort to CBR over a simple retrieval ranked list only by varying k . This answers a question that is important to designers and operators of CBR systems.

Our work focuses on a first-stage examination of a CBR list. An-other tuning question relates to a multi-stage scatter-gather work-flow: what should be the number of clusters chosen at each stage? Note that our work ties in to this question too, since we can remove from consideration the variation to query coverage caused by dif-ferent choices of k at each stage. However, the challenge will be to model the user X  X  behavior across multiple stages.
 [1] David Carmel, Haggai Roitman, and Naama Zwerdling.
 [2] Douglass Cutting, Jan Pedersen, David Karger, and John [3] Marti A. Hearst and Jan O. Pedersen. Reexamining the [4] Anton Leuski. Evaluating document clustering for [5] Xiaoyong Liu and Bruce W. Croft. Cluster-based retrieval [6] Christopher D. Manning, Prabhakar Raghavan, and Hinrich [7] K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate [8] Glenn W Milligan and Martha C Cooper. An examination of [9] Markus Muhr, Roman Kern, and Michael Granitzer.
 [10] Peter Pirolli, Patricia Schank, Marti Hearst, and Christine [11] Fiana Raiber and Oren Kurland. Exploring the cluster [12] M. Steinbach, G. Karypis, and V. Kumar. A comparison of [13] P. Willett. Recent trends in hierarchic document clustering: a
