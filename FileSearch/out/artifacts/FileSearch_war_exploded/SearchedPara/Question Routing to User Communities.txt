 An online community consists of a group of users who share a common interest, background, or experience and their col-lective goal is to contribute towards the welfare of the com-munity members. Question answering is an important fea-ture that enables community members to exchange knowl-edge within the community boundary. The overwhelming number of communities necessitates the need for a good question routing strategy so that new questions gets routed to the appropriately focused community and thus get re-solved. In this paper, we consider the novel problem of routing questions to the right community and propose a framework to select the right set of communities for a ques-tion. We begin by using several prior proposed features for users and add some additional features, namely language attributes and inclination to respond, for community mod-eling. Then we introduce two k nearest neighbor based ag-gregation algorithms for computing community scores. We show how these scores can be combined to recommend com-munities and test the effectiveness of the recommendations over a large real world dataset.
 H.1.2 [ Information Systems ]: User/Machine Systems X  Human information processing ; H.3.3 [ Information Stor-age and Retrieval ]: Information Search and Retrieval X  Retrieval models Community Question routing; Group recommendation
An online community is a group of users who interact with one another through Internet technologies [10]. The members of the community generally share a common in-terest, background, or experience and their collective goal is to contribute towards the welfare of the community mem-bers. Similar to the growth of online question answering sites, such as Yahoo Answers , the popularity of question-answering within communities is on a rise. E.g., IBM has a dedicated internal online community portal called connec-tions that allows its employees to join communities, create new communities, and post questions and discussions within the communities. There are more than 150,000 communities in connections and question answering within these commu-nities play an essential role in enabling information flow and collaboration among the community members. However, the overwhelming number of communities necessitates the need for a good routing strategy so that a new question gets routed to the appropriately focused community and thus is resolved in a reasonable time frame. This would alleviate the burden of finding the right community from the askers.
While question routing to the appropriate answerer is an important concept, and prior studies [13, 6, 7, 3] show that it enhances the user experience, a largely ignored area is question routing to the appropriate communities. Routing questions to communities instead of individual users is more useful as it can increase the likelihood of the question be-ing answered. Also, it does not clog the bandwidth of any individual. Moreover, the collective knowledge of the com-munity is greater that of an individual.

One of the main challenges for community question rout-ing is the task of modeling community features. There is extensive research on modeling user behavior, but how do we aggregate that to capture community behavior? Simple aggregation techniques, such as, min , max , and mean do not work because they fail to take into account the fact that a community consists of a large number of knowledge seekers and only a small number of knowledge creators. A second challenge is how do we capture community norms and prac-tices? Adherence of the question to community norms can be critical. To see this, consider an example. Let x and y be members of  X  X ava X  community. Additionally, x is a member of  X  X ython X  community. For a python question which only x and y can answer, greedy strategy dictates routing it to  X  X ava X  community. However that question might not be ac-ceptable to other community members. Hence community norms can be an important attribute besides user interests.
In order to address these challenges, we build upon the prior state of art in topic modeling and expertise identifi-cation and propose knn based algorithms to aggregate user features to compute community scores. We also propose some novel metrics, such as language analysis of questions, and inclination to respond, to model community character-istics like community norms. We evaluate several models that combine community scores to produce a ranked list of communities over IBM connections dataset. Our framework can run in real time for large scale datasets.
To the best of our knowledge the problem of routing a question to a focused community has not been explored pre-viously. However, question routing to individual users has been recently explored [13, 6, 7, 3]. Zhou et al. [13] present a mechanism to find the top-k potential experts to answer a given question. They model the expertise based on users X  forum postings and through the interconnections between the users. Li et al. [7] present a routing approach in a pro-gramming language forum. They use semantic information from user posts along with the post-reply relation to find the experts. Li et al. [6] exploit the hierarchical category information in forums to find potential answerers. Cao et al. [3] formulate the problem of question routing as a tree cut problem on the question graph and present a minimum description length based approach for selecting the best cut.
A closely related problem to community question rout-ing is group recommendation [9, 1]. Group recommendation is a task of recommending items (such as games, movies) to a group of users instead of a single user. However the key difference is that for group recommendation, the recom-mended item must appeal to all group members. As a result, min , max , or mean based aggregation work well in practice for group recommendation [9], but for question routing, one must consider the fact that most of the community mem-bers are information seekers and not active contributors, so a community profile based on all its members does not work.
Another related problem is question classification which aims at putting the question into several semantic cate-gories [11]. This is done by building a category profile based on the prior categorized questions. However typically these categories are orthogonal and they do not share topical simi-larities, whereas communities do not follow such constraints. In fact in most cases, communities compete with one another in the topic space. Additionally, the notion of user mem-bership and their intra-community dynamics is not present while question categorization, but is pivotal for routing.
Let U = { u 1 ,u 2 ,... } represent the set of users and C = { c 1 ,c 2 ,... } be the set of communities, such that, u indicates that the user u i is a member of the community c A user can be member of multiple communities.

Problem 1 (Community Question Routing). Given a question q asked by a user v , find a community c that the following holds: where V alue ( q,c ) is an estimate of the quality of the answers generated by the members of the community c to question q , provided that the question is asked in community c . In this paper, we consider the problem of finding top k com-munities based on their value to a given question q . The next section presents metrics that are used to model the community characteristics.
 Table 1: Question features. Note that features Q 1, Q 2, and Q 3 are not present for the routed question.
There are three different types of entities that need to be modeled for our problem. The first type is the ques-tion which includes the routed question and the existing questions in the communities. The second type is the user which includes the question asker and the community mem-bers. Finally, the third type is the community where the interaction between the users occur. Next, we present fea-tures extracted for these entities.
 A question typically consists of a summary title and an op-tional description. We combine these two fields together to create a question document ( QD ). Based on QD , we extract several different features as listed in Table 1. Most of these features are self-explanatory.

We compute the questions X  topic distribution ( Q 5) through the Latent Dirichlet Allocation (LDA) [2] algorithm. To run LDA, we first combine all QD s within a community to cre-ate a community document CD . Then, LDA is run over CD s and once it estimates the topic-word proportions, we infer the topic distribution of an individual question. We run topic model over CD s instead of QD s directly because it leads to a more reliable estimation of topics due to two main facts: a) there are fewer communities (hundreds to thousands) in comparison to questions (tens or hundreds of thousands), and, b) CD are wordy (  X  7000 words) whereas QD are terse (  X  100 words). These factors make CD more robust to noise and thus a more reliable topic estimation.
We carried out a language analysis ( Q 6) of the question text using Linguistic Inquiry and Word count (LIWC) tool. LIWC provides scores on 80-90 features for the input text. The most prominent of those features are: a) usage of nega-tive (and positive) words, b) usage of singular pronouns, c) usage of bad words, d) usage of greetings, and e) usage of special characters (e.g. @, #, ?, !, etc). We also computed the score for spelling mistakes through a dictionary (English words + Technical jargon) and Jaro-Winkler similarity mea-sure. The language analysis enables us to test whether a question adheres to the community norms (e.g. censorship of some communities towards bad words, excessive person-alization of a post, bad-readability of a post due to lots of spelling mistakes, sms language, or special characters). Table 2 provides a complete list of the user features. In order to compute users X  topical expertise U 4, we consider a variant of z-score expertise model [12]. To estimate the topical expertise, we consider topical z-score as follows: where t is a topic, a ( t ) indicates the sum of topic t  X  X  com-ponent for all the questions answered by the user, and sim-ilarly q ( t ) represents the sum of topic t  X  X  component in all the questions asked by the user.

We estimate user availability ( U 5) by examining the most recent two weeks X  hourly activity of the users and assigning a probability proportional to the frequency of activity per hour. Typically users are active for a short period of time followed by a long period of passiveness, hence examining recent two weeks gives a more accurate estimation of their current activity levels.

Inclination to respond to a user ( U 6) is computed based on the strength of the shortest path between the two users in the QA graph. The QA graph [12] is a directed weighted graph generated by connecting the question asker ( x ) to the answerer ( y ), where edge weight indicates the number of questions of x answered by y . We define the inclination to respond as the minimum weight on the shortest directed path from the asker to potential answerer. If no path ex-ists then inclination is set to 0 . 5. The intuition behind this feature is that if a user u has replied to another user v in the past or if u connects to v with a high strength directed path, then u might like to reply to v in the future as well. Table 3 presents a list of community features. The topical distribution ( C 2) is computed using LDA over community documents ( CD s). We compute the community expertise, availability, inclination, and language analysis in a similar fashion as we computed for the individual users by aggre-gating all the activity within the community.
We combine the features presented in Table 1, 2, and 3 to construct several similarity metrics. These metrics are computed from the perspective of a routed question q . To compute the similarity of q with the existing questions in the communities, we use Kullback-Leibler divergence ( KL ) between the topic distributions Q 5 as, where x i and y i are the i th component of the probability distribution x , y . Low KL value indicates high similarity and high KL value indicates low similarity. We use topic similar questions were asked. However, we need to incor-porate another important factor: how well those questions were answered. If those questions were not properly an-swered, then it might be unproductive to route q to their communities. To incorporate this factor, we propose the no-tion of question value V al ( p ) which is estimated based on the number of people who viewed a question and the number of unique users who answered it.
 The topic match between the routed question q and an ex-isting question q 0 is then defined as: Similarly, we compute the match between the language fea-tures of the two questions. tween two vectors a and b . Note we use cosine similarity here instead of KL because language features need not be probability distributions.
 To compute user question similarity metrics, we first com-pute the familiarity ( F ) of the potential answerers ( u ) with the question asker ( v ). This factor is computed based on the hypothesis that users who share a lot of communities, have similar demographics, or have interacted with one-another previously will be more familiar with one another.

F ( v,u ) = log where  X , X , X  are the weight parameters and J ( a,b ) = | a  X  b | is the Jaccard index between two sets a and b . We consider three metrics for finding potential answerers. where u -exprt estimates the topical expertise match of the user to the routed question. where u -lang estimates the language match between the question answered by u 0 previously and the routed question. where u -avail estimates the availability of the user around the time of the post of the question. Algorithm 1 Global-knn ( k,n,q,O,C, M ) 1: Sort o  X  O in decreasing order of their M ( q, o ). Let the 3: Compute community score CS G ( c ) based on the number of 4: Return top n communities based on their CS G score. First, we estimate the topical match of q with community c . To do this, we use the topic distribution of the question and the community. Additionally, a community contains a brief description supplied by its owners. This description could be useful as it might hold clues to the topical interest of the community. We combine these two factors to compute topical similarity as follows: c -topic ( q,c ) = KL Q 5 q || C 2 c  X   X   X  Cosine ( Q 4 q where  X  is the relative importance of community description over KL match. Now similar to u -exprt we compute the expertise of the community towards answering q as follows: In a similar fashion, we compute c -lang and c -avail .
In order to aggregate individual metrics ( q - X  , u - X  ) to com-pute community scores, we present two k nearest neighbor ( knn ) algorithms. The intuition behind the knn algorithms is that min , max , and mean based aggregations do not cap-ture the true scores for a community. This is primarily due to the skew in activity levels of the community members. The two algorithms take as input the following parameters: number of recommendations n to produce, number of near-est neighbors k , a question q , a set of objects O , a set of communities C which contain the objects in O , and a simi-larity metric M between q and the objects in the set O . Global-knn. Algorithm 1 presents Global-knn algorithm. It first picks k objects with the largest similarity with the routed question. 1 Then it computes the score of each com-munity based on how many of its objects are in the top k . The n communities with the highest scores are returned. Local-knn. Algorithm 2 presents Local-knn algorithm. Un-like the previous approach, it first picks top k objects per community with the largest similarity with the routed ques-tion. Then the average similarity of the top k objects con-stitute that community X  X  score and the top n communities with the highest score are recommended. One issue with this algorithm is that it could be biased towards communi-ties with a number of objects less than k . To tackle this problem, we normalize the community scores as: CS L ( c ) = CS L ( c )  X  log {| O ? k ( c ) |} . Log-based normalization ensures that the communities with small number of objects are penalized Note that for  X  -topic metrics, smaller value is preferred. This is handled by multiplying it with  X  1 beforehand. Algorithm 2 Local-knn ( k,n,q,O,C, M ) 2: Sort objects in O ( c ) in decreasing order of their M ( q, o ) 4: Return top n communities based on their CS L score.
Using the two knn algorithms, we get several scores for a community corresponding to each similarity metric. Let ~ represent all such scores of a community c for a routed ques-tion q . Now the problem is to use ~ f to rank communities. To do this, we consider three mechanisms.
 The first mechanism is to use linear regression for ranking. We first construct a binary response variable ( y ) which is set to 1 for the desired community and 0 for the non-desired ones. The optimal weights w lr are estimated using the linear regression. The closed form solution is w lr = ( F T F )  X  1 where F = [ ~ f c 1 ,q ~ f c 2 ,q ... ] T is the design matrix and Y = [ y
The second mechanism is to generate a ranked list per score type and then merge those ranked lists. The general version of this problem is NP-hard but there are several greedy algorithms. We consider a simple yet popular Borda count algorithm [4]. The algorithm simply computes the ag-gregate rank of a community and sorts then on their aggre-gate rank. We consider a weighted version of the algorithm, in which weighted aggregate rank is computed. In order to learn the weights, we use an iterative reweighing scheme, where weight of one ranked list is estimated by fixing the weights of other ranked lists and so on. The weights that minimize the rank of desired communities is chosen.
The third mechanism is to cast it as a convex optimiza-tion problem with pairwise constraints. We use ranking SVM which is a popular algorithm for learning from ranked lists [5]. Let R q and S q be the set of desired and non-desired communities for question q . Then the optimal weights w svm are obtained through the following minimization,  X  q,  X  a  X  R q ,  X  b  X  S q : w T ( ~ f a,q  X  ~ f b,q ) &gt; 1  X   X   X  q,  X  a  X  R q ,  X  b  X  S q :  X  a,b,q  X  0 For final ranking, we sort communities in decreasing order of their w T  X   X  ~ f c,q value.
We experimentally evaluate the performance of our model on IBM Connections datasets, which consists of tens of thou-sands of communities specific to certain goals, such as social-ization, collaboration, and knowledge sharing. The breadth of topics in the communities varied from development and Table 4: Overall performance of the two knn algorithms over different similarity metrics. coding to finance and human resource to fun and outings. The community software allows community members to post questions and answers. By default the communities are marked as public, which allows non-members to read the community data. We randomly selected a set of 2 , 087 pub-lic communities and crawled all their data. The crawled data consists of 59,561 questions asked by 44,318 users.
The goal of our evaluation is two fold. First, we want to evaluate the performance of the two knn based aggre-gation algorithms with prior used aggregation techniques: min , max , and mean . Second, we want to compare the combined models with the individual models (e.g. based on best answerers) and explore the predictive power of different similarity metrics.

For model evaluation, we use 10 fold cross validation. In each fold, we consider 80% data for training, 10% for pa-rameter estimation (hold out), and 10% for testing. The parameters  X , X , X , X  are estimated empirically over the hold out data. For the test questions, task is to retrieve the com-munities where those questions were asked. Note that for each question, there is only one correct community. We also ensure that only the answered questions ( V al &gt; 0) are con-sidered for training and testing.

We consider two evaluation metrics for assessing the rout-ing algorithms. The first measure is precision at position N (P@N), which is 1 if the desired community is among the top N communities, otherwise 0. 2 The second measure is Mean Reciprocal Rank ( MRR ) which is the inverse of rank of the correct community averaged for all question queries. These two measures are widely used in the IR domain [8].
We begin with a systematic evaluation of the similarity metrics and the two knn algorithms. To run these experi-ments we set the number of nearest neighbor to 5 ( k = 5). We also set the number of topics to 150. Table 4 shows the performance of the knn algorithms. We note here that min and mean based aggregation, which are popular for group recommendation, perform badly for the routing task. The best P @5 amongst them is 0 . 55, achieved by mean strat-egy over u-exprt , while both the knn methods over the same
P@N is a variant of precision. It is more suitable here because there is only one correct community per question. Since there is only one correct community per question, MRR is more suitable than Mean Average Precision. metric perform significantly better. Additionally, we make several key observations from the table as follows: Fig. 1a plots P @ N for u- X  and q- X  similarity metrics. We observe an increase in precision by 70 X 150% from N = 1 to N = 20. The precision tapers off for large N . We also note that if a metric is better that another metric, it is consistently better for all N . This consistency indicates high reliability of these metrics.
Here we evaluate the effectiveness of the topic model to-wards model performance. Note that the first step in com-putation of the similarity metrics is the inference of the topic distribution of the questions. One of the key parame-ters required by LDA topic model is the number of topics. Fig. 1b shows the precision of Local -knn algorithm for sev-eral choices of number of topics ( # topics). It shows that an increase in # topics leads to an increase in the perfor-mance. This increase is large for small values (around 50%) and is small for large values (around 1-2%). Intuitively, a small value of # topics leads to under-fitting in the topic space, which in turn leads to increase in false positives. On the other hand, a large value of # topics could lead to over-fitting, which in turn might eliminate desirable communities from the candidate set. Additionally, a large # topics leads to more iterations of the LDA algorithm to converge. Over-all this result shows that the models can be sensitive to the choice of number of topics.
An important parameter required by the knn algorithms is the number of neighbors k . Fig. 1c shows their perfor-mance for different values of k . We note that k = 1 leads to a max based aggregation and k =  X  is mean based ag-gregation. Both these aggregation appear to be significantly worse in comparison to k = 5, highlighting that trivial ag-gregation mechanisms are ineffective.

We observe that the performance of Global -knn increases and then ultimately decrease. The increase makes sense because as k is increased the score of a topically relevant community with a lot of similar questions would increase. However as k becomes large the performance degrades as the algorithms get biased towards communities with a lot of questions irrespective of their similarity. (a) Performance of the Local -knn al-gorithm for different values of N. Table 5: Performance of the recommendation models. For ranking SVM , we set the parameter  X  = 1.
 We observe a similar trend for the Local -knn algorithm. However here the performance starts to drop for k &gt; 5. This is because as k is increased the algorithm gets biased towards communities with fewer questions. To solve this problem, we multiply the community scores with log(min { k, # object } ). Fig. 1c ( Local -knn  X  ) shows that the performance of log scal-ing remains steady unlike its unscaled counterpart.
Here we test the performance of the three combined mod-els and two baseline models (B1 and B2). The B1 model generates a random permutation of communities. The B2 model sorts communities in order of their decreasing sizes, so it is sensitive towards communities with lots of objects. Table 5 shows the performance of the models. We observe that the combined models perform significantly better than the models which route a question to the community of best individual to answer that question (Table 4, u-exprt ), using one sided t-test ( p  X  0). They also perform significantly bet-ter than the baselines, and the individual models (Table 4). Among the combined models, the ranking SVM performs significantly better, using one sided t-test ( p  X  0 . 01). In-tuitively this makes sense, because the pair-wise constraints lead to a direct optimization of P @1 accuracy measure.
Here we estimate the relative predictive power of different community scores through r 2 of linear regression. Expertise and topic similarities lead to an r 2 of 0.43. Addition of lan-guage features leads to a jump in r 2 by 10%. On a deeper analysis, we found that the language features, such as us-age of technical terms, personalization, friendliness (hello, thanks, nice, please), negative sentiments are the most use-ful language features.
In this paper, we address a novel problem of question rout-ing to user communities. We used several prior proposed measures and added novel measures for modeling different entities. We performed a comprehensive evaluation of our proposed framework and showed its effectiveness over a large real world dataset. Our work is a first step to address the community question routing problem and as part of our fu-ture work, we will explore different ranking algorithms and test our framework on several other datasets.
