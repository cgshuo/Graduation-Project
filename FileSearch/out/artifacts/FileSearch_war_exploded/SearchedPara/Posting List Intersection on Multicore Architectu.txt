 In current commercial Web search engines, queries are pro-cessed in the conjunctive mode, which requires the search en-gine to compute the intersection of a number of posting lists to determine the documents matching all query terms. In practice, the intersection operation takes a significant frac-tion of the query processing time, for some queries domi-nating the total query latency. Hence, efficient posting list intersection is critical for achieving short query latencies. In this work, we focus on improving the performance of post-ing list intersection by leveraging the compute capabilities of recent multicore systems. To this end, we consider var-ious coarse-grained and fine-grained parallelization models for list intersection. Specifically, we present an algorithm that partitions the work associated with a given query into a number of small and independent tasks that are subse-quently processed in parallel. Through a detailed empir-ical analysis of these alternative models, we demonstrate that exploiting parallelism at the finest-level of granularity is critical to achieve the best performance on multicore sys-tems. On an eight-core system, the fine-grained paralleliza-tion method is able to achieve more than five times reduc-tion in average query processing time while still exploiting the parallelism for high query throughput.
 H.3.3 [ Information Storage Systems ]: Information Re-trieval Systems Algorithms, Design, Performance, Experimentation Web search engines, multicore architectures, query process-ing, posting list intersection, intra-query parallelism
Large-scale Web search engines run under strict perfor-mance constraints: they must operate at a high target peak throughput and achieve a short response time (i.e., query latency) for each query [8]. Given the size and extent of the Web, these tight constraints must be obtained by exploiting parallelism over very large search clusters. The Web col-lection is document-based partitioned, and each node in a cluster stores a subset of the collection and the correspond-ing index. Queries are processed in parallel, i.e., the same query is evaluated concurrently over all local indexes.
To increase the peak throughput, one may replicate re-sources of the search engine. Replicas of a search cluster serve different queries independent of each other, increasing the number of queries that can be concurrently processed [8, 12]. High throughput, however, is not the only indicator of the performance of a search engine; a low query latency is also important. Recently, it has been empirically shown that high query latencies not only degrade user satisfaction but may also lead to losses in revenues of a search engine [26]. Hence, it is critical to carefully devise optimizations to im-prove query latencies. If the time constraint cannot be satis-fied, queries may have to be processed in a degraded mode, i.e., only partial search results are returned [14]. Degrada-tion causes the quality of search results to drop.
Typically, the response time of a search node to a query is engineered to be under one hundred m illiseconds. In or-der to keep processing times within these bounds, a number of optimizations are employed in the search nodes. These include caching [4, 20], multi-threading [11], index compres-sion [2, 32], and early termination of processing [1, 13, 15, 23, 29]. Emerging multicore architectures provide a new op-portunity to decrease query lat encies further. Multicores, primarily motivated by energy and power constraints, pack two or more cores on a single die. They typically share an on-chip L2 cache as well as the front-side bus to main mem-ory. As these systems become more popular, the general trend has been from single-core to many-core: from dual-, quad-, eight-core chips to the ones with tens of cores. So far, however, very little has been done to exploit the full potential of these chips in our context.

In this paper, we focus on the posting list intersection problem, where the objective is to identify the set of docu-ments that appear in all posting lists corresponding to the query terms, as efficiently as possible. We consider various coarse-grained and fine-grained parallelization strategies for improving the efficiency of list intersection on multicore sys-tems. While there has been some efforts in parallel query processing, existing strategies fail to exploit the fine-grained parallelism, which is critical to achieve good performance on multicore systems. To this end, we propose a framework, based on the producer-consumer model, that generates fine-grained processing tasks to achieve some form of intra-query parallelism so that the same query can be concurrently pro-cessed on multiple cores. For task generation, we propose an algorithm that uses the skip information in the lists to create a set of intersection tasks. We evaluate our techniques on a system with eight cores, using a large document collection and a query log obtained from AltaVista Web search engine. When compared to the baseline approach that runs on a single core, we are able to speed up the intersection opera-tion by up to a factor of 5 . 75 while keeping the throughput degradation to be as small as 3 . 3%  X  previous approaches experience at least 36% degradation.

This paper is organized as follows. Section 2 gives an overview of query processing on a typical search node. In Section 3, we describe different models for parallelizing the list intersection process on multicore architectures. In Sec-tion 4, experimental results are presented. Section 5 sum-marizes the related work. We finish the paper with the con-clusions in Section 6.
In this section, we summarize the basic steps of query evaluation in a search engine, illustrating alternative op-tions. Due to the embarrassingly parallel nature of query processing in search clusters [8], we restrict the discussion to an individual search node. Interested readers may refer to previous work in the literature on parallelization at the level of a search cluster [24, 25].

Query processing in a search node typically contains two phases [15]. In the first phase, the posting lists that corre-spond to the terms in the query are intersected to identify the documents that contain all query terms. Identified docu-ments are ranked by means of a simple yet effective scoring function, such as a linear combination of the BM25 score with a query-independent score (e.g., a link analysis met-ric). In the second phase, a subset of the documents that are top-ranked by the first phase are re-ranked by a more ac-curate, but costly ranking function (e.g., a machine-learned ranker). Depending on the properties of the query, either of these phases may dominate the query processing time.
In this work, we focus on the efficiency of the first phase ranking and, in particular, posting list intersection, which dominates the processing cost, compared to the cost of scor-ing. The main reason search engines compute the first phase document scores using the intersection of posting lists (i.e., conjunction), instead of the union of the lists (i.e., disjunc-tion), is that intersection leads to shorter query latencies as the document collections in Web search are fairly large. Moreover, intersection leads to higher result quality as most Web queries are very short. We note that, herein, we omit a discussion on relevance as it is out of scope. We also omit discussions on some optimization techniques (e.g., document id reassignment, list processing order) as they are not di-rectly related and they complement our work.

A typical posting list is composed of a sequence of post-ings, each keeping some useful information about a docu-ment that contains the respective term for the list. In its simplest form, this information contains the document iden-tifier and the frequency of the term in the document. Al-though there are other options, postings in a list are typically sorted by decreasing frequency or increasing document id. The first option enables early-termination optimizations [1, 13, 23, 29], some of which speeds up processing while pre-serving the result quality for the top k results. The sec-ond option enables compression of postings based on d -gaps. Compressing posting lists [2, 32] leads to higher cache hit rates and fewer disk accesses, in exchange of increasing pro-cessing costs due to decompression overheads. A technique used to speed up processing of compressed posting lists is to place skip pointers at regular intervals between postings [28]. This provides a form of random access to postings in the list, which is not possible, otherwise, due to compression.
In our work, we assume that posting lists are compressed and contain skip information, and that the postings within lists are sorted by increasing document id. Typically, large-scale search engines dedicate significant portions of server memory to cache most frequently and/or recently accessed posting lists to prevent expensive disk accesses [32]. Hence, we assume that disk accesses do not form a bottleneck. Note that none of these assumptions are enforced by the proposed techniques as our methods are quite general. Selection of this particular setting is mainly because it reflects the cur-rent design practices in Web search.
Multicore systems constitute an important opportunity for developing efficient and scalable applications. The key to realize true benefits of these parallel systems lies in ef-fective fine-grained parallelization of workloads. We now present various models for parallelizing the list intersection process (see Fig. 1). We consider two important metrics to compare these alternative models: throughput and aver-age query latency . Throughput is measured as the number of queries processed per unit of time. Average latency is measured as the average time a query spends in the entire system. The latency of a query is defined as the difference between the time it arrives at the system and the time its processing is complete. Since all queries have to wait in the entry queue (associated with the search node) until their processing starts, the query latency includes both the wait-ing time in the queue and the processing time. Formally, we compute the latency ( q )ofquery q by where a ( q ) denotes the time q arrives at the system while s ( q )and f ( q ) are the times at which processing of q is started and finished, respectively. Given a query workload W ,the throughput T ( W ) and the average query latency L ( W ) un-der W are given respectively by: where | W | is the number of queries processed, and t ( W )is the total wall-clock time spent in processing W .

Web search engines are typically hosted on systems com-prising a number of clusters, where each cluster node is pos-I P Algorithm 1 ThreadFN ( W, L ) 1: while W is not empty do 3: D = Process ( q, L ) 4: R = Score ( q, D ) 5: Output ( R ) 6: end while sibly a multicore system. Here, we primarily focus on lever-aging the parallelism within a single cluster node .Next,we present two paradigms for parallel list intersection: inter-query and intra-query . While the former exploits the par-allelism across different queries , the latter focuses on the parallelism that is present within a single query .
The simplest way to leverage multicore systems is to parti-tion the query workload across different cores. Each incom-ing query is scheduled for execution on the next available processing core. All cores are kept busy by dispatching the queries from the entry queue to different processors using a simple first-come-first-serve strategy. This model explores the parallelism present among different queries. It relies on the fact that all queries are independent of each other in the sense that the output of one query does not affect the processing of any other query. Hence, we refer to this model as the inter-query processing model. This model is shown in Figure 1a  X  P i  X  X  in the figure refer to different process-ing cores present in the multicore system; and I denotes the inverted index that hosts all the posting lists.

Algorithm 1 illustrate the way queries are processed in the inter-query-parallel model. In this model, all threads execute the function ThreadFN in parallel. Each thread picks up a query from the workload W (or from a stream of queries), processes it using the posting lists L , and outputs a result set R .Weprocess P lists to compute their intersection using a modified version of the Small Adaptive algorithm, proposed by Demaine et al. [17]. An eliminator e is initially set as the first item (document id) of the first list. The algorithm then repeatedly cycles through all lists searching for the eliminator. If e is present in all lists then its relevance score is computed, and the next eliminator is then chosen from the first list. If e is not present in some list L then the algorithm selects the smallest item from L that is larger than e as the next eliminator.

In a real-world scenario, the loop at line 1 is an infi-nite loop because the query stream is typically unbounded as queries continuously arrive at the system. For the parallel implementation, only the function call at line 2 forms a critical section, which requires synchronization. At any given time point, only one thread can exist in the GetNextQuery method. Since the critical section is very small, the parallelization overhead incurred in this model is minimal. It is likely that this model achieves good par-allel efficiency, resulting in high system throughput as the number of cores in the system is increased.
Although the inter-query model improves the number of queries answered in a given time period, the time it takes to process an individual query does not change irrespective of the number of processing units in the system. This is be-cause a query is always processed by a single core. However, as mentioned earlier, most commercial search engines are typically driven by the query latency, and improving this is critical for user satisfaction. We next describe three differ-ent models that leverages the intra-query parallelism present within a single query.
This strategy partitions the document collection present on the search node equally among all processing cores. Each core is responsible for identifying matching documents for the given query in the allocated partition of documents. To form the final answer, we aggregate the partial sets of re-sults produced, one from each core. Recall that the search nodes summarize the information about all documents in the form of an inverted index. Hence, this strategy corresponds to partitioning the accesses over different parts of the index. The partitioning can be done in two ways  X  logical and physi-cal . According to logical partitioning, all cores share a single unified index but each core accesses a different portion of the index. Such a strategy is shown as DOCP-UI in Figure 1b. Each processing core P 1 to P n gets a copy of the query q and they process the query against disjoint portions of the index I . The partial results produced by each core are then combined in the result aggregator , which is typically imple-mented as a shared data structure. DOCP-UI method of parallelization has been suggested by Frachtenberg [19].
The alternative choice to implement the DOCP is to phys-ically partition the index I into n equal parts I 1 to I n n is equal to the number of cores. This strategy is shown as DOCP-PI in Figure 1c. As it is evident from the figure, both DOCP-UI and DOCP-PI differ only in the way index is partitioned and used among processing cores.

For a simpler design, we implement these strategies using a master-slave model. The master (a designated core) is responsible for several tasks  X  to fetch the next query from the query stream; to send a copy of the query to all the other cores; to process the query against the allocated index partition; and to send the combined results out of the search node. While more decentralized methods are plausible, they introduce various control overheads. It is important to note that these document partitioning based parallel strategies require some form of barrier synchronization at the end of processing each query.
Both DOCP-UI and DOCP-PI strategies divide the index accesses into n parts where n is equal to the number of cores. These coarse-grained strategies suffer from load imbalance, and they often provide suboptimal performance. Several earlier studies [30] have also shown that it is imperative to leverage the fine-grained parallelism to achieve good perfor-mance on multicore systems. We now present a mechanism that divides the work associated with a single query into several small and independent tasks (see Fig. 1(d)). These tasks are then processed in parallel to produce the result set. Small-sized tasks allow us to exploit the fine-grained paral-lelism within the query, and as a result, they enable mecha-nisms that guarantee good load balance among threads. It must be noted that very small tasks increase the paralleliza-tion overhead incurred in creating and managing these tasks. Independent tasks, on the other hand, reduce the need for synchronization and thereby help in realizing better parallel efficiency.
 The framework of our intra-query model is shown in Fig. 2. This framework adopts a producer-consumer model, where one of the threads acts as the producer and remaining threads act as consumers. The producer takes a query from the entry queue, fetches associated posting lists, and pro-duces multiple, independent posting list intersection tasks. Generated tasks are pushed into an intersection task pool. Each consumer thread iteratively obtains a task from this intersection task pool and processes its task. Processing a task mainly involves decompression and intersection of post-ing lists. Documents that are common in all lists are added to the scoring task pool. They are later picked up by differ-ent threads for scoring in order to produce the final, ranked result set. The producer thread also participates in process-ing of intersection and scoring tasks, whenever possible. In contrast to the producer-consumer model, one can consider a setup where all threads are capable of both producing and Figure 2: Framework of the intra-query model.
 Algorithm 2 Producer ( W, P ) 1: while W is not empty do 2: while | ITP | X   X  do 5: ITP = ITP  X  T 6: NotifyConsumers () 7: end while 8: { Act as a consumer } 9: while | ITP | &gt; X  do 10: t = GetNextTask ( ITP ) 11: D = ProcessTask ( t ) 12: STP = STP  X  X  D } 13: end while 14: end while consuming tasks. Such a decentralized approach, however, requires complex synchronization methods, and it is likely to incur higher control overhead.

The scoring phase requires the threads to iteratively pop a document from the scoring task pool and compute a score, which is then used to rank the document. Evidently, the scoring step is an embarrassing ly parallel problem that re-quires a simple data-parallel implementation.

Algorithms 2 and 3 show the steps of producer and con-sumer threads in the intra-query model, respectively. By Algorithm 2 (lines 2 X 7), the producer can choose to tem-porarily pause the task creation process and act as a con-sumer. If the size of the task pool is smaller than a threshold  X  (line 9), the producer resumes the task creation duty. The producer relies on the pool threshold  X  to decide whether or not to generate tasks for new queries. Higher values of  X  delays the task generation, and generated tasks are likely to spend more time in the task pool before they get picked up for processing. Further discussion on the impact of  X  is pre-sented in Section 4.2. Note that only the producer process is responsible for task creation in this approach. Other de-centralized variants are possible, but they are likely to incur high control overhead. Algorithm 3 Consumer () 1: while ITP is not empty do 4: STP = STP  X  X  D } 5: end while
The producer thread receives a new query from the work-load and partitions the work associated with the query into tasks by using the posting lists required by the query (lines 3 X 5 in Algorithm 2). We partition posting lists so that each task is responsible for intersecting some portion of the lists. Unfortunately, posting lists are stored in compressed format to save space and this limits the ability to perform random accesses required while creating tasks. To facilitate effective work partitioning while retaining the benefits of compres-sion, we store postings in the form of skip lists. A skip list is a sorted list of document identifiers (also referred to as items), denoted using a linked list that connects sparse sub-sequences of items [22]. Each pointer in the list forms a link i  X  j between two non-consecutive items i and j .Thenum-ber of items skipped between two skips 1 i and j is referred to as the skip block size or skip size. Two example skip lists are shown in Fig. 3, where a i  X  X  and b j  X  X  are the skips in lists L 1 and L 2 , respectively.

We compress all document ids between a pair of skips, but store the skips without compression. For example, in L 1 of Fig. 3, the ids 20, 35, 90, and 175 are stored as uncompressed values, but the ids between these skips are compressed. For convenience, we also store the highest document id in the list as a separate skip pointing to null. More formally, the posting list for a query term a is denoted as L ( a )=( S where S a is the sequence of skips in the skip list and C the sequence of document ids in compressed format.
The producer thread generates tasks by operating on these compressed skip lists. Without loss of generality, we assume that the lists are sorted in increasing order of their size. The main idea is, for each skip block in the first (or the shortest) list, to find the relevant skip blocks from the remaining lists. A task is then formed by the sequence of skips obtained from all lists in the query. The total number of tasks generated for a given query is equal to the number of skip blocks present in the shortest posting list.

Consider a query q with two terms a and b , whose posting lists are L ( a )=( S a ,C a )and L ( b )=( S b ,C b )with Foreachskippointerin L ( a ), we create a task with one
We use the term  X  X kip X  interchangeably to indicate a link or the end points of a link.
 Algorithm 4 CreateTask () 2: for i =1to n  X  1 do 3: InitTask ( t ) 6: add ( i, i +1) to t 7: for r =2to m do 9: add ( j, k )to t 10: end for 11: ITP = ITP  X  X  t } 12: end for or more skips from L ( b ). We generate a set of independent tasks { t 1 ,t 2 ,...,t | S a | } ,where Note that s i +1 is undefined for i = | S a | . Moreover, for a given s and s i +1 in L ( a ), the skips from L ( b ) are chosen such that s  X  s j and s i +1  X  s k . In other words, all documents in L ( a ) within a skip pointer s i  X  s i +1 fall in the id interval given by [ s ,s k ]in L ( b ). If the query contains more than two terms, for each skip block in the shortest list, a sequence of skips is found from all remaining lists. The entire procedure is summarized in Algorithm 4 for the general case of a query with m terms. Here, d [ L ( a ) ,i ] denotes the document id at the i th skip in L ( a ). The skips j and k in line 7 can be found either by a linear scan or by a binary search. Each task t that is generated by the algorithm has the following form: where m = | q | and s j k indicates the k th skip in the j th post-ing list. It is important to note that each task encapsulates the complete information required to intersect the relevant portions of all posting lists. Hence, a processor can inde-pendently process a task without requiring any information about other tasks. For the skip lists in Fig. 3, Algorithm 4 produces three tasks by considering the sequence of skips in L 1 and in L 2 .
All threads iteratively dequeue tasks from the task pool and process them to determine the common documents. Ev-ery entry ( s j k ,s j ), for 1  X  j  X  m ,inataskcanbetreated as a sublist having a sequence of skips ( s j k ,s j )andthecor-responding compressed skip blocks, as seen in Eq. (1). The problem of processing a task with m entries is then reduced to that of intersecting m sublists. Hence, any of the existing list intersection algorithms [5, 6, 7, 16, 17] can be used. As mentioned in Section 3.1, we use a modified version of the Small Adaptive algorithm [17] to intersect all sublists in a given task. Our task processing algorithm first decompresses a single skip block from all sublists. The intersection pro-cess is initiated on these decompressed blocks. Whenever the search for an eliminator overshoots the current block, the next skip block from the sublist is decompressed. This process is terminated either when there are no skip blocks for decompression or after all items in the shortest sublist are considered by the intersection process.
We evaluate different parallel query processing models using a subset of the AltaVista query log (about a quar-ter million queries). 2 Posting lists are obtained from a large crawl of the UK domain, obtained in May 2006. All lists are compressed with skip list information as de-scribed in Section 3.2.3. Skip blocks are compressed using the PForDelta [33] algorithm, which is recently shown to be efficient in terms of both compression quality and decom-pression time [32]. The default skip block size is set to 512, but is varied in some experiments. We assume that all post-ing lists are loaded into the main memory before the queries are processed. In other words, we do not account for the time spent in disk I/O while evaluating our models (see the discussion in Section 2).

In our experiments, we consider that the query arrival process follows a Poisson distribution with an intensity of  X  query/sec. We evaluate our models using the two metrics defined in Section 3: query throughput and average query latency . The parallel performance of our models is evaluated by considering the fastest sequential version as the baseline. This baseline is given by the inter-query model that runs on a single processor (denoted as INTER-1 ), where the queries are processed one after the other, on a first-come-first-serve basis. Therefore, the data points that correspond to running intra-query models on a single core are not shown in any of our performance plots.

All experiments are conducted on a multicore system with dual quad-core Clovertown E5345 2 . 33GHz processor (essen-tially, a total of eight cores) with 6GB main memory. The system has 8MB aggregated L2 cache space that is shared among eight cores. The parallel code is implemented in C++ using the pthreads library.
The performance differences between the two parallel query processing paradigms are shown in Figs. 4(a) and 4(b). We measure how throughput and average query latency vary with the number of cores used for query processing. For FGQP , we show two trends that differ in the value of pool threshold  X  . Recall from Section 3.2.2 that the value of  X  affects the producer process in generating tasks for new queries. The throughput achieved by INTER and FGQP mod-els increases almost linearly with the number of cores. This indicates that these models effectively make use of the avail-able computational resources. When using eight cores, the INTER is able to serve 850 query/sec while the FGQP (  X  =150) model is able to serve up to 826 query/sec. The marginal throughput difference between INTER and FGQP is primarily due to additional parallelization overhead incurred in FGQP . This includes the time spent in creating and managing tasks,
There are a total of 47 , 278 unique query terms.
Available from the University of Milan: http://law.dsi. unimi.it/webdata/uk-2006-05/ . and aggregating partial results from individual cores. On the other hand, the overhead incurred in INTER is minimal since each core operates independently on a different query, and thereby reduces the need for synchronization  X  see Sec-tion 4.5 for further details on parallelization overhead.
When compared to FGQP , the query throughput achieved by other intra-query models that rely on document par-titioning is small across the chart. This is mainly be-cause DOCP-UI and DOCP-PI target coarse-grained parallelism among large document partitions, whereas FGQP exploits fine-grained parallelism among small-sized tasks. For a given query, the skew among the finish times of different cores is very high in DOCP-UI and DOCP-PI when compared to that in FGQP . Consequently, document partitioning based strate-gies suffer from load imbalance, and as a result, they deliver poor parallel efficiency. When compared to the sequential baseline ( inter on a single core), INTER and FGQP (  X  =150) improve the throughput by a factor of 7 . 9and7 . 6whenall eight cores in the system are used. In contrast, DOCP-UI and DOCP-PI models are able to improve it only by a factor of 5 . 1and4 . 8, respectively.

In terms of average query latency (see Fig. 4(b)), the inter-query model is unable to leverage extra processing capabil-ities in the multicore system to reduce latency. In fact, the average query latency achieved by INTER is almost constant across the board since queries are always executed on a sin-gle core irrespective of the number of available processing units. On the other hand, strategies that leverage intra-query parallelism are able to reduce the query latency since any given query is processed by multiple cores, in parallel. Note, however, that the reduction is not linear with the num-ber of cores. For example, the average query latency from the sequential baseline INTER-1 is about 9 . 27ms. In con-trast, the latency achieved by DOCP-UI and DOCP-PI models has improved from 5 . 4ms to 1 . 9ms as the number of cores in-creases from 2 to 8. Similarly, the latency of FGQP (  X  =5) has reduced from 5 . 4ms to 1 . 61ms. When compared to INTER-1 , this amounts to an improvement by a factor of 5 . 75.
To summarize, INTER model gives the best query through-p ut as the parallelization overhead is minimal but it does not offer any improvement in query latency. The performance of both document partitioning based strategies ( DOCP-UI and DOCP-PI ) are very similar to each other. While they provide good improvements in query latency, their query through-put is significantly affected by load imbalance issues from coarse-grained parallelism. In contrast, FGQP leverages a fine-grained intra-query parallelism, and it delivers query throughput that is very close to INTER while providing ex-cellent latency improvements that are similar to document partitioning strategies.

The performance of FGQP , however, depends on the value of pool threshold  X  , which is illustrated in Fig. 4(c). We con-sidertwometricsaswevarythevalueof  X  : percent degra-dation in query throughput ( PDTP ) and improvement in average query latency ( IAQL ) due to task-based intra-query processing. These metrics are computed, respectively, as where TP (  X  ) and T (  X  ) refer to the realized query through-put and total wall-clock time spent in processing the entire query workload, respectively. We would like the through-put degradation to be as small as possible and the latency improvement to be as high as possible. As the value of  X  increases, the producer process creates new tasks more ag-gressively. This increases the average query latency as tasks spend more time in the task pool before they are consumed for processing. Aggressive task generation also reduces the chance of encountering an empty task pool. This increses the query throughput since the consumer processes are always kept busy processing the tasks. Therefore, higher  X  values result in higher query throughputs and higher query laten-cies, and lower  X  values give lower query throughputs and lower query latencies. Such a trade-off between throughput degradation and latency improvement is evident in Fig. 4(c). As the value of  X  is increased from 5 to 150, the degradation in throughput (when compared to INTER-1 ) drops from 16% to 3 . 3%, while the latency improvements drop from 5 . 75-fold to 3 . 7-fold. As a comparison, we observed the degradation in throughput for DOCP-UI and DOCP-PI when running on eight cores to be 36% and 41%, respectively.

The value of  X  can be tuned based on the performance requirements set by end applications. For a given upper bound on the query latency, the value  X  can be adjusted to maximize the query throughput while keeping the latency within the given limit. On both extremes, we observed the best query throughput at  X  = 150, and similarly the average query latency can be improved by as much as 5 . 75-times by setting  X  = 5. The rest of the results in this section, unless otherwise mentioned, are obtained with  X  =5.
To investigate the effect of system load, we model a Web search engine as a queue with  X  queries arriving and  X  queries getting served per second. 4 The system will be in a stable state as long as  X   X   X  .When  X &gt; X  , the average number of queries present in the system (both waiting and in processing) will increase over time. In this experiment, we analyze the performance in such overloaded conditions by measuring how the average query latency varies as we in-crease the query arrival rate  X  .Theservicerate  X  , i.e., the rate at which queries are processed is directly proportional to the throughput of the system, which in turn relies on the number of cores used to process the queries.

We now consider a system with a fixed number of cores (equivalent to a fixed  X  ) and analyze the effect of increasing the query load (see Fig. 5(a)). We vary the value of  X  from 100 query/sec up to 1400 query/sec and observe the change in the average query latency. As  X  increases, the latency stays constant until the system saturates, after which the latency increases steeply with  X  . The saturation point of the system is a point beyond which the arrival rate dom-inates the service rate. In case of INTER running on four cores (denoted as INTER -4), the system saturates around 400 query/sec. Any further increase in  X  beyond 400 query/sec results in up to three orders of magnitude increase in query latency. In the figure, it can also be observed that the satu-ration point shifts towards the right as the number of cores increases ( INTER -8 saturates around 800 query/sec). This highlights the fact that a system that is provisioned ith more computational resources can serve more requests without saturating. This is a significant result given the architec-tural trend that promises an increasing number of cores in
The service rate  X  in our models is primarily determined by the number of cores that are used in query processing. Figure 6: Performance with varying query arrival rate and query length. future multicore server systems. We also observed similar trends for models that leverage intra-query parallelism. Fur-thermore, the performance of DOCP-PI is very similar to the one of DOCP-UI , and hence we do not include DOCP-PI in rest of the experiments.

While INTER and FGQP saturate around the same value of  X  , the saturation point of DOCP-UI is notably much smaller. This can be attributed to the fact that document partition-ing based strategies provide lower query throughputs (see Section 4.2). At small values of  X  when the system is not saturated, the performance of FGQP is superior to that of IN-TER due to its task-level parallel processing. However, in a saturated system, this difference is small because the benefit from task-level processing is overshadowed by large queuing times experienced by the queries.
Query length is known to be positively correlated with the performance of a search task [9]. A recent study showed that, in the past year, there has been a 22% growth in the number of queries with eight or more terms. 5 There are also estimations that the average query length has a tendency to increase over time. 6 Moreover, search backends are more likely to receive long queries as most short queries are caught by the result cache [27]. Given these observations, we con-duct experiments to analyze the performance of our models as the query length is varied. For this experiment, we sep-arate the AltaVista query log into nine bins according to query length (lengths are between two and ten). Each bin contains 10 , 000 queries of a particular length. This experi-ment is conducted on eight compute cores.

As shown in Fig. 5(b), the query length directly affects the average query latency, and it is indirectly related to the system throughput. Longer queries involve intersection of a larger number of lists, and hence they lead to higher query processing times. The average latency of queries with ten terms is approximately an order of magnitude greater than that of queries with only two terms. This accounts for a 11-fold drop in system throughput when processing longer queries. The intra-query models consistently provide better query latencies when compared to INTER ,and FGQP performs marginally better than DOCP-UI . Fig. 5(c) indicates that the throughput of DOCP-UI is consistently lower than that of IN-TER and FGQP , and the difference between INTER and FGQP is only marginal. An interesting observation here is that the throughput difference among different models is higher when the queries are short. This is because the paralleliza-tion overhead is relatively significant when compared to the actual time spent in query processing.

The combined effect of query arrival rate  X  and query length on the performance is studied in Fig. 6. We con-sider queries of length 4 and 6 in the experiment, and we change query arrival rate from 100 to 800 query/sec. For a given number of cores, the saturation limit is indirectly cor-related to the query length  X  longer queries lead to earlier saturation. For a given query length, the trends are similar to those observed earlier in Fig. 5(a). In a saturated sys-tem, the queries suffer from large latencies, accounting for an increased waiting time spent in the queue. The amount of change in latency increases with the query length. It can again be noticed that DOCP-UI saturates prior to INTER and it experiences marginally higher latency at high values of  X  . On the other hand, FGQP has a lower latency than IN-TER when the system is not saturated. They both saturate around the same value of  X  , and beyond which they behave approximately in a similar manner. In this experiment, we measu re the overhead incurred in FGQP model due to task-level processing. The task size not only affects the processing time but also affects the amount of synchronization that is required among cores. The syn-chronization overhead (due to accumulation, accessing the task pool etc.) is high when a given query is partitioned into a large number of tasks. We measure this overhead by defining two quantities: S avg and O avg . While S avg is the average number of skips per list per task, O avg measures the average number of overlapped skips per list, per task. They indirectly indicate the amount of work required and the amount of work that is repeated per task.

Recall that a task is denoted as the sequence of skips from each posting list involved in the query. For a query with m terms, a task t can be denoted as shown in Eq. (1). The average number S t of skips in task t can be computed as The overall S avg is computed as where T is the set of all tasks generated for the entire work-load of queries. Due to the way the tasks are generated, it is possible for two consecutive tasks t 1 and t 2 to share at most one skip per list. We quantify this overlap as follows: where j 1 =1+ i 1 and where (  X  ) + indicates that the entry is retained only if the argument is positive. Then, the quantity O avg is computed as the average taken over all tasks in the workload. Note that the first task that is generated for every query does not contribute to the summation.

Fig. 7 shows different quantities for various skip block sizes ( B ) by considering the values for B = 512 as the baseline. As B is reduced by half, both the number of skips per list and tasks per query double. Also, the amount of time spent in generating the tasks increases significantly as the value of B is decreased. Surprisingly, the values of S avg and O remain almost constant across the board. For each value of B ,weobserve S avg and O avg to be equal to 10 . 2and0 . 97, respectively, i.e., out of 10 . 2 skips/list/task, 0 . 97 skips over-lap with the previous task. In other words, approximately 10% of the skips are processed twice. Overall, we observed that the time spent in task creation is only 2% of the total wall-clock execution time. The overhead due to task cre-ation is therefore very small when compared to the actual amount of work that is needed for posting list intersection.
In the literature, there are various techniques for parallel query processing on distributed architectures [3, 21, 24, 25], where the inverted index is partitioned over a number of nodes in a search cluster. These techniques are a form of high-level intra-query parallelism because parallelization is at the cluster-level and a query is processed concurrently by many index servers. Our work is complementary to these in that intra-query parallelism is employed further in every search node at the level of cores.

The problem of intersecting sorted lists has also attracted some attention [5, 6, 7, 16, 17]. Most of the proposed algo-rithms assume that the lists are already sorted. Typically, one of the lists is used as an eliminator and some random-access lookups are performed (via binary search, galloping search, or interpolation search) in the remaining lists for a target element in the eliminator list. Due to random ac-cesses, these algorithms are not applicable to compressed posting lists unless skips are employed.

So far, the only work that considers the list intersection problem on multicore architectures is that of Tsirogiannis et al. [31]. The authors propose two algorithms called Dynamic Probes and Quantile-based. The idea in the first algorithm is to restrict the search space by using a micro-index that keeps a history of previous lookups. The second algorithm statically partitions the posting lists for the sake of improved load balancing, by using summaries of lists obtained via or-der statistics. Both algorithms require random accesses to the lists. The partitioning overhead is observed to become a bottleneck for high number of cores if the cache is not shared between all cores (see Figs. 11 and 12 in [31]).

Ding et al. [18] propose a framework for query process-ing on graphics processors (G PUs). Queries are first pre-processed on a CPU and then selectively scheduled on the GPU. Performance of their list decompression and intersec-tion techniques developed for GPUs provide only marginal improvements over CPU-based implementations (see Table 2 in [18]). This is because the accesses to main memory from GPUs incur significant overhead, and more importantly, the way they parallelize the workload does not match the access pattern best suited to GPUs (sequential scans starting from some aligned memory locations).

Bonacic et al. [11] use a strategy similar to Inter that groups queries into batches and processes them sequentially on multicores. While their method is targeted at increasing query throughput, we focus also on reducing query latencies by exploiting the intra-query parallelism. Strohman and Croft [28] present a preliminary evaluation of in-memory query processing techniques on multicore systems. Their techniques exhibit modest performance improvements on a four-core system. Frachtenberg [19] also explores the perfor-mance differences between inter-query and document parti-tioning based strategies. In fact, the best approach sug-gested by Frachtenberg (referred to as FGMT in [19]) is same as DOCP-UI in Fig. 1(b). This work assumes that hits on document ids are evenly distributed. However, in prac-tice, hits have a skewed distribution due to reassignment of document ids [10]. Our fine-grained FGQP model is immune to these kinds of biases in posting lists as intersection tasks are dynamically created. Moreover, compared to [19], we provide experimental results on a broader set of parameters.
We explored two different paradigms for parallel posting list intersection on multicore systems. While the inter-query model can scale system throughput with the number of cores by executing independent queries in parallel, the intra-query models are able to scale system throughput and reduce the average latency as we increase the number of cores. Our key contribution is in the design of a work partitioning strategy that divides a single query into multiple small and indepen-dent tasks , to enable fine-grained paral lelism that is critical for effective parallel performance on multicores. When com-pared to the baseline approach that runs on a single core, this approach is able to speed up the intersection opera-tion by up to a factor of 5 . 75 while keeping the throughput degradation to be as small as 3 . 3%  X  previous approaches experience at least 36% degradation. We also evaluated the influence of query length and arrival rate on latency and throughput, showing that the fine-grained intra-query model does not introduce any unexpected behavior compared to the inter-query approach. Finally, our analysis of skip block size indicated that smaller skip blocks increase the number of tasks and hence the overhead to create tasks. However, the fraction of redundantly processed skip blocks per task is almost constant irrespective of the skip block size.
We considered a hybrid strategy that switches between intra-query and inter-query approaches according to query traffic. Our preliminary results did not show much improve-ment, and we are currently analyzing them carefully. As part of future work, we would like to investigate methods that leverage our parallel query processing models for improving the search quality, effective server resource provisioning, and designing power-aware algorithms that adaptively adjust the performance to stay within the budgeted power envelopes. This work has been partially supported by the COAST Project (ICT-248036) , funded by the European Community.
