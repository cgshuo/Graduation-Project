 Kai Zhang kzhang@nec-labs.com Vincent W. Zheng vincent.zheng@adsc.com.sg Qiaojun Wang qjwang@eden.rutgers.edu James T. Kwok jamesk@cse.ust.hk Qiang Yang qyang@@cse.ust.hk Department of Computer Science and Engineering Ivan Marsic marsic@ece.rutgers.edu In standard supervised learning, it is commonly as-sumed that the training and test data are drawn from the same distribution, such that a classifier learned on the former generalizes well to the latter. However, this assumption can be violated in practical situations. For example, the training and test data may be col-lected under different situations as in applications in bioinformatics or sensor networks. On the other hand, if the training procedure is expensive or labels in the new domain are limited, one might want to apply the knowledge learned from one domain to a different but related domain. In these cases, the traditional learn-ing framework is no longer suited, and how to handle the discrepancy of data distributions across domains becomes a crucial problem.
 In this paper, we consider the situation when the train-ing and test data are from different distributions, i.e. , P tr ( x ) tical or similar conditional distributions P tr ( y | x ) = P variate shift or sample selection bias.
 There have been a number of attempts to solve this problem. For example, in ( Bickel et al. , 2007 ), a discriminative model (MAP classifier) is proposed that directly characterizes the divergence between the training and test distributions. Daum  X e III &amp; Marcu ( 2006 ) investigated how to train a general model with data from both the source domain and target domain for domain adaptation in natural language processing tasks. Mansour et al. ( 2009 ) presented theoretical re-sults on a distribution-weighted combining rule that has a loss of at most a pre-defined value w.r.t. any target mixture of source distributions.
 Recently, several work ( Shimodaira , 2000 ; Zadrozny , 2004 ; Huang et al. , 2007 ; Sugiyama et al. , 2008 ) has converged to the direction of estimating a pointwise re-weighting on the training data to minimize the gen-eralization error in testing. For example, Huang et al. ( 2007 ) applied the kernel mean matching (KMM) to account for the distribution difference, such that the means of the training and test samples in a reproduc-ing kernel Hilbert space (RKHS) are close. A theo-retical analysis was given by ( Yu &amp; Szepesv  X ari , 2012 ). Sugiyama et al. ( 2008 ) proposed a framework to es-timate the importance ratio with simultaneous model selection, which finds an estimate of the density ra-tio such that the Kullback-Leibler divergence from the true test input density to its estimate is minimized. In ( Bickel et al. , 2009 ), the density ratio estimation is extended to a discriminative setting.
 Instead of learning a re-weighting scheme, Pan et al. ( 2011 ) proposed to learn the transfer components in the form of pre-parameterized empirical kernel maps, such that the kernel mean of  X ( X tr ) is close to that of  X ( X te ). For other recent methods on the more general problem of transfer learning, see ( Pan &amp; Yang , 2010 ). Most of the current methods study how to make the training and testing data have similar distribution-s in the input space ( Shimodaira , 2000 ; Zadrozny , 2004 ; Sugiyama et al. , 2008 ). In ( Huang et al. , 2007 ) and ( Pan et al. , 2011 ), although the objective function considered is the difference between the sample mean in the feature space, it is used as an indicator of the distance between two distributions in the input space. Therefore, minimizing such an objective is ultimately used to control the difference of distributions in the in-put space. While one may consider data distribution in the input space for non-kernel-based methods, the behavior of kernel methods are determined in a more complex mechanism due to the interplay between the kernel function and the data distribution. In partic-ular, kernel methods work by applying a linear algo-rithm in the kernel-induced feature space, where the algorithm performance depends directly on the data distribution in the Hilbert space.
 Motivated by this observation, we propose to make the training and testing data have similar distribu-tions in the Hilbert space, which we believe is a more direct way in tackling the covariate shift problem for kernel-based learning algorithms. In particular, con-sidering that the feature space geometry is determined uniquely by the kernel matrix, this can be reformu-lated as requiring the kernel matrix to be similar for the two domains under certain conditions 1 . One big technical difficulty, however, is that the kernel matri-ces are data-dependent, and how to evaluate similarity between kernel matrices across domains remains un-clear. To bridge this gap, we introduce the concept of surrogate kernels based on the Mercer X  X  theorem, the fundamental theorem underlying the reproducing kernel Hilbert space (RKHS). It provides a convenien-t interface for kernel matrices to compare with each other. By using the surrogate kernel, we can apply an explicit (linear) transform on the kernel matrix of the training data, forcing it to properly  X  X pproach X  that of the test data, such that the kernel machine learned on the training data generalizes well to the test domain. The rest of the paper is organized as follows. Sec-tion 2 introduces the concept of surrogate kernel. In Section 3 , we propose a symmetric transform to align kernel matrices across domains. Section 4 provides ex-perimental results, and the last section concludes the paper. Handling the distribution of data in the Hilbert s-pace is difficult, since the kernel-induced feature map usually cannot be explicitly represented. In particu-lar, not all operations involved can be reduced to in-ner products as required by the kernel trick. There-fore, in order for two samples (e.g., the training and testing samples) to have similar feature-space distri-butions, we instead require them to have similar k-ernel matrices. The latter can be somehow viewed as a sufficient condition of the former given a pre-defined empirical kernel map . To see this, note that given a kernel matrix K = ( KK  X  1 2 )( K  X  1 2 K ), the cor-responding empirical kernel map can be written as  X  Therefore, if two kernel matrices are the same, i.e., K
Z = K X , then their corresponding (empirical) fea-ture maps will also be the same, i.e.,  X ( Z ) =  X ( X ), and as a result the empirical distributions of the data in the kernel-induced feature space will be the same, i.e., p [ X ( Z )] = p [ X ( X )]. In other words, matching two data distributions in the feature space can be con-veniently cast as aligning two kernel matrices, which avoids the difficulty of handling the feature vectors  X ( x ) X  X . Inspired by this simple observation, we propose to transform the kernel matrix in the source domain such that it is more similar to that in the target domain. By doing this, the feature map (RKHS) embodied via the kernel matrices will be similar for the two domain-s, allowing models trained in one domain to general-ize well to the other. However, the kernel matrix is data-dependent. Given kernel matrices defined on two different data sets, it is difficult even to evaluate the closeness between them, since they may be of different dimensions and their rows/columns do not correspond, not to mention aligning one to the other.
 To solve this problem, we propose the concept of sur-rogate kernel . More specifically, suppose that we have a kernel matrix K X defined on the data set X . On the other hand, we are given a new data set Z . Here, we want to generate a surrogate kernel of K X by some-how  X  X rojecting X  it from X to Z , denoted K Z X  X  . The surrogate kernel K Z X  X  should inherit key structures of K X but, instead of being defined on X , K Z X  X  is defined on Z . Therefore, K Z X  X  can be used in re-placement of K X when we want to compare K X with any kernel matrix defined on Z .
 In order to define the structure of the kernel matrix and how to faithfully preserve it across domains, we will resort to the following theorem.
 Theorem 1. (Mercer) Let K ( x , y ) be a continuous symmetric non-negative function which is positive def-inite and square integrable w.r.t. the distribution p (  X  then Here, the non-negative eigenvalues  X  i  X  X  and the or-thonormal eigenfunctions  X  i  X  X  are the solutions of the following integral equation The Mercer X  X  theorem ( Sch  X olkopf &amp; Smola , 2001 ) is the fundamental theorem underlying reproducing ker-nel Hilbert space. It states that any psd kernel can be reconstructed by the kernel eigenfunctions ( 1 ). In particular, given a data set X with distribution p (  X  ) and corresponding kernel matrix K X , if we can com-pute the kernel X  X  eigenspectrum  X  i  X  X  and continuous eigenfunctions  X  i (  X  ) X  X  in ( 2 ), we will then be able to evaluate the kernel ( 1 ) on arbitrary pairs of points. If the evaluation is performed on a new data set Z , a regenerated kernel matrix on Z will be obtained. In other words, the Mercer X  X  theorem provides an explicit way to generate a kernel matrix on any sample. This regenerated kernel matrix builds entirely on the eigen-system of the kernel matrix K X . Therefore, we believe that it preserves key structures of K X , and can be used as its surrogate on the new sample Z . 2.1. Estimating Kernel Eigenfunctions Next comes the problem of estimating the eigen-spectrum and continuous eigenfunctions, i.e., the solution of the integral equation ( 2 ). Thanks to ( Shawe-Taylor et al. , 2005 ), it can be approxi-mated asymptotically by a finite-sample eigenvalue-decomposition on the empirical kernel matrix K X . In the following, we derive a concrete approxima-tion ( Williams &amp; Seeger , 2001 ). Suppose sample X = { x the integral in ( 2 ) by the empirical average: Choosing x in ( 3 ) from X leads to a standard eigenval-ue decomposition K X  X  X =  X  X  X  X , where K X ( i, j ) = k ( x i , x j ),  X  X  X  R n  X  n has orthonormal columns and  X 
X  X  R n  X  n is a diagonal matrix. The eigenfunctions  X  (  X  ) X  X  and eigenvalues  X  i  X  X  in ( 2 ) can be approximated respectively by the columns of  X  and the diagonal en-tries of  X , up to a scaling constant. According to ( 3 ), the eigenfunction  X  i ( x ) at any point x can be extrap-olated by  X  i ( x ) = 1 fore, if we want to evaluate the eigenfunctions  X  i (  X  ) X  X  ( i = 1 , ..., n ) on the new set Z , we can write them in matrix form as where K ZX is the cross-similarity matrix between Z and X , evaluated using kernel k .
 We illustrate the idea in Figure 1 . Let X be drawn from a normal distribution. In Figure 1(a) , we com-pute the RBF kernel matrix K X , and plot one of its eigenvectors. In Figure 1(b) , we estimate the continu-ous eigenfunction underlying this eigenvector ( 4 ) and plot it with solid curve. In Figure 1(c) , we project the eigen-structure of K X from X to a new sample set Z by evaluating this continuous eigenfunction on Z , which gives a reconstructed eigenvector on Z . As can be seen, in the process of projecting the eigenvector of K
X from X to Z , we proceed from a discrete eigenvec-tor (on X ) to a continuous eigenfunction (on the whole real domain), and again to a discrete eigenvector (on Z ). Here, sample X is the source of information, and sample Z only passively receives information from X . In other words, Z is only a sample on which we choose to reflect and rebuild the kernel matrix K X . 2.2. De nition of Surrogate Kernels The projected eigenvector on Z can be used to recon-struct a new kernel matrix on Z . In practice, K X has multiple eigenvectors, each weighted by the corre-sponding eigenvalue. Therefore, a natural solution to project the eigen-structure of the kernel matrix from X to Z is to project each of the eigenvectors of K X from X to Z as shown in Figure 1 , and combine them using the eigenvalues of K X , i.e., De nition 1. Given two samples X and Z , and a kernel function k (  X  ,  X  ) . Let K X  X  R | X | X |X| and K R |Z| X |Z| be the kernel matrices defined on X and Z , respectively, and K XZ  X  R |X| X |Z| be the kernel matrix defined among X and Z . The surrogate kernel of K X on sample Z , denoted by K Z X  X  , is defined as In case K X is positive semi-definite, a pseudo-inverse or a small jittering factor (added to K X ) can be used. Comments 1. The kernel matrix K X and its surro-gate kernel K Z X  X  share the same generating mecha-nism: they are constructed using the same set of eigen-functions and eigenvalues, but on different samples. The notion of surrogate kernel allows us to  X  X roject X  a given kernel matrix from one sample to an arbi-trary sample while preserving the key eigen-structures. This then serves as a bridge that allows us to compare (henceforth transform among) different kernel matri-ces. In the following, we propose a parametric trans-form to rectify the kernel matrix from the training do-main, such that it becomes more aligned to the kernel matrix in the test domain.
 Suppose that X comes from the test domain, Z comes from the training domain, and the two domains have different data distributions. Here we want to adapt the kernel matrix K Z from the training domain using a symmetric transformation matrix T  X  R |Z| X |Z| , as The goal is that the transformed kernel matrix will be more similar to that in the test domain X . The linear transform on K Z ( 6 ) implicitly enforces a nonlinear transform on Z , i.e.,  X  Z =  X  ( Z ), such that where  X (  X  ) is the kernel map underlying the kernel function. From ( 7 ), we can see that the transfor-m  X  underlying ( 6 ) is indeed a linear transformation  X (  X  Z ) =  X ( Z ) T in the feature space.
 The transformed kernel matrix  X  K Z is of size |Z|  X  |Z| , while the test-domain kernel matrix K X is |X |  X  |X | . Therefore, in order to align these two matrices, we will replace K X with its surrogate kernel on Z , as K Z X  X  = K ZX K  X  1 X K XZ ( 5 ). Then, we enforce the closeness between  X  K Z and K Z X  X  by using the following optimization problem Here, the term  X  T  X  2 F controls the complexity of the transformation T , and  X  controls the balance between enforcing an exact fit and the model complexity. Fig-ure 2 gives an illustration of kernel matrix alignment via surrogate kernels.
 By setting derivative of ( 8 ) w.r.t. T to zero, Generally, if  X  is too large, solution could be complex. This can be avoided by removing negative eigenvec-tors if it happens (very rare though) as we did in our experiments. Empirically, we simply fix  X  at a small value and it is observed that the performance of our algorithm is quite insensitive to the choice of  X  . Once T is obtained, we can compute the transformed kernel T  X  K Z T , which corresponds to the inner product of the transformed training data  X  Z . 3.1. Cross-Domain Similarity In order to use the transformed training data  X  Z in kernel-based learning algorithms, we will need to com-pute the composite kernel matrix defined on  X  Z  X  X : By design, we have We also need to compute the inner product between the transformed training data  X (  X  Z ) and the original test data  X ( X ), both of which could lie in an infinite-dimensional feature space. By using ( 7 ), we have  X (  X  Z ) =  X ( Z ) T, and so So we have the following composite kernel which can be used in any kernel-based learning algorithm: The kernel matrix G is always positive semi-definite since it is the inner product of [ X (  X  Z )  X   X ( X )  X  ]  X  3.2. Complexity Let |Z| = n 1 , and |X | = n 2 . The space complexity of our approach is O (( n 1 + n 2 ) 2 ). Computing ( 9 ) takes O ( n 1 n 2 + n 3 1 ) time; computing ( 10 ) takes O (( n time. Hence, the time complexity of our approach is O ( n 3 1 + ( n 1 + n 2 ) 2 ). This can be reduced by low-rank approximation, and will be studied in the future. 3.3. Prediction Let G Z be the sub-kernel matrix on the training sam-ple Z , and G XZ be the sub-block of G corresponding to X and Z . The learned kernel matrix G in ( 10 ) can be used in various kernel-based learning algorithms. For example, in kernel ridge regression, we predict the labels for the test data X as For SVM, after using (  X  G Z , y Z ) to train a classifier, we can predict the labels of the test data by where  X  is the Lagrange multipliers and b is the bias. In this section, we perform extensive empirical evalu-ation on a number of real-world data sets, including text mining (classification) and WiFi-localization (re-gression). The following methods will be compared: 1. support vector machine (SVM) for classification 2. kernel ridge regression (KRR) for regression tasks; 3. kernel mean matching (KMM) ( Huang et al. , 4. Kullback-Leibler importance estimation proce-5. transductive SVM (TSVM) ( Joachims , 1999 ); 6. Laplacian-regularized least squares (LAP-RLS) 7. transfer component analysis (TCA) ( Pan et al. , 8. the proposed method.
 For SVM and KRR, we apply them on the training data Z to obtain the model, and then use the learned model for prediction on the test domain X . For K-MM and KLIEP, a set of re-weighting coefficients are learned from the training data Z . These are then ap-plied either on a SVM or KRR to obtain a predictive model (using the LIBSVM package), which is used for prediction on the test data X . For the TSVM and LAP-RLS, we combine the data from the training and testing domains together, and use the data from the training domain as labeled data, and the data from the test domain as unlabeled data. For TCA and the pro-posed method, a kernel matrix G is learned from the training and testing data X  X  Z ; then the sub-kernel matrix G Z is used in SVM/KRR to obtain the model, and prediction is performed on the test data using this learned model together with G XZ .
 In all the experiments, we randomly choose 60% of the samples from the training and test domains. The experiments is repeated 10 times, and the average per-formance (together with the standard deviation) is re-ported. Similar to ( Pan et al. , 2011 ), we randomly select a very small subset of the test domain data to tune parameters for all the methods. 4.1. Text Classi cation In this experiment, evaluations are performed on the 20-newsgroups data 2 , which has been frequent-ly used to benchmark the performance of transfer learning algorithms with the covariate shift assump-tion ( Pan &amp; Yang , 2010 ). The data set is a collection of approximately 20,000 newsgroup documents, parti-tioned across 20 different newsgroups and organized in a hierarchical structure. Data from different subcate-gories under the same parent category are considered to be from different but related domains. They typ-ically have difference distributions and will be used respectively as training and testing data to examine the performance of transfer learning algorithms. The task is to predict the labels of the parent categories. Table 1 shows the five binary classification tasks: rec-vs-talk , rec-vs-sci , sci-vs-talk , comp-vs-sci , and comp-vs-talk . SVM with the linear kernel is used. Accuracies, averaged over 10 randomly drawn subset from the two domains, are reported in Table 2 . As can be seen, our approach attains the best performance on most of the tasks. 4.2. WiFi Localization In WiFi localization, we collected WiFi signals X = { x from d access points, where i is the measurement in-dex. The corresponding set of locations (labels) is de-noted Y = { y i  X  R } . Our WiFi data were collected from the hallway area of an academic building, which is around 64  X  50 squared meters. The hallway area is discretized into a space of 119 grids, each grid is of 1.5  X  1.5 squared meters. The task is to learn the map-ping from the signal space to the location space. This is usually formulated as regression. Due to differences of devices, environment, the training and testing da-ta can have different distributions. Therefore, this is a suitable task for evaluating transfer learning algo-rithms. We used the RBF kernel in this experiment. 4.2.1. Transfer across Time Periods Here, we collected WiFi data from 119 grids in three different time periods in the same hallway. Figure 3 demonstrates that WiFi signals collected at differen-t time periods can have different distributions. The three times are indexed t1 , t2 , and t3 . We use the data from one time period for training, and the data from another for testing. This gives rise to three tasks, namely, t1-vs-t2 , t1-vs-t3 , and t2-vs-t3 . In each task, the first period provides the training data and the second period provides the test data (Table 3 ). For performance evaluation, we transform the regres-sion error to localization accuracy as is common in the wireless sensor networks literature. For each sig-1.66 63.38  X  5.81 65.50  X  6.75 65.05  X  3.19 1.87 68.03  X  1.72 71.98  X  4.12 76.08  X  1.53 3.52 62.03  X  2.39 56.31  X  4.62 62.10  X  2.32 1.18 65.63  X  2.64 63.40  X  3.02 66.17  X  2.26 1.63 61.80  X  1.52 56.51  X  1.64 66.00  X  2.15 nal x i to be localized, the localization is accurate if the predicted position is within 3 meters from the true position. The accuracy averaged over all the signals is shown in Table 4 . As can be seen, our approach gives the best performance on most of the tasks. 4.2.2. Transfer Across Devices Here we perform WiFi localization on different devices. Different wireless devices usually have different signal sensing capacities, and consequently the signals from different devices will vary from each other. Figure 4 il-lustrates this by showing the signals from two different devices at the same location.
 We collected WiFi data from two different devices at 3 straight-line hallways. The first hallway includes 18 grids, the second hallway include 22 grids, and the third hallway includes 19 grids. We thus have 3 tasks, namely, hallway1 , hallway2 , and hallway3 . In each task, the first device provides the labeled training da-ta, and the second device provides the test data. In pre-processing the data, we remove those dimensions whose signal strengths are all zeros. Due to the differ-ence of the hallways, the effective dimensions for the three tasks are different. (Table 5 ).
 Similar to the performance evaluation in Section 4.2.1 , localization of each signal is considered accurate if the predicted position is within 6 meters from the true po-sition. Results are shown in Table 6 . Again, proposed approach yields best performance on all three tasks. We proposed a novel concept of surrogate kernel to align kernel matrices across domains, so as to match data distributions to compensate for the covariate shift in the Hilbert space. In the future, we will study dif-ferent types of transformation, as well as the use of la-beled and unlabeled samples ( Kulis et al. , 2012 ). The surrogate kernel has interesting connection with the Nystr  X om method ( Williams &amp; Seeger , 2001 ), the lat-ter mainly used for low-rank approximation of kernel matrices. An interesting topic is how to choose useful landmark points to compute the surrogate kernel for model adaptation, based on sampling or clustering like in ( Zhang et al. , 2008 )( Kumar et al. , 2012 ). Vincent Zheng is supported by the HSSP research grant from Singapore X  X  Agency for Science, Technolo-gy and Research (A*STAR). James Kwok is supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 614311). Qiang Yang thanks the support of Hong Kong CERG projects 621211 and 620812.
 1.32 82.35  X  1.08 86.85  X  1.61 90.36  X  1.22 1.15 94.96  X  1.04 80.48  X  2.73 94.97  X  1.29 1.05 85.34  X  1.88 72.02  X  1.32 85.83  X  1.31 6.77 53.68  X  0.45 65.93  X  0.86 76.36  X  2.44 4.09 56.18  X  0.59 62.44  X  1.25 64.69  X  0.77 3.44 51.53  X  1.04 59.18  X  0.56 65.73  X  1.57
