 Anti-virus systems developed by different vendors often demon-strate strong discrepancies in how they name malware, which sign-ficantly hinders malware information sharing. While existing work has proposed a plethora of malware naming standards, most anti-virus vendors were reluctant to change their own naming conven-tions. In this paper we explore a new, more pragmatic alterna-tive. We propose to exploit the correlation between malware nam-ing of different anti-virus systems to create their consensus clas-sification , through which these systems can share malware infor-mation without modifying their naming conventions. Specifically we present Latin , a novel classification integration framework leveraging the correspondence between participating anti-virus sys-tems as reflected in heterogeneous information sources at instance-instance, instance-name, and name-name levels. We provide results from extensive experimental studies using real malware datasets and concrete use cases to verify the efficacy of Latin in support-ing cross-system malware information sharing.
 H.2.8 [ Database Applications ]: Data mining; D.4.6 [ Security and Protection ]: Invasive software Malware Naming; Classification Integration; Consensus Learning
Mitigating and defending against malware threats (e.g., viruses, worms, rootkits, and backdoors) has been a prominent topic for security research for decades. A plethora of anti-virus ( AV ) sys-tems have been developed by different vendors (e.g., Kaspersky , Symantec , Norton ). These systems often feature drastically dif-ferent expertise (details in  X  2); conceivably it is beneficial to inte-grate malware intelligence across different systems, thereby lead-ing to more comprehensive understanding of the entire  X  X alware  X 
T he narrative of the tower of Babel is recorded in Genesis 11: everyone in the tower spoke the same language.
 Figure 1: Schematic example of malware classification by two A V systems, which demonstrate discrepancies in both syntactic repre-sentation of taxonomy and semantic entailment of categorization. universe X  and helping build defense systems with broader coverage and more informative diagnosis. In reality, however, the sharing of malware threat information is still limited to the scale of a few sys-tems [8], due to challenges such as conflict of interest, discrepancy of information, and incompatibility of data schemas.

Here we focus on one potent challenge -the discrepancy of mal-ware naming in different AV systems. Conceptually an AV system comprises two core parts, malware taxonomy and underlying clas-sification rules. The name assigned to an malware instance speci-fies its class in the taxonomy. For example,  X  Trojan.Spy  X  in Figure 1 represents a class of malware spying on user activities. Moreover names of a set of instances essentially dictate a partitioning of the set into different classes. Due to lack of standards, AV systems de-veloped by different vendors often demonstrate strong discrepan-cies in how they name malware instances, both syntactically (e.g.,  X  Spy-Trj  X  versus  X  Trojan.Spy  X ) and semantically (e.g., whether two instances belong to a same class). Hence sharing malware infor-mation solely based on malware names may lead to unpredictable consequences, because similar names could refer to completely dis-tinct concepts by different systems.

Existing work has attempted to remedy this issue by proposing various malware naming standards; however most major AV ven-dors were reluctant to change their naming conventions [4, 10, 25]. We argue that even if they did, the semantic discrepancy would still exist; for example, different vendors may hold conflicting opinions regarding how a set of malware instances should be classified, al-though they might fully agree on the syntactic representation of taxonomy. One may suggest use more intrinsic features (e.g., MD5 hash of malware binaries) instead of names to identify malware in-stances. This option however has the drawbacks such as lacking interpretability and eradicating all the semantic relationships be-tween malware (e.g., variation [23]).
Motivated by this, we explore a new, more pragmatic alternative premised on the fundamental assumption below: regarding a given set of malware instances, despite the disparate classification by in-dividual AV systems, there exists certain  X  X onsensus classification X  agreed by a majority of them. Therefore, via the probabilistic map-pings between their  X  X ocal X  classification and consensus classifica-tion, individual AV systems are able to share malware information with each other without modifying their own naming conventions. Note that we are not arguing that the consensus classification cor-responds to the ground truth; rather, we consider it as a common ground to facilitate malware information sharing.

To this end, we present Latin , a novel classification integra-tion framework which exploits the correlation between malware naming of different AV systems to create the consensus classifica-tion. Specifically (i) for participating AV systems, Latin exploits the correspondence between their local classification as reflected in various heterogenous information at instance-instance, instance-name, and name-name levels; (ii) it then applies novel consen-sus learning methods over such multifaceted correspondence to de-rive the consensus classification; and (iii) with the help of local-consensus mappings, Latin is capable of answering several funda-mental types of queries including: matching -determining whether given names from different systems refer to similar instances, and retrieval -identifying counterparts of given names in other AV sys-tems. Our proposal is illustrated in Figure 2.

With extensive experimental studies using real malware datasets and concrete use cases, we show that:  X  Latin effectively exploits various heterogenous information in-cluding: AV system scan results, malware encyclopedia records, and online malware analysis reports.  X  Latin has direct impact on a range of real-world applications.
For example, it allows analysts to search for relevant information in multiple AV systems when a newly found malware variant is only partially understood; it supports efficient integration of malware analysis reports generated by different AV systems; it also helps calibrate performance measures of malware analysis tools conducted using self-composed malware datasets.  X  Latin scales up to large-scale malware datasets (e.g., millions of instances) under modest hardware configuration.

The remainder of the paper proceeds as follows.  X  2 presents an empirical study on the current state of malware naming discrep-ancies and motivates the design of Latin .  X  3 and  X  4 describe in detail the design and implementation of core components of Latin . The proposed solution is empirically evaluated in  X  5.  X  6 surveys relevant literature. The paper is concluded in  X  7.
To understand and characterize the issue of malware naming dis-crepancies, in this section we conduct an empirical study on a set of representative AV systems.
We prepare a large malware corpus (235,974 distinct instances) from VX Heavens 1 , a well-known malicious code repository. The corpus covers most major types of malware (e.g., viruses, worms, backdoors, rootkits). We feed these instances to VirusTotal online malware scanner which generates analysis reports by query-ing multiple AV systems. We intend to understand malware naming discrepancies via analyzing the scan results.

We observe that except for generic-(e.g.,  X  Malware.Generic  X ) and heuristic-type names (e.g.,  X  BehavesLike-Win32-Trojan  X ), most mal-ware names assigned by target AV systems contain two common fields: h type i (primary category, e.g.,  X  Trojan  X ) and h family i (sec-ondary category, e.g.,  X  Spy  X ). In the following we focus on these two fields and consider their combination as malware names .
We identify two primary types of malware naming discrepancies: syntactic discrepancy and semantic discrepancy. The former refers to the disparate syntactic representation of malware names used by different AV systems; while the latter refers to the conflicting opin-ions of different systems regarding the relationships between given malware instances. Below we detail both types of discrepancies observed in the scan results.
To quantify syntactic discrepancy, we apply approximate string matching over malware names (i.e., h type i . h family i ) given by four AV systems 3 ( S 1 , S 2 , S 3 , and S 4 ). Figure 3 shows for each AV sys-tem the cumulative distribution of names matching certain counter-parts in another system under varying minimum edit distance con-straint. For each AV system the average length of malware names is shown as the right boundary.

We observe that in all the cases the edit distance between  X  X atch-able X  names from different systems is small (i.e., 1  X  4 characters), in contrast of the average name length. This implies that most AV systems adopt similar taxonomies. This also suggests one possible solution to addressing syntactic discrepancy (as proposed in previ-ous studies [4, 10, 25]): finding corresponding malware names in different AV systems and imposing unified naming standards. possible controversies. Figure 3: Cumulative distribution of matchable malware name s be-tween different AV systems with respect to varying edit distance constraint. The right boundary of horizontal axis represents the av-erage length of names in the corresponding system. Figure 4: Semantic similarity matrix of twelve representati ve AV systems, with each cell representing the Jaccard similarity of equiv-alent instance pairs in two AV systems. Unfortunately the syntactic discrepancy is only a part of the story. More subtle are various semantic discrepancies. First, similar names may refer to completely distinct instances by different AV systems, for designers and developers of different systems may hold con-flicting opinions about the connotation of these names. Thus even if one could completely eradicate syntactic discrepancy via standard-izing naming formats, this semantic discrepancy would still exist. Second, recall that given a set of instances, their names essentially partition the set into different classes; any two instances in a same class are considered  X  X quivalent X . Indeed the semantic discrepancy reflects at the disparity of such equivalence relationships in differ-ent AV systems. In the following we focus on the second type of semantic discrepancy.

To quantify this semantic discrepancy, we compare the pairs of equivalent malware instances in different AV systems. More specif-ically, consider two AV systems S k ( k = 1 , 2) ; denote by P set of equivalent instance pairs in S k . We measure the semantic dis-crepancy between S 1 and S 2 by calculating the Jaccard similarity of P 1 and P 2 : J ( P 1 , P 2 ) = | P 1  X  P 2 | / | P 1  X  P We apply this metric to scan results given by twelve top-ranked AV systems [7], with results shown in Figure 4 wherein each cell represents the Jaccard similarity of equivalent pairs in two systems. It is observed that for the majority of systems, their pairwise simi-larity is below 0.5. This implies that (i) the semantic discrepancy is substantial and prevalent in major AV systems and (ii) the feasibil-ity of imposing unified naming standards is questionable given the magnificence of discrepancy.
Instead of attempting to directly reconcile either syntactic or se-mantic discrepancy, the objective of Latin is to create the  X  X onsen-sus classification X  maximally agreed by participating AV systems and to use it as a common ground to support cross-system malware information sharing.
F igure 2 illustrates the system architecture of Latin , which com-prises two major components: name alignment and query process-ing . Informally name alignment is the process of constructing the consensus classification using all available (heterogenous) informa-tion (e.g., AV system scan results, malware encyclopedia records, online malware analysis reports). The way the consensus classi-fication is constructed also entails its mappings with the  X  X ocal X  classification of each individual AV system. These mappings are often probabilistic (i.e.,  X  X oft correspondence X ) due to the possible conflicting opinions from different AV systems, e.g., some may di-vide a set of malware instances into multiple classes whereas others may prefer lumping them together.

With the consensus classification as reference, Latin is capa-ble of answering several fundamental types of queries. It works in two phases: (i) folding-in , projecting names in the query to the space of consensus classification via local-consensus mappings in conjunction with other available information relevant to the query (e.g., malware behavior description), and (ii) query execution , e.g., reasoning about the relationships between query names on the con-sensus classification (matching) or further projecting query names to target AV systems (retrieval).

The details of name alignment and query processing are pre-sented in  X  3 and  X  4, respectively.
As illustrated in Figure 5, name alignment entails three steps: (1) relationship extraction , wherein it derives all types of relationships (i.e., instance-name, instance-instance, name-name) from available heterogenous information sources; (2) ISG construction , in which it creates a structure called instance similarity graph ( ISG ), which maximally preserves all relationships derived in the previous step; and (3) consensus classification derivation , in which it partitions ISG into high-quality clusters (corresponding to consensus classes) and derives probabilistic local-consensus mappings. Next we elab-orate each step.
In this step, Latin identifies all types of relationships among given malware instances and their names assigned by participating AV systems. To be succinct yet reveal the important issues, we use three heterogenous information sources as example to show how to extract such relationships, namely, AV system scan results , online malware analysis reports , and malware encyclopedia records .
We feed the corpus of malware instances to each AV system and obtain their scan results, which list the name assigned to each in-stance. We then incorporate this local classification (i.e., instance-name relationships) into a weighted bipartite graph as shown in Figure 6 (a), wherein nodes on the left side represent malware in-stances, ones on the right side represent names appearing in the classification results, and instance i and name c are adjacent only if i i s categorized as c by a certain AV system. Note that we differen-tiate identical names from different AV systems as they may have varying semantics.

As constructed over a common set of instances, this graph en-codes correlation between names from different systems.
 E XAMPLE 1. An instance classified as  X  Trojan-Proxy.Ranky  X  by S and  X  Backdoor.Bot  X  by S 2 correlates these two names.

Moreover from the scan results we can also derive name-name relationships. Recall that an malware name c consists of two parts, h type i and h family i , denoted by c t and c f . We compare the type and family fields of malware names c and c  X  separately. Specifically let ngram ( c t ) denote the set of n-grams of the type field of c ( n = 2 in our implementation). The similarity of c t and c  X  t is measured using their Jaccard coefficient:
A similar definition applies to the family field. The similarity of names c and c  X  is then defined as: where  X  balance the importance of type and family fields.
Now let A CC  X  R m  X  m denote the name-name similarity matrix with [ A CC ] c,c  X  = k ( c, c  X  ) as shown in Figure 6 (b).
Unfortunately for a particular AV system, the coverage of its lo-cal classification could be fairly limited or biased, highly depen-dent on the available malware corpus. Moreover the instance-name and name-name relationships alone are insufficient for constructing optimal consensus classification; rather, instance-instance relation-ships also need to be taken into account. Next we show how Latin integrates other forms of malware intelligence, thereby providing coverage beyond the corpus of malware instances and the instance-name and name-name relationships.
Many AV system vendors today make available online threat en-cyclopedias that provide information of a number of malware in-stances well-studied by analysts. For example, Figure 7 shows a sample entry of malware  X  Worm.Win32/Chyzvis.B  X  from S 2
In each encyclopedia entry, the field of aliases (i.e.,  X  X lso-known-as X ) is particularly valuable, for it specifies equivalent names of the malware instance in other AV systems as identified by security ana-l ysts; therefore they also reflect instance-name relationships, which complement the corpus of malware instances.

E XAMPLE 2. As highlighted in Figure 7, the aliases of mal-ware instance  X  Worm.Win32/Chyzvis.B  X  indicate that it is classified as  X  Worm.Chyzvis  X  by S 2 but  X  Trojan.Scar  X  by S 4 .

As shown in Figure 6 (c), we incorporate these instance-name relationships to the graph created using the malware scan results. Let I and C denote the set of instances and names in the graph, with |I| = n and |C| = m . We use a matrix A IC  X  R n  X  m represent the instance-name relationships, with [ A IC ] i,c classified as c in a certain system and 0 otherwise.
Another valuable information source is online malware analysis services (e.g., Anubis 4 ) which execute submitted malware bina-ries in sandbox environments and report observed activities (called  X  X ehavior profiles X ), as shown in Figure 8. Latin queries Anubis with the malware corpus and collects their analysis reports.
The behavior profile of an malware instance gives its detailed technical description, such as how it gets installed, how it spreads, and how it damages an infected system. Thus by comparing behav-ior profiles of two instances, we are able to derive their semantic similarity, i.e., instance-instance relationships, which complements scan results and malware encyclopedia.

However the challenge lies in that: (i) the schema of behavior profile varies from case to case, e.g., certain sections present in one profile may be missing in another, or some profiles may use free text description while others may use pseudo-code like languages; (ii) the detail level of documentation also varies, e.g., different parts may be missing for different instances, reflecting the varying depth of understanding analysts have regarding the behavior of different instances. Such inconsistency makes directly comparing behavior profiles extremely difficult.

We address the challenge by transforming different forms of be-havior profiles into a unified model. We observe that despite their diverse representation, the keywords of behavior profiles share a common vocabulary, specific to security research community. We resort to security glossaries (e.g., SANS 5 and RSA 6 glossaries) to build such a vocabulary, onto which each behavior profile is pro-jected. More formally, assuming a vocabulary T , the behavior pro-file of instance i  X  I is transformed into a bag of words: p { . . . , w i,t , . . . } ( t  X  T ) where w i,t denotes the frequency of term t for i  X  X  behavior profile.

E XAMPLE 3. In Figure 8, the behavior profile contains the fol-lowing set of security terms from the vocabulary: { worm, remov-able media, backdoor, root, UPX } .

The for each behavior profile, we consider its probability distri-bution over the latent topics generated by Latent Dirichlet Alloca-tion ( LDA ) [3] as its feature vector. The computation of the simi-larity of feature vectors is fairly straightforward, e.g., using cosine distance. We consider this similarity measure encodes the instance-instance relationship from behavior profile perspective, which com-plement the instance-name and name-name relationships, as shown in Figure 6 (d). In the following we use S p  X  R n  X  n to denote the instance-instance relationships derived from their behavior profiles,
In this step, Latin constructs instance similarity graph ( ISG ), a structure maximally preserving all the relationships derived in the previous step. Intuitively ISG embeds all instance-name and name-name relationships in the space of instance-instance relationships. More specifically, given instance-name relationship matrix A two malware instances are considered similar if they are associ-ated with similar names. We therefore derive the similarity matrix S r  X  R n  X  n for malware instances based on their relationships with different names: S r = A IC A IC T .

Given name-name relationship matrix A CC , we populate it to the space of malware instances by following a procedure similar to that of deriving S p from behavior profiles. The details are omitted here due to the space limitations. Below we use S n  X  R n  X  n represent the instance-instance similarity matrix derived from the name-name relationships.

Now we have collected evidence of instance-instance similarities in forms of three similarity matrices S r , S p , and S n evidences are integrated into an overall similarity measure using a linear combination: where  X ,  X  (  X ,  X   X  0 and  X  +  X   X  1 ) control the relative weight of different evidences (their setting will be discussed in  X  5). For mathematical convenience, we assume the diagonal elements of S are set as zero. Indeed S encodes ISG .
The previous steps extract comprehensive relationships between malware instances and names. Next we aim at applying consensus learning methods over these relationships to derive the consensus classification. However we face the major challenges of scalability. As most existing consensus learning methods rely heavily on tech-niques such as spectral graph analysis [18], thereby scaling poorly for our case (e.g., over 235K instances and over 25K classes in our experiments).

We derive a novel consensus learning algorithm based on power iterative methods [15], which significantly reduces the running time. Moreover we integrate this algorithm with clustering quality met-rics to automatically determine the optimal number of classes in one run. Specifically we achieve this goal as follows: (i) finding an extremely low-dimensional embedding of malware instances that captures all instance-instance, instance-name, and name-name rela-tionships; (ii) identifying the optimal partitioning of the embedding as the consensus classification; and (iii) deriving the mappings be-tween local and consensus classification through the relationships between instances and names. 3.3.1 Optimal Consensus Classification Denote by W the row-normalized version of S : It is known that the top eigenvectors of W give a lower dimen-sional embedding of instances that preserves their similarity struc-tures. We thus apply the power iteration method to deriving a one-dimensional vector v  X  , which not only well approximates this em-bedding but also is efficiently computable.

The extreme low dimensionality of embedding v  X  allows us to readily apply sophisticated clustering methods to find high-quality partitioning of v  X  , which corresponds to majority-agreed grouping of malware instances. We devise a simple, dynamic programming algorithm for this task that requires no prior knowledge of the num-ber of clusters. The details are referred to Appendix A. 3.3.2 Local-Consensus Mappings
The bijection between embedding v  X  and malware instances I implies that the partitioning of v  X  corresponds to the classifica-tion of I , which we consider as the consensus classification. Then by leveraging instance-name relationships A IC , we identify the probabilistic mappings between local and consensus classification. Specifically let c and c  X  denote a name in the local classification and a class in the consensus classification, respectively. The for-ward correspondence [ F ] c,c  X  is the probability that an instance with name c belongs to class c  X  , which is calculated as the fraction of in-stances associated with both c and c  X  , | c  X  c  X  | / | c | .
Similarly we derive the reverse correspondence from the con-sensus classification to each local classification. However as the consensus classification may contain instances not covered by the local classification, we have to exclude those instances in the com-putation. It is worth emphasizing that while it is possible to derive the soft correspondence between a pair of local classifications di-rectly, the drastically varying coverage of AV system often lead to mappings of inferior quality.
Equipped with both forward and reverse correspondence between local and consensus classification as reference, Latin effectively processes several fundamental types of queries.
The prerequisite of query processing is to first map malware in-stances in queries (from specific AV systems) to the space of con-sensus classification.

Given instance q and class c in the consensus classification, the class membership [ M ] q,c specifies the probability that q belongs to c . Intuitively the class memberships of instance q with respect to all classes in the consensus classification form a probability distri-bution. The process of inferencing this distribution is referred to as folding-in , which we detail below. Without loss of generality, we fix a particular class c  X  in the consensus classification and show how to estimate [ M ] q,c  X  . 4.1.1 Q uery with Only Name
We start with the simple case that only the name c q of q is avail-able. We assume c q has been normalized into the format of h type i + h family i . If c q appears in the local classification, the folding-in of q is straightforward, which is the forward correspondence of the class indicated by c q , i.e., [ M ] q,c  X  = [ F ] c q ,c  X 
In the case that c q is not found in the local classification, we need to first derive the relationships between c q and malware names { c } in the local classification using the name similarity metric defined in Eqn.(1) and then integrate the forward correspondence between { c } and c  X  to estimate [ M ] q,c  X  . More formally,
Here the summation iterates over all names in the local classifi-cation which c q associates with. 4.1.2 Query with Alias
With (optional) other information relevant to q available, we can further improve the estimation quality of M q,c  X  . Consider the case that the aliases of q are available. We first compute M q,c  X  cording to Eqn.(3) for each of its aliases with respect to the corre-sponding local classification and then use their average value as the overall estimation of M q,c  X  . 4.1.3 Query with Behavior Profile
In this case that the behavior profile p q of q is available, for each name c in the local classification, we incorporate the behavior pro-file similarity between p q and all instances associated with c :
W e compute [ M ] q,c  X  for each local classification and use their average value as the overall estimation.

Finally we integrate the estimation from both name (and alias) and behavior profile. Let [ M n ] q,c  X  and [ M p ] q,c  X  and profile-based estimation respectively. The overall estimation and  X  are the same parameters in Eqn.(2) controling the relative importance of names and behavior profiles.
Once the malware instances in queries are projected to the space of consensus classification, the query processing is fairly straight-forward. Here we consider the nontrivial case that the instances are not found in any local classification. 4.2.1 Matching In a matching query, a pair of instances ( q s , q t ) from different AV systems are given, the analyst intends to know whether q q belong to the same class in the consensus classification. Recall that we use the distribution M q to represent the membership of q with respect to each class in the consensus classification. Let M and M q t denote the membership vectors of q s and q t . The prob-ability that q s matches q t is estimated by any distance metrics for distributions; in current implementation, we use Jensen-Shannon divergence [16] as the metric. 4.2.2 Retrieval
In a retrieval query, an instance q from one specific AV system is given, the analyst intends to identify its alternative names in another system. Similar to folding-in but in the opposite direction, with the h elp of reverse correspondence, we translate the class memberships of q into the space of target local classification. Concretely consider a malware name c in the target classification, the probability of q associating with c is estimated by P c  X  [ M ] q,c  X  [ F ] can be interpreted in multiple ways. For example, one can select the name c with the largest probability as the most likely name of q ; or one can sum up the probability of names with the same type (e.g.,  X  Trojan  X ) and reason about the membership of q at the granularity of malware type.
In this section, we present an empirical analysis of Latin cen-tering around three metrics: (i) its effectiveness in reconciling the malware naming discrepancies of participating AV systems, (ii) its impact on real-world security applications, and (iii) its operation efficiency and sensitivity to training data. We start with describing the experimental setting.
Our experiments use a set of datasets collected from real AV sys-tems. The first one is the collection of 235,974 distinct malware instances with binaries obtained from VX Heavens . We feed this malware corpus to four AV systems ( S 1 , S 2 , S 3 , and S lect their classification results, from which we remove the results with generic-and heuristic-type malware names. The numbers of remaining instances and names are summarized in Table 1. Mean-while among these AV systems, S 2 and S 4 have online threat en-cyclopedias available, from which we retrieve all available alias information. Furthermore we feed the corpus of malware instances to the online malware analysis service Anubis and collect their be-havior description information. After combining all these datasets and de-duplicating instances appearing in multiple sources, we col-lect a total of 241,530 distinct malware instances associated by both aliases and behavior profiles.

All the algorithms are implemented using Python. All the ex-periments are conducted on a Linux workstation running Intel i7 2.2 GHz processor and 8 GB RAM. By default we set  X  = 0 . 6 (Eqn.(1)) and determine  X  and  X  (Eqn.(2)) for concrete tasks (match-ing or retrieval) using ten-fold cross validation.
In the first set of experiments, we take a close examination of the name alignment component of Latin on reconciling the aforemen-tioned semantic discrepancies. While the design of Latin allows it to flexibly consume most available intelligence (e.g., aliases, be-havior profiles, etc.), here we solely rely on the most commonly available information -the scan results of malware instances by dif-ferent AV systems -as input to train Latin and evaluate the quality of learned consensus classification.

Specifically the consensus classification integrates the scan re-sults of 231,663 distinct instances in the malware corpus (after ex-cluding generic and heuristic-type names). We contrast the consen-sus and local classifications from these three aspects.
 coverage than any local classification as it integrates intelligence Table 2: Statistics of large-size classes in local and consen sus clas-sifications ( S -consensus classification). from all individual systems. Specifically with respect to the total number of instances in corpus (235,974), the coverage of individual AV systems varies from 65% to 88% (Table 1), while the consensus classification achieve coverage above 98%.
 in local and consensus classification. Table 2 lists the size of the largest class and the number of classes containing at least 5% of to-tal number of instances in the corpus. The consensus classification ( S ) features the lowest numbers in both categories, indicating that intuitively the consensus classification prefers detailed over coarse-grained classification.
 Table 3: Confusion matrix of equivalent instance pairs agree d by both row and column systems (percentage relative to the total num-ber of equivalent pairs in the row system).
 in reconciling classification conflicts between different AV systems. Table 3 lists the percentage of equivalent instance pairs (i.e., both instances classified to a same class) agreed by two AV systems (rep-resented by respective row and column) with respect to the total number of equivalent pairs in the row AV system. It is clear that in contrast of severe conflicts between local classifications, the con-sensus classification achieves the maximum agreement with each local classification. This is explained by that for each local classifi-cation, the consensus classification inherits all its classification re-sults that are missed by other local classification and that are agreed by a majority of others. This also implies that the consensus clas-sification is able to serve as an ideal common ground for different AV systems to share malware information.
Next we demonstrate the application of Latin in three concrete use cases and evaluate its empirical efficacy.
 Use Case 1: Searching Others X  Systems Using Your Own Words One functionality crucial for information sharing between AV sys-tems is to support the retrieval of content from one system using the terms of another. For example, the analyst may wish to search dif-ferent AV systems for information relevant to a particular malware variant using any of its available information (e.g., name, alias, be-havior profile, etc.). We show how well Latin supports this sce-nario in terms of its efficacy of retrieving names of given malware instances in other AV systems, which can be easily generalized to other types of malware information.

Specifically we randomly sample 60% (144,918) of malware in-stances from the dataset created in the preparation phase together with their behavior profiles for training and regard the remainder 40% (96,612) as the test set (we will discuss the impact of training data size over the evaluation results shortly). For comparison pur-pose, we implement a baseline method that directly uses names of query instances in search ( Baseline ) and three variants of Latin : one that leverages both names of query instances and the consensus classification ( Latin L ), one that exploits both names and aliases in-formation as well as the consensus classification ( Latin A ), and one that further takes account of behavior profiles of query instances ( Latin P ). Particularly in Latin A and Latin P , for each instance, only one of its aliases (randomly selected) is used.

For each pair of AV systems we generate 5,000 retrieval queries at random, wherein each query is an malware instance with its name from the source system, its alias in another system (different from the target system), and its behavior profile, while the ground truth is its name in the target system. Recall that rather than suggest-ing a single name in the target system, Latin gives a probability distribution over all names of the target system representing their likelihood of being the true name for the query instance. We rank these names in descending order in terms of their likelihood and pick the top ones to form the candidate list. We measure the accu-racy of Latin in answering retrieval queries using the cumulative rate that the correct name appears in the candidate list as the size of the candidate list grows. It is worth emphasizing that the retrieval accuracy is inherently bounded by the naming discrepancy between two systems, as there may not exist an equivalent counterpart in an-other system for a given name. True Positive Rate
Figure 9 illustrates how the retrieval accuracy of different mod-els increases as the size of candidate list varies from 0% to 5% of the total number of names (ranging from 393 to 500) of the target system. As expected, the models that leverage the consensus classi-fication ( Latin L , Latin A , and Latin P ) all significantly outperform the baseline method ( Baseline ) that only compares names syntac-tically. For example, when the size of candidate list is fixed as 2.5% (about 10 names), all variants of Latin achieve accuracy ranging from 75% to 97% in all the test cases. Meanwhile it is observed that the incorporation of alias and behavior profile information im-proves the retrieval accuracy over the basic version. For example, in the case of S 1 -S 4 , the first guess of Latin P achieves accuracy and Latin L , respectively.
 Use Case 2: Caliberating Measures of Malware Analysis Tools Due to lack of standard benchmarks for AV systems, currently it is still fairly difficult to interpret performance measures of mal-ware analysis tools evaluated over self-composed test sets. Here we show that with the help of Latin it is now possible to bench-mark the performance measures of these tools provided that only the (local) malware names of the test sets are available.
Specifically we consider the case of malware clustering which groups malware instances exhibiting similar behavior patterns. Typ-ically the quality of clustering is measured by precision and recall . Precision measures the exactness of clustering, i.e., how well the clustering divides dissimilar malware into different clusters; while recall measures the completeness of clustering, i.e., how well the clustering groups similar malware into a same cluster. Formally let ( C reference classes respectively, we have ( n is the total number of malware instances): Precision =
Recall =
We apply Malheur [24], one of the state-of-the-art malware clus-tering tools, to analyzing our malware corpus and use names given by each AV system as reference. Conceivably the quality mea-sures vary significantly with such reference names. Table 4 lists the precision and recall scores of Malheur with respect to each AV system. It is observed that even with the same clustering results, the precision and recall scores vary from 0.37 to 0.48 and from 0.63 to 0.70, respectively. Now by leveraging the local-consensus mappings, one can  X  X ecalibrate X  the performance measures in the space of consensus classification. More specifically, we construct the calibrated reference as follows. For each instance classified as class c in local classification, we consider it is classified as c consensus classification according to the definition of forward cor-respondence [ F ] c,c  X  . The accuracy measures after calibration are listed in Table 4. It is clear that both precision (from 0.32 to 0.34) and recall (from 0.65 to 0.68) scores are more consistent across all four AV systems.
 Table 4: Clustering accuracy of M alheur measured against differ-ent reference systems before and after calibration.
 Use Case 3: Integrating Reports by Multiple AV Systems It is not uncommon in today X  X  enterprise networks that multiple AV systems are deployed together to improve the protection. How-ever the task of integrating the results reported by these systems is nontrivial [5]. For example, the analyst may wish to understand whether two instances detected by two systems essentially belong to a same malware class. To this end, we introduce the functionality of matching , which determines the similarity of malware instances (reported by different AV systems) based on any of their available information (e.g., names, aliases, behavior profiles).
 We follow the same partitioning of training and test data as in Use Case 1. For each pair of AV systems, we use malware instances in the test set to randomly generate 5,000 positive and 5,000 neg-ative cases, in which a positive case comprises the aliases of an instance in the two AV systems, while a negative case consists of a pair of names corresponding to two distinct instances from the two systems. Note that we use instance-level ground truth (whether two instances are identical) to evaluate Latin in determining whether two instances belong to the same class; thus the false positive and false negative rates here are overestimation.

We use receiver operating characteristic (ROC) curves to mea-sure the matching accuracy of Latin . Intuitively the ROC curve shows the relationship of true positive and false positive rates of a classifier as the discrimination threshold varies. We set the dis-crimination threshold on the distance between the projection of two instances in the space of consensus classification (see  X  4.2), which varies from 0 to 1. Similar to Use Case 1, we implement a base-line method ( Baseline ) and two variants of Latin , Latin L and Latin A , where Baseline uses the Jaccard coefficient as the simi-larity metric (Eqn.(1)). Here we focus on the name/alias informa-tion and consider comparing two malware instances based on their behavior profiles as an ongoing direction.

Figure 10 illustrates the ROC curves of different matching mod-els for each pairs of AV systems. It is observed that in all the cases It is also noticed that incorporating alias information considerably improves the matching accuracy. For example, with false positive Figure 11: Impact of training data size over matching and retr ieval. rate fixed as 0.05, true positive rates of Latin L and Latin A dif-fer about 0.3 in most cases. This is explained by the diverse ex-pertise of different AV systems: combining evidences from multi-ple systems generally helps more accurately pinpoint the classes of query instances in the consensus classification. The only exception is the case of S 3 -S 4 wherein Latin A performs slightly worse than cal classification (Table 3). For example, S 3 is much more consis-tent with S 4 than S 2 on malware classification; thus taking account of their aliases in S 2 when matching names from S 3 and S slightly  X  X ilute X  the matching accuracy. This exception however is well justified by the overall superior performance of Latin A . The ROC curves also shed light on how to set the optimal discrimination threshold for name matching models to determine the equivalence of two names. We can select the  X  X lbow point X  of an ROC curve as the threshold for it maximizes the ratio of true positive over false positive rates. For instance in the case of S 4 -S 2 , we may choose the threshold corresponding to the false positive rate around 0.05. Impact of Training Data We now discuss the impact of training data on the quality of query processing by Latin . Due to space limitations, we focus our dis-cussion on the size of training data. Figure 11 shows the perfor-mance of different matching and retrieval models as the size of training data varies from 20% to 80% of the entire malware col-lection. In particular, Figure 11 (a) illustrates the true positive rates of Baseline , Latin L , and Latin A , where the false positive rate is fixed as 0.2 and the results are averaged over every pair of AV sys-tems. It is observed that the true positive rate of Baseline stays around 0.42 as it is not subject to the consensus classification, while the performance of both Latin L and Latin A improve significantly as the training data size grows from 20% to about 50%, which is explained by that during this early stage the coverage of consen-sus classification over all the classes of participating AV systems increases fast. After this coverage reaches a certain threshold, both 0.8 respectively. We can thus conclude that (i) Latin is fairly tol-erant to the availability of training data and (ii) in the case that training data is extremely limited we can complement Latin with syntactic-based approaches. Similar conclusions can be drawn for the case of retrieval in Figure 11 (b) as well.
 Operation Scalability Finally we measure the scalability of Latin in processing queries. We evaluate the average processing time for each incoming query, under varying size of consensus classification, with and without ad-ditional threat intelligence. The size of consensus classification is controlled by the size of training set. Note that here we only evalu-a te the processing time for queries that require nontrivial folding-in (with class names not appearing in the local classification). Fig-and Latin P in matching and retrieval tasks, respectively, as the size of consensus classification increases. We break down the process-ing time into four parts, name processing, alias processing, behav-ior profile processing (corresponding to folding-in), and matching (or retrieval). We observe that (i) overall Latin scales almost lin-early with the size of consensus classification; (ii) the execution time for matching or retrieval is barely noticeable; (iii) the behav-ior profile processing accounts for the major overhead as it involves LDA operation.
The issues of malware naming discrepancies have been debated in security research communities for decades [1, 5, 12, 14, 20]. Bailey et al. [1] measured such inconsistency in terms of the ca-pability of AV systems to identify similar or identical malware in the same way and found that consistency is never a design goal of most AV systems, e.g., they assign the same name to identically be-haved malware only from 31% to 61% of the time. Recently Maggi et al. [20] quantitatively showed that high degrees of inconsistency in terms of naming distance and scatter score exist across different AV systems. Many factors may account for the malware naming discrepancies. The first and foremost may be the exponentially es-calated number of new malware instances created nowadays [4, 12, 14], which makes it practically impossible to examine every new malicious program and to agree on a reasonable name. Second, AV systems increasingly rely on generic signatures and other heuristics (behaviors, rare packers, etc.); ad hoc rules for naming the detected malware instances are often used and the cost is prohibitive to cor-rect or change such names afterwards [4].

Several existing attempts have been made to standardize the dis-parate naming conventions in well defined formats (e.g., platform, family, group, major variant, minor variant ) [4, 10, 25]. Unfor-tunately most major AV system vendors were reluctant to accept these proposals. Even if they did, the naming discrepancy would still exist, as there are no regulations on how these names would be semantically used. VGrep 7 and VirusTotal offer online services for users to search names from different AV systems for given mal-ware instances, yet providing no systematic approaches to reconcile the inconsistent or even conflicting names. To our best knowledge, this work represents one of the first attempts to systematically re-solve the malware naming discrepancies from both syntactic and semantic perspectives.

One line of research related to our work is schema matching [2] which studies how to reconcile schemas of different data sources. Particularly rule-based methods [11, 19, 21] exploit schema-level information (e.g., element names, data types) to discover similar elements in different schemas, while learning-based methods [9, 26] apply machine learning techniques over both schema-and d ata-level information to identify correspondence between schemas. An-other line of related work is object matching (also known as record linkage) [22, 6] which attempt to find records referring to the same entity across different relational databases. Our work differs in that it addresses not only disparate syntactic representation but also con-flicting classification by different AV systems.

There are several other lines of work we build upon. Consensus learning has been applied to malware behavior analysis [27, 17], while power iteration method has been applied to large-scale clus-tering [15]. To the best of our knowledge, the present work however is the first that considers the application of power iteration method in the context of consensus learning.
The malware naming discrepancies across different AV systems remain one potent barrier that prevents these systems from effec-tively sharing malware intelligence. To the best of our knowledge, this work represents one of the first few attempts of systematically reconciling both syntactic and semantic naming discrepancies at a large scale. We presented Latin , a novel classification integra-tion framework which exploits the correspondence between differ-ent AV systems reflected in various heterogenous information and creates optimal consensus classification to support cross-system in-formation sharing. We implemented and evaluated a prototype sys-tem, which demonstrates the potential of this methodology to rec-oncile malware naming discrepancy issues to a certain extent. [1] M. Bailey, J. Andersen, Z. M. Mao, and F. Jahanian.
 [2] P. A. Bernstein, J. Madhavan, and E. Rahm. Generic schema [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [4] V. Bontchev. Current status of the caro malware naming [5] P.-M. Bureau and D. Harley. A dose by any other name. In [6] S. Chaudhuri, K. Ganjam, V. Ganti, and R. Motwani. Robust [7] CNET. Most popular security software: [8] Damballa. Integration partners: [9] A. Doan, P. Domingos, and A. Y. Halevy. Reconciling [10] N. FitzGerald. A virus by any other name: Towards the [11] F. Giunchiglia and P. Shvaiko. Semantic matching. Knowl. [12] D. Harley. The game of the name malware naming, shape [13] J. A. Hartigan. Clustering Algorithms . John Wiley &amp; Sons, [14] T. Kelchner. The (in)consistent naming of malcode.
 [15] F. Lin and W. W. Cohen. Power iteration clustering. In [16] J. Lin. Divergence measures based on the shannon entropy. [17] B. Long, Z. M. Zhang, and P. S. Yu. Combining multiple [18] B. Luo, R. C. Wilson, and E. R. Hancock. Spectral clustering [19] J. Madhavan, P. A. Bernstein, and E. Rahm. Generic schema [20] F. Maggi, A. Bellini, G. Salvaneschi, and S. Zanero. Finding [21] S. Melnik, H. Garcia-Molina, and E. Rahm. Similarity [22] H. B. Newcombe and J. M. Kennedy. Record linkage: [23] M. D. Preda, M. Christodorescu, S. Jha, and S. Debray. A [24] K. Rieck, P. Trinius, C. Willems, and T. Holz. Automatic [25] G. Scheidl. Virus naming convention 1999 (vnc99). [26] T. Wang and R. Pottinger. Semap: a generic mapping [27] Y. Ye, T. Li, Y. Chen, and Q. Jiang. Automatic malware
We assume that v  X  = [ x 1 , . . . , x n ] T  X  R n has been sorted in non-descending order. Our goal is to partition v  X  into a set of tight clusters. Since v  X  is sorted, a cluster containing x s and x must also include { x j } t  X  1 j = s +1 . We use the squared deviation to measure the intra-cluster dispersion of a cluster of values: d [ s, t ] = P
Let w ( t, k ) denote the minimum sum of intra-cluster dispersion when we partition [ x 1 , . . . , x t ] into k clusters. The following opti-mal substructure exists:
Note that w ( n, k ) corresponds to an optimal k -clustering of v Meanwhile we are also interested in simultaneously determining the optimal k . We apply the Hartigan index [13] to evaluate the clustering quality, which is defined as
Intuitively h ( k ) captures the relative improvement when increas-ing the cluster number from k to k + 1 . The optimal cluster num-ber is the smallest k which produces h ( k )  X   X  (typically  X  = 10 ). Moreover, tnstead of computing w ( n, k ) for every possible k , the algorithm can stop immediately once it finds the minimum k satis-fying this condition.
