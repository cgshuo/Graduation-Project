 Since the presence of supervision in biological learning me chanisms is rare, organisms often have Recent neurobiological experiments [1] have suggested tha t the brain uses some type of slowness (SFA) [2] could be a possible mechanism for that. We establis h a relationship between the unsu-pervised SFA learning method and a commonly used method for s upervised classification learning: Fisher X  X  Linear Discriminant (FLD) [3]. More precisely, we show that SFA approximates the classi-adjacent samples in the input time series are likely to be fro m the same class. Furthermore, we demonstrate in simulations of a cortical microcircuit mode l that SFA could also be an important supports the idea of  X  X nytime X  computing, i.e., it provides information about the stimulus identity not only at the end of a trajectory of network states, but alre ady much earlier. SFA and FLD. We discuss the relationship between these metho ds for unsupervised and supervised Section 6 concludes with a discussion. 2.1 Slow Feature Analysis (SFA) Slow Feature Analysis (SFA) [2] is an unsupervised learning algorithm that extracts the slowest components y Thus the objective is to minimize The notation hi constraints of zero mean ( h y lution y ensures that they are decorrelated and ordered by decreasin g slowness, i.e., y extracted, y of a certain predefined function space that produce the slowe st possible outputs y these constraints.
 This optimization problem is hard to solve in the general cas e [4], but if we assume that the time series x has zero mean ( h x i simplifies to the objective The matrix h xx T i weight vector w which minimizes (2) is the solution to the generalized eigen value problem corresponding to the smallest eigenvalue  X  . To make use of a larger function space one typically considers linear combinations y = w T z of fixed nonlinear expansions z = h ( x ) and performs the optimization (2) in this high-dimensional space. 2.2 Fisher X  X  Linear Discriminant (FLD) Fisher X  X  Linear Discriminant (FLD) [3], on the other hand, i s a supervised learning method, since from the value of w T x is maximized.
 FLD searches for that projection direction w which maximizes the separation between classes while projected values: For C point sets S matrix given by the separation of the class means, S within-class covariance matrix given by S w optimizing (4) can be viewed as the solution to a generalized eigenvalue problem, corresponding to the largest eigenvalue  X  . SFA and FLD receive different data types as inputs: unlabele d time series for SFA, in contrast to labeled single data points for the FLD. Therefore, in order t o apply the unsupervised SFA learning algorithm to the same classification problem as the supervis ed FLD, we have to convert the labeled training samples into a time series of unlabeled data points that can serve as an input to the SFA both methods for a particular way of time series generation. S number of points. In order to create a time series x random point from the class that corresponds to the current s tate in the Markov model. We define the transition probability from state i  X  S to state j  X  S as with some appropriate constant a &gt; 0 . The stationary distribution of this Markov model is  X  = ( N 1 /N, N 2 /N, . . . , N C /N ) probability that point x express the matrices h xx T i between-class scatter matrices of the FLD (4), S Note that only h  X  x  X  x T i For small a we can neglect the effect of S the two point sets, and both matrices become approximately p roportional: h  X  x  X  x T i Moreover, if we assume that S objective (2) as min J SF A ( w )  X  max That is, the weight vector that optimizes the SFA objective ( 2) also optimizes the FLD objective eigenvalue problem (3) and inserting (7) and (8): where W = ( w a generalized eigenvalue problem (5). More precisely, the e igenvectors of the SFA problem are also eigenvectors of the FLD problem. Note that the eigenvalues c orrespond by  X  F LD which means the order of eigenvalues is reversed (  X  SF A slowest features is the same that optimizes separability in terms of Fisher X  X  Discriminant, and the slowest feature is the weight vector which achieves maximal separation.
 Figure 1A demonstrates this relationship on a sample two-cl ass problem in two dimensions for the special case of N p = a/ 2 or is left unchanged with probability 1  X  p . We interpret the weight vectors found by both methods as normal vectors of hyperplanes in the input sp ace, which we place simply onto the mean value  X  of all training data points (i.e., the hyperplanes are define d as w T x =  X  with  X  = w T  X  ). One sees that the weight vector found by the application of SFA to the training time series x the slowest varying feature by finding a direction that separ ates both classes. Figure 1: Relationship between SFA and FLD for a two-class pr oblem in 2D. ( A ) Sample point sets with 250 points for each class. The dashed line indicate s a hyperplane corresponding to the weight vector w solid line shows a hyperplane for the weight vector w dotted line displays an additional SFA hyperplane resultin g from a time series generated with p = the error between the weight vectors found by FLD and SFA on th e switching probability p . This error is defined as the average angle between the weight vecto rs obtained on 100 randomly chosen classification problems. Error bars denote the standard err or of the mean.
 approximates FLD very well. The angle increases moderately with increasing p ; even with higher values of p (up to 0.45) the approximation is reasonable and a good class ification by the slowest are chosen independently of their class, making the matrice s h  X  x  X  x T i in an average angle of about 45  X  . In the previous section we have shown that SFA approximates t he classification capability of FLD are chosen independently at each time step. In this section w e investigate how the SFA objective individual points only.
 First, we consider a time series x  X  t , which is embedded into noise input consisting of a random nu mber of points drawn from the  X   X  in the time series x such instances with a distinctive shape.
 Next, we consider a classification problem given by C sets of trajectories, T i.e., the elements of each set T series according to the same Markov model as described in the previous section, except that we do not choose individual points at each time step, rather we g enerate a sequence of trajectories. For this time series we can express the matrices h xx T i and between-class scatter of the individual points of the tr ajectories in T (8) [6]. While the expression for h xx T i this matrix additionally depends on the temporal covarianc e  X   X  trajectories in all sets T Whenever a trajectory is selected,  X  T points from the same class are presented in succession. duced to the FLD objective, but rather that there is a trade-o ff between the tendency to separate trajectories of different classes (as explained by the rela tion between S to produce smooth responses during individual trajectorie s (determined by the temporal covariance matrix  X   X  of FLD to the classification problem of the individual trajec tory points (note that S through h xx T i for the resulting w ) the slowest feature will be similar to the weight vector fou nd by FLD on the this case, (11) is dominated by the second term for the result ing w ). In the following we discuss several computer simulations of a cortical microcircuit of spiking neu-rons that demonstrate the theoretical arguments given in th e previous section. We trained a number of linear SFA readouts 3 on a sequence of trajectories of network states, each of whic h is defined space [7], thereby boosting the expressive power of the subs equent linear SFA readouts. Note, how-from experimental data [9, 10].
 Figure 2: Detecting embedded spike patterns. ( A ) From top to bottom: sample stimulus sequence, response spike trains of the network, and slowest features. The stimulus consists of 10 channels and is defined by repetitions of a fixed spike pattern (dark gra y) which are embedded into random Poisson input of the same rate. The pattern has a length of 250ms and is made up by Poisson spike trains of rate 20Hz . The period between two repetitions is drawn uniformly betw een 100ms and 12th neuron is plotted. Shown are the 5 slowest features, y integration,  X  = 100ms ) of individual slow features in response to a test sequence o f 50 embedded input). filter with  X  = 30ms and a sample time of 1ms ). The period of Poisson input in between two such patterns was randomly chosen.
 At first glance there is no clear difference in Figure 2A betwe en the raw SFA responses during However, we found that on average the slow feature responses during noise input are zero, whereas 2B) that the slow features span a subspace where the response during pattern presentations can be within the continuous input stream. Furthermore, this extr acted information is not only available the idea of  X  X nytime X  computing.
 In the second experiment we tested whether SFA is able to disc riminate two classes of trajectories algorithm in [15]) that serve as input to our microcircuit mo del (see Figure 3A). We tried to dis-Each cochleagram has 86 channels with analog values between 0 and 1 (white, near 1; black, near the black spike times correspond to the cochleagram shown ab ove). The response spike trains of the laminar circuit from [8] are shown separated into layers 2/3, 4, and 5. The number of neurons responses to the two stimulus spike trains in the panel above are shown superimposed with the cor-responding color. Each readout trace corresponds to a weigh ted sum (  X  ) of network states of the traces of readouts trained by FLD and SVM with linear kernel t o discriminate at any time between y rection of the respective feature from the direction found b y FLD. The thick curves in the shaded mean response over all test trajectories). remaining training samples a sequence of 100 input samples, recorded for each sample the response Rather, we trained linear SFA readouts on a completely rando m trajectory sequence. Figure 3B shows the 5 slowest features, y  X  X wo X . In this example, already the slowest feature y slowest feature y identity. One can say that, in principle, y location X  regardless of the identity of the pattern. The oth er slow features y of the FLD. It can be seen in Figure 3B that the slowest feature y according to (11), this constitutes an example where the sep aration between classes dominates, but is already significantly influenced by the temporal correlat ions of the circuit trajectories. Figure 3C shows phase plots of these slow features shown in Fi gure 3B plotted against each other. In the three plots involving feature y (nonlinear) SFA network trained with a sequence of one-dime nsional trajectories [2]. This experiment demonstrates that SFA extracts informatio n about the spoken digit in an unsuper-sufficient training, the slowest feature y supports the idea of  X  X nytime X  computing. It can be seen in th e bottom panel of Figure 3A that the slowest feature, which is obtained in an unsupervised manne r, achieves a good separation between the two test trajectories, comparable to the supervised met hods of FLD and Support Vector Machine (SVM) [16] with linear kernel. The results of our paper show that Slow Feature Analysis is in fact a very powerful tool, which is ship to the supervised method of Fisher X  X  Linear Discrimina nt (FLD). A more detailed discussion of jectories of network states information about the stimulus  X  without any  X  X eacher X , whose existence is highly dubious in the brain. We have shown in computer simu lations of a cortical microcircuit Moreover, SFA provides in these tasks an  X  X nytime X  classific ation capability.
 Acknowledgments We would like to thank Henning Sprekeler and Laurenz Wiskott for stimulating discussions. This paper was written under partial support by the Austrian Scie nce Fund FWF project # S9102-N13 and project # FP6-015879 (FACETS), project # FP7-216593 (SE CO) and project # FP7-231267 (ORGANIC) of the European Union. References
