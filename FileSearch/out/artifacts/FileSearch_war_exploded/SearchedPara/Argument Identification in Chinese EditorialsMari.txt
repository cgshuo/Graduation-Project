 Argumentation  X  the act of reasoning in support of an opinion or idea  X  frequently presents itself in all types of texts, from casual chat messages to online blogs. Argumentation mining aims to identify and determine a persuasive text X  X  argumentative compo-nents, or the atomic units of its underlying struc-ture. For example, an argumentation mining system might seek to locate and classify sections of claims and supporting evidence within an essay. More com-prehensive mining might map the relations between different units, such as the support of evidence or the opposition of counterarguments to the thesis.
Argument identification offers a wide variety of practical applications. If argumentative text can be identified accurately, then the main arguments of large sets of data may be extracted. For exam-ple, argument identification could isolate arguments surrounding subjects like U.S. immigration law, or summarize the arguments in research papers. Recent efforts in argumentation mining have included appli-cations such as automatic essay scoring (Song et al., 2014; Ong et al., 2014), online debates (Boltuzic and  X  Snajder, 2014), and arguments in specific domains such as online Greek social media sites (Sardianos et al., 2015). However, to our knowledge, no work in argumentation mining to date has been performed for Chinese, a large and rich domain for NLP work.
Here we focus on the first step of argumenta-tion mining, locating argumentative units within a text. We develop and evaluate several methods of argument identification when performed upon a cor-pus of Chinese editorials, making the assumption that editorials are opinionated texts, although a sin-gle editorial may contain both opinionated and non-opinionated paragraphs.

Our work met with several challenges. Although newspaper editorials can be assumed to carry an opinion of some sort, the opinion is not always ex-plicitly expressed at a word level, and methods of argumentation can vary widely from editorial to ed-itorial. For example, one might exhibit a thesis fol-lowed by supporting evidence, but others might only state facts until the final paragraph. Furthermore, ed-itorials commonly build arguments by citing facts. In our work, we not only had to define  X  X rgumen-tative X  and  X  X on-argumentative X , but also limit the scope of an argument. In order to capture the larger argument structure, our work focuses on identifying arguments in paragraph units of no more than 200 characters (around 3-5 sentences), although we do not concatenate shorter paragraphs to ensure a min-imum size.

Our task aims to label paragraphs such as the fol-lowing as argumentative:  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  X  X   X   X   X   X   X   X   X   X   X   X   X  ( X  X n-fortunately, nowadays at Shenzhen X  X  ten road inter-sections, cords are used to show that the white road lines lie uselessly on the ground. This legislation is tragic. X )
The contributions of this paper are collecting and annotating a dataset of Chinese editorials; manually creating a list of argumentative words; and the com-parison and analysis of three methods. 2.1 Corpora This work makes use of two corpora, one of Chi-nese editorials and one of Chinese reportage, both of which are subcorpora in the Lancaster Corpus of Mandarin Chinese (McEnery and Xiao, 2004). The LCMC, a member of the Brown family corpora, is a 1M balanced word corpus with seventeen sub-corpora of various topics. We used the Press: Re-portage and Press: Editorials subcorpora, contain-ing 53K and 88K words respectively. Samples in both subcorpora were drawn from mainland Man-darin Chinese newspaper issues published between 1989 and 1993, which increases the likelihood that both articles and editorials discuss the same topics and use a similar vocabulary.

Our unit of text was the paragraph, which typ-ically contains a single argument or thought. We decided to use paragraphs that were no more than 200 characters in our experimentation, assuming that longer paragraphs might hold multiple argu-ments. We split our raw data into two subsets: para-graphs 200 characters and below, and paragraphs larger than 200 characters. The small paragraphs were left in their original form, but we manually split the larger paragraphs into small sections under 200 characters, with individual paragraphs no smaller than 50 characters. We omitted large paragraphs which cannot reasonably be split up into sentences (for example, a long one-sentence paragraph). 2.2 Gold Standard Annotation To evaluate our experiments, we employed work-ers through Amazon Mechanical Turk to tag our set of 719 editorial paragraphs. For each para-graph, the worker was asked,  X  X oes the author of this paragraph express an argument? X  In response, the worker categorized the paragraph by selecting  X  X akes argument, X   X  X akes NO argument, X  or  X  X n-sure X . All text shown to the worker was written in both English and manually translated Mandarin. Instructions were screened by native speakers for clarity. Each paragraph was rated by three  X  X aster Workers, X  distinguished as accurate AMT workers.
Though we provided clear instructions and exam-ples for our categorization task, we found that the three workers for each task often did not all agree on an answer. Only 26% of paragraphs received an unambiguous consensus of  X  X as argument X  or  X  X o argument X  for the paragraph X  X  argumentative nature. The rest of the paragraph results contain at least two different opinions about the paragraph. Since paragraphs receiving three different answers were likely unreliable for identification, we threw out those paragraphs, leaving 622 paragraphs for our methods. Around 78% of paragraphs were rated as argumentative, and 22% as non-argumentative. Paragraph Consensus Count Percentage Makes an argument 484 67.32% Makes NO argument 138 19.19% Unsure 43 5.98%
No consensus 54 7.51% total 719 We first score paragraphs according to the meth-ods outlined below. Then, we determine the best score threshold for each method, and accord-ingly label paragraphs  X  X rgumentative X  or  X  X on-argumentative. X  3.1 Method 1: Identification by Comparative Our first method of evaluation is based on a process outlined by Kim and Hovy in a paper on identifying opinion-bearing words (Kim and Hovy, 2005). We first follow Kim and Hovy X  X  process to construct a list of word-score pairs. Then, we use these scores to evaluate our paragraphs of editorial text.
Kim and Hovy postulate that words which appear more often in editorials than in non-editorial text could be opinion-bearing words. For a given word, we use the Reportage and Editorials subcorpora to find its unigram probabilities in both corpora, then compute a score that indicates its frequency bias to-ward editorial or reportage text. Words that are rela-tively more frequent in editorial text are more likely argumentative.

Kim and Hovy further specify a way to elimi-nate words which do not have a repeated bias to-ward editorial or reportage text. We divide the Reportage and Editorial corpora each into three subsets, creating three pairs of reportage and ed-itorial subsets. Then, for each word, we com-pute word scores as specified above, but for each pair of reportage and editorial subsets. This cre-ates Score 1 ( W ) , Score 2 ( W ) , Score 3 ( W ) , which are essentially ratios between editorial or reportage appearances of a word. We only retain words whose scores are all greater than 1 . 0 , or all below 1 . 0 , since this indicates repeated bias toward either editorials or reportage (opinionated or non-opinionated) text.
After scoring individual words, we rate para-graphs by assigning a score based on the scores of the individual words which comprise them. If a paragraph P contains n opinion words with cor-responding frequency f 1 , f 2 , . . . f n and assigned scores s 1 , s 2 , . . . s n , then the score for the paragraph is calculated by following:
From these scores and our tagged data, we deter-mine a best score threshold by tuning on our tagged data, which produced a threshold of 40.0. 3.2 Method 2: Targeting Known Our second method involves creating a list of known argumentative words that appear in the Editorials corpus and scoring paragraphs based on how many of these words appear in them. First, we constructed a list of the most frequent argumentative words that appear in the Editorials corpus. Then, we assigned each paragraph a score based on presence of these words.

We manually selected the most frequent argumen-tative words in the Editorials corpus by sorting a list of the words and their frequencies. Words were selected for their likelihood of indicating argumen-tation. Generally, the most common words which indicated opinion also possessed non-argumentative meanings. For example, the common word  X   X   X  can mean  X  X o want X  as well as  X  X ust X  or  X  X f. X 
Scoring paragraphs based on this list was simple: we awarded a paragraph a point for each instance of any of the words on the list. We were inter-ested in whether the presence of a few argumenta-tive words could indicate argumentation in the entire paragraph. We determined the best word list size and the best threshold that returned the most accurate la-bels, a word list size of 15 words and a threshold of 1. For this model, a threshold of 1 means if the para-graph contains at least one word from the word list, it is labeled as argumentative. 3.3 Method 3: Combined Method Our third method of identifying arguments combines the previous two methods. Similar to the second method, we scored paragraphs based on a list of argumentative words. However, instead of manu-ally selecting argumentative words from a word fre-quency distribution, we created a list of opinionated words by picking a number of the highest-scoring words from the first method.

In structuring this combination method, we theo-rized that the highest-scoring words are those which are the most common opinionated words, since they have the highest probability of consistently appear-ing throughout the Editorials corpus and not the Re-portage corpus. By using these words instead of manually-picked argumentative words, we scored paragraphs using a list of words based on the compo-sition of the corpus itself, with the intent of creating a more relevant list of words.

Scoring remained similar to the second method, where we awarded a paragraph a point for each in-stance of any of the words on the list. Again, the threshold which produced the best results was 1. That is, if a paragraph contained at least one word from the list, it was labeled as argumentative. 4.1 Method 1: Identification by Comparative
Our experiments produced the best performance under the relative word frequency method, achieving 84% accuracy and an F 1 score of 0.91. These scores were closely followed by the second method with 80% accuracy and an F 1 score of 0.88.

Despite these high scores, we were surprised to find that our relative word frequency system had scored many non-argumentative words very high. For example, the highest-scoring word was  X   X   X  ,  X  X iberal Democratic Party. X  When we eliminated words with non-argumentative POS tags (i.e. nouns and other noun forms), the highest-scoring word was  X   X  ,  X  X o monitor X  (Table 4). These words were likely rare enough in the Reportage corpus that they were awarded high scores. Words indicative of ar-guments such as  X   X  ,  X  X ecessary, X  scored high, but largely did not make up the highest-scoring words.
As a result, our list of opinion-bearing words con-tained non-argumentative words along with argu-mentative identifiers, artificially raising paragraph scores. Paragraphs were more likely to score high, since our system labeled many non-argumentative paragraphs as argumentative. The inflated word scores are likely a result of mismatched editorial and reportage corpora, since a word that is rela-tively rare in the Reportage corpus and more com-mon in the Editorials corpus will score high, re-gardless of its actual meaning. However, this ap-proach still performed well, suggesting that these non-argumentative words, such as  X  X o monitor, X  may be used to persuade in context (e.g.  X  X he govern-ment monitors its people too closely X ). 4.2 Method 2: Targeting Known Our second method similarly performed well, with high accuracy and fewer false positives than the pre-vious method, due to the list of words that clearly indicated argumentation. The best performance was given by a threshold of 1. That is, the system per-formed best when it marked a paragraph argumenta-tive as long as it has at least one of the words from the list. Results did not significantly improve even if the list was expanded or the score threshold was raised, implying that adding words to the 10-word list, even if the new words had exclusively argumen-tative meanings, did not significantly improve per-formance. The more frequent semi-argumentative words like  X   X   X  ( X  X nd so, X   X  X et X ) had a greater posi-tive effect on accuracy than obviously argumentative words like  X   X   X   X  ( X  X ust X ) which do not appear as often in the corpus. 4.3 Method 3: Combined Method Since our combined method relied heavily upon the word scores generated by the relative word frequency approach, the results showed signifi-cant errors. Seeded with a word list that did not contain solely argumentative words (e.g.  X  X o monitor X  as well as  X  X o pollute X ), the combined method attempted to find argumentative paragraphs using words which did not exclusively indicate argumentation. Overall, the combined method rated many more argumentative paragraphs as non-argumentative than the reverse, and performed poorly overall with a F 1 score of 0.37. Prior work on argument identification has been largely domain-specific. Among them, Sardianos et al. (2015) produced work on argument extraction from news in Greek, and Bolt  X  uzic and Jan  X  Snajder (2015) worked on recognizing arguments in online discussions. Kiesel et al. have worked on a shared task for argument mining in newspaper editorials (Kiesel et al., 2015). They contributed a data set of tagged newspaper editorials and a method for mod-eling an editorial X  X  argumentative structure.
Because argument mining is a relatively new field within the NLP community, there has been no argu-ment identification study performed on Chinese edi-torials, although there has been a significant amount of work on opinion identification. In particular, Bin Lu X  X  work on opinion detection in Chinese news text (Lu, 2010) has produced a highest F-measure of 78.4 for opinion holders and 59.0 for opinion targets. In this study, we sought to computationally iden-tify argumentative paragraphs in Chinese editorials through three methods: using relative word frequen-cies to score paragraphs; targeting known argumen-tative words in paragraphs; and combining the two methods. Our experiments produced the best per-formance under the relative word frequency method, achieving 84% accuracy and an F 1 score of 0.91.
Despite these high scores, we found our rel-ative word frequency system scored many non-argumentative words very high. These words were likely rare enough in the Reportage corpus (but com-mon enough in the Editorials corpus) that they were awarded high scores. As a result, our list of opinion-bearing words contained non-argumentative words along with argumentative identifiers, raising para-graph scores and producing false positives.

Future work could be done to improve upon Kim and Hovy X  X  method in order to more accurately score words. In particular, it is necessary to avoid scor-ing non-argumentative words high, simply due to their presence in the Editorials corpus and absence in the Reportage corpus. In our experiment, we eliminated high-scoring non-argumentative words like  X   X   X   X   X  ( X  X iberal Democratic Party X ) by re-moving nouns from scoring. However, this also eliminates argumentative nouns like  X   X   X  , X  mean-ing  X  X roblem X  or  X  X ssue. X  One solution to remov-ing topic-specific nouns while keeping argumenta-tive nouns is identifying the editorial topic and its domain-specific words, which would prevent the method from scoring rare but non-argumentative words high. Another benefit to determining word context is distinguishing between argumentative and non-argumentative senses of a word. For example, if the word  X  X arbage X  appears in an article discussing landfills, it is likely not argumentative. However, if it appears in an editorial discussing recent movies, it is more likely to be an argumentative word (e.g.  X  X hat movie is garbage. X ). In our current system, we cannot distinguish between these two uses. If the word  X  X arbage X  appeared equally in news text as a neutral word (say, in an article discussing land-fills) and in the editorials corpus as an argumentative word ( X  X hat X  X  garbage! X ), then the score of  X  X arbage X  would be low, and we would be unable to identify the argumentative nature of  X  X arbage X  in editorials. Another solution is to observe the context in which words appear. If the word  X  X arbage X  appears in proximity to words like  X  X andfill X  and  X  X ecycling, X  then we could guess that the usage of this word is non-argumentative.

By improving the list of opinionated words in the relative word frequency method, we could not only improve its scoring system, but perhaps even im-prove upon our combined method to produce even better, more accurate results than the first two meth-ods used. We hope our research provides a bench-mark or foundation for future research in the grow-ing field of argument mining.
 I would like to thank the Princeton University De-partment of Computer Science for their generous support in the funding of this project, without which this work would not be possible. I would also like to thank Professor Christiane Fellbaum for her invalu-able guidance and advice throughout this project.
