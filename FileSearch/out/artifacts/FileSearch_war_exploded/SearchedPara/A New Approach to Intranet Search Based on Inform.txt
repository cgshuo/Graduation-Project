 accesses from a bout 500 employees per month. Feedbacks from users results of the system. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process ; I.7.m [ Document and Text Processing ]: Miscellaneous Factors Keywords: Intranet search, information extraction, metadata extraction, expert finding, definition search 15% to 35% of their work time on searching for information and information they need to do their jobs on their company intranets. relevance search, when a user types a query, the system returns a list of ranked documents with the most relevant documents on the top. definitions of a term and experts on a topic. The characteristic of commercial systems. be categorized into searches for information in different types. An verified the correctness of the assumption. The advantage of the new search approach lies in that it can help intranets, because in such space users are information workers and search needs are business oriented. find term definitions, homepages of groups or topics, employees X  intranets. to the system. Both the results of an analysis on a survey and the results also show that search by type is necessary at enterprise. intranets at enterprises have tens or even hundreds of times larger Much effort has been made recently on solutions both in i ndustry and in academia. Many commercial systems [35, 36, 37, 38, 39] dedicated to intranet search as a problem of conventional relevance search. approaches, and evaluation methodologies on intranet search have been proposed. Hawking et al [17] made ten suggestions on how to conduct high search were raised in their paper. company intranet. They developed methods that can automatically identify experts in an area using documents on the intranet. intranet search tools. usually single answers to the questions. When the answer is a personal name, a time expression, or a place name, the QA task is called  X  X actoid QA X . Many QA systems have question expansion, passage retrieval, answer ranking, and answer creation. single combined text [1, 11, 15, 33, 34]. A typical system consists generation. other hand, Question Answering (QA) is an ideal form for question or a query (a combination of keywords) as a description adopting a different approach. them in a single system. For the QA part, we can employ summarize the results in advance. This is exactly the proposal we make to intranet search. Can we categorize users X  search needs easily? We have found that will be explained in section 4. On intranets, users are information workers and their motivations first. (There is no reason why we cannot apply the same approach to the internet, however.) month. At Information Desk, we try to solve the most important types of them with the conventional relevance search. We will explain the working of Information Desk in section 5. In this section, we describe our analyses on intranet search needs using search query logs and survey results. In order to understand the underlying needs of search queries, we Obviously, this is not feasible. We conducted an analysis by using query log data. Here query log data means the records on queries the queries. their work, they categorized the search needs of users on internet by analyzing search query logs. requests users may have when conducting search on an intranet. Then, we modified the categories, including adding, deleting, and merging categories, by assigning queries to the categories. underlying search need: z the query itself z the documents returned by the search engine z the documents clicked on by the user homepage of .net, then we judged that the user was looking for a homepage of .net.
 conducting search are business oriented. the traditional relevance search. Many search needs are by nature difficult to be categorized, for example,  X  X  want to find documents category. We think that the search needs are not Microsoft specific; one can image that similar needs exist in other companies as well. We have randomly selected 200 unique queries and tried to assign categories. Table 2 shows the distribution. (There is no result for query logs.) For random queries, informational needs are dominating. For high important types for random queries are relevance search, personal information search, and manual search. The most important types search. log data cannot reveal users X  potential search needs. For example, searches from query log at a conventional search engine, because users understand that such search is not supported and they do not conduct the search. (i.e., it only asks people to answer pre-defined questions and thus different perspective. Category of Search Needs Percentage Table 2. Distribution of search needs for high frequency queries Category of Search Needs Relative Prevalence look for the web sites (or homepages) of (multiple choice) z technologies z products z services z projects z groups z persons z none of the above choice) z  X  X hat is X  -e.g., "what is blaster" z  X  X ow to X  -"how to submit expense report" z  X  X here X  -e.g., "where is the company store" z  X  X ho knows about X  -e.g., "who knows about data mining" z  X  X ho is X  -e.g., "who is Rick Rashid" z  X  X hen X  -e.g., "when is TechFest'05 " z none of the above order to (multiple choice) z none of the above In the survey, we have asked questions regarding to search needs survey. Figure 2 shows the questions and the corresponding results. download site is also a common request. four types are: 1.  X  X hat is X   X  search of definitions and acronyms. Given a term, boxes are checked. snapshot in figure 3). Users can also search for homepages (team about data mining X  (the third snapshot in figure 3). Users can also (the last snapshot in figure 3). The top ten key terms found in his documents are also given. been extracted, are also available on the search result UIs. and domain specific knowledge from a web site using information definition, acronym, and expert. The document metadata includes Word, PowerPoint, or HTML. Information Desk stores all the $ 9 ! "! 4 9 % ) ! , M N ! 4 2 -V% K-V % L * )7+ K L -V% $ ! K L ! : ! K L &gt; &gt; 7 ) ! '&amp;&amp;' '&lt;&lt;1 % *77'777&amp;F7F9))+2 # ! "! ! ! -V% -*)F+ $ M N KM NL M N M N &gt; 52 " A " 2 B KA" 2BL6 B B C $ ! ! ! $# "$ % ( ;''' ;''' B B 'F;&amp; '7F; ! D $ &amp; K B B L ! 2 B P % *F)+ 3 ! D , B '&lt;1&amp; '&lt;== 2 8$ , respectively. previous work has been done on metadata extraction from general documents. We report our title extraction work in details in [19]. of information possible. from all the documents. In this way, we know how many times each text segments within a pre-determined window size. the list of ranked persons. A person who has several documents with name co-occurs with the topic in many documents. document collection we crawl. Users X  feedbacks on the results show they are not (due to the lack of information). Craswell et al. developed a system called  X  X @NOPTIC X , which can documents as we do at Information Desk. We identify homepages (team web sites) using several rules. Most of SharePoint, a product of Microsoft. From SharePoint, we can obtain 100%. term using the URL lengths of the home pages. A home page with a shorter URL will be ranked higher. TREC has a task called  X  X ome/named page finding X  [8, 9], which is to find home pages talking about a topic. Many methods have been consider employing a similar method. survey and by recording system logs. well, but the  X  X ho knows about X  feature still needs improvements. Information Desk. homepage of X  and  X  X hat is X  are regarded useful by the responders in the survey. correctness of our claim on intranet search made in section 4. results. About 50% of the responders really want to use Information Desk to search for information. the system. information? z  X  X here is homepage of X  -finding homepages z  X  X hat is X  -finding definitions/acronyms z  X  X ho is X  -finding information about people z  X  X ho knows about X  -finding experts Desk? (multiple choice) z  X  X ow to X  -e.g., "how to activate Windows" z  X  X hen X  -e.g., "when is Yukon RTM" z  X  X here X  -e.g., "where can I find an ATM" z  X  X hy X  -e.g., "why doesn't my printer work" z others I visited Information Desk today to z conduct testing on Information Desk z search for information related to my work Please provide any additional comments, thanks! z Extremely successful searching so far! Very nice product z Typing in my team our website doesn X  X  come up in the z ... information has been extraction, are given in search). The log data Microsoft employees. In the log, there are 9,076 query submission records. The records related to the  X  X hat is X  feature, 29% related to  X  X here is homepage is X . A query can be related to more than one feature. homepage of X ,  X  X ho knows about X , and  X  X ho is X  features are 694, 1041, 200 and 372, respectively. Note that for  X  X hat is X ,  X  X here is retrieved information. The top ranked results are considered to be the two features. As for  X  X ho is X , ranking of a person X  X  documents evaluated in a different way. (For example, precision and recall of metadata extraction as we have already reported in section 5). In this paper, we have investigated the problem of intranet search using information extraction.  X  Based on the finding, we propose a new approach to intranet  X  We have developed a system called  X  X nformation Desk X , and combine them with conventional relevance search. Drew DeBruyne, Lauri Ellis, Mark Swenson, and Mark Davies for their supports to the project. [1] S. Blair-Goldensohn, K.R. McKeown, A.H. Schlaikjer. A [2] E. Brill, S. Dumais, and M. Banko, An Analysis of the [3] M. Chen, A. Hearst, A. Marti, J. Hong, and J. Lin, Cha-Cha: [4] C. L. A. Clarke, G. V. Cormack, T. R. Lynam, C. M. Li, and [5] N. Craswell, D. Hawking, and S.E. Robertson. Effective site [6] N. Craswell, D. Hawking, and T. Upstill. TREC12 Web and [7] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins. [8] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. [9] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. Task [10] H. Cui, M-Y. Kan, and T-S. Chua. Unsupervised Learning of [11] A. Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, D. [12] R. Fagin, R. Kumar, K. S. McCurley, J. Novak, D. [13] S. Feldman and C. Sherman. The high cost of not finding [14] H. Han, C. L. Giles, E. Manavoglu, H. Zha, Z. Zhang, and E. [15] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. [16] D. Hawking. Challenges in Intranet search. Proceedings of [17] D. Hawking, N. Craswell, F. Crimmins, and T. Upstill. [18] E. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C. Y. Lin. [20] A. Ittycheriah and S. Roukos, IBM's Statistical Question [21] J. Klavans and S. Muresan. DEFINDER: Rule-Based [22] C. C. T. Kwok, O. Etzioni, and D. S. Weld, Scaling question [23] Y. Li, H Zaragoza, R Herbrich, J Shawe-Taylor, and J. S. [24] B. Liu, C. W. Chin, and H. T. Ng. Mining Topic-Specific [25] D. Mattox, M. Maybury and D. Morey. Enterprise Expert [26] P. Ogilvie and J. Callan. Combining Structural Information [27] D. R. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal. [28] D. E. Rose and D. Levinson. Understanding user goals in [29] J. Savoy, Y. Rasolofo, and L. Perret, L. Report on the TREC-[30] D. Stenmark. A Methodology for Intranet Search Engine [31] V. N. Vapnik. The Nature of Statistical Learning Theory . [32] J. Xu, Y. Cao, H. Li, and M. Zhao. Ranking Definitions with [33] J. Xu, A. Licuanan, R. Weischedel. TREC 2003 QA at BBN: [34] H. Yang, H. Cui, M. Maslennikov, L. Qiu, M-Y. Kan, and T-[35] Intellectual capital management products. Verity , [36] IDOL server. Autonomy, [37] Fast data search. Fast Search &amp; Transfer, [38] Atomz intranet search. Atomz, http://www.atomz.com/ [39] Google Search Appliance. Google, 
