 1. Introduction
User-generated content has enjoyed an enormous growth. Many web content publishers have shifted from creating their the stage provided to exhibit their own creations (or even some representation of themselves), and they appreciate how collaborative systems allow like-minded people to discover those easily.

The flow chart in Fig. 1 shows an abstract view of collaborative systems. We distinguish two usage phases: indexing ,
Content can be indexed in many ways. In the traditional library or archive, indexing has been the task of professionals ing content through tagging is prone to unsystematic and inconsistent indexing that could harm retrieval performance.
Personalization of tagging systems could support users in both phases, to improve consistency of tag usage among the we show in Section 3 how the underlying personalized ranking scores for a given candidate (an item or a tag, depending different types of generative processes in the tagging data. We choose an optimal candidate model and its smoothing for preference data. 2. User tasks for personalization ization in a collaborative tagging system. Level 1 shows the three tasks that apply to users entering the system  X  T recommendation of tags when interesting content has been found  X  T  X  T
 X  , finding experts on a certain topic  X  T 6 ; T 9  X  , and, making friends and discovering relevant content through them  X  T
Actual collaborative tagging systems have effectively implemented support for most of these tasks. Table 1 lists some columns distinguish between systems that focus on publishing their users X  own creations (Video, Photographs, Recipes,
Art), and systems that allow users to maintain references to artifacts not necessarily created by themselves (Books, Web the assumption that injectors of self-created content can be expected to know best how to index it. system to the user X  X  preferences.
 We concentrate on three common user tasks:
In the indexing (or tagging) phase: 1. Collaborative tagging : personalizing the tagging process, when a user assigns tags to index content (Fig. 2, T putergame X  or  X  X omputer_games X . Ideally, the system should suggest tags from a common vocabulary that fits the user X  X  forces tags that have been used frequently by the target user as well as others.

In the retrieval phase: for navigating to your own items, but if the cloud is used for exploration of other content ( Fig. 2 , T tags for a specific user (see Section 3.3). ing a link or typing the word in a search box ( Fig. 2 , T and query tag, where a combination of the item X  X  popularity and  X  X reshness X  provides the ranking score. However, due to
The model unifies user preferences and tags in a probabilistic framework, to rank items for the user who issued a tag. 3. Personalization models 3.1. Tagging data To describe tagging data, let u be a discrete random variable over the sample space of users U random variable over the sample space of items (content) U can be viewed as a 3D matrix, where each element indicates whether a user tagged an item with a specific tag (the matrix simplified view of the original problem (analogously to Mika, 2005 ): User X  X ag (UT) : Element  X  u ; t  X  equals the number of items that user u tagged with tag t . Item X  X ag (IT) : Element  X  i ; t  X  equals the number of -s that tagged item i with tag t . User X  X tem (UI) : Element  X  u ; i  X  equals the number of tags that user u assigned to item i .
We now assume that tagging data can be viewed as the result of a two-stage generative process, where we first select a erences, and the tags will generate items modelling their instantiation in real-world items. 3.2. Collaborative tagging model
Personalized collaborative tagging refers to determining which tags to suggest to the user when tagging a given informa-tion item, from the pool of tags employed by other users (Fig. 2 , T tag and suggest the highest ranking ones to the user.

We obtain the conditional probability p  X  t j u ; i  X  from the generative model: where / t denotes same rank order with respect to t .

We instantiate the abstract two-stage generative model into the model shown in Fig. 3 . A particular user X  X  decision to model. More formally, for each user u 2 U U , we choose a tag -generative model H each tag, H I t . We will assume a multinomial distribution over the vocabulary of items as well. where p  X  H T u j u  X  is the posterior probability of model parameter H
In practice, it is common to approximate the full Bayesian integration over the model by estimating the  X  X ptimal X  model parameters b H T u (e.g., by Maximizing their A Posteriori probability (MAP)) and then setting p  X  H
We take the approximation approach, estimating model b H T and substitute it into Eq. (2). Doing the same for the item generation process gives from the candidate tag (keyword) model how probable the query item would be generated (a completely popularity-based the highest ranking scores.

Sparse observation data remains a problem for probability estimation using this model. Research on the language mod-eling approach for information retrieval has however identified various so-called smoothing methods to estimate the term
Table 2 summarizes the resulting probability estimations. The smoothing parameter in a user model balances the personal a a a a smoothing. As expected, the optimal value of l shows that tag suggestion performs best when combining personal and pop-ularity-based tags. 3.3. Collaborative browsing model
We now discuss how to personalize tag clouds, to improve support of the collaborative browsing task. Hereto, we need to predict the relevance of  X  X ew X  tags, i.e., tags that do not yet exist in the given user preference. The items).
 user preferences explicitly, such that they can be linked to the preferences of other users. Formally, q the set of items that this user has tagged or preferred, i.e., q date tags t given a user profile q u , i.e., p  X  t j q u  X  : parts: the relation between tag and user preference expressed by p  X  q be easily estimated from the occurrence frequency in the collection. To estimate the likelihood p  X  q (as query) being generated by the candidate tag model: resentation as a set of items and assume that each item in the user preference is independently generated, we get task, because smoothing results in an estimate of p  X  t j erence is independently generated, results in the following ranking score: user preference and thus the prediction comes mainly from the popularity part. The smoothing parameters balance the two tags and the prediction relies solely on popularity.
 user preferred (item-based user preference), or to the tags that the target user used (tag-based user preference). 3.4. Collaborative item search model
Most tagging systems support the retrieval of items annotated with a given tag, for example by clicking a tag in the browsing interface or typing a word in a search box ( Fig. 2 , T the basis of p  X  i j q u ; t  X  , the probability that item i is relevant to tag query t given user profile q the language models approaches to information retrieval (Zhai &amp; Lafferty, 2001 ).
Using Bayes X  and assuming conditional independence between users and tags given an item leads to erences by their previously used tags (the model using item-based user preference can be obtained similarly): its probability of generating the user preference  X  p  X  q
The model provides a personalized ordering of items in collaborative tagging systems. It combines user preferences for our model by marginalizing out the tags, p  X  i j q u  X  X  P
Riedl, 1999; Hofmann, 2004; Linden, Smith, &amp; York, 2003 ). 4. Experiments 4.1. Data set preparation
We are not aware of standard data sets suited for the evaluation of our models. We therefore collected data from two 2006. We collected a number of the most popular tags, found which users were using these tags, and downloaded the com-plete profiles of these users. We applied standard term tokenization techniques from text retrieval followed by stopword removal. Finally, we extracted the user X  X tem X  X ag triples from each of the user profiles. User IDs are randomly generated to keep the users anonymous. Table 3 summarizes the basic characteristics of the data sets; they can be downloaded from the author X  X  web-site. 2 4.2. Evaluation protocol 4.2.1. Evaluation methodology
Since the three user tasks have been transformed into predicting items or predicting tags, we can evaluate the perfor-from the remaining data (the training set). Prediction accuracy is then measured by ranking items or tags for test users their profile, as known from the held-out ground-truth.
 gestions made by the models.
 cross-validation again in all subsequent experiments, to estimate the model parameters from five newly sampled training and test splits. 4.2.2. Evaluation metrics
We evaluate the effectiveness of the proposed models using evaluation measures at fixed cut-offs, thus normalizing the lowing the recommendations of Hull, 1993 ).
 timate the true precision. On the other hand, we make the assumption that bookmarking an item on a public site indicates the item X  X  relevance, which may overestimate the true precision.
 deed suggested. We therefore evaluate collaborative tagging using both precision and recall. 4.3. Performance of personalization models
The first experiments assess the performance of collaborative tagging and collaborative item search. The purpose of the (Hiemstra, 2001 ): a generative model from candidate item to the query tag or vice versa). Tables 4 and 5 summarize our results for the two tasks. The models use Bayes X  smoothing (see Table 2 ).

The experimental results support the hypothesis that personalized collaborative models outperform significantly the alized ones is higher when we have more observations about user preferences.
 Finally, we evaluate the collaborative browsing task. Table 6 a shows that our model with Bayes X  smoothing (denoted as Item UP-BS ) outperforms the popularity-based ranking significantly, in all configurations.
When we treat tags as items, item-ranking based collaborative filtering could provide a competing approach. We compare item-based TF IDF-like (denoted as ItemProb) and user-based cosine similarity recommendation (Herlocker et al., 1999 ) (denoted as UserCos), as implemented in the Top -N -suggest recommendation engine tag process). 4.4. Representation of user profiles
Therefore, the correlation between two related tags within a user profile may be less strong than the relation between for this purpose.

For the collaborative item search model however, the two user profile representations perform differently on different data sets. Notice from Table 7 b and c that the tag-based user preference representation (Tag UP-BS) usually outperforms may indeed be expected to represent users X  interests more accurately than tags in del.icio.us would.
We also observe in Table 7 a that Bayes X  smoothing leads to better results than Jelinek X  X ercer smoothing, regardless of length X  (see explanation in Eq. (20)) and therefore performs better. 4.5. Impact of parameters
This section evaluates sensitivity and impact of the smoothing (hyper-)parameters in the del.icio.us data set. The first and k in Jelinek X  X ercer smoothing (JMS), respectively (on a logarithmic scale results of Bayes X  smoothing are relatively stable in a wide range between 10
Mercer smoothing is more sensitive to lambda using tag-based user preferences than for item-based user preferences. Tag-
Regarding collaborative tagging, recall how Eq. (6) is based on two generative models, the user X  X  tag-generation model means that the indexing model needs less smoothing from the background collection model, which can be explained by the assumption that users tend to prefer previously used tags when tagging new items. 5. Discussion and related work Collaborative tagging systems have recently emerged as tools to structure online databases and user-generated content.
To improve the understanding of these social categorization systems, Golder and Huberman conducted an investigation of problems have been confirmed by measurements on the online photo album Flickr by Marlow et al. (2006) . These works have ciency issues, proposing a fast tag recommendation method. Compared to our paper, these studies have implicitly removed the user from the relationship between tags and items. Our experimental results demonstrate however that the match between tags and information items is incomplete, and the user X  X  personal interests should not be ignored. We have dem-
In operational tagging systems, we would expect additional benefits from personalized collaborative tagging  X  T also result in improved categorization consistency.
 other but similar items to the target item (e.g., item-based collaborative filtering Deshpande &amp; Karypis, 2004 ). els have been proposed to compute recommendations (e.g., Hofmann, 2004 ), the interpretation of the hidden aspects in compared to a ranking solely on the tag query.
 users like to distribute their creations.
 ambiguity of textual queries. A recent study revealed however that personalized search, due to a lack of a mechanism to formance under some situations (Dou et al., 2007 ).

Our ranking models integrate the collaborative nature of recommendation systems and the smoothing methods from to better recommendations and retrieval, adapted to individual information needs.
 Appendix A. Probability estimation We detail parameter estimation for the user X  X  tag-generation model only. Treat a user u  X  X  parameters H ables ( Fig. 3 ) and estimate their value by maximizing the a posterior (Jebara, 2003 ): where n  X  u ; t  X  denotes the number of times that tag t has been used by user u and a T tribution (the hyper-parameters ). p  X  H T u jf n  X  u ; t  X g when we have observed some tags (denoted as f n  X  u ; t  X g tional to the product of the likelihood and the prior probability:
Likelihood p  X f n  X  u ; t  X g L t  X  1 j H T u  X / Q t  X  h t
Dirichlet) is chosen as prior to simplify estimation (Gelman et al., 2003 ): observing data corresponds again to a Dirichlet, with updated parameters: ities in the tag-generation model. a place smoothing estimator. Alternatively, the prior can be fit on the distribution of the tags in a given collection: (Zaragoza et al., 2003 )
Eq. (19) is equivalent to (details in Zhai &amp; Lafferty, 2001 ) where the background influence as k u  X  k results in Jelinek X  X ercer smoothing ( Zhai &amp; Lafferty, 2001 ). References
