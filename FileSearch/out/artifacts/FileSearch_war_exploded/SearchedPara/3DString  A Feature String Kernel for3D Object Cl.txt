 Classification of 3D objects remains an important task in many ar-eas of data management such as engineering, medicine or biology. As a common preprocessing step in current approaches to classi-fication of voxelized 3D objects, voxel representations are trans-formed into a feature vector description.

In this article, we introduce an approach of transforming 3D ob-jects into feature strings which represent the distribution of voxels over the voxel grid. Attractively, this feature string extraction can be performed in linear runtime with respect to the number of vox-els. We define a similarity measure on these feature strings that counts common k-mers in two input strings, which is referred to as the spectrum kernel in the field of kernel methods. We prove that on our feature strings, this similarity measure can be computed in time linear to the number of different characters in these strings. This linear runtime behavior makes our kernel attractive even for large datasets that occur in many application domains. Furthermore, we explain that our similarity measure induces a metric which allows to combine it with an M-tree for handling of large volumes of data. Classification experiments on two published benchmark datasets show that our novel approach is competitive with the best state-of-the-art methods for 3D object classification.
 H.2.8 Database Applications  X  Data Mining Algorithms, Management 3D Object Classification, String Kernel
Over the last years an ever increasing number of 3D data has been generated in fields as different as Computer Aided Design Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. (CAD) applications and biological structural databases like the Pro-tein Data Bank (PDB) [3]. A huge number of 3D objects can also be found scattered all over the World Wide Web. In experimental datasets like the Princeton Shape Benchmark Dataset (PSB) [26] or the NTU 3D Model Benchmark Dataset [5] such objects from the WWW have been collected and manually been classified according to their function or the human perception of geometric similarity.
Due to the ongoing work in the field of structural biology and the increasing interest in industrial 3D CAD systems, the amount of available 3D data will keep on growing over the next years. There-fore, the automatic classification of newly created or newly found 3D data into known classes will continue to be a topic of interest in data engineering. A very wide-spread technique in 3D object classification is to apply a voxelization preprocessing step to the original 3D data [13], e.g. provided by means of triangle meshes or in case of proteins by means of atomic coordinates. The advantage of a voxelized representation lies in the ability to level out small differences between similar models that may be due to the usage of different levels of detail. Furthermore, voxels are a more ele-gant way of modeling solid surfaces than a collection of connected points forming triangle meshes. Voxels can also be used to model filled interiors of 3D objects while triangle representations merely describe their surfaces. Finally, voxel representations usually have a lower complexity and thus allow for a more efficient computation of certain characteristics of 3D data.

Most work on the classification of voxelized data uses a feature-based approach [14, 8, 1, 15, 20], i.e. a specific number of numer-ical features is extracted for each voxelized object. These features form so-called feature vectors and the whole process can be re-garded as a mapping of a voxelized object to a (potentially) high dimensional space called the feature space.

Mapping voxelized data to feature vectors is attractive, as it trans-fers a complex data type into a simpler one, on which a huge fam-ily of distances, similarity measures and efficient data mining al-gorithms are available. Unlike vectors, strings are complex data, providing information about the structure of an object. Thanks to the suffix tree, string mining can nevertheless be performed very efficiently, and distances, similarity measures and fast data mining algorithms have been developed for strings as well. For this rea-son, we decided to explore a novel approach to 3D voxelized object classification: Instead of a vector transformation, we examined if a structured description of 3D objects, namely feature strings, can be used for fast and accurate classification.

The remainder of the paper is organized as follows: In Section 1.1 we will review current approaches to 3D object classification that are based on feature vector transformations. After giving a short introduction to kernel methods in Section 2, we will review
Figure 1: Voxelized Representations of a Protein and a Bolt existing kernel methods on strings (Subsection 2.3). In Section 3, we will define our 3D feature strings on voxelized data and a simi-larity measure for these strings, a special subclass of the so-called spectrum kernel. Its performance is then evaluated and compared to other approaches on benchmark datasets in Section 4. In Section 5 we discuss the outcome of the experiments. Section 6 concludes the paper.
Before we define our novel method, we will review the state-of-the-art in 3D object classification in the following. First, we will give a short introduction to voxelization in general and second, we will present the ideas of the most prominent approaches in 3D object classification.
 Voxelization. Voxels are the three dimensional equivalent of two dimensional pixels. In order to voxelize a 3D object, at first a min-imal bounding cube is constructed around the object. According to the desired voxel resolution r , each side of the surrounding cube is partitioned into r segments of the same size. This segmentation partitions the cube into r 3 small cubes, the voxels. A single voxel is considered filled or black if a triangle of the original triangle mesh intersects the voxel and is considered not filled or white otherwise. The collection of filled and not filled voxels inside the cube is called a voxel grid.

In [13] Kaufman introduced an algorithm that yields a conserva-tive approximation of the surface of a 3D object by means of voxels. In Figure 1 the voxel grids of a protein and a bolt are depicted. Shell Model. An intuitive technique to describe 3D shapes was presented by Ankerst et al. in [1]. At first the balance point M of an object is determined. Afterwards a number of spheres centered at M is constructed. The radius of the largest sphere is chosen to result in a minimum bounding sphere of the 3D object. The radii of the other spheres to be constructed are equally distributed between 0 and the radius of the largest sphere. The constructed spheres thus lead to a shell-like spatial partition of each object. Although the authors originally used surface points to represent the objects this technique can easily be applied to voxels. For each shell the number of filled voxels that lie inside this shell is determined. Being a cube actually, a filled voxel is considered lying inside a certain shell if the center of the voxel cube lies inside the shell. Thus each filled voxel is assigned to one shell only. The resulting histogram reflects the fraction of a 3D object that lies inside a certain shell. For each object its specific histogram is used as the feature vector. Spherical Harmonics. In [14] and [8] Kazhdan, Funkhouser et al. defined a mapping for voxelized objects to feature space based on spherical harmonics. According to the theory of spherical har-monics any spherical function f (  X ,  X  ) can be decomposed as where Y m l is the so-called spherical harmonic function. The au-thors define a number of spherical functions by intersecting spheres of different radii with the voxel grid. The function associated with a certain sphere yields 1 if the point specified by the radius of the sphere and the two angles  X  and  X  lies inside a filled voxel and 0 otherwise. Each of the spherical functions is subsequently decom-posed as described above. The coefficients a lm  X  C are finally used to calculate the value s l  X  IN for a certain choice of l where Thus every sphere intersecting the voxel grid yields several values s and the collection of all s l of all spheres constitutes the feature vector for the current 3D object.
 Eigenvalue Model. The authors of [15] use the Principal Com-ponent Analysis technique [12] to define meaningful numerical fea-tures for 3D objects. At first the minimum bounding box of an ob-ject is calculated. The largest extent of this box determines the size of the minimal bounding cube of the object into which the object is placed in such a way that the minimum bounding box is centered inside the minimum bounding cube. The cube is then partitioned into n 3 ( n  X  IN ) cubical partitions. Obviously n should be smaller than the voxel resolution r so that more than one voxel is covered by each spatial partition. After the object is voxelized, three numer-ical features are extracted for each of the n 3 cubical partitions. Let V = { v 1 ,...,v m } be a set of filled voxels lying inside the same cubical partition. In the next step, each v i  X  V is translated such that the balance point M of V coincides with the origin, and the covariance matrix C for V is computed as follows: The covariance matrix can be decomposed as C = VEV T , where V is an orthonormal matrix containing the eigenvectors of C is a diagonal matrix containing the eigenvalues of C . The eigenvec-tors are called principal axes of V . They describe the three orthog-onal axes where the scattering of the elements is greatest. The three eigenvalues describe the variance along the principal axes and thus can be used to characterize the shape of the elements of V . After three eigenvalues have been calculated for each partition, a feature vector for the complete object can finally be derived.
 Grid D2. In [20] Osada et al. presented a technique called D2. A number of points is distributed randomly on the surface of the object to be mapped to the feature space. Then all possible pairwise Euclidean distances are computed. These distances are finally used to create a histogram for each object reflecting the distribution of distances between the surface points of a specific object.
This technique was adapted by Shih et al. in [25] for the use with voxelized data. The method the authors call Grid D2 randomly chooses two filled voxels and calculates their Euclidean distance. This is repeated r 3 times (again, r is the voxel resolution) so that a histogram reflecting the voxel distribution can be created. The entries of the histogram are finally normalized by dividing them by r 3 . Again, the histogram for each object can be used as the corresponding feature vector.
In the following, we will shortly review the basic concepts of ker-nel methods, as we will employ kernel functions for 3D voxelized object comparison in this article. The interested reader is referred to excellent books [23] and tutorials [4] for a complete introduction to kernel methods, respectively.
Kernel methods represent a family of related machine learning and data mining algorithms. As their core component, they all rely on a kernel function, which can be thought of as a positive definite measure of similarity on input data. Based on this kernel function, tasks such as as classification via Support Vector Machines [29], re-gression [7], clustering [2] and Principal Component Analysis [24] or kernel Fisher Discriminant Analysis [19] can be handled.
All kernel methods utilize the so-called  X  X ernel trick X . Data points from input space are mapped into a usually higher dimen-sional feature space. Linear hypotheses in this feature space may correspond to non-linear hypotheses in input space, thereby allow-ing to use algorithms for linear problems in feature space in order to solve non-linear ones in input space. Evaluating a complex map-ping from input space to feature space might be computationally or numerically problematic. Yet a second  X  X ernel trick X  is the fact that all computations in feature space can be done implicitly by evaluat-ing the so-called  X  X ernel function X . This kernel function represents a scalar product in feature space and a measure of similarity in in-put space. Defining a kernel function therefore allows us to deal with hypotheses in feature space without explicitly mapping points from input into feature space.
As kernel methods were successful in many application areas, the interest in kernel methods on non-vectorial data grew. As an essential step, Sch  X  olkopf [22] realized that the kernel trick can also be applied to non-vectorial data, simply by expressing similarity between structured data via a kernel function. Haussler [9] and Watkins [31] were the first to define a principled way of designing kernels on structured objects. Based on this framework, kernels on structured objects such as strings and trees, transducers, dynamical systems, on nodes in graphs and on graphs have been defined over recent years.
Following [27], kernels for strings can be divided into five cat-egories: first, polynomial-like kernels (e.g. [32]); second, kernels derived from probabilistic models (e.g. [10]); third, kernels based on alignments (e.g. [18]); fourth, spectrum-like kernels that count common substrings in two input strings ([30, 17, 16]; fifth, kernels incorporating positional information on substrings (e.g. [21]). Most interesting for our application is the class of spectrum-like kernels. These are all built on the fundamental idea to count identical subse-quences, called k-mers, of input strings [16]. More formally, given two strings x and x from an alphabet  X  . Then the exact matching spectrum kernel is defined as: k ( x, x ):= where s and s are substrings of x and x respectively,  X  s weight assigned to string s and  X  s,s is the delta function that equals 1if s and s are identical, 0 otherwise. num s ( x ) is the number of occurrences of substring s in string x .

This class of string kernels comprises the  X  X ag of character ap-proach X  and the  X  X ag of words X  kernel which compare all pairs of characters and words in two strings, respectively [11]. One com-mon approach to save runtime is to consider strings up to a certain length only. This method is sometimes referred to as the limited range correlations string kernel. The alternative approach is to consider substrings of fixed length k only, usually referred to as the k-spectrum kernel.

While dynamic programming implementations of the spectrum kernel computation require quadratic runtime, computation with runtime linear to the added total length of input strings has been made possible by [30].

Beside exact matching kernels, string kernels that count gappy or non-completely identical substrings in two strings have been de-veloped, often motivated by the problem of aligning biological se-quences in bioinformatics [17].
In this section we will describe the generation of the feature strings we use to characterize 3D structures. At first we state the general technique of mapping 3D objects to a voxel grid. Then we will outline an algorithm that yields the string representations of 3D objects in linear runtime with respect to the size of a given voxel grid.
As mentioned above, a three dimensional object is at first trans-formed into a voxel grid. Such a three dimensional grid consists of r voxels where r is called the resolution of the voxel grid. A voxel is considered filled if and only if the original 3D object intersects the voxel. Each voxel can be addressed by its coordinates where 1  X  x, y, z  X  r and x, y, z  X  N . A voxel grid v can there-fore be considered as a function v : N 3  X  X  0 , 1 } v ( x, y, z )=
The straightforward feature extraction step we next describe is a key step of our application. Our motivation was to generate strings that describe the distribution of filled voxels along each of the three axes, x , y and z . For the computation of the 3D feature strings we iterate through the voxel grid, once for each dimension x , y and z , and create one feature string for each dimension.

Without loss of generality, let us assume now that we want to determine the feature string for dimension x . First, we consider all voxels with x =1 only; these voxels form a y -z -plane. We then count the number # filled of filled voxels in this y -z -plane. After-wards, we append # filled times the current value of x , which is  X 1 X , in the first iteration, to our feature string s x (see Figure 2). We repeat this procedure for all values of x up to r , i.e. we consider all y -z -planes defined by x from 1 to r . The following pseudocode illustrates the feature string generation: Given: empty strings s_x for (1&lt;=x&lt;=r)
Figure 2: First Considered Plane for the Construction of s for (1&lt;=y,z&lt;=r) //iterate through plane
The strings s y and s z can be constructed analogously. After three iterations through a given voxel grid, all the required strings have been constructed in linear time w.r.t. the number of voxels. Thus the complexity of the string extraction step is equal to O
Note that we append character representations of numbers to the strings. This is different from adding the single digits a number consists of. Imagine an empty string s . Let s 1 be the result of the concatenation of s with the numbers 1 and 2, i.e. s 1 = s + X 1 X + X 2 X . Similarly the attachment of the number  X 12 X  to s results in s we consider s 1 = s 2 . This can be implemented by using a spe-cial character as separator between the numbers or by bijectively mapping the numbers to an alphabet that is made up of r single-character elements.

Obviously, the feature strings generated are sorted, as all occur-rences of the same character appear in one consecutive block. We will exploit this characteristic of our feature strings for fast string kernel computation.

Note furthermore that our kernel can handle the strings in a com-pressed representation, e.g. the string  X  X AAAABBBBCCC X  can also be stored and be processed as  X 5A4B3C X .

To conclude this subsection, let us summarize the mapping from a three dimensional object to a set of strings describing this object: At first a given object is transformed into a voxel grid. Then this voxel grid is traversed plane-wise and whenever the voxel function v equals 1 for a triple of voxel coordinates in this plane, the string s x is elongated by one character, the x coordinate of this filled voxel. These strings reflect the distribution of filled voxels along the three axes of the coordinate system. Finally, after repeating this procedure for dimensions y and z , a three dimensional object is described by a set of three strings s x , s y , and s z .
We choose to use a basic similarity measure on these feature strings, namely a so-called spectrum kernel . The spectrum kernel counts pairs of identical substrings in two input strings as a simi-larity measure for two strings (see Figure 3).
We explore three variants of the spectrum kernel, namely one version that considers all k-mers in two input strings, and another Figure 3: Spectrum Kernel Computation for 4-mers on two In-put Strings. version that looks at all k-mers up to a certain length K , and a third kernel that scans strings for k-mers for one fixed k-mer length K only. We will refer to the first one as the all k-mer kernel, to the second one as the limited range correlation (lrc) kernel and the third one as the k-spectrum kernel. We will present formal definitions of all three in the following.

Given two strings x 1 and x 2 from an alphabet  X  . s and s are substrings of x 1 and x 2 respectively, which is denoted by s x and s x 2 . | x 1 | is the length of string x 1 .  X  s,s is the delta func-tion that equals 1 if s and s are identical, 0 otherwise. num is the number of occurrences of substring s in string x 1 the remainder of the paper.

D EFINITION 1(A LL K -MER K ERNEL ). Then the all k-mer ker-nel is defined as k ( x 1 ,x 2 ):= where  X  s =1 for all s .

D EFINITION 2 (LRC K ERNEL ). Under the same conditions, the lrc kernel with a maximum k-mer length K is defined as k ( x 1 ,x 2 ):= where  X  s =1 for | s | X  K and  X  s =0 for | s | &gt;K .

D EFINITION 3( K -SPECTRUM K ERNEL ). Under the same con-ditions, the k-spectrum kernel with a fixed k-mer length K is defined as k ( x 1 ,x 2 ):= where  X  s =1 for | s | = K and  X  s =0 for | s | = K .

Beside using the all k-mers kernel that considers all k-mers, it seems attractive to use the lrc kernel as well, because the all k-mers kernel weighs long k-mers much stronger than short k-mers, although  X  s =1 for all s . The reason is that every common k-mer contains two common (k-1)-mers, consequently the total weight of a k-mer is twice the weight of a (k-1)-mer plus 1. Hence, in our 3D object classification task, the all k-mer kernel would give high similarity scores to objects that have one long match in their feature vectors. It therefore seems plausible to explore a second approach, the lrc kernel, which neglects matches longer than a threshold K . Thus the lrc kernel is more likely to give high similarity scores to two objects with many common k-mers. The third approach we examine, the k-spectrum kernel measures similarity in terms of k-mers of one fixed length only. Longer and shorter matches are not considered for determining the match score.
Given two voxel objects v 1 and v 2 , we compute the spectrum kernel pairwise for the x -strings, y -strings and z -strings of v v and hence obtain three string kernel values for v 1 and v addition and pointwise multiplication of kernels preserve positive definiteness, a simple way of fusing these three similarity measures into one joint kernel is both addition and pointwise multiplication. We therefore defined two variants of a joint 3D string kernel on all three x -, y -, z -strings, namely the sum 3D string kernel and the pointwise product (pp) 3D string kernel as where .* denotes pointwise multiplication, not matrix multipli-cation. To scale all kernel values to the same range, we normalized all kernel values beforehand by
Our choice of similarity measure has two great advantages: First, as a positive definite kernel function, the spectrum kernel allows us to employ kernel methods for classification, such as Support Vector Machines, regression or PCA on our feature strings.

Second, the special characteristic of our 3D strings, namely that characters appear ordered only, allows us to compute the spectrum kernel of two 3D strings extremely fast. Whereas string kernels in general can be computed in time linear with respect to the total common length of two input strings (as shown in [30]), the spec-trum kernel on our feature strings can be computed in time linear to the number of different characters in both input strings. As the number of different characters in our strings is upper-bounded by the voxel grid resolution r , one could also describe the runtime complexity of the spectrum kernel on our feature strings as linear with respect to the voxel grid resolution.

We will show in the following that this result holds for all of our three 3D string kernels, the all k-mers, the lrc and the k-spectrum 3D string kernel.
First, we will explain the linear time computation of the all k-mers string kernel. Besides showing that this is possible in time linear to the voxel grid resolution, we will provide the actual algo-rithm for doing so in the proof of the following theorem.
T HEOREM 1. The all k-mers 3D string kernel can be computed in linear time with respect to the number of distinct characters in both input strings.

P ROOF . Given two input strings x 1 and x 2 . First, we assume that both strings contain the same character a only. Then the num-ber of k-mers s in string x of length | x | is obviously num | x | X  k +1 . The number of common k-mers in x 1 and x 2 is then num s ( x 1 )  X  num s ( x 2 ) . If length | x 1 | = n and z = min ( n, m ) , then the number of all common k-mers with 1  X  k  X  z can be computed as
Second, we assume that both strings contain the same two differ-ent characters a and b only. Then the number of common k-mers can be divided into three classes: pure a k-mers (consisting of a  X  X  only), pure b k-mers (consisting of b  X  X  only), and ab k-mers (con-sisting of a  X  X  and b  X  X ). The former two can be easily computed as described above. The number of ab k-mers in x 1 and x 2 can be computed as compute k mers two chars ( x 1 ,x 2 , [ a, b ]) = min( num a ( x 1 ) ,num a ( x 2 ))  X  min( num b ( x 1 ) ,num Third, we assume that both strings consist of three (or more) char-acters a, b, c each. Common k-mers that consist of three different characters a , b , c can only occur if the number of b  X  X  in x is identical. After determining all common pure a , pure b and ab k-mers, we transform all a  X  X  into b  X  X  in both strings (resulting in x and x 2 ) and then compute all bc k-mers as min( num b ( x 1 ) ,num b ( x 2 ))  X  min ( num c ( x 1 ) ,num
The number of common bc k-mers in x 1 and x 2 is obviously identical to the added number of common bc and abc k-mers in x and x 2 . This procedure recursively turns all k-mers based on three or more characters into k-mers of two different characters, which can be computed efficiently as described above.

This results in the following linear-time algorithm for computing the all k-mers kernel, where character(i) is the i-th character in the alphabet  X  (For ease of presentation, we assume that both strings contain every character in  X  at least once): function compute 3 D string kernel ( x 1 ,x 2 ) Given: strings x_1 and x_2, alphabet Sigma for i from 1 to size(Sigma) end for i from 2 to size(Sigma ) c = character(i); end return all_k_mers; The fact that we have to iterate over all characters twice only shows that our algorithm requires linear runtime to calculate the 3D string kernel.
 Second, if we are not interested in all k-mers, but only in k-mers up to a certain length K only, computation of the string kernel has to exclude all longer k-mers. We will show that computational effort remains linear to the voxel grid resolution in the following theorem.
T HEOREM 2. The limited range correlation 3D string kernel with a maximum k-mer length of K can be computed in linear time with respect to the number of distinct characters in both input strings.
P ROOF . The proof for the lrc kernel is analogous to the proof for the all k-mer string kernel, except for two definitions: As we are considering k-mers up to a fixed length K only, the functions compute k mers one char and compute k mers two chars have to be redefined.

To compute k-mers from two strings both consisting of a se-quence of the same character only: may be longer than K , or longer than any of the two strings.
To compute k-mers from two strings both consisting of the same two characters only, we have to define some notation first: Given two strings x 1 and x 2 . We define z a =min( num a ( x 1 and z b =min( num b ( x 1 ) ,num b ( x 2 )) . We then have to distin-guish two cases, namely
Case 1: z a + z b  X  K compute k mers two chars ( x 1 ,x 2 , [ a, b ]) = z a  X  z
Case 2: z a + z b &gt;K compute k mers two chars ( x 1 ,x 2 , [ a, b ]) = = = = 1 +( K  X  min ( z a ,z b ))  X  min ( z a ,z b ) (3)
This first case can be dealt with as in the all k-mers setting, as no common k-mer can be longer than K in this setting. In the second case, we have to compute k-mers from length 2 to K , as the
If we use min or max operators with more than 2 arguments, this simply means we are selecting the minimum or maximum element out of a set of given terms. number of k-mers cannot be larger than z a or z b or k  X  1 interested in k-mers that contain at least one a and at least one b .
Since both terms can be computed directly, i.e. in constant time, the overall runtime of our algorithm does not change, i.e. remains linear as for the all k-mer kernel.
 If as a third alternative, we want to consider k-mers of a fixed length K only, we have to set kernel values for pairs of longer and shorter matches to zero. Again, we explain how to achieve this in linear time in the following theorem and proof.

T HEOREM 3. The K-spectrum kernel 3D string kernel with a fixed k-mer length of K can be computed in linear time with respect to the number of distinct characters in both input strings.
P ROOF . The argumentation follows the two previous proofs. The two functions are now redefined as: compute K mers one char ( x 1 ,x 2 ,a )= max (0 , ( num a ( x 1 )  X  K +1))  X  max (0 , ( num a ( x 2 The number of common K-mers in x 1 and x 2 is the product of the number of K-mers in x 1 and x 2 . Obviously, there cannot be any K-mers in one string if this string contains less than K characters. This means that the number of common K-mers of our two strings is z a + z b  X  K +1 at maximum. There cannot be more than K  X  1 K-mers, as each K-mers has to contain a and b at least once. Furthermore, the number of matching K-mers is upper bounded by z and z b , i.e. the minimum of the number of occurrences of a and b in x 1 and x 2 .
Apart from a fast theoretical runtime, our feature string kernels allow handling of larger databases as well. As every kernel function induces a metric where p 1 and p 2 are objects in feature space, we can define a met-ric on voxel objects based on our 3D string kernels. We can then use this metric to create an M-tree index structure [6] for efficient storage and access of large datasets of 3D objects that do not fit into main memory.
In this section, we will present the results of our experimental evaluation. In particular we compared the feature-based techniques described in section 1.1 with our new string kernel based approach presented in this article. We will give a detailed overview of the set-tings used in our evaluations first. This includes a brief description of the considered datasets and the parameters used in combination with the different techniques. We will also describe how we mea-sured the ability to correctly classify 3D objects, before giving the actual results in the following subsection.
We will first explain which objects of the NTU [5] and the PSB [26] dataset we used for the classification experiments and describe the parameters used for the description of the 3D objects.
The NTU dataset consists of 1833 3D models that have been manually assigned to classes of similar functionality by the pub-lishers of this dataset. This resulted in 47 classes with 549 models in total. The remaining models were labeled with  X  X iscellaneous X . We discarded those unlabeled objects and made the 549 remaining objects with corresponding class labels our first benchmark dataset. It will be referred to as NTUALL in the remainder of this paper.
The second test set we created is based on the PSB set that contains 1814 3D objects collected from the World Wide Web. Along with the models, a hierarchical classification system is pro-vided. We decided to use the leaves of the classification system as class labels and consequently, the set is partitioned into 161 disjoint classes. For our experiments, we employed two datasets derived from PSB. The first one comprises all objects from PSB, 1814 in total from 161 classes, and will be called PSBALL from now on.
Additionally, to create a dataset with few classes and many in-stances per class, we scanned for classes with approximately 50 or more members. This resulted in a smaller dataset with 4 classes in which 100 objects are members of the class  X  X ighter-jet-airplane X , 100 models are assigned to the class  X  X uman-biped X , 51 objects are marked with  X  X otted-plant-plant X , and 51 more objects can be found in the class  X  X ectangular-table X . In total, the second test set based on the PSB set consequently contains 302 elements and will be re-ferred to as PSB302 set.
In this section, we shortly describe our choice of parameters cru-cial to the analyzed methods. In order to determine the best possi-ble parameter settings for each voxel resolution for each method we performed a number of classification experiments for each method on the NTUALL dataset. The so derived optimal parameters for a certain method and a certain voxel resolution were also used for the classification experiments on the other datasets. In the follow-ing we describe the exact parameter values for each of the methods we compared our 3D string kernel method to.
The authors of the different techniques use varying voxel reso-lutions. At first we set the voxel resolution for all compared ap-proaches to 15, i.e. 15 3 black and white voxels are used for the representation of each three dimensional object. To study the ef-fect of different voxelizations on classification accuracy, we also examined voxel resolutions of 20 and 25.
We varied the number of space partitioning shells and learned that 7 is the best choice for the number of shells for the voxel reso-lution of 15. Objects represented by 20 3 voxels are best described with 9 shells and for a voxel resolution of 25 we used 11 shells. In the following we use the abbreviations  X  X M7 X ,  X  X M9 X , and  X  X M11 X  to denote the Shell Model based on different numbers of shells.
The most important parameter of this method is the number of spherical functions defined on the voxel grid. For 15 3 voxels, 7 spherical functions led to the best classification results on our bench-mark dataset NTUALL. For both, 20 3 and 25 3 voxels the optimal setting was 9 spherical functions. So we use the notation  X  X H7 X  and  X  X H9 X  to refer to this method. Following the original work, we computed the s l sums for the first 16 frequencies.
The classification results of this similarity model vary with the number of cubical partitions that divide the 3D objects. We found that 5 3 partitions is the best suited value for this parameter for all the considered voxel resolutions. In the following we consequently refer to this model as  X  X M5 X .
An important parameter of this feature calculation method is the number of calculated distances. In the original work this number equals the number of filled and not-filled voxels, so we randomly selected 15 3 , 20 3 , and 25 3 pairs of voxels and calculated their Euclidean distances. In the following this method is denoted by  X  X D2 X .
We will now outline how we measured the classification ability of the different approaches.
The idea of the feature-based methods is to find a suitable map-ping of voxel grids to the feature space so that the distance between two points in the feature space reflects the human sense of geomet-rical or functional dissimilarity of the associated voxel grids.
While distances represent dissimilarity, kernel values are a mea-sure of similarity. It is, however, unproblematic to turn kernel val-ues of pairs of input data into distances. For example, this can be reached via distance ( p 1 ,p 2 )= where K is a kernel function and p 1 and p 2 are input data.
After defining distances for both feature vector-and kernel-based approaches, we were able to apply a Nearest Neighbor classifier to our datasets. Nearest Neighbor classifiers predict the class label of a test data point t by finding the data point in the training set with minimum distance to t , i.e. the Nearest Neighbor of t . The class label of t is then predicted to be the label of its Nearest Neighbor in the training set.
We performed Leave-One-Out Classification on all datasets. One object was used a test set, while all other objects belong to the train-ing set. This is repeated until each object has been part of the test set exactly once. The classification accuracy we report is the mean of these iterations. First, we tested our three versions of the 3D string kernel on the NTUALL dataset at a voxel resolution of 15. For the lrc kernel and the k-spectrum kernel we examined values of kin { 10 , 20 , 30 ,..., 1200 } , where 1200 is the maximum length of a feature string in NTUALL. We report results of these experiments in Figure 4 for addition and in Figure 5 for pointwise multiplication of x-,y-, and z-kernels.
 While the lrc kernel and the k-spectrum kernel yield different clas-sification accuracies for different k values, the all k-mer kernel ob-viously is independent of the choice for k. Note that we nonethe-less included the classification results of the all k-mer kernel in the above mentioned figures so that the results can be compared more easily. Figure 4: The Limited Range Correlation (LRC) Kernel, the k-Spectrum Kernel (KS), and the all k-mer Kernel (AKM) (Sum of x-,y-,z-Kernels) .
 Figure 5: The Limited Range Correlation (LRC) Kernel, the k-Spectrum Kernel (KS), and the all k-mer Kernel (AKM) (Point-wise Product of x-,y-,z-Kernels).

The lrc kernel outperforms both the all k-mer kernel and the k-spectrum kernel, both for pointwise multiplication and addition. Note that the lrc kernel converges to the all k-mer kernel as k in-creases. k-spectrum kernel and lrc kernel reach their best result for k in 10 to 50, indicating that longer matches are either seldom or mislead the classifier. In this experiment, the addition of kernels gave slightly better results than pointwise multiplication, although differences in accuracy are small, especially for small choices of k.
Afterwards we compared the results of the best 3D string kernel so far, the lrc kernel that had outperformed all other string kernels on NTUALL, to four state-of-the-art techniques (see Section 1.1). This comparison was conducted on NTUALL for all three reso-lutions 15, 20, and 25. We optimized parameters for each of the resolutions for each method individually. We report results in Fig-ure 6.

The lrc kernel outperforms 3 out of 4 competing state-of-the art methods. Only the EM method reaches slightly better accuracies for resolutions of 15 and 25, and the same accuracy for 20 voxel resolution.
 Figure 6: NTUALL Optimized-Kernel (3DS) in Comparison to other Techniques on the NTUALL Dataset. We then applied all methods to the datasets PSB302 and PS-BALL in Leave-One-Out-Validation. We used the parameteriza-tion optimized for NTUALL unchanged on these datasets, to avoid artificially good results by overfitting of parameters. Results for PSBALL are given in Figure 7 and for PSB302 in Figure 8. Figure 7: NTUALL Optimized-Kernel (3DS) in Comparison to other Techniques on the PSBALL Dataset.

Again, the 3D string kernel reaches better results than all meth-ods except for EM which is slightly better on the PSBALL dataset across different voxelizations. On PSB302, the 3D string kernel yields the best result. It outperforms all other method with a mar-gin of 4% for 15 voxels, 3% for 20 voxels, and 2% for 25 voxels. On all three datasets, the results achieved by the 3D string kernel are robust with respect to different voxelizations and do not change significantly.
We have presented a novel approach to 3D object classification based on feature strings and 3D string kernels as measure of sim-ilarity. We have shown that the feature string extraction can be performed in linear runtime with respect to the number of voxels. For all of our string kernels, we have given proofs and algorithms that they can be computed in time linear to the voxel grid resolu-tion on our feature strings. Our algorithm is therefore suited for Figure 8: NTUALL Optimized-Kernel (3DS) in Comparison to other Techniques on the PSB302 Dataset. 3D object classification on large datasets. If datasets do not fit into main memory, our kernel can be readily combined with an M-tree, as it induces a metric on 3D objects. In comparison experiments with four state-of-the-art techniques on two published benchmark datasets, our approach was always best or second best.

The good performance of our method might be caused by both the feature strings and the kernel we use. Our feature strings de-scribe the distribution of filled voxels over the voxel grid in three dimensions separately. Our kernel gives high similarity scores to two 3D objects if the voxel distributions in corresponding dimen-sions are similar.

One could also interpret our feature strings and string kernels as a histogram-based approach. In fact, we create histograms of the distribution of voxels in each dimension and then represent these histograms by strings instead of vectors. Unlike a histogram vec-tor approach that calculates some distance between corresponding bins in two histograms, our approach allows to measure similarity between neighboring bins as well. This is made possible by looking at k-mers made up of two or more different characters.

We have defined three different kernels on our 3D feature strings: one that examines all common k-mers (all k-mer kernel) in two strings, one that considers all common k-mers up to a fixed k (lrc kernel), and a third that looks at k-mers of one fixed k-mer length only (k-spectrum kernel). Among these, the lrc kernel outperforms all others in all of our experiments.

The lrc-kernel yields better results than the all k-mer kernel be-cause it does not overweight long matches, whereas a match that is one character longer gets twice the weight by the all k-mer ker-nel. Therefore the all k-mer kernel considers one long match much stronger than several shorter matches. According to our experi-ments, this is not a good choice in 3D object feature strings classi-fication and many shorter matches seem to indicate similarity.
The opposite of the all k-mer kernel, the k-spectrum kernel that considers k-mers of one fixed length only, is also not as successful as the lrc kernel in our experimental evaluation. The k-spectrum kernel considers object similar with many common fixed-length k-mers. However, in real-world datasets length of k-mers that are important for 3D object classification seem to differ in length.
As the lrc kernel considers all k-mers up to a fixed length only, it does not overweight long strings as the all k-mer kernel. As it is not limited to one k-mer length as the k-spectrum, it takes several potentially important k-mers lengths into account. These charac-teristics might explain its superior experimental performance.
In our benchmark experiments with state-of-the-art methods, the lrc kernel is successful and always comes in first or second. It is as accurate as the EM method on NTUALL at 15 voxel resolution. On the remaining resolutions for NTUALL and on PSBALL, the lrc kernel performs slightly worse than EM, but better than all other state-of-the-art approaches. On PSB302, our method outperforms all other techniques, at all resolutions.

A possible explanation for these results is the fact that the 3D string kernel finds objects that are very similar to each other as it examines the location of each voxel individually, whereas EM is better suited for remote similarity detection as it summarizes the object characteristics in terms of eigenvalues of the covariance ma-trix. PSBALL and NTUALL comprise many classes and many of those contain a few objects only, whereas PSB302 contains 302 ob-jects from 4 classes. If we are dealing with many small classes, re-mote similarity might be important, whereas few classes with many objects increase the chance that a query object is very similar to another object in its class. This would explain why the lrc kernel outperforms all others on PSB302, whereas EM is best on PSBALL and NTUALL.
The above experiments show that our 3D string kernel approach is comparable to the best competitor in classification accuracy.
As an additional advantage, our feature strings can be stored and processed in compressed format. The memory requirement for this compressed format is at maximum twice the resolution of the voxel grid, namely for each character and the number of its occurrences. This low memory requirement of our approach makes scaling to large datasets even easier.

Furthermore, our feature string description of 3D objects has at-tractive invariance properties: It is invariant with respect to scaling and translation. However, it is not rotational-invariant and requires -like many state-of-the-art approaches -a prior uniform orientation of the objects.

A further attractive feature of our feature string approach is the fact that all recent advances on string classification in machine learning can be transferred to the task of 3D object classification. For example, a technique using suffix trees that speeds up Support Vector Machine training on large sets of strings has recently been developed [27]. We could use this method on our feature strings as well and make them even more attractive for large 3D object dataset classification.

A possibility to enhance the accuracy of all k-mer kernel could be to assign smaller weights to longer strings, to avoid overweight-ing of longer matches. Furthermore, methods for learning weights of individual k-mers based on multiple kernel learning have re-cently been developed [28]. That could be exploited in our setting to detect k-mers that are especially important for correct classifica-tion of 3D objects. We plan to explore this weight learning on 3D object feature strings in future research. [1] M. Ankerst, G. Kastenm  X  uller, H.-P. Kriegel, and T. Seidl. 3D [2] A. Ben-Hur, D. Horn, H. Siegelmann, and V. Vapnik. A [3] H. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. Bhat, [4] C. J. C. Burges. A tutorial on support vector machines for [5] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On [6] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An efficient [7] H. Drucker, C. J. C. Burges, L. Kaufman, A. J. Smola, and [8] T. Funkhouser, P. Min, M. Kazhdan, J. Chen, A. Halderman, [9] D. Haussler. Convolutional kernels on discrete structures. [10] T. S. Jaakkola and D. Haussler. Exploiting generative models [11] T. Joachims. Making large-scale SVM learning practical. In [12] I. Jolliffe. Principal Component Analysis. Springer, 1986. [13] A. Kaufman. An Algorithm for 3D Scan-Conversion of [14] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. Rotation [15] H.-P. Kriegel, P. Kr  X  oger, Z. Mashael, M. Pfeifle, M. P  X  otke, [16] C. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: A [17] C. Leslie, E. Eskin, J. Weston, and W. S. Noble. Mismatch [18] L. Liao and W. S. Noble. Combining pairwise sequence [19] S. Mika, G. R  X  atsch, J. Weston, B. Sch  X  olkopf, and K.-R. [20] R. Osada, T. Funkhouser, B. Chazelle, and D. Dobkin. [21] G. R  X  atsch, S. Sonnenburg, and B. Sch  X  olkopf. Rase: [22] B. Sch  X  olkopf. Support Vector Learning . R. Oldenbourg [23] B. Sch  X  olkopf and A. Smola. Learning with Kernels . MIT [24] B. Sch  X  olkopf, A. J. Smola, and K.-R. M  X  uller. Kernel [25] J.-L. Shih, C.-H. Lee, and J. Wang. 3D object retrieval [26] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser. The [27] S. Sonnenburg, G. R  X  atsch, and B. Sch  X  olkopf. Large scale [28] S. Sonnenburg, G. R  X  atsch, and B. Sch  X  olkopf. A General and [29] V. Vapnik. Statistical Learning Theory . John Wiley and [30] S. V. N. Vishwanathan and A. J. Smola. Fast kernels for [31] C. Watkins. Dynamic alignment kernels. CSD-TR-98-11, [32] A. Zien, G. R  X  atsch, S. Mika, B. Sch  X  olkopf, T. Lengauer, and
