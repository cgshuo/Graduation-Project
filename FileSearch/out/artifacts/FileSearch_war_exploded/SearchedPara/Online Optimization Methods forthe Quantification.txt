  X  The estimation of class prevalence, i.e., of the fraction of a pop-ulation that belongs to a certain class, is an important task in data analytics, and finds applications in many domains such as the social sciences, market research, epidemiology, and others. For example, in sentiment analysis the goal is often not to estimate whether a specific text conveys a positive or a negative sentiment, but rather to estimate the overall distribution of positive and negative senti-ments, e.g., in a certain time frame. A popular way of performing the above task, often dubbed quantification , is to use supervised learning in order to train a prevalence estimator from labeled data.
In the literature there are several performance metrics for mea-suring the success of such prevalence estimators. In this paper we propose the first online stochastic algorithms for directly optimiz-ing these quantification-specific performance measures. We also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance. Our algorithms present a significant advancement in the theory of mul-tivariate optimization; we show, via a rigorous theoretical analysis, that they exhibit optimal convergence. We also report extensive ex-periments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization tech-niques used for these performance measures. Quantification [11] is defined as the task of estimating the preva-lence (i.e., relative frequency) of the classes of interest in an unla-beled set, given a training set of items labeled according to the same classes. Quantification finds its natural application in contexts char-acterized by distribution drift , i.e., contexts where the training data  X  This work was done while Shuai Li was research associate at QCRI-HBKU.  X 
Sanjay Chawla is on leave from University of Sydney.  X  Fabrizio Sebastiani is on leave from Consiglio Nazionale delle Ricerche, Italy.
 may not exhibit the same class prevalence pattern as the test data. This phenomenon may be due to different reasons, including the inherent non-stationary character of the context, or class bias that affects the selection of the training data.

A na X ve way to tackle quantification is via the  X  X lassify and count X  (CC) approach, i.e., to classify each unlabeled item independently and compute the fraction of the unlabeled items that have been at-tributed to each class. However, a good classifier does not nec-essarily lead to a good quantifier: assuming the binary case, even if the sum (FP + FN) of the false positives and false negatives is comparatively small, bad quantification accuracy might result if FP and FN are significantly different (since perfect quantification co-incides with the case FP = FN). This has led researchers to study quantification as a task in its own right, rather than as a byproduct of classification.

The fact that quantification is not just classification in disguise can also be seen by the fact that evaluation measures different from those for classification (e.g., F 1 , AUC) need to be employed. Quan-tification actually amounts to computing how well an estimated class distribution  X  p fits an actual class distribution p (where for any class c  X  C , p ( c ) and  X  p ( c ) respectively denote its true and esti-mated prevalence); as such, the natural way to evaluate the quality of this fit is via a function from the class of f -divergences [7], and a natural choice from this class (if only for the fact that it is the best known f -divergence) is the Kullback-Leibler Divergence (KLD), defined as Indeed, KLD is the most frequently used measure for evaluating quantification (see e.g., [3, 10, 11, 12]). Note that KLD is non-decomposable, i.e., the error we make by estimating p via  X  p cannot be broken down into item-level errors. This is not just a feature of KLD, but an inherent feature of any measure for evaluating quan-tification. In fact, how the error made on a given unlabeled item impacts the overall quantification error depends on how the other items have been classified 1 ; e.g., if FP &gt; FN for the other unla-beled items, then generating an additional false negative is actually beneficial to the overall quantification accuracy, be it measured via KLD or via any other function.

The fact that KLD is the measure of choice for quantification and
For the sake of simplicity, we assume here that quantification is to be tackled in an aggregative way, i.e., the classification of indi-vidual items is a necessary intermediate step for the estimation of class prevalences. Note however that this is not necessary; non-aggregative approaches to quantification may be found in [14, 22]. that it is non-decomposable, has lead to the use of structured out-put learners, such as SVM perf [17], that allow a direct optimization of non-decomposable functions; the approach of Esuli and Sebas-tiani [9, 10] is indeed based on optimizing KLD using SVM However, that minimizing KLD (or | FP  X  FN | , or any  X  X ure X  quan-tification measure) should be the only objective for quantification regardless of the value of FP + FN (or any other classification mea-sure), is fairly paradoxical. Some authors [3, 24] have observed that this might lead to the generation of unreliable quantifiers (i.e., systems with good quantification accuracy but bad or very bad clas-sification accuracy), and have, as a result, championed the idea of optimizing  X  X ulti-objective X  measures that combine quantification accuracy with classification accuracy. Using a decision-tree-like approach, [24] minimizes | FP 2  X  FN 2 | , which is the product of | FN  X  FP | , a measure of quantification error, and ( FN + FP ) , a measure of classification error; [3] also optimizes (using SVM a measure that combines quantification and classification accuracy.
While SVM perf does provide a recipe for optimizing general per-formance measures, it has serious limitations. SVM perf is not de-signed to directly handle applications where large streaming data sets are the norm. SVM perf also does not scale well to multi-class settings, and the time required by the method is exponential in the number of classes.

In this paper we develop stochastic methods for optimizing a large family of popular quantification performance measures. Our methods can effortlessly work with streaming data and scale to very large datasets, offering training times up to an order of magnitude faster than other approaches such as SVM perf . Quantification methods. The quantification methods that have been proposed over the years can be broadly classified into two classes, namely aggregative and non-aggregative methods. While aggregative approaches perform quantification by first classifying individual items as an intermediate step, non-aggregative approaches do not require this step, and estimate class prevalences holistically. Most methods, such as those of [3, 4, 10, 11, 24], fall in the former class, while the latter class has few representatives [14, 22].
Within the class of aggregative methods, a further distinction can be made between methods, such as those of [4, 11], that first use general-purpose learning algorithms and then post-process their prevalence estimates to account for their estimation biases, and methods (which we have already hinted at in Section 1) that in-stead use learning algorithms explicitly devised for quantification [3, 10, 24]. In this paper we focus the latter class of methods. Applications of quantification. From an application perspective, quantification is especially useful in fields (such as social science, political science, market research, and epidemiology) which are in-herently interested in aggregate data, and care little about individual cases. Aside from applications in these fields [16, 22], quantifica-tion has also been used in contexts as diverse as natural language processing [6], resource allocation [11], tweet sentiment analysis [12], and the veterinary sciences [14]. Quantification has indepen-dently been studied within statistics [16, 22], machine learning [2, 8, 29], and data mining [10, 11]. Unsurprisingly, given this varied literature, quantification also goes under different names, such as counting [23], class probability re-estimation [1], class prior esti-mation [6], and learning of class balance [8].

In some applications of quantification, the estimation of class prevalences is not an end in itself, but is rather used to improve the accuracy of other tasks such as classification. For instance, Ba-likas et al. [2] use quantification for model selection in supervised learning, by tuning hyperparameters that yield the best quantifica-tion accuracy on validation data; this allows hyperparameter tun-ing to be performed without incurring the costs inherent to k -fold cross-validation. Saerens et al. [29], followed by other authors [1, 32, 34], apply quantification to customize a trained classifier to the class prevalence exhibited in the test set, with the goal of improving classification accuracy on unlabeled data exhibiting a class distri-bution different from that of the training set. The work of Chan and Ng [6] may be seen as a direct application of this notion, as they use quantification to tune a word sense disambiguator to the esti-mated sense priors of the test set. Their work can also be seen as an instance of transfer learning (see e.g., [26]), since their goal is to adapt a word sense disambiguation algorithm to a domain different from the one the algorithm was trained upon.
 Stochastic optimization. As discussed in Section 1, our goal in this paper is to perform quantification by directly optimizing, in an online stochastic setting, specific performance measures for the quantification problem. While recent advances have seen much progress in efficient methods for online learning and optimization in full information and bandit settings [5, 13, 15, 30], these works frequently assume that the optimization objective, or the notion of regret being considered is decomposable and can be written as a sum or expectation of losses or penalties on individual data points. However, performance measures for quantification have a multi-variate and complex structure, and do not have this form.
There has been some recent progress [20, 25] towards develop-ing stochastic optimization methods for such non-decomposable measures. However, these approaches do not satisfy the needs of our problem. The work of Kar et al. [20] addresses the prob-lem of optimizing structured SVM perf -style objectives in a stream-ing fashion, but requires the maintenance of large buffers and, as a result, offers poor convergence. The work of Narasimhan et al. [25] presents online stochastic methods for optimizing perfor-mance measures that are concave or pseudo-linear in the canoni-cal confusion matrix of the predictor. However, their method re-quires the computation of gradients of the Fenchel dual of the per-formance measures, which is difficult for the quantification perfor-mance measures that we study, that have a nested structure. Our methods extend the work of [25] and provide convenient routines for optimizing the more complex performance measures used for evaluating quantification. For the sake of simplicity, in this paper we will restrict our anal-ysis to binary classification problems and linear models. We will denote the space of feature vectors by X  X  R d and the label set by Y = { X  1 , +1 } . We shall assume that data points are gen-erated according to some fixed but unknown distribution D over X  X Y . We will denote the proportion of positives in the popula-tion by p := Pr will receive a set of T training points sampled from D , which we will denote by T = { ( x 1 ,y 1 ) ,..., ( x T ,y T ) } .
As mentioned above, we will present our algorithms and analy-ses for learning a linear model over X . We will denote the model space by W  X  R d and let R X and R W denote the radii of do-main X and model space W , respectively. However, we note that our algorithms and analyses can be extended to learning non-linear models by use of kernels, as well as to multi-class quantification problems. However, we postpone a discussion of these extensions to an expanded version of this paper.

Our focus in this work shall be the optimization of quantification-specific performance measures in online stochastic settings. We will concentrate on performance measures that can be represented as functions of the confusion matrix of the classifier. In the binary setting, the confusion matrix can be completely described in terms of the true positive rate (TPR) and the true negative rate (TNR) of the classifier. However, initially we will develop algorithms that use reward functions as surrogates of the TPR and TNR values. This is done to ease algorithm design and analysis, since the TPR and TNR values are count-based and form non-concave and non-differentiable estimators. The surrogates we will use will be con-cave and almost-everywhere differentiable. More formally, we will use a reward function r that assigns a reward r ( X  y,y ) to a predic-tion  X  y  X  R for a data point, when the true label for that data point is y  X  Y . Given a reward function r , a model w  X  W , and a data point ( x ,y )  X  X   X Y , we will use to calculate rewards on positive and negative points. The average or expected value of these rewards will be treated as surrogates of TPR and TNR respectively. Note that since E
E the classification accuracy function, yields E TPR ( w ) . Here I [  X  ] denotes the indicator function. For the sake of convenience we will use P ( w ) = E
E functions. We shall assume that our reward function r is concave, L -Lipschitz, and takes values in a bounded range [  X  B r ,B Examples of Surrogate Reward Functions. Some examples of reward functions that are surrogates for the classification accuracy indicator function I [ y  X  y &gt; 0] are the inverted hinge loss function and the inverted logistic regression function We will also experiment with non-surrogate (dubbed NS) versions of our algorithms which use TPR and TNR values directly. These will be discussed in Section 5. The task of quantification requires estimating the distribution of unlabeled items across a set C of available classes, with |C| = 2 in the binary setting. In our work we will target quantification per-formance measures as well as  X  X ybrid X  classification-quantification performance measures. We discuss them in turn.
 KLD: Kullback-Leibler Divergence. In recent years this perfor-mance measure has become a standard in the quantification litera-ture, in the evaluation of both binary and multiclass quantification [3, 10, 12]. We redefine KLD below for convenience.
 For distributions p,p 0 over C , the values KLD ( p,p 0 ) can range be-tween 0 (perfect quantification) and +  X  . 2 . Note that since KLD is a distance function and all our algorithms will be driven by reward maximization, for uniformity we will, instead of trying to minimize KLD, try to maximize  X  KLD; we will call this latter NegKLD. NSS: Normalized Squared Score. This measure of quantifica-tion accuracy was introduced in [3], and is defined as NSS = formance measure attempts to reduce | FN  X  FP | , a direct measure of quantification error.
 We recall from Section 1 that several works have advocated the use of hybrid,  X  X ulti-objective X  performance measures, that try to balance quantification and classification performance. These mea-sures typically take a quantification performance measure such as KLD or NSS, and combine it with a classification performance measure. Typically, a classification performance measure that is sensitive to class imbalance [25] is chosen, such as Balanced Ac-curacy BA = 1 2 ( TPR + TNR ) [3], F-measure, or G-mean [25]. Two such hybrid performance measures that are discussed in literature are presented below.
 CQB: Classification-Quantification Balancing. The work of [24] introduced this performance measure in an attempt to compromise between classification and quantification accuracy. As discussed in Section 1, this performance measure is defined as i.e. a product of | FN  X  FP | , a measure of quantification error, and ( FN + FP ) , a measure of classification error.
 QMeasure. The work of Barranquero et al. [3] introduced a generic scheme for constructing hybrid performance measures, using the so-called Q-measure defined as that is, a weighted combination of a measure of classification accu-racy P class and a measure of quantification accuracy P quant the sake of simplicity, in our experiments we will adopt BA = ( TPR + TNR ) as our P class and NSS as our P quant . However, we stress that our methods can be suitably adapted to work with other choices of P class and P quant .
KLD is not a particularly well-behaved performance measure, since it is capable of taking unbounded values within the compact domain of the unit simplex. This poses a problem for optimization algorithms from the point of view of convergence, as well as nu-merical stability. To solve this problem, while computing KLD for two distributions p and  X  p , we can perform an additive smoothing of both p ( c ) and  X  p ( c ) by computing where p s ( c ) denotes the smoothed version of p ( c ) . The denomi-nator here is just a normalizing factor. The quantity = 1 often used as a smoothing factor, and is the one we adopt here. The smoothed versions of p ( c ) and  X  p ( c ) are then used in place of the non-smoothed versions in Equation 1. We can show that, as a result, KLD is always bounded by KLD ( p s ,  X  p s )  X  O log However, we note that the smoothed KLD still returns a value of 0 when p and  X  p are identical distributions.
We also introduce three new hybrid performance measures in this paper as a way of testing our optimization algorithms. We define these below and refer the reader to Tables 1 and 2 for details. BAKLD. This hybrid performance measure takes a weighted aver-age of BA and NegKLD; i.e. BAKLD = C  X  BA +(1  X  C )  X  (  X  KLD ) . This performance measure gives the user a strong handle on how much emphasis to place on quantification and how much on clas-sification performance. We will use BAKLD in our experiments to show that our methods offer an attractive tradeoff between the two.
We now define two hybrid performance measures that are con-structed by taking the ratio of a classification and a quantification performance measures. The aim of this exercise is to obtain perfor-mance measures that mimic the F-measure, which is also a pseu-dolinear performance measure [25]. The ability of our methods to directly optimize such complex performance measures will be in-dicative of their utility in terms of the freedom they allow the user to design objectives in a data-and task-specific manner. CQReward and BKReward. These hybrid performance measures are defined as CQReward = BA 2  X  NSS and BKReward = BA 1+ KLD tice that both performance measures are optimized when the nu-merator i.e. BA is large, and the denominator is small which trans-lates to NSS being large for CQReward and KLD being small for BKReward. Clearly, both performance measures encourage good performance with respect to both classification and quantification and penalize a predictor which either neglects quantification to get better classification performance, or the other way round.
In the past section we have introduced a variety of quantification and hybrid performance measures. Of these, NegKLD, NSS, and Q-measure were already prevalent in quantification literature and we introduced BAKLD, CQReward and BKReward. Our aim be-hind exploring a large variety of performance measures is to both, demonstrate the utility of our methods with respect to the quantifi-cation problem, as well as present newer ways of designing hybrid performance measures that give the user more expressivity in tai-loring the performance measure to the task at hand.

We also note that these performance measures have extremely diverse and complex structures. We can show that NegKLD, Q-measure, and BAKLD are nested concave functions, more specif-ically, concave functions of functions that are themselves concave in the confusion matrix of the predictor. On the other hand, CQRe-ward and BKReward turn out to be pseudo-concave functions of the confusion matrix. Thus, we are working with two very dif-ferent families of performance measures here, each of which has different properties and require different optimization techniques.
In the following section, we introduce two novel methods to op-timize these two families of performance measures. The previous discussion in Sections 1 and 2 clarifies two aspects of efforts in the quantification literature. Firstly, specific perfor-mance measures have been developed and adopted for evaluating quantification performance including KLD, NSS, Q-measure etc. Secondly, algorithms that directly optimize these performance mea-sures are desirable, as is evidenced by recent works [3, 9, 10, 24].
The works mentioned above make use of tools from optimiza-tion literature to learn linear (e.g. [10]) and non-linear (e.g. [24]) models to perform quantification. The state of the art efforts in this direction have adopted the structural SVM approach for optimizing these performance measures with great success [3, 10]. However, this approach comes with severe drawbacks.

The structural SVM [17], although a significant tool that allows optimization of arbitrary performance measures, suffers from two key drawbacks. Firstly, the structural SVM surrogate is not nec-essarily a tight surrogate for all performance measures, something that has been demonstrated in past literature [21, 25], which can lead to poor training. But more importantly, optimizing the struc-tural SVM surrogate requires the use of expensive cutting plane methods which are known to scale poorly with the amount of train-ing data, as well as are unable to handle streaming data.
To alleviate these problems, we propose stochastic optimization algorithms that directly optimize a large family of quantification performance measures. Our methods come with sound theoretical convergence guarantees, are able to operate with streaming data sets and, as our experiments will demonstrate, offer much faster and accurate quantification performance on a variety of data sets.
Our optimization techniques introduce crucial advancements in the field of stochastic optimization of multivariate performance measures and address the two families of performance measures discussed while concluding Section 3  X  1) nested concave per-formance measures and 2) pseudo-concave performance measures. We describe these in turn below. The first class of performance measures that we deal with are con-cave combinations of concave performance measures. More for-mally, given three concave functions  X  , X  1 , X  2 : R 2  X  R define a performance measure where we have where P ( w ) and N ( w ) can respectively denote, either the TPR and TNR values or surrogate reward functions therefor. Exam-ples of such performance measures include the negative KLD per-formance measure and the QMeasure which are described in Sec-tion 3.1. Table 1 describes these performance measures in canoni-cal form i.e. their expressions in terms of TPR and TNR values.
Before describing our algorithm for nested concave measures, we recall the notion of concave Fenchel conjugate of concave func-tions. For any concave function f : R 2  X  R and any ( u,v )  X  the (concave) Fenchel conjugate of f is defined as Clearly, f  X  is concave. Moreover, it follows from the concavity of f that for any ( x,y )  X  R 2 , Below we state the properties of strong concavity and smoothness. These will be crucial in our convergence analysis.
 A function f : R d  X  R is said to be  X  -strongly concave and  X  -smooth if for all x , y  X  R d , we have  X   X  2 We will assume that the functions  X  , X  1 , and  X  2 defining our per-formance measures are  X  -smooth for some constant  X  &gt; 0 . This is true of all functions, save the log function which is used in the definition of the KLD quantification measure. However, if we carry out the smoothing step pointed out in Section 3.1 with some &gt; 0 , then it can be shown that the KLD function does become O 1 smooth. An important property of smooth functions, that would be crucial in our analyses, is a close relationship between smooth and strongly convex functions
T HEOREM 2 ([33]). A closed, concave function f is  X  smooth iff its (concave) Fenchel conjugate f  X  is 1  X  -strongly concave.
We are now in a position to present our algorithm NEMSIS for stochastic optimization of nested concave functions. Algorithm 1 gives an outline of the technique. We note that a direct applica-tion of traditional stochastic optimization techniques [31] to such nested performance measures as those considered here is not possi-ble as discussed before. NEMSIS , overcomes these challenges by exploiting the nested dual structure of the performance measure by carefully balancing updates at the inner and outer levels. At every time step, NEMSIS performs four very cheap updates. The first update is a primal ascent update to the model vector which takes a weighted stochastic gradient descent step. Note that this step involves a projection step to the set of model vectors W de-noted by  X  W (  X  ) . In our experiments W was defined to be the set of all Euclidean norm-bounded vectors so that projection could be effected using Euclidean normalization which can be done in O ( d ) time if the model vectors are d -dimensional.

The weights of the descent step are decided by the dual param-eters of the functions  X  , X  1 , and  X  2 . Then NEMSIS updates the dual variables in three simple steps. In fact line numbers 15-17 can be executed in closed form (see Table 1) for all the performance measures we see here which allows for very rapid updates. See Appendix A for the simple derivations.

Below we state the convergence proof for NEMSIS . We note that despite the complicated nature of the performance measures being tackled, NEMSIS is still able to recover the optimal rate of convergence known for stochastic optimization routines. The proof 3 requires a careful analysis of the primal and dual update steps at different levels and tying the updates together by taking into ac-count the nesting structure of the performance measure.

T HEOREM 3. Suppose we are given a stream of random sam-ples ( x 1 ,y 1 ) ,..., ( x T ,y T ) drawn from a distribution D over X X  Y . Let Algorithm 1 be executed with step sizes  X  t =  X (1 / a nested concave performance measure  X (  X  1 (  X  ) , X  2 (  X  )) . Then, for some universal constant C , the average model w = 1 T P T output by the algorithm satisfies, with probability at least 1  X   X  , P
All proofs are presented in the expanded version of this paper which may be accessed at http://arxiv.org/abs/1605.04135 Algorithm 1 NEMSIS : NEsted priMal-dual StochastIc updateS 2: while data stream has points do 4: // Perform primal ascent 8: else 11: end if 14: // Perform dual updates 18: t  X  t + 1 19: end while constant C and L g denotes the Lipschitz constant of the function g . Narasimhan et al [25] proposed the SPADE algorithm, which of-fers stochastic optimization of concave performance measures. We note that although the performance measures considered here are indeed concave, it is difficult to apply SPADE to them directly since SPADE requires the computation of gradients of the Fenchel dual of the function P ( X  , X  1 , X  2 ) , which are difficult to compute given the nested structure of this function. NEMSIS , on the other hand, only requires the duals of the individual functions  X  , X  1 are much more accessible. Moreover, NEMSIS uses a much sim-pler dual update which does not involve any parameters and, in fact, has a closed form solution in all our cases. SPADE , on the other hand, performs dual gradient descent, which requires a fine tuning of yet another step length parameter. A third benefit of NEMSIS is that it achieves a logarithmic regret with respect to its dual updates (see the proof of Theorem 3) whereas SPADE incurs a polynomial regret due to its gradient descent-style dual update. The next class of performance measures we consider can be ex-mance measure. More formally, given a convex quantification per-formance measure P quant and a concave classification performance Table 2: List of pseudo-concave performance measures and their canonical expressions in terms of the confusion matrix  X ( P,N ) . Note that p and n denote the proportion of positives and negatives in the population.
 measure P class , we can define a performance measure We assume that both the performance measures, P quant and P are positive valued. Such performance measures can be very useful in allowing a system designer to balance classification and quantifi-cation performance. Moreover, the form of the measure allows an enormous amount of freedom in choosing the quantification and classification performance measures. Examples of such perfor-mance measures include the CQReward and the BKReward mea-sures. These were introduced in Section 3.1 and are represented in their canonical forms in Table 2.

Performance measures, constructed the way described above, with a ratio of a concave over a convex measures, are called pseudo-concave measures. This is because, although these functions are not concave, their level sets are still convex, which makes it possi-ble to optimize them efficiently. To see the intuition behind this, we need to introduce the notion of the valuation function correspond-ing to the performance measure. As a passing note, we remark that because of the non-concavity of these performance measures, NEMSIS cannot be applied here.

D EFINITION 4 (V ALUATION F UNCTION ). The valuation of a at any level v &gt; 0 , is defined as It can be seen that the valuation function defines the level sets of the performance measure. To see this, notice that due to the positivity v iff V ( w ,v )  X  0 . However, since P class is concave, P convex, and v &gt; 0 , V ( w ,v ) is a concave function of w .
This close connection between the level sets and notions of val-uation functions have been exploited before to give optimization algorithms for pseudo-linear performance measures such as the F-measure [25, 27]. These approaches treat the valuation function as some form of proxy or surrogate for the original performance mea-sure and optimize it in hopes of making progress with respect to the original measure.

Taking this approach with our performance measures yields a very natural algorithm for optimizing pseudo-concave measures which we outline in the CAN algorithm Algorithm 2. CAN repeat-edly trains models to optimize their valuations at the current level, then upgrades the level itself. Notice that step 4 in the algorithm is a concave maximization problem over a convex set, something that can be done using a variety of methods  X  in the following we will see how NEMSIS can be used to implement this step. Also notice that step 5 can, by the definition of the valuation function,
It turns out that CAN has a linear rate of convergence for well-behaved performance measures. The next result formalizes this statement. We note that this result is similar to the one arrived by [25] but only for pseudo-linear functions.
 Algorithm 2 CAN : Concave AlternatioN 1: Construct the valuation function V 6: t  X  t + 1 7: end while 8: return w t Algorithm 3 SCAN : Stochastic Concave AlternatioN 2: repeat 3: // Learning phase 5: while t &lt; s e do 6: Receive sample ( x ,y ) 9: t  X  t + 1 10: end while 12: // Level estimation phase 14: while t &lt; s 0 e do 15: Receive sample ( x ,y ) 17: t  X  t + 1 18: end while 20: until stream is exhausted 21: return w e
T HEOREM 5. Suppose we execute Algorithm 2 with a pseudo-concave performance measure P ( P quant , P class ) such that the quan-tification performance measure always takes values in the range [ m,M ] , where M &gt; m &gt; 0 . Let P  X  := sup w  X  X  P ( P be the optimal performance level and  X  t = P  X   X  X  ( P quant be the excess error for the model w t generated at time t . Then, for every t &gt; 0 , we have  X  t  X   X  0  X  1  X  m M t .
 This theorem generalizes the result of [25] to the more general case of pseudo-concave functions. Note that for the pseudo-concave functions defined in Table 2, care is taken to ensure that the quan-tification performance measure satisfies m &gt; 0 .

A drawback of CAN is that it cannot operate in streaming data settings and requires a concave optimization oracle. However, we notice that for the performance measures in Table 2, the valuation function is always at least a nested concave function. This moti-vates us to use NEMSIS to solve the inner optimization problems in an online fashion. Combining this with an online technique to approximately execute step 5 of of the CAN and gives us the SCAN algorithm, outlined in Algorithm 3.

Thoerem 6 shows that SCAN enjoys a convergence rate simi-lar to that of NEMSIS . Indeed, SCAN is able to guarantee an -approximate solution after witnessing e O 1 / 2 samples which is equivalent to a convergence rate of e O 1 / result (refer to footnote 3) is obtained by showing that CAN is ro-bust to approximate solutions to the inner optimization problems.
T HEOREM 6. Suppose we execute Algorithm 3 with a pseudo-ways takes values in the range [ m,M ] with m &gt; 0 , with epoch of increase, where the constant C  X  , X  1 , X  2 ,r is the effective constant for the NEMSIS analysis (Theorem 3) for the inner invocations of NEMSIS in SCAN . Also let the excess error for the model w erated after e epochs be denoted by  X  e = P  X   X  X  ( P quant Then after e = O log 1 log 2 1 epochs, we can ensure with probability at least 1  X   X  that  X  e  X  . Moreover, the number of samples consumed till this point, ignoring universal constants, is at most C Narasimhan et al [25] also proposed two algorithms AMP and STAMP which seek to optimize pseudo-linear performance mea-sures. However, neither those algorithms nor their analyses transfer directly to the pseudo-concave setting. This is because, by exploit-ing the pseudo-linearity of the performance measure, AMP and STAMP are able to convert their problem to a sequence of cost-weighted optimization problems which are very simple to solve. This convenience is absent here and as mentioned above, even af-ter creation of the valuation function, SCAN still has to solve a possibly nested concave minimization problem which it does by invoking the NEMSIS procedure on this inner problem. The proof technique used in [25] for analyzing AMP also makes heavy use of pseudo-linearity. The convergence proof of CAN , on the other hand, is more general and yet guarantees a linear convergence rate. We carried out an extensive set of experiments on diverse set of benchmark and real-world data to compare our proposed methods with other state-of-the-art approaches.

Data sets : We used the following benchmark data sets from the UCI machine learning repository : a) IJCNN, b) Covertype, c) Adult, d) Letters, and e) Cod-RNA. We also used the following three real-world data sets: a) Cheminformatics, a drug discovery data set from [19], b) 2008 KDD Cup challenge data set on breast cancer detection, and c) a data set pertaining to a protein-protein interaction (PPI) prediction task [28]. In each case, we used 70% of the data for training and the remaining for testing.

Methods : We compares our proposed NEMSIS and SCAN al-gorithms 4 against the state-of-the-art one-pass mini-batch stochas-tic gradient method ( 1PMB ) of [20] and the SVM perf technique of [18]. Both these techniques are capable of optimizing structural SVM surrogates of arbitrary performance measures and we mod-
We will make code for our methods available publicly. ified their implementations to suitably adapt them to the perfor-mance measures considered here. The NEMSIS and SCAN imple-mentations used the hinge-based concave surrogate.

Non-surrogate NS Approaches : We also experimented with a variant of the NEMSIS and SCAN algorithms, where the dual up-dates were computed using original count based TPR and TNR values, rather than surrogate reward functions. We refer to this version as NEMSIS-NS . We also developed a similar version of SCAN called SCAN-NS where the level estimation was performed using 0-1 TPR/TNR values. We empirically observed these non-surrogate versions of the algorithms to offer superior and more sta-ble performance than the surrogate versions.

Parameters : All parameters including step sizes, upper bounds on reward functions, regularization parameters, and projection radii were tuned from the values { 10  X  4 , 10  X  3 ,..., 10 held-out portion of the training set treated as a validation set. For step sizes, the base step length  X  0 was tuned from the above set and the step lengths were set to  X  t =  X  0 / the parameter setting in [20], setting the buffer size to 500 and the number of passes to 25.
 Comparison on NegKLD: We first compare NEMSIS-NS and NEMSIS against the baselines 1PMB and SVM perf on several data sets on the negative KLD measure. The results are presented in Figure 1. It is clear that the proposed algorithms have comparable performance with significantly faster rate of convergence. Since SVM perf is a batch/off-line method, it is important to clarify how it was compared against the other online methods. In this case, timers were embedded inside the SVM perf code, and at regular in-tervals, the performance of the current model vector was evaluated. It is clear that SVM perf is significantly slower and its behavior is quite erratic. The proposed methods are often faster than 1PMB . On three of the four data sets NEMSIS-NS achieves a faster rate of convergence compared to NEMSIS .

Comparison on BAKLD: We also used the BAKLD perfor-mance measure to evaluate the trade-off NEMSIS offers between quantification and classification performance. The weighting pa-rameter C in BAKLD (see Table 1), denoted here by CWeight to avoid confusion, was varied from 0 to 1 across a fine grid; for each value, NEMSIS was used to optimize BAKLD and its performance on BA and KLD were noted separately. In the results presented in Figure 2 for three data sets, notice that there is a sweet spot where the two tasks, i.e. quantification and classification simultaneously have good performance.

Comparison under varying class proportions: We next evalu-ated the robustness of the algorithms across data sets with varying different class proportions (see Table 3 for the dataset label propor-tions). In Figure 3, we plot positive KLD (smaller values are better) for the proposed and baseline methods for these diverse datasets. Again, it is clear that the NEMSIS family of algorithms of has better KLD performance compared to the baselines, demonstrating their versatility across a range of class distributions.
Comparison under varying drift: Next, we test the perfor-mance of the NEMSIS family of methods when there are drifts in class proportions between the train and test sets. In each case, we retain the original class proportion in the train set, and vary the class proportions in the test set, by suitably sampling from the original set of positive and negative test instances. 5 We have not included SVM perf in these experiments as it took an inordinately long time to complete. As seen in Figure 4, on the Adult and Letter data set the NEMSIS family is fairly robust to small class drifts. As expected, when the class proportions change by a large amount in the test set (over 100 percent), all algorithms perform poorly.
More formally, we consider a setting where both the train and test sets are generated using the same conditional class distribution P ( Y = 1 | X ) , but with different marginal distributions over in-stances P ( X ) , and thus, have different class proportions. Further, in these experiments, we made a simplistic assumption that there is no label noise; hence for any instance x , P ( Y = 1 | X = x ) = 1 or 0 . Thus, we generated our test set with class proportion p simply setting P ( X = x ) to the following distribution: with prob-ability p 0 , sample a point uniformly from all points with P ( Y = 1 | X = x ) = 1 , and with probability 1  X  p 0 , sample a point uni-formly from all points with P ( Y = 1 | X = x ) = 0 .

Comparison on hybrid performance measures: Finally, we tested our methods in optimizing composite performance measures that strike a more nuanced trade-off between quantification and classification performance. Figures 5 contains results for the NEM-SIS methods while optimizing Q-measure (see Table 1), and Figure 6 contains results for SCAN-NS while optimizing CQReward (see Table 2). The proposed methods are often significantly better than the baseline 1PMB in terms of both accuracy and running time. Quantification, the task of estimating class prevalence in problem settings subject to distribution drift, has emerged as an important problem in machine learning and data mining. Our discussion jus-tified the necessity to design algorithms that specifically solve the quantification task, with a special emphasis on performance mea-sures such as the Kullback-Leibler divergence, a de facto standard in the literature. Figure 3: A comparison of the KLD performance of various methods on data sets with varying class proportions (see Table 3).
Figure 4: A comparison of the KLD performance of various methods when distribution drift is introduced in the test sets. In this paper we proposed a family of algorithms which includes NEMSIS , CAN , SCAN , and their non-surrogate versions, in or-der to address the online quantification problem. By abstracting NegKLD and other hybrid performance measures as nested con-cave or pseudo-concave functions, we designed provably correct and efficient algorithms for optimizing these performance measures in an online stochastic setting.

We validated our algorithms on several data sets under varying conditions, including class imbalance and distribution drift. The proposed algorithms demonstrate the ability to jointly optimize both quantification and classification tasks. To the best of our knowledge this is the first work which directly addresses the online quantifica-tion problem; as such, it opens up novel application areas. The authors thank the anonymous reviewers for their helpful com-ments. PK thanks the Deep Singh and Daljeet Kaur Faculty Fel-lowship, and the Research-I Foundation at IIT Kanpur for support. SL acknowledges the support from 7Pixel S.r.l., SyChip Inc., and Murata Japan. [1] Roc X o Ala X z-Rodr X guez, Alicia Guerrero-Curieses, and Jes X s [2] Georgios Balikas, Ioannis Partalas, Eric Gaussier, Rohit [3] Jos X  Barranquero, Jorge D X ez, and Juan Jos X  del Coz. [4] Antonio Bella, C X sar Ferri, Jos X  Hern X ndez-Orallo, and [5] Nicol X  Cesa-Bianchi, Alex Conconi, and Claudio Gentile. [6] Yee Seng Chan and Hwee Tou Ng. Estimating class priors in [7] Imre Csisz X r and Paul C. Shields. Information theory and [8] Marthinus C. du Plessis and Masashi Sugiyama.
 [9] Andrea Esuli and Fabrizio Sebastiani. Sentiment [10] Andrea Esuli and Fabrizio Sebastiani. Optimizing text [11] George Forman. Quantifying counts and costs via [12] Wei Gao and Fabrizio Sebastiani. Tweet sentiment: From [13] Claudio Gentile, Shuai Li, and Giovanni Zappella. Online [14] V X ctor Gonz X lez-Castro, Roc X o Alaiz-Rodr X guez, and [15] Elad Hazan, Adam Kalai, Satyen Kale, and Amit Agarwal. [16] Daniel J. Hopkins and Gary King. A method of automated [17] Thorsten Joachims. A support vector method for multivariate [18] Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. [19] Robert N. Jorissen and Michael K. Gilson. Virtual screening [20] Purushottam Kar, Harikrishna Narasimhan, and Prateek Jain. [21] Purushottam Kar, Harikrishna Narasimhan, and Prateek Jain. [22] Gary King and Ying Lu. Verbal autopsy methods with [23] David D. Lewis. Evaluating and optimizing autonomous text [24] Letizia Milli, Anna Monreale, Giulio Rossetti, Fosca [25] Harikrishna Narasimhan, Purushottam Kar, and Prateek Jain. [26] Weike Pan, Erheng Zhong, and Qiang Yang. Transfer [27] Shameem P. Parambath, Nicolas Usunier, and Yves [28] Yanjun Qi, Ziv Bar-Joseph, and Judith Klein-Seetharaman. [29] Marco Saerens, Patrice Latinne, and Christine Decaestecker. [30] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and [31] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and [32] Jack Chongjie Xue and Gary M. Weiss. Quantification and [33] Constantin Zalinescu. Convex Analysis in General Vector [34] Zhihao Zhang and Jie Zhou. Transfer estimation of evolving The derivation of the closed form updates for steps 15-17 in the NEMSIS algorithm (see Algorithm 1) starts with the observation that in all the nested concave performance measures considered here, the outer and the inner concave functions, namely  X  , X  are concave, continuous, and differentiable. The logarithm function is non-differentiable at 0 but the smoothing step (see Section ref-formulation) ensures that we will never approach 0 in our analyses or the execution of the algorithm. The derivations hinge on the following basic result from convex analysis [33]:
L EMMA 7. Let f be a closed, differentiable and concave func-tion and f  X  be its concave Fenchel dual. Then  X  f  X  = (  X  f ) i.e. for any x  X  X  and u  X  X   X  (the space of all linear functionals over X ), we have  X  f  X  ( u ) = x iff  X  f ( x ) = u .
 Using this result, we show how to derive the updates for  X  . The updates for  X  and  X  follow similarly. We have By first order optimality conditions, we get that  X  t can minimize the function h (  X  ) =  X   X  q t  X   X   X  (  X  ) only if q t =  X   X  ing Lemma 7, we get  X  t =  X   X ( q t ) . Using this technique, all the closed form expressions can be readily derived.

For the derivations of  X , X  for NegKLD, and the derivation of  X  for Q-measure, the derivations follow when we work with defini-tions of these performance measures with the TP and TN counts or cumulative surrogate reward values, rather than the TPR and TNR values and the average surrogate rewards.
