 Sequence labeling, the task of assigning labels y = y as computational biology, computer vision, and natural lan guage processing. Conditional random labeling tasks [1]. CRFs define the conditional distributio n P relating labels to the input sequence.
 we wish to minimize Hamming loss, which measures the number o f incorrect labels, gradient-based such as maximum likelihood or maximum margin training, are s olved instead. of the Hamming loss incurred by the maximum expected accurac y decoding (i.e., posterior decod-function Q ( ) which trades off between the accuracy of the approximation a nd the smoothness of to the empirical risk minimization criterion for Hamming lo ss. 2.1 Definitions of all possible output labels. Furthermore, for a pair of con secutive labels y sequence x , and a label position j , let f ( y the feature mapping of the CRF.
 an input sequence x as where we define the summed feature mapping , F partition function Z ( x ) = P any set of model parameters w . 2 2.2 Maximum a posteriori vs. maximum expected accuracy parsing labeling, arg max bility) value for each label separately. Note that P w ( y  X  | x ) maximum expected number of correct labels.
 In practice, maximum expected accuracy parsing often yield s more accurate results than Viterbi maximum expected accuracy parser. is probably not much worse than its accuracy on the training s et.
 usual notions of per-label accuracy (such as Hamming loss) a re typically not only nonconvex but also not amenable to optimization by methods that make use of gradient information. 3.1 Previous objective functions 3.1.1 Conditional log-likelihood log probability of the true parse according to the model, plu s a regularization term: minima of the objective function.
 incorrectly labeled training examples). 3.1.2 Pointwise conditional log likelihood posterior probabilities (or equivalently, sum of log poste riors) for each predicted label: though the model may not provide a good fit for the training dat a as a whole. regular conditional log likelihood when dealing with diffic ult-to-classify outlier labels. 3.1.3 Maximum margin training The notion of Hamming distance is incorporated directly in t he maximum margin training proce-dures of Taskar et al. [9]: and Tsochantaridis et al. [10].
 Here,  X ( y , y ( t ) ) denotes the Hamming distance between y and y ( t ) , and  X  F F linearly with the Hamming distance betweeen y ( t ) and y .
 example is an upper bound on the Hamming loss between the corr ect parse and its highest scoring a maximum margin framework may be poor minimizers of empiric al risk. 3.2 Training for maximum labelwise accuracy smooth objective function for maximum expected accuracy pa rsing which more closely approxi-mates our desired notion of empirical risk. 3.2.1 The labelwise accuracy objective function Consider the following objective function, we can express the condition that the algorithm predicts the correct label for y ( t ) posterior probabilities of correct and incorrect labels as function Q ( ) , we obtain in (7). However, R labelwise ( w : D ) is smooth for any finite  X  &gt; 0 . improves; however, the approximation itself also becomes l ess smooth and perhaps more difficult 3.2.2 The labelwise accuracy objective gradient objective. For a fixed parameter set w , let  X  y ( t ) posterior probability at position j . Also, for notational convenience, let y y , . . . , y j . Differentiating equation (9), we compute  X  w R labelwise ( w : D ) to be 4 Using equation (1), the inner term, P standard forward and backward matrices used for regular CRF inference, which we define here as The two difficult terms that do not follow from the forward and backward matrices have the form, where Q  X  compute terms of this type, we define gramming. In particular, we have the base cases  X   X  ( i, 1) = 1 { i = y  X   X   X  ( i, L ) = 0 . The remaining entries are given by the following recurrenc es:  X   X  ( i, j ) = X  X   X  ( i, j ) = X It follows that equation (15) is equal to where  X  computation takes approximately three times as long and use s twice the memory of the analogous (a) (b) generalization performance. 4.1 Simulation experiments uncorrupted testing sequence generated by the original HMM .
 log-likelihood, the maximum-margin method of Taskar et al. [9] as implemented in the SVMstruct maximum labelwise accuracy at any noise level. For levels of noise above 0.05, maximum labelwise accuracy performs significantly better than the other metho ds.
 The maximum margin method performed best when Viterbi decod ing was used, while the other three methods had better performance with MEA decoding. Interest ingly, with no noise present, maxi-MEA decoding (0.796). 4.2 Gene prediction experiments lem, we trained a CRF to predict protein coding genes in the ge nome of the fruit fly Drosophila melanogaster . The CRF labeled each base pair of a DNA sequence according to its predicted func-types: transitions between labels and trimer composition.
 (a) (b) (c) (d) possible values. The green curve ( f ( x ) =  X  log( 1  X  x initialized to their generative model estimates. likelihood, maximum pointwise likelihood, and maximum lab elwise accuracy. Each run was started show declines in training and testing set accuracy, despite increases in the objective function. efficient dynamic programming recurrences for all computat ions.
 technique minimizes the loss incurred by maximum a posteriori , rather than maximum expected accuracy than maximum expected accuracy parsers [3, 4, 5]; w e are currently exploring whether a similar relationship also exists between MCE methods and ou r proposed training objective. that it performs much better than maximum likelihood and max imum pointwise likelihood training as each training example can be considered independently wh en evaluating the objective function labeling tasks is needed to investigate these questions. SSG and CBD were supported by NDSEG fellowships. We thank And rew Ng for useful discussions.
