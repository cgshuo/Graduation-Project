 We consider the problem of recommending the best set of k items when there is an inherent ordering between items, expressed as a site of  X  X odfather II X ). Since this general problem is computation-ally intractable, we develop 3 approximation algorithms to solve this problem for various prerequisite structures (e.g., chain graphs, AND graphs, AND-OR graphs). We derive worst-case bounds for these algorithms for these structures, and experimentally evaluate these algorithms on synthetic data. We also develop an algorithm to combine solutions in order to generate even better solutions, and compare the performance of this algorithm with the other three. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information Filtering Algorithms, Experimentation, Theory Graph Theory, Prerequisites, Recommendation Algorithms
Traditional recommendation systems deal with the problem of recommending items or sets of items to users by using various ap-proaches [1, 17]. However, most of these approaches do not take into account prerequisites while recommending an item: A prereq-uisite of an item i is another item j that must be taken or consumed (watched, read, ...) in advance of i . For example, university courses often have prerequisites. If course i cannot be taken unless j has been completed, then it does not make sense to recommend to a student course i if j has not been taken. We could recommend both i and j , or perhaps we could recommend some other course k that may be less desirable than i but whose prerequisites have been met.
We are interested in the problem of prerequisites in the context of our CourseRank project at Stanford University. CourseRank is a social tool developed in our InfoLab and is used by students to evaluate courses and plan their academic program. CourseRank is currently used by over 9,000 Stanford students (out of 14,000); CourseRank goals is to recommend courses that are not just  X  X ood X  but also help students meet academic requirements [15]. (Aca-demic requirements describe the constraints on the courses needed to complete a major.) In addition, we would like to take into ac-count prerequisites, which the current production system does not take into account. Since this shortcoming is serious, we have de-veloped a model and algorithms for recommendations constrained by prerequisites, which we describe and evaluate in this paper. Our plan is to incorporate one of our algorithms into the production sys-tem.

Although our focus is on prerequisites in an academic environ-ment, prerequisites also arise in other recommendation contexts. Movies, for instance, often are best watched in a sequence. For example, the movie  X  X odfather I X  should be watched before  X  X od-father II, X  and both these movies should be watched before  X  X od-father III. X  The problem is even more acute when it comes to tele-vision serials and novels. TV serials, especially those of the drama genre, tend to proceed in sequential fashion, and need to be watched in sequence. Novels can be sequential as well. While movies tend to have relatively few sequels, a fiction series could have several books that should be read in order.

There are at least two ways to approach the problem of recom-mendations with prerequisites:  X  Ranking. We are given a set of items, each with an initial score  X  Set Recommendation. In the second case, we again have a set
In this paper we only study set recommendations. Although we have not yet carefully compared both options, initially set recom-mendations seem more attractive and easier for the user to interpret. If a user wishes to take or watch a single item, then a set recommen-dation with k = 1 yields the best item that can currently be taken. (As stated earlier, the top ranked item in the ranking scenario may not be yet watchable.) If the user is planning ahead and wants to take k courses or watch k episodes, set recommendations provide one or more  X  X ackage X  recommendations that make sense as a unit.
Additionally, recommending each item that satisfies prerequi-sites one-by-one, instead of a  X  X ackage X  recommendation of size k , can prove to be sub-optimal, since at the start, we might end up not choosing items which are prerequisites for a lot of  X  X ood X  items and as a result we may have to make poor recommendations later on. The challenge, therefore, is to not just recommend a package that satisfies prerequisites, but one that is the  X  X est X .
We also study various kinds of prerequisite constraints. For ex-ample, in some domains, it may be the case that in order to be able to take an item a , one needs to take either b or c . In other domains, in order to be able to take an item a , one might need both b and c .
This paper extends the work in our short paper [13], which dis-cussed only chain graphs. Our contributions are the following:  X  We define the problem of recommendations with prerequisites  X  We prove hardness of recommendations for certain classes of  X  We provide a PTIME dynamic programming algorithm for a  X  We give algorithms that can adapt to any prerequisite structure  X  We provide an algorithm that takes sets recommended by dif- X  We experimentally evaluate the algorithms for two kinds of pre-
We now formally define the problem of recommendation with prerequisites. We wish to recommend a set of k items from a set of items V . We are also given a directed acyclic graph G ( V , E ) , where the vertices v  X  V correspond to items, and directed edges ( u,v )  X  E correspond to prerequisites, i.e., item u needs to be taken before item v . We assume that each item in V has been al-ready assigned a score , which corresponds to how  X  X ood X  the item is. This score could be obtained by various approaches  X  content-based, collaborative filtering [17, 1, 6] etc. Note that we do not in-clude in G nor in V items that have already been taken or watched. That is, if item i has item j as a prerequisite, but j is already taken, then we can ignore j and its prerequisite edge.

Our task is to pick a set A , of size | A | = k , such that score ( A ) is maximized:
In addition, we also have the following constraint to ensure that prerequisites are satisfied:
Note that these equations above inherently assume that the items being recommended are independent of each other, except for those that are related via set E . That is, the scores of items (not connected through E ) do not change if we recommend them together or sepa-rately.
 Figure 1: Hierarchy of prerequisite structures studied in this paper
We call the prerequisite graph above an AND Graph , because in order to be able to take any node, all of the parents need to be taken as well. Another graph variant is called an OR Graph , where in order to be able to take any node, at least one of the parents needs to be taken. In this case, Eq. 2 is modified to: OR graphs are common in the course recommendations, since there could be many prerequisites courses with overlapping content. For example,  X  X rogramming in C X  and  X  X rogramming in Java X  are both parents of  X  X lgorithms X , even though only one of them needs to be taken. A third variant (and generalization of the previous two) is an AND-OR Graph , where at each node, either all of the parents need to be taken (i.e., the node is an AND node ), or at least one of the parents needs to be taken (i.e., the node is an OR node ).
We also consider Chain Graphs , a special case for which we can obtain exact solutions. First, we define a chain to be a sequence of items a 1  X  a 2  X  ...  X  a n , such that there exists only the follow-ing edges involving a 1 ,...,a n : ( a 1 ,a 2 ) , ( a 2 ,a Note however that n could be 1 , in which case the node has no edges either coming into or going out of it. For example, if the items we wish to recommend are movies, then nodes correspond-ing to movies Godfather I, II and III would form a chain as follows: Godfather I  X  Godfather II  X  Godfather III. On the other hand, the movie  X  X hawshank Redemption X  would form a singleton node with no edges either going in or coming out. A graph consisting of a set of chains is called a chain graph . If a 1  X  a 2  X  ...  X  a a chain, then a 1  X  a 2  X  ...  X  a i ,i  X  n is a sub-chain , while a  X  a 2  X  ...  X  a n  X  ...  X  a n + m ,m  X  0 is a super-chain .
Note that Chain Graph is a special case of an AND graph as well as an OR Graph, since if every item has at most one parent, then that parent would need to be selected for both AND and OR graphs. The hierarchy of prerequisite structures is displayed in Fig. 1. We study recommendations for each of these structures in this paper.
Our algorithms can be generalized to handle fuzzy prerequisites and generalized score functions, with different worst case guaran-tees, as discussed in the extended technical report [14]. In particu-lar, fuzzy prerequisites allow multiple scores for a given item based on whether or not the prerequisites for that item are present. Fur-thermore, with generalized scoring functions, item scores need not be independent. (In this paper, we assume scores are independent.) We find that picking the best set satisfying prerequisites is NP-Hard for OR, AND and AND-OR graphs. We first prove the hard-ness result for AND graphs, and then for OR graphs. Trivially, either one of these reductions would be applicable to AND-OR graphs, making them NP-Hard as well.
The problem of picking the best set A, | A | = k , satisfying pre-requisites for AND Graphs is NP-Hard via a reduction from set cover. Consider a set cover instance with sets s 1 ,s 2 ,...,s ing some items from the set T = { t 1 ,t 2 ,...,t m } . We aim to find a set cover of size k , i.e., a selection of k sets such that all items in T are covered. We reduce set cover to the decision version of the prerequisite problem: Given a graph G , is there a set satisfying pre-requisites of size k  X  which has score  X  s . We reduce the given set cover instance to the following decision problem: Is there a set sat-isfying prerequisites of size k  X  = 2 mk + k + m 2 that has a score  X  m 2 in a graph G that we construct as follows: First create a node corresponding to each set s i . These nodes form the first layer. Now we create nodes corresponding to each item in s i . Thus if t directed edges from s i to each ( s i ,t j ) node created. Subsequently, we form a chain of 2 k  X  1 nodes below each ( s i ,t j ) , i.e., we create a chain ( s i ,t j )  X  ( s i ,t j ) 2  X  ( s i ,t j ) 3  X  ...  X  ( s sequently, we create nodes for each t i ,t j pair; i.e. for ( s and ( s q ,t j ) 2 k , where i 6 = j , we create a new node ( s such that there are directed edges from ( s p ,t i ) 2 k and ( s ( s m ,t i ,s l ,t j ) . Only the nodes with four labels, i.e., those at the last layer, have a score of 1, all other nodes have score 0. This con-struction is depicted for an example in Fig. 2. In the figure, we show just two sets s 1 ,s 2 covering items t 1 ,t 2 and t 1 ,t the edges between them (some labels are omitted for clarity).
If there is a set cover of size k , then it is easy to construct a set of size k  X  that satisfies prerequisites, and has score at least pick the nodes corresponding to the sets comprising the set cover at the first layer. Then pick m complete chains (one for each t such that each chain belongs to a set s i that has been chosen in the first layer. Such chains are always present because each item t present in at least one s j . Then, we pick all m 2 nodes at the last layer formed at the end of each pair of chains from the m chains have a node at the last layer). Thus the size of the set is  X  k has score m 2 . We now try to prove the converse.

If there is a set of size k  X  satisfying prerequisites of score  X  then the following hold:  X  At least m 2 items are picked in the last layer, because only  X  At least m items are picked in the last-but-one layer, because  X  If there are more than m items picked in the penultimate layer,  X  All m chains need to correspond to different items, otherwise  X  There are at most k  X   X  m 2  X  2 mk = k items chosen at the Thus, the sets that are picked correspond to a set cover, since each item from T is found in the set that is the parent of the chain corre-sponding to the item. Thus, there is a set cover. Hence, the reduc-tion holds.
The proof of NP-hardness for OR Graphs involves a reduction from set cover as well, and is much simpler. A brief outline of the proof follows: Consider a node corresponding to each set s { s 1 ,s 2 ,...,s n } in the set cover problem, and each item t { t 1 ,t 2 ,...t m } . We connect node s i to a node t j via a directed edge if t j  X  s i . Additionally, each t i has score 1 , while all s have score 0 . If the set cover problem asks for a set of size k , then the decision version of the prerequisite problem for the OR Graph asks if there exists a set of size k + m with score  X  m satisfying prerequisites. Since the problem of recommendation with prerequisites is NP-Hard for AND, OR and AND-OR Graphs, we can only provide ap-proximate solutions. Throughout this paper, the algorithms listed are for AND Graphs (and therefore for chain graphs as well), which forms a reasonable prerequisites structure for movies, books and other media. However, our algorithms, with simple modifications, work for OR and AND-OR graphs as well. We defer the modifica-tions to them for OR and AND-OR graphs to Section 3.7. We also walk through each algorithm for an example graph in the extended technical report [14].

For the special case of chains graphs, there is an expensive but exact PTIME algorithm that we examine first, in Sec. 3.1. Note that all algorithms return a set of size k assuming one exists. If not, the algorithms return the entire set of items.
For the case when G is a forest of chains C 1 ,...C n , the problem of finding the best set satisfying prerequisites is solvable using a PTIME dynamic programming algorithm.

The algorithm DP Chains generates an array a of size ( n + 1)  X  ( k + 1) . The ( i,j ) th entry of this array corresponds to the score of the best package that can be obtained by picking j items from the first i chains, while satisfying prerequisites. For entries in the i th row, we only need to consider the values of a for the previous row, and the scores of items in the current chain C i . In particular, consider entry ( i,j ) . We can pick l items where 0  X  l  X  j items from the chain C i and pick the remaining j  X  l items from the previous i  X  1 chains. The optimal score of picking j  X  l items not need to be recomputed. Hence there is an optimal substructure built into this formulation of the problem.

However, note that this algorithm cannot be generalized to the case of DAGs or even trees, since subgraphs picked in either case cannot be picked independently of each other, unlike in the chain graph case. Thus, there is no optimal substructure in the DAG case. To combat this problem, we provide three other algorithms in sub-sequent sections. However, these algorithms will be approximate.
Note also that the complexity of the dynamic programming algo-rithm, O ( nk 2 ) may be high because n may be high. The algorithms in subsequent sections describe algorithms which operate in O ( n ) or O ( nk ) and hence are more efficient. Note that in particular, the Greedy-value Pickings Algorithm that we describe performs almost as well as DP Chains Algorithm on synthetic chain graphs. DP Chains Algorithm 1: a  X  array of size ( n + 1 ,k + 1) 2: for all i in 0 ...n do 3: for all j in 0 ...k do 4: a [ i ][ j ]  X  0 5: if i == 0 then 6: continue 7: end if 8: s  X  0 9: for all l in 0 ...j do 12: end for 13: end for 14: end for 15: return a [ n ][ k ] Algorithm 1 Breadth-first Pickings 1: A  X  X  X  2: while size ( A ) &lt; k do 4: end while 5: B  X  external ( A ) 6: while there exist items in B do 7: pick b  X  B with largest score 10: A  X  A  X  X  a } X  X  b } 11: B  X  external ( A ) 12: else 13: remove b from B 14: end if 15: end while 16: return A
We define boundary ( A ) as the set of items in A each of which can be deleted without violating the prerequisites of any other items in the set, i.e., if x  X  boundary ( A ) , then there is no y  X  A and x ,x 2 ,...,x n such that there exists a sequence of edges ( x,x ( x 1 ,x 2 ) ,..., ( x n ,y ) in E .

We define external ( A ) as the set of items in V that are not in A and can be potentially added to A without violating prerequisites, i.e., if x  X  external ( A ) , then there is no y,x 1 ,x 2 Note that this set also contains the items in V that have no edges coming into them.

The modifications to these definitions for OR and AND-OR graphs can be found in Sec. 3.7, along with the modifications to the algo-rithms described below.
As listed in Algorithm 1 , we initialize the set A with the best k items by picking greedily the best item from among the items whose prerequisites have been satisfied, but are not already in the set A , i.e., external ( A ) (line 2-4).

We then greedily try to replace items from boundary ( A ) , i.e., the items that are non-essential to A , with those from external ( A ) , those whose prerequisites have been satisfied (line 7-14). However, we make sure that we do not delete the parent of a child (line 8).
At each iteration (line 6-15), we either increase the score of A , or we delete an item from B . Since, beyond a point, the score cannot grow, and since B is finite, we are guaranteed termination. Algorithm 2 Greedy-value Pickings 1: A  X  X  X  2: Q  X  X  X  3: for all items i  X  G do 4: C  X  X  i } 5: C  X  C  X  prerequisites of i 7: end for 11: A  X  A  X  M 14: size ( C )  X  no. of items in C  X  A 15: if size ( C ) 6 = 0 then 17: else 18: value ( C ) = 0 19: end if 20: end for 21: end if 22: end while 23: return A
We use a max-priority-queue for this algorithm, and insert sets of items into the queue. The max-priority-queue is sorted on the value , i.e., the average score of the items in the set, and on querying returns the set with the largest value .

We list the pseudocode in Algorithm 2 . We insert a set corre-sponding to each node in the graph G into the max-priority queue (line 3-7). This set contains the given node v , and all nodes a such that there is a path from a to v . These sets in the queue are sorted on average score , i.e., the sum of score of the items in the set, di-vided by the size of the set. On performing pop on the queue, the item with the largest average score is removed from the queue.
Now, as long as we have not picked enough items in A , we keep popped set is small enough to be added to A (line 10), we add it to A (line 11), and update the values of other sets that have a non-zero intersection with the set currently added to A , in two steps: Firstly, the number of items is reduced by the number of new items added to A that are also present in the set (line 14). Additionally, since those items no longer count towards the average score of the set, the value of the set is appropriately changed (line 15-19).
The algorithm has to terminate because the number of sets in the priority queue is bounded by the total number of items.
In Algorithm 3 we start with the best set of items of size k (line 1) by picking items with largest score without regard to prerequi-sites, and try to incrementally add prerequisites. We keep track of items that we have already added prerequisites for in B (line 6). We never let such items be deleted.

We pick the items in order of decreasing scores from A  X  B , i.e., the items that have not been examined already (line 4). We then check if the prerequisites of the item are already present in A (line 7), if so, we examine the next item.

If there are still some s prerequisites required (line 8), we replace items from A if possible (line 11-14,18). These items are picked from boundary ( A ) , but should not be present in the items already considered B (line 12-13).
 Algorithm 3 Top-down Pickings 1: A  X  best set of size k 2: B  X  X  X  3: while there exists items  X  A  X  B do 4: a  X  item with largest score in A  X  B 5: C  X  prerequisites of a 6: B  X  B  X  X  a } 7: if ( C  X  A ==  X  ) continue 8: s  X  size ( C  X  A ) /* no. of missing prereqs. */ 10: R  X  X  X  /* deletions from A */ 14: end while 15: if size ( R ) &lt; s then 16: replace a in A with item with largest score from external ( A ) 17: else 18: A  X  ( A  X  R )  X  C 19: end if 20: end while 21: return A
If sufficient items cannot be found we replace the item under consideration with an item from external ( A ) (line 16), i.e., those items that can be added without violating prerequisites because their prerequisites are already present. Note that such an item can always be found by simply picking the first unpicked item in a topo-logical sort of the graph.

We are guaranteed termination, because there are a finite number of items, and every item that is considered is added to B , and an item that is considered cannot be re-considered.
We have also designed an algorithm, Merge , that takes as input two sample solution sets A 1 and A 2 of size k (both satisfying pre-requisites) for an AND graph. Merge generates a new set of size k , which does not violate prerequisites, and also has a score greater than or equal to A 1 and A 2 . This set is formed only using ele-ments from A 1  X  A 2 . The algorithm starts with the set with higher score and iteratively replaces subgraphs with least average score with subgraphs with high average score from the other set, while retaining prerequisites and making sure we have k items. The de-scription of this algorithm and pseudocode can be found in the ex-tended technical report [14]. We now describe the modifications to definitions discussed in Sec. 3.2 for AND-OR graphs (and therefore for OR graphs as well), and then provide specifics on the modification of each algorithm for AND-OR graphs.

For AND-OR graphs, boundary ( A ) is the set of items in A such that each of the items is not a prerequisite or a potential prerequisite of any other item in A , i.e., if x  X  boundary ( A ) , then there does Table 1: Worst case bounds for each structure for each algo-rithm (all entries are multiplied by  X  ). Also,  X  = min( k,d )  X  1 not exist an item y  X  A and items x 1 ,x 2 ,...,x n such that there is a sequence of edges ( x,x 1 ) , ( x 1 ,x 2 ) ,..., ( x n ,y ) in E .
Similarly, external ( A ) is the set of items that can be added such that prerequisites are not violated. For an AND-OR graph, if the node x added is an AND node, then all items y such that there is a edge from y to x in E must also be present in A , as well as prerequisites for all such y . If the node x is an OR node, then at least one y (whose prerequisites are already in A ) such that there exists an edge from y to x should be present in A .
We replace line 8 in the algorithm to reflect the fact that a should not be necessary for b , i.e., b  X  X  prerequisites in A  X  X  a } are present.
For AND-OR graphs, as before, we insert sets corresponding to each item into the max-priority queue in a top-down fashion. For an AND node a , in line 5, we include the set of prerequisites that is the union of the prerequisite sets corresponding to all of a  X  X  imme-diate parents. For an OR node a , in line 5, the set of prerequisites corresponding to a  X  X  immediate parents with the largest value (as in the algorithm) is included. For OR graphs, this procedure cor-responds to selecting the best path to any root (i.e., the path with greatest value .) Sec. 3.7.2 to augment the set corresponding to each item.
In this section, we derive bounds on the worst case difference between the score of the optimal set and the score of the set that we return for each prerequisite structure. The worst case bounds are listed in Table 1. We prove the worst case bounds for chain graphs and some other prerequisite structures here and defer the remaining proofs to the extended technical report [14].

We define the following properties of the graph G : The coher-ence of G , i.e., the maximum difference between the minimum and maximum score of two items in any connected component in the graph is  X  . (For the case of chain graphs,  X  is the maximum dif-ference between two items in any chain.) Additionally, the depth of a graph G , i.e., the maximum length of any directed path, is d . (For chain graphs, the depth is the maximum number of items in any chain.) As before, a set of size k is desired.
We consider chain graphs first. Assume d &lt; k . The worst case is attained as in Fig. 3, when there are k singleton items that have a score of  X  , while there are several (  X  k , say) other items which have score of  X   X   X  , for very small  X  , but those items are each the start of a chain of ( d  X  1) items that have a score of  X   X   X  +  X  each. In this case, Alg. 1 picks k items that have a score of  X  to form part of A , and never discards them in favor of items with score  X   X   X  , which forms part of external ( A ) . However, the optimal algorithm would pick k/d such (complete) chains, because after the first item  X  k chains with d items in each chain in the chain, all the other items have a value of  X   X   X  +  X  . Thus the difference between the optimal score and the score of our algorithm would be: (ignoring  X  ) The worst case for d  X  k has a single long chain of size k as above and many (  X  k ) singleton items. The worst case difference is then ( k  X  1)  X  .

As a justification for the fact that this is actually the worst case, consider the following: Note that Alg. 1, on termination, returns a set A such that no item in external ( A ) is better than any item in boundary ( A ) . Also note that in each of the chains, the first item Now let there be a sub-chain in the optimal set no part of which is in A . The first item of this sub-chain (score  X   X   X  , say) forms part of external ( A ) for all iterations of Alg. 2. Thus, in the worst case, each item in A is equal to, or slightly better than  X   X   X  . Thus A has k items of score  X  . We now try to maximize the score of the re-maining items in the optimal set. The maximum score is when the first item of each of the sub-chains in the optimal set is not in A and has score (  X   X   X  ) . (If it had a higher score, then it would be chosen by A .) Each of these items are followed by d  X  1  X  X ood X  items of score  X  +  X   X   X  , which is the maximum such score. This situa-tion is precisely the one described above. Though we do not prove it here, the case when every sub-chain in the optimal set overlaps with some sub-chain in A does not change the worst case bound. (This case has lower difference because of high overlap between A and the optimal set, and the coherence constraint.) Note that if d  X  k , k takes the place of d above.

We prove the worst case bounds for other prerequisite structures in the extended technical report [14].
We consider chain graphs first. Assume that k &gt; d for now. Let there be d remaining items to be picked. Let the optimal algorithm and Alg. 2 return the same score until this point. The only unpicked items are shown in Fig. 4. Let there be only d/ 2 singleton items with a score of  X  +  X / 2 +  X  . However, there is a chain of d items where the first d/ 2 items in the chain have a score  X  , while the last d/ 2 have a score of  X  +  X  . (The average score of this chain is singleton node with score  X  +  X / 2 +  X  . We keep picking d/ 2 such singleton items. Later, since there are only d/ 2 items left, Alg. 2 picks the first d/ 2 items of the chain, each of which have value  X  . The optimal algorithm picks the chain of d items instead of the singleton nodes. The difference between the optimal score and the score of the Alg. 2 is (ignoring  X  )
If k  X  d , then a similar construction can be used, with k taking the place of d . Here, the worst case bound is k X / 4 .

For an informal proof of why this graph is the worst case when k &gt; d (the proof for k  X  d is similar), consider the following: Note that if we do not discard any sets in the execution of the algorithm via the condition in line 10, then we are guaranteed an optimal solution. To see this, consider A returned by the algorithm, and an optimal A  X  with higher score. Let the average score of the last set picked by the algorithm be v . Now, since A  X  has a higher score, ( A  X   X  A ) must have a sub-chain c that can be added to A without violating prerequisites and has higher average score than v . (If all sub-chains have same average score v , then there is no way A have higher score.) However, if c has higher average score than v , it should have been picked instead of the last set that was picked in the algorithm.

The only situation when the optimal set would contain a differ-ent set of items is when there is a chain (or sub-chain), C , whose average score is smaller than the average score v of the last chain added, but cannot be added due to insufficient capacity. (The case where there are multiple chains that are not picked is no worse.) As a result of this insufficient capacity, a sub-standard chain with fewer items is added. The worst possible sub-standard chain is a sub-chain of C of the remaining capacity itself. The worst pos-sible difference between the items that were added and those that were not added is  X  . Let us assume that there are d  X  r items in C that have value  X  that were added, followed by r items that have score  X  +  X  that were not added. The average of C is r X /d +  X  , each of the r items that are preferentially chosen have a score of at least  X  + r X /d . The difference in the scores is r X   X  r  X  ( r )(1  X  r/d ) . The optimal value of r , for which the difference is greatest, is d/ 2 . Thus the worst case bound is  X d/ 4 . For AND graphs, the worst case situation is displayed in Fig. 5. In the figure, there are k  X  x singleton items with score  X  +  X  . Additionally, there is another connected component which has x root items with score  X  , and k  X  x items with score  X  +  X  , each of which has all x root items as prerequisites. Note that this situation is similar to the one in Fig. 4, in the sense that in both the situations there are several good items at the end of a subgraph of bad items, but Alg. 2 never gets to the good items because it makes poor greedy choices early on.

In this case, the algorithm picks ( k  X  x ) items of score  X  + x +1 +  X  first, since those items have the highest average score. Then, the algorithm picks the x items that have a score of  X  . The optimal algorithm, on the other hand, picks all items from the first connected component.

We ignore  X  in the following calculation. The score of the set set is k X  + ( k  X  x )  X  . Thus the difference is:  X  ( k  X  x ) largest value of this difference (forming the worst case) is  X  ( k + 2  X  2
We prove the worst case bounds for OR and AND-OR graphs in the extended technical report [14].
For chain graphs, the worst case is attained when there are  X  k singleton items with score of  X  +  X   X   X  , but there are  X  k items which have scores of  X  +  X  . However, let these k items be at the end of  X  X ad X  chains, i.e., ( d  X  1) items with scores of  X  . This situation is given in Fig. 6.  X  k chains with d items in each chain
Algorithm 3 picks k items with score  X  +  X  , and then tries to include the prerequisites for those items. As a result, the algorithm terminates with k/d complete chains (each of size d ) that end with items with score  X  +  X  . The optimal solution in this graph is to pick k items with score  X  +  X   X   X  .

The worst case difference (ignoring  X  ) between the optimal so-lution and the solution returned by the algorithm is: k  X  d , the worst case is still Fig. 6 but with a single chain of size k . The worst case difference is then ( k  X  1)  X  .

We prove that Fig. 6 is the worst case for chain graphs and derive bounds for other structures in the extended technical report [14].
In summary, for chains, the worst case bound of Alg. 2 is better than the worst case bound of Alg. 1 and 3, both of which have worst case bounds of the same magnitude. As can be seen in Table 1, this relationship holds true for other prerequisite structures as well. The only case where Alg. 3 and Alg. 1 have different worst case bounds is for OR graphs, wherein Alg. 3 can add at most d bad elements (i.e., prerequisites) by making a poor greedy choice, while Alg. 1 adds k  X  1 bad elements.

We now prove that the three approximation algorithms are in-comparable, i.e., there exist cases where each algorithm does better than the other two. The proof is for chain graphs and therefore also holds for OR, AND-OR and AND graphs.

For the case when the graph is Fig. 4 (and k = d ), Alg. 3 would pick the optimal set, while Alg. 1 and Alg. 2 would pick sub-optimal sets. There is also a case where Alg. 1 performs better than both Alg. 2 and Alg. 3. Consider the following graph:  X  a (0 . 5)  X  b (0 . 9)  X  c (0 . 6)  X  d (0 . 6)  X  e (0 . 85) Let k = 3 . In this case, Alg. 1 would pick c,d and then e  X  a score of 2.05. However, Alg. 2 and Alg. 3 would pick { a,b } and then pick { c } , a score of 2.0.

Alg. 2 would do better than Alg. 1 and Alg 3 on the following example (with k = 3 ):  X  a (0 . 51)  X  b (0 . 9)  X  c (0 . 5)  X  d (0 . 8)  X  e (0 . 85) Here, Alg. 1 and Alg. 3 would pick { a,b,c } = 1 . 91 , while Alg. 2 picks { c,d,e } = 2 . 15 .

Thus, the three algorithms are incomparable. Therefore, given the resources, we might wish to implement all three algorithms in order to improve our recommendations.
We now examine the worst case complexity for the exact al-gorithm, three heuristic algorithms and the merge algorithm for chains. Let the number of chains be n .

The exact dynamic programming algorithm maintains a matrix of size O ( nk ) , and computes each entry in time O ( k ) . Thus the overall complexity is O ( nk 2 ) .

For the rest of the algorithms, we can prune the search for the best set at the start itself. We use a fibonacci heap, which takes  X ( n ) to construct and O (log n ) to delete any element.
For Alg. 1, we make a pass of the first item in each of the n chains, and extract at most k chains with the best first items. This operation can be done in O ( n + k log n ) using the fibonacci heap. Subsequently, we take at most O ( k 2 ) items (from the k chains), and then run Alg. 1 on those items. If we use two fibonacci heaps to store the items in external and boundary , we insert each item at most once in each of the heaps, and extract each item at most once ( k 2 extractions of O (log k 2 ) ). Thus, this phase is O ( k Thus, Alg. 1 is O ( n + k log n + k 2 log k )  X  O ( n ) if n &gt;&gt; k .
For Alg. 2, we first extract k best sub-chains of each size from 1 ...k . This operation can be done in O ( nk + k 2 there are O ( nk ) sub-chains, and since we extract k items from k heaps of size O ( n ) (one heap corresponding to each length of sub-chain from 1 to k ). Subsequently, all sub-chains of the k corresponding to the k 2 sub-chains picked earlier (i.e., at most k sub-chains) are used in Alg. 2. We use k fibonacci heaps (one cor-responding to chains of each size from 1 ...k ), and extract at most k items in total. Each such item will then trigger the change of the average score of at most k items. Thus, we insert items in heaps in O ( k 3 ) , extract items in O ( k log k ) , and modify score in O ( k 2 log k ) , giving a complexity O ( nk + k 2 log n + k if n &gt;&gt; k .

For Alg. 3, we make a pass through all the O ( nk ) items to ex-tract the best k items, and also the best k items at the that do not have any prerequisites (to replace the items whose prerequi-sites cannot be added), in O ( nk + k log n ) . (Note that this step is faster for Alg. 3 which simply needs to pick the k best elements than Alg. 2 which needs to pick the k 2 best subchains (by average score).) Subsequently, these 2 k chains of size at most k are used in Alg. 3. We use two max-heaps, one to store items whose prerequi-sites have not been added yet, and one of the items in boundary , and one min-heap, to store items whose prerequisites have not been added (and therefore can be deleted). Each of these heaps con-tain at most k 2 items, and at most k 2 items are deleted. Thus the complexity is O ( k 2 log k ) . The total complexity is therefore, O ( nk + k log n + k 2 log k )  X  O ( nk ) , if n &gt;&gt; k .
Thus, we see that Alg 2 has the highest runtime complexity, fol-lowed by Alg 3 and then Alg 1. In fact, Alg 1 does not even need to look at the entire data. The relationships between the al-gorithms with respect to complexity (i.e., Alg 2 &gt; Alg 3 &gt; Alg 1) still hold true even when we consider AND, AND-OR and OR graphs. (See [14].) This is primarily because Alg. 2 ends up mod-ifying the scores of all other sets that have some overlap with the set under consideration (which, in the worst case, could be all the sets,) while none of the other two algorithms have such an expen-sive step. Alg. 1 is the least expensive because it performs a search for a local maxima near the start of the DAGs.

Alg. 4 has the same complexity as the second phase of Alg. 2, i.e., O ( k 2 log k ) , not dependent on n (the number of chains), and thus is more efficient than any of Alg. 1, 2, or 3.
We analyze the algorithms for chain graphs in Sec. 6.1, and for other structures in Sec. 6.2. We assess the average performance of the three algorithms of Sec. 3 and the Merge algorithm of Sec. 3.6.
We study the performance of the algorithms by running them on several random graph instances, and collecting the score of the set returned by the algorithms. As we will see in the following, Alg. 2 performs the best out of the three approximation algorithms, and very close to the optimal DP Chains algorithm (from Sec. 3.1). However, we have seen in Sec. 5 that Alg. 2 has the highest com-plexity of the three algorithms, which makes it less desirable. We therefore implemented the Merge algorithm (which has lower com-plexity than any of the three algorithms), and used the Merge algo-rithm to merge the sets returned by Alg. 1 and Alg. 3 (we refer to this scheme as merge2 ). For comparison, we also implemented al-gorithm top2 which picks the set with the higher score out Alg. 1 and Alg. 3 for each input instance. As a baseline, we also imple-mented the DP Chains algorithm of Sec. 3.1.
We generated random instances of the graph G in the following fashion: We set the number of chains in the graph to be n . Each chain, with probability p , is a long chain, i.e., has length greater than or equal to 2. Thus a chain is a singleton item with probability of the chain be a discrete random variable, uniformly distributed among integers between 2 and d , the maximum depth. We let the score of each item be a continuous random variable, exponentially distributed, with mean 0 . 5 . k represents the size of the desired set.
For each experiment described in the following sections, we took several random instances of graphs generated as described above and determined the average ratio of the score of the set returned by each of the 3 algorithms  X  Breadth-first Pickings ( bf ), Greedy-value Pickings ( greedy ) and Top-down Pickings ( td ), score of top2 and score of merge2 to the score returned by the DP algorithm.
Due to space limitation, we only provide here a sample of our results. We have experimented with other parameter settings and distributions, and the conclusions are not that different from what we show here. For some additional experiments refer [14].
When we vary the size of the desired set k , we find:  X  Alg. 2 is always better than the other two approximation algo- X  Alg. 3 is better than Alg. 1 when k is large compared to n ,  X  All 3 approximation algorithms of Sec. 3 return sets whose  X  top2 does better than both Alg. 1 and Alg. 3.  X  merge2 does better than top2, and is almost as good as Greedy-
Fig. 7(a) illustrates some of our results. For this experiment we set n = 50 , p = 0 . 2 and d = 5 and generated 500 random graph instances as described above. In Fig. 7(a) the horizontal axis shows k varying from 5 (small compared to n ) to 45 (large compared to n ). The vertical axis shows the score of the set returned by each algorithm, as a fraction of the best possible score for that graph instance (averaged over all graph instances). For example, if we set k = 15 , we find that on average, both Alg. 1 and Alg. 3 return a set that has a score of approximately 95 % of the optimal set. On the other hand, on average, Alg. 2 returns a set that has a score of nearly 100% of the optimal score. top2 , which picks the best set from Alg. 1 and Alg. 3, does significantly better at around 97%. merge2 performs even better than top2 , returning a set with score of 99% of the optimal score.

We find that when k is small, Alg. 1 does better than Alg. 3, probably because Alg. 1 does a better job of exploring items that are close to the start of chains (and since k is small, we can only include items that are very close to the start of chains). We find that around k = 20 , Alg. 3 starts doing better than Alg. 1. As k becomes  X  n , Alg. 3 is much better than Alg. 1, probably because Alg. 1 tends to do a local search close to the start of chains, while Alg. 3 actively tries to include the top items.

While top2 has some definite gains over Alg. 1 and Alg. 3, merge2 does even better than top2 . This is because it does not simply pick the best set returned by the 2 algorithms of Sec. 3, but combines diverse elements picked by each of the algorithms to get an even better set. In all cases, merge2 does not depart from more than 1% of the optimal set. merge2 has low complexity (as seen in Sec. 5), and thus can be used as an effective replacement for Alg. 2.
In all cases, Alg. 2 beats all three algorithms, but as mentioned earlier, is more computationally expensive.
As we vary p , the probability that a chain is  X  X ong X , we have the following results:  X  Alg. 2 still performs the best of all the algorithms  X  The relative ordering between the scores of the algorithms tend  X  merge2 combines sets with poor score to get very close to the
In this case, we set n = 50 ,d = 5 . For each value of p rang-ing from 0.05 to 0.60, we generated 500 random graph instances as described above. We then ran the three algorithms on those in-stances and measured the average ratio of score versus the optimal set, and plotted the values for k = 10 in Fig. 7(b), and for k = 45 (i.e., k  X  n ) in Fig. 7(c).

We find that all algorithms except greedy tend to do badly if p is increased beyond a certain point, probably because the best items tend to be buried in big chains instead of being singletons. The merge algorithm once again proves invaluable in merging sets with very low scores to give a set that is within 2% of optimal.
For the experiments in this section, in addition to the three algo-rithms, we also implemented top2 , which returns set with the better score from the sets returned by Alg. 1 and 3, and top3 , which re-turns the set with the best score from the sets returned by the three algorithms. We compare our algorithms against the score of the best set if no prerequisites are taken into account, called NoPrereq .
In order to test the performance of our algorithms for other struc-tures, we would need to generate random instances of directed AND-OR acyclic graphs. In this paper, we test the algorithms on a collection of balanced trees, which are easier to generate randomly. Testing the algorithms on truly random AND-OR DAGs is left for future work. Note that since each node has a single parent, these balanced trees are AND as well as OR graphs.

Our graph G therefore consists of a set of n trees, disconnected from each other. With probability p , as in Sec. 6.1.1, a tree has more than one item. We allowed the number of items in each tree to be up to m . We first let the depth d of the tree be a random number between 1 to d max ( &lt; m ) . Subsequently, the branching factor is a random number from 1 to b m 1 /d c . The score of each item is exponentially distributed with mean 0.5.

As before, we took several random instances of DAGs as de-scribed above and determined the average ratio of the score of the set returned by each of the three algorithms ( greedy, td and bf ), top2 and top3 to the score of the algorithm NoPrereq . We plot these ratios on varying various parameters.
We have the following results:  X  Alg. 2 performs better than Alg. 1 and 3, and almost the same  X  When k is small, Alg. 3 performs better than Alg. 1, with the  X  When the number of chains and the number of items to be picked  X  top2 does better than Alg. 2 when the number of items and num- X  On varying the number of chains, all algorithms perform simi- X  All algorithms are within 20% of each other.

Fig 8(a), Fig 8(b) and Fig. 8(c) show the variation with increas-ing p from 0 . 05 to 0 . 65 . The vertical axis shows the score returned aged over 500 graph instances. For Fig. 8(a) and Fig. 8(b), we set n = 40 , while for Fig. 8(c), we set n = 100 . For Fig. 8(a) and Fig. 8(c), we set k = 20 while k = 40 for Fig. 8(b) The maximum number of items in a chain, m is set to be 10 for all the algorithms. the three algorithms, nearly indistinguishable from top3 . However, Alg. 3 follows Alg. 2 very closely, with less than 5% difference. For example, on setting p = 0 . 4 in Fig 8(a), on average Alg. 1 re-turns a set with score 80% of the score of the no-prereq set, while Alg. 2 and Alg. 3 return sets with score 90% and 92% respectively.
In fact, in Fig. 8(b), Alg. 3 performs better than Alg. 2 (even though it is not clearly distinguishable in the plot) when p is small. However, note that when the number of chains are large, Alg. 2 does the best, as is evident from Fig. 8(c). (In this case, Alg. 1 does better than Alg. 3.)
We then tried to examine the variation in performance with in-creasing number of chains. We set n = 100 , m = 10 and p = 1 , and repeated the experiment over 500 random graph instances. Fig 9(a) shows the variation with increasing number of items be-ing picked. As is expected, when the items being picked becomes large, Alg. 1 starts performing better than Alg. 3, with two plots intersecting when k = 60 .

Fig 9(b) shows the variation for the extreme case when both the number of chains and the number of items to be picked are small, i.e., k = 3 ,n = 3 ,m = 3 , repeated over 500 trials. In this case, top3 and top2 (indistinguishable in the graph) do better than the rest by almost 1%.

Fig 9(c) shows the variation of the algorithms on increasing the number of chains for k = 20 ,p = 1 ,m = 10 over 500 trials. As expected, all three algorithms improve when the number of chains increase because they have more shots at picking the best items. While we have not implemented the merge algorithm for AND-OR graphs, we believe that it will give us a further boost compared to top2 , thereby narrowing the gap to Alg. 2.
We are not aware of any prior work in the area of set recommen-dations that take prerequisites into account.

However, there is a large body of work on traditional recom-mendation systems, aimed at coming up with a single  X  X core X  for each item, combining approaches that look at using ratings given liked [16], and other approaches [6]. All of these techniques could be used in generation of the score function that we use as a black box, therefore our work builds on top of other recommender sys-tems work.

The body of work on Top-N recommendation systems [8] solve a different problem. Their aim is: given a user X item matrix of scores, and given the set of items that a given user has consumed, recommend an ordered set of up to N items that the user has not consumed. In this case there is no inherent ordering of items that needs to be respected when recommending N items, which is the case in our problem.

Ziegler et. al. [20] consider recommending lists of items taking not considered; additionally, our algorithms can be generalized to handle the case of complex scoring functions where the score of a set is not just the sum of scores of the items contained in the set.
There has been some recent work on incorporation additional constraints into the recommender systems problem, for example, group recommendations [3, 11] dealing with the problem of recom-mending items to a group of people with diverse interests, out-of-the-box recommendations and recommending items that are non-obvious and diverse [18, 19], and the work on recommendation of paper assignment to reviewers [7]. small k (b) small n and larger k (c) large n and small k k and n are both very small (c) with n
Some of the recommendation questions we pose can be written in RQL (Recommendation Query Language) [2], or expressed as constraints [9], however, our aim in this paper is to consider effi-cient algorithms that solve those recommendation questions, and not posing those questions themselves.

Our problem is an instance of preference-based optimization [5, 4], which considers preferences between subsets of items. How-ever, we leverage the fact that the subsets that we recommend have to satisfy specific constraints, i.e., prerequisites to provide efficient algorithms. Additionally, considering all subsets of items in our case is computationally hard.
As recommender systems are applied to more and more domains [12, 10], it is important to incorporate into our recommendations the constraints introduced by the domain, or more generally, con-textual information. If we do not incorporate such constraints we can end up making recommendations that do not make sense to the user, e.g., suggesting courses they are unable to take.

In our case, we considered prerequisite constraints and how they affect the problem of recommendations. We focused on the prob-lem of recommending a set of items with high score, while satis-fying prerequisites, for various prerequisite structures. We proved that this problem is NP-Hard for most prerequisite structures and suggested approximate algorithms to solve this problem. For the case of chain graphs, we presented an exact algorithm.

In our experiments, we find that Greedy-value Pickings performs exceedingly well. For most practical situations, say for example, we have 100,000 chains to choose 10 items from, and chains with depth around 10, we would prefer using Greedy-value Pickings, which takes 10 5  X  10 time steps, i.e., 1 10 the time as DP, and makes nearly the same (good) recommendations. If we wish to save even more on time, say for example we need to make live recommen-dations on the web, we might prefer performing both Top-down Pickings and Breadth-first Pickings and doing a Merge . (If we are implementing Top-down Pickings , then there is no good reason to not use Breadth-first Pickings and Merge , since both of these have significantly lower time complexity.) If we simply need to make fast recommendations, we would opt for Breadth-first Pickings .
