 k -nearest neighbor ( k -NN) search aims at finding k points nearest to a query point in a given dataset. k -NN search is important in various applications, but it becomes extremely expensive in a high-dimensional large dataset. To address this performance issue, locality-sensitive hashing (LSH) is suggested as a method of probabilistic dimension reduction while preserving the relative distances between points. How-ever, the performance of existing LSH schemes is still in-consistent, requiring a large amount of search time in some datasets while the k -NN approximation accuracy is low.
In this paper, we target on improving the performance of k -NN search and achieving a consistent k -NN search that performs well in various datasets. In this regard, we pro-pose a novel LSH scheme called Signature Selection LSH (S2LSH). First, we generate a highly diversified signature pool containing signature regions of various sizes and shapes. Then, for a given query point, we rank signature regions of the query and select points in the highly ranked signature regions as k -NN candidates of the query. Extensive experi-ments show that our approach consistently outperforms the state-of-the-art LSH schemes.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process k-nearest neighbor search; locality sensitive hashing
Multimedia search engines and recommendation systems such as Google image search and YouTube video recom-mender need to perform k-nearest neighbor ( k -NN) search over a large collection of high-dimensional data points [3].  X  corresponding author c  X  Since they need to adaptively refine k -NN results as soon as each user clicks on an image or a video, the performance of a k -NN search algorithm becomes increasingly important.
Locality sensitive hashing (LSH) is known as one of the most efficient approximation methods for k -NN search [7]. Using locality sensitive hash functions that assign close points to the same hash bucket with high probability, one can pick points sharing buckets with a given query point as candidates for k -NN search. However, recently proposed approaches such as Collision Counting LSH (C2LSH) [4], LSB-tree and forest [8], and Exact Euclidean LSH (E2LSH) [7, 2] show a poor performance in some datasets while they work well in some others. In fact, our experiments show that their performance is significantly affected by the char-acteristics of a dataset such as the number of points and the feature extraction method.

In this paper, we propose a consistently efficient LSH scheme for fast k -NN search, called Signature Selection LSH (S2LSH), that performs well in various types of datasets. To describe our key intuition more clearly, let us first de-fine two concepts, a signature and a signature region of a point q . A locality sensitive hash function g projects q onto a lower dimensional space, and the projected coordinate of q is called the signature of q by g . Then, g defines a region in the projected space that contains a set of points represented as the same signature. We denote the region containing q defined by g as the signature region of q by g . Note that LSH schemes usually define multiple signature regions of q using multiple locality sensitive hash functions in order to capture the neighborhood of q more effectively.

Figure 1 demonstrates our key observation on the relationship between signature regions and the performance of k -NN search.
 The signature regions of q are not equally important for k -NN search of q . Let us consider that we need to find k -NN points of q (the star node) by select-ing 5 candidates from a large collection of points. All of the neighborhood of q . However, R sig, 1 contains some points that are far from q . It is because q is near the boundary of the signature region and the size of R sig, 1 is large. Since R sig, 2 and R sig, 3 are relatively small, the points in the union of the two regions become good k -NN candidates of q .
Notice that a point in a small signature region is more likely to be a k -NN point of q than a point in a large signa-ture region is. Therefore, it is usually more effective to pick the points in small signature regions as k -NN candidates of q . A large signature region is not effective in k -NN search of q especially if q is located near the boundary of the re-gion; the points on the other side of the region are actually distant from q , but included as k -NN candidates of q , which is undesirable. Rather, we suggest to pick k -NN candidates of q from the union of small signature regions of q . How-ever, even though we prefer small signature regions, we also need large ones since the distances from q to its actual k -NN points can be large if the density of points around q is low.
Our approach, S2LSH, is composed of two stages, signa-ture pool generation and signature selection. Since the dis-tances from each point to its k -NN points may vary signifi-cantly, we generate a highly diversified signature pool that contains signature regions of various sizes and shapes. Once a query point q is given, we measure the effectiveness of each signature region of q . Then, we choose a predefined number of candidates from the signature regions with high effective-ness scores. Experiments show that S2LSH is consistently efficient in generating k -NN results with high approximation accuracy.
Locality sensitive hashing (LSH) converts high-dimensional points into low-dimensional points called signatures while preserving the relative distances between them. We can per-form approximate k -NN search based on the distances be-tween signatures of points, instead of using the distances be-tween the original coordinates of points. However, even very effective hypersphere-based hash functions fail in achieving high approximation accuracy if the dimension of signatures is low [6]. k -NN approximation accuracy is improved with high-dimensional signatures, but it takes very long time to calculate distances between signatures.

Instead, state-of-the-art LSH schemes suggest to use sig-nature regions to choose k -NN candidates of q . The fol-lowings are three existing LSH schemes that are known as efficient k -NN approximation methods: (1) Exact Euclidean LSH (E2LSH) [7, 2] constructs multiple signature regions for q . If a point p is in one of the signature regions of q , p is selected as a k -NN candidate of q . This means that E2LSH selects all the points in the union of the signature regions of q as k -NN candidates of q . (2) LSB-tree and LSB-forest [8] compress the signatures generated by E2LSH further into one-dimensional values using the z-order curve. Then, they select a point that has a z-order value with the greatest LLCP (length of longest common prefix) as a candidate for q . (3) Collision Counting LSH (C2LSH) [4] counts the num-ber of signature regions in which q and a point p co-occur. If the collision count of p is equal to or greater than a given threshold l , it selects p as a candidate for q . However, the collision counting step of C2LSH requires a number of set intersection computations, which is very expensive. C2LSH suggests to use virtual rehashing technique, but its applica-tion is limited to random projection [5, 7].
The motivation of our approach is that among multiple signature regions of q , there are some signature regions that are more effective in performing a fast k -NN search of q than the rest. The example in Figure 1 shows that points in smaller signature regions are more likely to be k -NN points of q .

Ideally, the most effective signature region of q for the ap-proximate k -NN search is the signature region containing q and k -NN points of q only, since it does not introduce any false positive or false negative. However, generating such a signature region of q takes as much effort as finding k -NN points of q directly among all the points in a dataset. Instead, we keep merging signature regions of q starting from small ones until the number of points in the merged region reach a fixed threshold. However, notice that since the points in a dataset are not uniformly distributed, the neighborhood regions of points can be very diverse both in size and shape. Therefore, we first construct a large collec-tion of highly diversified signature regions, called a signature pool .
When a collection of points, D , is given, we construct a highly diversified signature pool in order to improve both k -NN approximation accuracy and the search time in D . To this end, we generate a signature pool of size L by defining L compound hash functions, { g 1 , g 2 ,  X  X  X  , g L } . Each g to be locality-sensitive. The i th compound hash function, g defines the i th signature region of q , R sig,i ( q ). Then, for a given query point q , our signature pool provides L signature regions of q using the compound hash functions. Notice that R sig,i ( q ) covers a point p in D along with q if the i signature of p is g i ( q ), i.e., g i ( p ) = g i ( q ).
Since each g i is locality-sensitive, the points in R sig,i tend to be close to q . If we pick all the points in the L signature regions of q as k -NN candidates of q , the hashing performance of each g i directly determines the overall per-formance of k -NN search. However, in S2LSH, we reduce the dependence on the hashing performance of each g i and improve the average performance of k -NN search by increas-ing the diversity of signature regions in a signature pool and using highly effective signature regions only.
 We can use various types of compound hash functions in S2LSH such as random hyperplane [1] and Spherical Hash-ing [6] by considering the characteristics of a given dataset. If the distance between points in D is best measured by the cosine distance, we can use, for instance, random hyperplane to define g i . The number of hyperplanes defining g i corre-sponds to the length of signatures generated by g i add one more hyperplane, we append +1 to the signatures of points on a side of the hyperplane and  X  1 to those on the other side of it. For a dataset in Euclidean space, we can use a hypersphere-based hash function such as Spherical Hashing. In this case, the number of hyperspheres of g i termines the length of signatures generated by g i . Through-out this paper, we mainly use Spherical Hashing to define compound hash functions of S2LSH and baseline approaches since it achieves a high hashing performance by considering the distribution of points in D . In order to generate signa-ture regions of diverse shapes and sizes, each g i is allowed to be defined using a various number of hyperspheres.
To generate a diversified signature pool based on Spher-ical Hashing, we need three more parameters: H , m 1 , and m 2 . m 1 and m 2 define the minimum and the maximum lengths of signatures and H represents the total number of hyperspheres from which we randomly select a subset of hy-perspheres and use them to define g i . For example, let us assume that H , m 1 and m 2 are 300, 5 and 15 respectively. Then, after generating 300 hyperspheres for D , we gener-ate each g i composed of n i hyperspheres randomly chosen among them. n i is a random number between m 1 and m 2 , and it corresponds to the length of signatures generated by g . Notice that we execute Spherical Hashing just once to generate 300 hyperspheres, instead of executing it whenever we define g i . Through experiments, we see that this sig-nificantly reduces the time for generating a signature pool without affecting k -NN approximation accuracy.
For a given query q , we want to select a subset of signature regions from which we can get high-quality k -NN candidates of q . Recall that the state-of-the-art LSH schemes either introduce too many false positives by picking the points in all of the signature regions as k -NN candidates, or spend too much time in selecting the points frequently co-occurring with q in many of the signature regions.

In S2LSH, we define an effectiveness measure of a signa-ture region by considering the followings: Firstly, assuming that a point in a smaller signature region of q is more likely to be a k -NN point of q , we assign a higher effectiveness score to a small signature region than to larger ones. Notice that the distance between q and a point in R sig,i ( q ) is bounded by the size of R sig,i ( q ). Secondly, if two signature regions of q are of similar size, we assign a higher effectiveness score to the signature region in which q is located closer to the cen-ter of the region. Lastly, we should be able to compute the effectiveness scores of signature regions without heavy com-putation overhead. Otherwise, the execution time of k -NN approximate search will significantly increase.

We identify a set of features of signature regions and mea-sure the correlation between every combination of features and the performance of k -NN approximation. Then, we se-lect four features that are highly correlated with the perfor-mance of k -NN approximation. We consider them as strong indicators of the effectiveness of a signature region for k -NN search of q . Let us describe the four selected features by com-paring two signature regions of q , R sig,i ( q ) and R sig,j ( F 1 ) Signature Length. A signature length corresponds ( F 2 ) Cardinality. If R sig,i ( q ) contains fewer points than ( F 3 ) Average Distance from Query. If the average dis-( F 4 ) Maximum Distance from Query. If the maximum
While F 1 and F 2 can be computed efficiently using tech-niques such as bucket hashing 1 , calculating F 3 and F computationally very expensive because they are query-specific. In S2LSH, we approximate F 3 and F 4 using intermediate re-sults of distance calculations performed for previous queries. Therefore, S2LSH can compute approximate values, F 0 3 and F , only when it performs k -NN search of multiple queries. Accordingly, we define two effectiveness measures, E E 00 , separately, and use E 00 only if multiple queries are given.
E 0 ( R sig,i ( q ) , q ) =
For a given query q , S2LSH selects a fixed number of can-didates from highly ranked signature regions of q among L signature regions of q from the signature pool. Currently, the number of candidates is determined by considering both k and the time budget. Experimental Settings. (a) Dataset: We use 7 datasets containing different numbers of points and represented with various dimensions as shown in Table 1. They include au-dio data, images, and textual data with various feature ex-traction methods. (b) Baselines: The three baseline ap-proaches are Brute-force, E2LSH [7, 2], and C2LSH [4]. To our best knowledge, C2LSH is the most recent and efficient LSH scheme for k -NN search. However, we include E2LSH as a baseline since we observed while comparing many exist-ing LSH approaches that E2LSH outperforms C2LSH with careful parameter tuning. We applied various optimization techniques to enhance the performance of the three base-lines for fair comparison. (c) Accuracy Measure: We use the fraction of correct k -NN points( k = 10) as a measure of k -NN approximation accuracy. The average k -NN search time we report in this section is the average of 1000 queries. (d) Effectiveness Measure: For most of the experiments in this section, we use E 0 only since we execute each query seperately. For the experiments with multiple queries shown in Figure 4, we use both E 0 and E 00 .
 Impact of Dataset. Table 2 shows the performance comparison results of S2LSH and the three baselines. We measured the average k -NN search time in 7 datasets listed in Table 1. S2LSH is shown to generate more accurate k -NN search results with less average execution time in all the http://www.mit.edu/~andoni/LSH/ 7 datasets. This means that we can use S2LSH in various types of datasets.
 Table 2: The average k -NN search time per query and the corresponding approximation accuracy.
 Impact of Underlying Hash Function. To show that the superiority of S2LSH is not limited to a particular hash function, we experimented with two different hash functions, Spherical Hashing [6] and random hyperplane [1]. We chose them since Spherical Hashing is data-dependent and based on Euclidean distance while random hyperplane does not consider the original data distribution and it uses the cosine distance. As shown in Figure 2 and Figure 3, S2LSH con-sistently shows the best performance regardless of the type of underlying hash functions. Figure 2: Performance comparison using Spherical Hashing as underlying hash function. Figure 3: Performance comparison using random hyperplane as underlying hash function.
 Impact of E 00 . In the experiment in Figure 4, we used all the points in a given dataset as queries and performed k -NN search for each query point. Since multiple queries are given, S2LSH can efficiently approximate F 3 and F 4 using the results of distance calculations performed for previous queries. We increased the number of queries from 10K to Figure 4: Performance comparison with multiple query points (Spherical Hashing is used). 50K and measured the total elapsed time. It is shown that by using E 00 along with E 0 , the performance of S2LSH can be further improved. Also, the gap between the total search time of S2LSH and those of the baselines grows rapidly as the number of queries increases.
We proposed a novel LSH scheme for k -NN approximate search, called S2LSH . In S2LSH, we first generate a highly diversified signature pool for a given dataset. Then, for a given query, we rank signature regions of the query using the effectiveness measures, and select points in highly ranked signature regions as k -NN candidates of the query. Experi-ments show that S2LSH consistently outperforms baseline approaches in various datasets, with different underlying hash functions, and when executing with multiple queries.
This work was supported by the National Research Foun-dation of Korea (NRF) grant funded by the Korea Govern-ment (MSIP) (No.2015R1A2A2A04005646). [1] M. S. Charikar. Similarity estimation techniques from [2] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. [3] J. Davidson, B. Liebald, J. Liu, P. Nandy, T. Van Vleet, [4] J. Gan, J. Feng, Q. Fang, and W. Ng. Locality-sensitive [5] A. Gionis, P. Indyk, R. Motwani, et al. Similarity [6] J.-P. Heo, Y. Lee, J. He, S.-F. Chang, and S.-E. Yoon. [7] P. Indyk and R. Motwani. Approximate nearest [8] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Quality and
