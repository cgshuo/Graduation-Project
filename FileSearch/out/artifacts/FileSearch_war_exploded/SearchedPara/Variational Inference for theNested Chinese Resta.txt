 For many application areas, such as text analysis and image analysis, learning a tree-based hierarchy is an appealing approach to illuminate the internal structure of the data. In such settings, however, the combinatoric space of tree structures makes model selection unusually daunting. Traditional techniques, such as cross-validation, require us to enumerate all possible model structures; this kind of methodology quickly becomes infeasible in the face of the set of all trees.
 The nested Chinese restaurant process (nCRP) [ 1 ] addresses this problem by specifying a generative probabilistic model for tree structures. This model can then be used to discover structure from data using Bayesian posterior computation. The nCRP has been applied to several problems, such as fitting hierarchical topic models [1] and discovering taxonomies of images [2, 3].
 The nCRP is based on the Chinese restaurant process (CRP) [ 4 ], which is closely linked to the Dirichlet process in its application to mixture models [ 5 ]. As a complicated Bayesian nonparametric model, posterior inference in an nCRP-based model is intractable, and previous approaches all rely Gibbs sampling [ 1 , 2 , 3 ]. While powerful and flexible, Gibbs sampling can be slow to converge and it is difficult to assess the convergence [ 6 , 7 ]. Here, we develop an alternative for posterior inference for nCRP-based models.
 Our solution is to use the optimization-based variational methods [ 8 ]. The idea behind variational methods is to posit a simple distribution over the latent variables, and then to fit this distribution to be close to the posterior of interest. Variational methods have been successfully applied to several Bayesian nonparametric models, such as Dirichlet process (DP) mixtures [ 9 , 10 , 11 ], hierarchical Dirichlet processes (HDP) [12], Pitman-Yor processes [13] and Indian buffet processes (IBP) [14]. The work presented here is unique in that our optimization of the variational distribution searches the combinatorial space of trees. Similar to Gibbs sampling, our method includes an exploration of a latent structure associated with the free parameters in addition to their values. First, we describe the tree-based stick-breaking construction of nCRP, which is needed for variational inference. Second, we develop our variational inference algorithm, which explores the infinite tree space associated with the nCRP. Finally, we study the performance of our algorithm on discrete and continuous data sets. The nested Chinese restaurant process (nCRP) is a distribution over hierarchical partitions [ 1 ]. It generalizes the Chinese restaurant process (CRP), which is a distribution over partitions. The CRP can be described by the following metaphor. Imagine a restaurant with an infinite number of tables, and imagine customers entering the restaurant in sequence. The d th customer sits at a table according to the following distribution, where m k is the number of previous customers sitting at table k and  X  is a positive scalar. After D customers have sat down, their seating plan describes a partition of D items.
 In the nested CRP, imagine now that tables are organized in a hierarchy: there is one table at the first level; it is associated with an infinite number of tables at the second level; each second-level table is associated with an infinite number of tables at the third level; and so on until the L th level. Each customer enters at the first level and comes out at the L th level, generating a path with L tables as she sits in each restaurant. Moving from a table at level ` to one of its subtables at level ` + 1 , the customer draws following the CRP using Equation 1. (This description is slightly different from the metaphor in [1], but leads to the same distribution.) The nCRP mixture model can be derived by analogy to the CRP mixture model [ 15 ]. (From now on, we will use the term  X  X odes X  instead of  X  X ables. X ) Each node is associated with a parameter w , where w  X  G 0 and G 0 is called the base distribution. Each data point is drawn by first choosing a path in the tree according to the nCRP, and then choosing its value from a distribution that depends on the parameters in that path. An additional hidden variable x represents other latent quantities that can be used in this distribution. This is a generalization of the model described in [ 1 ]. For data D = { t n } N n =1 , the nCRP mixture assumes that the n th data point t n is drawn as follows: The parameters  X  and  X  are associated with the latent variables x and data generating distribution, respectively. Note that W c n contains the w i s selected by the path c n . Specific applications of the nCRP mixture depend on the particular forms of p ( w ) , p ( x ) and p ( t | W c , x ) . The corresponding posterior of the latent variables decomposes the data into a collection of paths, and provides distributions of the parameters attached to each node in those paths. Even though the nCRP assumes an  X  X nfinite X  tree, the paths associated with the data will only populate a portion of that tree. Through this posterior, the nCRP mixture can be used as a flexible tree-based mixture model that does not assume a particular tree structure in advance of the data.
 Hierarchical topic models. The nCRP mixture described above includes the hierarchical topic model of [ 1 ] as a special case. In that model, observed data are documents, i.e., a list of N words from a fixed vocabulary. The nodes of the tree are associated with distributions over words ( X  X opics X ), and each document is associated with both a path in the tree and with a vector of proportions over its levels. Given a path, a document is generated by repeatedly generating level assignments from the proportions and then words from the corresponding topics. In the notation above, p ( w ) is a Dirichlet distribution over the vocabulary simplex, p ( x ) is a joint distribution of level proportions (from a Dirichlet) and level assignments ( N draws from the proportions), and p ( t | W c , x ) are the N draws from the topics (for each word) associated with x .
 Tree-based hierarchical component analysis. For continuous data, if p ( w ) , p ( x ) and p ( t | W c , x ) are appropriate Gaussian distributions, we obtain hierarchical component analysis, a generalization of probabilistic principal component analysis (PPCA) [ 16 , 17 ]. In this model, w is the component parameter for the node it belongs to. Each path c can be thought as a PPCA model with factor loading W c specified by that path. Then each data point chooses a path (also a PPCA model specified by that path) and draw the factors x . This model can also be thought as an infinite mixtures of PPCA model, Figure 1: Left. A possible tree structure in a 3 -level nCRP. Right. The tree-based stick-breaking construction of a 3 -level nCRP. where each PPCA can share components. In addition, we can incorporate the general exponential family PCA [18, 19] into the nCRP framework. 1 2.1 Tree-based stick-breaking construction CRP mixtures can be equivalently formulated using the Dirichlet process (DP) as a distribution over the distribution of each data point X  X  random parameter [ 21 , 4 ]. An advantage of expressing the CRP mixture with a DP is that the draw from the DP can be explicitly represented using the stick-breaking construction [ 22 ]. The DP bundles the scaling parameter  X  and base distribution G 0 . A draw from a DP(  X ,G 0 ) is described as where  X  are the stick lengths, and P  X  i =1  X  i = 1 almost surely. This representation also illuminates the discreteness of a distribution drawn from a DP.
 For the nCRP, we develop a similar stick-breaking construction. At the first level, the root node X  X  stick length is  X  1 = v 1  X  1 . For all the nodes at the second level, their stick lengths are constructed 1 . The stick-breaking construction is then applied to each of these stick segments at the second level. For example, the  X  11 portion of the stick is divided up into an infinite number of pieces according to the stick-breaking process. For the segment  X  1 k , the stick lengths of its children are  X  continues for L levels. This construction is best understood by Figure 1 (Right).
 Although this stick represents an infinite tree, the nodes are countable and each node is uniquely identified by a sequence of L numbers. We will denote all Beta draws as V , each of which are independent draws from Beta (1 , X  ) (except for the root v 1 , which is equal to one). The tree-based stick-breaking construction lets us calculate the conditional probability of a path given V . Let the path c = [1 ,c 2 ,  X  X  X  ,c L ] , By integrating out V in Equation 2, we recover the nCRP. Given Equation 2, the joint probability of a data set under the nCRP mixture is This representation is the basis for variational inference. The central computational problem in Bayesian modeling is posterior inference: Given data, what is the conditional distribution of the latent variables in the model? In the nCRP mixture, these latent variables provide the tree structure and node parameters. Posterior inference in an nCRP mixture has previously relied on Gibbs sampling, in which we sample from a Markov chain whose stationary distribution is the posterior [ 1 , 2 , 3 ]. Variational inference provides an alternative methodology: Posit a simple (e.g., factorized) family of distributions over the latent variables indexed by free parameters (called  X  X ariational parameters X ). Then fit those parameters to be close in KL divergence to the true posterior of interest [8, 23].
 Variational inference for Bayesian nonparametric models uses a truncated stick-breaking represen-tation in the variational distribution [ 9 ]  X  free variational parameters are allowed only up to the truncation level. If the truncation is too large, the variational algorithm will still isolate only a subset of components; if the truncation is too small, methods have been developed to expand the truncated stick as part of the variational algorithm [ 10 ]. In the nCRP mixture, however, the challenge is that the tree structure is too large even to effectively truncate. We will address this by defining search criteria for adaptively adjusting the structure of the variational distribution, searching over the set of trees to best accommodate the data. 3.1 Variational inference based on the tree-based stick-breaking construction We first address the problem of variational inference with a truncated tree of fixed structure. Suppose that we have a truncated tree T and let M T be the set of all nodes in T . Our family of variational distributions is defined as follows, no variational parameters; (2) Distributions q ( w i ) and q ( v i ) for i  X  M T contain the variational parameters that we want to optimize for the truncated tree T ; (3) Distribution q ( c n ) is the variational multinomial distribution over all the possible paths, not just those in the truncated tree T . Note that there are infinite number of paths. We will address this issue below; (4) Distribution q ( x n ) is the In summary, this family of distributions retains the infinite tree structure. Moreover, this family is defined over T 1 are a special case of those defined over T 2 . Theoretically, the solution found using T 2 is at least as good as the one found using T 1 . This allows us to use greedy search to find a better tree structure.
 With the variational distributions (Equation 4) and the joint distributions (Equation 3), we turn to the details of posterior inference. Equivalent to minimizing KL is tightening the bound on the likelihood of the observations D = { t n } N n =1 given by Jensen X  X  inequality [8], log p ( t 1: N )  X  E q [log p ( t 1: N , V , W , x 1: N , c 1: N )]  X  E q [log q ( V , W , x 1: N , c 1: N )] , L ( q ) . (5) We optimize L ( q ) using coordinate ascent. First we isolate the terms that only contain q ( c n ) , Then we find the optimal solution for q ( c n ) by setting the gradient to zero: plug the optimal q ( c n ) (Equation 7) into Equation 6 to obtain the lower bound Two issues arise: 1) the variational distribution q ( c n ) has infinite number of values, and we need to find an efficient way to manipulate this. 2) the lower bound log P c S n, c (Equation 8) contains infinite sum, which pose a problem in evaluation. In the appendix, we show that all the operations can be done only via the truncated tree T . We summarize the results as follows. Let  X  c be a path in T , either an inner path (a path ending at an inner node) or a full path (a path ending at a leaf node). Note that the inner path is only defined for the truncated tree T . The number of such  X  c is finite. In the compute these quantities efficiently: Consequently iterating over the truncated tree T using  X  c is the same as iterating all the full paths in the nCRP tree. And these are all we need for doing variational inference.
 Next, we move to optimize q ( v i | a i ,b i ) for i  X  X  T , where a i and b i are variational parameters for term that only contains v i from the lower bound (Equation 5), After plugging Equation 2 into 10 and setting the gradient to be zero, we obtain the optimal q ( v i ) , where the infinite sum involved can be solved using Equations 9.
 The variational update functions for W and x depend on the actual distributions we use, and deriving them is straightforward. If they include an infinite sum then we apply similar techniques as we did for q ( v i ) . 3.2 Refining the tree structure during variational inference Since our variational distribution is nested, a larger truncated tree will always (theoretically) achieve a lower bound at least as tight as a smaller truncated tree. This allows us to search the infinite tree space until a certain criterion is satisfied (e.g., relative change of the lower bound). To achieve this, we present several heuristics to guide us to do so. All these operations are performed on the truncated tree T .
 Grow. This operation is similar to what Gibbs sampling does in searching the tree space. We implement two heuristics: 1) Randomly choose several data points, and for each of them sample according to g (  X  c ) , and expand it to full path.
 Prune. If a certain path gets very little probability assignments from all data points, we eliminate this path  X  for path c , the criterion is P N n =1 q ( c n = c ) &lt;  X  , where  X  is a small number. We use  X  = 10  X  6 ). This mimics Gibbs sampling in the sense that for nCRP (or CRP), if a certain path (table) gets no assignments in the sampling process, it will never get any assignment any more according to Equation 1.
 Merge. If paths i and j give almost equal posterior distributions, merging these two paths is i )] T . We use 0 . 95 as the threshold in our experiments.
 In theory, Prune and Merge may decrease the lower bound. Empirically, we found even sometime it does, the effect is negligible. (but reduced the size of the tree). For continuous data settings, we additionally implement the Split method used in [24]. In this section, we demonstrate variational inference for the nCRP. We analyze both discrete and continuous data using the two applications discussed in Section 2. Table 1: Test set likelihood comparison on three datasets. Var. inference (G): variational inference initialized from the initialization of Gibbs sampling. Variational inference can give competitive performance on test set likelihood. 4.1 Hierarchical topic modeling For discrete data, we compare variational inference compared with Gibbs sampling for hierarchical topic modeling. Three corpora are used in the experiments: (1) JACM : a collection of 536 abstracts from the Journal of the ACM from years 1987 to 2004 with a vocabulary size of 1,539 and around 68K words; (2) Psy. Review : a collection of 1,272 psychology abstracts from Psychological Review from years 1967 to 2003, with a vocabulary size of 1,971 and around 137K words; (3) PNAS : a collection of 5,000 abstracts from the Proceedings of the National Academy of Sciences from years 1991 to 2001, with a vocabulary size of 7762 and around 895K words. Those terms occurring in fewer than 5 documents were removed.
 Local maxima can be a problem for both Gibbs sampling and variational inference. To avoid them in Gibbs sampling, we randomly restart the sampler 200 times and take the trajectory with the highest average posterior likelihood. We run the Gibbs sampling for 10000 iterations and collect the results for post analysis. For variational inference, we use two types of initializations 1) similar to Gibbs sampling, we gradually add data points during the variational inference as well  X  add a new path for each document in the initialization; 2) we initialize the variational inference from the initialization for Gibbs sampling  X  using the MAP estimate using one Gibbs sample. We set L = 3 for all the experiments and use the same hyperparameters in both algorithms. Specifically, the stick-breaking prior parameter  X  is set to 1.0; the symmetric Dirichlet prior parameter for the topics is set to 1 . 0 ; the prior for level proportions is skewed to favor high levels (50 , 20 , 10) . (This is suggested in [1].) We run the variational inference until the relative change of log-likelihood is less than 0.001. Per-word test set likelihood. We use test set likelihood as a measure of performance. The proce-of D test given D train . We use the same method in Teh et al. [ 12 ] to approximate it. Specifically, we use posterior means  X   X  and  X   X  to represent the estimated topic mixture proportions over L levels and topic multinomial parameters. For the variational method, we use where  X   X  and  X   X  are estimated using mean values from the variational distributions. For Gibbs sampling, we use S samples and compute where  X   X  s and  X   X  s are estimated using sample s [ 25 , 12 ]. We use 30 samples collected at a lag of 10 after a 200-sample burn-in for a document in test set. Actually, 1 /S P S s =1 P c  X  c s shows the test likelihood comparison using five-fold cross validation. This shows our model can give competitive performance in term of the test set likelihood. This discrepancy is similar to that in [ 12 ] when variational inference is compared the collapsed Gibbs sampling for HDP.
 Topic visualizations. Figures 2 and 3 show the tree-based topic visualizations from JACM and PNAS datasets. These are quite similar to those obtained by Gibbs sampling (see [1]). 4.2 Modeling handwritten digits using hierarchical component analysis For continuous data, we use the hierarchical component analysis for modeling handwritten digits (http://archive.ics.uci.edu/ml). This dataset contains 3823 handwritten digits as a training set and Figure 2: A sub network discovered on JACM dataset, each topic represented by top 5 terms. The whole tree has 30 nodes, with an average branching factor 2.64. Figure 3: A sub network discovered on PNAS dataset, each topic represented by top 5 terms. The whole tree has 45 nodes, with an average branching factor 2.93. 1797 as a testing set. Each digit contains 64 integer attributes, ranging from 0-16. As described in section 2, we use PPCA [ 16 ] as the basic model for each path. We use a global mean parameter  X  for all paths, although a model with an individual mean parameter for each path can be similarly derived. We put broad priors over the parameters similar to those in variational Bayesian PCA [ 17 ]. The stick-breaking prior parameter  X  = 1 is set to be 1.0; for each node, w  X  X  ( 0 , 10 3 ) ;  X   X  X  ( 0 , 10 3 ) ; the inverse of the variance for the noise model in PPCA is  X  and  X   X  Gamma(10  X  3 , 10  X  3 ) . Again, we run the variational inference until the relative change of log-likelihood is less than 0.001. We compare the reconstruction error with PCA. To compute the reconstruction error for our model, we first select the path for each data point using its MAP estimation by  X  c n = arg max c q ( c n = c ) . Then we use the similar approach [26, 24] to reconstruct t n , We test our model using depth L = 2 , 3 , 4 , 5 . All of our models run within 2 minutes. The reconstruction errors for both the training and testing set are shown in Table 2. Our model gives lower reconstruction errors than PCA. In this paper, we presented the variational inference algorithm for the nested Chinese restaurant process based on its tree-based stick-breaking construction. Our result indicates that the variational Table 2: Reconstruction error comparison (Tr: train; Te: test). HCA stands for hierarchical component analysis. PCA uses L largest components. In the first column, 2(9) means L = 2 with 9 nodes inferred using our model. Others are similarly defined. HCA gives lower reconstruction errors. inference is a powerful alternative method for the widely used Gibbs sampling. We also adapt the nCRP to model continuous data, e.g. in hierarchical component analysis.
 Acknowledgements. We thank anonymous reviewers for insightful suggestions. David M. Blei is supported by ONR 175-6343, NSF CAREER 0745520, and grants from Google and Microsoft.
 Case 1: All nodes of the path are in T , c  X  X  T . Let Z 0 , E q [log p ( t n | x n , W c )] . We have Case 2: At least one node is not in T , c 6 X  M T . Although c 6 X  M T , c must have some nodes outside the truncated tree is the same prior distribution. We have
P where v  X  Beta (1 , X  ) . Such cases contain all inner nodes in the truncated tree T . Note that Case 1 is a special case of Case 2 by setting ` 0 = L . Given all these, P c S n, c can be computed efficiently. Furthermore, given Equations 13 and Equation 7, we define which corresponds the sum of probabilities from all paths in child (  X  c ) . We note that this organization only depends on the truncated tree T and is sufficient for variational inference.
