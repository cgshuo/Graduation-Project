 Generic expressions come in two basic forms: generic noun phrases and generic sentences. Both express rule-like knowledge, but in different ways.
A generic noun phrase is a noun phrase that does not refer to a specific (set of) individual(s), but rather to a kind or class of individuals. Thus, the NP The lion in (1.a) 1 is understood as a ref-erence to the class  X  X ion X  instead of a specific in-dividual. Generic NPs are not restricted to occur with kind-related predicates as in (1.a). As seen in (1.b), they may equally well be combined with predicates that denote specific actions. In contrast to (1.a), the property defined by the verb phrase in (1.b) may hold of individual lions. (1) a. The lion was the most widespread mam-
Generic sentences are characterising sentences that quantify over situations or events, expressing rule-like knowledge about habitual actions or situ-ations (2.a). This is in contrast with sentences that refer to specific events and individuals, as in (2.b). (2) a. After 1971 [Paul Erd  X  os] also took am-
The genericity of an expression may arise from the generic (kind-referring, class-denoting) inter-pretation of the NP or the characterising interpre-tation of the sentence predicate. Both sources may concur in a single sentence, as illustrated in Ta-ble 1, where we have cross-classified the exam-ples above according to the genericity of the NP and the sentence.

This classification is extremely difficult, be-cause (i) the criteria for generic interpretation are far from being clear-cut and (ii) both sources of genericity may freely interact.

The above classification of generic expressions is well established in traditional formal semantics (cf. Krifka et al. (1995)) 2 . As we argue in this paper, these distinctions are relevant for semantic processing in computational linguistics, especially for information extraction and ontology learning and population tasks. With appropriate semantic analysis of generic statements, we can not only formally capture and exploit generic knowledge, but also distinguish between information pertain-ing to individuals vs. classes. We will argue that the automatic identification of generic expressions should be cast as a machine learning problem in-stead of a rule-based approach, as there is (i) no transparent marking of genericity in English (as in most other European languages) and (ii) the phe-nomenon is highly context dependent.

In this paper, we build on insights from for-mal semantics to establish a corpus-based ma-chine learning approach for the automatic classi-fication of generic expressions. In principle our approach is applicable to the detection of both generic NPs and generic sentences, and in fact it would be highly desirable and possibly advanta-geous to cover both types of genericity simulta-neously. Our current work is confined to generic NPs, as there are no corpora available at present that contain annotations for genericity at the sen-tence level.

The paper is organised as follows. Section 2 in-troduces generic expressions and motivates their relevance for knowledge acquisition and semantic processing tasks in computational linguistics. Sec-tion 3 reviews prior and related work. In section 4 we motivate the choice of feature sets for the au-tomatic identification of generic NPs in context. Sections 5 and 6 present our experiments and re-sults obtained for this task on the ACE-2 data set. Section 7 concludes. 2.1 Interpretation of generic expressions Generic NPs There are two contrasting views on how to formally interpret generic NPs. Ac-cording to the first one, a generic NP involves a special form of quantification . Quine (1960), for example, proposes a universally quantified read-ing for generic NPs. This view is confronted with the most important problem of all quantification-based approaches, namely that the exact determi-nation of the quantifier restriction (QR) is highly dependent on the context, as illustrated in (3) 3 . (3) a. Lions are mammals. QR: all lions
In view of this difficulty, several approaches restrict the quantification to only  X  X elevant X  (De-clerck, 1991) or  X  X ormal X  (Dahl, 1975) individu-als.

According to the second view, generic noun phrases denote kinds . Following Carlson (1977), a kind can be considered as an individual that has properties on its own. On this view, the generic NP cannot be analysed as a quantifier over individuals pertaining to the kind. For some predicates, this is clearly marked. (1.a), for instance, attributes a property to the kind lion that cannot be attributed to individual lions.
 Generic sentences are usually analysed using a special dyadic operator, as first proposed by Heim (1982). The dyadic operator relates two semantic constituents, the restrictor and the matrix :
By choosing GEN as a generic dyadic operator, it is possible to represent the two readings (a) and (b) of the characterising sentence (4) by variation in the specification of restrictor and matrix (Krifka et al., 1995). (4) Typhoons arise in this part of the pacific.
In order to cope with characterising sentences as in (2.a), we must allow the generic operator to quantify over situations or events, in this case,  X  X ormal X  situations which were such that Erd  X  os took amphetamines. 2.2 Relevance for computational linguistics Knowledge acquisition The automatic acquisi-tion of formal knowledge for computational appli-cations is a major endeavour in current research and could lead to big improvements of semantics-based processing. Bos (2009), e.g., describes sys-tems using automated deduction for language un-derstanding tasks using formal knowledge.

There are manually built formal ontologies such as SUMO (Niles and Pease, 2001) or Cyc (Lenat, 1995) and linguistic ontologies like Word-Net (Fellbaum, 1998) that capture linguistic and world knowledge to a certain extent. However, these resources either lack coverage or depth. Au-tomatically constructed ontologies or taxonomies, on the other hand, are still of poor quality (Cimi-ano, 2006; Ponzetto and Strube, 2007).

Attempts to automatically induce knowledge bases from text or encyclopaedic sources are cur-rently not concerned with the distinction between generic and non-generic expressions, concentrat-ing mainly on factual knowledge. However, rule-like knowledge can be found in textual sources in the form of generic expressions 5 .

In view of the properties of generic expressions discussed above, this lack of attention bears two types of risks. The first concerns the distinction between classes and instances, regarding the attri-bution of properties. The second concerns mod-elling exceptions in both representation and infer-encing.

The distinction between classes and instances is a serious challenge even for the simplest methods in automatic ontology construction, e.g., Hearst (1992) patterns. The so-called IS-A pat-terns do not only identify subclasses, but also in-stances. Shakespeare , e.g., would be recognised as a hyponym of author in the same way as temple is recognised as a hyponym of civic building .
Such a missing distinction between classes and instances is problematic. First, there are predicates that can only attribute properties to a kind (1.a). Second, even for properties that in principle can be attributed to individuals of the class, this is highly dependent on the selection of the quantifier X  X  re-striction in context (3). In both cases, it holds that properties attributed to a class are not necessarily inherited by any or all instances pertaining to the class.

Zirn et al. (2008) are the first to present fully au-tomatic, heuristic methods to distinguish between classes and instances in the Wikipedia taxonomy derived by Ponzetto and Strube (2007). They re-port an accuracy of 81.6% and 84.5% for differ-ent classification schemes. However, apart from a plural feature, all heuristics are tailored to specific properties of the Wikipedia resource.

Modelling exceptions is a cumbersome but necessary problem to be handled in ontology building, be it manually or by automatic means, and whether or not the genericity of knowledge is formalised explicitly. In artificial intelligence research, this area has been tackled for many years. Default reasoning (Reiter, 1980) is con-fronted with severe efficiency problems and there-fore has not extended beyond experimental sys-tems. However, the emerging paradigm of Answer Set Programming (ASP, Lifschitz (2008)) seems to be able to model exceptions efficiently. In ASP a given problem is cast as a logic program, and an answer set solver calculates all possible answer sets, where an answer set corresponds to a solution of the problem. Efficient answer set solvers have been proposed (Gelfond, 2007). Although ASP may provide us with very efficient reasoning sys-tems, it is still necessary to distinguish and mark default rules explicitly (Lifschitz, 2002). Hence, the recognition of generic expressions is an impor-tant precondition for the adequate representation and processing of generic knowledge. Suh (2006) applied a rule-based approach to auto-matically identify generic noun phrases. Suh used patterns based on part of speech tags that iden-tify bare plural noun phrases, reporting a precision of 28.9% for generic entities, measured against an annotated corpus, the ACE 2005 (Ferro et al., 2005). Neither recall nor f-measure are reported. To our knowledge, this is the single prior work on the task of identifying generic NPs.

Next to the ACE corpus (described in more de-tail below), Herbelot and Copestake (2008) offer a study on annotating genericity in a corpus. Two annotators annotated 48 noun phrases from the British National Corpus for their genericity (and specificity) properties, obtaining a kappa value of 0.744. Herbelot and Copestake (2008) leave su-pervised learning for the identification of generic expressions as future work.

Recent work by Mathew and Katz (2009) presents automatic classification of generic and non-generic sentences, yet restricted to habitual interpretations of generic sentences. They use a manually annotated part of the Penn TreeBank as training and evaluation set 6 . Using a selec-tion of syntactic and semantic features operating mainly on the sentence level, they achieved preci-sion between 81.2% and 84.3% and recall between 60.6% and 62.7% for the identification of habitual generic sentences. 4.1 Properties of generic expressions Generic NPs come in various syntactic forms. These include definite and indefinite singular count nouns, bare plural count and singular and plural mass nouns as in (5.a-f). (5.f) shows a construction that makes the kind reading unam-biguous. As Carlson (1977) observed, the generic reading of  X  X ell-established X  kinds seems to be more prominent (g vs. h). (5) a. The lion was the most widespread mam-
Apart from being all NPs, there is no obvious syntactic property that is shared by all examples. Similarly, generic sentences come in a range of syntactic forms (6). (6) a. John walks to work.
Although generic NPs and generic sentences can be combined freely (cf. Section 1; Table 1), both phenomena highly interact and quite often appear in the same sentence (Krifka et al., 1995). Also, genericity is highly dependent on contex-tual factors. Present tense, e.g., may be indica-tive for genericity, but with appropriate temporal modification, generic sentences may occur in past or future tense (6). Presence of a copular con-struction as in (5.a,b,d) may indicate a generic NP reading, but again we find generic NPs with event verbs, as in (5.e) or (1.b). Lexical semantic fac-tors, such as the semantic type of the clause predi-cate (5.c,e), or  X  X ell-established X  kinds (5.g) may favour a generic reading, but such lexical factors are difficult to capture in a rule-based setting.
In our view, these observations call for a corpus-based machine learning approach that is able to capture a variety of factors indicating genericity in combination and in context. 4.2 Feature set and feature classes In Table 2 we give basic information about the individual features we investigate for identifying generic NPs. In the following, we will structure this feature space along two dimensions, distin-guishing NP-and sentence-level factors as well as syntactic and semantic (including lexical seman-tic) factors. Table 3 displays the grouping into cor-responding feature classes.
 NP-level features are extracted from the local NP without consideration of the sentence context.
Sentence-level features are extracted from the clause (in which the NP appears), as well as sen-tential and non-sentential adjuncts of the clause. We also included the (dependency) relations be-tween the target NP and its governing clause.
Syntactic features are extracted from a parse tree or shallow surface-level features. The feature set includes NP-local and global features.

Semantic features include semantic features abstracted from syntax, such as tense and aspect or type of modification, but also lexical semantic features such as word sense classes, sense granu-larity or verbal predicates.

Our aim is to determine indicators for genericity from combinations of these feature classes. give descriptions. All features may have a NULL value. Name Descriptions and Features
Set 2 Five best feature tuples:
Set 3 Five best feature triples: 5.1 Dataset As data set we are using the ACE-2 (Mitchell et al., 2003) corpus, a collection of newspaper texts annotated with entities marked for their genericity. In this version of the corpus, the classification of entities is a binary one.
 Annotation guidelines The ACE-2 annotation guidelines describe generic NPs as referring to an arbitrary member of the set in question, rather than to a particular individual. Thus, a property at-tributed to a generic NP is in principle applicable to arbitrary members of the set (although not to all of them). The guidelines list several tests that are either local syntactic tests involving determin-ers or tests that cannot be operationalised as they involve world knowledge and context information.
The guidelines give a number of criteria to iden-tify generic NPs referring to specific properties. These are (i) types of entities ( lions in 3.a), (ii) suggested attributes of entities ( mammals in 3.a), (iii) hypothetical entities (7) and (iv) generalisa-tions across sets of entities (5.d). (7) If a person steps over the line, they must be
The general description of generic NPs as de-noting arbitrary members of sets obviously does not capture kind-referring readings. However, the properties characterised (i) can be understood to admit kinds. Also, some illustrations in the guide-lines explicitly characterise kind-referring NPs as generic. Thus, while at first sight the guidelines do not fully correspond to the characterisation of generics we find in the formal semantics literature, we argue that both characterisations have similar extensions, i.e., include largely overlapping sets of noun phrases. In fact, all of the examples for generic noun phrases presented in this paper would also be classified as generic according to the ACE-2 guidelines.

We also find annotated examples of generic NPs that are not discussed in the formal semantics liter-ature (8.a), but that are well captured by the ACE-2 guidelines. However, there are also cases that are questionable (8.b). (8) a.  X  X t X  X  probably not the perfect world, but
This shows that the annotation of generics is dif-ficult, but also highlights the potential benefit of a corpus-driven approach that allows us to gather a wider range of realisations. This in turn can con-tribute to novel insights and discussion.
 Data analysis A first investigation of the corpus shows that generic NPs are much less common than non-generic ones, at least in the newspaper genre at hand. Of the 40,106 annotated entities, only 5,303 (13.2%) are marked as generic. In or-der to control for bias effects in our classifier, we will experiment with two different training sets, a balanced and an unbalanced one. 5.2 Preprocessing The texts have been (pre-)processed to add sev-eral layers of linguistic annotation (Table 5). We use MorphAdorner for sentence splitting and Tree-Tagger with the standard parameter files for part of speech tagging and lemmatisation. As we do not have a word sense disambiguation system available that outperforms the most frequent sense baseline, we simply used the most frequent sense (MFS). The countability information is taken from Celex. Parsing was done using the English LFG grammar (cf. Butt et al. (2002)) in the XLE pars-ing platform and the Stanford Parser.

As the LFG-grammar produced full parses only for the sentences of 56% of the entities (partial parses: 37% of the entities), we chose to integrate the Stanford parser as a fallback. If we are unable to extract feature values from the f-structure pro-duced by the XLE parser, we extract them from the Stanford Parser, if possible. Experimentation showed using the two parsers in tandem yields best results, compared to individual use. 5.3 Experimental setup Given the unclear dependencies of features, we chose to use a Bayesian network. A Bayesian net-work represents the dependencies of random vari-ables in a directed acyclic graph, where each node represents a random variable and each edge a de-pendency between variables. In fact, a number of feature selection tests uncovered feature depen-dencies (see below). We used the Weka (Witten and Frank, 2002) implementation BayesNet in all our experiments.

To control for bias effects, we created balanced data sets by oversampling the number of generic entities and simultaneously undersampling non-generic entities. This results in a dataset of 20,053 entities with approx. 10,000 entities for each class. All experiments are performed on balanced and unbalanced data sets using 10-fold cross-validation, where balancing has been performed for each training fold separately (if any). Feature classes We performed evaluation runs for different combinations of feature sets: NP-vs. S-level features (with further distinction between syntactic and semantic NP-/S-level features), as well as overall syntactic vs. semantic features. This was done in order to determine the effect of different types of linguistic factors for the detec-tion of genericity (cf. Table 3). Feature selection We experimented with two methods for feature selection. Table 4 shows the resulting feature sets.

In ablation testing , a single feature in turn is temporarily omitted from the feature set. The fea-ture whose omission causes the biggest drop in f-measure is set aside as a strong feature. This pro-cess is repeated until we are left with an empty feature set. From the ranked list of features f 1 to f n we evaluate increasingly extended feature sets f .. f i for i = 2..n. We select the feature set that yields the best balanced performance, at 45.7% precision and 53.6% f-measure. The features are given as Set 5 in Table 4.

As ablation testing does not uncover feature de-pendencies, we also experimented with single, tu-ple and triple feature combinations to determine features that perform well in combination. We ran evaluations using features in isolation and each possible pair and triple of features. We select the resulting five best features, tuples and triples of features. The respective feature sets are given as Set 1 to Set 3 in Table 4. The features that appear most often in Set 1 to Set 3 are grouped in Set 4. Baseline Our results are evaluated against three baselines. Since the class distribution is unequal, a majority baseline consists in classifying each en-tity as non-generic. As a second baseline we chose the performance of the feature Person , as this fea-ture gave the best performance in precision among those that are similarly easy to extract. Finally, we compare our results to (Suh, 2006). The results of classification are summarised in Ta-ble 6. The columns Generic and Non-generic give the results for the respective class. Overall shows the weighted average of the classes.
 Comparison to baselines Given the bias for non-generic NPs in the unbalanced data, the ma-jority baseline achieves high performance overall (F: 80.6). Of course, it does not detect any generic NPs. The Person -based baseline also suffers from very low recall (R: 10.2%), but achieves the high-est precision (P: 60.5 %). (Suh, 2006) reported only precision of the generic class, so we can only compare against this value (28.9 %). Most of the features and feature sets yield precision values above the results of Suh.
 Feature classes, unbalanced data For the identification of generic NPs, syntactic features achieve the highest precision and recall (P: 40.1%, R: 66.6 %). Using syntactic features on the NP-or sentence-level only, however, leads to a drop in precision as well as recall. The recall achieved by syntactic features can be improved at the cost of precision by adding semantic features (R: 66.6  X  72.1, P: 40.1  X  37). Semantic features in sep-aration perform lower than the syntactic ones, in terms of recall and precision.

Even though our results achieve a lower pre-cision than the Person baseline, in terms of f-measure, we achieve a result of over 50%, which is almost three times the baseline.
 Feature classes, balanced data Balancing the training data leads to a moderate drop in perfor-mance. All feature classes perform lower than on the unbalanced data set, yielding an increase in re-call and a drop in precision. The overall perfor-mance differences between the balanced and un-balanced data for the best achieved values for the generic class are -4.7 (P), +13.2 (R) and -1.7 (F). This indicates that (i) the features prove to perform rather effectively, and (ii) the distributional bias in the data can be exploited in practical experiments, as long as the data distribution remains constant.
We observe that generally, the recall for the generic class improves for the balanced data. This is most noticeable for the S-level features with an increase of 55 (syntactic) and 29.7 (semantic). This could indicate that S-level features are useful for detecting genericity, but are too sparse in the non-oversampled data to become prominent. This holds especially for the lexical semantic features.
As a general conclusion, syntactic features prove most important in both setups. We also ob-serve that the margin between syntactic and se-mantic features reduces in the balanced dataset, and that both NP-and S-level features contribute to classification performance, with NP-features generally outperforming the S-level features. This confirms our hypothesis that all feature classes contribute important information.
 Feature selection While the above figures were obtained for the entire feature space, we now dis-cuss the effects of feature selection both on per-formance and the distribution over feature classes. The results for each feature set are given in Ta-ble 6. In general, we find a behaviour similar to Table 7: Best performing features by feature class the homogeneous classes, in that balanced train-ing data increases recall at the cost of precision.
With respect to overall f-measure, the best sin-gle features are strong on the unbalanced data. They even yield a relatively high precision for the generic NPs (49.5%), the highest value among the selected feature sets. This, however, comes at the price of one of the lowest recalls. The best per-forming feature in terms of f-measure on both bal-anced and unbalanced data is Set 5 with Set 4 as a close follow-up. Set 5 achieves an f-score of 53.6 (unbalanced) and 51.0 (balanced). The highest re-call is achieved using Set 4 (69.6% on the unbal-anced and 83.1% on the balanced dataset). The results for Set 5 represent an improvement of 3.5 respectively 2.6 (unbalanced and balanced) over the best achieved results on homogeneous feature classes. In fact, Table 7 shows that these features, selected by ablation testing, distribute over all ho-mogeneous classes.

We trained a decision tree to gain insights into the dependencies among these features. Figure 1 shows an excerpt of the obtained tree. The clas-sifier learned to classify singular proper names as non-generic, while the genericity of singular nouns depends on their predicate. At this point, the classifier can correctly classify some of the NPs in (5) as kind-referring (given the training data contains predicates like  X  X idespread X ,  X  X ie out X , ...). This paper addresses a linguistic phenomenon that has been thoroughly studied in the formal se-mantics literature but only recently is starting to be addressed as a task in computational linguis-tics. We presented a data-driven machine learn-ing approach for identifying generic NPs in con-text that in turn can be used to improve tasks such as knowledge acquisition and organisation. The classification of generic NPs has proven difficult even for humans. Therefore, a machine learning approach seemed promising, both for the identifi-cation of relevant features as for capturing contex-
Figure 1: A decision tree trained on feature Set 5 tual factors. We explored a range of features using homogeneous and mixed classes gained by alter-native methods of feature selection. In terms of f-measure on the generic class, all feature sets per-formed above the baseline(s). In the overall clas-sification, the selected sets perform above the ma-jority and close to or above the Person baseline.
The final feature set that we established charac-terises generic NPs as a phenomenon that exhibits both syntactic and semantic as well as sentence-and NP-level properties. Although our results are satisfying, in future work we will extend the range of features for further improvements. In particular, we will address lexical semantic features, as they tend to be effected by sparsity. As a next step, we will apply our approach to the classification of generic sentences. Treating both cases simul-taneously could reveal insights into dependencies between them.

The classification of generic expressions is only a first step towards a full treatment of the chal-lenges involved in their semantic processing. As discussed, this requires a contextually appropriate selection of the quantifier restriction 8 , as well as determining inheritance of properties from classes to individuals and the formalisation of defaults.
