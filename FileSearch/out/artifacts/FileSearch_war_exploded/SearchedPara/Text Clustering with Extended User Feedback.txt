 Text clustering is most commonly treated as a fully auto-mated task without user feedback. However, a variety of re-searchers have explored mixed-initiative clustering methods which allow a user to interact with and advise the clustering algorithm. This mixed-initiative approach is especially at-tractive for text clustering tasks where the user is trying to organize a corpus of documents into clusters for some par-ticular purpose (e.g., clustering their email into folders that reflect various activities in which they are involved). This paper introduces a new approach to mixed-initiative clus-tering that handles several natural types of user feedback. We first introduce a new probabilistic generative model for text clustering (the SpeClustering model )and show that it outperforms the commonly used mixture of multinomi-alsclusteringmodel,evenwhenusedinfullyautonomous mode with no user input. We then describe how to incor-porate four distinct types of user feedback into the cluster-ing algorithm, and provide experimental evidence showing substantial improvements in text clustering when this user feedback is incorporated.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Clustering General Terms: Algorithms, Theory Keywords: text clustering, user feedback, mixed-initiative learning
We as human beings are quite familiar with clustering objects into categories based on features of these objects. For example, a computer user may sort her emails into fold-ers that are personally meaningful because each one rep-resents a particular activity she is involved in, or because they are emails from a particular group of people, etc. For each folder, or cluster, the user may have in mind a rich category description, but assigns objects to these categories based on their surface features (e.g., the words in the email, Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. or recipients in the header). There are many other exam-ples: we may informally cluster news stories into categories such as sports, politics, etc., or we may easily recognize in a supermarket what type of products a corridor belongs to.
Computer algorithms for clustering are typically cast as fully automated, unsupervised learning algorithms; that is, the algorithm is given only the collection of instances and the surface features that describe each, without any infor-mation about the nature of the clusters. Recently, however, a variety of researchers have studied ways of allowing a user to provide limited information to improve clustering quality. One approach is to allow the user to provide cluster labels for some of the instances, indicating which cluster that in-stance belongs to. For example, [11][2][8] use labels of this type to form initial cluster descriptions, which are then re-fined using both the unlabeled and labeled instances. A second type of input information consists of pair-wise con-straints among instances [13]. These constraints may assert that two documents must belong to the same cluster without indicating which one it is, or may assert that two documents must belong to different clusters. Various constraint-based methods and distance-based methods have been proposed to use this type of information. See [1] for a short survey on different approaches and also for an approach to integrat-ing distance-based and constraint-based approaches into a probabilistic framework. A third type of additional input involves background knowledge to enrich the set of features that describe each instance. For example, [6] enriches their document representation by using an ontology (WordNet) as background knowledge. A fourth type of extra informa-tion, which we are primarily interested in, is information about the key surface features for a particular class, or clus-ter. For example, [9] uses a few user-supplied keywords per class and a class hierarchy to generate preliminary labels to build an initial text classifier for the class. [10] proposes an interesting technique in which they ask a user to identify interesting words among automatically selected representa-tive words for each class of documents, and then use these user-identified words to re-train the classifier as in [9].
Researchers working on active learning have also studied using feedback about key features. For example, [5] converts a user-recommended feature into a mini-document which is used to help train an SVM classifier. An altenative approach to using this information is proposed by [12] who adjust the SVM weights associated with these key features to a pre-defined value in binary classification tasks.

We are interested in how to best incorporate user input into automated clustering algorithms, and more generally into mixed-initiative clustering approaches that allow the user and computer to jointly arrive at coherent clusters that capture the categories of interest to the user. Note this goal of discovering clusters of interest to the user is somewhat dif-ferent from the objective optimized in totally unsupervised clustering algorithms that attempt to maximize some sta-tistical property of the clusters (such as data likelihood, or inter-cluster distance). We are specifically interested in how to incorporate into clustering algorithms the user X  X  emerg-ing understanding about a category 1 , stimulated by seeing the instances that are clustered together, and by seeing (and editing )summaries of these emerging clusters. A user X  X  un-derstanding about a category may be expressed in a variety of forms, such as by keywords, important person names, other types of entities, and relationships among entities. It may encapsulate a variety of types of information, and it may be difficult for a user to articulate fully their notion of the cluster.

The chief contribution of this paper is to introduce a new probabilistic model for clustering that outperforms standard unsupervised clustering in our experiments, and that also can accomodate a variety of types of user feedback to iter-atively refine the clusters. We present experiments in both an email clustering domain, and in a second document clus-tering domain (20 Newsgroups )showing the performance of this clustering approach.

The research we report here is part of our larger research effort to build computer algorithms to automatically infer the key activities, or projects, a user is involved in, given the contents of their workstation (e.g., their emails, files, directories, calendar entries, personal contacts lists, etc.). For example, a user may be involved in activities such as teaching a particular course, participating in a particular committee, hanging out with a particular group of friends, etc. In our previous work[7], we have shown that unsuper-vised clustering of emails can result in useful descriptions of user activities, such as the one shown in Figure 1. The work we report in this paper is motivated in part by our interest in developing a more mixed-initiative approach to inferring such activity clusters, using both computer analysis of work-station data and user feedback based on examining proposed clusters.
 Figure 1: An example output of activity extractor,
We will describe our probabilistic model and the associ-ated clustering algorithm in the next section. Section 3 then
We use the word  X  X luster X  to indicate a set of similar in-stances grouped together by a clustering algorithm, and the word  X  X ategory X  to indicate a concept in a user X  X  mind which may or may not be reflected by some cluster of instances. discusses how several types of user feedback can be incorpo-rated into the clustering algorithm. The experimental setup and evaluation are described in section 4 and conclusions are presented in section 5.
We present here a clustering algorithm based on a novel probabilistic model. One commonly used probabilistic model for text clustering is the multinomial naive Bayes model de-scribed in [11], which models a document as a vector of words with each word generated independently by a multino-mial probability distribution conditioned on the document X  X  class (i.e., conditioned on which cluster it belongs to). Our SpeClustering model also assumes words are generated prob-abilistically, but differs in an important way from this stan-dard model. In particular, the SpeClustering model assumes that only some of the words in the document are conditioned on the document X  X  cluster, and that other words follow a more general word distribution that is independent of which cluster the document belongs to. To see the intuition behind this model, consider a cluster of emails about skiing. There will be some words (e.g.,  X  X now X  )that appear in this cluster of emails because the topic is skiing, and there will be other words (e.g.,  X  X ontact X  )that appear for reasons independent of the cluster topic. The key difference between the stan-dard model and our SpeClustering model is that our model assumes each document is generated by a mixture of two multinomials  X  one associated with the document X  X  cluster, and the other shared across all clusters. As we show below, our more elaborate SpeClustering model can lead to im-proved accuracy when used for automatic clustering, and it also provides a formalism that can easily accomodate several important types of user feedback to support mixed-initiative clustering.

To construct this SpeClustering model, we extend the standard multinomial model in two ways. The first mod-ification is to add a G topic variable that is intended to capture general topics not related to the cluster. The sec-ond modification is to introduce a hidden boolean variable, X, associated with each word O in each document. If X =1, the observation O is generated by the cluster-specific topic S, and if X = 0, the observation O is generated by a general topic G. Throughout this paper we simplify the model by assuming there is only one general topic instead of multiple topics, so the value of G is fixed at G = g .Figure2shows the graphical model representation of the model. Here the outer rectangle (or plate )is duplicated for each of the D doc-uments, and the inner plate is duplicated for each of the N observations O and associated variables X. Note the general topic G is constant across all documents and words, whereas the cluster topic S is different for each document.
The Speclustering model  X  has four sets of parameters: Figure 2: Graphical representation of SpeClustering where c  X  X  1 , 2 ,..., | S |} , g  X  X  1 } for the simplified case and v  X  X  1 , 2 ,..., | O |} .

Given a corpus C that contains D instances C = { d 1 ,d 2 } , and d i is represented as a vector of observations { o ij { 1 , 2 ,...,n i }} , we use the notation s i to indicate the value of the hidden S variable for instance d i and x ij to indicate the value of the hidden X variable associated with observation o . The corpus likelihood of C given  X  is defined as follows: P ( C |  X  )= which can be written in terms of the model parameters as follows:
Note the probability P ( X =1 | S = c,O = f ;  X  ),which can be derived from the model parameters, describes the probability that any particular feature f is generated by a particular cluster c , as opposed to the general topic g.
In the most general case we are interested in unsuper-vised clustering of documents given just the observed fea-tures O of a set of documents, where the values for the S and X variables are unobserved. Because of the existence of unobserved variables, we use an EM process [3] for pa-rameter estimation. The EM algorithm is commonly ap-plied to find a (local )maximum-likelihood estimate of the parameters in situations when the observable data is incom-plete and the model depends on unobserved latent variables. Given X and Y as the incomplete and complete data, the algorithm iterates through two steps: in the E step, we eval-uate Q (  X  |  X  t )= E [log P ( Y|  X  ) |X , X  t )],andinMstep,weob-tain new estimation of parameters  X  t +1 =argmax  X  Q (  X  | In our SpeClustering model, the incomplete data is X = { o ij  X  i  X  X  1 ,...,D } j  X  X  1 ,...,n i }} and complete data is Y = { s i ,x ij ,o ij  X  i  X  X  1 ,...,D } j  X  X  1 ,...,n i }} .Theexact estimation for each parameter in M step is listed below. where the following quantities are computed in the E step:
By iterating through E step and M step, the likelihood will converge to a (local )maximum and values of parameters will be stabilized.
In some cases instances may be described by multiple types of features. For example, when clustering emails we might describe each email by the set of words in its body, plus the set of email addresses the email is sent to. If there are multiple types of features in an instance, we can ex-tend the SpeClustering model. Figure 3 shows the extended model with two feature types. The model adds one new block { Y,Q } for the introduction of a new feature type. {
Y,Q } is identical and parallel to { X,O } .Intheactivity-discovery-via-emails task, we can apply this model to rep-resent an activity in terms of both its key words and the primary participants of the activity.

Parameter estimation in the extended SpeClustering model is nearly identical to that described in section 2.2. The only exception is a change to the posterior probability esti-mate in Eq 1. The new posterior probability estimate in the extended model combines generative probabilities from multiple feature types. Eq 3 shows the estimate from two different feature types. Figure 3: Graphical representation of the SpeCluster-
As discussed earlier, we are particularly interested in al-lowing extended forms of user feedback to help direct the clustering process. In this section we discuss how several types of user feedback are incorporated to guide the clus-tering algorithm. We describe each feedback type in terms of the task of clustering emails to discover descriptions of a user X  X  activities. The types of user feedback allowed are: 1. Remove an activity cluster 2. An email belongs (or does not )to its assigned cluster 3. A keyword belongs (or does not )to its assigned cluster 4. A person belongs (or does not )to its assigned cluster 5. A short text description T for an activity cluster
The posterior probabilities in the SpeClustering model turn out to be highly related to the above types of feed-back. To be more specific, type 1 and 2 feedback are related toEq.3andtype3,4,and5feedbackarerelatedtoEq.2.

There are two methods to initialize the SpeClustering model with user feedback. The simple method inherits pre-vious clustering results for which the user gives her feedback. When feedback includes removing a cluster S = c , we reset the initial value of P ( s i | d i ;  X  t )for each d i with s distributing the probability mass uniformly among all clus-ters but halving the probability to cluster c . Alternatively, the joint method uses multiple feedback types to initialize the model. We first select several documents that have the highest cosine similarity with confirmed documents and key-words (where we treat keywords as a mini-document )and associate them with current clusters. We then search for a small set of similar documents that maximize inter-cluster distances and replace any cluster that is removed in the feed-back.

During each EM iteration while training the SpeCluster-ing model, we perform type 2 to 4 adjustments. For type 2 feedback, we adjust the value of P ( s i = c | d i ;  X  t one if the email( d i )-to-cluster( c )bound is confirmed by the user or set it to zero if the bound is disapproved by the user. Proper adjustment to normalize posterior probabili-ties of { P ( s i = c | d i ;  X  t )  X  c = c } is also required in this case. For type 3 and 4 feedback, we adjust the value of to-cluster( c )bound is confirmed by the user or set it to zero if the bound is disapproved by the user. For type 5 feedback, we tokenize the description T and make each token of T a confirmed keyword as in type 3 feedback.

Figure 4 summarizes this Mixed-Initiative-Clustering pro-cess which integrates user feedback into the clustering pro-cess.
 Algorithm : Mixed-Initiative-Clustering Input : Corpus C with D instances.
 Output : A list of activity clusters A ,whereeachactivity cluster is described by its top K features for each feature type.
 Method : 1. Generate initial model  X  ini and summarization of 2. Add user X  X  feedback regarding A t into F . 3. (  X  t +1 , A t +1 )=SpeClustering-with-Feedback( C , X  5. repeat step 2 to 4 until user X  X  satisfaction.
 Algorithm : SpeClustering-with-Feedback Input : Corpus C with D instances.  X  t as the current model. F as the collection of user X  X  feedback.
 Output :  X  t +1 as the model after adaption according to user X  X  feedback. A t +1 as the new summarization of clusters according to  X  t +1 .
 Method : 1. Estimate posterior probabilities P t of Eq 3 and Eq 2 2. Adjust P t according to F to obtain P t adj . 3. Re-estimate model parameters using P t adj to obtain 4.  X  t =  X  t adj ; repeat step 1 to 3 until the model Figure 4: The algorithm for mixed-initiative clustering. We have described details of the SpeClustering model. However, the model is not restricted to clustering; it can also be applied to supervised classification tasks. The dif-ference in classification is that the topic variable S is no longer a hidden variable. We can treat the classification tasks as knowing all the type 2 user feedback and replace the estimate of posterior probabilities P ( s i = c | d i the true value specified by the instance label.
To test the SpeClustering algorithm we used two data sets. The first is an email dataset ( EmailYH )from one of the authors that contains 623 emails. This dataset had pre-viously been sorted into 11 folders according to the user X  X  activities. It contains 6684 unique words and 135 individ-ual people after pre-processing 2 . The second data set is the publicly available 20-Newsgroups collection. This data set contains text messages from 20 different Usenet newsgroups, with 1000 messages harvested from each newsgroup. We derived three datasets according to [1]. The first, News-Similar-3 , consists of messages from 3 similar newsgroups (comp.graphics, comp.os.ms-windows.misc, comp.windows.x) where cross-posting occurs often between these three news-groups. News-Related-3 consists of messages from 3 re-lated newsgroups (talk.politics.misc, talk.poli-tics.guns and talk.politics.mideast). News-Different-3 contains 3 news-groups of quite different topics (alt.atheism, rec.sport.baseball, and sci.space).

We only use the text part of messages in the three news-group datasets because a reviewer won X  X  have the knowledge needed to decide which author is the key-person with re-gard to which newsgroup. For the text part, we applied the same pre-processing we used in ( EmailYH ). There are 3000 messages in these datasets. News-Different-3 contains 8465 unique words, News-Related-3 contains 9998 unique words and News-Similar-3 has 10037 unique words.
We use two measurements to estimate cluster quality: folder-reconstruction accuracy, and normalized mutual in-formation (NMI )[4].

In order to calculate the folder-reconstruction accuracy, we search through all possible alignments of cluster indices I , to folder indices I f in order to find the alignment result-ing in optimal accuracy, then report the accuracy under this optimal alignment:
The normalized mutual information measurement is de-fined as Eq. 5, where I ( S ; F )is the mutual information be-tween cluster assignment S and folder labels F, H ( S )is the entropy of S and H ( F )is the entropy of F. It measures the shared information between S and F.

These two measurements are correlated but show differ-ent aspects of clustering performance. Accuracy calculates
The pre-processing for words includes stemming, stop word removal and removal of words that appear only once in the dataset. The pre-processing for people contains reference-reconciliation over email senders and recipients, and removal of people that are involved in only one email. the ratio between major chunks of clusters to its reference. NMI measures the similarity between cluster partitions and reference partitions.
To experimentally study the SpeClustering model and al-gorithms, we consider three distinct algorithms. First, we consider the standard multinomial naive Bayes text clus-tering[11] algorithm as a baseline approach representing a typical probabilistic approach to text clustering. We mod-ified this baseline approach by allowing it to search for a good cluster initialization and to avoid situations in which one cluster gets eliminated during the EM iterations[7]. Two versions of SpeClustering algorithm are tested. The fist ver-sion is the original SpeClustering algorithm as described in Section 2. The second version, SpeClustering-bound , adds range constraints on parameter values  X  : for word features, the range is [0 . 1 , 0 . 4] and for person features, the range is [0 . 6 , 0 . 9]. The reason for introducing these range constraints is to avoid situations where some values of  X  converge to 1 or 0. This is undesirable because the value of  X  reflects the percentage of specific features ( X = 1 )occuring over all observations. Both SpeClustering algorithms are initialized using the output from the baseline naive Bayes clustering. First we compared our SpeClustering approach to the Naive Bayes baseline in fully autonomous clustering with-out user feedback. We made 50 individual runs on EmailYH dataset and 20 runs each on News-Similar-3 , News-Related-3 ,and News-Different-3 . Table 1 shows the average accuracy and NMI results of different datasets and the three cluster-ing algorithms. Notice in all datasets, the SpeClustering algorithm performs better than the naive Bayes algorithm, and the SpeClustering-bound model performs better than SpeClustering. The naive Bayes clustering results are used to initialize its associated SpeClustering and SpeClustering-bound runs, so the performance gain are directly due to the difference between the SpeClustering probabilistic model and naive Bayes model. When we examined the details of individual runs, we found that every one of the runs re-sulted in SpeClustering-bound outperforming Naive Bayes in terms of the NMI measure, and that in the vast majority of these runs it also outperformed Naive Bayes in terms of the accuracy measure. We next studied the impact of user feedback on the bounded SpeClustering model. In particular, we chose 5 clustering results using the multinomial naive Bayes model with the best log-likelihood among 50 runs on EmailYH and pre-sented each of these to the user. We also chose one best run from 20 runs on News-Different-3 , News-Related-3 ,and News-Similar-3 . The user gave feedback using the interface shown in Fig 5. The top left panel shows a list of docu-ments that are clustered into the selected cluster label, the top right panel shows 5 key-persons of the cluster and the bottom right panel shows 20 keywords of the cluster. The keywords and key-persons of the cluster are selected using a Chi-squared measurement [14]. When a user clicks on a document in the document list, the content of the document shows in the bottom left panel. The user can give various types of feedback described in Section 3 and the interface dateset method Accuracy (%) NMI (%) Email-naive Bayes 48 . 44  X  7 . 01 48 . 02  X  3 . 93 YH SpeCluster 52 . 28  X  8 . 61 53 . 25  X  5 . 65 News-naive Bayes 46 . 31  X  7 . 21 9 . 86  X  7 . 34 Sim-3 SpeCluster 51 . 38  X  6 . 33 15 . 80  X  6 . 82 News-naive Bayes 60 . 18  X  10 . 64 34 . 36  X  10 . 58 Rel-3 SpeCluster 60 . 61  X  11 . 08 36 . 06  X  10 . 71 News-naive Bayes 91 . 24  X  13 . 45 79 . 76  X  14 . 56 Diff-3 SpeCluster 93 . 80  X  11 . 49 83 . 57  X  14 . 27 Table 1: Clustering results of different datasets and displays feedback the user has entered so far. The user can also go back and forth to correct conflict assumptions she has made to achieve consistent cluster interpretations.
An interesting observation we found is that displaying key-words and key-persons tremendously helps the user make judgements about a cluster. In fact, to decide the meaning of a large cluster based only on examining the documents is extremely difficult. A reviewer would tend to decide based on the first several documents she goes through even when the cluster contains more than hundreds of documents, and the biased decision often causes conflicts with later clusters. The reviewer usually chooses to remove a cluster, if the key-words and key-persons don X  X  show any consistency and are not meaningful to the user, or if documents in the cluster are a hodgepodge from several categories. If the keywords or key-persons make sense to the user, the user gives feed-back about document-cluster associations according to these meanings. We don X  X  put constraints on how the reviewer does the feedback, so the reviewer can make decisions freely based on how she perceives the clustering results, and gives feedback using her own interpretation of the results.
This way of collecting feedback may result in a situation where the meaning in the reviewer X  X  mind doesn X  X  match the majority of documents associated with the cluster because the reviewer rationalizes clusters mostly according to key features. Keyword selection favors words that occur in the cluster and don X  X  appear in other clusters, so if a category contains many documents and gets spread out to several clusters, even the majority of documents in the cluster be-long to that category, the keyword selection may give low scores to words belong to that category because those words appear in other clusters.

We use the following notation to indicate various feedback types: run doc # CR PP WX HX Email1 623 3 99 37 30 Email2 623 3 73 35 31 Email3 623 4 92 48 26 Email4 623 7 32 28 15 Email5 623 4 91 43 28 Sim1 3000 2 39 9 -Rel1 3000 1 29 20 -Diff1 3000 0 16 39 -Table 2: Entry numbers of different feedback types for Figure 6: Performance of using single feedback types
Table 2 shows how many entries of different feedback types the reviewer enters for each selected run. The re-viewer spends about 15 mins to finish one run from Emai-lYH dataset and 5-10 mins to finish one run from newsgroup datasets.

We ran the SpeClustering-bound algorithm with user feed-back and compared the results to the naive Bayes base-line and the SpeClustering-bound algorithm without feed-back. The difference between SpeClustering with and with-out feedback is the parameter adjustment described in Sec-tion 3.

We used the simple initialization method on EmailYH dataset in order to break down feedback to single types. Figure 6 shows the results using just one type of feed-back on 5 selected runs from EmailYH dataset. The CR feedback is independent from other types of feedback and all other types involve feedback only from clusters that are not removed. All 5 runs with CR or PP feedback, 4 runs with WX feedback and 3 runs with HX feedback outperform both naive Bayes baseline and SpeClustering-bound with-out feedback. Figure 7 shows the results using combina-tion of feedback types. User X  X  feedback gives huge improve-ments in all runs (19.55% average accuracy improvements from naive Bayes results to SpeClustering-bound with full feedback). SpeClustering-bound with full feedback performs best in 4 out of 5 runs. In the remaining one run, CR+PP feedback performs best. The quantity of PP feedback is about 1/7th to 1/9th to the whole dataset and even higher if we exclude documents in removed clusters. The number of WX+HX feedback are fewer than PP feedback in these runs. However, CR+WX+HX performs better than CR+PP in 2 runs, which shows that meanings of clusters gives compa-rable information like document-cluster association. More compellingly, it is also much easier to get CR+WX+HX feedback than CR+PP in terms of time efficiency. In [12], they measure the time spend on labeling a document or a feature, and they find a person only need 1/5th of time to label a feature compared to the time to label a document.
For the 3 newsgroup datasets, the ratio of the amount of feedback to the corpus size is very small. In this case, the inheritance of old results which is noisy in the simple initialization overwhelms the training process so we used the joint initialization method to remedy the problem.
The user feedback is quite different across these three runs. For the selected run of News-Similar-3 , the naive Bayes results are extremely noisy and the cluster summa-rization is hardly recognizable by the reviewer. It turns out the feedback contains the removal of two out of three clusters and the reason that one is kept is because some keywords weakly indicates the meaning of one newsgroup, but the documents in the remaining cluster contain huge chunks from each newsgroup. For the selected run of News-Related-3 , talk.politics.guns and talk.politics.mideast are re-ferred to two remaining clusters while talk.politics.misc has no reference due to the removal of the last cluster, which the reviewer cannot figure out its meaning. The cluster summarization is noisy but comprehensible, so the reviewer can make positive and negative feedback easily. For News-Different-3 , the baseline accuracy is very high so most feed-back is positive about the automatically generated summa-rization.

Figure 8 shows experimental results from user feedback on one selected run from each newsgroup dataset. It is dif-ficult to improve on the already accurate News-Different-3 run. Incorporating feedback gives no significant improve-ment on the selected News-Similar-3 runs whose feedback is based on extremely noisy clusters and a user is barely able to associate meaningful criterion to any cluster. However, one sees huge improvement from using feedback on the noisy but still meaningful cluster results. The accuracy of the se-lected News-Related-3 run jumps from 63.23% to 81.07%.
In this paper, we focus on the problem of how to cluster text documents based on the meanings of categories a user understands or wants. Often the meanings of clusters be-come clear to a user only after examining their descriptions Figure 7: Performance of using combination of feed-and providing feedback to explore the space of possible clus-ters.
 Our solution to this problem involves three components. First, we propose a new SpeClustering model that separates the features of a document that are specific to a cluster from other general features that are unrelated to the cluster X  X  se-mantics. The second component is a method to collect user feedback about the meanings of the clusters. We present an interface that enables a user to browse through cluster results and provide several types of feedback. The process requires the user X  X  understanding of desired categories, and her judgement about which cluster is associated with the meaning of which category. The third component is an al-gorithm for integrating user feedback with the SpeClustering model. The structure of the SpeClustering model provides a natural way to adjust parameters according to a variety of types of user feedback.

Our experimental results show our unsupervised SpeClus-tering algorithm outperforms the commonly used multino-mial naive Bayes clustering algorithm for both of the text datasets we considered. Furthermore, when provided with user feedback, the SpeClustering model gains significant im-provement in a personal email dataset and in the newsgroup dataset when the clustering results is noisy but meaningful. Our approach combines the advantage of the machine X  X  com-putational power to analyze huge amount of data, with the advantages of a human X  X  understanding of categories of in-terest. The results indicate that cooperation between com-puters and human beings is a promising direction for future work. There are many future challenges, such as using active learning principles to optimize the summarization of a clus-ter, and building more sophisticated models to allow more natural types of user feedback.

Acknowledgments We thank Sophie Wang for useful discussions and contributions to earlier versions of the clus-tering email algorithm. This work was supported in part by Darpa contract NBCD030010, as part of the Personal Assistants that Learn project.
 Figure 8: Experiments results of SpeClustering with
