
This paper provides a blueprint for constructing collabo-rative and distributed knowledge discovery systems within 
Grid-based computing environments. The need for such sys-tems is driven by the quest for sharing knowledge, informa-tion and computing resources within the boundaries of sin-gle large distributed organisations or within complex Virtual 
Organisations (VO) created to tackle specific projects. The proposed architecture is built on top of a resource federation management layer and is composed of a set of different re-sources. We show how this architecture will behave during a typical KDD process design and deployment, how it enables the execution of complex and distributed data mining tasks with high performance and how it provides a community of e-scientists with means to collaborate, retrieve and reuse both KDD algorithms, discovery processes and knowledge in a visual analytical environment. 
This paper proposes an architecture to support the KDD process in a Grid-enabled distributed computing environ-ment. The approach is generic but originates from the needs of the knowledge discovery processes in the bioinformatics industry, where complicated data analysis processes are con-structed using a data-pipelined approach. At different stages of the discovery pipeline researchers need to access, integrate and analyse data from disparate sources, in order to use that data to find patterns and models, and feed these models to further stages in the pipeline. At each stage, new analysis is conducted by dynamically combining new data with previ-ously developed models. 
As a motivating example, consider an automated labora-tory experiment, where a range of sensors produces large volumes of data about the activity of genes in cancerous cells. A short time series is produced that records how each permission an~or a fee. 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. gene responds to the introduction of a possible drug. The ini-tial requirement of the analysis is to filter interesting time series from uninteresting ones; one approach is to use clus-tering [5]. If a group of interesting genes is found then a crucial step in the scientific discovery process is to verify ff the clusters can be explained by referring to existing bi-ological knowledge. Bioinformatics researchers have made available a significant amount of information on the Internet about various biological items and processes (Genes, Pro-teins, Metabolism and Regulation). These semi-structured resources can be accessed, from remote online databases over the Internet, through a range of search mechanisms, includ-ing a key based lookup to biosequence similarity searches. The need to integrate this information within the discovery process is inevitable since it dictates how the discovery may proceed. Furthermore, recording an audit trail of how this was acquired and used is essential since it allows researchers to document and manage their discovery procedures, re-use the same procedure in similar scenarios, and in many cases it may help them in managing intellectual property activities such as patent applications, peer reviews and publications. 
Another feature of such discovery pipelines is that the analysis components used can be tied to remote computing resources, e.g. similarity searches over DNA sequences exe-cuting on a shared high performance machine. New services and tools for executing similar operations are continually being made available over the Internet for access by various researchers. Also, the discovery process itself is almost al-ways conducted by teams of collaborating researchers who need to share the data sets, the results derived from these data sets and, more importantly, details about how these results were derived. 
This data-pipelined approach is gaining grounds beyond life sciences, where similar needs arise for cross-referencing patterns discovered in a dataset, with patterns and data stored in remote databases, and for using shared high per-formance resources. Examples abound in the analysis of het-erogeneous data in fields such as geological analysis, environ-mental sciences, astronomy, and particle physics. Support-ing the data-pipelined knowledge discovery process requires KDD tools that flexibly operate in an open system allowing:  X  Dynamic integration of new servers, new databases and new algorithms within in the KDD process. 
The above requirements can be contrasted to the ser-vices offered by existing tools that mainly focus on extract-ing knowledge within closed systems such as a centralised database or a data warehouse where all the data required for an analysis task can be materialised locally at anytinm, before feeding them to data mining algorithms and tools that were predefined at the configuration stage of the tool. 
Recently [12], the Grid, a novel IT infrastructure, has been proposed to provide a well-defined resource management in-frastructure for virtual organisations (VO) and allow end users to share both information and computing resources in secure environments. Grid concepts and tools offer a flexible but secure computing infrastructure that meets some of the requirements of the data-pipelined knowledge discovery pro-cess. However, a gap still exists between the services offered by Grid-based methodologies and the requirements of the KDD process. 
This paper proposes a service layer and architecture that aims to make best use of existing KDD practice [1] and tools [2, 8] as well as making use of existing Grid infrastructure and concepts.so as to bridge the gap between the traditional KDD process (from its definition to its deployment as an application for knowledge discovery) and its mapping onto the VO's resources. 
Increased research demands in fields such as high energy particle physics, astronomy and environmental modelling led to the exploitation of fast networks and large-scale dis-tributed computing techniques. This at first took place ex-clusively within the scientific community (Seti project [19]) where individual computers were given processing tasks in their 'spare time' with the results being assembled in one centralised location. Very soon this idea of a global comput-ing platform was dubbed 'The Grid', and numerous research groups (many of which are now in the Global Grid Forum ]15]) started devoting time and effort to developing archi-tectures to support this new infrastructure. 
Initial Grid work was directed purely towards 'processor-stealing' algorithms, concentrating on hardware resources that could be shared, like CPU, memory and bandwidth, but this soon changed to encompass a far broader, softer, class of resources, like programs, data sources, knowledge repositories etc. In their seminal paper [12} Foster, Kessel-man &amp; Tuecke state that the actual problem that the Grid is trying to solve is 'coordinated resource sharing and prob-lem solving in dynamic, multi-institutional virtual organisa-tions', where virtual organisations are considered to be any formal or informal communities that are sharing a certain set of resources under some well-defined rules. Furthermore, they define the layers of a Grid architecture with lower levels providing middlewaxe support for higher level application-specific services, thereby opening the door to the develop-ment and porting of more ambitious systems, like Knowledge Discovery or bioinformatics platforms [16], onto the Grid. 
At the present stage, most of the current Grid middleware products suffer from lack of interoperability and an insuffi-cient focus on the software dimension of the problem. Even The Globus Toolkit 2110], widely acknowledged to be the most sophisticated middleware available, does not run on 
Windows, so we are still far from being able to completely abstract over any middleware platform in a commercially viable system. Still, there are some running grids that can be useful sources for further information, such as European 
Data Grid [6], EuroGrid [7], NASA Information Power Grid [17] and a number of university projects. Fortunately, the next version of Globus (GT3) will include an Open Grid Services Architecture (OGSA) [11] imple-mentation that combines the Grid work performed with the broad experience of Web service technologies [21] to provide an industry-usable platform. Using WSDL [22] and UDDI [20] it effectively provides the service-based IT infrastruc-ture level that can be used in constructing and providing application layer of Knowledge Discovery and Data Mining services. Regardless of the implementation details, it is our belief that the next generation of data mining tools will be run-ning on the Grid, therefore the KDD community should participate in the project and influence the work so as to ensure that the resulting environment will be well-suited to its needs as well. The aim of this paper is to propose a way forward and provide the first step towards the next genera-tion of data mining architectures, Grid enabled and capable of meeting the challenges posed by the evolved distributed computing landscape. In the context of this paper, we use the term Knowledge 
Discovery Service to describe the building blocks used in a data-pipelined KDD process. As with the traditional KDD process[91 the Grid-based version spans all activities from data collection to modelling and deployment. This definition thus encapsulates KDD algorithms as well as components that extract data from a database. Regarding these compo-nents as services is essential since it allows us to separate their definition from their implementation. In this paper, we also use the term Knowledge to be any structure that a 
Knowledge Discovery Service needs as input or generates as output. We classify knowledge discovery services into two cate-gories: Computation services and Data services. A computa-tion service allows users to define and compose their analysis processes by assembling together data preparation and data mining algorithms. In contrast, a data service allows users to define their analysis data set as a composition of rela-tional tables queried from databases and other data sources available from the VO which can augment the training set with additional information. A computation service can be seen as a classical KDD al-gorithm. Its input and output can be any number of Knowl-edge objects. Each of its inputs can be linked one or more other service's output allowing the definition of distributed and parallel data mining processes. Since a computation ser-vice has an implementation, it can also have resources con-straints such as:  X  Location: A service might be bound to a particular re-source as either its implementation is platform-specific 3.1.2 Data Services 
In contrast to computation services, which enable the com-position of functions as discovery processes, data services do not provide implementations, instead they provide meta-information for composing data sets from heterogeneous and distributed data sources. 
Data services are used to model data for analysis. In a large VO the required data is produced from different de-vices and is retrieved through different protocols (RDBMS, XML on Web servers, etc...) in different locations. The infor-mation required for a specific task is a composition of these sources, in effect an inner-join operation over heterogenous and distributed data. A data service describes its functional-ity using metadata descriptors for the source of information it will attempt to use in order to materialise the data set. 
The metadata also describes the form of the data in terms of the different features and their data types. Features from different data services can be extracted and composed in a similar way to computation services to create a Knowledge Schema which is effectively a composition of data services. This schema can then be incorporated at any point of the discovery process. 
In order to de-couple resources from service definitions and service implementations, we define four types of services that are applicable to both computation and data services: 
Any non-triviai discovery process requires the composition of existing services to create discovery processes by combin-ing different services into larger, more complicated, ones. In a Grid-based environment, this may involve the augmenta-tion of a training data set using a data service, accessing external data sources that are driven by the knowledge ex-tracted by the computation service, etc. In this case, data services can be composed to create a view of the data that is used for analysis and to compose computational services to develop an analysis pipeline. Composition may be specified by the end-nser through a GUI for component composition that provides access to a li-brary of services including data importation, data cleaning, data normalisation, mining and statistical algorithms, data selection and data exportation. Internally, however, allow-ing such composition requires the use of a service definition language. plementations; e.g. specialised implementations for specific platforms. Each service must thus be catalogued and cate-gorised, afiowing the user to browse the available services, or even locate and retrieve them through queries. This requires each discovery process to be well-defined through a descrip-tor registered with a specific server. This registration mech-anism allows new services to be added to system through service descriptors including: Input and Output types; Pa-rameters, Constraints (range of validity, discrete set of possi-ble values, optional or required, ..); Registration information and Service type. executed may have a strong impact on the overall perfor-mance of the process execution. When dealing with very large data sets, it is more efficient to keep as much of the computation as near to the data as possible. Therefore a location resolution method must be provided to take that particular constraint into account. This service / location resolution method can be achieved as follows: vices, and therefore we will assume throughout that this lower layer of services, dealing with security and communi-cation issues, is robust and stable. However, this does not mean that the data available on one of the grid's computa-tional node can be moved to another node since the data set itself can have restricted permissions. Neither does it mean that the connection is necessarily fast enough to favour mov-ing data to another machine instead of mining it locally. 
Figure 1 shows the different components of the Discov-ery Net architecture. In the shown diagram, the user uses client tools to construct and define his discovery procedures. 
Knowledge Servers are knowledge bases which allow one user to store / retrieve or publish knowledge. The Resource Dis-covery Server (Resource Service Lookup Server) is a knowl-edge base of published service definitions. It also performs the role of resource resolution server, as resources are never requested directly from a client but only through a request to resolve the location of a particular service (if this ser-vice is not bound to a particular location or is a template service). The Discovery Meta-information Server is used to store information about each type of knowledge. 
A standard means is required to allows users to specify the required tasks This representation should be exteusible with respect to the discovery services it can specify, as well as not being bound to specific resources as these will be resolved by the lookup server. To meet this need we devel-oped an XML-based language called called Discovery Pro-cess Markup Language (DPML). 
DPML represents a process as a data flow graph of nodes, each representing a service. DPML is typically authored us-ing a visual programming interface. Each node descriptor contains three parts: service parameters (identifier and user set parameters) history (record of past parameter settings) and notes (user comments about operation). 
For the purpose of executing tasks only the first section of this descriptor is necessary. However, our implemented client software allows users to annotate their process descriptions and automatically record how parameters are set to create a comprehensive representation of the discovery process. 
The semantics of the service descriptor are defined by the owner of the namespace specified for the service parameters. 
This allows for an unambiguous description of what a service provides and for multiple equivalent implementations to be mapped to it using the lookup service. An example node descriptor is shown in Figure 2 for a simple pre-processing operation that deletes columns from its table input. 
The resource discovery server (aka lookup server) is a cen-tral part of the Discovery Net architecture as it allows the services to register their descriptors, the other components of the architecture to emphbrowse/ retrieve these descrip-&lt;/notes&gt; 
Figure 2: DPML Representation of a computation service tors, the resolution of the location of services to be deployed and mapped on computational resources for execution. 
As described in section 3.1.3, some services might not be bound to a particular resource, therefore any resource willing to participate and provide computational power must addi-tionally register with the Resource Discovery Server which is then responsible for resolving the service location if needed. 
An overview of the location resolution method was presented in Section 3.4. 
The Knowledge Server stores and provides access to dis-covery processes performed within the Discovery Net. It warehouses the VO's process information (past experience) and allows this knowledge to be re-used. Its three main func-tions are: 
Storage service The server acts as a repository for the VO 
Reporting service The server can provide the knowledge 
Application generation service Users are able to easily 4.4.1 Process repository 
By archiving all the processes stored within the VO the knowledge servers contain a structured representation of the activities and knowledge of the VO. We contrast with the 
ERP system SAP [18] where a range of static business pro-cesses are already implemented. The activity of discovery, by its very nature, does not have a set of universal pre-existing processes for finding knowledge: it is the role of the VO to create these. The VO's knowledge servers provide a means for creating such a resource dynamically. 
The DPML format allows additional information to be stored about how a discovery process has been authored (a complete change history), the motivation and justification for why operations were performed, as well as the tagging of steps to indicate that a process model or methodology was being followed. Since we store information in an XML based format we are able to transform our process data into human-readable reports on demand. These reports are pro-vided over the web and can include applets and other inter-active elements to allow users to explore discoveries. Addi-tionally, this store of structured information enables meth-ods to find useful knowledge about how an organisation con-ducts discoveries, recta-mining the repository for insight into how people make discoveries. 
Once users have constructed successful processes they wish to be re-used they can publish them as new services. This involves two steps. 
First, the parameters of the process that they wish to be modified at runtime need to be defined. This is analogous to declaring properties for the deployed application 'compo-nent '. 
Second, the new application service needs to be regis-tered with the lookup server. 
Once registered this new process can be used as a service in its own right or as a part of a more complex process. 
To enable access for users who have no particular client software (eg bench scientists) these published procedures can be accessed via the web using a familiar forms based interface. 
As an additional feature, each deployed DPML procedure can be wrapped within a WSDL (Web Services Description Language) component to make use of the existing web ser-vice technologies and enable deployment over Grid-based OGSA services. This is achieved by providing the WSDL descriptor for each component. 
The Meta-Information(MIS) server is responsible for the management of data types used by services in the Discov-ery Net. Data types are represented as different type defi-nitions for operations in the system. For example, an ana-lytical component can define the output of its operation as a PMML[3] model. Similarly data sources available in the system can register database metadata to the MIS so that the data service can be type-checked, composed and finally queried. The main roles of the Meta-Information Server are: Storage service Type definitions are stored when a new Type checking To verify that a composed data analysis 
Data composition To allow complex data views to be ob-implementation and content. The highest level of structure is the Relational Database where tables of known types are or-ganised uniformly. Semi-structured databases include collec-tions of documents written in a mark-up language or other pre-defined format. Unstructured databases include collec-tions of Free text or image files. A mixture of these different types of data services is required in some scientific processes. guage to model type definitions. The semi-structured nature of XML allows the easy modelling of tabular data, and also has the benefit of being able to model highly nested and un-structured databases. The XML schema language also pro-vides the necessary typing mechanisms required for accurate recta-data provision. natural XML format is performed by wrappers. A database wrapper is a transformation layer that operates between the databases and the Discovery Net. The wrapper transforms the data from its natural form into the XML representation that was described by the schema representing the database. a specific database. When browsing the Meta-Information 
Server, the client's view is of a single, large, virtual ware-house of data. This contains a collection of data sources that are each described by their individual Type Definitions. A data analysis task is unlikely to require all of the data avail-able within the Discovery Net, so the role of the Knowledge 
Schema is to represent a selection of features from different information sources. vidual features from the Type Definition and using them as end user information, eg. a description of a biological function, or for cross-referencing them with another type definition from another database. Once the complex schema required for the specific task has been built and the user has specified the information required for the interpretation of the analysis task, a personal warehouse has been described. to the computation grid services that perform information retrieval tasks. The computation processes that are involved in the rea~sation include Federated Database approaches such as Discovery Link[4] from IBM or the functional signa-ture approach seen in geneticXchange[14]. architecture that provides a recta-level interface to data sources within the Grid infrastructure, and described how it can be used to fit traditional KDD processes into this new environment. Our approach is based on the service model concept that allows any service-based architecture of the emerging Grid technologies (e.g. Jini/Globus/OGSA) to be used. The Knowledge Discovery Services presented here are 
