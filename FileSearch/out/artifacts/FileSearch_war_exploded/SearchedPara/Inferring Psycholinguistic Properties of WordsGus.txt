 Throughout the last three decades, much has been found on how the psycholinguistic properties of words influence cognitive processes in the human brain when a subject is presented with either writ-ten or spoken forms. A word X  X  Age of Acquisition is an example. The findings in (Carroll and White, 1973) reveal that objects whose names are learned earlier in life can be named faster in later stages of life. Zevin and Seidenberg (2002) show that words learned in early ages are orthographically or phono-logically very distinct from those learned in adult life.

Other examples of psycholinguistic properties, such as Familiarity and Concreteness, influence one X  X  proficiency in word recognition and text com-prehension. The experiments in (Connine et al., 1990; Morrel-Samuels and Krauss, 1992) show that words with high Familiarity yield lower reaction times in both visual and auditory lexical decision, and require less hand gesticulation in order to be de-scribed. Begg and Paivio (1969) found that humans are less sensitive to changes in wording made to sen-tences with high Concreteness words.

When quantified, these aspects can be used as features for various Natural Language Processing (NLP) tasks. The Lexical Simplification approach in (Jauhar and Specia, 2012) is an example. By combining various collocational features and psy-cholinguistic measures extracted from the MRC Psycholinguistic Database (Coltheart, 1981), they trained a ranker (Joachims, 2002) that reached first place in the English Lexical Simplification task at SemEval 2012 . Semantic Classification tasks have also benefited from the use of such features: by combining Concreteness with other features, (Hill and Korhonen, 2014) reached the state-of-the-art performance in Semantic Composition ( denota-tive / connotative ) and Semantic Modification ( inter-sective / subsective ) prediction.

Despite the evident usefulness of psycholinguis-tic properties of words, resources describing such properties are rare. The most extensively developed resource for English is the MRC Psycholinguistic Database (Section 2). However, it is far from com-plete, most likely due to the inherent cost of manu-ally entering such properties. In this paper we pro-pose a method to automatically infer these missing properties. We train regressors by performing boot-strapping (Yarowsky, 1995) over the existing fea-tures in the MRC database, exploiting word em-bedding models and other linguistic resources for that (Section 3). This approach outperform various strong baselines (Section 4) and the resulting prop-erties lead to significant improvements when used in Lexical Simplification models (Section 5). Introduced by Coltheart (1981), the MRC (Machine Readable Dictionary) Psycholinguistic Database is a digital compilation of lexical, morphological and psycholinguistic properties for 150,837 words. The 27 psycholinguistic properties in the resource range from simple frequency measures (Rudell, 1993) to elaborate measures estimated by humans, such as Age of Acquisition and Imagery (Gilhooly and Lo-gie, 1980). However, despite various efforts to pop-ulate the MRC Database, these properties are only available for small subsets of the 150,837 words.
We focus on four manually estimated psycholin-guistic properties in the MRC Database:  X  Familiarity : The frequency with which a word is seen, heard or used daily. Available for 9,392 words.  X  Age of Acquisition : The age at which a word is believed to be learned. Available for 3,503 words.  X  Concreteness : How  X  X alpable X  the object the word refers to is. Available for 8 , 228 words.  X  Imagery : The intensity with which a word arouses images. Available for 9,240 words.

All four properties are real values, determined based on different quantifiable metrics. We focus on these properties since they have been proven use-ful and are some of the most scarce in the MRC Database. As we discussed in Section 1, these prop-erties have been successfully used in various ap-proaches for Lexical Simplification and Semantic Classification, and yet are available for no more than 6% of the words in the MRC Database. In order to automatically estimate missing psy-cholinguistic properties in the MRC Database, we resort to bootstrapping. We base our approach on that by (Yarowsky, 1995), a bootstrapping algorithm which aims to learn a classifier over a reduced set of annotated training instances (or  X  X eeds X ). It does so by performing the following five steps: 1. Initialise training set S with the seeds available. 2. Train a classifier over S . 3. Predict values for a set of unlabelled instances U . 4. Add to S all instances from U for which the pre-diction confidence c is equal or greater than  X  . 5. If at least one instance was added to S , go to step 2 , otherwise, return the resulting classifier.
One critical difference between this approach and ours is that our task requires regression algorithms instead of classifiers. In classification, the predic-tion confidence c is often calculated as the maxi-mum signed distance between an instance and the estimated hyperplanes. There is, however, no analo-gous confidence estimation technique for regression problems. We address this problem by using word embedding models.

Embedding models have been proved effective in capturing linguistic regularities of words (Mikolov et al., 2013b). In order to exploit these regularities, we assume that the quality of a regressor X  X  prediction on an instance is directly proportional to how similar the instance is to the ones in the labelled set. Since the input for the regressors are words, we compute the similarity between a test word and the words in the labelled dataset as the maximum cosine similar-ity between the test word X  X  vector and the vectors in the labelled set.

Let M be an embeddings model trained over vo-cabulary V , S a set of training seeds,  X  a minimum confidence threshold, sim ( w,S,M ) the maximum cosine similarity between word w and S with respect to model M , R a regression model, and R ( w ) its prediction for word w . Our bootstrapping algorithm is depicted in Algorithm 1.

Algorithm 1 : Regression Bootstrapping input: M , V , S ,  X  ; output: R ; repeat until k S k converges ;
We found that 64,895 out of the 150,837 words in the MRC database were not present in either Word-Net or our word embedding models. Since our boot-strappers use features extracted from both these re-sources, we were only able to predict the Familiarity, Age of Acquisition, Concreteness and Imagery val-ues of the remaining 85,942 words in MRC. Since we were not able to find previous work for this task, in these experiments, we compare the perfor-mance of our bootstrapping strategy to various base-lines. For training, we use the Ridge regression algo-rithm (Tikhonov, 1963). As features, our regressor uses the word X  X  raw embedding values, along with the following 15 lexical features:  X  Word X  X  length and number of syllables, as deter-mined by the Morph Adorner module of LEXen-stein (Paetzold and Specia, 2015).  X  Word X  X  frequency in the Brown (Francis and
Kucera, 1979), SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016),
Wikipedia and Simple Wikipedia (Kauchak, 2013) corpora.  X  Number of senses, synonyms, hypernyms and hy-ponyms for word in WordNet (Fellbaum, 1998).  X  Minimum, maximum and average distance be-tween the word X  X  senses in WordNet and the the-saurus X  root sense.  X  Number of images found for word in the Getty
We train our embedding models using word2vec (Mikolov et al., 2013a) over a corpus of 7 billion words composed by the SubIMDB corpus, UMBC and New, 2009), Wikipedia and Simple Wikipedia (Kauchak, 2013). We use 5 -fold cross-validation to optimise parameters:  X  , embeddings model architec-ture (CBOW or Skip-Gram), and word vector size (from 300 to 2,500 in intervals of 200). We include four strong baseline systems in the comparison:  X  Max. Similarity : Test word is assigned the prop-erty value of the closest word in the training set, i.e. the word with the highest cosine similarity according to the word embeddings model.  X  Avg. Similarity : Test word is assigned the aver-age property value of the n closest words in the training set, i.e. the words with the highest co-sine similarity according to the word embeddings model. The value of n is decided through 5 -fold cross validation.  X  Simple SVM : Test word is assigned the prop-erty value as predicted by an SVM regressor (Smola and Vapnik, 1997) with a polynomial ker-nel trained with the 15 aforementioned lexical features.  X  Simple Ridge : Test word is assigned the property value as predicted by a Ridge regressor trained with the 15 aforementioned lexical features.  X  Super Ridge : Identical to Simple Ridge, the only difference being that it also includes the words embeddings in the feature set. We note that this baseline uses the exact same features and regres-sion algorithm as our bootstrapped regressors.
The parameters of all baseline systems are opti-mised following the same method as with our ap-proach. We also measure the correlation between each of the aforementioned lexical features and the psycholinguistic properties. For each psycholinguis-tic property, we create a training and a test set by splitting the labelled instances available in the MRC Database in two equally sized portions. All train-ing instances are used as seeds in our approach. As evaluation metrics, we use Spearman X  X  (  X  ) and Pear-son X  X  ( r ) correlation. Pearson X  X  correlation is the most important indicator of performance: an effec-tive regressor would predict values that change lin-early with a given psycholinguistic property.
The results are illustrated in Table 1. While the similarity-based approaches tend to perform well for Concreteness and Imagery, typical regressors cap-ture Familiarity and Age of Acquisition more effec-tively. Our approach, on the other hand, is con-sistently superior for all psycholinguistic proper-ties, with both Spearman X  X  and Pearson X  X  correlation scores varying between 0 . 82 and 0 . 88 . The differ-ence in performance between the Super Ridge base-line and our approach confirm that our bootstrapping algorithm can in fact improve on the performance of a regressor.

The parameters used by our bootstrappers, which are reported below, highlight the importance of pa-rameter optimization in out bootstrapping strategy: its performance peaked with very different configu-rations for most psycholinguistic properties:  X  Familiarity : 300 word vector dimensions with a
Skip-Gram model, and  X  =0 . 9 .  X  Age of Acquisition : 700 word vector dimensions with a CBOW model, and  X  =0 . 7 .  X  Concreteness : 1,100 word vector dimensions with a Skip-Gram model, and  X  =0 . 7 .  X  Imagery : 1,100 word vector dimensions with a Skip-Gram model, and  X  =0 . 7 .
 composed of over 7 million sentences extracted from subtitles of  X  X amily X  movies and series, has good lin-ear correlation with Familiarity and Age of Acquisi-tion, much higher than any other feature. For Con-creteness and Imagery, on the other hand, the results suggest something different: the further a word is from the root of a thesaurus, the most likely it is to refer to a physical object or entity. Here we assess the effectiveness of our bootstrap-pers in the task of Lexical Simplification (LS). As shown in (Jauhar and Specia, 2012), psycholinguis-tic features can help supervised ranking algorithms capture word simplicity. Using the parameters de-scribed in Section 4, we train bootstrappers for these two properties using all instances in the MRC Database as seeds. We then train three rankers with (W) and without (W/O) psycholinguistic features:  X  Horn (Horn et al., 2014): Uses an SVM ranker trained on various n-gram probability features.  X  Glavas (Glava  X  s and  X  Stajner, 2015): Ranks can-didates using various collocational and semantic metrics, and then re-ranks them according to their average rankings.  X  Paetzold (Paetzold and Specia, 2015): Ranks words according to their distance to a decision boundary learned from a classification setup in-ferred from ranking examples. Uses n-gram fre-quencies as features.

We use data from the English Lexical Simplifica-tion task of SemEval 2012 to assess systems X  per-formance. The goal of the task is to rank words in different contexts according to their simplicity. The training and test sets contain 300 and 1,710 in-stances, respectively. The official metric from the task  X  TRank (Specia et al., 2012)  X  is used to mea-sure systems X  performance. As discussed in (Paet-zold, 2015), this metric best represents LS perfor-mance in practice. The results in Table 2 show that the addition of our features lead to performance in-creases with all rankers. Performing F-tests over the rankings estimated for the simplest candidate in each instance, we have found these differences to be sta-tistically significant ( p &lt; 0 . 05 ). Using our features, the Paetzold ranker reaches the best published re-sults for the dataset, significantly superior to the best system in SemEval (Jauhar and Specia, 2012). Overall, the proposed bootstrapping strategy for re-gression has led to very positive results, despite its simplicity. It is therefore a cheap and reliable alternative to manually producing psycholinguistic properties of words. Word embedding models have proven to be very useful in bootstrapping, both as surrogates for confidence predictors and as regres-sion features. Our findings also indicate the use-fulness of individual features and resources: word frequencies in the SubIMDB corpus have a much stronger correlation with Familiarity and Age of Ac-quisition than previously used corpora, while the depth of a word X  X  in a thesaurus hierarchy correlates well with both its Concreteness and Imagery.
In future work we plan to employ our boot-strapping solution in other regression problems, and to further explore potential uses of automatically learned psycholinguistic features.

The updated version of the MRC resource can be downloaded from http://ghpaetzold.github.io/data/ BootstrappedMRC.zip.

