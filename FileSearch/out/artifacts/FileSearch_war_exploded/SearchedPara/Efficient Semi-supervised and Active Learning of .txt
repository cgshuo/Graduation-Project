 Maria-Florina Balcan ninamf@cc.gatech.edu Christopher Berlind cberlind@gatech.edu Steven Ehrlich sehrlich@cc.gatech.edu Yingyu Liang yliang39@gatech.edu The following lemma bounds the number of connected components in the commonality graph by the number of compatible hypotheses. For notational definitions, refer to Section 2 in the main body of the paper. Lemma 1. Let G be the graph that results from re-moving all non-indicators from G com ( U ) , and sup-pose G is divided into T connected components. If m ity at least 1  X   X  .
 Proof. Since G has no non-indicators, a hypothesis is compatible with U if and only if every component is made entirely of indicators of the same type. There are two possible choices for each component, so the number of fully compatible hypotheses is | C U, X  (0) | = 2 T . To complete the proof, it is sufficient to show that C
U, X  (0)  X  C D, X  ( ). Since any hypothesis in C U, X  (0) is compatible with any example containing variables from only one component, we only need to show that there is at most probability mass of examples that contain variables from multiple components. All such examples correspond to edges that are absent from G com ( U ), so we only need to show that G com ( U ) was constructed with enough examples so that nearly all significant edges appear in the graph.
 To see this, fix any pair of variables x i ,x j . If Pr x  X  D [ x i = 1  X  x j = 1] &lt; /n 2 , we can ignore this pair since all such pairs together constitute a probability mass strictly less than . Now suppose Pr x  X  D [ x i = 1  X  x j = 1]  X  /n 2 . The probabil-ity that x i and x j do not appear together in any of the examples in U is at most (1  X  n 2 ) m u , so if m  X /n 2 . By a union bound over all such pairs, with probability at least 1  X   X  all corresponding edges ap-pear in G com ( U ), and the probability mass of examples containing variables from multiple components is at most . This means that every fully compatible hy-pothesis has unlabeled error at most , so we have T = log 2 | C U, X  (0) | X  log 2 | C D, X  ( ) | . The following theorem formalizes the computational difficulty of finding a fully consistent and compatible two-sided disjunction in the semi-supervised setting. Theorem 4. Given data sets L and U , finding a hy-pothesis h  X  C that is both consistent with L and com-patible with U is NP -hard.
 Proof sketch. The proof is by reduction from 3-SAT . Given a 3-SAT instance  X  on variables x ,...,x n we produce the following data sets L and U containing examples on the 4 n variables x L contains examples of the form ( { x + i ,  X  x + i } , +1) and clause in  X  of the form ( ` i  X  ` j  X  ` k ) where ` i ,` j can each be positive or negative literals, L contains the example ( { ` + i ,` + j ,` + k } , +1). The unlabeled set U contains examples of the form { x + i ,x  X  i } and {  X  x for 1  X  i  X  n . The labelings that are consistent and compatible with all the non-clause examples correspond precisely to assignments of x 1 ,...,x n , and the clauses are compatible with a given hypothesis only if they are satisfied in the underlying assignment. The set of positive indicators of any hypothesis h = ( h + ,h  X  )  X  C that is both consistent with L and compatible with U corresponds to a truth assignment to x 1 ,...,x n that satisfies  X  , therefore finding such a hypothesis is NP -hard.
 Here we consider the problem of learning two-sided disjunctions under random classification noise, where the label of each example is flipped with probability 0  X   X  &lt; 1 / 2 independently. Our goal is to extend our algorithms to this setting so that they still success-fully output a low error hypothesis without significant increase in sample complexity.
 More specifically, we have a distribution D X,Y over labeled examples ( X,` ( X )), and the Bayes decision rule is a two-sided disjunction h  X   X  C , which we also refer to as the target concept. We have  X  ( h  X  ,D ) = 1 where D is the margin of D X,Y over X , and For a hypothesis h , let err D ( h ) denote its error over the distribution D X,Y , i.e. Let err L ( h ) denote its empirical error on the noisy labeled examples L , i.e. For convenience, we define the distance between h and h  X  to be We aim to find a hypothesis h with error Note that it is sufficient to have d ( h,h  X  )  X  , since by triangle inequality err D ( h ) = Pr[ h ( x ) 6 = ` ( x )] In the following two subsections, we show how to extend the algorithms (Algorithm 2 and 3) for semi-supervised and active learning respectively. In the last subsection, we include the extension of Algorithm 1. Note that due to the noise in the labeled examples, we cannot hope to find a consistent and compatible hypothesis. The extension of Algorithm 1 only outputs a hypothesis that has low error. However, it achieves better sample complexity bound than the extension of Algorithm 2. 3.1. Semi-supervised Learning In the noise-free setting, we build a hypothesis based on the commonality graph, and then check and update the hypothesis until it has low error rate. The key idea in the noisy setting is that we label variables (i.e. identify variables as positive/negative potential indicators) by majority labels of sufficiently many examples containing the variables, and we use a sufficiently large set of labeled examples each time we check the hypothesis. A brief description is provided below, while the details are provided in Algorithm 4 and Theorem 5.
 First, we build the commonality graph and the poten-tial indicator sets. We only label a variable if it is use majority label of the examples to correctly decide its type. We call such variables significant . If we draw  X  O ( 1 non-indicator is significant and thus labeled. Then we build a hypothesis and draw a set of labeled examples to check it. If the empirical error is small, we output the hypothesis since it is guaranteed to have small error. Otherwise, either a component without any labeled variables in it or a non-indicator causes large error. In the first case, we need sufficiently many examples fall in the component so that we can use majority voting to decide the type of the variables is the number of connected components in the graph that results from removing all non-indicators from the commonality graph. In the second case, to reveal the non-indicator, we must be able to distinguish between an error rate of  X  (the error rate caused by noise) and  X  +  X ( (1  X  2  X  ) /k ) (the error rate caused by noise and the non-indicator leading to large error). This requires  X  ples at each check, so that either a component that previously contained no labeled variables is labeled, or a non-indicator is revealed. After at most ( k + T ) updates, we are guaranteed to have a hypothesis with small error.
 Theorem 5. For any distribution D X,Y over { 0 , 1 } n  X  { X  1 , 1 } and target concept h  X   X  C in the random clas-sification noise model such that h  X  has at most k non-indicators, and the minimum non-indicator probability is 0 , if m u = O ( n 2 log n  X  ) and Algorithm 4 Learning a Low-error Hypothesis for Two-Sided Disjunctions under Random Classification Noise Input: data sets U and L , parameters ,  X  , k , 0 , T Set G = G com ( U ) ,V 0 + = V 0  X  =  X 
Set L 0 = sample 100 Set size ( v ) = |{ x  X  L 0 : x 3 v }| ,  X  v  X  V for each v  X  SIG do while L 6 =  X  and err S ( h ) &gt;  X  + 1  X  2  X  2 do
Output: the hypothesis h then with probability at least 1  X   X  , Algorithm 4 outputs a hypothesis h in polynomial time such that err D ( h )  X   X  + .
 Proof. Generalization Error: Suppose we check the hypothesis at most k + T times (proved later), where T is the number of connected components in the graph that results from removing all non-indicators from G com ( U ). Then when w.h.p. all hypotheses h with d ( h,h  X  ) &gt; will have empirical error on S larger than  X  + 1  X  2  X  2 . So when the algorithm stops the hypothesis satisfies d ( h,h  X  )  X  and thus err D ( h )  X   X  + .
 Bounding the Number of Updates: We now show that the hypothesis is indeed updated at most ( k + T ) times. We begin by proving that when w.h.p. every non-indicator is labeled and every labeled indicator gets the correct label, so that the hypothesis is updated correctly when its error is large. First, by Cher-noff and union bounds, the probability that there exists a non-indicator that appears in less than 10 (1  X  2  X  ) 2 examples is bounded by k exp { O ( 0 | L 0 | ) } X  O (  X  ). So w.h.p. every non-indicator appears in enough examples and thus is labeled. Second, the type of an indicator is decided by the majority label of O ( 1 (1  X  2  X  ) 2 examples. By Hoeffding and union bounds, w.h.p. the types of all indicators appearing in enough examples are decided correctly.
 We now prove that when the error of the hypothesis is large, we can make progress by either labeling a previously unlabeled component or identifying a non-indicator, and thus it is updated at most ( k + T ) times. When if err S ( h ) &gt;  X  + 1  X  2  X  2 , then w.h.p. d ( h,h  X  each v  X  V +  X  V  X  , let X ( v ) denote those examples whose nearest labeled variable is v , i.e.
 For each R  X  X  , let X ( R ) denote those examples fall into the component R , i.e.
 Note that for any indicator v  X  V +  X  V  X  , its type is cor-rectly decided, so the hypothesis makes no mistake on X ( v ). Since |R| X  T and the number of non-indicators is bounded by k , either there is a component R 0 such that or there is a non-indicator v 0 such that In the first case, w.h.p. there are more than These examples are sufficient to decide the type of the indicators in the component correctly. Also, w.h.p. for S ( R ), the type of the indicators in the component are correctly decided. This means that we correctly label at least one component not labeled previously. This type of updates happen at most T times.
 In the second case, we have and thus When amples and we have Also, for any indicator v  X  V +  X  V  X  , Then w.h.p. we have This means that we can correctly identify a non-indicator. This type of updates happen at most k times. Hence, we update the hypothesis at most ( k + T ) times. Sample Complexity and Running Time: When building the commonality graph, we need Each time we check the hypothesis, we need The number of labeled examples then follows from bounding the number of checks by ( k + T ) and bound-ing T by log | C D, X  ( ) | . The algorithm runs in polyno-mial time since building the commonality graph and checking the hypothesis take polynomial time. 3.2. Active Learning There are two main changes in extending our algorithm to the noisy setting. First, instead of simply determin-ing the type of an indicator with one example, we need to check many examples that contain it. A majority vote will correctly identify the type of the indicator. Second, instead of doing binary search over nodes in the path that connect positive and negative examples, we need to search over edges. This is because with noise, an indicator may appear both in negative and positive examples similar to a non-indicator, so it is not straightforward to identify a non-indicator merely by the types of examples it appears in. On the other hand, an edge that contains an indicator will have its true label match that of the indicator, so we can reli-ably determine the type of an edge if there are enough examples that contain both variables, and then iden-tify a vertex in edges of two different types to be a non-indicator. More specifically, Subroutine 5 can be used to decide the type of a pair of variables (when u 6 = v ) or that of one variable (when u = v ), which is a building block for the extension of the active learning algorithm to the noisy setting. A brief description of the extension is provided below, while the details are provided in Algorithm 6 and Theorem 6.
 First, we build the commonality graph and an initial hypothesis. Let F be the set of all examples that contain some pair of variables appearing together in unlabeled data U \ F to construct the commonality graph, so that every edge corresponds to a pair of variables whose indicator type can be decided. Then we pick a variable in each component in the graph and T is the number of connected components in the graph that results from removing all non-indicators from the commonality graph G com ( U \ F ). A hypothesis is then constructed which labels an input example by the type of the nearest labeled variable.
 Second, we check and update the hypothesis on a set of examples. We randomly sample a set S of  X  O ( 1 (1  X  2  X  ) 2 2 examples from U \ F , and compute err S ( h ). If err S ( h ) is at most  X  + 1  X  2  X  2 , we output the hypothesis since it has small error. Otherwise, it can be shown that on  X ( ) fraction of examples in S the hypothesis has different labels from the target concept h  X  . This fact can be used to identify a non-indicator. We randomly sample  X  O ( 1 ) examples from S , and for each example x , pick min ( k + 1 , | x | 1 ) variables and decide their types, where | x | 1 is the number of variables appearing in x . This ensures that we will eventually pick an indicator in an example x such that h ( x ) 6 = h  X  ( x ). Then we find a path connecting a positive indicator and a negative indicator, and thus can identify a non-indicator by binary search on the edges along the path. Therefore, after at most k updates, we are guaranteed to have a hypothesis with small error.
 Theorem 6. For any distribution D X,Y over { 0 , 1 } n  X  { X  1 , 1 } and target concept h  X   X  C in the random clas-sification noise model such that h  X  has at most k non-Subroutine 5 DecideType( U , u , v ) Input: unlabeled data U , variables u and v Set t = sign( P x  X  S ` ( x )) Output: { ( u,t ) , ( v,t ) } Algorithm 6 Actively Learning Two-Sided Disjunc-tions under Random Classification Noise Input: unlabeled data U , parameters  X ,, X ,k Set U ( u,v ) = |{ x  X  U : { u,v } X  x }| ,  X  u,v  X  V .
Set F = { x  X  U :  X  u,v  X  x, U ( u,v ) &lt;
Set U 0 = U \ F , G = G com ( U 0 ), and L =  X  for each connected component R of G do while err S ( h ) &gt;  X  + 1  X  2  X  2 do
Output: the hypothesis h m q = O label queries, with probability at least 1  X   X  , Algorithm 6 outputs a hypothesis h in polynomial time such that err D ( h )  X   X  + .
 Proof. Generalization Error: Assuming the hypoth-esis is updated at most k times (proved later), we bound the probability that the output hypothesis h has d ( h,h  X  )  X  . We begin by showing that the ignored examples F have small probability mass. When U is sufficiently large, w.h.p. all pairs of variables that ap-pear together with probability at least 8 n 2 will appear in sufficiently many examples in U . Assuming this is true, we have This means when d ( h,h  X  ) &gt; , and Then we have Union bounding over the k updates, we have that w.h.p. the hypothesis h output has d ( h,h  X  )  X  , which leads to err D ( h )  X   X  + .
 Correctness of Subroutine 5: Here we show that w.h.p. the majority voting method always decides cor-rectly the type of the indicators, so that we build and update the hypothesis correctly. Fix a pair of variables ( u,v ) containing at least one indicator. Let B u,v denote the event that ( u,v ) appear in at least 100 (1  X  2  X  ) 2 amples in U but the algorithm fails to decide the type. This happens when the labels of more than half of the examples queried are flipped. We have by Hoeffding bound Pr[ B u,v ]  X  exp  X  2(1  X  2  X  ) 2 and thus Pr[  X  u,v B u,v ]  X   X  4 .
 Queries per Stage: To build the hypothesis, we de-cide the type of one variable for each connected com-ponent of G . The number of components is bounded We now show that by using sufficient many queries at each check, we make sure that when the hypothesis has large error a non-indicator is identified, so that the hypothesis is updated at most k times. If d ( h,h  X  )  X  / 4, then and thus Therefore, if err S ( h ) &gt;  X  + 1  X  2  X  2 , we have d ( h,h / 4. This means on at least / 16 fraction of the exam-ples in S we have h ( x ) 6 = h  X  ( x ). By sampling 100 times from S and then picking min ( k + 1 , | x | 1 ) variables in the sampled x , w.h.p. we will eventually pick such an example, and pick at least one indicator in it, whose type is different from the nearest indicator. Then we find a path connecting positive and negative indicators, and discover a non-indicator by binary search. So we need queries each time we check and update the hypothesis. Query Complexity and Running Time: Since when the hypothesis has large error a non-indicator is identified, it is checked and updated at most k times. Then the number of queries follows by bounding T by log | C D, X  ( ) | . Notice that T is the number of connected components in G com ( U \ F ) (instead of G com ( U )) after removing the non-indicators. However, when U is suf-ficiently large, w.h.p. the probability mass of F is at most / 8, i.e. all significant edges appear in the graph, so we still have T  X  log | C D, X  ( ) | . Building, checking and updating the hypothesis all take polynomial time, so the algorithm runs in polynomial time. 3.3. Extension of Algorithm 1 The key idea is that we can still build the indicator graph by majority voting, and then enumerate over compatible hypotheses. A description is provided be-low, while the details are provided in Algorithm 7 and Theorem 7.
 First, we draw enough examples to make sure that all non-indicators are significant, and use majority voting to correctly decide the types of significant variables. This requires  X  O ( 1 construct the indicator graph as we did in the noise-free setting. However, since we only label significant variables (i.e. identify them as potential indicators), possibly not enough variables are labeled. Then many of the components in the graph that results from re-moving all non-indicators from the commonality graph are not connected to any labeled variables, and thus all the hypotheses built according to minimal vertex covers in the indicator graph do not have small errors. To address this, we take an additional step to add more variables to the potential indicator sets before building the the indicator graph. More precisely, after removing the significant variables from the commonality graph, we know that every component must contain only indi-a component, we can safely decide the type of the vari-ables in it to be the majority label of these examples, and add these variables to the corresponding potential indicator sets. So we draw  X  O ( T (1  X  2  X  ) 2 ) examples to make sure only components with probability mass at most / (8 T ) are not labeled. Here T is the number of connected components in the graph that results from Algorithm 7 Semi-supervised Learning with Random Classification Noise via Enumeration Input: data sets U and L , parameters ,  X  , 0 ,  X  Set G = G com ( U ) ,V 0 + = V 0  X  =  X 
Set L 0 = sample 100 for each variable v appearing in more than for each component R in G \ ( V 0 +  X  V 0  X  ) do
Set G I = G ind ( G,V + ,V  X  ) for each minimal vertex cover S of G I do
Output: hypothesis h = ( h + ,G 0 \ h + ) removing all non-indicators from G com ( U ), and can be bounded by log | C D, X  ( ) | . Now hypotheses built on the indicator graph that are compatible with the unlabeled data can only make mistakes on these components. After the additional step, we build the indicator graph and enumerate over compatible hypotheses built ac-cording to minimal vertex covers. To ensure that the output hypothesis has small error, we must be able to distinguish under the noise between a hypothesis h with d ( h,h  X  ) &gt; and one with d ( h,h  X  )  X  / 4, i.e. to distinguish between a hypothesis h with and one with So we need to bound the deviation of the empirical error by O ( (1  X  2  X  )), which requires  X  O ( 1 (1  X  2  X  ) 2 2 beled examples. Union bounding over all compatible hypothesis introduces an extra term O (log | C D, X  ( ) | ). Theorem 7. For any distribution D X,Y over { 0 , 1 } n  X  { X  1 , 1 } and target concept h  X   X  C in the random clas-sification noise model such that h  X  has at most k non-indicators, and the minimum non-indicator probability is 0 , if m u = O ( n 2 log n  X  ) and m l = O then with probability at least 1  X   X  , Algorithm 7 outputs a two-sided disjunction h  X  C such that  X  ( h,U ) = 1 , and err D ( h )  X   X  + . Furthermore, the algorithm runs in polynomial time when k = O (log n ) .
 Proof. Reducing to the Noise-free Setting: We show that all non-indicators are in V +  X  V  X  , and the types of all indicators in V +  X  V  X  are decided correctly, so that the indicator graph and the hypotheses are built correctly as in the noise-free setting. When all non-indicators are in V 0 +  X  V 0  X  and thus in V +  X  V correctly only when the labels of more than half of the examples containing it are flipped, which happens with probability at most  X / (8 n ). By union bound, we have that with probability at least 1  X   X / 8, the types of all indicators in V 0 +  X  V 0  X  are decided correctly. Furthermore, the types of all indicators added to V +  X  V  X  in later steps are also correct. Since all non-indicators are in V 0 +  X  V 0  X  , any component R in G \ ( V 0 +  X  V 0  X  ) must contain only one type of indicators. Then with probability at least 1  X   X / 8, the types of all R with O ( 1 (1  X  2  X  ) 2 log n  X  ) labeled examples are decided correctly.
 Generalization Error: First note that when all hypotheses compatible with U fall in C D, X  ( ) with probability at least 1  X   X / 8. Fix a hypothesis h 0 C
D, X  ( ) with d ( h 0 ,h  X  ) &gt; . By Hoeffding bound, when we have Then by union bound, with probability at least 1  X   X / 8, 2 . So a hypothesis h satisfying the exit condition satisfies d ( h,h  X  )  X  and thus err D ( h )  X   X  + . Now it suffices to show that we can always find a suitable minimal vertex cover that leads to such a hypothesis. Note that at least one endpoint of every edge in G I must be a non-indicator, so there must be a subset  X  S of non-indicators that is a minimal vertex from the minimal vertex cover  X  S . We show that  X  h is compatible and err L (  X  h )  X   X  + 1  X  2  X  2 . If an example contained both positive and negative indicators, this would imply an edge still present in G
I , which is impossible. So Next we show that err L (  X  h )  X   X  + 1  X  2  X  2 . Consider the components in G \ ( V +  X  V  X  ). Suppose when build-ing the hypothesis, the variables in some components R 1 ,R 2 ,...,R t are not correctly decided. First, there are just a few such components. Each such component is either connected to non-indicators in G \  X  S that have label of a different type, or is not connected to any labeled variable but the variables in it are positive indi-cators. Then such components are components in the graph  X  G that results from removing all non-indicators from the commonality graph G . So t is no larger than the number T of components in  X  G , and we have when m u = O ( n 2 log n  X  ). Second, each such component has small probability mass. These components are also components in G \ ( V 0 +  X  V 0  X  ). When all components in G \ ( V 0 +  X  V 0  X  ) with probability more than / (8 T ) have at least 10 (1  X  2  X  ) 2 log n  X  labeled exam-ples and thus are added to V +  X  V  X  . Then each of R 1 ,...,R t has probability at most / (8 T ). This means and Then w.h.p. we have Therefore, we are guaranteed to find such a hypothesis when the algorithm stops.
 Sample Complexity and Running Time: To en-sure the indicator graph is built correctly, we need To ensure the hypothesis output has small error, we need Then the sample complexity of the algorithm follows from T  X | C D, X  ( ) | . The time for building the indicator graph is clearly polynomial. The running time for checking the hypotheses is the same as in the noise-free setting, so it is polynomial when k = O ( log n ). Therefore, the algorithm runs in polynomial time when
 Maria-Florina Balcan ninamf@cc.gatech.edu Christopher Berlind cberlind@gatech.edu Steven Ehrlich sehrlich@cc.gatech.edu Yingyu Liang yliang39@gatech.edu In many modern applications, like web-based informa-tion gathering, unlabeled data is abundant but labeling it is expensive. Consequently, there has been substan-tial effort in understanding and using semi-supervised learning (using large amounts of unlabeled data to aug-ment limited labeled data) and active learning (where the algorithm itself asks for labels of carefully cho-sen examples with the goal of minimizing the human labeling effort) (Zhu, 2011; Dasgupta, 2011). Conceptually, what makes unlabeled data useful in the semi-supervised learning context (Balcan &amp; Blum, 2010; Zhu, 2011), is that for many learning problems, the natural regularities of the problem involve not only the form of the function being learned but also how this function relates to the distribution of data; for example, that it partitions data by a wide margin as in Transduc-tive SVM (Joachims, 1999a) or that data contains re-dundant sufficient information as in Co-training (Blum &amp; Mitchell, 1998). Unlabeled data is useful in this con-text because it allows one to reduce the search space from the whole set of hypotheses, down to the set of hypotheses satisfying the regularity condition with re-spect to the underlying distribution. Such insights have been exploited for deriving a variety of sample complexity results (Dasgupta et al., 2001; Kaariainen, 2005; Rigollet, 2007; Balcan &amp; Blum, 2010). However, while in principle semi-supervised learning can provide benefits over fully supervised learning (Balcan &amp; Blum, 2010; Zhu, 2011), the corresponding algorithmic prob-lems become much more challenging. As a consequence there has been a scarcity of efficient semi-supervised learning algorithms.
 In this paper we provide efficient algorithms with nearly optimal sample complexity for semi-supervised and ac-tive learning of disjunctions under a natural regularity assumption introduced in (Balcan &amp; Blum, 2005). In particular we consider the so called two-sided disjunc-tions setting, where we assume that the target function is a monotone disjunction satisfying a margin like reg-ularity assumption 1 . In the simplest case resolved in (Balcan &amp; Blum, 2005), the notion of  X  X argin X  is as follows: every variable is either a positive indicator for the target function (i.e., the true label of any example containing that variable is positive) or a negative in-dicator (i.e., the true label of any example containing that variable is negative), and no example contains both positive and negative indicators. In this work, we consider the much more challenging setting left open in (Blum &amp; Balcan, 2007) where non-indicators or ir-relevant variables , i.e., variables that appear in both positive and negative examples, are also present. In the semi-supervised learning setting, we present an algorithm that finds a consistent hypothesis that furthermore is compatible (in the sense that it satisfies our regularity assumption). This algorithm is proper (it outputs a disjunction), has near-optimal labeled data sample complexity provided that each irrelevant variable appears with non-negligible probability, and it is efficient when the number of irrelevant variables is O ( log n ). We next present a non-proper algorithm that PAC learns two-sided disjunctions with nearly the same sample complexity and whose running time is polynomial for any k .
 In the active learning setting, we present an efficient active learning algorithm for two-sided disjunctions. This algorithm outputs a consistent, compatible hy-pothesis, with sample complexity linear in the number of irrelevant variables and independent of the proba-bility of irrelevant variables appearing, a quantity that appears in the bounds in the semi-supervised setting. Combined with the NP -hardness result we show for two-sided disjunctions (see supplementary material), the algorithm also shows that the active query ability can help to overcome computational difficulty. We also discuss how our algorithms can be adapted to deal with random classification noise.
 Discussion: To see why the presence of irrelevant vari-ables significantly complicates the algorithmic problem, note that in the absence of non-indicators (the case studied in (Balcan &amp; Blum, 2005)), we could construct an approximation of the so called commonality graph, defined on n vertices (one per variable), by putting an edge between two vertices i and j if there is any example x in our unlabeled sample with x i , x j set to 1. If the target function indeed satisfies the regular-ity assumption, then no component will get multiple labels, so all we need to learn is a labeled example in each component. Furthermore, if the number of com-ponents in the underlying graph is small, then both in the semi-supervised and active learning setting we can learn with many fewer labeled examples then in the supervised learning setting.
 Introducing non-indicators into the target concept com-plicates matters, because components can now have multiple labels. We could think of the non-indicators as forming a vertex cut in the commonality graph separat-ing variables corresponding to positive indicators from those corresponding to negative ones. To learn well, one could try to find such a cut with the necessary prop-erties to ensure compatibility with the unlabeled data (i.e. no examples are composed only of non-indicators). Unfortunately, this is a difficult combinatorial prob-lem in general. Interestingly, we will be able to find such cut for k = O ( log n ) and for general k we will be still be able to learn with nearly optimal rates, if each non-indicator appears with non-negligible probability; we do this by identifying a superset of non-indicators and carefully making inferences using it. Furthermore, since mistakes reveal vertices in both sides of the cut, our adaptive query ability in the active learning model will allow us to actively search for vertices in the cut. Related work: While several semi-supervised learn-ing methods have been introduced (Chapelle et al., 2006; Zhu et al., 2003; Joachims, 1999b), much of the theoretical work has focused either on sample com-plexity (e.g., (Dasgupta et al., 2001; Kaariainen, 2005; Rigollet, 2007)) or on providing polynomial time al-gorithms with error bounds for surrogate losses only (e.g., (Rosenberg &amp; Bartlett, 2007)). The few exist-ing results with guarantees on the classification error loss hold under very stringent conditions about the underlying data distribution (e.g., independence given the label (Blum &amp; Mitchell, 1998)). By contrast, we provide (PAC-style) polynomial time algorithms for learning disjunctions with general guarantees on the classification error loss.
 We note that while a lot of the research on active learning (Dasgupta, 2005; Balcan et al., 2006; Hanneke, 2007a;b; Dasgupta et al., 2007; Beygelzimer et al., 2010; Koltchinskii, 2010) has not made an explicit regularity assumption as in semi-supervised learning, this is an interesting direction to study. As our results reveal, ac-tive learning could help overcome computational hard-ness limitations over (semi-supervised) passive learning in these settings. Let X = { 0 , 1 } n be the instance space, Y = { X  1 , 1 } be the label set, and D denote any fixed probability distribution over X . Following (Balcan &amp; Blum, 2005), a two-sided disjunction h is defined as a pair of mono-tone disjunctions 2 ( h + ,h  X  ) such that h + ( x ) =  X  h for all x  X  D , and h + is used for classification. Let the concept class C be the set of all pairs 3 of monotone disjunctions and for any hypothesis h = ( h + ,h  X  )  X  C , define h ( x ) = h + ( x ).
 For a two-sided disjunction ( h + ,h  X  ), variables included in h + are the positive indicators , and variables in h  X  negative indicators . Variables appearing neither in h + nor in h  X  are called non-indicators , as the value of any such variable has no effect on the label of any example. To simplify the discussion, we will often identify binary strings in X = { 0 , 1 } n with subsets of the variables V = { x 1 ,...,x n } . In other words, we say an example x contains x i if the i -th coordinate of x is set to 1. This allows us to speak of variables  X  X ppearing in X  or  X  X eing present in X  examples rather than variables being set to 1. We will use similar language when referring to hypotheses, so that a two-sided disjunction h = ( h + ,h  X  ) consists of a set h + of positive indicators and a set h  X  of negative indicators (which completely determine a third set of non-indicators).
 In the semi-supervised learning setting, we will assume that both labeled examples L and unlabeled examples U are drawn i.i.d. from D and that examples in L are labeled by the target concept h  X  , where h  X  is a two-sided disjunction with at most k non-indicators. We will let | L | = m l and | U | = m u ; both m l and m u will be polynomial throughout the paper. In the active setting, the algorithm first receives a polynomially sized unlabeled sample U and it can adaptively ask for the label ` ( x ) = h  X  ( x ) of any example x  X  U . In the random classification noise model (studied in Section 5) we assume that the label of each labeled example is flipped with probability  X  .
 The generalization error of a hypothesis h is given by misclassifying a random example drawn from D . For a set L of labeled examples, the empirical error is given by err L ( h ) = | L |  X  1 P x  X  L I [ h ( x ) 6 = h  X  ( x )]. If err for some h we say that h is consistent with the data. To formally encapsulate the regularity or compatibility assumption for two-sided disjunctions described in the introduction, we consider the compatibility function  X  :  X  ( h,x ) = I [ h + ( x ) =  X  h  X  ( x )] for any hypothe-sis h and example x  X  X . In addition, we define (overloading notation) the compatibility between h and the distribution D as  X  ( h,D ) = E x  X  D [  X  ( h,x )] = Pr x  X  D [ h + ( x ) =  X  h  X  ( x )]. For a set U of unlabeled examples, define the empirical compatibility between h and U as  X  ( h,U ) = | U |  X  1 P x  X  U I [ h + ( x ) =  X  h If  X  ( h,U ) = 1 we say that h is compatible with the data. Thus a hypothesis is consistent and compatible with a set of examples if every example contains exactly one type of indicator and every labeled example contains an indicator of the same type as its label. We will assume throughout that the target function is compatible. We define, for any &gt; 0, the reduced hypothesis class C
D, X  ( ) = { h  X  C : 1  X   X  ( h,D )  X  } , the set of hypotheses with  X  X nlabeled error X  at most . Similarly, C
U, X  ( ) = { h  X  C : 1  X   X  ( h,U )  X  } for an unlabeled sample U . The key benefit of using unlabeled data and our regularity assumption is that the number of labeled examples will only depend on log | C D, X  ( ) | which for helpful distributions will be much smaller than log | C | . 2.1. The Commonality Graph The basic structure used by all of our algorithms is a construct we call the commonality graph . As mentioned in the introduction, the commonality graph is the graph on variables that contains an edge between two vertices if the corresponding variables appear together in a common example. That is, given the set U of unlabeled examples, define the commonality graph G com ( U ) = ( V,E ) where V = { x 1 ,...,x n } and E contains an edge ( x i ,x j ) if and only if there is some x  X  U such that x i and x j are both set to 1 in x . Furthermore, given the set L of labeled examples, let V + L be the set of variables appearing in positive examples and V  X  L be the set of variables appearing in negative examples. The edge structure of the commonality graph and the labeled examples will allow us to draw inferences about which vertices in the graph correspond to positive indi-cators, negative indicators, and non-indicators in the target concept. Any variable that appears in a labeled example cannot be an indicator of the type opposite of the label. In addition, an edge between two variables implies they cannot be indicators of different types. This means that any path in the commonality graph between positive and negative indicators must contain a non-indicator. Similarly, paths that pass only through indicator variables can be used to propagate labels to the unlabeled examples. Our general strategy is to identify non-indicators and remove them from the commonality graph, reducing this problem to the simpler case. Notice that each non-indicator that appears in the unlabeled data is significant; failing to identify it can lead to incorrect inferences about a large probability mass of examples. A variable is obviously a non-indicator if it appears in both positive and negative examples. A na  X  X ve approach would be to draw enough labeled examples so that ev-ery significant non-indicator appears in examples with both labels. The problem with this approach is that some non-indicator can appear much more frequently in positive examples than in negative examples. In this case the number of examples needed by the na  X  X ve approach is inversely proportional to the probability of that non-indicator appearing in negative examples. This sample complexity can be worse than in the fully supervised case.
 In our approach, it is enough to ensure each non-indicator appears in a labeled example, but not nec-essarily in both positive and negative examples. The number of examples needed in this case will now de-pend on the minimum probability of a non-indicator appearing. This allows the sample complexity to be significantly smaller than that of the na  X  X ve approach; for example, when a non-indicator appears in positive examples with constant probability while in negative examples with probability /n .
 Our approach can still identify non-indicators, now by examining paths in the commonality graph. In paths whose interior vertices appear only in unlabeled exam-ples (i.e. are indicators) and whose endpoints appear in oppositely labeled examples, one of the endpoints must be a non-indicator. When k = O ( log n ) we can enumerate over all consistent compatible hypotheses efficiently by restricting our attention to a small set of paths.
 If the number of non-indicators is larger, we can still find a good hypothesis efficiently by finding the non-indicators one at a time. Each time our working hy-pothesis makes a mistake this reveals a path whose endpoint is a non-indicator.
 The number of labeled examples we require will depend on the minimum non-indicator probability defined by For notational convenience denote it simply by 0 with-out ambiguity. To guarantee with high probability that each non-indicator appears in some labeled example, it suffices to use  X  O ( 1 3.1. Finding a Consistent, Compatible We now give an algorithm, along with some intuition, for finding a two-sided disjunction that is consistent and compatible with a given training set. We note that this problem is NP -hard in general (see Section 2 in supplementary material). Given example sets L and U , the algorithm begins by constructing the commonality graph G = G com ( U ) and setting G to G \ ( V +  X  V  X  ). This removes any variables that appear in both positive and negative examples as these must be non-indicators. To identify the rest of the non-indicators, we consider a new graph. Using u  X  G v to denote the existence of a path in the graph G between vertices u and v , we define the indicator graph G ind ( G,V + ,V  X  ) to be the bipartite graph with vertex set V +  X  V  X  and edge set { ( u,v )  X  V +  X  V  X  : u  X  G \ ( V idea is that an edge in this graph implies that at least one of its endpoints is a non-indicator, since the two variables appear in oppositely labeled examples but are connected by a path of indicators.
 Note that the target set of non-indicators must form a vertex cover in the indicator graph. By iterating over all minimal vertex covers, we must find a subset of the target non-indicators whose removal disconnects Algorithm 1 Finding a consistent compatible two-sided disjunction Input: unlabeled set U , labeled set L Set G = G com ( U ), V + = V + L , V  X  = V  X  L Set G = G \ ( V +  X  V  X  ) Set V + = V +  X  G , V  X  = V  X   X  G
Set G I = G ind ( G,V + ,V  X  ) for each minimal vertex cover S of G I do
Output: hypothesis h = ( h + ,G 0 \ h + ) positive examples from negative examples, and this corresponds to a consistent compatible hypothesis. The algorithm is summarized in Algorithm 1.
 The key step in Algorithm 1 is enumerating the minimal vertex covers of the indicator graph G I . One way to do this is as follows. First find a maximum matching M in G I , and let m be the number of disjoint edges in M . Enumerate all 3 m subsets of vertices that cover M (for every edge in M , one or both of the endpoints can be included in the cover). For each such cover S , extend S to a minimal vertex cover of G I by adding to S every variable not covered by M that has no neighbors already in S . This extension can always be done uniquely, so there is a one-to-one correspondence between covers of M and minimal vertex covers of G I . This enumeration method gives us both a concrete way to implement Algorithm 1 and a way to bound its running time. We prove in Theorem 1 that given enough data, Algorithm 1 correctly outputs a consistent compatible hypothesis with high probability. We then bound its time and sample complexity.
 Theorem 1. For any distribution D over { 0 , 1 } n and target concept h  X   X  C such that  X  ( h  X  ,D ) = 1 , h  X  has at most k non-indicators, and the minimum non-indicator probability is 0 , if m u  X  1 h log 2 | C |  X  i and then with probability at least 1  X  2  X  , Algorithm 1 outputs a hypothesis h  X  C such that err L ( h ) = 0 ,  X  ( h,U ) = 1 , and err ( h )  X  . Furthermore, when k = O ( log n ) the algorithm runs in time at most poly ( n ) .
 Proof. We separate the proof into three sections, first proving consistency and compatibility of the output hypothesis, then giving the sample sizes required to guarantee good generalization, and finally showing the overall running time of the algorithm.
 Consistency and Compatibility: The exit condi-tion for the loop in Algorithm 1 guarantees that the algorithm will output a consistent compatible hypothe-sis, so long as a suitable minimal vertex cover of G I is found. Thus, it suffices to show that such a vertex cover exists with high probability when L is large enough. By the definition of 0 along with the independence of the samples and a union bound, if m l &gt; 1 then with probability at least 1  X   X  , all non-indicator variables appear in some labeled example. We will assume in the remainder of the proof that all variables not in V + L  X  V  X  L are indicators.
 Since an edge in G between indicators forces both endpoints to be of the same type, a path through indicators does the same. Edges in G I correspond to such paths, but the endpoints of such an edge cannot be indicators of the same type because they appear in differently labeled examples. It follows that at least one endpoint of every edge in G I must be a non-indicator. Now let V 0 be the set of non-indicators in the target hypothesis. The above observations imply that V 0 contains a vertex cover of G I . It follows that there must exist a subset  X  S  X  V 0 that is a minimal vertex cover of G I . Let  X  h = (  X  h + ,  X  h  X  ) be the hypothesis h formed from the minimal vertex cover S =  X  S . We only need to show that  X  h is both consistent and compatible. Every indicator of h  X  is also an indicator of  X  h since only true non-indicators were removed from G and all remaining variables became indicators in  X  h . Since every example contains an indicator of h  X  , every example must contain an indicator of  X  h of the correct type. Furthermore, if an example contained both positive and negative indicators, this would imply an edge still present in G I . But removing a vertex cover removes all edges, so this is impossible. Hence  X  h is a consistent, fully compatible hypothesis.
 Generalization Error: If m u  X  1 [log 2 | C |  X  ] and m ysis states that Algorithm 1 will fail to produce a consistent compatible hypothesis with probability at most  X  . Furthermore, an algorithm with true error rate greater than will be fully consistent with a labeled set of size m l with probability at most  X /C D, X  ( ). Union bounding over all compatible hypotheses we see that a consistent compatible hypothesis will fail to have an error rate less than with probability at most  X  . By a union bound over the two failure events, the overall probability of failure is  X  2  X  .
 Running Time: Since checking consistency and com-patibility can be done in time polynomial in the number of examples, the limiting factor in the running time is the search over minimal vertex covers of G I . In a bipartite graph, the size of the minimum vertex cover is equal to the size of the maximum matching. The set of k non-indicators in the target hypothesis includes a vertex cover of G I , so the size m of the maximum matching is at most k . There is one minimal vertex cover for each of the 3 m covers of the maximum match-ing, so the number of minimal vertex covers to search is at most 3 k . 3.2. A General Semi-supervised Algorithm Algorithm 1 is guaranteed (provided the labeled set is large enough) to find a hypothesis that is both consis-tent and compatible with the given data but is efficient only when k is logarithmic in n . When k is instead poly-logarithmic in n , our algorithm is no longer efficient but still achieves a large improvement in sample complexity over supervised learning. We now present an efficient algorithm for finding a low-error (but not necessarily consistent and compatible) hypothesis which matches the sample complexity of Algorithm 1.
 The algorithm, summarized in Algorithm 2, begins by constructing the commonality graph from the unlabeled examples and identifying potential indicators from a subset of the labeled examples. We use sample ( m,S ) to denote a random sample of m elements from set S . An initial hypothesis is built and tested on the sequence of remaining labeled examples. If the hypothesis makes a mistake, it is updated and testing continues. Each update corresponds to either identifying a non-indicator or labeling all indicators in some connected component in the commonality graph, so the number of updates is bounded. Furthermore, if the hypothesis makes no mistakes on a large enough sequence of consecutive examples, then with high probability it has a small error rate overall. This gives us a stopping condition and allows us to bound the number of examples seen between updates.
 The hypotheses in Algorithm 2 use a variation on near-est neighbor rules for classification. Given a common-ality graph G and a set L of labeled examples, the associated nearest neighbor hypothesis h G,L classifies an example x the same as the nearest labeled example in L . The distance between two examples x and x 0 is the measured by the minimum path distance in G between the variables in x and the variables in x 0 . If no examples in L are connected to x , then h G,L classifies x negative by default. For convenience, we use nn G,S ( x ) to denote the vertex in the set S nearest to a variable Algorithm 2 Learning a Low-error Hypothesis Input: data sets U and L , parameters ,  X  , k
Set L 0 = sample( 1
Set h = h G,L 0 and c = 0 while L 6 =  X  and c  X  1 log k + T  X  do
Output: the hypothesis h in the example x via a path in G . If no such vertex ex-ists, nn G,S ( x ) returns the empty set. Using hypotheses of this form ensures that the neighbor variable used to classify an example x is connected to x by a path through indicators, which allows us to propagate its la-bel to the new example. If the example is misclassified, we must have been fooled by a non-indicator.
 The number of examples used by Algorithm 2 depends on T , the number of connected components in the commonality graph after removing all non-indicators. Lemma 1 bounds T by the number of compatible hy-potheses and is proved in the supplementary material. Theorem 2 bounds the number of examples sufficient for Algorithm 2 to output a low-error hypothesis. Lemma 1. Let G be the graph that results from re-moving all non-indicators from G com ( U ) , and sup-pose G is divided into T connected components. If m ity at least 1  X   X  .
 Theorem 2. For any distribution D over { 0 , 1 } n and target concept h  X   X  C such that  X  ( h  X  ,D ) = 1 , h  X  has at most k non-indicators, and the minimum non-indicator probability is 0 , if m u  X  2 n 2 log n  X  and m then with probability at least 1  X  3  X  , Algorithm 2 outputs a hypothesis h in polynomial time such that err ( h )  X  . Proof. Generalization Error: First note that accord-ing to the loop exit condition, Algorithm 2 outputs the first hypothesis it encounters that correctly classifies a sequence of at least 1 log k + T  X  i.i.d. examples from D . If err ( h ) &gt; for some hypothesis h , then the prob-ability that h correctly classifies such a sequence of Algorithm 2 updates its hypothesis at most k + T times, a union bound over the k + T hypotheses considered guarantees that with probability at least 1  X   X  , the hypothesis output by Algorithm 2 has error rate at most . In the remainder of the proof, we will bound the total number of samples required and show that it makes at most k + T updates to its hypothesis. Mistake Bound: By the definition of 0 , the initial set of m l labeled examples ensures that with probability at least 1  X   X  all non-indicators are included in the potential indicator set P , so all variables outside P (call this set Q ) are indicators. We will assume such an event holds throughout the remainder of the proof. In particular, this means that any paths through Q must consist entirely of indicators of the same type. Suppose at some point during the execution of Algo-rithm 2, the intermediate hypothesis h misclassifies an example x . There are two types of such mistakes to consider. If the variables in x are not connected to any variables in P , then by the above observation, all variables connected to x are indicators of the same type, and in particular, they are indicators of the type corresponding to the label of x . This means that this type of mistake can occur only when h knows of no labeled examples connected to x . Once h is updated to be h G,L 0 where x  X  L 0 , h can make no further mistakes of this type on any examples connected to x . Thus, Algorithm 2 can make at most T mistakes of this type before all components have labeled examples.
 The hypothesis h G,L 0 labels x with the label of the example of L 0 containing nn G,P ( x ). If x is labeled in-correctly, then this must be an example with label op-posite that of x . But since the path between nn G,P ( x ) and x consists only of vertices not in P , i.e. indicators, we conclude that nn G,P ( x ) must be a non-indicator. Algorithm 2 can make at most k mistakes of this type before all non-indicators are removed from G . Sample Complexity and Running Time: We have shown that after Algorithm 2 makes k + T updates, all non-indicators have been removed from G and all connected components in G contain a variable that has appeared in a labeled example. Since at most log k + T  X  examples can be seen between updates, the total number of labeled examples needed is at most Straightforward algebra and an application of Lemma 1 yields the bound given in the theorem statement, and a union bound over the three failure events of probability  X  yields the stated probability of success. The time complexity is clearly polynomial in n per example and therefore polynomial overall. We now consider the problem of learning two-sided disjunctions in the active learning model, where the learner has access to a set U of unlabeled examples and an oracle that returns the label of any example in U we submit. The additional power provided by this model allows us to use the same strategy as in the semi-supervised algorithm in Section 3.2 while achieving sample complexity bounds independent of 0 .
 As in Section 3.2, the goal will be to identify and re-move non-indicators from the commonality graph and obtain labeled examples for each of the connected com-ponents in the resulting graph. In the semi-supervised model we could identify a mistake when there was a path connecting a positive labeled example and a neg-ative labeled example. To identify non-indicators we guaranteed that they would lie on the endpoints of these labeled paths. In the active learning setting, we are able to check the labels of examples along this path, and thus are able to remove our dependence on the minimum non-indicator probability.
 The algorithm we propose can be seen as a slight modi-fication of Algorithm 2. The idea is to maintain a set of at least one labeled example per connected component and to test the corresponding nearest neighbor hypoth-esis on randomly chosen examples. If the hypothesis misclassifies some example, it identifies a path from the example to its nearest neighbor. Since these examples have opposite labels, a non-indicator must be present at a point on the path where positive indicators switch to negative indicators, and such a non-indicator can found in logarithmically many queries by actively choosing examples to query along this path in a binary search pattern. The search begins by querying the label of an example containing the variable at the midpoint of the path. Depending on the queried label, one of the endpoints of the path is updated to the midpoint, and the search continues recursively on the smaller path whose endpoints still have opposite labels. Let BinarySearch G,L ( x ) return the non-indicator along the path in G from a variable in x to nn G,L ( x ). The algo-rithm halts after removing all k non-indicators or after correctly labeling a long enough sequence of random examples. The details are described in Algorithm 3, and the analysis is presented in Theorem 3.
 Theorem 3. For any distribution D over { 0 , 1 } n and target concept h  X   X  C such that  X  ( h  X  ,D ) = 1 and h  X  has at most k non-indicators, let T be the number of connected components in the graph G that results Algorithm 3 Active Learning Two-Sided Disjunctions Input: unlabeled data U , parameters ,  X  , k
Set G = G com ( U ) and L =  X  for each connected component R of G do
Set h = h G,L and c = 0 while c  X  1 log k  X  do
Output: the hypothesis h from removing all non-indicators from G com ( U ) . If m label queries, with probability  X  1  X  2  X  , Algorithm 3 outputs a hypothesis h in polynomial time such that err ( h )  X  .
 Proof. Generalization Error: According to the exit condition of the loop in Step 3, Algorithm 3 outputs the first hypothesis it encounters that correctly classifies a sequence of at least 1 log k  X  i.i.d. examples from D . If err ( h ) &gt; for some hypothesis h , then the probability that h correctly classifies such a sequence of examples is at most (1  X  ) 1 log k  X   X   X  k . Assuming Algorithm 3 updates its hypothesis at most k times, a union bound over the k hypotheses considered guarantees that with probability at least 1  X   X  , the hypothesis output by Algorithm 3 has error rate at most . In the remainder of the proof, we will bound the total number of samples required by Algorithm 3 and show that it makes at most k updates to its hypothesis.
 Queries per Stage: In the loops over connected com-ponents of G , one label is queried for each component. The components are those formed by removing from G a subset of the non-indicators, so the total number of queries made in these loops is at most T , the number of components after removing all non-indicators. Now suppose the hypothesis h misclassifies an example x . Let x 0 be the nearest labeled example to x , and let x i and x j be the endpoints of the shortest path from x to x 0 in G . If each variable along the path appears in examples of only one label, then there could be no path between x i and x j , which appear in examples with different labels. Thus, there must exist a variable along the path from x i to x j that appears in both positive and negative examples, i.e. a non-indicator. Since the commonality graph was constructed using the examples in U , we can query the labels of examples that contain variables between x i and x j in order to find the non-indicator. Using binary search, the number of queries is logarithmic in the path length, which is at most n . Query Complexity and Running Time: Each mis-take results in removing a non-indicator from the G , so at most k mistakes can be made. For each mistake, O ( log n ) queries are needed to find a non-indicator to remove and at most 1 log k  X  more queries are used before another mistake is made. Combined with the queries for the connected components, we can bound the total number of queries by O T + k log n + 1 log k  X  . We can further bound T by log | C D, X  ( ) | via Lemma 1, and pay the price of an additional  X  probability of failure. The running time is clearly polynomial. We also consider the more challenging problem of learn-ing two-sided disjunctions under random classification noise, where the observed label of each example is reversed from that given by the target concept in-dependently with probability  X   X  [0 , 1 / 2). The key modification we make to extend our algorithms is to determine the indicator type of each variable by taking a majority vote of the labels of the containing examples. To guarantee success with high probability this scheme is used only for variables that are significant , that is, they appear in at least  X  O ( 1 (1  X  2  X  ) 2 ) labeled examples. We discuss some details below for the extensions to Algorithms 2 and 3, and provide complete proofs for all three algorithms in the supplementary material. Semi-supervised Learning: We first draw enough examples to ensure that all non-indicators are signifi-cant and then build a hypothesis in a similar manner to the noise-free setting by deciding indicator types via majority voting. We then test the hypothesis on a set of labeled examples and output it if the empirical error is small. Otherwise, the high error may be caused either by a component without any labeled variables (which can be corrected by a majority vote of the ex-amples in that component) or a non-indicator (which can be identified by using enough labeled examples). As in the noise-free setting, this allows us to bound the number of updates to the hypothesis.
 Active Learning: Besides using majority voting to determine indicator types, another alteration is to per-form binary search over edges (instead of vertices) in order to distinguish disagreement caused by non-indicators from that caused by noise. If an edge con-tains an indicator, a majority vote of examples con-taining the two variables in the edge will agree with the type of the indicator, so any variable contained in edges of different labels must be a non-indicator. One drawback of our semi-supervised algorithms is that their dependence on the minimum non-indicator probability restricts the class of distributions they can efficiently learn. Additionally, the class of target con-cepts for which Algorithm 1 can efficiently learn a con-sistent and compatible hypothesis is restricted, and the reduction given in the supplementary material proves that some such restriction is necessary as the general problem is NP -hard. One surprising result of our work is that both restrictions can be lifted entirely in the ac-tive learning setting while improving label complexity at the same time. The ability to adaptively query the labels of examples allows us to execute a strategy for identifying non-indicators that would require too many labeled examples in the semi-supervised setting. While this represents the first known example of how active learning can be used to avert computational difficulties present in semi-supervised learning, it would be worth-while to give more such examples and to understand more generally when active learning provides this type of advantage.
 It is important to note that the problem of learning two-sided disjunctions can be viewed as learning un-der a large-margin assumption. We can represent a two-sided disjunction h as a linear threshold function h ( x ) = sign ( w T x ) where w i = +1 for positive indica-tors, w i =  X  1 for negative indicators, and w i = 0 for each of the k non-indicators. If h is fully compatible with the distribution D , it is straightforward to show that for every example x  X  D , | w T x | k w k corresponds to an L  X  L 1 margin of O (1 /k ). This is a different notion of margin than the L 2 L 2 margin appearing in the mistake bounds for the Perceptron algorithm (Rosenblatt, 1958) and the L 1 L  X  margin in the bounds for Winnow (Littlestone, 1988). One interesting area of future work is to provide generic al-gorithms with bounds depending on the L  X  L 1 margin. Acknowledgements This work was supported in part by NSF grant CCF-0953192, AFOSR grant FA9550-09-1-0538, and a Mi-crosoft Research Faculty Fellowship.
 Balcan, M.-F. and Blum, A. A PAC-style model for learning from labeled and unlabeled data. In Pro-ceedings of the 18th Annual Conference on Compu-tational Learning Theory (COLT) , 2005.
 Balcan, M. F., Beygelzimer, A., and Langford, J.
Agnostic active learning. In Proceedings of the 23rd International Conference on Machine Learn-ing (ICML) , 2006.
 Balcan, Maria-Florina and Blum, Avrim. A discrimi-native model for semi-supervised learning. Journal of the ACM , 57(3), 2010.
 Beygelzimer, A., Hsu, D., Langford, J., and Zhang, T. Agnostic active learning without constraints. In
Advances in Neural Information Processing Systems (NIPS) , 2010.
 Blum, Avrim and Balcan, Maria-Florina. Open prob-lems in efficient semi-supervised PAC learning. In
Proceedings of the 20th Annual Conference on Com-putational Learning Theory (COLT) , 2007.
 Blum, Avrim and Mitchell, Tom. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT) , 1998.
 Chapelle, O., Schlkopf, B., and Zien, A. Semi-Supervised Learning . MIT press, 2006.
 Dasgupta, S. Coarse sample complexity bounds for active learning. In Advances in Neural Information Processing Systems (NIPS) , 2005.
 Dasgupta, S. Active learning. Encyclopedia of Machine Learning , 2011.
 Dasgupta, S., Littman, M. L., and McAllester, D. PAC generalization bounds for co-training. In Advances in Neural Information Processing Systems (NIPS) , 2001.
 Dasgupta, Sanjoy, Hsu, Daniel, and Monteleoni, Claire. A general agnostic active learning algorithm. In
Advances in Neural Information Processing Systems (NIPS) , 2007.
 Hanneke, S. A bound on the label complexity of agnos-tic active learning. In Proceedings of the 24th Inter-national Conference on Machine Learning (ICML) , 2007a.
 Hanneke, S. Teaching dimension and the complexity of active learning. In Proceedings of the 20th An-nual Conference on Computational Learning Theory (COLT) , 2007b.
 Joachims, T. Transductive inference for text classifica-tion using support vector machines. In Proceedings of the 16th International Conference on Machine Learning (ICML) , 1999a.
 Joachims, Thorsten. Transductive inference for text classification using support vector machines. In Pro-ceedings of the 16th International Conference on Machine Learning (ICML) , 1999b.
 Kaariainen, Matti. Generalization error bounds us-ing unlabeled data. In Proceedings of the 18th An-nual Conference on Computational Learning Theory (COLT) , 2005.
 Koltchinskii, V. Rademacher complexities and bound-ing the excess risk in active learning. Journal of Machine Learning , 11:2457 X 2485, 2010.
 Littlestone, Nick. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning , 2(4):285 X 318, 1988.
 Rigollet, Philippe. Generalized error bounds in semi-supervised classification under the cluster assump-tion. Journal of Machine Learning Research , 8, 2007. Rosenberg, David S. and Bartlett, Peter L. The rademacher complexity of co-regularized kernel classes. In Proceedings of the Eleventh International
Conference on Artificial Intelligence and Statistics (AISTATS) , 2007.
 Rosenblatt, Frank. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review , 65:386 X 407, 1958. Zhu, X., Ghahramani, Z., and Lafferty, J. Semi-supervised learning using gaussian fields and har-monic functions. In Proceedings of the 20th Inter-national Conference on Machine Learning (ICML) , 2003.
 Zhu, Xiaojin. Semi-supervised learning. Encyclopedia
