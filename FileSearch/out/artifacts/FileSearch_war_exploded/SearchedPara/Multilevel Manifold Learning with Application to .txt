 In the past decade, a number of nonlinear dimensionality reduction methods using an affinity graph have been devel-oped for manifold learning. This paper explores a multilevel framework with the goal of reducing the cost of unsuper-vised manifold learning and preserving the embedding qual-ity at the same time. An application to spectral clustering is also presented. Experimental results indicate that our multilevel approach is an appealing alternative to standard techniques.
 I.5.3 [ Pattern Recognition ]: Clustering; G.2.2 [ Graph theory ]: Graph algorithms; F.2.1 [ Numerical Algorithms and Problems ]: Computations on matrices Algorithms, Experimentation Multilevel Methods, Manifold Learning, Spectral Clustering
Real world high dimensional data can often be represented as points or vectors in a much lower dimensional nonlin-ear manifold. Examples include face databases, continu-ous video images, digital voices, microarray gene expression data, and financial time series. The observed dimensions is the size of the number of pixels per image, or generally the number of numerical values per data item, and can be characterized by far fewer features.  X  This work was supported by NSF grants DMS-0810938 and DMR-0940218 and by the Minnesota Supercomputing Insti-tute.

In the past decade, a number of algorithms have been developed to  X  X earn X  the low dimensional manifold of high dimensional data sets. Given a set of high dimensional data represented by vectors x 1 ,...,x n in R m , the task is to rep-resent these with low dimensional vectors y 1 ,...,y n  X  R d with d m , such that nearby points remain nearby, and distant points remain distant.

Multilevel techniques, which aim at reducing the prob-lem size and improving computational efficiency, have been successfully applied to various scientific problems, such as graph and hypergraph partitioning, e.g., [12, 13]. On the other hand, their incorporation into dimensionality reduc-tion methods is currently under-explored. Inspired by their success in other applications, we presented a graph-based multilevel scheme for linear dimensionality reduction [19]. Here we expand this work to a multilevel framework for nonlinear dimensionality reduction . The framework of these methods relies on an affinity graph and so it can be especially useful for affinity-graph-based manifold learning methods.
The multilevel framework proposed in this paper consists of three phases: data coarsening, nonlinear dimension re-duction, and data refining. To coarsen the data, we employ a graph coarsening algorithm based on maximum indepen-dent sets. After this, we project the coarsened data at the coarsest level using one of several known manifold learning methods. Finally, we recursively refine the data level by level, by solving a linear system to go from a given lower level to a higher level. The linear system comes from a least squares optimization which aims to preserve the closeness of data points between two adjacent levels.

Landmark versions of manifold learning algorithms by ran-dom sampling have been proposed to reduce the problem size and therefore the computational cost, e.g., Isomap [8], maximum variance unfolding [30], Locally Linear Embed-ding and Laplacian Eigenmaps [3]. These methods can be seen as learning eigenfunctions of some kernel. When the cost of a manifold learning algorithm is dominated by the spectral factorization of a symmetric matrix, an alternative way to reduce cost is via low-rank matrix approximation techniques [26].

The method proposed in this paper has three distinct properties from the landmark approach. First, maximum independent sets contain repelled points that potentially provide a better basis of the original data than random samples. Second, by recursive coarsening we obtain a suc-cession of graphs on which our refining scheme is based and this phase is independent of the dimensionality reduc-tion method. Third, the multilevel structure propagates the geodesic information into the coarsened graphs and this may be beneficial to some manifold learning algorithms.
Here we give an example showing that bad landmarks may result in an unsatisfactory embedding which can be pre-vented by our multilevel approach. Figure 1(a) presents a set of 1,000 points sampled on a Swissroll in three-dimensional space, with the embedding by Isomap given in Figure 1(e). In all we use a k NN graph with k = 12. The results by our multilevel-Isomap are in Figures 1(b) and 1(f) down to r =2 , 3 levels, respectively, where n r is the number of sam-ple points at the bottom level. The results by Landmark-Isomap are in Figures 1(c) and 1(g), where the landmarks are uniformly distributed which can be expected but not guaranteed by random sampling. The number of landmarks is denoted by n s . All these methods unfolded the Swissroll in a reasonable way. However, as shown in Figures 1(d) and 1(h), if the landmarks cluster in the left end, then the em-bedded points in the right end scatter. Such worst case scenarios can be prevented by our multilevel method. The details will be given in Section 3.
In this paper we consider three manifold learning algo-rithms: Isomap [27], Locally Linear Embedding (LLE) [18, 21], and Laplacian Eigenmaps [2], which are representative in manifold learning [22]. Note that our multilevel frame-work is not limited to these methods. It can be applied to virtually all affinity-graph-based manifold learning meth-ods, such as maximum variance unfolding [31], Hessian LLE [10], conformal Isomap [8], diffusion maps [7, 14], conformal eigenmaps [23], and minimum volume embedding [24].
Spectral clustering methods, e.g., [17], perform nonlinear dimensionality reduction on the input data, and apply a vector modeled clustering algorithm to the mapped data in the spectral space for classification. This type of methods is closely related to manifold learning. We will also show the application of our multilevel nonlinear dimensionality reduction technique to spectral clustering. Wesaythatagivenopenset X   X  R m in m -dimensional Euclidean space resides in a lower d -dimensional manifold (typically d m ), if there is a continuously differentiable function g : X   X  R m on an open domain  X   X  R d ,such that g ( X ) =  X . The parameterized manifold  X  = g ( X ) is called regular , if the Jacobian matrix J ( y )of g ( y )hasfull rank for all y  X   X , and g ( y ) does not self-intersect; i.e., y = y j implies g ( y i ) = g ( y j ). A regular manifold mapping g : X   X   X  has an inverse function f : X   X   X . Manifold learning methods attempt to find a function f that maps points in  X   X  R m into points of a lower dimension R d .In practice, we often have a discrete and possibly noisy sampled data x 1 ,...,x n  X  R m of  X , and the objective is to find the corresponding low dimensional embedding y 1 ,...,y n  X  . The goal of the mapping is to preserve the closeness of nearby points, for which an affinity graph G =( V, E ), normally a k NN graph, is employed.

In this paper, we use matrices X =[ x 1 ,...,x n ]  X  R m  X  n and Y =[ y 1 ,...,y n ]  X  R d  X  n ( d&lt;n ) to denote the origi-nal high dimensional data and the mapped low dimensional data, respectively. The column vector of ones is denoted by e . We also use integers 1 ,...,n to denote the vertices of the affinity graph G =( V, E ), i.e., V = { 1 ,...,n } .
Isomap [27] is a nonlinear generalization of the linear mul-tidimensional scaling (MDS). It replaces the Euclidean dis-tances in MDS by the geodesic distances approximated by an affinity graph G =( V, E ), whose vertices 1 ,...,n in V correspond to the input data x 1 ,...,x n  X  R m ,andedges in E define the closeness of them. The length of the short-est path between vertices x i and x j , denoted by  X  d ij approximate geodesic distance between them.

The algorithm can be summarized as follows. It starts by constructing an affinity graph, typically a k NN graph for the data. With this, the all-pair shortest path problem is solved and all the squared approximate geodesic distances  X  d 2 saved in a symmetric matrix e D  X  R n  X  n . The next step is to compute the Grammian matrix e B =  X  1 2 J e DJ  X  R n  X  n where J = I  X  1 n ee T  X  R n  X  n with I  X  R n  X  n the identity ma-trix and e  X  R n a column vector of ones. Then Isomap maps X =[ x 1 ,...,x n ]  X  R m  X  n nonlinearly to Y =[ y 1 ,...,y R d  X  n by minimizing e B  X  Y T Y F .Tobeprecise,denote by  X  i  X  R and v i  X  R n the i th eigenvalue and eigenvector of e
B in decreasing order. Let  X  d  X  R d  X  d be the diagonal matrix formed by  X  1 ,..., X  d , and the columns of V d  X  R n  X  d be v 1 ,...,v d . The mapped data is Y = X  1 / 2 d V T d  X  R d  X  n
Locally linear embedding (LLE) [18, 21] maps the high dimensional input data x 1 ,...,x n  X  R m to y 1 ,...,y n in a lower dimensional space (i.e., d&lt;n ) by three steps.
First, a k NN graph is constructed. Second, the recon-struction weights W =[ w ij ]  X  R n  X  n are obtained by mini-mizing the cost function: subject to that w ij =0if x j is not one of k nearest neigh-bors of x i ,and x squares problem for each i =1 ,...,n . Finally, the high dimensional data X =[ x 1 ,...,x n ]  X  R m  X  n is mapped to the low dimensional data Y =[ y 1 ,...,y R d  X  n by minimizing the embedding cost function: Two constraints are added for the problem to be well-posed. First, it is required that the projected data be centered, i.e., P i =1 y i = 0. Second, the mapped data, subject to scaling, must have unit covariance, i.e.,
Let M =( I  X  W ) T ( I  X  W ). Then Me =0,where e is the column vector of ones. Therefore, e is a eigenvector of M associated with the smallest eigenvector 0. Other eigen-vectors v satisfy v T e = 0. The embedding is formed by the d right singular vectors of I  X  W associated with the second to the ( d +1)st smallest singular values.
In Laplacian eigenmaps, a k NN graph of X =[ x 1 ,...,x n R m  X  n is also constructed. The low dimensional embedding Y =[ y 1 ,...,y n ]  X  R d  X  n is the minimizer of the cost func-tion:  X ( Y )= where W =[ w ij ] is a symmetric weight matrix, D is a diag-onal matrix with d ii =
A popular weighting scheme is the Gaussian weights: for each pair of neighboring points x i ,x j ,where t&gt; 0isa preset parameter. This weighting scheme is also called heat kernel in [2]. In our experiments we set t equal to the median of x i  X  x j 2 2 of all neighboring points x i ,x j . Setting  X  = in (4), we obtain the induced binary weighting method.
To make the minimization of (3) well-posed, the con-straints YDY T = I and YDe = 0 are imposed, where e is the column vector of ones. The problem is transformed to solving the generalized eigenvalue problem ( D  X  W ) z =  X Dz , whose d generalized eigenvectors corresponding to the sec-ond to the ( d +1)st eigenvalues form Y . The bottom gener-alized eigenvector e associated with eigenvalue 0 is ignored.
This section presents our multilevel framework for nonlin-ear dimensionality reduction for manifold learning. This ap-proach consists of three phases: data coarsening, nonlinear dimension reduction, and data refining. Figure 2 provides an illustration. In a nutshell, a few levels of coarsening are per-formed leading to a sequence of smaller and smaller graphs. The analysis of the data is done at the coarsest level using a standard dimension reduction technique such as Isomap, LLE, or Laplacian eigenmaps. Then an  X  X ncoarsening X  step of this low dimensional data is performed backing up to the highest level. Details are provided next.
Coarsening a graph G =( V, E ) means finding a  X  X oarse X  approximation b G =( b V, b E )thatrepresents G =( V, E ), Figure 2: A sketch of the multilevel nonlinear di-mensionality reduction. where | b V | &lt; | V | . By recursively coarsening we obtain a succession of smaller graphs which approximate the original graph G .

For graph coarsening steps we used maximum indepen-dent sets, which have been in use for multilevel graph par-titioning [1, 5]. Connectivity of an affinity graph is impor-tant to many manifold learning algorithms but coarsening by maximum independent sets does not guarantee that the coarse graph is connected. However, Algorithm 1 visits the vertices in a special order to build the maximum indepen-dent set, so that it preserves the connectivity of the graph in the coarsening stage. This is now explained.
 Input: Graph G =( V, E )with V = { 1 ,...,n } .

Output: The coarsened graph b G =( b V, b E ). b
V  X  X  X  maximum independent set b U  X  X  X  complement set of b V
Randomly pick k 0  X  V ; S  X  X  k 0 } . (  X  ) repeat until S =  X  b
E  X  X  X  edge set of b G for all i, k  X  b V do end for Algorithm 1: Graph coarsening by a maximum indepen-dent set.

Consider the steps of Algorithm 1 to compute the coarse graph b G =( b V, b E ). We claim that for each vertex k added to b
V , other than the very first element k 0 added to S ,there exists a path consisting of edges in b E linking vertices k and k . We now prove our claim by induction. All vertices in b
V are from S in (*) and added in (**). Each element k ever in S , except the very first k 0 in (  X  ), is added to S in (  X  X  ), where there exist ( i, j ) , ( j, k )  X  E with i already in b V . Since there is a path i  X  j  X  k in the fine graph, if k is added into b V in some later iteration, then there will be an edge ( i, k )  X  b E in the coarse graph as instructed by the bottom part of the algorithm. Assuming that previous vertices added to  X  V satisfy our claim, there exists a path consisting of edges in b E linking k 0 and i , unless i = k ( i, k )  X   X  E , k 0 also links to k via a path in the coarse graph. This proves our claim by induction. Therefore, the coarse graph b G =( b V, b E ) is guaranteed to be connected under the condition that the original graph is.

Algorithm 1 provides an affinity graph b G =( b V, b E )ofthe coarse level. Therefore, it is not necessary to compute a k NN graph for the graphs obtained at each level. In addition, we need the distances between nearby points in the coarse graph in the following two situations. First, some manifold learning algorithms, such as Isomap, need distances between nearby points to compute the mapping. Second, the mul-tilevel refining stage, to be described later, will require the edge weights, and some weighting schemes, such as Gaussian weights, depend on the distances between nearby points.
We use  X  and  X   X  to denote the distances at the fine and coarse levels, respectively. Given ( i, j )  X  b E , one can simply use the actual distance  X   X  ( x i ,x j )= x i  X  x j 2 for the coarse level. Alternatively, we define Then distance computations are avoided at the coarse level. More importantly, if we compute distances by (5) at all levels, the computed distances indeed approximate geodesic distances. This is especially important to Isomap which aims at preserving geodesic distances.

By recursively coarsening the graph, we obtain a succes-sion of graphs G 1 ,G 2 ,...,G r ,where G i =( V i ,E i )isthe coarse graph of level i for i =1 ,...,r ,and G r is the coars-est level graph. The corresponding data sets are denoted by matrices X i  X  R m  X | V i | for i =1 ,...,r .
Given a data set X =[ x 1 ,x 2 ,...,x n ]  X  R m  X  n , a dimen-sionality reduction algorithm produces Y =[ y 1 ,y 2 ,...,y R d  X  n ( d&lt;m ) such that Y preserves certain features of X . In our multilevel framework, presented in Figure 2, we apply a dimensionality reduction method to the data set X r  X  R m  X | V r | of the coarsest level ( r th level), and obtain aset Y r  X  R d  X | V r | ( d&lt;m ). The dimensionality reduction methods considered for this task are affinity-graph-based, such as Isomap, LLE, and Laplacian eigenmaps, where the graph from the multilevel framework is used. Recall that it is not necessary to build a k NN graph at the coarsest level.
Note that Isomap and Laplacian eigenmaps use an undi-rected affinity graph (i.e., applying symmetrization to a k NN graph), whereas LLE uses a directed affinity graph (i.e., a k NN graph w/o symmetrization). In our multilevel frame-work the affinity graph is undirected, regardless of the di-mensionality reduction method applied at the bottom level.
The objective of the refining phase is to obtain a reduced representation Y  X  R d  X  n of the data X  X  R m  X  n ,where n = | V 1 | , at the finest level, starting from the reduced repre-level ( r th level).

We refine the data level by level in the low dimensional space as follows. We denote by G =( V, E )and b G =( b V, the two graphs of the k th and ( k +1)st levels, respectively. For each level k = r  X  1 ,r  X  2 ,..., 1, we recursively build the reduced representation Y of the k th level from b Y of the ( k +1)st level in a low dimensional space, by minimizing where W =[ w ij ] is a symmetric weight matrix; each en-try w ij is nonzero only if the vertices i, j are adjacent (i.e., connected by an edge).

Yet not specified are the weights between nearby data points. We adopt the the Gaussian weighting scheme w ij = tion  X  ( x i ,x j ) between x i and x j can be the Euclidean dis-tance x i  X  x j 2 as that in (4). With our multilevel frame-work we use the approximate geodesic distance (5) across all levels. When  X  =  X  , we obtain the binary weights, i.e., w ij = 1 for all adjacent vertices and otherwise 0.
We denote the vertex set of the coarse level by b V  X  V , and its complement by b U = V \ b V . We rewrite (6) as The first term of (7) can be written as where Y 1  X  R d  X | b U | includes the points to be determined in Y , W 1  X  R | b U | X | b U | is the matrix of edge weights with edges connecting points in Y 1 ,and D 1  X  R | b U | X | b U | is the diagonal matrix whose entries are the row/column sums of W 1 .
The second term of (7) is a constant, since it depends only on points in Y 2  X  R d  X | b V | that have been already determined at the coarse level.

The term of (8), after some algebra, can be written as where W 12  X  R | b U | X | b V | is the matrix of edge weights with edges connecting points to be determined (i.e., indexed by b U ) and those already determined (i.e., indexed by b V ), D
U | X | b U | is the diagonal matrix whose entries are the column sums of W 12 ,and C is a constant.
Putting the expressions (9) and (10) together back into (7), we obtain a quadratic function: 2trace[ Y 1 ( D 1  X  W 1 + D 12 ) Y T 1 ]  X  4trace[ Y 2 W T plus a constant term. To minimize E , we set the partial derivatives of (11) to zero, and obtain the equation where L 1 = D 1  X  W 1  X  R | b U | X | b U | is the Laplacian matrix of thepointstobedetermined.

Two remarks must be made. First, L 1 is symmetric and diagonally dominant with a positive diagonal. By the Ger-shgorin circle theorem, L 1 is positive semidefinite. It also has the smallest eigenvalue 0 associated with eigenvector e , the column vector of ones. D 12 is diagonal with nonnega-tive entries. Therefore, the objective function (11) is convex, and Y 1 is the minimizer if and only if (12) holds. Second, our data coarsening method is based on maximum indepen-dent sets, and we refine the mapping level by level. Hence each undetermined vertex i  X   X  U has at least one determined neighbor j  X   X  V associated with a positive weight w ij &gt; 0. So D 12 has a positive diagonal. Recall that L 1 is symmetric positive semidefinite. By a theorem of Weyl [25, Corollary 4.9], stated below, L 1 + D 12 is positive definite and therefore nonsingular. Thus, the solution to the linear system (12) is unique, and so is the minimizer of (11).

Theorem 1 (Weyl). Let A , B be two n  X  n Hermitian matrices and  X  k ( A ) ,  X  k ( B ) ,  X  k ( A + B ) be the eigenvalues of A , B ,and A + B arranged in increasing order for k = 1 ,...,n . Then for k =1 ,...,n , we have
Putting the points as columns of Y 1 (i.e., indexed by b U ) and those already determined as columns of Y 2 (i.e., indexed by b V ) together, we obtain the reduced representation of the finer level (i.e., vertices indexed by b U  X  b V ). By recursively re-fining the data this way, we obtain a reduced representation of the original data.
In this section we illustrate the application of the proposed multilevel manifold learning scheme. We use the three non-linear dimensionality reduction methods, Isomap [27], LLE [21], and Laplacian eigenmaps [2], and the versions of the multilevel algorithms which incorporate these techniques at the coarsest level as described earlier.

All experiments were performed in Matlab in sequential mode on a PC equipped with a four-core Intel Xeon E5504 @ 2.0GHz processor. The k NN graph construction is by a brute-force algorithm, which can be improved by an ap-proximation algorithm [6]. We used a C/C++ implemen-tation of Dijkstra X  X  algorithm [9] by John Boyer to solve the all-pair shortest path problem, which arises in Isomap and multilevel-Isomap. We also implemented Algorithm 1 for graph coarsening in C/C++. For all eigencomputations, we used the Matlab routine eigs which invokes the Fortran library ARPACK [15].

Since Algorithm 1 for coarsening the data is randomized, we report the average numbers from 100 random runs for each data set, each method, and each level r =2 , 3 , 4in Tables 1 and 2, which display the average number of images at each coarsening level, and the average CPU time used for graph coarsening, processing for dimensionality reduc-tion, and data refining. For all methods, processing time in-cludes the time used for eigencomputation. For Isomap and multilevel-Isomap, processing time also includes the time to compute the geodesic distances; the time for computing the all-pair shortest distances is also reported as the braced number. For LLE, it includes the time to obtain the recon-struction weights.
In order to compare the quality of the nonlinearly mapped data, we adopt two embedding evaluation metrics, the trust-worthiness and continuity of the proximity re lationships of data entries [28, 29].

Let x 1 ,...,x n be the points in the high dimensional space, and y 1 ,...,y n be the mapped points in the low dimensional space. Denote by r ( i, j ) the rank of x j in the ordering ac-cording to the distance from x i . The longest vertex x j x i has r ( i, j ) = 1, and the shortest vertex x j from x i r ( i, j )= n  X  1. Likewise, denote by  X  r ( i, j ) the rank of y the ordering according to the distance from y i . The trust-worthiness is defined by where U p ( i ) contains the indices of p nearest neighbors of y in the low dimensional space. The continuity is defined by where V p ( i ) contains the indices of p nearest neighbors of x in the high dimensional space.

The higher the trustworthiness or continuity, the better the manifold mapping. Both T ( p )and C ( p ) are bounded above by 1. The upper bound 1 is reached if and only if U ( i )= V p ( i )for i =1 ,...,n , which means that the p near-est neighbors for each data entry in the high dimensional space coincide with those in the low dimensional space.
The Frey Face data set [21] 1 contains 1,965 face images of a single person, Brendan Frey, taken from sequential frames of a small video. Each image is of size 20-by-28 in grayscale, and hence in 560-dimensional space after vectorization.
We report the result using a k NN graph with k =12 and embedding dimensions d = 2. In our multilevel frame-work Figure 3 illustrates the two-dimensional mappings of the these images obtained by LLE and multilevel-LLE. We can observe that all plots exhibits two intrinsic attributes, i.e., pose (left-right) and expression (serious-happy), which are correlated with the coordinate axes. This property is also more or less reflected in the plots by Isomap, multilevel-Isomap, Eigenmaps, and multilevel-Eigenmaps, which are not shown to save space.

The average computation time for manifold learning, dis-played in Table 1, consists of four parts: the k NN graph construction, embedding process, and for multilevel methods the graph coarsening time and data refining. Our multilevel http://cs.nyu.edu/~roweis/data.html technique reduced the computation time significantly. The savings with r = 2 levels for Isomap, LLE, and Eigenmaps are about 80.2%, 42.6%, and 12.2%, respectively. Using more levels resulted in more time savings.

Figure 4 displays the plots of trustworthiness and conti-nuity as a function of p , the size of the neighborhood used in measuring them, where we set the number of levels up to four. Our multilevel technique improved Isomap and LLE in both computation time and embedding quality, while multilevel-Eigenmaps performed comparable to Eigenmaps using this data set.
The Labeled Faces in the Wild (LFW) data set [11] 2 includes 13,233 images of size 250-by-250 in RGB color of 5,749 unique individuals. We resized these images to 50-by-50 and converted them to grayscale for manifold learning experiments. Figure 5 lists the sample images of four indi-viduals: Naoto Kan (1-4 images in row 1), Sally Field (5-8 images in row 1), Helen Clark (1-4 images in row 2), and Gilberto Rodriguez Orejuela (5-8 images in row 2).
We report the experimental results using a k NN graph with k = 6 and embedding dimensions d =2. Intheexper-iments on this data set, some of the vertices in the coarse http://vis-www.cs.umass.edu/lfw/ Figure 4: Trustworthiness and continuity of Frey Face database. graphs have their numbers of neighbors increased signifi-cantly. However for LLE and Eigenmaps, the number of neighbors per vertex is kept modest to preserve the  X  X ocal-ity X  in the embedding. Therefore, we trimmed the graph at the coarse level such that each vertex has at most k =6 neighbors, i.e., outgoing edges. This results in an unsym-metric affinity matrix for LLE. For Eigenmaps we applied symmetrization as before. We did not apply this step for the global method Isomap.
 Figure 6 illustrates the two-dimensional mappings of the LFW data set, using Eigenmaps and multilevel-Eigenmaps, where sample images listed in Figure 5 are displayed. To some extent, these images are clustered into four groups of the individuals, and our multilevel method preserved their relative locations in the mapping. On the other hand, the mapped data tends to lose cohesiveness when the number of levels increases.

Table 2 reports the average computation time for mani-fold learning, which consists of four parts: the k NN graph construction, embedding process, and for multilevel meth-ods the graph coarsening time and data refining. Note that our multilevel technique generally achieved significant sav-ings in CPU time. Using r = 2 levels, our multilevel tech-nique achieved about 57.3% savings in computation time for Isomap, 11.0% savings for LLE, and 5.4% savings for Eigen-maps. Omitting the time for k NN graph construction, the savings were 77.0%, 69.4%, and 62.9% for Isomap, LLE, and Eigenmaps, respectively.

Figure 7 displays the plots of trustworthiness and conti-nuity as a function of p , the size of the neighborhood used in the measurement, where we set the number of levels up to four. In this experiment our multilevel technique improved LLE and Eigenmaps, and multilevel-Isomap performed com-parably to Isomap. Figure 7: Trustworthiness and continuity of the LFW data set.
Given set of points X =[ x 1 ,x 2 ,...,x n ]inEuclideanspace, the objective of clustering is to partition it into a certain number of subsets, called clusters, which are as distinct as possible. The K-means algorithm, as one of the best-known clustering methods available, (locally) minimizes the quan-tization error: where s ( i ) is the index of the cluster to which x i belongs, and c ( j ) is the prototype, e.g., the centroid of cluster j .
The performance of K-means clustering can be improved by a spectral clustering algorithm in [17], which is presented in Algorithm 2. { Given x 1 ,  X  X  X  ,x n  X  R m , partition them into K clusters.
Form a symmetric affinity graph G =( V, E )of x 1 ,...,x n
Compute W =[ w ij ]  X  R n  X  n with w ij =exp(  X  x i  X  Compute diagonal D =[ d ii ]  X  R n  X  n with d ii = Compute the normalized matrix A = D  X  1 / 2 WD  X  1 / 2 .
Compute the K eigenvectors v 1 ,...,v K of A correspond-Form Y =[ y 1 ,...,y n ]=[ v 1 ,...,v K ] T  X  R K  X  n . Normalize  X  y i = y i / y i 2 for i =1 ,...,n .

Apply the K-means clustering, initialized randomly, to Assign x i to cluster j if  X  y i was assigned to cluster j . Two observations on Algorithm 2 deserve noting. First, I  X  A = D  X  1 / 2 LD  X  1 / 2 ,where L is the graph Laplacian, and A and I  X  A share the same eigenvectors with eigenvalues changed form  X  i to 1  X   X  i . Therefore, the mapping to Y is indeed performing normalized Laplacian Eigenmaps. Sec-ond, the normalized Laplacian Eigenmaps multiplies each of the Eigenmaps points y 1 ,...,y n by a constant number. However, Algorithm 2 applies  X  y i = y i / y i for i =1 ,...,n , the previous scaling is ineffective. Therefore, we can simply apply the Laplacian Eigenmaps without dropping the bot-tom eigenvector, followed by the normalization  X  y i = y for i =1 ,...,n .
 Both K-means clustering and the clustering algorithm in Algorithm 2 have the same drawback, that it is sensitive to initialization. Random initialization could yield poor re-sults in extreme cases, which may be avoided to some extent by a structured initialization scheme utilizing our multilevel technique. The procedure is sketched next.
 We first recursively apply the graph coarsening method in Algorithm 1, and obtain a succession of graphs G i =( V i for i =1 ,...,r ,where V 1 = { 1 ,...,n } is the set of indices of the given data x 1 ,...,x n . Then we cluster the data points at the bottom level by Algorithm 2, where we also obtain the mapped data  X  Y r in the spectral space.

We also refine the data level by level. However, since the mapped data are expected to unit norm. We normalize the newly mapped points from solving (12) in each data refining level. This normalization step indeed matches the relaxation theory for multiclass clustering [32]. We obtain a succession of sets of low dimensional points  X  Y r ,..., column norms are all ones). For each level i = r  X  1 ,r  X  2 ,..., 1, we still do the K-means clustering to the normalized  X  Y , initialized by the clustering centroids at ( i + 1)st level. The pseudocode is given in Algorithm 3. { Given x 1 ,  X  X  X  ,x n  X  R m , partition them into K clusters.
Form a symmetric affinity graph G =( V, E )of x 1 ,...,x n Form a succession of graphs G 1 ,...,G r by Algorithm 1.
Apply Algorithm 2 to the data points at the bottom level for i = r  X  1 ,..., 1 do end for { The mapped data at top level are denoted by  X  y 1 ,...,  X  y Assign x i to cluster j if  X  y i was assigned to cluster j .
Algorithm 3: A multilevel spectral clustering algorithm.
We compare empirically the performance of three cluster-ing algorithms discussed in Section 5. 1. K-means clustering. 2. Spectral clustering (Algorithm 2). 3. Multilevel spectral clustering (Algorithm 3).

In all experiments, we assume the number of classes is knowninadvancewhichisusedasthenumberofclusterings in the three clustering algorithms. We also used r = 2 levels in the multilevel spectral clustering algorithm in all tests.
The quality of clusters are evaluated by purity and entropy [33]. For each algorithm and each data set, we report the average of the results from 100 random initializations. The purity and entropy are defined as: entropy = where K is the number of clusters, n j i is the number of en-tries of class j in cluster i ,and n i is the number of data entries in cluster i .
 Both purity and entropy are bounded between 0 and 1. The larger the purity, or the smaller the total entropy, the better the performance. The optimal value 0 of entropy and the optimal value 1 of purity are met, if and only if the clusters match exactly the classes.
 We have used several data sets for clustering experiments. Our multilevel spectral clustering algorithm does not always yield better cluster quality than the spectral method. How-ever, we have observed the expected computational savings on clustering larger data sets. For example in clustering the USPS digit set 3 which consist of 11,000 digit images, we obtained 22.1% CPU time savings (excluding k NN graph construction time in the comparison). http://cs.nyu.edu/~roweis/data.html
Now we report the results of our experiments on three face image databases, Yale faces, ORL faces [20], AR faces [16], and one digit image databases, USPS digits. These images are all in grayscale. The sample images are displayed in Figures 8-11. The results of clustering ex periments are summarized in Table 3. As can be observed, the K-means clustering was outperformed by the spectral clustering which was further improved by our multilevel technique, except for the USPS database we got the worse entropy.

The last experiment we report is on a collection of docu-ment sets J1-J11 from [4]. All these 11 sets consist of the same 185 documents but differ in the number of key words from 183 to 10,536, where J1 contains all words and forms a matrix of size 10,536-by-185. Each document has been as-signed by hand a label according to its topic. There are 10 different labels (topics) in total. The clustering results are presented in Figure 12. In all we normalized the document vectors and used a k NN graph with k = 4. Multilevel spec-tral clustering gave better results than spectral clustering in 7 of the 11 cases. Both methods outperformed the K-means algorithm significantly. The details are omitted.
The class of multilevel nonlinear dimension reduction meth-ods for manifold learning presented in this paper aim at re-ducing cost without sacrificing accuracy. Experiments indi-cate that the proposed multilevel framework usually reduces the computational cost of some existing methods for mani-fold learning, while yielding comparable or better results.
We have shown an application of the method to spectral clustering, by incorporating our multilevel technique with a spectral clustering algorithm for structured initialization. Experiments show that this may result in an improvement in clustering quality.

The multilevel nonlinear dimension reduction techniques represented in this paper are based on maximum indepen-dent sets, which may result in rapid coarsening that affect the performance. Future work will investigate the use of other coarsening techniques (e.g., based on maximal match-ing) to alleviate this behavior. [1] S. T. Barnard and H. D. Simon. A fast multilevel [2] M. Belkin and P. Niyogi. Laplacian eigenmaps for [3] Y. Bengio, J.-F. Paiement, P. Vincent, O. Delalleau, [4] D. L. Boley. Principal direction divisive partitioning. [5] T. Chan, B. Smith, and J. Zou. Multigrid and domain [6] J. Chen, H.-r. Fang, and Y. Saad. Fast approximate [7] R. R. Coifman and S. Lafon. Diffusion maps. Appl. [8] V. de Silva and J. B. Tenenbaum. Global versus local [9] E. W. Dijkstra. A note on two problems in connexion [10] D. L. Donoho and C. Grimes. Hessian eigenmaps: [11] G. B. Huang, M. Ramesh, T. Berg, and [12] G. Karypis and V. Kumar. Multilevel k -way [13] G. Karypis and V. Kumar. Multilevel k -way [14] S. Lafon and A. B. Lee. Diffusion maps and [15] R. B. Lehoucq, D. C. Sorensen, and C. Yang.
 [16] A. M. Mart  X   X nez and A. C. Kak. PCA versus LDA. [17] A. Y. Ng, M. Jordan, and Y. Weiss. On spectral [18] S. T. Roweis and L. K. Saul. Nonlinear dimensionality [19] S. Sakellaridi, H.-r. Fang, and Y. Saad. Graph-based [20] F. S. Samaria and A. C. Harter. Parameterisation of a [21] L. K. Saul and S. T. Roweis. Think globally, fit locally: [22] L. K. Saul, K. Q. Weinberger, J. H. Ham, F. Sha, and [23] F. Sha and L. K. Saul. Analysis and extension of [24] B. Shaw and T. Jebara. Minimum volume embedding. [25] G. W. Stewart and J.-g. Sun. Matrix Perturbation [26] A. Talwalkar, S. Kumar, and H. A. Rowley.
 [27] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A [28] J. Venna and S. Kaski. Neighborhood preservation in [29] J. Venna and S. Kaski. Local multidimensional [30] Q. Weinberger, B. D. Packer, and L. K. Saul. [31] K. Q. Weinerger and L. K. Saul. Unsupervised [32] S. X. Yu and J. Shi. Multiclass spectral clustering. In [33] Y. Zhao and G. Karypis. Empirical and theoretical
