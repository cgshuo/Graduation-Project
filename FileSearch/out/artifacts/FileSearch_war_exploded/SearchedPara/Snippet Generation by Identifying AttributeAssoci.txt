 It has become common for peop le to access search engin es to gather information about entities such as restaurants to find the entity that fits the user X  X  need. For example, the user chooses his/her destination from entities whose types are  X  X estaurant near travel destination X  by comparing the information of price from documents recovered by the search engine. In another example, before buying a cleaner, the user selects the entity, who se types are cleaner, that offers the best performance. In this way, it has become popular to use web search results to select an entity from multiple entiti es of the same type according one or more attributes of the entities. We call this search task attribute-oriented entity search .
When a user searches for a specific entity type, the user inputs a type query and an attribute query into a web search engine [13]. Type query represents the type of entities (e.g.,  X  X estaurants X  or  X  X otels X ) or geographic-oriented information (e.g.,  X  X yoto X ). Attribute query represents the demand for specific attribute(s) of each entity to allow selection, fo r example  X  X rice X  or  X  X tmosphere X .
When users conduct a search, they first indicate the target by type query, then find the best entity by comparing with the information returned against the attribute query. For example, if the user wants a restaurant with nice atmosphere in Kyoto, he/she inputs  X  X yoto restaurant X  as the type query and  X  X tmosphere X  as the attribute query.

Entity search engines are not effective for this task because they target only structured information. Unfortunately, most documents that contain the infor-mation about a wide range of entity types and attributes are unstructured. Document authors use various words to express the information about the at-tributes of entities. For example,  X  X om antic X ,  X  X uiet and calm X  or  X  X oisy X  are terms used to express the atmosphere of restaurants. This makes it difficult to extract and organize attribute information from the web.

The results of an attribute query should be shown in free format. This is because information that is written in free format such as  X  X tmosphere X ,  X  X har-acters of staff X  may be lost by structuring. For example, though one restaurant is described as having  X  X eautiful night view X  and the other as  X  X e can see a famous spot from the windows X  in web documents, both restaurants are stuck with the same tag of  X  X cenic View X  after being structured. Users have to check the information by clicking each docu ment, because the tag of  X  X cenic View X  has very poor discrimination power. Mor eover, the degree of attractiveness of the view is stripped from structured documents. Thus, our goal is to use un-structured documents on the we b to satisfy the user X  X  needs.

In this paper, we use the method of query-biased summarization to generate effective snippets of search results [1]. These snippets allow users to compare entities by looking at the information associated with the attribute query. They also allow users to select the best document. In their work, they used the objec-tive function that consists of two factors; Fidelity model and Relevance model . Fidelity model summarizes documents so that users can imagine the contents of the original document. Relevance model confirms that the document contains thetypequery.

In the framework of our method, the conventional relevance model can be applied to type queries without any modification. Users are likely to compare entities of the same type as the input type query. Therefore, type query should be contained directly in the snippets. This requirement of a snippet corresponds to the function of the relevance model in the conventional method. However, this previous framework is ineffective for attribute queries because of the following issue. The words used in the attribute query may not be written in all documents. For example,  X  X ight X  can be expressed by words such as  X  X iew X ,  X  X cenic location X  or sentences such as  X  X he scenery is beaut iful. X ,  X  X he interior design is good. X . In a such case, the probability of the relevance model is set to be zero since the attribute query does not directly appear in the document. This makes the total probability be zero and thus the conventional method generates messy snippets as a result.

We resolve this problem by introducing a novel probabilistic model for at-tribute queries; it allows appropriate snippets to be generated in terms of both type and attribute queries. We refer to the probabilistic model as the ambigu-ous relevance model in this paper. The ambiguous relevance model uses a lot of words that are similar or related to the input attribute query to calculate the probability. Because of this, our method can generate snippets that contain information about an attribute query but written in a wide variety of words.
In this study, we assume that the type of input queries can be distinguished automatically. That is, the system can treat a type query and an attribute query separately. We consider that it is possible to identify the type of the input query automatically by modifying conventional methods since several studies [8,9,15] achieve stable accuracy in extracting entities and contexts (i.e., attribute queries in this paper) from a given query.

Our method makes it possible to generate snippets that contain not only type query responses but also the information that best satisfies the attribute query. Experiments that compare the proposed method with a conventional method show that our method can generate such snippets.

The major contributions of our research are:  X  We introduce a novel probabilistic model, ambiguous relevance model ,into  X  We propose an implementation of the ambiguous relevance model. It expands  X  We verify that our method can generate better snippets in terms of infor-There are many kinds of queries that user input to entity search engines. Pound [12,13] said that there are four kinds of queries: entity query, type query, attribute query, and relation query. In attribute-oriented entity search, a user inputs type query(ies) and attribute query(ies) to find the best-fit entity. Thus, we target type query and attribute query in this paper. Here, the definitions of type query and attribute query are identical to those of Pound X  X  [12,13].

The conventional ranking method for entity search can be useful for ranking in attribute-oriented entity search. Dalton [4] proposed a ranking method for entity search based on the four kinds of queries proposed by Pound. We consider that the method would have a similar effect when it targets only two queries for rank-ing in attribute-oriented entity search. We suppose that the system for attribute-oriented entity search use the conventional method for ranking. Therefore, we preferentially addr ess the problem of generating s nippets of attribute-oriented entity search results in this paper.

This work is related to previous studies that treat search users X  intents. Broder [3] outlined three main user intents: informational, navigational, and transac-tional. Rose [14] provided more subcategories in 11 finer-grained intents, and Yin [18] proposed a method to organize taxonomies of phrases created to ex-press these intents. Lin [6] addressed the problem of finding actions that can be performed on entities. Jain [5] proposed a method for open-domain entity extrac-tion and clustering over query logs. Pantel [10,11] estimated what the user was searching for by classifying the user X  X  intent into 70 types like Songs, Newspaper or Place based on the entity included in queries and the words input before and after the entity. All these studies focused on entity query to estimate user X  X  intent by using query log data. Our method treats the attribute query as indicative of the user X  X  intents. This is differ from previous research.

Snippet generation can be considered as the task of query-biased summa-rization [16]. Tombros [16] proposed a rule-based query-biased summarization method that gives a different score to each term in a document to generate a summary that contains relevant terms. Several studies have used the machine learning approach to realize query-biased summarization. Wang [17] formulated query-biased summarization as the task o f classifying sentences in a document. Metzler [7] uses supervised machine learning to resolve the query-biased sum-marization task as a sentence ranking problem. Recently, it has become common to find the best combination of sentences/words that maximizes a pre-defined objective function. There are two tasks in the approach. The first task is how to model a suitable objective function. The second task is to develop a search algorithm that maximizes the objective function. In this paper, we tackle the first task by defining an objective function that reflects both type query and attribute query to generate an appropriate snippet in terms of the input queries. In the experiment conducted to verify the effectiveness of the objective function, we use greedy search as the search algorithm, but other search algorithms could be used. We propose a method to generate snippets that reflect type and attribute queries separately. Our method is based on a conventional method for query-biased sum-marization. In 3.1, we show the conventional method. Then, in 3.2, we describe a new model as an extension of the conventional model to treat attribute queries. 3.1 Probabilistic Models of Query-Biased Summarization Berger [1] proposed a probabilistic method to generate a query-biased summary from a document and a given query consisting of one or more terms. The query-biased summary is a subset of sentences in the document. The method chooses the best subset that maximizes the objective function. The objective function in Berger X  X  method is modeled as a product of two probabilistic models; the fidelity model and the relevance model.

Candidates are sentences in the document. Given document d and query q , query-biased summary s  X  is created by Each summary is scored by these two probabilistic models: fidelity model P ( s | d ) and relevance model P ( q | s ).
 The fidelity model: The fidelity model represents snippet accuracy. A summary should have content that accurately out lines the document because it is used as a substitute for the document. The model adopts P ( s | d ) as the fidelity measure, that is, how well the words in the candidate summary express the content of the document. The method models fidelity by the multinomial distribution of word frequency in the candidate summaries generated from document d . p i = k i /n , where n is the total number of words in d . Candidate summary s contains words which appear each c 1 , ..., c | c | times in s . Probability p i is calculated by maximum likelihood estimation, i.e., p i = k i /n ,where k i is the occurrence number of the i -th word in document d . Fidelity score follows a multinomial distribution where each word is chosen c i times in p i ,and C  X  | c | i =1 c i .
 The relevance model: The relevance model represents the relatedness between query and snippets. If a part of the document is related to the query while the others are not, the part is expected to contain more query terms than the other parts in the document. Thus, it is proper that the summary contains as many query terms as possible. P ( q | s ) indicates how well the summary is related to the query. The method models P ( q | s ) by a multinomial distribution of frequency of query terms q in summary s .Ifthewordsoftypequeryappear the relevance model follows a multinominal distribution when each query word appears t i times with probability p i = k i /n in the string of words of the candidate summary consisting of n words.
 As mentioned in Section 1, it is not effective to treat type and attribute queries the same way in this method. 3.2 Extended Probabilistic Model for Attribute Queries We extend the conventional model to treat attribute queries independent of type queries to generate more inf ormative snippets. Our model uses the conventional relevance model to calculate the score of type query responses. We note that the fidelity model is independent of the users X  input.
 First, we show how to add a new probabilistic model for attribute queries. The snippet that best reflects type and attribute queries is s  X  given type query q , attribute query a , and a document d . We derive the conditional probability in a similar way to Berger X  X  method. Applying Bayes X  theorem, is also the same value as P ( q | s ), because q is independent of a and d . Since the denominator is independent of s , Equation (5) is approximated as This model assumes that the snippet with the highest product of these three prob-abilities is the most suitable. The difference between our method and Berger X  X  method is the use of P ( a | s )inEquation(6).

P ( a | s ) expresses how much information snippet s contains that is relevant to attribute query a .Inthispaper,wecallthismodelthe ambiguous relevance model . The model calculates the score of all sen tences in each candidate snippet. Given attribute query a that contains attributes a 1 ,..., a | a | and candidate snippet s that contains the sentences l 1 ,..., l | l | , the ambiguous relevance model is given by where S ( a x ,l y ) is the similarity between sentence l y in s and the attribute query a x in a , Z is the normalizing constant. If there are multiple attribute queries in a query, it is virtually impossible to find one sentence that contains information about all attribute queries. To resolve this problem, the model considers only one attribute, the one that has the highest score among all attributes for the sentence, in calculating S . That is, we use as the probability of the ambiguous relevance model.

Here, we consider the fact that that words in the attribute query are frequently not written in the documents. For example, if the attribute query includes the word  X  X ight X , the document sentence  X  X ight view was beautiful. X  is meaningful with regard to the attribute query. Because this sentence doesn X  X  contain the word  X  X ight X , the system cannot understand that this sentence has information relevant to the attribute query. Because of this, we use not only direct word matches but also word similarity. However, while similarity is more effective than direct word matching, it still fails to extract the greatest possible amount of relevant information. For example, the sentence  X  X e can see the blue sea X  contains information relevant to sight. Though the most meaningful word  X  X ea X  is clearly associated with sight,  X  X ea X  is not a synonym of  X  X ight X  and so this sentence would not normally be part of the snippet. To counter this problem, we expand the semantic space of attribute words.

We introduce a method to create sets of associated attribute words. We use past queries held in the query logs to make these sets. The assumption is that the logs reflect the interests of a diverse ra nge of users and thus coverage, in terms of associate attribute words, is sufficient. First, for each category, we select the most common type queries. For example, when users search in the category of  X  X ating X , type queries such as  X  X unch X ,  X  X estaurants X ,  X  X afes X  appear frequently in the query logs. The system extracts the queries that contain any one of these type queries and at least one other word, which is assumed to be the attribute word. The system drops type words, and clusters the remaining words, such as  X  X tmosphere X ,  X  X rice X  by using the K-means method. As a result of clustering, highly related words are put into the same cluster, for example  X  X aby, child, baby food X ,  X  X ight view, sight, sea X . These clu sters are the sets of a ssociated attribute words. When the attribute query contains  X  X ight X , all words in the set containing  X  X ight X  are selected as attribute words. This allows the meaning of  X  X ight X  to be used in extracting relevant information such as  X  X ight view X  and  X  X ea X .
Our ambiguous relevance model is based on a measure of similarity between each word of the snippet sentence and the attribute word set. When attribute query a x in a is a set of words, n x 1 , ..., n x | a of candidate summary s , is associated with words w y 1 ,..., w y | l similarity, S , between a x and l y as where sim expresses the average similarity between a word of l y and a word in the set of a x . The method used to calculate wor d similarity can be chosen freely, for example thesaurus-based methods or word co-occurrence methods. 3.3 Algorithm We detail here the algorithm of the method. The method used to make snippet candidates is set in Step 4-6. In this ex periment, we select the greedy method to add sentences, one by one, from the document so as to maximize the score of the snippet. Given document d ,typequery q , attribute query a , and the total character count limit of the snippet, d is separated into sentences l 1 , ..., l m .The system calculates the probability score of each candidate of snippet SUMM . The candidate sentence that has the hi ghest score is chosen as the temporary snippet summMax . Next, each sentence not included in the temporary snippet is added, one by one, to the snippet to create the next snippet candidate. If the candidate exceeds the to tal character count limit, the last addition is eliminated from the candidate. If no further updating is possible, the candidate is output as the snippet bestSumm of the document. Finally the sentences of bestSumm is sorted in the order they occur in the document (Step 18). We conducted an experiment to verify that our method can generate better snip-pets than the conventional method in terms of attribute-oriented entity search. In the experiments we use the following settings.
 Algorithm 1. makeSummary( d , q , a , maxLength ) Type Query: The queries used are listed in Table 1. We examined two situations  X  X ightseeing X  and  X  X ating X . We prepared four type queries for each situation. Each type query consisted of two words.
 Attribute Query: We prepared six attribute queries for each situation. Six at-tribute queries were created, three consisted of one word and three consisted of two words (all pairs yielded by three words). We used the query log of a practical web search engine, gathered from January to June 2011, to produce the associ-ated attribute word sets. We selected three words used frequently when searching for information related to each category from the query logs;  X  X ravel X ,  X  X ight-seeing X ,  X  X rive X  for sightseeing, and  X  X estaurants X ,  X  X afes X ,  X  X unch X  for eating. Next, we clustered the words that appeared with these words in the query log by the K-means algorithm, which yielded the associated attribute word sets used in the experiment.
 Documents: The documents were the blog articles that the authors crawled. We collected 20 documents using each combin ation of type and attribute queries. We used two retrieval styles as follows. 1. AND retrieval of two query words. e.g) Disneyland AND lodgings 2. AND retrieval of type and attribute queries. Given that some documents may contain no information relevant to the attribute query, we asked annotators to judge whether each document contained such information or not. We found that 57% of the documents collected by the two retrieval styles contained information relevant to the attribute queries. Snippets: The three types of snippets created in the experiment are as follows.  X  Proposed method: Snippets reflect fidelity, relevance, and ambiguous rele- X  Baseline A: Snippets reflect only fidelity and relevance, attribute query is  X  Baseline B: Snippets reflect only fidelity and relevance, the relevance model Total number of characters per snippet is limited to 120. To calculate word sim-ilarity we used the method based on the con cept vectors statistically generated from the semantic expressions of words [2].
 Evaluation method: The annotators were three peo ple. They scored snippets on three levels based on two aspects: inform ation about type query and information about attribute query. The evaluation guidelines are shown in Table 2. 4.1 Results We show the average score for each method in Table 3. It shows that the pro-posed method matches baseline A in terms of type query evaluation. This shows that the proposed method can include information about the type query in the snippets, the same as the baseline. This indicates that our extension of the model does not degrade the merit of the conventional model.

With regard to the attribute query evaluation, the results show that the pro-posed method has higher score than either of the baselines. Therefore, the pro-posed method can generate snippets that contain more information, not only type query but also attribute query, than the baseline methods. The scores of method B are the smallest of the three methods so it is ineffective for generating useful snippets given type query and attribute query.

We show the average scores of documents for each combination of type and attribute query in Fig.3. More than 90% of the snippets output by the proposed method lie in the top right area (scores of both type query and attribute query exceed 2), so our snippets contain informa tion satisfying both type and attribute queries. The result also says that most snippets yielded by baseline A lie in the right side of the graph. It means that most snippets by method A satisfy only type queries, and only a few snippets satisfy attribute queries. This result is consistent with the feature of the base lines. Snippets generated by baseline B lie in the center and the bottom of the graph. This means that they don X  X  satisfy attribute queries. Moreover, some method B snippets fail to satisfy either type or attribute queries. It means that it is not always possible to contain the attribute information in snippets if attribute queries are treated in the same way as type queries. This shows that type and attri bute queries must be treated separately to make snippets that satisfy both queries.

Subjecting these results to the t -test shows that the difference in attribute query scores of information is significant ( p value &lt; 0.01). There is no significant difference in the type query scores between the proposed method and baseline A. It means that our method could contain the information about type query almost as much as conventional methods.

Next, we analyzed baseline B, the worst of the three methods. Its poor per-formance is caused by the snippet char acter limit, 120 in the experiment and the equal level of importance placed by baseline B on both type and attribute queries. In several instances, information satisfying the type query was added to the snippet which prevented the type query information from being added (assuming that this would exceed the cha racter limit). These snippets are un-suitable for search results pages because our requirements for snippets, defined in Section 1, state that the snippets must contain words relevant to the type query. Without such words, users cannot find informative documents about the desired entities in the search results. Therefore methods that treat the two kinds of queries in the same manner cannot generate useful snippets. By contrast, our snippets give priority to type query words in a different manner than attribute query words. As a result, our proposed method is able to generate snippets that contain query information and information related to attribute query simulta-neously, and so produces the most informative snippets. In this paper, we tackled the problem of maximizing the effectiveness of attribute-oriented entity searches. We proposed a method that generates snippets that contain information about attribute queries as well as type queries. The model treats type queries and attribute queries differently to capture the key feature of the latter, the unstructured documents found on the web use many expressions to represent the same attribute. In addition to the fidelity model for documents and the relevance model for queries (baseline method), we proposed the am-biguous relevance model; it represents the similarity between words in the at-tribute queries and the words in the documents. We also proposed the method of calculating ambiguous relevance model scores. An experiment showed that the proposed method can generate better snippets that contain information clearly associated with both type query and attribute query than the conventional meth-ods that employ only fidelity and relevance models. We plan to apply our method to other contents that contain information about entities, for example user X  X  re-views in travel sites, shopping sites and so on.
 Acknowledgments. We sincerely would like to thank Hitoshi Nishikawa of NTT Media Intelligence Laboratories, for his helpful comments and discussions which have enhanced this work.

