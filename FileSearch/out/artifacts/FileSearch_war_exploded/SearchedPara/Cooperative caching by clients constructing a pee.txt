 1. Introduction
Recently, there has been an increasing interest in a push-based broadcast system where a server delivers various data to push-based broadcast system is a higher throughput for data access from a huge number of clients [4,6,17,28] . The push-based broadcast system is used for services where information with high publicity, such as movies, sounds, news, and charts is delivered to a massive number of users using satellite or terrestrial broadcasting. However, the server has to broadcast data item, and cache data items with large benefits. These studies assume that clients have two ways to access data; access their own cache and listen broadcasted data.
 Today, there has been also an interest in a new type of information sharing called peer-to-peer (P2P) systems [13].Ina
P2P system, terminals called peers construct a logical network ( P2P network ) by connecting with each other. If a peer which wants a certain data item sends an access request ( query ) to its adjacent peers in the P2P network, the query be propagated query. Since each peer behaves in autonomous and distributed ways, this system has high scalability.

In a push-based broadcast system, it is expected that the average response time for data access could be further reduced if clients construct a P2P network and they access requested data not only from the broadcast server and their own cache but also from the P2P network, i.e., other clients X  cache. Based on this idea, in this paper, we assume that many clients that re-ceive the push-based broadcast service construct a P2P network, and propose two caching strategies by which clients coop-access probabilities but also queries from other clients in order to reduce the average response time in the whole system.
The first strategy aims to reduce the response time by considering data access probabilities and broadcast periods. The reason we take the broadcast periods into account is that caching data items which are not frequently broadcasted has a large advantage in terms of benefit of response time. However, this strategy has a drawback in terms of performance. Spe-cifically, it always caches data items with very long broadcast periods even when they will be broadcasted very soon on the broadcast schedule, i.e., their broadcast times are approaching. This behavior might be harmful to reduce the average re-sponse time.

Therefore, in the second strategy, we aim to extend the first by taking into account the remaining time until each data item is broadcasted next. We also report the results of simulation experiments to verify the effectiveness of our proposed strategies.

It should be noted that the contributions of this paper are an appropriate integration of data broadcasting and P2P tech-niques and extensive simulation experiments. The summary of the contributions is as follows: 1. The main contribution of this paper is an appropriate integration of data broadcasting and P2P techniques. While we choose two typical strategies for caching broadcasted data, PIX and PT, and a simple flooding-based search method in a P2P network as the basis of our integrated caching strategies, we can replace these conventional techniques with other ones. Specifically, any other existing P2P techniques are applicable to our proposed strategies if they can provide the information that are necessary to estimate the access frequencies of neighboring peers. We can also apply some existing collaborative caching techniques in P2P networks. 2. In this paper, we assume applications that broadcast various kinds of contents to a massive number of users using satel-lite or terrestrial broadcasting. The broadcasted contents could be news contents, short movies like those in U-tube, and (long) video movies. The results of extensive simulations in Section 5 show that our proposed strategies can shorten the average response time by about half or one third of those of the conventional caching strategies (PIX and PT) in a sparse network and by about one tenth in a dense network. The performance gains are greatly beneficial for users of those appli-cations because reducing the waiting times and users X  frustration is very important for content broadcasting services. 3. Some of the results of this paper have been reported in [19]. The extension points from [19] are (i) proposal of the second strategy (C-PT), (ii) extensive simulation experiments regarding performance evaluation, and (iii) the implementation of a prototype and some experiments using the prototype.

The reminder of this paper is organized as follows. We introduce conventional caching strategies in Section 2 and describe the system model in Section 3. We propose cooperative caching strategies for data broadcasting in Section 4, and evaluate them using simulation experiments in Section 5. We also evaluate the performance of a prototype on which our proposed strategies are implemented in Section 6. We show some related works in Section 7. Finally, in Section 8, we summarize this paper. 2. Conventional caching strategies
In this section, we introduce two typical caching strategies in push-based broadcast systems. In PIX [1] and PT [2] strat-egies, it is assumed that clients can access data items from only their own cache and broadcast channel. When a client re-quests for a certain data item, it checks whether it caches the requested data item. If it does, it can access the item item is broadcasted next.

Our strategies proposed in this paper extend the PIX and PT strategies to adapt to a situation where clients construct a P2P network. We have chosen the PIX and PT strategies as the basis of our strategies because they are well-known strategies for caching broadcasted data and do not require special information for determining cache replacement but only use the general information on access frequencies, broadcast intervals, and remaining time until each data item is broadcasted next, which can be easily obtained in a broadcast information system. It is known that the PT strategy gives the shortest response time among existing caching strategies for broadcasted data items which are not correlated (dependent) with each other. 2.1. PIX strategy
The algorithm of the PIX strategy is as follows: 1. The PIX value, K j  X  p j y j , is calculated by each client for each data item j  X  1 items which are broadcasted by the server, p j is the probability that the client accesses data item j , and y period of data item j . 2. The client caches c data items which have the c highest K
The PIX strategy reduces the response time of data access by caching items which have high access probabilities and long broadcast periods. 2.2. PT strategy
The algorithm of the PT strategy is as follow: 1. Every time when each data item, k , is broadcasted, the PT values are calculated by each client for all data items in the client X  X  cache and data item k . The PT value, L j , of data item j is calculated by L time and u j  X  Q  X  is the time when data item j is broadcasted next. 2. If at least one data item in the cache gives a lower PT value than L by k .

The PT value, L j , represents the expected value of increase in response time if the client does not have data item j in its cache. The PT strategy compares the increases in response time of data items if they are discarded from the cache, and pre-fetches data items with larger gains in response time. 2.3. Limitations of the PIX and PT strategies
The PT and PIX strategies assume that each node determines its cache replacement independently of other nodes. Here, in a real environment, there generally exits an access skew, thus, many nodes cache same data items when using the PT and PIX strategies. Thus, if these nodes collaboratively cache data items and reduce the duplication of the cached items, the cached data items can be used (shared) more effectively and the response times are expected to be shorter. This is our motivation to propose cooperative caching strategies. 3. System model
Fig. 1 shows a system model assumed in this paper. In this system model, peers (clients) can send/receive data items to/ from other peers in the P2P network. Specifically, we make the following assumptions.

The system has a single broadcast server, and peers do not send any access requests to the server, i.e., pure push-based broadcast.
 We assign a unique data identifier to each data item located in the system. The set of all data items is denoted by
D  X f 1 ; 2 ; ... ; n g , where n is the total number of data items and j  X  1 Each peer has a storage and can cache a limited number of data items.
 All data items are of the same size and not updated. It takes one unit of time (one time slot) to broadcast one data item.
The assumption that all data items are of the same size is for the purpose of simplicity. We can easily consider environ-ments where data items have different sizes. A simple and popular way is to consider different sizes of data items when replacing cache. For example, if the criteria for cache replacement include access frequency, a data item with access fre-if we consider data items with different sizes, the performance of the caching strategies is affected not only by cooperative caching using a P2P network but also by an approach of how to deal with differences in data size. The examination of the impact of such an approach is beyond the scope of this paper. Therefore, we made this assumption in order to fairly exam-ine the effectiveness of our approach as well as for the purpose of simplicity.

The server composes a broadcast program and circularly broadcasts it. Each peer knows the broadcast program. It can be realized by several ways, e.g., the server periodically broadcasts the program information.

In the P2P network constructed by clients, the delay for sending a small message, i.e., query and reply messages, is ignor-able and the time to process a query is also ignorable. This assumption is for the purpose of simplicity and is based on the fact that the sizes of these messages are much smaller than the sizes of data items and the time for exchanging them is also much smaller, e.g., at most a few seconds, than the time for broadcasting and exchanging data items. Even if we assume applications in which the query processing time cannot be ignored compared with the time for broadcasting and exchanging a data item, the strategies proposed in this paper can work without any modification. Also, the impact of the query processing time on the response time of our proposed strategies can be easily estimated. Specifically, when the target data item is accessed from the local cache of the request issuing peer or through the broadcast channel, there is no increase in the response time. Only when the target item is accessed from the P2P network, the query processing time becomes the direct increase in the response time.

In the P2P network, the time to transmit a data item between two peers (clients) is different according to pairs of sender and receiver and also might change dynamically as time passes. We assume that the sender of a data item can estimate the available network bandwidth in real time by using an existing bandwidth estimation approach such as [3]. 4. Cooperative caching strategy
In this section, we propose new cooperative caching strategies using a P2P network. First, we explain the procedure for query processing in our proposed strategies. Then, we illustrate the cache replacement mechanisms. 4.1. Query processing
Fig. 2 shows the flow of query processing in our proposed strategies. When a peer wants to access a data item, it chooses a way that gives the shortest response time among the three access methods: accessing the item stored in its own cache, receiving the item from the broadcast, and receiving the item from another peer X  X  cache in the P2P network. In this paper, it is assumed that the response time when accessing an item in its own cache is 0. Thus, if a request issuing peer holds the item in its own cache, it always accesses the cached item.

If the peer does not hold, it issues a query by using flooding to check the availability of the requested data item in the P2P
Ifanadjacentpeerdoesnotholdtherequesteditem,itre-broadcaststhequerytoallitsadjacentpeers,andthisrepeatsuntilthe query reaches a peer that holds the requested item or the logical hop count from the request issuing peer exceeds the TTL.
This query process also has the objective to estimate the available network bandwidths between the request issuing peer and holders of the requested data item. If a peer that holds the requested item is found (in the following, it is denoted that  X  X  X he query hits X ), the peer sends a reply message to the request issuing peer along two different paths. The first path is through all the peers that relayed the query on the reverse direction, which is used to record the information used for esti-mating access frequencies to the data item. The second path is the direct path (without intermediate peers) from the sender of the reply to the request issuing peer, which is used to estimate the available network bandwidth.

If the request issuing peer does not receive any reply messages (represented by  X  X  X he query does not hit X ), the request issu-ing peer waits until the data item is broadcasted next. Otherwise, the peer calculates the response times of the two cases, receiving from the broadcast channel and receiving the data from another peer. The response time when receiving a re-quested item from the broadcast is the time until the data item is broadcasted next. The response time when receiving the data from another peer is the time for sending the requested data from the data holder with the largest available band-the propagation delay from the sender to the receiver, we can simply calculate the response time when receiving the data from another peer as D = B max , where D is the size of the data item.

If the former (receiving from the broadcast channel) is shorter or there is no peer holding the requested data, i.e., no re-sponse is received, the peer waits until the item is broadcasted next. Otherwise, it receives the requested data item from the peer with the highest available network bandwidth. This data transmission is directly performed between the two peers using unicast. 4.2. Cooperative caching 4.2.1. Query information from other peers
In order to cooperatively cache data items, peers should know what data items are already cached by other peers and what data items are frequently accessed. However, since there are a huge number of peers in a push-based broadcast system, it is impractical that peers precisely know this information. Our main idea is that each peer guesses this information only age response time by determining cache replacement from this guessed information. This approach is reasonable because a query propagates only within a certain area determined by the TTL and thus the information guessed from queries indicates what items are cached and frequently accessed by neighboring peers within the TTL. In the following, we explain how to guess the information.

To guess the above information, in the proposed strategies, each peer classifies queries that the peer issued or received from its neighbors. When a query arrives at a peer, the peer counts the query as one of the following three categories based on the result of looking up. Each of the three categories is counted for each data item.

F (Failure) query : The query that did not hit, i.e., neither the peer nor further peers that the query propagated had the requested data item.
 the requested data item.

C (Connected) query : The query that hit at the peer and the requested item was downloaded, i.e., the peer had the requested item and actually sent it to the request issuing peer.

Let us suppose a situation in which peer a is adjacent to peers b , c , and d as shown in Fig. 3 and only peer d caches data d , peer d counts the query as a C query instead of S query.

Here, it should be noted that a query is counted only once even if the same query or its results reached through multiple routes. The priority is given in the order of C, S, and F queries. For example, in the above case, if peer a receives the same query via another route and the TTL of the query is 0, the query cannot reach peer d and thus cannot find a peer that has the requested data item. In this case, while the query can be categorized to both S and F queries at peer a , only S query is counted according to the priority mentioned above.
 By categorizing and counting queries, the following facts can be found.

If a peer counts many F queries for a data item, it is shown that the data item is frequently requested by its neighboring peers including itself, but there is no neighboring peer that caches it.

If a peer counts many S queries for a data item, it is shown that the data item is frequently requested by its neighboring peers including itself and some peers or itself cache it.

If a peer counts many C queries for a data item, it is shown that the data item is frequently requested by its neighbors and the item cached by the peer is actually sent to the neighbors.

Increasing rates of the three categories dynamically change every time when a peer replaces its cache. For example, when one of the neighboring peers caches item i , many S queries will be counted at the peers, whereas many F queries had been counted until now. 4.2.2. Cache replacement
The cooperative caching strategies proposed in this paper extends the PIX and the PT strategies to take into account data accesses from other peers in the P2P network. The proposed strategies, Cooperative PIX (C-PIX) and Cooperative PT (C-PT), calculate the benefits of the expected response time in the entire system when a peer replaces one of the cached data items with the broadcasted data item. Based on the calculation, the C-PIX and C-PT strategies determine the cache replacement. In the following, we explain the details of the C-PIX and C-PT strategies.
 The C-PIX strategy:
For data item j in a peer X  X  cache, the expected value of increase in response time in the entire system when the peer dis-cards j from its cache is defined by the following equation:
We call this the C-PIX value . Here, as described in Section 2.1, y the expected average response time of accessing j from the broadcast channel. l denotes the average time required for send-ing a data item from the peer to another peer, which can be calculated as the average transmission time for several latest data transmissions. Thus, y j = 2 l denotes the increase of the expected average response time of accessing j when j is re-moved from the cache and the neighboring peers have to access it from the broadcast channel. P ability of item i per unit time, and F j ; S j ; C j denote the arrival rates of F, S, C queries per unit time. Therefore, G expected value of increase in response time in the entire P2P network when the peer removes j from its cache. For broadcasted data item k which is not in the peer X  X  cache, the expected value of decrease in response time in the entire
P2P network when the peer caches k is defined as the k  X  X  C-PIX value, G peers do not cache data item k , and peer A and its neighbors have to access k from the broadcast channel. Therefore, for peer
A and its neighbors, the expected average response time of accessing k is y
Thus, the expected response time of accessing k is l . From the above discussions, G Here, a k denotes the forecast ratio of F queries that will change to C queries when the peer caches data item k .
When a peer discards data item i from its cache, a i is set as the value of a equation:
Here, x  X  0 6 x 6 1  X  is the parameter that determines how much the new a an unnecessary large value, the system cannot sensitively adapt to changes of the environment. It is important to determine an appropriate value of x considering the feature of the system.

In the C-PIX strategy, each peer calculates the C-PIX values for all data items stored in its cache and finds the minimum the query counts of F, S, and C queries for the item discarded from the cache and the newly cached item are set to 0. Now, we define the warm-up time, T , that represents the time necessary for receiving enough queries for calculating the
C-PIX value after caching a new data item. Until T units of time passes after caching data item j , G but by the following equation: Here, F j denotes the value of F j before caching data item j .
 The C-PT strategy:
Similar to the C-PIX strategy, the C-PT strategy calculates the expected value of increase in response time in the entire system when the peer discards each data item stored in the cache. Only the difference is that the C-PT strategy calculates it more precisely. Specifically, the expected response time of accessing data item j from the broadcast channel is calculated the C-PIX strategy, the expected response time is simply calculated as y
As a result, in the C-PT strategy, for data item j in a peer X  X  cache, the expected value of increase in response time in the entire system when the peer discards j from its cache is defined by the following equation: We call this the C-PT value .
 In addition, Eqs. (2) and (4) are modified as follows:
Except for the above modifications, the behavior of the C-PT strategy is same as that of the C-PIX strategy. 4.3. Discussions
As mentioned above, the C-PT strategy determines cache replacement by taking into account the time remaining until each data item is broadcasted next, while the C-PIX strategy only considers the broadcast period. Thus, the C-PT strategy has possibility to further shorten the response time for data retrieval.

However, the adaptive cache replacement in the C-PT strategy according to the broadcast status causes more often changes of data items in the cache than the C-PIX strategy. This might affect badly from several aspects. For example, query counts recorded by each peer tend to become unreliable when cache replacement often occurs. Also, a large number of peers might replace data items with low C-PT values with broadcasted items simultaneously, thus, there might be heavy data duplication among peers. This means that the advantage of sharing data items in the P2P network might decrease. Moreover, while this paper assumes that a query is propagated by flooding, many existing query routing protocols reuse the routing ment degrades the effectiveness of the cached route information.

Thus, in our simulation experiments described later, we evaluate not only the average response time but also the fre-quency of cache replacement in our strategies. 5. Performance evaluation: simulations
In this section, we show the results of simulation experiments for performance evaluation of our proposed strategies. 5.1. Simulation environment
It is known that an unstructured P2P network constructed on the Internet follows the power-law [12]. If a network follows the power-law, the degree of peer j , d j , which is the number of j  X  X  adjacent peers, follows the following rule: on this fact, we determined the degree of peer j , d j , by the following equation:
Here, w max denotes the maximum number of adjacent peers. Without losing generality, we assume r work which is constructed by connecting peers at random according to the power-law is called a Power-law Random Graph (PLRG).

In our simulations, the number of peers was set to 500 and we used a PLRG network, where ( w low, while R  X  0 : 4 represents a dense network. Although our simulations assumed a small P2P network with 500 peers, their results are not limited to such a special case but are valid for general large P2P networks with a few thousand or more peers. This is because in our proposed strategies, each peer just collects the information on queries issued by neighbor peers within a predetermined TTL, and the behavior of the proposed strategies is not affected by the scale of the network. There-fore, the P2P network in our simulations can be viewed as a part of a large scale P2P network.

The access probability at each peer was determined based on the Zipf distribution [30], where the smaller the identifier of their values are the same at all peers. Specifically, access probability p equation: our simulations, h j was set as 0.8.

The probability that each peer issues an access request at each time slot was set to 0.1. Therefore, the access frequency of data item i at peer j becomes P ji  X  p ji 0 : 1.

The server periodically broadcasts 1000 data items according to the schedule composed by the Broadcast Disks strategy [1]. In broadcast disks, all data items are sorted in descending order of access frequency. Note that in our simulations data items are sorted by access frequency in advance and are given data identifiers as the rank. The data items are classified into
Lastly, the broadcast program is composed by choosing disks and chunks circularly based on their broadcast frequencies. By doing so, broadcast disks can compose a cyclic program where data items with higher access frequencies are broadcasted with shorter broadcast intervals. Table 1 shows the details of the composed program.

The size of a data item was set as 12.5 (Mbyte) (=100 (Mbit)). The network bandwidth from the broadcast server to clients 1 (s) as a time slot (a unit of time). We assumed that the network bandwidth between every pair of peers in the P2P network is 10 (Mbps), thus, the time for downloading a data item from the P2P network is always 10 (s). We also assume that every peer can accurately estimate the available network bandwidth. While our proposed strategies can work in an environment where available network bandwidth changes according to the situation, a simple case is assumed for the purpose of simplic-ity in our simulations. Here we have conducted several preliminary experiments in advance and confirmed that differences in network bandwidth between peers do not affect the differences in performance between our proposed strategies and other existing strategies. We also confirmed that errors in estimation of available network bandwidth do not affect the per-formance of our proposed strategies and the differences in performance between ours and other strategies. This is because errors in estimation are either of the two cases; underestimation or overestimation. When an underestimation occurs, the actual response time is longer than the estimated one, and vice versa. Thus, in a long run, errors in estimation do not affect both the average response time and the differences in performance among the strategies. Some simulation results that back up these facts are shown in Sections 5.4 and 5.5 .

Initially, data items were cached at each peer according to the PIX strategy. The initial value of a each peer was set to 1. The TTL of each query was set to 3.

Based on the above simulation environment, we evaluated the average response times of the proposed strategies, C-PIX and C-PT, during 300,000 time slots. For the purpose of comparison, we also evaluated the average response times of four other cases; the random caching, Least Recently Used (LRU), PIX, and PT strategies. To eliminate the unfairness, we slightly changed the four strategies, random caching, LRU, PIX, and PT, to allow peers to receive data items from the P2P network, similar to the C-PT and C-PIX strategies. Thus, in these four strategies, each peer chooses an access method that gives the shortest response time among the three access methods: accessing the item stored in its own cache, receiving the item from the broadcast, and receiving the item from another peer X  X  cache.

From our preliminary experiments, when the cache size is 50 (items), the C-PIX strategy gives the shortest response time where x  X  0 : 52 and T  X  580 in the sparse network and where x  X  0 : 08 and T  X  1380 in the dense network. As for the C-PT strategy, the shortest response time is given where x  X  0 and T  X  560 in the sparse network and where x  X  0 : 22 and
T  X  1800 in the dense network. Therefore, we use these values in the following simulations. 5.2. Impact of cache size
Fig. 4 shows the average response times of the proposed strategies and the other four strategies where the cache size var-approaches, the C-PIX and C-PT strategies, always give the shorter average response time than other existing approaches. The egies each peer takes into account its own access frequencies to data items for cache replacement and all peers have the same access characteristics in our simulation environments. Specifically, since in the PIX and PT strategies, peers cache the same items, they can hardly find requested items in the P2P network. The random caching strategy gives shorter average response time than the PIX and PT strategies because connected peers can cache more kinds of data items due to the random selection of cached data items. In other words, peers incidentally share more data items and can find requested items more often in the P2P network.
The LRU strategy gives shorter average response time than the PIX, PT, and random caching strategies. In this strategy, peers cache data items which they request more frequently similar to the PIX and PT strategies, but the timings of caching the data items are different among peers because the timings of accessing these data items are also different. As a result, peers incidentally share date items. Because peers can cache data items which they frequently request, the LRU strategy gives shorter response time than the random caching in which peers randomly cache data items independent of their access frequencies.

Because the C-PIX and C-PT strategies intentionally distribute cached data items among peers according to access re-quests exchanged among neighboring peers, their average response times become shorter than those of other strategies.
Even when the cache size of each peer is small, these strategies can improve the hit ratio of queries and shorten the average response time. In particular, the C-PIX strategy always gives the shortest response time, where it gives almost 30% shorter response time than the C-PT strategy when the cache size is large. This seems to be due to the bad effect of frequently chang-ing data items and their heavy duplication in the C-PT strategy while it takes into account the remaining time until each data item is broadcasted next. Comparing the sparse and dense networks, the impact of our approaches is more remarkable in the dense network. This is because the number of peers within the given TTL is larger than that in the sparse network, thus, a larger number of peers can cooperatively cache data items. As a result, they can find their requesting data items in the P2P network more frequently.

To confirm the above discussions, we further examined the result for the dense network in Fig. 4 b, and checked the ratios of accessing data items stored in peers X  own cache, receiving data items from the broadcast, and receiving data items from other peers in the P2P network. Fig. 5 shows the examination results.
 From this result, it can be seen that the ratios of accessing peers X  own cache are high in the three strategies, PIX, PT, and
LRU, because each peer caches data items which are frequently requested by itself. The ratios of receiving data items from other peers in the P2P network are high in the four strategies, C-PT, C-PIX, LRU, and random caching, because cached data C-PIX, and LRU, thus, they give shorter average response time than the other strategies.

Comparing the average response times and the ratios of receiving data items from the broadcast of these three strategies, (from low to high) is C-PIX, LRU, and C-PT while the order of the response time (from short to long) is C-PIX, C-PT, and LRU.
This shows the effectiveness of cache replacement in the C-PIX and C-PT strategies. Specifically, since in the C-PIX strategy neighboring peers cooperatively cache data items, it can access data items cached by other peers more frequently than the
LRU strategy, and thus, the ratio of accessing data items from the broadcast is lower than the LRU strategy and the response time is shorter. On the other hand, comparing the C-PT and LRU strategies, the C-PT strategy gives higher ratio of receiving data items from the broadcast than the LRU strategy when the cache size is large. However, it gives shorter response time than the C-PIX strategy. The reason why the C-PT strategy gives higher ratio of receiving data items from the broadcast is that it takes the remaining time until each data item is broadcasted next into account. That is, the C-PT strategy removes data items whose remaining times until the next broadcast are short from the cache, and this increases the occasions that data items which will be broadcasted soon are received from the broadcast channel. This policy accelerates effective use of the broadcast channel. Therefore, higher broadcast access ratio does not simply mean longer average response time. 5.3. Frequency of cache replacement
As mentioned in Section 4.3, the C-PT strategy causes more often changes of data items in the cache and this might affect badly in terms of the system performance. Thus, we then examine the frequency of cache replacement in the C-PT and C-PIX strategies. Fig. 6 shows the examination result in the dense network where the vertical axis indicates the average frequency of cache replacement occurred at a peer during a time slot (one second).

From this result, in the C-PIX strategy, the frequency of cache replacement is not much affected by the cache size. This is because the C-PIX strategy uses broadcast periods, which are static parameters, as a metric for cache replacement. More spe-cifically, as time passes, data items cached at each peer tend to be fixed, thus, the property of queries received from neigh-bors also does not change much, which causes more static cache placement. On the contrary, in the C-PT strategy, as the cache size gets larger, the frequency of cache replacement gets higher. Since the C-PT strategy takes into account the remain-ing time until each data item is broadcasted next, which is a dynamic parameter, for cache replacement, it causes frequent cache replacement. In particular, when the cache size is large, the broadcasted data item has more chance to be cached by discarding another data item stored in the cache.

As mentioned in Section 4.3, frequent cache replacement causes query counts recorded by each peer tend to become unreliable and also causes heavy data duplication among peers. Actually, as shown in Fig. 4 b, the C-PT strategy gives longer response time than the C-PIX strategy. This shows that the effectiveness of cache replacement by considering time remaining until each data item is broadcasted next could not overcome the disadvantages cause by the frequent cache replacement. 5.4. Impact of differences in network bandwidth
In this subsection, we examine the impact of differences in network bandwidth between peers on the performance of each strategy. We conducted an experiment in which the parameter setting is basically same as the experiment in Section 5.2 except for that the network bandwidth between peers in the P2P network is different with each other. Specifically, we set the network bandwidth between each pair of peers according to the normal distribution where the average is 10 (Mbps) and the standard deviation is 4 (Mbps). We only show the result for the dense network due to the limitation of space.
Fig. 7 shows the simulation result. From this result, the performance of each strategy is almost same as that in Fig. 4 b although the differences in performance between our proposed strategies and other strategies become slightly smaller. It can be seen that the impact of differences in network bandwidth on the performance is little, and thus, we confirm the val-idness of our statement in Section 5.1. 5.5. Impact of accuracy of network bandwidth estimation
Next, we examine the impact of accuracy of network bandwidth estimation on the performance of each strategy. We con-ducted an experiment in which the parameter setting is basically same as the experiment in Section 5.2 except for that the actual network bandwidth between peers in the P2P network becomes different from the estimated one. Specifically, we basically set the network bandwidth between every pair of peers as 10 (Mbps) and request issuing peers always estimate the available network bandwidth as 10 (Mbps). However, when data items are actually transmitted between the data holder and the request issuing peer, the network bandwidth is determined according to the normal distribution where the average is 10 (Mbps) and the standard deviation is 2 (Mbps).

Fig. 8 shows the simulation result for the dense network. From this result, the performance of each strategy is almost same as that in Fig. 4 b and it can be seen that the impact of errors of bandwidth estimation is little. Thus, we confirm totally affect little on the performance. 5.6. Impact of TTL
In a P2P network, by setting TTL as a large value, each peer can find the requesting data item with high probability. More-over, in our approaches, C-PT and C-PIX, each peer can collect the information on queries from more peers, thus, more peers can cooperatively cache data items, which results in shortening the response time. However, setting TTL as a large value also increases traffic for data retrieval and this may degrade the entire network performance.

Therefore, we lastly evaluate the average response times of the four strategies, random caching, LRU, C-PIX, and C-PT, where TTL varies from 1 to 7 and the cache size is fixed as 50 (items). Fig. 9 shows the simulation result. From this result, random caching strategy is much affected by TTL. This is because in these strategies, each peer basically caches data items with high access probabilities, and thus, the increase of the query range is more beneficial than the random caching strategy. The average response time of all the four strategies becomes almost constant where TTL is equal to or larger than 5.
In a real environment, an appropriate value of TTL depends on several factors such as the network topology, access fre-quencies to data items, and cache size. Thus, TTL should be changed adaptively according to the situation. 6. Experiments on a prototype
We have implemented a prototype which has mechanisms of our proposed caching strategies (and the PIX and PT strat-egies) and measured its performance. Here, because of a massive number of clients or peers, it is generally difficult to fully evaluate a protocol in both unstructured P2P systems and information broadcasting systems on a practical platform. That is why almost all conventional works have extensively evaluated the proposed protocols using simulation studies and some of them conducted performance measurements of small scale on a prototype to verify the results of the simulations. Therefore, our measurements on the prototype also used a small number of peers to examine the protocol overhead of the proposed caching strategies and partially verify the results of simulation experiments shown in the previous section. 6.1. Outline of the implementation
In our implemented prototype, we used P2P Interactive Agent eXtension (PIAX) [20] to construct the P2P network. PIAX is a mobile agent platform based on a P2P architecture. In PIAX, a PIAX peer runs on each machine and mobile agents move among PIAX peers. Mobile agents are programmed by Java language and we can make mobile agents execute various processes.

On the PIAX platform, we have implemented 10 peers  X  p 0 ; ... ; p
PIX, and C-PT strategies. The topology of the P2P network is shown in Fig. 10 . These peers run on ten different machines; p ; ... ; p 8 (Sun Fire1600, OS: RedHatLinux9, CPU: Mobile AMD Athlon XP-M 1800+, 1532.629 (MHz), Memory: 1,029,920 (kB)) and p 9 (NEC Express5800 110Ba-e3, OS: TurboLinux10, CPU: Intel Pentium M processor, 1100.209 (MHz), Memory: 507,400 (kB)). These peers connected with each other via TCP connections in a LAN, where the bandwidth between every pair of two peers was limited by 10 (Mbps). Since the main objective of this experiment is to measure the computational overhead of the caching strategies and the
P2P query overhead, we virtually implemented the broadcast mechanism so that each peer behaves as data items are broad-casted via the broadcast channel according to the predetermined schedule while no data items are actually broadcasted.
Other parameters such as the size of data items, the network bandwidths for broadcasting, and the query issuing frequencies were equal to those used in the simulation experiments. x and T were set as appropriate values which were determined by some preliminary simulation experiments based on the same setting.

Based on the above setting, we measured the performance of the implemented prototype during 1 (h) for each run. 6.2. Computational overhead for cache replacement
First, we examine the computational overhead for cache replacement in the four strategies. Fig. 11 shows the measure-the cache size and the vertical axis indicates the computational overhead (ms). The computational overhead is defined as the average time required for determining cache replacement for a broadcasted data item, i.e., calculating and comparing the metric (PIX, PT, C-PIX, and C-PT values) for cache replacement.

From this result, in every strategy, the computational overhead is almost proportional to the cache size. The computa-tional overhead of our proposed strategies is slightly higher than that of the PIX and PT strategies. However, even when the cache size is 100 (items), the computational overhead is about 10 (ms), thus, it can be ignored compared with the data transmission time and waiting time for the broadcasted data. It also shows that even if the cache size is much larger than 100 (items), e.g., 10,000 (items), the computational overhead is not a big problem in terms of performance, e.g., a few second. 6.3. P2P query overhead
Next, we examine the P2P query overhead for data looking up in the P2P network. Fig. 12 shows the measurement result where the cache size is set as 60 (items) and TTL varies from 1 to 3. In this graph, the horizontal axis indicates TTL and the vertical axis indicates the P2P query overhead (ms). The P2P query overhead is defined as the average time required for data looking up for a requested data item in the P2P network, i.e., propagating the query, searching the requested data item in the cache, and sending back the reply.

From this result, the C-PIX and C-PT strategies give similar P2P query overhead. It can be seen that the P2P query over-head is not completely proportional to TTL. Here, the time for completing a P2P query is the time until the last reply is re-turned back. Therefore, as TTL gets larger and the number of peers to which a query is propagated gets higher, the time until the last reply is returned back becomes longer. However, even when TTL is 3, the P2P query overhead is about 150 (ms), thus, it can be ignored compared with the data transmission time and waiting time for the broadcasted data. It also shows that even if the propagation delay is much larger than that in LANs, e.g., 500 (ms) between two remote sites in the Internet, and TTL is set as a larger value, the P2P query overhead is not a big problem in terms of performance, e.g., a few second. 6.4. Average response time
Lastly, we examine the average response time of the four caching strategies on the implemented prototype. This exam-result is less meaningful. Fig. 13 shows the measurement result where TTL is set as 3 and the cache size varies from 20 to 100 low number of peers, difference in network topology, and short experimental time.
We also have conducted a simulation experiment where the network topology is same as that in the prototype measure-ment. Fig. 14 shows the result. Although there are still differences between the results of the simulation and the prototype measurement, the differences are much smaller than that with the results in Fig. 4 . Moreover, we can observe some similar inversely proportional to the increase of the cache size, (ii) the differences among the four strategies are smaller than the results in Fig. 4 , and (iii) the PT-based strategies (PT and C-PT strategies) give shorter response times than the PIX-based strategies (PIX and C-PIX strategies). These characteristics seem to be mainly due to the small number of peers, i.e., peers cannot effectively share data items even in our proposed strategies. 7. Related works
P2P systems are classified into two categories; structured [21,25] and unstructured [9]. Structured systems have precise control over the network topology and locations of data items in the whole network, while unstructured ones do not. Since blind methods such as flooding are used to look up requested data items in unstructured one, they also have a disadvantage that network traffic and looking up delay are larger than structured ones. Instead, unstructured ones have an advantage of being built easily and flexibly. This is because many P2P systems currently in service use unstructured networks for data looking up [13]. Thus, in this paper, we assumed an unstructured P2P system.

There are many conventional works that address cooperative caching in some research fields such as web caching [11], dis-web caching, several strategies in which proxy servers cooperatively cache Web contents. These strategies aim to reduce the networktrafficandbalancetheprocessingloadofWebserverswhichholdoriginalcontents.Inoneofstrategies [11],proxiesare hierarchically coupled like domain name system (DNS), where the root of the hierarchy is the server which holds original con-tents. Whena clientrequestscontents,it first askswhetherthe proxy whichis responsible to its domaincachesthem. Ifnot, the request is forwarded to proxies of higher level in the hierarchy. These approaches are similar to structured P2P systems since proxiesarehierarchicallycoupledanddatarequestsareroutedbasedonparticularrules,andthus,differentfromourapproach.
In the research field of P2P systems, data caching or replication is one of the hottest research topics and a large number of caching or replication strategies have been proposed [5,16,23,24,26] . As mentioned in Section 1, while we chose the PIX and
PIX strategies for broadcasted data as the basis of our proposed caching strategies, we can also apply some ideas of these existing collaborative caching techniques in P2P systems.

In [14], we proposed cooperative caching strategies by mobile clients in push-based information systems. These strategies are similar to the strategies proposed in this paper because in both approaches clients cooperatively cache broadcasted data.
However, the strategies proposed in [14] determine cached data items centrally by the coordinators which are chosen peri-odically, thus are different from our approach in this paper in which peers behave autonomously for cache replacement. Re-cently, there have been several works that address cooperative caching or prefetching of broadcasted data by mobile clients.
In [7,8], the authors proposed a cooperative caching scheme in which tightly-coupled groups (TCGs) are formed for sharing data items in a mobile broadcast environment. In [27], the authors proposed a cooperative prefetching scheme to share dif-ferent data items among neighboring peers in a mobile broadcast environment. This scheme is based on the announcement of information among peers. These conventional schemes are different from our approach because these schemes require extra message exchanges to form TCGs or to get information for determining the data placement. To the best of our knowl-edge, there is no conventional work before us that addresses cooperative caching in which peers determine cache replace-ment autonomously without any extra message exchanges in a push-based information system. 8. Conclusion
In this paper, we have proposed new cooperative caching strategies by multiple clients in a push-based broadcast system, which replace cached items based on benefits of the waiting time. A key idea is that the clients construct a logical P2P net-work and each of them determines the replacement of its own cache by taking into account access probabilities to data items from neighboring clients and the broadcast periods of data items or the times remaining until these items are broadcasted next. This decision is autonomously made by each client from the collected information on queries issued by other clients.
Since our proposed strategies calculate the benefit in response time when replacing a cached data item with the broad-casted item, they are specially designed for push-based broadcast systems. However, the mechanism to predict the status of allocation, in general P2P networks.

We have confirmed that the proposed strategies reduce the average response time by simulation experiments. While in our simulation experiments, we assume that all peers have the same access characteristics, we also conducted other exper-iments where peers have different access characteristics. We do not mention about the details in this paper, the results basi-cally show almost the same properties while the differences in performance between our proposed strategies and other existing strategies slightly change.

We have also implemented a prototype which has mechanisms of our proposed caching strategies and measured the computational overhead and the P2P query overhead. From this result, we have confirmed that both overheads are not big problems for a practical use of our proposed strategies.

While in this paper, we assume simple flooding for query routing, there are several approaches such as random walk and expanding rings [18]. Our proposed strategies can be applied in an environment where such routing methods are used if they can provide the information for our strategies to estimate the access frequencies of neighboring peers. However, the applied query routing method has an impact on the performance of our strategies because it affects the available information for predicting the access frequencies. As part of our future work, we plan to evaluate the performance of the proposed C-PT and C-PIX strategies in environments where different query routing methods are used.
 Acknowledgments
This research was partially supported by Grant-in-Aid for Scientific Research on Priority Areas (18049050) of the Ministry of Education, Culture, Sports, Science and Technology, Japan.

References
