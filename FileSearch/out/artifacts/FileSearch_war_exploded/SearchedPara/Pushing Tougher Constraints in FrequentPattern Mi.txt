 Frequent itemsets play an essential role in many data mining tasks that try to find interesting patterns from databases, such as association rules, correlations, sequences, episodes, classifiers, clusters and many more. Although the collection of all frequent itemsets is typically very large, the subset that is really interest-ing for the user usually contains only a small number of itemsets. This situation is harmful for two reasons. First, performance degrades: mining generally be-comes inefficient or, sometimes, simply unfeasible. Second, the identification of the fragments of interesting knowledge, blurred within a huge quantity of mostly useless patterns, is difficult. Therefore, the paradigm of constraint-based mining was introduced. Constraints provide focus on the interesting knowledge, thus reducing the number of patterns extracted to those of potential interest. Addi-tionally, they can be pushed deep inside the pattern discovery algorithm in order to achieve better performance [9, 10, 14, 15, 16, 17, 18].
 be a set of distinct literals, usually called items , where an item is an object with some predefined attributes (e.g., price, type, etc.). An itemset X is a non-empty subset of I .If | X | = k then X is called a k-itemset . A constraint on itemsets is a function C :2 I  X  X  true , false } . We say that an itemset I satisfies a constraint if and only if C ( I )= true . We define the theory of a constraint as the set of itemsets which satisfy the constraint: Th ( C )= { X  X  2 I |C ( X ) } .A transaction database D is a bag of itemsets t  X  2 I , usually called transactions . The support of an itemset X in database D , denoted supp D ( X ), is the number of transactions which are superset of X . Given a user-defined minimum support  X  , an itemset X is called frequent in D if supp D ( X )  X   X  . This defines the minimum frequency constraint: C freq [ D , X  ] ( X )  X  supp D ( X )  X   X  . When the dataset and the minimum support threshold are clear from the context, we indicate the frequency constraint simply C freq . Thus with this notation, the frequent itemsets mining problem requires to compute the set of all frequent itemsets Th ( C freq ). In general, given a conjunction of constraints C the constrained frequent itemsets mining problem requires to compute Th ( C freq )  X  Th ( C ).
 Related Work and Constraints Classification. A first work defining classes of constraints which exhibit nice properties is [15]. In that paper is introduced an Apriori-like algorithm, named CAP, which exploits two properties of constraints, namely anti-monotonicity and succinctness , in order to reduce the frequent item-sets computation. Given an itemset X , a constraint C AM is anti-monotone if  X 
Y  X  X : C AM ( X )  X  X  AM ( Y ). The frequency constraint is the most known example of a C AM constraint. This property, the anti-monotonicity of frequency , is used by the Apriori [1] algorithm with the following heuristic: if an itemset X does not satisfy C freq , then no superset of X can satisfy C freq , and hence they can be pruned. Other C AM constraints can easily be pushed deeply down into the frequent itemsets mining computation since they behave exactly as C freq :if they are not satisfiable at an early level (small itemsets), they have no hope of becoming satisfiable later (larger itemsets).
 not, can be determined based on the singleton items which are in X .A C S con-straint is pre-counting pushable , i.e. it can be satisfied at candidate-generation time: these constraints are pushed in the level-wise computation by substituting the usual generate apriori procedure, with the proper (w.r.t. C S ) candidate gen-eration procedure. Constraints that are both anti-monotone and succinct can be pushed completely in the level-wise computation before it starts (at pre-processing time). For instance, consider the constraint min ( S.price )  X  v :ifwe start with the first set of candidates formed by all singleton items having price greater than v , during the computation we will generate only itemsets satisfying the given constraint. Constraints that are neither succinct nor anti-monotone are pushed in the CAP [15] computation by inducing weaker constraints which are either anti-monotone and/or succinct.
 Given an itemset X , a constraint C M is monotone if:  X  Y  X  X : C M ( X )  X  C M ( Y ). Since the frequent itemset computation is geared on monotone, C M constraints have been considered more hard to be pushed in the computation and less effective in pruning the search space [2, 8, 7, 12]: while anti-monotone constraints can be used to effectively prune the search space to a small downward closed collection, the upward closed collection of the search space satisfying the monotone constraints cannot be pruned at the same time. Recently, it has has been shown that a real synergy of these two opposite types of constraints exists and can be exploited by reasoning on both the itemset search space and the input database together , using the ExAnte data-reduction technique [4]. Using data reduction techniques, anti-monotone and monotone pruning strengthen each other recursively [3, 5].
 growth based methodology to push such constraints is proposed. A constraint C
CAM is convertible anti-monotone provided there is an order R on items such that whenever an itemset X satisfies C CAM , so does any prefix of X . A constraint C
CM is convertible monotone provided there is an order R on items such that whenever an itemset X violates C CM , so does any prefix of X . In [16, 17], two FP-growth based algorithms are introduced: FIC A to mine Th ( C freq )  X  Th ( C CAM ), and FIC M to mine Th ( C freq )  X  Th ( C CM ). A major limitation of any FP-growth based algorithm is that the initial database (internally compressed in the prefix-tree structure) and all intermediate projected databases must fit into main mem-ory. If this requirement cannot be met, these approaches can simply not be ap-plied anymore. This problem is even harder with FIC A and FIC M : in fact, using an order on items different from the frequency-based one, makes the prefix-tree lose its compressing power. Thus we have to manage much greater data struc-tures, requiring a lot more main memory which might not be available. This fact is confirmed by our experimental analysis reported in Section 4: sometimes FIC A is slower than FP-growth, meaning that having constraints brings no ben-efit to the computation. Another important drawback of this approach is that it is not possible to take full advantage of a conjunction of different constraints, since each constraint in the conjunction could require a different ordering of items.
 of how to push constraints which are not convertible , is [13]. The framework proposed in that paper is based on the concept of finding a witness , i.e. an itemset such that, by testing whether it satisfies the constraint we can deduce information about properties of other itemsets, that can be exploited to prune the search space. This idea is embedded in a depth-first visit of the itemsets search space. The main drawback of the proposal is the following: it may require quadratic time in the number of frequent singleton items to find a witness. The cost can be amortized if items are reordered, but this leads to the same problems discussed for FP-growth based algorithms. Moreover, even if a nearly linear time search is performed, this is done without any certainty of finding a witness which will help to prune the search space. In fact, if the witness found satisfies the given constraint, no pruning will be possible and the search time will be wasted. Our approach is completely orthogonal: while they try to explore the exponentially large search space in some smart way, we massively reduce the dataset as soon as possible, reducing at the same time the search space and obtaining a progressively easier mining problem.
 Paper Contribution. The contribution of this paper is threefold. First, we extend the actual state-of-art classification of constraints that can be pushed in a frequent pattern computation, by showing how to push tough constraints as those ones based on variance or standard deviation . Second, we show that it is possible to push convertible constraints in a level-wise Apriori-like computation, we propose a general Apriori-like algorithm, based on data-reduction techniques, which is able to push all possible kinds of constraint studied so far. In this Section we introduce a new class of tougher constraints, which is a proper superclass of convertible anti-monotone. The following example shows that exist interesting constraints which are not convertible, and thus cannot be exploited within a prefix pattern framework.
 Example 1 ( var constraint is not convertible). Calculating the variance is an important task of many statistical analysis: it is a measure of how spread out a distribution is. The variance of a set of number X is defined as: A constraint based on var is not convertible. Otherwise there is an order R of items such that var ( X ) is a prefix increasing (or decreasing) function. Con-sider a small dataset with only four items I = { A, B, C, D } with associated prices P = { 10 , 11 , 19 , 20 } . The lexicographic order R 1 = { ABCD } is such that var ( A )  X  var ( AB )  X  var ( ABC )  X  var ( ABCD ), and it is easy to see that we have only other three orders with the same property: R 2 = { BACD } , R 3 = {
DCBA } , R 4 = { CDBA } .But,for R 1 , we have that var ( BC ) var ( BCD ), which means that var is not a prefix increasing function w.r.t. R 1 .Moreover, since the same holds for R 2 , R 3 , R 4 , we can assert that there is no order R such that var is prefix increasing. An analogous reasoning can be used to show that it neither exists an order which makes var a prefix decreasing function. such as for instance those ones based on standard deviation ( std )or unbiased variance estimator ( var N  X  1 )or mean deviation ( md ), are not convertible as well. Luckily, as we show in the following, all these constraints share a nice property that we name  X  X oose Anti-monotonicity X  . Recall that an anti-monotone constraint is such that, if satisfied by an itemset then it is satisfied by all its subsets. We define a loose anti-monotone constraint as such that, if it is satisfied by an itemset of cardinality k then it is satisfied by at least one of its subsets of cardinality k  X  1. Since some of these interesting constraints make sense only on sets of cardinality at least 2, in order to get rid of such details, we shift the definition of loose anti-monotone constraint to avoid considering singleton items. Definition 1 (Loose Anti-monotone constraint). Given an itemset X with |
X | &gt; 2 , a constraint is loose anti-monotone (denoted C LAM ) if: C LAM ( X )  X   X  i  X  X : C LAM ( X \{ i } ) The next proposition and the subsequent example state that the class of C LAM constraints is a proper superclass of C CAM (convertible anti-monotone con-straints). outperforming previously proposed FP-growth based algorithms [16, 17]. Third, Proposition 1. Any convertible anti-monotone constraint is trivially loose anti-monotone: if a k-itemset satisfies the constraint so does its ( k  X  1) -prefix itemset. Example 2. We show that the constraint var ( X.A )  X  v is a C LAM constraint. Given an itemset X , if it satisfies the constraint so trivially does X \{ i } , where i is the element of X which has associated a value of A which is the most far away from avg ( X.A ). In fact, we have that var ( { X \{ i }} .A )  X  var ( X.A )  X  v , until | X | &gt; 2.Taking the element of X which has associated a value of A which is the closest to avg ( X.A ) we can show that also var ( X.A )  X  v is a C LAM constraint. Since the standard deviation std is the square root of the variance, it is straightforward to see that std ( X.A )  X  v and std ( X.A )  X  v are C LAM .The mean deviation is defined as: md ( X )=( i  X  X | i  X  avg ( X ) | ) / | X | . Once again, we have that md ( X.A )  X  v and md ( X.A )  X  v are loose anti-monotone. It is easy to prove that also constraints defined on the unbiased variance estimator, var N  X  1 =( i  X  X ( i  X  avg ( X )) 2 ) / ( | X | X  1) are loose anti-monotone. straints. The next key Theorem indicates how a C LAM constraint can be ex-ploited in a level-wise Apriori-like computation by means of data-reduction. It states that if at any iteration k  X  2 a transaction is not superset of at least one frequent k-itemset which satisfy the C LAM constraint (a solution), then the transaction can be deleted from the database.
 Theorem 1. Given a transaction database D , a minimum support threshold  X  , and a C LAM constraint, at the iteration k  X  2 of the level-wise computation, a transaction t  X  X  such that: X  X  t, | X | = k, X  X  Th ( C freq [ D , X  ] )  X  Th ( C LAM ) can be pruned away from D , since it will never be superset of any solution itemsets of cardinality &gt;k .
 Proof. Suppose that exists Y  X  t, | Y | = k + j, Y  X  Th ( C freq [ D , X  ] )  X  Th ( C LAM ). For loose anti-monotonicity this implies that exists Z  X  Y, | Z | = k + j  X  1 such that C LAM ( Z ). Moreover, for anti-monotonicity of frequency we have that C freq [ D , X  ] ( Z ). The reasoning can be repeated iteratively downward to obtain that must exist X  X  t, | X | = k, X  X  Th ( C freq [ D , X  ] )  X  Th ( C LAM ). monotone constraint anymore, and therefore each constraint in a conjunction must be treated separately. However, a transaction can be pruned whenever Theorem 1 does not hold for even only one constraint in the conjunction (this is implemented by line 14 of the pseudo-code in Figure 1).
 wise Apriori-like computation by means of data-reduction. The recently introduced algorithm ExAMiner [3], aimed at solving the prob-lem Th ( C freq )  X  Th ( C M ) (conjunction of anti-monotonicity and monotonicity), generalizes the ExAnte idea to reduce the problem dimensions at all levels of a level-wise Apriori-like computation. This is obtained by coupling the set of data reduction techniques in Table 2 (see [3] for the proof of correctness), which are based on the anti-monotonicity of C freq , with the data reduction based on the C M constraint. Here, in order to cope with the mining problem Th ( C freq )  X  Th ( C LAM ), we couple the same set of C freq -based data reduction techniques with the C LAM -based data reduction technique described in Theorem 1. The resulting algorithm is named ExAM iner LAM .
 ation k  X  1 produces a reduced dataset D k to be used at the subsequent iteration k . Each transaction in D k , before participating to the support count of candidate itemsets, is reduced as much as possible by means of C freq -based data reduction, and only if it survives to this phase, it is effectively used in the counting phase. Each transaction which arrives to the counting phase, is then tested against the C
LAM property of Theorem 1, and reduced again as much as possible, and only if it survives to this second set of reductions, it is written to the transaction database for the next iteration D k +1 . The procedure we have just described, is named count &amp; reduce LAM , and substitutes the usual support counting proce-dure of the Apriori algorithm from the second iteration on ( k  X  2). Therefore to illustrate the ExAM iner LAM algorithm we just provide the pseudo-code of the count &amp; reduce LAM procedure (Figure 1), avoiding to provide the well-known Apriori algorithm pseudo-code [1]. We just highlight the we adopt the usual no-tation of the Apriori pseudo-code: C k : to denote the set of candidate itemsets, and L k to denote the set of frequent (or large) itemsets at iteration k . k takes in input the actual database D k , the minimum support threshold  X  ,a user-defined conjunction of loose anti-monotone constraints C LAM , a user-defined conjunction of monotone constraints C M , the actual set of candidate itemsets C k , and an array of integers V k  X  1 of the size of I . Such array is used in order to implement the data-reduction G k ( i ). The array V k records, for each singleton item, the number of frequent k -itemsets in which it appears. This information is then exploited during the subsequent iteration k + 1 for the global pruning of items from all transaction in D k +1 (lines 4 and 5 of the pseudo-code). On the contrary, data reductions T k ( t )and L k ( i ) are put into effect during the same iteration in which the information is collected. Unfortunately, they require information (the frequent itemsets of cardinality k ) that is available only at the end of the actual counting (when all transactions have been used). However, since the set of frequent k -itemsets is a subset of the set of candidates C k ,we can use such data reductions in a relaxed version: we just check the number of candidate itemsets X which are subset of t ( t.count in the pseudo-code, lines 8 and 15) and which are superset of i ( i.count in the pseudo-code, lines 6, 11 and 16). Analogously, the data reduction based on loose anti-monotonicity described in Theorem 1, is exploited in the same relaxed version with candidates instead of frequent itemsets. In the pseudo-code, for each constraint C in the given conjunction of loose anti-monotone constraints C LAM ,wehaveaflag t.lam [ C ] which is set to true as soon as an itemset X  X  C k , such that X  X  t, X  X  Th ( C ), is found (line 10). A transaction which has even only one of the t.lam [ C ] flags set to false after the counting phase, will not enter in the database for the next iteration D k +1 (line 14 of the pseudo-code). In fact, such a transaction has not covered any candidate itemset which satisfies the constraint C , for some C in the conjunction C LAM , therefore it will not support any itemset satisfying such constraint, and thus any solution itemset. In this Section we describe in details the experiments we have conducted in order to assess loose anti-monotonicity effectiveness on both convertible con-straints (e.g. avg ( X.A )  X  m ) and tougher constraints (e.g. var ( X.A )  X  m ). The results are reported in Figure 2. All the tests were conducted on a Windows XP PC equipped with a 2.8GHz Pentium IV and 512MB of RAM memory, within the cygwin environment. The datasets used in our tests are those ones of the FIMI repository 1 , and the constraints were applied on attribute values generated randomly with a gaussian distribution within the range [0 , 150000].
 var ( X.A )  X  m . We compare ExAM iner LAM against two unconstrained com-putation: FP-Growth and ExAMiner without constraint (i.e. it only exploits C freq -based data reduction). Such tests highlight the effectiveness of loose anti-monotonicity: we have a speed up of much more than one order of magnitude, and a data reduction rate up to four order of magnitude.
 order of magnitude faster than ExAMiner as reported in Figure 2(c). Con-versely, FIC A is not able to bring such improvements. In Figure 2(d) we re-port the speed-up of ExAM iner LAM w.r.t. ExAMiner and FIC A w.r.t. FP-growth. The tests conducted on various datasets show that exploiting loose anti-monotonicity property brings a higher speed up than exploiting convertibility. In fact, ExAM iner LAM exhibits in average a speed up of factor 100 against its own unconstrained computation, while FIC A always provides a speed up w.r.t. FP-growth of a factor lower than 10, and sometimes it is even slower than its unconstrained version. In other words, FP-Growth with a filtering of the output in some cases is better that its variant FIC A , which is explicitly geared on con-strained mining. As discussed before, this is due to the items ordering based on attribute values and not on frequency. As already stated, one of the most important advantage of our methodology is that, pushing constraints by means of data-reduction in a level-wise framework, we can exploit different properties of constraints all together, and the total ben-efit is always greater than the sum of the individual benefits. In other words, by means of data-reduction we exploit a real synergy of all constraints that the user defines for the pattern extraction: each constraint does not only play its part in reducing the data, but this reduction in turns strengthens the pruning power of the other constraints. Moreover data-reduction induces a pruning of the search space, and the pruning of the search space in turn strengthens future data reductions.
 of
C LAM and a set of C M constraints: obviously if the set of C LAM constraints is empty we obtain the standard ExAMiner count &amp; reduce [3] (no C LAM data reduction); while if we have an empty set of C M constraints, the C M testing (lines 7 and 17 of the pseudo code) always succeed and thus the  X  -reduction is never applied. Whenever we have both C M and C LAM constraints (i.e. a query corre-sponding to the mining problem Th ( C freq )  X  Th ( C M )  X  Th ( C LAM )) we can benefit of all the data-reduction techniques together, obtaining a stronger synergy. Example 3. The constraint range ( S.A )  X  v  X  max ( S.A )  X  min ( S.A )  X  v ,is both monotone and loose anti-monotone. Thus, when we mine frequent itemsets which satisfy such constraint we can exploit the benefit of having together, in the same count &amp; reduce LAM procedure, the C freq -based data reductions of Table 2, the  X  -reduction for monotone constraints, and the reduction based on C LAM . different properties of constraints all together. In other words, our contribution can be easily integrated with previous works (e.g. [15, 3]), in a unique Apriori-like computational framework able to take full advantage by any conjunction of possible constraints. In particular, anti-monotone ( C AM ) constraints are ex-ploited to prune the level-wise exploration of the search space together with the frequency constraint ( C freq ); succinct ( C S ) constraints are exploited at candi-date generation time as done in [15]; monotone ( C M ) constraints are exploited by means of data reduction as done in [3]; convertible anti-monotone ( C CAM ) and Loose anti-monotone ( C LAM ) constraints are exploited by means of data reduction as described in this paper.
 tional framework (within the P 3 D project 2 ) which will be soon made available to the community.

