 1. Introduction
Information about anaphoric relations could be beneficial for applications such as summarization and seg-mentation that involve extracting (possibly very simplified) discourse models from text. In this work we inves-aspects of the summarization task. First of all, we used anaphoric information to enrich the latent semantic representation of a document, from which a summary is then extracted. Secondly, we used anaphoric infor-mation to check that the anaphoric expressions contained in the summary thus extracted still have the same interpretation that they had in the original text.
 Our approach to summarization follows what has been called a term-based summarization, the most important information in a document is found by identifying its main  X  X erms X  (also sometimes called  X  X opics X ), and then extracting from the document the most important information based X  approaches. Lexical approaches to summarization use word similarity and other lexical relations to iden-tify central terms ( Barzilay &amp; Elhadad, 1997 ); we would include among these previous approaches based on ( Landauer &amp; Dumais, 1997 ), such as Gong and Liu (2002) and Steinberger and Jezek (2004) . Coreference-or anaphora-based approaches 1 ( Baldwin &amp; Morton, 1998; Bergler, Witte, Khalife, Li, &amp; Rudzicz, 2003;
Boguraev &amp; Kennedy, 1999; Stuckardt, 2003 ) identify these terms by running a coreference-or anaphoric resolver over the text. We are not aware, however, of any attempt to use both lexical and anaphoric information to identify the main terms, other than our own previous work ( Steinberger, Kabadjov, &amp; Poesio, 2005 ).
In this paper, we present a new LSA -based sentence extraction summarizer which uses both lexical and ana-phoric information. We already found in previous work that feeding (automatically extracted) information about anaphoric relations to a summarizer can improve its performance ( Steinberger et al., 2005 ). In the work discussed here, however, we improve our previous methods in several respects. Firstly of all, we improved our methods for extracting a summary from a document in two ways. Firstly, we developed an improved version which are often used to indicate key terms. Secondly, we improved our method for extracting summaries from an LSA -style representation by developing a new approach for finding the dimensionality reduction on the basis of the required (percentage) size of the summary. The new system was evaluated on the standard reference corpus from DUC-2002, making it possible to compare its performance not only with that of two summarizers using only lexical information, but also with that of the other systems participating in DUC-2002, using the standard ROUGE evaluation measure.

In addition, we also propose here a method for using anaphoric information to check the entity-coherence of a summary once this has been extracted. Summarization by sentence extraction may produce summaries with  X  X angling X  anaphoric expressions  X  expressions whose antecedent has not been included in the summary, and therefore cannot be interpreted or are interpreted incorrectly. Our algorithm checks that the interpreta-tion of anaphoric expressions in a summary is consistent with their interpretation in the original text. The algorithm can be used irrespective of whether the summary was produced using our own methods or other methods.

The structure of the paper is as follows. In Section 2 , some background information is presented. Our pre-vious work using pure Latent Semantic Analysis ( LSA ) for summarization is discussed, and we present two based summarizers that only use lexical information to identify the main topics of a document. We then make the case for using anaphoric information as well as lexical information, and introduce our anaphora resolution system (Section 3 ). Then, in Section 4 , we discuss our methods for using anaphoric information in their evaluation using the DUC-2002 corpus. In Section 5 we present the last step in our summarization approach, the summary reference checker. In the end, we discuss our vision of applying the presented ideas to multi-document summarization. 2. Using LSA for summarization 2.1. Previous work
LSA ( Landauer &amp; Dumais, 1997 ) is a technique for extracting the  X  X idden X  dimensions of the semantic rep-resentation of terms, sentences, or documents, on the basis of their use. It has been extensively used in edu-cational applications such as essay ranking ( Landauer &amp; Dumais, 1997 ), as well as in including information retrieval ( Berry, Dumais, &amp; O X  X rien, 1995 ) and text segmentation ( Choi, Wiemer-Hastings, &amp; Moore, 2001 ).
 More recently, a method for using LSA for multi-and single-document summarization has been proposed in
Gong and Liu (2002) . This purely lexical approach is the starting point for our own work. The heart of Gong and Liu X  X  method is a document representation developed in two steps. The first step is the creation of a term by sentences matrix A =[ A 1 , A 2 , ... , A n ], where each column A of sentence i in the document under consideration. The vector A where t ji denotes the frequency with which term j occurs in sentence i , L ( t to work best is using a binary local weight and an entropy-based global weight: number of sentences in the document.

If there are m terms and n sentences in the document, then we will obtain an m  X  n matrix A for the doc-ument. The next step is to apply Singular Value Decomposition ( A is defined as where U =[ u ij ]isan m  X  n column-orthonormal matrix whose columns are called left singular vectors. R = sorted in descending order. V =[ v ij ]isan n  X  n orthonormal matrix, whose columns are called right singular vectors. The dimensionality of the matrices is reduced to r most important dimensions and thus, U is m  X  r , R is r  X  r and V T is r  X  n matrix.

From a mathematical point of view, SVD derives a mapping between the m -dimensional space specified by the weighted term-frequency vectors and the r -dimensional singular vector space.

From an NLP perspective, what SVD does is to derive the latent semantic structure of the document repre-sented by matrix A : i.e. a breakdown of the original document into r linearly-independent base vectors which express the main  X  X opics X  of the document. SVD can capture interrelationships among terms, so that terms and sentences can be clustered on a  X  X emantic X  basis rather than on the basis of words only. Furthermore, as dem-tern will be captured and represented by one of the singular vectors. The magnitude of the corresponding singular value indicates the importance degree of this pattern within the document. Any sentences containing this word combination pattern will be projected along this singular vector, and the sentence that best repre-sents this pattern will have the largest index value with this vector. Assuming that each particular word com-bination pattern describes a certain topic in the document, each singular vector can be viewed as representing such a topic ( Ding, 2005 ), the magnitude of its singular value representing the degree of importance of this topic.

The summarization method proposed by Gong and Liu (2002) uses the representation of a document thus obtained to choose the sentences to go in the summary on the basis of the relative importance of the  X  X opics X  they mention, described by the matrix V T . The summarization algorithm simply chooses for each  X  X opic X  the the k th right singular vector in matrix V T . 2.2. Sentence selection by vector length
The main drawback of Gong and Liu X  X  method is that when l sentences are extracted the top l topics are treated as equally important. As a result, a summary may include sentences about  X  X opics X  which are not par-ticularly important.
 Ding (2005) proved that the statistical significance of each by changing the selection criterion to include in the summary the sentences whose vectorial representation in each  X  X opic X . Intuitively, the idea is to choose the sentences with greatest combined weight across all topics, possibly including more than one sentence about an important topic, rather than always choosing one sen-tence for each topic as done by Gong and Liu. More formally: after computing the matrix, we compute matrix B :
Then, we measure the length s k of each sentence vector in B : for summarization too. We then include in the summary the sentences with the highest values in vector s .We
Liu X  X  method. 2.3. Automatic dimensionality reduction The algorithm proposed by Steinberger and Jezek (2004) still requires a method for deciding how many dimensions/topics to include in the latent space and therefore in the summary. If we take too few, we may lose topics which are important from a summarization point of view. But if we take too many, we end up including less important topics. One of the novelties of the present work with respect to our own previous proposals is a new approach for computing this level of dimensionality reduction ( r ) automatically.

When we perform SVD on an m  X  n matrix, we can view the new dimensions as some sort of pseudo sen-within the document. From a summarization point of view, the number of extracted sentences is dependent on the summary ratio. We know what percentage of the full text the summary should be: part of the input to the summarizer is that a p % summary is needed. (The length is usually measured in the number of words, but there ply extract the top r pseudo sentences, where r = p /100 * that  X  X verlap the most X  in terms of vector length with top r pseudo sentences. In addition, our algorithm takes into account the significance of each dimension by multiplying the matrix V
The summarizer can thus automatically determine the number of significant dimensions dependent on the summarization ratio. The larger the summary (measured in the percentage of the full text), the more topics are considered important in the process of summary creation. And because we know the contribution of each topic from the square of its singular value we can measure how much information is considered important by the dimensionality reduction approach for each full text percentage. Fig. 1 shows the logarithmic depen-dency between summary ratio and sum of relative significances of r most important dimensions a 10% summary contains the sentences that best cover 40% of document information, whereas a 30% summary will contain the sentences that most closely include 70% of document information. 3. Using anaphora resolution to find the most important terms 3.1. Motivations
Boguraev and Kennedy (1999) use the following news article to illustrate why being able to recognize ana-phoric chains may help in identifying the main topics of a document.
As Boguraev and Kennedy point out, the title of the article is an excellent summary of the content: an entity and the pope are the central characters is crucial to provide a summary of texts like these. that help us to identify such  X  X ain characters, X  the fact that an entity is repeatedly mentioned is clearly important.

Methods that only rely on lexical information to identify the main topics of a text, such as the lexical-based methods discussed in the previous section, can only capture part of the information about which entities are frequently repeated in the text. As example (7) shows, stylistic conventions forbid verbatim repetition, hence the six mentions of Fernandez in the text above contain only one lexical repetition,  X  Fernandez  X . The main problem are pronouns, that tend to share the least lexical similarity with the form used to express the ante-cedent (and anyway are usually removed by stopword lists, therefore do not get included in the the antecedent was expressed with a proper name. The form of mention which more often overlaps to a degree with previous mentions is proper nouns, and even then at least some way of dealing with acronyms is neces-sary (cfr. European Union / E.U. ). On the other hand, it is well-known from the psychological literature that proper names often are used to indicate the main entities in a text. What anaphora resolution can do for us is to identify which discourse entities are repeatedly mentioned, especially when different forms of mention are used. We can then use the anaphoric chains identified by the anaphoric resolvers as additional terms in the initial matrix A in (4) .
The anaphora resolution system we use, GUITAR ( Poesio &amp; Kabadjov, 2004; Kabadjov, Poesio, &amp; Steinber-the-shelf component of a NLP pipeline. In our previous work ( Steinberger et al., 2005 ) we used a version that could only resolve pronouns and definite descriptions. However, as example (7) shows, proper nouns such as
Juan Fernandez Krohn (and quasi-proper nouns such as the Pope ) are generally used in at least one mention of the system, also able to link proper nouns in coreference chains. 3.2.1. Preprocessing The anaphora resolution proper part of GUITAR is designed to take MAS-XML , and produce an output in the same format, but which additionally contains anaphoric annotation.
The system can therefore work with a variety of preprocessing methods, ranging from a simple part-of-speech tagger to a chunker to a full parser, provided that appropriate conversion routines into mented. The version used for these experiments uses Charniak X  X  parser ( Charniak, 2000 ). 3.2.2. Anaphora resolution algorithms We used two versions of the system in the experiments discussed in this paper. The earlier version, 2.1, includes an implementation of the MARS pronoun resolution algorithm ( Mitkov, 1998 ) to resolve personal and possessive pronouns. This system resolves definite descriptions using a partial implementation of the algo-rithm proposed in Vieira and Poesio (2000) , augmented with a statistical discourse new classifier. The latest version, GUITAR 3.2, includes also an implementation of the shallow algorithm for resolving coreference with proper names proposed by Bontcheva, Dimitrov, Maynard, Tablan, and Cunningham (2002) . 3.2.3. Evaluation
GUITAR has been evaluated over a variety of corpora. We report here the results with two different corpora: a corpus in which noun phrases have been identified by hand  X  the texts from different domains  X  and a corpus in which noun phrases are identified by the Charniak parser, 37 corpus). We expect the performance of the anaphoric resolver on the DUC corpus to be similar to its perfor-mance on the CAST corpus. The results of version 3.0 of the system with each corpus, and each type of ana-phoric expression, are summarized in Table 1 .
 4. Combining lexical information and anaphoric information to build the LSA representation  X  X urely lexical X  LSA determines the main  X  X opics X  of a document on the basis of the simplest possible notion easily integrated in an mixed lexical/anaphoric LSA representation by generalizing the notion of  X  X erm X  used in whenever the anaphoric resolver identifies a noun phrase occurring in s as a mention of d . 4.1. SVD over word terms and discourse entities The simplest way of using anaphoric information with LSA is the words as terms, and use anaphora resolution as a preprocessing stage of the anaphoric chain. In example (7) , for example, all occurrences of elements of the anaphoric chain beginning with A Spanish priest would be substituted by A Spanish priest . The resulting text would be as follows:
This text could then be used to build an LSA representation as discussed in the previous section. We will show shortly, however, that this simple approach does not lead to improved results.

A better approach, it turns out, is what we call the ADDITION METHOD anaphoric chains as another type of  X  X erm X  that may or may not occur in a sentence. The idea is illustrated in in the sense of discourse entities, represented by anaphoric chains. The representation of a sentence then
With this representation, the chain  X  X erms X  may tie together sentences that contain the same anaphoric chain even if they do not contain the same word. The resulting matrix would then be used as input to before. 4.2. First experiments: the CAST corpus A pilot evaluation of the methods just discussed using an early version of novel experiments with the DUC-2002 corpus, discussed in the following section.
 4.2.1. The CAST corpus In this pilot evaluation, we used the corpus of manually produced summaries created by the ( Orasan et al., 2003 ). The CAST corpus contains news articles taken from the Reuters Corpus and a few popular science texts from the British National Corpus. Summaries are specified by providing information about the the summary. The corpus also contains annotations for linked sentences, which are not significant enough to be marked as important/essential, but which have to be considered as they contain information essential for the understanding of the content of other sentences marked as essential/important.

Four annotators were used for the annotation, three graduate students and one postgraduate. Three of the annotators were native English speakers, and the fourth had advanced knowledge of English. Unfortunately, not all of the documents were annotated by all of the annotators. To maximize the reliability of the summaries used for evaluation, we chose the documents annotated by the greatest number of the annotators; in total, our evaluation corpus contained 37 documents.

For acquiring manual summaries at specified lengths and getting the sentence scores (for relative utility evaluation) we assigned a score 3 to the sentences marked as essential, a score 2 to important sentences and a score 1 to linked sentences. 4.2.2. Evaluation measures Evaluating summarization is a notoriously hard problem, for which standard measures like Precision and with such measures is that human judges often disagree on what are the top N most important sentences in a document, so by using precision and recall, two equally good summaries may be judged very differently. Sup-pose that a manual summary contains sentences [1 2] from a document, but that sentence 3 is an equally good alternative to sentence 2. Suppose now that two systems, A and B, produce summaries consisting of sentences [1 2] and [1 3], respectively. Using precision and recall as evaluation measures, system A will be ranked much higher than system B, whereas if sentences 2 and 3 are equally important, the two systems should get the same score.

In this early study, we addressed the problem by using a combination of evaluation measures. As a main measure we chose relative utility ( RU )( Radev et al., 2000 ) because it could be computed automatically given the already existing annotations in the CAST corpus. RU allows model summaries to consist of sentences with variable ranking. With RU , the model summary represents all sentences of the input document with confidence values for their inclusion in the summary. For example, a document with five sentences [1 2 3 4 5] is repre-sented as [1/5 2/4 3/4 4/1 5/2]. The second number in each pair indicates the degree to which the given sen-tence should be part of the summary according to a human judge. This number is called the utility of the sentence. Utility depends on the input document, the summary length, and the judge. In the example, the sys-both summaries [1 2] and [1 3] carry the same number of utility points (5 + 4). Given that no other combina-tion of two sentences carries a higher utility, both systems [1 2] and [1 3] produce optimal extracts. To com-pute relative utility, a number of judges, ( N P 1) are asked to assign utility scores to all n sentences in a document. The top e sentences according to utility score 4 can then define the following system performance metric: where u ij is a utility score of sentence j from annotator i , using the standard formula: where X and Y are representations of a system summary and its reference summary based on the vector space model. The third measure is Main Topic Similarity . This is a content-based evaluation method based on mea-suring the cosine of the angle between first left singular vectors of a system summary X  X  and its reference sum-mary X  X  SVD s. (For details see Steinberger &amp; Jezek (2004) .) 4.2.3. How much may anaphora resolution help? An upper bound
In order to determine whether anaphoric information might help, and which method of adding anaphoric knowledge to the LSA summarizer is best, we annotated by hand all the anaphoric relations in the 37 docu-ments in the CAST corpus using the annotation tool MMAX ( Mueller &amp; Strube, 2003 ).

Results for the 15% (resp. 30%) summarization ratio using a variety of summarization evaluation measures are presented in Table 2 (resp. Table 3 ). The tables show that even with perfect knowledge of anaphoric links, the performance when using Substitution method does not change much. The problem that happened in some of the documents was that SVD deteriorated when a frequently used entity was substituted by its full nominal expression. As a result, the score of the sentence was extremly boosted when it contained the mention of the entity. And thus, sentences that contained the mention of this entity were all considered important, no matter what else they contained.

On the other hand, the addition method could potentially lead to substantial improvements. 4.2.4. Results with GUITAR 2.1
To use GUITAR , we first parsed the texts using Charniak X  X  parser ( Charniak, 2000 ). The output of the parser was then converted into the MAS-XML format expected by GUITAR the system. (This step includes heuristic methods for guessing agreement features.) Finally, add anaphoric information to the files. The resulting files were then processed by the summarizer.
GUITAR achieved a precision of 56% and a recall of 51% over the 37 documents. For definite description resolution, we obtained a precision of 69% and a recall of 53%; for possessive pronouns resolution, precision was 53%, recall was 53%; for personal pronouns, precision was 44%, recall was 46%.

The results obtained by the summarizer using GUITAR  X  X  output are presented in Tables 4 and 5 (relative utility, f-score, cosine, and main topic).

Tables 4 and 5 clearly show that using GUITAR and the addition method leads to significant improvements over our baseline LSA summarizer. The improvement in Relative Utility measure was significant (95% confi-dence by the t -test). On the other hand, the substitution method did not lead to significant improvements, as was to be expected given that no improvement was obtained with  X  X erfect X  anaphora resolution (see previ-ous section). 4.2.5. ROUGE evaluation of pilot study
We also evaluated the results using the ROUGE measure (see Section 4.3.3 ) X  Tables 6 and 7  X  obtaining improvements with the addition method, but the differences were not statistically significant. 4.2.6. Pilot study conclusion
In conclusion, this pilot study showed that (i) we could expect performance improvements over purely lexical LSA summarization using anaphoric information, (ii) that significant improvements at least by the
Relative Utility score could be achieved even if this anaphoric information was automatically extracted, and (iii) that, however, these results were only achievable using the Addition method.

What this earlier work did not show was how well are our results compared with the state of the art, as measured by evaluation over a standard reference corpus such as DUC-2002, and using the by now standard
ROUGE measure. Furthermore, these results were obtained using a version of the anaphoric resolver that did not attempt to identify coreference links realized using proper names, even though proper names tend to be used to realize more important terms. Given our analysis of the effect of improvements to the anaphoric resolver ( Kabadjov et al., 2005 ), we expected an improved version to lead to better results. The subsequent experiments were designed to address these questions. 4.3. Experiments with the DUC-2002 corpus
In the work just discussed we had not compared our purely lexical summarizer with other summarizers, which raised the question of whether the improvements we found were simply due to the poor quality of the original summarizer. In addition, we only achieved non-significant improvements by the
To address these problems with our earlier work, we developed an improved version of the system, and we evaluated both the lexical and the anaphoric + lexical summarizers using the DUC-2002 corpus and the ROUGE measure, which would make it easier to contrast our results with those published in the literature. 4.3.1. A new anaphoric resolver and a new summarizer
The version of our system used for this second experiment differs from the versions discussed in previous work in two respects. First of all, we developed the new method for automatic dimensionality reduction, dis-cussed in 2.3 . Secondly, we developed a new version of our anaphoric resolver, above, also resolves proper names.

We expected this new version could lead to improvements in performance, as well as being more usable. 4.3.2. The DUC-2002 corpus
DUC-2002 included a single-document summarization task, in which 13 systems participated. 2002 is the last version of DUC that included single-document summarization evaluation of informative summaries.
Later DUC editions (2003 and 2004) contained a single-document summarization task as well, however only very short summaries (75 Bytes) were analyzed. However, we are not focused on producing headline-length summaries. The DUC-2002 corpus used for the task contains 567 documents from different sources; 10 asses-sors were used to provide for each document two 100-word human summaries. In addition to the results of the 13 participating systems, the DUC organizers also distributed baseline summaries (the first 100 words of a document). The coverage of all the summaries was assessed by humans. 4.3.3. The ROUGE evaluation metric In DUC-2002, the SEE evaluation tool was used, but in later editions of the initiative the introduced ( Lin &amp; Hovy, 2003 ), which is now standard. We used that participated in DUC.
 The ROUGE -N score is computed as follows: where Count match ( gram n ) is the maximum number of n -grams co-occurring in a candidate summary and a ref-erence summary (from a reference summary set-RSS ) and Count ( gram erence summary. (Notice that the average n -gram coverage score, C ROUGE , such as ROUGE-L  X  X  longest common subsequence measure X  X nd the addition of unigrams as counting unit ( Lin, 2004 ).
 As (11) shows, ROUGE-N is actually a family of metrics; however, different ways with human assessments. As shown in Table 8 , there is a strong correlation between humans and (and ROUGE-L ). However, we used all those four ROUGE scores to determine significance results. 4.3.4. ROUGE evaluation over DUC-2002 data We show in Table 9 the ROUGE scores 6 of two purely lexical approach) and L e LSA (Length strategy, our approach); of our summarizer combining both lexical and ana-phoric information ( L e LSA+AR ); and of the 13 systems which participated in DUC-2002. and a random summarizer (the lowest baseline). Table 10 shows a multiple comparison of between systems. Systems not sharing a common letter are significantly different (at the 95% confidence level). The first result highlighted by these tables is that the two dimensionality reduction methodology are state of the art. The performance of cantly worse only than that the best system in DUC-2002, system 28, in and significantly better than that of 9 in ROUGE-1 ,7in ROUGE-2 that participated in that competition. The second result is that both
Gong and Liu X  X  LSA approach ( GLLSA ). However, our L e LSA+AR the baseline in ROUGE-L at the 90% confidence level, and it is not significantly worse than any of the systems. 4.4. An example: a summary before and after anaphora resolution Examples (12) and (13) illustrate the difference between a summary created by the pure the corresponding one created by the summarizer enhanced by anaphora resolution (addition method).
From the examples it can be seen that the first two sentences selected by the summarizers are the same, whereas the third one is different. When using anaphora resolution, sentence selection was affected by strong together throughout the process influencing the outcome of that summarizer.
 document were significantly better when anaphora resolution was used. 5. A summary (entity) coherence checker
Anaphoric expressions can only be understood with respect to a context. This means that summarization by sentence extraction can wreak havoc with their interpretation: there is no guarantee that they will have an interpretation in the context obtained by extracting sentences to form a summary, or that this interpretation will be the same as in the original text. Consider the following example.
 ( S1 ) [Prime Minister Margaret Thatcher] 1 said Monday [[the Irish Republican Army] ( S2 )  X  X  X The young men whom we lost] 4 were murdered by [common murderers who must be found and ( S3 ) [Gerry Adams, president of [Sinn Fein, the legal political arm of [the IRA] ( S4 )  X  X  X We] 8 want an end to all violent deaths arising out of the present relationship between our two coun-
If sentence S2 were to be extracted to be part of the summary, but S1 was not, the pronoun she would not be understandable as it would not have a matching antecedent anymore. The reference to the school would also be uninterpretable. The same would happen if S5 were extracted without also extracting S2; in this case, the problem would be that the antecedent for the barracks is missing.

Examples such as the one just shown suggested another use for anaphora resolution in summarization X  correct-ing the references in the summary. Our idea was to replace anaphoric expressions with a full noun phrase in the cases where otherwise the anaphoric expression could be misinterpreted. We discuss this method in detail next. 5.1. The reference correction algorithm
Our correction algorithm works as follows: (1) Run anaphora resolution over the source text, and create anaphoric chains. (2) Identify the sentences to be extracted using a summarization algorithm such as the one discussed in the (3) For every anaphoric chain, replace the first occurrence of the chain in the summary with its first occur-(4) Run the anaphoric resolver over the summary.
This method can be used in combination with the summarization system discussed in earlier sections, or with other systems; and becomes even more important when doing sentence compression, because intrasenten-tial antecedents can be lost as well. However, automatic anaphora resolution can introduce new errors. We discuss our evaluation of the algorithm next. 5.2. Evaluation
To measure the recall of the reference checker algorithm we would need anaphoric annotations, that were not available for DUC data. We measured its precision manually as follows. To measure the precision of the step where the first occurrences of a chain in the summary were replaced by the first mention of that chain in the source text, we took a sample of 155 documents 8 and measured precision by hand, obtaining the results shown in Table 11 .

We can observe that full texts contained on average 19 anaphoric chains, and summaries about 7. In 66% of the summary chains the sentence where the first chain occurrence appeared was selected into the summary, and in 9% there was no need to replace the expression because it already had the same form as the first element of the chain. So overall the first chain occurrence was replaced in 25% of the cases; the precision was 68.6%. This suggest that the success in this task correlates with anaphora resolver X  X  quality.

After performing anaphora resolution on the summary and computing its anaphoric chains, the anaphors without an antecedent are replaced. We analyzed a sample of 86 documents
Overall, 145 correct replacements were made in this step and 65 incorrect, for a precision of 69%. Table 12 analyzes the performance on this task in more detail.

The first row of the table lists the cases in which an expression was placed in a chain in the summary, but not in the source text. In these cases, our algorithm does not replace anything.

Our algorithm however does replace an expression when it finds that there is no chain assigned to the expression in the summary, but there is one in the source text; such cases are listed in the second row. We
The third row lists summarizes the most common case, in which the expression was inserted into the same element of the chain in the source text.) When this happens, in 83% of cases no replacement is necessary.

Finally, there are two subcases in which different chains are found in the source text and in the summary (in this case the algorithm performs a replacement). The fourth row lists the case in which the original chain is the cases in which the anaphor was correctly resolved in the summary but it was substituted by an incorrect expression because of a bad full text resolution; the second column shows the cases in which the anaphor was incorrectly resolved in both the full text and the summary, however, replacement was performed because the expression was placed in different chains. 5.3. A summary before and after reference checking
Examples (15) and (16) illustrate the difference between a summary before and after reference checking. A reader of (15) may not know who the 71-year-old Walton or Sively are, and what store it is the text is talking about. In addition, the pronoun he in the last sentence is ambiguous between Walton and Sively . On the other lems are fixed by step 3 of the summary coherence checker algorithm. The ambiguous pronoun he in the last sentence of the summary is resolved to Sively in the summary and Walton in the source text. anaphor occurs in different chains in the summary and in the full text, it has to be substituted by the head of the first chain occurrence noun phrase, Walton . The singer in the last sentence is resolved identically in the summary and in the full text: the chains are the same, so there is no need for replacement.
The 71-year-old Walton , considered to be one of the world X  X  richest people, grabbed a note pad Tuesday quickly. Walton , known for his down-home style, made a surprise visit to the store that later Tuesday staged a concert by country singer Jana Jea in its parking lot. Walton often attends promotional events for his Arkan-sas-based chain, and Sively said he had suspected the boss might make an appearance. He also joined the singer on stage to sing a duet and led customers in the Wal-Mart cheer. Wal-Mart founder Sam Walton , considered to be one of the world X  X  richest people, grabbed a note pad
Tuesday evening and started hand-writing merchandise prices for customers so their bills could be tallied
Panhandle city that later Tuesday staged a concert by country singer Jana Jea in its parking lot. Walton often attends promotional events for his Arkansas-based chain, and store manager Paul Sively said he had suspected the boss might make an appearance. Walton also joined the singer on stage to sing a duet and led customers in the Wal-Mart cheer. 6. Future work: multi-document summarization We are currently working to apply the methods proposed here to multi-document summarization.
The single-document LSA approach can be easily extended to process multiple documents by including all sentences in a cluster of documents in the SVD input matrix. The latent space would be then reduced to r dimen-sions according to the dimensionality reduction approach as done currently (see Section 2.3 ). The sentence selection approach can be used as well; however, care has to be taken to avoid including very similar sentences from different documents. Therefore, before including a sentence in the summary we have to check if there are any sentences whose similarity with the observed one is above a given threshold. (The easiest way of measuring the similarity between two sentences is to measure the cosine of the angle between them in the term space.)
Cross-document coreference, on the other hand, is a fairly different task from within-document coreference, as even in the case of entities introduced using proper names one cannot always assume that the same object is intended, let alone in the case of entities introduced using definite descriptions. We are currently working on this problem. 7. Conclusion and further research
In this paper we presented three main contributions. First of all, we developed a method for using both anaphoric and lexical information to build an LSA representation which works well for summarization and, we believe, can also be used in other applications  X  e.g., in our work on text segmentation ( Kabadjov, 2007 ). This method involves two novel ideas. First of all, we improved the Gong and Liu method for extract-ing a summary from an LSA representation, by providing a new method for automatically determining the dimensionality reduction given the summary ratio. Secondly, we developed the Addition method for incorpo-rating anaphoric information in an LSA matrix. Combined, these two developments yield a summarizer which performs as well as the best DUC-2002 systems, and better than our purely lexical-based summarizer, even if this system is already performing as well as some of the best DUC systems. In other word, adding anaphoric information yields an improvement in performance even if the performance of our anaphoric resolver is still far from perfect.

The third contribution is a method for using anaphoric information to make the text produced by the sum-marizer more coherent by looking for anaphors that may be interpreted incorrectly in the summary, and replacing them with the original expressions. The precision at this task is 69%.

Combined, these contributions suggest that using anaphoric information does indeed lead to better summa-rizers. In future work, we plan to apply these ideas to multi-document summarization, also using sentence compression algorithms.
 Acknowledgement This research was partly supported by project 2C06009 (COT-SEWing).
 References
