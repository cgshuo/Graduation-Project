 ORIGINAL PAPER Tony M. Rath  X  R. Manmatha Abstract Searching and indexing historical handwrit-ten collections are a very challenging problem. We de-scribe an approach called word spotting which involves grouping word images into clusters of similar words by using image matching to find similarity. By anno-tating  X  X nteresting X  clusters, an index that links words to the locations where they occur can be built auto-matically. Image similarities computed using a number of different techniques including dynamic time warping are compared. The word similarities are then used for clustering using both K-means and agglomerative clus-tering techniques. It is shown in a subset of the George Washington collection that such a word spotting tech-nique can outperform a Hidden Markov Model word-based recognition technique in terms of word error rates. 1 Introduction Traditional libraries contain an enormous amount of handwritten historical documents that they would like to make available electronically on the Internet or on digi-tal media. Examples include the collections of Presidents like George Washington at the Library of Congress and scientists like Isaac Newton at the Uni-versity of Cambridge Library. Such large collections (George Washington X  X  handwritten papers exceed 140,000 pages.) can only be accessed efficiently if a searchable or browsable index exists, just like in the back of a book. The current state-of-the-art approach to this task is to manually create an index for the collec-tion. Since manual indexing is expensive, automation is desirable in order to reduce costs.

Automatic handwriting recognition has seen great improvements over the last decades and the error rates have dropped to a level that makes commercial appli-cations feasible. Offline handwriting recognition, where only an image of the produced writing is available, has only been successful in domains with very limited vocab-ularies, such as automatic mail sorting and check pro-cessing. In addition, these domains usually provide good quality images, while the quality of historical documents is often significantly degraded due to faded ink, stained paper, and other adverse factors (see Fig. 1). Conse-quently, traditional optical character recognition (OCR) techniques that usually recognize words character-by-character, fail when applied to historical manuscripts.
For collections of handwritten manuscripts written by a single author (or a few authors), 1 the images of multiple instances of the same word are likely to look similar. Wordspotting, which was initially proposed by Manmatha et al. [20], treats a collection of documents as a collection of word images. First the document is seg-mented into word images. The idea of wordspotting (see Fig. 2) is to use image matching for calculating pairwise  X  X istances X  between word images, which can be used to cluster all words occurring in a collection of hand-written documents. Ideally, each cluster would contain all the words with a particular annotation. Clusters that contain terms which are  X  X nteresting X  for an index for the document collection are selected and labeled man-ually. By assigning the cluster labels to all word images contained in a cluster, we get a partial transcription of the document collection. This in turn allows us to cre-ate a partial index for the collection, which allows us to retrieve text portions that contain only the manually assigned labels.

Historical handwritten documents are often of poor quality, and unlike printed documents, there is a varia-tion in the way the words are written. The George Wash-ington collection, for which we present results here, was not scanned from the originals, but from micro-films which further decreases the quality. Thus, both segmentations of a page into words and the matching of word images (i.e., finding similarity) are challenging problems for such documents. Previous work by [17,18] has dealt with the problem of segmenting such images of historical documents. We, therefore, use the output of their algorithm as input to our system. The image matching problem is difficult and has prompted a num-ber of publications that propose algorithms and features for the approach [16,19,28 X 30,32].

In this paper, we discuss an approach to matching images using dynamic time warping (DTW) of profiles of the word images. DTW has been widely used to match 1-D signals in the speech processing, bio-informatics, and also the online handwriting communities. DTW can handle local distortions in word images and is not re-stricted to a single global transform. DTW is compared with a number of other techniques including XOR, affine-corrected Euclidean Distance Matching, shape context [2], intensity correlation using sum-of-squared differences, an affine matching point matching algorithm due to Scott and Longuet-Higgins [36] and a point cor-relation voting algorithm [32]. We show that for image matching DTW outperforms all other algorithms (the point correlation voting algorithm comes close).
Previous work on word spotting has focused entirely on the matching process. We go further in this paper and cluster the results of the matching algorithm. K-means and a number of different agglomerative clustering algo-rithms are compared. We simulate the cluster annota-tion process and show that in terms of word error rates (WERS) this approach outperforms one previously pub-lished HMM word recognizer on a standard subset of the George Washington collection. DTW, while faster than many of the other algorithms suggested, is computa-tionally expensive. An alternative approach, which com-putes a discrete Fourier representation from the profile features, overcomes this disadvantage. The clustering is done directly using these DFT features. The results show that this approach can outperform even DTW while still being relatively fast. We believe that this paper shows that the word spotting approach is practical and may be used to create indices or even retrieve handwritten pages given text queries.
 The rest of this paper is organized as follows. Section 2 discusses related work in this area. This is followed by a discussion of which clusters are  X  X nterest-ing X  to index based on Luhn X  X  ideas [15]. Profile features are introduced in Sect. 4, followed by Sect. 5 on word image matching. The DTW algorithm is explained and its results compared with other techniques for matching word images. Clustering is discussed in Sect. 6 followed by the conclusion. 2 Related work There are at least three different ways of approach the problem of indexing/retrieving historical documents.
As mentioned in the introduction, the word spot-ting approach for historical handwritten documents was first proposed by Manmatha et al. [20], and a num-ber of different word matching algorithms were inves-tigated in [16,19,28 X 30,32]. Given the difficulty of the image matching problem for handwriting, it is under-standable that none of these papers went beyond and investigated the clustering of these word images. Seg-mentation for historical documents was investigated in [17,18]. We would like to note that the idea of spotting keywords has been proposed before in speech [8] and for printed documents [3,10]. Khoubyari and Hull [10] showed that Euclidean distance mapping (EDM) could be used to match printed words but as will be later shown the handwriting domain is much more challenging and EDM does not suffice for it.

Rath et al. [31] proposed the first automatic retrieval system for historical handwritten documents using rel-evance models. Their data set consisted of 1,000 man-uscript pages from the George Washington collection. The word spotting approach would be an alternative to building such a system. Results are not directly compa-rable since the aim of the paper [31] was to do retrieval while the results reported here focus on WERs.
Another approach involves the use of handwriting recognition, followed by X  X ay X  X  text search engine. How-ever, handwriting recognition of large vocabulary his-torical documents is still a very challenging task. Nagy [22] discusses the papers published in PAMI on docu-ment analysis during the past 20 years. In recent years, research in handwriting recognition [25] has advanced to a level that makes commercial applications (e.g. Tab-let PCs) feasible. However, this success has been mostly limitedtothe online handwritingrecognitioncase, where the pen movements are recorded as a user writes. Offline handwriting recognition, that is, recognition of text from an image of the writing, has only been success-ful in small-vocabulary and highly redundant domains such as automatic check processing and mail sorting (e.g. [12]). Srihari and Kim [37] described an early sys-tem for reading unconstrained handwriting. More recently, the community has started to look at large-vocabulary tasks [41].

In [21], the authors discuss the application of a Hid-den Markov model (HMM) for recognizing handwritten material that was produced specifically for this purpose. First, they asked a number of subjects to write out a set of pages. To improve the quality of the writing, the subjects were asked to use rulers and not to split words across lines. Recognition was performed with a HMM with 14 states for each character. These Markov models were concatenated to produce word and line models. A statistical bigram language model was used, and the authors showed that this improved the results by about 10%. The authors showed a recognition rate of about 60% for vocabulary sizes ranging from 2,703 to 7,719 words. The paper also contains a discussion of recog-nition rates obtained by other researchers X  X hese varied from a recognition rate of 42.5% for a 525-word vocabu-lary and 37% for a 966-word vocabulary reported in [24], to a recognition rate of 55.6% in a 1,600-word vocabu-lary reported by [11].

There is much less work on historical handwritten documents which are much more challenging. Tomai et al. [38] have shown how difficult this can be: the au-thors aligned a page of Thomas Jefferson with its man-ually generated transcript using recognition. Even after restricting the lexicon to about 13 words, their alignment accuracy on this single page was 83% (given a perfect transcript!). Govindaraju and Xue [4] also investigated the problem of handwriting recognition in historical doc-uments. Lavrenko et al. [13] trained an HMM model on 19 pages of the data set used in this paper and tested on the remaining page. With the 20-fold cross validation they obtained a WER of 41% (excluding out of vocabu-lary terms) and 50% (when including out of vocabulary terms). By including bigrams from a Jefferson corpus and a Washington corpus (exluding the test set) they reduced the WER to 35 and 45%, respectively. We note that the best approach in this paper has an even lower WER, showing that the word spotting approach is quite competitive. 3 Interesting clusters Early work in information retrieval by Luhn [15] lets us concretize the notion of  X  X nteresting X  clusters. A plot of term frequencies, where terms are ordered by decreas-ing frequency of occurrence, exhibits a distribution that is known as Zipf X  X  law [42]. That is, the frequency of the k th most frequent term has a frequency that is f 0 / where f 0 is the frequency of the most frequent term. Luhn argued that index terms should be taken from the middle of that distribution. Figure 3 shows an example of the actual distribution of term frequencies and the dis-tribution predicted by Zipf. Note that the large amount of mass is concentrated in high-frequency terms and the long tail of the distribution to the right, which continues beyond the shown range.

The reason is that terms with frequencies that are high (left side of the plot) are often stop words , such as and/the/... , which do not carry any meaning. Terms with very low frequencies are often sporadic, and are not descriptive of the content in the collection. Terms that are descriptive of the content can often be found in the middle of the plot. Their repeated, but not excessive use suggests that they are essential to describe the content of the collection and should consequently be part of the index.

In the following sections, we assume the output of a page segmentation algorithm and describe approaches to matching pairs of word images and clustering experi-ments. 4 Features The images we operate on are all grayscale with 256 levels of intensity [ 0 ... 255 ] . Before column features can be extracted from an image, inter-word variations such as skew and slant angle have to be detected and nor-malized. All of the column features we describe in the following are normalized to the range [ 0 ... 1 ] . Specific pixel intensity values in an image I  X  R h  X  w are referred to as I ( r , c ) , where r and c indicate the row and column index of the pixel. Our aim was to choose a variety of features presented in the handwriting recognition litera-ture (e.g. [40]), such that an approximate reconstruction of a word from its features would be possible. 4.1 Projection profile Projection profiles capture the distribution of ink along one of the two dimensions in a word image. A vertical projection profile is computed by summing the intensity values 2 in each image column separately: pp ( I , c ) =
Due to the variations in quality (e.g. contrast, faded ink) of the scanned images, different projection pro-files do not generally vary in the same range. To make them comparable, the range of the projection profiles is normalized to the range [ 0 ... 1 ] which gives f 1 ( I , c Figure 4 shows an example projection profile and the original image it was extracted from. 4.2 Word profiles Word profiles capture part of the outlining shape of a word. The current word matching algorithm uses upper and lower word profiles: let is_ink ( I , r , c ) be a function that returns 1 if the pixel I ( r , c ) is an  X  X nk pixel X , and 0 if the pixel is a background pixel. This function is currently realized using a thresholding technique which we have found to be sufficient for our purposes. For more sophisticated foreground/background separation, see [14]. Using is_ink, the upper and lower word profiles can be calculated as follows: up ( I , c ) = lp ( I , c ) =
If a column does not contain ink pixels, up and lp will be undefined (no distance to the nearest ink pixel from top or bottom of word image bounding box). A num-ber of factors, such as pressure on the writing instru-ment and fading ink, affect the occurrence of such gaps, which is not consistent for multiple instances of the same word. Therefore, gaps where up and lp are undefined were closed by linearly interpolating between the near-est defined values: up ( I , c ) = f (and similarly f 3 from lp ) can be obtained by nor-malizing up to the range [ 0 ... 1 ] . Figure 5 shows an upper word profile feature, generated from the original in Fig. 4a. 4.3 Background/ink transitions So far, the above features represent the distribution of ink in the columns of a word image and the outlining shape of the word. To capture a part of the  X  X nner X  structure of a word, we chose to record the number of background to ink transitions n bit ( I , c ) in an image column as the last feature. The range of this feature is normalized with a (conservatively determined) constant that ensures a range of [ 0 ... 1 ] : f ( I , c ) = n bit ( I , c )/ 6. (5) With this feature set at hand, we will now demonstrate its effectiveness when used within the proposed DTW matching algorithm.

We tried other features, including Gaussian filter re-sponses [29], but the above set seemed to work the best. 5 Word image matching One of the key parts of the wordspotting approach is the image matching technique for comparing word images. Several techniques have been investigated [28,30,32], with the best performing being dynamic time warping matching [28], which we explain here in detail.
For DTW matching, word images are represented by multidimensional profile features (see Sect. 4). These profiles are then matched using DTW, a dynamic pro-gramming algorithm that is able to account for writing variations, which cause the profile features to be com-pressed and stretched nonlinearly with respect to one another.

The advantage of DTW over simple distance mea-sures such as linear scaling followed by a Euclidean dis-tance calculation is that it determines a common  X  X ime axis X  (hence the term time warping) for the compared signals, on which corresponding profile locations appear at the same time. Due to the variations in handwriting, two profiles of the same word do not generally line up very well if they are just scaled linearly (see Fig. 6). 5.1 Dynamic time warping When determining the DTW-distance 3 dist ( X , Y ) be-tween two time series X = ( x 1 , ... , x M ) and Y = ( y ... , y D ( i , j )( 1  X  i  X  M ,1  X  j  X  N ) is the cost of aligning the subsequences X 1: i and Y 1: j .

Each entry D ( i , j ) is calculated from some D ( i , j ) an additional cost d , which is usually some distance be-tween the samples x i and y i . For instance, our implemen-tation of the algorithm uses D ( i , j ) = min The recursive definition of D ( i , j ) based on the given three values is a local continuity constraint (cf. Fig. 7a). It ensures smoothness of the recovered warping, e.g. no sample can be left out in a warped sequence. For a more detailed discussion of continuity constraints and alter-natives to the one used in this work, we refer the reader to [35].

Table 1 contains pseudo-code of the DTW algorithm (adapted from [39]) using the local continuity constraint from Figure 7a. 4 The algorithm determines a warping path composed of index pairs (( i 1 , j 1 ) , ( i 2 , j 2 j )) , which aligns corresponding samples in the input sequences X and Y . To prevent DTW from recovering pathological warpings that match a small portion of one series to a large portion in the other, global path con-straints are used. They force the recovered paths to stay close to the diagonal of the DTW matrix. Our imple-mentation of DTW uses the Sakoe X  X hiba band [33] (see Fig. 7b; the warping path must lie in the shaded region whose width is 2 r ), but the Itakura parallelo-gram [7] is also a popular choice. As a side effect, the constraint speeds up the computation of the DTW matrix, since it does not have to be entirely evaluated. Our implementation of the word matching algorithm uses r = 15 samples. Recent work [26] shows that the size and shape of the global path constraint can be adapted, leading to faster DTW computations and better match-ing performance.

Once all necessary values of D have been calculated, the warping path can be determined by backtracking along the minimum cost path starting from ( M , N ) .We are just interested in the accumulated cost along the warping path, which is stored in D ( M , N ) .Asitis,this matching cost would be lower for shorter sequences, so we offset this bias by dividing the total matching cost by the length K of the warping path, yielding dist ( X , Y ) = D ( M , N )/ K .(7) 5.2 Matching word images with DTW We represent word images with multi-dimensional pro-file features. Single-dimensional profiles that were extracted from the same word have the same length, and can be  X  X tacked X  to create multi-dimensional profiles. For the experiments in this paper we used four-dimen-sional profiles, consisting of the projection profile, the upper profile, the lower profile, and the background to ink transitions. Hence, when matching word images, the sequences X and Y consist of samples of dimensionality d  X  1, i.e. x
In order to use DTW to match such profiles, we need to define a distance measure d (  X  ,  X  ) that determines the (local) distance of two samples in a profile. Our imple-mentation uses the square of the Euclidean distance d ( x i , y j ) = where the index k is used to refer to the k th dimension of x i and y j . With this distance measure defined, we can now calculate the matching distance between two word images by comparing their profile features using DTW and Eq. (7). 5.3 Experimental setup The performance of the DTW word image matching algorithm was evaluated in a retrieval-by-example setup, wherewordimages inacollectionarerankedbydecreas-ing similarity to a given template. Experiments were conducted on two test sets of different qualities, both 10 pages in size (2,381 and 3,370 word images). The first set is of acceptable quality, see Fig. 8a). The sec-ond set is very degraded (see Fig. 8b) X  X ven people have difficulties reading these documents. We used this data set to test how badly matching algorithms perform on manuscripts of such poor quality. Each page in the two test sets was segmented into words with an automatic page segmentation procedure. While the quality of the segmentation algorithm has been improved in the mean-time, we used the same segmentation results as in [9], for comparability.

We conducted four experiments on the test sets and compared the performance of various matching approaches. Each experiment involves selecting one of the above two data sets and identifying a subset that will be used for querying. Each of the queries is used to rank the images in the data set according to their similarity to the query. The similarity scores are determined by a matching algorithm. Four experiments were conducted (A and C were initially proposed in [9]):
Experiment A: Fifteen images from test set 1 were
Experiment B: All images in test set 1 were used as
Experiment C: Thirty two images from test set 2 were
Experiment D: All images in test set 2 were used as Experiments A and C allow us to test algorithms which would otherwise take too long to run on the entire data set.

In order to reduce the number of pairwise compar-isons that have to be made, we pruned the total set of matches by comparing scalar features. If thresholding applied to the features extracted from the query and a candidate image determines that the images are dissim-ilar, we do not assign a similarity score to the candidate image (see [28] for details of this process). Table 2 shows the reduction of candidates through pruning, and how many true positives remain in the pruned candidate set.
Each word in the data sets was labeled with its ASCII equivalent. In the case of segmentation errors, a tag cor-responding to all visible characters in the segmented word image was assigned. Based on this annotation, relevance judgments were produced for the data sets. Two word images were considered relevant, if they have the same tags. For the evaluation, we used the trec_ eval program [34] to compute mean average precision scores for all experiments. 5.4 Experimental results Table 3 shows mean average precision results for all data sets obtained with a range of different matching techniques: 1. XOR [9]: The images are aligned to compensate for 2. SSD [9]: This approach translates the query and 3. SLH [9]: Scott and Longuet-Higgins algorithm [36]. 4. SC [2]: Shape context matching. Sample points are 5. EDM [9]: Euclidean distance mapping. In the XOR 6. CORR [32]: This technique recovers similarities be-7. DTW [28]: Dynamic time warping word image
The obtained mean average precision scores for experiments A and B had to be corrected because of an evaluation problem in [9]. The reason is that Kane et al. ranked all the images in the data set, including the query image. Queries with only one single relevant item (the query itself) produce average precision values of 1 (because the query image is retrieved at rank 1), which artificially inflates the retrieval scores. To solve this problem, we have chosen to disregard 13 of the queries in set C and 960 queries in set D. Table 3 reflects the values after this correction.

Despite this correction, the remaining queries con-tinue to retrieve the query image at rank 1, still inflating the scores. Hence, in order to give the most accurate picture of the actual matching performance, we have re-calculated the mean average precision scores for test runs that were available to us in a ranked-list format. Table 4 shows the average precision scores of four runs, where the top ranked image is removed from all ranked result lists, effectively discarding each query image from their respective candidate set.

For experiment A, results areavailablewithall match-ing algorithms. EDM, DTW, and CORR clearly outper-form any of the other techniques. SC was run with a number of sample points proportional to the width of the words being matched, with about 100 sample points for a word like Alexandria . More sample points would probably improve the effectiveness of the technique, but at the cost of further increasing the matching time (for 100 sample points it is already about 50 s, see Table 5).
The DTW and CORR algorithms were also used in experiment B (all images used as templates). The other algorithms were too slow to realistically run on this data set. On query set B, the average precision scores for DTW and CORR are lower than that on the smaller subset A. We attribute this effect mostly to the pruning method, which works much better on the smaller set A: while the pruning preserves about 91% of the relevant documents for data set A, it only produces 71% recall on data set B. The lower recall on set B (due to the prun-ing) then results in a lower average precision score after matching. While the performance of DTW was slightly worse than CORRs on the smaller query set A, DTW outperforms CORR on query set B, which is much larger and makes for a better comparison.

We compared the results of the SC, CORR, EDM, and DTW techniques on data set C. While the perfor-mance of all approaches is generally low on data set C, DTW X  X  and CORR X  X  performance is almost four times better than that of EDM (58.81 and 59.96% vs. 15.05%). DTW also performs similarly on the rest of the data set (51.81% average precision on data set D). This shows that the DTW and CORR matching techniques are more robust to document degradation than EDM, with DTW X  again X  X howing superior performance to CORR on the exhaustive query set. We would expect the results to be better, if a more careful pruning was applied: after pruning, the recall percentages have already dropped to about 56% for sets C and D (see Table 2). This lim-its the maximum average precision achievable with the matching algorithms.

These results show that DTW performs best among the set of algorithms tried. However, a look at Table 4 shows that significant efforts need to be made in order to perform well on such challenging data sets as C and D. Whether this improvement will come from the prepro-cessing or from the matching algorithm itself remains to be seen.

Comparing the running times of the investigated algo-rithms (see Table 5) shows CORR in the lead. CORR X  X  superior execution is a result of the very few corner points that are considered for establishing correspon-dences between the query and a candidate image. DTW is second in execution time, but we believe its perfor-mance can be improved substantially with optimization. The other algorithms (including our implementation of SLH) all use the actual images (rather than the 1D pro-files used by DTW) and hence are much slower. 6 Word image clustering experiments All of the previous work on wordspotting has concen-trated mostly on finding effective similarity measures for word image matching, but the clustering of word images has not been tackled. In this section, we perfom word image clustering experiments, followed by simu-lated cluster annotations that are designed to imitate a human annotator.

Before we start clustering, we need to get a good esti-mate of the number of clusters that our data will form. Heaps X  law , an empirical rule, provides the tool for the estimation, which is discussed in the following section. With an accurate cluster estimate we then move on to various clustering techniques that we apply to group word images. 6.1 Heaps X  law Many clustering algorithms often require that the num-ber of clusters to be created is known. In fact, all of the clustering algorithms that were used in our experiments have the target number of clusters as an input parame-ter. In the ideal case, each cluster contains all instances of a particular word, so there are as many clusters as there are distinct words in the collection at hand. In other words, the number of clusters is equivalent to the vocabulary size.

Early work in information retrieval by Heaps [6] pro-vides an empirical estimate for the vocabulary size of a collection from the size of the collection in words. The rule, which is known to be quite effective [1], has become known as Heaps X  law . It predicts that the vocabulary size of a collection of n words can be estimated to be V ( n ) = K  X  n  X  ,(9) where K and  X  are parameters that depend on the language of the collection.

We estimated K and  X  by fitting Heaps X  law to the ground truth transcription of a collection of 100 pages (21324 word images) from George Washington X  X  letters, which does not include our testing set on which we performed clustering experiments. We simulated doc-uments of sizes n = 1to n = 21, 324 by only considering the first n transcription words. For each n , we deter-mined the vocabulary size and then fitted Heaps X  law to the resulting curve. Figure 9 shows a plot of the vocab-ulary size V as a function of n and the fitted curve.
The fitting was performed with the  X  X elder-Mead X  optimization procedure [23], which minimizes the sum of squared differences between the actual vocabulary sizes and the ones predicted by Heaps X  law. For the col-lection at hand, we estimated the optimal parameter settings to be K = 7.2416 and  X  = 0.6172, resulting in a tight fit.

We used these parameters to estimate the vocabu-lary size of a collection of 20 pages, our testbed for the clustering experiments in the following section. The annotated data set is publicly available and was origi-nally used in [13] for doing recognition experiments. It consists of 4,860 word images. 6
Table 6 shows the accuracy that Heaps X  law achieves when predicting the vocabulary size of the data set. The vocabulary size of the collection is overestimated by 15%. This appears acceptable, given the small size of the collection. It is also possible that a larger text source for the parameter estimation could yield better predic-tion results. 6.2 Clustering With the desired number of clusters at hand, we can now turn to grouping word images based on pairwise simi-larity and then determine the accuracy of the generated clusters. Two sets of experiments were performed using two different image similarity metrics. The main differ-ence between them lies in the features that are used for the representation:  X  : The first experimental set consists of 4,860 word  X  : This set consists of all the 4,860 word images in
We experimented with both the K-means cluster-ing algorithm and various agglomerative clustering ap-proaches on these data sets with one exception: Since data set  X  is not represented in feature space, but rather in terms of pairwise distances, K-means clustering can-not be applied. K-means keeps track of cluster centers, a notion that does not exist in pairwise distance space.
Numerous clustering techniques are described in the literature. The following is a brief overview of the clus-tering approaches that were used in our experiments. More detailed descriptions of clustering techniques can be found in relevant literature, e.g. [5]. Except for K-means clustering, all others techniques are agglom-erative bottom-up procedures, which build a hierarchi-cal cluster tree by successively linking clusters. For such clustering techniques, we only list how inter-cluster dis-similarity is determined:
K-means The algorithm is initialized with K ran-domly selected cluster centers. Then each feature vector is assigned to the nearest cluster, and the cluster centers are recalculated. This procedure is repeated until con-vergence.

Single linkage The inter-cluster dissimilarity bet-ween two clusters is the distance between the closest items within the two clusters.

Complete linkage The distance between the two fur-thest items in the clusters is used as the cluster dissimi-larity.

Average linkage Herethedistancebetweentwoclus-ters is the average distance between all item pairs in the clusters.
 Weighted linkage A slight variation of the Average Linkage technique, which uses a weighted average for the cluster distance calculation.

Ward X  X  linkage This linkage uses the sum of squares measure to assess the similarity between clusters. The sum of squares is the total squared distance of all items in a cluster relative to the cluster centroid. The distance of two clusters is then taken to be the increase in the sum of squares measure, before joining them and taken together.

Each of our experiments involves selecting one of  X  and  X  and a clustering method. First, the desired num-ber of clusters is estimated using Heaps X  law. Then, we start the clustering of the data. In the case of K-means clustering, the feature vectors form the input for the clustering algorithm. All other clustering routines use a dendrogram as input, which can be constructed from pairwise distances between word images or their feature vectors (we used the Euclidean distance measure to cal-culate distances between feature vectors). The output of the clustering is a vector of cluster labels, which assigns each word image to a single cluster.

The accuracy of a particular clustering output is eval-uated by simulating the task of labeling clusters, which would be performed by a human annotator if we were to perform wordspotting. For the purpose of the simula-tion, it is assumed that a human annotator would label a cluster with the vocabulary term that occurs most fre-quently in a cluster. This strategy is sound, because it minimizes the total number of wrong annotations, when cluster labels are spread over all word images within a cluster. Ground truth data is available for all word images, so this process can be easily simulated. Once all clusters have been annotated in this fashion, we assign each cluster label to all word images within the cluster, essentially transcribing the entire collection. In practice, one could imagine a scenario where the user very quickly browses (overviews) a set of word images in a cluster and then assigns a label to the cluster. Table 7 shows the word error rates of such transcriptions obtained from various clustering approaches. (d) (c) The clustering algorithms tend to perform quite well. Data set  X  yielded the best overall result, with Ward linkage clustering. Interestingly, the DTW dissimilar-ity data (set  X  ) performs slightly worse using the Ward linkage, but otherwise consistently better than set  X  , and consistently well with word error rates between 34 and 36% (except for the single linkage algorithm). This sug-gests that the DTW distance measure captures different aspects of word image similarity than the features used in set  X  . The matrix with DTW distances has not been entirely computed due to pruning, which should have an adverse effect on performance (the pruning assigns a distance of  X  to a large portion of all possible word image pairs). Without the pruning, it would be impossi-ble to run the DTW algorithm in a realistic amount of time on this data set.

Figure 10 shows histograms of the sizes of clusters that have been generated with the best performing methods on sets  X  and  X  (average linkage and Ward linkage), as well as the output of a clustering technique with higher WER (K-means). The clustering techniques with lower word error rates generate better matches for the actual distribution of cluster sizes. This is also true of tech-niques for which no plots are provided. It is important for a good clustering approach to produce clusters of a variety of sizes. The output of the K-means clustering in Fig. 10 d shows that clusters which should have been large, were broken down into smaller pieces.

Of course, our objective is not to obtain labels for all word images in the collection. Following Luhn X  X  line of thought, we can identify clusters that should make good candidates for an index. We constrained the simulated annotation to clusters with at least 3 members, but not more than 50, and calculated the WER for the simulated annotation that is restricted only to the selected clusters. Table 8 shows the word error rates that were achieved for such clusters and the number of word images in the collection that were assigned a label.

Most of the results show increased WERs (compared toTable7), indicatingthat theclusteringperforms slightly better on words that were excluded from the word er-ror rate calculation (e.g. stop words). The cases with lower word error rates substantially underestimate the number of images (#Img). This indicates inferior clus-tering because it leads to a much smaller word index and hence is of limited practical use. Hence, the word error rate should not be the only criterion for choosing the clustering algorithm but coverage should also be a factor. Generally, the clusterings of sets  X  and  X  come close to the desired number 2,567 of #Img. For a more detailed investigation see [27].

In order to put the obtained word error rates into perspective, we compared them with the output of a holistic word recognition approach for historical docu-ments [13]. The recognizer uses an HMM based on a word bigram model that is estimated from external text corpora. The test collection consisted of the same 20 pages that were used here, but each page was recognized separately by using the remaining 19 pages as training data. With bigram models estimated from the 19 train-ing pages and external corpora of writings by Thomas Jefferson and George Washington, the WER was 44.9%. This is inferior to the performance of the best cluster-ing algorithms above: For example, the average link-age algorithm achieves a WER of 41.66%, even when restricted to Luhn clusters (cf. Table 8). Word spotting performs very well, and it does not depend on training data to estimate word models or external text corpora to exploit statistical regularities of natural language (the requirements of a word-level HMM). 7 Conclusion Wordspotting appears to be an attractive alternative to the seemingly obvious recognize-then-retrieve approach to historical manuscript retrieval. With the capability to match word images quickly and accurately, partial transcriptions of a collection can be achieved with rea-sonable accuracy and little human intervention. Word-spotting has the capability to automatically identify indexing terms, making it possible to use costly human labor more sparingly than a full transcription would re-quire. For example, using the Ward linkage clustering on data set  X  , it would be possible to obtain 2,867 word image labels with a WER of 38.12%, by annotating just 291 clusters (cluster sizes between 3 and 50 members). That is, the wordspotting procedure would have reduced 2,867 annotations to about 10% of that. Even greater savings (in terms of percent) can be expected from larger collections, since vocabularies grow sub-linearly in the size of the corresponding collections.
 References
