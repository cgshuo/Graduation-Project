 Information in various applications is often expressed as character sequences over a finite alphabet ( e.g. , DNA or protein sequences). In Big Data era, the lengths and sizes of these sequences are growing explosively, leading to grand challenges for the classical NP-hard problem, namely search-ing for the Multiple Longest Common Subsequences ( MLC-S ) from multiple sequences. In this paper, we first unveil the fact that the state-of-the-art MLCS algorithms are unable to be applied to long and large-scale sequences alignments. To overcome their defects and tackle the longer and large-scale or even big sequences alignments, based on the pro-posed novel problem-solving model and various strategies, e.g. , parallel topological sorting, optimal calculating, reuse of intermediate results, subsection calculation and serializa-tion, etc., we present a novel parallel MLCS algorithm. Ex-haustive experiments on the datasets of both synthetic and real-world biological sequences demonstrate that both the time and space of the proposed algorithm are only linear in the number of dominants from aligned sequences, and the proposed algorithm significantly outperforms the state-of-the-art MLCS algorithms, being applicable to longer and large-scale sequences alignments.
 Multiple Longest Common Subsequences (MLCS); Non-redu-ndant Common Subsequence Graph (NCSG); Topological Sorting; Subsection Calculation and Serialization
Information in various applications often can be abstract-ed as character sequences over a finite alphabet  X , e.g. , DNA or protein sequences in biology. Searching for the Multiple Longest Common Subsequences from a group of sequences ( MLCS ) over a finite alphabet  X  is a classical NP-hard prob-lem [13] and has found many important applications in many areas, e.g. , bioinformatics, computational genomics, pattern recognition, data mining, etc. For example, in bioinformat-ics, sequence is the most basic mathematical model, which can describe the primary structure of the nucleic acid and protein molecules. Searching for their LCS s/ MLCS s is an important way to identify the sequence similarity, which can be utilized in gene discovery, the construction of an evolutionary tree, the evidence of the species X  common o-rigin [20], etc. With the successful implementation of the Human Genome Project, the lengths, sizes of biological [4, 8] or other types of sequences are growing explosively and exponentially [16]. Mining the MLCS s from these sequences is becoming a more and more important research topic and facing severe challenges.
 In the past forty years, in order to efficiently tackle the LCS/MLCS problem, various types of LCS/MLCS algo-rithms [2, 3, 6, 7, 11, 12, 14, 15, 16, 17, 19] and tool-been proposed, which can be divided into two categories: classical dynamic programming and dominant-point-based algorithms. It has been demonstrated that the dominant-point-based LCS/MLCS algorithms have an overwhelming advantage over classical dynamic programming ones due to their great reduction in the size of search space by orders of magnitude [16]. In particular, FAST LCS [2] and Quick-DPPAR [16] are the most efficient parallel MLCS algorithms among dominant-point-based algorithms. However, it will be shown in this paper with both theoretical and empiri-cal study that these algorithms suffer severely from many unnecessary and redundant storage, computation, compari-son and deletion of multi-dimensional dominants. Therefore, this kind of algorithms is essentially inapplicable to long and large-scale sequences alignments. Moreover, both of the two types of algorithms are in fact not linear. To overcome this problem, we present a new problem-solving graphical model and propose a real linear and parallel algorithm for multiple longest common subsequences mining , equipped with several carefully designed strategies. Our main contributions are as follows:
The rest of this paper is organized as follows. Section 2 first gives a formal definition of the MLCS problem, and then briefly reviews the related work on classic dynamic programming and dominant-point-based algorithms in in-troducing some necessary preliminaries, and finally reveal-s some inherent limitations of the leading dominant-point-based algorithms. To overcome the limitations, Section 3 discusses the proposed novel problem-solving model NCSG and two topological sorting strategies on the NCSG . Sec-tion 4 first elaborates upon several well-designed strategies tackling longer and large-scale sequences alignments, and then gives our novel efficient parallel MLCS algorithm, fol-lowed by a detailed study of the time and space complexities. Comprehensive experiments are conducted in Section 5. Fi-nally, Section 6 concludes this work.
In this part, we first present series of preliminaries used in our model and then discuss some related works.
A subsequence of a given sequence over a finite alpha-bet  X  can be obtained by deleting zero or more (not neces-sarily consecutive) characters from the sequence. Let X = x x 2 ...x n and Y = y 1 y 2 ...y m be two sequences with lengths n and m , respectively, over a finite alphabet  X , i.e. , x i and the Longest Common Subsequence ( LCS ) problem is to find out all the longest common subsequences of X and Y . Similarly, the Multiple Longest Common Subsequence ( MLCS ) problem is to find out all the longest common sub-sequences of d ( d  X  3) sequences with equal length n or unequal length. Obviously, the LCS is a special case of MLCS .

Note that, given d sequences over a finite alphabet  X , there exists more than one MLCS s in general. For ex-ample, given three sequences, S 1 = GT ACT AGC , S 2 = ACT GT CAG and S 3 = T CAGT GCA over  X  = { A, C, G, T } , there are 4 MLCS s with length 4, namely MLCS 1 = GT CA , MLCS 2 = AT GC , MLCS 3 = CT GC and MLCS 4 = T CAG .
Based on the methods adopted, existing LCS/MLCS al-gorithms can be divided into two categories: classical dy-namic programming and dominant-point-based algorithms. Depending on whether they are parallelized, the algorithms can also be divided into two types: serial and parallel ones. 1) Classical Dynamic Programming Algorithms : These al-gorithms are based on dynamic programming [14, 15]. In the simplest case, given two sequences X = x 1 x 2 ...x n Y = y 1 y 2 ...y m with lengths n and m , respectively, over a finite alphabet  X , where X [ i ] = x i , Y [ j ] = y j , x 1  X  i  X  n and 1  X  j  X  m , a dynamic programming algorith-m iteratively constructs an ( n + 1)  X  ( m + 1) score matrix L , in which L [ i, j ] is the length of an LCS between two prefixes X  X  = x 1 x 2 ...x i and Y  X  = y 1 y 2 ...y j of X and Y , calculated as follows:
Once the score matrix L is calculated, all the LCS s can be obtained by tracing back from the end element L [ n, m ] to the starting element L [0 , 0]. Both the time and space complexities of this algorithm are O ( mn ). In general, given d sequences S 1 , S 2 , ..., S d with arbitrary equal or unequal lengths, n 1 , . . . , n d , the matrix L can be naturally extended to d dimensions for the MLCS problem, in which the element L [ i 1 , i 2 , ..., i d ] can be calculated by Eq. 2 in a similar manner to Eq. 1, the time and space complexities are therefore both where  X  L = { L [ i 1  X  1 , i 2 , . . . , i d ] , L [ i 1 L [ i 1 , i 2 , . . . , i d  X  1 , i d  X  1] } .
 Fig. 1 illustrates the score matrix L of two sequences S 1 = ACT AGCT A and S 2 = T CAGGT AT over the al-phabet  X  = { A, C, G, T } and the process of extracting an LCS = CAGT A from L .

To further reduce time and space complexities, various im-proved dynamic programming LCS/MLCS algorithms [1, 3, 6] have been proposed. For example, Hirschberg [3] present-ed a new LCS algorithm based on the divide-and-conquer approach, which reduces the space complexity to O ( m + n ); however, its time complexity remains to be O ( mn ). Masek and Paterson [14] put forward an improved dynamic pro-gramming LCS algorithm for two sequences with length n using a fast computing method of edit distance, whose worst time complexity is O ( n 2 / log n ). Unfortunately, most of the aforementioned algorithms only address the LCS problem but not MLCS and have high time and space complexities. 2) Dominant-point-based Algorithms : In order to clearly illustrate the dominant-point-based LCS/MLCS algorithms, we first introduce the following definitions. (a) The score matrix L of se-quences S 1 and S 2 . Figure 1: The score matrix L of S 1 = ACT AGCT A and S 2 = T CAGGT AT over a finite alphabet  X  = { A, C, G, T } and LCS ( CAGT A ) extracted from L . The regions of the same entry values are bounded by thick contours; the corner points of the contours are dominant points (circled) and the greyed points are matched points.

Definition 1 : For a sequences set T = { S 1 , S 2 , ..., S over a finite alphabet  X , and | S i | = n . 4 Let S i [ p  X ) be the p i -th ( p i  X  X  1 , 2 , ..., n } ) character in S if S 1 [ p 1 ] = S 2 [ p 2 ] = ... = S i [ p i ] = ... = S
Definition 2 : For two matched points, p = ( p 1 , p 2 , ..., p and q = ( q 1 , q 2 , ..., q d ), of T , we say that p = q if and only if p i = q i for i = 1 , 2 , ..., d . If  X  i , p i &lt; q strongly dominates q , denoted as p  X  q , where p is referred to as a dominating point ( dominant for short) and q as a dominated point or successor of p . Further, if there is no matched point r = ( r 1 , r 2 , ..., r d ) for T such that p  X  r  X  q , we say that q is an immediate successor of p and p is an immediate predecessor of q .

Definition 3 : A matched point p = ( p 1 , p 2 , ..., p d called the k-th dominant (the k-level dominant for short), if the score matrix L [ p 1 , p 2 , ..., p d ] = k (see Eq. 2). The set of all the k -th dominants is denoted as D k , and the set of all dominants of T is denoted as D .

Fig. 1 shows that a dominant must be a matched point, but not vice versa, and the size of the dominant set D is smaller than that of the matched points, which have been proved by [2, 7, 16].

Comparing with dynamic programming LCS/MLCS al-gorithms, the dominant-point-based LCS/MLCS algorithms only need to calculate the dominants instead of all the ele-ments in the score matrix L . The algorithms consist of two procedures as follows. (1) Constructing MLCS-DAG . First of all, introduce two dummy points, source point (0 , 0 , ..., 0) and sink point (  X  ,  X  , ...,  X  ), where the sink point is defined as the imme-diate successor of those points without an immediate succes-sor. Afterwards, let k = 0, and D 0 = { (0 , 0 , ..., 0) } . Next, with a forward iteration procedure (0  X  k ), the ( k + 1)-th dominants D k +1 are computed based on the k -th dominants 0  X  k  X  | MLCS | X  1 ( | MLCS | denotes the length of MLC-S s of T ). As a result, a directed acyclic graph consisting of all the MLCS s of T ( MLCS-DAG , for short) is constructed level by level in the following two steps:
Step 1 : Based on D k , all the immediate successors of each dominant from D k are calculated, denoted as D k +1 init
Step 2 : There exist massive repeated dominants and dominated points in D k +1 init , both collectively called redun-dant points for short. Since the redundant points do not con-tribute to the MLCS s, the operation Minimal ( D k +1 init imal () for short) of eliminating them should be conducted in order to save time and space. In general, the operation Minimal () is performed by comparing between the domi-nants and redundant points in D k +1 init one dimension after another, resulting in the baseline D k +1 over D k +1 init (2) Computing MLCSs . All the MLCS s of T are then found by tracing back through the constructed MLCS-DAG from the last dominant set to D 0 iteratively. An instance of this kind of algorithms is shown in Example 1 and Fig. 2.
As the size of the dominants set D is far smaller than that of the matrix L , i.e. , | D | X  X  L | , both theoretical analysis and experimental results have shown that the dominant-point-based LCS/MLCS algorithms are overwhelmingly faster than classical dynamic programming algorithms [16].
 Hunt et al. [7] first proposed a dominant-point-based LC-S algorithm with time complexity O (( r + n ) log n ), where r is the number of all dominants of two sequences with length n . Afterwards, a variety of dominant-point-based LCS/MLCS algorithms have been presented [1, 6]. To fur-ther improve the efficiency, some parallel dominant-point-based LCS [12, 19] and MLCS algorithms [2, 10, 16] were proposed. Korkin [10] first proposed a parallel MLCS al-gorithm with time complexity O ( |  X  || D | ). Chen et al. [2] presented an efficient parallel MLCS algorithm over the al-phabet  X  = { A, C, G, T } , FAST LCS , with series of pruning rules. Wang et al. [16] developed an efficient parallel MLCS algorithm Quick-DPPAR , claiming that the proposed algo-rithm has reached a near-linear speedup with respect to its serial version Quick-DP . It is worth mentioning that [11, 17] made attempts to develop efficient parallel algorithms on GPUs for the LCS problem and on cloud platform for the MLCS problem, respectively. Regretfully, [17] is not suit-able for the general MLCS problem, as a large amount of synchronous cost [11] remains to be solved. For large-scale MLCS problems in practice, Yang et al. [18] presented a new progressive algorithm Pro-MLCS with its efficient par-allelization, which can find an approximate solution quickly.
Parallel dominant-point-based MLCS algorithms are cur-rently a better solution for the MLCS problem. However, as we will show in next subsection, such algorithms are fac-ing serious challenges with the explosive expansion in the lengths and sizes of sequences from varieties of applications.
We first give an example to illustrate the limitations of the leading dominant-point-based MLCS algorithms, and then make a further study.
 Example 1 : Given sequences S 1 = T GACGAT C , S 2 = AT GCT CAG and S 3 = CT AGT ACG over the alphabet  X  = { A, C, G, T } , construct their MLCS-DAG s and find out all the MLCS s of S 1 , S 2 and S 3 by the general dominant-point-based algorithms.

Based on the dominant-point-based MLCS algorithm, the constructed MLCS-DAG of S 1 , S 2 and S 3 is shown in Fig. 2. The construction process is as follows. First of all, let k = 0, D 0 = { (0 , 0 , 0) } (0-level dominant). By Definition 2, all the Figure 2: The constructed MLCS-DAG of S 1 , S 2 and S 3 over  X  = { A, C, G, T } by the dominant-point-based algorithms, the blue (red) points denote repeated (dominated) points, which would be deleted after the operation Minimal (), and a dashed arrow indi-cates an added edge after deleting a point. All the MLCS s (marked by red arrows) can be obtained by tracing back from the sink point (  X  ,  X  ,  X  ) to the source point (0 , 0 , 0) on the MLCS-DAG . immediate successors D 1 init = { (3, 1, 3), (4, 4, 1), (2, 3, 4), (1, 2, 2) } corresponding to the characters { A, C, G, T } from D 0 are calculated. The operation Minimal () is then performed so as to eliminate the redundant point (2 , 3 , 4) (a dominated point of the point (1 , 2 , 2)), leading to the end of procedure D 0  X  D 1 (1-level dominants D 1 = { (3, 1, 3), (4, 4, 1), (1, 2, 2) } ). Repeating the same procedure as above D inants can be obtained in turn, where D 2 = { (4, 4, 7), (5, 3, 4), (7, 2, 5), (6, 7, 3), (7, 5, 2), (3, 7, 3), (2, 3, 4) } , D 3 = { (5, 8, 8), (6, 7, 6), (8, 4, 7), (7, 5, 5), (5, 8, 4), (3, 7, 6), (4, 4, 7) } and D 4 = { (8, 6, 7), (5, 8, 8) } , respectively. Since init =  X  , let D 5 = { (  X  ,  X  ,  X  ) } and end the construc-tion of MLCS-DAG . Finally, based on the MLCS-DAG , all the MLCS s of S 1 , S 2 and S 3 corresponding to the character sequences of points on the longest paths (marked by red ar-rows) over the MLCS-DAG ( MLCS 1 = AGT C, MLCS 2 = T GT C, MLCS 3 = T GAG and MLCS 4 = T GCG with length 4) can be obtained by tracing back from the sink point (  X  ,  X  ,  X  ) to the source point (0 , 0 , 0).
Fig. 2 clearly shows that there are numerous redundan-t points in the MLCS-DAG , and that many points, e.g. , (2 , 3 , 4), (4 , 4 , 7) and (5 , 8 , 8), have been recalculated and compared with other points many times. Moreover, there exist many points in the MLCS-DAG , which are useless to the MLCS s of sequences S 1 , S 2 and S 3 .

To further show the limitations of the dominant-point-based MLCS algorithms, we conducted a statistical study with various types of the redundant points of the MLCS-DAG from sequences with various lengths and sizes of al-phabet over the real-world and synthetic datasets utilized in Section 5, from which we draw the following conclusions: 1) In each iteration, procedure D k  X  D k +1 generates a great number of redundant points, leading to an excessive computational time in the operation Minimal ().

The statistical data show that in D k +1 init , there exist t-wo types of a significant number of redundant points (de-noted as N redu ), i.e. , repeated points (denoted as N repeat and dominated points/successors (denoted as N suc ). Let N = | D k +1 init | , the average ratio of the redundant points N to N reaches 59%, and the ratio can be up to 79%. These redundant points will result in an excessive computation-al time in the operation Minimal (). Moreover, tremendous comparisons in d dimensions among N points are inevitable besides the massive amount of redundant points in the oper-ation Minimal () of the process D k  X  D k +1 . For d sequences with length n ( d  X  3), it is proved that the time for the comparisons among N d -dimensional points dimension by dimension in a brute-force manner is O ( dN 2 ) [5, 9]. Even if the divide-and-conquer strategy is adopted, O ( dN log d  X  2 comparisons are still needed [16]. 2) The constructed MLCS-DAG contains a large number of useless points not contributing to any MLCS s, called non-critical points .

The statistical data shows that the ratio of | K | (the total number of the key points that contribute to the MLCS s in the MLCS-DAG ) to | D | (the total number of the points in MLCS-DAG ) ranges from only 1 : 10 to 1 : 100 , 000. Moreover, the larger the d , n and |  X  | , the smaller the ratio of | K | / | D | is. The massive non-critical points in the MLCS-DAG introduce another serious space problem of storing the MLCS-DAG in RAM and time problem when tracing back to find its MLCS s on the MLCS-DAG .

Above all, since Minimal () is the key operation in the pro-cess D k  X  D k +1 of the general dominant-point-based MLCS algorithms, and by the above argument, it is clear that the time complexity of the dominant-point-based MLCS algo-rithms are nonlinearly related to d and | D | . In addition, the length of MLCS , | MLCS | is proportional to the aligned sequences length n , and | D k | tends to grow explosively in the range 1  X  k  X  | MLCS | / 2. Therefore, both the above analysis and our comprehensive experimental results (see Section 5) show that the leading dominant-point-based MLCS algorithms are not applicable to long and large-scale sequences alignments .
As mentioned above, all the MLCS s of aligned sequences set T are constructed by their relevant dominants. For the dominants set D from T , since the immediate predecessor-successor relationships between the dominants in D consti-tute a partial order set , we can represent the relationships by a directed acyclic graph G = ( D, ). Similar to the con-struction of MLCS-DAG , two dummy d -dimensional points (0 , 0 , ..., 0) (the source point) and (  X  ,  X  , ...,  X  ) (the sink point) are introduced into D , with all the other dominants in D being the successors of (0 , 0 , ..., 0) and the predeces-sors of (  X  ,  X  , ...,  X  ). The construction of G is as follows. 1) From the dummy source point (0 , 0 , ..., 0), calculate all of its immediate successors and connect them with direct-ed edges from the source point to the immediate successors. 2) From these calculated immediate successors, compute al-l their immediate successors. If the immediate successor found is already in G , only add a directed edge to it; oth-erwise, add it to G and connect it by a directed edge. If a successor has no immediate successors, connect it with the sink point by a directed edge. 3) Repeat above procedure 2) until all points in G have computed their immediate suc-cessors. Notably, such G is in fact an MLCS-DAG but with Figure 3: The constructed NCSG of S 1 , S 2 and S 3 over the alphabet  X  = { A, C, G, T } , where the optimal subgraph Sub-NCSG 4 of the NCSG (marked red) is constructed from the optimal dominant (1, 2, 2). the following properties: no repeated dominants and com-parison of dominants in the dimension by dimension manner on constructing G . Due to that, finding an MLCS can now be regarded as identifying the longest path over G from the source point to the sink point, and vice versa. Since G has no repeated dominants, and any path of G corresponds to a common subsequence of T , we refer to it as Non-redundant Common Subsequence Graph , abbreviated as NCSG in the following. The constructed NCSG of the above sequences S , S 2 and S 3 is shown in Fig. 3.
Given a constructed NCSG , we need to design an efficient and effective strategy to extract all MLCS s from it. To this end, let X  X  start by reviewing the following concepts from the graph theory.

Definition 4 : For a directed acyclic graph G = h V, i , topological sorting is to find an overall order of the vertices V in G from the partial order [5, 9].

Definition 5 : The topological sorting algorithm [9] is to complete the topological sorting over the vertices V in G from the partial order . To this end, it iteratively performs the following two steps until all the vertices V in G have been traversed and processed: 1) output the vertices with in-degree 0; 2) delete the edges connecting to the vertices. We found that the topological sorting over the vertices V in G from the partial order is only associated with the cardinality of the in-degree of vertices, but not related to the dimensions of vertices. We do not need to perform the com-parison of the dominants dimension by dimension to sort and layer them as in the construction of MLCS-DAG for leading dominant-point-based MLCS algorithms. Inspired by this observation, we present a novel efficient method to sort and layer all the dominants of the NCSG , i.e. , sorting and layering all of the dominants of the NCSG from the 0-level dominant set D 0 to the k -level dominant set D k el by level with the topological sorting algorithm (called a forward topological sorting algorithm , denoted as Algorith-m ForwardTopSort ), 1  X  k  X  | MLCS | X  1. An example of sorting and layering all the dominants on the NCSG of the sequences S 1 , S 2 and S 3 is shown in Fig. 4.

Moreover, based on the sorted and layered NCSG shown in Fig. 4, how do we find all MLCS s at once with a very low cost of time and space without multiple backtracking Figure 4: Sorting and layering all the dominants on NCSG of S 1 , S 2 and S 3 by ForwardTopSort , in which all the longest paths are marked by red arrows. Figure 5: With BackwardTopSort to the sorted and layered NCSG of S 1 , S 2 and S 3 shown in Fig. 4, the optimized NCSG can be obtained, in which any path corresponds to an MLCS of the sequences. processes as in the leading dominant-point-based MLCS al-gorithm? By investigating the sorted and layered NCSG , we find that the sum of the numbers of the forward levels ( from { (0 , 0 , ..., 0) } to { (  X  ,  X  , ...,  X  ) } ) and the backward levels ( called key points ) residing in the longest paths correspond-ing to the MLCSs is exactly equal to | MLCS | + 1 ; however non-critical points would not meet the property (see Fig 5) 5 . Based on the observation, we replace the in-degree with the out-degree and layer the NCSG by the topological sorting algorithm from the point { (  X  ,  X  , ...,  X  ) } to { (0 , 0 , ..., 0) } (called a backward topological sorting algorithm , denoted as Algorithm BackwardTopSort ). Thanks to that, all the non-critical points in the NCSG are now identified and can be easily removed. Fig. 5 (called the optimized NCSG ) demon-strates the result with BackwardTopSort to Fig. 4. In par-ticular, the NCSG shown in Fig. 5 contains only those key points, that is, each path in the optimized NCSG corre-sponds to an MLCS of S 1 , S 2 and S 3 .
 In summary, based on our novel problem-solving model NCSG and the MLCS s mining strategy, Algorithms For-wardTopSort and BackwardTopSort , we can overcome the defects of the leading dominant-point-based MLCS more efficiently and effectively, which is verified by our exten-sive experiments. However, unfortunately, the theoretical and experimental results also show that the proposed model and the strategy are impractical to big sequences ( e.g. , the motivates us to explore more efficient method. In the next section, we shall present a parallel solution towards this end.
For the convenience of discussion, we first introduce fol-lowing key concepts, and give a part of our statistical study in Table 1 over synthetic and real-world biological sequences datasets (see Section 5). Table 1: The total number of dominants of aligned sequences with various lengths and alphabet sizes
Definition 6 : Given a NCSG , a dominant is called the op-timal dominant from the NCSG , if and only if the dominant belongs to 1-level dominants (  X  D 1 ) and has minimal coor-dinates values compared with the other 1-level dominants. if a subgraph Sub-NCSG i of the NCSG is constructed start-ing from optimal dominant i , the Sub-NCSG i is called the optimal subgraph of the NCSG .

For example, by Definition 6, dominant (1 , 2 , 2) (the fourth dominant from 1-level dominants D 1 ) shown in Fig. 3 is an optimal dominant, from which the constructed subgraph (marked red) of the NCSG is an optimal subgraph denoted as Sub-NCSG 4 .

The most fundamental challenges to longer and large-scale sequences alignments for MLCS algorithms are their unbear-able huge time and insufficient space for calculating and s-toring the massive dominants of the NCSG . The statistical data shown in Table 1( N 1 denotes the total number of the dominants from the optimal subgraph of the NCSG , and N 2 is the total number of the dominants from the NCSG ) bring out the following facts: 1) With the increase in the length of sequences, the number of dominants from the NCSG gets an exponential explosive growth; 2) Most of the dominants come from the optimal subgraph Sub-NCSG i , e.g. , Table 1 shows that the ratio of N 1 /N 2 is as high as 97%. From the above facts, we present the following strategies so as to tack-le the challenges. Notably, in line with other parallel algo-rithms towards big data, our model is based on the intuition that the results of parallel algorithms should be compatible in divide-and-conquer style and support combinative.
Strategy 1 : Successor Table, ST . One of the funda-mental needs in constructing NCSG is to search all the immediate successors for each dominant efficiently due to the massive number of dominants in the NCSG . To achieve that, according to the searching strategy from [2], the suc-cessor tables { ST 1 , ST 2 , ..., ST d } of the aligned sequences set T = { S 1 , S 2 , ..., S d } should be built first, i.e. , given a sequence S l = x 1 , x 2 , ..., x n from T over a finite alpha-bet  X  = {  X  1 ,  X  2 , ...,  X  k } , its successor table ST dimensional array, where ST l [ i, j ] denotes the element of the i -th row and the j -th column in ST l , defined as below:
Obviously, ST l [ i, j ] is in fact the minimal subscript posi-tion m of the sequence S l after position j according to  X  when x m =  X  i , see an example in Fig. 6.

It has been proved in [2] that all the immediate succes-sors of a d -dimensional dominant p = ( p 1 , p 2 , ..., p be obtained efficiently in O ( d |  X  | ) based on the successor ta-bles. For example, for the dominant (2 , 3 , 4) of the sequences S , S 2 and S 3 , we can couple the corresponding lines 1-4 of the second, third and forth columns from the successor ta-Figure 6: The constructed successor tables S T 1 , ST 2 and ST 3 corresponding to the sequences S 1 , S 2 and S , where the notation  X   X   X  indicates  X  . bles ST 1 , ST 2 and ST 3 to obtain all its immediate successors characters A, C, G, and T, respectively, while there is no immediate successor for the dominant (6 , 7 , 3) due to the coupling results ( , , 6) , (8 , , 7) , ( , 8 , 4) and (7 , , 5), which indicates none of them is an im-mediate successor according to Eq. 3.

Strategy 2 : DM(Index, Point) . During the construction and processing of NCSG , we have to access the coordinates based on the corresponding index, and vice versa, respective-ly. Hence, we present a bidirectional hash table DM(Index, Point) , wherein the Point represents a d -dimensional coor-dinates of a dominant and Index is a serial number corre-sponding to the dominant. As a result, we can compressing-ly store all the dominants of the NCSG with serial numbers instead of their d -dimensional coordinates. The dominants shown in Fig. 7 stored in DM are as follows: h 17 , (6 , 7 , 3) i , h 18 , (7 , 5 , 2) i , } .
Strategy 3 : The Optimal Calculation and Reuse of In-termediate Results . Our extensive experiments and analy-sis reveal the fact that the optimal subgraph Sub-NCSG i of NCSG not only contains most of the dominants of the NCSG (45%-97%, average 88%, see Table 1), but also con-tributes to most of the MLCS s (75%-100%, average 85%). For example, the optimal dominant (1 , 2 , 2) shown in Fig. 3 contributes to three MLCS s of the sequences S 1 , S 2 and S which is 3 out of 4 MLCSs , accounting for 75%. Moreover, as a dominant may locate in different paths of the NCSG , the number of levels of a dominant located in the longest paths (corresponding to MLCSs X  paths) of NCSG must be greater than that in the non-longest paths . For example, the dominants (4 , 4 , 7) and (5 , 8 , 8) shown in Fig. 7 simultane-ously locate in the non-longest path ((0 , 0 , 0)  X  (3 , 1 , 3)  X  (4 , 4 , 7)  X  (5 , 8 , 8)  X  (  X  ,  X  , ...,  X  ), denoted as path-1) and in the longest path ((0 , 0 , 0)  X  (1 , 2 , 2)  X  (2 , 3 , 4)  X  (4 , 4 , 7)  X  (5 , 8 , 8)  X  (  X  ,  X  , ...,  X  ), denoted as path-2). It is clear that the numbers of levels of the dominants (4 , 4 , 7) and (5 , 8 , 8) resided in path-1 (being 2 and 3, respectively) are smaller than the numbers of levels they reside in path-2, namely 3 and 4, respectively. With the above observations, in order to save time and space of constructing NCSG of aligned sequences, we construct the NCSG in the following manner: 1) let D 0 = { 0 , 0 , ..., 0 } and perform the procedure D 0  X  D 1 ; 2) calculate the optimal dominant i from D 1 ; 3) construct the optimal subgraph Sub-NCSG i , the step called Figure 7: With Strategy 3, we can construct the NCSG of S 1 , S 2 and S 3 quickly, the red arrows indi-cate the longest paths of the NCSG , the dash lines mean reuse of the intermediate results available (the levels information from the optimized subgraph Sub-NCSG 4 ) and red nodes are key points of Sub-NCSG 4 . optimal calculating ; 4) sort and layer the Sub-NCSG i by Algorithm ForwardTopSort ; 5) identify and remove all of the non-critical dominants on the Sub-NCSG i by Algorithm BackwardTopSort resulting in an optimized Sub-NCSG i , i.e., all dominants are key points in the optimized Sub-NCSG i ; 6) construct another subgraph of Sub-NCSG j using the lev-els information of the dominants from the optimal subgraph Sub-NCSG i , which have already been computed in previous steps, thus this step is called reuse of intermediate results . Since the NCSG of T is equal to the sum of the above sub-graphs, with Strategy 3, we can construct the NCSG of T quickly and effectively. For convenient of further discussion, the above steps 1)-6) in Strategy 3 are denoted as Func-tion OptCalReusing1 (), while the above steps 1)-4) and 6) in Strategy 3 (not including step 5)) are denoted as Func-tion OptCalReusing2 (). An example of using Strategy 3 to construct the NCSG of the sequences S 1 , S 2 and S 3 is shown in Fig. 7.
 Example 2: With Strategy 3, construct the NCSG of S , S 2 and S 3 and find out all of the MLCS s of the sequences.
From Fig. 7, we can easily see that dominant 4 correspond-ing to dominant (1 , 2 , 2) (see the contents of DM ) is an op-timal dominant. Hence, with step 3) of Strategy 3, we can construct the optimal subgraph Sub-NCSG 4 from the opti-mal dominant 4. And then with steps 4) and 5) of Strategy 3, we can get all the MLCS s of Sub-NCSG 4 . Next, with step 6) of Strategy 3, we can easily construct other non-optimal subgraphs Sub-NCSG 1 , Sub-NCSG 2 and Sub-NCSG 3 from the non-optimal dominants 1, 2 and 3 in turn and quickly find a new MLCS (the path 1  X  13  X  7  X  11  X  12).
 From above procedures and Fig. 7, we can easily see that the amount of calculations of constructing non-optimal sub-graphs ( Sub-NCSG 1 , Sub-NCSG 2 and Sub-NCSG 3 ) from the non-optimal dominants 1, 2 and 3 and searching for all of their MLCS s is very small due to the reuse of intermedi-ate results available, the levels information of key points of optimized subgraph Sub-NCSG 4 .

Strategy 4 : Subsection Calculation and Serialization . As discussed above, for longer aligned sequences ( n  X  10 3 ), the challenge is that their MLCS-DAG is too large to be stored and calculated in RAM at all (see Table 1), resulting in lead-ing dominant-point-based MLCS algorithms X  failure in this Figure 8: The sketch of searching for all the MLCS s of S 4 and S 5 with Strategy 4. ( (a) The left side of case. Therefore, we must devise a new subsection calculation method to deal with the case. And yet, we cannot use gener-al subsection method to obtain all the MLCS s of the longer sequences in the case because the MLCS s of the sequences are certainly not equal to the connection of the MLCS s of their sub-sequences in most cases. For example, we split the sequences S 4 = T GACGAT C and S 5 = AT CGT CAG into two subsections S 41 = T GAC and S 51 = AT CG with S 42 = GAT C and S 52 = T CAG in their subscripts 4, and all of the MLCS s of the sequences S 4 and S 5 is not equal to the connection of the MLCS s of the splitted subsection-s S 41 and S 51 with S 42 and S 52 as some dominants from the sequences S 4 and S 5 are lost in such splitting manner. Hence, how to split these longer sequences to accurately ob-tain their MLCS s is a non-trivial problem, which motivates us to develop a new subsection calculation method to tackle it. We found that as long as all of the dominants are not lost from aligned sequences and the immediate predecessor-successor relationships among dominants are preserved in the splitting position, we can solve the non-trivial problem efficiently and effectively with our proposed subsection cal-culation and serialization methods, which are explained with Example 3 and Fig. 8.

Example 3: With Strategy 4, find out all of the MLCS s of the sequences S 4 = T GACGAT C and S 5 = AT CGT CAG . For the sake of generality, suppose we split the sequences S 4 and S 5 into two subsections in their subscripts 6 on the premise of no loss of dominants of the two sequences. To this end, we first split the dominants those all their coor-dinates are less than or equal to 6 in the NCSG of the sequences to the first subsection, otherwise to the second subsection. Therefore, call function OptCalReusing2 () (not function OptCalReusing1 (), as no dominant is lost from the first subsection), we can construct the first subsection sub-graph of the NCSG , denoted as Sub-NCSGs 1 (Fig. 8(a)), and then serialize the constructed Sub-NCSGs 1 (left part shown in Fig. 8(b)) to the disk. Next, we continue constructing the second subsection subgraph Sub-NCSGs 2 by calling function OptCalReusing1 () (right part shown in Fig. 8(b)). To find all of the MLCS s of the sequences S 4 and S 5 , we first back-ward sort the Sub-NCSGs 2 by Algorithm BackwardSortTop . Secondly, we deserialize the Sub-NCSGs 1 and perform Sub-NCSGs 1 = Sub-NCSGs 1  X  Sub-NCSGs 2 shown in Fig. 8(c), and then continue backward sorting the Sub-NCSGs 1 by Al-gorithm BackwardSortTop , after which all of the MLCS s of the sequences S 4 and S 5 are obtained (denoted as red arrows shown in Fig. 8(c)).

Remark: As discussed in Strategy 4, for the longer se-quences, their NCSG will be too large to be stored and cal-culated in memory resulting in the leading dominant-point-based MLCS algorithms X  failure. To tackle the challenges, we propose Strategy 4 by splitting the NCSG of the se-quences into subsections. Experimental results justify that the memory overflow for large NCSG has been well avoided by Strategy 4 as expected. More interestingly, the efficiency is also improved due to the strategy. For instance, given |  X  | = 20 with length 300 of 5 aligned sequences, the running time of our algorithm is 3.449s, 1.969s, 1.807s and 0.846s, re-spectively, if the number of the splitting subsections for the aligned sequences is 2 , 3 , 4 and 5, respectively. Such an inter-esting phenomenon results from the following facts. Firstly, with algorithm ForwardSortTop for an arbitrary subsection, many useless edges will be eliminated (see Fig. 4). e.g. , with algorithm ForwardSortTop , the edges (0 , 0 , 0)  X  (2 , 3 , 4), (4 , 4 , 1)  X  (8 , 6 , 7) etc., shown in Fig. 3, have been delet-ed), which saves much time in BackwardSortTop performed afterwards. Secondly, as the number of subsections increas-es, not only the number of deleted edges increases but also the effect of concurrent execution amplifies, both of which lead to a significant decrease in running time. Notably, al-though the increase in the number of subsections may lead to additional serialization and deserialization operations, the running time of these operations is in fact negligible compar-ing to that of ForwardSortTop and BackwardSortTop for the massive dominants, which has been saved due to Strategy 4.
Strategy 5 : The Multiple Concurrent Execution . Since the optimal subgraph NCSG i of the aligned sequences al-ways contributes to most of the MLCSs , ranging from 75% to 100%, we can concurrently construct the non-optimal sub-graph NCSG j of the non-optimal dominants with the reuse of intermediate results of the optimal subgraph NCSG i avail-able (called multiple concurrent execution ) so as to further improve the efficiency of the proposed algorithm.
For large-scale and big sequences alignments, based on the above strategies, we present a novel efficient parallel MLC-S algorithm RLP MLCS , which is shown in Algorithm 1. Notably, on one hand, all the primary procedures of our al-gorithm, the construction of NCSG of aligned sequences, a forward and a backward topological sorting to the NCSG , are run in parallel. On the other hand, the parallel efficiency of our algorithm with Strategies 4 and 5 is further enhanced Algorithm 1 RLP MLCS( { S 1 , S 2 , ..., S d } ,  X  , StepLength ) and enlarged. To the best of our knowledge, we are the first to present a parallel MLCS algorithm in this manner. For lack of space, we give the framework of our algorithm.
The time complexity of our algorithm in every stage is given first, followed by the total time complexity.
For each sequence S l of T over alphabet  X  with length n , O ( |  X  | n ) time is required for constructing its successor table ST l by Eq. 3. Therefore, the time complexity of seri-ally constructing d sequences is O ( d |  X  | n ). The main opera-tions of serially constructing the NCSG consist of establish-ing the predecessor-successor relationship among dominants and computing the in-degree of each point of the NCSG . Therefore, the time complexity of serially constructing the NCSG should be O ( | E | ), where | E | is the number of edges in the NCSG . Given the points set V of the NCSG , since both forward and backward topological sorting need to traverse every point in V , both the forward and backward topological sorting take the time O ( | V | ).

As a result, the total serial time complexity of RLP MLCS and experimental results show that O ( d |  X  | n )  X  O ( | E | ) + over, with O ( | E | ) being of the same order as that of O ( | V | ) and | E | is at most several times of | V | , the time complex-ity of RLP MLCS is 1 N communication overhead of the parallel execution algorithm RLP MLCS and N p is the number of threads. From the above discussion, we can see that the time complexity of RLP MLCS is linear in | V | . It is important to note that | V | should be replaced with | K | + | V si | with subsection cal-culation of our proposed algorithm, where | K | is the total number of the key points on the NCSG , and | V si | is the to-tal number of the dominants on a subsection subgraph Sub-NCSGs i of the aligned sequences (as the above discussion, we can see that | K | + | V si | X  X  V | ).

Similarly, the storage space of successor tables is O ( d |  X  | n ), the storage space of the NCSG is O ( d | V | + | E | ), and the s-pace complexity of RLP MLCS is O ( d |  X  | n + d | V | + | E | )  X  O ( | V | + | E | ) = O ( | V | ) (given d aligned sequences, d is a constant). Similar to the analysis in time complexity, with subsection calculation, the space complexity of RLP MLCS should be O ( | K | + | V si | ).
 Notably, the state-of-the-art dominant-point-based MLC-S algorithms, FAST MLCS [2] with effective pruning tech-niques and Quick-DPPAR [16] with a fast divide-and-conquer technique in the calculation of the dominants, were claimed to be efficient. However, both the algorithms FAST MLCS and Quick-DPPAR do not eliminate the inherent defects of the general dominant-point-based MLCS algorithms (see Section 3). The claimed linear time complexity O ( | MLCS | ) without considering the time of computing dominants is not reasonable for FAST MLCS , while the time complexity of Quick-DPPAR , 1 /N p (1+  X  ( n )) O ( n |  X  | d + | D ||  X  | d ( log log d  X  2 |  X  | )), where lim n  X  X  X   X  ( n ) = 0, is obviously non-linear, as | D | is the number of the vertices of MLCS-DAG which is much larger than | V | of NCSG . Thus the complexities of time and space for our proposed algorithm are much lower than those of FAST MLCS and Quick-DPPAR . In experiments, all the algorithms ( FAST LCS, Quick-DPPAR and the proposed RLP MLCS ) were run on Inspur Corporation K1 800 high-performance key host (Intel X-eon E7-8870, 4 chip, 4 cores/chip, 2 threads/core, 2.80 GHz and 1TB RAM ) in the High Performance Computing Cen-ter of Xidian University, written in Java with JDK 1.7 and tested on the benchmark datasets provided by real biolog-synthetic sequences randomly drawn from alphabets of the DNA and amino acid, wherein ten groups of real biologi-cal sequences and synthetic sequences are randomly select-ed from the datasets, respectively, and each group consists of 5 or d sequences. We tested the algorithms 10 times on the above 20 groups of benchmark datasets. Notably, the result MLCS s are consistent over all the algorithms as long focus on the comparison in efficiency and report the average running times over 10 runs in milliseconds (ms).
 Firstly, we test the performance of our parallel algorithm RLP MLCS and compare it with the state-of-the-art dominant-point-based parallel FAST LCS [2] and Quick-DPPAR [16] algorithms (these algorithms were reported to have been im-plemented in corresponding literature and run on the same hardware platform). Due to space limit, only parts of the experimental results are shown in Table 2 and Table 3, re-spectively, where  X + X  stands for the memory overflow such that the algorithm fails in the corresponding case.
Table 2 shows that the time performance of the proposed parallel RLP MLCS algorithm is superior to that of the oth-er two competitors, reaching up to 2  X  3 orders of magnitude faster for long sequences. For example, the running time of our algorithm RLP MLCS shown in Table 2 is in aver-Table 2: The running times of FAST LCS (A1), Quick-DPPAR (A2) and RLP MLCS (A3) for 5 se-quences with various lengths on 32 threads and step length 100. Table 3: The running times of FAST LCS (A1), Quick-DPPAR (A2) and RLP MLCS (A3) for d se-quences with lengths 100 and 250, on 32 threads with step lengths 100 and 250, respectively. age 58 times / 47 times shorter than that of the algorithm FAST LCS (ranging from 6  X  127 times / 4  X  115 times), and 113 times / 84 times shorter than that of the algorithm Quick-DPPAR (ranging from 2  X  374 times / 2  X  257 times) in different |  X  | individually. Moreover, with the increasing length of aligned sequences, the advantage of RLP MLCS in time performance is even more obvious compared with algo-rithms FAST LCS and Quick-DPPAR . Obviously, Table 2 shows that our algorithm RLP MLCS is not only vastly su-perior to the algorithms FAST LCS and Quick-DPPAR , but also is more practical to longer sequences alignments.
As discussed above, thanks to the well-designed strate-gies adopted in RLP MLCS , we have eliminated redundant dominants, got rid of dimension by dimension comparisons of dominants, and saved much storage space compared with the state-of-the-art competitors. Table 3 reveals that these advantages of our algorithm RLP MLCS get more obvious as the number of sequences alignments is increased, e.g., for the different |  X  | , when testing on 6 sequences with the lengths of 100 (resp., 250) individually, the running time of RLP MLCS has already been up to 272 (resp., 486) times better than that of FAST MLCS and 180 (resp., 378) times better than that of Quick-DPPAR . It is a remarkable fac-t that FAST MLCS and Quick-DPPAR cannot work due to memory overflow when the number of aligned sequences is larger than 6. In comparison, our proposed algorith-m RLP MLCS can also run correctly and efficiently even though the number of aligned sequences reaches 1000. More-over, with the increasing number of aligned sequences, the number of dominants firstly increases and then usually de-creases. Thus, the experimental results of our algorithm RLP MLCS shown in Table 3 is reasonable, and the algo-rithm is very suitable for large-scale sequences alignments.
In addition, we further evaluated the speedup of our algo-rithm RLP MLCS by varying the number of threads. Due to space limit, we briefly describe the high-level results. The results show that our algorithm RLP MLCS achieves a n-early linear speedup, and that the larger d , n , and |  X  | , the better speedup is.

In a word, the time and space performances of RLP MLCS are not only very superior to the state-of-the-art FAST LCS and Quick-DPPAR , but also more practical to longer and large-scale or even big sequences alignments.
In order to overcome the disadvantages of the leading dominant-point-based MLCS algorithms and tackle the chal-lenges of longer and large-scale aligned sequences, we first present a novel general problem-solving model NCSG and series of new strategies, e.g. , the parallel topological sorting, optimal calculating, reuse of intermediate results, subsec-tion calculation, etc. Based on that, we present a novel real linear and parallel MLCS algorithm RLP MLCS . In addition to the external communication overhead T com of parallel execution with N p threads, theoretical study show that the time and space complexities of the proposed algo-rithm are only linear to and even smaller than the number of the dominants from aligned sequences, i.e. , 1 N T com and O ( | V | ), respectively. In particular, with subsec-tion calculation of our algorithm, | V | should be replaced by | K | + | V si | , where | K | is the total number of the key points on the NCSG , | V si | is the total number of the dom-inants on a subsection subgraph Sub-NCSGs i , and | K | + | V si | X  | V | . Finally, our algorithm is evaluated by compre-hensive experiments on datasets of both random synthetic and real biological sequences. The results show that the proposed problem-solving model NCSG and the strategies are efficient and effective, and that the proposed algorithm RLP MLCS not only greatly outperforms the leading state-of-the-art dominant-point-based parallel MLCS algorithms available, but also is practical for longer and large-scale se-quences alignments.

As part of our future work, we will further improve the efficiency of the proposed algorithm RLP MLCS and explore applications in big sequence analysis.
 This work was supported by the National Natural Science Foundation of China (No.61472296, 61472297 and 61202179) and National High Technology Research and Development Program (863 Program) (Grant No. 2015AA016007). [1] A. Apostolico, S. Browne, and C. Guerra. Fast [2] Y. Chen, A. Wan, and W. Liu. A fast parallel [3] D. S. Hirschberg. A linear space algorithm for [4] I. L. Hofacker, M. A. Huynen, P. F. Stadler, and P. E. [5] E. Horowitz and S. Sahni. Fundamentals of data [6] W. Hsu and M. Du. Computing a longest common [7] J. W. Hunt and T. G. Szymanski. A fast algorithm for [8] G. Ifrim and C. Wiuf. Bounded coordinate-descent for [9] D. E. Knuth. The Art of Computer Programming, [10] D. Korkin. A new dominant point-based parallel [11] Y. Li, Y. Wang, and L. Bao. Facc: a novel finite [12] M. Lu and H. Lin. Parallel algorithms for the longest [13] D. Maier. The complexity of some problems on [14] W. J. Masek and M. S. Paterson. A faster algorithm [15] D. Sankoff. Matching sequences under [16] Q. Wang, D. Korkin, and Y. Shang. A fast multiple [17] J. Yang, Y. Xu, and Y. Shang. An efficient parallel [18] J. Yang, Y. Xu, G. Sun, and Y. Shang. A new [19] T. K. Yap, O. Frieder, and R. L. Martino. Parallel [20] M. Zvelebil and J. Baum. Understanding
