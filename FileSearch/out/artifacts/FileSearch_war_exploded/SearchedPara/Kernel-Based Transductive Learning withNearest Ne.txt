 T he k -n earest n e i ghb o r (KNN) a l g o r i thm [1] i sas i mp l ea n de ff ect iv esuper -h i gh perf o rma n ce .B ut in app li cat ion s li ke o b j ect i de n t i ficat ion, te x tcateg o-r i zat ion, a l arge am o u n t o f l abe l ed data n eed a lo t o fhuma n e ffo rts ,w hereas u nl abe l ed data i squ i te cheap a n deas y t oo bta in. T h i s i sthereas on that tra n s -duct iv e l ear nin g o rsem i-super vi sed l ear nin ghasbee n de v e lo ped in rece n t y ears [ 2 ][3][ 4 ][5][6][ 7 ][ 8 ][9][10].

T ra n sduct ion o rtra n sduct iv e l ear nin g w as first in tr o duced b yVl ad i m i r V ap -ni k w h o th in ks i t i s preferab l et oin duct ion because in duct ion requ i res s olvin g am o re ge n era l pr o b l em bef o re s olvin gaspec i fic pr o b l em , e . g . dec i s ion trees , SVM s , a n dtra n sduct ion s olv es the spec i fic pr o b l em d i rect ly. T h i s i ss o-ca ll ed t o capture the g lo ba l structure o f the data that i sassumedt o be he l pfu l t o pre -d i ct c l ass l abe l s .B ut th i s appr o ach requ i res assumpt ion s on the structure o fdata  X  first , c l uster assumpt ion that data in thesamec l uster are supp o sed t o be in the same c l ass , a n dsec on d , ma ni f ol d assumpt ion that the h i gh -d i me n s ion a l data us in gpa i r wi se d i sta n ce t o a voi dthecurse o fd i me n s ion a li t y.

C on s i der the t wo in ter lo ck r in gs pr o b l em in Fi g .1(l eft ). W ea i mt o fi n dthe c l ass l abe l f o reachp oin t , g iv e nonly t wo l abe l ed p oin ts .Mix ture m o de l sca nno t s olv e i ts in ce d i str i but ion o fp oin ts in ac l uster i su n k nown in stead o fsta n dard i t because o ft oo fe wl abe l ed p oin ts .T he d i fficu l t yo fapp lyin g supp o rt v ect o r mach in es (SVM s )on such data i sthat i t i shardt o fi n dther i ght ker n e l fu n ct ion t o map the data fr o mthe o r i g in a l space t o ah i gher d i me n s ion a l space w here the data bec o mes lin ear ly separab l e .Al s o, ker n e ll ear nin g i st i me -c on sum in g .Sow e d ono tc on s i der SVM s in th i spaper .

In stead ,w epr o p o se a n appr o ach ca ll ed TKNN, w h i ch app li es tra n sduct ion t o the KNN a l g o r i thm t o s olv eth i spr o b l em .T rad i t ion a lly, the n earest n e i ghb o rs in the KNN a l g o r i thm are defi n ed on the l abe l ed data only. In th i spaper ,w e re -e x am in ethemeth o da n de x te n d KNN b y redefi nin g n earest n e i ghb o rs as fr o mb o th l abe l ed data a n du nl abe l ed data .Two gr o ups o f n earest n e i ghb o rs are s i mu l ta n e o us ly c on s i dered f o r each data p oin t  X  X n egr o up fr o m l abe l ed data , a n dthe o ther fr o mu nl abe l ed data .A ker n e l fu n ct ion o rrad i a l bas i sfu n ct ion i susedt oin crease w e i ghts f o r n earer n e i ghb o rs .T he n g lo ba l structure o fdata i scaptured in agraph .T ak in gad v a n tage o fg lo ba l structure ,l abe l spr o pagate thr o ugh the graph , a n dthet wo in ter lo ck r in gs pr o b l em ca n be s olv ed c o mp l ete ly.
No te that TKNN i sm o re s o ph i st i cated tha n s i mp l e lo ca l pr o pagat ion start in g t ion fa il s because ,wi th o ut g lo ba l structure o fdata , the l abe l pr o pagates acr o ss the sparse area bet w ee n t wo c l asses . Other wi se in TKNN, due t o the ker n e l fu n ct ion, l abe l pr o pagat ion in sparse areas g iv es w a y t o that in de n se areas a n d fi n a lly the sparsest area i s i de n t i fied as the b o u n dar y.

Our C on tr i but ion sareasf ollow s :  X  W eapp ly tra n sduct iv e l ear nin gt o KNN, wi th the ker n e l fu n ct ion ad j ust in g  X  W eder iv e the recurre n ce re l at ion bet w ee nl abe l ed data a n du nl abe l ed data , T he rema inin gpart o fth i spaper i s o rga ni zed as f ollow s .InS ect ion 2 w ed i scuss the m o de l f o rmu l at ion, in c l ud in gder ivin g recurre n ce re l at ion bet w ee nl abe l ed data a n du nl abe l ed data .InS ect ion 3 w epr o p o se s ol ut ion sa n dder iv ethe i ter -wo rk .S ect ion 6 i sthec on c l us ion. 2.1 Problem Description and Notation In th i spaper ,w ea i mt o s olv e the pr o b l em o fc l ass i ficat ion b y app lyin gtra n s -duct iv e l ear nin gt o the KNN a l g o r i thm ,i. e ., tak in gad v a n tage o fu nl abe l ed data as w e ll as l abe l ed data .
 A ssume w eareg iv e n the set o fp oin ts D = D L  X  X  U in a p -d i me n s ion a l Euc li dea n space R p ,w here D L = { x i } 1  X  i  X  l i stheset o f l abe l ed p oin ts a n d D ass i g n ed t o each member o f D .T he set o fc l asses i sde no ted b y C = { c r } 1  X  r  X | C | |
C | i sthecard in a li t yo f C a n d r i sa nin teger .
 T he c l ass l abe lo fp oin t x i  X  X  i sde no ted b y y ( x i )o r y i ,w here y i  X  C . Each p oin t in D L has bee nl abe l ed o rass i g n ed t oon ec l ass in C ,w h il ethe c l ass l abe lo feachp oin t in D U i su n k nown a n d n eeds t o be determ in ed .In th i spaper , k l de no tes the n umber o f n earest n e i ghb o rs in l abe l ed data , a n d k u de no tes the n umber o f n earest n e i ghb o rs in u nl abe l ed data .T he k l n earest b y { m iq } 1  X  q  X  k u ,w here m iq i s x i  X  s q th n earest n e i ghb o r in D U . 2.2 Extending KNN In super vi sed KNN l ear nin g [1 2 ][1], the n earest n e i ghb o rs in c l uded only l abe l ed paper ,w epr o p o se TKNN after e x te n d in g KNN b yin c l ud in gb o th l abe l ed a n d u nl abe l ed p oin ts in the n earest n e i ghb o rs o fap oin t .

A fter app lyin gthe KNN t o de n s i t y est i mat ion [1], f o rap oin t x the p o ster io r pr o bab ili t y p ( c r | x ) ca n be est i mated as w here k ( r )i sthe n umber o fp oin ts wi th c l ass l abe l r am on gthe k n earest n e i ghb o rs o f x . Ob vio us ly, | C | r =1 k ( r )= k .
In super vi sed KNN l ear nin g [1 2 ][1], k ( r )in Eq .(1)i sc o mputed based on TKNN, there are t wo gr o ups o f n earest n e i ghb o rs f o reachp oin t . O n egr o up n e i ghb o rs fr o mu nl abe l ed p oin ts .Wi th the he l p o fu nl abe l ed data , the am o u n t o f l abe l ed data ca n be sma ll.

L et k l ( r ) be the n umber o f l abe l ed p oin ts in c l ass c r , a n d k u ( r ) be the n umber Fo rc onv e ni e n ce o f descr i pt ion, w edefi n ethec l ass matr ix.
 Class Matrix. T he c l ass matr ix f o radatasetdescr i bes pr o bab ili t i es that data are ass i g n ed t o c l asses , data as r ow sa n dc l asses as c ol um n s .T he n the sum o f v a l ues in each r ow i s 1.
 Definition 1. The c l ass matr ix of data set D = { x i } 1  X  i  X  l + u is defined as P = ( p T he c l ass matr ix P ca n be w r i tte nin t wo b lo cks : w here P L  X  sr ow sc o rresp on dt ol abe l ed data a n d P U  X  sr ow sc o rresp on dt o u nl abe l ed data .

P L i sdefi n ed acc o rd in gt o pr io rk nowl edge o f l abe l ed data .Fo ra ny p oin t x y th c ol um ni ssett o1.Fo rma lly, P L =( p ij ) l  X | C | ,w here 2.3 Kernel-Based Weights for Neighbors fu n ct ion, t o c o mpute w e i ghts .
 Kernel Function. In th i spaper ,w euseGauss i a n fu n ct ion as ker n e l w here  X  i sEuc li dea n d i sta n ce a n d h i s the ba n d wi dth .

T he ba n d wi dth h in Eq .(3)i sa l s o ca ll ed sm oo th parameter .I ts se l ect ion i mpacts the perf o rma n ce o ftheker n e l-based a l g o r i thm .
 Weight Matrix. B ased on the ker n e l fu n ct ion, w edefi n ethe w e i ght matr ix. T he w e i ght matr ix o f a data set descr i bes i mpacts o f n e i ghb o rs t o each data p oin t .I ts r ow sa n dc ol um n sarea ll the data p oin ts .Fo reachr ow, only c ol um n s c o rresp on d in gt o t wo gr o ups o f n earest n e i ghb o rs are g iv e nw e i ghts , a n d o ther c ol um n are g iv e n0.T he n the matr ix i s no t n ecessar ily s y mmetr i c .B ased on the w e i ght matr ix, a KNN graph i sc on structed .
 Definition 2. Let  X  be a real number in [0 , 1] .The w e i ght matr ix of data set D where { n iq } 1  X  q  X  k l and { m iq } 1  X  q  X  k u are x i  X  X  neighbors from labeled data and unlabeled data.
 T he parameter  X  , ca ll ed infl ue n ce fact o r o fu nl abe l ed data ,i susedt o ad j ust u n-l abe l ed data  X  s infl ue n ce in the graph .I f  X  =0, u nl abe l ed data ha v e no infl ue n ce . I f  X  =1, u nl abe l ed data ha v ethesame infl ue n ce as l abe l ed data .

W er ow-no rma li ze W ,i. e ., add in gr ow e l eme n ts t o gether a n dd ivi d in geach e l eme n tb y the c o rresp on d in gr ow t o ta l s .T he nw egetr ow-no rma li zed matr ix V . Ob vio us ly, the sum o feachr ow X  se l eme n ts in V equa l s 1. W eca ll V the normalized weight matrix . C on s i der in g D = D L  X  X  U , V c on s i sts o ff o ur b lo cks : respect iv e ly. S uperscr i pts L a n d U in V LU mea n r ow sfr o m l abe l ed data a n d c ol um n sfr o mu nl abe l ed data , respect iv e ly. 2.4 Recurrence Relation A pp lyin gEq .(1) t o x i ,w e get est i mat ion f o r  X  p ( c r | x i ) that i s p ir : D ue t oin tr o duct ion o ftheker n e l fu n ct ion, k l ( r ) a n d k u ( r ) are rea ln umbers in stead o f in tegers .T he sum o fthemca n be est i mated based on infl ue n ce t o x i fr o m o ther data p oin ts : Eq .(6) a n d ( 7 ) c o mb in et o g iv e Eq .( 8 ) ca n be w r i tte nin matr ix: P = VP . C on s i der in gEq .( 2 ) a n d (5), Eq .( 8 ) ca n be further w r i tte n as : Sin ce w ed ono tc on s i der the infl ue n ce t o a l abe l ed data p oin tfr o m o ther p oin ts , w e subst i tute the i de n t i t y matr ix I f o r V LL , a n d a zer o matr ix 0 f o r V LU . Eq .(9) bec o mes F r o mEq .(10),w e o bta in ed the recurre n ce re l at ion in the data set : 3.1 Solution 1: Matrix Solution F r o m the recurre n ce re l at ion in Eq .(11),w eha v ethematr ix s ol ut ion assum in g that the inv erse o fmatr ix ( I  X  V UU ) e xi sts .T h i s i sthematr ix s ol ut ion.
A fter P L a n d V are k nown, b y Eq .(1 2 ), w e o bta in P U , the pr o bab ili t y d i s -tr i but ion s o fa ll u nl abe l ed p oin ts ov er c l asses .T he c l ass l abe lwi th the l argest pr o bab ili t y f o reachr ow o f P U i sse l ected f o rc l ass i ficat ion dec i s ion. A ssume l f o rmatr ix inv ers ion a n dmu l t i p li cat ion, w here u i sthe n umber o fu nl abe l ed data . T h i ss ol ut ion i s appr o pr i ate f o rsma ll a n dmed i um -s i ze data sets . Special Cases. I f  X  =0, the n V UU = 0 a n dEq .(1 2 ) bec o mes P U = V UL P L , a n dthes ol ut ion i sd own graded t o super vi sed l ear nin g  X  ker n e l-based w e i ghted KNN. I f  X  =1, the u nl abe l ed data ha v ethesame infl ue n ce as l abe l ed data in the graph . 3.2 Solution 2: An Iterative Algorithm In s o me cases the matr ix ( I  X  V UU )in Eq .(1 2 )i ss in gu l ar o r appr oxi mate ly s in gu l ar , caus in gthat i ts inv erse ( I  X  V UU )  X  1 d o es no te xi st (w he n det ( I  X  V
UU )=0)o r appr o aches in fi ni t y. T he n the matr ix res ol ut ion fa il s o rbec o me W eprese n ta ni terat iv ea l g o r i thm that i sm o re effic i e n ta n dm o re fl e xi b l e . B ased on Eq .( 8 ), w epr o p o se a ni terat iv epr o cess t o s olv e P U .
 Iterative Process  X  Ini t i a li zat ion:  X  Repeat u n t il c onv erge n ce : Pl ease no te that w he n updat in g p ir ,w eusepre vio us ly c o mputed p jr  X  sass oon as the y are a v a il ab l e ,in stead o fus in g ol dest i mated p jr  X  s .

Ano ther reas on that o ur a l g o r i thm c onv erges qu i ck ly i sthatf o reachu nl abe l ed T he n the rate o f l abe l pr o pagat ion i sfast .Fo rm o st p oin ts , the i r l abe l sca n be stab l equ i ck ly. T hat makes o ur a l g o r i thm faster tha n s o me pre vio us graph -based i terat iv ea l g o r i thms .In add i t ion, c on s i der in g V i s a sparse matr ix, each i terat ion X  st i me c o mp l e xi t yi sactua lly O ( ku | C | )w here k = k l + k u ,in stead o f O ( nu | C | ).
 Convergence. W eca n pr ov ethatth i s i terat iv epr o cess i sc onv erge n t , c on s i d -decreases m ono t oni ca lly in the pr o cess .
 Flexibility. T h i sa l g o r i thm i sa l s ofl e xi b l ea n dca n be c o mb in ed wi th onlin e l ear nin g .D ur in g onlin e l ear nin g , the l ear n er rece iv es feedback a n ds o me u nl a -be l data are l abe l ed a n daddedt ol abe l ed data .To address th i spr o b l em , the 3.3 Outlier Detection take .Wi th in the update pr o cess ,w epr o p o se a meth o dt o detect o ut li ers in D L . Diff ere n tfr o m S ect ion 2 . 4 ,w ec on s i der the infl ue n ce a ll ov er the data set ,in-i ts est i mat ion f o r o ut li er detect ion i s  X  P L .F r o mEq .(9),w eha v e F r o mEq .(1 2 ), w eha v e Algorithm 1. TKNN  X  tra n sduct iv e l ear nin g wi th n earest n e i ghb o rs S ubst i tute  X  P U in Eq .(1 4 )wi th Eq . (15) a n d w eha v e T h i s i stheest i mat ion o f P L f o r o ut li er detect ion. In Eq .(16), V LL re fl ects the infl ue n ce bet w ee nl abe l ed data d i rect ly, a n d V LU ( I  X  V UU )  X  1 V UL re fl ects the infl ue n ce bet w ee nl abe l ed data through u nl abe l ed data ,w h i ch i sfirst in tr o duced data sets .

Loo k in gf o r o ut li ers ,w ec o mpare each r ow o f  X  P L wi th the c o rresp on d in gr ow o f P [0] L .A ssume t wo c o rresp on d in gr ow sarether ow v ect o r  X  p fr o m  X  P L a n dther ow p T h i s i sf o r on edatap oin t .Fo ra ll l abe l ed data ,w eha v ethec ol um nv ect o r d i ag on a l e n tr i es o fasquarematr ix.

W edec i de i f l abe l ed data are o ut li ers b yv ect o r a .A ssume a i sac o mp on e n t o f a .I f a&lt;p thresh , a  X  sc o rresp on d in gdatap oin tc o u l dha v ebee n m i s l abe l ed a n d tected o ut li ers are treated as u nl abe l ed p oin ts ;l abe l ed a n du nl abe l ed data are re -part i t ion ed bef o re c l ass i ficat ion. 3.4 Algorithm scr i be o ur a l g o r i thm ca ll ed TKNN in Al g o r i thm 1. W e first detect o ut li ers in l a -be l ed data a n d treat them as u nl abe l ed .T he n f o rasma ll o rmed i um -s i ze data set , w eusematr ix c o mputat ion s ol ut ion. An df o ra l arge data set ,w eusethe i terat iv e pr o cess s ol ut ion. 4.1 Data Sets ar o u n dt wo r in gs in 3D space wi th Gauss i a nnoi se . O nly t wo p oin ts are l abe l ed ,on e s ion s , 1500 p oin ts . Digit1-10 mea n s 10 p oin ts are l abe l ed a n d Digit-100 mea n s 100 mea n s 10 p oin ts are l abe l ed a n d USPS-100 mea n s 100 p oin ts are l abe l ed . 4.2 Experimental Results Fi rst ,w eha v et o me n t ion the parameter b .InS ect ion 2 .3 w e in tr o duced ba n d -wi dth h o ftheker n e l fu n ct ion. T he se l ect ion o f h has i mpact t o perf o rma n ce .B ut h ra n ges l arge ly f o rd iff ere n tdatasets .T he nw edefi n e the ba n d wi dth rat io b as se l ect ion, h f o raspec i fic data i s o bta in ed .

Fi g . 2sh ow sthec onv erge n ce pr o cess o f o ur TKNN i terat iv ea l g o r i thm on the ca n be o bser v ed that dur in g the pr o cess , the n umber o fra n d o m ly c l ass i fied p oin ts t ion. S ec on d , the n e w est data are i mmed i ate ly used t o c o mpute n e i ghb o rs  X  X  mpact t o ap oin tateach i terat ion.

W ec o mpare o ur a l g o r i thm t o 1NN (KNN wi th k =1), Rad i a lB as i s F u n ct ion ( R BF) a n dthec on s i ste n c y meth o dassh own in Fi g .3.T he Gauss i a n fu n ct ion i s used f o rR BF. T he best perf o rma n ce i srec o rded f o reachmeth o d .W eca no bser v e that w he n the n umber o f l abe l ed p oin ts i s l ess ( 2 -r in gs ,Di g i t 1-10 a n d USPS-10), tra n sduct iv e l ear nin g i mpr ov es m o re fr o msuper vi sed l ear nin gthat i s 1NN a n d R BF here .W h il eb o th TKNN a n dthec on s i ste n c y meth o dach i e v e 1.0 accurac y on the 2 -r in gs data set ,TKNN has the best perf o rma n ce on o ther data sets . rameters .T he perf o rma n ce i sbest w he n the ba n d wi dth rat io 0 . 1 &lt;b&lt; 0 . 3, the n umber o f n earest u nl abe l ed n e i ghb o rs 3  X  k u  X  10 a n dthe n umber o f n earest l abe l ed n e i ghb o rs k l =1.T he best v a l ue o f k u depe n ds on data .T he best v a l ue k l =1 agrees t o huma nin tu i t ion that k l /k u sh o u l dbepr o p o rt ion a l t o rat io o f n umber o f l/u , the rat io o fs i zes o f l abe l ed data a n du nl abe l ed data . Our wo rk i sre l ated t o sem i-super vi sed l ear nin g , espec i a lly graph -based l ear nin g . [ 4 ]in tr o duced a meth o dthattakead v a n tage o fu nl abe l ed data f o rc l ass i ficat ion tasks .A fter that , sem i-super vi sed l ear nin g o rtra n dsduct iv e l ear nin gha v ebee n e x-te n s iv e ly researched .An d graphs are in tr o duced in sem i-super vi sed l ear nin g [1 4 ]. Wo rk in [10][13][15] i sm o st re l ated t oo ur wo rk .[10] pr o p o sed a sem i-super vi sed l ear nin gbased on aGauss i a n ra n d o mfie l dm o de l. L abe l ed a n du nl abe l ed data are matr ix meth o ds o rbe li ef pr o pagat ion. [13] pr o p o sed the c on s i ste n c y meth o dthat g o r i thm f o r l arge data sets a n df o rc i rcumsta n ces w here matr ix c o mputat ion fa il s because o fthe non e xi ste n t inv erse matr ix. [15] in tr o duced a t wo-stage appr o ach . In the first stage , am o de li sbu il tbased on tra inin gdata .In the sec on d stage , the m o de li susedt o tra n sf o rm u nl abe l ed data in t ow e i ghted pre -l abe l ed data set w h i ch i sused in the c l ass i fier . Our wo rk i s no tt wo-stage a n d infl ue n ce bet w ee n l abe l ed a n du nl abe l ed data are c on s i dered at the same t i me .[9] pr o p o sed a graph -i t y s i mu l ta n e o us ly. In KNN a l g o r i thm ,onlyl abe l ed data ca n be the n earest n e i ghb o rs .In th i spaper , w ee x te n d KNN t o atra n sduct iv e l ear nin ga l g o r i thm ca ll ed TKNN. W ec on s i der t wo gr o ups o f n earest n e i ghb o rs . O n efr o m l abe l ed data a n dthe o ther fr o mu nl a -be l ed data .W ethe n der iv e the recurre n ce re l at ion in adataset , c on s i der in gthat c i se est i mat ion o f parameters a n dapp li cat ion on data wi th m o re tha n t wo c l asses . Acknowledgments. T h i s wo rk i s supp o rted in part b y the f ollowin g NSF gra n ts : IIS-0 4 1 4 9 8 1 a n dC NS-0 4 5 42 9 8 .

