 for automatic Named Entity Recognition (NER) is highlighted in order to enhance the collect all the existing NEs in a dictionary . One reason is because the amount of NEs is tremendous, and another reason is due to the variability of NE set, new NEs appear while old NEs are eliminated. As a result, NER plays a vital role in NLP. 
In the 7 th Message Understanding Conference (MUC-7), NER mainly aims at seven classes of NE including Person Name (PN), Location Name (LN), Organization vided into two parts, that is, Beginning of PN (PN-B) and Inner of PN (PN-I). Simi-larly, LN can be divided into Beginning of LN (LN-B) and Inner of LN (LN-I). Since ONs usually consist of several words and the last word usually has obvious character-istic (e.g.  X   X  X  X  X  /Department of Defense X ), ONs are divided into three parts, that is, Beginning of ON (ON-B), Inner of ON (ON-I) and Ending of ON (ON-E). Since time and digital expressions are relatively easy to be recognized, our work focuses on sub-categories related to PN, LN and ON. 
In comparison with NER for Indo-European languages, there are more difficulties for Chinese NER, such as no space between words and no capitalization information usually calls for abundance training data to overcome these problems. This paper presents a fusion pattern of multiple features for Chinese NER based on Conditional Random Field (CRF) model. First, lists of human knowledge are con-ture templates are constructed based on the lists of human knowledge to extract both model. Using the trained model, the most possible label of each character can be cal-culated, and we combine characters based on their labels to get NEs. 
After trained on the NEs labeled corpus from People X  X  Daily and tested on corpus from People X  X  Daily as Open-Test-I and corpus from PKU in SIGHAN 2005 as Open-performance of our model is better than pure rule-based or statistics-based models, or speed of performance between the model without human knowledge and the model with human knowledge suggests that human knowledge is a reasonable way to combined in the model. English NER system in MUC-7 achieved 95% precision and 92% recall [3]. How-(Precision, Recall) of the best Chinese NER system is (66%, 92%), (89%, 88%) and (89%, 88%) for PN, LN and ON respectively [4]. 
Nowadays, the method of NER is taking a transition from simple rule-based mod-model and human knowledge. The pure statistical models that use single features are data sparseness and overfitting, which may reduce the accuracy. Thus, linguistic fea-tures are usually combined with pure statistics-based models to achieve better results. 
Among various statistical models, Maximum Entropy (ME) model has many mer-its such as flexibility of incorporating arbitrary features and it performs well in many between different states cannot be considered. Thus, Maximum Entropy Markov Model (MEMM) was proposed to combine the probability of transferring between states [3]. Nevertheless, MEMMs tend to get an optimal solution based on the optimi-Conditional Random Field (CRF) was proposed to overcome such problem [6]. CRF is widely used in many NLP tasks such as Segmentation, Shallow Parsing and Word Alignment, and it has been proven to be one of the best classifiers in NLP [7]. 
Besides the choice of classifier, another crucial aspect that greatly affects the per-formance of the model is the choice of features. A good classifier cannot work with-out good features, and a less powerful classifier may also perform well with a set of deliberate chosen features [8]. 
In fact, owing to the nature of Chinese, the use of character to construct NEs, espe-lead to the weight of two features differ a lot, but as a common sense, the probability of a person following those two words should be equal. As a result, we think that the the section 4. exponent distribution [6]. A common special case of CRFs is linear chain, which has CRF is undirected graph. A linear chain CRF has form described as follows: answer is a probability. The feature function actually consists of two kinds of feature, How to get these features will be discussed in section 5. CRF++ Toolkit uses L-BFGS for optimization, which converges much faster but requires more memory. In this section, we introduce human knowledge we used based on which features are extracted. All these word lists are constructed from the training data. (2) Chinese Surname List  X  Includes 887 items. Chinese surnames such as  X   X  (4) Person Title List  X  Includes 401 items. Person titles such as  X   X  X  X  /Minister X  and (5) Transliteration Characters List  X  Includes 799 items. Since the characters used to (6) Location Name List  X  Includes 2,059 items. Since many LNs do not have obvi-(8) Organization Suffix List  X  Includes 188 items. Suffixes of ONs such as  X   X  X  X  X  NER can be viewed as a classification problem that should predict the category of the candidate word using various features in the context. The features used in the model former is constructed based on neighboring tokens and the token itself, and the latter is extracted from other occurrences of the same token in the whole document. 
To take advantage of the ability of sequence labeling of CRFs, we give each char- X   X  /PN-I X . Since the length of ON could be very long and suffix characteristics of ON tagged as ON-E. 5.1 Local Feature There are two types of contextual informatio n to be considered when extracting local features, namely, internal lexical information and external contextual information. For provide the internal lexical information. 
Candidate features are extracted from the training corpus with feature templates. A ture templates are described as follows: (1#) C -4  X  The forth previous character. (2#) C -3  X  The third previous character. (3#) C -2  X  The second previous character. (4#) C -1  X  The previous character. (5#) C 0  X  The current character. (6#) C 0  X  Whether current word is in List (2). (7#) C 0  X  Whether current word is in List (3). (8#) C 0  X  Whether current word is in List (5). (9#) C 0  X  Whether current character is in List (6). (10#) C 0  X  Whether current character is in List (7). (11#) C 0  X  Whether current character is in List (8). (12#) C 0  X  Whether current character is in List (9). (13#) C 1  X  The next character. (14#) C 2  X  The second next character. (15#) C 3  X  The third next character. (16#) C 4  X  The forth next character. (17#) C -1,0  X  Whether this word is in List (1). (20#) C -1,0  X  Whether this word is in List (4). (21#) C -1,0  X  Whether this word is in List (6). (22#) C -1,0  X  Whether this word is in List (7). (23#) C -1,0  X  Whether this word is in List (8). (26#) C -2,-1, 0  X  Whether this word is in List (4). (27#) C -2,-1, 0  X  Whether this word is in List (6). (28#) C -2,-1, 0  X  Whether this word is in List (7). (29#) C -2,-1, 0  X  Whether this word is in List (8). (30#) C -3,-2,-1, 0  X  Whether this word is in List (4). (31#) C -3,-2,-1, 0  X  Whether this word is in List (6). (32#) C -3,-2,-1, 0  X  Whether this word is in List (7). (33#) C -3,-2,-1, 0  X  Whether this word is in List (8). (34#) C -4,-3,-2,-1,0  X  Whether this word is in List (4). (35#) C -4,-3,-2,-1,0  X  Whether this word is in List (6). (36#) C -4,-3,-2,-1,0  X  Whether this word is in List (7). (37#) C -4,-3,-2,-1,0  X  Whether this word is in List (8). from the training corpus and thus form a candidate feature base. 5.2 Global Feature Within a same document, a certain NE ofte n occurs repeatedly and it usually appears in the abbreviated forms in the latter text. Furthermore, some candidate words appear following three sentences: order in the document. NEs in Sentence (1) are easy to be recognized, and NEs in (2) the first glance since both external feature and internal feature provide limited useful used, this problem can be solved. features may play a crucial role. So we need global features to take full use of global information. Two global feature templates are constructed as follows: (1#) Other Occurrences with Same Form  X  Mainly aims to provide information of the (2#) Other Occurrences for Prefix and Suffix  X  Mainly aims to provide the important The label of each character is evaluated during the testing. After that, the continuous combined to form a PN  X   X  X  X   X   X . During the step of tagging, the weight of each fea-ability can be computed as follows: character. 7.1 Data Sets and Evaluation Metrics and is tagged with POS according to Chines e Text POS Tag Set provided by PeiKing University (PKU) of China. The size of the corpus is 2,248,595 bytes (including POS includes 4,907 PNs, 10,120 LNs and 2,963 ONs. One testing corpus Open-Test-I is randomly selected from the raw texts of People X  X  politics, economy, entertainment, sports, and so on. The size of this corpus is 121,124 bytes, including 270 PNs, 508 LNs and 118 ONs. 
Three parameters are used in the experimental evaluation. Precision (P) is the per-cent of the correctly recognized NEs in all the recognized NEs. Recall (R) is the per-cent of the correctly recognized NEs in all the NEs included in the testing corpus. F-measure (F) is a weighted combination of precision and recall, especially in the evaluation, PNs and LNs in nested form are also included, and only correct recognitions. 7.2 Experimental Results Based on Open-Test-I The experimental testing on Open-Test-I are based on the following three patterns. (1) CRF-based  X  Based on the basic CRF-based model only estimated with the 
Furthermore, to show that human knowledge can smooth the model, we divided training data into 10 pieces, and we compare the speed of raise in the performance of Base model and Base+LF+GF separately. The results are shown in Fig. 1. performance only using a little amount of training data. 7.3 Other Experimental Re sults Based on Open-Test-II experiments on Open-Test-II are also completed. The experimental results are shown in Table 4. 7.4 Analysis and Discussion with the performance of the current best Chinese NER system (as described in Section 2), we find that CRF model is exactly an effective statistical model for Chinese NER , and can get much better performance as well as or than other machine learning mod-els . can not only reduce the search space and improve the processing efficiency , but also increase the recognition performance significantly . 
Comparing Table 3 and Table 2 with Table 1, it can be seen that information from a sentence is sometimes insufficient to class ify a NE correctly and global context from the whole document is available , so high performance can be achieved through com-bining local features and global features in the proposed hybrid model.
Comparing Table 4 with Table 1, Table 2 and Table 3, the same conclusions as ex-periments on Open-Test-I can be confirmed , which shows the consistence of the estab-lished model on different testing data . 
Through analysis for the recognized NEs with failure and error, it can be found that there are some typical errors shown as follows. (2) No separation between consecutive NEs  X  For example,  X   X   X  /  X  X  X  X  X  X  X   X  / X . (6) Fuzzy bound of ON  X  Current means used to choose candidate word may cause (8) Sparse training of ONs  X  Since ONs are constructed with several words and our This paper introduces some research work on Chinese NER based on CRF model. An approach which has a CRF framework with the fusion of multiple features and applies human knowledge as the optimization strategy is presented. The experimental results showing the advantage of CRF approach, this model also shows that it X  X  an effective pattern to combine human knowledge in statistical model. In addition, the comparison with human knowledge shows that human knowledge can not only enhance perform-ance but also smooth the model to overcome the problem of data sparseness. 
Up to now, the features used in our model not only include word information, but speech tagging also has dependency on the correctness of NER. Also the feature tem-text, different window size, and so on. In addition, human knowledge used in feature representation is also a kind of feature. Theoretically, they could be used as the form of feature, and all words in the same list would be assigned the same weight. That will further reduce the manual interference and enhance the cohesion of the model. All the aspects above will be our research focuses in the future. 
