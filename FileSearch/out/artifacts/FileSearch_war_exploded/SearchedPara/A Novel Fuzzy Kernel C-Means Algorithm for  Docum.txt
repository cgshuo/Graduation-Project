 Fuzzy Kernel C-Means (FKCM) algorithm [2] extends kernel methods to Fuzzy C-Means algorithm which is introduced by Bezdek [1]. FKCM algorithm achieves better performance than classical FCM for nonlinearly separable data and clusters with overlaps. FCM algorithm often minimizes the sum of square of Euclidean distance between samples and centroids. The assumption behind this measure is the belief that the data space consist of isolated elliptical regions. However, such an assumption is not satisfies the requirement of the optimization measure. 
Though FKCM algorithms have excellent performance in many applications, it significant computation time and memory space for introducing kernel function, local optimal and bad convergence speed, These drawb acks restrict application in real world especially for large scale document clustering. The major challenges in using FKCM algorithm on text clustering lie not in performing the clustering itself, but rather in choosing the number of clusters and tacking with the high dimensional, sparse document vectors. Worse, kernel functions always consume significant computation time and store space especially for large text collection. More unfortunately, extremely sparse feature vect ors and the large difference size of clusters make some vectors be merged into the larg er clusters. All these drawbacks are subject to generalize in practical applications. 2.1 Kernelised Validity Index It X  X  often unfeasible to predefine the number of clusters in advance for large, high-dimensional text data. With an exponential increase in the complexity and volume of data, it is blind to assign labels to document without knowing any information about compactness and separation [4, 5, 6]. i ntroduced by Bensaid [6]. Then the validity index function can be rewritten as Let )) , ( 1 (
It X  X  readily shown that the following advantages are true. First, it is able to observing compactness sum of each cluster, it X  X  not sensitive to the size of cluster. 2.2 Sparse Format and Scatter-and-Gathering Each document vector generally has small percentage of nonzero elements. Therefore, also reduce considerable space to store it. 
Inherent drawbacks of the kernel do lie in dot production computations consume expensive kernel computational and store cost, we introduce Scatter-and-Gathering strategy to further enhance performance [9]. 
The Scatter-and-Gathering strategy is an efficient way of computing vector dot production for sparse vectors. The main idea is to first scatter the sparse vector into a evaluate the vector product. The strategy can explore the pipeline effect of the CPU to reduce the number of CPU cycles and lead to significant computing saving. Therefore, the strategy releases the expensive burden for kernel and makes it suitable for large-scale text data. 2.3 Semi-supervised Learning Text feature vectors are always very high dimensional and extremely sparse, leading to clustering suffers from great impact. The key advantage of incorporating prior knowledge into clustering algorithm lies in their ability to enforcing the correlations of feature vectors and enhancing the speed of convergence. Constraint-based and Distance-based methods [8, 10]. We introduce the Semi-Supervised Learning into FKCM algorithm. Then, We briefly discusses the problem. Let labeled vector B=[ j b ], n j , , 2 , 1 ... = In the help of labeled vectors, the better the degree of membership F is obtained F= ] [ ij f n j c i  X   X   X   X  1 , 1 For simultaneously obtaining the minimum of distance from clustering data to clustering center and the degree of prior membership, we rewrite The  X  is proportional to labeled data. Due to labeled data are far smaller than unlabeled data, we calculate  X  with the equation number of objects and the number labeled data respectively. Minimizing the objective function, we introduce the Lagrange multiplier  X  . Setting the partial derivative of the Lagrange  X  and variable respectively of equation (3) to zero st u yield From validity of clustering, we take the optimal value in interval [1.5, 2.5] [4]. Bezdek [1, 6] considered 2 = m is the optimal value. So we take 2 = m , then we have AS Step2: Compute and update the degree of membership ) ( t ij u using equation (5) Step3: Compute and update kernel ) , ( j i x v K using 3.1 Dataset Experiments were carried out on the popular dataset which was evaluated perfor-ance of text clustering algorithms. 422 Y. Yin et al. Reuters-21578: We constructed the subset of Reuter-21578 by sampling 320 documents from the original dataset. Featur es were extracted by removing stopwords by stoplist, performing stemming. The 1 pe rcentage documents of each category have been not removed topics. The subset embodies the feature characteristics of a typical text collection, which are high dimensional, sparse, some significantly overlapping and skewed. The briefly describes as follows. 3.2 Results The results on Reuter-21578 are shown in Figure 1. From the results, we see that the three algorithms perform well regarding balanced dataset with high overlapping. ASSFKCM obtains a small amount of improvement than FKCM for balanced dataset. FKCM gain better results indicate that the boundaries of text clusters are in possession Incorporating limited labeled data can marked improve in term of skewed dataset. A few things need to be noted regarding the run time and the store space in Figure2. vector in sparse format, we save marked st ore space. We need not load big matrix into sparse format and scatter-and  X  X atheri ng strategy achieve sharp decrease. FKCM algorithms which are based on minimizing the objective function have two parameter c . The second is that local optimal and slow convergence speed for skewed clusters. In this paper, we first have introduced a kernelised validity index measures the ensure the best description of the data structures. Then, semi-supervised learning has been explored to enhance the convergence and performance of clustering algorithms. introduced spare format and scatter-and-gathering strategy. In the end, the experiments on the popular benchmark datasets indicate that the algorithm proposed has high ability to automatic tackling with skewed and pronounced overlapping data clustering. 4. Pal, N.R., Bezdek, J.C.: On clustering for the fuzzy c-means model. IEEE Transaction on 7. Li, K., Liu, Y.: KFCSA:A Novel clustering Algorithm for High-Dimension Data. In: Wang, 
