 of the form where A  X  R m  X  n is a known measurement matrix, y  X  R m represents a vector of measurements and w  X  R m is a vector of measurements errors (noise).
 analyzed here.
 One simple and popular approximate algorithm is orthogonal matching pursuit (OMP) developed Among other results, Tropp and Gilbert show that when the num ber of measurements scales as n and k  X   X  . Deterministic conditions on the matrix A that guarantee recovery of x by OMP are given in [12].
 that the constant 4 can be reduced to 2.
 measurements result goes further by allowing uncertainty in the sparsity level k . of lasso for these problems is not warranted.
 the lasso minimization (14) can be replaced by matrices, this minimization will recover the correct vecto r with certain scenarios.
 error probability grows as n  X  k , not k ( n  X  k ) , independent events. (1). Let which is the support of the vector x . The set I sequence of estimates  X  I ( t ) , t = 0 , 1 , 2 , . . . , of the sparsity pattern I In the description below, let a Algorithm 1 (Orthogonal Matching Pursuit) Given a vector y  X  R m , a measurement matrix as follows: Note that since P ( t ) is the projection onto the orthogonal complement of a not select the same vector twice.
 The algorithm above only provides an estimate,  X  I estimate, where the minimization is over all vectors v such v the projection of the noisy vector y onto the space spanned by the vectors a pattern estimate  X  I and not the vector estimate b x . We analyze the OMP algorithm in the previous section under th e following assumptions. Assumption 1 Consider a sequence of sparse recovery problems, indexed by the vector dimension n components in x . Also assume: on this range are necessary for proper selection of the thres hold level &gt; 0 . sufficient for asymptotic reliable recovery. In the special case when k is known so that k k also compare it to the scaling law for lasso in Section 4.
 to-noise ratio (SNR) goes to infinity. Specifically, if we defi ne the SNR as then under Assumption 1(e), it can be easily checked that Since x has k nonzero components, k x k 2  X  kx 2 OMP with SNR that remains bounded above is an interesting ope n problem. Note that even if SNR = O ( k  X  ) for any  X  &gt; 0 , Assumption 1(d) will be satisfied. Gaussian noise w .
 OMP method in Algorithm 1 will asymptotically detect the cor rect sparsity pattern in that Moreover, the threshold levels can be selected simply as a function of k recovery. x in (1) by solving the quadratic program structure [10]. However, it is generally believed that lass o has superior performance. Gaussian measurement matrices and high SNR, there is no addi tional performance improvement with the more complex lasso method over OMP. stopping condition.
 if  X   X  ( t ) &gt; for some threshold .
 vides a range k  X  [ k one can make the range [ k condition. We have provided an improved scaling law on the number of meas urements for asymptotic reli-recovery results such as [24 X 27] can be obtained for OMP.
 been observed in [29] and sparse Bayesian learning methods i n [30, 31]. 7.1 Proof Outline Due to space considerations, we only sketch the proof; addit ional details are given in [28]. Hence, this genie algorithm can never select an incorrect in dex j 6 X  I y terminate with correct sparsity pattern estimate I To this end, define the following two probabilities: Both probabilities are implicitly functions of n . The first term, p ergy  X  p rect X  indices j 6 X  I  X  X enie X  algorithm, and therefore recover the sparsity patt ern. This shows that So we need to show that there exists a sequence of thresholds = ( n ) &gt; 0 , such that p p where  X  is from (9). Then, define the threshold level 7.2 Probability of Missed Detection The proof that p This is done by separately considering the components of w in the span of the vectors a I One then follows the Tropp X  X ilbert proof for the noise-free case to show that for large k . Hence, using (9) and (20) one can then show which shows that p 7.3 Probability of False Alarm This part is harder. Define so that  X  Therefore, they are independent of a Gaussian components with variance 1 /m , conditional on P variance 1 /m . Hence, m X  Now, there are k ( n  X  k ) values of  X  proof bounds the maximum of these k ( n  X  k ) value by the standard tail bound let S ( s ) be the normalized Brownian motion We then show that, for every j , there exists times s such that the vector is identically distributed to Hence, yields an improved bound, Combining this with (20) shows which shows that p [1] S. Mallat. A Wavelet Tour of Signal Processing . Academic Press, second edition, 1999. [4] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory , 52(4):1289 X 1306, April
