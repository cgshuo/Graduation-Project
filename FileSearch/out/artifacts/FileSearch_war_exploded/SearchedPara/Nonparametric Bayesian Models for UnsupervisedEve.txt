 were used in the tasks of Topic Detection and Tracking (TDT), as reported in [24]. coreference clusters of event mentions from a collection of unlabeled documents. Our approach [16, 22, 25] and by the advantages of using a large amount of da ta at no cost. One model was inspired by the fully generative Bayesian mode l proposed by Haghighi and Klein (HDP) [28]. The second uses an Infinite Hidden Markov Model (iHMM) [5] coupled to an Infinite Factorial Hidden Markov Model (iFHMM) [30].
 those from the example presented in Section 1, is illustrate d in Figure 1. 2.1 Notation As input for our models, we consider a collection of I documents, each document i having J properties of an event mention, em , as a vector of pairs h ( FT feature value index i ranges in the feature value space associated with a feature t ype. 2.2 Linguistic Features words of the mention ( LHL , RHL ), and lemmas of left and right mentions ( LHE , RHE ). WordNet Features (WF) We build three types of clusters over all the words from WordN et [9] ( with a new id for each of the three cluster types.
 mentions, we use s semantic parser [8] trained on PropBank(P B) [23] and FrameNet(FN) [4] cor-emphasized mentions from our example evoke the ARREST frame from FN.) Feature Combinations (FC) We also explore various combinations of features presented above. Examples include HW + POS , HL + FR , FE + A 1 , etc. these models, we consider Z the set of indicator random variables for indices of events,  X  all random variables that represent observable features.
 this probability is computed by integrating out all model pa rameters: in H&amp;K X  X  paper. 3.1 The One Feature Model The one feature model, HDP global distribution drawn from this DP prior, denoted as  X  specific distribution  X  from document i , Z where HL Franchise (CRF) representation [28]: Here, n already in Z  X  i,j ,  X  z weight for the unknown mixture component.
 concentration  X   X   X  where HL Lidstone X  X  smoothing method to this distribution. 3.2 Adding More Features A model in which observable components are represented only by one feature has the tendency to entity coreference (or topic detection), and therefore it i s desirable to extend the HDP a generalized model where additional features can be easily incorporated. want to incorporate another feature (e.g., F R ) in the previous model, the formula becomes: L feature variables, the inference formula for the Gibbs samp ler is defined as: sentation of a Naive Bayes classifier.
 depicted in Figure 2(d) the posterior probability is given b y: In this model, P ( F R variables considered are X = h HL , POS , FR i . features (e.g., unspecified PB argument). components and the number of features associated with objec ts, we combine this mechanism with an HDP model to form an mIBP-HDP hybrid. Finally, to account f or temporal dependencies, we employ an mIBP extension, called the Infinite Factorial Hidden Markov Model (iFHMM) [30], in combination with an Infinite Hidden Markov Model (iHMM) to form the iFHMM-iHMM model. 4.1 The Markov Indian Buffet Process Markov dynamics. Specifically, if we denote by M the total number of feature chains and by T over a binary matrix F with T rows, which correspond to observations, and an unbounded nu mber of columns ( M  X   X  ), which correspond to features. An observation y F a binary Markov chain associated to a feature f m is defined as and the initial state F m object y out analytically, we use the counting variables c 00 probabilities: accordingly. 4.2 The mIBP-HDP Model HDP inference process. features for y auxiliary variable for y set B get selected in B 4.3 The iFHMM-iHMM Model The iFHMM is a nonparametric Bayesian factor model that exte nds the Factorial Hidden Markov Model (FHMM) [13] by letting the number of parallel Markov chains M be learned from data. the iHMM represents a nonparametric extension of the Hidden Markov Model (HMM) [27] that components (or, in our case, latent events).
 We denote by ( s mentions ( y each mention y the transition probability  X  is defined as  X  according to a likelihood model F that is parameterized by a state-dependent parameter  X  s  X  X  (  X  s The beam sampling algorithm combines the ideas of slice samp ling and dynamic programming for have independent priors [5], Van Gael and colleagues [29] al so used the HDP mechanism to al-employs a forward filtering-backward sampling technique.
 scribed in Section 4.1, and the auxiliary variable u  X  described in [29]: Here, the dependencies involving parameters  X  and  X  are omitted for clarity. In the backward step, we first sample the event for the last sta te s and then, for all t : T  X  1 , 1 , we sample each state s s distribution ( o where n of features for y Event Coreference Data One corpus used for evaluation is ACE 2005 [18]. This corpus a nnotates document event coreference, we created the EventCorefBank corpus (ECB). 8 This new corpus con-45289 and 21175 system mentions respectively.
 MUC F-score, while the baseline listed in Table 2 achieves 72 .9% MUC F-score. HDP Extensions Due to memory limitations, we evaluated the HDP the HDP HDP categories described in Section 2.2. For the results of the H DP explored the conditional dependencies between the HL , FR , and FEA types. As can be observed from Table 2, the results of the HDP increase by 4-10% over the HDP more flexible representation for clustering objects charac terized by rich properties. HDP framework, the mIBP-HDP model did not achieve a satisfac tory performance in comparison compared with the restricted set of features considered by t he HDP model is to consider other distributions for automatic sele ction of salient feature values. iFHMM-iHMM In spite of the automatic feature selection employed for the iFHMM-iHMM model, were hand tuned). As shown in Table 2, most of the iFHMM-iHMM r esults fall in between the HDP better framework than HDP in capturing the event mention dep endencies simulated by the mIBP feature sampling scheme. Similar to the mIBP-HDP model, to a chieve these results, the iFHMM-of the iFHMM-iHMM results reported in Table 2, we set  X   X  =50,  X   X  =0.5, and  X   X  =0.5.
