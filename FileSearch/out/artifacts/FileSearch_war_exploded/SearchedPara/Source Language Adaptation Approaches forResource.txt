 Machine Zone, Inc.
 Qatar Computing Research Institute, HBKU National University of Singapore
Most of the world languages are resource-poor for statistical machine translation; still, many a resource-poor language POOR into a target language TGT by adapting and using a large bitext for a related resource-rich language RICH and the same target language TGT. We assume a small POOR X  X GT bitext from which we learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-rich and the resource-poor language. guideline for people building machine translation systems for resource-poor languages. adapted resource-rich bitext yields 7.26 BLEU points of improvement over the unadapted one and 3.09 BLEU points over the original small bitext. Moreover, combining the small POOR X  X GT bitext with the adapted bitext outperforms the corresponding combinations with the unadapted bitext by 1.93 X 3.25 BLEU points. We also demonstrate the applicability of our approaches to other languages and domains. 1. Introduction
Contemporary statistical machine translation (SMT) systems learn how to translate from large sentence-aligned bilingual corpora of human-generated translations, called bitexts . Unfortunately, collecting sufficiently large, high-quality bitexts is difficult, and thus most of the 6,500+ world languages are resource-poor for SMT. Fortunately, many of these resource-poor languages are related to some resource-rich language, with whom they overlap in vocabulary and share cognates, which offers opportunities for bitext reuse.
 Finnish X  X stonian, Swedish X  X orwegian, Russian X  X krainian, Irish X  X aelic Scottish, Standard German X  X wiss German, Modern Standard Arabic X  X ialectical Arabic (e.g., Gulf, Egyptian), Turkish X  X zerbaijani, and so on.
 resource-rich language to X (e.g., X = English) to improve machine translation from a resource-poor language to X (Nakov and Ng 2009, 2012). Here we take a different, orthogonal approach: We adapt the resource-rich language to get closer to the resource-poor one.
 language S 2 and the same target language T . We use these bitexts to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-poor and resource-rich languages, S 1 and S 2 . We propose three approaches to adapt (the source side of) the large bitext for S 2  X  T : word-level paraphrasing, phrase-level paraphrasing, and text rewriting using a specialized decoder. The first two approaches were proposed in our previous work (Wang, Nakov, and Ng 2012), and the third approach is novel and outperforms the other two in our experiments. in translation quality compared with both training on the unadapted large bitext
S  X  T , and training on the small bitext for the resource-poor language S achieve very sizable improvements when combining the small bitext S large adapted bitext S 0 2  X  T , compared with combining the former with the unadapted bitext S 2  X  T .
 demonstrate the applicability of our approach to another language pair, Bulgarian X  Macedonian, which is also from a different domain.
 of related work. Section 3 introduces our target resource rich X  X oor language pair:
Malay X  X ndonesian. Then, Section 4 presents our three approaches for source language adaptation. Section 5 describes the experimental set-up, after which we present the experimental results and discussions in Section 6. Section 7 contains deeper analysis of the obtained results. Finally, Section 8 concludes and points to possible directions for future work. 2. Related Work
One relevant line of research is on machine translation between closely related languages, which is arguably simpler than general SMT, and thus can be handled using word-for-word translation, manual language-specific rules that take care of the necessary morphological and syntactic transformations, or character-level translation/ transliteration. This has been tried for a number of language pairs including Czech X  2002), Irish X  X cottish Gaelic (Scannell 2006), and Macedonian X  X ulgarian (Nakov and
Tiedemann 2012). In contrast, we have a different objective: We do not carry out full 278 language X ).
 the same language, for example, between Cantonese and Mandarin (Zhang 1998), or ple, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and
Belinkov 2013). Here again, manual rules and/or language-specific tools and resources formal contexts but rather only in informal online media such as social networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from
Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. Sajjad, Darwish, and Belinkov (2013) first normalized a dialectal Egyptian Arabic to look like Modern Standard Arabic, and then translated the transformed text to English.
 SMS messages and Tweets for just any language (Aw et al. 2006; Han and Baldwin 2011;
Wang and Ng 2013; Bojja, Nedunchezhian, and Wang 2015). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language to another. when done specifically for improving SMT into another language. For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian Portuguese (BP) to European Portuguese (EP), which they used to adapt BP X  X nglish bitexts to EP X  X nglish.
They report small improvements in BLEU for EP X  X nglish translation when training on the adapted  X  X P X  X  X nglish bitext compared with using the unadapted BP X  X nglish (38.55 vs. 38.29 BLEU points), or when an EP X  X nglish bitext is used in addition to the adapted/unadapted one (41.07 vs. 40.91 BLEU points). Unlike that work, which heavily independent; moreover, our improvements are much more sizable.
 without or with very little adaptation, which works well for very closely related lan-guages. For example, our previous work (Nakov and Ng 2009, 2012) experimented with various techniques for combining a small bitext for a resource-poor language (Indonesian or Spanish) with a much larger bitext for a related resource-rich language (Malay or Portuguese), pretending that Spanish is resource-poor; the target language of all bitexts was English. However, that work did not attempt language adaptation, except for very simple transliteration for Portuguese X  X panish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay X  X ndonesian, which use unified spelling. Still, once we with the small bitext; thus, in the following we will directly compare and combine these two approaches.
 using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and
Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor  X  rich  X  X ) would require an additional parallel poor X  X ich bitext, which we do not have. Pivoting over the target X (rich  X  X  X  poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich X  X bitext).
 translation bitexts from comparable corpora (Munteanu, Fraser, and Marcu 2004;
Snover, Dorr, and Schwartz 2008). This is orthogonal to our efforts here, as our focus is on adapting resources for a related resource-rich language, rather than directly mining source X  X arget translation pairs from comparable corpora. 3. Malay and Indonesian
Malay and Indonesian are closely related, mutually intelligible Austronesian languages with 180 million speakers combined. They have a unified spelling, with occasional differences, for example, kerana vs. karena ( X  X ecause X ), Inggeris vs. Inggris ( X  X nglish X ), and wang vs. uang ( X  X oney X ).
 Malay typically follows the English pronunciation, whereas Indonesian tends to follow Dutch, for example, televisyen vs. televisi , Julai vs. Juli , and Jordan vs. Yordania . false friends, for example, polisi means policy in Malay but police in Indonesian. There are also many partial cognates, for example, nanti means both will (future tense marker) and later in Malay but only later in Indonesian.
 example, Article 1 of the Universal Declaration of Human Rights: is much higher X  X or example, there is only one word in the Malay text that does not exist in Indonesian: samarata ( X  X qual X ). Other differences are due to the use of different morphological forms, for example, hendaklah vs. hendaknya ( X  X onscience X ), derivational variants of hendak ( X  X ant X ).
 cosine similarity between them based on the Universal Declaration of Human Rights . results are shown in Table 1. We can see that the average similarity between English 280 and { Indonesian, Malay, French, Spanish } is 0.001 X 0.033, whereas for closely related language pairs it ranges from 0.302 to 0.802. Of course, this cosine calculation compares surface word overlap only and does not take minor morphological variants into consid-eration. Yet, this gives an idea of the relative proximity between the languages. native speaker of Indonesian to adapt the Malay version to Indonesian while preserving as many words as possible, and we obtained the following result: word-level operations: Additionally, it requires a phrase-level substitution of bertindak di antara with bergaul . the process of learning when to apply these operations. Thus, in the following we focus our attention on the simplest and most common operation of word/phrase potentially useful operations X  X or example, a correct translation for the Malay samarata can be obtained by splitting it into the Indonesian sequence sama rata .
 that is needed for the following Malay X  X ndonesian sentence pair: 4. Methods
Assuming a resource-rich bitext (Malay X  X nglish) and a resource-poor bitext (Indonesian X  X nglish), we improve statistical machine translation from the resource-poor language (Indonesian) to English by adapting the bitext for the related resource-rich language (Malay) and English to the resource-poor language (Indonesian) and
English. We propose three bitext adaptation approaches: word-level paraphrasing, phrase-level paraphrasing, and text rewriting with a specialized decoder.
 these three adaptation approaches to generate a ranked list of n corresponding adapted  X  X ndonesian X  sentences. Then, we pair each such adapted  X  X ndonesian X  sentence with the English counterpart in the Malay X  X nglish bitext for the Malay sentence it was derived from, thus obtaining a synthetic  X  X ndonesian X  X  X nglish bitext. Finally, we combine this synthetic bitext with the resource-poor Indonesian X  X nglish bitext to train the final Indonesian X  X nglish SMT system, using various bitext combination methods. approach, followed by the phrase-level paraphrasing approach; then, we describe the text rewriting decoder. Finally, we describe the bitext combination methods we experi-ment with. 4.1 Word-Level Paraphrasing
Given a Malay sentence, we generate a confusion network containing multiple Indo-nesian word-level paraphrase options for each Malay word. Each such Indonesian option is associated with a corresponding weight in the network, which is defined as the probability of this option being a translation of the original Malay word, calculated using Equation (1). We decode this confusion network using a large Indonesian lan-guage model, thus generating a ranked list of n corresponding adapted  X  X ndonesian X  sentences.
 options and the corresponding weights for the Malay words. Then, we explain how we build, decode, and improve the confusion network. 4.1.1 Inducing Word-Level Paraphrases. We use pivoting over English to induce potential Indonesian word translations for a given Malay word.
 for the Indonesian X  X nglish bitext using IBM model 4 (Brown et al. 1993), and then we combine them using the intersect+grow heuristic (Och and Ney 2003). We then induce Malay X  X ndonesian word translation pairs assuming that if an Indonesian word i and a Malay word m are aligned to the same English word e , they could be mutual mated by pivoting over English: ments. Following Callison-Burch, Koehn, and Osborne (2006), we further assume that i is conditionally independent of m given e . 282 as a translation option for the Malay word adakah , since the two words are both aligned to the same English word whether in the word alignments for the Indonesian X  X nglish bitext and the Malay X  X nglish bitext, respectively.
Indonesian confusion network, where each Malay word is augmented with a set of alter-natives, represented as network transitions: possible Indonesian word translations. The weight of such a transition is the conditional Indonesian X  X alay translation probability as calculated by Equation (1); the original Malay word is assigned a weight of 1. only those Malay words that we believe not to exist in Indonesian (e.g., because they do not appear in our Indonesian monolingual text). This is necessary because of the large number of false friends and partial cognates between Malay and Indonesian (see Section 3).

Indonesian language model, and we extract an n -best list. For balance, in case of fewer than n adaptations for a Malay sentence, we randomly repeat some of the available ones. Table 2 shows the 10-best adapted  X  X ndonesian X  sentences we generated for the confusion network in Figure 2. According to a native Indonesian speaker, options 1 and 3 in the table are perfect adaptations, options 2 and 5 have a wrong word order, and the rest are grammatical though not perfect. 4.1.3 Further Refinements. Many of our Malay X  X ndonesian paraphrases are bad: Some have very low probabilities, and others involve rare words for which the probability estimates are unreliable. Moreover, the options we propose for a Malay word are inherently restricted to the small Indonesian vocabulary of the Indonesian X  X nglish bitext. We now describe how we address these issues.
 tion (1)) are lower than some threshold (tuned on the development data set), for example, 0.01.
English bitext and one copy of the large Malay X  X nglish bitext, where the value of k is selected so that we have roughly the same number of Indonesian and Malay sentences. Then, we generate word-level alignments for the resulting bitext. Finally, we truncate these alignments keeping them for one copy of the original Indonesian X  X nglish bitext only. Thus, we end up with improved word alignments for the Indonesian X  English bitext, and ultimately with better estimations for Equation (1). Because Malay and Indonesian share many cognates, this improves word alignments for
Indonesian words that occur rarely in the small Indonesian X  X nglish bitext, but are words.

Malay word using morphology. Because the set of Indonesian options for a Malay word 284 in pivoting is restricted to the Indonesian vocabulary of the small Indonesian X  X nglish
Indonesian text, we first build a lexicon of the words in the text. Then, we lemmatize these words using two different lemmatizers: the Malay lemmatizer of Baldwin and
Awab (2006), and a similar Indonesian lemmatizer. These two analyzers have different strengths and weaknesses, therefore we combine their outputs to increase recall.
Next, we group all Indonesian words that share the same lemma, for example, for minum we obtain { diminum, diminumkan, diminumnya, makan-minum, makananminuman, meminum, meminumkan, meminumnya, meminum-minuman, minum, minum-minum, minum-minuman, minuman, minumanku, minumannya, peminum, peminumnya, perminum, terminum } . Because Malay and Indonesian are subject to the same morphological pro-cesses and share many lemmata, we use such groups to propose Indonesian translation options for a Malay word. We first lemmatize the target Malay word, and then we find all groups of Indonesian words the Malay lemma belongs to. The union of these groups is the set of morphological variants that we will add to the confusion network as additional options for the Malay word. Although the different morphological forms typically have different meanings, for example, minum ( X  X rink X ) vs. peminum ( X  X rinker X ), in some cases the forms could have the same translation in English, for example, minum ( X  X rink X , verb) vs. minuman ( X  X rink X , noun). This is our motivation for trying morpholog-ical variants, even though they are almost exclusively derivational, and thus generally quite risky as translational variants. For example, given seperminuman ( X  X rinking X ) in the Malay input, we first find its lemma minum , and then we get the above example set of
Indonesian words, which contains some reasonable substitutes such as minuman ( X  X rink X ).
 which is one minus the minimum edit distance ratio (Ristad and Yianilos 1998) between the Malay word m and the Indonesian word i : where EditDistance( i , m ) is the Levenshtein edit distance between the Indonesian word i and the Malay word m . len ( w ) is the length of a word w (i.e., the number of characters in w ). In the confusion network, the weight of the original Malay word is set to 1. The weight of a morphological option is Score( i , m ) multiplied by the highest probability for all pivoting variants for the Malay word X  X hat is, we trust pivoting options more than morphological options. As an example, assuming a morphological variant with would finally give the morphological one a weight of 0 . 9  X  0 . 8 = 0 . 72 and the pivoting option a weight of 0.8. 4.2 Phrase-Level Paraphrasing
Word-level paraphrasing ignores context when generating Indonesian variants, relying only on the Indonesian language model to make the right contextual choice. This might not be strong enough. Thus, we also try to model context more directly by generating adaptation options at the phrase level . 4.2.1 Inducing Phrase-Level Paraphrases. We use standard phrase-based SMT techniques (Koehn et al. 2007) to build separate phrase tables for the Indonesian X  X nglish and the Malay X  X nglish bitexts. We then pivot over the English phrases to generate Indonesian X 
Malay phrase pairs. As in the case of word-level pivoting, we derive the paraphrase probabilities from the corresponding probabilities in the two phrase tables, again using Equation (1).
 the Malay side of the Malay X  X nglish bitext to get closer to Indonesian. We use mono-the log-linear model on a development set using minimum error rate training (MERT) (Och 2003). 4.2.2 Cross-Lingual Morphological Variants. Although phrase-level paraphrasing models context better, it remains limited in the size of its Indonesian vocabulary by the small
Indonesian X  X nglish bitext, just like word-level paraphrasing. We address this by trans-forming the Indonesian sentences in the development and the test Indonesian X  X nglish bitexts into confusion networks (Dyer 2007; Du, Jiang, and Way 2010), where we add Malay morphological variants for the Indonesian words, weighting them based on
Equation (2). Note that we do not alter the training bitext; we just transform the source side of the development and the test data sets into confusion networks. 4.3 Text Rewriting with a Specialized Decoder
In this section, we introduce a third approach to source language adaptation, which uses a text rewriting decoder to iteratively find the best adaptation for an input sentence. text rewriting decoder we propose. We then introduce the decoding algorithm, the different hypothesis producers, and the feature functions we use for source language adaptation. 4.3.1 Differences from Typical Beam-Search Decoders. Beam-search decoders are widely used in natural language processing applications such as SMT, for example, in the phrase-based Moses decoder (Koehn et al. 2007), and automatic speech recognition (ASR), for example, in the HTK hidden Markov model toolkit (Young et al. 2002). Given an input sentence in the source language, various hypotheses about the output sentence in the target language are generated in a left-to-right fashion.
 following translation options: { ( s 1 , t 2 ), ( s 1 s hypothesis is expanded by adding one more target phrase to the output sentence. This
Hypotheses with the same maps and the same target output are recombined, and those with the same number of translated words are kept in the same beam. For efficiency reasons, beams are limited in size, and thus only the highest scoring hypotheses make incomplete, which means that sentence-level feature functions could not be computed exactly for them, for example, type/token ratio (Hardmeier et al. 2013) feature function that models readability. 286 S:---T: writing decoder works at the sentence-level, that is, all hypotheses are complete sen-tences. This means that we can use truly sentence-level features. We will show an example in Section 7.5.
 same translation options as in the beam decoder example from Figure 3. The search starts from the initial hypothesis, which is then expanded by replacing a source phrase with a target phrase using one phrase pair from the translation options; then, the process continues recursively with each of the new hypotheses. s 4.3.2 Beam-Search Algorithm for Text Rewriting. Given an input sentence, our decoder searches for the best rewriting. It repeats two steps for a number of iterations: ing in the beams the n -best hypotheses (as also implemented in the Moses decoder). Hypotheses with the same number of modifications are grouped in the same beam.
The maximum number of iterations is equal to the number of tokens in the input sentence, that is, we suppose each token needs at most one modification on average. Upon completion, we select the best hypothesis across all beams.
 Algorithm 1 Beam-Search Text Rewriting INPUT: an INPUT sentence of length N
RETURN: the best rewritten form for INPUT 1: initialize hypothesisBeams [0... N ] and hypothesisProducers; 2: add the initial hypothesis INPUT to beam hypothesisBeams [0]; 3: for i  X  0 to N -1 do 4: for each hypo in hypothesisBeams[ i ] do 5: for each producer in hypothesisProducers do 6: for each newHypo produced by producer from hypo do 7: add newHypo to hypothesisBeams [ i +1]; 8: prune hypothesisBeams [ i +1]; 9: return the best hypothesis in hypothesisBeams [0... N ]; 4.3.3 Hypothesis Producers. Hypothesis producers generate new hypotheses by modify-ing existing ones. We use three types of hypothesis producers: 288
In principle, we can also use some rule-based hypothesis producers to adapt Malay to Indonesian. For example, the number format of Malay is different from that of
Indonesian: Malay numbers are written in accordance with the British convention, that is,  X . X  is the decimal point and  X , X  denotes digit grouping, whereas in Indonesian, producer to convert Malay numbers to Indonesian ones, for example, which would convert the hypothesis KDNK Malaysia dijangka cecah 8.1 peratus pada tahun 2010. to
KDNK Malaysia dijangka cecah 8,1 peratus pada tahun 2010. However, such a rule-based hypothesis producer would be language-specific. In the present work, we have chosen to stick to statistical hypothesis producers only in order to keep our decoder as language-independent as possible. This makes it potentially applicable to many closely related language pairs, which we will demonstrate in Section 7.4. 4.3.4 Feature Functions. The text rewriting decoder assesses the quality of a hypothesis based on a log-linear model and a number of feature functions, which can be grouped into two general types.
 modifications that a given hypothesis producer has made. They allow the decoder to distinguish good hypothesis producers from bad ones. More precisely, if the decoder weight in order to let it perform more modifications.
 4.3.5 Model. We use a log-linear model, which combines all features to obtain the score for a hypothesis h as follows: where f i is the i th feature function with weight  X  i .
 the best hypothesis as the one with the highest score ( h ) across all beams. ranking optimization or PRO (Hopkins and May 2011). We optimize BLEU+1 (Liang et al. 2006), a sentence-level approximation of BLEU, as is standard with PRO. 4.4 Combining Bitexts
We have presented our source language adaptation approaches in Sections 4.1, 4.2, and 4.3. Now we explain how we combine the Indonesian X  X nglish bitext with the synthetic  X  X ndonesian X  X  X nglish bitext we have generated. We consider the following three bitext combination approaches: simply train an SMT system on their concatenation.
 has exactly n very similar variants for each Malay sentence. Moreover, the original
Malay X  X nglish bitext is much larger than the Indonesian X  X nglish one and now it has further expanded n times to become  X  X ndonesian X  X  X nglish, which means it will heavily dominate the concatenation. To counter balance this, we repeat the smaller Indonesian X 
English bitext enough times to make its number of sentences roughly the same as for  X  X ndonesian X  X  X nglish; then we concatenate them and train an SMT system on the resulting bitext.
 for combining phrase tables proposed in Nakov and Ng (2009, 2012). The first phrase table is extracted from word alignments for the balanced concatenation with repetitions, which are then truncated so that they are kept for only one copy of the Indonesian X 
English bitext. The second table is built from the simple concatenation. The two tables are then merged as follows: All phrase pairs from the first one are retained, and to them are added those phrase pairs from the second one that are not present in the first one.
Each phrase pair retains its original scores, which are further augmented with 1 X 3 extra 290 feature scores indicating its origin: The first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise. We experiment using all three, the first two, or the first feature only; we also try setting the features to 0.5 instead of 0. This makes six combinations (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use the one that achieves the highest BLEU score on the development set.
 coding paths (Birch, Osborne, and Koehn 2007), simple linear interpolation, and direct merging with extra features (Callison-Burch, Koehn, and Osborne 2006); they were previously found inferior to the last two approaches above (Nakov and Ng 2009, 2012). 5. Experiments
With a small Indonesian X  X nglish bitext and a larger Malay X  X nglish bitext, we use three approaches for source language adaptation to adapt the Malay side of the Malay X 
English bitext to look like Indonesian, thus obtaining a synthetic  X  X ndonesian X  X  X nglish bitext. With the synthetic bitext, we run two kinds of experiments: and the same Indonesian X  X nglish test set for evaluation; see below. 5.1 Data Sets In our experiments, we use the following data sets, which are required for Indonesian X 
English SMT:
Note that the monolingual sentences of EN-LM were all collected in the same manner and from the same domains as the other three bilingual texts, in order to reduce the impact of domain mismatch.
 monolingual Indonesian text for building an Indonesian language model: Section 4.1.3, we use a large monolingual Indonesian corpus, IN-LM , in order to induce
Indonesian morphological variants for a Malay word. We built all these monolingual and bilingual data sets from texts we crawled from the Internet.
 phrase-based SMT decoder in the phrase-level paraphrasing approach of Section 4.2.1, and our source language adaptation decoder of Section 4.3. We created this bitext synthetically: We translated the English side of the IN2EN-dev into Malay using Google Translate, 3 and we paired this translated Malay with the Indonesian side of
IN2EN-dev : 5.2 Baseline Systems
We built five baseline systems  X  two using a single bitext, ML2EN or IN2EN , and three combining ML2EN and IN2EN , using simple concatenation, balanced concatenation, and sophisticated phrase table combination. The last combination is a very strong baseline and the most relevant one that we need to improve upon.
 alignments using IBM model 4 (Brown et al. 1993) for both directions, and we combined them using the intersect+grow heuristic (Och and Ney 2003). Based on these alignments, build a phrase table, where each phrase pair has five features (Koehn 2013): forward tion probabilities, and a phrase penalty. We further used a 5-gram language model trained using the SRILM toolkit (Stolcke 2002) with modified Kneser-Ney smoothing (Kneser and Ney 1995). We combined all features in a log-linear model, namely: (1) the five features in the phrase table, (2) a language model score, (3) a word penalty, that is, the number of words in the output translation, and (4) distance-based reordering cost.
 on the development set IN2EN-dev using MERT (Och 2003), and we used them for translation with the phrase-based SMT decoder of Moses.
 292 5.3 Isolated Experiments In the isolated experiments, we train the SMT system on the adapted  X  X ndonesian X  X 
English bitext only, which allows for a direct comparison to using ML2EN or IN2EN only. 5.3.1 Using Word-Level Paraphrases. In our word-level paraphrasing experiments, we adapted Malay to Indonesian using three kinds of confusion networks (CN) (see
Section 4.1.3 for details): (1) the minimum pivoting probability threshold for the Malay X  X ndonesian word-level paraphrases, and (2) the number of n -best Indonesian-adapted sentences that are to threshold and { 1, 5, 10 } for n . 5.3.2 Using Phrase-Level Paraphrases. In our phrase-level paraphrasing experiments, we used pivoted phrase tables (PPT) with the following features for each phrase table entry (in addition to the phrase penalty; see Section 4.2 for more details): phrase-level paraphrasing systems on ML2IN-dev . 5.3.3 Using a Text Rewriting Decoder. For our text rewriting decoder (DD), we con-ducted four experiments with different hypothesis producers (see Section 4.3.3 for more details):
Section 5.3.1 on IN2EN-dev . For the last two (phrase-based) experiments, we only needed to tune the second parameter of the two. We tried the same values for ML2IN-dev .
 this performed about the same as the phrase-level mapping hypothesis producer alone.
This may be because the two mappings are extracted from the word alignments of the same Malay X  X nglish and Indonesian X  X nglish bitexts by pivoting. Thus, we can expect that the phrase-level mapping already contains most, if not all, of the word-level mapping. 5.4 Combined Experiments
These experiments assess the impact of our source language adapted bitext when combined with the original Indonesian X  X nglish bitext IN2EN , as opposed to combining
ML2EN with IN2EN as was in the last three baselines above. We experimented with the same three combinations: (1) simple concatenation, (2) balanced concatenation, and (3) sophisticated phrase table combination. We tuned the parameters as before; for the last combination, we further had to include in the tuning the extra phrase table features (see Section 4.4 for details). 6. Results and Discussion sign test, over the baseline are in bold ; in case of two baselines, we use underline for the second baseline. 6.1 Baseline Experiments on ML2EN instead of IN2EN yields over 4 points absolute drop in BLEU (Papineni 294 et al. 2002) score, even though ML2EN is about 10 times larger than IN2EN and both bitexts are from the same domain. This confirms the existence of important differences between Malay and Indonesian. Simple concatenation does not help, but balanced concatenation with repetitions improves by 1.12 BLEU points over IN2EN , which shows the importance of giving IN2EN a proper weight in the combined bitext. This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points. 6.2 Isolated Experiments paraphrasing ( CN:* ) improves by up to 5.56 and 1.39 BLEU points over the two baselines (both results are statistically significant). Compared with ML2EN , CN:word yields an absolute improvement of 4.41 BLEU points, CN:word CN:word 0 +morph adds 0.56 more. The scores for TER (v. 0.7.25) (Snover et al. 2006) and METEOR (v. 1.3) (Banerjee and Lavie 2005) are on par with those for BLEU (NIST v. 13). indicates that they are robust to noise, probably because bad source-side phrases are unlikely to match the test-time input. Note also the effect of repetitions: Good word choices are shared by many n -best sentences, and thus have higher probability. by vocabulary differences between Malay and Indonesian. Compared with IN2EN , all CN:* models have higher 2/3/4-gram precision. However, CN:word has lower uni-gram precision, which could be due to bad word alignments, as the results for CN:word show.
 by almost 1 BLEU point over CN:word 0 . This shows the importance of morphology for overcoming the limitations of the small Indonesian vocabulary of the IN2EN bitext. related languages like Malay and Indonesian, which are rich in false friends and partial cognates.
 the Indonesian vocabulary with cross-lingual morphological variants is still helpful, though not as much as at the word-level.
 better: It further increases the improvements up to 6.57 and 2.40 BLEU points absolutely over the two baselines (statistically significant).

MEMT (Heafield and Lavie 2010) yields even further gains, which shows that the three approaches are somewhat complementary. The best BLEU score for our isolated experiments is 21.76, which is already better than all five baselines in Table 3, including the three bitext combination baselines, which only achieve up to 20.10. 6.3 Combined Experiments
Table 5 shows the performance of the three bitext combination strategies (see Section 4.4 for details) when applied to combine IN2EN with the original ML2EN (i), and with various adapted versions of ML2EN (ii X  X v).
 tions except CN:word perform significantly better than their corresponding baselines, but the improvements are most sizeable for simple concatenation. Note that whereas there is a difference of 0.31 BLEU points between the balanced concatenation and the versions. This is probably due to the sophisticated combination assuming that the second bitext is worse than the first one, which is not really the case for the adapted versions: As Table 4 shows, they all outperform IN2EN .
 paraphrasing, and they are both outperformed by the text rewriting decoder ( DD:* ).
Finally, system combination with MEMT yields even further gains. These results are consistent with those for the isolated experiments. 7. Further Analysis
In this section, we perform a more in-depth analysis of the obtained results. 296 7.1 Paraphrasing Non-Indonesian Words Only
In the CN:* experiments, we paraphrased each word in the Malay input. This was motivated by the existence of false friends such as polisi and of partial cognates such as nanti . However, doing so also risks proposing worse alternatives, for example, changing beliau ( X  X e X , respectful) to ia ( X  X e X , casual), which the weights on the confusion network edges and the language model would not always handle properly. Thus, we tried paraphrasing non-Indonesian words only, that is, those not in IN-LM . Because IN-LM occasionally contains some Malay-specific words, we also tried paraphrasing words that occur at most t times in IN-LM . Table 6 shows that this can yield a loss of up to 1 BLEU point for t = 0; 10, and a bit less for t = 20; 40. 7.2 Manual Evaluation
We asked a native Indonesian speaker who does not speak Malay to judge whether our  X  X ndonesian X  adaptations are more understandable to him than the original Malay input for 100 random sentences. We used two extremes: the conservative CN:word,t =0 for it. Table 7 shows that CN:word,t =0 is better/equal to the original 53%/31% of the better:worse ratio is 45%:43%. Still, this latter model works better, which means that phrase-based SMT systems are robust to noise and prefer more variety rather than better translations in the training bitext. That is, humans usually like high precision, whereas what the downstream SMT system really needs should be high recall. Note also that the judgments were at the sentence level, although phrases are sub-sentential, that is, there can be many good phrases to be extracted from a  X  X ad X  sentence. For example,
CN:word 0 +morph adapted perisian navigasi kereta 3D di pasaran Malaysia menjelang akhir tahun ( X 3D car navigation software hits Malaysia by year-end X ) to the following three versions (changes are underlined ):
All three converted manjelang ( X  X y X ) to pada ( X  X t X ), which is not needed, as manjelang is also an Indonesian word. Our human translator did not like the first two versions, but liked the last one better, compared to the original Malay sentence. The first two versions did not adapt perisian ( X  X oftware X ) correctly, but all three successfully adapted kereta to mobil ( X  X ar X ), and also pasaran to pasar ( X  X arket X ), which would encourage good phrase pairs in the phrase table extracted from the adapted bitext.
 298 7.3 Reversed Adaptation In all these experiments, we were adapting the Malay sentences to look like Indonesian. Here we try to reverse the direction of adaptation, that is, to adapt Indonesian to Malay:
We thus built an Indonesian-to-Malay confusion network for each dev/test Indonesian sentence using word-level paraphrases extracted with the method of Section 4.1.1. We then use the confusion network as an input to a Malay X  X nglish SMT system trained on the ML2EN data set. We tried two variations of this idea: because lattice encodes many options, but does not use a Malay language model, and 1-best uses a Malay language model, but has to commit to 1-best. In contrast, CN:word uses both n -best outputs and an Indonesian language model. Designing a similar in future work, since the two reversed adaptation approaches have some advantages over the three adaptation approaches proposed in Section 4; for example, the reversed approaches could be more efficient. 7.4 Adapting Bulgarian to Macedonian to Help Macedonian X  X nglish Translation
In order to show the applicability of our framework to other closely related languages and other domains, we experimented with Macedonian ( MK ) and Bulgarian ( BG ), using data from a different, non-newswire domain: the OPUS corpus of movie subtitles (Tiedemann 2009). We used data sets of sizes that are comparable to those in the previous Malay X  X ndonesian experiments: 160K MK2EN and 1.5M BG2EN sentence pairs (1.2M and 11.5M English words). Because the sentences of movie subtitles were short, we used 10K MK2EN sentence pairs for tuning and testing (77K and 72K English words), respectively. For language modeling, we used 9.2M Macedonian and 433M English words.
 balanced concatenation with unadapted BG2EN . Moreover, system combination with
MEMT improves even further. This indicates that our approach can work for other pairs of closely related languages and even for other domains.

Indonesian adaptation. This may be because our monolingual Macedonian data set is much smaller than the monolingual Indonesian data set (10M Macedonian vs. 20M
Indonesian words). Also, our monolingual Macedonian data set is too noisy, because it contains many optical character recognition errors, typos, concatenated words, and even some Bulgarian text. Moreover, Macedonian and Bulgarian are arguably some-what more dissimilar than Malay and Indonesian, as can be seen in Table 1. 7.5 Improving the Readability of the Adapted Bitext
Motivated by Hardmeier et al. (2013), we also experimented with two sentence-level fea-tures that aim to improve the readability of the source side of the adapted  X  X ndonesian X  X 
English bitext. The two features are type/token ratio (TTR) and word variation index (OVIX) (Stymne et al. 2013). The latter is a reformulation of TTR that is less sensitive to sentence length. The definitions of TTR and OVIX are shown in Equations (4) and (5), respectively, where Count ( tokens ) is the number of tokens, and Count ( types ) is the number of word types.
 300
DD:phrase4+morph ) in Table 4. The results are shown in Table 10, where we can see that the two features yield slightly lower BLEU scores, which is similar to what Hardmeier may result in a lower BLEU score, as simple texts would likely not match complicated reference translations, especially if the reference translations were not produced with high readability in mind in the first place. 7.6 Our Text Rewriting Decoder vs. Phrase-Level Paraphrasing
The results of our experiments show that phrase-level paraphrasing outperformed word-level paraphrasing, and they were both outperformed by the text rewriting decoder. Here, we discuss the differences between our text rewriting decoder and using phrase-level paraphrasing with a standard SMT phrase-based decoder like Moses: a wide space of feature functions and hypothesis producers, and allows us to easily test many different ideas. Furthermore, because the original input sentence could be itself a valid hypothesis, the structure of evaluating rewrites is a natural fit to our problem. 8. Conclusion and Future Work
We have presented work on improving machine translation for a resource-poor lan-guage by making use of resources for a related resource-rich language. This is an important line of research because most world languages remain resource-poor for resource-rich language(s). We have proposed three approaches, which all adapt a bitext for a related resource-rich language to get closer to the resource-poor one: (1) word-level paraphrasing using confusion networks, (2) phrase-level paraphrasing using pivoted phrase tables, and (3) adaptation using a specialized text rewriting decoder. a small POOR  X  TGT bitext for a related resource-poor language, we use one of the three proposed approaches to adapt the RICH side of the RICH  X  TGT bitext to get closer to
POOR , thus obtaining a synthetic  X  POOR  X  X  TGT bitext, which we then combine with the original POOR  X  TGT bitext to improve the translation from POOR to TGT . bitext for the resource-poor Indonesian X  X nglish language pair, and adapting the former baselines: (1) +7.26 BLEU points over an unadapted version of the Malay X  X nglish bitext, (2) +3.09 BLEU points over the Indonesian X  X nglish bitext, and (3) 1.93 X 3.25 BLEU points over three bitext combinations of the Malay X  X nglish and Indonesian X  X nglish bitexts. We thus have shown the potential of the idea that source-language adaptation of a resource-rich bitext can improve machine translation for a related resource-poor language. Moreover, we have demonstrated the applicability of the general approach to other languages and domains.
 because it can provide a useful guideline for people building statistical machine trans-lation systems for resource-poor languages. They can adapt bitexts for related resource-rich languages to the resource-poor language, and thus subsequently improve the resource-poor language translation using the adapted bitexts.
 302 Acknowledgments References 304
