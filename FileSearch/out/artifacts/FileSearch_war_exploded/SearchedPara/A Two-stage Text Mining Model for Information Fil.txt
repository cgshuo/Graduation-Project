 Mismatch and overload are the two fundamental issues re-garding the effectiveness of information filtering. Both term-based and pattern (phrase) based approaches have been em-ployed to address these issues. However, they all suffer from some limitations with regard to effectiveness. This paper proposes a novel solution that includes two stages: an ini-tial topic filtering stage followed by a stage involving pat-tern taxonomy mining. The objective of the first stage is to address mismatch by quickly filtering out probable irrel-evant documents. The threshold used in the first stage is motivated theoretically. The objective of the second stage is to address overload by apply pattern mining techniques to rationalize the data relevance of the reduced document set after the first stage. Substantial experiments on RCV1 show that the proposed solution achieves encouraging per-formance.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms Information Filtering, Text Mining, Decision Rules, Thresh-olds, Weighting Schema
An Information Filtering (IF) [2] system monitors an in-coming document stream to find the documents that match information needs of users. There are two fundamental is-sues regarding the effectiveness of information filtering: mis-match and overload. Mismatch means some useful or inter-esting information has been omitted (loss of recall), whereas, overload means some filtered information is not relevant (loss of precision). The representation of the user infor-mation need is variously refereed to as user profile, or topic profile. As the quality of the profiles directly influences the quality of information filtering, the issue of how to built ac-curate, reliable profiles is a crucial concern [17]. IF is usually subdivided into  X  X ocument routing X  or  X  X daptive filtering X . In the former, the profile remains constant and the task is to match an incoming stream of documents to a set of pro-files. In the latter, the profile is adapted dynamically in the presence of feedback. This paper is concerned with routing.
Most profiles are term based, such as Rocchio and proba-bilistic models [1], rough set models [11], BM25 and SVM [19] based filtering models. The advantage of term-based profiles is efficient computational performance as well as mature the-ories for term weighting, which have emerged over the last couple of decades from the IR and machine learning com-munities. However, term-based profiles suffer from the prob-lems of polysemy and synonymy. In addition, the threshold for accepting or rejecting documents is usually determined empirically. As IF systems are sensitive to data sets, it is a challenge to set the optimal threshold.

Over the years, IR has often held the hypothesis that phrase-based approaches should perform better than the term-based ones, as phrases are more discriminative and arguably carry more  X  X emantics X . This hypothesis has not fared too well in the history of IR [9, 23, 24]. Although phrases are less ambiguous and more discriminative than individual terms, the likely reasons for the discouraging per-formance include: ( i ) phrases have inferior statistical prop-erties to terms, ( ii ) they have low frequency of occurrence, and ( iii ) there are large numbers of redundant and noisy phrases among them [24].

In the presence of these setbacks, sequential patterns used in data mining community have turned out to be a promising alternative to phrases [27, 5] because patterns enjoy good statistical properties like terms. To overcome the disadvan-tages of phrase-based approaches, pattern-based approaches (or pattern taxonomy models (PTM) [26, 27]) have been proposed for IF from within the data mining community. These pattern based approaches have shown encouraging improvements on effectiveness, but at the expense of compu-tational efficiency. In regard to the aforementioned problem of redundancy and noise, PTM adopts the concept of closed patterns, or pruned non-closed patterns. However, it is a still challenging issue for PTM to deal with low frequency patterns because the measures used from data mining (e.g.,  X  X upport X  and  X  X onfidences X ) to learn the profile turn out be not suitable in the filtering stage. By way of illustration, given a specified topic, a highly frequent pattern (normally a short pattern with large support) is usually a general pat-tern, or a specific pattern of low frequency. This parallels the situation in term indexing where words of high frequency (stop words) or very low frequency (highly information bear-ing uncommon words) are not considered useful.

In this paper, a two-stage model is proposed to address the limitations of term-based approaches and pattern based approaches. The first stage is called the  X  X opic filtering X , and the second stage is called the  X  X attern taxonomy min-ing X . The objective of the topic filtering stage is to solve the problem of mismatch, which tries to quickly filter out the most likely irrelevant information based on term-based profiles. The intention after the first stage is that only a relatively small amount of potentially highly relevant docu-ments remain as input to the second stage. The objective of the second stage is to solve the problem of overload by us-ing a pattern taxonomy mining approach. It aims to assign large weight to the most likely relevant documents by ex-ploiting patterns based in the pattern taxonomy. This stage is a precision oriented, and as only a relatively small amount of documents are involved, the previously mentioned com-putational cost can be markedly reduced. In addition, the setting of the threshold is theoretically, rather than empiri-cally determined.

The remainder of the paper is organized as follows. Sec-tion 2 highlights previous research in related areas and com-pares this research to that presented here. Section 3 intro-duces the existing approaches for setting thresholds theoret-ically. It also briefly discusses the new method for setting thresholds for the first stage. The proposed two-stage IF model which integrates topic filtering and pattern taxonomy mining is thereafter illustrated in Section 4 and Section 5. The empirical results are reported in Section 6. Section 7 de-scribes the findings of the experiments and discusses the re-sults. Concluding remarks and future research are sketched in Section 8.
In this section, relevant literature is reviewed from the ar-eas of IF and text mining. IF systems were originally consid-ered to have the same function as IR systems did. Different from IR systems, IF systems were commonly personalized to support long-term information needs of users [2]. The main distinction between IR and IF was that IR systems used  X  X ueries X  but IF systems used  X  X ser profiles X .
At first, IF systems used artificial intelligence based tech-niques such as rules to generate profiles. Machine-learning based techniques subsequently arrived on the scene. Based on user feedback, machine-learning based systems (e.g., [29]) tended to learn a map f : D  X  R such that f ( d ) corre-sponded to the relevance of a document d , where D denoted the set of incoming documents, R was the set of real num-bers.

To decrease the burden of on-line learning, a decompo-sition model for IF was presented in [17]. In this model, the map f was replaced by f 1  X  f 2 , where f 1 ( f 1 : D  X  { C respectively; and C 1 , C 2 , . . . , C m were clusters. This model tried to represent user profiles using a set of clusters based on a kind of classification method, e.g., the neural network [16].
Usually, users do not know or do not like to use classi-fication structures for their information needs. They just simply want IF systems to return relevant or interesting in-formation. Rough set based decision models were developed for this purpose. Such models represent profiles using three clusters: positive region, boundary region and negative re-gion [11] [14]. Even though rough set based models have emerged from the field of data mining, their principle objec-tive is very closed aligned to the aim of the filtering track in TREC [19]. This track measures the ability of IF systems to build profiles using sets of training documents to separate relevant and non-relevant documents. The basic term-based IF models used in TREC 2002 were SVM, Rocchio X  X  algo-rithm, probabilistic models, and BM25.

The tasks of the filtering track included adaptive filtering, and batch and routing filtering. Adaptive filtering involves feedback to dynamically adapt IF systems [8] [30]. In this paper, the focus is on batch filtering or  X  X outing X .
Term-based IF models have been developed recently which take into consideration more constraints in relation to the labeled data in training sets. For instance, Rocchio-style classifiers [10]; ranking SVM [18]; and BM25 for structured documents [21]. However, the research on term-based IF models has arguably hit somewhat of a wall in terms of per-formance improvement possibly due to the ambiguity prob-lem mentioned earlier.

Many text mining methods have been developed in order to achieve the goal of information filtering or Web person-alization [15, 3, 4]. Typically, text mining discusses asso-ciations between terms at a broad spectrum level, paying little heed to duplications of terms, and labeled information in the training set [13]. Usually, the existing data mining techniques return numerous discovered patterns (e.g., sets of terms) from a training set. Not surprisingly, among these patterns, there are many redundant patterns [28].
To overcome the disadvantages of using phrases, sequen-tial patterns and closed patterns have been developed in pattern taxonomy models (PTM) [26, 27]. The following comparisons drawn from the literature place PTM and IF systems in perspective: ( i ) PTM methods are more com-putationally intensive to train; ( ii ) sequential patterns are more effective than normal patterns; ( iii ) closed sequen-tial patterns are better than frequent patterns; and ( iv ) too much noise in the input data (incoming document stream) adversely affects PTM systems.

The main objective of the research work presented in this paper is to develop a novel IF model which integrates topic filtering and pattern taxonomy mining strategies to pro-vide more precise document filtering. The idea of inte-grating term-based approaches (topic filtering) and pattern-based approaches (pattern taxonomy mining) for IF systems has evolved from these two well established, but largely disparate fields. This proposed method intends to exploit the advantages of term-based approaches (IR) and pattern-based based approaches (data mining) within the one sys-tem.
As mentioned previously, the first stage aims to quickly filter out likely non-relevant documents. As a result only a relatively small amount of potentially relevant documents remain as input into the second stage. The objective of the second stage is to employs a pattern taxonomy mining approach to boost precision. As the mining is performed on a relatively small set of documents, the computational cost is markedly reduced.

The basic hypothesis for the first stage is that incoming documents are likely non-relevant if they are not closed to the feature descriptions of positive documents in the train-ing set. As mentioned in the introduction, thresholds for IF systems are usually determined empirically, because IF sys-tems are sensitive to data sets. In this section, we introduce two existing approaches for determining thresholds theoreti-cally by describing the features of positive documents in the training set.

Let D be a training set of documents, which consists of a set of positive documents, D + ; and a set of negative doc-uments, D  X  . Let T = { t 1 , t 2 , . . . , t m } be a set of terms (or keywords) which are extracted from the set of positive documents, D + .
 Given a training set, usually an IF model (e.g., Rocchio, BM25 or SVM) attempts to find feature descriptions for both positive documents and negative documents in the training set. It then uses a technique to weaken the common feature descriptions of the positive documents and negative documents. For example, the Rocchio algorithm [22], which has been widely adopted in information retrieval, which can build text representation of a training set using a Centroid ~c as follows: where  X  and  X  are empirical parameters; and ~ d denotes a document.

If Eq. 1 is used to describe the feature of only positive documents in the training set, the equation can be simplified as (notice:  X  can be ignored here because it is a common factor when we ignore negative documents). This kind of feature description denotes that incoming documents are likely rele-vant if they include all terms that are selected in the positive documents.

The obvious advantage of using the above approach is that the IF system can have a high precision; however, the recall is very low. The interesting and hard question is how to find suitable subsets of terms (or conditions) for deciding thresholds for IF systems.

For this question, a rough-set based theory [32] (also see [33]) has been used for IF systems in [11], and the theory has been further developed for setting thresholds to determine relevant information in [12, 13]. The theory tried to de-scribe the possible boundary between relevance documents and non-relevance documents rather than to determine the suitable subsets of terms.

The rough-set based theory firstly classified incoming doc-uments into three groups: the positive region, boundary re-gion and negative region. It also provided associated de-cision rules for the partitioning of the incoming document stream into the three regions. This approach differs from many of the state-of-the-art methods which tend to set thresh-olds empirically.

Let d i  X  D + be a positive document and d be an incoming document. The basic assumption for the rough-set based approach is that d is possibly relevant if d  X  d i . The set of all document d such that d i  X  d is called the covering set of d .

The union of all covering sets of all d i  X  D + is called the positive region ( POS ) of incoming documents. The set of all documents d such that  X  d i  X  D +  X  d i  X  d 6 =  X  is called the boundary region ( BND ). Also, the set of all documents d such that  X  d i  X  D +  X  d i  X  d =  X  is called the negative region ( NEG ).

Given a document d , the decision rules can be determined ideally as follows:
The theory also recommended a method to theoretically determine thresholds for finding relevant documents if there is a probability function pr for terms in T , where T is a set of terms that occur in positive documents, but do not occur in the negative documents. For example, in [12], this function was defined as for all terms t  X  T .

Theorem. The probability function pr on T has the fol-lowing property: for all d  X  P OS .

This theorem suggested to use as a threshold for determining relevant documents because all documents d  X  POS can obtain a larger weight than the minimum weight of positive documents in the training set, that is, the method is complete in determining relevant documents.

To compare with the two approaches, rough-set based ap-proach can largely increase the recall of IF systems; however, the precision can be very low because the threshold would be very small if the training set includes many positive doc-uments. In this paper, we will present a method to balance the two approaches in the next section. The new method tries to describe the common theme (features) of all positive documents in the training set.
To improve the robustness of IF systems, in the topic fil-tering stage, we do not use the feature descriptions for nega-tive documents because the coverage of negative documents can be very large, therefore it is impossible to describe their common features. In this paper, The feature descriptions are employed for only positive documents in the training set in order to identify potentially relevant documents for the second stage.

In contrast to many term based information filtering sys-tems, topic filtering tries to filter out the likely negative (non-relevant) documents. In this section, we first discuss how to represent positive documents in term weight distri-butions. We also describe the method for deciding suitable thresholds for filtering out likely non-relevant documents for the second stage. In addition, we present topic filtering al-gorithms in this section.
A set of terms is referred to as a termset . Given a positive document d i and a term t , tf ( d i , t ) is defined as the number of occurrences of t in d i . A set of term frequency pairs is referred to as an initial r-pattern (rough pattern) in this paper.

Let termset ( p ) = { t | ( t, f )  X  p } be the termset of p . In this paper, r-pattern p 1 equals to r-pattern p 2 if and only if termset ( p 1 ) = termset ( p 2 ). A r-pattern is uniquely de-termined by its termset . Two initial r-patterns can be com-posed if they have the same termset . In this paper, we use the composition operation,  X  , that defined in [13] to com-pose r-patterns. For example, (Notice:  X  is also suitable for patterns with different termsets, e.g., { ( t 1 , 2) , ( t 2 , 5) } X  X  ( t 1 , 1) } = { ( t 1
Based on the above definitions, we can obtain a set of composed r-patterns in D + , RP = { p 1 , p 2 , . . . , p r  X  n , and n = | D + | is the number of positive documents in D . The support of a r-pattern p i is the fraction of the initial r-patterns that are composed to form p i .
Table 1 shows a set of positive documents in a training set, where T = { t 1 , t 2 , . . . , t 7 } , and the numbers are term frequencies in the corresponding documents. We also can view the documents in Table 1 as initial r-patterns.
Table 2 illustrates the discovered r-patterns and their sup-ports by using the composition operation to the initial r-patterns in Table 1, where p 1 = d 1 , p 2 = d 2 , p 3 = d and p 4 = d 5  X  d 6 for all discovered r-patterns.
Up to now, the positive documents in the training set have been represented as r-patterns. In the topic filtering stage, discovered r-patterns are employed to filter out most irrele-vant documents rather than to identify relevant documents.
Formally the relationship between r-patterns and terms can be described as the following association mapping if we consider term frequencies: such that where p i  X  RP is a r-pattern; and if we assume p i = { ( t 1 , f 1 ) , ( t 2 , f 2 ) , . . . , ( t
We call  X  ( p i ) the normal form of r-pattern p i in this pa-per. The association mapping  X  can derive a probability function for the weight distribution of terms on T in order to show the importance of terms in the positive documents, which satisfies: for all t  X  T .

Based on the above discussion, a positive document d i can be described as an event that represents what users want with the probability value For the first stage, the focus is on  X  X emoving X  the  X  X oises X  (irrelevant documents). The basic assumption is that docu-ment d is irrelevant if it is not closed to the common feature of the positive documents in the training set. Therefore, the threshold can be determined as follows: where, m is the mean of the probabilities of positive docu-ments in D + , m = 1 n P d coefficient;  X  is the standard deviation of the probabilities of positive documents and skew is the skewness of the probabilities
An efficient training procedure for calculating the derived probability function pr  X  in topic filtering is described in Al-gorithm TF1T.

In order to improve efficiency, composition operations are not actually used in Algorithm TF1T. All initial r-patterns Algorithm T F 1 T ( D + , T ) Input: the set of positive documents D + and terms T . Output: a probability function pr  X  and RP .
 Method: (1) RP =  X  ; (2) for (document d  X  D + ) { (3) for (term t  X  T ) pr  X  ( t ) = 0; (4) for (r-pattern p  X  RP ) Algorithm T F 1 F ( D + , T , pr  X  , prob , RP ,  X  ) Input: Positive documents D + , terms T , pr  X  , prob , Output: a set of retained (possible relevant) documents rel . Method: (2) threshold = m +  X  (  X  + skew ); (3) rel =  X  ; RP are collected in steps (1) and (2). The probability distri-bution over T is then initialized to zero in step (3). Finally, each initial r-pattern is normalized and the probability val-ues are accumulated in step (4). We have verified experi-mentally that the probability function thus approximated is very close to the theoretically motivated probability function just described in Theorem 2. The time complexity of Algo-rithm TF1T in the training phase is O ( nmq ), since it only needs a single traversal through positive documents, where q is the average size of documents; n = | D + | and m = | T | .
Algorithm TF1F describes the filtering process of the first stage of the filtering process using the threshold from The-orem 2. Furthermore, a relevance value for each document in the testing set is assigned, where  X  ( t, d ) = 1 if t  X  d ; otherwise  X  ( t, d ) = 0.

The time complexity of Algorithm TF1F in the testing phase is O ( nm ) + O ( mqu ) = O ( m ( n + qu )) = O ( mqu ) since it only needs a traversal through each incoming document, where q is the average size of testing documents; u is the size of the testing set, and usually n &lt; u .

Based on the above analysis, we believe that both algo-rithms for the topic filtering stage are efficient. After the topic filtering task has been carried out, the most irrelevant documents have been removed from test set. The second stage is to process the remaining documents using a pattern taxonomy. In the second stage, all the documents remaining after the first stage are split in paragraphs. So a given document d i yields a set of paragraphs DP .
Given a termset X in document d , we use p X q to denote the covering set of X , which includes all paragraphes dp  X  DP such that X  X  dp , i.e., p X q = { dp | dp  X  DP, X  X  dp } . Its support is the fraction of the paragraphs that contain pattern if its support  X  min sup , a minimum support.
Table 3 lists a set of paragraphs for a given document, where DP = { dp 1 , dp 2 , . . . , dp 6 } , and duplicate terms re-moved. Let min sup = 50% giving rise to the ten frequent patterns of Table 3. Table 4 illustrates these frequent pat-terns and their covering sets.

Not all frequent patterns in Table 4 are useful. For ex-ample, pattern { t 3 , t 4 } always occurs with term t 6 in para-graphs. Therefore, we believe that the shorter one { t 3 , t is a noise pattern and expect to keep the larger pattern { t 3 , t 4 , t 6 } only.

Given a termset X , its covering set p X q is a subset of paragraphs. Similarly, given a set of paragraphs Y  X  DP , we can define its termset , which satisfies The closure of X is defined as follows: A frequent pattern X (also a termset) is called closed if and only if X = Cls ( X ).

Based on the above definitions, there are only the three closed patterns given in Table 4. They are { t 3 , t 4 , t and { t 6 } .
Patterns can be structured into a taxonomy by using the is a (or X  X ubset X ) relation. Figure 1 illustrated an example of the pattern taxonomy for the frequent patterns in Table 3 and Table 4, where the nodes represent frequent patterns and their covering sets; non-closed patterns can be pruned; the edges are X  X s-a X  X elation, e.g., pattern { t 6 } would become a direct sub-pattern of { t 3 , t 4 , t 6 } after pruning non-closed patterns.

For the above consideration, we first need to evaluate a term X  X  support and then calculate a specificity value for each pattern. The evaluation of term supports (weights) is dif-ferent to the normal term-based approaches. In the term based approaches, a component of a given term X  X  weighting is based on its appearance in documents. In the following, terms are weighted according to their appearance in discov-ered patterns.

Formally, for all positive document d i  X  D + , we first de-ploy its closed patterns on a common set of terms T in order to obtain the following r-patterns: where t i j in pair ( t i j , n i j ) denotes a single term and n its support in d i which is the number of closed patterns that contain t i j .

These r-patterns are composed by using an association mapping (see Eq. 2), and then supports of the terms in D + can be calculated in Algorithm PTM2.
The concept of frequent and closed pattern is also suit-able for sequential patterns. A sequential pattern s = &lt; t , . . . , t r &gt; ( t i  X  T ) is an ordered list of terms. A sequence s 1 = &lt; x 1 , . . . , x i &gt; is a sub-sequence of another sequence s 2 = &lt; y 1 , . . . , y j &gt; , denoted by s 1  X  s 2 , iff  X  j such that 1  X  j 1 &lt; j 2 . . . &lt; j y  X  j and x 1 = y y , . . . , x i = y j y .

To improve the efficiency of the mining of the pattern taxonomy, an algorithm, SP M ining , was proposed in [27] to find all closed sequential patterns, which used the well known Apriori property in order to reduce the searching space.

Algorithm PTM2 describes the training process of pattern taxonomy mining. For every positive document, the SPMin-ing algorithm is first called in step (2) giving rise to a set of closed sequential patterns SP . Additionally, all discov-ered patterns in a positive document are composed into an r-pattern giving rise to a set of r-patterns RP in step (2). Algorithm PTM2 ( D + , min sup ) Input: D + ; minimum support, min sup .
 Output: a set of r-patterns RP , and supports of terms. Method: (1) RP =  X  ; (2) for (document d  X  D + ) { (3) T = { t | ( t, w )  X  p, p  X  RP } ; (4) for (r-pattern p  X  RP ) Thereafter in step (3) an (4), the support is calculated for all terms that appear in the r-patterns, where the normal forms  X  ( p ) for all r-pattern p  X  RP is used.

After the support of terms have been computed from the training set, a given pattern X  X  specificity to the given topic can be defined as follows: It is also easy to verify spe ( p 1 )  X  spe ( p 2 ) if p 1 pattern of pattern p 2 . This property shows that a docu-ment should be assigned a large weight if it contains large patterns. Based on this observation, we will assign the fol-lowing weight to a document d for ranking documents in the second stage:
In this section, we first discuss the data collection used for our experiments. We also describe the baseline models and their implementation. In addition, we present the ex-perimental results for comparing the proposed model with the baseline models and the pattern mining model.
Reuters Corpus Volume 1 (RCV1) was used to test the ef-fectiveness of the proposed model. RCV1 corpus consists of all and only English language stories produced by Reuter X  X  journalists between August 20, 1996, and August 19, 1997 with total 806,791 documents. The document collection is divided into training and test sets. The training set con-sists of all documents with dates up to and including 30 September 1996. The testing set consists of all remaining documents.

TREC (2002) has developed and provided 100 topics for the filtering track aiming at building a robust filtering sys-tem. The topics are of two types: 1) A first set of 50 topics are developed by the assessors of the National Institute of Standards and Technology (NIST)(i.e., assessor topics); The relevance judgements have been made by assessor of NIST. 2) A second set of 50 topics have been constructed artifi-cially from intersections of pairs of Reuters categories (i.e., intersection topics) [25]; Difference from the assessor topics, the relevance judgements have been made by machine learn-ing method not by human being for intersection topics. The assessor topics are more reliable and the quality of the in-tersection topics is not quite good. The detail comparatione of these two types topic can be found in [19, 25]. For this reason, all the topics are divided into the first 50 (assessor topics) and the second 50 topics (intersection topics) set.
RCV1 collection is marked in XML. To avoid bias in ex-periments, all of the meta-data information in the collection have been ignored. The documents are treated as plain text documents by preprocessing the documents. The tasks of re-moving stop-words according to a given stop-words list and stemming term by applying the Porter Stemming algorithm are conducted.
Four baseline models are used: the classic Rocchio model, a BM25 based IF model, a SVM based model, and PTM model. In this paper, our new model is called Two-Stage Model (T-SM).

The Rocchio algorithm [22] has been widely adopted in the areas of text categorization and information filtering. It can be used to build the profile for representing the concept of a topic which consists of a set of relevant (positive) and irrelevant (negative) documents. The Centroid ~c of a topic can be generated by using Eq. 1, where we set  X  = 1.0 and  X  = 0.0. in this paper.

BM25 [6, 7, 20] is the one of sate-of-the-art retrieval func-tions used in document retrieval. The term weights are es-timated using the following BM25 based equation:
W ( t ) = tf  X  ( k 1 + 1) where N is the total number of documents in the training set; R is the number of positive documents in the training set; n is the number of documents which contain term t ; r is the number of positive documents which contain term t ; tf is the term frequency; DL and AVDL are the document length and average document length, respectively; and k 1 and b are the experimental parameters (the values of k 1 and b are set as 1.2 and 0.75, respectively, in this paper).
Information filtering can also be regarded as a special in-stance of text classification [24]. SVM is a statistical method that can be used to find a hyperplane that best separates two classes. SVM achieved the best performance on the Reuters-21578 data collection for document classification [31]. The decision function in SVM is defined as: where x is the input object; b &lt; is a threshold and w = P i =1 y i  X  i x i for the given training data: ( x i , y i where x i  X  &lt; n and y i equals +1 (  X  1), if document x beled positive (negative).  X  i &lt; is the weight of the training example x i and satisfies the following constraints: To compare with other baseline models, we tried to use SVM to rank documents rather than to make binary deci-sions. For this purpose, threshold b can be ignored. We also believe that the positive documents in the training set should have the same importance to user information needs because the training set was only simply divided into rele-vant documents and non-reagent documents. So we assign the same  X  i value (e.g., 1) to each positive document first, and then determine the same  X  i (e.g.,  X   X  1 ) value to each negative document based on Eq. 5. Therefore, we use the following weighting function to estimate the similarity be-tween a testing document and a given topic: where  X  means inner product ; d is the term vector of the testing document; and
PTM model is also selected as one of the baselines models because we want to verify that the  X  X oises X  are removed in the topic filtering stage and then This leads to PTM model achieve a significant improvement of performance in the rela-tive X  X lean X  X ncoming document steam. The two stage model (T-SM) uses the 150 terms in the first stage. To keep a suffi-cient amount of documents in the testing set for each topic, we set the experimental coefficient  X  = 0 . 25 (see Eq 4). In the second stage, min sup = 0 . 2, and the size of the term set is 4000.

For each topic, we also choose 150 terms in the positive documents based on if*idf values for all baseline models. Effectiveness was measured by four different means: The F-beta ( F  X  ) measure, Mean Average Precision (MAP), the break-even point ( b/p ), and 11-points measure.
 F  X  is calculated by the following function: The parameter  X  = 1 is used in our study, which means that recall and precision is weighed equally. Mean Average pre-cision is calculated by measuring precision at each relevant document first, and averaging precision over all topics. The b/p break-even point indicates the value at which precision equals recall. The larger a b/p or MAP or F  X  -measure score is, the better the system performs. 11-points measure is used to compare the performance of different systems by averag-ing precisions at 11 standard recall levels (i.e., recall=0.0, 0.1, ..., 1.0).

Statistical method is also used to analyze the experimental results. The t-test assesses whether the means of two groups are statistically different from each other. The paired two-tailed t-test is used in this paper. If DIF represents the difference between observations, the hypotheses are: Ho : DIF = 0 (the difference between the two observations is 0). Ha : DIF 6 = 0 (the difference is not 0). N is the sample size of group. The test statistic is t with N  X  1 degrees of freedom ( df ). If the p -value associated with t is low ( &lt; 0.05), there is evidence to reject the null hypothesis. Thus, there is evidence that the difference in means across the paired ob-servations is significant. The T-SM model is compared with Table 5: Results of PTM and T-SM on 100 top-ics, where % means the percentage change in per-formance PTM, Rocchio, BM25, and SVM models for each variable b/p , M AP , F  X  =1 over all 100 topics, respectively.
In this section, T-SM is compared with PTM model. The results for the IR/IF standard measure methods and paired t-test over all 100 topics are shown in the tables 5 and 6, respectively.

As shown in Table 5, the performance of T-SM model is extremely better than PTM model, and in Table 6, the P values are less than 0.0001, by conventional criteria, this dif-ference is considered to be extremely statistically significant. Therefore, we conclude that the topic filtering stage is very useful for removing the  X  X oises X  in the incoming documents.
In this section, T-SM is compared with term-based base-line models including Rocchio, BM25, and SVM. For the set of terms, T-SM first chooses terms from positive documents. It also removed terms that occur in negative documents, and uses the top 150 terms. There is a little bit improvement to remove terms that occur in negative documents for T-SM model.

The experimental results of term based filtering models are reported in this section. For the IR/IF standard measure methods, the results illustrated in tables 7, and 8, are results on the first 50 topics, and the second 50 topics, respectively.
Table 9 illustrates the results for all 100 topics. The re-sults of 11-points on all 100 topics are reported in Figure 2. For the paired t-test results are reported in the Table 10, Table 11, and Table 12.

As shown in the tables 7, 8, 9 and the Figure 2, the proposed new model (T-SM) has achieved the best perfor-Table 7: Results on assessor topics, where % chg means the percentage change over the best term-based model Table 8: Results on intersection topics, where % chg means the percentage change over the best term-based model Table 9: Results on all 100 topics, where % chg means the percentage change over the best term-based model
Figure 2: Results of 11-points on all 100 topics mance results for both assessor and intersection topics. The improvements are consistent and very significant on the all above measures.

As shown in tables 10, 11, 12, the P value in all these three tables are less than 0.0001. The improvements are considered to be extremely statistically significant.
Previously it was mentioned that pattern taxonomy min-ing is sensitive to the noisy data. In response to this, the theory was put forward to set the threshold for the first stage of filtering. The goal is to produce a relatively small set of mostly relevant documents as input into the second stage filter based on pattern taxonomy mining.

It is very difficult to determine a suitable threshold the-oretically for identifying relevant information because the two fundamental issues. In this paper, however, we argue that it can be possible to determine a suitable threshold for identifying likely irrelevant information. The experimental results strongly support this hypothesis.

The question to be addressed is how well the theoreti-cally motivated threshold performed in practice. Post hoc Table 10: Paired t test results between Rocchio model and T-SM Table 11: Paired t test results between BM25 model and T-SM analysis depicted in Table 13 reveals that the first stage topic filter removed, on average, 79% of the documents and 82% of these were indeed irrelevant. In the remaining docu-ments intended for the second stage, the average percentage change of positive documents is +71.4% (increase); and the average percentage change of negative documents is -19.0% (decrease). On the whole, the threshold seems to have de-livered enough relevant documents for an effective pattern taxonomy to be mined.

Compared with term based approaches, patterns appear to capture more  X  X emantic X  information which, we specu-late, is the reason for the boost in performance. In the ex-periments, 150 terms were used for the term-based models because larger numbers of terms degraded performance. In contrast, 4000 terms were used for the pattern mining in the second stage.

Because patterns have low frequency of occurrence, we have to use a small min sup in order to find interesting pat-terns. The consequence is that some noise terms and their combinations (patterns) are also retained and that make some negative documents obtain large weights in pattern mining model. This interesting finding is also verified by the experiments. Table 14 illustrates the statistical results. T-SM divides testing documents into two sets: the set of re-tained documents ( rel ) and the set of filtered out documents. The latter includes 50.77 positive documents in average; and 134.3 documents X  weights in pattern mining are greater than or equal to the smallest document weight in rel . If we do not use topic filtering firstly, pattern mining method will move the 134.3 documents into the rel set, that is, it will make 83.53 (134.3-50.77) errors for deciding relevant documents. The significant improvement of the pattern mining model is mainly due to the success of noise information removal by the topic filtering stage.
 In short, the experimental results provide evidence that Table 12: Paired t test results between SVM model and T-SM Table 13: Statistical results for 100 topics, where % chg p and % chg n denote the average percentage changes of positive or negative documents after fil-tering; % f u means the average percentage of fil-tered out documents; and % n f iltered means the av-erage percentage of negative documents in the fil-tered out documents. After filtering +71.4 -19.0 79 82 Table 14: Statistical results of 100 topics for the interesting finding, where min w is the minimum of document weights in rel , retained testing docu-ments; N w  X  min w is the number of filtered out doc-uments which weighs  X  min w ; N P OS is the number of positive documents that are filtered out. Avg. 3.003312527 134.3 50.77  X  83.53 the proposed two stages model (including topic filtering and pattern mining) can significantly improve the effectiveness of IF systems.
This paper illustrates a new model which integrates topic filtering and pattern taxonomy mining together to alleviate information overload and mismatch problems. The proposed method has been evaluated using the standard TREC rout-ing framework with encouraging results.
 Compared with PTM model (pattern based model) and Rocchio, BM25, and SVM based models (term based mod-els), the results of experiments on RCV1 collection demon-strate that the performance of information filtering can be significantly improved by the proposed new model (T-SM). The significant improvement is mainly due to the superior threshold is applied to the topic filtering stage, and the  X  X e-mantic X  nature of patterns is helpful in the second stage. This research provides a promising methodology for using patterns in information filtering.
This paper was partially supported by Grant DP0556455 from the Australian Research Council. The authors also wish to thank Dr. Sheng-Tang Wu for providing the testing environment for PTM model. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] N. J. Belkin and W. B. Croft. Information filtering [3] R. Feldman and H. Hirsh. Mining associations in text [4] J. D. Holt and S. M. Chung. Multipass algorithms for [5] N. Jindal and B. Liu. Identifying comparative [6] K. S. Jones, S. Walker, and S. E. Robertson. A [7] K. S. Jones, S. Walker, and S. E. Robertson. A [8] R. Y. K. Lau, P. Bruza, and D. Song. Belief revision [9] D. D. Lewis. An evaluation of phrasal and clustered [10] X. Li and B. Liu. Learning to classify texts using [11] Y. Li, C. Zhang, and J. R. Swan. An information [12] Y. Li and N. Zhong. Web mining model and its [13] Y. Li and N. Zhong. Mining ontology for [14] Y. Li and Y. Y. Yao. User profile model: a view from [15] B. Mobasher, H. Dai, T. Luo, and M. Nakagawa. [16] J. Mostafa and W. Lam. Automatic classification [17] J. Mostafa, S. Mukhopadhyay, W. Lam, and M. J. [18] T. Qin, X.-D. Zhang, D.-S. Wang, T.-Y. Liu, W. Lai, [19] S. E. Robertson and I. Soboroff. The trec 2002 [20] S. E. Robertson, S. Walker, and M. Hancock-Beaulieu. [21] S. E. Robertson, H. Zaragoza, and M. J. Taylor. [22] J. Rocchio. Relevance feedback in information [23] S. Scott and S. Matwin. Feature engineering for text [24] F. Sebastiani. Machine learning in automated text [25] I. Soboroff and S. Robertson. Building a filtering test [26] S.T.Wu, Y.Li, Y. Xu, B. Pham, and P.Chen.
 [27] S.-T. Wu, Y. Li, and Y. Xu. Deploying approaches for [28] Y. Xu and Y. Li. Generating concise association rules. [29] T. W. Yan and H. Garcia-Molina. Sift -a tool for [30] Y. Yang, A. Lad, N. Lao, A. Harpale, B. Kisiel, and [31] Y. Yang and X. Liu. A re-examination of text [32] Y. Y. Yao and S. K. M. Wong. A decision theoretic [33] Y. Y. Yao. Probabilistic rough set approximations
