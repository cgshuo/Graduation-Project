 Traditional co-clustering methods identify block structures from static data matrices. However, the data matrices in many applications are dynamic; that is, they evolve smoothly over time. Consequently, the hidden block structures em-bedded into the matrices are also expected to vary smoothly along the temporal dimension. It is therefore desirable to en-courage smoothness between the block structures identified from temporally adjacent data matrices. In this paper, we propose an evolutionary co-clustering formulation for iden-tifying co-cluster structures from time-varying data. The proposed formulation encourages smoothness between tem-porally adjacent blocks by employing the fused Lasso type of regularization. Our formulation is very flexible and allows for imposing smoothness constraints over only one dimen-sion of the data matrices, thereby enabling its applicability to a large variety of settings. The optimization problem for the proposed formulation is non-convex, non-smooth, and non-separable. We develop an iterative procedure to com-pute the solution. Each step of the iterative procedure in-volves a convex, but non-smooth and non-separable prob-lem. We propose to solve this problem in its dual form, which is convex and smooth. This leads to a simple gradi-ent descent algorithm for computing the dual optimal solu-tion. We evaluate the proposed formulation using the Allen Developing Mouse Brain Atlas data. Results show that our formulation consistently outperforms methods without the temporal smoothness constraints.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms Sparsity learning, evolutionary co-clustering, optimization, bioinformatics, neuroscience
Clustering is one of the major techniques for unsupervised discovery of hidden structures from complex data sets. Com-mon clustering methods include the K -means algorithm, the spectral methods, and the hierarchical clustering techniques. These approaches treat the two dimensions of the data ma-trix as instances and features, respectively, and group the instances into clusters based on all the features. When the data matrix is re-ordered according to the clustering results, the re-ordered matrix usually assumes a banded structure along the instance dimension, since instances in the same cluster are assumed to be similar, while those in different cluster are assumed to be dissimilar.

The classical clustering paradigm assumes that all the fea-tures are equally relevant to all instances. In many appli-cations, certain group of instances are only similar to each other with respect to a subset of features. That is, the hid-den structure of the data matrix can be more accurately described by a  X  X heckerboard X  structure in which a subset of the rows and a subset of the columns form a block. Co-clustering, also known as bi-clustering, aims at identifying the block structures of the data matrix by clustering the rows and columns of the data matrix simultaneously into co-clusters [17, 6, 9, 10, 28]. Currently, co-clustering finds applications in many areas, including biological data analy-sis [25, 20], text mining [10, 9], and social studies [14].
As a class of powerful methods for unsupervised pattern mining, existing co-clustering methods invariably assume that the data matrices are static; that is, they do not evolve over time. However, in many real-world domains, the pro-cesses that generated the data are time-evolving. Hence, the observed data are usually dynamic. As a consequence, the block structures embedded into the time-varying data should also evolve smoothly over time. Therefore, it is de-sirable to incorporate the temporal smoothness constraint into the co-clustering formalism.

In this paper, we propose an evolutionary co-clustering formulation for identifying co-clusters from time-varying data. The proposed formulation employs sparsity-inducing regu-larization [29] to identify block structures from the time-varying data matrices. Meanwhile, it applies fused Lasso type of regularization [30] to encourage temporal smooth-ness over the block structures identified from contiguous time points. The proposed formulation is very flexible and can be applied to encourage temporal smoothness over either one or both dimensions of the data matrices. The optimiza-tion problem for the proposed formulation is non-convex, non-smooth, and non-separable. We propose an iterative procedure to compute the solution, and each of the iterative step involves a convex, but non-smooth and non-separable problem. To enable efficient optimization, we derive the dual form of this problem and employ a gradient descent algorithm to solve the smooth dual problem. We evaluate the proposed formulation using the Allen Developing Mouse Brain Atlas data [22, 19]. Results show that the proposed method consistently outperforms other methods by identify-ing blocks that are consistent with classical neuroanatomy.
The rest of this paper is organized as follows: We intro-duce the sparse singular value decomposition method for co-clustering in Section 2. In Section 3, we describe the pro-posed evolutionary co-clustering formulation. We discuss related work in Section 4 and report the experimental eval-uation in Section 5. This paper concludes with conclusions and future work in Section 6.
 Notations : We use boldface lower-case letters, e.g., u ,to denote vectors and upper-case letters, e.g., A , to denote ma-trices. e n denotes a vector of all ones of length n .Fora vector u ,its 1 -norm, defined as the summation of the ab-solute values of its components, is denoted as u 1 .For amatrix A , its Frobenius norm is denoted as A F . denotes component-wise multiplication, and  X  denotes the Kronecker product. The soft-thresholding operator T  X  ,act-ing on a vector x , is defined component-wise as:
The problem of co-clustering is closely related to the sin-gular value decomposition (SVD) of the data matrices [9, 36, 21]. In [9, 36], the spectral clustering formalism is ex-tended to derive a spectral formulation for co-clustering. In these spectral co-clustering formulations, the data are pro-jected onto the left and the right singular vector spaces be-fore they are concatenated and clustered to identify the co-clusters. Motivated by the relationship between SVD and co-clustering, a sparse SVD formulation is proposed in [21] for co-clustering. Formally, let B  X  R m  X  n beadatamatrix. The first singular value and the corresponding left and right singular vectors of B can be computed as where s  X  R is the first singular value, and p  X  R m and q  X  R n are the corresponding left and right singular vectors, respectively, and  X  F denotes the matrix Frobenius norm. It is well known that the matrix s pq T is the optimal rank one approximation to the matrix B [12]. Note that p and q lie in the row space and column space, respectively, of In addition, the singular vectors p and q are usually not sparse; that is, most of their components are nonzero.
Motivated by the optimal rank one approximation prop-erty of the SVD, a sparse SVD formulation is proposed in [21]. Furthermore, it is shown that this sparse SVD formu-lation can be employed for solving co-clustering problems. Specifically, the following sparsity-inducing formulation is involved in sparse SVD: where  X  denotes the vector 1 -norm, and  X  and  X  are the regularization parameters. It is well known that the 1 -norm regularization on p and q encourages sparse solutions [29]. Thus, when  X  and  X  are set to large values, many entries of p and q will be set of zero. The regularization parameters  X  and  X  control the tradeoff between the quality of the rank one approximation and the sparsity of p and q , respectively.
It is shown in [21] that the sparse SVD formulation can be readily employed to solve co-clustering problems. Specif-ically, the rows and columns of B corresponding to nonzero entries of p and q , respectively, can be naturally interpreted to form a co-cluster. If multiple co-clusters are desired, sub-sequent co-clusters can be identified by removing the rank one approximation from the data matrix and solving the optimization problem in Eq. (3) using the residual matrix. It is shown that this sparse SVD method outperforms prior co-clustering methods by identifying distinctive gene expres-sion profiles corresponding to various pathological conditions from a microarray gene expression data set.

The optimization problem in Eq. (3) is non-convex and non-smooth. An iterative procedure has been developed in [21] to compute the solution. In this procedure, one of the vector variables is fixed while the other one is optimized, and this process is alternated between the two vector variables until it converges to a locally optimal solution. Specifically, when p is fixed, q can be computed by solving where  X  q = s q .After  X  q is obtained, we have s =  X  q and q =  X  q /s . Similarly, when q is fixed, the following problem is involved: and p =  X  p /s where s =  X  p . It can be shown that the problems in Eqs. (4) and (5) are convex and can be solved analytically.
 The objective function in Eq. (4) can be written as Taking the subdifferential of Eq. (6) with respect to  X  q ,we have where SGN(  X  ) is defined component-wise as Note that the subdifferential of a function is a set, and when the function is differentiable, the set is a singleton containing the derivative [27]. It follows from the optimality condition for unconstrained problems [27] that  X  q  X  is an optimal solu-tion to Eq. (4) if and only if 0  X   X  X  (  X  q  X  ). Hence, it can be Time point 1 Time point 2 Time point t Figure 1: Illustration of the evolutionary co-clustering problem. easily verified that the optimal  X  q  X  is given by Similarly, the optimal  X  p  X  for the optimization problem in Eq. (5) is given by The iterative procedure in [21] applies Eqs. (9) and (10) alternately until a locally optimal solution is reached.
In the traditional co-clustering framework [17, 6, 9, 20, 28, 25], we assume that the data matrix is time-invariant; that is, it does not evolve along the temporal dimension. In many application domains, each data matrix is usually asso-ciated with a particular time point, and it evolves smoothly over time as shown in Figure 1. For example, in the devel-oping mouse brain gene expression analysis, the spatial gene expression patterns at a particular developing time point is captured by a data matrix in which one dimension corre-sponds to the genes and the other dimension corresponds to the spatial locations. Since gene regulation acts sequen-tially, the expression patterns usually evolves smoothly over time, thereby resulting a series of time-stamped data matri-ces, one for each sampled developing time point. A simple approach for mining these time-evolving data matrices is to treat the data matrices at different time points separately. This approach, however, ignores the time-dependent nature of the underlying process, thereby yielding results that are not amenable to domain interpretation. In this paper, we propose an evolutionary co-clustering formulation for uncov-ering patterns from time-evolving data matrices. The pro-posed formulation encourages smooth changes in the row and/or column patterns over time, thereby capturing the time-evolving nature of the underlying process faithfully. The proposed framework is very flexible and can be applied to applications in which only one dimension of the data ma-trices evolves.

Given a set of time-evolving data matrices A i  X  R m  X  n for i =1 ,  X  X  X  ,t ,where t is the number of sampled time points, we are interested in identifying block structures from each of the data matrices. A simple approach is to compute the sparse SVD for each data matrix separately, leading to the following optimization problem: where u i  X  R m and v i  X  R n are associated with the rows and columns, respectively, of A i ,and s i is the corresponding singular value. However, this approach decouples the data matrices for contiguous time points and ignores the temporal evolving nature of the underlying process that generated the data matrices.
To incorporate the temporal smoothness constraints into the co-clustering framework, we propose the following sparsity-inducing evolutionary co-clustering formulation: min where  X  and  X  and tunable parameters. In this formula-tion, the last two regularization terms are fused Lasso type of regularization [30], and they encourage the u i and v i contiguous time points to be similar. Specifically, these reg-ularization terms encourage the differences of contiguous u and v i to be zero, thus enforcing many entries of contiguous u i and v i to be identical. These fused Lasso type of reg-ularization naturally incorporates the time-evolving nature of the data matrices by encouraging the block structures for contiguous time points to be similar. Note that we can also encourage only the rows or the columns of the block structures to be similar by setting either  X  or  X  to zero.
The objective function in Eq. (11) can be expressed equiv-alently as where  X  u =( s  X  e m ) u , s =[ s 1 ,s 2 ,  X  X  X  ,s t ] T ,  X  v =( s u =[ u T 1 , u T 2 ,  X  X  X  , u T t ] T  X  R mt , v =[ v T 1 ,
The objective function in Eq. (11) is non-convex and non-smooth. In addition, the fused Lasso regularization terms are non-separable [33, 13]. We propose an iterative proce-dure to compute u and v . Specifically, we optimize u by fixing v and then optimize v by fixing u . This iterative process is repeated until convergence. In the following, we discuss the detailed procedure of computing v when u are fixed. The other case can be derived in a similar way. Specif-ically, when u are fixed,  X  v can be computed by solving the following optimization problem: min The objective function in Eq. (14) is convex, but non-smooth and non-separable. In the following, we develop an efficient algorithm to compute the optimal  X  v  X  .
A central challenge for solving the optimization problem in Eq. (14) is the 1 -norm and the fused Lasso regulariza-tion terms, which are non-smooth and non-separable. A key property that leads to an efficient algorithm to this problem is that the 1 -norm term and the fused Lasso term can be solved sequentially in two steps, giving rise to a two-step procedure. This result is originally given in [13, 24] and is summarized in the following theorem:
Theorem 3.1. Define Then for any  X , X   X  0 , we have
Proof. We consider the case when  X  =0: where  X  X   X   X  (  X  v )=  X  v  X  A T u +  X  SGN(  X  v )+  X F T SGN( F  X  X   X  (  X  v )=  X  v  X  A T u +  X F T SGN( F  X  v ) , (20) where It follows from the optimality condition for unconstrained problems [27] that Hence, there exists such that  X  0  X  ( u )= A T u  X  F T y  X  . Define where sgn( x ) is defined component-wise as (sgn( x )) i =1if x &gt; 0, (sgn( x )) i =  X  1if x i &lt; 0, and 0 otherwise. It can be verified that a  X  A T u + b + F T y  X  = 0 and b  X   X  SGN( a ). It follows from the definition of a ,thefactthateachrowof F consisting of two nonzero elements 1 and  X  1, and Eq. (22) that y  X   X   X  SGN( F a ). Therefore, we have This completes the proof of this theorem.

Theorem 3.1 shows that we can solve the optimization problem in two sequential steps. Specifically, we can first solve the problem in Eq. (14) with  X  = 0 to obtain the intermediate solution  X  0  X  ( u ). Then the final optimal solution  X  ( u ) can be obtained by applying the soft thresholding operator to the intermediate solution as in Eq. (16). We now discuss how the  X  = 0 case can be solved efficiently in its dual form.
A key to the two-step procedure in Section 3.2 is to solve the optimization problem in Eq. (17), which can be rewritten in its full form as We propose to solve this problem in its dual form. To this end, we introduce the dual variable and obtain the following equivalent min-max problem: min The existence of the saddle point to this min-max problem is guaranteed by the Von Neumann Lemma [26], because  X  (  X  ,  X  is differentiable, convex in  X  v , and concave in w . Exchanging the min and max and setting the derivative of  X  (  X  v , w )with respect to  X  v to zero, we obtain Substituting Eq. (25) into Eq. (24), we obtain the following dual problem: that we have changed max to min in Eq. (26) by negating the objective function for ease of presentation. The dual formulation in Eq. (26) is convex and smooth. Hence, it can be solved by gradient descent algorithms.
The dual problem in Eq. (26) is a constrained quadratic program (QP) and can be solved by general QP solvers. However, direct application of general QP solvers would ig-nore the special structure of this problem, incurring exces-sive computational cost. In this paper, we propose to solve this dual formulation by a gradient descent algorithm, since the objective function is differentiable. Note that the Hes-sian of  X  ( w )isa n ( t  X  1)  X  n ( t  X  1) matrix and can be ex-press as Since F is a full rank matrix, the Hessian matrix FF T is positive definite. Thus a unique solution exists for the opti-mization problem in Eq. (26).

In this gradient descent algorithm, we have the following iterative update in each iteration: where g k =  X  ( w k )= FF T w k  X  FA T u is the gradient of the objective function at w k ,  X  max is the largest eigenvalue of the Hessian matrix FF T ,and is the projection onto the feasible region. It follows from the analysis in [27] that this algorithm has a linear convergence rate as where w 0 is the initial starting point, and  X  min denotes the smallest eigenvalue of the Hessian matrix. This algorithm can also be accelerated by the Nesterov X  X  method [27].
The gradient descent algorithm is an iterative procedure, and thus a criterion is required to assess the convergence of the algorithm. Following [24], we define a duality gap for the min-max problem in Eq. (24) and derive a simple equation for computing the duality gap in each iteration. We use this duality gap as the stopping criterion in our experiments, and the gradient descent algorithm returns when the duality gap is smaller than 10  X  8 .

Let  X  w be an appropriate solution computed by the gra-dient descent algorithm. Note that  X  w  X   X   X  ,asithas been projected onto the feasible region in each step. Let  X  v = A T u  X  F T  X  w be the corresponding solution for the pri-mal formulation. We can define the duality gap for Eq. (24) at (  X  v ,  X  w )as The following results show that the duality gap in Eq. (31) is an upper bound for the errors in both the primal and the dual formulations. In addition, it can be computed easily by a simple equation.

Theorem 3.2. The duality gap defined in Eq. (31) can be computed as In addition, we have the following results: The proof of this theorem is similar to that of Theorem 3 in [24] and is thus omitted.
The regularization parameter  X  controls the temporal smooth-ness over v i .Thatis,when  X  is larger than a certain value  X  max , v i and v i +1 ,for i =1 , 2 ,  X  X  X  ,t  X  1, will be enforced to be identical. We show that such a  X  max can be computed via solving a linear system of equations. To this end, we need to state the optimality condition for the problem in Eq. (26).
It follows from the optimality condition for constrained problems [27] that w  X  ( w  X   X   X   X  ) is a minimizer of Eq. (26) if and only if This is the well-known variational inequality, and it gives the optimality condition for constrained optimization problems.
Basedontheaboveresult,weshowthat  X  max can be com-puted via solving a linear system of equations with a special structure.

Theorem 3.3. Let  X  w denote the unique solution of the linear system and let Then for any  X   X   X  max , we have  X  v i =  X  v j ,  X  i, j
Proof. Since the Hessian of  X  (  X  ) is positive definite, the linear system in Eq. (36) has a unique solution  X  w . For any  X   X   X  max , it can be easily verified that  X  w  X  =  X  max  X   X  and  X  (  X  w )= FF T  X  w  X  FA T u = 0 . It follows from the opti-mality condition in Eq. (35) that  X  w is the optimal solution to Eq. (26) when  X   X   X  max . In addition, when  X   X   X  max ,we have  X   X  ( u )= A T u  X  F T  X  w from Eq. (25). It follows that Therefore, we have  X  v i =  X  v j ,  X  i, j .
 The value of  X  max can be used to guide the selection of an appropriate value for  X  in practice. We evaluate the effec-tiveness of  X  in the experiments and observe that the best performance is achieved when  X  =  X  max on the biological data sets.
Simultaneous row and column clustering for identifying block structures from matrix data has been initially studied in [17]. Recent surge of interests in co-clustering is moti-vated by biological applications, which aim at identifying subset of genes co-expressed in a subset of samples from microarray gene expression data [6]. Co-clustering has also been applied in many other applications, including simulta-neous clustering of words and documents [10, 9], authors and conference [32], etc. Early work on co-clustering focuses on defining an error measure and then identifying blocks that minimize this measure using heuristic search algorithms [17, 6]. These early work has recently been reformulated us-ing matrix and optimization techniques [8]. Following the spectral clustering formalism, it has been shown recently that co-clustering is closely related to the singular value de-composition (SVD) of the data matrix [4]. In [9, 36], co-clustering is formulated as a bipartite graph cut problem, and the data are projected onto the left and right singular vector spaces before they are concatenated and clustered to identify row and column co-clusters. It is shown in [21] that sparsity-inducing regularization can be employed to com-pute sparse singular vectors, which in turn can be used to form co-clusters. showninthecoronalview.
 and brain regions for each age are summarized in this table.
This work is also related to recent studies on mining from time-evolving data, which is becoming an increasingly im-portant topic. Chakrabarti et al. [5] first proposed the con-cept of evolutionary clustering and extended the K -means and the hierarchical clustering algorithms for uncovering smooth patterns from time-evolving data matrices. In [7], the spectral clustering formalism is systematically extended to the evolutionary setting by incorporating a temporal cost into the objective function, leading to a suite of formulations for evolutionary spectral clustering. In [23], the nonnega-tive matrix factorization is employed for soft clustering, and a temporal cost is included for mining from time-evolving data. Evolutionary nonnegative matrix factorization is also studied in [34].

The fused Lasso penalty was originally proposed in [30] for encouraging smoothness over related coefficients in regres-sion problems. This type of penalty is very attractive and has been applied for encouraging smoothness over spatial and temporal smoothness in many applications, including biological data analysis [31] and social studies [1]. A critical challenge in employing the fused Lasso formalism is that this class of penalty is non-smooth and non-separable and thus is very challenging to optimize. In [13], a modified coordi-nate descent algorithm is developed to solve the fused Lasso formulation. However, this algorithm is not guaranteed to give the exact solution. In [18], a path algorithm is pro-posed to solve the fused Lasso signal approximator. Instead of solving the original primal problem, Liu et al. developed a dual formulation for the fused Lasso signal approximator and devised a gradient descent algorithm for computing the dual solution [24].

The formulation proposed in this work is radically dif-ferent from the evolutionary clustering and matrix factor-ization formalisms studied in the literature [5, 7, 23, 34]. The differences lie in both the studied problems and in the adopted approaches. Specifically, the existing evolutionary methods deal with clustering problems while our work is concerned with co-clustering problem. Indeed, to the best of our knowledge, our work is the first systematic study of co-clustering on time-varying data. In addition, our work is based on the optimization framework of sparsity-inducing formulations, while the current evolutionary clustering meth-ods is mostly motivated by matrix decomposition techniques.
We evaluate the proposed evolutionary co-clustering for-mulation using the Allen Developing Mouse Brain Atlas data, which are publicly available 1 . This data set contains in situ hybridization gene expression pattern images in the developing mouse brain across 7 developmental ages (see Figure 2). The 3D images are registered to a reference at-las separately for each age, and a regular grid is applied to partition the 3D brain space into voxels. The expression en-ergy within each voxel is given as a numerical value. The statistics of the data are summarized in Table 1. There is one data matrix associated with each of the 7 developing ages. The rows of the matrices correspond to brain voxels while the columns correspond to genes. Note that the brain voxels are not registered across ages, and the data for each age contain different number of voxels. Hence, we only ap-ply the fused Lasso regularization over the columns (genes); that is, we set  X  = 0. This is one of the unique advantages of the proposed formulation in which the smoothness con-straint can be applied to either or both dimensions. We use the duality gap as the stopping criterion for the gradient descent algorithm and the error tolerance is set to 10  X  8 the experiments.

To measure the co-clustering performance, we consider the annotated brain region of each voxel as its class and compare the clustering results with the region labels of vox-els, since it has been shown that the results of gene ex-pression data clustering are largely consistent with classi-http://developingmouse.brain-map.org/ Figure 3: Performance of the proposed method (  X  =  X  max ), denoted as CC evol ,incomparisonwith three other methods measured using the S index.
 CC SVD denotes the co-clustering method based on SVD proposed in [21]; CC spectral denotes the spec-tral co-clustering method proposed in [9]; K -means denotes the K -means method applied to gene ex-pression vectors of each voxel. cal neuroanatomy [3]. Following [2, 3], the S index is used to quantitatively measure the correspondence of the clus-tering results with the classical neuroanatomy reflected in the region annotations. Specifically, let R = { r 1 ,  X  X  X  ,r be a partition of the set of brain voxels in which each r comprises the set of indices of the voxels that map to that cluster (or anatomical label). The spatial overlap between a region from the annotation and the clustering result is de-fined as: P ij = | r i  X  r j | / | r j | .Fromthe P ij values that are computed over all pairs of brain regions and cluster result, we can then derive a global scalar index of similarity be-tween the two partitions. Since P ij = P ji , X ij is defined as X ij =max { P ij ,P ji } along with W ij = U ij / U ij , where U S index is defined as S =1  X  4 ij W ij X ij (1  X  X ij ) . The S index lies in [0,1], and larger value indicates higher con-sistency between the clustering results and the annotated regions.
To evaluate the performance of the proposed evolution-ary co-clustering method, we compare the proposed method with two other co-clustering methods and one clustering method. The two co-clustering methods are the one based on sparse SVD in [21] and the spectral co-clustering method proposed in [9, 36]. We also compare our method with the K -means algorithm when it is applied to cluster the voxels of the data set for each age separately. Note that the evo-lutionary clustering methods [5, 7, 23] cannot be applied to this data set, since the brain voxels are not registered across ages.

The performance of the four methods on the seven data sets is reported in Figure 3. We observed that the best per-formance is achieved when  X  =  X  max and report the results under this parameter setting. Detailed studies on parameter sensitivity are reported in Section 5.3. It can be observed from Figure 3 that the proposed evolutionary co-clustering method outperforms other compared methods consistently across all seven data sets, demonstrating that incorporation Figure 4: Performance of the proposed method as the value of the fused Lasso regularization parame-ter  X  increases. The performance is measured using the S index and is averaged across the 7 data sets. of the smoothness constraints between contiguous age data yield improved performance. We can also observe that co-clustering based methods consistently outperform clustering based method. This result is in accordance with the com-mon observation that co-clustering of gene expression data usually leads to improved performance. In addition, the co-clustering method based on sparse SVD outperforms the spectral co-clustering method on all seven data sets.
In order to fully understand how the fused Lasso regu-larization parameter  X  affects the performance, we conduct a series of experiments and report the results in the follow-ing. We first investigate how the performance changes as the value for  X  changes. To this end, we vary the value for  X  0 . 01 to  X  max =10 4 and report the performance on each data set in Table 2 and summarize the average performance across data sets in Figure 4. We can observe that the performance improves in general as the value for  X  increases. Indeed, the proposed formulation achieves the highest performance when  X  =  X  max . This demonstrate that incorporation of the fused Lasso regularization is very effective in boosting the performance.

To evaluate the effectiveness of the fused Lasso regular-ization in encouraging smoothness over the temporal dimen-sion, we again vary  X  from 0 . 01 to 10 4 and report the 1 differences between temporally adjacent variable vectors in Figure 5. We can observe that, as  X  increases, the values for the fused Lasso regularization terms decrease monotonically until they reach zero, where the adjacent variables are forced to be identical.

We also evaluate the effectiveness of the defined duality gap in determining the convergence of the gradient descent algorithm. To this end, we plot the values of the duality gap in the first 50 iterations of the gradient descent algorithm under multiple  X  values in Figure 6. We can observe that the duality gap decreases monotonically in all cases. In addi-tion, as the value of  X  increases, the duality gap approaches zero at a slower speed. This is because more computations are required to fuse adjacent variables when the value for increases. We use the duality gap as the stopping criterion in all experiments, and the error tolerance is set to 10  X  8 and the statistics are given in Table 1 Figure 5: The values of the fused Lasso regulariza-tion terms as  X  increases.
 In all cases, the duality gap is reduced below the tolerance level within a relatively small number of iterations.
In this paper, we propose an evolutionary co-clustering method for identifying block structures from time-evolving data. The proposed formulation employs the fused Lasso type of regularization to encourage the smoothness of the block structures, and it is applicable to scenarios in which only one sides of the blocks are required to be temporally smooth. The resulting optimization problem is non-convex, non-smooth, and non-separable, and we employ an iterative procedure to compute the solution. Each step of the iter-ative procedure involves a convex problem. We derive the dual form of this problem and employ a gradient descent al-gorithm to compute the dual optimal solution. Experimen-tal results on the Allen Developing Mouse Brain Atlas data show that the proposed method yields consistently higher performance in comparison to other methods.

It has been shown that nonnegative matrix factorization (NMF) can be used for clustering [11] and co-clustering [35]. However, to the best of our knowledge, NMF has not been employed to perform evolutionary co-clustering. We plan to investigate how NMF [15, 16] can be adapted for co-clustering on time-evolving data. In this paper, we solve the dual form of the convex problem in each iteration. In the literature, coordinate descent and path algorithms have been developed to solve the fused Lasso signal approxima-tor. We will explore and compare other alternative meth-Figure 6: The duality gap in the first 50 iterations for different  X  values. ods for solving this convex problem. This paper focuses on evaluating the proposed method on the mouse brain gene expression data, but this method can be applied to many other domains. We plan to apply our method to other data sets in the future.
We thank the Allen Institute for Brain Science for making the developing mouse brain data available to us. This re-search is supported by NSF DBI-1147134, NSFC 60905035, and by Old Dominion University. [1] A. Ahmed and E. P. Xing. Recovering time-varying [2] J. W. Bohland, H. Bokil, C. B. Allen, and P. P. Mitra. [3] J.W.Bohland et al. Clustering of spatial gene [4] S.Busygin,O.Prokopyev,andP.M.Pardalos.
 [5] D. Chakrabarti, R. Kumar, and A. Tomkins.
 [6] Y. Cheng and G. M. Church. Biclustering of [7] Y. Chi, X. Song, D. Zhou, K. Hino, and B. L. Tseng. [8] H. Cho, I. S. Dhillon, Y. Guan, and S. Sra. Minimum [9] I. S. Dhillon. Co-clustering documents and words [10] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [11] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [12] C. Eckart and G. Young. The approximation of one [13] J. Friedman, T. Hastie, H. H  X  ofling, and R. Tibshirani. [14] E. Giannakidou, V. Koutsonikola, A. Vakali, and [15] N. Guan, D. Tao, Z. Luo, and B. Yuan. Non-negative [16] N. Guan, D. Tao, Z. Luo, and B. Yuan. NeNMF: An [17] J. A. Hartigan. Direct clustering of a data matrix. [18] H. H  X  ofling. A path algorithm for the fused Lasso [19] S. Ji. Computational network analysis of the [20] Y.Kluger,R.Basri,J.T.Chang,andM.Gerstein.
 [21] M. Lee, H. Shen, J. Z. Huang, and J. S. Marron. [22] E. S. Lein et al. Genome-wide atlas of gene expression [23] Y.-R. Lin, Y. Chi, S. Zhu, H. Sundaram, and B. L. [24] J. Liu, L. Yuan, and J. Ye. An efficient algorithm for a [25] S. C. Madeira and A. L. Oliveira. Biclustering [26] A. Nemirovski. Efficient methods in convex [27] Y. Nesterov. Introductory Lectures on Convex [28] C. Soneson and M. Fontes. A method for visual [29] R. Tibshirani. Regression shrinkage and selection via [30] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and [31] R. Tibshirani and P. Wang. Spatial smoothing and [32] H. Tong, S. Papadimitriou, P. S. Yu, and C. Faloutsos. [33] P. Tseng. Convergence of a block coordinate descent [34] F. Wang, H. Tong, and C.-Y. Lin. Towards [35] J. Yoo and S. Choi. Orthogonal nonnegative matrix [36] H. Zha, X. He, C. Ding, H. Simon, and M. Gu.
