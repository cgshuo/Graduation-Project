 Multivariate real-valued data appears in many real-world data sets, and a lot of research is being focused on the development of multivariate real-valued distributions. One of the challenges in con-structing such distributions is that univariate continuous distributions commonly do not have a clear multivariate generalization. The most studied exception is the multivariate Gaussian distribution ow-ing to properties such as closed form density expression with a convenient generalization to higher dimensions and closure over the set of linear projections. However, not all problems can be ad-dressed fairly with Gaussians (e.g., mixtures, multimodal distributions, heavy-tailed distributions), and new approaches are needed for such problems.
 While modeling multivariate distributions is in general difficult due to complicated functional forms and the curse of dimensionality, learning models for individual variables (univariate marginals) is often straightforward. Once the univariate marginals are known (or assumed known), the rest can be modeled using copulas , multivariate distributions with all univariate marginals equal to uniform distributions on [0 , 1] (e.g., [2, 3]). A large portion of copula research concentrated on bivariate copulas as extensions to higher dimensions are often difficult. Thus if the desired distribution de-composes into its univariate marginals and only bivariate distributions, the machinery of copulas can be effectively utilized.
 Distributions with undirected tree-structured graphical models (e.g., [4]) have exactly these prop-erties, as probability density functions over the variables with tree-structured conditional indepen-dence graphs can be written as a product involving univariate marginals and bivariate marginals corresponding to the edges of the tree. While tree-structured dependence is perhaps too restrictive, a richer variable dependence can be obtained by averaging over a small number of different tree distributions with an ensemble-of-trees model [1]. In this paper, we extend this tree-averaged model to continuous variables with the help of copulas and derive a learning algorithm to estimate the parameters within the maximum likelihood framework with EM [6]. Within this framework, the parameter estimation for tree-structured and tree-averaged models requires optimization over only univariate and bivariate densities potentially avoiding the curse of dimensionality, a property not shared by alternative models that relax the dependence restriction of trees (e.g., vines [7]). The main contributions of the paper are the new tree-averaged model for multivariate copulas, a parameter estimation algorithm for tree-averaged framework (for both categorical and real-valued complete data), and a new model for multi-site daily precipitation amounts, an important application in hydrology. In the process, we introduce previously unexplored tree-structured copula density and an algorithm for estimation of its structure and parameters. The paper is organized as follows. First, we describe copulas, their densities, and some of their useful properties (Section 2). We then construct multivariate copulas with tree-structured dependence from bivariate copulas (Section 3.1) and show how to estimate the parameters of the bivariate copulas and perform the edge selection. To allow more complex dependencies between the variables, we describe a tree-averaged copula , a novel copula object constructed by averaging over all possible spanning trees for tree-structured copulas, and derive a learning algorithm for the estimation of the parameters from data for the tree-averaged copulas (Section 4). We apply our new method to a benchmark data set (Section 5.1); we also develop a new model for multi-site precipitation amounts, a problem involving both binary (rain/no rain) and continuous (how much rain) variables (Section 5.2). Let X = ( X 1 ,...,X d ) be a vector random variable with corresponding probability distribution F (cdf) defined on R d . We denote by V the set of d components (variables) of X and refer to individual variables as X v for v  X  V . For simplicity, we will refer to assignments to ran-dom variables by lower case letters, e.g., X v = x v will be denoted by x v . Let F v ( x v ) = F ( X v = x v ,X u =  X  : u  X  X  \{ v } ) denote a univariate marginal of F over the variable X v . a = ( a 1 ,...,a d ) , so a is a vector of quantiles of components of x with respect to corresponding univariate marginals. Next, we define copula, a multivariate distribution over vectors of quantiles. Definition 1. The copula associated with F is a distribution function C : [0 , 1] d  X  [0 , 1] that satisfies If F is a continuous distribution on R d with univariate marginals F 1 ,...,F d , then C ( a ) = F F  X  1 1 ( a 1 ) ,...,F  X  1 d ( a d ) is the unique choice for (1).
 Assuming that F has d -th order partial derivatives, the probability density function (pdf) can be obtained from the distribution function via differentiation and expressed in terms of a derivative of a copula: Suppose we are given a complete data set D = x 1 ,..., x N of d -component real-valued vec-tors x n = x n 1 ,...,x d 1 under i.i.d. assumption. A maximum likelihood (ML) estimate for the parameters of c (or p ) from data can be obtained my maximizing the log-likelihood of D of p , and the second term to the log-likelihood of its d -variate copula. These terms are not inde-pendent as the second term in the sum is defined in terms of the probability expressions in the first summand; except for a few special cases, a direct optimization of (3) is prohibitively complicated. for the marginals. The univariate marginals can be accurately estimated by either fitting the parame-ters for some appropriately chosen univariate distributions or by applying non-parametric methods 1 as the marginals are estimated independent of each other and do not suffer from the curse of di-quantiles. Under the above heuristic, ML estimate for copula density c is computed by maximizing ln c ( A ) = P N n =1 ln c ( a n ) . ture of the graph captures the conditional independence relations of the variables. The joint distri-bution is then represented as a product of functions over subsets of variables. We would like to keep the number of variables for each of the functions small as the number of parameters and the number of points needed for parameter estimation often grows exponentially with the number of variables. Thus, we focus on copulas with tree dependence. Trees play an important role in prob-abilistic graphical models as they allow for efficient exact inference [10] as well as structure and parameter learning [4]. They can also be placed in a fully Bayesian framework with decomposable priors allowing to compute expected values (over all possible spanning trees) of product of functions pendence, a copula density can be computed as products of bivariate copula densities over the edges of the graph. This property allows us to estimate the parameters for the edge copulas independently. 3.1 Tree-Structured Copulas We consider tree-structured Markov networks, i.e., undirected graphs that do not have loops. For a distribution F admitting tree-structured Markov networks (referred from now on as tree-structured can be rewritten as This formulation easily follows from the Hammersley-Clifford theorem [11]. Note that for { u,v } X  E , a copula density c uv ( a u ,a v ) for F ( x u ,x v ) can be computed using Equation 2: Using Equations 2, 4, and 5, c p ( a ) for F ( x ) can be computed as Equation 6 states that a copula density for a tree-structured distribution decomposes as a product of bivariate copulas over its edges. The converse is true as well; a tree-structured copula can be constructed by specifying copulas for the edges of the tree.
 is a valid copula density.
 For a tree-structured density, the copula log-likelihood can be rewritten as pairs { u,v }  X  E . The tree structure can be learned from the data as well, as in the Chow-Liu algorithm [4]. Full algorithm can be found in an extended version of the paper [12]. While the framework from Section 3.1 is computationally efficient and convenient for implementa-tion, the imposed tree-structured dependence is too restrictive for real-world problems. Vines [7], for example, deal with this problem by allowing recursive refinements for the bivariate probabilities over variables not connected by the tree edges. However, vines require estimation of additional char-of variables, which is not advisable when the amount of available data is not large. Our proposed method would only require optimization of parameters of bivariate copulas from the corresponding two components of weighted data vectors. Using the Bayesian framework for spanning trees from trees allowing a much richer set of conditional independencies than a single tree.
 Meil  X  a and Jaakkola [1] proposed a decomposable prior over all possible spanning tree structures. Let  X  be a symmetric matrix of non-negative weights for all pairs of distinct variables and zeros on all spanning tree structures over V is defined as Even though the sum is over | E | = d d  X  2 trees, Z can be efficiently computed in closed form using a weighted generalization of Kirchoff X  X  Matrix Tree Theorem (e.g., [1]).
 Theorem 2. Let P ( E ) be a distribution over spanning tree structures defined by (7). Then the first ( d  X  1) rows and columns of the matrix L (  X  ) given by: The decomposability property of the tree prior (Equation 7) allows us to compute the average of tree-structured distributions over categorical variables. Similarly, we define a tree-averaged copula density as a convex combination of copula densities of the form (6): is a copula, so r ( a ) is a copula density. 4.1 Parameter Estimation Given a set of estimated quantile values A , a suitable parameter values  X  (edge weight matrix) and  X  (parameters for bivariate edge copulas) can be found by maximizing the log-likelihood of A : we are dealing with a mixture model (granted, one where the number of mixture components is super-exponential), we propose performing the parameter optimization with the EM algorithm [6]. 2
Inputs: A complete data set D of d -component real-valued vectors; a set of of bivariate para-metric copula densities c = { c uv : u,v  X  X }
Output: Denoting a u =  X  F ( x u ) and a v =  X  F ( x v ) ,  X  p ( x ) = Q While there are d d  X  2 possible mixture components (spanning trees), in the E-step, we only need to compute the posterior probabilities for d ( d  X  1) / 2 edges. Each step of EM consists of find-parameter values  X  ,  X  where M  X  0 ,  X  0 ;  X  ,  X  = s ( { u,v } ) one needs to compute the sum of probabilities of all trees containing edge { u,v } . is obtained by removing row and column w from L . Then be computed simultaneously with time complexity of a single ( d  X  1)  X  ( d  X  1) matrix inversion, by setting to 0 . (See [12] for more details.) The parameters of the tree prior can be updated by maximizing an expression concave in ln  X  uv  X { u,v } .  X  0 can be updated using a gradient ascent algorithm on ln  X  uv  X { u,v } , with time complexity O d 3 per iteration. The outline of the EM algorithm is shown in Figure 1. Assuming the complexity of each bivariate copula update is O ( N ) , the time complexity of each EM iteration is O Nd 3 .
 The EM algorithm can be easily transferred to tree averaging for categorical data. The E-step does not change, and in the M-step, the parameters for the univariate marginals are updated ignoring bivariate terms. Then, the parameters for the bivariate distributions for each edge are updated con-strained on the new values of the parameters for the univariate distributions. While the algorithm does not guarantee a maximization of the expected log-likelihood, it nonetheless worked well in our experiments. 5.1 MAGIC Gamma Telescope Data Set First, we tested our tree-averaged density estimator on a MAGIC Gamma Telescope Data Set from the UCI Machine Learning Repository [13]. We considered only the examples from class gamma (signal); this set consists of 12332 vectors of d = 10 real-valued compo-nents. The univariate marginals are not Gaussian (some are bounded; some have multiple modes). Fig. 2 shows an average log-likelihood of models trained on training sets with N = 10 training and test sets). The marginals were estimated using Gaussian kernel density estima-tors (KDE) with Rule-of-Thumb bandwidth selection. All of the models except for full Gaussian have the same marginals, differ only in the multivariate dependence (copula). As expected from the curse of dimensionality, product KDE improves logarithmically with the amount of data. Not only the marginals are not Gaussian (evidenced by a Gaussian copula with KDE marginals outper-forming a Gaussian distribution), the multivariate dependence is also not Gaussian, evidenced by a tree-structured Frank copula outperforming a tree-structured and a full Gaussian copula. However, model averaging even with the wrong dependence model (tree-averaged Gaussian copula) yields superior performance. 5.2 Multi-Site Precipitation Modeling We applied the tree-averaged framework to the problem of modeling daily rainfall amounts for a regional spatial network of stations. The task is to build a generative model capturing the spatial and temporal properties of the data. This model can be used in at least two ways: first, to sample sequences from it and to use them as inputs for other models, e.g., crop models; and second, as a descriptive model of the data. Hidden Markov models (possible with non-homogeneous transi-for modeling of temporal dependence, and the emission distributions capturing most of the spatial dependence. Additionally, HMMs can be viewed as assigning rainfall daily patterns to  X  X eather states X  (or corresponding emission components), and both these states (as described by either their parameters or the statistics of the patterns associated with it) and their temporal evolution often offer useful synoptic insight. We will use HMMs as the wrapper model with tree-averaged (and tree-structured) distributions to model the emission components.
 The distribution of daily rainfall amounts for any given station can be viewed as a non-overlapping mixture with one component corresponding to zero precipitation, and the other component to posi-itation: For a pair of stations { u,v } , let  X  uv denote the probability of simultaneous positive amounts and c then p ( r u ,r v |  X  u , X  v , X  uv ,  X  u ,  X  v ) = respectively, over the amounts:  X  uv ( r ) = c We applied the models to a data set collected from 30 stations from a region in Southeastern Aus-tralia (Fig. 3) 1986-2005, April-October, (20 sequences 214 30-dimensional vectors each). We models HMM-TA, HMM-Tree, and HMM-CI, respectively. For HMM-TA, we reduced the number of free parameters by only allowing edges for stations adjacent to each other as determined by the selected edges and to 0 for the rest. To make sure that the models do not overfit, we computed their out-of-sample log-likelihood with cross-validation, leaving out one year at a time (not shown). (5 states were chosen because the leave-one-year out log-likelihood starts to flatten out for HMM-TA  X  0 . 9522 , and  X  1 . 0222 for HMM-TA, HMM-Tree, and HMM-CI, respectively. To see how well the models capture the properties of the data, we trained each model on the whole data set (with 50 restarts of EM), and then simulated 500 sequences of length 214. We are particularly interested in how well they measure pairwise dependence; we concentrate on two measures: log-odds ratio for occurrence and Kendall X  X   X  measure of concordance for pairs when both stations had positive amounts. Both are shown in Fig. 4. Both plots suggest that HMM-CI underestimates the pairwise dependence for strongly dependent pairs (as indicated by its trend to predict lower absolute values for log-odds and concordance); HMM-Tree estimating the dependence correctly mostly for strongly dependent pairs (as indicated by good prediction for high values), but underestimating it for mod-erate dependence; and HMM-TA performing the best for most pairs except for the ones with very strong dependence.
 This work has been supported by the Alberta Ingenuity Fund through the AICML. We thank Stephen Charles (CSIRO, Australia) for providing us with precipitation data.
 Figure 2: Averaged test set per-feature log-likelihood for MAGIC data: independent KDE (black solid ), product KDE (blue dashed  X  ), Gaussian (brown solid  X  ), Gaussian copula (or-ange solid + ), Gaussian tree-copula (magenta dashed x), Frank tree-copula (blue dashed ), Gaussian tree-averaged copula (red solid x). Figure 4: Scatter-plots of log-odds ratios for occurrence (left) and Kendall X  X   X  measure of concor-dance (right) for all pairs of stations for the historical data vs HMM-TA (red o), HMM-Tree (blue x), and HMM-CI (green  X  ).
