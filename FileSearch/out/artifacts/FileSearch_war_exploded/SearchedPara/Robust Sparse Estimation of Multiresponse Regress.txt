 We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data: (i) learn-ing a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional depen-dency structure among responses while adjusting for their predictors. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecifica-tion. This issue is exacerbated when dealing with high di-mensional noisy data. In this work, we propose instead to minimize a regularized distance criterion, which is motivated by the minimum distance functionals used in nonparamet-ric methods for their excellent robustness properties. The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm. We provide theoretical justification such as estimation consistency for the proposed estimator. Additionally, we shed light on the robustness of our estimator through its linearization, which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness. We demonstrate the merits of our frame-work through simulation study and the analysis of real fi-nancial and genetics data.
 I.2.6 [ Artificial Intelligence ]: Learning Robust estimation, high dimensional data, sparse learning, variable selection, multiresponse regression, inverse covari-ance, L2E
We focus on multiresponse regression where both predic-tor and response spaces may exhibit high dimensions. We propose a robust framework to jointly and synergistically solve two important tasks: (i) learning the sparse functional mapping between inputs and outputs while taking advan-tage of the coupling among responses, and (ii) estimating the conditional dependency structure among responses while adjusting for the covariates. This is motivated by the cru-cial need of integrating genomic and transcriptomic datasets in computational biology in order to solve two fundamental problems effectively: identifying the genetic variations in the genome that influence gene expression levels (a.k.a. expres-sion quantitative trait loci eQTLs mapping), and uncovering gene expression networks. In fact, the accuracy of the first problem can then be improved by leveraging the gene relat-edness, and similarly the accurate and faithful estimation of the gene expression networks can be obtained by accounting for the confounding genetic effects on gene expression.
Multiresponse regression [5] generalizes the basic single-response regression to model multiple responses that might significantly correlate with each other. As opposed to treat-ing each response independently, one can jointly learn multi-ple regression mappings to improve the estimation and pre-diction accuracy by exploiting the conditional dependencies among responses. Variable selection in multiresponse re-gression can be accomplished via the penalized approaches including lasso [31] and multitask lasso [24].

Sparse estimation of inverse covariance matrix is an im-portant area in the multivariate analysis with broad applica-tions in graphical models. A major focus in this area is that of penalized maximum likelihood formulations [15, 34, 1, 16]. Alternatively, modified Cholesky decompositions based on the likelihood can be used to estimate the sparse inverse co-variance [17, 4, 21]. A simpler approach of  X  X eighborhood selection X  [22] estimates sparse graphical models using lasso to regress on each variable with the others as predictors.
Combining multiresponse regression and inverse covari-ance estimation has recently begun to attract more attention in the machine learning community. Rothman et al. [27] pro-posed a multivariate regression with covariance estimation (MRCE) to jointly estimate the sparse regression and inverse covariance matrices. They demonstrated that exploiting the correlation structures can significantly improve the predic-tion accuracy. The same model was also studied by Lee and Liu [20] who provided some theoretical properties for their developed method. An alternative parameterization was considered by Sohn and Kim [29], which is based on the joint distribution of predictors and responses and yields an l -penalized conditional graphical model ( l 1 -CGGM). An-other relevant method is that of covariate adjusted preci-sion matrix estimation (CAPME) [7], a two-stage approach to estimate the conditional dependency structure among re-sponse variables by adjusting for covariates. The first stage is to estimate the regression coefficients via a multivariate extension of the l 1 Dantzig selector [8], and the second stage is to estimate the inverse covariance matrix using l  X  error with an l 1 penalty.

Robustness is an important aspect often overlooked in the sparse learning literature, while critical when dealing with high dimensional noisy data. Traditional likelihood-based estimators such as MRCE and l 1 -CGGM lack resilience to outliers and model misspecification. Additionally, to the best of our knowledge, estimates based on Dantzig selector have not been compared to lasso counterparts in terms of ro-bustness. Thus it is unclear whether CAPME, for instance, can address the robustness issue. There is limited existing work on robust sparse learning methods in high-dimensional modeling. The LAD-lasso [32] performs single response re-gression using the least absolute deviation combined with an l penalty. The tlasso [14] performs inverse covariance esti-mation using penalized log-likelihood with the multivariate t distribution. However, neither of these methods can be easily extended to the setting of this paper.

We propose a robust approach to jointly estimate mul-tiresponse regression and inverse covariance matrix. Our ap-proach is based on a regularized distance criterion motivated by minimum distance estimators. Minimum distance esti-mators [33] are popularized in nonparametric methods and have exhibited excellent robustness properties [3, 12]. Their use for parametric estimation has been discussed in [28, 2]. In this work, we propose a penalized minimum distance cri-terion for robust estimation of sparse parametric models in the high dimensional settings. Our key contributions to this robust approach are as follows. The strength of our method is demonstrated via simulation data with and without outliers. Our study also confirms that outliers can severely influence the variable selection accuracy of some existing sparse learning methods. Experiments on real financial and eQTL data further illustrate the merits of the proposed method.
Denote the response vector y = ( y 1 ,...,y q ) 0  X  R the predictor vector x = ( x 1 ,...,x p ) 0  X  R p . We consider a multiresponse linear regression model where B = ( b ij ) is a p  X  q matrix of coefficients and the k th column is the coefficients associated with k th response y regressing on the predictors x . The q  X  q covariance matrix  X  describes the covariance structure of response vector y given the predictors x . Moreover, its inverse  X   X  1 = ( c represents the partial covariance structure [19] and has been widely used to learn a sparse graphical model under Gaus-sian assumption. Note that  X   X  1 in (1) captures the partial covariances among responses y after adjusting for the effects of covariates x . For simplicity of notation, we assume the data are centered so that the model (1) does not contain intercepts.

Suppose there are n observational vectors x i = ( x i 1 ,...,x i = 1 ,...,n and the corresponding response vectors are y coefficient matrix B and precision matrix  X   X  1 , we consider a loss function L n ( B ,  X  ) that measures the goodness-of-fit on the multivariate response. The sparse structures of B and  X   X  1 are encouraged by using l 1 penalties. Specifically, the penalized loss function L n, X  ( B ,  X  ) is written as matrix norms. Following the principle of parsimony, we con-sider l 1 penalty functions to seek a most appropriate model that adequately explains the data. With carefully selected tuning parameters  X  1 and  X  2 , we can achieve an optimal trade-off between the parsimoniousness and goodness-of-fit of the model.

The loss L n ( B ,  X  ) is typically derived from a likelihood-based approach. For instance the MRCE method [27] uses L ( B ,  X  ) = trace ( Y  X  XB ) T ( Y  X  XB )  X   X  1  X  log  X  Altenatively, if one ignores the contribution of the inverse covariance matrix (by implicitely assuming that it is the identity matrix), one can consider the traditional squared loss L n = k Y  X  XB k 2 F and a straighforward generalization of the traditional Lasso estimator [31] to the mutiresponse setting.
We begin this section by showing how a minimum dis-tance criterion yields our proposed estimator for achieving robustness under the model in (1).
We first apply the Integrated Squared Error (ISE) cri-terion to the conditional distribution of response vector y given the predictors x . It leads to an L 2 distance between the true conditional distribution f ( y | x ) and the parametric distribution function f ( y | x ; B ,  X  ) as follows L ( B ,  X  ) = where f ( y | x ; B ,  X  ) is the probability density function of multivariate normal N ( B 0 x ,  X  ) and R f ( y | x ) 2 d y is a con-stant independent of B and  X  .

Note that f ( y | x ; B ,  X  )  X  f ( y  X  B 0 x ;  X  ) because of the conditional distribution assumption. Since = y  X  B 0 x are independently and identically distributed, one can consider approximating E [ f ( y | x ; B ,  X  )] by the empirical mean Similar approximation techniques have also been used for Gaussian mixture density estimation [28]. Now the resulting empirical loss function of  X  L ( B ,  X  ) can therefore be written as  X  L n ( B ,  X  ) = where R f 2 ( y | x ; B ,  X  ) d y = 1 / (2 q  X  q/ 2 |  X  | assume a parametric family for the model while using a non-parametric ISE criterion to measure goodness of fit.
From the perspective of the loss function, ISE is a more robust measure of the goodness-of-fit compared with the likelihood-based loss function. It can match the model with the largest portion of the data because the integration in (3) accounts for the whole range of the squared loss function.
Using ISE criterion as the loss function, the objective func-tion in (2) becomes However, the minimization of the objective in (4) is challeng-ing. To circumvent this difficulty, we consider minimizing an upper bound of (4) which retains the robustness property.
For that purpose, we introduce a lemma which is essential for deriving the proposed objective (8) for estimating the sparse multiresponse regression model in (1).

Lemma 1. For a positive definite matrix  X   X  1 with di-mension q , the relation between its determinant value and l norm can be described in the following inequality |  X   X  1 The proof of Lemma 1 is provided in the Appendix. Using Lemma 1, we can derive an upper bound for the objective function (4) as follows c where c  X  = 2  X  q (  X q )  X  q/ 2 is a constant. The above optimiza-tion problem amounts to minimizing where  X  L n ( B ,  X  ) =  X  2 n P n i =1 f ( y i | x i ; B ,  X  ) and  X  propriately chosen. The value of  X   X  1 here should be slightly larger than the value of  X  1 in (4). Moreover, the diagonal elements of  X   X  1 are also penalized.

Note that  X  L n, X  ( B ,  X  ) is an upper bound of  X  L n, X  trolled by the penalty term  X   X  1 k  X   X  1 k 1 in (6). By properly adjusting the value of  X   X  1 , we can make the difference reason-ably small. Therefore, by minimizing  X  L n, X  ( B ,  X  ), we expect to approach the solution of  X  L n, X  ( B ,  X  ) and thus still retain the robustness property in the estimators.
 Taking the logarithm on  X  L n ( B ,  X  ), we obtain the loss ( B ,  X  ) =  X  log  X  1 We note that the logarithm is employed to strike a better balance between goodness of fit and the sparsity inducing penalty (similarly one considers the penalized negative log-likelihood rather than dealing with the likelihood directly). This yields the estimator proposed and studied in this paper, the Regularized Integrated Square Error (REG-ISE) estima-tor, which minimizes the following objective function: For notational convenience, here and in later sections, we use  X  1 for  X   X  1 .

Some intuition can already be gained on the objective robustness by considering the ratio between data and model this ratio to infinity, in which case the log-likelihood is also is always bounded. This property makes the L 2 -distance a favourable choice when dealing with outliers. We note that a similar reasoning holds in the context of density estimation, as pointed out in the recent work of [30] on density-difference estimation.
The REG-ISE objective function (8) is non-convex and non-smooth. In order to solve it, one could consider ap-proximating the  X  X og-sum-exp X  term in the objective, com-bined with alternate optimization for B and  X   X  1 respec-tively (as done in MRCE). However, the convergence of al-ternate optimization can be very slow, as observed in the case of MRCE [27].
Instead, we propose to adopt a sequential quadratic pro-gramming algorithm recently developed by Curtis &amp; Over-ton [9] for the non-smooth and non-convex optimization. The basic idea of their algorithm SLQP-GS is to combine sequential quadratic approximation with a process of gradi-ent sampling so that the computation of the search direction is effective in nonsmooth regions. The only requirement for SLQP-GS to be applicable is that the objective and con-straints (if any) be continuously differentiable on open dense subsets, which is satisfied in our case. We also benefit from the convergence guarantees of SLQP-GS, namely that the algorithm is guaranteed to converge to a solution regardless of the initialization with probability one.

We employed the Matlab implementation of SLQP-GS provided by the authors, which is available from http:// coral.ie.lehigh.edu/~frankecurtis/software . Due to space constraints, we refer the reader to Curtis &amp; Overton [9] for details on the algorithm, its matlab implementation and convergence results. We note that the gradient sampling step in SLQP-GS can be efficiently parallelized for fast com-putation in high dimensional applications. Alternatively one can perform adaptive sampling of gradients over the course of the optimization process as described in [10].
The L 2 distance estimators are known to strike the right balance between statistical efficiency and robustness [2, 28]. In this section, we add to this body of evidence by showing that the REG-ISE estimator is root-n consistent for the set-tings of the fixed dimensionality p . Denote by  X  B the true regression coefficient matrix, and by  X   X  the true covariance matrix. We assume the following conditions: (C1) 1 n X 0 X  X  A , where A is positive definite. (C2) There exist Condition (C2) can be replaced by some technical regular-ity conditions as in [13] so as to guarantee the consistency of ordinary maximum likelihood estimators.

Theorem 1. Consider sequences  X  1 ,n and  X  2 ,n of regu-larization parameters, such that Then under the conditions (C1) and (C2), there exists a local minimizer (  X  B ,  X   X   X  1 ) of REG-ISE such that k ( vec (  X  B ) 0 ,vec (  X   X   X  1 ) 0 ) 0  X  ( vec (  X  B ) The proof is provided in the Appendix.

The above theoretical results hold for the case where the dimensionality p is fixed, while the sample size n is allowed to grow. As a future work we plan to extend our results to the case where p is allowed to grow with the sample size n. We conjecture that condition (C1) might be replaced by conditions on sample and population covariance while (C2) is still achieved by certain penalized maximum likelihood estimators [23]. We note however that such an extension is highly non-trivial. In fact, to our knowledge, no theory is yet available for joint estimation in the high dimensional case even for the standard maximum likelihood estimator, the non-convexity being the source of the difficulty. Nev-ertheless, in view of our superior empirical results, we hope that theoretical results can be obtained by making use of the techniques in [26] which solely deal with inverse covariance estimation, and those in [23] which solely concern regression.
We provide some insights for the robustness of REG-ISE by considering a first order approximation of the  X  X og-sum-exp X  term in (8).

Define the parameter set  X  = ( B ,  X   X  1 ) and denote g i 2 ( y i  X  B approximation for log 1 n P n i =1 exp( g i (  X  )) with respect to  X  as follows: where  X  0 is an initial estimate and C 0 is some constant independent of  X  .

Using the fact that g i (  X  )  X  g i (  X  0 ) +  X  g i (  X  0 we have the following log up to some constant independent of  X  . Therefore, the ob-jection function (8) can be approximated by up to some constant and where
By defining S  X  = S  X  (  X  0 ) = 1 n P n i =1 w i ( y i  X  B B x i ) 0 , we can rewrite (9) as  X  log |  X   X  1 | + trace[  X   X  1 S  X  (  X  0 )] +  X  1 k  X   X  1 Note that S  X  can be viewed as a weighted sample covariance matrix where weights are with respect to n observations.
One could then envision an approximate iterative proce-dure where given initial estimates, data are first re-weighted by w i in (10) and then alternately passed to lasso and l penalized inverse covariance solvers (e.g. QUIC, glasso) to provide new estimates, and the procedure would be repeated until convergence (see details in the appendix). This intu-itively elaborates the robustness property of REG-ISE. In-deed the weights w i are proportional to the likelihood func-tions of individual data points, i.e., w i = L ( y i | x i Thus data with high likelihood values are given more weights in the estimation. Conversely, data with low likelihood val-ues, which are more likely to be outliers, contribute less to the estimation. The connection between the likelihood functions and weights nicely explains the resilience of the proposed estimator to outliers.
Approaches for choosing tuning parameters include cross-validation (CV) [4], the hold-out validation set method [21], and information criteria such as Bayesian information crite-rion (BIC) [34]. Here we proposed a modified scheme for the cross-validation method. The common K -fold CV consists in randomly partitioning the data into K folds, and then leaving out one fold of data as validation set while all the other folds are used as training set in each CV iteration. Note that CV assumes that the data are i.i.d. distributed, and therefore the validation set and training set are consid-ered statistically equivalent. However, such an assumption is no longer valid in the presence of outliers since the propor-tions of the outliers in the validation data and training data can be different. Consequently, the validation set cannot be used to evaluate the model obtained by the training set.
To tackle this issue, we develop a modified cross-validation scheme motivated by the idea of sliced designs [25]. Specifi-cally, we perform K -fold cross-validation for n = mK obser-vations as follows. Based on initial estimates of the model parameters, we first rank the observed data according to the values of their likelihood functions. Then the first K data points are randomly assigned to K folds, one point per fold. Subsequently the next K data points are randomly assigned to K folds. This procedure is repeated m times. In this way, the data in each fold are more likely to have similar distributions. This modified scheme can also be applied to tuning via hold-out validation set method. We compare the proposed REG-ISE with MRCE [27], l 1 -CGGM [29], CAPME [7], and LAD-Lasso [32]. Note that LAD-Lasso can only estimate regression coefficients.
In our experiments, the rows of n  X  p predictor matrices X are sampled independently from N ( 0 ,  X  x ) where (  X  x ) 0 . 5 | i  X  j | . We consider the following two cases.

Case 1: The covariance matrix is set to  X  i,j = 0 . 7 | i  X  j | which corresponds to an AR(1) model with banded  X   X  1 . We randomly select 10 percent of the predictors to be irrelevant to all the responses. Then for each response, we randomly select half of the remaining predictors to be relevant for that response. The corresponding non-zero entries in the regres-sion matrix B are sampled independently from N (0 , 1) . We consider 60 predictors, 20 responses and 100 observations.
Case 2:  X   X  1 is the graph Laplacian of a tree with outde-gree of 4 and edge weights uniformly sampled from [0 . 3 , 1 . 0]. For each response we randomly select 10 percent of the pre-dictors to be relevant, and sample the corresponding non-zero enties in B independently from N (0 , 1) . We consider 1000 predictors, 100 responses and 400 observations.
To address the robustness issue, we consider various per-centages of outliers contaminating the responses. The un-contaminated data are generated from y  X  N ( B 0 x ,  X  ), where B and  X  are specified above. Two scenarios pre-senting outliers are considered: (i) outliers with respect to the mean: y  X  N ( B 0 x + C ,  X  ) where C is a constant vec-tor of 5 and (ii) outliers regarding the covariance structure: y  X  X  ( B 0 x , I ) where I is an identity matrix.

To measure variable selection accuracy, we use the F score defined by F 1 = 2 PR/ ( P + R ) , where P is precision (fraction of correctly selected variables among selected vari-ables) and R is recall (fraction of correctly selected vari-ables among true relevant variables). To measure the esti-mation accuracy of B , we report the model error defined as ME (  X  B , B ) = tr h (  X  B  X  B ) T  X  x (  X  B  X  B ) i the estimated regression coefficient matrix. The estima-tion accuracy for  X   X  1 is measured by its l 2 loss, defined as k  X   X   X  1  X   X   X  1 k F , under Frobenius norm where  X   X  estimated inverse covariance matrix.

For each of the above settings, we generate 50 simulated dataset. For all five comparison methods, for each dataset we use the modified 5-fold cross-validation described in Sec-tion 3.5 to tune parameters  X  1 and  X  2 .
 The choice of initial parameter estimates is important for MRCE and REG-ISE as their respective objective functions are non-convex. The initial estimates for B are obtained using ridge regression. In addition for REG-ISE,  X  is ini-tialized as the inverse of the sample covariance matrix of ridge regression residuals perturbed by a positive diagonal matrix.
 Figures 1 and 2 present the results for Case 1. Results for Case 2 are summarized in Table 1 Similar behavior in terms of robustness is observed for both cases. Our proposed REG-ISE estimator clearly outperforms other methods due to its robustness against outliers with respect to both mean and covariance deviations. Performance of MRCE, l 1  X  CGGM and CAPME seriously degrade once outliers are introduced. Surprisingly, LAD-Lasso does not show much resilience to outliers. Moreover, with  X  X lean X  data, its estimation and variable selection accuracy is inferior to other methods as it ignores the dependencies among responses. Interestingly, even when there are no outliers in the data, REG-ISE is competitive, since it is more likely to distinguish true signals from various noise amplitudes.
 F igure 1: Average model error ME (  X  B , B ) (top) and F 1 scores (bottom) for B estimated by REG-ISE, MRCE, l 1  X  CGGM, LAD-Lasso, and CAPME on simulated data of Case 1. Outliers in terms of the mean (left), and covariance (right). The x-axis cor-responds to the percentage of outliers. Table 1: Simulation results for Case 2. Top: Model error for B / l  X  F igure 2: Average estimation error k  X   X   X  1  X   X   X  1 (top) and F 1 scores (bottom) for  X   X  1 estimated by REG-ISE, MRCE, l 1  X  CGGM, LAD-Lasso, and CAPME on simulated data of Case 1. Outliers in terms of the mean (left), and covariance (right). The x-axis corresponds to the percentage of outliers.
In this section, we illustrate the usefulness of the proposed robust methods through two motivating applications and compare the results of our robust estimators with those of existing methods.
As a toy example for multivariate time series, we analyze a financial dataset which has been studied in [27] and [34]. This dataset contains weekly log-returns of 9 stocks for year 2004. Given multivariate time series data of log-returns y for weeks t = 1 ,...,T , a first-order vector autoregressive model is considered as follows where the response matrix y t is formed by observations at week t and the predictor matrix y t  X  1 contains observations at the previous week t  X  1. Following the analysis in Roth-man et al. [27], we used log-returns of the first 26 weeks as training set, and log-returns of the remaining 26 weeks as testing set. The tuning parameters were selected using the modified 10-fold cross-validation described in Section 2.4. Table 2 reports the mean squared prediction error (MSPE) of the five comparison methods. Even though all methods are competitive on this dataset, REG-ISE estimator achieves the smallest prediction error. Figure 3 presents the graphs induced by the estimates of  X   X  1 using MRCE and REG-ISE, respectively. Comparing the two graphs, both MRCE and REG-ISE indicate that companies from the same indus-try are partially correlated, e.g. GE and IBM (technology), Ford and GM (auto industry). AIG (insurance company) seems to be partially correlated with most of the other com-panies. However, there are some discrepancies between the two graphs, e.g., GM is found to be partially correlated to IBM by REG-ISE but to be uncorrelated by MRCE. Over-all, the results from REG-ISE have reasonable financial in-terpretation.
We analyze yeast eQTL dataset [6] which contains geno-type data for 2,956 SNPs (predictors) and microarray data for 6,216 genes (responses) regarding 112 segregants (in-stances). We extracted 1,260 unique SNPs, and focused on 125 genes belonging to cell-cycle pathway provided by the KEGG database [18]. For all methods, the tuning pa-rameters were chosen via 5-fold modified cross-validation de-Table 2: Prediction accuracy measured by MSPE for various methods for the asset return dataset.
 [1] O. Banerjee, L. Ghaoui, and A. d X  X spremont. Model [2] A. Basu, I. R. Harris, N. L. Hjort, and M. C. Jones. [3] R. Beran. Robust location estimates. Annals of [4] P. J. Bickel and E. Levina. Regularized estimation of [5] L. Breiman and J. H. Friedman. Predicting [6] R. Brem and L. Kruglyak. The landscape of genetic [7] T. T. Cai, H. Li, W. Liu, and J. Xie. Covariate [8] E. Candes and T. Tao. The dantzig selector: [9] F. E. Curtis and M. L. Overton. A Sequential [10] F. E. Curtis and X. Que. An Adaptive Gradient [11] D. L. Donoho and R. C. Liu. The  X  X utomatic X  [12] J. Fan and R. Li. Variable selection via nonconcave [13] M. Finegold and M. Drton. Robust graphical [14] J. Friedman, T. Hastie, and R. Tibshirani. Sparse [15] C. Hsieh, M. Sustik, I. Dhillon, and P. Ravikumar. [16] J. Huang, N. Liu, M. Pourahmadi, and L. Liu.
 [17] M. Kanehisa, S. Goto, M. Furumichi, M. Tanabe, and [18] S. L. Lauritzen. Graphical Models . Oxford: Clarendon [19] W. Lee and Y. Liu. Simultaneous multiple response [20] E. Levina, A. J. Rothman, and J. Zhu. Sparse [21] N. Meinshausen and P. Buhlmann. High-dimensional [22] S. Negahban, P. Ravikumar, M. Wainwright, and [23] G. Obozinski, B. Taskar, and M. Jordan. Multi-task [24] P. Z. G. Qian and C. F. J. Wu. Sliced space-filling [25] P. Ravikumar, M. Wainwright, G. Raskutti, and [26] A. Rothman, E. Levina, and J. Zhu. Sparse [27] D. Scott. Parametric statistical modeling by minimum [28] K. Sohn and S. Kim. Joint estimation of structured [29] M. Sugiyama, T. Suzuki, T. Kanamori, M. C.
 [30] R. Tibshirani. Regression shrinkage and selection via [31] H. Wang, G. Li, and G. Jiang. Robust regression [32] J. Wolfowitz. The minimum distance method. Annals [33] M. Yuan and Y. Lin. Model selection and estimation
