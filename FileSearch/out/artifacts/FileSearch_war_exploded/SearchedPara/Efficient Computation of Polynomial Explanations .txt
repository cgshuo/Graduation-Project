 Answering a Why-Not question consists in explaining why a query result does not contain some expected data, called missing answers. This paper focuses on processing Why-Not questions in a query-based approach that identifies the culprit query components. Our first contribution is a general definition of a Why-Not explanation by means of a polynomial. Intuitively, the polynomial provides all possible explanations to explore in order to recover the missing answers, together with an estimation of the number of recoverable answers. Moreover, this formalism allows us to represent Why-Not explanations in a unified way for extended relational models with probabilistic or bag semantics. We further present an algorithm to efficiently compute the polynomial for a given Why-Not question. An experimental evaluation demonstrates the practicality of the so-lution both in terms of efficiency and explanation quality, compared to existing algorithms.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms Why-Not question, explanation, provenance
The increasing load of data produced nowadays is coupled with an increasing need for complex data transformations that develop-ers design to process these data in every-day tasks. These trans-formations, commonly specified declaratively, may result in unex-pected outcomes. For instance, given the sample query and data of Fig. 1 on airlines and destination countries, a developer (or trav-eller) may wonder why Emirates does not appear in the result. Traditionally, she would repeatedly manually analyze the query to identify a possible reason, fix it, and test it to check whether the missing answer is now present or if other problems need to be fixed. c  X  Figur e 2: Reordered query trees for the query of Fig. 1 and algorithm results (Why-Not  X  , NedExplain ? , Conseil  X  )
Answering such Why-Not questions , that is, understanding why some data are not part of the result, is valuable in a series of appli-cations, such as query debugging and refinement, data verification or what-if analysis. To help developers explain missing answers , different algorithms have recently been proposed for relational and SQL queries and other types of queries (top-k, reverse skyline).
For relational queries, Why-Not questions can be answered for example based on the data (instance-based explanations), the query (query-based explanations), or both (hybrid explanations). We fo-cus on solutions producing query-based explanations, as these are generally more efficient while providing sufficient information for query analysis and debugging. Essentially, a query-based expla-nation is a set of query conditions that are responsible for pruning out data relevant to the missing answers. Existing methods pro-ducing query-based explanations are not satisfactory as they are designed over query trees, making the explanations depending on the topology of a given tree. Consequently, they return different explanations for the same SQL query and may miss explanations.
E XAMPLE 1.1. Consider again the SQL query and data of Fig. 1 and assume that a developer wants an explanation for the absence of Emirates from the query result. Fig. 2 shows two possible query trees. It also shows the tree operators that Why-Not [7] (  X  ) and NedExplain [3] ( ? ) return as query-based explanations as well as the tree operators returned as part of hybrid explanations by Con-seil [13, 14] (  X  ). Each algorithm returns a different result for each of the two query trees, and in most cases, it is only a partial result as the true explanation of the missing answer is that both the selection is too strict for the tuple ( Emirates, 1985 , 3) from table Airline and this tuple does not find join partners in table Country .
The above example clearly shows the shortcomings of existing algorithms. Indeed, the developer first has to understand and reason at the level of query trees instead of reasoning at the level of the declarative SQL query she is familiar with. Second, she always has to wonder whether the explanation is complete, or if there are other explanations that she could consider instead. In this paper, we make the following contributions: Extended formalization of Why-Not explanation polynomial. We have recently introduced polynomials as Why-Not explanations in the context of the relational model under set semantics [2]. A polynomial provides a complete explanation and is independent of a specific query tree representation, solving the problems illustrated by Ex. 1.1. We now extend Why-Not explanations to the relational model under bag and probabilistic semantics. This confirms the ro-bustness of the chosen polynomial representation, making it a good fit for a unified framework for representing Why-Not explanations. In parallel, we considerably simplify our initial framework by elim-inating the formerly used notion of query tableaux.
 Efficient Ted++ algorithm. We show that a naive algorithm com-puting Why-Not explanations, as presented in [2], is impractical. We thus propose a novel algorithm, Ted++ , capable of efficiently computing the Why-Not explanation polynomial, based on tech-niques like schema and data partitioning (allowing for a distributed computation) and advantageous replacement of expensive database evaluations by mathematical calculations.
 Experimental validation. We validate our solutions both in terms of efficiency and effectiveness. Our experiments include a com-parative evaluation to existing algorithms computing query-based explanations for SQL queries (or sub-languages thereof) as well as a thorough study of Ted++ performance w.r.t. different parameters. Note that such an evaluation was missing from [2].

The remainder of this paper is structured as follows. Sec. 2 cov-ers related work. Sec. 3 defines in detail our problem setting and the Why-Not explanation polynomials. Next, we discuss the Ted++ al-gorithm in Sec 4. Finally, we present our experimental setup and evaluation in Sec. 5 and conclude in Sec. 6.
The work presented in this paper falls in the category of data provenance research and specifically on explaining missing answers from query results. Due to the lack of space, we focus here on this sub-problem, thus on algorithms answering Why-Not questions, summarized in Tab. 1. This table classifies the algorithms accord-ing to the type of explanation they generate and reports the class of query and Why-Not question (simple or complex) 1 they support. Query-based and hybrid explanations. Why-Not [7] takes as in-put a simple Why-Not question and a selection, projection, join, and union (SPJU) query and returns the erroneous query operators as query-based explanations. Similarly, NedExplain [3] consid-ers selection, projection, join, and aggregation and unions thereof (SPJUA queries) and simple Why-Not questions as well. The com-mon drawback of the two algorithms is that their design is depen-dent on a specific query tree representation, thus the explanations proposed are tied to this tree. Moreover, the generated explanations are incomplete. To address this problem, Ted [2] proposes explana-tions in the form of a polynomial. The shortcomings of this work have already been presented in Sec. 1.

Conseil [13, 14] produces hybrid explanations that include an instance-based and a query-based component. The latter consists in a set of picky query operators. However, as Conseil considers
A simple Why-Not question involves conditions that impact one relation only, otherwise it is complex (see Sec. 3).
 both the data to be possibly incomplete and the query to be possi-bly faulty, the set of picky query operators associated to a hybrid explanation depends on the set of source edits of the same hybrid explanation.
 Instance-based explanations. Missing-Answers (MA) [16] and Artemis [15] compute explanations in the form of source table edits for SPJ queries and SPJUA queries respectively. Meliou et. al. [19] study the concepts of causality and responsibility of instance-based explanations for data present or missing in a conjunctive query re-sult. Calvanese et. al. [5] compute instance-based explanations on data represented by a DL-Lite ontology.
 Ontology-based explanations. Cate et. al. [6] have recently in-troduced this type of explanation that is based on external or data-workload generated ontologies. However, they are completely in-dependent of the query to be analyzed.
 Refinement-based explanations. Another approach to answering Why-Not questions is by directly proposing queries that include in their result the missing answers. Several algorithms follow this di-rection for different types of queries, like relational, Top-K, reverse sklyline queries, etc, as listed in Tab. 1. Although these approaches are generally very interesting, they do not focus on pinpointing the erroneous parts of the query. Indeed, the refined queries may con-tain changes that are not necessarily tied to an erroneous part of the query. Moreover, the changes are based on the database values and do not take into account any semantics or domain knowledge that could render a refinement meaningful for the user.
This section introduces a polynomial formalization of query-based Why-Not explanations. We assume the reader familiar with the relational model [1], and we only briefly revisit some rele-vant notions in Sec. 3.1 while we formalize Why-Not questions. In Sec. 3.2, we define the explanation of a Why-Not question as a polynomial. In Sec. 3.3 we provide a unified general framework for Why-Not explanations in the context of set, bag, and probabilistic semantics databases.
For the moment, we limit our discussion to relational databases under set semantics. A database schema S is a set of relation schemas. A relation schema R is a set of attributes. We assume each attribute of R qualified, i.e., of the form R.A and for the sake of simplicity we assume a unique domain Dom . I denotes a database instance over S and I | R denotes the instance of a rela-tion R  X  X  . We assume that each database relation R has a special attribute R.Id , which is used as identifier for the tuples in I any object O (relational or database schema, condition etc), A ( O ) denotes the set of attributes occurring in O . Finally, a condition c over S is defined as an expression of the form R.A  X  a where a  X  Dom or of the form R.A  X  S.B , where R.A,S.B  X  X  ( S ) , and  X   X  X  = , 6 = ,&lt;,  X } . A condition over two relations is complex , oth-erwise it is simple . In this article, we consider conjunctive queries with inequalities. Note that in our approach, the database schema S denotes the query input schema. In an SQL-like approach, each time we need an instance of a relation, we refer to it by a differ-ent name. In this way, we are able to correctly define Why-Not questions in case of self-joins as well.

D EFINITION 3.1 (Q UERY Q). A query Q is specified by the triple ( S ,  X  ,C ) , where S is a database schema,  X   X  A ( S ) is the projection attribute set, and C is a set of conditions over A ( S ) . The semantics of Q are given by the relational algebra expression  X  The result of Q over I is denoted by Q [ I ] . Note here that we are not concerned about the evaluation/optimization of Q and that any equivalent rewriting of the algebraic expression given in Def. 3.1 is a candidate for evaluating Q .

E XAMPLE 3.1. Fig. 3 describes our running example. Fig. 3(a) displays an instance I over S = { R,S,T } . Fig. 3(b) displays a query Q over S , whose conditions have been named for conve-nience. R.B = T.B and T.D = S.D are complex whereas the others are simple conditions. Moreover, Q [ I ]= { ( R.B :5 ,S.D :4 ,T.C :9) } .
In our framework, a Why-Not question specifies missing tuples from the result of a query Q through a conjunctive set of condi-tions. A Why-Not question is related to the result of Q and so its conditions are restricted to the attributes of the output schema of Q .
D EFINITION 3.2 (W HY -N OT QUESTION ). A Why-Not ques-tion WN w.r.t. Q is defined as a set of conditions over  X  .
The notion of complex and simple conditions is extended to com-plex and simple Why-Not questions in a straightforward manner. Due to lack of space, we do not provide here more real-world exam-ples of Why-Not questions and refer the reader to scenarios in [4].
As we said, a Why-Not question WN summarizes a set of (miss-ing) tuples that the user expected to find in the query result. To be able to obtain these missing tuples as query results, data from the input relation instances that satisfy WN need to be combined by the query. The candidate data combinations are what we call com-patible tuples and can be computed using WN as in Def. 3.3. D EFINITION 3.3 (C OMPATIBLE TUPLES ). Consider the query Q WN =( S , A ( S ) ,WN ) , where S is also the input schema of Q . The set CT of compatible tuples is the result of the query Q over I .

We further introduce the notion of a well founded Why-Not ques-tion. Intuitively, a Why-Not question can be answered under a query-based approach, only if some data in I match the Why-Not question (otherwise instance-based explanations should be sought for). Moreover, a Why-Not question is meaningful if it tracks data not already returned by the query.
 A Why-Not question WN is said to be well founded if CT 6 =  X  and  X  [ CT ]  X  Q [ I ]=  X  .

E XAMPLE 3.2. Continuing Ex. 3.1, we may wonder why there is not a tuple for which R.B&lt;S.D and T.C  X  9 . According to Def. 3.2, this Why-Not question can be seen as the conjunc-tion of the conditions R.B&lt;S.D  X  T.C  X  9 (Fig. 3(c)). Since R.B&lt;S.D is a complex condition, WN is a complex Why-Not question. The compatible tuples set CT is the result of the query Q For example, one compatible tuple is  X  1 =( R.Id :1 ,R.A :1 ,R.B :3 , S.Id :5 ,S.D :4 ,S.E :8 ,T.Id :8 ,T.B :3 ,T.C :4 ,T.D :5) .
Each tuple in CT could have led to a missing tuple, if it was not eliminated by some of the query conditions. Thus, explaining WN amounts to identifying these blocking query conditions.
To build the query-based explanation of WN , we start by spec-ifying what explains that a compatible tuple  X  did not lead to an answer. Intuitively, the explanation consists of the query conditions pruning out  X  .

D EFINITION 3.5 (E XPLANATION FOR  X  ). Let  X   X  CT be a compatible tuple w.r.t. WN , given Q . Then, the explanation for  X  is the set of conditions E  X  = { c | c  X  C and  X  6| = c } . E XAMPLE 3.3. Consider the compatible tuple  X  1 in Ex. 3.2. The conditions of Q (see Ex. 3.1), not satisfied by  X  1 are: c and c 4 . So, the explanation for  X  1 is E  X  1 = { c 1 ,c
Having defined the explanation w.r.t. one compatible tuple, the explanation for WN is obtained by simply summing up the ex-planations for all the compatible tuples in CT , leading to the ex-pression P  X  with a product (meaning conjunction) of conditions by the fact that in order for  X  to  X  X urvive X  the query conditions and give rise to a missing tuple, every single condition in the explanation must be  X  X epaired X . The sum (meaning disjunction) of the products for each  X   X  CT means that if any explanation is  X  X orrectly repaired X , the associated  X  will produce a missing tuple.

Of course, several compatible tuples can share the same expla-nation. Thus, the final Why-Not explanation is a polynomial hav-ing as variables the query conditions and as integer coefficients the number of compatible tuples sharing an explanation. assumption as before, the Why-Not explanation for WN is defined as the polynomial where E = 2 C and coef E  X  X  0 ,..., | CT |} .

Intuitively, E contains all potential explanations, and each of these explanations prunes from zero to at most | CT | compatible tuples. Moreover, an important property of PEX is the fact that coef E = |{  X   X  CT |E is the explanation for  X  }| , meaning that coef equals the number of compatible tuples with the same explanation.
Each term of the polynomial provides an alternative explanation to be explored by the user who wishes to recover some missing tuples. Additionally, the polynomial offers, through its coefficients, some useful hints to users interested in the number of recoverable tuples. More precisely, by choosing an explanation E to repair, we obtain an upper bound for the number of compatible tuples that can be recovered. The upper bound is the sum of the coefficients of all the explanations that are sub-sets of (the set of conditions of) E . Consequently, the coefficients could be used to answer Why-Not questions of the form Why-Not $x missing tuples? .

E XAMPLE 3.4. In Ex. 3.3 we found the explanation { c 1 ,c which is translated to the polynomial term c 1  X  c 3  X  c 4 consideration all the 12 compatible tuples of our example, we ob-tain the following PEX polynomial: 2  X  c 1  X  c 4 + 2  X  c 1 c  X  c 2  X  c 4 + 2  X  c 1  X  c 2  X  c 3 + 2  X  c 1  X  c 2  X  c 3  X  c each addend, composed by a coefficient and an explanation, cap-tures a way to obtain missing tuples. For instance, the explanation c  X  c 2  X  c 4 indicates that we may recover some missing answers if c and c 2 and c 4 are changed. Then, the sum of its coefficient 4 and the coefficient 2 of the explanation c 1  X  c 4 ( { c 1 ,c indicates that we can recover from 0 to 6 tuples.

As the visualization of the polynomial per se may be cumber-some and thus not easy for a user to manipulate, some post-processing steps could be applied. Depending on the application or needs, only a subset of the explanations could be returned like for instance min-imum explanations (i.e., for which no sub-explanations exist), or explanations giving the opportunity to recover a specific number of tuples, or have specific condition types etc. So far, we have considered databases under set semantics only. In this section, we discuss how the definition of Why-Not expla-nation (Def. 3.6) extends to settings with conjunctive queries over bag and probabilistic semantics.

K -relations, as introduced in [10], capture in a unified manner relations under set, bag, and probabilistic semantics. Briefly, tuples in a K -relation are annotated with elements in K . In our case, we consider that K is a set of unique tuple identifiers, similar to our special attribute R.Id in Sec. 3.1.

In what follows, we use the notion of how -provenance of tu-ples in the result of a query Q . The how-provenance of t  X  Q ( I ) is modelled as the polynomial obtained by the positive algebra on K -relations, proposed in [10]. Briefly, each t is annotated with a polynomial where variables are tuple identifiers and coefficients are natural numbers. Roughly, if t results from a selection operator on t annotated with Id 1 , then t is also annotated with Id 1 . If t is the result of the join of t 1 and t 2 , then t is annotated with Id We compute the generalized Why-Not explanation polynomial PEX gen as follows. Firstly, we compute the how-provenance for compatible tuples in CT by evaluation of the query Q WN (Def. 3.3) w.r.t. the algebra in [10]. Recall that Q WN contains only selection and join operators. Thus, each compatible tuple  X  in CT is anno-tated with its how-provenance polynomial, denoted by  X   X  .
Then, we associate the expressions of how and why-not prove-nance. In order to do this, for each compatible tuple  X  in CT , we combine its how-provenance polynomial  X   X  with its explanation E (Def. 3.5). So, each  X  is associated with the expression  X 
Finally, we sum the combined expressions for all compatible tu-ples, which leads to the expression P
We now briefly comment on how PEX gen is instantiated to deal either with the set, bag, or probabilistic semantics. Indeed, the  X  X pe-cialization X  of PEX gen relies on the interpretation of the elements in K, that is on a function Eval from K to some set L . For the set semantics, each tuple in a relation occurs only once. This re-sults in choosing L to be the singleton { 1 } and mapping each tuple identifier to 1 . It is then quite obvious to note, for the set seman-tics, that PEX gen = PEX (Def. 3.6). In the same spirit, for bag semantics, L is chosen as the set of natural numbers N and each tuple identifier is mapped to its number of occurences. Finally, for probabilistic databases, L is chosen as the interval [0 , 1] and each tuple identifier is mapped to its occurrence probability.
Thus, the generalized definition of Why-Not explanation is para-metrized by the mapping Eval of the annotations (elements in K ) in the set L .
 D EFINITION 3.7. (Generalized Why-Not explanation polynomial) Given a query Q over a database schema S of K -relations, the generalized Why-Not explanation polynomial for WN is where E =2 C ,  X   X  is the how-provenance of  X  , and Eval : K  X  L maps the elements of K to values in L .
The naive Ted algorithm [2] implements the definitions of [2] for Why-Not explanations in a straightforward manner. Essentially, Ted first enumerates the set of compatible tuples. Then, it computes the explanation for each compatible tuple, leading to the computa-tion of the final Why-Not explanation. However, both of these steps make Ted computationally prohibitive. Not only is the computation of the set of compatible tuples time and space consuming as it often requires cross product executions, but the same holds for the iter-ation over this (potentially very large) set. Ted X  X  time complexity is O ( n |S| ) , n = max ( {| I R |} ) ,R  X  X  . As experiments in Sec. 5 confirm, this complexity renders Ted impractical.

To overcome Ted X  X  poor performance, we propose Ted++ . The main feature of Ted++ is to completely avoid enumerating and it-erating over the set CT , thus it significantly reduces both space and time consumption. Instead, Ted++ opts for (i) iterating over the space of possible explanations, which is expected to be much smaller, (ii) computing partial sets of passing compatible tuples, and (iii) computing the number of eliminated compatible tuples for each explanation. Intuitively, passing tuples w.r.t. an explanation are tuples satisfying the conditions of the explanation. Finally, we compute the polynomial based on mathematical calculations.
Alg. 1 provides an outline of Ted++ . The input includes the query Q =( S ,  X  ,C ) , the Why-Not question WN and the input in-stance I . Firstly in Alg. 1, line 1, all potential explanations (com-binations of the conditions in C ) are enumerated ( E =2 remaining steps, discussed in the next subsections, aim at comput-ing the coefficient of each explanation. To illustrate the concepts Algorithm 1: Ted++ introduced in the detailed discussions, we will rely on our running example, for which Fig. 4 shows all relevant intermediate results. It should be read bottom-up. For convenience, in our examples, we use subscript i instead of c i .

The subsequent discussion on Ted++ can be considered as a proof sketch of the following theorem.

T HEOREM 4.1. Given a query Q , a Why-Not question WN and an input instance I , Ted++ computes exactly PEX .
Using the conditions in WN , Ted++ partitions the schema S (Alg. 1, line 2) into components of relations connected by the con-ditions in WN (Def. 4.1 ).

D EFINITION 4.1. (Valid Partitioning of S ). Given WN , the partitioning of a database schema S into k partitions, denoted P = { Part 1 ,...,Part k } , is valid if each Part i , i  X  X  1 ,...,k } is minimal w.r.t. the following property: if R  X  Part i and R 0  X  X  s.t.  X  c  X  WN with A ( c )  X  X  ( R A ( c )  X  X  ( R ) 6 =  X  then R 0  X  Part i .

The partitioning of S allows for handling compatible tuples more efficiently, by  X  X utting X  them in distinct meaningful  X  X hunks X . We refer to chunks of compatible tuples as partial compatible tuples and group them in sets depending on the partition they belong to.
The set CT | Part , where Part  X  X  is obtained by evaluating the query Q Part =( Part, A ( Part ) , WN | Part ) over I | Part line 4). WN | Part and I | Part denote the restriction of WN and I over the relations in Part , respectively.

E XAMPLE 4.1. The valid partitioning of S is Part 1 = { R,S } (because of the condition R.B&lt;S.D ) and Part 2 = { T } . The sets of partial compatible tuples CT | Part 1 and CT | Part 2 are given in the bottom line of Fig. 4.

It is easy to prove that the valid partitioning of S is unique and that the set CT can be computed from the sets CT | Part . L EMMA 4.1. Let P be the valid partitioning of S . Then,
Lemma. 4.1 makes it clear how to compute CT from partial compatible tuples. Our algorithm is designed in a way that avoids computing CT and relies on the computation of CT | Part only.
Next, we compute the number of compatible tuples pruned by each potential explanation, using the partial compatible tuple sets. In this way we calculate the coefficient of the terms in the poly-nomial. Since from this point on we are only handling compatible tuples, we omit the word  X  X ompatible X  to lighten the discussion. Algorithm 2: coefficientEstimation
Each set E in the powerset E is in fact a potential explanation that is further processed. This process is meant to associate with E (i) the set of partitions part E on which E is defined, (ii) the view definition V E meant to store the passing partial tuples w.r.t. E (iii) the number  X  E of tuples eliminated by E .

Alg. 2 describes how we process E in ascending order of ex-planation size. This enables us to reuse results obtained for sub-explanations and in combination with mathematics, avoid cross product computations.

We first determine the set of partitions for an explanation E as part E =  X  c  X  X  { Part c } , where Part c contains at least one relation over which c is specified.

E XAMPLE 4.2. Consider E 1 = { c 1 } and E 2 = { c 2 } . From Fig. 3(b) and the partitions in Fig. 4, we can see that c 1 impacts only Part whereas c 2 spans over Part 1 and Part 2 . Hence, part E 1 and part E 2 = { Part 1 ,Part 2 } . Then, E = { c 1 ,c 2 the union of part E 1 and part E 2 , thus part E = { Part
We use Eq. (A) to calculate the number  X  E of eliminated tuples, using the number  X  E of eliminated partial tuples and the cardinality of the partitions not in part E . Intuitively, this formula extends the partial tuples to  X  X ull X  tuples over CT  X  X  schema. where part E = P\ part E . Note that when part E is empty, we abu-sively consider that Q  X  =1 .

The presentation now focuses on calculating  X  E . Two cases arise depending on the size of E .
 Atomic explanations. We start with atomic explanations E con-taining only one condition c (Algorithm 2 lines 3-5). We firstly compute the set of passing partial tuples w.r.t. c , i.e., the tuples that satisfy c , which we store in the view V c :
We choose to store passing rather than eliminated tuples as they are usually less numerous. In an optimized version this decision could be made dynamically based on view cardinality estimation.
Then, the number of eliminated partial tuples by E is
E XAMPLE 4.3. For c 2 , we have part c 2 = { Part 1 ,Part V sults in | V c 2 | =4 , and by Eq. (B) we obtain  X  | CT | Part 2 | X  X  V c 2 | =3  X  4  X  4=8 . Since all partitions of P are in part c 2 , applying Equ. (A) results in  X  c 2 =  X   X  c 3 = | CT | Part 2 | X  V c 3 =4  X  2=2 , so  X  c 3 =3  X  2 = 6 . Fig. 4 (second level) displays the process for all atomic explanations. Non atomic explanations. Now, assume that E = { c 1 ,...,c n &gt; 1 (Alg. 2, lines 6-16). For the moment, we assume that the conditions in E share the same schema, so the intersection and union of V ci for i = 1 ,...,n are well-defined. Firstly, we compute the view V E of passing partial tuples w.r.t. E as V E = V To compute the number of partial tuples pruned out by E , we need to find the number of partial tuples pruned out by c 1 and . . . and c , i.e.,  X  E = | V c 1  X   X  X  X   X  V cn | . By the well-known DeMor-gan law [22], we have  X  E = | V c 1  X  X  X  X  X  V cn | , which spares us from computing the complements of V ci .

To compute the cardinality of the union of the V ci , we rely on the Principle of Inclusion and Exclusion for counting [11]: We further rewrite the previous formula to reuse results obtained for sub-combinations of E , obtaining Eq. (C).
At this point, we can compute  X  E . However, so far we assumed that the conditions in E have the same schema. In the general case, this does not hold and we have to  X  X xtend X  the schema of a view V c to the one of V E , in order to ensure set operations to be well-defined. The cardinality of an extended V ext c is given by Eq. (D).
Based on Eq. (D) we obtain Eq. (E) that generalizes Eq. (C). | ( In Eq. (E) we have replaced the intersection with natural join. The cardinalities of the views V E 0 = ( 1 V cj ) j  X  J 1 with E 0 for | J | &lt; n  X  1 , have already been computed by previous steps and have only to be extended to the schema of V E . When | J | = n  X  1 , then V E 0 = V E . A detailed discussion on how and when we materialize the view V E is given shortly after.

Now, we trivially compute the number  X  E of eliminated partial tuples as the complement of | ( number of  X  X ull X  eliminated tuples is then calculated by Eq. (A).
E XAMPLE 4.4. To illustrate the concepts introduced above, please follow on Fig. 4 below discussion.

For the explanation c 2 c 3 , Eq. (E) gives: | ( V 2  X  V 3 | V 3 | X  X  ( V 2 is  X  23 = { R _ Id,S _ Id,T _ Id } . The view V 2 has already a match-ing schema, thus | V ext 2 | = | V 2 | =4 . For V 3 ,  X  thus apply Eq. (D) and obtain | V ext 3 | = | V Part 1 | X  X  V Still, | V 23 | = | ( V 2 1 V 3 ) ext | remains to be calculated. Intu-itively, because V 2 and V 3 target schemas share attribute T _ Id , V 23 = V 2 1 T _ Id V 3 . The view V 23 is materialized and contains 2 tuples (as shown in Fig. 4). So, finally, from Eq. (E) we obtain | ( V 2  X  V 3 ) ext | =4+6  X  2=8 . Since | Part 1 | X  X  Part 2 | = 12 then  X  23 =12  X  8=4 , and by Eq. (A)  X  23 =4 .

We now focus on the explanation c 3 c 5 . The schemas of V and V 5 are disjoint and intuitively V 35 = V 3 " V 5 . Here, V not materialized, we simply calculate | V 35 | = | V 3 | X  X  V |  X  35 | =12  X  (12+6  X  6)=0 . As we will see later, these steps are never performed in our algorithm. The fact that c 5 eliminate any tuple (see  X  5 =0 in Fig. 4) implies that neither do any of its super-combinations. Thus, a priori we know that  X  35 =  X  235 = ... =0 .

Finally, we illustrate the case of a bigger size combination, for example c 2 c 3 c 4 of size 3. Eq. (E) yields | ( V 2  X  V V V ) ext | . All terms of the right side of the equation are available from previous iterations, except for | ( V 2 1 V 3 1 V before, we check the common attributes of the views and obtain V 2  X  1 + 0=9 and  X  234 =  X  234 =12  X  9 = 3 . In the same way, we compute all the possible explanations until c 1 c 2 c 3 c View Materialization: when and how. To decide when and how to materialize the views for the explanations, we partition the set of the views associated with the conditions in E . Consider the relation  X  defined over these views by V i  X  V j if the target schemas of V and V j have at least one common attribute. Consider the transitive closure  X   X  of  X  and the induced partitioning of V E through  X 
When this partitioning is a singleton, V E needs to be material-ized (Alg. 2, line 9). The materialization of V E joining the views associated with the sub-conditions, which may be done in more than one way, as usual. For example, for the com-bination c 2 c 3 c 4 , V 234 can either be computed through V V 24 1 V 3 or V 34 1 V 2 or V 2 1 V 3 1 V 4 . . . because all these views are known from previous iterations. The choice of the query used to materialize V E is done based on a cost function. This function gives priority to materializing V E by means of one join, which is always possible: because V E needs to be materialized, we know that at least one view associated with a sub-combination of size n  X  1 has been materialized. In other words, priority is given to us-ing at least one materialized view associated with one of the largest sub-combinations. For our example, it means that either V or V 24 1 V 3 or V 34 1 V 2 is considered. In order to choose among the one-join queries computing V E , we favour a one-join query V minimal w.r.t. | V i | + | V j | . For the example, and considering also Fig. 4 we find that | V 2 | + | V 34 | = | V 4 | + | V 23 So, the query used for the materialization is V 3 1 being empty in our example). Nevertheless, we avoid the mate-rialization of V E if the partitioning is a singleton (Alg. 2, line 9 &amp; 16), when for some sub-combination E 0 of E it was computed that  X  E 0 =0 . In that case, we know a priori that  X  E =0 (see Ex. 4.4).
If the partitioning is not a singleton, V E is not materialized (Alg. 2, line 14). For example, the partitioning for c 3 c 5 is not a singleton and so the size | V 35 | = | V 3 | X | V 5 | =6 .
 Post-processing. In Alg. 2 we associated with each possible ex-planation E the number of eliminated tuples  X  E . However,  X  includes any tuple eliminated by E , even though the same tuples may be eliminated by some super-combinations of E (see Ex. 4.5). This means that for some tuples, multiple explanations have been assigned. To make things even, the last step of Ted++ (Alg. 1, line 6) is about calculating the coefficient of E by subtracting the coefficients of its super-combinations from  X  E : E XAMPLE 4.5. Consider known coef 1234 =2 and coef 123 =2 . We have found in Ex. 4.4 that  X  23 =4 . With Eq. (F), coef 23 =4  X  2  X  2=0 . In the same way coef 2 =4  X  0  X  2  X  2=0 . The algorithm leads to the expected Why-Not explanation polynomial already provided in Ex. 3.4.
In the pseudo-code provided by Alg. 1, we can see that Ted++ divides into the phases of (i) partitioning S , (ii) materializing a view for each partition, (iii) computing the explanations, and (iv) com-puting the exact coefficients. When computing the explanations, according to Alg. 2, Ted++ iterates through 2 | C | condition combi-nations and for each, it decides upon view materialization (again through partitioning) before materializing it, or simply calculates | V
E | before applying equations to compute  X  E . Overall, we con-sider that all mathematical computations are negligible so, the worst case complexities of steps (i) through (iv) sum up to O ( |S| + | WN | )+ O ( |S| ) + O (2 | C | ( |S| + | C | ))+ O (2 ficiently large queries, we can assume that |S| + | C | &lt;&lt; 2 which case the complexity simplifies to O (2 | C | ) .

The complexity analysis above does not take into account the cost of actually materializing views; in its simplified form, it only considers how many views need to be materialized in the worst case. Assume that n = max ( {|I R || R  X  X } ) . The materialization of any view is bound by the cost of materializing a cross product over the relations involved in the view -in the worst case O ( n yields a combined complexity of O (2 | C | n |S| ) . However, Ted++ in the general case (more than one induced partitions), has a tighter for all combinations E and N = 2 | C | .
We perform an experimental evaluation of Ted++ on real and synthetic datasets. In Sec. 5.1, we compare Ted++ to Ted [2], NedExplain [3], and Why-Not [7]. Sec. 5.2 studies the runtime of Ted++ w.r.t. various parameters that we vary in a controlled manner. All Java implementations of the algorithms ran on MAC OS X 10.9.5 with 1.8 GHz Intel Core i5, 4GB memory, and 120GB SSD. PostgreSQL 9.3 was used as database system.
The comparative evaluation to Why-Not and NedExplain consid-ers both efficiency (runtime) and effectiveness (explanation qual-ity). When considering efficiency, we also include Ted in the com-parison (Ted producing the same Why-Not explanation as Ted++ ).
For the experiments we have used data from three databases named crime , imdb , and gov . The crime database (available at http://infolab.stanford.edu/trio/) is a synthetic database about crimes and involved persons (suspects and witnesses). The imdb database contains real-world movie data from IMDB (http://www.imdb.com). Finally, the gov database contains information about US congress-men and financial activities (data from http://bioguide.congress.gov, http://usaspending.gov, and http://earmarks.omb.gov).

For each dataset, we have created a series of scenarios (crime1-gov5 in Tab. 3 -ignore remaining scenarios for now). Each scenario consists of a query further defined in Tab. 2 (Q1-Q7) and a simple Why-Not question, as Why-Not and NedExplain support only this type of Why-Not question. We have designed queries with a small set of conditions (Q6) or a larger one (Q1, Q3, Q5, Q7), containing self-joins (Q3, Q4), having empty intermediate results (Q2), as well as containing inequalities (Q2, Q4, Q5, Q6). Tab. 1 states that the explanations returned by Why-Not and Ned-Explain consist of sets of query conditions, whereas Ted++ returns a polynomial of query conditions. For comparison purposes, we trivially map Ted++  X  X  Why-Not explanation to sets of conditions, e.g., 3 c 3  X  c 4 + 2 c 3  X  c 6 maps to {{ c 3 ,c 4 } , { c ness, we abbreviate condition sets, e.g., to c 34 ,c 36 .
Tab. 4 summarizes the Why-Not explanations of the three algo-rithms. These scenarios make apparent that the explanations by NedExplain or Why-Not are incomplete, in two senses. First, they produce only a subset of the possible explanations, failing to pro-vide alternatives that could be useful to the user when she tries to fix the query. Second, even the explanation they provide may lack parts, which can drive the user to fruitless fixing attempts. On the contrary, Ted++ produces all the possible, complete explanations.
For the first argument, consider the scenario gov 2 . Why-Not and NedExplain return c 1 and c 3 respectively, but they both fail to indicate that both the explanations are valid, as opposed to Ted++ . Then, consider crime 8 . NedExplain returns the join c 2 ( S -Why-Not does not produce any explanations. Ted++ indicates that except for this join, the selection c 3 (  X  name&lt;  X  B stance is also an explanation. From a developer X  X  perspective, se-lections are typically easier or more reasonable to change. So, hav-ing the complete set of explanations potentially provides the devel-oper with useful alternatives.

For the second argument consider crime 5 . NedExplain returns c ( C 1 sector W ). The explanation of Ted++ does not contain the atomic explanation c 1 , but there exist combinations including c a part, like c 15 . This means that the explanation by NedExplain is incomplete; a repair attempt of c 1 alone will never yield the de-sired results. Similarly, crime 7 illustrates a case, when the Why-Not algorithm produces an explanation ( c 3 ) that misses some parts. Then, in gov3 NedExplain and Why-Not both return c 2 . However, let us now assume the developer prefers to not change this condi-tion. Keeping in mind that those algorithms X  answers may change when changing the query tree, she may start trying different trees to possibly obtain a Why-Not explanation without c 2 . Knowing the explanation of Ted++ prevents her from spending any effort on this, as it shows that all explanations contain c 2 as a part.
By mapping the explanation of Ted++ to sets of explanations, the usefulness of the coefficients of the polynomial has been neglected. For example, the Why-Not explanation polynomial of crime 8 is 2384  X  c 23 + 20  X  c 3 + 4  X  c 1 + 8  X  c 2 . Assume that the developer would like to recover at least 5 missing tuples, by changing as few conditions as possible. The polynomial implies to change either c or c 2 : they both require one condition change and provide the possibility of obtaining up to 20 and 8 missing tuples, respectively. c can recover up to 4 tuples, whereas c 2 c 3 require two condition changes. Clearly, the results of NedExplain or Why-Not are not informative enough for such a discussion. Ted++ vs. NedExplain and Why-Not. Fig. 5 shows the run-times in logarithmic scale for each algorithm and scenario. We observe that Ted++ and NedExplain are comparable and that in some cases, Ted++ is significantly faster than Why-Not.

Why-Not traces compatible tuples based on tuple lineage stored in Trio. As already stated in [3, 7], this design choice slows down Why-Not. On the contrary, both NedExplain and Ted++ compute Table 4: Ted++ , Why-Not, NedExplain answers per scenario Figure 5: Runtime for Ted++ , Ted, NedExplain, and Why-Not compatible data more efficiently. We claim that a better implemen-tation choice for tuple tracing in Why-Not would yield a runtime comparable to NedExplain, a claim backed up by their comparable runtime complexities. Another problem of NedExplain and Why-Not lies in the choice to trace compatible data w.r.t. tuples from the input relations but not necessarily compatible ones.

Let us see what happens when Ted++ is slower than -but still comparable to -NedExplain, for example in gov 1 -gov 3 . In Ned-Explain all compatible tuples are pruned out by conditions very close to the leaf level of the query tree, so the bottom-up traversal of the tree can stop very early. Ted++ always  X  X hecks X  all condi-tions so cannot benefit from such an early termination. However, this runtime improvement of NedExplain often comes at the price of incomplete explanations (e.g., gov 1 ).
 Ted++ vs. Ted. Fig. 5 reports runtimes for Ted on 6 out of 15 sce-narios as for the others, Ted runs out of time. To examine this be-havior, we compare the time distribution in Ted and Ted++ (Fig. 6). The algorithms are divided in four common phases. Note that in scenarios crime 7 , gov 1  X  gov 3 the diagram bars for Ted are not totally displayed as the execution time is much higher compared to the other scenarios and to the runtime of Ted++ (the runtime of the coefficientEstimation phase is the label on the respective bars) .
As said in Sec. 4, Ted X  X  main issue is its dependence on the num-ber of compatible tuples. This is experimentally observed in Fig. 6: with the growth of the set of compatible tuples, the time dedicated to coefficientEstimation also grows (the scenarios are reported in an ascending order of number of compatible tuples). The number of compatible tuples affects Ted++ too, but not as much. This can be seen in crime 8 and crime 7 , or gov 3 and gov 1 ; while the number of tuples grows, Ted++  X  X  runtime remains roughly steady.
We now study Ted++  X  X  behavior w.r.t. the following parameters: (i) the type (simple or complex) of the input query Q and the num-ber of Q  X  X  conditions, (ii) the type of the Why-Not question (simple
Figure 7: Ted++ runtime w.r.t. number of conditions in Q or complex) and the number and selectivity of conditions the Why-Not question involves, and (iii) the size of the database instance I . Note that (ii) and (iii) are tightly connected with the number of compatible tuples, which is one of the main parameters influencing the performance. Another important factor is the selectivity of the query conditions over the compatible data.

For the parameter variations (i) and (ii), we use again the crime , imdb , and gov databases. To adjust the database instance size for case (iii), we use data produced by the TPC-H benchmark data gen-erator (http://www.tpc.org/tpch/). We have generated instances of 1GB and 10GB and further produced smaller data sets of 10MB and 100MB to obtain a series of datasets whose size differs by a factor of 10. In this paper, we report results for the original query Q3 of the TPC-H set of queries. It includes two complex and three simple conditions, two of which are inequality conditions. Since the origi-nal TPC-H query Q3 is an aggregation query, we have changed the projection condition. The queries used in this section are Q (Tab. 2) and the scenarios are crime s -tpch c (Tab. 3). Adjusting the query. Given a fixed database instance and Why-Not question, we start from query Q1 and gradually add simple conditions, yielding the series of queries Q1, Q2, Q s 3 , Q evolution of Ted++ runtime for these queries is shown in Fig. 7 (a). Similarly, starting from query Q j , we introduce step by step com-plex conditions, yielding Q j -Q j 4 . Corresponding runtime results are reported in Fig. 7 (b).

As expected, in both cases, increasing the number of query con-ditions (either complex or simple) results in increasing runtime. The incline of the curve depends on the selectivity of the intro-duced condition; the less selective the condition the steeper the line becomes. This is easy to explain, as the view for the explanations involving a low selective condition contains more tuples (=passing partial tuples). This, leaves space for further optimization by dy-namically deciding on passing vs eliminated tuples materialization. Adjusting the Why-Not question. The scenarios considered for Fig. 8 (a) have as starting point the simple Why-Not question of crime 5 (see Tab. 3). Keeping the same input instance and query, we add attibute-constant comparisons (i.e., simple conditions) to WN , resulting in fewer tuples in each step. As expected, the more conditions (the less tuples) the faster the Why-Not explanation is returned, until we reach a certain point (here from crime 5
Figure 8: Ted++ runtime w.r.t. number of conditions in WN From this point, the runtime is dominated by the time to communi-cate with the database that is constant over all scenarios.
In Fig. 8 (b) we examine complex Why-Not questions. As we add complex conditions in a Why-Not question, the number of generated partitions (potentially) drops as more relations are in-cluded in a same partition. To study the impact of the induced number of partitions in isolation, we keep the number of the com-patible tuples constant in our series of complex scenarios ( imdb imdb cc 3 ). The number of partitions entailed by imdb cc and imdb cc 3 are 3, 2, and 1, respectively. The results of Fig. 8 (b) confirm our theoretical complexity discussion, i.e., as the num-ber of partitions decreases, the time needed to produce the Why-Not explanation increases.
 Increasing size of input instance. Now we increase the database size for scenarios with one simple or one complex Why-Not ques-tion WN , over the same query Q tpch . The simple WN includes two inequality conditions, in order to be able to compute a rea-sonable number of compatible tuples. The complex WN contains one complex condition, one inequality simple condition and one equality simple condition. It thus represents an average complex Why-Not question, creating two partitions over three relations.
Fig. 9 (a) shows the runtimes for both scenarios. The increas-ing runtime is tightly coupled to the fact that the number of com-puted tuples is augmenting proportionally to the database size, as shown in Fig. 9 (b). We observe that for small datasets (&lt;500MB) in the complex scenario Ted++  X  X  performance decreases with a low rate, whereas the rate is higher for larger datasets. For the simple scenario, runtime deteriorates in a steady pace. This behavior is aligned with the theoretical study; when the number of partitions is decreasing the complexity rises.

In summary, our experiments have shown that Ted++ generates a more useful and complete Why-Not explanation than the state of the art. Moreover, Ted++ is competitive in terms of runtime. The dedicated experimental evaluation on Ted++ verifies that it can be used in a large variety of scenarios with different parameters. Finally, the fact that the experiments were conducted on an ordinary laptop supports Ted++  X  X  feasibility.
This paper provides a framework for Why-Not explanations based on polynomials, which enables to consider relational databases under set, bag and probabilistic semantics in a unified way. To efficiently compute the Why-Not explanation polynomial under set semantics we have designed a new algorithm Ted++ , whose main feature is to completely avoid enumerating and iter-ating over the set of compatible tuples, thus significantly reducing both space and time consumption. Our experimental evaluation showed that Ted++ is at least as efficient as existing algorithms while providing useful insights in its Why-Not explanation for a developer. Also, we show that Ted++ scales well w.r.t various pa-rameters, making it a practical solution.

Why-Not explanation polynomials are easy to extend for unions of conjunctive queries, whereas an extension for aggregation queries is subject to future work. Currently, we have been working on exploiting the Why-Not explanation polynomial to efficiently rewrite a query in order to include the missing answers in its re-sult set. As there are many rewriting possibilities, we plan to se-lect the most promising ones based on a cost function, built with the polynomial. For instance, we may rank higher rewritings with minimum condition changes (i.e., small combinations), minimum side-effects (i.e., small coefficients), etc.
