 In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid. Our approach uses the Expectation-Maximization (EM) algorithm to recover both the mean curve shapes for each cluster, and the most likely shifts, offsets, and cluster memberships for each curve. We demonstrate how Bayesian estimation methods can im-prove the results for small sample sizes by enforcing smooth-ness in the cluster mean curves. We evaluate the methodol-ogy on two real-world data sets, time-course gene expression data and storm trajectory data. Experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets. The proposed approach pro-vides a non-parametric, computationally efficient, and ro-bust methodology for clustering broad classes of curve data. I.2.6 [ Artificial Intelligence ]: Learning curve clustering, alignment, transformation invariance, EM, mixture model
Clustering is widely used as a general technique for ex-ploring and understanding large data sets. Curve cluster-ing focuses on the clustering of sets of variable-length se-quences that consist of (possibly multidimensional) mea-surements observed over an independent variable such as time. An example we will discuss later in this paper is trajectories of cyclones, where each trajectory consists of a short 3-dimensional time-series, the dimensions being x-y spatial coordinates and cyclone intensity. Other examples  X  Copyright2003ACM1-58113-737-0/03/0008... $ 5.00. Figure 1: Curves of cyclone intensities from genesis to death. of curve data sets include time -course measurements from sets of genes [12], estimated trajectories of individuals or vehicles from video data [15], and biomedical measurements of the response of different individuals to drug therapy over time [21].

A practical problem with such data is that the curves tend to be misaligned in various ways. For example, Fig-ure 1 shows a set of curve data that represents the inten-sity of individual cyclones over their respective lifetimes. These curves can be made more similar by shifting individ-ual curves along the time axis. The lack of alignment here is both an artifact of the methods used to extract these curves (from algorithms that track cyclone centers in sea-level pres-sure data [17]), as well as being due to natural variability in the underlying dynamic processes generating the data. Other types of transformations may also have been applied to observed curve data, such as offsets and scaling in the ob-served measurements or more complex forms of non-linear warping of the signal. Clustering in this context can be prob-lematic due to the  X  X hicken and egg X  problem: we may not be able to effectively cluster the data without first removing the transformations, but on the other hand we may not be able to effectively remove the transformations without first clustering the curves.

One approach is to preprocess or post-process the sets of curves by employing alignment techniques such as dynamic time-warping before or after clustering [37, 38]. The dis-advantage of such an approach is that the discovery of the curve transformations and curve clustering are decoupled from each other, which can weaken the ability of a cluster-ing algorithm to detect structure in the data.

In this paper we address this specific problem, namely si-multaneously clustering and  X  X etransforming X  sets of curves. The focus of the paper is the development and evaluation of learning algorithms that can recover both the cluster mod-els as well as the most likely alignments for each curve, given sets of observed curves that are assumed to be gen-erated from a particular class of generative models. The specific class of curve transformations we address includes discrete-valued shifts along the time (or independent vari-able) axis and real-valued additive offsets in each of the mea-surement (dependent variable) axes. Extensions to include other forms of transformations such as multiplicative scal-ing of the curve measurements or more general nonlinear deformations (such as non-linear warping of the time axis) can also be handled in principle within the mixture-based framework we propose but are not specifically addressed in this paper.

We will assume that we can achieve useful results by re-stricting attention to shifts t hat are  X  X n-grid X  in terms of the independent variable, i.e., that shifts are constrained to occur on the same sampling grid that the data are mea-sured on. This is in direct contrast to  X  X ff-grid X  methods that interpolate between the gridded observations, such as polynomial or spline models. The advantages of the on-grid approach (as we will see later in the paper) are that (a) we can use a completely non-parametric model for the mean curves within each cluster by avoiding parametric assump-tions on the interpolating function, and (b) we get a com-putationally feasible procedure for solving the joint cluster-ing/transformation problem. Of course for certain applica-tions (for example when data are very sparse for each curve) the interpolative (or functional modeling) methods might be more appropriate. In this paper, however, the focus is on the  X  X n-grid X  class of modeling techniques. Experimental results later in the paper bear out that substantial and sys-tematic improvements in modeling power can be gained by the  X  X n-grid X  approach alone.

The advantages of a probabilistic approach to clustering are well-known, for example the ability to estimate the most likely number of clusters given the data [35] or to add back-ground clusters to account for outliers in a systematic man-ner [13]. For curve clustering with transformations the prob-abilistic approach to clustering is particularly valuable since it allows us to directly address the afore-mentioned  X  X hicken and egg X  problem by treating the transformations as hidden information over which we can learn distributions informed by the observed data. Furthermore, we can couple the learn-ing of these distributions over hidden transformations to the problem of learning cluster models.

The paper is organized as follows. Section 2 introduces some basic terminology for the paper and Section 3 discusses relevant prior work in this area. Section 4 introduces the ba-sic model and parameter estimation methods, and Section 5 then extends this model to a hierarchical Bayesian approach. Section 6 defines the experimental methods that we use to evaluate our proposed clustering methodology. Sections 7 and 8 describe experimental results with real datasets: time-course gene expression data and trajectories of extra-tropical cyclones (ETCs). Section 9 summarizes and concludes the paper.
We will use the term  X  X urve X  to denote a variable-length series of data measurements observed as a function of some independent variable such as time. More generally, each in-dividual  X  X urve X  can consist of set of multi-dimensional (vec-tor) measurements as a function of the independent variable, rather than a single measurement. In statistics, such data are sometimes referred to as  X  X unctional data X  [33], empha-sizing the fact that the observed data are functions of an independent variable.

The term  X  X easurement offset X  will be used to refer to a real-valued scalar added to all of the measurements in a curve X  X r a vector of reals, one for each measurement di-mension if the curve has multiple measurement dimensions.
For simplicity of notation we will refer to the indepen-dent variable as time, although in general it could refer to position or some other sequential ordering. We will use the term  X  X ime-shift X  to refer to an integer-valued global shift of the curve measurements (simultaneously in all dimensions) along the time-axis (the independent variable axis).  X  X lignment X  (in this paper) refers to the process of find-ing estimates for either (or both) measurement offsets and time-shifts for specific curves. Thus, we could refer to (for example)  X  X lignment with time-shifts X  or  X  X lignment with both time-shifts and measurement offsets X . For example, could represent the alignment of curve Y i intoanewcurve Y i where  X  i is an integer time-shift and  X  i is a real-valued scalar (or a vector of reals if Y is a vector measurement), and i is an index on curves.
The primary novel contribution of this paper is the learn-ing of curve clusters and curve alignments within a sin-gle unified framework. While there has been a significant amount of prior work on each topic in isolation (as we dis-cuss below) there has been no work that we are aware of that addresses simultaneous clustering and alignment of curve data.

Clustering of curves of equal length L can be achieved by representing the curves in a vector space of dimension L and using standard multivariate clustering techniques. For example, one can use K-means [20] or the model-based clus-tering approach of Gaussian mixtures [2]. More general tech-niques perform curve clustering directly in the curve space and, thus, can handle curves of variable lengths (for exam-ple) in a natural manner. These techniques have their ori-gins in regression-based mixture models that focus on the finding of two or more underlying functions (e.g., polyno-mials) from which the observed data might have been gen-erated. These methods are often referred to as regression mixtures [10, 22, 15] and they extend the standard fixed-dimensional unconditional mixture framework to the case where component density models are replaced with condi-tional regression density models. More recent work along these lines focuses on learning individual models for each curve during the clustering. This can be handled for exam-ple, through the integration of linear random effects models [24] with regression mixtures [17, 25].

Further extensions have been developed that use non-parametric models for the mean curves, such as the kernel regression models in [15] and the mixtures of splines in [21]. The clustering component of our proposed approach can be loosely considered to fall within this class of non-parametric curve clustering techniques X  X e use a model that allows for individually varying means at specific time points but where the means are loosely coupled together via a smooth-ing prior.

None of the above work on clustering addresses the issue of curve alignment. However, there is a considerable body of prior work on curve alignment (without clustering) in various forms under many names: time warping [37], curve registration [32], structural averaging [23], and image regis-tration or point-set matching as commonly used for image matching [18].

One area where there has been some success to date in simultaneous registration and clustering is with image data. Frey and Jojic [14] propose the use of EM to learn mixtures of images subject to various forms of linear transformations. Chui et al. [7] use the ideas of transformation invariant dis-tance measures proposed in [19] and develop deterministic annealing algorithms for simultaneous clustering and match-ing of point sets. In some sense these approaches represent a two-dimensional version of what we propose here, albeit with somewhat different goals. The novelty of our approach lies in the application and extension of mixture modeling to the specific problem of translation-invariant curve-clustering. In addition we demonstrate how these techniques can be ap-plied to two real-world scientific data analysis problems in-volving gene expression data and cyclone trajectory data. In related work [5] we have shown how a general Bayesian network framework can be used for simultaneous local (non-linear) time-warping and clustering of curve data.
In this section, we describe the proposed generative model for simulating multidimensional curves observed on a sub-set of a fixed time grid. We implicitly assume that the grid points (measurement times) on the time-grid are equally spaced. In theory this assumption could be relaxed by treat-ing data on non equi-spaced grids as observed samples from an assumed (but hidden) higher-resolution equi-spaced grid and making inferences accordingly. However, we do not pur-sue this in this paper, and focus here on data sets involving equi-spaced measurements along the time axis.

In many situations the curves are univariate, for example, gene expression measurements. In applications like object tracking, however, there can be multiple measurements per time point, e.g., the estimated 2D or 3D location of the ob-ject and possibly other features of the object such as shape, color, mean intensity and so forth. Our generative model is designed to work with multidimensional curves. Throughout this paper we will use the term  X  X urve X  when referring to a series of D -dimensional observations of an object on a time grid. The number of D -dimensional observations (the length of the curve) can be different for different objects. In the approach taken here we do not explicitly model the lengths of the observed curves X  X e specify the model for generating curves conditioned on the curve lengths (this conditioning is implicit and not explicitly written out in all likelihood expressions below).

We begin with a standard mixture model with K com-ponents to allow for heterogeneity in the generated curves [28]. The probability of an individual curve Y i given a set of model parameters  X  is defined as where  X  k is the probability of component k and Z i is a random variable indicating cluster membership for curve Y We will denote the vector of measurements over time in the d th dimension for the i th observation by Y d i .

To generate curves with relative time shifts, we introduce a latent variable  X  i that denotes the amount of shifting on the time grid for observation Y i and takes integer values from a fixed range [0 ..M ]. Therefore, where  X  k,m is the probability of time shift m in component k :  X  k,m = P (  X  = m | Z = k ).
 If the curve Y i is observed on the entire time grid of length T (where T can be thought of as the maximum length of a curve, e.g., the length of the longest observed curve plus the maximum allowed time shift), we can represent it as apointina T -dimensional space, and model its distribu-tion as a multivariate Gaussian with mean  X  k and diagonal covariance matrix C k . The assumption of a diagonal co-variance structure is equivalent to assuming independence of measurements at different time points given cluster mem-bership, proper alignment, and parameters of the cluster. In the next section we extend this model to handle smooth-ness constraints on the neighboring means of the Gaussians by employing a hierarchical Bayesian model that constrains neighboring means to be similar to each other.

We assume that within each component the measurements taken in different dimensions are conditionally independent, providing a relatively simple but often effective way to han-dle multi-dimensional data:
Given the cluster membership Z i and time shift  X  i of a partially observed curve Y i of length L i &lt;T ,wecancalcu-late its probability under a T -dimensional Gaussian distri-bution by integrating out unobserved measurements. The integration is trivial due to the diagonal covariance struc-ture, and we obtain an L i -dimensional normal density.
To allow real-valued offsets in the measurement space (note that this is different from the time-shifts discussed above), we define a likelihood that is invariant to transla-tions of the axes. Namely, the conditional probability of curve Y d i is given by a Gaussian density with mean  X  d k covariance C d k , evaluated at translated curve Y d i  X   X  value of the offset  X  d i is chosen so that the translated curve is best aligned with the corresponding portion of the mean curve under some norm. Note that the value of the offset  X  i depends on the particular cluster that we align with, as well as the assumed time-shift, and thus standard meth-ods of dealing with offset translations (such as subtracting the mean value) are neither applicable nor optimal in this context. The likelihood of a single curve in dimension d is defined as  X  i ( k, m )=argmin  X  Y d i where a m denotes the point on the time grid corresponding to the m th time-shift. We use the Euclidean norm in Equa-tion 2, but other notions of similarity could equally well be used to define the best offset  X  in the measurement space, perhaps based on prior knowledge of the process generating the data.

Conditioned on the length of the curve, Equations 1 through 2 provide a generative model that can be used to simulate spatially offset  X  X nippets X  of a variety of curve prototypes. These snippets start from different initial points in time, have different lengths, and hav e arbitrary offsets in the mea-surement space. The simulation has four stages:
We employ the expectation maximization (EM) algorithm [9] to learn maximum likelihood estimates of the model pa-rameters from the observed curves, as is usual for models with latent structure. The algorithm starts from a random initialization of parameter values and proceeds by alternat-ing the following two steps:
The time complexity of a single iteration for this algorithm is linear with respect to each of the following: the number of curves N ,thesizeofthetimegrid T , the dimensionality of the curves D , the number of clusters K ,andthemaxi-mum amount of shifting allowed M , i.e., it is O( NTDKM ). This is only a factor of M more expensive than EM for regu-lar mixture of Gaussians with diagonal covariance matrices. Note in particular that the O ( N 2 ) computation of calculat-ing all pairwise alignments curves is avoided. The proposed method aligns observed curves only with the current esti-mates of the cluster means. Full details of the E-Step and M-Step equations are provided in [6].
In the models above, the consecutive means (e.g., in time) of the Gaussians are unconstrained. While this provides the flexibility of non-parametric modelling, it also ignores phys-ical constraints and reasonable prior expectations about the smoothness of the mean curves. It is natural to employ Bayesian ideas in this context by introducing prior distri-butions on the parameters of the models that favor smooth solutions.

We begin with the conventional factorization of the prior distribution as typically used in Bayesian analysis of Gaus-sian mixture models in a standard  X  X on-curve X  setting (e.g., [11] and [34]):
We use conjugate prior distributions for the Gaussian noise covariance matrices C , component weights  X  , and shift prob-abilities  X  .

In standard multivariate mixture modeling it is reasonable to assume that the means  X  d k ( j ) are independent from each other. Here, however, we specifically want to couple the means and to that effect we introduce a prior distribution on the means of the components that correlates the means at time t +1and t .

We propose a hierarchical Bayesian model, where the first level of the hierarchy introduces dependence between the means at consecutive time points P (  X  ( t +1) |  X  ( t ) , X  the second level controls the degree of smoothness in the means P (  X  2 ). Similar  X  X moothing priors X  have been suc-cessfully used in computer tomography, medical imaging and image processing in general to enforce smoothness in the estimated signal (see [30] and [27] and references therein). Specifically, at the first level we assume a Gibbs prior such that the mean  X  d k ( t +1)attime( t +1) in cluster k and di-mension d is apriori normally distributed around the mean at time t within the same cluster with some variance [  X  d
We have also investigated a more complex Markov ran-dom field prior that couples each point with both neighbors. While we found that this prior often leads to improvement in prediction performance, it requires more complex param-eter estimation techniques and we do not pursue the details of this approach any further in this paper.

At the second level of the hierarchy, we assume that the parameters [  X  d k ] 2 that control the degree of smoothness are generated from some common distribution, P ([  X  d k ] 2 ). We use a conjugate prior, and model the precision 1 [ by  X  d , as a Gamma distribution with parameters ( A d  X  ,B
We use conjugate Gamma priors for the diagonal covari-ance terms in the Gaussian mixture components and Dirich-let priors for the mixture component probabilities and time shift probabilities within the clusters.
Having specified prior distributions for all parameters of the model, we obtain point estimates of the parameters and the hyperparameters using a Gibbs sampling approach. This is commonly done for models with hyperparameters [31, 26]. The algorithm iterates between updating the parameters of the model (cluster means, covariance matrices, time shift and component probabilities) given current values of the hy-perparameters, and then updating the posterior distribution of hyperparameters given all other parameters in the model. Specifically, we iterate through the following steps: 1. Sample values of hyperparameters [  X  d k ] 2 from the cor-2. Use the EM algorithm to find maximum a posteriori 3. Update the posterior distribution of the hyperparame-4. Sample values for the hyperparameters [  X  d k ] 2 from their 5. Repeat steps 2 through 4.

MAP estimates of the parameter values result from in-cluding log P ( X  |  X  2 ) in the M-step of the EM algorithm. Closed-form solutions for the parameter values are no longer possible in this case, since we have introduced a dependence between consecutive means. The maximization can be re-duced to iteratively solving a tri-diagonal linear system of equations, until a fixed point is reached. Finding a single M-step solution usually converges within the first few iterations when the initial approximation is taken to be maximum like-lihood estimates of the parameter values. Full details are provided in [6].

The parameters A and B of the hyperprior allow us to express belief about the expected difference between consec-utive means and to control the strength of the prior. The smaller the variance of the hyperprior, the stronger the effect of the prior in the sense that the model is more constrained to find estimates of the parameters that agree with the cor-responding mean value of  X  2 . Other estimation strategies, such as empirical Bayes methods, could also be used to set the value of the hyperprior mean.
The quality of a clustering algorithm can be character-ized empirically for a given data set using a number of dif-ferent measures. For example, a particularly useful feature of the probabilistic clustering approach is that it provides a full density function for the data, allowing one to ob-jectively compare different models and methods on out-of-sample data X  X etter models should yield higher probability for unseen data. Note that more complex models will not necessarily produce better predictions out of sample, simply by virtue of having more degrees of freedom. In fact, be-cause of the usual bias-variance trade-offs, unless the more complex models can offset the increased variance in param-eter estimation with lower bias, the more complex models will perform worse out of sample relative to simpler mod-els. In this context the experiments below are intended to empirically answer the question of whether the additional model complexities proposed in this paper are validated by producing better predictions on test data.

The specific measures we use in this paper for evaluation include
Experimental results using synthetic data are reported in detail in Chudova et al. [6] but are omitted here for space reasons. The primary result fr om these experiments is that on simulated noisy curve data with noisy offsets the EM-based methods in this paper were substantially more accu-rate at recovering the true underlying cluster structure when compared to more traditional clustering algorithms such as Gaussian mixtures or K-means.
Time-course gene expression data consists of expression levels from a set of genes measured at different time points X  each curve (also sometimes ref erred to as a  X  X rofile X ) con-sists of the intensity measurements from a specific gene over time. Clustering is an important tool for analyzing gene expression data since for many expression sets it is hypothe-sized that there exists different groups (or clusters) of genes with different dynamic behaviors, but where the behavioral characteristics of genes within each clusters are relatively homogeneous and correlated [12, 39]. Moreover, in cer-tain situations subsets of genes are hypothesized to exhibit  X  X aster-slave X  or  X  X eader-follower X  relationships. To a first approximation expression levels of the slave genes can be reasonably assumed to follow that of their master gene with Figure 2: Time-course curves from the gene expres-sion data set. an unknown time lag. It is therefore desirable that a cluster-ing algorithm that detects patterns of distinct behavior in such data is able to ignore the differences in profiles that are explained by simple shifts of the time axes. Analysis of the genes that resemble each other X  X  expression levels may reveal very useful information about regulation mechanisms. For example, finding sets of genes that exhibit  X  X aster-slave X  relationships can provide a starting point for building more realistic models of regulatory behavior.

Curve clustering is a useful methodology in this context for analysis of time-course genomic data, from exploratory data analysis to model-building of regulatory networks. Much of the previous work on gene expression clustering has fo-cused on non-probabilistic techniques such as hierarchical clustering methods [12]. Time-curve alignment, via tech-niques such as dynamic time-warping, is often applied sepa-rately from any clustering of the data [1]. In terms of prob-abilistic clustering, Bar-Joseph et al. [3] use mixtures of splines (defined in continuous time) with random effects to cluster time-course profiles. They also align the curves, but the alignment is again carried out independently from the clustering. Standard Gaussian mixture models have also been applied to gene profile clustering with some success, but without curve alignment [39, 29].

In the results presented in this section we integrate align-ment and clustering of expression data using the methods presented earlier in the paper. We might speculate that the goal of producing clusters that are invariant to time-shifts is somewhat different from conventional clustering in that the two methodologies enable us to answer slightly different scientific questions about the data. For example, traditional clustering (no time-shifts) allows one to identify groups of genes that peak at the same interval, while shift-invariant clustering recovers groups of genes that participate in simi-lar regulatory patterns that unfold along the time axis.
For our experiments we used normalized gene expression measurements (log-ratios) of the activity of cell cycle-regulated genes in yeast. The data set contains time course measure-ments for 800 genes in yeast Saccharomyces cerevisiae iden-tified as cell cycle-regulated based on analysis provided in [36]. Specifically, we use the alpha arrest data that captures gene expression levels at 7 minute intervals for two consec-Figure 3: Within-cluster mean variance (out-of-sample) for methods with and without time-shifts on gene expression data. utive cell cycles, for a total of 17 measurements per gene (Figure 2). There are no missing measurements in any of the curves in this data set. The goal is to discover equiva-lence classes of genes such that the genes in the same class exhibit similar behavior subject to translation in the time axes. We did not use measurement offsets in the experiments for this data set since we cluster normalized log-ratios of the true measured intensities (a standard pre-processing step for gene expression data [36]). The analysis of the log-ratios in the yeast cell cycle data set [39] suggests that the log-ratios are better modeled by the normal distribution than the raw intensities.

Figure 3 shows the difference in the within-cluster mean variance (out-of-sample) for the different models evaluated both with and without alignment in the time axis as the number of clusters K is varied. The mixture model with time-shift alignment systematically produces the most com-pact clusters. We used models that allow symmetric shifts of up to M = 2 time steps to the left and to the right, for a total of 5 possible alignments for each curve. Increasing M to  X  3 does not result in large imp rovements in performance as the maximum possible shift approaches the length of the cell cycle, i.e., the period of the measured curves.
Figure 4 plots the out-of sample logP scores for the same models as in Figure 3. Each score was obtained by 10-fold cross validation; higher scores indicate better fit of the model to the data. As expected, the score starts to climb quickly for small values of K , and then flattens out (with some ev-idence of a decrease) after K  X  20. Again, models with alignment provide systematically better density estimation performance (as measured by logP) than those models that do not.

Figure 5 shows the cross-valid ated mean squared-error of predicting every measurement within each observed curve, given previous measurements up to the time of prediction. As shown, the models with time-shift alignment lead to a reduction in prediction error. There was little difference in the prediction quality of the models with M =  X  2and M =  X  1.

We also evaluated the impact of using priors on the quality of the solution as a function of the size of the training data. Figure 6 shows the average decrease in logP score on a vali-Figure 4: Cross-validated logP scores on gene ex-pression data with V =10 . Figure 5: Cross-validated MSE of one-step-ahead prediction on gene expression data. Figure 6: Average improvement in out-of-sample logP scores for various tr aining sample sizes using Bayesian estimation on gene expression data with K =10. dation set for models trained on sub-samples of the full gene expression data set. The numbers are presented relative to the logP score of the best model trained on the full data set. Figure 6 shows the performance of two models, both with K = 10 and maximum time shift M =  X  2. The first model was fit using maximum likelihood estimation while the sec-ond one was fit using MAP estimates obtained by the Gibbs sampling approach described in Section 5. Each point on the plot is an average decrease in the logP score over 10 dif-ferent train/validation splits. For a single fixed validation set of size n = 200 curves, we incrementally created training sets ranging in size from 60 to 600 curves, so that each train-ing set contains all training sets of smaller sizes. The x -axis indicates the amount of data that was used for training while the y -axis indicates the difference in logP score (relative to the best model overall) due to limited training data. The results show that the Bayesian methods provide systemati-cally better density estimation performance. The improved performance is particularly noticeable at very small sam-ple sizes agreeing with our general intuition about Bayesian estimation.

We have also performed experiments on two other gene expression data sets described in [36]. These data sets in-clude measurements of the same set of genes but under a different set of initial conditions. On these data sets, we see similar improvements in terms of logP scores for small to medium values of K (e.g., up to 20 or so). For larger values of K there is still a systematic improvement due to time-shifting, albeit a smaller improvement than that obtained in Figures 4 and 5.
We also applied our methodology to clustering of ETC (Extra-Tropical Cyclone) track s or trajectories obtained from gridded records of sea-surface pressure data over time. At-mospheric scientists are interested in the spatio-temporal patterns of evolution of ETCs for a number of reasons. For example, it is not well-understood how long-term climate changes (such as global warming) may influence ETC fre-quency, strength, occurrence and spatial distribution. Also of concern is how changes in ETC patterns may in turn in-fluence long-term climatic processes.

Much work in this area is spent on the identification and tracking of ETCs which results in a set of cyclone trajec-tories. The trajectories consist of sequences of latitude, longitude, and intensity tuples observed over time. Clus-tering is usually performed on just the latitude-longitude position measurements over time. For example, Blender et al. [4] convert the two-dimensional lat-lon trajectories into fixed-dimensional vectors for clustering by the K-means al-gorithm. In our prior published work on this data we have focused on parametric model-based regression mixtures with no translation modelling [16, 17].

This prior work has not addressed the problem of simulta-neously aligning and clustering cyclone trajectories. As was shown earlier in Figure 1, this is an important concern.
The cyclone dataset used in this paper consists of 614 cyclones tracked over the North Atlantic (see [16] for full details). Each trajectory consists of a variable length se-quence of latitude, longitude, and intensity measurements observed over 6-hour intervals. Figure 7 shows some of the Figure 7: A subset of the North Atlantic cyclone trajectory data. Figure 8: Cross-validated logP scores on cyclone data. cyclones in this dataset mapped over the North Atlantic.
It is hypothesized by atmospheric scientists that there are subgroups of cyclones, where each subgroup has distinct dy-namic behavior [4]. This suggests clustering the cyclones in latitude-longitude space to capture similar spatial pat-terns, and clustering in intensity space to capture similar intensification patterns over time. To achieve some degree of translation invariance in lat-lon space, clustering of storm trajectories in the atmospheric science literature is typically carried out by subtracting the first lat-lon observation of each cyclone from its entire trajectory (e.g., [4]), which we will refer to as first-observation alignment (in measurement space). This can be problematic if the cyclones themselves are shifted in time. Shifts in time cause cyclones to be aligned to incorrect starting observations since the observed starting measurements are not the actual starting observa-tions. The methodology proposed in this paper allows for translation in time via time-shifts as well as for translation in the lat-lon measurements via measurement offsets. This alignment is carried out in a data-driven manner as an in-tegrated part of the clustering process. Note that in the intensity measurement dimension for this data we allow for time-shifts, but we do not allow intensity measurement off-sets since there is no a priori reason to believe that the intensities are offset relative to each other.
We compare the clustering performance of four different Figure 9: Average within-cluster variance on cy-clone data. methods on the cyclone data: (1) standard Gaussian mix-tures with simple first-observation alignment, (2) Gaussian mixtures where lat-lon measurement offsets are estimated via EM, (3) method 2 followed by a simple one-pass within-cluster time-shift alignment, and (4) joint clustering and alignment (in both time and measurement axes) using EM, as proposed in the paper. The one-pass alignment tech-nique of Method 3 takes each of the returned clusters (in turn) from Method 2 and runs Method 4 (within each cluster with the number of clusters set to 1), separately aligning the clusters in time. For the purposes of presenting the figures we denote Method 1 as [  X  ] (which denotes no alignments at all), Method 2 as [ X, Y ] (which denotes separate offsets esti-mated for both the latitude and longitude measurement di-mensions, i.e. X and Y ), Method 3 as [ X, Y ] , [ T ](whichde-notes measurement offsets are first estimated simultaneously in latitude and longitude space during clustering, and then followed by estimation of time-shifts after clustering), and Method 4 as [ X, Y, T ] (which denotes alignments that are simultaneously estimated in latitude, longitude, and time, all during clustering).
 Figure 8 shows a plot of logP scores for these methods. The scores are obtained using 10-fold cross-validation with the number of clusters varied from 3 to 9. All methods are allowed 10 random starts of EM at each fold and initial-ization is carried out by selecting K random curves as the initial K cluster means.

Method 1 performed poorly enough that its logP scores are not included in the figure. Method 2 performs much better than Method 1 because of its data-driven alignment in measurement space. Method 3 improves on Method 2 by taking the curves within each cluster in Method 2 and aligning them in time. Finally, the  X  X ull X  joint clustering and alignment method (Method 4) systematically outperforms all of the others. Of note, is that our proposed method 4 results in a higher score at K = 3 than the standard method (method1)evenwhenitisallowedtoreach K =9.

Figure 9 shows the mean within-cluster variance for Meth-ods 2, 3, and 4, over the same K -values, trained on all of the cyclone data. This plot again demonstrates the superior performance of Method 4 in that it finds the most compact clusters.

Figure 10 shows the types of clusters that each method discovers. The measured cyclone trajectories in lat-lon space, clustered and aligned by the corresponding algorithms, are shown by black curves; the thick grey line in the center of each cluster corresponds to the estimated cluster mean curve. The results were obtained by running Methods 1, 2, and 4 on the complete set of cyclone data with the number of clusters set to 5. We display (from left-to-right) the five clusters from Method 1 in column one, those for Method 2 in column two, and Method 4 in column three. The elements in each column were chosen so as to line-up similar looking clusters across the rows.

The clusters in column one all emanate from a common genesis point. This is due to the first-observation alignment enforced by Method 1. In contrast, Method 2 (column 2) allows for a data-driven alignment in measurement space and thus the clusters in column two seem more natural than those in column one. The full joint clustering and alignment of Method 4 appears to produce in even more compact and distinct clusters, as shown in column three. From this pic-ture it is reasonable to suggest that we are getting a cluster-ing that is more interpretable from a scientific point-of-view.
In this paper we addressed the general problem of cluster-ing multi-dimensional curve data where we allow for curve-specific shifts in both the independent variable (typically time) and the measurement variables. We proposed a gen-eral mixture model framework for this problem and demon-strated on two real-world data sets that the methodology systematically leads to lower variance clusters (compared to ignoring alignments), better predictions in terms of both density estimation and mean-squared error on unseen curves, and generally leads to more interpretable results (which is important from a scientific viewpoint). Space limitations prevented a full discussion of many other aspects of this problem. For example, it is quite easy to allow for multi-plicative amplitude scaling using this same mixture frame-work and our experiments to date indicate that it also leads to systematically better clustering results. Non-linear de-formations, such as non-linear warping of the time-axis [5], are also a natural extension of the methods proposed in this paper. [1] J. Aach and G.Church. Aligning gene expression time [2] J.D.BanfieldandA.E.Raftery.Model-based [3] Z. Bar-Joseph, G. Gerber, D. K. Gifford, T. Jaakkola, [4] R. Blender, K. Fraedrich, and F. Lunkeit.
 [5] D. Chudova, S. Gaffney, , and P. Smyth. Probabilistic [6] D. Chudova, S. Gaffney, E. Mjolsness, and P. Smyth. [7] H. Chui, J. Zhang, and A. Rangarajan. Unsupervised [8] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. [9] A. P. Dempster, N. M. Laird, and D. B. Rubin. [10] W. S. DeSarbo and W. L. Cron. A maximum [11] J. Diebolt and C. P. Robert. Estimation of finite [12] M. B. Eisen, P. T. Spellman, P. O. Brown, and [13] C. Fraley and A. E. Raftery. Model-based clustering, [14] B. J. Frey and N. Jojic. Transformation-invariant [15] S. Gaffney and P. Smyth. Trajectory clustering with [16] S. J. Gaffney, A. Robertson, and P. Smyth. Clustering [17] S. J. Gaffney and P. Smyth. Curve clustering with [18] S. Gold, A Rangarajan, C.-P. Lu, S. Pappu, and [19] S. Gold, A. Rangarajan, and E. Mjolsness. Learning [20] J. A. Hartigan and M. A. Wong. Algorithm AS 136: a [21] G. M. James and C. A. Sugar. Clustering for sparsely [22] P. N. Jones and G. J. McLachlan. Fitting finite [23] A. Kneip and T. Gasser. Statistical tools to analyze [24] N. M. Laird and J. H. Ware. Random effects models [25] P. J. Lenk and W. S. DeSarbo. Bayesian inference for [26] J. S. Liu. Monte Carlo Strategies in Scientific [27] J. Mateos, A. Katsaggelos, and R. Molina. A Bayesian [28] G. J. McLachlan and K. E. Basford. Mixture Models: [29] G. J. McLachlan, R. W. Bean, and D. Peel. A mixture [30] E. U. Mumcuoglu, R. M. Leahy, and S. R. Cherry. [31] R. M. Neal. Bayesian Learning for Neural Networks . [32] J. O. Ramsay and X. Li. Curve registration. J. Royal [33] J.O. Ramsay and B. W. Silverman. Functional Data [34] S. Richardson and P. J. Green. On Bayesian analysis [35] P. Smyth. Model selection for probabilistic clustering [36] P. T. Spellman, G. Sherlock, M. Q. Zhang, V. R. Iyer, [37] K. Wang and T. Gasser. Alignment of curves by [38] K. Wang and T. Gasser. Synchronizing sample curves [39] K. Y. Yeung, C. Fraley, A. Murua, A. E. Raftery, and
