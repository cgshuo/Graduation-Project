 ORIGINAL PAPER Jie Zou  X  Daniel Le  X  George R. Thoma Abstract The set of references that typically appear toward the end of journal articles is sometimes, though not always, a field in bibliographic (citation) databases. But even if ref-erences do not constitute such a field, they can be useful as a preprocessing step in the automated extraction of other bib-liographic data from articles, as well as in computer-assisted indexing of articles. Automation in data extraction and index-ing to minimize human labor is key to the affordable creation and maintenance of large bibliographic databases. Extracting the components of references, such as author names, arti-cle title, journal name, publication date and other entities, is therefore a valuable and sometimes necessary task. This paper describes a two-step process using statistical machine learning algorithms, to first locate the references in HTML medical articles and then to parse them. Reference locating identifies the reference section in an article and then decom-poses it into individual references. We formulate this step as a two-class classification problem based on text and geomet-ric features. An evaluation conducted on 500 articles drawn from 100 medical journals achieves near-perfect precision and recall rates for locating references. Reference parsing identifies the components of each reference. For this sec-ond step, we implement and compare two algorithms. One relies on sequence statistics and trains a Conditional Random Field. The other focuses on local feature statistics and trains a Support Vector Machine to classify each individual word, followed by a search algorithm that systematically corrects low confidence labels if the label sequence violates a set of predefined rules. The overall performance of these two reference-parsing algorithms is about the same: above 99% accuracy at the word level, and over 97% accuracy at the chunk level.
 Keywords HTML document analysis  X  Document Object Model (DOM)  X  Reference parsing  X  Support Vector Machine (SVM)  X  Conditional Random Field (CRF) 1 Introduction The automatic extraction of bibliographic data from medical journal articles is key to the affordable creation of citations in MEDLINE , the flagship database of the U.S. National Library of Medicine (NLM), containing over 17 million records and searched over 3 million times per day world-wide. The references that typically appear at the end of such articles provide valuable information not only for generating bibliographic data items, such as Comment-On/Comment-In articles (commentary article pairs) [ 19 ], but also for systems that index articles by automatically assigning Medical Sub-ject Headings to them [ 1 ], as well as many other applications.
Hence, the analysis of these references is an important pre-processing step. Our method to accomplish this is a two-step process: (1) Locate references : to identify the reference section in an (2) Parse references : to extract entities from individual ref-Several well-known citation-indexing systems, e.g., CiteSeer [ 23 ], ISI Web of Knowledge [ 41 ] and Google Scholar [ 42 ], implement algorithms for locating and parsing references. These systems usually use Web search engines to crawl the Web and download PDF and PostScript articles. After converting these to text, they first locate the reference section and then parse each reference to extract fields such as title, author and year of publication.

In contrast, our focus is on HTML-formatted medical jour-nal articles, which differ from PDF/PS-converted text files in several ways. One problem with HTML-formatted texts is that visually similar pages can be implemented with com-pletely different HTML codes. We therefore choose to use statistical machine learning approaches, rather than relying on HTML-tag-based heuristic rules.

While the most straightforward method for locating ref-erences in HTML journal articles is to use HTML tags, the HTML syntax is overly flexible and is designed for displaying and manipulating, rather than to semantically understand, the HTML pages. Consequently, references in these pages can be implemented by completely different HTML codes, leading to incorrect results when using predefined HTML tags for reference locating [ 39 ].

First, we observe the following with regard to biblio-graphic references: (1) They contain distinctive text, e.g., author names, abbre-(2) They have similar geometric features, e.g., they occur (3) All references are consecutive neighbors, adjacent ones
These observations suggest formulating reference loca-tion as a two-class classification. The procedure would be the following: after rendering the HTML article in a browser, segment the pages into zones, extract geometric and text fea-tures from each zone, and use an SVM classifier to classify each zone as either a reference or a non-reference zone .The third observation listed earlier is a useful constraint that can expedite the process and increase its reliability.

Formulating a procedure for parsing references is chal-lenging because of the variety of formats appearing in the 5,200 journals indexed by NLM. Table 1 is a partial list of these reference styles. While only shorter references are shown in the table for brevity, their lengths vary from less than 10 to more than 100 words.

Shown in each part of the table is a reference as it appears in an article with HTML tags removed, and this reference labeled in an XML-like format. We see that these references vary considerably in style, and many variations are listed in this paragraph. For example, (a) has a Citation Number, but reference (b) is identified by a combination of first author and publication year. Some other references, on the other hand, have neither a Citation Number nor any indication of sequence. There are also many different formats for Author Names: initials followed by last names, e.g., (a); last names followed by initials, e.g., (e); not all authors listed, e.g., (c); and the first author and the remaining authors following dif-ferent formats, e.g., (d). In most cases, the Article Title exists, but, sometimes not, as in (e). Most Journal Titles are abbre-viated, while some are not. Publication Year may or may not be inside a parenthesis. Pagination may be in the full format, e.g., 495 X 499 in (a), an abbreviated format, e.g., 131 X 5 in (b), or only indicate the starting page, e.g., 275 in (g). They may be preceded by  X  X p. X ,  X  X . X , or by nothing. Page num-bers may also contain non-digits, e.g., H1145-H1152 in (f). There are also several different volume X  X age combinations. The eight entities of interest to us (listed earlier) may vary in the order they appear. Most references cite journal papers, but variations in citations appear for books, e.g., (h), reports, e.g., (i) and edited book chapters, e.g., (j). Occasionally, the  X  X uthors X  may be organizations, e.g., (k). In addition to all these stylistic differences, there are many minor variations in the use of commas, spaces, semicolons or periods to separate different entities. Some references have all words in the Arti-cle Title capitalized and others just the first word; and so on.
These variations pose challenges to the accurate parsing of the references. For this purpose, we have implemented and compared two algorithms, each based on a state-of-the-art machine learning technique. One uses the Conditional Random Field (CRF), a statistical sequence model, to model the word sequence of a reference.

The other involves local word classification and is itself a two-step process. The first step is a multi-class (in our case, 8-class) classification, which assigns an entity label to each word in the reference. We examine local features of each word, including the attributes of the word itself and those of its adjacent neighbors.

In addition, there are rules that always hold, regardless of the many styles and variations. For example:  X 
Citation Number (&lt;N&gt;) is always the first entity, if it exists.  X   X  X p. X  or  X  X . X , if it appears and is labeled as pagination, has to be followed by at least one other pagination word (usually the actual page numbers).
 The complete set of such rules is listed in Sect. 4.2.2 . These rules prove to be useful global constraints with which the label sequence must comply. In the second step of the algo-rithm, labels exhibiting low confidence are systematically corrected if the entire label sequence violates these global rules.

This paper is organized as follows: in Sect. 2 ,wereview existing methods for both locating and parsing references. We also briefly discuss the rationale and novelty of our approach. We discuss our methods in detail in Sects. 3 and 4 . Experimental evaluation is presented in Sects. 5 and 6 and a summary is given in Sect. 7 . 2 Related work 2.1 Related work on reference locating Existing algorithms for HTML page understanding are typ-ically designed in a straightforward way, i.e., they depend heavily on the HTML tags. For example, Buyukkokten et al. and Kaasinen et al. use the &lt;P&gt;, &lt;TABLE&gt; and &lt;UL&gt; tags to divide Web pages for subsequent conversion and summa-rization [ 4 , 18 ]. Diao et al. use four types of tags, including &lt;P&gt;, &lt;TABLE&gt;, &lt;LI&gt;/&lt;UL&gt; and &lt;H1&gt; paragraph, table, list and headings, respectively [ 11 ].
For specifically locating references in HTML articles, there does not appear to be any reported work. A related problem however, which has been studied recently by sev-eral researchers, is mining data records from Web pages .Data records are a list of similarly structured items, e.g., a list of products on sale. Liu et al. exploit the Web page structure, mostly depending on string matching HTML-tag sequences to detect data records [ 25 ]. Zhai and Liu extended this work, using visual information and tree matching to detect data records, and then a partial tree alignment algorithm to align data records, and extract information from each one [ 38 ]. Reis et al. assumed that certain groups of Web pages share common format and layout characteristics and designed a tree-matching algorithm to extract content from news pages [ 34 ].

These data record X  X ining algorithms have been used to extract consumer product reviews, news, Internet forum post-ings and several other applications. These algorithms are also mostly based on the HTML DOM (Document Object Model) tree and HTML tags. The occurrence of similar DOM tree structures in the Web page is the primary cue for locating and aligning data records and for extracting information from them.

On the other hand, to extract data from scanned docu-ments, geometric features are by far the most important. Scanned document layout analysis, which includes geomet-ric and logical layout analyses, has been extensively doc-umented in the literature. Geometric layout analysis, as its name suggests, concentrates on analyzing document images based on their geometric features. Most of these algorithms follow either a top-down or a bottom-up approach. Top-down algorithms recursively divide a whole page into smaller zones. The process terminates when certain criteria are met. Typical top-down methods include the X-Y cut [ 14 , 27 ], shape-directed-covers-based algorithms [ 2 ] and several oth-ers. Bottom-up algorithms start with the image pixels, cluster them into connected components, then into words, lines and finally zones. Typical bottom-up methods include Docstrum [ 29 ], Block Adjacency Graph (BAG) [ 17 ]. Hybrid methods combining split and merge strategies have also been proposed in [ 15 , 32 ]. Logical layout analysis is used to analyze the logical components of the scanned document, though most algorithms also consider geometric features [ 20 , 21 ]. In other scanned document analysis algorithms designed for special purposes, such as name extraction [ 24 ], geometric features (sometimes also called visual cues), such as gaps between text zones, are also extensively utilized. A review of meth-ods for scanned document image analysis is given in [ 28 ].
Our approach to locating references from HTML arti-cles uses both text and geometric features. By rendering the HTML articles in a Web browser (e.g., Microsoft Inter-net Explorer), both text and geometric information can be extracted. The text features include orthographic and binary features indicating whether or not certain words appear. The geometric features extracted are the normalized locations and sizes of the zone bounding boxes. These features, listed in Table 2 , are much more reliable cues compared to HTML tags, and we therefore formulate reference locating as a two-class classification based on these features. 2.2 Related work on reference parsing In contrast to reference locating, reference parsing has received considerable attention in the literature. Existing approaches fall into two categories: rule-based methods and those based on machine learning . Rule-based methods rely on rules based on a domain expert X  X  observation. For exam-ple, Chowdhury [ 6 ] and Ding et al. [ 12 ] have manually crafted templates to summarize the recognizable patterns formed by either the data and/or text surrounding the data. A set of rules is usually associated with the templates, and when the text matches the templates, the data are extracted according to the rules. Day et al. [ 9 , 10 ] extended the template-aided mining approach, and used INFOMAP, a hierarchical framework, for knowledge (template) representation. Huang et al. used a gene sequence alignment tool, BLAST (Basic Local Align-ment Search Tool), to extract citation metadata [ 16 ].
Journal publishers usually require authors to strictly fol-low predefined citation styles, and carefully edit submitted material before publication. Therefore, for a homogeneous set of journals, rule-based methods can be very successful. On the other hand, these methods require domain experts not only to handcraft the rules, but also maintain them over time. The rigidity of the rules prevents adaptability and makes it difficult to tune the system for a heterogeneous set of journals. In MEDLINE, citations are drawn from articles in over 5,200 journals from hundreds of publishers, leading to a large var-iation of citation styles. This poses a challenge to rule-based approaches for reference parsing.

In contrast, by automatically learning the knowledge from training samples, machine learning approaches exhibit good adaptability and have therefore attracted a great deal of inter-est. Parmentier and Bela X d developed a concept network to hierarchically represent and recognize structured data from bibliographic citations [ 31 ]. Besagni et al. took a bottom-up approach based on Part-of-Speech (PoS) tagging [ 3 ]. In this approach, basic PoS tags, which are easily recog-nized, are first grouped into homogeneous classes. Confus-ing tokens are then classified by either a set of PoS correction rules or a structure model generated from correctly detected records.

Cortze et al. propose a knowledge-based approach for reference parsing, called FLUX-CiM [ 7 ]. This is an unsupervised approach based on a frequency-tuned lexicon and includes four stages: blocking, matching, binding and joining.
 Hidden Markov Model (HMM) and Conditional Random Field (CRF), as successful machine learning tools for infor-mation extraction from sequences, have also been studied for parsing references. For example, Takasu applied HMM for parsing erroneous references [ 37 ], and Councill et al. used CRF to implement an open source reference-parsing package [ 8 ]. Since CRF has recently been reported to out-perform HMM [ 33 ], we pick CRF as one of our reference-parsing methods.

Another frequently adopted machine learning method for information extraction is the Support Vector Machine (SVM) classifier. For example, Okada et al. combined SVM and HMM for bibliographic component extraction [ 30 ]. In one of our reference-parsing algorithms, therefore, we use the SVM to classify each individual word. We make the intui-tive assumption that adjacent words in a reference are more likely than not to belong to the same entity. We exploit this important local dependency by using not only the features extracted from the word itself, but also those extracted from its neighbors. 3 Reference locating Our method begins by rendering the HTML article in Inter-net Explorer and then creating an HTML DOM (Doc-ument Object Model) tree. DOM tree is a well-defined model published by W3C (World Wide Web Consortium) for accessing and manipulating HTML documents. However, DOM tree usually over segments the HTML article. Figure 1 illustrates the HTML codes of two consecutive references, their rendering results, and their corresponding DOM sub-trees.
 Following the DOM convention, we use &lt;&gt; to indicate ele-ment nodes and use # to indicate text nodes. The two ref-erences, as shown in Fig. 1 a, are simple text lines, but correspond to complicated DOM sub-trees, shown in Fig. 1 c. (Dashed bounding boxes indicate zone sub-trees and are explained below.) HTML DOM tree is the starting point for our reference locating algorithm, but a preprocessing step is required for pruning over-segmented sub-trees that are unnecessary, such as the DOM sub-trees in the lower two bounding boxes.

To explain our method, we first define two types of HTML tags: Inline tags are those that do not introduce line breaks. A complete inline tag list in our algorithm includes: &lt;A&gt;, &lt;ACRONYM&gt;, &lt;ABBR&gt;, &lt;B&gt;, &lt;BIG&gt;, &lt;CITE&gt;, &lt;U&gt;, &lt;VAR&gt;.

Line-break tags are the remaining tags, which do intro-
We merge all consecutive inline DOM nodes. This gener-ates another tree structure that we call a zone tree . Each zone node contains either a set of consecutive inline DOM nodes, or a single line-break node. Examples are shown in Fig. 1 c, in which dashed bounding boxes correspond to zone nodes. Two child zones are formed due to the line-break &lt;BR&gt; nodes. Their parent is a zone corresponding to the &lt;TD&gt; DOM node. This is the only step in our algorithm that uses the HTML-tag information. After this step, the text lines without line breaks are usually formed into one zone. Subsequent steps of the algorithm are independent of HTML tags and are con-ducted on the zone tree.

From each zone node containing non-space text, 59 geo-metric and text features are extracted. The first 9 features with brief explanations are listed in the first 9 rows of Table 2 . The remaining 50 are binary features which indicate whether the specified words appear in the text. These 50 words are selected by the GSS measure [ 13 ].

GSS is named after the three authors who proposed a method for selecting informative words. In a survey of text categorization by Sebastiani [ 35 ], the GSS measure is recog-nized as one of the best methods for this purpose. Specifically for our two-class classification, we make a slight modifica-tion and define a joint GSS measure for each word t k to be: GSS ( t k ) = P ( t k , c 1 ) P (  X  t k , c 0 )  X  P ( t k where P (  X  t k , c i ) indicates the probability that, given a ran-dom zone, word t k does not occur in the zone and that the zone belongs to category c i . The GSS measure reflects the intuition that the best words are the ones distributed most dif-ferently in the reference and non-reference zones. P ( t k and P (  X  t k , c i ) are estimated by counting occurrences in the training samples, and the top 50 words with the highest GSS measures are selected and listed in the last row of Table 2 . The words are listed in descending order of their GSS val-ues. Because our training samples are medical articles, many of the selected words are abbreviated medical journal titles. For locating references in general publications, the most informative word list can be easily created by following the same procedure. Also note that special words like  X  X ross-ref X ,  X  X edline X ,  X  X copus X  and  X  X nfotrieve X  are also highly ranked. These are usually placed at the end of references to provide quick access to external links. Intuitively, they are informative words for detecting references.
 We used LibSVM [ 5 ], an SVM library developed at the National Taiwan University, to implement our reference zone classification. We adopted Radial Basis Function (RBF) as the kernel function, and the values for two parameters, C (penalty parameter of the errors) and  X  (RBF parameter), were selected through exhaustive grid-search using cross-validation on training samples.

This SVM classifier assigns each zone tree node a prob-ability value for being a reference zone. Because references are consecutive neighbors, they must be consecutive siblings in the zone tree. We use the following three-step heuristic to label the reference zones: (1) We find a parent zone node, which has the most refer-(2) Under this parent, we search for the best locations of (3) We label all consecutive sibling zones between t  X 
These steps are necessary for two reasons. One is that for some journals, the footnotes and references share the same list, with footnotes appended below the reference section in an article. We would like to remove those footnote siblings. The other much more serious reason is that the visual layout of an HTML page can be very different from the structure layout of the underlying HTML code. Even though some reference sections may visually appear to be stand-alone sec-tions, in the actual HTML implementation, they may share the same DOM parent with non-reference zones. The steps given earlier are indispensable for handling these cases. 4 Reference parsing For the step following reference locating, we have imple-mented two reference-parsing algorithms. One relies on sequence statistics and trains a Conditional Random Field (CRF) sequence model. The other focuses on local feature statistics and trains a Support Vector Machine (SVM) to clas-sify and label each individual word, followed by a search algorithm that systematically corrects low confidence labels if the label sequence violates a set of predefined rules. We describe these in the following sub-sections, and compare them in Sect. 6 . 4.1 CRF for reference parsing CRF is a probabilistic model designed for labeling sequence data [ 22 , 36 ]. It is defined as the conditional probability of a state sequence, s ={ s 1 , s 2 ,..., s N } , given an input observation sequence o ={ o 1 , o 2 ,..., o N }: p ( s | exp N t = 1 F ( s , o , t ) , where N is the length of the sequence, and F ( s , o , t ) is the sum of CRF feature functions at position t . There are two types of CRF feature functions: edge feature functions, f i (  X  ) , that characterize state X  X tate transitions, and state feature functions, g j (  X  ) , that char-acterize the observation-state relations. We use first-order Markov chain in our CRF model and the observations are extracted from the word itself and its immediate left and right neighbors. Therefore, our CRF feature functions can be written as: F ( s , o , t ) =
The goal of training a CRF is to estimate the parameters  X  and  X  j , i.e., the weights of feature functions. The trained CRF model can then be used to assign labels to unknown sequences.

In order to extract observation features, we generated word dictionaries for Author Names, Article Titles and Journal Titles from 10 years of MEDLINE data. There are a total of 236,748 Author Name words, 108,484 Article Title words and 6,909 Journal Title words. The observation feature vector o at position t contains not only the word itself, but also 14 other binary features as well. The first 3 features of a word, Author Name Feature, Article Title Feature and Journal Title Feature, are binary features indicating whether the word exists in the corresponding dictionaries. We also extract an additional 11 binary features. All 14 binary features and their brief explanations are listed in Table 3 .We used MALLET [ 26 ], a machine learning JAVA library for language processing, developed by McCallum and colleagues, to implement our CRF reference-parsing algorithm. 4.2 Combining SVM and global rules for reference parsing In our second algorithm, we treat reference parsing as a multi-class classification of each individual word in a reference using an SVM classifier. This classifier is trained on a set of manually labeled references, and then applied to test refer-ences in which it classifies (labels) every word. Our method follows this step by ensuring that the sequence of the class labels does not violate certain heuristic rules (Sect. 4.2.2 ) that are observed to always hold. If the label sequence violates these rules, a search algorithm is used to find a sequence which complies with them at the highest proba-bility (Sect. 4.2.3 ). 4.2.1 Single word classification using SVM The SVM uses 15 features from each word. The first 14 are the same ones listed in Table 3 . The 15th feature is the normalized position, i.e., the position of the word normalized by the total number of the words in the reference.

Intuitively, we expect adjacent words in a reference to have a higher probability of belonging to the same entity. In order to exploit these local contextual dependencies, the features used for the classification are extracted not only from the word itself, but also from its neighbors.
 As done for reference locating, we used LibSVM with RBF kernel function for this single word classification. Sim-ilarly, the two parameters, C (penalty parameter of the errors) and  X  (RBF parameter), were also selected through exhaus-tive grid-search using cross-validation on training samples. 4.2.2 Global rules for references By inspection, we have found that the following rules always hold for references.  X   X  X  X ,  X  X . X  or  X  X ournal X  cannot be labeled as an isolated  X   X  X p. X  or  X  X . X , if labeled as pagination, has to be followed  X  Except for  X  X ther X  entity (defined in Sect. 1 ), the remain- X  A Citation Number must be the first entity, if it exists.  X  Author entity must appear before Article Title and Journal  X  Article Title entity must appear before Journal Title, if  X  Journal Title must appear before Volume and Pagination,  X  Volume must appear before Pagination, if this exists. These global rules are very strong and useful constraints, but most of them are difficult to model with statistical models. We therefore explicitly check whether the label sequences obey the rules with a search algorithm described below. 4.2.3 A search algorithm for finding the optimal label Due to the high accuracy of single word classification, most references can already be correctly parsed. For those that do not pass the global rule test, nearly all of them are close to the correct label sequence with only a few words mislabeled. The goal is then to identify and correct those mislabeled words. We present a systematic search algorithm to find a label sequence that is valid (obeys the global rules) and is most likely (has the highest probability).

Given an N word reference, { w 1 ,w 2 ,...,w N } , and M (in our case, M = 8) entity labels, { c 1 , c 2 ,..., c M } word classification calculates an M  X  N probability matrix P . An element of P , p ( c j | w i ) , represents the posterior prob-ability of word w i belonging to entity c j , which is the output from the single word SVM classifier. To avoid computational overflow, log-probability, l ( c j | w i ) = ln p ( c j | w the following discussions.

The log-probability of a label sequence, L ={ c 1 , c 2 ,..., c } , where c sequence can also be calculated as: Cost ( c i  X  c i l ( c words, K  X  N in a label sequence is then: Cost ( L  X  L | w
The process of finding the most likely and valid label sequence then becomes a search for possible label sequence modifications in the ascending order of their costs. The search stops at the first label sequence, which obeys the global rules. Because there are M N  X  1 possible modifications, it is com-putationally prohibitive to calculate costs for all possible modifications and then sort them. We present an algorithm which enumerates sequence modifications in ascending order of their costs.

We first calculate the costs for all N ( M  X  1 ) possi-ble single-token modifications (only one word X  X  label is modified) and sort them in ascending order. This is not computationally expensive. We arrange these N ( M  X  1 single-token modifications in the middle line of Fig. 2 (marked with a dashed bounding box) in ascending order of their costs. &lt;1&gt; indicates the single-token modifica-tion with the minimum cost, and so on. It is easy to see that the first and second sequence modifications must be the first two single-token modifications. In each subse-quent column, we list all possible multi-token modifica-tions , which are all possible combinations of the previous single-token modification and all other previous single-and multi-token modifications. For example, in Column 3, the previous single-token modification is &lt;2&gt;, and there is only one other modification, i.e., &lt;1&gt;, so there is only one multi-token modification, i.e., &lt;2,1&gt;. Assuming that the cost of in the column. In Column 4, the previous single-token modi-fication is &lt;3&gt;, and all other possible previous modifications modifications. They are arranged according to their costs as shown in Column 4. In this case, the cost of &lt;3,1&gt; is smaller than that of &lt;4&gt;, and therefore it is placed above &lt;4&gt;. The therefore they are placed below &lt;4&gt;. Let us assume that &lt;1&gt; and &lt;3&gt; are the modifications to the same word, so the modi-fications &lt;3,1&gt; and &lt;3,2,1&gt; are meaningless. We mark them with dashed circles and abandon them. Similarly, we create are assumed to be single-token modifications of the same word, and &lt;2&gt; and &lt;4&gt; are single-token modifications of the other two words. Meaningless multi-token modifications are marked with dashed circles.

For each column, let us call the modifications above the single-token modification the upper column , and the mod-ifications below the single-token modification the lower column . Although the modifications in each column are ordered, the modifications in the lower column may have higher cost than the modifications in the following columns. However, a key observation is that the modifications in an upper column must be smaller than those in the lower col-umn and the following columns. This is the key for creating new columns dynamically and enumerating all modifications in ascending order of their costs. The algorithm is shown below: 1. Calculate costs for all N ( M  X  1 ) single-token mod-2. Test the first single-token modification. If it obeys 3. Test the second single-token modification. If it 4. Create Column 3, and save all modifications into an 5. Repeat for K = 3, 4,..., N(M-1)-1: 6. Finish testing remaining ordered list. 7. End
It is clear that the algorithm is still an exhaustive search, but it searches from the label sequence generated by sin-gle word classification, which, in our case, is close to the correct solution. Most searches, therefore, terminate very quickly. Because the search is conducted in the ascending order of costs, it is guaranteed to find the most likely mod-ification that obeys the rules. In an actual implementation, it is of course better to set a limit on the maximum num-ber of modifications to be tested to avoid lengthy compu-tation. In our implementation, the search terminates after 10,000 modifications have been tested. In practical sys-tems, if the search does not terminate when the limit is reached, this is an indication that the parsing may not be accurate. 5 Evaluation of reference locating To evaluate reference locating, we collected a random set of 1,000 articles from the top 100 journals cited in the MED-LINE 2006 database. We randomly selected 500 of these articles as training samples, and the remaining 500 as test samples.
 In the 500 training samples there are 21,709 references. On the other hand, there are significantly more non-reference zones. Because the SVM classifier is biased toward the class label with more training samples [ 40 ], we retain the same number of reference and non-reference zones in our training set for a total of 43,418 zones. These zones are used to find the 50 most informative words using the GSS measure to train the SVM classifier.
Our reference locating method is very reliable. From 22,147 reference zones in 500 test articles, the algorithm achieves near-perfect precision and recall rates, producing only 6 false positives and 2 false negatives. 6 Evaluation of reference parsing To evaluate reference parsing, we manually labeled 2,400 references of which 600 are randomly selected from the 500 training articles as training samples, and the remaining 1,800 are the test samples randomly selected from the 500 test arti-cles. We evaluate the algorithm performance at two levels. One is at the word level, i.e., the labeling accuracy of individ-ual words. The other is at a chunk level, i.e., the percentage of the entity chunks 1 correctly identified. 6.1 CRF-based method We conducted an evaluation of our CRF-based parsing algo-rithm by varying the number of training sequences using 10, 25, 50, 100, 300 and all 600 sequences. For 10, 25 and 50 sequences, we randomly selected 5 different sets of training sequences and repeated the experiments 5 times. For 100 and 300 sequences, we randomly selected 3 different sets of train-ing sequences and repeated the experiments 3 times. For 600 sequences, we have used all available training sequence, and can conduct the experiment only once. The averaged results of the repeated experiments are shown in Table 4 . There are 53,622 words in the 1800 test references, and the accuracy reported in Table 4 is the overall accuracy for all 8 entities. Higher accuracy is indeed achieved with more training sam-ples. Table 5 shows the accuracy at chunk level for each entity with all 600 training sequences. 6.2 Method based on SVM and global rule correction 6.2.1 Evaluation of single word classification For the second approach, i.e., combining SVM and global rule correction, we first conducted a comprehensive evalua-tion of the single word classification by varying the number of training samples and the number of words from which the features are extracted. Following the same experimental protocol, we tested with 10, 25, 50, 100, 300 and all 600 training sequences. To vary the number of words from which the features are extracted, we tested with the word itself (15 features), the word and two adjacent neighbors (the immedi-ate left and right words, giving 45 features), and the word and four adjacent neighbors (the immediate two left and two right words, amounting to 75 features). The experimental results are shown in the third column of Table 6 . 6.2.2 Evaluation of global rule correction All the above-mentioned experiments are continued with the global rule correction algorithm described in Sect. 4.2.3 , and the accuracies are reported in the fourth column of Table 6 . We find that accuracies increase after the global rule cor-rection. For chunk-level evaluation, we conducted an exper-iment with all 600 training sequences and with 45 features. The chunk-level accuracy of each entity is reported in Table 7 . 6.3 Evaluation on FLUX-CiM data We conducted an evaluation with the 2000 health science references from the publicly available FLUX-CiM data set, using our method combining SVM classification and global rule correction. No retraining is conducted using FLUX-CiM data. All FLUX-CiM references are similar in style and have no Citation Number and Other entities. Results appear in Table 8 . Besides the word and chunk-level accuracies, which can be compared to Tables 4 , 5 , 6 and 7 ,wealso include chunk-level precision, recall and F-measure. These three measures have been used in Table 3 of [ 7 ] for the same FLUX-CiM dataset. We have included precision, recall and F-Measure reported in [ 7 ] in the last three rows of Table 8 , for easy comparison.

Compared to the performance reported in [ 7 ], our method shows lower performance for the Author and Journal entity chunks, 2 but higher performance on the other four entity chunks. Compared to Tables 5 and 7 , the chunk-level accu-racy of Table 8 is lower. The following may explain the per-formance of our algorithm: (1) we classify 8 entities, while FLUX-CiM health science references have only 6 entities. The more classes, the more difficult the problem; (2) we do not retrain our algorithm with FLUX-CiM references. There are some noticeable ground-truth labeling discrep-ancies between our ground-truth labeling and FLUX-CiM ground-truth labeling. For example, in FLUX-CiM data, pub-lishers (e.g., Macmillan Publishing Company) and some-times addresses (e.g., New York) are labeled as Journal, but we label them as  X  X ther X . We have labeled 84 entities as  X  X ther, X  which contributes to the errors. 6.4 Discussion We summarize the following observations from our experiments. First of all, the strong local contextual depen-dencies among reference words should be exploited in refer-ence-parsing algorithms. This has been clearly demonstrated by the single word classification experiments. Regardless of the number of training samples, the accuracies are sig-nificantly improved if features extracted from the imme-diate left and right neighbors are combined (45 features). Combining features from two additional adjacent neighbors (75 features), on the other hand, achieves only slight accu-racy improvements, as shown consistently in each cell of Table 6 . This is in agreement with many studies of statis-tical sequence models, where usually only the first-order correlation is modeled, and the first-order Markov chain is the underlying graphic model.

Secondly, we find that global rule correction is effective as shown in Table 6 . We believe that global rule correction is a good practical heuristic to correct minor errors. When it fails, it also serves as a good indicator that the parsing may be incorrect, and requires operator attention.

The article title contains the most heterogeneous text, and therefore is the most difficult entity to extract accurately. Both CRF-parsing and SVM-parsing yield the lowest accu-racy in Title chunk identification. On the other hand, both algorithms achieve high accuracy (around 99%) for entities having distinctive features, such as Number, Volume, Year and Pagination.

We see from a comparison of Tables 5 and 7 that both ref-erence-parsing methods (CRF and SVM) essentially achieve the same overall performance: about 99% accuracy at word level and above 97% accuracy at chunk level. SVM-pars-ing missed only 3 Publication Years. SVM is a sophisticated classifier, which is expected to achieve better performance on entities having distinctive features. On the other hand, CRF achieves 1% higher accuracy on Title chunk identification. Titles contain heterogeneous text, i.e., have varying features. It is possible that CRF, by modeling the entire sequence, performs better with such text. We anticipate that overall performance for all entities may be improved if the advan-tages of SVM (sophisticated local classifier) and CRF (powerful sequence model) can be combined.

Most references in our collection are citations to journal papers (Examples (a) X (g) and (k) in Table 1 ). Our methods make few errors for this kind of  X  X tandard X  references; even organizational authors (Examples (k) in Table 1 ) can usu-ally be successfully labeled. However, in our collection, a small percentage of references are citations to reports and books (Examples (h) X (j) in Table 1 ), and our current algo-For the edited books especially (Examples (j) in Table 1 ), the long word sequence naming the editors sometimes con-fuses the algorithms. Further research is needed to solve this problem. 7 Summary We have presented approaches for locating and parsing refer-ences in HTML-formatted medical journal articles. We for-mulate reference locating as a two-class classification, and have demonstrated that text and geometry are very reliable for locating references. An SVM classifier based on these features has achieved near 100% accuracy.

The first-order correlation between reference words is important contextual information and is used in reference-parsing algorithms. We implemented and compared two reference-parsing algorithms. CRF-parsing focuses on modeling the word sequence with Conditional Random Fields, and SVM-parsing concentrates on local single word classification. The overall performance of these two approaches is about the same: above 97% accuracy at chunk level.

Our algorithm has been applied to medical journal articles only. However, we expect that it could be easily applied to other domains by collecting a set of ground-truth samples and re-training the SVM and CRF models.
 References
