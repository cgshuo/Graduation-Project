 When automatically extracting information from the world wide web, most established methods focus on spotting single HTML-documents. However, the problem of spotting complete web sites is not handled adequately yet, in spite of its importance for various applications. Therefore, this paper discusses the classification of complete web sites. First, we point out the main differences to page classification by discussing a very intuitive approach and its weaknesses. This approach treats a web site as one large HTML-document and applies the well-known methods for page classification. Next, we show how accuracy can be improved by employing a preprocessing step which assigns an occurring web page to its most likely topic. The determined topics now represent the information the web site contains and can be used to classify it more accurately. We accomplish this by following two directions. First, we apply well established classification algorithms to a feature space of occurring topics. The second direction treats a site as a tree of occurring topics and uses a Markov tree model for further classification. To improve the efficiency of this approach, we additionally introduce a powerful pruning method reducing the number of considered web pages. Our experiments show the superiority of the Markov tree approach regarding classification accuracy. In particular, we demonstrate that the use of our pruning method not only reduces the processing time, but also improves the classification accuracy. Algorithms, Performance. web content mining, web site mining, web site classification, Markov classifiers. personal or classroom use is granted without fee provided that copies ate not made or distxibuted for profit or commercial advantage and that otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGKDD 02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. 
In recent years the world wide web (www) has turned into one of the most important distribution channels for private, scientific and business information. One reason for this development is the relatively low cost of publishing a web site. Compared to other ways like brochures or advertisements in newspapers and magazines, the web offers a cheaper and more up to date view on a business for milfions of users. So even very small companies are enabled to present their products and services in the www. 
Furtbermore, many companies like online-shops operate via the intcrnct, so presenting themselves there comes naturally. But with dramatically increasing numbers of sites, the problem of finding the ones of interest for a given problem gets more and more difficult. 
The problem of spotting new web sites of special interest to a user, is not yet handled adequately. Though directory services like 
Yahoo[1 l] and DMOZ[4] can offer useful information, the entries there are often incomplete and out of date due to manual maintenance. Furthermore, the directory services do not support all the categories relevant to some specific user. Since companies need to know who their potential competitors, suppliers and customers are, web-crawlers and spiders have to be enhanced to solve problems where single web pages are not the main focus. 
For example in the IT-business, where products and services can change quickly, a system that spots special kinds of web sites and offers the opportunity to search them, will turn out to be very useful. Other reasons for focusing on whole sites instead of single pages are: There are much less sites than single pages on the intemet so that the search space can be dramatically reduced. The mining for whole web sites can offer a filter step when searching for detailed information. For example when looking for flight prices you might try to spot travel agencies first. One final reason is the higher stability of sites. Sites appear, change and disappear less often then single pages, which might be updated daily. Of course a part of the site is modified too, but in most cases this will not change the class of a site. 
The classification of text (and web) documents has received a lot of attention in the research community fore many years. Methods such as Naive Bayes [7] [12] and support vector machines [5] The web site of a domain D is a directed graph GD(N,E). A Node nEN represents an HTML-page whose URL starts with D. A link between nl and nz with nt, n2~ N is represented by the directed edge (nl, n2)E E. Thus, every HTML-document under the same domain name is a node in the site graph of the domain and the hyperlinks from and to other pages within the same domain are the connecting edges. This rather simple definition holds for our running application of spotting small and medium-size business sites. Most companies rent their own domain, so the possibility that the actual site starts below the domain is rather low. More complex appearances in the web spreading over several domain names are mostly found in rather large companies, which are already known to most people and which we do not consider in this paper. To download a site from the web, the following algorithm can be applied. Begin with the page whose URL consists of the domain name only. This is the only page that can be derived from the domain name directly. We call it therefore the starting page. After reading it, we can use an HTML-parser to determine the links to the other pages within a site. Note that considering FRAME-and EMBED-tags as links is necessary here to get an as complete picture of a site as possible. After link extraction, every link to a page, beginning with the same domain name, is followed. It is necessary to mark the pages already visited to avoid circles. Hence, every reachable page is visited and every link found is followed until the picture of the reachable parts of the site is completed. The most common way to classify single HTML-documents is using the Naive Bayes classifier [7] [12] or support vector machines [5] [12] on a feature space of terms. Here the quality of the results is highly dependent on the right choice of terms. To improve it, several very useful methods have been introduced. The elimination of stop words and stemming (reduction of words to their basic stems) are common techniques to determine the right terms and to reduce the number of terms to be considered. Another interesting possibility is to expand the feature space to include structural components. The number of words and images, the occurrence of forms or frames or the number of links from a page can offer vital information depending on the specified classes. We define the task of web site classification as follows. Given a set of site classes C and a new web site S consisting of a set (or any other data structure such as a graph) of pages P, the task of web site classification is to determine the element of C which best categorizes the site S. 
The simplest way of classifying a web site is to extend the methods used for page classification to our definition of web sites. We just generate a single feature vector counting the frequency of terms over all HTML-pages of the whole site, i.e. we represent a web site as a single "superpage". Therefore, we call this simple approach classification of superpages. The advantage of this approach is that it is not much more complex than the classification of single pages. You just have to walk through the nodes of the site graph and count terms. Afterwards the vector can be classified by any standard data mining package, e.g. the weka-package[10] we used in our experiments. However, the superpage classifier has several conceptual drawbacks. The right choice of the key terms proves to be very delicate for this approach. As mentioned before, sites can contain documents in several languages. Structural features like the occurrence of frame tags loose most of their significance. Another very important problem here is the loss of local context. Keywords appearing anywhere within the site are aggregated to build up a bag-of-words view of the whole web site graph. As shown in section 6, this simple classifier performed poorly in most experiments. 
The main reason why the superpage approach does not perform adequately, is the fact that it makes no difference between an appearance within the same sentence, the same page or the same site. However the context plays an important role considering that sites can spread over up to several thousand single HTML-documents, containing information about various topics. For example, the meaning of the terms 'network administration' and 'services' on the same page impfies that this company offers network administration as one of its services. But without the constraint that both terms appear on the same page the implication gets much weaker. Any company, offering any service and looking for a network administrator, will provide those terms too. 
To overcome these problems, we need more natural and more expressive representations of web sites. In this section, we introduce such representations. Then, we present two advanced methods of web site classification based on these representations. 
In this paper, we use the discovery of potential customers, competitors or suppfiers as our running appfication. However, we would like to argue that all the proposed methods for web site classification are not restricted to corporate websites and have a much broader range of apphcations. 
Compared to the superpage approach, we change the focus of site classification from single key terms to complete HTML-documents. In order to summarize the content of a single web page, we assign a topic out of a predefined set of topics (page classes) to it. Since the terms only influence the topic Of the page they appear on, the local context is preserved. Actually, the preprocessing step can use all mentioned techniques for the selection of terms introduced for page classification without any restriction. To conclude, we introduce page classes besides the web site classes. 
The choice of topics (page classes) for our experimental evaluation was based upon the observations that we made during the examination of many business sites in several trades. 
Although their trades varied widely, the following categories of pages are to be found in most classes of business-sites: company, company philosophy, online contact, places and opening hours, products and services, referencesand partners, employees, directory, vacancies and other. The "other"-category stands for any topic not specified more precisely. Note that these page 
Each considered topic defines a dimension of the feature space. For each topic, the feature values represent the number of pages within the site having that particular topic. This representation does not exploit the link structure of the site, but it considers a web site as a set of labelled web pages. 
To capture the essence of the link structure within a site, we represent it as a labelled tree. The idea here is, that the structure of most sites is more hierarchic than network-like. 
Sites begin with a unique root node provided by the starting page and commonly have directory-pages that offer an overview of the topics and the links leading to them. 
Furthermore, the information in most sites begins very general in the area around the starting page and is getting more and more specific with increasing distance. For example, we observed that pages regarding the company itself are more often to be found within only a few links from the starting page than ones about specific product features. [ ~t,,v.,,~ r.)I ~ I~a'~orl3 J I topic probabilities for I and J: classes, the case I: The predicted ..... "~-............... _.....,..-~'~--class Changes from J to C'~ --~,'~'~:~'-~ ! a,n .......... ~00 ~*' decreases very strongly, '----0 ~-~-_.0_1_27) J_ b (_0. ...... the dashed nodes i b ( .0127) ! following the dotted .................... node are pruned. 
Now, let S be the site to be classified, C~ a site class and let nc' . be the probability of the occurrence of the topic of the page n in class Ci. Furthermore, let rj he the number of topics. Thus, the probability is calculated by taking the r:th power of every topic probability and then multiplying those factors for every topic. This is equivalent to a multinomial process except for the difference that the multinomial coefficient can be neglected due to its equal occurrence in every class C~. 
To explain the different results compared to Naive Bayes, the following differences can be pointed out. Naive Bayes considers the occurrence of a topic to be independent from the occurrences of the other topics. But since Naive Bayes calculates the probability of the total number of occurrences within the site, the probability of just one occurrence is not independent from other occurrences of the same topic. Depending on the used distribution model for a dimension, a further occurrence of a topic that is very typical for a certain site class, will even decrease the probability for that class, if the number of oecurenees differs strongly from the estimated mean value. The 0-order Markov tree on the other hand always increases the conditional probability p(sIc), further topic specific to the site class C~ occurs in the site S. A further important difference is the consideration of the number of total pages of a site. Since a large number of occurrences will automatically decrease the number of occurrences in all other topics, the 0-order Markov tree uses knowledge that Naive Bayes does not. 
A further interesting property of 0-order Markov trees is the possibility to calculate the probabilities incrementally. For every disjunctive segmentation (st,..,s2) of our site S, the following equation holds: 
In other words, if the probability p(sjlC~) for the class C~ is higher than for any other class, the subset sj will increase the probability that P(SIC) is also higher than for any other class. This property will play an important role in our pruning method. 
The efficiency of web site classification crucially depends on the number of web pages downloaded, since the download of a remote web page is orders of magnitude more expensive than in-memory operations. Therefore, we introduce a classification method, downloading only a small part of a web site, which still achieves high classification accuracy. This method performs some pruning of a web site and applies the classifier only to the remaining web pages. For the purpose of pruning, it is important to spot a limited area that carries the information necessary for accurate classification. The existence of such an area is very likely due to the hierarchical design of most sites. The challenge is to detect the exact border of this area. 
For the following reasons, the naive approach of reading the first n pages of a web site does not yield a good accuracy. First, the topology of a web site is a matter of individual design and therefore tends to be very heterogeneous. Many sites contain large amounts of pages providing just structure but not content. For example, animated introductions or flames are instruments of making a site more usable or attractive, but in most cases they do not contain any ecmtent recognized by a page classifier. Another 
The conditional probabilities p(slCi) yield the information about the degree that a path s supports a site .class Cj. Since the focus lies on the importance of a path for site classification, the actual class or classes it supports are not relevant here. To quantify the importance for the complete site, we use the variance of the conditional probabilities over the set of all web site classes. 
Since the variance is a measure for the heterogeneity of the given proposition. For length(s) ~ oz the criterion is likely to prohibit the extension of a path unless a topic occurs that can significantly raise the variance. With increasing length(s) it is getting more and more unlikely that an additional factor can increase the variance strongly enough. Due to the required growth of variance and the decreasing influence of the additional factor, most paths are cut off after a certain length. This corresponds to the requkement made by our second proposition that a path will not provide general information about the class of the web site after a certain length. Hence, we avoid reading large subtrees without any impact on site classification. 
The parameter m is used to adjust the trade-off between classification accuracy and the number of web pages downloaded. 
Since the tolerance for the' change of weight(s) depends on the ratio length(s), m is the length from which on an extension of the path is only permitted, if the variance increases. Thus, a good estimate for m is the distance from the starting page in which the relevant information is assumed. Our experiments (see section 6) will show that the interval of reasonable values for m is relatively wide and that the choice of cois not deficate. 
Pruning is not only a means of increasing the efficiency of web site classification, but it can also improve the classification accuracy. When classifying a complete web site, all of the introduced methods (with the exception of Markov trees with k_~l) consider all pages equally important and independently from their position within the site. Thus, unspecific subtrees can drive the classification process into the wrong direction. By providing an effective heuristic to disregard areas that are unlikely to contain the relevant information, the classifier gets a better description of the web site and therefore, it will offer better accuracy. To conclude, the introduced pruning rule tries to cut off misleading areas from the web site tree and thus, can reduce the processing time and also increase the classification accuracy. 
This section presents the results of our experin~ntal evaluation of the proposed methods of web site classification. As mentioned before our running appfication is the categorization of corporate web sites belonging to 2 different trades. Our testbed consisted of 82,842 single HTML-documents representing 207 web sites. For the considered trades we chose florists and IT-service providers to have a significant distinction in the business. The distribution of the web site classes was: 112 "other", 21 "florist" and 74 "IT-service provider". The "other"-sites were taken randomly from various categories in yahoo[l l]. To make the experiments reproducable, the downloaded information was stored locally. To classify the pages into the topics listed in section 4, we labelled about 2 % of the pages in the testbed and obtained a classification accuracy of about "72 % using 10-fold cross-validation with Naive 
Bayes on the manually labelled pages. As implementation for this and the rest of the standard algorithms, we used the well-implemented weka-package [10]. The remaining 98 % of the pages were labelled by Naive Bayes based upon this training set. 
The evaluation of our methods consisted of two main parts. First, we compared the methods with respect to their classification accuracy on complete sites. The second part shows the impact of our pruning method and its parameter o~ on the classification accuracy using the 0-order Markov tree. 
Table 1 shows the overall classification accuracy as well as precision and recall for the single site classes. Since the snperpage approach provided only an accuracy of about 55 %, it seems not to be well-suited for web site classification. On the other hand, all approaches based on the preprocessing steep (introducing page class labels etc.) obtained reasonable results. The best method using the complete web site turned out to be the 0-order Markov tree which yielded 3.4 % more classification accuracy than the 
C4.5 [9] decision tree classifier. It also clearly outperformed the 1-order Markov tree by 4.3%. As a comparison the 0-order 
Markov tree, applying the introduced pruning method, increased the accuracy by one percent to 87 % on reading only 57 % of the data. 
Our experimental evaluation demonstrates that using higher values than 0 for the order k did not improve the results, when applying a Markov tree classifier. This is due to the following reasons. First of all, the above mentioned problem of "phantom paths" increases with the length of the considered context (represented by the order k), depending on the error rate of page classification. We already noted that the overall error rate p of wrongly recognized pages in the site is about the same as the classification error for the single pages. But the prohabifity of a correctly observed transition is only (l-p) 2, since it takes two correctly classified pages to recognize a transition. This problem gets worse with increasing order k. Thus, a distribution based 
