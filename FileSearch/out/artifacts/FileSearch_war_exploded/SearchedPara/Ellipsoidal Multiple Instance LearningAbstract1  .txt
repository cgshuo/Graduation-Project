 Gabriel Krummenacher gabriel.krummenacher@inf.ethz.ch Department of Computer Science, ETH Zurich, Switzerland Cheng Soon Ong chengsoon.ong@unimelb.com.au NICTA, Victoria Research Laboratory, Melbourne, Australia Joachim M. Buhmann jbuhmann@inf.ethz.ch Department of Computer Science, ETH Zurich, Switzerland In many applications of supervised learning, the cost of obtaining ground truth labels is a significant bottle-neck. This has led to research on weakly labeled data, among which the framework of multiple instance learn-ing (MIL) has shown promising results. In parallel, there has been developments in robust optimization, where data uncertainty is taken into account. Mo-tivated by a real world application of defect detec-tion based on multiple partial observations, we pro-pose a novel approach based on both MIL and robust optimization. In this paper, we consider the binary classification problem (labels y  X  { X  1 , +1 } ) in the MIL setting (Dietterich et al., 1997). Instead of hav-ing one label per example x j , we are given B bags of examples where each bag { x i 1 ,..., x ij ,..., x in i consists of n i instances. Unlike standard supervised learning, labels are provided only at the bag level such that y i = +1 if at least one of y i 1 ,...,y in i is pos-itive, and the bag is negative ( y i =  X  1) only if all y i 1 = ... = y in i =  X  1. Unfortunately, due to the weak labeling, it is unclear during training time how to allo-cate the positive label. Since any number of examples in a positive bag may be positive, one would naively have to look at all possible labelings that include at least one positive label. This results in potentially ex-pensive computations involving the solution of a com-binatorial optimization problem. See Kim &amp; la Torre (2010) for an overview of recent MIL methods.
 We propose ellipsoidal multiple instance learning (eMIL) where we relax the function over the set of instance labels and approximate a bag by the first and second moment of the empirical distribution. I.e., we take the arithmetic mean and empirical covariance ma-trix of the within bag instances. Neglecting higher or-der moments gets rid of the combinatorial optimisation problem and enforces regularisation.
 Our proposed method (eMIL) results in a modular two stage algorithm: (1) estimate the ellipsoids, and (2) optimise the generalised large margin algorithm. An additional benefit is that our approach gives an instance level classifier (instead of a bag level classi-fier), which may be important in some applications. We first derive the optimization problem to find a maximum-margin type classifier for ellipsoids, where one class of ellipsoids can overlap with the decision boundary (Section 2). Two different ways of scal-ing the empirical covariance matrix for different dis-tributional assumptions on the bags are presented in Section 2.3 and Section 2.4. We show in Section 2.4 that solving this optimisation problem is equivalent to treating each bag as a random variable and robustly maximizing the margin between instances distributed according to this random variable under asymmetric probabilistic constraints over all distributions with fi-nite mean and covariance. To solve the resulting non-convex optimisation problem a quasi Newton method and a decomposition of the objective into the differ-ence of two convex functions is presented in Section 3. The method compares favourably to state of the art MIL methods with respect to accuracy on benchmark datasets (Section 4). Finally we introduce our moti-vating application: a safety critical real world problem of detecting wheel defects, and show that eMIL has better accuracy than recent methods (Section 5). We derive a maximum-margin type classifier for the problem of learning with positive and negative ellip-soidal examples. Our aim is to exploit the structure of a bag in the MIL setting and not just treat the in-stances as individual separate points. We capture this bag structure by the empirical mean and the empirical covariance matrix of all the instances in a bag. This naturally leads to the interpretation of a bag as an ellipsoid. The notion that a positive bag label only guarantees one instance to be positive, is represented by letting ellipsoids with positive label overlap the neg-ative half space. Negatively labeled ellipsoids on the other hand are required to be maximally distant from the decision surface, since we know that all instances in a bag with negative label are indeed negative. Recall that an ellipsoid in x  X  R d is given by a positive semidefinite covariance matrix P  X  S d + , and a central vector q  X  R d by Given a set of examples and corresponding labels rating hyperplane with w  X  R d and b  X  R which follows the maximum margin principle. There-fore for the predictor based on ellipsoid i given by larized empirical risk problem where  X  is the regularisation parameter and ` ( t ) = max(0 , 1  X  t ) is the hinge loss. 2.1. Optimisation problem The prediction function f ( P i , q i ) should give the signed distance of the ellipsoid to the hyperplane. By reasoning about the geometry of the problem (refer to Figure 1), we get the following distance for any ellip-soid to the hyperplane.
 Proposition 1. Given an ellipsoid, Equation (1) , and a hyperplane, Equation (2) , and taking the asymme-try of positive and negative ellipsoids into account, the signed distance from the ellipsoid to the hyperplane is given by See Appendix C in the supplementary file for the full derivation of this distance. Therefore the prediction function is given by Substituting Equation (5) into Equation (3) we obtain the following optimisation problem, which we call el-lipsoidal multiple instance learning (eMIL). min Equation (6) is subtly different from robust opti-mization problems. We discuss this further in Sec-tion 2.5. Note that the optimization problem given by Equation (6) is non-convex. This is due to the term  X  p w &gt; P i w in the hinge loss for positive bags (max 0 , 1  X  p w &gt; P i w + w &gt; q i + b ), which is a concave function in w . It can also be observed that the problem is not a second order cone program by decomposing the hinge loss: min s . t . k A i w k X  X  X   X  i  X  w &gt; q i  X  b + 1 ,  X  i : y i = +1 Where we have used P i = A &gt; i A i . The constraints for positive ellipsoids are not second order cone con-straints. 2.2. Ellipsoid estimation We model the i th bag, { x i 1 ,..., x in i } with n i stances, by the empirical mean and covariance of the instances, given by q When the number of instances per bag n i is larger than the dimensionality of the feature space d , the covari-ance P i is of full rank and strictly positive definite. However, in many datasets n i &lt; d , resulting in a low rank P i . This is usually the case in the test datasets we consider. The average number of instances in a bag is much lower than d , resulting in a semidefinite covariance P i .
 The covariance matrix gives the shape of the ellipsoid. To find the volume, we derive two types of scaling factors for the covariance matrix under two different distributional assumptions in the following sections. 2.3. Confidence regions Under the assumption of approximately Gaussian dis-tributed instances per bag we can use the following fact. Recall that for a random variable x distributed as a p dimensional Gaussian N p (  X  ,  X  ), the quadratic form ( x  X   X  ) &gt;  X   X  1 ( x  X   X  ) is distributed as  X  2 degrees of freedom. This implies that the ellipsoid contains 1  X   X  of the total probability mass.
 For an ellipsoid ( q , P ) to cover 1  X   X  -percent of a p -dimensional multivariate Gaussian distribution with a covariance matrix  X  we set P =  X   X  F  X  1  X  2 F quantile q . We can use this fact to scale the empirical covariance matrix, estimated from the bag instances. In the next section a more general scaling factor for non Gaussian distributions is derived. 2.4. Probabilistic multiple instance learning In this section we show that eMIL maximizes the mar-gin between instances from any within bag data dis-tribution with finite mean and covariance with high probability, while enforcing the asymmetry inherent to multiple instance learning. We use similar minimax techniques to robust optimization approaches (Lanck-riet et al., 2002; Shivaswamy et al., 2006), that are based on a multivariate Chebyshev X  X  inequality (Bert-simas &amp; Popescu, 2001).
 Assuming the instances in a bag are drawn from a probability distribution with mean q i and covariance  X  , we want to find a hyperplane that maximises the margin between instances from the two classes. Re-member, that for instances in a negative bag we know the label, for instances in a positive bag we only know at least one instance is positive. We formalize this with the following optimization problem: min s . t . inf where sup / inf x over all distributions for x i having mean q i and co-variance  X  i . The constraints in Equation (9) differ for instances from positive bags and instances for nega-tive bags. For instances from negative bags we want the smallest (over all distributions with mean q i and covariance  X  i ) probability of correct classification to be higher than  X  i  X  (0 , 1]. For small  X  i this gives high worst-case probability of correct classification for all instances in a negative bag. For instances from positive bags the highest (again over all distributions with mean q i and covariance  X  i ) probability of nega-tive classification needs to be larger than  X  i , in other words, there exists a distribution with negative clas-sification higher than  X  i . This may seem counter-intuitive at first, but recall that for a positive bag to be classified correctly only one instance needs to be classified correctly, i.e. many of the instances will be classified negative. Here for small  X  i this gives low negative classification probability.
 We show that considering the worst case distribu-tion with finite mean and covariance results in the same constraints as making an assumption of ellip-soidal bags.
 Proposition 2. The optimization problem in Equa-tion (9) is equivalent to eMIL (Equation (7) ). To prove Proposition 2 we use the following Lemmas: Lemma 3. Where x i is a random vector and the supremum is over all distributions for x i with mean q i and covariance matrix  X  i .
 Proof sketch. This can be shown by using the mul-tivariate Chebyshev inequality from Lanckriet et al. (2002) and setting S = { x i | y i (  X  w , x i  X  + b )  X  1  X   X  (See Appendix A in the supplementary file for a more detailed proof.) Lemma 4. For x i  X  ( q i ,  X  i ) : Proof sketch. By considering the distance d to the hy-perplane, we obtain the above result. This follows the same logic as the proof in Lanckriet et al. (2002); Shiv-aswamy et al. (2006), see Appendix A in the supple-mentary file for a more detailed proof.
 Proof of Proposition 2. Since the objective function in both optimization problems are the same, it is suffi-cient to show that the constraints are equivalent. For the probabilistic constraint of negative bags we rewrite it as Then we can use Lemma 3 to rewrite the constraints as where d 2 is defined as in Lemma (3).
 Next we use Lemma 4 and rearrange the terms to fi-nally get the constraints  X  X  w , q i  X  X  X  b  X  1  X   X  i +  X  (  X  i ) p w &gt;  X  i w ,  X  y Now, if A i in Equation (7) is set to A i =  X  (  X  i )  X  1 / 2 the equivalence can be seen.
 This shows that by considering the worst case distri-bution (not necessarily Gaussian) with finite mean and covariance scales the ellipsoid by a factor  X  (  X  i ) 2 . 2.5. Relation to robust classification In robust classification, the goal is to find a classifier, that is robust to random perturbations in the feature space (Ben-Tal et al., 2009). If we assume an ellip-soidal uncertainty set U i = { u i : u &gt; i P i u i  X  1 } n seek to find a maximum-margin classifier, we get the formulation of a robust SVM (Sra et al., 2011): The difference to eMIL (Equation (7)) is the fact that k P 1 / 2 i w k is not multiplied with the label y i . This leads to a hyperplane that separates ellipsoids, whereas in eMIL the positive ellipsoids can overlap the hyper-plane. See Figure 2(b) for an illustration.
 2.6. Other MIL approaches The MIL setting is very natural in many applications such as text classification (Andrews et al., 2002), im-age retrieval (Gehler &amp; Chapelle, 2007) and object detection (Viola et al., 2006). For example, content based image retrieval represents an image as a bag containing image patches (examples x ij ) and for a particular query, one is interested in returning images (bags { x i 1 ,..., x in i } ) that contain the object, instead of solving the more complex problem of labeling every patch in the image.
 Unfortunately, due to the weak labeling, it is unclear during training time of MIL methods how to allocate the positive label. Since any number of examples in a positive bag may be positive, one would naively have to look at all possible labelings that includes at least one positive label. For learning a classifier, the bag label is traditionally inferred as the max over the classification of all instances in the respective bag: y = sgn max n i j =1 (  X  w , x ij  X  + b ) (Andrews et al., 2002). This results in a non-convex optimization problem for finding a maximum-margin hyperplane for classifica-tion due to the negative max function not being con-vex. It also results in potentially expensive computa-tions which involve optimising a combinatorial prob-lem. Recently, there has been several proposals of making some assumptions about the structure of the bag such as using Markov random fields (Warrell &amp; Torr, 2011) and low dimensional manifolds (Babenko et al., 2011).
 In Section 4, we compare our method to the follow-ing algorithms for MIL: Two traditional approaches to solve the MIL problem, the earliest one being the method of axis-parallel rectangles (APR) (Dietterich Algorithm 1 eMIL: Sequential SOCP
Initialise ( w 0 ,b 0 ) according to Equation (18) while ` ( w k ,b k )  X  ` ( w k +1 ,b k +1 ) &gt; do end while et al., 1997), that was specifically designed for the MUSK1 and MUSK2 datasets and an extension of the diverse-density algorithm (Maron &amp; Lozano-P  X erez, 1998), EMDD, using Expectation-Maximization to find a positive witness (Zhang et al., 2002). We also compare to two extensions of a support vector machine mi-SVM (maximizing instance margin) and MI-SVM (maximizing bag margin), that lead to mixed-integer programs (Andrews et al., 2002); deterministic an-nealing methods to solve mi-SVM (AL-SVM) and MI-SVM (AW-SVM) (Gehler &amp; Chapelle, 2007); a convex semi definite programming to the maximum instance margin problem (SDP) (Guo, 2009); MICA, an algo-rithm that uses convex combinations of positive bag instances (Mangasarian &amp; Wild, 2008); and a recent approach developing a Gaussian process by building bag likelihood models from the GP latent variables (GPMIL) (Kim &amp; la Torre, 2010). 3.1. Difference of Convex Functions We propose two approaches to optimize the result-ing non-convex optimization problem (6). We derive a concave convex procedure (CCCP) in the following subsection, and a quasi-Newton approach (L-BFGS) in Section 3.3. While CCCP gives consistently lower optimal values on all the datasets that we tried, the gradient based method is usually much faster, espe-cially in very high dimensional problems. However, the lower objective value typically also does not translate into significant improvements on test accuracy. For both approaches, we initialize by setting w 0 ,b 0 = arg min which is the maximum-margin hyperplane that sepa-rates the means of the bags. This can be seen as a first order approximation of the within-bag distribution. 3.2. Solving eMIL with CCCP We can express the objective function (6) as a differ-ence of convex functions and use CCCP to solve it, by solving a series of convex programs. See Yuille &amp; Rangarajan (2003) for the introduction of the CCCP, Sriperumbudur &amp; Lanckriet (2009) for its convergence proof, and Le Thi &amp; Pham Dinh (2005) for an overview on difference of convex functions algorithm. The solu-tion of eMIL with CCCP is shown in Algorithm 1. The decomposition of (6) into the difference of two convex functions g ( w ,b ), h ( w ,b ) is as follows: where the first three lines (Equation (19)) corre-spond to g ( w ,b ) and the last line (Equation (20)) to  X  h ( w ,b ). Given the decomposition, CCCP proceeds by linearizing the concave part  X  h ( x ,b ) at w k ,b k , solv-ing the resulting convex optimization problem Equa-tion (21), obtaining the optimal value w k +1 ,b k +1 and repeating until convergence. The linearisation of  X  h ( x ,b ) (Equation (20)) at w k ,b k is given by:  X  X  w , X  X  ( w k ,b k )  X  . By taking the sum over the posi-tive examples out of the inner product we arrive at: By introducing slack variables  X  , using P i = A &gt; and finally converting the remaining objective function into second order cone constraint, we can now rewrite Equation (21) to get the equivalent (in terms of opti-mal solution ( w ,b )) constrained optimisation problem Equation (22), which is a second order cone program (SOCP). s . t . Where  X  is just a placeholder for
See Appendix B in the supplementary file for details. 3.3. Solving eMIL with BFGS Another way of solving eMIL is to find a local min-imum with a gradient based method. We use the quasi-Newton method L-BFGS (Byrd et al., 1995). To get a gradient of eMIL we use a smoothed ver-sion of the hinge-loss similar to (Chapelle, 2007; Wang et al., 2008), which has the following form ` ( P i ; q i ,y i , w ,b ) =  X   X   X   X   X  1  X  y i  X  f ( P i ; q i )  X   X  2 if y i  X  f ( P i ; q i )  X  1  X   X  0 if y i  X  f ( P i ; q i ) &gt; 1 , where we choose an appropriately small  X  . The gra-dient is shown in Appendix D in the supplementary file. We compare the performance of eMIL on the following datasets: The MUSK1 and MUSK2 datasets described in (Dietterich et al., 1997), three image annotation datasets (Elephant, Fox, Tiger) introduced in (An-drews et al., 2002) and 7 splits of the TREC9 dataset (TST1, TST2, TST3, TST4, TST7, TST9 and TST10) also described in (Andrews et al., 2002). The TREC9 datasets are extremely high-dimensional and sparse, having 66000 to 67000 features of which only a maxi-mum of 30 are non-zero per instance. On average the MUSK1 and MUSK2 datasets contain approximately 6 and 60 instances per bag respectively. The average bag sizes for the image annotation and TREC9 data are 7 and 8 respectively. We minimize the regularized empirical risk function, Equation (6) using L-BFGS (Section 3.3). 1 To avoid numerical problems, when the ellipsoids are low rank we add a tiny positive con-stant to the diagonal of P i . This preserves the shape of the ellipsoid and makes P i positive definite. We also experimented with different scaling factors (see Section 2.3 and Section 2.4), but could not generally improve test accuracy compared to simply using the estimated covariance matrix. By setting  X  (  X  i ) = 1 we implicitly use  X  i = 0 . 5 for all bags.
 To be able to compare the performance of our method with previous methods we follow (Andrews et al., 2002) and employ the following procedure on all of the datasets: We use 10-fold cross-validation and search coarsely for an optimal regularization parameter  X  . This procedure is repeated 10 times on random per-mutations of the data and the results are averaged. 4.1. Feature space corresponding to kernels For the MUSK-datasets we use a Gaussian kernel with  X  = 10  X  6 . Since we optimise eMIL in the primal, we use kernel PCA to project the infinite dimensional feature vector to a lower-dimensional subspace. For MUSK2 we additionally restrict the number of basis vectors to 2500 to save memory.
 To be able to optimise in the primal, we explicitly compute a finite dimensional representation of the fea-tures corresponding to the kernel. Following Zien et al. (2007), we use kernel PCA (Sch  X olkopf &amp; Smola, 2002) to find a d dimensional representation of the data from the kernel k ( x i , x j ). Since the representer theorem en-sures that the optimal solution w lies in a finite dimen-sional subspace, we first find a basis for this subspace and then represent the instances in terms of this basis. The basis needs to satisfy two criteria: (1) each ba-sis vector has to be expressed in terms of the feature maps, and (2) the basis vectors should be orthonor-mal. Hence for a kernel matrix K , we need to find a set of coefficients in a matrix A such that A &gt; KA = I . One way to do so is to compute the eigenvalue decom-position of K = V  X  V &gt; and set A = V  X   X  1 2 . 4.2. Results On the musk datasets (Table 1, top ) eMIL shows comparable performance to the methods motivated by finding a witness instance for positive bags (MI-SVM, MICA, AL-SVM) and the instance level maximum margin methods (mi-SVM, AW-SVM). The good ac-curacy of APR on the musk datasets can be explained by the fact that the hypothesis class of axis-parallel rectangles was specifically developed for this particu-lar dataset. For the image annotation datasets (Ta-ble 1, middle ) eMIL has the best accuracy on the tiger and elephant dataset and beats the MIL-SVM meth-ods on the Fox dataset. Our method achieves highest accuracy for some of the TREC9 datasets (Table 1, bottom ), and comparable accuracy to the best method (GPMIL or mi-SVM) on all the TREC9 datasets, apart from TST2. In this section we apply eMIL to a real world MIL problem: Detecting defective wheels of freight trains from multiple dynamic vertical wheel force measure-ments. Late or undetected wheel defects on railway vehicles result in increased infrastructure maintenance due to damage of the railway infrastructure, like track systems or civil engineering works, and reduced avail-ability of the vehicle pool, maintenance compounds and infrastructure. Most importantly, wheel defects are the major source of noise and vibration emissions of rail traffic. This makes the automatic, reliable and timely detection of wheel defects an essential part of any railway infrastructure safety monitoring system. For detecting wheel defects, we are given eight mea-surements per wheel, obtained by eight sensors in-stalled on the tracks as the train runs over the mea-surement site in full operational speed. A defect only impacts a measurement if it hits the part of the track where the sensor is installed directly. This results in eight measurements per wheel, with usually one mea-surement affected by a defective wheel. By considering all measurements from different sensors for the same wheel as a bag a natural setting for MIL is obtained. The labeled data was obtained by running a test train with a known configuration of wheel defects, resulting in 100 positive and negative bags. 5.1. Experimental protocol Each measurement (instance) consists of a time series of the vertical wheel force. We use the Global Aligne-ment (GA) kernel for time series, described in Cuturi et al. (2007) and Cuturi (2011). The GA kernel can be seen as a generalization of dynamic time warping (DTW) with a soft-max over all the alignments. To optimise eMIL in the primal we again project the fea-tures corresponding to this kernel to a lower dimen-sional subspace (see Section 4.1).
 We compare our algorithm (eMIL) to the three de-terministic annealing methods described in Gehler &amp; Chapelle (2007) in Table 2. We chose these methods because they solve the mi-SVM and MI-SVM formu-lations of MIL and our method could be seen as a generalization of the maximum bag margin method MI-SVM. Furthermore, an implementation was read-ily available. ALP-SVM is a balancing extension of AL-SVM, it needs to know an estimate of the frac-tion of positive points in a positive bag p ? a priori. For ALP-SVM, we provide extra information by set-ting p ? to 1 / 8 because we expect one sensor on aver-age to see a defect. In addition, we compare a baseline method using a standard Support Vector Machine (de-noted  X  X VM X ). To convert from bag labels to instance labels, we set all instance labels to the bag label. The reported accuracy is averaged over 10 random permutations of the following two stage evaluation scheme: Half of the data is split of for model selection and half for evaluation. On the first half of the data the optimal parameter for regularization is searched over  X   X  10 [  X  2 ,...,  X  5] . This is done with estimating test error with 5-fold cross validation for all values of  X  and the  X  with lowest test error is kept. This  X  is then used to train the classifier on the full first half of the dataset and test error is computed on the second evaluation half of the dataset. If multiple parameter values give the same test accuracy, the one closest to the average is kept for training. 5.2. Results on the wheel data From Table 2 we see that eMIL has the highest average accuracy on the test set. However, due to the large variation between the different splits of the data, the standard deviation is large. We also compared the ranks of the methods for each split, with the method with highest accuracy obtaining rank 1, and the lowest rank 5. We see in Table 2 that eMIL has average rank 1.5, which is the best among all considered methods. Interestingly, the naive SVM approach performs well. Motivated by the real world application of detecting wheel defects from multiple dynamic force measure-ments, we derive an ellipsoidal algorithm to solve MIL, resulting in a classifier that optimises a class condi-tional distance between an ellipsoid and a hyperplane. We show that representing bags as ellipsoids amounts to finding a robust solution. Using only the assump-tion that the instances are samples from a distribution with finite mean and covariance, we derive an appro-priate scaling factor for eMIL. We propose two ap-proaches to solve the optimization problem: a CCCP approach which results in a sequential SOCP, and a quasi-Newton method based on L-BFGS.
 Our algorithm results in state of the art performance on benchmark MIL datasets, demonstrating the effec-tiveness of the method. For classifying defective wheels with multiple instance time series data eMIL con-sistently outperforms AL-SVM, AW-SVM and ALP-SVM, which are recent improvements to SVM type MIL approaches. We are currently working with our collaborators in the rail industry to test this approach in the safety monitoring system.
 Acknowledgements We would like to thank Stefan Koller from SBB for help with the freight train data, Brian McWilliams for helpful discussions and one anonymous reviewer for detailed comments. NICTA is funded by the Aus-tralian Government as represented by the Department of Broadband, Communications and the Digital Econ-omy and the Australian Research Council through the ICT Centre of Excellence program.
 Andrews, Stuart, Tsochantaridis, Ioannis, and Hof-mann, Thomas. Support vector machines for multiple-instance learning. In NIPS , 2002.
 Babenko, Boris, Verma, Nakul, Doll  X ar, Piotr, and Be-longie, Serge. Multiple instance learning with man-ifold bags. In ICML , pp. 81 X 88, 2011.
 Ben-Tal, Aharon, Ghaoui, Laurent El, and Ne-mirovski, Arkadi. Robust Optimization . Princeton University Press, 2009.
 Bertsimas, Dimitris and Popescu, Ioana. Optimal in-equalities in probability theory: A convex optimiza-tion approach. SIAM Journal on Optimization , 15: 780 X 804, 2001.
 Byrd, R.H., Lu, P., Nocedal, J., and Zhu, C. A limited memory algorithm for bound constrained optimiza-tion. SIAM Journal on Scientific Computing , 16(5): 1190 X 1208, 1995.
 Chapelle, Olivier. Training a support vector machine in the primal. Neural Computation , 19(5):1155 X  1178, 2007.
 Cuturi, M., Vert, J.-P., Birkenes, O., and Matsui, T. A kernel for time series based on global alignments. In ICASSP , volume 2, 2007.
 Cuturi, Marco. Fast global alignment kernels. In ICML 2011 , 2011.
 Dietterich, Thomas G., Lathrop, Richard H., and
Lozano-Perez, Toms. Solving the multiple instance problem with axis-parallel rectangles. Artificial In-telligence , 89(1-2):31  X  71, 1997.
 Gehler, Peter V. and Chapelle, Olivier. Deterministic annealing for multiple-instance learning. In AIS-TATS , 2007.
 Guo, Yuhong. Max-margin multiple-instance learn-ing via semidefinite programming. In Advances in Machine Learning , volume 5828 of Lecture Notes in Computer Science , pp. 98 X 108. Springer, 2009. Kim, M. and la Torre, F. De. Gaussian processes multiple-instance learning. In ICML , 2010.
 Lanckriet, Gert R. G., Ghaoui, Laurent El, Bhat-tacharyya, Chiranjib, and Jordan, Michael I. A ro-bust minimax approach to classification. Journal of Machine Learning Research , 3:555 X 582, 2002.
 Le Thi, Hoai An and Pham Dinh, Tao. The DC (dif-ference of convex functions) programming and DCA revisited with DC models of real world nonconvex optimization problems. Annals of Operations Re-search , 133:23 X 46, 2005. ISSN 0254-5330.
 Mangasarian, OL and Wild, E.W. Multiple in-stance classification via successive linear program-ming. Journal of Optimization Theory and Applica-tions , 137(3):555 X 568, 2008.
 Maron, O. and Lozano-P  X erez, T. A framework for multiple-instance learning. In NIPS , 1998.
 Sch  X olkopf, Bernhard and Smola, Alexander J. Learn-ing with Kernels . MIT Press, 2002.
 Shivaswamy, Pannagadatta K., Bhattacharyya, Chi-ranjib, and Smola, Alexander J. Second order cone programming approaches for handling missing and uncertain data. Journal of Machine Learning Re-search , 7:1283 X 1314, 2006.
 Sra, S., Nowozin, S., and Wright, S.J. Optimization for Machine Learning . Mit Press, 2011.
 Sriperumbudur, Bharath K. and Lanckriet, Gert R. G.
On the convergence of the concave-convex proce-dure. In NIPS , pp. 1759 X 1767, 2009.
 Viola, P., Platt, J., and Zhang, C. Multiple instance boosting for object detection. In NIPS , 2006. Wang, Li, Zhu, Ji, and Zou, Hui. Hybrid huberized support vector machines for microarray classifica-tion and gene selection. Bioinformatics , 24(3):412 X  419, 2008.
 Warrell, J. and Torr, P. Multiple-instance learning with structured bag models. Energy Minimazation
Methods in Computer Vision and Pattern Recogni-tion , pp. 369 X 384, 2011.
 Yuille, A.L. and Rangarajan, A. The concave-convex procedure. Neural Comput. , 15(4):915 X 936, 2003. Zhang, Qi, Goldman, Sally A., Yu, Wei, and Fritts, Ja-son E. Content-based image retrieval using multiple-instance learning. In ICML , 2002.
 Zien, A., De Bona, F., and Ong, C.S. Training and approximation of a primal multiclass support vector
