 A recent trend in mobile adve rtising is the emergence of programmatic buying in real time bidding (RTB) based marketplace, wher e each advertiser bid on individual im-pression in real time. Unlike the conventional mediation type marketplace with pre-negotiated fixed clearing cost, the clearing price in the RTB marketplaces depends upon the bid that each advertiser submits. In second price auction [14], which is commonly used in the mobile advertising RTB marketplace, the clearing price is ac-tually the second highest bid. While at one hand this unique behavior in RTB market-based on their own need and best interest, it also makes the market more competitive and demand solutions to some new problems that don X  X  exist in mediation type mar-ketplaces. Some of the specific problems are predicting the win rate given a bid, esti-mating the most likely clearing price in the second price auction setup, and optimizing the bid based on various estimates. In this paper, we describe our approaches of esti-mating the win rate and winning price given the bid and correspondingly how do we carry out bidding strategies in different RTB auction setups. 
The general workflow in a RTB marketplace is the following: As an end-user that ad-exchange. Those bidders get the information regarding the property, the availa-vice, e.g. device type, os version etc., and they need to decide if they want to place a bid for this particular request, and if so, how much to bid. Once ad-exchange receives all the response from those bidders, it will pick the winner with the highest bid, deter-mine the cost and notify the winner. As an analogy to financial market, an ad-exchange is on the  X  X ell X  or  X  X upply X  side, and a bidder is on the  X  X uy X  or  X  X emand X  side. 
Win rate estimation refers to the problem of estimating the likelihood of winning difficulties to the ad-exchange on the sell side and the bidder on the buy side. For the from all the participants in the auction and it can hereby construct a win rate estimate fair easily by taking a histogram on the winning bid. The task of win rate estimation is more challenging to a bidder in the RTB, in the sense that he only knows his own bid and the outcome of the auction, and he has no idea about other people X  X  bid. In other words, there is a missing attribute situation to the bidder in RTB. Being able to utilize a huge quantity of data with partial missing attribute is the key to a successful demand side win rate estimation. Winning price estimation is another unique problem in the second price auction type RTB exchanges, a bidder will only know the winning price information if his bid win the auction. Most RTB exchanges, especially those with big volume, are highly com-petitive in terms of number of bidders participating in the auction, and that consequent-him for the winning price prediction and that makes his estimation more difficult, price makes the winning price prediction a critical component in the RTB auction. 
Despite all the above challenges on the win rate and winning price prediction tasks, in section 4 that win rate is essentially the cumulative distribution function (cdf) of the corresponding winning price distribution. Solutions of one problem bring the solutions first model the win rate using a logistic regression model, and then take the derivative expected value of the distribution under the bid price as the winning price estimate. 
While win rate and winning price model enables a RTB bidder to predict the like-lihood of success and the associated cost, th e actual programmatic bidding has to be done through a bidding strategy. A bidding strategy is actually an optimization mate), win rate and winning price estimate, and generate the final bid price according to some pre-defined objective functions. Some specific strategies could be to maxim-revenue and profit. 
The paper will be organized as the following. Section 2 reviews some of the exist-ing works in the field. In section 3 and 4 we describe our effort of estimating the win various bidding strategies that we have tested. The experimental setup and results are illustrated in section 6, and we conclude in section 7 with our contributions. Our win rate estimation techniques are based on logistic regression methods, and there are some existing works of estimating various probabilistic events in advertising using contextual effect on click rates [12] and bounce rate in sponsored search [13] etc. To machine learning based approaches as in [3][5][6] and historical observation or simula-tion based approaches [5-8]. R. Schapire et al. [3] used boosting based conditional density estimation in Tac-2001 competition [4]. They treated the price estimation as a approach to estimate the selling price. For bidding strategies, many existing work [3-8] the fact that they are motivated by series of TAC competitions [4]. 
Despite all those existing work, we see very few publications regarding the estima-operations. For example, many of the existing winning price estimation work are based on the assumption that a buyer has complete observation regarding the past auction outcome [3], while this assumption is apparently not valid in our daily operation. 3.1 Logistic Regression Based Win Ra te Estimation and Corresponding The likelihood of winning an auction given a bid price depends on two high level factors. One is the characteristics of the incoming request, and the other is the bid that the request, the size of the available ad-sp ace, the geo location of the user, and many other attributes. The second factor that aff ects our win rate is apparently the bid price itself: for the same request, the higher our bid is, the more likely we will win the auc-tion. Those two factors have to be included in the win rate estimation as features. 
As a probability term bounded between 0 and 1, win rate makes itself a perfect candidate of using logistic regression model [10], as in Eq. 1: , where winRate is the estimated win rate,  X  is the intercept,  X   X  is individual feature extracted from the request, and  X   X  is the corresponding model weight. 
With no surprise, the list of the features that we have constructed resonates with the two high level factors we mentioned above. We extracted various features regard-ing the nature of the request, our bid, and we combine them to make the win rate pre-diction. Some of the features are  X  X tand-alone X  features that describe single attribute called  X  X ross X  features that describe the inter-action between individual  X  X tand-alone X  features. For example, we have a  X  X ross X  feature to describe mobile app name and day of the week that we receive the request. With  X  X tand-alone X  and  X  X ross X  features all together, there are about 1 Million total features in our win rate model. 3.2 Scaling Up Win Rate Prediction wi th Distributed Machine Learning One of the challenges we are facing with win rate prediction, as with many other ma-our win rate prediction model, either as negative or positive data, depending on if our bid win the auction or not. With such huge amount of available data, it will be a crime to down-sample and use only a small percentage of the data in order to fit into exist-ing non-distributed machine l earning toolkit. Instead, we choose to utilize as much of data does help on the prediction accuracy. 
Our approach of utilizing such huge amount of data is to use distributed machine learning algorithm and toolkit, for example Mahout[1] and Vowpal Wabbit(VW) packages[2]. We have tried both packages, and we end up using the VW. VW is more specialized in the classification tasks using general linear learner, while Mahout fo-cuses more on the recommendation, clustering and general machine learning tasks. From our own observations, VW is faster than Mahout, especially for the large-scale sparse training data that we have used. On average our win rate model utilizes about 1 or 2 billion of records as training data for each model update, and the training process can be finished in couple of hours using VW. VW is faster because it uses true paral-lel processing with message passing interface, while Mahout is built on top of the MapReduce framework. 3.3 Feature Selection, Regulariza tion and Missing Feature traditional machine learning questions before we can build an accurate prediction extracted from the available data. As mentioned earlier, the original number of unique features is in the order of million, and we need to avoid directly feeding all those fea-tures into model building process. Typically there are two high level approaches to address this high feature dimension problem: Feature selection and regularization. Feature selection techniques can effectively reduce the number of unique features before model building actually take places, but it has to be done off-line first and it X  X  quite expensive. On the other hand, regularization techniques mix the feature selec-tion process with model building process an d are more efficient than the separate feature selection approaches. Within regularization, L1 regularization automatically regularization can put more emphasis into more discriminant features. In our case, we decided to do regularization directly during model training without a separate feature selection process. 
The second question we need to answer is regarding the new attributes. Regardless of how frequently do we update the model, and even if we use online model updating new mobile apps to their inventory. Almost every week, we see new app become undefined. This significantly affects the accuracy of our prediction model. 
Our solution to address this new attribute problem is to add  X  X iller X  feature into the model building process. During the model training, we will remove certain features and replace those removed features with corresponding  X  X iller X  features to build model. When we make predictions for new attribute values, e.g. a new mobile app (property) name, we just use  X  X iller X  feature to represent the value and generate win rate estimate. The cost in an auction depends on the format of the auction. In the first price auction, winning price is exactly the same as the bid. In the second price auction [14], winning price is the second highest bid that the ad-exchange receives. Due to business con-know is that the winning price is at least the same as his bid, since otherwise he would win the auction. Also, as we mentioned earlier, because there are many bidders in large mobile ad-exchange is in the order of single digit. All the above factors translate into two problems for machine learning based winning price estimation: 1) unba-lanced distribution among positive and negative training data, and 2) missing value issue in the negative training data. 4.1 Linear Regression B Maybe the simplest way o Specifically, we could extr a to fit a linear regression fu n in the followings three way all the features, including p the positive data, the obser the negative data for the c don X  X  contain as valuable i n some information, e.g. the terministic process, for th e always return one fixed w i on the behavior of our co m winning price to fluctuate, Due to those limitations o f perform well in winning pr i 4.2 Solving Win Rate a Fortunately, there is one g o it X  X  not a separate problem Imaging that we have a pr o as in the Fig. 1, then for ev e winning corresponds to th e the bid b , which is the mar k In other words, for each bid b, the win rate is the same as this probability , which is the cumulative distribution function (cdf) of the probability density function and we take the integral up to the bid price, we will have the win rate; if we have the win rate distribution and we take the derivative with respective to the price, we have the winning price distribution. They are dual problems that can be solved with one uniform solution. Assuming the win rate estimation is based on logistic regression as in Eq. (1), we can re-format the Eq. (1) as Eq. (4) C is a constant factor that covers the exponential term of all the features that are unre-lated to the bid price, and  X   X  represents the model weights associated with bid price. 
If we take the derivative of Eq. (4) with respect to the bid price, we will get: , and that is exactly the probability density function (pdf) of the winning price distri-bution given all the attributes of an incoming request. Having the closed form solution on the winning price distribution, we compute the actual winning price for each given bid by taking numerical approximation of the distribution. Having the estimate of win rate and the winning price, we can derive our bidding strategy to automatically calculate the bid for each incoming request. Bidding strategy is indeed an optimization function, it takes as input the monetization capability on the individual request, the win rate estimation function and the corresponding winning different bidding strategies that we use to drive different business objectives. For the win the auction and expected revenue when we places the bid (those two revenue terms are different) , winRate(bid) as win rate estimate for the specific bid, and cost(bid) as the cost for winning the auction. pRev is the  X  X ffective cost per-thousand impression X  (CPM), i.e., what we charge our client for serving thousand impressions for them; cost(bid) is either the winning price in the second price auction, as discussed in section 4, or the bid price itself in the first price auction. Among those terms, pRev is a constant term independent of how do we construct the bid, eRev, cost(bid) and winRate(bid) are all monotonically non-decreasing functions of the bid price. 5.1 Strategy That Maximizes the Revenue The first specific strategy is to maximize the revenue. If we assume that the term pRev is accurate, then whether or not we can realize this revenue pRev solely depends on if we could win the auction and serve the ad. In other words, the expected revenue for each incoming request eRev can be formulated as: The corresponding bidding strategy that maximizes the revenue can then be formu-lated as: Since winRate(bid) is a monotonically non-decreasing function with respect to the bid, the bid* that maximize the Eq. (9) is simply the one that maximizes the win rate, which is pRev . 
Bid with pRev is the optimal revenue generating strategy in the first price auction scenario, as we pay what we bid and we can maximize our bid to the extent that we don X  X  lose money. On the other hand, most of the RTB ad-exchanges that we partici-pate operate on the second price auction mechanism [14], bid with pRev may indeed price is the same as the pRev . This can be described as in Eq. (10):  X  X  X   X   X  X  X  X  X  X  X   X   X  X  X  X  X  X  X   X   X  X  X   X   X  X  X  X  X   X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X , (10) argWinning(x) is a function that returns the bid whose corresponding winning price is function. 5.2 Strategy That Maximizes the Profit business is to make profit, and it quite often that we want to have a bidding strategy that maximizes the profit produced from the business operation. Unlike the revenue, cost increases with our bid. At one hand, if we win the auction, the higher our bid the less profit we would realize; on the other hand, the higher our bid the higher likelih-ood of us winning the auction, and if we don X  X  win the auction, the cost and revenue above factors, which can de described as in Eq. (11): price auction or the winning price estimate of the bid as described in section 4. 
One thing worth noting is that the strategy of maximizing the profit is different from the one that maximize the profit margin. Regardless whether profit margin is defined as the ratio of revenue minus cost over revenue, or directly the revenue over cost, it all has the fixed revenue term as either the denominator or the numerator, and the bid that maximizes the profit margin will simply be the one that minimize the any auction with 0 bid price. This is another reason why we don X  X  use profit margin as the objective for our bidding. 5.3 Strategy That Maximize the Combined Profit and Revenue Goal revenue, we could also combine those two factors together and maximize a combined objective during bidding. Specifically, we can mix the two objective functions togeth-applied to the profit based objective function, then we can make the combined strate-gy as the following: The winrate(bid)*pRev term in the profit and revenue strategy component will indeed cancel the effect of alpha, and we will have the mixed bidding strategy as in Eq. (13): This mixed strategy covers both the revenue and profit objective during business op-v.s. revenue when we bid. When alpha equals to 0, this combined strategy becomes the one that maximize the revenue, as in section 5.1; when alpha becomes 1, this strategy falls back into the profit optimization strategy. In addition to this flexibility, this bidding strategy gives us another advantage in the sense that we can dynamically adjust the value of alpha in real time based on the performances of the bidding system so far. For example, we can set a goal on either the revenue or profit metric, check the progress of the bidding system toward the goal at fixed time intervals, and adjust the alpha value accordingly to hit the pre-defined revenue or profit goal. 6.1 Evaluation Metrics We tested the performance of our various estimation methods and bidding strategies using one of the leading mobile ad exchange platforms that we participate. On every-day we bid on more than a few billion requests in that ad exchange. 
The performance metrics we used to evaluate our methods are the followings:  X  For win rate estimation: We used the log-loss of predicted results collected  X  For winning price estimation: We used two metrics, RMSE and ratio-RMSE.  X  For bidding strategy: We looked at the revenue and profit margin per unit per-6.2 Experiment Setup and Results for the Win Rate Estimation Since we conducted the experiments using domain specific real business operation data, all the numbers reported here were relative performance metrics. Nevertheless, comparing different approaches and id entifying better modeling techniques. 
In the win rate and winning price estimation, we conducted the experiments by splitting the data into training and testing set. Testing set has the data collected from 1 week time period during January 2013. We bu ilt models on the training set and tested the model on the testing data, all done in off-line fashion. 
Our win rate estimation baseline approach was to simply use the historical ob-served win rate. We sliced all the auction winning and auction not winning instances observed in the past time period according to combination of their attributes. For each chunk, we collected all the auction winning instances, divided by the total auction instances to produce the win rate for that specific price chunk given the combination of attributes. 
We tested the baseline method with two different historical time windows. One was baseline_7 and the other was baseline_14 . baseline_7 uses the past 7 days train-ing data and baseline_14 covers 2 week period instead. 
One issue with our baseline approach is that we won X  X  know the win rate for the new apps (new property) or new ad dimension. We used the fall back approach to handle that. Every time we can X  X  find the historical win rate based on the feature com-bination look up mentioned above, we fall back to a combination with one less factor and check if we could find the win rate, and continue to fall back if the historical win rate is still missing. 
The first experiment with our logistic regression model was to use the  X  X tand-window historical data, and the performance is labeled as logistic_standalone_7 and logistic_standalone_14 respectively. 
We then tested the performance of adding  X  X ross X  features into win rate estima-tion. The performances were summarized as  X  X ogistic_all_7 X  and  X  X ogistic_all_14 X  respectively. Table 1 lists the performance metrics of all the win rate estimation methods. 
From table 1, it X  X  clear that our win rate model performs better than baseline. The were statistically significant. 6.3 Experiment Setup and Results for the Winning Price Estimation Experimental results for winning price estimation are listed in table 2. 
The baseline results was achieved by slicing the historical data of winning price rate. We also applied the same fall back logic if there was any attribute value missing. Results are labeled as price_baseline_7 and price_baseline_14 in table 2. 
The second result sets were based on linear regression approaches. The features using 7 and 14 days of data as well. linear_all_7 and linear_all_14 in table 2 are the corresponding results. 
The third set of results, logistic_price, came from logistic regression based me-thods. We calculated the expected value of winning price based on the distribution price. logistic_price_7 was generated using the 7 days of training data, and logis-tic_prce_14 was generated using 14 days of training data. 
Two baseline approaches using different time window yield very similar results, probably due to the fact the mean value of winning price for each combination didn X  X  change that much from 1 week to 2 weeks time period.  X  X ogistic_price X  based win-ning price out-performed the other two approaches, but the lift wasn X  X  substantial. We believe that was due to the intrinsic high variance among the winning prices. Natural-ly, the winning price depends on our competitor X  X  bidding behaviors, and their bid-ding behaviors can be heavily influenced by their business needs. We see quite often individual request to another, in a very short time period of minute. Indeed, we com-puted an  X  X racle X  experiment using the mean value of winning price observed per set (in other words, train and test on the same testing set). The RMSE and ratio-RMSE was 22.8 and 0.34 respectively. This  X  X racle X  experiment performance can be treated as the upper bound of all winning price estimation methods. 6.4 Bidding Strategy Experiment Setup and Results price. While all win rate and winning price prediction approaches can be evaluated offline using previously collected historical data, for bidding strategy it has to be eva-ding strategy drive the bid inside one bucket. We ran multiple of those testing buckets testing buckets based on their relative revenue and profit measurements with respect to the strategy that maxsimize the revenue, and the results are summarized in table 3. Maximizing revenue 1 1 Maximizing profit 0.9 1.35 Combined strategy, alpha=0.3 0.97 1.1 Combined strategy, alpha=0.8 0.95 1.23 
It X  X  clear from table 3 that each strategy does what it supposed to do. We obtained the maximum revenue, with the sacrifice on profit margin using the revenue maximiz-ing strategy, and vice verse for profit maximizing strategy. between different strategies. In general, we observed that it was easier to gain on the profit margin side than to grow the revenue. In our specific example below, in order to grow the revenue by 10% relative from the profit maximizing strategy to the reve-nue maximizing strategy, we need to sacrifice nearly 35% of the profit margin. This is not a surprise to us. We need to bid more to get higher revenue scale, and a higher bid will result in higher cost per unit traffic. In other words, revenue grows linearly with the traffic, while cost grows faster than linear with the traffic scale. We described our effort of estimating win rate, winning price and corresponding bidding strategies in real time bidding (RTB) based ad-exchange in mobile advertising. We explained our effort of building large scale logistic regression based accurate win rate estimation. We have also demonstrated the dual relationship between win rate and winning price in the s econd price auction scenario, revealed that the two problems can be solved with one solution, and proposed a corresponidng winning price estimation method. Based upon the win rate and winning price estiamtion, we outlined various bidding strategies that we used in our daily operation. Comparison data from real business operation confimed the superiority of our proposed methods against various baseline approaches. 
