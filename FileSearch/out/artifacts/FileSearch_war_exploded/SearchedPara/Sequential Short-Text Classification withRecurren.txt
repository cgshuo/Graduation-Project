
Short-text classification is an important task in many areas of natural language processing, includ-ing sentiment analysis, question answering, or dia-log management. Many different approaches have been developed for short-text classification, such as using Support Vector Machines (SVMs) with rule-based features (Silva et al., 2011), combin-ing SVMs with naive Bayes (Wang and Manning, 2012), and building dependency trees with Con-ditional Random Fields (Nakagawa et al., 2010).
Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014;
Kalchbrenner et al., 2014) and recursive neural net-works (Socher et al., 2012).

Most ANN systems classify short texts in iso-lation, i.e., without considering preceding short texts. However, short texts usually appear in se-quence (e.g., sentences in a document or utter-ances in a dialog), and therefore using information from preceding short texts may improve the clas-sification accuracy. Previous works on sequential short-text classification are mostly based on non-ANN approaches, such as Hidden Markov Models (HMMs) (Reithinger and Klesen, 1997; Stolcke et al., 2000; Surendran and Levow, 2006), maximum entropy (Ang et al., 2005), naive Bayes (Lendvai and Geertzen, 2007), and conditional random fields (CRFs) (Kim et al., 2010; Quarteroni et al., 2011).
Inspired by the performance of ANN-based sys-tems for non-sequential short-text classification, we introduce a model based on recurrent neural net-works (RNNs) and CNNs for sequential short-text classification, and evaluate it on the dialog act classi-fication task. A dialog act characterizes an utterance in a dialog based on a combination of pragmatic, se-mantic, and syntactic criteria. Its accurate detection is useful for a range of applications, from speech recognition to automatic summarization (Stolcke et al., 2000). Our model achieves state-of-the-art re-sults on three different datasets. Our model comprises two parts. The first part gener-ates a vector representation for each short text using either the RNN or CNN architecture, as discussed in Section 2.1 and Figure 1. The second part classifies the current short text based on the vector representa-tions of the current as well as a few preceding short texts, as presented in Section 2.2 and Figure 2.
We denote scalars with italic lowercases (e.g., k, b ), vectors with bold lowercases (e.g., s , x i ), and matrices with italic uppercases (e.g., W f ). We use the colon notation v i : j to denote the sequence of vectors ( v i , v i +1 ,..., v j ) . 2.1 Short-text representation
A given short text of length ` is represented as the se-quence of m -dimensional word vectors x 1: ` , which is used by the RNN or CNN model to produce the n -dimensional short-text representation s . 2.1.1 RNN-based short-text representation We use a variant of RNN called Long Short Term
Memory (LSTM) (Hochreiter and Schmidhuber, 1997). For the t th word in the short-text, an LSTM takes as input x t , h t  X  1 , c t  X  1 and produces h t based on the following formulas: ces and b j  X  R n are bias vectors, for j  X  X  i,f,c,o } . The symbols  X  (  X  ) and tanh (  X  ) refer to the element-wise sigmoid and hyperbolic tangent functions, and is the element-wise multiplication. h 0 = c 0 = 0 .
In the pooling layer, the sequence of vectors h 1: ` output from the RNN layer are combined into a sin-gle vector s  X  R n that represents the short-text, us-ing one of the following mechanisms: last, mean, and max pooling. Last pooling takes the last vector, i.e., s = h ` , mean pooling averages all vectors, i.e., s = 1 wise maximum of h 1: ` . 2.1.2 CNN-based short-text representation Using a filter W f  X  R h  X  m of height h, a convolu-tion operation on h consecutive word vectors start-ing from t th word outputs the scalar feature row is x i  X  R m , and b f  X  R is a bias. The symbol  X  refers to the dot product and ReLU (  X  ) is the element-wise rectified linear unit function.

We perform convolution operations with n dif-ferent filters, and denote the resulting features as c  X  R n , each of whose dimensions comes from a distinct filter. Repeating the convolution operations for each window of h consecutive words in the short-text, we obtain c 1: `  X  h +1 . The short-text representa-tion s  X  R n is computed in the max pooling layer, as the element-wise maximum of c 1: `  X  h +1 . 2.2 Sequential short-text classification
Let s i be the n -dimensional short-text representation short text in the sequence. The sequence s i  X  d is fed into a two-layer feedforward ANN that pre-dicts the class for the i th short text. The hyperpa-rameters d 1 ,d 2 are the history sizes used in the first and second layers, respectively.

The first layer takes as input s i  X  d puts the sequence y i  X  d y = tanh trices, b 1  X  R k is the bias vector, y j  X  R k is the class representation , and k is the number of classes for the classification task.

Similarly, the second layer takes as input the se-quence of class representations y i  X  d z  X  R k : weight matrices and bias vector.

The final output z i represents the probability dis-tribution over the set of k classes for the i th short-text: the j th element of z i corresponds to the proba-bility that the i th short-text belongs to the j th class. 3.1 Datasets
We evaluate our model on the dialog act classifica-tion task using the following datasets:  X  DSTC 4: Dialog State Tracking Challenge 4 (Kim et al., 2015; Kim et al., 2016).  X  MRDA: ICSI Meeting Recorder Dialog Act Cor-pus (Janin et al., 2003; Shriberg et al., 2004). The 5 classes are introduced in (Ang et al., 2005).  X  SwDA: Switchboard Dialog Act Corpus (Jurafsky et al., 1997).
 For MRDA, we use the train/validation/test splits provided with the datasets. For DSTC 4 and SwDA, presents statistics on the datasets.
 3.2 Training The model is trained to minimize the negative log-likelihood of predicting the correct dialog acts of the utterances in the train set, using stochastic gradient descent with the Adadelta update rule (Zeiler, 2012). At each gradient descent step, weight matrices, bias vectors, and word vectors are updated. For regular-ization, dropout is applied after the pooling layer, and early stopping is used on the validation set with a patience of 10 epochs. To find effective hyperparameters, we varied one hy-perparameter at a time while keeping the other ones fixed. Table 2 presents our hyperparameter choices.
We initialized the word vectors with the 300-dimensional word vectors pretrained with word2vec on Google News (Mikolov et al., 2013a; Mikolov et al., 2013b) for DSTC 4, and the 200-dimensional word vectors pretrained with GloVe on Twit-ter (Pennington et al., 2014) for MRDA and
SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe,
SENNA (Collobert, 2011; Collobert et al., 2011) and RNNLM (Mikolov et al., 2011) word vectors.
The effects of the history sizes d 1 and d 2 for the short-text and the class representations, respectively, are presented in Table 3 for both the LSTM and
CNN models. In both models, increasing d 1 while keeping d 2 = 0 improved the performances by 1.3-4.2 percentage points. Conversely, increasing d 2 while keeping d 1 = 0 yielded better results, but the performance increase was less pronounced: incor-porating sequential information at the short-text rep-resentation level was more effective than at the class representation level.

Using sequential information at both the short-text representation level and the class representa-tion level does not help in most cases and may even lower the performances. We hypothesize that short-text representations contain richer and more gen-eral information than class representations due to their larger dimension. Class representations may not convey any additional information over short-text representations, and are more likely to propa-gate errors from previous misclassifications.
Table 4 compares our results with the state-of-the-art. Overall, our model shows competitive results, while requiring no human-engineered features. Rig-orous comparisons are challenging to draw, as many important details such as text preprocessing and train/valid/test split may vary, and many studies fail to perform several runs despite the randomness in some parts of the training process, such as weight initialization.
 In this article we have presented an ANN-based ap-proach to sequential short-text classification. We demonstrate that adding sequential information im-proves the quality of the predictions, and the per-formance depends on what sequential information is used in the model. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.

We warmly thank Regina Barzilay, Tommi Jaakkola and the anonymous reviewers for their helpful feed-back and suggestions.

