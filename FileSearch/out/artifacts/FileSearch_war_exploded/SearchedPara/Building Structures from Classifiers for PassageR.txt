 This paper shows that learning to rank models can be ap-plied to automatically learn complex patterns, such as rela-tional semantic structures occurring in questions and their answer passages. This is achieved by providing the learning algorithm with a tree representation derived from the syn-tactic trees of questions and passages connected by relational tags, where the latter are again provided by the means of automatic classifiers, i.e., question and focus classifiers and Named Entity Recognizers. This way effective structural relational patterns are implicitly encoded in the representa-tion and can be automatically utilized by powerful machine learning models such as kernel methods.

We conduct an extensive experimental evaluation of our models on well-known benchmarks from the question answer (QA) track of TREC challenges. The comparison with state-of-the-art systems and BM25 show a relative improvement in MAP of more than 14% and 45%, respectively. Further comparison on the task restricted to the answer sentence reranking shows an improvement in MAP of more than 8% over the state of the art.
 I.2.7 [ Natural Language Processing ]: [Language parsing and understanding, Text analysis] Algorithms, Experimentation Question Answering; Learning to Rank; Kernel Methods; Structural Kernels
Automated Question Answering (QA) is a complex task that often requires manual definition of rules and syntactic patterns to detect the relations between a question and its candidate answers in text fragments. Simple heuristics just refer to computing a textual similarity between the question and one of its answer passages but the most accurate method is to manually design specific rules that are triggered when patterns in the question and in the passage are found. Such rules are based on syntactic and semantic patterns. For example, given a question 1 : and a relevant passage, e.g., retrieved by a search engine: the QA engineer usually applies a syntactic parser to obtain the parse trees of the above two sentences, e.g., like those in the top of Fig. 3. Then she/he derives rules like: where the NPs are noun phrases and ADJ is an adjectival phrase recognized by the syntactic parser.

Previous work, e.g., carried out in TREC 3 [46, 47, 48], has shown that such an approach can lead to the design of accurate systems. However, it suffers from two major draw-backs: (i) being based on heuristics it does not provide a definitive methodology, since natural language is too com-plex to be characterized by a finite set of rules; and (ii) given the latter claim, new domains and languages require the definition of new specific rules, which in turn require a large engineering effort.

An alternative to manual rule definition is the use of ma-chine learning, which often shifts the problem to the easier task of feature engineering. This is very convenient for sim-ple text categorization problems, such as document topic classification, where simple bag-of-words models have been shown very effective, e.g., [21]. Unfortunately, when the learning task is more complex such as in QA, features have to encode the combination of syntactic and semantic prop-erties, which basically assume the shape of high-level rules: these are essential to achieve state-of-the-art accuracy. For example, the famous IBM Watson system [14] also uses a We use this question/answer passage (q/a) pair from TREC QA as a running example in the rest of the paper.
If the point-wise answer is needed rather than the entire passage, the rule could end with: returns NP 1 http://trec.nist.gov/data/qamain.html learning to rank algorithm fed with hundreds of features. For the extraction of some of the latter, articulated rules are required, which are very similar to those constituting typical manually engineered QA systems.

In this paper, we show that learning to rank models can be applied to automatically learn structural patterns. These are relational structures occurring in question and answer passages and are based on the semantic information au-tomatically derived by additional automatic classification modules. In particular, we first derive a representation of the question and answer passage (q/a) pair, where we fol-low our approach in [39] by engineering a pair of shallow syntactic trees connected with relational nodes (i.e., those matching the same words in the question and in the answer passages).

Secondly, we include a large sample of basic and advanced features that represent a strong baseline model for answer passage reranking. Additionally, we explore various methods in addressing the  X  X exical gap X  problem: words in a ques-tion and its candidate answer can be semantically similar but having different surface forms. To establish relational links between a question and an answer, we exploit Word-Net hierarchy [25], LDA topic models [9, 20], SuperSense tagging [10] and word alignments from translation models, e.g., [38, 43]. Their failure in improving our structural repre-sentation provides a strong indication that word generaliza-tion is not sufficient to improve on strong statistical-based retrieval systems, and more principled linking strategy is required.

To enable a more principled linking strategy, we model and implement question and focus classifiers based on ker-nel methods. Then, we use the output of such classifiers together with a named entity recognizer (NER) to establish relational links [40]. The focus classifier determines the con-stituent of the question to be linked to the named entities (NEs) of the answer passage. The target NEs are selected based on the compatibility of their category and the category of the question, e.g., an NE of type PERSON is compatible with a category of a question asking for a HUMAN.

Next, we provide extensive experiments on TREC QA combining our models with traditional feature vectors and the improved relational structures. The results show that our methods greatly improve over previous state-of-the-art QA systems for answer sentence selection. In particular, dif-ferently from previous work [39], our models can effectively use NERs and the output of different automatic modules. These improve the system in [39] by more than 14% in MAP and also provide promising directions for fully exploiting se-mantic resources.

We show that highly discriminative features can be, in fact, automatically extracted and learned by using our ker-nel learning framework. Our approach does not require man-ual feature engineering to represent input structures. We do not fully rely on traditional similarity features that encode the degree of similarity between a question and its answer. We treat the input q/a pairs directly encoding them into linguistic trees. More powerful features can be encoded by injecting additional semantic information directly into the tree via special tags and additional tree nodes. We believe such way of engineering features is more intuitive and re-quires less effort, since the final patterns are automatically extracted by expressive tree kernels.
 Figure 1: Kernel-based Answer Passage Reranking Sys-
In the reminder of this paper, Section 2 describes our kernel-based reranker, Section 3 illustrates our relational structures along with the classifiers used to generate seman-tic information. Section 4 reports on the baseline feature vectors used by our rerankers. Section 5 contains answer passage reranking experiments on TREC QA data, while Section 6 compares our models for answer sentence rerank-ing to the current state-of-the-art systems. Section 7 reports on the related work; and finally, Section 8 derives the con-clusions.
Our QA system is based on a rather simple reranking framework as displayed in Figure 1: given a question q , a search engine retrieves a list of candidate passages ranked by their relevancy. Next, the question together with its can-didate answers are processed by a rich natural language pro-cessing pipeline that performs basic tokenization, sentence splitting, lemmatization, stopword removal. Various NLP components embedded in the pipeline as UIMA 4 annota-tors perform more involved linguistic analysis, e.g., part-of-speech tagging, chunking, named entity recognition, con-stituency and dependency parsing, etc. These annotations are then used to produce structural models (described in Sec. 3), which are further used by a question focus detector and question type classifiers to establish relational links for a given q/a pair. The resulting tree pairs are then used to train a kernel-based reranker, which outputs the model to refine the initial ordering of the retrieved answer passages.
As pointed out in the introduction, engineering rules and features is the major bottleneck in the design of a QA sys-tem. To tackle this problem, one viable approach is the use of kernel methods (see [27]). A kernel function computes an implicit scalar product between input examples, typically in high dimensional spaces. In our case, they measure simi-larity between structural objects, e.g., parse trees, in terms of the number of common substructures. Different kernels map objects in different spaces. In this paper, we make use of two types of tree kernels applied to several types of syn-tactic/semantic structures: http://uima.apache.org/ Syntactic Tree Kernel (STK) also known as a subset tree kernel (SST) [11], which maps objects in the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. In other words, substructures are composed of atomic building blocks corresponding to nodes along with all of their direct children. These, in case of a syntactic parse tree, are com-plete production rules of the associated parser grammar. For example, given the parse tree of the question in Fig. 3, some of its syntactic fragments are shown in Fig. 2.(a). It can be noted that each of such fragment can be an important pat-tern indicating the type of question. For example, the first fragment encodes the first part of the manually designed rule described in the introduction, where REL is a relation tag, which is clarified in Sec. 3.
 Partial Tree Kernel (PTK) [26] is a function that can be effectively applied to both constituency and dependency parse trees. It generates all possible tree fragments, as above, and sibling nodes can be also separated (so they can be part of different tree fragments). In other words, a frag-ment is any possible tree path, from whose nodes other tree paths can depart. Consequently, an extremely rich feature space is generated, which results in higher generalization ability. For example, Fig. 2.(b) illustrates some fragments only generated by PTK: such patterns are more general and compact than those provided by STK. To enable the use of kernels for learning to rank with SVMs, we use preference reranking [22], which reduces the task to binary classification. More specifically, the prob-lem of learning to pick the correct candidate h i from a candidate set { h 1 ,...,h k } is reduced to a binary classifi-cation problem by creating pairs : positive training instances  X  h 1 ,h 2  X  ,...,  X  h 1 ,h k  X  and negative instances  X  h 2 ,h h  X  . This set can then be used to train a binary classifier. At classification time the standard one-versus-all binarization method is applied to form all possible pairs of hypotheses. These are ranked according to the number of classifier votes they receive: a positive classification of  X  h k ,h i  X  gives a vote to h k whereas a negative one votes for h i .

A vectorial representation of such pairs is the difference between the vectors representing the hypotheses in a pair. However, this assumes that features are explicit and already available whereas we aim at automatically generating im-plicit patterns with kernel methods. Thus, for keeping im-plicit the difference between such vectors we use the follow-ing preference kernel: where h i and h 0 i refer to two sets of hypotheses associated with two rankings and K is a kernel applied to pairs of hy-potheses. It should be noted that we represent the latter as pairs of question and answer passage trees. More for-mally, given two hypotheses, h i =  X  h i ( q ) ,h i ( a )  X  and h  X  h i ( q ) ,h 0 i ( a )  X  , whose members are the question and answer passage trees, we define K ( h i ,h 0 i ) as where TK can be any tree kernel function, e.g., STK or PTK. Finally, it should be noted that, to add traditional fea-ture vectors to the reranker, it is enough to sum the product ( ~x h 1  X  ~x h 2 )  X  ( ~x h 0 1  X  ~x h 0 2 ) to the structural kernel P ~x h is the feature vector associated with the hypothesis h .
In this section we present our structural models aimed at capturing structural similarities between a question and an answer. We use tree structures as our base representation since they provide sufficient flexibility in representation and allow for easier feature extraction than, for example, graph structures. In addition, trees allow for efficient automatic feature engineering.
We first describe our shallow models that we explored in [39]. Next, we propose models to bridge the lexical gap between the words in the question and in the answer pas-sages, using various sources of semantic annotation.
Our baseline structural model is the shallow tree represen-tation we first proposed in [39]. This is essentially a shallow syntactic structure built from part-of-speech tags grouped into chunks. Each question and its candidate answer passage are encoded into a tree where part-of-speech tags are found at the pre-terminal level and word lemmas at the leaf level. The sequences of part-of-speech (POS) tags are further or-ganized into chunks. To encode the structural relationship for a given q/a pair, a special REL tag links the related struc-tures. We adopt a simple strategy to determine such links: lexicals from a question and an answer that have a common lemma get their parents (POS tags) and grandparents, i.e., chunk labels, marked by prepending a REL tag. An example of a q/a pair encoded using shallow chunk trees is given in Figure 3 (top). Our empirical evaluation in [39] showed that such representation is superior to more simple bag-of-words and sequences of POS tags models.
As shown in [39], the use of a special tag to mark the related fragments in the question and answer tree represen-tations is the key to learn more accurate relational models. However, we just used a na  X   X ve hard matching between the word lemmas. To alleviate the word sparsity problem, where semantically similar words have non-matching surface forms, we explore WordNet, topic models, SuperSense tagging and word alignments used in statistical machine translation. Wordnet. To establish the matching between the struc-tures in the question and a candidate answer we experi-ment with using synonym groups provided by WordNet. We also experiment with a similarity metric computed between words w 1 and w 2 using the WordNet concept hierarchy that defines hyponym/hypernym relations between the words: where d defines the distance in the hierarchy between two concepts, CP denotes a common parent of two words and R is the root of the WordNet hierarchy. We consider a match between two words if their sim W N is below a specified threshold value. We tried various thresholds and found 0.2 to serve as a meaningful boundary. We did not use any sense disambiguation algorithms and opted for a simple strategy to take the most frequent sense (this seems the most effective approach, according to previous studies, e.g., [44]). LDA. It has become a popular tool in discovering deeper relationships between q/a pairs, e.g., [20, 9]. It comes from a family of generative probabilistic models, where each doc-ument d in the collection D is viewed as a mixture of a fixed number of topics z  X  Z . Each topic z represents a distri-bution over the unique words w  X  V in the vocabulary V . More specifically, given a training corpus, an LDA model infers two distributions:  X  ( w ) z , the probability of a word w being assigned to a topic z , and  X  ( d ) which defines a distri-bution of topics for a document d .

Different from previous applications of LDA, where it is primarily used to compute various similarity scores for a given q/a pair, we consider using the obtained vector of topic assignments to each word in the document as a way to generalize hard matching on lemmas. Table 1 gives an example of several topics learnt from a large Aquaint corpus Table 1: Top 5 words from 5 randomly picked LDA from TREC QA. It exemplifies that topics can be used to cluster words into semantically coherent groups. Hence, we explore them to link related fragments in a q/a pair. SuperSense matching. We explore an alternative ap-proach to link question and answer words that belong to the same semantic category. For this purpose we use a Su-perSense tagger 5 [10] that annotates words in a text with the tagset of 41 Wordnet supersense classes for nouns and verbs, e.g., act, event, relation, change, person, motion , etc. Words in a question and answer that have the same tag are used to link the related structures.
 Word alignment. Recently, the utility of translation mod-els to alleviate the lexical gap between questions and an-swers has been explored by many QA systems, e.g., [43, 38, 45]. The typical approach is to learn question-to-answer and answer-to-question transformations using a translation model. The translation model is finally used to compute the similarity score relating a given q/a pair, which is then integrated as a similarity feature into the learning to rank model [45]. Different from the previous approaches, we ex-plore the utility of a translation model to align words in a given q/a pair for relational linking. To obtain the align-ments, we use METEOR 6 monolingual word aligner [13] that includes flexible word and phrase matching using exact, syn-onym and paraphrase matches. Similar to the string match-ing strategy, we mark the structures spanning the aligned words with a relational tag.
In this section we briefly describe our alternative strategy to establish relational links first proposed in [40]. We use question category to link the focus word of a question with the named entities extracted from the candidate answer. For this purpose, we briefly introduce our models for building a question focus detector and question category classifier. http://sourceforge.net/projects/supersensetag/ http://www.cs.cmu.edu/~alavie/METEOR
The question focus is typically a simple noun represent-ing the entity or property being sought by the question [34]. It can be used to search for semantically compatible candi-date answers in document passages, thus greatly reducing the search space [33]. While several machine learning ap-proaches based on manual features and syntactic structures have been recently explored, e.g., [35, 12, 8], we opt for the latter and rely on the power of tree kernels to handle auto-matic feature engineering.
Question classification is the task of assigning a question to one of the pre-specified categories. We use the coarse-grain classes described in [23]: six non overlapping classes: Abbreviations (ABBR), Descriptions (DESC, e.g., defini-tions or explanations), Entity (ENTY, e.g., animal, body or color), Human (HUM, e.g., group or individual), Loca-tion (LOC, e.g., cities or countries) and Numeric (NUM, e.g., amounts or dates). These categories can be used to determine the Expected Answer Type for a given question and find the appropriate entities found in the candidate an-swers. Imposing such constraints on the potential answer keys greatly reduces the search space.
Question focus captures the target information need posed by the question but, to make this piece of information ef-fective, the focus words need to be linked to the target can-didate answer. They can be lexically matched with words present in an answer, or the match can be established us-ing semantic information. One method exploiting the latter approach involves question classification.

Once the question focus and class are determined, we pro-pose to link the focus word w focus in the question, with all the named entities whose type matches the question class. Table 2 provides the correspondence between the question classes and named entity types. We perform tagging at the chunk level and use two types of the relational tag: plain REL-FOCUS and a tag typed with the question class, e.g., REL-FOCUS-HUM . Fig. 4 shows an example q/a pair where the typed relational tag is used to link the chunk containing the question focus word name with the named entities of the corresponding type Person (according to the mapping defined in Table 2).
While the primary focus of our study is on the structural representations and relations between the q/a pairs we also include some basic and more advanced features widely ap-plied in QA. We use several similarity functions between q and a, computed over various input representations to form a feature vector. These feature vectors are used along with the structural models.
 N-gram overlap features. We compute a cosine similar-ity over question and answer: sim COS ( q,a ), where the input vectors are composed of: (i) word lemmas, (ii) bi-grams, (iii) part-of-speech tags, (iv) topics, and (v) dependency triplets. For the latter, we simply hash the string value of the pred-icate defining the triple together with its argument, e.g., poss(name, twain) . We also generalize the arguments of the predicates by using topics instead of words.
 Tree kernel similarity. For the structural representa-tions we also define a similarity based on the PTK score: sim P T K ( q,a ) = PTK ( q,a ), where the input trees can be raw constituency trees and shallow chunk trees used in struc-tural representations. Note that this similarity is computed between the members of a q/a pair, thus, it is very differ-ent from the one defined by Eq. 1. We also compute these features over the trees where the lexicals are replaced with their associated topics.
 NER relatedness. We also compute a feature that repre-sents a match between a question category and the related named entity types extracted from the candidate answer. We simply count the proportion of named entities in the an-swer that correspond to the question type returned by the question classifier.
 LDA. The similarity between a question q and a candidate answer a can be captured by the similarity between their topic distributions  X  ( q ) and  X  ( a ) . For this purpose, we use the symmetrized KL divergence: LDA provides yet another way to compute the similarity between two documents using conditional probabilities of one document given the topic distribution of the other. For a question q and its candidate answer a it can be estimated as follows: where the last term P ( z = t | a ) is simply the probability of a topic z = t under the topic distribution of an answer a . Hence, we define two similarities : sim LDA 1 ( q,a ) = P ( q | a ) and sim LDA 2 ( q,a ) = P ( a | q ), which compute the probability of the question being generated from the topic distribution of the answer and vice versa. Differently from the features derived from translation-based language models [53], which extract knowledge from q/a pairs, topic models use the dis-tribution of words over the entire collection.

The total number of our basic features is 24. Although far from being complete, our features represent a good sample of typical features used in many QA systems to rank candidate answers. Nevertheless, in our study feature vectors serve a complementary purpose, while the main focus is to study the virtue of structural representations for reranking. The effect of a more extensive number of features computed for the q/a pairs has been studied elsewhere, e.g., [45].
In these experiments, we evaluate our kernel models ex-ploiting pairwise structural relationships on the answer pas-sage re-ranking task, where the data is derived from the factoid open-domain QA corpus of TREC QA. SVM re-ranker. To train our models, we use SVM-light-TK 7 , which enables the use of structural kernels [26] in SVM-light [22]. We use default parameters as described in [39]. We choose PTK as the re-ranker kernel to estab-lish pairwise similarities between tree structures, since PTK is the most general kernel able to generate a vast number of tree fragments. It is also particularly suitable for tree repre-sentations using dependency structures. For explicit feature vectors we use polynomial kernel of degree 3, as it has better discriminative power w.r.t. linear kernel.
 LDA model. To train an LDA model we use an imple-mentation of a parallelized Gibbs sampler from MALLET 8 library. We fix the number of iterations of the Gibbs sam-pler to 1000 and fix the other parameters as their default values. Since the LDA implementation performs an auto-matic tuning of prior parameters for the document-topic and word-topic distributions, we fixed them at their default val-ues. As an input training data we perform basic stopword removal and lemmatization. At the inference time on the unseen test documents we set the number of iterations of the Gibbs sampler to 300.
 Metrics. To measure the impact of the re-ranker on the output of our QA system, we use metrics most commonly used to assess the accuracy of QA systems: Precision at rank 1 (P@1), Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP). P@1 is the percentage of questions with a correct answer ranked at the first position, MRR is com-the position of the first correct answer in the candidate list. For a set of queries Q MAP is the mean over the average precision scores for each query: 1 Q P Q q =1 AveP ( q ). Pipeline. We built the entire processing pipeline on top of the UIMA Framework, which provides a convenient abstrac-tion for developing NLP annotators for analyzing unstruc-tured content.

We have included many off-the-shelf NLP tools wrapping them as UIMA annotators to perform sentence detection, tokenization, Named Entity Recognition, parsing, chunking and lemmatization. We included other tools such as the http://disi.unitn.it/moschitti/Tree-Kernel.htm http://mallet.cs.umass.edu Stanford Parser, the Berkeley parser, the ClearTK depen-dency parser, the Illinois Chunker, the RitaWordnet Lem-matizer, Mallet for LDA and the Snowball stemmer. More-over, we created annotators for building new sentences rep-resentations starting from tools X  annotations. For example, the component producing a tree representations containing POS tags and chunks outputs the result as a UIMA anno-tation. Fully integrated into the pipeline, there are also the question focus and question classifiers.
Our experiments are carried out on TREC QA data, which is widely used in the evaluation of QA systems. In this task, answer passages containing correct information nuggets, i.e. answer keys, have to be extracted from a given text cor-pus, typically a large newswire corpus. It should be noted that the passages may contain multiple sentences. This set-ting is thus rather different from the one we used in Sec. 6. In our experiments, we opted for questions from 2002 and 2003 years (TREC 11-12), which totals to 824 ques-tions. AQUAINT newswire corpus is used for searching the supporting answers. To train Question Focus and Category classifiers we follow the same setup as described in [40].
For the questions from TREC 11-12 the supporting corpus is AQUAINT which represents a large collection of newswire text (3Gb) containing about 1 million documents. We per-form indexing at the paragraph level by splitting each doc-ument into a set of paragraphs which are then added to the search index. The resulting index contains about 12 million items. For the Answerbag we index the entire 180k answers. For both TREC and Answerbag we retrieve a list of 50 can-didate answers for each question.

We tested 3 open-source search-engines: Lucene, Whoosh and Terrier. The results are given in Table 3. Lucene im-plements the most basic retrieval model based on the co-sine similarity and represents the weakest baseline, while Whoosh and Terrier, which use a more accurate BM25 scor-ing model, demonstrate better performance. Hence, in our further experiments we use BM25 implemented by Terrier as our baseline model.
To evaluate the performance of our retrieval model, we evaluate the following models previously introduced in Sec. 3: BM25 -initial ranking obtained by the BM25 search engine model.
 CH -our shallow chunk tree model in proposed [39]. V -reranker model using the set of features defined in Sec. 4. CH+V -a combination of tree structures encoding q/a pairs with similarity scores stored in the feature vector. F -relational linking of the question focus word and named entities of the corresponding type using Focus and Question classifiers. Table 4: Answer passage reranking on TREC QA.  X  Table 5: CH+V (LEMMA) representation. Relational TF -a typed relational link refined with the question cate-gory.

Table 4 reveals that using feature vectors provides a good improvement over BM25 baseline. Most interestingly, the structural representations give a bigger boost in the perfor-mance. Combining the structural and feature vector repre-sentation in a single model results in further improvement.
Here, we explore the effect of using word similarity mea-sures derived from WordNet, SuperSense tags, word align-ments and LDA topic models on the performance of the reranker. We test WordNet synonym groups (SYN) and a distance (DIST) metric between two concepts to identify se-mantically close words. For the latter we fix the similarity threshold at 0.2. For the topic model we match words based on their topic assignments obtained when performing infer-ence on the unseen documents. We train a set of LDA mod-els using a fixed number of topics Z  X  { 50 , 100 , 250 , 500 } . Table 5 shows that using SSENSE or LDA topic match-ing actually results in lower performance w.r.t. plain string matching used by CH+V. Additionally, using WordNet and word alignment for relational linking does not provide any interesting improvement. Our intuition is that most of the answer candidates have a relatively high word overlap with the question (since search engine retrieves candidates based on metrics derived from word-overlap measures), hence us-ing a plain string match on lemmas results in a rather good coverage of words between question and answer. Differently, using coarser word classes from SuperSense tagger and LDA results in much higher linking but, at the same time, they add a considerable large amount of noise. Thus, the use of larger coverage linking strategies seems to requires a defini-tion of a more principled approach.

We provided a possible solution in [40], where we explore a set of supervised components to semantically link impor-tant concepts between questions and answers. We include such strategy in our experiments to build an even stronger baseline. The next section briefly reports the results using such linking models.
In the following set of experiments, we test another strat-egy for linking structures for a given q/a pair. We use an automatically detected question focus word and a question category obtained from the question classifier to link the focus word with the related named entities in the answer (namely model F). Then, we refine the relational link by typing it with the question category (namely model TF).
Table 4 summarizes the performance of the CH+V model when coupled with F and TF strategies to link structures in a given q/a pair. The structural representations with F yields an interesting improvement, while further refining the relational tag by adding a question category (TF) gives no improvement with CH structure.
In this section, we test our structural CH and CH+F mod-els on a sentence reranking task. For the latter several stud-ies using a common dataset are available, thus it gives us the possibility to compare with several reranking systems on exactly the same test set. First, we briefly describe the experimental setup to replicate the setting of the previous work. Then, we propose an additional set of features to build a strong feature vector baseline model and finally, we compare our models to the previous state-of-the-art systems.
We test our models on the manually curated 9 TREC QA dataset 10 from Wang et al. [50], which has been widely used for comparison of answer rerankers in previous work 11 .
In their setup, 100 manually judged questions from TREC 8-12 are used for training, while questions from TREC-13 are used for testing. Additionally, Wang et. al [50] provide a  X  X oisy setting X  experiment where 2,393 questions from the entire TREC 8-12 collection is used for training. This results in a lower performance of their system, which is explained by the inclusion of erroneous candidate answers treated as
Manual judgement of candidate answer sentences was car-ried out for the entire TREC 13 set and for the first 100 questions from TREC 8-12. The motivation behind this annotation effort is that TREC provides only the answer patterns to identify if a given passage contains a correct an-swer key or not. This results in many unrelated candidate answers marked as correct simply because they happen to contain a regex match with the answer key. http://cs.stanford.edu/people/mengqiu/data/ qg-emnlp07-data.tgz
Different from the experiments above, where we learn a candidate answer reranker over paragraphs, here the re-trieved answer candidates represent a single sentence. correct by TREC. Nevertheless, we compare with their best results that they obtained using their manually judged train-ing set.

While we use the same test collection, our training data consists of only 824 questions from TREC 2002, 2003 (TREC 11-12), since TREC 8-10 require a separate license. To gen-erate the candidate answer sentences, we extract sentences from the previously retrieved paragraphs (as used in Sec-tion 5) that have at least one non-stopword in common with the question. We build examples for our SVM reranker by pairing each correct candidate sentence with at most top 10 incorrect candidates, which results in total 8,730 examples.
This section describes a set of more involved features to model the similarity of q/a pairs. The features below are largely inspired by the feature sets used by the top perform-ing system [6] in Semantic Textual Similarity (STS) task [2]. Additional word-overlap measures. We include ad-ditional word-overlap similarity metrics over lemmas for a given q/a pair by computing the longest common substring (subsequence) measures, and Greedy String Tiling. The longest common substring measure [16] determines the length of the longest string which is also a substring shared by a pair of text fragments. The longest common subsequence measure [4] considers as subsequence also strings that dif-fer from word insertions or deletions. Greedy String Tiling [51] detects similarity of reordered text parts as it is able to identify multiple shared contiguous substrings.
 Knowledge-based word similarity. We compute Resnik similarity [37] which is based on the WordNet hypernymy hi-erarchy and on semantic relatedness between concepts. The hierarchy is used to find a path between two concepts. Then, the semantic relatedness is determined by the lowest com-mon concept subsuming both of them. The specificity of the subsuming concept affects the similarity measure: more specific concepts contributes more than generic ones. The aggregation strategy by Mihalcea et al. [24] is applied to scale the measure from pairs of words to sentences. Explicit Semantic Analysis (ESA) Similarity. ESA [15] maps a document into a vector of concepts extracted from Wikipedia. Thus, the meaning of text fragment is modeled by a set of natural concepts, which are described and defined by humans. Moreover, we used WordNet and Wiktionary as additional concepts.
 Lexical Substitution. A supervised word sense disam-biguation system [7] finds substitutions for a wide selection of frequent English nouns. Resnik and ESA features are then computed adding the substitutions to the text. This feature enables additional matches aleviating the lexical gap between text fragments.
 Translation model. We integrate two similarity score features obtained from the METEOR scorer when treating both a question and a candidate as a translation source.
This feature set adds 19 more features to our basic features from Section 4, which results in total 43 features for our advanced vector-based model ( V adv ).
Table 6 compares V adv and CH structural model coupled with F relational linking strategy with the previous state-of-the-art systems 12 . In particular, we compare to four most
P@1 metric is omitted since it is not reported in the previ-ous work.
 Table 6: Answer sentence reranking on TREC 13.  X  recent state-of-the-art reranker models [50, 17, 49, 52] that report their performance on the same questions and candi-date sets from TREC 13 as provided by [50].

First note that, our combined set of basic and advanced features V adv already represents a rather strong baseline model. Furthermore, combining it with CH representations gives state-of-the-art performance providing an improvement over the previous work with a large margin 13 . Finally, aug-menting CH representation with F linking strategy yields additional 2 points in MAP and one point in MRR. This strongly indicates on the utility of using supervised compo-nents, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair.

Our kernel-based learning to rank approach is conceptu-ally simpler than approaches in the previous work, as it re-lies on the structural kernels, e.g., PTK, to automatically extract salient syntactic patterns relating questions and an-swers. Moreover, the computational complexity of previous approaches limited their application to reranking of answer sentences, while our approach is demonstrated to work well also at the paragraph level.
Work in QA shows that semantics and syntax are essential to retrieve answers, e.g., [18, 42]. However, most approaches in TREC were based on many complex heuristics and fine manual tuning, which require large effort for system engi-neering and often made the result not replicable. In con-trast, our passage re-ranker is adaptable to any domain and can be also used as front end of more complex QA systems.
Previous studies closely to ours carry out passage rerank-ing by exploiting structural information. In this perspective, a typical approach is to use subject-verb-object relations, e.g., as in [5]. Unfortunately, the large variability of natu-ral language makes such triples rather sparse thus different methods explore soft matching (i.e., lexical similarity) based on answer types and named entity types, e.g., see [3]. Pas-sage reranking using classifiers of question and answer pairs were proposed in [36, 19]. In this context, several approaches focused on reranking the answers to definition/description questions, e.g., [41, 31, 45]. [1] propose a cascading learn-ing to rank approach, where the ranking produced by one ranker is used as input to the next stage.

Regarding kernel methods, the work in [31, 27, 28, 29, 30, 39] were the first to exploit tree kernels for modeling
Given the fact that our system was trained on a smaller subset of TREC 8-12, we expect a potential increase in ac-curacy when trained on the full data. answer re-ranking. However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate rela-tional patterns. In this respect, a solution based on enumer-ating relational links was given in [55, 56] for the textual entailment task but it is computationally too expensive for the large dataset of QA. Some faster versions were provided in [32, 54], which may be worth to try. In contrast, we design our re-ranking models with shallow trees encoding the output of question and focus classifiers connected to the NE information derived from the answer passage. This pro-vides more effective relational information, which allows our model to significantly improve on previous rerankers.
Our relational structures are based on a shallow tree from [39] and we reuse our semantic linking strategy from [40], where question focus is linked to the related named enti-ties (as identified by the question category). Additionally, this paper studies a number of linking strategies to establish connections between related tree fragments from questions and answer passages. Regarding the experimental evalua-tion, different from our previous work, where experiments were conducted for an answer passage reranking task on a subset of TREC QA data, in this work, we also include ex-periments for the answer sentence selection task on a pub-lic TREC 13 benchmark, such that we can compare to the previous state-of-the-art methods. We show that highly dis-criminative features can be, in fact, automatically extracted and learned by using our kernel learning framework. Our approach does not require manual feature engineering to en-code input structures via similarity features. We treat the input q/a pairs directly encoding them via linguistic trees, and more powerful features can be encoded by injecting ad-ditional semantic information directly into the tree nodes.
Regarding previous state of the art in answer sentence rerankers, Wang et al., 2007 [50] use quasi-synchronous gram-mar to model relations between a question and a candi-date answer with the syntactic transformations. Heilman &amp; Smith, 2010 [17] develop an improved Tree Edit Distance (TED) model for learning tree transformations in a q/a pair. They search for a good sequence of tree edit operations using complex and computationally expensive Tree Kernel-based heuristic. Wang &amp; Manning, 2010 [49] develop a probabilis-tic model to learn tree-edit operations on dependency parse trees. They cast the problem into the framework of struc-tured output learning with latent variables. The model of Yao et al., 2013 [52] applies linear chain CRFs with fea-tures derived from TED to automatically learn associations between questions and candidate answers.
This paper shows a viable research direction in the au-tomatic QA engineering. One of its main characteristics is the use of structural kernel technology to induce rich feature spaces from structural semantic representations of question and answer passage pairs. The same technology is also used to provide adaptable question and focus classifiers, which provides the learning to rank algorithm with important re-lational information.

We introduce a powerful feature-based model that when combined with the CH model and coupled with focus link-ing improves the previous state-of-the-art systems with a large margin. Moreover, our approach is conceptually sim-pler than previous systems, as it relies on the kernel-based learning and simple structures, which can be built using off-the-shelf syntactic parsers and NERs, along with little train-ing data for the question/focus classifiers.
 Finally, the inefficacy of using topic models, WordNet, SuperSense and word alignment studied in this paper sug-gests that information produced by unsupervised methods has still to be carefully considered. Therefore, studying ways to utilize semantic resources at their best is a natural future extension of this paper.
This research is partially supported by the EU X  X  7 th Frame-work Program (FP7/2007-2013) (#288024 LiMoSINe project) and an Open Collaborative Research (OCR) award from IBM Research. The first author is a recipient of the Google Europe Fellowship in Machine Learning, and this research is supported in part by this Google Fellowship. [1] A. Agarwal, H. Raghavan, K. Subbian, P. Melville, [2] E. Agirre, D. Cer, M. Diab, and Gonzalez-Agirre. [3] E. Aktolga, J. Allan, and D. A. Smith. Passage [4] L. Allison and T. I. Dix. A bit-string [5] G. Attardi, A. Cisternino, F. Formica, M. Simi, and [6] D. Bar, C. Biemann, I. Gurevych, and T. Zesch. Ukp: [7] C. Biemann. Creating a system for lexical [8] R. Bunescu and Y. Huang. Towards a general model [9] A. Celikyilmaz, D. Hakkani-Tur, and G. Tur. Lda [10] M. Ciaramita and Y. Altun. Broad-coverage sense [11] M. Collins and N. Duffy. New Ranking Algorithms for [12] D. Damljanovic, M. Agatonovic, and H. Cunningham. [13] M. Denkowski and A. Lavie. Meteor 1.3: Automatic [14] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, [15] E. Gabrilovich and S. Markovitch. Computing [16] D. Gusfield. Algorithms on strings, trees, and [17] M. Heilman and N. A. Smith. Tree edit models for [18] A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, [19] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [20] Z. Ji, F. Xu, B. Wang, and B. He. Question-answer [21] T. Joachims. Text categorization with support vector [22] T. Joachims. Optimizing search engines using [23] X. Li and D. Roth. Learning question classifiers. In [24] R. Mihalcea, C. Corley, and C. Strapparava.
 [25] G. A. Miller. Wordnet: A lexical database for english. [26] A. Moschitti. Efficient convolution kernels for [27] A. Moschitti. Kernel Methods, Syntax and Semantics [28] A. Moschitti. Syntactic and Semantic Kernels for [29] A. Moschitti and S. Quarteroni. Kernels on Linguistic [30] A. Moschitti and S. Quarteroni. Linguistic Kernels for [31] A. Moschitti, S. Quarteroni, R. Basili, and [32] A. Moschitti and F. M. Zanzotto. Fast and effective [33] C. Pinchak. A probabilistic answer type model. In [34] J. M. Prager. Open-domain question-answering. [35] S. Quarteroni, V. Guerrisi, and P. L. Torre.
 [36] F. Radlinski and T. Joachims. Query chains: Learning [37] P. Resnik. Using information content to evaluate [38] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, [39] A. Severyn and A. Moschitti. Structural relationships [40] A. Severyn, M. Nicosia, and A. Moschitti. Learning [41] D. Shen and M. Lapata. Using semantic roles to [42] S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin, [43] R. Soricut and E. Brill. Automatic question answering [44] C. Stokoe, M. P. Oakes, and J. Tait. Word sense [45] M. Surdeanu, M. Ciaramita, and H. Zaragoza.
 [46] E. M. Voorhees. Overview of the TREC 2001 Question [47] E. M. Voorhees. Overview of TREC 2003. In TREC . [48] E. M. Voorhees. Overview of the TREC 2004 question [49] M. Wang and C. D. Manning. Probabilistic tree-edit [50] M. Wang, N. A. Smith, and T. Mitaura. What is the [51] M. J. Wise. Yap3: improved detection of similarities [52] P. C. Xuchen Yao, Benjamin Van Durme and [53] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [54] F. M. Zanzotto, L. Dell X  X rciprete, and A. Moschitti. [55] F. M. Zanzotto and A. Moschitti. Automatic Learning [56] F. M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
