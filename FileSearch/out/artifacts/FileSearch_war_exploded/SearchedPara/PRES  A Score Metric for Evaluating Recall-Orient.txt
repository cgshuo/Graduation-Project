 Information retrieval (IR) eval uation scores are generally designed to measure the effectiveness with which relevant documents are identified and retrie ved. Many scores have been proposed for this purpose over the years. These have primarily focused on aspects of precision and recall, and while these are often discussed with equal importa nce, in practice most attention has been given to precision focused metrics. Even for recall-oriented IR tasks of growing importa nce, such as patent retrieval, these precision based scores remain the primary evaluation measures. Our study examines different evaluation measures for a recall-oriented patent retrieval task and demonstrates the limitations of the current scores in comparing different IR systems for this task. We introduce PRES, a novel evaluation metric for this type of application taking account of recall and the user X  X  search effort. The behaviour of PR ES is demonstrated on 48 runs from the CLEF-IP 2009 patent retrieval track. A full analysis of the performance of PRES shows its suitability for measuring the retrieval effectiveness of systems from a recall focused perspective taking into account the user X  X  e xpected search effort. H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval; H.3. 4 Systems and software  X  performance evaluation. Measurement, Performance, Experimentation. PRES; Recall-Oriented Information Retrieval; Patent Retrieval; Evaluation Metric The objective of an information retrieval (IR) system is to retrieve relevant documents to satisfy user information needs. The evaluation of IR systems should thus test their ability to achieve this objective. Evaluation of IR systems has been the focus of much research in recent years [ 18, 29 ]. A number of evaluation methods and metrics ha ve been proposed and explored for the wide range of IR tasks now under investigation, e.g. web search, question answering and struct ured document retrieval. Laboratory IR tests generally adopt the Cranfield evaluation framework paradigm [11]. Metric s used in these experiments generally measure how early relevant documents are retrieved with less focus on the system recall. While this situation is reasonable for precision-oriented applications, where a small number of relevant documents are sufficient to satisfy the user information need, they are less informative of system behaviour for recall-oriented tasks, where all relevant documents are required to be retrieved. Howeve r, while metrics such as, mean average precision (MAP) are not sufficient, they have been used as the central evaluation measures in applications such as patent retrieval [13, 25]. Viewing recall-or iented tasks purely in terms of measuring recall is actually rather simplistic. In practice the user X  X  effort expended in the search is often also a key consideration. Thus it can be important for an ev aluation metric to take account the ranks at which relevant items are retrieved. This paper describes a study analyzing the behaviour of current evaluation metrics when applied to recall-oriented IR tasks. The results of this analys is are used to motivate the proposal of a novel evaluation metric which combines recall with the quality of ranking of the retrieved relevant results. This allows us to distinguish between systems of si milar recall giving higher scores to systems with better ranking of relevant documents. A study performed on the CLEF-IP 2009 patent retrieval task [25] shows the advantage of the new score over existing recall and precision metrics. The new score showed a 0.87 correlation to recall and 0.66 correlation to precision, wh ich demonstrates how it reflects both recall and precision with more emphasis on recall. Additional analysis shows that the new score also works well for other recall-oriented IR applications such as legal search when the number of relevant documents is typically very large. The remainder of the paper is or ganized as follows; Section 2 surveys background on IR evaluation scores; Section 3 explores the effectiveness of the current IR evaluation scores for measuring system performance for recall-orient ed IR applications; Section 4 explains normalized recall, which is one of the classic IR evaluation scores used later to develop our new PRES evaluation metric, Section 5 formally intro duces PRES; Section 6 explores the behaviour of PRES by use of illustrative examples and by testing it on the 48 CLEF-IP 2009 r uns, in addition, it reports the behaviour of PRES for other ta sks; Section 7 discusses the theoretical meaning of the score and compares it to the normalized recall; and finally, Section 8 conc ludes the paper with suggestions for possible future research directions. While many evaluation metrics have been proposed for ad hoc type IR tasks, by far the most popul ar in general used is MAP [5]. The standard scenario for use of MAP in IR evaluation is to assume the presence of a collection of documents representative of a search task and a set of test topics (user queries) for the task along with associated manual relevance data for each topic. The relevance data for each topic is assumed to be a sufficient proportion of the documents from th e collection that are actually relevant to that topic.  X  X ufficient X  here relates to the fact that the actual number of relevant documents each topic is unknown without manual assessment of the complete document collection for each topic. Several techniques are available for determining sufficient relevant documents for each topic [8, 15, 26]. As its name implies, MAP is a precision metric, which emphasizes returning a greater number of relevant documents earlier. The impact on MAP of locating relevant documents later in the search of a ranked list is very weak, even if very many such documents have been retrieved. Thus while MAP gives a good and intuitive means of comparing systems for IR tasks emphasising precision, it will often not give a meaningful interpretation for recall focused tasks. A detailed analysis of the behaviour of MAP is described in [19]. Some other IR evaluation metrics are found to be more representative than MAP for other types of IR task. For example, Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) are used for IR applications such as question answering and web sear ch respectively [10, 28]. MRR measures performance when l ooking for one specific  X  X nown item X  in a document collection [3]. Mean reciprocal rank is simply the inverse of the rank of the relevant document in the retrieved list. NDCG treats the re levant documents differently where the relevant documents are classified into classes according to the degree of relevance to the query. The objective is to find highly relevant documents earlier in the ranked list than less relevant ones. Additional IR evaluation scores have been introduced with the advent of new IR applications such as mean average generalized precision (MAgP) for structured document retrieval [1, 16] and GMAP which is the same as MAP but using geometric mean instead of the arithmetic mean, GMAP was used in the Robust Track at TREC [30]. Recently some scores have been introduced as alternatives to the MAP in order to overcome its shortcomings. Bpref, inferred average precision (infAP), and rank-biased precision (RBP) are examples of these scores. Bpref is designed to overcome the probl em of incomplete relevance judgements [9]. infAP is designed for a similar purpose, where it collapses to MAP when judgement s are complete [2]. RBP is designed to reflect a better modelling of user behaviour in terms of how deep they are willing to go down in the results list [19]. Similar to MAP, these IR eval uation metrics focus on measuring effectiveness at retrieving relevant documents earlier rather than on the system recall. While this is sufficient and reasonable for precision focused tasks, it is not suitable for tasks where the objective is to find  X  X ll X  relevant documents, and in particular if the objective is to find all relevant documents with minimum effort for the user. In this kind of application, the user is willing to exert much effort to go deeper in the list in order to find relevant documents. Additionally, for recall-oriented IR applications the maximum number of documents to be checked by the user (the cut-off of the retrieved results) is also very important, since it has a direct impact on the cost of user effort and on recall. This concern was the reason behind using recall along with MAP in evaluating similar IR tasks [25, 31]. The maximum number to be checked by the user is completely overlooked by most of the metrics considered so far, and is variable in measures such as the f-score [21]. The f-score combines recall with precision, and has been used for legal IR [20]; although this score includes recall, it has the problem that the number of documents to be retrieved is not fixed, which is often a prac tical concern of real users. Other measurements such as re trievability and findability have been used for analyzing query formulation on the retrieval effectiveness [4, 6]. Although these scores give some analysis for the effect of query formulation on system performance, they fail to compare performance of different systems on a set of topics. The simplest solution to measuring performance in a recall focused IR task is of course simply to evaluate the recall. However, as noted in the previous section, the problem of doing this is that it fails to reflect how early a system retrieves the relevant documents and thus the user effort involved. Although recall is the object ive for such applicati ons, the score should be able to distinguish between systems that retrieve relevant documents earlier than those that retrieve them later. To overcome this problem the f-score can be used, but at a fixed number of retrieved documents. However the same problem will arise, since applying it after retrieving N -documents for two systems that retrieved the same number of rele vant documents, the f-score will be the same. This situation arises since the f-score is designed for classification tasks, but for reca ll-oriented IR a pplications, the problem is viewed as a ranking problem with a cut-off for a maximum number of documents to be checked N max . One modification for using the f-score is to calculate it as a combination between the recall and the average precision ( AP ) instead of using the absolute precision (equation 1). Such a modified f-score will reflect the system recall in addition to its average precision. However, while this captures the recall, it will have the same disadvantages for recall focused tasks with respect to AP which were noted earlier. 
F '  X  where, AP : Average precision of a topic R : recall at a given number of retrieved documents  X  : weight of recall to precision Table 1 shows an illustrative example of how different metrics perform with four different IR systems when searching a collection for a single query. In this case it is known that there are four relevant documents, and it is assumed that the user is willing to check the top 100 documents retrieved by each system. 
Table1. Performance of different scores with different IR In Table 1, system 3 is the prefect result with all relevant documents retrieved at the top ranks. System 1 has the lowest recall, while system 2 has mode rate performance retrieving all relevant documents in the middle of the ranked list, System 4 has fair performance since it ranks one relevant document at rank 1, but achieves 100% recall only after checking the full list of 100 top results. From the table it can be seen that AP for system 1 is much higher than for system 2, which is unfair, since system 2 has been able to retrieve all relevant documents in the middle of the list, but system 1 has failed to retrieve mo re than one relevant document in the full list. The same situation arises when comparing system 4 to system 2, even though both systems ha ve been able to retrieve the full list of relevant documents, system 2 has done so at much higher ranks than system 4. Recall and F 1 score fail to differentiate between systems 2, 3, and 4, even though these systems have very different behaviour. F X  does not focus on the recall, which is the objective of recall-oriented applications. To empha size recall a modified f-score, F X  was tried giving recall four times the weight of the average precision (  X  = 4 in Equation 1). Initial inspection suggests that F X  looks to be a good representati on of the system performance, however on deeper analysis, it can be seen that system 4 is evaluated to be nearly twice as good as system 2, even though while it retrieves a relevant docume nt at rank 1 no further relevant documents are found until the end of the list and that while system 2 failed to return any relevant documents among the first half of the list, all relevant documents are retrieved by rank 54. For two systems such as 2 and 4 for a re call-oriented task with users willing to check the first 100 docum ents, system 2 will give more confidence to the user that ther e is little chance of finding further relevant documents after rank 100; since the presence of low ranked relevant documents in syst em 4 may suggest that further ones are more to be present. Hence, F X  4 fails to evaluate system 2 and system 4 fairly from the perspective of a recall-oriented application in practical usage . One of the proposed IR evaluation metrics that has never found its way into wide usage is normalized recall ( R norm ) [21, 24], shown in Equation 2. This measures the effectiveness in ranking documents relative to the best and worst ranking cases, where the best ranking case is retrieval of all relevant documents at the top of the list, and the worst is retrieving them only after retrieving the full collection. Figure 1 shows an illustrative graph of how to calculate R norm , where R norm is the area between the actual and worst cases divided by the area between the best and worst cases. 
Figure 1. Illustration of how R norm curve is bounded by the 
R i norm  X  where: r i : the rank at which the i th relevant document is retrieved, N : collection size, and n : number of relevant docs Normalized recall can be seen as a good representative measure for recall-oriented IR applications . This measure is greater when all relevant documents are retrie ved earlier. However it requires ranking of the full collection. Applying R norm on collections of very large numbers of documents is infeasible, since it is nearly impossible to rank a collection of potentially many millions of documents. In addition, some re levant documents may have no match to the query leading to them not being retrieved at all. One approximation to address this problem is to consider any relevant documents not retrieved in the top N max the end of the collection. Using this approximation to enable the calculation of R norm leads to its value being nearly equal to the system recall at a cutoff of N max . For example, for a collection of tens of thousands of documents and when retrieving the top 1000 documents; if recall at 1000 equals 50%, R norm with the previous approximation will equal 49.99% (Figure 2). 
Figure 2. Illustration of how R norm curve behaves with large In the previous sections we dem onstrated that current evaluation metrics do not represent system performance well in recall-oriented IR applications . In this section, a novel score is presented based on modifications to the normalized recall measure. As outlined in the previous section, R norm can be seen as a good score for evaluating recall-oriented a pplications but only for small collections. Our new score  X  Patent Retrieval Evaluation Score  X  (PRES) is based on the same idea as the R norm but with a different definition for the worst case. The new assumption for the worst case is to retrieve all the relevant documents just after the maximum number of documents to be checked by user ( N The idea behind this assumption is that getting any relevant document after N max leads to it being missed by the user, and getting all relevant documents after N max leads to zero recall, which is the theoretical wors t case scenario. Applying this assumption in equation 2, N is replaced with N the number of relevant documents. Any relevant document not retrieved in the top N max is assumed to be the worst case (Figure 3). For example, for a retrieved ranked list for a topic with 10 relevant documents ( n = 10) and for which the user is willing to check the top 100 documents ( N max = 100); the best case will be finding the 10 relevant documents at ranks {1, 2, ... 10}, and the worst case will be finding them in the ranks {101, 102, ... 110}, which means the user missing all the relevant documents. Assuming retrieval of only 7 relevant documents in the top 100, then the missing 3 relevant documents will be assumed to be found at ranks {108, 109, 110}. 
Figure 3. PRES curve is bounded between the best case and the Equation 3 shows the calculation of PRES. Equation 4 shows the direct calculation of the su mmation of ranks of relevant documents in the general case, wh en some relevant documents are missing in the top N max documents. where, R : Recall (number of relevant retrieved docs in the 1st N From equation 3, it can be inferred that PRES is a function of the recall of the system, the ranks of the retrieved documents, and the maximum number of results to be checked by user. For a given N max , PRES behaves as shown in Figure 4(a). For recall = R , the PRES value ranges from R , when retrieving all relevant document on the top of the list, to nR 2 /N max when retrieving them at the bottom of the list. For the special case where the number of relevant documents for a topic is one ( n =1), PRES will have a linear characteristic. Figure 4(b) shows the difference between PRES and MRR performance with different ranks for the case where n=1. In this case PRES could be used as an alternative measure for evaluating question answering instead of MRR. For example, if the user is willing to check the first 10 answers for a question before reformula ting it [10], PRES with N max = 10 could be used instead as it will assign a low penalty to systems that retrieve the relevant document within the first 10 ranks, and a full penalty to systems that retr ieve the document afterward. Figure 4(a). PRES performance with various recalls and rank
Figure 4(b). PRES vs MRR for different rank when n =1 In this section, PRES is tested on the same sample examples as Table 1, with additional illustrative real samples from one run in the CLEF-IP 2009 patent retrieval task. In addition, the average performance is tested on real exam ples of 48 participants X  runs from CLEF-IP 2009. The aim of the CLEF-IP track is to automatically find prior art citations for patents. The topics for this task are patents filed in the period after 2000, and the searched collection contains about one million patents filed in the period from 1985 to 2000 [25]. The objective is to use some text from each patent topic to automatically retrieve all cited patents found in the collection. The design of the patent test collection assumes that filed patents examined by the patent office for novelty, are the training and test collections, and that the patent citations, which are mostly added by the patent office, are considered as the relevant document set [13, 14, 25]. Table 2. Performance of PRES with different IR systems Table 2 shows how PRES perform s with the sample examples presented in Table 1. From Table 2, it can be seen that PRES is a better representative measure for the system performance as a combination between system recall and average ranking of relevant documents. Some real sa mples of topics from one run of the CLEF-IP 2009 track are presented in Table 3 with maximum number of results to be checked by user N max = 1000. In Tables 2 and 3, PRES is always less than or equal to recall, i.e. PRES is a portion of the recall depending on the quality of ranking of the relevant documents relative to N max . For example, getting a relevant document at rank 10 will be very good when N max good when N max =100, but bad when N max = 15, and very bad when N max =10. Systems with higher recall can achieve a lower PRES value when compared to systems with lower recall but better average ranking. This is clear in Table 3, where one topic with 67% recall has 63.6% PRES because of good ranking (41 and 54 among 1000), and one topic with 100% recall got 52.5% for PRES because of the moderate ranking where 60% of them are below rank 500 out of 1000. Comparing PRES to average precision ( AP ) for the samples in Table 3, it can be seen that AP is more sensitive to how early the first relevant document is fo und regardless of the number of documents to be checked by us er. However, PRES is more sensitive to the average ranking of the relevant retrieved documents as a whole relative to the maximum number of documents the user is willing to check. The last sample topic in the table has a PRES of 96.43% even though relevant documents are not ranked in the top 10 or even 20 results. The reason is that N max =1000, and the ranks {32, 35, 46} are considered relatively good compared to this number. Nevertheless, when calculating PRES with N max =100, the PRES value will be 64.33% which represents the average ranking of the relevant documents relative to the maximum number of documents to be checked. Table 3. AP / R /PRES performance with real samples of topics PRES was tested on 48 different submissions from 15 participants to the CLEF-IP 2009 Patent Track [25]. Table 4 shows the score for each submission in MAP, recall, and PRES. Participant IDs are anonymous and the number of topics for each participant used was 400 instead of the official 500 in order to further mask participant identities and to avoid violating the privacy of any of the participants. For all topics, N max = 1000 was used. The average number of relevant documents per topic is 6 ( n avg results, it can be seen that PRES reflects the recall with the average quality of the ranking, which is mainly reflected in the MAP. Run 21 (R21) which achieved the highest MAP and recall also achieved the highest PRES, with the same behaviour being observed for the lowest scoring r uns. However, some submissions which achieved high precision but low recall were punished and received only a moderate PRES score. For systems which achieved high recall but low precis ion (which reflects bad ranking such as system R18), the PRES score was moderate too. Figure 5 plots the three scores of the same 48 submissions sorted by PRES from low to high values. From Figur e 5, it can be noted that PRES is a good single score that can represent both the precision and recall of each run. Figure 6 show s the change in ranking of the submissions with the three scores . It can be seen that ranking using PRES is more biased towa rds recall, than MAP. However, this is not always the case, for example R12 has moderate ranking in both recall and MAP, but lower ranking in PRES, which is due to the fact that MAP is more sensitive to the high ranking of some of the relevant documents, but PRES is dependent on relative average ranking of  X  X ll X  relevant documents to N max shows that the scores have high agreement on the ranking of systems with very high or very low performances. In order to check the agreement of the three scores, pair wise comparison of submissions was carried out with each two runs being compared: 1) the first run is statistically significantly better than second run, 2) the second run is statistically significant better than 1 st run, and 3) Both runs are statistically indistinguishable [7]. Wilcoxon significance test with confidence level of 0.95 was used for comparing each of the two runs [12]. Comparing 48 runs in a pair wise manner led to 1, 128 comparisons. The agreement of scores for each comparison is plotted in Figure 7.From Figure 7, it is clear that PRES is an intermediate score between recall and MAP. In addition, in a smal l number of cases (1%) PRES disagrees when recall and MAP agree. These situations are mainly for examples where recall and MAP agree that system 1 (1 is better than system 2 (2 nd run), but PRES shows that both systems have the same performance, or when recall and MAP agree that two systems are sta tistically indistinguishable, but PRES prefers one over the other. Calculating the Kendall X  X  tau corre lation between the ranking of runs according to the three scores [17], it is found that the correlations are as follows: MA P and recall = 0.56, PRES and recall = 0.87, and PRES and MAP = 0.66. This emphasizes that PRES lies between MAP and recall with a bias towards recall. Table 4. MAP/Recall/PRES for 48 submissions in CLEF-IP 
Figure 5. MAP/Recall/PRES for 48 submissions in CLEF-IP Figure 6. Ranking change of 48 submissions according to 
Figure 7. Agreement chart of MAP/Recall/PRES on pair wise Cut-off value of documents to be checked is considered one of the key variables that affect the value of PRES. It is the same case for recall, as the more documents that are retrieved the more possibility there is to find further relevant documents, hence the higher the system recall. Additionally, for PRES N value even if no more relevant documents are found, since for different cut-offs, the relative ranking of relevant documents is different. This effect has been shown earlier in one of the examples (section 6.1). For recall-oriented applications, the actual number of documents to be checked by the user is typically higher than other IR applications. This number can exceed a hundred documents in the case of a patent examiner before he/she thinks of reformulating the query 1 . Different factors can affect the decision to stop checking for relevant documents; one of these can be the failure to find a relevant document for some while in the list, or the user can decide to check a fixed number of documents, but when less relevant documents are found while checking the list the user will generally move more quickly th rough the list leading to more rapid task completion. For both scenarios the effort the user exerts to find a relevant document will be greater as long as he/she continues to find relevant documents deep in the list. This is the reason of why PRES penalizes finding documents deeper in the list of the N max ranked results. Figure 8. MAP/Recall/PRES performance for different values Figure 8 shows the effect of changing the value of N max recall, and PRES. Three sample runs from CLEF-IP 2009 (R12, R18, and R21) were selected to examine the variation of the three scores at different values of N max . In figure 8, the effect of findi ng more relevant documents on MAP is very poor regardless of the number of documents to be checked by the user and regardless of the number of relevant documents found deeper in the li st. PRES and recall performances look similar in general, howev er, for the example, when N 10, PRES judges R12 to be better than R23, but recall is judged to be the opposite. Furthermore, fo r R18 the recall curve with N has a higher slope than the PRES curve. This returns us to the issue of recall neglecting the ranking of documents by recall, which is taken into account by PRES. Usually for recall-oriented applications, when all or at least a significant portion of the relevant documents are required to be retrieved, the user will check a number of retrieved results higher than the expected number of rele vant documents. However, this scenario can be neglected in some applications where the number of relevant documents is very hi gh and the task is to evaluate different IR systems for the abil ity to find the largest number of relevant documents. This is the exact scenario in recall-oriented IR applications such as legal search. The legal track at TREC seeks to evaluate the ability of different systems to retrieve relevant legal documents [27]. Th e number of relevant documents for a topic can reach tens of thousands. Several scores and methods have been proposed to overcome this problem by estimating the number of releva nt documents and the actual system precision and recall. In this subsection, the behaviour of PRES is studied for cases like this where the number of relevant documents ( n ) is higher than the maximum number of documents to be checked by the user ( N As shown in Figure 9, the best cas e will never be applicable as retrieving all relevant documents at the top ranks will exceed the cut-off value, and the user will never be able to achieve 100% recall. However, the calculation of PRES in this case can still be applied without any modification. As mentioned before, for a recall = R , PRES will range from nR 2 / N max , to R . The only difference here is that the maximum applicable R will be N which is the case when all the retrieved documents are relevant. Although the PRES calculation is s till applied, the PRES value will have some limitation in expressing the general system performance. Hence, estimated an PRES can be calculated to approximate the full performance of the system as shown in Equation 5. 
R  X  = ( 6 ) where, PRES est : estimated PRES, R max : maximum possible recall ( R max = 1 when N While this provides an estimate of system performance, it is advisable only to use PRES est in evaluation campaigns where there are a large number of runs with a very large number of relevant documents and it is impractical to evaluate the very long submitted lists of many systems. For an accurate evaluation using PRES, N max should be carefully selected according to the user and application models, and for a recall-oriented application, N should be higher than n In the previous sections, it wa s shown how PRES was derived from normalized recall ( R norm ) after changing the worst case scenario definition. Although both scores are very similar in characteristics and calculations, this small modification led to a significant change in the performance and the theoretical meaning of the PRES score. Normalized recall was first proposed by Rocchio in 1964 [24] as an IR evaluation score that is independent of the cut-off value of the retrieved documents, as it requir es (as was shown in section 3) returning all documents of the co llection ranked by relevance. In 1969, Robertson showed that R norm is the same area under the recall-fallout curve (operating characteristic curve), which makes R norm equal to the probability of pairwise error in ranking, and which leads to R norm = 0.5 for random ranking of documents in the collection [22]. This is not the case for PRES, where the PRES value is directly dependent on the cut-off value. Furthermore, random ranking of documents will eventually lead to PRES = 0 for the current common collecti on sizes, as the probability of finding a relevant document = n / N , where N is the collection size which is typically millions or bill ions of documents in case of web search. Normalized recall was a suitable evaluation measure at the time it was introduced, but with the current collection sizes and type of applications, R norm is found to be an impractical measure for operational use. This is the reason why it has never found its way into wide spread usage. PRES can be considered as an IR evaluation measure that has the characteristics of the classic R but with a different meaning. PR ES is designed specifically for recall-oriented applications to emphasize the system quality in retrieving the most significant nu mber of relevant document as early as possible within a specif ic number of results in a ranked list. In this paper, a study of recall -oriented applications has been described and a novel score  X  X RES X  has been presented that is designed for these applications. Th e score is a refinement of the normalized recall score. It has been tested and compared to the most widely used IR scores on a pa tent retrieval task. Illustrative samples and real data examples demonstrated the effectiveness of the new score. The score reflects the system recall combined with the quality of relative ranking of retrieved relevant documents within the maximum numbers of documents to be checked by a user. The PRES value varies from R to nR 2 /N max according to the average quality of ranking of rele vant documents; hence it can be seen as a function of system re call, ranking of relevant documents, and the maximum number of documents to be checked by a user (which directly affects the recall and relative ranking). In future work, the utility of PRES as a measure for the patent retrieval could be investigated furt her by direct consultations with professional patent expe rts. Such a study s hould have a practical and theoretical analysis of the user model represented by PRES (similar to the study in [23]). Additionally, PRES could be applied to other recall-oriented IR applications such as chemical IR and legal IR [32], which can be characterized by different experimental environments, differ ent users, and different numbers of relevant documents. Although the performance of PRES has been analyzed for legal search in this paper, real sets of runs are needed in order to explore its behaviour on this type of data. This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (CNGL) project. [1] Ali M. S., Consens, M. P., Kazai, G., and Lalmas, M. [2] Aslam J. A., and E. Yilmaz. Estimating average precision [3] Azzopardi L., de Rijke, M., and K. Balog. Building [4] Azzopardi, L. and Vinay, V. Retrievability. An evaluation [5] Baeza-Yates, J., and Ribeiro-Neto, B. Modern Information [6] Bashir, S., and Rauber A. Analyzing Document [7] Buckley, C., and Voorhees, E. M. Evaluating Evaluation [8] Buckley, C., Dimmick, D., Soboroff, I., and E. Voorhees. [9] Buckley, C., and Voorhees, E. M. Retrieval evaluation with [10] Carterette, B., Bennett, P. N. Chickering, D. M., and Dumais, [11] Cleverdon, C. The Cranfield Tests on Index Language [12] Hull, D. Using statistical testing in the evaluation of retrieval [13] Fujii, A., Iwayama, M., and Kando, N. Overview of Patent [14] Graf, E., and Azzopardi, L. A methodology for building a [15] Jordan, C., Watters, C., and Ga o, Q. Using controlled query [16] Kamps, J., Pehcevski, J., Kazai, G., Lalmas, M., and [17] Kendall, M. A new measur e of rank correlation. Biometrika , [18] Mandl, T. Recent developmen ts in the evaluation of [19] Moffat, A., and Zobel, J. Rank-biased precision for [20] Oard, D. W., Hedin, B., Tomlin son, S., and Baron, J. R. [21] van Rijsbergen, C. J. Information Retrieval, 2nd edition. [22] Robertson S. E. The parametric description of the retrieval [23] Robertson, S. A new interpreta tion of average precision. In [24] Rocchio J. Performance indices for document retrieval [25] Roda G., Tait J., Piroi F. , and Zenz V. CLEF-IP 2009: [26] Tague J., Nelson, M., and Wu, H. Problems in the simulation [27] Tomlinson S., Oard, D. W., Baro n, J. R., and Thompson, P. [28] Voorhees, E. M., and Tice, D. M. The TREC-8 Question [29] Voorhees, E. M. The Philosophy of Information Retrieval [30] Voorhees, E. M. The TREC r obust retrieval track. In SIGIR [31] Xue, X., and Croft W. B. Automatic Query Generation for [32] Zhu, J., and Tait, J. A propos al for chemical information 
