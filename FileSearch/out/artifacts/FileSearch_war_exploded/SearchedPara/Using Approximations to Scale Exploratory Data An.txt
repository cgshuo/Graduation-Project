 may be prohibitive. This paper describes a technique that 
Exploratory Data Analysis (EDA) is a technique [7, 81 that uncovers structure in data. EDA is performed without any a-priori hypothesis in mind: rather it searches for  X  X xceptions X  of the data values relative to what those values would have been if anticipated by an statistical model. This statistical model fits the rest of the data values rather well and can be accepted as a description of the dataset. When applied to multidimensional tables, EDA also uncovers the attribute values that have the greatest effect on the data points. As pointed out by Ill], these exceptions can be used by an analyst as starting points in the search for anomalies, guiding the analyst work across a search (i.e., not every cell has a value). On the other hand, although median polish is more resistant to outliers and holes, its performance is very adversely affected by holes which increase substantially the number of iterations needed by the process [S]. Increasing the number of iterations can drastically impose enormous I/O demands (by requesting many passes over a large dataset) and therefore render the process impractical for large datacubes. Previous work in using EDA for datacubes [ll] has chosen to use mean polish, precisely for being less demanding on the number of iterations needed to finish. 
This paper explores a method that benefits from the high robustness of median polish, while at the same time scales well for large datacubes. Our method uses statistical models to approximate subsets of the datacube, eliminating the need to do several passes over the dataset in order to compute the medians. However, by doing so, the row effects and the residuals computed are only an approximation of those that would have been computed by applying median polish to the dataset. We show that this tradeoff of lesser I/O demands versus accuracy in the results is very beneficial. 
This paper is organized as follows. In Section 2, we present the basis of our method. Section 3 presents the experimental results obtained by our method using real and synthetic datasets. Finally, Section 4 presents the conclusions and directions of future work. As we pointed out before, median polish takes every row in the table and computes its median, substracting it from every cell X  X  value to make the row X  X  median equal to zero. It is easy to see that this method cannot scale well. During each iteration, we need full access to the entire dataset. If the dataset does not fit in main memory, this implies severe I/O demands: a row of the table would be brought to memory only to be replaced by other portions of the dataset later, and brought back in the next iteration. In this section we describe in detail how we scale the median polish procedure to large datacubes. The aim of our method is to avoid having to bring data cells to main memory repeatedly in order to compute the medians. We do that by characterizing portions of the core cuboid of the datacube 2, employing statistical models that can be later used to estimate the values of the individual cells. To avoid incurring large errors by using the estimated cell values, we retain all the cell values whose estimated values are too erroneous (farther away from the real value by more than a preestablished threshold). We have the same chunk size {pi,.  X  . , PA}. We use ct to the lst-level chunks is given by di = n ,$ . Clearly, we require that /J: 5 llAi[l for all i; moreover, we choose not to divide those dimensions with small domains (for them we make pi = IIAilj). At any point during the process of partitioning we may decide to further divide a 1-st level chunk into several 2nd level chunks, and successively an j-th level chunk into j + 1-th level chunks. The size of an j-th level chunk is predetermined to be (~1, ... ,pi}, for j = 1, ... ,MAXLEVEL, where MAXLEVEL is the maximum level of any chunk. There are three possible states for a chunk after completing the partitioning: STAT-NULL for a chunk with no cells, STAT-SPARSE for a chunk with very few cells, and STAT-MODL for a chunk has been modeled. There are three parameters used to drive the partitioning process: l (Y : this marks the minimum acceptable density for .p: this is the maximum error level tolerated in l y : this is the maximum percentage of outlier cells 
The algorithm (whose pseudo-code cannot, for rea-sons of space, be presented here, but can be found in [3]) proceeds to divide the initial cuboid in 1-st level chunks, assigning cells to each one and proceeds to clas-sify each chunk, further dividing it if necessary, until the chunk is declared STAT-SPARSE, STAT-NULL or STAT-MODL. It is worth noticing that the algorithm will read the number of cells in the core cuboid into memory only once. Then each chunk that is neither sparse not null will be read into memory (with its re-spective cells) and processed (subdivided, modeled) un-til no more processing is needed for the chunk. If each chunk fits in memory, then it is guaranteed that the each cell will be read into memory only once, making the input activity equivalent to two passes of the data and the write activity also equivalent to two passes of the data. If a chunk does not fit in memory several reads and writes to the chunk will be needed, making the total read activity loosely bound by MAXLEVEL number of passes through the data. (In practice the I/O activity will be much less, since there will be child chunks that many (or all) the chunks in memory decreases with the size of the chunks. This, of course, assumes that the regions covered by chunks whose state is STAT-MODL are indeed dense (i.e., contain few holes). If this is not the case, the zero (or non-zero 3, cells need to be indexed, making the description larger. So our procedure is more effective for data that is highly clustered in subspaces along the datacube. Fortunately, this is the case in many real datasets. (Commercial systems exploit this property: for instance, Arbor Software X  X  Essbase, stores dense and sparse regions of the core cuboid using different data structures [5]). However, as the results will show, we still get a substantial benefit in performance if the density in the clusters is low. Since our procedure is approximate, we need to define ways to compare the results with those that are obtained by applying the median polish method to the base cuboid. In this subsection we describe some of the measures we use for that purpose. (The complete set can be found in [3].) 
Let us define the sets E: and Et: as the sets of the Ic topmost ranked effects and the k lowest ranked effects in row i respectively, where k = I] E+ 11 = 11 Ei: 11. Given the equivalent sets &amp; and I$:, obtained by the approximate median polish, we can define the positive and negative effect recall (Re+,Re-) in the manner shown in Equations 1 and 2. 
Finally for the residuals we define the set R as the set of the lc-th largest residuals, with I] R 1 = ,+ computed by the standard median polish and R defined for the residuals obtained with the approximate cuboid. Then we define the Recall (Ret) in Equation 3. The experiments were conducted in a SUN Ultra 2, with two processors, and 500 Mbytes of RAM. We conducted the experiments in several datasets, but we only report the results for two of them in this paper. 83 s P c 2.5 -G a = c 2-B : t Figure 3: Execution time for the standard and approx-imate method for dataset 3. The Y axis shows the ex-ecution time in seconds; The X axis shows the number of non-zero cells in thousands. dense chunks is done randomly. We conducted all the experiments with a p = 0.2, y = 0.2, and two values of Q! (0.33 and 0.95). 
Figure 3 plots the execution time of the standard method and the approximate method for chunk den-sities of 33% and 95%. As can be observed, the approx-imate method scales well with the size of the dataset, while the execution time of the standard method gets to be impractical for large sizes (more than 10 hours for 2 million non-zero cells). It is also worth pointing out that in the high density case, the approximate al-gorithm performs better since there is no need to index the non-zero cells in the chunk. Nevertheless, in the case of low density, the approximate algorithm still outper-forms the standard by a ratio of 1O:l (as opposed to a ratio of more than 3O:l in the dense case). In this paper we have described an approximate method to do EDA, using the median polish procedure over datacubes. We have shown that the method scales well with the size of the cube, allowing the user to perform EDA in cases where the cost of doing the standard algorithm would be prohibitive. 
The price one pays for using the approximate method is a decrease on the quality of the results. However, as shown in Section 3, the quality of the results obtained by the approximate method is good. We are currently continuing this work by looking at methods to improve the quality of our results (by using different 
