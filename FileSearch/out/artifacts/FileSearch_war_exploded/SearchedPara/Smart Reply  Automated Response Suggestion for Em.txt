 In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse sug-gestions that can be used as complete email responses with just one tap on mobile. The system is currently used in In-box by Gmail and is responsible for assisting with 10% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning.

We describe the architecture of the system as well as the challenges that we faced while building it, like response di-versity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only a modest amount of explicitly labeled data.
 Email; LSTM; Deep Learning; Clustering; Semantics
Email is one of the most popular modes of communica-tion on the Web. Despite the recent increase in usage of social networks, email continues to be the primary medium for billions of users across the world to connect and share information [2]. With the rapid increase in email overload, it has become increasingly challenging for users to process and respond to incoming messages. It can be especially time-consuming to type email replies on a mobile device. Equal contribution.

An initial study covering several million email-reply pairs showed that  X  25% of replies have 20 tokens or less. Thus we raised the following question: can we assist users with composing these short messages? More specifically, would it be possible to suggest brief responses when appropriate, just one tap away? N o Smart
Suggested Figure 2: Life of a message. The figure presents the overview of inference.

To address this problem, we leverage the sequence-to-sequence learning framework [23], which uses long short-term memory networks (LSTMs) [10] to predict sequences of text. Consistent with the approach of the Neural Conver-sation Model [24], our input sequence is an incoming mes-sage and our output distribution is over the space of possible replies. Training this framework on a large corpus of con-versation data produces a fully generative model that can produce a response to any sequence of input text. As [24] demonstrated on a tech support chat corpus, this distribu-tion can be used to decode coherent, plausible responses.
However, in order to deploy such a model into a product used globally by millions, we faced several challenges not considered in previous work:
To tackle these challenges, we propose Smart Reply (Fig-ure 1), a novel method and system for automated email response suggestion. Smart Reply consists of the following components, which are also shown in Figure 2: 1. Response selection: At the core of our system, an 2. Response set generation: To deliver high response 3. Diversity: After finding a set of most likely responses 4. Triggering model: A feedforward neural network de-
The combination of these components is a novel end-to-end method for generating short, complete responses to emails, going beyond previous works. For response selection it ex-ploits state-of-the-art deep learning models trained on bil-lions of words, and for response set generation it introduces a new semi-supervised method for semantic understanding of user-generated content.

Moreover, since it directly addresses all four challenges mentioned above, it has been successfully deployed in I nbox. Currently, the Smart Reply system is responsible for assist-ing with 10% of email replies for I nbox on mobile.
Next, we discuss the related work in Section 2, followed by a description of our core system components in Sections 3, 4, 5, and 6. We close by showing modeling results in Section 7 and our conclusions in Section 8.
As we consider related work, we note that building an automated system to suggest email responses is not a task for which there is existing literature or benchmarks, nor is this a standard machine learning problem to which existing algorithms can be readily applied. However, there is work related to two of our core components which we will review here: predicting responses and identifying a target response space.
 Predicting full responses. Much work exists on analyz-ing natural language dialogues in public domains such as Twitter, but it has largely focused on social media tasks like predicting whether or not a response is made [3], predicting next word only [14], or curating threads [4].

Full response prediction was initially attempted in [16], which approached the problem from the perspective of ma-chine translation: given a Twitter post,  X  X ranslate X  it into a response using phrase-based statistical machine transla-tion (SMT). Our approach is similar, but rather than using SMT we use the neural network machine translation model proposed in [23], called  X  X equence-to-sequence learning X .
Sequence-to-sequence learning, which makes use of long short-term memory networks (LSTMs) [10] to predict se-quences of text, was originally applied to Machine Trans-lation but has since seen success in other domains such as image captioning [25] and speech recognition [6].

Other recent works have also applied recurrent neural net-works (RNNs) or LSTMs to full response prediction [21], [20], [19], [24]. In [21] the authors rely on having an SMT system to generate n-best lists, while [19] and [24], like this work, develop fully generative models. Our approach is most similar to the Neural Conversation Model [24], which uses sequence-to-sequence learning to model tech support chats and movie subtitles.

The primary difference of our work is that it was deployed in a production setting, which raised the challenges of re-sponse quality, utility, scalability, and privacy. These chal-lenges were not considered in any of these related works and led to our novel solutions explained in the rest of this paper.
Furthermore, in this work we approach a different domain than [21], [20], [19], and [24], which primarily focus on social media and movie dialogues. In both of those domains it can be acceptable to provide a response that is merely related or on-topic. Email, on the other hand, frequently expresses a request or intent which must be addressed in the response. Identifying a target response space. Our approach here builds on the Expander graph learning approach [15], since it scales well to both large data (vast amounts of email) and large output sizes (many different underlying seman-tic intents). While Expander was originally proposed for knowledge expansion and classification tasks [26], our work is the first to use it to discover semantic intent clusters from user-generated content.

Other graph-based semi-supervised learning techniques have been explored in the past for more traditional classification problems [27, 5]. Other related works have explored tasks in-volving semantic classification [12] or identifying word-level intents [17] targeted towards Web search queries and other forums [7]. However, the problem settings and tasks them-selves are significantly different from what is addressed in our work.

Finally, we note that Smart Reply is the first work to address these tasks together and solve them in a single end-to-end, deployable system.
The fundamental task of the Smart Reply system is to find the most likely response given an original message. In other words, given original message o and the set of all possible responses R , we would like to find:
To find this response, we will construct a model that can score responses and then find the highest scoring response.
We will next describe how the model is formulated, trained, and used for inference. Then we will discuss the core chal-lenges of bringing this model to produce high quality sug-gestions on a large scale.
Since we are scoring one sequence of tokens r , conditional on another sequence of tokens o , this problem is a natural fit for sequence-to-sequence learning [23]. The model itself is an LSTM. The input is the tokens of the original message { o 1 ,...,o n } , and the output is the conditional probability distribution of the sequence of response tokens given the input: As in [23], this distribution can be factorized as:
P ( r 1 ,...,r m | o 1 ,...,o n ) =
First, the sequence of original message tokens, including a special end-of-message token o n , are read in, such that the LSTM X  X  hidden state encodes a vector representation of the whole message. Then, given this hidden state, a softmax output is computed and interpreted as P ( r 1 | o 1 ,...,o the probability distribution for the first response token. As response tokens are fed in, the softmax at each timestep t is interpreted as P ( r t | o 1 ,...,o n ,r 1 ,...,r t  X  1 torization above, these softmaxes can be used to compute P ( r 1 ,...,r m | o 1 ,...,o n ).
 Training Given a large corpus of messages, the training objective is to maximize the log probability of observed re-sponses, given their respective originals:
We train against this objective using stochastic gradient descent with AdaGrad [8]. Ten epochs are run over a mes-sage corpus which will be described in Section 7.1. Due to the size of the corpus, training is run in a distributed fashion using the TensorFlow library [1].

Both our input and output vocabularies consist of the most frequent English words in our training data after pre-processing (steps described in Section 7.1). In addition to the standard LSTM formulation, we found that the addition of a recurrent projection layer [18] substantially improved both the quality of the converged model and the time to converge. We also found that gradient clipping (with the value of 1) was essential to stable training.
 Inference At inference time we can feed in an original mes-sage and then use the output of the softmaxes to get a prob-ability distribution over the vocabulary at each timestep. These distributions can be used in a variety of ways: 1. To draw a random sample from the response distribu-2. To approximate the most likely response, given the Query Top generated responses
Hi, I thought it would be I can do Tuesday. great for us to sit down I can do Wednesday. and chat. I am free How about Tuesday? Tuesday and Wenesday. I can do Tuesday!
Can you do either of I can do Tuesday. What those days? time works for you? Thanks! I can do Tuesday or  X  X lice How about Wednesday? 3. To determine the likelihood of a specific response can-
Table 1 shows some example of generating the approxi-mate most likely responses using a beam search.
As described thus far, the model can generate coherent and plausible responses given an incoming email message. However, several key challenges arise when bringing this model into production.
 Response quality In order to surface responses to users, we need to ensure that they are always high quality in style, tone, diction, and content.

Given that the model is trained on a corpus of real mes-sages, we have to account for the possibility that the most probable response is not necessarily a high quality response. Even a response that occurs frequently in our corpus may not be appropriate to surface back to users. For example, it could contain poor grammar, spelling, or mechanics ( your the best! ); it could convey a familiarity that is likely to be jarring or offensive in many situations ( thanks hon! ); it could be too informal to be consistent with other Inbox intelligence features ( yup, got it thx ); it could convey a sentiment that is politically incorrect, offensive, or otherwise inappropriate ( Leave me alone ).

While restricting the model vocabulary might address sim-ple cases such as profanity and spelling errors, it would not be sufficient to capture the wide variability with which, for example, politically incorrect statements can be made. In-stead, we use semi-supervised learning (described in detail in Section 4) to construct a target response space R com-prising only high quality responses. Then we use the model described here to choose the best response in R , rather than the best response from any sequence of words in its vocab-ulary.
 Utility Our user studies showed that suggestions are most useful when they are highly specific to the original message and express diverse intents. However, as column 1 in Table 2 shows, the raw output of the model tends to (1) favor com-mon but unspecific responses and (2) have little diversity.
First, to improve specificity of responses, we apply some light normalization that penalizes responses which are ap-plicable to a broad range of incoming messages. The results of this normalization can be seen in column 2 of Table 2. For example, the very generic  X  X es! X  has fallen out of the top ten. Second, to increase the breadth of options shown to the user, we enforce diversity by exploiting the semantic structure of R , as we will discuss in Section 5. The results of this are also shown at the bottom of Table 2.

We further improve the utility of suggestions by first pass-ing each message through a triggering model (described in Section 6) that determines whether suggestions should be generated at all. This reduces the likelihood that we show suggestions when they would not be used anyway.
 Scalability Our model needs to be deployed in a production setting and cannot introduce latency to the process of email delivery, so scalability is critical.

Exhaustively scoring every response candidate r  X  R , would require O ( | R | l ) LSTM steps where l is the length of the longest response. In previous work [23], this could be afforded because computations were performed in a batch process offline. However, in the context of an email deliv-ery pipeline, time is a much more precious resource. Fur-thermore, given the tremendous diversity with which people communicate and the large number of email scenarios we would like to cover, we can assume that R is very large and only expected to grow over time. For example, in a uniform sample of 10 million short responses (say, responses with at most 10 tokens), more than 40% occur only once. There-fore, rather than performing an exhaustive scoring of every candidate r  X  R , we would like to efficiently search for the best responses such that complexity is not a function of | R | . Our search is conducted as follows. First, the elements of R are organized into a trie. Then, we conduct a left-to-right beam search, but only retain hypotheses that appear in the trie. This search process has complexity O ( bl ) for beam size b and maximum response length l . Both b and l are typically in the range of 10-30, so this method dramatically reduces the time to find the top responses and is a critical element of making this system deployable. In terms of quality, we find that, although this search only approximates the best responses in R , its results are very similar to what we would get by scoring and ranking all r  X  R , even for small b . At b = 128, for example, the top scoring response found by this process matches the true top scoring response 99% of the time. Results for various beam sizes are shown in Figure 3.
Additionally, requiring that each message first pass through a triggering model, as mentioned above, has the additional benefit of reducing the total amount of LSTM computation. Privacy Note that all email data (raw data, preprocessed data and training data) was encrypted. Engineers could only inspect aggregated statistics on anonymized sentences that occurred across many users and did not identify any user. Also, only frequent words are retained. As a result, verifying model X  X  quality and debugging is more complex.

Our solutions for the first three challenges are described further in Sections 4, 5, and 6.
F requency of matching exhaustive search F igure 3: Effectiveness of searching the response space R . For a sample of messages we compute the fre-quency with which the best candidate found by a beam search over R matches the best candidate found by exhaus-tively scoring all members of R . We compare various beam sizes. At a beam size of 16, these two methods find the same best response 93% of the time.
Two of the core challenges we face when building the end to end automated response system are response quality and utility . Response quality comes from suggesting  X  X igh qual-ity X  responses that deliver a positive user experience. Util-ity comes from ensuring that we don X  X  suggest multiple re-sponses that capture the same intent (for example, minor lexical variations such as  X  Yes, I X  X l be there.  X  and  X  I will be there.  X ). We can consider these two challenges jointly.
We first need to define a target response space that com-prises high quality messages which can be surfaced as sug-gestions. The goal here is to generate a structured response set that effectively captures various intents conveyed by peo-ple in natural language conversations. The target response space should capture both variability in language and in-tents. The result is used in two ways downstream X (a) de-fine a response space for scoring and selecting suggestions using the model described in Section 3, and (b) promote di-versity among chosen suggestions as discussed in Section 5.
We construct a response set using only the most frequent anonymized sentences aggregated from the preprocessed data (described in Section 7.1). This process yields a few million unique sentences.
The first step is to automatically generate a set of canon-ical responses messages that capture the variability in lan-guage. For example, responses such as  X  Thanks for your kind update.  X ,  X  Thank you for updating!  X ,  X  Thanks for the status update.  X  may appear slightly different on the surface but in fact convey the same information. We parse each sentence using a dependency parser and use its syntactic structure to generate a canonicalized representation. Words (or phrases) that are modifiers or unattached to head words are ignored.
In the next step, we want to partition all response mes-sages into  X  X emantic X  clusters where a cluster represents a meaningful response intent (for example,  X  thank you  X  type of response versus  X  sorry  X  versus  X  cannot make it  X ). All mes-sages within a cluster share the same semantic meaning but may appear very different. For example,  X  Ha ha  X ,  X  lol  X  and  X  Oh that X  X  funny!  X  are associated with the funny cluster.
This step helps to automatically digest the entire infor-mation present in frequent responses into a coherent set of semantic clusters. If we were to build a semantic intent pre-diction model for this purpose, we would need access to a large corpus of sentences annotated with their corresponding semantic intents. However, this is neither readily available for our task nor at this scale. Moreover, unlike typical ma-chine learning classification tasks, the semantic intent space cannot be fully defined a priori. So instead, we model the task as a semi-supervised machine learning problem and use scalable graph algorithms [15] to automatically learn this information from data and a few human-provided examples.
We start with a few manually defined clusters sampled from the top frequent messages (e.g., thanks , i love you , sounds good ). A small number of example responses are added as  X  X eeds X  for each cluster (for example, thanks  X   X  Thanks!  X ,  X  Thank you.  X ). 1
We then construct a base graph with frequent response messages as nodes ( V R ). For each response message, we fur-ther extract a set of lexical features (ngrams and skip-grams of length up to 3) and add these as  X  X eature X  nodes ( V F the same graph. Edges are created between a pair of nodes ( u,v ) where u  X  V R and v  X  V F if v belongs to the feature set for response u . We follow the same process and create nodes for the manually labeled examples V L . We make an observation that in some cases an incoming original mes-sage could potentially be treated as a response to another email depending on the context. For example, consider the following (original, response) message pairs: Inter-message relations as shown in the above example can be modeled within the same framework by adding extra edges between the corresponding message nodes in the graph.
The constructed graph captures relationships between sim-ilar canonicalized responses via the feature nodes. Next, we learn a semantic labeling for all response nodes by propa-gating semantic intent information from the manually la-beled examples through the graph. We treat this as a semi-supervised learning problem and use the distributed EX-PANDER [15] framework for optimization. The learning framework is scalable and naturally suited for semi-supervised graph propagation tasks such as the semantic clustering problem described here. We minimize the following objec-tive function for response nodes in the graph:
In practice, we pick 100 clusters and on average 3 X 5 labeled seed examples per cluster. s ||  X  C i  X  C i || 2 +  X  pp ||  X  C i  X  U || 2 where s i is an indicator function equal to 1 if the node i is a seed and 0 otherwise,  X  C i is the learned semantic cluster distribution for response node i , C i is the true label distri-bution (i.e., for manually provided examples), N F ( i ) and N
R ( i ) represent the feature and message neighborhood of node i ,  X  np is the predefined penalty for neighboring nodes with divergent label distributions,  X  C j is the learned label distribution for feature neighbor j , w ij is the weight of fea-ture j in response i ,  X  pp is the penalty for label distribution deviating from the prior, a uniform distribution U .
The objective function for a feature node is alike, except that there is no first term, as there are no seed labels for feature nodes:
The objective function is jointly optimized for all nodes in the graph.

The output from EXPANDER is a learned distribution of semantic labels for every node in the graph. We assign the top scoring output label as the semantic intent for the node, labels with low scores are filtered out. Figure 4 illustrates this process.

To discover new clusters which are not covered by the labeled examples, we run the semi-supervised learning algo-rithm in repeated phases. In the first phase, we run the label propagation algorithm for 5 iterations. We then fix the clus-ter assignment, randomly sample 100 new responses from the remaining unlabeled nodes in the graph. The sampled nodes are treated as potential new clusters and labeled with their canonicalized representation. We rerun label propa-gation with the new labeled set of clusters and repeat this procedure until convergence (i.e., until no new clusters are discovered and members of a cluster do not change between iterations). The iterative propagation method thereby al-lows us to both expand cluster membership as well as dis-cover (up to 5X) new clusters, where each cluster has an interpretable semantic interpretation.
Finally, we extract the top k members for each semantic cluster, sorted by their label scores. The set of (response, cluster label) pairs are then validated by human raters. The raters are provided with a response R i , a corresponding clus-ter label C (e.g., thanks ) as well as few example responses belonging to the cluster (e.g.,  X  Thanks!  X ,  X  Thank you.  X ) and asked whether R i belongs to C .

The result is an automatically generated and validated set of high quality response messages labeled with semantic intent. This is subsequently used by the response scoring model to search for approximate best responses to an in-coming email (described earlier in Section 3) and further to enforce diversity among the top responses chosen (Sec-tion 5).
As discussed in Section 3, the LSTM first processes an incoming message and then selects the (approximate) best responses from the target response set created using the method described in Section 4. Recall that we follow this by some light normalization to penalize responses that may be too general to be valuable to the user. The effect of this normalization can be seen by comparing columns 1 and 2 of Table 2. For example, the very generic  X  X es! X  falls out of the top ten responses.

Next, we need to choose a small number of options to show the user. A straight-forward approach would be to just choose the N top responses and present them to the user. However, as column 2 of Table 2 shows, these responses tend to be very similar.

The likelihood of at least one response being useful is greatest when the response options are not redundant, so it would be wasteful to present the user with three variations of, say, I X  X l be there. The job of the diversity component is to select a more varied set of suggestions using two strate-gies: omitting redundant responses and enforcing negative or positive responses.
This strategy assumes that the user should never see two responses of the same intent . An intent can be thought of as a cluster of responses that have a common communication purpose, e.g. confirming, asking for time or rejecting partic-ipation. In Smart Reply, every target response is associated with exactly one intent. Intents are defined based on auto-matic clustering followed by human validation as discussed in Section 4.

The actual diversity strategy is simple: the top responses are iterated over in the order of decreasing score. Each re-sponse is added to the list of suggestions, unless its intent is already covered by a response on the suggestion list. The resulting list contains only the highest-scored representative of each intent, and these representatives are ordered by de-creasing score.
We have observed that the LSTM has a strong tendency towards producing positive responses, whereas negative re-sponses such as I can X  X  make it or I don X  X  think so typically receive low scores. We believe that this tendency reflects the style of email conversations: positive replies may be more common, and where negative responses are appropri-ate, users may prefer a less direct wording.

Nevertheless, we think that it is important to offer nega-tive suggestions in order to give the user a real choice. This policy is implemented through the following strategy:
A positive response is one which is clearly affirmative, e.g. one that starts with Yes , Sure or Of course . In order to find the negative response to be included as the third sugges-tion, a second LSTM pass is performed. In this second pass, the search is restricted to only the negative responses in the target set (refer Table 2 for scored negative response exam-ples). This is necessary since the top responses produced in the first pass may not contain any negatives.

Even though missing negatives are more common, there are also cases in which an incoming message triggers exclu-sively negative responses. In this situation, we employ an analogous strategy for enforcing a positive response.
The final set of top scoring responses (bottom row in Ta-ble 2) are then presented to the user as suggestions. The triggering module is the entry point of the Smart Reply system. It is responsible for filtering messages that are bad candidates for suggesting responses. This includes emails for which short replies are not appropriate (e.g., con-taining open-ended questions or sensitive topics), as well as emails for which no reply is necessary at all (e.g., promo-tional emails and auto-generated updates).

The module is applied to every incoming email just after the preprocessing step. If the decision is negative, we finish the execution and do not show any suggestions (see Fig-ure 2). Currently, the system decides to produce a Smart Unnormalized Responses Normalized Responses Yes, I X  X l be there. Sure, I X  X l be there.
 Yes, I will be there. Yes, I can.
 I X  X l be there. Yes, I can be there.
 Yes, I can. Yes, I X  X l be there.
 What time? Sure, I can be there.
 I X  X l be there! Yeah, I can.
 I will be there. Yeah, I X  X l be there.
 Sure, I X  X l be there. Sure, I can.
 Yes, I can be there. Yes. I can.
 Yes! Yes, I will be there.
 Normalized Negative Responses Sorry, I won X  X  be able to make it tomorrow.
 Unfortunately I can X  X .
 Sorry, I won X  X  be able to join you.
 Sorry, I can X  X  make it tomorrow.
 No, I can X  X .
 Sorry, I won X  X  be able to make it today.
 Sorry, I can X  X .
 I will not be available tomorrow.
 I won X  X  be available tomorrow.
 Unfortunately, I can X  X .
 Final Suggestions Sure, I X  X l be there.
 Yes, I can.
 Sorry, I won X  X  be able to make it tomorrow.
 Table 2: Different response rankings for the message  X  X an you join tomorrow X  X  meeting? X  Reply for roughly 11% of messages, so this process vastly reduces the number of useless sugestions seen by the users. An additional benefit is to decrease the number of calls to the more expensive LSTM inference, which translates into smaller infrastructure cost.

There are two main requirements for the design of the triggering component. First, it has to be good enough to figure out cases where the response is not expected. Note that this is a very different goal than just scoring a set of re-sponses. For instance, we could propose several valid replies to a newsletter containing a sentence  X  Where do you want to go today?  X , but most likely all of the responses would be useless for our users. Second, it has to be fast: it processes hundreds of millions of messages daily, so we aim to process each message within milliseconds.

The main part of the triggering component is a feedfor-ward neural network which produces a probability score for every incoming message. If the score is above some thresh-old, we trigger and run the LSTM scoring. We have adopted this approach because feedforward networks have repeatedly been shown to outperform linear models such as SVM or lin-ear regression on various NLP tasks (see for example [9]).
In order to label our training corpus of emails, we use as positive examples those emails that have been responded to. More precisely, out of the data set described in Section 7.1, we create a training set that consists of pairs ( o , y ), where o is an incoming message and y  X  X  true,false } is a boolean label, which is true if the message had a response and false otherwise. For the positive class, we consider only messages that were replied to from a mobile device, while for negative we use a subset of all messages. We downsample the negative class to balance the training set. Our goal is to model P ( y = true | o ), the probability that message o will have a response on mobile.

After preprocessing (described in Section 7.1), we extract content features (e.g. unigrams, bigrams) from the mes-sage body, subject and headers. We also use various so-cial signals like whether the sender is in recipient X  X  address book, whether the sender is in recipient X  X  social network and whether the recipient responded in the past to this sender.
We use a feedforward multilayer perceptron with an em-bedding layer (for a vocabulary of roughly one million words) and three fully connected hidden layers. We use feature hashing to bucket rare words that are not present in the vocabulary. The embeddings are separate for each sparse feature type (eg. unigram, bigram) and within one feature type, we aggregate embeddings by summing them up. Then, all sparse feature embeddings are concatenated with each other and with the vector of dense features (those are real numbers and boolean values mentioned in Section 6.1).
We use the ReLu [13] activation function for non-linearity between layers. The dropout [22] layer is applied after each hidden layer. We train the model using AdaGrad [8] opti-mization algorithm with logistic loss cost function. Similarly to the LSTM, the training is run in a distributed fashion us-ing the TensorFlow library [1].
In this section, we describe the training and test data, as well as preprocessing steps used for all messages. Then, we evaluate different components of the Smart Reply system and present overall usage statistics.
To generate the training data for all Smart Reply models from sampled accounts, we extracted all pairs of an incom-ing message and the user X  X  response to that message. For training the triggering model (see Section 6), we addition-ally sampled a number of incoming personal messages which the user didn X  X  reply to. At the beginning of Smart Reply pipeline (Figure 2), data is preprocessed in the following way: Language detection The language of the message is iden-Tokenization Subject and message body are broken into Sentence segmentation Sentences boundaries are identi-Normalization Infrequent words and entities like personal Quotation removal Quoted original messages and forwarded Salutation/close removal Salutations like Hi John and
After the preprocessing steps, the size of the training set is 238 million messages, which include 153 million messages that have no response.
The most important end-to-end metric for our system is the fraction of messages for which it was used. This is cur-rently 10% of all mobile replies. Below we describe in more detail evaluation stats for different components of the sys-tem. We evaluate all parts in isolation using both offline analysis as well as online experiments run on a subset of accounts.
In order to evaluate the triggering model, we split the data set described in Section 6.1 into train (80%) and test (20%) such that all test messages are delivered after train messages. This is to ensure that the test conditions are similar to the final scenario. We use a set of standard binary classifier metrics: precision, recall and the area under the ROC curve. The AUC of the triggering model is 0 . 854. We also compute the fraction of triggered messages in the deployed system, which is 11%. We observed that it may be beneficial to slightly over-trigger, since the cost of presenting a suggestion, even if it is not used, is quite low.
We evaluate the LSTM scoring model on three standard metrics: Perplexity, Mean Reciprocal Rank and Precision@K.
Perplexity is a measure of how well the model has fit the data: a model with lower perplexity assigns higher likelihood to the test responses, so we expect it to be better at pre-dicting responses. Intuitively, a perplexity equal to k means that when the model predicts the next word, there are on average k likely candidates. In particular, for the ideal sce-nario of perplexity equal to 1, we always know exactly what should be the next word. The perplexity on a set of N test samples is computed using the following formula: where W is the total number of words in all N samples,  X  P is the learned distribution and r i , o i are the i -th response and original message. Note that in the equation above only response terms are factored into P r . The perplexity of the Smart Reply LSTM is 17 . 0. By comparison, an n-grams language model with Katz backoff [11] and a maximum or-der of 5 has a perplexity of 31 . 4 on the same data (again, computed only from response terms).

While perplexity is a quality indicator, it does not actually measure performance at the scoring task we are ultimately interested in. In particular, it does not take into account the constraint of choosing a response in R . Therefore we also evaluate the model on a response ranking task: for each of N test message pairs ( o,r ) for which r  X  R , we compute of R . Then we sort the set R = { s,x 1 ,...,x N } in descending order. Finally, we define rank i = argmin j ( R j | R j = s ). Put simply, we are finding the rank of the actual response with respect to all elements in R . Multiclass-BOW 0 . 345 0 . 425 0 . 197 Using this value, we can compute the Mean Reciprocal Rank:
Additionally we can compute Precision@K. For a given value of K it is computed as the number of cases for which target response r was within the top K responses that were ranked by the model.

We compare the Smart Reply response selection model to three baselines on the same ranking task. The Random baseline ranks R randomly. The Frequency baseline ranks them in order of their frequency in the training corpus. This baseline captures the extent to which we can simply suggest highly frequent responses without regard for the contents of the original message. The Multiclass-BOW baseline ranks R using a feedforward neural network whose input is the original message, represented with bag of words features, and whose output is a distribution over the elements of R (a softmax).

As shown in Table 3, the Smart Reply LSTM significantly improves on the Frequency baseline, demonstrating that con-ditioning on the original message is effective; the model suc-cessfully extracts information from the original message and uses it to rank responses more accurately.

It also significantly outperforms the Multiclass-BOW base-line. There are a few possible explanations for this. First, the recurrent architecture allows the model to learn more sophisticated language understanding than bag of words fea-tures. Second, when we pose this as a mulitclass prediction problem, we can only train on messages whose response is in R , a small fraction of our data. On the other hand, the sequence-to-sequence framework allows us to take advantage of all data in our corpus: the model can learn a lot about original-response relationships even when the response does not appear in R exactly.

Note that an added disadvantage of the multiclass formu-lation is that it tightly couples the training of the model to the construction of R . We expect R to grow over time, given the incredible diversity with which people communi-cate. While a simpler application such as chat might only need a small number of possible responses, we find that for email we will need a tremendous number of possible sugges-tions to really address users X  needs.
We justify the need for both the diversity component and a sizable response space R by reporting statistics around unique suggestions and clusters in Table 4. The Smart Re-ply system generates daily 12 . 9k unique suggestions that belong to 376 unique semantic clusters. Out of those, peo-ple decide to use 4 , 115, or 31 . 9% of, unique suggestions and 313, or 83 . 2% of, unique clusters. Note, however, that many suggestions are never seen, as column 2 shows: the user may not open an email, use the web interface instead of mobile
F requency of response usage (log scale)
Table 4: Unique cluster/suggestions usage per day or just not scroll down to the bottom of the message. Also, only one of the three displayed suggestions will be selected by the user. These statistics demonstrate the need to go well beyond a simple system with 5 or 10 canned responses.
Figure 5 and Figure 6 present, respectively, the distribu-tion of the rank for suggested responses and the distribution of suggested clusters. The tail of the cluster distribution is long, which explains the poor performance of Frequency baseline described in Section 7.2.2.

We also measured how Smart Reply suggestions are used based on their location on a screen. Recall that Smart Reply always presents 3 suggestions, where the first suggestion is the top one. We observed that, out of all used suggestions, 45% were from the 1 st position, 35% from the 2nd position and 20% from the 3rd position. Since usually the third po-sition is used for diverse responses, we conclude that the diversity component is crucial for the system quality.
Finally, we measured the impact of enforcing a diverse set of responses (e.g., by not showing two responses from the same semantic cluster) on user engagement: when we com-pletely disabled the diversity component and simply sug-gested the three suggestions with the highest scores, the click-through rate decreased by roughly 7 . 5% relative.
We presented Smart Reply, a novel end-to-end system for automatically generating short, complete email responses. The core of the system is a state-of-the-art deep LSTM model that can predict full responses, given an incoming email message. To successfully deploy this system in Inbox by Gmail , we addressed several challenges:
Our clearest metric of success is the fact that 10% of mo-bile replies in Inbox are now composed with assistance from the Smart Reply system. Furthermore, we have designed the system in such a way that it is easily extendable to address additional user needs; for instance, the architecture of our core response scoring model is language agnostic, therefore accommodates extension to other languages in addition to English. The authors would like to thank Oriol Vinyals and Ilya Sutskever for many helpful discussions and insights, as well as Prabhakar Raghavan for his guidance and support.
