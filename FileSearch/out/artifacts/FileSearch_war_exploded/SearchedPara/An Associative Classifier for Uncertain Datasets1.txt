 Typical relational databases or database s in general hold coll ections of records representing facts. These facts are obser vations with known values stored in the fields of each tuple of the database. In other words, the observation represented by a record is assumed to have taken place and the attribute values are assumed to be true. We call these databases  X  X ertain database X  because we are certain about the recorded data and their values. In contrast to  X  X ertain X  data there is also  X  X ncertain data X ; data for which we may not be sure about the observation whether it really took place or not, or data for which the attribute values are not ascertained with 100% probability.

Querying such data, particularly computing aggregations, ranking or discov-ering patterns in probabilistic data is a challenging feat. Many researchers have focused on uncertain databases, also called probabilistic databases, for managing uncertain data [1], top-k ranking uncertain data [2], querying uncertain data [3], or mining uncertain data [4,5]. While many approches use an existancial uncer-tainty attached to a record as a whole, our model targets uncertain databases with probabilities attached to each attribute value.

This paper addresses the problem of devising an accurate rule-based classi-fier on uncertain training data. There are many classification paradigms but the classifiers of interest to our study are rule-based. We opted for associative clas-sifiers, classifiers using a model based on association rules, as they were shown to be highly accurate and competitive with other approaches [6].

After briefly reviewing related work for associative classification as well as published work on classifying in the presence of uncertainty, we present in Section 3 our novel classification method UAC. Finally in Section 4 we present empirical evaluations comparing UAC with other published works. Recently, a considerable amount of studi es in machine learning are directed to-ward the uncertain data classification, including: TSVC [7] (inspired by SVM), DTU [8] (decision tree), UNN [9] (based o n Neural Network), a Bayesian clas-sifier [10], uRule [11] (rule based), uHARMONY [12] and UCBA [13] (based on associative classifiers). However, models suggested by the previous work do not capture some possible types of uncertainty. In previous studies, numerical attributes are only modeled by intervals, while they may exist in other forms such as probability vectors. Categorical attributes are modeled by a probability distribution vector over th eir domain where the vector is unrealistically assumed to be completely known. We use a probability on each attribute value.
High accuracy and strong flexibility are some of the advantageous characteris-tics of the rule based classifiers. Investi gating rule based uncertain data classifiers has been the theme of many studies. One of these studies is uRule [11], which defines the information gain metric in presence of uncertainty. The probability of each rule classifying the instance is computed based on the weighting system introduced by uRule.

Associative classification is a large category of rule based classification in which the rule induction procedure is based on the association rule mining tech-nique. Some of the prominent associative classifiers are CBA [14], ARC [15], and CMAR [16]. In this paper, we introduce an associative classifier for uncertain datasets, which is based on CBA. CBA is highly accurate, flexible and efficient both in time and memory [14].

CBA directly adopts Apriori to mine the potential classification rules or strong ruleitems from the data. Ruleitems are those association rules of form a  X  c , where the consequence ( c ) is a class label and the antecedent ( a )isasetof attribute assignments .Each attribute assignment consists of an attribute and a value which belongs to the domain of that attribute. For example, if A 1 and A 2 are two attributes and c is a class label, r =( A 1 : u 1 ,A 2 : u 2  X  c ) is a ruleitem. r implies that if A 1 and A 2 have values of u 1 and u 2 respectively, the class label should be c . A ruleitem is strong if its support and confidence are above the predefined thresholds.

After mining the strong ruleitems, a large number of them are eliminated by applying the database coverage approach. This method of filtering rules is applied by all rule-based classifiers, particularly associative classifiers. However, in the case of uncertain data, database coverage presents a significant challenge. Rule based classifiers often need to evaluate various rules to pick the best ones. This level is critical in maintaining a high accuracy. The evaluation often involves the answer to the following question: To which training instances can a rule be applied? Yet, the answer is not obvious for uncertain datasets. Many uncertain dataset instances may satisfy the antecedent of a rule, each with a different probability. Existing uncertain data rule based classifiers have suggested various answers to this problem. uHARMONY suggested a lower bound on the probability by which the in-stance satisfies the rule antecedent. Th is approach is simple and fast, but the difficulty or even impossibility of setting the threshold is a problem. This is ex-plained in more detail in Section 3.2. uRule suggested to remove the items in the antecedent of the rule from the instance, to leave only the uncovered part of the instance every time. In contrast to uHARMONY, this method uses the whole dataset but it may cause sensitivity to noise which is undesirable. UCBA, wich is based on CBA, does not include the un certainty in the rule selection process; they select as many rules as possible. Th is method does not filter enough rules; so may decrease the accuracy.

In UAC, we introduce a new solution to the coverage problem. This compu-tation does not increase the running time complexity and needs no extra passes over the dataset. In this section, we present our novel algorithm, UAC . Before applying UAC to uncertain numerical attributes in the train sets, they are first transformed into uncertain categorical attributes using U-CAIM [10], assuming the normal distribution on the intervals. After discretization, the value of the i -th attribute for the j -th instance is a list of value-probability pairs, as shown in Equation 1. Building an associative classifier consist s of two distinct steps: 1-Rule Extrac-tion, 2-Rule Filtering. In this section each step of UAC is explained. Later, the procedure of classifying a new test instance is described. 3.1 Rule Extraction In uncertain datasets, an association rule is considered strong if it is frequent and its confidence (Conf ) is above a user defined threshold called minimum confi-dence . A ruleitem is frequent if its Expected Support (ES) is above a user defined threshold called minimum expected support . The definitions of the expected sup-port and the confidence are as follows.
 Definition. If a is an itemset and c is a class label, expected support (ES) and confidence (Conf) of a ruleitem are calculated by Equation 2. Here, the ruleitem is denoted by r = a  X  c and T is the set of all transactions.
 Some studies have criticized expected support and defined another measure which is called probabilistic support [17] [18]. Probabilistic support is defined as the probability of an itemset to be frequent with respect to a certain minimum expected support. However, probabilistic support increases the time complexity significantly. Therefore to be more effi cient, UAC uses the expected support. uHARMONY defines another measure instead of confidence which is called expected confidence . The computation of this measure takes O ( | T | 2 )timewhere |
T | is the number of instances. Computing confidence is only O (1), thus we use confidence for efficiency reasons. Our experimental results in Section 4 empiri-cally shows that our confidence based method can reach high accuracies.
Our rule extraction method is based on UApriori [4]. The candidate set is first initialized by all rules of form a  X  c where a is a single attribute assignment and c is a class label. After removing all infrequent ruleitems, the set of candidates is pruned by the pessimistic error rate method [19]. Each two frequent ruleitems with the same class label are then joined together to form the next level candi-date set. The procedure is repeated until the generated candidate set is empty, meaning all the frequent ruleitems have been found. Those ruleitems that are strong (their confidence is above the predefined threshold) are the potential clas-sification rules. In the next section, the potential ruleitems are filtered and the final set of rules is formed. 3.2 Rule Filtering The outcome of the rule extraction is a set of rules called rawSet . Usually the number of ruleitems in rawSet is excessive. Excessive rules may have negative impact on the accuracy of the classificat ion model. To prevent this, UAC uses the database coverage method to reduce the set of rules while handling the uncertainty. The initial step of the database coverage method in UAC is to sort rules based on their absolute precedence to accelerate the algorithm. Absolute precedence in the context of uncer tain data is define d as follows: Definition: Rule r i has absolute precedence over rule r j or r i " r j ,if a ) r i has higher confidence than r j ; b ) r i and r j have the same confidence but r i has higher expected support than r j ; c ) r i and r j have the same confidence and the same expected support but r i have less items in its antecedent than r j .
When data is not uncertain, confidence is a good and sufficient measure to examine whether a rule is the best classifier for an instance. But when uncertainty is present, there is an additional parameter in effect. To illustrate this issue, assume rules r 1 :[ m, t  X  c 1 ]and r 2 :[ n  X  c 2 ] having confidences of 0 . 8and0 . 7, respectively. It is evident that r 1 " r 2 . However, for a test instance like I 1 :[( m : According to CBA, r 1 should be used because its confidence is higher than that of r 2 . However, the probability that I 1 satisfies the antecedent of r 1 is small, so r 1 is not likely to be the right classifier. We solve this problem by including another measure called PI . PI or probability of inclusion , denoted by  X  ( r i ,I k ), is described as the probability by which rule r i can classify instance I k . PI is defined in Equation 3. In the example above  X  ( r 1 ,I 1 )isonly0 . 3  X  0 . 4=0 . 12, While  X  ( r 2 ,I 1 )is0 . 6. Next, we define applicability , denoted by  X  ( r i ,I k ) in Equation 4. Applicability is the probability by which rule r i correctly classifies instance I k and is used as one of the main metrics in UAC. For the previous example,  X  ( r 1 ,I 1 )=0 . 096 and  X  ( r 2 ,I 1 )=0 . 42. Thus, it is more probable that I 1 is correctly classified by r 2 than r 1 . Now based on the applicability, we define the concept of relative precedence of rule r i over rule r j with respect to I k .Thisisdenotedby r i " [ I defined as follows: Definition: Rule r i has relative precedence over rule r j with respect to instance I k denoted by r i applicability with respect to I k but r i has absolute precedence over r j . Having r " from the definition, that the concept of  X  X ore reliable X  rule in an uncertain data classifier is relative. One rule can be more reliable than the other when dealing with an instance, and the opposite may be true for another instance. In the previous example, r 2 has relative precedence over r 1 , even though r 1 has absolute precedence over r 2 .

UAC uses the relative precedence as well as the absolute precedence to filter rawSet. The database coverage algorithm of UAC has 3 stages that are explained below.
 Stage 1: Finding ucRules and uwRules. After sorting rawSet based on the absolute precedence, we make one pass ove r the dataset to link each instance i in the dataset to two rules in rawSet: ucRule and uwRule . ucRule is the rule with the highest relative preceden ce that correctly classifies i . In contrast, uwRule is the rule with the highest relativ e precedence that wrongly classifies i .The pseudocode for the first stage is presented in Algorithm 1.

In Algorithm 1, three sets are declared. U contains all the rules that clas-sify at least one training instance correctly. Q is the set of all ucRule swhich have relative precedence over their corresponding uwRule s with respect to the associated instances. If i.uwRule has relative and absolute precedence over the corresponding ucRule , a record of form &lt; i.id, i.class, ucRule, uwRule &gt; is put in A . Here, i.id is the unique identifier of the instance and i.class represents the class label.

To find the corresponding ucRule and uwRule for each instance, the procedure starts at the first rule of the sorted rawS et and descends. For example, if there is a rule that correctly classifies the target instance and has applicability of  X  , we pass this rule and look for the rules with higher applicabilities to assign as ucRule . Searching continues only until we reach a rule that has a confidence of less than  X  . Clearly, this rule and rules after it (with less confidence) have no chance of being ucRule . The same applies to uwRule . Also as shown in Algorithm 1 lines 4 and 6, the applicability values of ucRule and uwRule are stored to expedite the process for the next stages.

The purpose of the database coverage in UAC is to find the best classifying rule (coverage) for each instance in th e dataset. The covering rules are then contained in the final set of rules and others are filtered out. The best rule, that is the covering rule, in CBA is the hi ghest precedence rule that classifies an instance. This definition is not sufficien t for UAC because the highest precedence rule may have a small PI .

To solve the aforementioned problem, uHARMONY sets a predefined lower bound on the PI value of the covering rule, a method with various disadvantages. Clearly, not only estimating the suitable lower bound is critical, but it is also intricate, and even in many cases impo ssible. When predicting a label for an instance, rules that have higher PI than the lower bound are treated alike. To improve upon this, it is necessary to set the lower bound high enough to avoid low probability rules covering the instances. However, it remains that it is possible that the only classifying rules for some of the instances are not above that lower bound and are removed. Additionally, setting a predefined lower bound filters out usable information, while the purpose of the uncertain data classifiers is to use all of the available information. Moreover, having a single bound for all of the cases is not desirable. Different instances may need different lower bounds.
Given all the above reasons, we need to evaluate the suitable lower bound for each instance. The definition of the covering rule in UAC is as follows, where we use the applicability of i.ucRule as our lower bound for covering i . Definition: Rule r covers instance i if: a ) r classifies at least one instance cApplic represents the maximum rule applicability to classify an instance correctly. Thus, it is the suitable lower bound for the applicability of the covering rules. This will ensure that each instance is covered with the best classifying rule ( ucRule ) or a rule with higher relative and absolute precedence than ucRule .In the next two stages, we remove the rules that do not cover any instance from rawSet.
 Stage 2: Managing Replacements. In this stage (Algorithm 2), cases that were stored in A at Stage 1 are managed. A contains all cases where i.uwRule has relative and absolute precedence over i.ucRule ,thus i.ucRule may not cover i .If i.uwRule is flagged in Stage 1, i is covered by i.uwRule (lines 3, 4, and 5). Otherwise based on the definition of the covering rule in Stage 1, i may get the coverage by the other rules such as w which have the following characteristics: a ) w classifies i incorrectly; b ) w has relative precedence over i.ucRule with respect to i ; c ) w has absolute precedence over i.ucRule .

Function allCoverRules (line 7) finds all such rules as w within U ,whichare called the replacements of i.ucRule . The replacement relation is stored in a DAG (directed acyclic graph) called RepDAG . In RepDAG, each parent node has a Algorithm 1. UAC Rule Filtering: Stage 1 pointer to each child node via the replace set (line 12). The number of incoming edges is stored in incom (line 14). Each node represents a rule and each edge represents a replacement relation.

Each rule has a covered array in UAC where r.covered [ c ]isusedto store the total number of instances covered by r and labeled by class c .If r.covered [ r.class ]=0,then r does not classify any training instance correctly and is filtered out. Starting from line 22, we traverse RepDAG in its topologically sorted order to update the covered array of each rule. Rule r i comes before r j in the sorted order, if r i " r j and there is no instance such as I k where r j " [ I If a rule fails to cover any instance corr ectly (line 26), it does not have any effect on the covered array of the rules in its replace set. At the end of this Stage, enough information has been gathered to start the next stage, which finalizes the set of rules.
 Stage 3: Finalizing Rules. At stage 3 (Algorithm 3), the set of rules is fi-nalized. In this Stage, UAC filters the rules based on a greedy method of error reduction. Function computeError counts the number of instances that are cov-ered by rule r but have a different class label than r.class . The covered instances are then removed from the dataset. Function addDefaultClass finds the most frequent class label among the remaining instances (line 6). In line 8, the number of instances correctly classified by t he default class is calculated. totalError is the total errors made by the current rule r and the default class. In fact, each rule with positive coverage over its cla ss, is associated with a particular totalError , defClass ,and defAcc (line 10). After processing the rules, we break the set of rules from the minimum error and assign default and defApplic . defApplic is used in rule selection as an estimate of applicability of the default class.
Our rule filtering algorithm has a runtime of O ( | T | X | R | )intheworstcase scenario, where | T | is the number of instances in the dataset and | R | is the size Algorithm 2. UAC Rule Filtering: Stage 2 of rawSet. The worst case scenario is when at Stage 1, at least one ucRule or uwRule is the last rule in the sorted rawSet. This case rarely happens because the rules are sorted based on their absol ute precedence. UAC also makes slightly more than one pass over the dataset in the rule filtering step. Passes are made in Stage 1 and 2. Note that array A is usually small, given that most of the instances are usually classified by the highest ranked rules. The number of passes is an important point, because the dataset may be very large. Specially for datasets that can not be loaded into memory at once, it is not efficient to make multiple pases. This is an advantage for UAC over UCBA, which passes over the dataset once for each rule in rawSet. Next section explains the rule selection that is the procedure of classifying test instances based on the set of rules.
 Algorithm 3. UAC Rule Filtering: Stage 3 3.3 Rule Selection Rule selection is the procedure of classifying a test instance. In the previous sections, excessive rules were filtered ou t from rawSet. The remaining set of rules is called finalSet and classifies the test instances. UAC selects one classifying rule for each instance. The selected classifyin g rule has the highest relative precedence with respect to the test instance.

The role of the default class ( default in Algorithm 3 line 15) is to reduce the number of rules. The default class predicts the labels of those instances that are not classified by the rules in the finalSet. So the best predicting label for some of the test instances may be default class. But UAC may prefer rules with small PI values to the default class if we follow the procedure of  X  X ertain X  data classifiers. To prevent this, defApplic is used as an estimate for applicability of the default rule. This value shows the number of tr aining instances that were expected to be classified by the default rule. For example, when two classes, such as a and b , have the same population in the dataset but no rule labeled b exists, default rule has a very important role. Consequently, the value of the default applicability is high. As a result, if the highest preceden ce rule with respect to a test instance has less applicability than the default rule, the default rule will predict the label for that.
 We use an empirical study to compare UAC against the existing rule based meth-ods. In all of the reported experiments on UAC, the minimum support is set to 1%, the minimum confidence to 0 . 5 and the maximum number of mined associ-ation rules to 80 , 000. Each reported number is an average over 10 repetitions of 10-fold cross validations.

Since there is no public repository of uncertain datasets, we synthetically added uncertainty to 28 well known UCI datasets. This method was employed by all the studies in the field including uHARMONY, DTU and uRule, uncertain svm, UCBA, etc. and gives a close estimation of the classifier performance in the real world problems. We selected the same datasets as in [12] to compare our method with the results reported in their paper for uHARMONY, uRule and DTU. This also ensures that we did not choose only the datasets on which our method performs better.

To compare our method against other classifiers, we employ averaging tech-nique and case by case comparison [20]. The same method was employed by many other studies including CBA, uHARMONY, DTU and uRule to prove the better performance of their algorithms. Table 1 provides a comparison between UAC and other existing rule based meth ods in terms of accuracy. The reported accuracies for uHARMONY (#3), DTU (#4) and uRule (#5) are reproduced from [12]. We applied UAC (#2) to the same datasets generated by the same procedure of adding uncertainty as [12] to make the comparison meaningful. Value N/A , existing in the experiments reported by [12], shows that the classi-fier has run out of resources in their exper iments. In Table 1, uncertainty level is U10@4 meaning that datasets have 10 percent uncertainty, where only four of the attributes with the highest information gain are uncertain. To add a level 10 uncertainty to an attribute, it is attached with a 0 . 9 probability and the remain-ing 0 . 1 is distributed randomly among the other values present in the domain. The accuracies in this table are reported on already discretized versions of the dataset that are available online and referenced in [12].

The accuracies reported show that in most cases UAC has reached higher accuracies. For some data sets the improvement is significantly high, such as wine dataset with 36 . 79% and bands dataset with 19 . 77% improvement over the existing maximum accuracy. UAC reache s higher accuracies on the average too.
We have conducted further extensive experiments comparing UAC and UCBA since both stem from CBA. Due to lack of space we report here only the summary and refer the reader to [21] for furthe r details. Using a new and more general uncertainty model that we propose [21], we compared the accuracy of UAC and UCBA on all 28 datasets as in the previous experiments in Table 1 and show that UAC outperforms UCBA. On average, over the 28 datasets, the accuracy of UAC was 74.7% while UCBA averaged 67.5% if a sampled-based model is used when a numerical attribute is assign ed a set of possible values; and respec-tively 70.3% versus 66.7% if an interval-based model is used [21]. In short, for a numerical attribute, the sampled-based model considers the attribute value to be expressed by a set of values with their respective probabilities, while the interval-based model considers the attribute value to be an interval with a prob-ability distribution function. Moreover, comparing the training time, UAC was in many cases about 2 orders of magnitude faster (i.e. X100) than UCBA and produced significantly less rules for all tested uncertainty levels. This demon-strates the efficacy of the rule pruning startegy managing to preserve a better set of rules than UCBA. In this paper we propose an effective way to prune associative classification rules in the presence of uncertainty and present a complete associative classifier for uncertain data that encompasses this pruning. Empirical results show that our algorithm outperforms 4 existing rule-based methods in terms of accuracy on average for 28 datasets and also show that UAC outperforms UCBA signifi-cantly for these 28 datasets in terms of accuracy even though UAC produces less classification rules and has a smaller runtime than UCBA.

