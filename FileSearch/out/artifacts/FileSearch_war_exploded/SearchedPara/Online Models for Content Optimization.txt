 The web has become the central distribution channel for info rmation from traditional sources such as news outlets as well as rapidly growing user-generated co ntent. Developing effective algorithmic approaches to delivering such content when users visit web p ortals is a fundamental problem that has not received much attention. Search engines use automat ed ranking algorithms to return the most relevant links in response to a user X  X  keyword query; li kewise, online ads are targeted using automated algorithms. In contrast, portals that cater to us ers who browse a site are typically pro-grammed manually. This is because content is harder to asses s for relevance, topicality, freshness, and personal preference; there is a wide range in the quality ; and there are no reliable quality or trust metrics (such as, say, PageRank or Hub/Authority weights fo r URLs).
 Manual programming of content ensures high quality and main tains the editorial  X  X oice X  (the typical mix of content) that users associate with the site. On the oth er hand, it is expensive to scale as the number of articles and the number of site pages we wish to prog ram grow. A data-driven machine learning approach can help with the scale issue, and we seek t o blend the strengths of the editorial and algorithmic approaches by algorithmically optimizing content programming within high-level constraints set by editors. The system we describe is curren tly deployed on a major Yahoo! portal, and serves several hundred million user visits per day.
 The usual machine-learning approach to ranking articles sh own to users uses feature-based models, trained using  X  X ffline data X  (data collected in the past). Af ter making a significant effort of feature engineering by looking at user demogrpahics, past activiti es on the site, various article categories, keywords and entities in articles, etc., we concluded that i t is difficult to build good models based solely on offline data in our scenario. Our content pool is sma ll but changing rapidly; article life-times are short; and there is wide variability in article per formance sharing a common set of feature values. Thus, we take the approach of tracking per-article p erformance by online models, which are initialized using offline data and updated continuously using real time data. This online aspect opens up new modeling challenges in addition to classical fe ature based predition, as we discuss in this paper. We consider the problem of optimizing content displayed in a module that is the focal point on a major Yahoo! portal; the page also provides several other se rvices (e.g., Mail, Weather) and content links. The module is a panel with four slots labelled F1, F2, F 3, F4. Slot F1, which accounts for a large fraction of clicks, is prominent, and an article displ ayed on F1 receives many more clicks than when it is displayed at F2, F3 or F4.
 The pool of available articles is created by trained human ed itors, and refreshed continually. At any point in time, there are 16 live articles in the pool. A few new articles programmed by ed itors get pushed into the system periodically (every few hours) and replace s ome old articles. The editors keep up with important new stories (e.g., breaking news) and eliminate irrelevant and fading stories, and ensure that the pool of articles is consistent with the  X  X  oice X  of the site (i.e., the desired nature and mix of content). There is no personalization in the edito rially programmed system; at a given time, the same articles are seen by all users visiting the pag e.
 We consider how to choose the best set of four articles to disp lay on the module to a given user. Since the mix of content in the available pool already incorp orates constraints like voice , topicality, etc., we focus on choosing articles to maximize overall clic k-through rate (CTR), which is the total number of clicks divided by total number of views for a time in terval. To simplify our presentation, we only describe learning from click feedback obtained from the most important F1 position; our framework (and system) can use information from other posit ions as well. 2.1 System Challenges Our setting poses many challenges, the most important of whi ch are the following: A significant effort was required to build a scalable infrast ructure that supports near real-time data collection. 1 Events (users X  clicks and page views) are collected from a la rge number of front-end web servers and continuously transferred to data collectio n clusters, which support event buffering to handle the time lag between the user viewing a page and then clicking articles on the page. The event stream is then fed to the modeler cluster which runs lea rning algorithms to update the models. Periodically, the front-end web servers pull the updated mo dels and serve content based on the new models. A complete cycle of data collection, model update, a nd model delivery takes a few minutes. Figure 1: CTR curves of a typical article in two buckets. (a) shows the a rticle X  X  CTR decay when shown continuously in a bucket at position F1; (b) shows the a rticle X  X  CTR in the random bucket. 2.2 Machine Learning Challenges A serving scheme is an automated or manual algorithm that decides which artic le to show at different positions of our module for a given user. Prior to our system, articles were chosen by human editors; we refer to this as the editorial serving scheme. A random sample of the user population is ref erred to as a bucket .
 We now discuss the issues that make it tricky to build predict ive models in this setting. We tried the usual approach of building offline models based on retros pective data collected while using the editorial serving scheme. User features included Age, Gend er, Geo-location and Inferred interests based on user visit patterns. For articles, we used features based on URL, article category (e.g., Sports, Entertainment) and title keywords. However, this a pproach performed poorly. The reasons include a wide variability in CTR for articles having a same s et of feature values, dramatical changes of article CTR over time, and the fact that retrospective dat a collected from non-randomized serving schemes are confounded with factors that are hard to adjust f or (see Section 5). Also, our initial studies revealed high variability in CTRs for articles shar ing some common features (e.g., Sports ar-ticles, Entertainment articles). We achieved much better p erformance by seeking quick convergence (using online models) to the best article for a given user (or user segment); a lost opportunity (failure to detect the best article quickly) can be costly and the cost increases with the margin (difference between the best and selected articles). We now discuss some of the challenges we had to address. Non-stationary CTRs : The CTR of an article is strongly dependent on the serving sc heme used (especially, how much F1 exposure it receives) and it may cha nge dramatically over time. Hence, learning techniques that assume process stationarity are i napplicable. In order to ensure webpage stability, we consider serving schemes that don X  X  alter the choice of what to show a user in a given slot until a better choice is identified. Figure 1 (a) shows th e CTR curve of a typical article subject to such a serving scheme. The decay is due to users getting exposed to an article more than once. Exposure to an article happens in different ways and to diffe rent degrees. A user may get exposed to an article when he/she sees a descriptive link, or clicks o n it and reads the article. A user may also click multiple  X  X ee also X  links associated with each ar ticle which may perhaps be a stronger form of exposure. In our analysis, we consider such related c licks to be a single click event. View exposure is more noisy since our module is only one of many con tent pieces shown on the portal. A user may be looking at the Weather module when visiting the p ortal or he may have looked at the article title in the link and not liked it. Hence, explain ing the decay precisely in terms of repeat exposure is difficult. For instance, not showing an article t o a user after one page view containing the link may be suboptimal since he may have overlooked the li nk and may click on it later. In fact, a large number of clicks on articles occur after the firs t page view and depends on how a user navigates the portal. Instead of solving the problem by impo sing serving constraints per user, we build a component in our dynamic model that tracks article CT R decay over time. We still impose reasonable serving constraints to provide good user experi ence X  X e do not show the same article to a user x minutes ( x = 60 worked well) after he/she first saw the article.
 In addition to decay, the CTR of an article also changes by tim e of day and day of week. Figure 1 (b) shows the CTR of a typical article when served using a rand omized serving scheme (articles served in a round-robin fashion to a randomly chosen user pop ulation). The randomization removes any serving bias and provides an unbiased estimate of CTR sea sonality. It is evident that CTRs of articles vary dramatically over time, this clearly shows the need to adjust for time effects (e.g., diurnal patterns, decay) to obtain an adjusted article scor e when deciding to rank articles. In our current study, we fitted a global time of day curve at 5 minute resolution to data obtained from randomized serving scheme through a periodic (weekly) adap tive regression spline. However, there are still interactions that occur at an article level which w ere difficult to estimate offline through article features. Per-article online models that put more w eight on recent observations provide an effective self adaptive mechanism to automatically accoun t for deviations from the global trend when an article is pushed into the system.
 Strong Serving Bias : A model built using data generated from a serving scheme is b iased by that scheme. For example, if a serving scheme decides not to show a rticle A to any user, any model built using this data would not learn the popularity of A from users X  feedback. In general, a serv-ing scheme may heavily exploit some regions in the feature sp ace and generate many data points for those regions, but few data points for other regions. Mod els built using such data learn very little about the infrequently sampled regions. Moreover, e very non-randomized serving scheme in-troduces confounding factors in the data; adjusting such fa ctors to obtain unbiased article scores is often difficult. In fact, early experiments that updated mod els using data from editorial bucket to serve in our experimental buckets performed poorly. This bi as also affects empirical evaluations or comparisons of learning algorithms based on retrospective data, as we discuss later in Section 5. Interaction with the editorial team : The project involved considerable interaction with human ed-itors who have been manually and successfully placing artic les on the portal for many years. Under-standing how that experience can be leveraged in conjuction with automated serving schemes was a major challenge, both technically and culturally (in that editors had to learn what ML algorithms could do, and we had to learn all the subtle considerations in what to show). The result is our frame-work, wherein editors control the pool and set policies via c onstraints on what can be served, and the serving algorithm chooses what to show on a given user vis it. Experimental Setup : We created several mutually exclusive buckets of roughly equal sizes from a fraction of live traffic, and served traffic in each bucket usi ng one of our candidate serving schemes. All usual precautions were taken in the bucket creation proc ess to ensure statistical validity of results. We also created a control bucket that ran the editorial servi ng scheme. In addition, we created a separate bucket called the random bucket, which serves articles per visit in a round-robin fas hion. Framework : Our framework consists of several components that are desc ribed below. Tracking article CTR in an online fashion is a well studied ar ea in time series with several methods available in the literature [3][7]; but the application to c ontent optimization has not been carefully studied. We provide a description of three dynamic models th at are currently used in our system. 4.1 Estimated Most Popular: EMP This model tracks the log-odds of CTR per article at the F1 pos ition over time but does not use any user features. The subscript t in our notation refers to the t th interval after the article is first displayed in the bucket. Let c the empirical logistic transform defined as y Gaussian for large n we get roughly 300  X  400 observations at the F1 position per article in a 5 -minute interval in the random bucket, hence the above transformation is appropria te for EMP and SS with few tens of user segments. Given that there may be a decay pattern in log-odds of CTR at the F1 position with increasing article lifetime, we fit a dynamic linear growth c urve model which is given by In Equation 1, o tion), the series over the time interval from t  X  1 to t , evolving during that interval according to the addi-tion of the stochastic element  X  X  Model parameters are initialized by observing values at t = 1 for an article in random bucket, actual tracking begins at t = 2 . In general, the initialization takes place through a featu re based offline model built using retrospective data.
 To provide more intuition on how the state parameters  X   X  through weighted least squares. The addition of non-zero ev olution makes this straight line dynamic and helps in tracking decay over time. In fact, the values of s tate evolution variance components  X  2 and  X  2 the model; large relative values smooth more by using a large r history to predict the future. Model fitting is conducted through a Kalman filter based on a discounting concept as explained in [7]. Details are discussed in [1]. 4.2 Saturated Segmented Model: SS This model generalizes EMP to incorporate user features. In particular, user covariates are used to create disjoint subsets (segments), a local EMP model is bui lt to track item performance in each user segment. For a small number of user segments, we fit a separate EMP model per user segment for a given item at the F1 position. As the number of user segments g rows, data sparseness may lead to high variance estimates in small segments, especially duri ng early part of article lifetime. To address this, we smooth article scores in segments at each time point through a Bayesian hierarchical model. In particular, if ( a different user segments at time t , we derive a new score as follows: where  X  a age X  towards the most popular is obtained by the DerSimonian and Laird estimator [10], widely used in meta-analysis. 4.3 Online Logistic Regression: OLR The SS does not provide the flexibility to incorporate lower o rder interactions when working with model based on two-factor interations (age-gender, age-ge o, and gender-geo) may provide better performance. We use an efficient online logistic regression approach to build such models. The OLR updates parameter for every labelled event, positive or neg ative. Instead of achieving additivity by empirically transforming the data as in EMP and SS, it posits a Bernoulli likelihood for each event and achieves linearity by parametrizing the log-odds as a li near function of features. However, this makes the problem non-linear; we perform a quadratic ap proximation through a Taylor series expansion to achieve linearity. Modeling and fitting detail s are discussed in [1]. In this section, we present the results of experiments that c ompare serving schemes based on our three online models (EMP, SS, and OLR) with the current edito rial programming approach (which we refer to as ED). We show our online models significantly out perform ED based on bucket testsing the four alternatives concurrently on live traffic on the por tal over a month. Then, by offline analysis, we identified the reason personalization (based on user feat ures in OLR or segmentation in SS) did not provide improvement X  X t is mainly because we did not ha ve sufficiently diverse articles to exploit, although SS and OLR are more predictive models than EMP. Finally, by extensive bucket tests on live traffic (which is an expensive and unusual oppor tunity for evaluating algorithms), we cast serious doubts on the usefulness of the common practice of comparing serving algorithms based on retrospective data (collected while using another servi ng scheme), and suggest that, without a random bucket or an effective correction procedure, it is essential to conduct tests on live traffic for statistical validity.
 Bucket Testing Methodology : After conducting extensive offline analysis and several sm all pilots with different variations of models (including feature sel ection), we narrowed the candidates for live-traffic evaluation to the following: (1) EMP, (2) SS wit h Age  X  Gender segments, (3) OLR with features terms: Article + Position + Age  X  ContentCategory + Gender  X  ContentCategory (geo-location and user behavioral features were also bucket-tes ted in some other periods of time and provided no statistically significant improvement), and (4 ) ED. We used each of these schemes to serve traffic in a bucket (a random set of users) for one month; the four buckets ran concurrently. We measure the performance of a scheme by the lifts in terms of CTR relative to the baseline ED scheme. We also obtained significant improvements relative to round-robin serving scheme in the random bucket but do not report it to avoid clutter.
 Online Model Comparison : Our online models significantly increased CTR over the orig inal man-ual editorial scheme. Moreover, the increase in CTR was achi eved mainly due to increased reach , i.e., we induced more users to click on articles. This provid es evidence in favor of a strategy where various constraints in content programming are incorporat ed by human editors and algorithms are used to place them intelligently to optimize easily measura ble metric like CTR. Figure 2 (a) shows the CTR lifts of different algorithms during one month. All t hree online models (EMP, SS and OLR) are significantly better than ED, with CTR lifts in the range o f 30%  X  60% . This clearly demonstrates the ability of our online models to accurately track CTRs in r eal-time. Shockingly, the models that are based on user features, SS and OLR, are not statistically different from EMP, indicating that per-onalization to our selected user segments does not provide a dditional lift relative to EMP, although both SS and OLR have better predictive likelihoods relative to EMP on retrospective data analysis. Figure 2: Experimental results: (a) and (b) show bucket test results. (c) on the x-axis is the CTR of a polar article in a segment, on the y-axis is the CTR of the g lobal best article (during the polar article X  X  lifetime) in the same segment. Refer to text for de finition of polar.
 Figure 2 (b) shows the lift relative to ED in terms of the fract ion of clicking users. It shows that the lift achieved by the online models is not confined to a small co hort of users, but reflects conversion of more users to clickers.
 Analysis of Personalization : We did not expect to find that personalization to user segmen ts pro-vided no additional CTR lift relative to EMP despite the fact that user features were predictive of article CTR. A closer look at the data revealed the main cause to be the current editorial content generation process, which is geared to create candidate art icles that are expected to be popular for all users (not for different user segments). In fact, there were articles that have more affinity to some user segments than others X  X e define these to be articles whose CTR in a given segment was at least twice the article X  X  overall CTR, and refer to them as polar articles. However, whenever polar articles were in the pool of candidate articles, there was usually a no n-polar one in the pool that was more popular than the polar ones across all segments. As a result, we should chose the same non-polar one for all segments. Figure 2 (c) shows, for the gender segmenta tion, that polar articles almost always co-exist with an article whose overall CTR is greater than ev en the CTR in the segment of the polar article. For the AgeXGender segmentation, the global-best article was the same as the segment-best article about 65% of the intervals; the maximum expected CTR lift over global r anking was only about 1% . We observe similar patterns for segmentations based on oth er user features.
 Retrospective Evaluation Metrics vs. Bucket Tests : It is common practice in existing literature to evaluate a new serving algorithm using predictive metric s obtained by running the algorithm on retrospective data (collected while using another serving scheme). For instance, such an approach has been used extensively in studying ad matching problems [ 11]. In our setting, this is equivalent to comparing a new serving scheme (e.g., EMP, SS, or OLR) to ED by computing some predictive metric on retrospective data obtained from ED. We found the p erformance differences obtained using retrospective data do not correlate well to those obta ined by runing on live traffic [1]. This finding underscores the need for random bucket data, effecti ve techniques to correct the bias, or a rapid bucket testing infrastructure to compare serving sch emes. Google News personalization [13], which uses collaborativ e filtering to provide near real-time rec-ommendation of news articles to users, is the most closely re lated prior work. However, while they select from a vast pool of unedited content aggregated from n ews sites across the globe, we recom-mend a subset from a small list of items chosen by editors. On t he one hand, this allows us to build per-article models in near real-time; on the other, the edit orially controlled mix of items means all articles are of high quality (making it hard to achieve lift b y simply eliminating bad articles). Recent work on matching ads to queries [11] and ads to webpages [2] ar e related. However, their primary emphasis is on constructing accurate feature-based offline models that are updated at longer time in-tervals (e.g., daily), such models provide good initializa tion to our online models but perform poorly for reasons discussed in section 2.2. In [9], the authors con sider an active exploration strategy to improve search rankings, which is similar in spirit to our ra ndomization procedure. Our problem is also related to the rich literature on multi-armed bandit pr oblems [5][8][14][12]. However, we note that many of the assumptions made in the classical multi-arm ed bandit and reinforcement learning literature are not satisfied in our setting (dynamic set of ar ticles, short article lifetime, batch-learning, non-stationary CTR, lagged response). In fact, short artic le lifetimes, dynamism of the content pool and the importance of learning article behaviour very quick ly are the major challenges in our sce-nario. Preliminary experiments performed by obvious and na tural modifications to the widely used UCB1 scheme [8] performed poorly. In a recent study [4] of a co ntent aggregation site, digg.com, Wu et al. built a model for story popularity. However, their a nalysis is based on biased retrospective data, whereas we deployed our models and present results fro m tests conducted on live traffic. In this paper, we described content optimization , the problem of selecting articles to present to a user who is intent on browsing for information. There are man y variants of the problem, depending on the setting. One variant is selecting from a very large and diverse pool of articles. Examples include recommending RSS feeds or articles from one or more R SS feeds, such as Google X  X  news aggregation, and segmentation and personalization are lik ely to be effective. The variant that we addressed involves selecting from a small, homogeneous set of articles; segmentation may not be effective unless the pool of articles is chosen to be diverse , and there is a high premium in quickly estimating and tracking popularity per-article.
 Our work suggests offline feature based models are not good en ough to rank articles in a highly dynamic content publishing system where article pools are s mall, dynamic and of high quality; life-times are short; and the utility metric being measured (e.g. , CTR) has a strong dynamic component. In fact, the biased nature of data obtained from a non-random ized serving scheme also underscores the need to obtain some percentage of data from a randomized e xperimental design. The delicate tradeoffs involved in maximizing utility (e.g., total numb er of clicks) by quickly converging to the best article for a given user (or user segment) through onlin e models that are effectively initialized through offline feature based models (after adjusting for co nfounding factors), and performing unbi-ased exploration through small randomized experiments are the key machine learning challenges in this setting. While we have addressed them sufficiently well t o handle small content pools, dealing with larger pools will require significant advances, and is t he focus of our current research.
