 TAKASHI TSUNAKAWA, NAOAKI OKAZAKI, and XIAO LIU University of Tokyo and JUN X  X CHI TSUJII UniversityofTokyo,UniversityofManchester,NationalCentreforTextMining UK 9: 2  X  T. Tsunakawa et al.
 1. INTRODUCTION The bilingual lexicon is a critical resource for multilingual applications in nat-ural language processing (NLP) including machine translation (MT) [Brown et al. 1990] and cross-lingual information retrieval (CLIR) [Nie et al. 1999]. A bilingual lexicon includes pairs of terms ; each pair shares some common meanings. Hereinafter, a term denotes a lemma of a lexical item in the lexica. A term consists of one or multiple words segmented by spaces. 1 A number of bilingual lexica have been constructed manually, despite their expensive com-pilation costs. It is unrealistic to construct bilingual lexica for all language pairs in the world. Moreover, it is difficult to maintain a bilingual lexicon with neologism. Consequently, comprehensive bilingual lexica are available only for small subsets of language pairs and are unavailable for most language pairs.
This article proposes an integrated framework for building a bilingual lex-icon between the Chinese ( L c ) and Japanese ( L j ) languages. Figure 1 shows the framework and resources for building the lexicon. Since the language pair L  X  L j does not include English ( L e ), which is a central language in the world, few large-scale bilingual resources between L c and L j have been constructed. One solution to this problem is to build a L c  X  L j bilingual lexicon through a [Tanaka and Umemura 1994; Schafer and Yarowsky 2002; Zhang et al. 2005]. The basic idea of this approach is that a source term c is translated into a tar-get term j if an English term e exists such that the lexica L c  X  L e and L e  X  L j suffers from the out-of-vocabulary problem in two bilingual lexica; a transla-tion of a term c can only be obtained if the lexicon L c  X  L e includes the term c , and the lexicon L e  X  L j includes its English translation e .

Meanwhile, both the Chinese and Japanese languages use Chinese char-acters (or Han characters ), and each language encodes semantic informa-tion as ideographic characters. Because Han characters were imported from Chinese ( hanzi ) to Japanese ( kanji ) more than a thousand years ago, strong correlations still exist between these languages. For this reason, Japanese and Chinese people can often guess the meaning of Han characters in the other language even though these two languages belong to different language families. This characteristic between Japanese and Chinese is also useful for building a bilingual lexicon, as exemplified by various bilingual applica-tions including bilingual text alignment [Tan and Nagao 1995], cross-lingual A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 3 information retrieval [Gey et al. 2005], and bilingual dictionary construction [Zhang et al. 2004 and Goh et al. 2005].

In order to address the out-of-vocabulary problem in the previous pivotal ap-proach, we apply the statistical machine translation (SMT) framework [Koehn et al. 2003] for bilingual dictionary construction. We can solve this problem because we extract phrase translation pairs that consist of a portion of terms. A translation model between L c and L j is obtained by assuming that two lex-method attempts to search the most appropriate phrase translation for each part of  X  w c , and thus generates the optimal translation. Another advantage to this method is its ability to incorporate various characteristics that asso-ciate languages L c  X  L j . For example, we can integrate an additional transla-tion model obtained from a direct bilingual lexicon L c  X  L j . We also explore the use of hanzi-kanji similarity , which measures the similarity of characters in the source c and target j terms. The hanzi-kanji similarity is also obtained from the direct bilingual lexicon L c  X  L j by using the SMT technique. 1.1 Han Characters For those who are unfamiliar with Han characters in Japanese and Chinese languages, we will briefly overview the similarity/dissimilarity of Han char-acters in these languages. In the Unicode standard, most Han characters are stored in the block of CJK Unicode Ideographs (4E00 X 9FFF). Since the CJK block was designed to be shared by the Japanese, Chinese, and Korean languages, identical Unicode numbers are likely to be assigned to Han char-acters of the same shape in Japanese and Chinese. This is a useful clue to associate Japanese and Chinese characters; we can associate Japanese and Chinese characters without any conversion on the Unicode.
 9: 4  X  T. Tsunakawa et al.
 Nonetheless, some characters have different meanings in Japanese and Chinese due to the regional and historical changes of the languages. More-over, during the standardization process of the Unicode, regional or historical variants of Han characters are unified. Therefore, it is not always successful to associate Japanese and Chinese characters on the Unicode.

Table I shows the situation of Han characters in the Unicode in six cases.  X  Case 1: The kanji and the hanzi (Simplified Chinese, Traditional Chinese) are identical in terms of Unicode, shape, and their meanings. We can directly map kanji into hanzi via Unicode.  X  Case 2: The kanji and the hanzi (Simplified Chinese, Traditional Chinese) are identical in terms of Unicode and shape, but their meanings are differ-ent. We should not map kanji into hanzi via Unicode since incorrect transla-tions would result.  X  Case 3: The kanji is the same as the one in Traditional Chinese, but different from the one in Simplified Chinese even though they share the same mean-ing in these languages. In this case, we can map kanji into hanzi in Tra-ditional Chinese via Unicode, and then convert hanzi between Traditional
Chinese and Simplified Chinese.  X  Case 4: This case is similar to Case 3, but the kanji and hanzi express differ-ent meanings. Thus, the mapping between the kanji and hanzi will cause a translation error. The naive approach described in Case 3 will be unsuccess-ful here.

A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 5  X  Case 5: The kanji and the hanzi (Simplified Chinese, Traditional Chinese) present the same meaning. However, the Unicode standard assigns different values for kanji and hanzi, due to the slight difference in shape.  X  Case 6: These characters are no longer similar in terms of Unicode and shape although the meaning is the same. In fact, these characters share the origin, but were simplified in different ways.

From Table I, we can find a naive approach in which kanji is mapped to hanzi with identical Unicode characters that can only handle the Case 1. A more advanced approach may use the mappings between Traditional Chinese and Simplified Chinese to deal with Case 3, but may also increase translation errors as shown in Case 4. To make matters worse, the conventional approach cannot associate kanji and hanzi in other cases. This is because Unicode en-coding is not a linguistically based encoding scheme; it is rather an initiative to facilitate interoperability with the existing standards. 2. CONSTRUCTING A CHINESE-JAPANESE LEXICAL TRANSLATION We employ the phrase-based SMT framework for constructing a Chinese-Japanese lexical translation system. In this study, we make use of bilingual lexica as well as a parallel sentence corpus, which is the Chinese-Japanese parallel corpus with 85,982 sentence pairs used in our experiment. We assume a pair of source and target terms as an  X  X ligned sentence pair X  for training an SMT model. 2.1 Phrase-Based Statistical Machine Translation 2.1.1 Log-Linear Model. A log-linear model of SMT [Och and Ney 2002] translates a source term  X  w c into the target language by finding the term  X  w j that maximizes the translation probability, linear modeling is useful to attach various  X  X cores X  in the form of feature minimum error rate training [Och 2003] so that the translation model maxi-mizes the BLEU score [Papineni et al. 2001] on the development set. 2.1.2 Decoding. A decoding step of SMT tries to find the output sentence that has the largest probability based on the translation model among the can-didate sentences. Because searching all combinations of translation pairs in the phrase table is unreasonable, we need an efficient method that would re-duce the search space. Thus, the decoder maintains a search tree of hypotheses 9: 6  X  T. Tsunakawa et al.
 that correspond to the states of the partial translations for the input words. Starting with an initial hypothesis, the decoder continues to expand the hy-potheses by translating a phrase that covers some of the untranslated input words. Among the hypotheses that cover all input words, the decoder finds a hypothesis with the highest probability. The final translation result is the tar-get sentence generated in the most probable hypothesis. n -best translations can also be obtained by finding n -best hypotheses in the search space, after the hypotheses that have the same partial translation are merged. Since the number of hypotheses is exponential to the term length, we prune unlikely hypotheses so that the algorithm produces a translation within a reasonable time frame. For pruning, hypotheses that have the same number of input words translated thus far are placed in the same stack, in which hypothe-ses with low probability are pruned out. Two kinds of pruning methods are applied: threshold pruning and histogram pruning. The former discards hy-potheses with probabilities that less have than a factor  X  of a probability of the best hypothesis in each stack, where  X  is a threshold. The latter keeps only w hypotheses in each stack where w is the beam search width. 2.2 Combining of Two Sets of Phrase Pairs Through a Pivot Language Recently, several researchers have proposed the use of pivot languages for phrase-based statistical machine translation [Wang et al. 2006; Utiyama and Isahara 2007; Wu and Wang 2007]. In this approach, the translation scores be-tween source and target phrases are calculated through the phrases in a pivot language. Inspired by this approach, we build a Chinese-Japanese translation system for technical terms by using English as the pivot language. Figure 2 illustrates the overall framework of our translation system. Our goal is to construct a Chinese-to-Japanese SMT system specialized for technical terms. In order to obtain a Chinese-Japanese phrase table with translation scores, we merge two phrase tables, the Chinese-English and Japanese-English tables, each of which is built by the corresponding bilingual lexicon.
 Let us suppose that we have two bilingual lexica L c  X  L e and L e  X  L j :
A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 7 the maximum likelihood estimation from the cooccurrence frequencies that are consistent with the word alignment in the bilingual lexica:  X  w x and  X  translation probabilities between the source and target phrases,  X  w j and  X  w c , 2.3 Features for the Log-Linear Translation Model (  X  w uses the translation probability in the inverse direction, p (  X  w c |  X  w j ), 2.3.1 Lexical Weighting. Lexical weighting [Koehn et al. 2003] is a mea-sure for evaluating the validity of phrase pairs. When we derive a phrase quency under the word alignments. The lexical weight p w for a phrase pair (  X  w 9: 8  X  T. Tsunakawa et al. phrase method presented in Wu and Wang [2007]. The method first induces the word alignment a between the source and target phrases. Let a 1 and a 2 be by Thus, the cooccurrence frequency of the word pair ( j , c ) is estimated by (  X  w probability. The lexical weight is estimated as Similar to Equations 7 and 8, we define the lexical weighting features h 2.3.2 Language Model. The language model in the target language L j is in-Here, p (  X  w j ) presents the probability of  X  w j in the target language. 2.3.3 Distance-Based Reordering Model. Following the previous work [Koehn et al. 2003], we introduce the penalty score when phrases are reordered, Here x i is the position of the beginning word of the i -th translated source ing ( y 0 = 0). If the translation is monotonic, that is, a phrase reordering does not occur, this feature yields the maximum value 0. 2.3.4 Term Length. In order to consider the term length in the decoder, the term length feature is introduced, Here |  X  w | denotes the number of words in  X  w . 2.3.5 Phrase Translation Probability from a Direct Bilingual Lexicon and Parallel Corpus. Although we obtain the phrase table through a pivot lan-guage, an existing direct Chinese-Japanese lexicon and parallel corpus can
A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 9 provide useful information in evaluating the translation phrases. Hence, we use this direct lexicon and parallel corpus to introduce a feature function, phrase translation probabilities derived from the direct lexicon and parallel corpus by word alignment and phrase extraction. These probabilities are cal-culated from the relative frequency based on the maximum likelihood estima-tion. In addition, we introduce the target-to-source probabilities as feature 2.3.6 Hanzi-Kanji Similarity. In this section, we introduce a novel feature based on Han characters included in Chinese and Japanese terms. For tak-ing into account the characteristics of identical or semantically similar Han characters into the Chinese-Japanese lexical translation, we introduce the term  X  w c .
 h c and kanji h j . We apply the word alignment technique for the Chinese-Japanese bilingual lexicon, assuming that each Han character as a  X  X ord X . The alignment model is based on the IBM Model 4 [Brown et al. 1993]. Af-ter convergence, we obtain estimated translation probabilities of characters are satisfied:  X  h j and h c have the same code on the Unicode,  X  h j and h c are found in a hanzi-kanji table as a corresponding pair. We use a hanzi-kanji table obtained from A S W ! S W  X  W S  X  g h (Tra-ditional/Simplified Chinese Characters and Shinjitai Table) [Sato 2007]. 3 If these conditions are not satisfied, we employ a simple smoothing method by assuming that the total number of Han characters is 50,000; in other words, for each unknown character pair ( h c , h j ), we assign  X  = 1 / 50000 as the similarity score. In summary, we define the hanzi-kanji similarity score csim( h c , h j ) as: 9: 10  X  T. Tsunakawa et al.
 where h c = h j shows that h c and h j share an identical code on the Unicode, and h c  X  h j denotes that h c and h j are found in the hanzi-kanji table as a corresponding pair.
 phrase  X  w j and a Chinese phrase  X  w c is calculated by the algorithm described in Figure 3. This algorithm searches for the most probable character align-ment based on the character similarity csim( h c , h j ), and returns the largest F 0 , the index set v , and l is the number of alignments that have been identified previously. F  X  w By using this function, we define h hksim (  X  w j ,  X  w c ) by where  X  is an empty set.
 A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 11 we adopt a beam search technique by limiting the recursive calculation of F 3. EXPERIMENT 3.1 Data For our pivotal model, we prepared the Japanese-English lexicon released by JST (527,206 term pairs), and the Chinese-English lexicon compiled by Wanfang Data 4 (525,259 term pairs). Both cover a wide range of named en-tities and technical terms that may not be included in the following direct Chinese-Japanese lexicon.
 As a direct Chinese-Japanese bilingual lexicon, we used the Japanese-English-Chinese trilingual lexicon 5 (245,558 term pairs) in which Chinese translations were added to EDR 6 Japanese-English lexicon by JST. 7 We also this bilingual lexicon. Additionally, we used the in-house Chinese-Japanese parallel corpus, 8 which contains 85,982 sentence pairs related to domains of science, engineering, and medicine.

A part of the Wanfang Chinese-English lexicon contains Japanese trans-lations (48,251 term pairs) that were annotated manually by JST. This part consists of terms in the medical domain. We extracted development (1,000 term pairs) and test (1,000 term pairs) sets from this part for our experiments. The remaining part of the Wanfang Chinese-English lexicon was used as a training set for constructing the phrase table. Table II shows the numbers of translation pairs in these lexica and the average numbers of words per term. We lower-cased and tokenized all terms by the following analyzers: JUMAN 9 for Japanese, the MEMM-based POS tagger 10 for English, and cjma [Nakagawa and Uchimoto 2007] for Chinese. The Chinese terms in these lexica are written in Simplified Chinese. 9: 12  X  T. Tsunakawa et al. 3.2 Translation by Converting Hanzi into Kanji and Looking Up Bilingual Lexica Since Han characters sometimes share the same meaning between Chinese and Japanese, the simple conversion from hanzi to kanji may be effective in translating a Chinese term to a Japanese term. As baseline systems, we use the following methods:  X  Unicode: Converting hanzi to kanji by Unicode only.  X  Hanzi-kanji table: This strategy applies the hanzi-kanji table A S W ! S W  X  W S  X  g h (Traditional/Simplified Chinese Characters and
Shinjitai Table) presented in Section 2.3.6. Hanzi that do not appear in this table are converted by the Unicode strategy. strategy outputs the Japanese term  X   X  w j = argmax  X  pivot-term : For an input Chinese term, this strategy looks up their English translation terms by using the Chinese-English bilingual lexicon in the test set. We look up Japanese translation terms of the English terms by the
Japanese-English bilingual lexicon. If the source Chinese term reaches one or multiple Japanese translation terms in this procedure, this strategy out-puts them. If no Japanese terms are obtained, it uses the Unicode strategy. 3.3 Experimental Settings We applied word alignment and phrase extraction for the bilingual lexica, and built phrase tables. As sources of the phrase table, we used the following sets of bilingual lexica and a parallel corpus:  X  direct : Chinese-Japanese EDR lexicon  X  pivot : Wanfang Chinese-English lexicon and JST Japanese-English lexicon  X  corpus : In-house Chinese-Japanese parallel corpus  X  direct + pivot  X  direct + pivot + corpus For each set of phrase tables extracted from these lexica and corpus, we exam-ine contributions of hanzi-kanji similarity features. We mainly used the Web Japanese N-gram Version I [Kudo and Kazawa 2007] as the language model. It includes 1-to-7-grams extracted from 20 billion sentences on the Web. We used trigrams of this resource for calculating the language model scores.
We gave Chinese terms in the development/test set to our system imple-mented by the Moses toolkit [Koehn et al. 2007], and obtained 10 Japanese translations ranked by their probabilities. We evaluated the performance by using BLEU 11 [Papineni et al. 2001], NIST [Doddington 2002], and accu-racy measures. For calculating BLEU and NIST, we must assume that the word segmentations both in Chinese and Japanese were correct. In order to A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 13 avoid the effect of word segmentation errors, we also computed the letter-based BLEU score (denoted as BLEU*). Since the input and output are not sentences but rather are terms, we consider word-based BLEU scores from unigram to 4-gram.

The one best accuracy indicates the ratio in which top-ranked translation is correct over all the translations. The 10 best accuracy measures the ratio in which at least one of the 10 best translations is correct. Since the pivot-term baseline strategy cannot output ranked translation candidates, we as-sume that this strategy yields a correct translation only if one of candidates is identical to the reference translation. In other words, the performance of the pivot-term system is overestimated by this treatment. Also, we calculate MRR (mean reciprocal rank) defined in the following formula: where r i is the highest rank of the correct translations in the n -best list of the i -th term ( 1 r terms in the test set. 3.4 Results Table III shows the evaluation results on the test set measured by the accu-racy, MRR, BLEU*4, BLEU4, and NIST4. Table IV reports the evaluation results on the same test set measured by word-based BLEU scores. In the tables, BLEU*4, BLEU4, and NIST4 represent letter-based 4-gram BLEU, word-based 4-gram BLEU, and word-based 4-gram NIST scores, respectively. The accuracy of pivot-term shows the ratio that the generated outputs include the correct translation.
 9: 14  X  T. Tsunakawa et al.
 3.4.1 Results of the Baseline Systems. For the baseline methods, we calcu-late accuracy and letter-based BLEU* scores because they generate transla-tions without using the word segmentation information.

The Unicode method revealed that 6.6% of the terms in the test set could be translated with the identity of Han characters in Unicode. The result of the hanzi-kanji table strategy improved the accuracy to 11.1%. Introducing the hanzi-kanji similarity also had a positive effect in increasing the performance.
Examples of the translation results are shown in Table V. As in Example 1, when the Unicode strategy succeeded, other approaches generated similar results. The hanzi-kanji table outperformed the Unicode strategy, but some failure cases exist when the simple conversion method fails. In Example 2, even though the hanzi-kanji table could convert the hanzi s and  X  into  X  and  X  successfully, it did not know the semantic association between the hanzi  X  (muscle in Chinese) and K (muscle in Japanese). The hanzi-kanji similar-ity strategy successfully learned the relations of  X  and K from the bilingual lexicon, and chose K as the translation of  X  . This example reflected the ef-fectiveness of hanzi-kanji similarity for modeling semantic similarity of Han characters between the Chinese and Japanese languages. By nature, these baseline methods could generate Japanese translations in which the numbers of letters are the same as those in Chinese terms. Therefore, these methods fail when the numbers of letters in an input Chinese term and its reference Japanese term differ. In addition, these methods cannot handle letter reorder-ing during translation as shown in Example 4.

Translations by pivot English terms (pivot-term) achieved a much higher performance than the methods mentioned above. By leveraging the favor-able setting of the test set that the input terms can always have their Eng-lish translation in the Chinese-English lexicon, this method can map 31.4% of the input terms into Japanese terms via pivot English terms. Among the mapped Japanese translations, 39.2% of them were correct; that is, 12.3% of the input terms were translated correctly. Furthermore, this method obtained 5.3% of the correct translations by the Unicode method. In total, it achieved an accuracy of 17.6%. 3.4.2 Result of the Proposed Methods. The results of the direct approaches indicated that the direct approaches could hardly improve the performance against the hanzi-kanji converting strategies. According to these results, we found that incorporating the SMT framework into lexical translation by using A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 15 direct bilingual lexicon was not necessarily effective. In Example 3 of Table V, the direct method attempted to translate each of the Chinese words, J (half; semi-),  X  { (deadly; lethal), and  X   X  (sudden change; mutation) indepen-dently, and unfortunately selected inappropriate translation words J (half),  X  { (deadly; lethal), and % Y X  (change suddenly), respectively.
 9: 16  X  T. Tsunakawa et al.

Our pivot approach improved the accuracy and BLEU scores even when compared to the pivot-term. Although the phrase tables derived from direct and pivot methods could not change the translation results, the hanzi-kanji similarity was effective in improving the translation performance to 21.7% 1-best accuracy. The improvements of the BLEU/BLEU* scores were sta-tistically significant at the 95% confidence level with respect to the paired bootstrap resampling [Koehn 2004]. This method with the hanzi-kanji similar-ity achieved the highest unigram, bigram, and trigram BLEU scores, shown in Table IV. These shorter n -gram BLEU scores demonstrate that this method is also effective to translate terms that consists of fewer words. While one best accuracy and MRR were improved by the hanzi-kanji similarity, 10 best accuracy did not change. This indicates that the hanzi-kanji similarity has an effect of reranking of n -best results. Comparing the BLEU/BLEU* scores with the NIST score, we could not observe the improvement of the NIST score by introducing the hanzi-kanji similarity. Since the NIST score emphasizes content words, the translation results might be underestimated because the hanzi-kanji similarity handles character-level similarities. Unfortunately, in-corporating a Japanese-Chinese parallel corpus could not improve the perfor-mance, except for the 10 best accuracy and NIST score. One possible reason is that the phrase table generated from the parallel corpus might include in-correct translation pairs caused by the word alignment. In general, aligning words from parallel sentences is more difficult than aligning from the bilingual lexicon.

Example 6 of Table V shows the improvement of the translation result by introducing hanzi-kanji similarity. We consider the  X  direct + pivot + Hanzi-kanji sim. X  method selected a Japanese word  X   X  ^  X  because the hanzi-kanji similar-ity prefer a term that includes identical or similar Han characters.
We examined more detailed analysis between  X  direct + pivot  X  (denoted as [w/o]) and  X  direct + pivot + Hanzi-kanji sim. X  (denoted as [with]) for confirming the effectiveness of the hanzi-kanji similarity. Table VI shows the statistics of the translation results classified by the following criteria: whether the trans-lation result is correct, and whether it is identical to the input Chinese term on the Unicode. Among 66 examples whose Japanese and Chinese terms are A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 17 identical on the Unicode, 12 the hanzi-kanji similarity successfully gained three correct translations without any loss.

Table VII is examples when only [with] X  X  translation result was correct and identical to the result of the Unicode method. We consider that [w/o] selected more frequent word as the translation. In the second example, while both of  X  T  X  and  X  f  X  can be used for representing societies and associations,  X  f  X  must be used for academic societies only.

Tables VIII, IX, and X show examples when the translation results of nei-ther [w/o] nor [w] match the input Chinese term. In Table VIII, the first ex-ample indicates that the hanzi-kanji similarity helped to select the translation words including corresponding Han characters:  X   X  for  X   X  , X   X   X   X  for  X  s , X  and  X   X   X  for  X   X  . X  In the second example, although both of Japanese words  X   X   X  and  X   X  X  X  X   X  (system) are appropriate as the translation of a Chinese word  X   X  fl , X  the transliteration  X   X  X  X  X   X  should not be used as a portion of this term.

Table IX shows examples that [with] output the different term from the reference Japanese term, while [w/o] X  X  translation was correct. The first and 9: 18  X  T. Tsunakawa et al.
 second examples show the cases that different kanji are correct as Japanese translations. Although  X  .  X  and  X   X  B  X  in Japanese actually correspond to  X   X   X  and  X   X   X   X  in Chinese, respectively, the correct translations use dif-ferent words for the Chinese words. The third example is more problematic. The Chinese word  X  - X   X  (and also the English word  X  X enter X ) can represent both of the positional/conceptual center and the facilities. The Japanese word  X  - X   X  cannot mean the facilities, which are represented by the transliteration  X   X  X  X   X  . X  Comparing this with the second example of Table VIII, we must introduce another feature for judging whether the translation must be selected for handling this problem.

Table X shows examples that both outputs of [w/o] and [with] were not correct. In the first example, although [with] successfully generated the cor-responding kanji  X   X  for  X  , X  it could not translate a Chinese word  X   X   X  into a Japanese word  X   X   X  . X  In the second example, both outputs of [w/o] and [with] means  X  X achytene, X  [with] selected the output including kanji because  X  *  X  (thick) and  X   X   X  (string; thread) are semantically related to  X   X   X  (thick; coarse) and  X   X  , X  (string; line) respectively. The output of [with] in the third example has a similar meaning to the reference Japanese term, although the reference uses the transliteration  X   X  X  X  X   X  (block). These errors might be corrected by augmenting the resources and introducing more features includ-ing a feature with the transliteration. 4. RELATED WORK Numerous researchers have explored the use of pivot languages (third lan-guages) as intermediary languages to build bilingual lexica [Tanaka and Umemura 1994; Bond et al. 2001; Shirai and Yamamoto 2001; Paik et al. 2001; Schafer and Yarowsky 2002; Zhang et al. 2005; Goh et al. 2005]. The basic idea bilingual lexicon can be created between languages L j and L c by bridging lex-bilingual lexicon between L j and L c , even if no bilingual lexicon exists between these languages.

The previous studies have focused mainly on solving the problem of how to reduce inappropriate term pairs in L j and L c which connected accidentally by polysemic terms in L e . For instance, a Japanese term  X  K , X  dote : embank-ment, levee, may be associated with a Chinese term  X   X  L , X  y  X  X ngh  X ang : banking institution, finance institution, using the pivot term  X  X ank X  in English. Tanaka A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 19 et al. [1994] and Shirai et al. [2001] proposed the use of the structure of bilin-gual dictionaries to select correct translation equivalents. As a clue of the selection, they count the terms in the pivot language that can bridge both of the source and target language terms. For example, a Chinese term  X   X  L  X  and the corresponding Japanese term  X   X  L , X  gink  X o , have a number of com-mon English translations such as bank, banking house, banking corporation, and financial bank;  X   X  L  X  and  X  K  X  share only a English term  X  X ank. X  This indicates that  X   X  L  X  is more appropriate for the translation of  X   X  L  X  than  X  K . X  Bond et al. [2001] utilized semantic classes to rank translation equiv-alents; term pairs with compatible semantic classes are preferred over those with dissimilar classes. Paik et al. [2001] utilized multiple pivot languages to improve the accuracy of dictionary construction; their study used English and Chinese as pivot languages for building a Korean-Japanese lexicon. Schafer et al. [2002] presented a method for inducing translation lexica between two distant languages through a pivot language, using cross-language context sim-ilarity, weighted Levenshtein distance, relative frequency, and the burstiness similarity measures.
 Some studies on Chinese and Japanese [Tanaka and Umemura 1994 and Zhang et al. 2005] employed the similarity of Han characters as an additional clue. In the example of  X  X ank X ,  X   X  L  X  is still the appropriate translation for  X   X  L  X  because they share a Han character  X  L . X  We also introduced the simi-larity between Han characters as a feature of the log-linear model.
On the other hand, we focus mainly on another issue that arises from merging terms in the pivot language L e from different bilingual lexica. Since two bilingual lexica L c  X  L e , and L e  X  L j are constructed independently, we cannot assume that the two lexica always use the identical term to describe a single entity. For example, if the Chinese-English lexicon has an entry ( X   X   X  ( ji  X eh  X egu X an ), X   X  X oint pipe X ) and the Japanese-English one has ( X   X   X  ( setsug  X okan ), X   X  X onjugating tube X ), it is impossible to associate the Chinese and Japanese terms. In addition, a bilingual lexicon developed for technical terms is likely to contain a number of pivot terms that another bilin-gual lexicon does not include. For example, even if a Japanese-English lexicon is large enough to include a technical term,  X   X   X   X   X   X  X  X  X  ( sekitan-henkan-purosesu ) X  (coal conversion process), we can obtain its Chinese translation,  X  d l  X  ( m  X eizhu  X anhu`a-gu`och  X eng ) X  only when the Chinese-English lexicon includes the identical English term. To the best of our knowledge, our study is the first attempt to tackle with these problems, by using not only pivot terms but also phrase tables extracted from the bilingual lexica. 5. CONCLUSION This article proposed a novel method for lexical machine translation by using a phrase-based SMT system from L c to L j by merging the lexica into a phrase translation table L c  X  L j . Our experimental results showed that this pivotal approach, in addition to combining the hanzi-kanji similarity, significantly 9: 20  X  T. Tsunakawa et al. improved the translation performance in comparison to the translation by hanzi-kanji conversion, pivot English terms, and L c  X  L j lexicon.
The future direction would be to incorporate characteristics of compound nouns and other resources, such as other pivot languages, into the SMT system for improving the precision and the coverage of the obtained lexicon. Further, we are also planning on evaluating a machine translation system that integrates our model.
 A Chinese-Japanese Lexical Machine Translation through a Pivot Language  X  9: 21
