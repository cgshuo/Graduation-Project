 This paper presents a novel wrapper-based feature selectio n method for Support Vector Regression (SVR) using its prob-abilistic predictions. The method computes the importance of a feature by aggregating the difference, over the feature space, of the conditional density functions of the SVR pre-diction with and without the feature. As the exact com-putation of this importance measure is expensive, two ap-proximations are proposed. The effectiveness of the measure using these approximations, in comparison to several other existing feature selection methods for SVR, is evaluated on both artificial and real-world problems. The result of the experiment shows that the proposed method generally per-forms better, and at least as well as the existing methods, with notable advantage when the data set is sparse. H.2.8 [ Database Management ]: Database Applications-data mining; I.2.6 [ Artificial Intelligence ]: Learning-in-duction Algorithms Support vector regression, feature selection, feature ran king, probabilistic predictions, random permutation.
Feature selection plays an important role in pattern recog-nition, data mining, information retrieval and has been the subject of intense research in the past decade, see, for exam -ple, [9, 10] and the references therein. Generally, methods for feature selection can be classified into two categories: filter and wrapper methods [10]. Wrapper methods rely heavily on the specific structure of the underlying learning algorithm while filter methods are independent of it. Due to its more involved nature, wrapper methods usually yield better performance than filter methods but have a heavier computational load.

With a few exceptions [11, 22, 17, 23], most feature se-lection methods are developed for use in classification prob -lems. One possible reason for this is the ease of formulation of criteria for feature selection by exploiting the discrim -inability of classes. While some methods can be extended from classification to regression applications [11, 20], ot h-ers may not. Straightforward adaptation by discretizing (o r binning) the target variable into several classes is not alw ays desirable as substantial loss of important ordinal informa tion may result.

This paper proposes a new wrapper-based feature selec-tion method for SVR, motivated by our earlier work on clas-sification problem using Support Vector Machine (SVM) [21] and Multi-Layer Perceptrons (MLP) neural networks [25]. Under the probabilistic framework, the output of a standard SVR can be interpreted as p ( y | x ), the conditional density function of target y  X  R given input x  X  R d for a given data set. The proposed method relies on the sensitivity of p ( y | x ) with respect to a given feature as a measure of importance of this feature. More exactly, the importance score of a featur e is the aggregation, over the feature space, of the difference of p ( y | x ) with and without the feature. The exact computa-tions of proposed method is expensive, two approximations are proposed. Each of the two approximations, embedded in an overall feature selection scheme, is tested on artifici al and real-world data sets and compared with several other existing feature selection methods. The experimental resu lt shows that the proposed method performs generally better, if not at least as well, than other methods in almost all ex-periments.

The following notations are used: data set D = { x i , y i I
D is assumed given with x i  X  R d being the i th sample hav-ing d features, y i  X  R is the corresponding output and I the set of indices of samples of D with |I D | being its cardi-j x been removed from x i . Equivalently, x  X  j,i = Z d j x i is the ( d  X  1)  X  d matrix obtained by removing the j th row of the d  X  d identity matrix. When v is a vector of appropriate dimension, v  X  is its transpose.

This paper is organized as follow: Section 2 reviews the formulation of probabilistic SVR. Details of the proposed feature ranking criterion and the two approximations are presented in Section 3. Section 4 shows the overall fea-ture selection scheme. Result of numerical experiment of the proposed method, benchmark against other methods, are reported in Section 5. Section 6 concludes the paper.
Standard SVR [24] obtains the regressor function, f ( x ) :=  X   X  ( x ) + b , for a data set D by solving the following Primal Problem (PP) over  X  , b,  X ,  X   X  : where x is mapped into a high dimensional Hilbert space, H , by the function  X  : R d  X  H . Here,  X   X  H , b  X  R are variables that define f ( x ) and  X  i ,  X   X  i are the non-negative slack variables needed for enforcing constraints (2) and (3 ). The regularization parameter, C &gt; 0, tradeoffs the size of  X  and the amount of slacks while parameter,  X  &gt; 0, specifies the allowable deviation of the f ( x i ) from y i . In practice, PP is often solved through its Dual Problem (DP): max where  X  i and  X   X  i are the respective Lagrange multipliers of (2) and (3), regressor function is known to be
Expression (7) provides an estimate, f ( x ), for output y for any x but provides no information on the confidence level of this estimate. Recognizing this shortcoming, seve ral attempts to incorporate probabilistic values to SVR out-put has been reported in the literature. Following the ap-proach of Bayesian framework for neural network [16], Law and Kwok [14] proposed a Bayesian support vector regres-sion (BSVR) formulation incorporating probabilistic info r-mation. Gao et al. [7] improved upon BSVR by deriving the evidence and error bar approximation. Chu et al.[4] proposed the use of a unified loss function over the standard  X  -insensitive loss function and provided better accuracy in evidence evaluation and inferences.

Another approach to obtaining probabilistic output of the regressor is that used in the Neural Networks framework [2]. It assumes that the output of the regressor is corrupted with noise in the form of where  X  belongs to the Gaussian distribution. Lin and Weng [15] also considered the case where  X  belongs to the Laplace distribution. Equivalently, this means that density funct ions of y for a given x are for the Laplace and Gaussian cases respectively. Like the Neural Network approach, the intention is to obtain esti-mates of  X  of (9) and (10) from D . If p ( x, y ) is the joint density function of x and y , the likelihood function, as a function of  X  , of observing D is given by under the assumption of independent and identically dis-tributed samples. By further assuming that p ( x ) is indepen-dent of  X  , the expressions of  X  can be obtained by maximiz-ing the logarithm function of L (  X  ) [2, 5]. These expressions are for the Laplace and Gaussian distributions respectively. I t has been shown [15] that this approach is competitive in terms of performance to the BSVR methods. In view of this, the proposed feature selection method uses this approach and relies on (9) and (10) for its computation.
The proposed method of feature importance relies on mea-sures of difference between two density functions. Our choic e of this measure is the Kullback-Leibler divergence (KL di-vergence), D KL ( ; ). Given two distributions p ( y ) and q ( y ), respect to its arguments. The last property is a result of treating p ( y ) as the reference distribution. In cases where symmetry of the arguments is important or that a reference distribution does not exist, modifications to D KL ( ; ) can be easily achieved.

In the case of SVR, the density function p ( y | x ) at any x is assumed to be (9) or (10) with f ( ) being the solution obtained from (7). Given x  X  R d , x  X  j  X  R d  X  1 can be ob-tained by removing the j th feature from x , or, equivalently, x  X  j = Z d j x . With this, the difference of the two density func-tions p ( y | x ) and p ( y | x  X  j ) at a particular x (and hence x measure is an aggregation of D KL ( p ( y | x ); p ( y | x x in the x space. More exactly, the measure is The motivation for defining S D is simple: the greater the D
KL divergence between p ( y | x ) and p ( y | x  X  j ) over the x space, the greater the importance of the j th feature. For convenience, (14) is termed SD measure, short for Sensitiv-ity of Density Functions.

In (14), p ( y | x ) is either (9) or (10) with f ( ) of (7) trained on D . Similarly, p ( y | x  X  j ) is obtained from f ( ) trained on the derived dataset D  X  j := { ( x  X  j,i , y i ) | x  X  j,i quire the training of SVR d times, each with a different D Clearly, this is a computationally expensive process. Fol-lowing our work in SVM [21], a random permutation (RP) process is used to approximate p ( y | x  X  j ) such that the re-training of SVR is avoided. The basic idea of RP process is to randomly permute the values of the j th feature in D while keeping the values of all other features unchanged. Specifi-cally, let {  X  1 , ,  X  n  X  1 } where n = |D| be a set of uniformly distributed random numbers in the interval (0 , 1) and  X   X   X  be the largest integer that is less than  X  . Then, for each i starting from 1 to n  X  1, compute  X  =  X  n  X   X  i  X  + 1 and swap the values of x j i and x j  X  .

Let x ( j )  X  R d be the sample derived from x after the RP process on the j th feature and let p ( y | x ( j ) ) be the condi-tional density function of y given x ( j ) . When the data set is sufficiently rich, this RP process of feature j would destroy any correlation of feature j with all other features. With this implicit assumption, the following theorem, the proof of which is given in [21], is known.

Theorem 1.
In the case when there are very few data points, the equal-ity in (15) becomes an approximation. Our experiment shows that even with sparse data set, the approach based on (15) is effective.

The utility of Theorem 1 is clear. The density function p ( y | x  X  j ) of (14) can be replaced by p ( y | x ( j ) ment brings about significant computational advantage sinc e p ( y | x ( j ) ) can be evaluated from (9) or (10) using f ( x tained from the SVR training using D . This avoids the ex-pensive d -time retraining of SVR on D  X  j . Correspondingly, (14) can be equivalently stated as: Figure 1 shows a plot of p ( y i | x i ) and p ( y i | x ( j ) ,i of x i for a typical SVR problem with d = 1. To compute the S
D , further approximation of (16) is needed, resulting in sian functions, explicit expressions of  X  S D ( j ) exist. From of Laplace function. The KL divergence of two Laplace dis-Figure 1: Demonstration of the proposed feature ranking criterion with d = 1 . Dots indicate locations of y i tributions can be simply derived as for a given x where  X  L is that given by (11) and  X  L ( j ) obtained from (11) by replacing f ( x ) with f ( x ( j ) ). Using (18) in (17) and removing associated constants yields  X 
S L D ( j ) =
Following the same development for the case when p ( y | x ) is Gaussian, the expressions are and  X  S
D ( j ) =
In summary,  X  S D ( j ) can be computed for all j = 1 , , d , after a one-time training of SVR, one-time evaluation of  X  (or  X  G ), d -time RP process, d -time evaluation of  X  L ( j )  X  ( j ) ) and d -time evaluation of D KL .
Remark 1. The kernel matrix is different for each of the computations. Such computations can be kept low using up-ples before and after the RP process is applied to feature j . It is easy to show that K ( x ( j ) ,r , x ( j ) ,q ) = K ( x x K ( x r , x q )  X  exp[  X  ( x j r  X  x j q ) 2  X   X  ( x j ( j ) ,r parameter  X  for Gaussian kernel.

Remark 2. It is possible to develop D KL distance mea-sure that is symmetrical with respect to its arguments. One lowing the preceding discussion, the use of such a measure will lead to a equivalently-defined SD metric. Details of suc h a measure are not included here as numerical experiment shows no significant difference in its performance from that obtained from using the one-sided D KL .
The proposed  X  S L D and  X  S G D can be used in two ways. The most obvious is when it is used once to yield a ranking list of all features based on a one time training of SVR on D . It can also be used for more extensive ranking schemes like the recursive feature elimination (RFE) scheme. Basically , the RFE approach removes the least important feature, as ings of the SVR. Accordingly, the overall scheme with re-SD-L-RFE (SD-G-RFE) and its main steps are listed in Al-gorithm SD-L-RFE. Inputs to Algorithm SD-L-RFE are D and  X  = { 1 , , d } , while the output is a ranked list of fea-tures in the form of an index set  X   X  = {  X   X  1 , ,  X   X   X  j  X   X  for each j = 1 , , d in decreasing order of impor-tance.

Algorithm SD-L-RFE : Main steps of the feature se-lection method SD-L-RFE.
 Input : D ,  X 
Output :  X   X  := {  X   X  1 , ,  X   X  d } while |  X  | &gt; 0 do end
As shown above, the while loop at step 1 is invoked d times. Each time except for the last, SVR is trained with a reduced data set D (step 4) and generates a ranked list J of all features in D (step 6) based on the scores of  X  S (step 5). The least important feature (the last element of J ) is removed from  X  and stored in the ranked list  X   X  . The corresponding feature is also removed from the data set D (step 8). The while loop is then invoked on the reduced sets of  X  and D again. This process continues, each time removing the least important feature from  X  and storing in the last available position of  X   X  , until  X  has only one feature, which becomes the most important feature naturally.
Corresponding Algorithm SD-G-RFE involving metric  X  S G D can be obtained by replacing  X  S L D by  X  S G D in steps 5 and 6.
To improve the efficiency of Algorithm SD-L-RFE for large dimensional data set, more than one feature can be removed at one time through a slight modification to step 7 and 8 in the algorithm. Like other wrapper methods, the current al-gorithm does not involve the re-tuning hyper-parameters of SVR at step 4 for computational consideration. This section presents results of numerical experiment of SD-L-RFE and SD-G-RFE on artificial and real-world data sets. They are also compared against three other exist-ing methods for SVR: the widely-used correlation coefficient method (Corr) [9, 17], a recently proposed dependence maxi-mization method (HSIC) [22] and the well-known SVM-RFE method ( X  k  X  k 2 ) [11, 8]. The first two of them are filter methods while the last is wrapper. All methods, except for Corr since it is not meaningful, are implemented using the RFE scheme described by Algorithm SD-L-RFE for ranking the features. These benchmark methods are hereafter re-ferred to as Corr, HSIC-RFE and  X  k  X  k 2 -RFE, respectively. There are other regression feature selection methods [23, 1 8, 13] but they are not included here for comparison because they work for the case of linear kernel.

For each data set, the result of the experiment is reported over 30 realizations, which are created by random (stratifie d) sampling of the given set D into subsets D trn and D tst for 30 times. As usual, D trn is used for SVR training, hyper-parameters tuning and feature ranking while D tst is used for unbiased evaluation of the feature selection performan ce. For each realization, D trn is normalized to zero mean and unit standard deviation and its normalization parameters are then used to normalize D tst . The kernel function used for all problems is K ( x i , x j ) = exp(  X   X  k x i  X  x j k 2 kernel parameter. In each experiment, all hyper-parameter s ( C,  X ,  X  ) are chosen by a 5-fold cross-validation on the first five realizations of D trn , and the hyper-parameters corre-sponding to the lowest average cross-validation error amon g five realizations is chosen. The grid over the ( C,  X ,  X  ) is [2
The well-known regression performance measures, mean squared error (MSE), is used to evaluate the performance. It is given by where y i and  X  y i are the true and predicted target values respectively .

Using this performance measures, the average MSE among 30 realizations against the number of top-ranked features f or each feature selection method is plotted. This is followed b y result of statistical paired t -test using MSE for all problems. Specifically, paired t -test between SD-L-RFE and each of the other methods is conducted using different number of top ranked features. Herein, the null hypothesis is that the mean MSE of the two tested methods are same against the alternate hypothesis that they are not. The chance that this null hypothesis is true is measured by the returned p -value and the significance level is set at 0 . 05 for all experiments. The symbols  X + X  and  X   X   X  are used to indicate the win or loss situation of SD-L-RFE over the other tested method.
In all experiments, the numerical algorithm for training of SVR is implemented by the LIBSVM package [3], where sequential minimal optimization method is used to solve the dual problem (5).
In this subsection, an artificial regression problems is use d to evaluate the performance of every feature selection meth od. This problem is used in [6] and has 10 variables, x 1 , , x uniformly distributed over the range of [0,1]. The target variable depends only on the first five variables and is given by y = 0 . 1 exp(4 x 1 )+ where  X   X  N (0 , 0 . 1) is a Gaussian random noise. This has 2000 samples and 30 realizations are generated from the 2000 samples by randomly splitting it into D trn and D tst with |D trn | : |D tst | =1:9. To investigate the effect of sparseness of the training set, decreasing sizes of |D trn | are also used while |D tst | is maintained at 1800. More exactly, four settings of decreasing |D trn | at 200, 100, 70 and 50 are considered in this problem.

Table 1 presents the number of realizations (out of 30 re-the first five most important features by the various meth-ods for the four settings of |D trn | . The best performance in each setting is highlighted in bold. From this table, the advantage of the proposed methods over benchmark meth-ods is evident in all settings. Even in the easiest setting of |D trn | = 200, none of benchmark methods are able to pro-duce the correct ranked list for 30 realizations. As the size of |D trn | decreases, the performance of proposed methods degrades much less than that of benchmark methods. Table 1: The number of realizations that feature 1 , 2 , 3 , 4 , 5 are successfully ranked in the top five po-sitions over 30 realizations for the artificial problem. The best performance for each |D trn | is highlighted in bold.

Figure 2 shows the plots of the MSE against the number top-ranked features used in SVR. Again, it shows the ad-vantage of the proposed methods over the benchmark meth-ods. The advantage is more significant when |D trn | becomes smaller. This performance difference is also statistically sig-nificant, as shown in the paired t -tests result of Table 2. Table 2 also shows that SD-L-RFE is significantly better than all benchmark methods for all sizes of |D trn | while the Table 3: Description of real-world data sets. |D trn | , |D tst | , d , C ,  X  and  X  refer to the number of train-ing samples, number of test samples, number of fea-tures, and SVR hyper-parameters C ,  X  ,  X  respec-tively.
 Dataset mpg 353 39 7 2 6 2  X  4 2 abalone 1254 2923 8 2 6 2  X  5 2 cpusmall 820 7372 12 2 6 2  X  5 2 housing 456 50 13 2 6 2  X  4 2 pyrim 67 7 27 2 0 2  X  6 2  X  5 triazines 168 18 60 2  X  1 2  X  6 2  X  3 differences between SD-L-RFE and SD-G-RFE are not sig-nificant.
Six real-world data sets from the Statlib 1 , UCI repository [1] and Delve archive 2 are used for evaluation purposes. De-scription of these data sets and the parameters used in the experiments are given in Table 3. Figures 3-8 show MSE against the number of top-ranked features for mpg, abalone, cpusmall, housing, pyrim and triazines respectively. Stat is-tical paired t -test is also conducted on all real-world problem. In this paper, we only show the t -test results of problem mpg in Table 4 and abalone in Table 5 under the consideration of limited space.

For the mpg problem, Figure 3 shows the MSE against the number of top-ranked features in SVR for the various methods. It can be observed that given the same number of features used, the proposed methods consistently perform a t least as well, if not better than benchmark methods. Specif-ically, both proposed methods perform significantly better than two filter methods HSIC-RFE and Corr at different number of features, while they perform comparably with  X  k  X  k 2 -RFE. This is confirmed by the paired t -tests X  result in Table 4.

For the other real-world problems (abalone, cpusmall, hous -ing, pyrim and triazines), the experimental result shows si m-ilar patterns to that of mpg, as shown in Figures 4 to 8 respectively. Generally, the t -test result shows that the pro-posed methods almost perform better than the benchmark methods in all real-world problems.
In summary, the effectiveness of the proposed feature se-lection method is demonstrated for both artificial and real-world problems. In the artificial problem, the proposed method can consistently yield better performance than all three benchmark methods, and the advantage is more evi-dent when |D trn | is small. This is confirmed by statistical paired t -test results. Furthermore, when the training data become sparse, the performances of proposed methods de-grade much less than the benchmark methods. In real-world problems, it can be observed from all plots and t -test results that the proposed methods consistently perform at least as well, if not better than benchmark methods for all problems. http://lib.stat.cmu.edu/datasets/ http://www.cs.toronto.edu/  X  delve/data/datasets.html settings.
 highlighted in bold.
 Figure 3: Average MSE against top-ranked features over 30 realizations for mpg
The better performance of proposed method over Corr is expected since this common filter method assumes that all features are independent. The other filter method, HSIC-RFE, appears to be quite effective in dealing with data hav-ing interacting features, and generally shows nearly compa -rable performance with the wrapper method  X  k  X  k 2 -RFE. However, it is not as effective as the proposed methods from the results on the artificial problem, especially when the training data is sparse, and on real-world data sets of mpg, abalone and cputime. The better performance of the pro-posed methods over  X  k  X  k 2 -RFE is interesting and deserves more attentions, since both of them are wrapper-based fea-ture selection methods for SVR. These two wrapper methods use the same RFE scheme but differ in their ranking crite-ria. The proposed method uses the  X  X ggregat X  sensitivity of SVR probabilistic predictions with respect to a feature over the feature space as the feature ranking criterion, whi le  X  k  X  k 2 -RFE uses the sensitivity of the cost function of SVR with respect to a feature. Another difference is that  X  k  X  k RFE has an additional assumption that the SVR solution remains unchanged when one feature is removed while it is unnecessary for the proposed method. These two differences are likely to contribute to the better performance of the pro -posed methods over  X  k  X  k 2 -RFE.

Another advantage of the proposed method is the mod-est computational complexity. As mentioned in Section 3, the evaluation of scores for d features includes a one-time training of SVR about O ( n 2 )  X  O ( n 3 ) [12, 19], one-time evaluation of  X  L (or  X  G ) about O ( mn ), m is the number of support vectors which is often much less than n , d -time RP process about O ( dn ), d -time evaluation of  X  L ( j ) about O ( dmn ), and d -time evaluation of D KL about O ( dn ). Hence, after one-time training SVR, the proposed criterion scales linearly with respect to d and n .
This paper presents a new wrapper-based feature selection method for SVR. This method measures the importance of a feature by the aggregation, over the feature space, of the sensitivity of SVR probabilistic prediction with and with-out the feature. Two approximations of the criterion with random permutation process are proposed. The numeri-cal experiment on both artificial and real-world problems Figure 4: Average MSE against top-ranked features over 30 realizations for abalone Figure 5: Average MSE against top-ranked features over 30 realizations for cpusmall Figure 6: Average MSE against top-ranked features over 30 realizations for housing top ranked features.
 of top ranked features.
 Figure 7: Average MSE against top-ranked features over 30 realizations for pyrim Figure 8: Average MSE against top-ranked features over 30 realizations for triazines suggests that the proposed method generally performs as least as well, if not better than three benchmark methods, Corr, HSIC-RFE and  X  k  X  k 2 -RFE. The advantage of the proposed methods is more significant when the training data is sparse, or has a low samples-to-features ratio. As a wrap-per method, the computational cost of proposed methods is moderate. [1] A.Asuncion and D.J.Newman. UCI machine learning [2] C. M. Bishop. Neural Networks for Pattern [3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [4] W. Chu, S. S. Keerthi, and C. J. Ong. Bayesian [5] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern [6] J. H. Friedman. Multivariate adaptive regression [7] J. B. Gao, S. R. Gunn, C. J. Harris, and M. Brown. A [8] O. Gualdr  X on, J. Brezmes, E. Llobet, A. Amari, [9] I. Guyon and A. Elisseeff. An introduction to variable [10] I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, [11] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [12] T. Joachims. Making large-Scale SVM Learning [13] Y. Kim and J. Kim. Gradient lasso for feature [14] M. H. Law and J. T. Kwok. Bayesian support vector [15] C. J. Lin and R. C. Weng. Simple probabilistic [16] D. MacKay. The evidence framework applied to [17] A. Navot, L. Shpigelman, N. Tishby, and E. Vaadia. [18] A. Y. Ng. Feature selection, l1 vs. l2 regularization, [19] J. C. Platt. Using sparseness and analytic QP to speed [20] A. Rakotomamonjy. Variable selection using [21] K. Q. Shen, C. J. Ong, X. P. Li, and E. P.
 [22] L. Song, A. Smola, A. Gretton, J. Bedo, and [23] R. Tibshirani. Regression shrinkage and selection via [24] V. N. Vapnik. Statistical Learning Theory .
 [25] J. B. Yang, K. Q. Shen, C. J. Ong, and X. P. Li.
