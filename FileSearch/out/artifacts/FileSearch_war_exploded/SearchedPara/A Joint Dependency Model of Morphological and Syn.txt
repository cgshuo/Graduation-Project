 When translating between two languages that dif-fer in their degree of morphological synthesis, syntactic structures in one language may be re-alized as morphological structures in the other. Machine Translation models that treat words as atomic units have poor learning capabilities for such translation units, and morphological segmen-tations are commonly used (Koehn and Knight, 2003). Like words in a sentence, the morphemes of a word have a hierarchical structure that is rel-evant in translation. For instance, compounds in Germanic languages are head-final, and the head is the segment that determines agreement within the noun phrase, and is relevant for selectional prefer-ences of verbs. 1. sie erheben eine Hand|gep X ck|geb X hr. Table 1: Surface realizations of particle verb weggehen  X  X alk away X .
In example 1, agreement in case, number and gender is enforced between eine  X  X  X  and Geb X hr  X  X ee X , and selectional preference between erheben  X  X harge X  and Geb X hr  X  X ee X . A flat representation, as is common in phrase-based SMT, does not en-code these relationships, but a dependency repre-sentation does so through dependency links.
In this paper, we investigate a dependency rep-resentation of morphologically segmented words for SMT. Our representation encodes syntactic and morphological structure jointly, allowing a single model to learn the translation of both. Specifi-cally, we work with a string-to-tree model with GHKM-style rules (Galley et al., 2006), and a relational dependency language model (Sennrich, 2015). We focus on the representation of German syntax and morphology in an English-to-German system, and two morphologically complex word classes in German that are challenging for transla-tion, compounds and particle verbs.

German makes heavy use of compounding, and compounds such as Abwasserbehandlungsanlage  X  X aste water treatment plant X  are translated into complex noun phrases in other languages, such as French station d X  X  X uration des eaux r X siduaires .
German particle verbs are difficult to model be-cause their surface realization differs depending on the finiteness of the verb and the type of clause. Verb particles are separated from the finite verb in main clauses, but prefixed to the verb in subordi-nated clauses, or when the verb is non-finite. The infinitive marker zu  X  X o X , which is normally a pre-modifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. The main focus of research on compound split-ting has been on the splitting algorithm (Popovic et al., 2006; Nie X en and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of com-pounds. For splitting, we use an approach simi-lar to (Fritzinger and Fraser, 2010), with segmen-tation candidates identified by a finite-state mor-phology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003).
German compounds are head-final, and pre-modifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally:  X (city part) project) X ) and Stadt(teilprojekt)  X  X ity sub-project X . We opt also split linking elements, and represent them as a postmodifier of each non-final segment, includ-ing the empty string (" "). We use the same repre-sentation for noun compounds and adjective com-pounds.
 compound representation is shown in Figure 1. Importantly, the head of the compound is also the parent of the determiners and attributes in the noun phrase, which makes a bigram depen-dency language model sufficient to enforce agree-ment. Since we model morphosyntactic agree-ment within the main translation step, and not in a separate step as in (Fraser et al., 2012), we deem it useful that inflection is marked at the head of the compound. Consequently, we do not split off inflectional or derivational morphemes.

For German particle verbs, we define a common representation that abstracts away from the vari-ous surface realizations (see Table 1). Separated Figure 1: Original and proposed representation of German compound. Figure 2: Original and proposed representation of German particle verb with infixed zu -marker. verb particles are reordered to be the closest pre-modifier of the verb. Prefixed particles and the zu -infix are identified by the finite-state-morphology, and split from the verb so that the particle is the closest, the zu marker the next-closest pre-modifier of the verb, as shown in Figure 2. Agree-ment, selectional preferences, and other phenom-ena involve the verb and its dependents, and the proposed representation retains these dependency links, but reduces data sparsity from affixation and avoids discontinuity of the verb and its particle. We follow Williams et al. (2014) and map de-pendency trees into a constituency representation, which allows for the extraction of GHKM-style translation rules (Galley et al., 2006). This con-version is lossless, and we can still apply a de-pendency language model (RDLM). Figure 3 (a) shows the constituency representation of the ex-ample in Figure 1.

Our model should not only be able to produce new words productively, but also to memorize words it has observed during training. Looking at the compound Handgep X ckgeb X hr in Figure 3 (a), we can see that it does not form a constituent, and cannot be extracted with GHKM extraction heuris-tics. To address this, we binarize the trees in our training data (Wang et al., 2007).

A complicating factor is that the binarization should not impair the RDLM. During decoding, we map the internal tree structure of each hypoth-esis back to the unbinarized form, which is then scored by the RDLM. Virtual nodes introduced by the binarization must also be scorable by RDLM if they form the root of a translation hypothesis. A simple right or left binarization would produce vir-tual nodes without head and without meaningful dependency representation. We ensure that each virtual node dominates the head of the full con-cally, we perform right binarization of the head and all pre-modifiers, then left binarization of all post-modifiers. This head-binarized representa-
Head binarization ensures that even hypotheses whose root is a virtual node can be scored by the RDLM. This score is only relevant for pruning, and discarded when the full constituent is scored. Still, these hypotheses require special treatment in the RDLM to mitigate search errors. The virtual node labels (such as OBJA) are unknown symbols to the RDLM, and we simply replace them with the original label (OBJA). The RDLM uses sibling context, and this is normally padded with special start and stop symbols, analogous to BOS/EOS symbols in n -gram models. These start and stop symbols let the RDLM compute the probability that a node is the first or last child of its ances-tor node. However, computing these probabilities for virtual nodes would unfairly bias the search, since the first/last child of a virtual node is not nec-essarily the first/last child of the full constituent. We adapt the representation of virtual nodes in Figure 3: Unbinarized (a) and head-binarized (b) constituency representation of Figure 1.
 RDLM to take this into account. We distinguish between virtual nodes based on whether their span is a string prefix, suffix, or infix of the full con-stituent. For prefixes and infixes, we do not add a stop symbol at the end, and use null symbols, which denote unavailable context, for padding to the right. For suffixes and infixes, we do the same at the start. For SMT, all German training and development data is converted into the representation described in sections 2 X 3. To restore the original represen-tation, we start from the tree output of the string-to-tree decoder. Merging compounds is trivial: all segments and linking elements can be identified by the tree structure, and are concatenated.

For verbs that dominate a verb particle, the orig-inal order is restored through three rules: 1. non-finite verbs are concatenated with the 2. finite verbs that head a subordinated clause 3. finite verbs that head a main clause have the Previous work on particle verb translation into German proposed to predict the position of parti-cles with an n -gram language model (Nie X en and Ney, 2001). Our rules have the advantage that they are informed by the syntax of the sentence and consider the finiteness of the verb.

Our rules only produce projective trees. Verb particles may also appear in positions that violate projectivity, and we leave it to future research to determine if our limitation to projective trees af-fects translation quality, and how to produce non-projective trees. 5.1 Data and Models We train English X  X erman string-to-tree SMT sys-tems on the training data of the shared transla-tion task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data.
We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM transla-tion system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data.

We report case-sensitive B LEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry
We split all particle verbs and hyphenated com-pounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5 ).
For comparison with the state-of-the-art, we train a full system on our restructured representa-tion, which incorporates all models and settings of our WMT 2015 submission system (Williams et Table 2: English X  X erman translation results (B
LEU ). Average of three optimization runs. Table 3: Number of compounds [that would be split by compound splitter] and particle verbs (separated, prefixed and with zu -infix) in new-stest2014/5. Average of three optimization runs. uses the dependency representation of compounds and tree binarization introduced in this paper; we achieve additional gains over the submission sys-tem through particle verb restructuring. 5.2 SMT Results Table 2 shows translation quality (B LEU ) with dif-ferent representations of German compounds and particle verbs. Head binarization not only yields improvements over the baseline, but also allows for larger gains from morphological segmenta-tion. We attribute this to the fact that full com-pounds, and prefixed particle verbs, are not al-ways a constituent in the segmented representa-tion, and that binarization compensates this the-oretical drawback.

With head binarization, we find substantial im-provements from compound splitting of 0.7 X 1.1 B
LEU . On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hier-archical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds gener-ated include Staubsauger|roboter  X  X acuum cleaner robot X , Gravitation|s|wellen  X  X ravitational waves X , and NPD|-|verbot|s|verfahren  X  X PD banning pro-
Particle verb restructuring yields additional gains of 0.1 X 0.4 B LEU . One reason for the smaller effect of particle verb restructuring is that the diffi-cult cases  X  separated particle verbs and those with infixation  X  are rarer than compounds, with 2841 rare compounds [that would be split by our com-pound splitter] in the reference texts, in contrast to 553 separated particle verbs, and 176 particle verbs with infixation, as Table 3 illustrates. If we only evaluate the sentences containing a particle verb with zu -infix in the reference, 165 in total for newstest2014/5, we observe an improvement of 0.8 B LEU on this subset (22.1  X  22.9), signifi-cant with p &lt; 0 . 05 .

The positive effect of restructuring is also ap-parent in frequency statistics. Table 3 shows that the baseline system severely undergenerates com-pounds and separated/infixed particle verbs. Bi-narization, compound splitting, and particle verb restructuring all contribute to bringing the distri-bution of compounds and particle verbs closer to the reference.

In total, the restructured representation yields improvements of 1.4 X 1.8 B LEU over our base-line. The full system is competitive with official submissions to the WMT 2015 shared translation tasks. It outperforms our submission (Williams et al., 2015) by 0.4 B LEU , and outperforms other phrase-based and syntax-based submissions by 0.8 B
LEU or more. The best reported result accord-ing to B LEU is an ensemble of Neural MT systems (Jean et al., 2015), which achieves 24.9 B LEU . In the human evaluation, both our submission and the Neural MT system were ranked 1 X 2 (out of 16), with no significant difference between them. 5.3 Synthetic LM Experiment We perform a synthetic experiment to test our claim that a dependency representation allows for the modelling of agreement between morphemes. For 200 rare compounds [that would be split by our compound splitter] in the newstest2014/5 ref-erences, we artificially introduce agreement errors by changing the gender of the determiner. For in-stance, we create the erroneous sentence sie er-heben ein Handgep X ckgeb X hr as a complement to Example 1. We measure the ability of language models to prefer (give a higher probability to) the original reference sentence over the erroneous one. In the original representation, both a Kneser-Ney 5-gram LM and RDLM perform poorly due to data sparseness, with 70% and 57.5% accuracy, re-spectively. In the split representation, the RDLM reliably prefers the correct agreement (96.5% ac-curacy), whilst the performance of the 5-gram model even deteriorates (to 60% accuracy). This is because the gender of the first segment(s) is ir-relevant, or even misleading, for agreement. For instance, Handgep X ck is neuter, which could lead a morpheme-level n-gram model to prefer the de-terminer ein , but Handgep X ckgeb X hr is feminine and requires eine . Our main contribution is that we exploit the hi-erarchical structure of morphemes to model them jointly with syntax in a dependency-based string-to-tree SMT model. We describe the dependency annotation of two morphologically complex word classes in German, compounds and particle verbs, and show that our tree representation yields im-provements in translation quality of 1.4 X 1.8 B LEU
The principle of jointly representing syntactic and morphological structure in dependency trees can be applied to other language pairs, and we ex-pect this to be helpful for languages with a high degree of morphological synthesis. However, the annotation needs to be adapted to the respective languages. For example, French compounds such as arc-en-ciel  X  X ainbow X  are head-initial, in con-trast to head-final Germanic compounds.
 This project received funding from the Euro-pean Union X  X  Horizon 2020 research and innova-tion programme under grant agreements 645452 (QT21), 644402 (HimL), 644333 (TraMOOC), and from the Swiss National Science Foundation under grant P2ZHP1_148717.

