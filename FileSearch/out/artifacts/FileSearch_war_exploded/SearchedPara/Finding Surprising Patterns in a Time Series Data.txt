 The problem of finding a specified pattern in a time se-ries database (i.e. query by content) has received much attention and is now a relatively mature field. In con-trast, the important problem of enumerating all sur-prising or interesting patterns has received far less at-tention. This problem requires a meaningful definition of "surprise", and an efficient search technique. All pre-vious attempts at finding surprising patterns in time series use a very limited notion of surprise, and/or do not scale to massive datasets. To overcome these lim-itations we introduce a novel technique that defines a pattern surprising if the frequency of its occurrence dif-fers substantially from that expected by chance, given some previously seen data. tions-Data Mining Time series, Suffix Tree, Novelty Detection, Anomaly Detection, Markov Model, Feature Extraction. 
The problem of finding a specified pattern in a time series database (i.e. query by content) has received much attention and is now a relatively mature field [8, 18, 15, 17]. In contrast, the problem of enumerating all surprising or interesting patterns has received far less attention. The utility of such an algorithm is quite ob-vious. It would potentially allow a user to find surpris-ing patterns in a massive database without having to specify in advance what a surprising pattern looks like. Permission to make digital or hard copies of all or part of provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, requires prior specific permission and/or a fee. SIGKDD '02, July, 23-26 2002, Edmonton, Alberta, Canada. Copyright 2002 ACM 1-58113-567-X/02/0007...$5.00. 
Note that this problem should not be confused with the relatively simple problem of outlier detection. Hawkins' classic definition of an outlier is "... an observation that deviates so much from other observations as to arouse suspicion that it was generated from a different mech-anism" [14]. However we are not interested in finding individually surprising datapoints, we are interested in finding surprising patterns, i.e., combinations of data-points whose structure and frequency somehow defies our expectations. The problem is referred to under var-ious names in the literature, including novelty detection [6] and anomaly detection [28]. 
The problem requires a meaningful definition of "sur-prise". The literature contains several such definitions for time series; however they are all too limited for a useful data-mining tool. Consider for example the no-tion introduced by Shahabi et al. [24]. They define sur-prise in time series as "...sudden changes in the original time series data, which are captured by local maximums of the absolute values of (wavelet detail coefficients)". However it is not difficult to think of very surprising patterns that defy this rule. 
Several other definitions of surprise for time series ex-ist, but all suffer from similar weaknesses [4, 28, 29, 6]. To overcome these limitations we introduce a novel def-inition that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance, given some previously seen data. This no-tion has the advantage of not requiring an explicit defi-nition of surprise, which may in any case be impossible to elicit from a domain expert. Instead the user simply gives the algorithm a collection of previously observed data, which is considered normal. The measure of sur-prise of a newly observed pattern is considered relative to this data collection, and thus eliminates the need for a specific model of normal behavior. 
Note that unlike all previous attempts to solve this problem, the measure of surprise of a pattern is not tied exclusively to its structure. Instead it depends on the departure of the frequency of the pattern from its expected frequency. This is the crucial distinction of our approach :from all the others. 
Our definition of surprise would be of little utility to the data mining community without a technique that al-lowed efficient determination of the expected frequency 
I~11~ h 9 ravin iito tow~ tm v.4,r int +1 
EXTRACT_FEATURE(X[Li+tl] ) 
MAP_.REAL_TO_INT (boundaries, features[i]) 
Let us decompose a text x in uvw, i.e., x = uvw where 
We write x[i], 1 &lt; i &lt; Ix[ to indicate the i-th sym-
We say that a string y has an occurrence at position 
Throughout this document, variables y and w usually 
We consider a string generated by a stationary Markov 
The stationary Markov chain is completely determined 7r(y[1,M], c) = P(Xi+i = c IX[i_M+l,i] = Y[1,M]) 
When the true model is unknown, the transition and ~'(Y[1,M], C) ~--fx(U[1,M]) (2) P(Y[1,M]) = n --M + 1" (3) 
Substituting in equation (1) for the estimators (2) and 
A precise relationship between the expectation of y 
LEMMA 3.1. Let y be a substring of x and wl = y[2,m], 
A simple method to count the number of occurrences a ~ ba aba ba aba aba,.$ ~$abb~a$.ba aba ba aba aba..$ suffix_tree PREPROCESS (string r, string x) letup= Ixl-m+l ANNOTATE_f(w)(Tr) 
ANNOTATE_f (w)(Tx) visit T~ in breadth-first traversal, for each node u do return T~ 
Table 3: Outline of the preprocessing algorithm for the computation of the scores obtained com-paring the trees of a reference string r against the string under analysis x 
The inputs are the reference database R, the database to be examined X, and the three parameters which con-trol the feature extraction and representation. The al-gorithm begins by discretizing the data to the desired granularity. The two resultant strings are passed to the 
PREPROCESS algorithm which constructs the annotated suffix tree T~. After this has been accomplished, the surprise of each substring found in x can determined. 
Those substrings which have surprising ratings exceed-ing a certain user defined threshold (as defined by the absolute value of z(w)) can be returned and examined by the user. 
The length 12 of the sliding window is connected with the feature window length 11 and the alphabet size a (which have been discussed in Section 2). We sug-gest choosing 12 &lt; lOgl~ I [x[ because words longer than logln I {x[ have extremely small expectations and belong to a different probabilistic regime. In fact, scores z(w) are asymptotically Gaussian distributed when ]w[ &lt; logln Ix I and Poisson distributed for longer words [23] I 
The threshold c can be identified by gathering statistics about the distribution of the scores and/or assuming the distribution of the scores to be normal. 6. EXPERIMENTAL EVALUATION that the heart of the algorithm relies on comparing two suffix trees, "tree to tree". Tarzan (R) is a registered tradermark owned by Edgar Rice Burroughs, Inc. void TARZAN (time_series R, time_series X, let x = DISCRETIZE_TIME_SERIES (X, 11, a) let r = DISCRETIZE_TIME_SERIES (R, 11, a) let Tx = PREPROCESS (r,x) for i = l, lxl -12 + l x[i,i+t2_l] 
Table 4: Outline of the Tarzan algoritm: ll is the feature window length, a is the alphabet size for the discretization, 12 is the scanning window length and c is the threshold 
We compare our approach with the TSA-tree Wavelet based approach of Shahabi et al. [24] and to the Im-munology (IMM) inspired work of Dasgupta and Forrest [6], which are the only obvious candidates for compari-son. More details about these approaches are contained in Section 7. 
We begin with a very simple experiment as a reality check. We constructed a reference dataset by creating a sine wave with 800 datapoints and adding some Gaus-sian noise (each complete sine wave is 32 datapoints long). We then built a test dataset using the same pa-rameters as the reference set, however we also inserted an artificial anomaly by halving the period of the time series in the region between the 400 th and 432 th dat-apoints. In other words, that small subsection of the test time series has two short sine waves instead of one. We compared all three approaches under consideration. 
The results are shown in Figure 2. We used a feature window of length ll = 12 for TARZAN and IMM, and an alphabet of size a = 4 for TARZAN. 
The IMM approach was unable to find the anomaly, and it introduced some false alarms. The TSA ap-proach also failed to find the anomaly. In contrast to the other techniques TARZAN shows a strong peak for the duration of the anomaly. Note that for consis-tency with the other techniques we flipped the results for TARZAN upside down, so the low expectation for the anomaly shows as a peak. 
Testing the ability of the algorithms to find surprising patterns on real data is a greater challenge, since the results may be subjective. To address this problem we consider a dataset that contains the power demand for a Dutch research facility for the entire year of 1997 [26]. 
The data is sampled over 15 minute averages, and thus contains 35,040 points. The nice feature of this dataset is that although it contains great regularity, as shown in 
Figure 3, it also contains regions that could objectively he said to be surprising or anomalous. In particular, there are several weeks on which one or more days were national holidays, and thus the normal pattern of five weekday peaks, followed by a relatively fiat weekend, is disturbed. 
We used from Monday January 6 th to Sunday March 23 "d as reference data. This time period is devoid of 
Figure 2: A comparison of three anomaly de-tection algorithms on the same task. A) The training data, a slightly noisy sine wave. B) A time series containing a synthetic "anomaly", it is a noisy sine wave that was created with the same parameters as the training sequence. 
Then the period of the sine wave between the 400 th and 432 th points (denoted by the gray bar) was halved. C) The IMM anomaly detection aN gorithm failed to find the anomaly, and intro-duced some false alarms. D) The TSA-Tree ap-proach is also unable to detect the anomaly. E) 
Tarzan shows a strong peak for the duration of the anomaly 
Figure 3: The first three weeks of the power de-mand dataset. Note the repeating pattern of a strong peak for each of the five weekdays, fol-lowed by relatively quite weekends Figure 4: The three most surprising weeks in the power demand dataset, as determined by Tarzan, TSA-Tree and IMM national holidays. We processed the remainder of the year with TARZAN, with a window size equivalent to 4 hours (11 = 16 datapoints), and an alphabet of size a = 4. Because of the size of the dataset we will just show the three most surprising sequences found by each algorithm. For each of the three approaches we show the entire week (beginning Monday) in which the three largest values of surprise fell. The results are shown in Figure 4. pear to be normal workweeks, however TARZAN returned three sequences that correspond to the weeks that con-tain national holidays in the Netherlands. These re-sults present strong visual evidence that TA1RZAN is able to find surprising patterns in time series. 
The task of finding surprising patterns in data has been an area of active research, which has long attracted the attention of researchers in biology, physics, astron-omy and statistics, in addition to the more recent work by the data mining community. The problem, and closely related tasks are variously referred to as the detection of "Aberrant Behavior" [19], "Novelties" [6], "Faults" [29], "Surprises" [24, 4], "Deviants" [16], "Temporal Change" [3, 10], and "Outliers" [14]. 
Jagadish et al. [16] introduced a technique for min-ing deviants in time series, however deviants are simply surrounding points", and thus this work may be consid-ered more of a generalization of classic outlier detection [14]. 
In [24] and several follow up papers, Shahabi et aL suggest a method to find both trends and "surprises" in large time series datasets. The authors achieve this us-ing a wavelet-based tree structure (TSA-Tree) that can represent the data at different scales, e.g., the weather trend in last month vs. last decade. However the defini-tion of surprise used seems limited to dramatic shifts in the signal. In particular, this approach is not suitable for detecting unusual data patterns that hide inside the normal signal range. For example, the system would not had flipped upside down, since the wavelet-based "sur-prise" features are invariant to this transformation of the data. The immunological based approach of Dasgupta and Forrest [6], is inspired by the negative selection mech-anism of the immune system, which discriminates be-tween self and non-self. In this case self is the model of the time series learned from the reference dataset, and non-self are any observed patterns in the new dataset that do not conform to the model within some toler-ance. A major limitation of the approach is that it is only defined when the space of self is not exhaustive. However, if you examine enough random walk data (or financial data, which is closely modeled by random walk [8]), self rapidly becomes saturated with every possible pattern, and thus non-self is the null set, and nothing encountered thereafter is considered surprising. Sequences: Computer Science and Computational Biology. Cambridge University Press, 1997. on Applied .Probability ~ Statistics. Chapman and Hall, London, 1980. time-series data. In S. Chandhuri and D. Madigan, editors, Proc. Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 282-286. ACM Press, Aug. 15-18 1999. Mining deviants in a time series database. In Proc. ~5th International Conference on Very Large Data Bases, pages 102-113, 1999. S. Mehrotra. Locally adaptive dimensionality reduction for indexing large time series databases. SIGMOD Record (A CM Special Interest Group on Management of Data), 30(2):151-162, June 2001. of time series which allows fast and accurate classification, clustering and relevance feedback. In Proc. $th International Conference on Knowledge Discovery and Data Mining, pages 239---241, 1998. identifying and predicting aberrant behaviour in time series. In Proc. l~th Internat. Conf. on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, 2001. Design, Implementation, and Applications to Pattern Discovery in Biosequences. PhD thesis, Department of Computer Sciences, Purdue University, August 2001. construction algorithm. J. Assoc. Comput. Mach., 23(2):262-272, Apr. 1976. searches for similar subsequences of different lengths in sequence databases. In Proc. International Conference on Data Enginesring, pages 23-32, 2000. Probabilistic and statistical properties of words: An overview. J. Comput. Bio., 7:1-46, 2000. wavelet-based approach to improve the efficiency of multi-level surprise and trend queries. In Proc. l~h International Conference on Scientific and Statistical Database Management, 2000. Algorithmica, 14(3):249--260, 1995. calendar-based visualization of time series data. In Proc. IEEE Symposium on Information Visualization, pages 4-9, Oct. 25-26 1999. Prec. 14th Annual IEEE Symposium on Switching and Automata Theory, pages 1-11, Washington, DC, 1973. approximation approach to anomaly detection in propulsion system test data. In Proc. AIAA/SAE/ASME/ASEE ~gth Joint Propulsion Conference, Monterey, CA, June 1993. mining association rules from house-keeping data. In Proc. of International Symposium on Artificial Intelligence, Robotics and Automation in Space, 2001. 
