 REGULAR PAPER Liang Chen  X  Ruoyu Chen  X  Sharmin Nilufar Abstract It has been proven that districted matching schemes (e.g., the US pres-idential election scheme, also called the Electoral College) are more stable than undistricted matching schemes (e.g., the popular voting scheme for selecting a governor in California), and that the theory can be used in pattern classification applications, such as image classification, where by its nature an object to be clas-sified consists of elements distributed in a bounded 2D space. However, the objects of some pattern classification applications consist of features/values of elements lying on a limited 1D line segment. This paper will prove that districted matching scheme can still outperform undistricted matching scheme in these applications, and the improved performance of districted vote scheme is even more substantial for these 1D objects than for 2D objects. The theoretical result suggests the use of districted matching schemes for pattern recognition of 1D objects. We verified the theoretical analysis through artificial neural network-based approaches for the prediction of start codons of nucleotide sequences.
 Keywords Districted voting  X  Undistricted voting  X  Start codon  X  Stability 1 Introduction This paper discusses a typical supervised machine-learning problem for pattern classification applications. The aim of such a problem is to approximate a mapping f : R m  X  L ,where L is a set of class labels and m an integer, so that we can always find a label for any m -dimensional input vector x . We take the problem as a kind of voting problem of selecting a winning candidate from L based on the preference values of voters as elements of x . We assume that the samples for the training purpose are always noise polluted. There have been numerous algorithms and approaches developed for this problem, such as neural networks, SVM, and principle component analysis, to list a few. When the problem size is large, i.e., the sample set is large, divide-and-conquer techniques are always used: we may partition the space into sections, which may or may not be disjointed, so that we can find the section where a problem instance is located before we further determine its label. The labeling of an object strongly depends on the section that x belongs to.
 of a large number of elements lying on a 2D bounded space (nation), i.e., m is large, another type of divide-and-conquer approach, called a districted matching approach is frequently used. A districted matching approach first partitions the 2D bounded space into blocks, i.e., partitions the input vector x into a union of lower-dimensional vectors, each of which constitutes a group of neighboring elements (variables) in the 2D space; then makes classifications for each block based on features/values extracted from it, and finally obtains the final decision according to a kind of voting on the classification labels of all the blocks. This approach is actually remarkably close to the US presidential election system (also called Elec-toral College), as opposed to an undistricted matching where the label is decided based on features/values extracted from the entire bounded 2D space (nation). The simplest version of undistricted matching, called undistricted voting, is a simple method for image recognition where the label or winner of the image is determined by applying a simple majority principle on pixels where each pixel comprises one vote. Using districted voting, the simplest districted matching scheme, the label is determined by the majority of locally elected labels in pre-partitioned, equally sized regions, where each local label is selected according to the majority principle of all the pixels in the region.
 districted and undistricted voting schemes, where they are called regional vot-ing and national voting, respectively. We proved that districted voting scheme is always more stable than undistricted voting scheme, and that districted voting schemes with smaller sized regions are always more stable than those with larger sized regions, so long as the size of regions is not so small to violate a consistency assumption (average distribution assumption). As it is mentioned above, in most real applications, we use complex undistricted matching and districted matching schemes based on the features extracted from the entire nation and each region, respectively, rather than the simplest undistricted and districted voting based on simple vote counting, because of the difficulties of registration. We believe that the conclusion for simple districted voting and undistricted voting is valid for these complex districted and undistricted matchings. Actually, in a recent paper, Keren [ 4 ] has showed the advantage of districted matching in image  X  X tyle X  clas-sification, where the matching process of each region involves a comparison of the extracted features in the form of DCT coefficients.
 motivated by as well as, based and evaluated on image recognition/classification. However, in many applications of pattern recognition, the objects to be recognized usually consist of elements that tightly lie on a line segment. We call these objects 1D objects. Under such situations, we can of course still define a districted voting (matching) scheme and an undistricted voting (matching) scheme. An example of these applications is the prediction of start codons (translation initiation sites) in nucleotide sequences. There are many AUG triplets in an mRNA sequence. A translational initiation usually takes place at the first occurrence of the triplet AUG in an mRNA sequence, but it is not always so [ 10 ]. That is to say, in some cases the initiation happens at an AUG further downstream. The choice of start codon is context-dependent. The problem of predicting translation initiation sites is a non-trivial task [ 10 , 14 ]. It is fairly standard to use a sliding window covering fixed lengths of the upstream and the downstream of an AUG and test if the nucleotide sequence in a window matches with any pattern of start codon [ 7 , 10 ]. recognition of start codons, where the inputs are the nucleotides in a sliding window. Taking this method as a general, undistricted matching scheme, we can obtain a districted matching scheme from it: partition the input window into a cer-tain number of sub-windows, use a neural network for each of the sub-windows; then use another neural network whose inputs are the outputs of the previous neu-ral networks for the determination of the start codons.
 where the x constitutes elements lie on a line by nature? This paper X  X  motivation is the answer to this question. 2 Theoretical model We consider a simple voting problem where all the voters lie on a single interval of a line, called the nation. Following the assumption in paper [ 2 ], we only consider a two-candidate situation, say, two candidates A and B ; and each voter votes for one and only one candidate. We let  X  and  X  denote, respectively, the proportions of the (total) votes that candidates A and B get from the whole interval in the absence of noise. Now, we suppose that the voting is carried out on the interval [ 0 , N ] which consists of N unit cells (or pixels) each having exactly one vote to exercise. Without loss of generality, we assume  X  +  X  = 1and  X &gt; X  .
 population of the nation vote either for candidate A or candidate B , and a candidate wins if and only if shegetsamajorityofthe N votes. The districted voting system is defined as the voting system that, first partitions the nation into N / r intervals, each of which is called a region; a population of r cells in each region vote for candidate A or B and a majority of these votes determines the winner of the region. A majority of the winners of the N / r regions determines the winner for the nation. To simplify the analysis, we further assume that N is divisible by r .
 Definition 1  X  We call a set of noise anti-A noise (or anti-B noise ) if all the cells  X  We call a vote noise contaminated if the vote of a cell happens to undergo a  X  A set of anti-A white noise (or anti-B white noise) is dispersed uniformly over  X  A set of anti-A concentrated noise (or anti-B concentrated noise) is defined as  X  The region is defined to be anti-A concentrated noise polluted (anti-B concen- X  In accordance with the above two types of noise, the anti-A noise contami-ties throughout this paper, we consider only the anti-A noise in the analysis. Thus, when we refer to noise, concentrated noise, white noise, or noise contaminated votes hereinafter, anti-A noise, anti-A concentrated noise, anti-A white noise, anti-A noise contaminated votes, respectively, are implied.
 and the number of white noise contaminated votes, respectively. 2.1 Main theorems In the analysis, we assume that there is only anti-A noise, as we want to establish a lower bound of the breakdown point in the prevailing situation of  X &gt; X  .The result for districted voting will be established in Theorem 2 while the exact bound for undistricted voting is given in Theorem 1 .
 Assumption 1 We assume that the size of equally partitioned regions is suffi-ciently large that the average distribution assumption holds in each region; where the average distribution assumption is defined as follows: In the absence of noise, the voting distribution of the undisturbed vote prevails in any sufficiently large size areas whether consisting of a continuous part of the nation or of randomly chosen blocks of cells.
 tistical behavior of the proportions of A and B supporters prevails in each of the regions such that there are almost  X  r cells voting for A and  X  r cells voting for B . We conclude that if candidate A (or B ) wins in the nation, then so does candidate A (or B ) in each of the regions.
 Theorem 1 Undistricted voting will preserve the original candidate A if Proof Since only  X  portion of the cells originally vote for A , the size of the anti-A noise concentrated area can easily be calculated as  X  c / X  . Because of the unifor-mity of the distribution of white noise contaminated votes, among  X  w anti-A white noise contaminated votes,  X  c / X  N  X  X  w votes come from the anti-A noise concen-trated area. This is to say that  X  c / X  N  X  X  w cells are overlapped between  X  c anti-A concentrated noise contaminated votes and  X  w anti-A white noise contaminated votes. Therefore, the sum of noise contaminated votes is  X  c  X + w  X  voting is able to preserve the original candidate selection if and only if the number of overall anti-A concentrated noise contaminated votes is less then  X   X   X  2  X  N . Theorem 2 The original candidate selection of districted voting will be retained if Proof Firstly, according to our assumption, the proportions of A supporters and B supporters in each region are the same as those in the nation, i.e.,  X  and  X  , respectively. It is easy to see that, when  X  w &lt;( X   X   X )/ 2  X  N , white noise is not enough to reverse the candidate selection in any region.
 regions. Therefore, we have where S c and S r denote the size of the noise concentrated area and the total size of concentrated noise contaminated regions, respectively.
 total number of regions, original candidate selection in districted voting shall be preserved. Now, even if we suppose each concentrated noise contaminated region is a Pro-B region 3 ,when S r &lt; N / 2, the number of Pro-A regions should be still larger than that of Pro-B regions. Equation ( 1 ) ensures S r &lt; N / 2when Substituting S c with  X  c = S c  X  , the conclusion on concentrated noise of the theorem follows.
 in districted voting, a larger subdivision of the nation, namely partitioning it into smaller sized regions, leads to a higher stability, provided that the regions are large enough to hold the average distribution assumption. 2.2 Further improvement In Theorem 2 above, the ceiling operations are used to develop a sufficient con-dition of stability that constitutes a worst possible condition whereby each of the noise blocks contaminates a maximum number of regions. It is unlikely that the worst situation for each noise block happens at the same time. Some appropriate averaging will be introduced here by shifting the partitions with noise distribution fixed. To simplify the analysis, we suppose the two ends of the interval are glued together so that there are r different partitionings for partitioning the whole inter-val into regions of size r . Notice that each partitioning partitions the nation into N / r regions. We could see that as a total, these r partitionings partition the nation into N non-equivalent regions, many of which are overlapped, of course. undistricted matching scheme in each of these N regions generated by all r par-titionings; the winner of districted matching is selected according to the majority of the number of regions each candidate wins.
 Theorem 3 Generalized districted voting will retain the candidate selection if Proof To prove the theorem, we must show that, among all the possible r different partitions, each of the n -sized noise block is capable of polluting a total of n + r  X  1 different regions. (1) For n  X  r , among all the possible r partitions, there are n  X  1 partitions that (2) For n &gt; r , enumerate each of the r different partitions to examine how many concentrated noise polluted regions be S r . Now, counting all the polluted regions by the generalized districted voting, the relationship between S c and S r is S B regions, even if we take all concentrated noise polluted regions as Pro-B re-gions. Thus, the generalized regional voting should be able to retain the origi-nal voting selection when S r &lt; rN / 2. S r &lt; rN / 2 should be satisfied, when S Noting that  X  c = S c  X   X  ,wehaveprovedthetheorem.
 2 actually show the noise contaminated votes that a distributed voting can accom-modate on average, even if we still take all the concentrated noise polluted regions as Pro-B regions . 2.3 Conclusion By considering an equilibrium case of  X   X   X  = 0 . 02, Fig. 1 illustrates the number of noise contaminated votes that districted and undistricted voting can accommo-date before the original voting decision is reversed. We use generalized districted voting in the figure so as to see an averaged situation.
 noise contaminated votes districted voting system can accommodate increases continuously up to a certain point. Beyond this point we could expect that the regions might be too small to accommodate the average distribution assumption, although we do not have a distinct lower boundary of the region sizes for assuring the average distribution assumption, because it should be application dependent. large regions and small amount of white noise looks smaller than that of undis-tricted voting for concentrated noise. This is due to the strategy we have adopted in the analysis where we have regarded all of the concentrated noise polluted re-gions as Pro-B regions in counting the winning regions by districted voting. This strategy, of course, leads to an over-estimation of the effect of the noise such that only the regions that remain entirely free of noise contamination are counted to remain Pro-A . In fact, many of the Pro-B transformed regions still remain Pro-A since the number of noise contaminated votes in a region may not be large enough to reverse candidate selection in the region. The over-estimation is most serious when the size of the regions is large. Actually, we can see that if the size of regions is close to the size of the nation, districted voting will be close to undistricted vot-ing again. This implies that if the effect of over-estimation of the concentrated noise polluted regions is properly removed, the stability margin estimation will increase so that the right-back portion of the surface representing districted voting in Fig. 1 will move slightly up. Thus, we conclude that districted voting is always more stable than undistricted voting as long as the size of regions is large enough to hold the average distribution assumption. Districted and undistricted voting will become identical when the size of regions is as large as that of the nation. margin of districted voting for 2D objects is  X  c &lt;( can see that the improved performance of districted voting scheme for 1D objects is even more substantial than that for 2D objects. 3 Experiments 3.1 Dataset To examine our theory, we compare the performances of the original artificial neural network approach proposed by Pedersen and Nielsen [ 10 ], and also its  X  X istricted matching X  version, in predicting start codons in mRNA sequences. We have carried out the experiments on the well-known Arabidopsis thialiana TIS set and vertebrate TIS set provided by Pedersen and Nielsen. Each sequence in these datasets is actually not an mRNA but the corresponding processed DNA. There-fore, our aim is to determine if an ATG triplet in the sequence is a start codon. all introns were removed, analogous to the slicing of mRNA sequences. Only high quality sequences containing at least 10 nucleotides upstream and 150 nucleotides downstream of the initiation point were selected, and the redundancy were reduced so as to avoid over-estimated performance resulting from biased over-represented samples. The A. thialiana TIS set contains 523 sequences, and the vertebrate TIS set 3312 sequences. Following the setting of Pedersen and Nielsen, we gen-erate one datum point for each potential start codon (the triplet ATG) on each sequence.
 centered around the respective ATG triplet. In case a sequence window  X  X alls off the edge, X  i.e., the triplet ATG lies less than 100 nucleotides from either end of the available sequence any positions missing from the 203 nucleotide window are filled with E, the symbol for unknown. We show in Fig. 2 how we are able to extract three data points from a nucleotide sequence.
 vertebrate set, and 2048 for the A. thialiana set. We use roughly 80% of data points of each set for training, and the remaining 20% for testing. The vertebrate test set contains a total 2700 data points of which 666 are centered with start codons. The A. thialiana test set contains 410 data points of which 107 are centered with start codons.
 in Fig. 3 . It has three layers with 30 hidden units and two outputs. One of the outputs is used for predicting whether the centered ATG of the input is a start codon, the other for predicting whether the centered ATG is a non-start codon. The output of the network is interpreted by believing the output neuron with the highest score. The inputs are presented by encoding DNA sequences into a binary string using a sparse coding scheme where each nucleotide is represented by five binary digits (personal communication), A = 00001, C = 00010, G = 00100, T = 01000, and E = 10000. Thus, the neural network has 203  X  5 = 1015 inputs. To simplify explanation, we will always claim that there are a total of 203 inputs by keeping in mind that each input is actually one of the five binary sequences. 3.2 Detailed methods and results All the neural network experiments are written in MATLab 6. There are many cri-teria in evaluating the performances pattern classifiers. For example, sensitivity, specificity, and overall accuracy can be used to evaluate the fraction of true posi-tive data points that are labeled positives, the fraction of true negative points that are labeled negative, and the fraction of points that are correctly labeled, respec-tively. However, there is a tradeoff between sensitivity and specificity, the speci-ficity usually decreases while the sensitivity increases, and vice versa. There are technical difficulties in fixing the sensitivity/specificity level to meaningfully com-pare the specificities/sensitivities of different neural network approaches. An over-all accuracy is also not very useful for the start codon prediction problem, since the number of positive data points (start codons) is way less than that of negative data points (non-start codons). 4 For problems where the proportions of positive and negative samples differ greatly, usually the Matthews correlation coefficient (MCC) is the best way to evaluate a classifier X  X  performance. Therefore, the per-formances of our experiments will be compared mainly based on the Matthews correlation coefficients [ 8 ]. The Matthews correlation coefficient [ 8 ]ofaclassifier is defined as where TP is the number of true positive examples labeled as positive by the clas-sifier, TN is the number of true negative examples correctly labeled as negative, FN is the number of positive examples incorrectly labeled as negative, and FP is the number of negative examples incorrectly labeled as positive. 3.2.1 Undistricted matching version As is well known, different implementations of a neural network approach might result in slightly different performance in practice, we re-implement the origi-nal version of Pedersen and Nielsen X  X  approach using MATLab 6 5 The activation function used for hidden layers is hyperbolic tangent sigmoid transfer function (tansig). The activation function used for output neuron is saturating linear trans-fer function (satlins). The gradient descent with momentum and adaptive learning rate back-propagation algorithm (traingdx) is used for neural network tainting. correlation coefficient of 0.5955 with overall accuracy of 85.52%, sensitivity of 64.41%, and specificity of 92.43%. The best network for Arabidopsis dataset yielded Matthews correlation coefficient of 0.7058, with overall accuracy of 88.78%, sensitivity of 76.63%, and specificity of 93.07%. 6 3.2.2 Districted matching version For the districted matching version of the neural network approach, we partition the input vectors into blocks. We will use a neural network for each block, then another neural network to generate the final results. The structure of the districted matching version of the neural network is shown in Fig. 4 .
 We partition each datum point into K equivalent subsequences, Win ( 1 ), Win ( 2 ), ..., Win ( K ) , and associate all these subsequences with the label  X  X tart codon X  or  X  X on-start codon X  indicating that the ATG centered at the original window se-quence is a start codon or a non-start codon. When 203 is not divisible by K ,welet the last block contains slightly more nucleotides than the other blocks. For every i , corresponding to the subsequences Win ( i ) of all the data points, we implement a neural network with the same structure described above, called a regional neural network. We have K total regional neural networks, each with two outputs. The numbers of hidden neurons are of course smaller than that for the undistricted matching version of the neural network. In our experiments, we use 10 hidden neurons for all these regional neural networks in all cases.
 2 outputs, and K hidden neurons. The inputs of the final voting neural network come from the outputs of the regional neural networks, and the outputs are the results, indicating if the centered ATG of the concatenated sequence of the in-puts of the regional neural networks is a start codon or a non-start codon. The activation function used in the final voting stage neural network is hyperbolic tangent sigmoid transfer function (tansig) for the hidden layer and log-sigmoid transfer function (logsig) for output neurons. The training function (trainlm) is used to update weight and bias values according to Levenberg X  X arquardt optimization.
 remaining 20% for testing. As we have claimed, we should compare the Matthews correction coefficients to evaluate the performances. The MCC values of the dis-tricted versions of NN approach for A. thialiana and vertebrate datasets for dif-ferent sizes of regions are shown in Figs. 5 and 6 . (For completeness, the overall accuracies, sensitivities, and specificities for these two datasets using the districted matching version neural network are shown in the Appendix.) approach performs much better than the undistricted matching version, as long as region sizes are not too small, which verifies our theorems. We can also see that, roughly speaking, the MCC increases while the region size decreases, then starts to decrease. 7 We think the average distribution assumption is most likely to be held when the region sizes are larger than 29 or equal to 22 (correspond-ing to the districted matching with seven or more regions) for both A. thialiana Matthews Correlation Coefficient and vertebrate data. When the size of the regions becomes smaller than or equal to 22 (corresponding to the districted matching with nine or more regions), the average distribution assumption cannot be fully satisfied, although may partially be satisfied. The regions of size 5 (corresponding to the districted matching with 40 regions) for A. thialiana data, of 15, 10, and 5 (corresponding to the districted matching with 13, 20, and 40 regions, respectively) for vertebrate data, should be taken as the cases that the average distribution assumption can never hold. 4 Concluding remarks This paper proved theoretically that districted matching is more stable than undistricted matching in pattern recognition/classification for 1D objects. The correctness of the theory is verified by using districted and undistricted versions of neural network approaches for the prediction of start codons of nucleotide sequences.
 [ 6 ], SVM [ 14 , 13 ], PCA [ 12 ], SOM [ 5 ], C4.5 [ 11 ], etc., are also very frequently used in pattern recognition; experiments on the districted versions of these ap-proaches for 1D object classification/recognition should be valuable and very in-teresting.
 matching, and districted matching with smaller sized partitioned regions is always better than that with larger sized regions, as long as the size is not too small. How-ever, we do not have the definition on what we meant by  X  X oo small X  here. The concept of  X  X oo small X  strongly depends on the application. It will be very inter-esting and valuable to investigate by experiment what the  X  X oo small X  meant in different applications, as we believe that a  X  X mallest X  region for districted match-ing is strongly relevant to the measure of a group of elements that can function relatively independent of other elements around.
 have a huge effect on districted matching approaches for certain applications. It has been realized [ 9 ] that a shifting process based on topographical con-straints can reduce the errors due to mismatching in the face recognition appli-cation of a two-level Hamming network for associative memory, which is ac-tually equivalent to what we call districted matching approach. We believe that such a shifting approach should also work for districted matching in many ap-plications; experiments on the start codon prediction problem using districted matching approach enhanced with shifting strategy may come out with even better results.
 Appendix References
