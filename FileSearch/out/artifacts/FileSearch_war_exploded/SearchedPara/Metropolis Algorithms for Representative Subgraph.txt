
While data mining in chemoinformatics studied graph data with dozens of nodes, systems biology and the Internet are now generating graph data with thousands and millions of nodes. Hence data mining faces the algorithmic chal-lenge of coping with this significant increase in graph size: Classic algorithms for data analysis are often too expensive and too slow on large graphs.

While one strategy to overcome this problem is to design novel efficient algorithms, the other is to  X  X educe X  the size of the large graph by sampling. This is the scope of this paper: We will present novel Metropolis algorithms for sampling a  X  X epresentative X  small subgraph from the original large graph, with  X  X epresentative X  describing the requirement that the sample shall preserve crucial graph properties of the original graph. In our experiments, we improve over the pioneering work of Leskovec and Faloutsos (KDD 2006), by producing representative subgraph samples that are both smaller and of higher quality t han those produced by other methods from the literature.
Graphs are ubiquitous: They are used in various ap-plication domains to represen t objects and their relation-ships, including fields such as bioinformatics, systems biol-ogy, social network analysis and even software engineering. Whereas in the past, graph structures in application domains such as chemoinformatics included dozens of nodes, nowa-days, bioinformatics and the Internet are generating graphs with thousands or even tens and hundreds of thousands of nodes.
 This increase in graph size is a challenge for data mining: In many applications we either need to run expensive algo-rithms such as simulations (routing protocols, virus propa-gation,  X  X iral marketing X  analysis) on these graphs, or we want to get an impression of the graph topology from vi-sualization. The runtime effort for all these tasks usually scales at least polynomially in the size of the graph, i.e. its number of nodes n . For large graphs, these runtimes of O ( n c ) , where often c  X  3 , are not affordable. As we are highly interested in data analysis on these large graphs, we have to work around this problem. Strategy one is to de-velop scalable algorithms that improve on the c term such that the overall runtime is better than quadratic. This usu-ally comes at the price of employing heuristics which do not guarantee an optimal solution to the problem at hand. Strategytwowouldbetoimprovethe n term in the run-time effort. Reducing n is equivalent to reducing the graph size. One way to do this is by sampling a subgraph from the large graph such that this subgraph approximates the origi-nal graph well. This is the problem we study in this article. Why could such a subgraph sample be of interest to us? First, we can perform simulations on this sample graph that are too expensive on the large graph. Second, the sample graph may be deemed a  X  X odel X  of the large graph, repre-senting its properties in a compact manner. Third, choosing a subgraph sample might provide interesting insights into the nodes that belong to this sample; they are representa-tive of the graph as a whole. Fourth, sampling a smaller subgraph is more  X  X rue X  to the data than generating an artifi-cial small graph that approximates the original graph. Any observations we make on the subgraph sample can be pin-pointed to a particular node or set of nodes in the original graph. For a generated graph, there is no 1:1 correspon-dence to nodes in the original graph.

But how does one measure if a graph sample is  X  X ood X  and approximates the original graph well? How do we mea-sure if it is a  X  X epresentative X  sample of the large graph? The idea is to find a subgraph sample S that approximates topo-logical properties S of the original graph G , such as its de-gree distribution. This problem of finding a  X  X epresentative subgraph sample X  can be cast into the following optimiza-tion problem: where G is the original graph, S is a subgraph of G with |
S | = n nodes,  X  ( X ) is a topological property of graph X , and  X  is a distance function on these topological properties.
For most graph properties and distance functions, solv-ing problem (1) optimally is intractable. We demonstrate this by showing that problem (1) is closely related to a NP-complete problem which we refer to as FIT. We define the FIT problem as for a given vector  X  1 .
 Theorem 1 FIT is NP-complete.
 Proof We s h ow t h a t C L I Q U E  X  p FIT. CLIQUE is the problem to decide whether a graph G contains a clique of n nodes. Searching for a size n -clique in a graph is equivalent to solving the following instance of the FIT problem: Assume  X  ( S ) is the degree distribution of subgraph S (normalized to 1) and  X  1 is equivalent to the degree distribution of a graph whose nodes all have the same degree n  X  1 (normalized to 1 as well). If FIT finds a subgraph S such that  X (  X  ( S ) , X  1 )=0 ,then G obviously contains a clique of size n .If min  X (  X  ( S ) , X  1 ) &gt; 0 ,then G does not contain a clique of size n . Hence CLIQUE is polynomial-time reducible to FIT, and thus FIT is NP-complete.

To cope with the NP-completeness of the FIT problem, we define and explore the use of Metropolis-based sampling and optimization techniques to find  X  X ood X  solutions to (1) as measured by  X (  X  ( S ) , X  ( G )) .

Another question we are interested in is whether captur-ing  X  ( G ) for a number of key properties  X  will result in a subgraph S which might embody many other properties of G . Empirically, we observe, for instance, that by matching the degree distribution of G and S , we can preserve other interesting graph properties.

Note that, as commonly done in the graph mining liter-ature, we are dealing with undirected graphs without self-loops and multiple edges here. The samples of our methods always represent induced subgraphs of the original graph, that is we keep all edges between nodes in the sample sub-graph that are present in the original graph. Methods from the literature that we compar e to may generate non-induced subgraphs as well.
Unlike our approach, existing graph sampling algorithms do not compute properties of the original graph for the ac-tual sampling step. They can be cl assified into three concep-tual categories [9]: Random node selection, random edge selection and sampling by exploration.

There are slightly different variants of randomly select-ing nodes ( RandomNode , RandomPageRank , RandomDe-greeeNode ) or edges ( RandomEdge , RandomNodeEdge ). For comparison purposes, in this paper, we use the most common instances, namely uniformly distributed settings of RandomNode ( RN )and RandomEdge ( RE ). Both al-gorithms generate induced subgraphs of the original graph from a set of randomly selected nodes or edges.

In contrast, sampling algorithms that perform a graph exploration randomly select a starting node and then visit nodes and edges in its vicinity. The spectrum of explorative algorithms reaches from RandomNextNeighbor over Ran-domWalk and RandomJump to ForestFire exploration tech-niques. As ForestFire ( FF ) with an appropriate forward burning probability p f has been reported to perform best among all explorative algorithms [9], we use it for com-parison to our novel approach, both in an induced ( FF i ) and not-induced fashion ( FF ). Beginning with a randomly picked seed node ForestFire is recursively  X  X urning X  a ge-ometrically distributed number of outgoing links together with the corresponding neighbor-nodes. Any  X  X urning X  neighbor itself burns a random number (mean: ( p f / (1  X  p )) of its own links. This procedure proceeds until enough ( n ) nodes are burned. Not-induced ForestFire is adding all burned nodes and edges to the sample, while induced ForestFire is constructing an induced subgraph using the set of burned nodes. Thus induced ForestFire leads to samples being more connected.

In previous work also attempts were made to sample graphs by reduction. Instead of selecting nodes or edges that will be included in the future sample, nodes or edges are deleted from the original graph. Thus the original graph is shrunken to a sample. The number of nodes can be reduced by as much as 70% while preserving important graph prop-erties [8]. Furthermore sampling schemes were developed and investigated for visualization [15]. Some visualization-approaches are compressing the large graph based on a pre-computed ranking of vertices [4]. Repeatedly constructive graph modeling approaches are also proposed to be used as sampling algorithms by aborting graph growth ahead of time [10].
Whereas existing graph sampling algorithms do not compute properties of the original graph G , the key idea of our approach is to use graph properties of the original graph G that are efficient to compute or to approximate to guide us to  X  X ood X  representative subgraph samples. To-wards this end, we design graph sampling strategies based on the Metropolis Algorithm and Simulated Annealing.
Empirically, our approach shows two remarkable advan-tages over existing methods:  X  First, it generates subgraph samples of high quality :  X  Second, it allows to find high quality samples of small
This article is structured as follows. In Section 2, we re-view the Metropolis Algorithm and Simulated Annealing, which are the foundations of our approach to representa-tive subgraph sampling. In Section 3, we present our three novel strategies to subgraph sampling. In Section 4, we dis-cuss criteria for assessing the quality of a graph sample. We evaluate the practical performance of all proposed methods in Section 5, before concluding with a short discussion of our findings in Section 6.
From a statistical point of view, induced graph sampling is the task to draw a set S of n nodes from the n := | V | nodes of the original graph G =( V, E ) . For example in the case of RandomNode the samples S are uniformly dis-tributed on the sample space X = { S V | n = | S |} . By contrast the distribution of better performing explorative sampling algorithms like induced ForestFire is not explic-itly known.

The main idea of Metropolis graph sampling is to draw a sample from the sample space X following a specific den-sity ( S ) . This density should reflect subgraph sample qual-ity well, which means good induced samples S should be drawn more frequently than worse ones. Thus ( S ) depends on the quality of the sample S .

From this point of view an obvious choice of ( S ) is one depending on a distance measure with respect to a prepro-cessed graph property. The severe problem with such a den-sity ( S ) is that it is only given in an unnormalized manner ( S ) . To obtain the normalized density ( S ) we have to calculate and sum over n n summands which is obviously intractable:
How can we draw samples from the sample space X if the underlying normalized density ( S ) is not given explic-itly? This problem is solved by the Metropolis algorithm.
The Metropolis Algorithm [11, 5] can draw samples from any probability distribution, provided that a function  X  proportional to the density ( S ) can be calculated for any S  X  X .
 Such a stochastic simulatio n is reached by using Markov Chains. A Markov Chain is a stochastic process with the property that future states depend only on the current state but not on past states. In our case each state depicts a single set of nodes S which in turn represents a sample S with n X  nodes. In general Markov Chains are defined as follows: Definition 2 (Markov Chain) Let  X  denote a stochastic matrix. Let ( X n ) n  X  0 be a sequence of random variables op-erating on a probability space ( X  , F ,P ) with values in The sequence ( X n ) n  X  0 is called Markov Chain with state-space X and transition matrix  X  ,if  X  S 0 ,...,S n +1  X  X with P ( X 0 = S 0 ,...,X n = S n ) &gt; 0 : P ( X n +1 = S n +1 | X 0 = S 0 ,...,X n = S n )= X ( S n ,S For the Metropolis algorithm the central property of Markov Chains is their ergodicity under certain conditions [13]. The ergodicity-theorem states that the Markov Chain converges through transitions to a stationary distribution if and only if any state is reachable by any other state in a finite number of transitions.

This property is used alongside with detailed balance to obtain a Markov Chain with equilibrium distribution ( S ) . Therefore we can simulate transitions on this Markov Chain until it converges and then draw a sample which is distributed according to ( S ) . The clue is that the un-normalized density  X  ( S ) suffices to the simulation of the Markov Chain because it is proportional to ( S ) and the normalizing constant can be canceled out by detailed bal-ance.

In the basic Metropolis algorithm, the transition prob-ability  X ( S, S ) is separated into a proposal distribution Q ( S, S ) , describing the proba bility of proposing a move to state S from state S , and an acceptance probability a ( S, S ) , thus  X ( S, S )= Q ( S, S )  X  a ( S, S ) with S = S . The acceptance probability has to be chosen in such a way that detailed balance is still ensured: a ( S, S )= can be easy to compute, for instance, it can be uniformly distributed over some set of states S . Thus instead of cal-culating the probability of any possible transition the algo-rithm only needs to calculate the acceptance probabilities of transitions which are actually proposed. A non-uniform proposal distribution can be used to preferentially propose certain transitions. In this article we always utilize sym-metric proposal distributions Q ( S, S )= Q ( S ,S ) . Hence the acceptance probability is simplified to: a ( S, S )=
The following proposition proves our approach to be ac-curate: Proposition 3 Assume that the constructed Markov Chain only has transitions between adjacent states (displaying ad-jacent subgraphs) using a symmetric proposal distribution and the above acceptance pr obability. Under t hese condi-tions, the equilibrium distribution of the Markov Chain is exactly the desired density ( S ) .

The proof of this proposition is straigthforward and due to space limitations only shown in the ap-pendix ( http://mlg.eng.cam.ac.uk/  X  karsten/ RSS_ICDM08/icdm_appendix.pdf ).

We use the Metropolis algorithm for optimization rather than for approximating a distribution. We turn Metropolis into an optimization algorithm by choosing an appropriate unnormalized density  X  ( S ) , which in our case is inversely proportional to a distance measure  X (  X  ( G ) , X  ( S )) =  X  G, X  ( S ) between the real graph G and the sample S .
As the search space is exponential in the size of the sub-graph sample, we have to reward  X  X ood X  samples extremely because otherwise lower-quality samples would dominate the process of sampling due to their large number. We do this by exponentiating the difference  X  G, X  ( S ) by a large positive scalar p . Hence we define  X  as where p  X  R + and p 0 .
The problem of Metropolis is that the convergence of the Markov Chain can be slowed down enormously if the Markov Chain gets stuck in a local maximum of the un-derlying density ( S ) . If the Markov Chain enters such a state it is unlikely that this state is left within a reasonable amount of time. An approach to solve this problem is Sim-ulated Annealing [7]: It is based on the idea that states with high density ( S ) (called low energy states) are mostly not uniformly distributed on the state space X . Thus we change the density ( S ) in such a way that in the beginning, more transitions are accepted (high temperature) and in the very end, only transitions are accepted which are an improve-ment (low temperature). Thus energy barriers arising from regions of low density separating regions of high density can be overcome and local energy minimums (maxima of density) can be left in time. The hope is that in the end the simulation ends up on the right side of the barrier where the global maximum or at least the better local maximum can be found. This approach is based on the idea that the distribu-tion at higher temperature is a good guide to the distribution at lower temperature. We define the temperature-dependent density as p,T ( S )= e so that a new acceptance-probability follows a ( S, S )= min (1 , assuming a symmetric proposal distribution Q ( S, S )= Q ( S ,S ) .

For our experiments, we use Simulated Annealing with a geometric annealing sch edule, which means that we gradually reduce the temperature according to T t +1  X T t with 0 &lt; X &lt; 1 ,where t is the t -thstepinoursam-pling procedure.
We use the Metropolis algorithm in order to optimize a randomly picked initial sample. The key idea is that after picking a random subgraph sample S from G , we search the space of subgraphs by removing a node from and adding a new node to S in each iteration.
As stated earlier, we choose a  X  ( S ) that is inversely pro-portional to  X  G, X  ( S ) p .  X  G, X  ( S ) can be any distance mea-sure between topological properties  X  of the sample S and the original graph G , for instance, a distance on degree dis-tributions. We discuss these graph properties in Section 4.1 and suitable distance measures in Section 4.2. On the one hand, these graph properties help us to find a representative subgraph sample. On the other hand, we have to compute these properties for the original graph G as well as for ev-ery sample visited by the Markov Chain. For this reason, we restrict ourselves to graph properties that are efficient to compute (like e.g. the degree distribution). The pseudocode of our Metropolis Graph Sampling algorithm (without an-nealing) is depicted in Algorithm 1.
There are 4 crucial steps in our Metropolis subgraph sampling: First, the time effort to read the graph G ,which is O ( n  X  d avg ) ,where d avg is the average degree of a node in G ; second, the computation time for the graph property  X  ( G ) , denoted by R (  X  ( G )) ; third, the time effort to pick an initial sample, which is in O ( n  X  d avg ) ; fourth, the runtime effort for each of the # it iterations of the Markov Chain, which includes updating our subgraph sample (removing one node, adding one node) in O ( n  X  d avg ) and computing its graph properties in R (  X  ( S current )) . Hence the over-all complexity is O ( n  X  d avg + R (  X  ( G ))+# it ( d avg  X  n + R (  X  ( S Algorithm 1 Metropolis Subgraph Sampling Input: Graph G =( V, E ) , distance function  X  G, X  (  X  ) , sample size n , number of possible transitions # it ,ex-ponent p S current  X  uniformly at random from G for i := 1 to # it do end for 3.3 Choice of the exponent p
The choice of the exponent p depends on the original graph G .As p is responsible for good samples being fa-vored over worse ones, it very much depends on the real graph G and the distance measure  X  . In principle, any p&gt; 0 will result in the same optima, however larger p cre-ate a more peaked probability distribution.

If G has a high fraction of medium-quality samples then p needs to be of high value to make the small group of extraordinary samples competitive. Most graph properties used as similarity measures evaluating a sample S , firstly depend on whether S is connected. Because of that a suitable p is depending on the edges-per-node ratio of G which determines the total number of possible connected subgraphs. Besides graphs of large size also demand high values of p as the overall fraction of good samples shrinks with the graph size n . We use the following rule of thumb in all of our experiments (with k being the number of edges of G ): p G =10  X  k n log 10 n .

Why do we not generally use either enormous p values or an exponential function of  X  G, X  ( S ) ? To answer this ques-tion, one should bear in mind that large differences in den-sity are seldom overcome by the Markov Chain. Large or exponential choice of p might enhance convergence, but one then faces an increased risk of getting stuck in a state which is a local maximum of the density.
As we use Metropolis for optimizing (as described in 2.1), convergence of the Markov Chain is equivalent to the achievement of an appropriate sample. In this section we discuss a common and a new graph specific approach to de-crease the number of required iterations needed to achieve convergence of the Markov Chain and thus a good sample. By decreasing the number of performed iterations we hope to notedly reduce the CPU runtime.

Simulated Annealing in graph setting A common way to avoid that the Metropolis algorithm gets stuck in local optima is to use Simulated Annealing. Since local optima can slow down convergence, Simulated Annealing may be useful for speeding up the sampling process.

Chaining We also propose a new graph-specific ap-proach called Chaining to sp eed up convergence: As mostly the real graph G is connected or at least consists of few con-nected components, it can be observed that good samples are extremely often connected.

Thus the idea arises of restricting the search space X to connected samples. Let us assume for all of the following results that the original graph G is connected. If G has more than 1 component, the following sampling algorithm is still applicable, but will sample a subgraph within the compo-nent of the initial sample only.

Restricting ourselves to connected samples shrinks the state space of the Markov Chain, so we have to adapt the proposal distribution Q ( S, S ) , so that detailed balance is achieved. We do this by initially deleting a randomly se-lected node from S . The reduced sample is called S Then we add another random node from N ( S r )= { v  X  V ( G ) | v/  X  V ( S r )  X  X  X  w  X  V ( S r ) s.t. e =( v, w ) of adjacent nodes to the reduced set of nodes S r .Ifthe resulting induced subgraph S is not connected because a bridge node  X  whose removal disconnects the sample  X  was deleted, we reenter the old state S .Otherwise we decide according to the acceptance probability a ( S, S ) whether we should pass into the new state S .

In contrast to original Metropolis graph sampling all states of Chaining X  X  reduced state space are of a minimum quality because of being connected. Thus less proposed transitions are rejected due to higher values of a ( S, S ) in average. For this reason the hope is that the amount of nec-essary iterations to achieve convergence can be decreased.
We want to show that the Markov Chain with restricted state space X con converges to ( S )=  X  ( S ) der the assumption that the original graph G is connected. The proof of proposition 3 can be directly adopted if we show the restricted proposal distribution to be symmetric: Proposition 4 For the Chaining algorithm X  X  construction of new states (as described above) it holds that Q ( S, S )= Q ( S ,S ) .
 Proof For S = S or | S  X  S | &lt;n  X  1 this is trivial. Oth-erwise S and S are adjacent. The pr oposal proba bilities Q ( S, S ) and Q ( S ,S ) describe the cases of constructing S out of S and S out of S respectively. In both cases first of all we delete a randomly drawn node out of the start-ing set of nodes ( S or S ). The probability of picking ex-actly the node S and S differ in is ( 1 n ) . This probabil-ity is independent of the direction of the transition. The resulting graph S  X  S containing n  X  1 nodes is equal in both cases. Thus the probability of constructing S and S respectively out of S  X  S is 1 /N ( S  X  S ) . Hence Q ( S, S )= 1 n  X  1 N ( S  X  S ) = Q ( S ,S ) .
 As stated earlier (proposition 3) with this requirement of symmetry detailed balance, the ergodicity-theorem and thus the stationary distribution can be followed.

It has to be mentioned that the reduction of the state space X to X con leads to occasional construction of ille-gal unconnected subgraphs by deletion of bridge nodes. Thus connectivity has to be checked and if necessary tran-sitions have to be reversed. The connectivity-test of the new sample is performed by depth-first-search requiring O ( n + k )= O ( n  X  d avg ) additional operation time per transition. Furthermore a list of adjacent nodes N ( x ) has to be set up in O ( n  X  d avg ) time. The updating of this neigh-borhood takes O ( d avg ) . Thus the complexity of Chaining in addition to mere Metropolis is O ( n  X  d avg +# it  X  n  X  d In particular, Chaining is attractive when graph properties of the subgraph sample are expensive to compute, as Chaining decreases the overall number of iterations.
Before testing our sampling algorithm, we have to an-swer an important question: Which graph properties do we consider and which distance or similarity measures do we employ to measure the discrepancy between properties of the sample graph and the original graph, and hence the  X  X uality X  of the subgraph sample?
It is important to bear the following in mind: Both our graph properties and the similarity measures on them can be used i) inside our subgraph sampling algorithms to guide us to  X  X etter X  subgraphs ( usage for sampling ) and ii) to evaluate afterwards if the generated subgraph preserves properties of the original graph well ( usage for evaluation ). Note that we may use one property A for sampling, but employ a differ-ent property B for evaluation of the subgraph sample (as done in Section 5). We used the following graph properties to guide our Metropolis algorithms to find a solution to problem (1).
A well-known graph property is a graph X  X  degree dis-tribution , with the degree of a node being the number of its incident edges. Its popularity is certainly derived partly from the fact that the degree distribution can be obtained in O ( n  X  d avg ) using adjacency lists, which is not more expen-sive than the runtime effort for simply reading the graph.
The clustering coefficient C v of a node v with degree d ( v ) is defined as the number of edges actually existing between neighbors of v divided by the maximum possible number of such edges between its neighbors, d ( v )( d ( v ) 1) / 2 . We represent the clustering coefficients of a graph in terms of a vector with values C d (called clustering coeffi-cient  X  X istribution X  in [9]), where C d is defined as the aver-age C v over all nodes v of degree d . The worst-case com-plexity of determining this clustering coefficient vector is O ( n 3 ) , but can be determined efficiently on the real-world sparse graphs in our experiments.

Besides these well-known topological properties, we also consider the graphlet distribution of our graphs [14, 12]. Given a graph G ,a k -graphlet is a connected and in-duced subgraph of G of size k . We use the distribution of 3-, 4-, and 5-graphlets in our graphs. As a sparse graph has O (( d max ) k  X  1 ) graphlets [6] (with d max being the maxi-mum degree), we have to resort to sampling if we want to determine the graphlet distribution efficiently. Towards this end, we use a subgraph sampling algorithm by Wer-nicke [17].

We use the aforementioned properties both for sampling subgraphs and for evaluating the quality of the produced sample afterwards. In addition, we used the following graph properties exclusively for evaluation.

The diameter of a graph is the maximum shortest path length between any pair of nodes in the graph, computable in a worst-case runtime of O ( n 3 ) .
A proper way of measuring similarity between properties of our original graph and its subgraph sample is a key com-ponent of our approach and graph sampling in general. To-wards this end, we employ the following distance function, with M denoting the universe of the particular graph prop-erty, for instance the set of all existing types of graphlets, or the set of all existing node degrees. Let g =  X  ( G ) de-note the properties of the original graph, and s =  X  ( S ) the properties of the subgraph sample in the following.
Graphlets: Distributions with nominal measurement scale like the graphlet distribution of the graph ( g )and sample ( s ) are compared using a double sided ver-sion of the well known entropy-distance:  X  1 ( g, s )= we add the extra term s ( i ) to the denominator. The reason is that in certain existing graph sampling algorithms (such as RandomEdge ) we are sampling non-induced subgraphs from a graph. For this reason, it may happen that the sam-ple contains graphlets which do not appear in the original graph, as they are only non-induced subgraphs in the orig-inal graph, but induced subgraphs in the sample. The extra term s ( i ) then avoids division by zero. In the literature of graphlets [14] another distance measure has been proposed:  X ( g, s )= cause of several weaknesses: First of all in the above case of the sample containing a graphlet not existing in the orig-inal graph the measure is undefined. Second the measure is infinite if any sort of graphlet occurring in the graph does not exist in the sample, which often occurs.

Degree distribution: We compare distributions with at least ordinal measurement scal e, such as the degree distri-bution, using the Kolmogorov-Smirnov D-Statistics, which corresponds to the maximum difference between the two cumulative distribution functions F Y of g and F Y of s over the range of the random variables Y and Y on M . Y and Y are distributed according to g and s respectively:  X  2 ( g, s )= max i  X  X  | F Y ( i )  X  F Y ( i ) | = max i  X  X  i )  X  P ( Y  X  i ) | . It is derived from the Kolmogorov-Smirnov test.

Clustering coefficient: The clustering coefficient (as defined here) is a vector with values in [0 , 1] and therefore will be compared using the L 1 -norm. Higher L-norms in-cluding L  X  are not suitable for our purposes because the coefficient of a single degree should not dominate the dis-tance. As we normalise the possible distances to [0 , 1] , we are in fact calculating the av erage (absolute) differ-ence in clustering coefficients over all degrees:  X  3 ( g, s )=
Diameter: The diameter of the sample graph is mea-sured as percent deviation from the original diameter. Thus the sample-diameter can take values in [0 , 1] and therefore is comparable to the ab ove distance measures.
We test our described algorithms on the following 5 datasets, each of them representing a single graph: Dob-son&amp;Doig ( DD 1 BK 0 ) , which is a graph model of a protein [3]; Autonomous systems ( AS NOV 97 ) ,which is a graph model of the Internet [2]; a Network of trust ( Epinions ) from epinions.com, describing who trusts whom in a social network; the yeast PPI network ( yeast 20071104 ) which is a network of protein interac-tions [16]; a Citation network ( HEP  X  PH ) describing who cites whom in the discipline of high energy physics [1]. Crucial statistics of these datasets are shown in Table 5.1, more detailed descriptions can be found in the appendix.
Sampling On these 5 datasets, we ran 15 different sam-pling strategies to generate a representative subgraph sam-ple with n = 100 nodes each. 1
Table 1. Statistics on the real-world graphs used in our
These 15 approaches included 4 state-of-the-art meth-ods for representative subgraph sampling (see Section 1.1): RandomNode ( RN ) and RandomEdge ( RE ), that ran-domly sample nodes and edges from the graph, and Forest-Fire, once for sampling induced subgraphs ( FF i ), once for sampling non-induced subgraphs ( FF ).

We compared these state-of-the-art methods to our novel approaches to representative subgraph sampling: Metropo-lis subgraph sampling ( M ), and Chaining ( CH ). We ran Metropolis once each for approximating the degree distribu-tion ( M d ), the graphlet distribution ( M g ), the clustering co-efficient ( M c ) and for the unweighted combination ( M dcg and a weighted combination ( M 10 dcg ) of these three 3 cri-teria (described in Section 5.3).

We s e t  X  as in equation (3) and determined the exponent p by the rule of thumb from Section 3.3. For the two criteria which gave the best results using Metropolis, namely de-gree distribution ( d ) and weighted combination ( 10 dcg repeated the same experiments using Chaining ( Ch d and
To assess the effect of Simulated Annealing both on runtime and sample quality, w e performed Metropolis and Chaining using a Simulated Annealing Schedule, both for the degree distribution ( M SA d and Ch SA d ) and the weighted
We repeat each experiment 25 times to avoid random ef-fects.

Evaluation To measure the quality of the sampled sub-graphs, we compute their distance to the original graph in terms of the degree distribution ( degree ), the diameter ( diam ), the clustering coefficient ( clust ), and the graphlet distribution ( graphlet ), employing the distance function from Section 4.2. In addition, we compute the mean of these 4 distances, which gives us the average distance, denoted by AV G . We also show the empirical standard deviation of the average distance ( SD ). We report sample quality in terms of these distances between the subgraph sample and the original graph in Table 2 as averages over 25 repetitions on all 5 datasets. Results for individual datasets and vari-ance of sample quality are shown in the Appendix.

In addition to the quality of the sample, the runtime t (in seconds) of each algorithm is shown in the last column. Because all proposed algorithms need to precompute some graph properties, the amount of time (also in seconds) this pre-calculation requires is stated as t prep . t read denotes the time required to read the original graph and to set up its adjacency list. t run is the runtime of the actual sampling stage.
Before we discuss our results, we provide the crucial pa-rameter settings that allow the reproduction of our findings.
Forest Fire The quality of samples obtained by ForstFire depend on the accurate choice of the forward burning prob-ability p f . In [9] a forward burning probability p f &gt; 0 . 6 is proposed for sampling. This is consistent with our results in initial test runs, hence we use p f =0 . 7 (performing best in these test runs).

Metropolis The number of iterations # it , which de-scribes the maximum number of transitions performed by the Markov Chain, is set to 10,000 and the exponent p is de-termined via p =10  X  k n log 10 n , as described in Section 3.3.
When using a combination of degree distribution, graphlet distribution and clustering coefficient, we consider an unweighted sum of the distances on them ( M dcg ), or a weighted sum in which the distances on the degree distribu-tion get 10 times more weight than those on graphlets and clustering coefficient ( M 10 dcg ), as the degree distribution is the criterion that reaches the b est results on its own (see Sec-tion 5.4). When using these weighted or unweighted com-binations of three graph properties, we performed 20,000 it-erations instead of 10,000 to be sure that the Markov Chain will converge and the pre-calcu lation of 3 graph properties was not in vain.

Chaining As Chaining remarkably speeds up conver-gence, but requires additional runtime to check connected-ness of samples, we only perform one third of the iterations executed in standard Metropolis.
Overall sample quality As can be seen from Table 2 (column AVG), 8 out of the 11 variants of our Metropolis sampling algorithms outperform all existing state-of-the-art sampling algorithms in terms of sample quality. They pre-serve the graph properties of the original graph in a sub-graph sample of size n = 100 better than all other meth-ods. We further investigated the empirical standard devia-tion (column SD) and observed that results of Metropolis sampling (including its variants Simulated Annealing and Chaining) on average show less variance than those of ex-isting subgraph sampling algorithms.

Metropolis On 4 out of 5 of our datasets, Metropolis sampling approximating the degree sequence ( M d ) outper-forms all existing sampling algorithms. Dataset specific results are shown in the appendix. The only dataset on which our methods do not perform excellent is HEP-PH. The problem for Metropolis is that the sample size is close to the number of components on HEP (64 components vs. 100 nodes in sample), so samples spreading their nodes uniformly over different components cannot really capture graph properties well, and are hence rejected. All sampling methods have similar problems with this dataset, but we still ran tests on it, as it was used in the pioneering paper on sub-graph sampling [9].

It is a remarkable fact that M d , although approximat-ing the degree distribution of the original graph G only, is able to approximate the cluster ing coefficient vector and the graphlet distribution of G on average better than any of the state-of-the-art methods. Even in terms of the approximat-ing the diameter, it is second best only to FF i among the existing methods.

In terms of runtime, we make two important observa-tions: On the one hand, our methods require more run-time than those in the literature, because they are comput-ing properties of the original graph ( t prep ) and of the sub-graph samples during sampling (part of t run ). On the other hand, our methods still exhibit attractive runtimes: Even on the largest graph (Epinions, 75,879 nodes), our slowest method ( M 10 dcg ) generates a high-quality subgraph sample in roughly 367 seconds. Our fastest method M SA d generates a high-quality sample in 2 seconds on the same dataset.
There are interesting differences in runtime between dif-ferent variants of our sampling approach. M d is faster than the other variants of Metropolis ( M c , M g , M dcg M 10 dcg ), as the degree distribution can be computed more efficiently than the other graph properties.

Chaining Chaining leads to a speed-up in runtime if its computational overhead, that is guaranteeing connectedness of subgraph samples, is smaller than its computational sav-ings by speeding-up convergence. This speed-up is observ-able if the graph properties that we want to approximate and that have to be determined in each iteration of our sam-pling procedure are rather expensive to compute (for in-stance, a weighted combinatio n of several properties such as in Ch 10 dcg ), but not for properties such as the degree distribution that are efficient to compute.

Simulated Annealing In our empirical evaluation, Sim-ulated Annealing led to a slightly lower runtime on average, but also a slight loss in sample quality compared to basic Metropolis. Still, it outperforms on average all state-of-the-art methods with respect to sample quality without the need to pick an exponent p .

Variation of exponent p For Metropolis and Chaining in its standard version, the exponent p is a central parameter. To assess its influence on sample quality, we vary p from 50 to 1,000 in steps of 50 ,andrun M d on the Epinions dataset using these values of p (and in addition for p =25 ). We achieve the smallest distance to the original graph (as
Figure 1. Distance between sample graph and original an average over several graph properties) for p = 350 (see Figure 1). Small choices of p lead to higher distances to the original graph X  X  properties, and choosing larger values of p slowly increases the distance as well.

Our rule of thumb from Section 3.3 apparently makes us pick values of p that let us outperform state-of-the-art methods on our 5 datasets in the vast majority of test runs. Note that if one wants to avoid setting p at all, one may resort to Simulated Annealing.

Variation of iterations The impact of a duplication of the number of performed iterations is relatively small and very much depends on the type of investigated graph G . Further information is given in the appendix. Table 3. Distance between sample and original graph
Variation of sample size So far, we have generated rep-resentative subgraph samples with n = 100 nodes, which approximate the original graph extremely well. Can we also achieve these good results for larger subgraph samples?
We study this question on the Autonomous Systems graph ( AS NOV 97 ) by sampling subgraphs. Table 3 shows the quality with respect to the degree distribution, while Figure 2 shows the quality referring to the average of our four graph properties used for evaluation. For both evalua-tion criteria (degree and average) and for all different sam-ple sizes, we observe that Metropolis ( M d ) generates sub-graph samples that approximate the original graph X  X  proper-ties better than the samples from induced ForestFire ( FF
A thrilling result of the observations in Table 3 is that samples constructed using Metropolis can outperform sam-ples drawn by ForestFire in terms of sample quality, even if they are ten times smaller (Metropolis M d : n n =0 . 05 ,  X ( M d )=0 . 006 ; ForestFire: n 0 . 118 ). At the same time, for larger samples, our methods
Figure 2. Distance between sample graph and original Figure 3. Metropolis Sampling: Runtime vs. distance . require more runtime (see Table 3), as it gets more expen-sive to compute the graph properties of larger samples and the size of the sample search space increases. Thereby the quality of the sample betters itself as shown in figure 3.
In this article, we have proposed novel approaches to rep-resentative subgraph sampling. They are based on the key idea to compute or approximate properties of the original graph G , which are then to be approximated well by the sample graph S . While existing sampling algorithms only manage to produce good samples with a minimum size of 15% (of nodes of the original graph) [9], our algorithms suc-ceed in constructing representative samples of much smaller size (down to 0 . 14% on Epinions).

As a practical note: Among the different variants of our approach, Metropolis graph sampling approximating the degree distribution ( M d ) of the original graph offers the best trade-off between runtime and sample quality. If runtime is less of an issue, the Chaining variant, which approximates degree distribution, clustering coefficient and graphlet dis-tribution of the input graph ( Ch 10 dcg ), is the best choice.
