 One of the fundamental problems in image search is to rank image documents according to a given textual query. Exist-ing search engines highly depend on surrounding texts for ranking images, or leverage the query-image pairs annotat-ed by human labelers to train a series of ranking functions. However, there are two major limitations: 1) the surround-ing texts are often noisy or too few to accurately describe the image content, and 2) the human annotations are re-sourcefully expensive and thus cannot be scaled up.
We demonstrate in this paper that the above two funda-mental challenges can be mitigated by jointly exploring the cross-view learning and the use of click-through data. The former aims to create a latent subspace with the ability in comparing information from the original incomparable views (i.e., textual and visual views), while the latter explores the largely available and freely accessible click-through data (i.e.,  X  X rowdsourced X  human intelligence) for understanding query. Specifically, we propose a novel cross-view learning method for image search, named Click-through-based Cross-view Learning (CCL), by jointly minimizing the distance between the mappings of query and image in the latent sub-space and preserving the inherent structure in each orig-inal space. On a large-scale click-based image dataset, C-CL achieves the improvement over Support Vector Machine-based method by 4.0% in terms of relevance, while reduc-ing the feature dimension by several orders of magnitude (e.g., from thousands to tens). Moreover, the experiments also demonstrate the superior performance of CCL to several state-of-the-art subspace learning techniques.  X  H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Algorithm, Experimentation.
 Image search, cross-view learning, subspace learning, click-through data, DNN image representation.
Keyword-based image search has received intensive re-search attention since the early of 1990s [20]. The signif-icance of the topic can be partly reflected from the huge volume of published papers, particularly for addressing the problems of learning the rank or similarity functions. De-spite these efforts, the fact that the queries (texts) and search targets (images) are of two different modalities (or views) has resulted in the open problem of  X  X emantic gap. X  Specifically, a query in the form of textual keywords is not directly comparable with the visual content of images. The commercial search engines to date primarily reply on tex-tual features extracted from the surrounding texts of im-ages. However, the text description might not fully depict the salient aspect of visual content, not to mention that some images actually do not come along with any text descrip-tion. One feasible solution is learning image rankers from the query-image pairs labeled by human subjects. Howev-er, the labeling process is generally time consuming, and in practice difficult to ensure the quality of labels. Further-more, as the user search intents are not likely to always align with these pre-defined labels, image rankers used to suffer from the poor generalization performance.

Inspired by the success of multi-view embedding [31], this paper studies the cross-view (i.e., text to image views) search problem by learning a common latent subspace that allows direct comparison of text queries and images. Specifically, by mapping to the latent subspace, the relevance or simi-larity between a textual query and an image can be directly measured between their projections, making the information from the original incomparable cross-view space comparable in the shared subspace. In addition, the dimensionality of the latent subspace is significantly reduced compared with that of any input view, making the memory costs much saved fo r existing search systems.

Moreover, we consider exploring user click-through data, aiming to bridge the user intention gap for image search. In general, image rankers obtain training data by manually labelling the relevance of query-image pairs. However, it is difficult to fathom user intent based on the queries, espe-cially for those ambiguous queries. For example, given the query  X  X ustang cobra, X  experts tend to label the images of animal  X  X ustang X  and  X  X obra X  as highly relevant. However, empirical evidence suggests that most users wish to retrieve images of a car of brand  X  X ustang cobra. X  The experts X  la-bels therefore could be erroneous. This will bias the training set and the ranker will be learned sub-optimal. On the other hand, the click-through data provide an alternative to ad-dress this problem. In an image search engine, users browse image search results before clicking a specific image. The decision to click is likely dependent on the relevance of an image. Therefore, the click data can serve as a reliable and implicit feedback for image search. We hypothesize that, most of the clicked images are relevant to the given query judged by the real users.

By jointly integrating cross-view learning and click-through data, this paper presents a novel Click-through-based Cross-view Learning (CCL) approach to image search, as shown in Figure 1. Specifically, a bipartite graph between the user queries and images is constructed based on the search logs from a commercial image search engine. An edge between a query and an image is established when the users who is-sued the query clicked the image. Moreover, the textual and visual space is formed by constructing a graph on each view, respectively. The link between every two nodes in each space represents the query or image similarity. The spirit of CCL is to learn a latent subspace in the way of minimizing the distance between the mappings of query and image, while preserving the inherent structure in each original space. Af-ter the optimization of subspace learning, the relevance s-core between a query and an image in the original spaces can be directly computed based on their mappings. For any query, the image search list will be returned by sorting their relevance scores with the query.

In summary, this paper makes the following contributions:
The remaining sections are organized as follows. Section 2 describes related work on multi-view embedding and the use of click data, while Section 3 presents our click-through-based cross-view learning method. Section 4 provides empir-ical evaluations, followed by the discussions and conclusions in Section 5.
We briefly group the related work into two categories: multi-view embedding, and search by using click data. The former draws upon research in integrating multiple views to improve learning performance by exploiting either the consensus or the complementary principle, while the latter investigates Web search by mining click-through data.
The research in this direction has proceeded along three dimensions: co-training [16][22][33], subspace learning [2][9][25], and multi-kernel learning [5][14][17].

Co-training seeks consensus on two distinct views of the data. Muslea et al. combined active learning with co-training and proposed robust semi-supervised learning al-gorithms [22]. Yu et al. developed a Bayesian undirect-ed graphical model for co-training and a novel co-training kernel for Gaussian process classifiers [33]. Kumar et al. advanced co-training for data clustering and designed effec-tive algorithms for multi-view data [16]. The idea of sub-space learning is similar to co-training except the consensus is solved by learning a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Canonical correlation analysis (CCA) [9], a classical technique, explored the mapping matrices by maximizing the correlation between the projections in the subspace. Similarly, Partial Least Squares (PLS) also aims to model the relations between two or more sets of data by projecting them into the latent subspace [25]. The differ-ence between CCA and PLS is that CCA utilizes cosine as the similarity function while PLS learns dot product. Lat-er in [2], polynomial semantic indexing (PSI) is performed by learning two low-rank mapping matrices in a learning to rank framework, and then a polynomial model is considered to measure the relevance between query and document.
Different from co-training and subspace learning, multi-kernel learning exploits different kernels to different views and fuses them either linearly or non-linearly for exploring complementary properties of different views. In [17], a linear (or convex) combination of a set of predefined kernels were learned to identify a good target kernel for the applications. Later in [5], Kernel target alignment was proposed to learn the entries of a kernel matrix by using the outer product of the label vector as the ground-truth. Kloft et al. extended the multi-kernel learning framework to arbitrary l p -norm by adding a regularizer over the mixing coefficients [14].
In summary, our work belongs to subspace learning. D-ifferent from these aforementioned subspace learning meth-ods, our approach contributes by studying not only forming the shared latent subspace with the standard objective of subspace learning (i.e., the consensus between views is max-imized) but also preserving the inherent structure in each original space.
Click-through data has been studied and analyzed wide-ly with different Web mining techniques for improving the efficacy and usability of search engines. The use of the click-by constructing the two learning components of CCL. Then th e joint overall objective and its optimization strategy are provided. Finally, the whole algorithm for image search is presented. It is worth noticing that although the two views here are visual (image) and textual (query), our approach is applicable to any other domain. Let G = ( V , E ) denote a click-through bipartite. V = Q  X  V is the set of vertices, which consists of a query set Q and an image set V . E is the set of edges between the query and image vertices. The number associated with the edge represents the clicked times in the image search results of the query. Suppose there are n triads { q i , v i , c i } n i =1 from the click-through bipartite in total, where c i is the click counts of image v i in response to query q i . Let Q = { q 1 , q 2 , . . . , q n }  X   X  R n  X  d q and V = { v 1 , v 2 R q i and v i are the textual and visual feature of query q i and image v i , and d q and d v are the feature dimensionality, respectively. The click matrix C is a diagonal n  X  n matrix with its diagonal elements as c i . Please note that the query q and image v i may not be unique in each view as one single query can correspond to multiple clicked images.
We assume that a low-dimensional common subspace ex-ists for the representation of query and image. The linear mapping function can be derived from this subspace by where d is the dimensionality of the common subspace, and W q  X  R d q  X  d and W v  X  R d v  X  d are the transformation ma-trices that project the query textual semantics and image content into the common subspace, respectively.

To measure the relations between the textual query and image visual content, one natural way is to measure the distance between their mappings in the latent subspace as where tr (  X  ) denotes the trace function. The matrices W and W v have orthogonal columns, i.e., W  X  q W q = W  X  v W I , where I is an identity matrix. The constrains restrict W and W v to converge to reasonable solutions rather than go to 0 which is meaningless in practice.

Specifically, we view the click number of a query and an image as an indicator of their relevance. As most image search engines display results as thumbnails. The users can see the entire image before clicking on it. As such, barring distracting images and intent changes, users predominant-ly tend to click on images that are relevant to their query. Therefore, click data can serve as a reliable connection be-tween the queries and images. The underlying assumption is that the higher the click number, the smaller the distance between the query and the image in the latent subspace. To learn this shared latent subspace, we intuitively incorporate the distance as a regularization on the mapping matrices W q and W v weighted by the click numbers.
Structure preservation or manifold regularization has been shown effective for semi-supervised learning [21] and multi-view learning [7]. This regularizer indicates that similar points in the original space should be mapped to the po-sitions closely in the shared latent subspace. The estima-tion of the underlying structure can be measured by the ap-propriate pairwise similarity between the training samples. Specifically, it can be given by
X where S q  X  R n  X  n and S v  X  R n  X  n denote the affinity matri-ces defined on the queries and images, respectively. Under the structure preservation criterion, it is reasonable to mini-mize Eq.(3), since it will incur a heavy penalty if two similar examples are mapped far away.

There are many ways of defining the affinity matrices S q and S v . Inspired by [7], the elements are computed by Gaus-sian functions in this work, i.e., where t  X  { q, v } for simplicity, i.e., t can be replaced by any one of q and v .  X  t is the bandwidth parameters. N k ( t represents the set of k nearest neighbors of t i .
By defining the graph Laplacian L t = D t  X  S t for t  X  { q, v } , where D t is a diagonal matrix with its elements de-fined as D t ij = P j S t ij , Eq.(3) can be rewritten as
By minimizing this term, the similarity between exam-ples in the original space can be preserved in the learned latent subspace. Therefore, we add this regularizer in our framework for optimization.
The overall objective function integrates the distance be-tween views in Eq.(2) and structure preservation in Eq.(5). Hence we get the following optimization problem +  X  tr ( QW q )  X  L q ( QW q ) + tr ( VW v )  X  L v ( VW v where  X  is the tradeoff parameter. The first term is the cross-view distance, while the second term represents structure preservation.

For simplicity, we denote L ( W q , W v ) as the objective function in Eq.(6). Thus, the optimization problem can be rewritten as
The optimization above is a non-convex problem. Never-theless, the gradient of the objective function with respect to W q and W v can be easily obtained as follows:  X  W q L ( W q , W v ) = 2 Q  X  C ( QW q  X  VW v ) + 2  X  Q  X   X  W v L ( W q , W v ) = 2 V  X  C ( VW v  X  QW q ) + 2  X  V  X 
To address the difficult non-convex problem in Eq.(7) due to the orthogonal constrains, we use a gradient descent op-timization procedure with curvilinear search [29] for a local optimal solution in this work.

In each iteration of the gradient descent procedure, given the current feasible mapping matrices { W q , W v } and their corresponding gradients { G q =  X  W q L ( W q , W v ) , G  X  W v L ( W q , W v ) } , we define the skew-symmetric matrices P q and P v as
The new point can be searched as a curvilinear function of a step size  X  , such that
Then, it is easy to verify that F q (  X  ) and F v (  X  ) lead to several characteristics. The matrices F q (  X  ) and F v (  X  ) satis-derivatives with respect to  X  are given as In particular, we can obtain F q  X  (0) =  X  P q W q and F v use the classical Armijo-Wolfe based monotone curvilinear search algorithm [26] to determine a suitable step  X  as one satisfying the following conditions: where  X  1 and  X  2 are two parameters satisfying 0 &lt;  X  1 and is calculated by  X  X ticular, we have L  X  ( F q (0) , F v (0)) =  X  X Please refer to [29] for the theoretical proof details of curvi-linear search algorithm.
After the optimization of W q and W v , we can obtain the linear mapping functions defined in Eq.(1). With this, original incomparable textual query and visual image be-come comparable. Specifically, given a test query-image pair Algorithm 1 Cl ick-through-based Cross-view Learning (C-CL) 2: for iter = 1 to T max do 3: compute gradients G q and G v via Eq.(8). 4: if k G q k 2 F + k G v k 2 F  X   X  then 5: exit. 6: end if 7: compute P q and P v by using Eq.(9). 8: compute L  X   X  ( F q (0) , F v (0)) according to Eq.(14). 9: set  X  = 1. 10: repeat 11:  X  =  X  X  12: compute F q (  X  ) and F v (  X  ) via Eq.(10). 13: compute L  X   X  ( F q (  X  ) , F v (  X  )) via Eq.(13). 14: until Armijo-Wolfe conditions in Eq.(12) are satisfied 15: update the transformation matrices: 16: end for 17: Output: the pair as This value reflects how relevant the query could be used to describe the given image, with lower numbers indicating higher relevance. For any query, sorting by its corresponding values for all its associated images gives the retrieval ranking for these images. The algorithm is given in Algorithm 1.
The time complexity of CCL mainly depends on the com-putation of G q , G v , P q , P v , F q (  X  ), F v (  X  ), and L Obviously, the computation complexity of G q and G v is O ( n 2  X  d q ) and O ( n 2  X  d v ), respectively. P q and P O ( d 2 q  X  d ) and O ( d 2 v  X  d ).
 inate the computation of F q (  X  ) and F v (  X  ) in Eq.(10). By forming P q and P v as the outer product of two low-rank ma-trices, the inverse computation cost decreases a lot. As de-fined in Eq.(9), P q = G q W  X  q  X  W q G  X  q and P v = G v W v G  X  v , P q and P v can be equivalently rewritten as P q X q Y  X  q and P v = X v Y  X  v , where X q = [ G q , W q ] , Y [ W q ,  X  G q ] and X v = [ G v , W v ] , Y v = [ W v ,  X  G ing to Sherman-Morrison-Woodbury formula, i.e., ( A +  X  XY  X  )  X  1 = A  X  1  X   X  A  X  1 X ( I +  X  Y  X  A  X  1 X ) the matrix inverse ( I +  X  2 P q )  X  1 ca n be re-expressed as Fu rthermore, F q (  X  ) can be rewritten as Fo r F v (  X  ), we can get the corresponding conclusion. S-ince we typically have d  X  d q , the cost of inverting ( I + R the computation complexity of F q (  X  ) i s O ( d q d 2 ) + O ( d Similarly, F v (  X  ) is O ( d v d 2 ) + O ( d 3 ). The computation of L  X  ( F q (  X  ) , F v (  X  )) has a cost of O ( n 2  X  d q ) + O ( n O ( d q d 2 ) + O ( d v d 2 ) + O ( d 3 ).

As d  X  d q , d v  X  n , the overall complexity of the Al-gorithm 1 is T max  X  T  X  O ( n 2  X  max ( d q , d v )), where T is the number of searching for appropriate  X  which satisfies the Armijo-Wolfe conditions and it is usually less than ten in our experiments. Take the training of W q and W v on one million { query, image, click } triads with d v = 1 , 024 and d q = 10 , 000 for example, our algorithm takes about 32 hours on a server with 2.40GHz CPU and 128GB RAM.
Although we only present the distance function between query and image on the learned mapping matrices in the Al-gorithm 1, the optimization actually can also help learning of query-query and image-image distance. Similar to the distance function between query and image, the distance between query and query, image and image, is computed as k  X vW v  X   X vW v k 2 ), respectively. Furthermore, the obtained distance can be applied for several IR applications, e.g., query suggestion, query expansion, image clustering, image classification, and so on.
We conducted our experiments on the Clickture dataset [10] and evaluated our approaches for image search.
The dataset, Clickture, is a large-scale click based image dataset [10]. It was collected from one year click-through data of one commercial image search engine. The dataset comprises two parts, i.e., the training and development (dev) sets. The training set consists of 23.1 million { query, image, click } triads, where query is a textual word or phrase, image is a base64 encoded JPEG image thumbnail, and click is an integer which is no less than one. There are 11.7 millions distinct queries and 1.0 million unique images of the train-ing set. Figure 2 shows a few exemplary images with their clicked queries and click counts in the Clickture. For ex-ample, users clicked the first image 146 times in the search results when submitting query  X  X bama X  in total. It is worth noting that there is no surrounding text or description of images provided in the Clickture.

In the dev dataset, there are 79,926 h query, image i pairs generated from 1,000 queries, where each image to the corre-sponding query was manually annotated on a three point or-dinal scale: Excellent, Good, and Bad. In the experiments, the training set is used for learning the latent subspace, while the dev set is used for performance evaluation.
Task. We investigate whether our proposed approach can be used to improve image search in this work. Specifically, we use Clickture as  X  X abeled X  data for semantic queries and train the ranking model. The task is to estimate the rele-vance of the image and the query for each test query-image pair, and then for each query, we order the images based on the prediction scores returned by our trained ranking model.
Textual and Visual Features. We take the word in queries as  X  X ord features. X  Words are stemmed and stop Figure 2: Examples in Clickture dataset (upper row: clicked images; lower row: search query with click times on the upper image). words are removed. With word features, each query is repre-sented by a tf vector in the query space. In our experiments, we use the top 10,000 most frequent words as the word vo-cabulary. Inspired by the success of deep neural networks (DNN) [4], we use it to generate image representation in this work, which is a 1024-dimensional feature vector. Specifi-cally, similar to [15], the used DNN architecture is denoted as Image  X  C 64  X  P  X  N  X  C 128  X  P  X  N  X  C 192  X  C 192  X  C 128  X  P  X  F 4096  X  F 1024  X  F 1000, which contains five convolutional layers (denoted by C following the number of filters) while the last three are fully-connected layers (denot-ed by F following the number of neurons); the max-pooling layers (denoted by P ) follow the first, second and fifth convo-lutional layers; local contrast normalization layers (denoted by N ) follow the first and second max-pooling layers. The weights of DNN are learned on ILSVRC-2010 1 , which is a subset of ImageNet 2 dataset with 1.26 million training im-ages from 1,000 categories. For an image, its representation is the neuronal responses of the layer F 1024 by input the image into the learned DNN.

Compared Approaches. We compare the following ap-proaches for performance evaluation: Figure 3: The average overall objective value of Eq. (6) for each query-image pair with the increase of the iteration. The changes of the value are given at different dimensionality of the latent subspace.
Parameter Settings. N-Gram SVM is a baseline with-out low-dimensional latent subspace learning, thus the rele-vance score is predicted on the original visual features. For the other four subspace learning methods, the dimensionali-ty of the latent subspace is in the range of { 40 , 80 , 120 , 160 } . The k nearest neighbors preserved in Eq.(4) is chosen with-in { 100 , 500 , 1000 , 1500 , 2000 } . The tradeoff parameter  X  in We set  X  =0 . 3,  X  1 =0 . 2, and  X  2 =0 . 9 in the curvilinear search by using a validation set.

Evaluation Metrics. For the evaluation of image search, we adopted Normalized Discounted Cumulative Gain ( N DCG ) which takes into account the measure of multi-level relevan-cy as the performance metric. Given an image ranked list, Figure 4: The NDCG of different approaches for im-age search. The numbers in the brackets represent the feature dimension used in each approach. the N DCG score at the depth of d in the ranked list is defined by: where r j = { Excellent = 3 , Good = 2 , Bad = 0 } is the manually judged relevance for each image with respect to the query. Z d is a normalizer factor to make the score for d Excellent results 1. The final metric is the average of N DCG @ d for all queries in the test set.
As we choose the step  X  satisfying the Armijo-Wolfe condi-tions to achieve an approximate minimizer of L ( F q (  X  ) , F in Algorithm 1 instead of finding the global minimization due to its computationally expense, we depict the average overall objective value of Eq.(6) for one query-image pair versus iterations to illustrate the convergence of the algo-rithm. As shown in Figure 3, the value does decrease as the iterations increase at all the dimensionality of the la-tent subspace. Specifically, after 100 iterations, the average objective value between query mapping and image projec-tion is around 10 when the latent subspace dimension is 40. Thus, the experiment verifies that our algorithm can always reach a reasonable local optimum.
Figure 4 shows the NDCG performances on image search of five runs averaged over 1,000 queries in Clickture dev dataset. It is worth noting that the prediction of N-Gram SVM is performed on the original image visual features of 1,024 dimensions and for other four methods, the perfor-mances are given by choosing 80 as the dimensionality of the latent subspace.

Overall, our proposed CCL consistently outperforms the other runs across different depths of NDCG. In particular, the NDCG@10 of CCL can achieve 0.5738, which makes the improvement over N-Gram SVM model by 4.0%. More importantly, by learning a low-dimensional latent subspace, the dimension of the mappings of textual query and visual image is reduced by several orders of magnitude. Further-more, CCL by additionally incorporating structure preser-vation leads to a performance boost against PLS and CCA . The result basically indicates the advantage of minimizing distance between views in the latent subspace and preserv-ing similarity in the original space simultaneously. is provided at the top left corner for each image.
There is a performance gap between CCA and PLS . Though both runs attempt to learn linear mapping functions for forming a subspace, they are different in the way that C-CA learns cosine as a similarity function, and PLS learns dot product instead. As indicated by our results, maxi-mizing the correlation between the mappings in the latent subspace can lead to a better performance. Moreover, PSI utilizing click-through data as relative relevance judgements rather than absolute click numbers is superior to PLS , but is still lower than CCL . Another observation is that the per-formance gain is almost consistent when going deeper into the list. This further confirms the effectiveness of CCL .
Figure 5 shows the top 10 image search results by d-ifferent approaches for the query  X  X ustang cobra, X   X  X old-en anchor cabins, X   X  X omen bicycle, X  and  X  X umpkin faces. X  We can easily see the proposed CCL method gets the most satisfying ranking results. Specifically, compared to other baselines, the top images by CCL are more visually simi-lar to each other, especially of the query  X  X omen bicycle X  and  X  X umpkin faces. X  That is mainly caused by the effect of structure preservation regularization term in the overall objective, which restricts the similar images in the original s-pace to remain close in the low-dimensional latent subspace. Therefore, the ranks of these group of images are likely to be moved up.
In order to show the relationship between the performance and the dimensionality of the latent subspace, we compared Figure 6: The NDCG@25 performance with differ-ent dimensionalities of the latent subspace. We can see that CCL achieves the best performance among the four methods. the results of the dimension in the range of 40, 80, 120, and 160. As the method N-Gram SVM performs training and prediction by only using the original features rather than learning a latent subspace, it is excluded in this comparison.
The results are shown in Figure 6. Compared to the oth-er three runs, performance improvement is consistently ob-served at each dimensionality of the latent subspace by our proposed CCL method. Furthermore, CCL achieves the best result at the latent subspace dimensionality of 80, and the results at other dimensionality are pretty close to the best one. This observation basically verifies that CCL has a good Figure 7: The NDCG@25 performance curve at d-ifferent dimensionalities of the latent subspace with different numbers of nearest neighbors. property of being affected very slightly with the change of the dimensionality of the latent subspace.

Another important observation is that when the dimen-sionality of the latent subspace increases, the performances of all the methods are not always improved accordingly. For example, the best performance of CCL happens at the di-mensionality of 80 and for the method CCA , the highest NDCG@25 is observed at the dimensionality of 40. This somewhat indicates a general conclusion that the selection of the latent subspace dimensionality is related to the opti-mized objective considered in learning the subspace.
The number of nearest neighbors considered in the struc-ture preservation is another parameter in CCL . In the pre-vious experiments, the number was fixed to 2,000. Next, we conducted experiments to evaluate the performance of our CCL method with the number of nearest neighbors in range of { 100 , 500 , 1000 , 1500 , 2000 } at different dimensionality of the latent subspace.

The NDCG@25 with the different number of nearest neigh-bors are shown in Figure 7. As illustrated in the figure, the optimal k differs at different dimensionality of the laten-t subspace. However, at each dimensionality of the latent subspace, the performance difference by using different num-ber of nearest neighbors is within 0.0002, which softens the difficulty on choosing the optimal number of nearest neigh-bors in practice.
A common problem with multiple regularization terms in a joint optimization objective is the need to set the param-eters to tradeoff each component. In the previous experi-ments, the tradeoff  X  is optimally set in order to examine the performance of CCL on image search irrespective of the parameter influence. We further conducted experiments to test the sensitivity of  X  towards search performance.
Figure 8 shows the NDCG@25 performance with respec-t to different values of  X  at different dimensionality of the latent subspace. Similar to the effect of the number of n-earest neighbors, we can see that the performance curve is at each dimension of the latent subspace. Specifically, when the dimension of the latent subspace is 80, the performance F i g u r e 8 : T h e N D C G @ 2 5 p e r f o r m a n c e c u r v e a t d ifferent dimensionalities of the latent subspace with different  X  . fluctuates within the range of 0.001. Thus, the performance is not sensitive to the change of the tradeoff parameter.
In this paper, we have investigated the issue of directly learning the multi-view distance between a textual query and an image by leveraging both click data and subspace learning techniques. The click data represent the click rela-tions between queries and images, while the subspace learn-ing aims to learn a latent common subspace between mul-tiple views. We have proposed a novel click-through-based cross-view learning to solve the problem in a principle way. Specifically, we use two different linear mappings to project textual queries and visual images into a latent subspace. The mappings are learned by jointly minimizing the distance of the observed query-image pairs on the click-through bipar-tite graph and preserving the inherent structure in original single view. Moreover, we make orthogonal assumptions on the mapping matrices. Then the mappings can be obtained efficiently through curvilinear search. We take l 2 norm be-tween the projections of query and image in the latent sub-space as the distance function to measure the relevance of a pair of (query, image).

Our future works are as follows. First, the two learned mapping matrices can be extended to the learning of query-query and image-image distances. Next, the learned dis-tances will be further explored for applications such as query expansion, query suggestion, and image clustering, in the learned low-dimensional space. Furthermore, we will inves-tigate the kernel version of our method, making it applicable when kernel matrices instead of features are available.
This work was partially supported by the National Natu-ral Science Foundation of China (No. 61390514, No. 61272290), the Fundamental Research Funds for the Central Universi-ties (No. WK2100060011), and the Shenzhen Research In-stitute, City University of Hong Kong. [1] R. A. Baeza-Yates and A. Tiberi. Extracting semantic [2] B. Bai, J. Weston, D. Grangier, R. Collobert, [3] D. Beeferman and A. L. Berger. Agglomerative [4] C. F. Cadieu, H. Hong, D. Yamins, N. Pinto, N. J. [5] C. Cortes, M. Mohri, and A. Rostamizadeh. Two-stage [6] N. Craswell and M. Szummer. Random walks on the [7] Z. Fang and Z. Zhang. Discriminative feature selection [8] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A [9] D. Hardoon, S. Szedmak, and J. Shawe-Taylor.
 [10] X.-S. Hua, L. Yang, J. Wang, J. Wang, M. Ye, [11] V. Jain and M. Varma. Learning to re-rank: [12] T. Joachims. Optimizing search engines using [13] T. Joachims, L. Granka, B. Pan, H. Hembrooke, [14] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. [15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. [16] A. Kumar, P. Rai, and H. Daume. Co-regularized [17] G. R. G. Lanckriet, N. Cristianini, P. L. Bartlett, [18] X. Li, Y.-Y. Wang, and A. Acero. Learning query [19] Q. Mei, D. Zhou, and K. W. Church. Query suggestion [20] T. Mei, Y. Rui, S. Li, and Q. Tian. Multimedia Search [21] S. Melacci and M. Belkin. Laplacian support vector [22] I. Muslea, S. Minton, and C. Knoblock. Active [23] Y. Pan, T. Yao, K. Yang, H. Li, C.-W. Ngo, J. Wang, [24] B. Poblete and R. A. Baeza-Yates. Query-sets: using [25] R. Rosipal and N. Kr  X  amer. Overview and recent [26] W. Sun and Y.-X. Yuan. Optimization theory and [27] M. Trevisiol, L. Chiarandini, L. M. Aiello, and [28] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user [29] Z. Wen and W. Yin. A feasible method for [30] W. Wu, H. Li, and J. Xu. Learning query and [31] C. Xu, D. Tao, and C. Xu. A survey on multi-view [32] T. Yao, T. Mei, C.-W. Ngo, and S. Li. Annotation for [33] S. Yu, B. Krishnapuram, R. Rosales, and R. Rao.
