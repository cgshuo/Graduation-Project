 Over the last two decades, neuroscience has witnessed remarkable advances in the development of methods for research on human brain function, including high-density electroencephalography (EEG) and event-related potentials (ERP). The ERP ("brainwave") method is a direct measure of neuronal activity.. ERP methods have comparisons ("meta-analyses") of ERP data from different studies.. The inability to compare results across experiments has made it difficult to achieve a high-level synthesis and understanding of the vast majority of ERP results. To address this problem, we have been working to design a system, called Neural ElectroMagnetic Ontologies, or NEMO [3, 6], for data sharing and integration of results across different ERP analysis methods, experiment paradigms, and research work by introducing a method for identifying correspondences, or mappings, between alternative sets of ERP spatial and temporal measures. These alternative measures reflect different ways that ERP pattern feat ures can be summarized. For example, one research group might use a measure of peak latency (time of maximum amplitude) to summarize the timecourse of the "P100" pattern in a visual object processing experiment [14, 15], while another group might use measures of pattern onset and offset to characterize the same data. Given that different analysis methods may yield distinct and complementary insights, it is likely that this "embarrassment of riches" in ERP analysis will persist. The challenge then becomes how to develop an automatic way to find valid correspondences between features of ERP datasets that are derived from different analysis methods. To this end, we create simulated ERP data using a tool that we develop, called NEMOautolabel 1 . We extract alternative measures of latency and scalp topography (see Appendix in [5] for example) to simulate heterogeneities that arise when distinct measure generation techniques are adopted by two different research groups. Our goal schema mapping (or matching, we use them interchangeably in the present paper) datasets. Due to the nature of the ERP data, we face several unique challenges: 1. Useful schema information is limited, since the data under study is semi-structured. 2. Language-based or linguistic schema-level matcher that makes use of name and totally different names (see experiments in Section 4 for example). 3. Values of alternative measures are numerical. Conventional instance-level matcher that handles mapping between numerical elem ents based on extraction of statistical characterization, such as range, mean and standard deviation, are not appropriate, since they are too rough to capture patterns that are crucial in determining the correspondence. To address these challenges, we propose a novel method that explores sequence similarity search techniques and the NEMO ontologies, resulting in a framework for ontology-based mining of ERP data. Ontology-based mining has recently emerged as a new area of data mining, in which ontologies are used as formal domain knowledge to guide the data mining process in order to enhance performance and to represent the across datasets to determine the appropriate mapping across measures. The key problem then becomes to align subsequences of values in a principled way, thus enabling valid comparisons among instances of spatial and temporal measures across datasets. If the correspondence between two measures is not known a priori (as against the arbitrary instance numbers associated with the two datasets, the resulting graph will show no clear pattern and thus no correspondence between alternative (initially random) point-sequence curves by applying clustering to extract similar subsequences, which are further labeled using terms defined in the NEMO ontologies. These subsequences can then be aligned across the datasets, and correspondences between measures established using standard techniques for time-sequence similarity search (see Fig. 1, right frame). This approach exploits prior (domain) knowledge of visual perception) while asserting no prior knowledge about the measures. 
The rest of this paper is organized as follows: In Section 2 we give a brief simulated ERP data design and methods for point-sequence matching. In Section 4, assumptions and constraints of these methods and discuss future research directions, highlighting the contributions of this work to future research on schema matching and meta-analysis of ERP data. 2.1 Schema Matching Our study of mapping alternative measure sets is closely related to the schema matching problem. A schema matching algorithm may use multiple matching considers instance data or only schema information. Our ontology-based mining approach should be considered as one kind of instance-level method. According to the type of instance value, various instance-based approaches have been developed in previous research. For example:  X 
For textual attributes, a linguistic characterization based on information retrieval techniques can be applied [18].  X  preferred approach. Larson et al . [10] and Sheth et al . [11] discussed how relationships and entity sets could be integrated primarily based on their domain relationships: EQUAL, CONTAINS, OVERLAP, etc. Similarity of partially 
Hamming distance and Jaccard coefficient.  X  characterize the attributes X  X .g.,  X  X SN X  and  X  X honNo X  can be distinguishable since their data patterns, such as value distributions, and averages, are different [18]. Hybrid systems that combine several appro aches to determine matching often achieve better performance. For example, SemInt [16, 17] is a comprehensive matching prototype exploiting up to 15 constraint-based and 5 content-based matching criteria. Instance data is used to enhance schema-level information by providing actual value distributions, numerical averages, etc. SemInt determines a match signature for each networks or distance-based similarity measures over signatures can be used for determining an ordered list of match candidates. 
The LSD (Learning Source Descriptions) system uses machine-learning techniques to match a new data source against a previously determined global schema [18]. It represents a composite match scheme with an automatic combination of match results. In addition to a name matcher they use several instance-level matchers mapping from a data source to the global schema, the system trains multiple learners, thereby discovering characteristic instance patterns. These patterns and rules can then be applied to match other data sources to the global schema. 
The iMAP [9] system can semi-automatically discover one-to-one and even complex mappings between relational database schemas. The goal is to reformulate the matching problem as a search in a match space, To perform the search effectively, iMAP uses multiple basic matchers, called searches, e.g., text, numeric, category, unit conversion, each of which addresses a particular subset of the match space. 
An important limitation of the above instance-based matching methods is their inability to properly handle numerical instances in some certain domain application. range, mean and standard deviation, to determine match. However such information is too rough to capture patterns in ERP data that are crucial in determining the correspondence. By contrast, our proposed sequence similarity search technique is specifically designed to handle attributes with numerical values for ERP data: a spatial distance measure is used to calculate the similarity between point-sequence curves representing the numerical attributes after subsequence reordering based on clustering, as described in Section 3. 2.2 Subsequence Similarity Search We assume that similarity between point-sequence curves implies similarity between the metrics they represent. Therefore, we view the discovery of mappings between metric sets as a similarity search among two sets of point-sequence (time series) data. 
Sequence similarity search has emerged as an active area of research. In general, methods for sequence similarity search belong to one of two categories [1]: 1) Whole Matching X  X he sequences to be compared have the same length (after interpolation or offset adjustment if necessary); and 2) Subsequence Matching X  X he query sequence is smaller; we look for a subsequence that best matches the query sequence. 
The ERP metric mapping problem is a whole matching problem. Furthermore, we consider the cross spatial distance join [4] problem as a special case of whole following set: The distance function L represents a similarity measure. 
Performing the sequence similarity search task consists primarily of making the following choices: 1) a distance function L ; 2) a method to generate cross pairs ( a , b ); application-dependent. Although a wide spectrum of similarity measures has been proposed, a comprehensive survey by Keogh et al [7], which carried out extensive performance tests on different similarity measures, demonstrated that Euclidean distance outperformed other distance metrics. Therefore, we chose Euclidean distance as the distance function L in our study. 
The problem of performing efficient spatial joins in relational database systems has multiple-scan query where objects have to be accessed several times and therefore, execution time is generally not linear but superlinear in the number of objects. They achieve almost optimal I/O time. 
For performance issues, indexing is also essential for similarity searches in sequence databases. Indexing is a technique that extracts k features from every sequence, maps them to a k-dimensional space, and then discovers how to store and search these points. This method can help alleviate the "curse of dimensionality" and to preserve spatial locality in disk pages for I/O optimization. 
In the present study, we adopt a "na X ve" approach that computes similarity on every cross-join pair of conjugate sequences. The cross join is performed by multiple sequential scans of the two datasets, and we do not perform indexing on the original sequences. The rationale is that scalability is not a major concern since the number of sequences (i.e., number of measures) in most ERP datasets is relatively small (&lt;20). We propose to view the feature value vector of each ERP summary metric as forming a point-sequence curve. The problem of matching discovery between metric subsequences in each feature vector, we use clustering and label discovered clusters MFN , and P300 . All of them are defined in the NEMO ontologies). By labeling the feature instances in this way, we can group them in each dataset based on their pattern labels and then align the instance groups across datasets accordingly. This step can be viewed as a subsequence reordering process. We then apply a sequence leveraging the rich collection of sequence similarity search algorithms presently available. The final step is to evaluate the similarity of the structured point-sequence curves that now represent our two simulated ERP datasets as quantified by their respective measures. This evaluation is achieved by using the cross-spatial join to way, we can discover matching pairs of measures. Each of these steps is described in the following sections. 3.1 Simulated ERPs where each ERP comprises simulated measurement data at 150 time samples and 129 The 40 simulated subjects are randomly divided into two datasets, SG1 and SG2 , each comprising 40 ERPs (20 subjects and 2 experimental conditions). Each ERP consists activity, 9 dipoles are located and oriented within a 3-shell spherical model to simulate the topographies of 5 ERP components commonly seen in studies of visual word recognition. Each dipole is then assigned a 600 ms activation consistent with the temporal characteristics of its corresponding ERP. Simulated "scalp-surface" electrode locations are specified with a 129-channel montage, and a complex matrix of simulated noise is added to mimic known properties of human EEG. Because of volume complex spatial and temporal superposition of the 5 modeled ERP patterns. 
Spatiotemporal components are extracted from the two datasets, SG1 and SG2, using two techniques: temporal Principal Components Analysis (tPCA) and spatial Independent Components Analysis (sICA), two data decomposition techniques that are widely used in ERP research. Two alternative metric sets, m1 and m2 , are the spatiotemporal characteristics of the extracted patterns. 3.2 Data Partitioning and Reordering two alternative sets of measures using Expectation Maximization (EM) algorithm. metrics). We label the resulting clusters with pattern labels defined in the NEMO ontologies (P100, N100, etc.) using rules specified by domain experts. 
Following clustering and labeling, the pattern labels are used to align groups of right-hand graphs of Fig. 1, the point-sequence curves for metrics IN-O1 and IN-LOCC (plotted using their original orderings prior to grouping/reordering on the left-hand side) are manifestly more similar after reordering subsequences in the two curves by aligning instances that belong to the same (or similar) patterns. 3.3 Sequence Post-processing After alignment of the subsequences according to pattern labels defined in the NEMO ontology, we carry out three post-processing steps: (1) Normalization, i.e., scaling all reduce within cluster variance; and (3) Interpolation of curves, if the number of points in two point-sequence curves is different. Fig. 2 illustrates the results of normalization, Fig. 1. 3.4 Sequence Similarity Search The following heuristic assumptions are adopted in our sequence matching procedure. 
First, we assume that the two datasets from which these alternative measures are extracted contain the same or similar ERP patterns. This assumption is critical, since it permits us to reorder the two point-sequence curves by aligning subsequences that are associated with the same ERP pattern labels. 
Second, we assume that there exists a 1-to-1 mapping between pairs of metrics within the same column. 
For example, Table 1(a) illustrates a scenario where the 1-to-1 mapping assumption is violated: the value in each cell is the Euclidean distance between two point-sequence curves denoted by the row and column header of the cell. If we select cells with minimum distance value in each row, we end up with two cells within the same column being selected, suggesting that both IN-LOCC and IN-ROCC are selected using the 1-to-1 mapping heuristic coupled with the global minimum heuristic (see below). Finally, we assume a global minimum heuristic: we select those cells whose Euclidean distance values sum up to a minimum value. 1-to-1 mapping heuristic. The global minimum heuristic requires us to favor 2(b) minimum suggest the most stable mapping result. The global minimum heuristic requires a non-greedy implementation that sh ould take into consideration all possible selections. When the number of metrics is large, this implementation becomes more computationally challenging. The experiment is conducted on the simulated datasets described in Section 3.1. The test cases for the matching discovery experiment are derived as follows: each test case group (SG1 or SG2) characterized with one metric set (m1 or m2) and formulated under one decomposition method (sICA or tPCA), and from the other subject group with the alternative metric set and decomposition method. This yields 2 (subject includes two different datasets, two alternative metric sets and two decomposition instances, thus resulting in a total of 40 enriched test cases. 
We test our method on each of these test cases. Table 3, for example, shows a distance table calculated by cross-spatial join of tPCA-derived data from SG1-m1 and SG2-m2. The highlighted cells indicate similarity pairs between two point-sequence curves representing two measures (row header and column header which meet at this cell) and are selected by using the 1-to-1 mapping and global minimum discovered by our methods. For example, from this table we derive the following mappings: IN-O1  X  IN-LOCC , IN-O2  X  IN-ROCC , IN-C3  X  IN-LPAR , etc. Note that the orders of the row and column header labels are such that the golden standard mapping falls along the diagonal cells. Therefore we can easily conclude that the shifted off from the diagonal. 
The performance of our methods among the 40 test cases is quite good. Table 4 summarizes the precision for each test case. The table consists of eight divisions, each of which illustrates the precision measures for the datasets generated by five samples of replication to one of the original eight test schemes with random instance ordering. Since the fact that the precision of mapping by making a random guess is almost zero and that the results demonstrate consistent performance on randomly ordered data, the precision of our method appears markedly robust. Combining the mapping results in the 40 test cases into an ensemble model by a majority vote of each individual mapping, we obtain the ensemble mapping result. The overall precision is 11/13=84.6%. We compare the performance our algorithm with SemInt [16, 17] as the baseline. Since the data contains only numerical instances, SemInt extracts from each feature value vector 5 discriminators, namely, MIN, MAX, Average, Coefficient of variance, and Standard Deviation. The feature value vector is then projected to a match signature characterized by these discriminators. A neural network is trained based on datasets from the 40 test cases with one metric set and tested on the rest datasets with the alternative metric set to determine the match. The result shows 19.23% precision. As we point out in Section 1, the reason why our algorithm significantly outperforms SemInt is that we are able to systematically exploit prior knowledge about patterns in ERP data that is crucial to determine the matching. In this paper, we describe a method for identifying correspondences, or mappings, between alternative sets of ERP measures that might be used by different ERP research labs to characterize patterns of br ain electrical activity. The contributions of this work include the following:  X 
Use of an ontology to assign meaningful labels to ERP patterns (clusters) and thereby impose structure that is used to align alternative metrics across datasets;  X 
Application of sequence similarity search in discovering mappings across alternative metrics;  X 
Extension of the instance-level approach in schema matching, especially to handle numerical values; and  X 
Articulation of a global minimum heuristic in selecting  X  X imilarity pairs X  from the distance table. This heuristic proved to be useful and empirically robust in our experiment. Mappings between alternative spatial and temporal metrics can be used to link different representations of ERP data and thus to support representations of ERP results with the help of formal ontologies [6]. In this way, our work is closely related to schema/ontology matching, which has been an active research field for a number of years [8, 18]. In the course of developing and testing these methods, we have collected a corpus of real data from different experiments [5] and have observed a large number that a method for identifying mappings between features or metrics across two datasets may have widespread applications for ontology-based integration, beyond the specific applications discussed in the present study. 
Following we summarize some basic assumptions and limitations of the current study and then discuss some possible directions for future work. 
The proposed method assumes some domain-specific knowledge, as well as certain features of the input data. First, the source and target datasets are assumed to contain the same or similar ERP patterns. If the two datasets contain dissimilar patterns, there will be few instances that can be aligned according to common pattern labels, resulting in a poor sequence similarity search result. Second, there is assumed to be a 1-to-1 mapping between alternative data metrics. This assumption may be violated in many real-world cases. For example, in ERP data, the temporal metrics TI-begin and TI-end together capture the same information as the metric TI-duration. Our method will need to be modified in the future to handle these more complex mappings. 
Other challenges include the scalability of calculations for the global minimum in the distance table, which is essentially an NP-hard problem. It could be remedied by proper implementation such as dynamic programming, but remains computationally intractable when the number of metrics is very large. Future work will seek to find an appropriate approximation method that balances the interest in accuracy and scalability. In addition, the simulated ERP data used in the present study were carefully designed to mimic many, but not all, features of real ERP datasets. In particular, we minimized variability in latency and spatial distribution of patterns would remain tractable and relatively straightforward to interpret. In future work, we have been collected, analyzed, and stored in our NEMO ERP ontology database [20]. 
