 Word sense disambiguation is a critical task in many applications like translation and semantic search. Many approaches are implemented in order to facilitate it; some of them are supervised and some others are unsupervised. In English some sense tagged corpora which are tagged with WordNet can be found, like SemCor [1]. Thus supervised approaches have the feasibility to be implemented. But in some languages like Persian, there isn X  X  any sense tagged corp us to be used for the learning phase. In this paper, we introduce a software system which extracts the candidate senses (and so synsets) of the words within a context using FarsNet relations; and then with extracted semi-automatically and added to FarsNet. The results show a significant improvement in precision with these new relations. This paper discusses a key concept extrac tion method to be used in word sense disambiguation. Thus in this section we briefly point to the related work on WSD and keyword extraction. 
According to [2] word sense disambigua tion methods are categorized into three categories; supervised, unsupervised and knowledge based. Some other researchers have different categorizations. Tsatsa ronis and colleagues [3] categorize WSD methods into supervised and unsupervised and say that unsupervised WSD methods comprise corpus-based [4], knowledge-based such as Lesk-like [5] and graph-based [6] methods, as well as ensembles [7] that combine several methods.

Supervised approaches (such as [8]) use corpora which are tagged with the concepts of ontologies, for their training phase. While semi-supervised approaches examples of an ontology and some raw text resources and extract their subject-predicates and predicate-objects as collo cation words. Then the training data are obtained with two approaches: SRP which means that within all possible senses sets of a word collocation, the one which has the most redundant information between synonyms, etc. and finding the most co-occurring ones. 
Unsupervised approaches often are used for languages which have no or less concept tagged corpus for training phase. Re sults show that supervised approaches are often more precise but limited to those words that have sense tagged data [10 quoting [11]. For example, Tsatsaronis and colleagues [12] utilize WordNet, and use neural network and a spread activation method to disambiguate the words of sentences. Tran many internet web pages. Then for disambiguation of a word in a context, the glosses the most score will be selected for that word.

Knowledge based approaches use some knowledge resources like dictionaries or thesauri for WSD task. As Navigli [2] mentioned, their precision is less than supervised methods but their coverage is more expanded because of the expanded resources they have. Lesk [5] and extended Lesk [14] are two of these methods. They use the glosses of the senses in WordNet to disambiguate the words.

Recent research results [2] show that  X  X he accuracy of the state of the art supervised WSD methods is above 60% with an upper bound reaching 70% for all words, fine-grained WSD for English, while the accuracy of unsupervised methods is usually between 45  X  60% X . There are some known Baselines which can be used for evaluation phase of WSD works. The best known of them is the First-Sense approach. Also Lesk method can be used as a baseline, as Navigli [2] mentioned. 
In Persian, as there is no corpus tagged by word senses, there is no supervised work on WSD. Saedi and Shamsfard [15] propose a knowledge based WSD method to be used in a Persian to English machine translation system. Faili [16] introduces an English to Persian translation method which has a WSD approach on English texts senses according to a Persian WordNet so far. 
Keyword extraction is another field related to the subject of this paper. Many features are used for keyword extraction pr ocess. For example Xu and colleagues [17] use Wikipedia to derive a set of novel word features which reflect the document X  X  background knowledge. These features are the inlink, outlink, category and infobox information of the document X  X  related articles in Wikipedia. Ercan and colleagues [18] concern the relations between the words of the document to extract the keywords. In fact, using a supervised method, a lexical chain is developed from each document by using WordNet to be used for keyword extraction. 
Hulth [19] explains some methods for extracting the keywords. Some methods use the syntactic information of words (such as [20]), some have supervised learning phase (such as [21]) and some others are statistical (such as [22]). The proposed method is composed of three essential processes: Stemming and tokenizing, Word sense disambiguation and Key-synset extraction. Stemming and tokenization of Persian documents are done by STeP-1 software [23]. In the rest of this section, we will discuss WSD and Key-synset extraction approaches in more details. 3.1 Persian Word Sense Disambiguation FarsNet [24] is a Persian WordNet, recently developed in NLP lab of Shahid Beheshti University. In this lexical ontology, various kinds of relations are defined between synsets including: Hypernym, Hyponym, Meronym, Holonym, Antonym and Cause. Many researchers have used these kinds of relations to disambiguate the words senses. For example Fragos et al. [25] proposed a method to find the words senses using WordNet relations. Here, we have used synsets X  relations of FarsNet to find the senses of words. 
Our experiments showed that the above relations are not enough to find the word senses, because some combinations of related words haven X  X  got any of these kinds of relations. For instance, human X  X  mind comprehends a semantic relation between  X   X  X  X   X  (shir, means: lion) and  X   X  X  X  X   X  (jangal, means: jungle). But this relation is not among specific name or label on them. Neither FarsNet nor most of the other WordNets include this kind of relation. We have extended FarsNet relations with a few new  X  X s related to X  relations for some concepts semi-automatically. Results of using these new relations for disambiguation showed that they really increase the precision. Semi-automatic extraction of semantic relations. As it was described before, extracting semantic relations between synsets can improve the precision of search. Here, we have used a semi-automatic appro ach to do it. To find the words which are method and retrieve some highest ranked documents. In each document, we extract the words within a 5-words sized window around the target word which are not stop words. Then, all possible synsets of these adjacent words and their hypernyms up to two levels are extracted from FarsNet and added to a list. The frequency of occurrence of each of these synsets in the obtained list is considered as its rank.  X  X  X  best synsets with respect to their ranks are semantically related synsets with the target word. It should be considered that the syns et of the target word is assigned manually, but the synsets of its co-occurring words are obtained automatically. Thus the approach is semi-automatic. 
Some of the co-occurring words of our target words are not presented in FarsNet and though with this approach we will lose them. Thus, we introduce a new relation type which is between a synset and an unkown word (word which is not in FarsNet). It means that although we haven X  X  got that word in FarsNet and don X  X  know which synset it can occurs in, but we know that this word is co-occurring with some specific FarsNet in the process of disambiguation. Our results show some improvements in precision by adding these new relations. Using FarsNet for ranking the synsets' semantic relations. Having FarsNet relations, we can find the weight of the relation between any two FarsNet synsets. Equation (1) shows how to calculate this weight. In this equation distance(S i ,S j ) is the number of relations that should be passed from S i to arrive to S j . disambiguation algorithm. Finding the set of words X  synset s of each block of content. To disambiguate the words of each document, we need to split it to smaller blocks. Then, in each block we can find the relatedness weight of any two synsets of any two words to find the best synsets of the block X  X  words. In this work, we examined some different number of words within blocks to know which one is better. The results and comparisons are jump number of blocks has an important effect on the efficiency of the approach. Jump number is the number of words that we will pass after disambiguation of a block, in order to obtain the next block. For example, if we set the block X  X  number of words to 4 words and set the jump number to 2, the sentence  X  X hat is the past tense of split? X  will be split to these three blocks (here without omission of stop words): Having each block of  X  X  X  words, each possible sense (and so synset) of each word is found from FarsNet, and then the relatedness weight of any two synsets of them will and ignore indirect ones to reduce the computation time. Now we have a set of synset pairs with their similarity weights. Equation (2) shows how to calculate the total score second and first words in the current block. After acquiring the set of pair synsets with their total scores, each two pairs which are not mutex will be merged together to build a bigger set. Two pairs are mutex if they contain at least a similar word with different senses. After merging the pairs, the score of the new collection is the addition of the scores of its components. This process will be continued until no new collection can be added. After that, the collections will be sorted by their scores, and better collections are extracted for the next phase. 3.2 Key-Synset Extraction Approach The main idea of this method of key synset extraction is taken from a general principle about information density. For example, in clustering algorithms, the points which are inside a dense part have more probability to be a cluster, and points which have less density may be noises.

Inspired by this idea, we claim that those senses of a document which have more continuation of this section, this method will be described in more details.
 Calculating the pseudo-frequency. As we described in previous sections, better combinations of synsets of each block of  X  X  X  words will be used to find the key synsets. Actually, each synset which occurs in any of the best combinations can be a better ones. To calculate this rank, first using equation (3) we will compute a pseudo-frequency for each synset, which somehow shows the amount of its occurrences in the document. In this equation, the pseudo-frequency of i th synset is calculated. ColSet i is the set of collections that i th synset occurs in them and Score(Col k ) is the score of k th collection. each synset in each document, which is the main criterion to find the Key-synsets. What we need, is to calculate the relation score between each two candidate synsets of the document as described before. Here, we used indirect relations with the threshold We used this parameter (relation score) before for some other reasons. Here we are used it to disambiguate the senses of the words of  X  X  X  words blocks.

Finally we have everything we need! Using Equation (4) we can calculate the total number of candidate synsets. As there is no Persian sense tagged corpus being tagged with FarsNet, we had to make our own test corpus. We have used Hamshahri-1 corpus for this task. First of all, we divided the corpus into %70 training and %30 testing corpora randomly. Using these corpora, we can find the new relations and evaluate our approach. 4.1 Training Phase First we chose three ambiguous words that more than one of their senses occur in the corpus. These words are  X   X  X  X   X  ( X  X hir X  means: lion, faucet, breastfeeding, milk),  X   X  X  X   X  flower, goal). Then these three words have b een searched within training corpus with tf-idf measure. For our search, we used Lucene 1 search engine that uses tf-idf measure in order to rank the results of search. Within the highest ranked documents, we extracted the co-occurring words. Then with the explained method, new relations between synsets and between unknown words and synsets were extracted. 4.2 Building the Test Corpus and about 600  X  X early 200 character phrases X  around our specified words were tagged manually within the test set and the content of each phrase was stemmed with STep-1. Then the proposed method was evaluated via this built corpus over the tagged words. 4.3 Evaluation of the Method The proposed method was tested over the built test corpus with different states. Each state will be described below and its results will be presented. The programs are written in java and the tests are being done on an ordinary PC with 2GB RAM and 2.66GHz CPU. Precision of baselines. Lesk and extended Lesk approaches were implemented as our baselines, because there was no Persian WSD method working with FarsNet to be compared with our method. Also as there were no sense tagged corpora, the precision the test corpus and used it as another baseline. Table 1 shows the precision of baselines. Precision of our approach with di fferent parameters and features. The proposed approach has some kinds of parameters and features. Results show that changing them have a significant effect over the precision. Here, first we will show the effect of these parameters, then we will compare our approach with baselines and eventually we will show the effect of features. Word number and jump number effect. We have calculated the precision of our approach over the test corpus with different words number of blocks and jump number. Fig. 1 and Fig. 2 show the experimental results. Fig. 3 shows the comparison of the precision of our approach with the base lines. Our approach X  X  precision is calculated with different words number of blocks and jump number of one. Results show that our approach has out-performed the baselines. Different features effect. We have calculated the precision of our approach with and without each of the proposed features. Table 2 shows the results of the test. Complexity of the method. If we assume that the words number of block is  X  X  X , the average senses of each word is  X  X  X , the jump number is  X  X  X  and the average number disambiguating each block. We have presented an approach for WSD with FarsNet on Persian texts. Our approach uses both FarsNet relations and some other relations that are extracted by a semi-supervised approach and added to FarsNet. In addition, we made an automatic other words, we found the Key-synsets of the distinct words within the context. The results show improvement in the precision with adding these new features. Also our approach out-performs First sense, Lesk and extended Lesk with respect to the computation time which can be better with making some optimizations in approach. Of course it should be considered that th e computation times are calculated within our runs over an ordinary PC and they would be much better over a stronger server. 
For our future work we might optimize our approach to make it faster. Then we will develop a semantic search engine to use this approach for indexing the documents. We think that common search engines don X  X  work well in Persian and have no sense to the semantic of queries and documents. Thus, we are going to apply these semantic methods over search engines to make the results more admissible. 
