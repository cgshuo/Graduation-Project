
The ability to identify the mineral composition of rocks and softs is an important tool for the exploration of geological sites. For instance, NASA intends to design robots that are sufficiently autonomous to perform this task on plan-etary missions. Spectrometer readings provide one impor-tant source of data for identifying sites with minerals of interest. Reflectance spectrometers measure intensities of light reflected from surfaces over a range of wavelengths. 
Spectral intensity patterns may in some cases be sufficiently distinctive for proper identification of minerals or classes of minerals. For some mineral classes, carbonates for example, specific short spectral intervals are known to carry a distinc-tive signature. Finding similar distinctive spectral ranges for other mineral classes is not an easy problem. We pro-pose and evaluate data-driven techniques that automatically search for spectral ranges optimized for specific minerals. In one set of studies, we partition the whole interval of wave-lengths available in our data into sub-intervals, or bins, and use a genetic algorithm to evaluate a candidate selection of subintervals. As alternatives to this computationally expen-sive search technique, we present an entropy-based heuris-tic that gives higher scores for wavelengths more likely to distinguish between classes, as well as other greedy search procedures. Results are presented for four different classes, showing reasonable improvements in identifying some, but not all, of the mineral classes tested. 1.5.2 [Pattern Recognition[: Design methodology; 1.5.4 [Pattern Recognition]: Applications 
Algorithms, Experimentation, Performance tion of mineral composition of rocks and soil samples with varying degrees of success. This kind of spectrometer mea-sures the amount of sunlight reflected by a rock or soil sam-ple over a range of wavelengths. The reflectance obtained under different wavelengths can then be used to predict which minerals are present in that sample. exploration that would be sufficiently autonomous to inter-pret spectrometer data and report only the results back to 
Earth. Robots equipped with automatic classifiers of rocks would also be useful for automatically planning which dif-ferent regions of a geological site would be more promising for prospecting certain classes of minerals in a more efficient way. of reflectance intensity of a given rock at different wave-lengths. The intensity data are typically measured relative to a reference surface in order to be invariant with respect to the total amount of sunlight in the environment. ing a predictive model out of this data is running a regres-sion model for each rock or soil sample, where the dependent variable is the reflectance intensity of the unknown rock and the independent variables are the reflectance intensities of a variety of different pure minerals that are possible com-ponents of the rock, measured over the same wavelengths. 
Libraries of such pure mineral spectra exist; in particular, the Jet Propulsion Laboratory has produced a library of spectra for ]35 different pure minerals [6], each containing reflectance intensities for 826 different wavelengths between 0.4 and 2.5 p,m. nation of the intensity of its components, a regression model is built using each reflectance value at a wavelength as a data point. Then, only those minerals whose coefficients on the regression model pass a given test of statistical significance are considercd components of the rock. A successful learn-347 ing algorithm should commit as few errors as possible, where an error is accepting a given mineral as part of a rock when this is not true, and rejecting a given mineral as part of a rock when in fact it is. Ramsey, et al. present evidence in [13] that a modified Bayesian network learning algorithm, the PC algorithm [15], performs better than simple linear regression for classifying carbonates. The PC algorithm tests partial correlation be--tween a rock spectrum and different subsets of the library of mineral spectra, eliminating library spectra (hypothesized components) which are not correlated with the input. The; advantage over regression is a more refined search that cal-. culates partial correlations conditional on reduced subsets of the remaining variables, instead of considering all of them at once, as in regression. The remaining library spectra are assumed to be components of the input, and a classification can be performed based on the minerals that were not dis-carded, as explained in the previous paragraph. All of our work described below is based on this classification proce-dure. 
Experiments with the specific class of carbonates have shown that restricting the input of the PC algorithm to a smaller region of the spectrum can improve accuracy. In particular, a region suggested by prior expert knowledge (a region used by experts to identify carbonates) produces much better results than allowing the algorithm to consider the entire spectrum. In other words, the filtered spectrum does not include noninformative or noisy data that could confound mineral identification. This is a promising result that arguably can be extended to other classes. 
Carbonates show a very typical curve on the spectral re-gion between 2.0 and 2.5 #m, which motivated the scientists to focus on this region. However, coming up with a good range of wavelengths is not an easy task because little is known for other mineral classes. No automated method has been applied by the authors of [13] to find subintervals that would be more appropriate for identifying given classes and subclasses of minerals. 
Our goal is to find intervals of the spectrum, specific to each class of minerals, for which the PC algorithm performs better than the same algorithm using the entire spectrum. This is a search problem that complements other data pre-processing issues described in Section 4.We tried several methods, a collection representative of both heuristic and computational intensive approaches that also bear relation with feature selection techniques. 
Finding an appropriate subset of the spectrum range can be cast as a problem of search among the space of pos-sible subsets. Since we have over 800 available channels, an exhaustive search is infeasible. Also, a larger number of evaluated candidates increases the chance of overfitting [2]. One must decide how to trade-off the complexity of the search space depending on the chosen search algorithm, the available computational resources, and the amount of data available. 
By the terminology used in feature selection research, as described in [9], we are basically building wrappers over the PC algorithm. Four algorithms were tried: a computation-ally demanding genetic algorithm, two greedy hill-climbing algorithms and a simple grid search strategy over a rather reduced number of parameters of a customized evaluation function. 
The data filtering methodologies described here should be applied to each class of minerals at a time, since a interval that is suitable to one class is unlikely to be useful to other. Each experiment is therefore a binary classification problem. 
One general property of the data that is assumed in this work is a relative locality of importance for the reflectance signal. In other words, the informative spectrum for each rock and mineral must be smooth enough so that grouping the whole range into a reasonably small number of subinter-vals does not harm the predictive accuracy of the signal. 
A genetic algorithm is an algorithm for combinatorial op-timization [5], which is directly related to the task of finding useful subsets of the spectra. The most straightforward rep-resentation of a candidate is through a string of 826 bits, where a positive bit represents that the respective channel will be used. However, due to the reasons explained in the beginning of this section, we divided the spectrum into a fixed number of blocks, each represented by a bit. Thus, all channels in the same block are selected or not selected at the same time. 
The evaluation function is very time-consuming: it con-sists in running the modified PC algorithm over a whole set of rock samples. The fitness of a candidate is the propor-tion of rocks that are correctly classified as containing or not containing the respective mineral. On our available im-plementation, it takes about 30 seconds to evaluate a single candidate feature mask on a Pentium III 733MHz processor. 
We also used a greedy, hill-climbing algorithm that uses the same representation for search states and the same eval-uation function. On the initial state, all bits are activated. The next states are generated from the current state by set-ting to zero one of the currently activated bits. If the current candidate has n activated bits, it will generate n new can-didates. The candidate with the highest evaluation value is chosen to be the next state. 
This is another greedy algorithm that is also used for rule induction ow'x continuous/ordered attributes [4]. It consists of trimming the extremes of an interval by some percent-age of the data and evaluating the new interval obtained. A typical strategy starts with the complete interval and, at each subsequent step, generates three new candidates: the current interval with the bottom a% of the ordered data discarded, the current interval with the upper (~% of the or-dered data discarded, and an interval constructed by drop-ping the bottom and upper ~% from the current interval. 
The underlying assumption of this algorithm is that in-teresting intervals are continuous. Unlike the previous algo-rithms, all selected subintervals are of tile form [a, b], where a and b are points of the original interval. It may clearly result in suboptimal selections, at the advantage of being much less time demanding. 
A more straightforward approach would be to construct a 
The information gain algorithm for selecting a channel 
When we have calculated the expected gain for each chan-
Under this technique, we optimize the number of bins and For our experiments we used the NASA Jet Propulsion 
Also, most features of spectra which are diagnostic of the 
We performed experiments using four of the mineral classes Tables 1 and 2 show the results for running the modified  X  the genetic algorithm is computationally intensive;  X  we wanted a reasonable amount of data on both train-ing and test sets. Using a high number of folds can in fact lead to worse generalization estimates when we have few data points and the prediction error is high, as it is typical of this domain [14]. 
The whole spectrum interval was divided in 15 subinter-all the spectrum is used. 
For the genetic algorithm, we used 35 individuals. The training proceeded for at most 40 generations. In all cases, by the last generation the pool of individuals was almost; completely dominated by copies of a single individual (and in many cases, all individuals were identical), suggesting that further optimization would not improve the result ob-tained significantly. The code of the genetic algorithm was adapted from [10], with its default parameters: 0.6 chance of crossover and a low (0.0001) chance of mutation. 
We also used cached statistics to scale up the algorithm: instead of passing through all the data points when comput-ing an element of the correlation matrix (as required by the PC algorithm), we precomputed the summations and inner products of variables for the data falling under each block. Getting a new element of the correlation matrix required only a pass over these cached statistics. This procedure re-duced the computational time by over 30%. 
For the standard hill-climbing search, we adopted the fol-lowing stopping criterion: as a trade-off to avoid bad lo-. cal maxima without searching till the last state, the search stopped when we did not get improved results for five con-secutive states. The best selection on this search path was the output. For the peeling algorithm, we used a value of 5% for a. We used the same stop criterion applied on the previously described hill-climbing technique. 
To find appropriate parameter values for the entropy heuris-tic, each training set was used to evaluate the masks pro-duced by several different parameter settings. In particular, all possible combinations of 3, 4 or 5 bins with thresholds of 0.1, 0.2, 0.3, 0.4 or 0.5 standard deviations above the mean gain were tried. For each training set, the mask that pro-duced the best accuracy was selected as the optimal mask and its fitness was measured with the corresponding test set. 
Using the interval selected by experts for carbonate clas-sification, we get an accuracy of 67.7% for the raw data and 66.1% for the processed data. By comparison with the re--suits obtained, it is clear that some of our approaches were overall able to find selections with similar performance, but unable to significantly improve over it. We should not forget, however, that these results were attained without relying on background knowledge and hence provide evidence that for cases where this knowledge is actually unavailable this set. of approaches can be a useful tool. 
The data pre-processing by taking hull differences can help in some occmsions, as it was the case for carbonates. For the inosilicates, however, reasonable better results were ob-tained using the raw data. As any smoothing procedure, it can be useful in some situations, but not always. The in-formation gain heuristic proved especially sensitive to this technique. 
While our performance on carbonates and inosilicates im-proved relative to the baseline of enabling all channels, we got unimpressive results with phyllosilicates and oxides. It was expected that for some classes the reflectance spectrum information is not sufficient to provide a good separation between those classes and the remaining ones. In this ill-defined situation, data filtering would not be able to help much. Notice that taking hull differences actually harmed the predictive accuracy of our classifier for these cases. 
The variance of the results is due not only to sample vari-ance, but also to the variance of the underlying classifier, a simplified PC algorithm. Depending on the data selection algorithm, we have also small or big variance on the selected intervals. Figure 2 depicts the number of times each cell was chosen for some of the algorithms on the raw data. Due to its simplicity and reduced number of parameters, the entropy heuristic was the most stable. 
Also, it is interesting to point out that simple algorithms such as the hill climbing algorithms were competitive when compared with the genetic algorithm. Since our data sets were small, computational time was not a major issue, but in applications where a larger number of measurements is performed, they may be viable solutions. 
Under the assumption that cross-validation is a valid mech-anism for estimation of generalization error, these exper-iments can be used to decide which algorithm should be trained in the whole data set in order to be used in a real world application. For example, the genetic algorithm and information gain can be applied to the whole JHU data set and generate one mask for future classification of inosilicates using the raw data, since they have very close accuracy, but are both reasonably superior to the non-filtered data. The model generated from the whole data set would be the final model I . 1One alternative would be to combine masks generated for l~amsey, et a/.[13] discuss extensive experiments analyz-ing the performance measure of different classification algo-rithms, including decision trees that already carry out an entropy-based selection of data. However, none of these re-sults performed better than the baseline for the PC algo-rithm with no wavelength selection depicted in Tables 1 and 2. Our approach can be interpreted as an effective combi-nation of different inductive biases that works better than standard decision trees for the case of entropy selection. 
The techniques applied in this work are related to the areas of feature selection and data cleaning. Wettschereck, Aha and Mohri [17] formulate a framework for feature weight-ing methods under the context of lazy learning. 2 Even though in a strict sense the wavelength channels are in fact rows of our data set, not attributes, in principle one can use these techniques to weight the relevance of each data point (or intervals for practical purposes). According to the categories of Wettschereck et al's framework, the genetic al-gorithm and hill-climbing approaches would be classified as having: 
The performance bias is also commonly described as a wrapper approach [9]: our selection policies use the modi-fied PC algorithm as a black box that outputs a measure of performance. each fold and count the different votes in an independent test set. However, since our main purpose in this paper was to provide a more reliable comparison of different approaches with respect to the baseline achieved by using the whole wavelength range, and our sample of rocks was small, we opted to use the whole sample for cross-validation. 2In this survey, the authors do not compare different batch optimization techniques: among this class of learning algo-rithms, only a gradient-based one is used. 
Unlike general feature selection problems, we do not have the concern of selecting features that present fewer miss-ing values on the available data bases, nor do we have to consider which are more expensive to measure (e.g., some medical exams for diagnosis problems). That makes our fit-ness function even simpler than most ones used in feature selection literature [11, 16, 18]. These approaches are vir-tually identical to the genetic algorithm for data selection described in this work, where the difference is mainly a more complicated evaluation function. Demiroz and Cuvenir [3] also describe mechanisms for learning continuous weights between 0 and 1, which arguably are not very useful for our problem, where we have too little data to accomodate such a precise tuning of parameters. 
In contrast, the information gain heuristic operates as hy-brid between a wrapper and a filter approach. The filter approach applies for each feature a measure of importance that is independent of the learning algorithm that will be used. Hall [8] provides a comparison of filters and wrap-pers, as well as an overview of feature selection. He favors the filter approach due to its much higher scalability, but in his discussion it is mentioned that ideally the features themselves should be a function of the bias of the learning algorithm that will be used. An intermediate approach such as using the entropy measurements to search for a combina-tion of prominent intervals, which can then be successfully used by the modified PC algorithm, is a way to trade-off these issues. 
Entropy measures are commonly related to the degree of unexpectedness of a pattern, and such a characteristic has been explored for data set cleaning. Guyon, Matic and Vap-nik [7] describe different ways of using intbrmation theoret-ical measures to identify outliers or highly informative ex-amples. Data points are ranked according to information gain and then submitted to a expert that will classit~ X  them as outliers or representative examples. Guyon  X t al. warn against the risk of getting improved results during training by dropping t, he most difficult examples and then achieving bad generalization accuracy. 
Another application of information theoretical measures for data clealfing is discussed by Pyle [12], where it is also described how to find ill-defined regions of a function by checking symmetries between the input and output vari-ables. This specially affects inverse function estimators. Pyle also describes what he calls "attention processing" of data: how to efficiently perform data surveying in a large combinatorial space of potentially problematic regions of the data. 
The most important lesson from this study is that signal-processing algorithms can benefit from search procedures that automatically decide which parts of the signal must be taken into account when making a decision. From the initial hypothesis that the behavior observed in carbonates could be replicated by automated procedures, our experiments were able to achieve a similar performance from scratch. An interesting experimental hypothesis is applying a sim-ilar framework for other domains. It must be emphasized that this approach is a complement to other smoothing pro-cedures, as illustrated by running experiments along with the application of hull differences. Even though our main techniques are straightforward, to the best of our knowl-edge there are no experiments exploring similar ideas for this problem. One of the main goals of this work is pointing out simple yet effective alternative approaches of tackling spectroscopic analysis of material composition and possibly component detection problems of other blind source separa-tion domains. 
However, sampling variability may be a concern and the fact that the underlying classifier provides its own source of variability may amplify this problem. Kohavi and George [9] report that feature selection algorithms may over fit easily. Approaches to minimize this problem and perform more reli-able performance assessment include resampling techniques such as bootstrapping [1]. 
For example, it may be possible that more robust masks of selected intervals can be obtained by the combination of different masks. One simple policy is obtaining multiple masks by resampling and then giving to each bin a weight proportional to the number of times each one appears. 
This improved reliability does not come for free, and more computational time is required. For instance, Punch et al. [11] reported experiments with genetic algorithms for fea-ture selection that took 14 days. In this case, one might not want genetic algorithms, since the difference in accu-racy when compared with other approaches may not be great; enough to justify the extra effort. Alternatively, one could just gather more labelled data. For example, the U.S. Geo-. logical Survey has produced a data set of about 400 labelled rocks. However, some of these labels are wrong, or incon-sistent with the classification scheme of the JPL data set. Before combining these data with the JHU data, additional preprocessing would be required. 
Concerning the variability of the underlying classifier, a straightforward way to alleviate this problem is to modify the evaluation function of the search algorithms to consider the outcome of an ensemble of classifiers. Future experi-ments may include this approach. 
We would like to thank Joseph Ramsey for his valuable help during the preparation of this work. 
This work was partially funded by the National Aeronau-tics 8z Space Administration, grant number NAG5-9309. Clark Glymour, Philosophy Department, Carnegie Mellon University and Inst. for Human and Machine Cognition, University of West Florida, email : cg09@andrew, cmu. edu [1] Cohen, P. (1995). Empirical Methods for Artificial [2] Cohen, P. and Jensen, D. (1997). "Overfitting [3] Demiroz, G. ~ Guvenir, H. (1996). "Genetic [4] Friedm~m, J. &amp; Fisher, N. (1999). "Bump hunting in [5] Goldberg, D. (1989). Genetic Algorithms in Search, [6] Grove, C. I., S. J. Hook, and E. D. Paylor II. 1992. [7] Guyon, I.; Matic, N. &amp; Vapnik, V. (1995). [8] Hall, M. (1999). Correlation-based Feature Selection [9] Kohavi, Ron &amp; John, George H. (1998) "The Wrapper [10] Masters, T. (1993). Neural Network Recipes in C++. [11] Punch, W.; Goodman, E.; Pei, M.; Chic-Shun, L.; [12] Pyle, D. (1999). Data Preparation for Data Mining. [13] Ramsey, Joseph; Gazis, Paul; Roush, Ted; Spirtes, [14] Sarle, W. (2000). The Neural Network [15] Spirtes, P., Glymour, C., &amp; Scheines, R. (2000). [16] Vafaie, V. 8z DeJong, K. (1998). "Feature space [17] Wettschereck, D., Aha, D. W., &amp; Mohri, T. (1997). [18] Yang, J. 8z Honavar, V. (1998). "Feature subset 
