 Question-Answering Challenges (QAC) [3] handles interesting topics and con-textual questions in particular. Contextual question answering can be considered to be interactive. Contextual questions are currently defined as a series of ques-tions with contexts. For example, the first question is  X  X hat is the capital of Japan? X  and the one succeeding is related to the first such as  X  X hat was it called in ancient times? X . As we can determine the answers to these kinds of questions, we can automatically evaluate the answers that a question-answering system will output. Automatic evaluation is very important for scientific studies. We participated in NTCIR X  X  QAC-1, QAC-2, and QAC-3, which were evalua-tion workshops for contextual question answering held by the National Institute of Informatics of Japan and studied question-answering systems for contextual questions. Our question-answering system used numerous effective methods to answer questions. We used a method of connecting question sentences to handle contextual questions. We also used a method of using multiple documents to obtain more accurate answers. We then used a special method of handling cases when multiple answers could be correct for a question [2].
 2.1 Prediction of Answer Type The system applies manually defined heuristic rules to predict the answer type. There are 39 of these rules. Some of them are listed here: 1. When dare  X  X ho X  occurs in a question, a person X  X  name is given as the 2. When itsu  X  X hen X  occurs in a question, a time expression is given as the 3. When donokurai  X  X ow many X  occurs in a question, a numerical expression 2.2 Document Retrieval Our system extracts terms from a question by using a morphological analyzer, ChaSen [1]. The analyzer first eliminates terms whose part of speech is a preposi-tion or a similar type; it then retrieves by using the extracted terms. Our system extracts documents where a term occurs near each other. 2.3 Answer Detection Our system first generates candidate expressions to detect the answer from the extracted documents. We initially used morpheme n-grams for the candidate expressions, but this approach generated too many candidates. Instead, we now use candidates only consisting of nouns, unknown words, and symbols. More-over, we used the ChaSen analyzer to determine morphemes and their parts of speech.

Our approach to determining whether a candidate was a correct answer was to add score for the candidate, under the condition that it was near an extracted term, to score based on heuristic rules according to the answer type. The system then selected the candidates with the highest number of total points as the correct answers. (See our paper [2] for the detail.) 2.4 Use of Multiple Documents as Evidence with Decreased Our system also use the method of decreased adding with multiple answers. Let us assume that the question,  X  X hat is the capital of Japan? X , is input into a question-answering system, with the goal of obtaining the correct answer,  X  X okyo X . A typical question-answering system would output the candidate an-swers and scores listed in Table 1. These systems would also output document ID from which each candidate answer had been extracted. The system outputs the incorrect answer,  X  X yoto X , as the first answer for the example in Table 1. We developed a method of using multiple documents with decreased weighting as evidence. The method adds scores by assigning decreasing weights to them. Now suppose that we implement our method by multiplying the score of the i -th candidate by a factor of 0 . 3 ( i  X  1)1 before adding up the scores. In this case, the obtain the results in Table 2. As expected,  X  X okyo X  achieves the highest score. 2.5 Method for Cases Where Multiple Answers Can Be Correct A question can have multiple answers in the QAC test collection. For example, when the question is  X  X hich counties are permanent members of the United Na-tions Security Council? X , there are five answers,  X  X he United States of America X ,  X  X he United Kingdom X ,  X  X ussia X ,  X  X rance X , and  X  X hina X .

We proposed the select-by-rate method for cases where multiple answers can be correct. The Select-by-rate method outputs answers having a score more than a certain rate (rate for selection) of the highest score. For example, when we use 0.9 as the rate for selection and the highest score of answer candidates is 1100, we extract answer candidates with scores of more than 990 as the desired answers.

We also proposed the select-one method , that only extracts the top answer in every case. We also proposed the select-two method , that only extracts the top two answers in every case. 2.6 Method for Cases of Contextual Questions Contextual questions in QAC are asked to answer questions. Contextual ques-tions are defined as a series of questions with contexts. For example, the first question is  X  X hat is the capital of Japan? X  and the one succeeding is related to the first such as  X  X hat was it called in ancient times? X .

We developed a method for cases of contextual questions, where we used the concatenation of all the questions from the first to the current question sentence in which an interrogative pronoun, adjective, or adverb in the first sentence had been changed to dummy symbols (e.g.  X  @@@  X ) as the current question.
For example, when the previous question is  X  X hat is the capital of Japan? X  and the current question is  X  X hat was it called in ancient times? X , the question,  X  @@@ is the capital of Japan? What was it called in ancient times? X  was con-structed, and it was processed by question-answering systems. We refer to this method as Conc1 .

We also used a method of adding answers for previous questions to the ques-tion itself. For example, when the previous question is  X  X hat is the capital of Japan? X  and the current question is  X  X hat was it called in ancient times? X , the answer  X  X okyo X  was extracted [,/from?] the question,  X  @@@ is the capital of Japan? Tokyo . What was it called in ancient times? X  was constructed, and it was processed by question-answering systems. We refer to this method as Conc2 . 3.1 QAC-1 Our results in the Task-3 (the tasks for contextual questions) of QAC-1 at the NTCIR-3 evaluation workshop are in Table 3. The task involved 20 sets of contex-tual questions each of which had two related questions. The average F-measure (MF) was used for evaluation. Our system obtained 0.17 for MF. Our team was in second place of the 7 teams who participated. The score for the best team was 0.19. Although our system was inferior to the best team X  X , another system of ours that changed the method for contextual questions (Conc1  X  Conc2) after the workshop obtained an F-measure of 0.23, which was higher than that for the best team. We could not use a decreased method of adding using multiple documents as evidence in QAC-1, because we developed this after the QAC-1 workshop.
As the test collection for QAC-1 only had a total of 40 questions, the results were not valid. 3.2 QAC-2 Our results in the Task-3 (the tasks for contextual questions) of QAC-2 at the NTCIR-4 evaluation workshop are in Table 4. The task involved 36 sets of con-textual questions each of which had five to ten related questions. There were a total of 251 questions. MF was used for evaluation. The RunIDs in the table indicate the ID numbers for our submitted systems. Our system obtained 0.22 for MF. Our team was in first place of the 7 teams who participated. The MF for the second best team was 0.20. Although there were many questions, the test collection was inconvenient as it could not be used for automatic evaluation or for experiments after the workshop. 3.3 QAC-3 Our results in QAC-3 at the NTCIR-5 evaluation workshop are in Table 5. Only a task for contextual questions was set. It involved 50 sets of contextual questions each of which had five to ten related questions. There were a total of 360 questions. MF was used for evaluation. The  X  X otal X ,  X  X irst X , and  X  X est X  in the table indicate the results for all the questions, those for the first questions in sets of contextual questions, and those f or the remaining questions in the sets. Our system obtained 0.25 for MF. Our team was in first place of the 7 teams who participated. The MF for the second best team was 0.19. We found there were vast differences between our team and the second best team. The MF for  X  X irst X  was higher than that for  X  X est X . This indicates that the first questions in the sets of contextual questions were relatively easy and the other questions were difficult. To compare Systems 1 and 3, we found that System 3, which used Conc2 to add answers in the previous questions to the current question, obtained a lower MF than System 1, which did not use Conc2. We found that Conc2 was not effective in the QAC-3 test collection.

Reference run 1 data were given in QAC-3, and experiments using these data were conducted. Questions in the data were transformed into normal questions by supplementing omitted expressions (e.g., pronouns). For example, when the previous question was  X  X hat is the capital of Japan? X  and the current question was  X  X hat was it called in ancient times? X , the current question was trans-formed into  X  X hat was Tokyo called in ancient times? X  and it was used to answer questions. We could estimate how difficult a contextual question was by comparing the experimental results for contextual questions with the results for the reference run data. The results are in Table 6. We found that the MF for the reference run data was higher than that for contextual questions. This meant that answering questions in the referen ce run data was easier than answering contextual questions. By comparing the results for  X  X irst X  and  X  X est X  in the reference run data, we found that their MFs were very different and questions other than the first questions were more difficult than the first questions.
Our method for contextual questions was simple and connected to previous questions. It obtained 0.75 (= 0.218/0.292) of the MF for reference run 1 data.
Reference run 2 data were given in QAC-3, and experiments using these data were undertaken. Questions in the data exactly equaled the original question, but we could not use the previous questions. For example, when the previous question was  X  X hat is the capital of Japan? X  and the current question was  X  X hat was it called in ancient times? X , we could only use the current question  X  X hat was it called in ancient times? X . The results are in Table 7. The MFs were very low. The MF was 0.099 (Table 7) when we did not use a simple connecting method. We obtained 0.14 (= 0.02/0.292) of the MF for reference run 1 data, while the simple connecting method obtained 0.75 (= 0.218/0.292) of the MF for reference run 1 data, indicating that our simple connecting method was effective.
The method of decreased adding obtained an MF of 0.250 and without it we obtained an MF of 0.197 (Table 5). This indicated that our method of decreased adding was effective.

The method of select-by-rate adding obtained the best MF when the rate for selection was 0.90.

As QAC-3 had numerous questions, and the test collection could be used for automatic evaluation and for experiments after the workshop, it was convenient and useful.
 Our question-answering system used many effective methods to answer ques-tions. We used a method of connecting question sentences to handle contextual questions. We used another method of using multiple documents to obtain more accurate answers. We used a special me thod for handling cases when multiple answers could be correct for a question. Our question-answering system obtained the second best accuracy in QAC-1 and the best accuracy in both QAC-2 and QAC-3 for contextual question answering. It is thus a high-performance system.
