 Ulrich R  X  uckert RUECKERT @ IN . TUM . DE Stefan Kramer KRAMER @ IN . TUM . DE chine learning. One of the main reasons for the popularity venient representation of regularities and interrelations for humans. Many learning settings can be adapted to allow for the application of rule learning systems. Usually, the goal in rule learning is to find a set of rules that has high predictive accuracy and that is as small as possible. His-torically, most research on rule learning has concentrated on separate-and-conquer approaches, often combined with sophisticated pruning and postprocessing methods to avoid overfitting. While there is a lot of empirical evidence show-ing that those approaches work very well in practice, it is very hard to investigate them analytically. This is due to the combinatorial complexity inherent in rule learning and the fact that the impact of pruning and postprocessing meth-ods on predictive accuracy is hard to estimate a priori. In contrast, decision trees have been the subject of many em-pirical and theoretical investigations (e.g. (Breiman, 2001; Golea et al., 1998; Mansour &amp; McAllester, 2000)). The main goal of this work is to design a rule learning system that is not only competitive with established rule learning accessible to an analytical investigation. In particular, we would like to give non-trivial upper bounds on the predic-tion error of the proposed algorithm.
 learning settings. If a data set features many (possibly re-lated or noisy) continuous attributes, a conjunction of liter-als of the form  X  X ttribute &lt; value X  allows only for a rough and angular separation of the instance space. In those cases it is usually more sensible to use smooth separating func-tions as in Support Vector Machines or Neural Networks. In this work we restrict ourselves therefore to nominal-valued data sets. Another problem is that pure rule learn-ing algorithms tend to be unstable , in the sense that a small change of the training set often leads to a large change in the induced rule set. This is due to the combinatorial nature of rule learning. The instability can increase the variance component of the prediction error. We therefore use en-sembles of rule sets to keep the variance low and improve predictive accuracy.
 The paper is organized as follows: after introducing the learning setting in section 2, we describe a novel rule learn-ing system in sections 3 and 4. In section 5 we investigate the system theoretically, and then present empirical results in section 6. To give a precise description of the learning algorithm and the underlying theory, we need to introduce a few defini-tions and state basic assumptions. First of all, training and test data are given as sets of labeled instances , taken from an instance space X . The instance space is structured as the cartesian product of n domains A is represented by an n -tuple ( x x cretized in order to fit into our framework. Furthermore, unknown distribution D on the pairs ( x, y )  X  X  X Y . A training set S is generated by drawing m labeled instances ( x, y ) independently according to D . The learning algo-rithm is given the sample S . Upon termination it outputs a concept c that maps instances to classes. The main goal, of course, is to come up with an algorithm that generates a concept c with provably low error Pr Since we are working in the field of rule learning, we use a particular representation for the concepts to be learned. First of all, a literal is of the form ( a for any 1  X  i  X  n , where a v is a value taken from the corresponding domain A conjunction of literals is a rule . A rule is said to cover an instance, if the conditions imposed by all literals in the rule are met by the instance. Not surprisingly, a rule set is a set of rules. A rule set covers an instance, if any rule in a labeled ruleset r class label. We can use a labeled rule set r class r The class of all possible y R . In the next section we deal with the problem of finding rule sets that agree with the labeled instances of a training set. One can easily transform any learning problem with nom-inal attributes into an equivalent problem containing only boolean attributes. If we do so, the problem of finding rule sets of a fixed size with low error on a given training set be-comes the optimization version of the k -term DNF learning problem : Given  X  a set of n attributes A 1 , . . . , A n , where all  X  a training set of m labeled instances  X  a natural number k Find a +1-labeled rule set r containing exactly k rules over the literals on A fied instances  X  indicator function).
 Unfortunately this problem is known to be NP-hard use an approximation algorithm to keep the computational costs of learning low. The set covering approach, that is taken by most traditional rule learning systems, does not allow to restrict the size of the induced rule set and is there-fore not suited for our purposes. Instead, we use a stochas-tic local search (SLS) approach (Hoos, 1998). SLS algo-rithms differ from other approaches in that they perform a local, random walk search. In its basic incarnation, an SLS algorithm starts with a randomly generated solution candi-date. It then iterates in a two-step loop: in the first step it examines a fixed set of  X  X eighboring X  candidates according to some predefined neighborhood relation. Each neighbor is evaluated according to a global scoring function. In the second step the SLS algorithm selects the neighbor with the best score as next candidate. Such a greedy hill climb-ing approach is obviously susceptible to getting caught in local optima. Most SLS algorithms therefore select with a fixed probability p a random neighboring candidate in-stead of the candidate with the highest score. In this way they can escape local optima through random steps. In the following we describe an SLS algorithm for k-term DNF learning, that has been shown to work well on hard learn-ing problems (R  X  uckert &amp; Kramer, 2003).
 For k-term DNF learning, the score to be minimized is the number of misclassified instances. The SLS algorithm starts with a random rule set r of k rules. It then randomly picks a misclassified instance x . Assume x is labeled +1, but not covered by r . Thus, r is obviously too specific: we have to remove at least one literal in order to let r cover Consequently, the algorithm generalizes the rule t in r that differs in the smallest number of literals from x . It gen-erates for each literal l by removing l lowest score is the new candidate to be further optimized. If the randomly chosen instance x is labeled -1, but erro-neously covered by r , the current rule set r is too general. Let t be the rule that covers x . We have to add a literal to in order to make x uncovered. Again we can generate a set of neighbors by adding one literal to t and then choose that neighbor whose score is the lowest. In order to escape lo-cal optima, the algorithm replaces each decision step with a random decision from time to time. Algorithm 1 sketches the idea. A more elaborate description of the algorithm can be found in (R  X  uckert &amp; Kramer, 2003). Learning algorithms inducing individual rule sets are unsta-ble in the sense that small perturbations in the training data Algorithm 1 An SLS algorithm for k-term DNF learning. k is the number of rules per rule set. Global variables taking default values: maxSteps specifies the maximal length of search, p steps. procedure SLSearch( k ) end procedure Algorithm 2 The rule learning algorithm. S is the training set, k number of rule sets per ensemble and rule set size k , and is the noise probability. procedure LearnEnsemble( S , k end procedure may result in large changes in the rules. In order to avoid unstable behavior, we now consider learning ensembles of rule sets. Algorithm 2 learns one ensemble per class. The rule sets in the ensemble are sampled using the SLS algo-rithm. Since we do not know the the optimal number of rules per rule set k in advance, we sample a constant num-ber of rule sets for each k up to a user-specified limit k Of course, the sampled rule sets differ in their ability to ex-plain the training set. It makes sense to keep, along with each rule set, a probability Q ( r ror on the training set. If the user has information about the noise behavior of the underlying target distribution, she can use this information to generate a matching noise model and generate Q accordingly. If, for example, the noise level is higher for some parts of the instance space X , one would want to base Q ( r if a rule set misclassifies only  X  X oisy X  instances, a higher fies relatively stable instances. As a conservative default strategy we use a Q that assigns a probability that is expo-nentially decreasing with its empirical error on S to each r : where  X  l ( r ple S (cf. section 5), and  X  specifies the rate of decay. This corresponds to a white noise model, where the error prob-ability is constant  X  := exp(  X   X /m ) across all instances. Similar models have been studied, e.g., in (Littlestone &amp; Warmuth, 1994).
 The learning algorithm should also be able to handle multi-class problems directly. Up to now, we only consider the case of two-class problems. Dealing with multi-class prob-lems has traditionally been problematic for rule learning systems. For the sake of simplicity, we employ a sim-ple one-vs-all scheme here: we learn p ensembles Q where ensemble Q and Y \{ y how certain an ensemble is about its prediction (cf. section 5.2), it makes sense to choose that class y score c ( Q In the following, we will present a theoretical analysis of the above algorithm. The goal is to bound its expected pre-diction error. In the first part, we show how McAllester X  X  PAC-Bayesian theorem can be used to bound the error in ensemble rule learning. In the second part, we prove that the PAC-Bayesian bound can be further improved by al-lowing the learner to abstain from uncertain predictions. Together, these results are more generally applicable, but they serve well the purpose of bridging the gap between theory and practice for rule learning. 5.1. The PAC-Bayesian Bound for Rule Learning Two-class problems are generally easier to handle than multi-class problems. We therefore use characteristic func-tions  X  let Y be a set of p class labels. Just like r defined to be +1, if y = y mate the rule set X  X  predictive behavior we are especially interested in the cases where the class y rule set disagrees with the  X  X rue X  class y of an instance x . To measure the rate of misclassification, we introduce a loss function : let ( x, y ) be a labeled instance drawn ac-cording to D , and r l ( r i , x, y ) := I( r i ( x ) 6 =  X  i ( y )) with the  X  X rue X  class y , and 1 otherwise. Intuitively, l 0-1 loss for measuring whether the prediction of a rule set agrees with an observation drawn from D , when we are only distinguishing between class label y ing class labels.
 The expected loss l ( r how often the rule set r on average. Given a sample S of size m , the empiri-cal loss  X  l ( r tion of instances in S that are misclassified by r PAC-Bayesian theorem deals with (prior and posterior) distributions on the space R Q is an arbitrary probability measure on R gle rule sets:  X  l ( Q, S ) := E E distributions, we need the well known Kullback-Leibler di-vergence, denoted by D ( Q k P ) :=  X  As a notational shortcut we write  X   X  S  X ( S ) instead of Pr S  X  D [ X ( S )]  X  1  X   X  to express that  X  holds for all but a fraction  X  of the cases. With this we can state the PAC-Bayesian theorem: Theorem 1 (PAC-Bayesian, (McAllester, 1999)). Let y i  X  Y space of y
B ( Q, P, m,  X  ) := Then, where Q ranges over all distributions on R This theorem can be used to upper-bound the expected er-ror of an ensemble classifier in the following way: the user provides a prior distribution P she has some information about which rule sets are most likely to resemble the  X  X rue X  target distribution D , she can use this information by selecting a P probabilities to those rule sets. If she does not have any such information, she can simply select an uninformative, flat prior. In the next step she draws a sample S of size m from D . It is now the task of the learning algorithm to select a distribution Q algorithm aims at finding Q side of the inequality and thus provides a tight upper bound on the expected error of rule sets drawn according to Q Using those Q construct a voting ensemble of weighted rule sets, and bound its generalization error. Let  X  Q := ( Q Then: c ( Q, x ) is the score of Q on x and c V (  X  Q, x ) denotes the ing classifier is thus l y )] . The following theorem bounds the expected error of the voting classifier for two-class problems, i.e.  X  ( Q in a similar fashion.
 Theorem 2. For i  X  { 1 , 2 } let y be distributions over the spaces of y let  X  &gt; 0 , and  X  Q = ( Q  X 
S  X  Q l V (  X  Q )  X  B ( Q 1 , P 1 , m,  X  ) + B ( Q 2 , P Proof. First, observe that for  X  Q = ( Q l V (  X  Q ) = Pr Additionally, 1  X  2 l ( Q i ) = 1  X  2 E (6) uses the fact that I( a 6 = b ) = 1 { X  1 , +1 } , (7) uses a 2 = 1 for a  X  { X  1 , +1 } . It follows from theorem 1: Now, let C := 2  X   X  random variable. Since C  X  0 for all x, y, Q apply Markov X  X  inequality: By definition of C ,  X   X  &gt; 0 : Pr 2  X  2  X  +  X  E [  X  1 ( y ) c ( Q 1 , x )]+  X  E [  X  2 ( y ) c ( Q and because of (8)  X   X  &gt; 0  X   X  S : Pr 2  X  2  X  ( B ( Q 1 , P 1 , m,  X  ) + B ( Q 2 , P 2 , m,  X  ))  X  The theorem follows from (5) by setting In order to keep this bound as tight as possible, one would like to find Q S and that differ from the P this may be possible theoretically, such a  X  X AC-Bayesian-optimal X  algorithm would require calculating the empiri-cal error of all possible concepts in the underlying concept class. Of course, this is not practical for our purposes, be-cause the space of rule sets is way too large to be evaluated exhaustively.
 We try to keep the computational costs within a reason-able range by taking two measures: first, we limit the max-imum size of the rule sets to be considered. This is neces-sary anyway, because rule sets of arbitrary size can repre-sent all possible dichotomies. If a flat prior is used and we choose Q only depending on the training set, this is effec-tively bias-free learning, which is known to be equivalent to rote learning. Thus, we restrict our concept space to the set of all rule sets with at most k bias towards short hypothesis is motivated by the principle of William of Ockham and  X  in one form or the other  X  included in virtually any existing rule learning algorithm. Second, instead of considering all possible rule sets for a Q , we sample a small number of rule sets from R the probability measure Q the sample. In this way, we have to deal with only a rather small number of concepts; the calculation of c ( Q volves only the summation over the few rule sets with non-zero Q expected error becomes feasible.
 Unfortunately, it is a bad idea to sample uniformly or ac-cording to P empirical error  X  l on the training set. Thus, if the sampled rule sets have a high error on the training set, the bound will be loose. It is therefore essential that we select rule sets with low empirical error. As stated above, we employ an SLS algorithm that finds DNFs with low empirical error in a randomized fashion to achieve this goal. 5.2. Improving the Bound Through Abstaining Most rule learning systems are designed to assign a class to any instance that was input for classification. In practice, though, it is often the case that some instances clearly be-long to one class, while other instances are just in between two classes or particularly susceptible to noise. A classifier that abstains from classification for instances of the latter kind might feature a much higher predictive accuracy, be-cause it avoids errors on uncertain predictions. Sometimes, abstaining can give important hints to the user, e.g. about the existence of a previously unknown class label. These considerations lead to a different approach for get-ting tighter bounds: allowing the classifier to abstain from a classification for uncertain instances. In our case we can assess the deviation of the votes in an ensemble as a mea-sure of how certain the corresponding prediction is. If all rule sets in an ensemble vote for the same class label, the weight of the rule sets that vote for y small margin from the weight of the rule sets that vote for a different y Thus, it might make sense to abstain from a classification, if the absolute value of the margin is lower than a certain threshold  X  &gt; 0 . For the two-class setting, the following definition gives the abstaining voting classifier c  X  the concept can be easily extended to the multi-class set-ting. c The expected error of this abstaining classifier is l (  X  Q ) := E The following adaption of theorem 2 improves the PAC-Bayesian bound for an ensemble  X  Q , if the abstaining voting classifier is used instead of the voting classifier. Theorem 3. Let the y above, let  X  &gt; 0 . Then:  X  S  X   X  Q l  X  V (  X  Q )  X  Proof. The result follows from (11) and by setting in (10). In this section we describe an empirical evaluation of the presented rule learning algorithm. The goal of the first experiment is to investigate how the presented algorithm compares to modern rule learning algorithms. To get re-sults on learning problems with varying characteristics, we select 34 data sets from the UCI repository (Blake &amp; Merz, 1998). Since the presented rule learning algorithm works only on nominal attributes, we discretize continuous at-tributes using a frequency-based discretization with ten in-tervals. If a data set contains unknown values, we simply add a new value  X  X nknown X  to the corresponding domains, so that unknown values are treated just like any other value. To estimate the predictive accuracy of the algorithms we averaged over ten runs of tenfold cross-validation. The presented algorithm was set up to build rule sets with up to eight rules per rule set, and we set the o parameter to twenty rule sets per level. To calculate the Q ( r ) probabili-ties, we chose the  X  X hite noise X  model described in section 4 with the noise parameter  X  set to 0.9. The SLS algo-rithm was set up to search for 5000 iterations, with p spectively. We compare the results for the presented al-gorithm with the results of a support vector machine with RBF kernel and two state-of-the art rule learning systems. PART (Frank &amp; Witten, 1998) is a separate-and-conquer-based rule learning algorithm, that avoids over pruning by obtaining rules from partial decision trees. JRIP (an imple-mention of Cohen X  X  RIPPER (Cohen, 1995) in the WEKA workbench (Witten &amp; Frank, 1999)) combines separate-and-conquer with incremental reduced error pruning and an iterated post-processing optimization step. To include ensemble-based approaches, we estimated the predictive accuracy of twentyfold-bagged versions of the two algo-how different methods compare to each other. Each en-try indicates the number of data sets for which the method associated with its row is significantly more accurate than the method associated with its column according to a paired two-sided t-test on a 1% significance level over the runs. As can be seen, the presented algorithm performs favorably. It SVM 16 16 10 11 9 PART 18 16 2 5 7 JRIP 14 9 0 1 4 Bagged PART 22 28 27 14 14 Bagged JRIP 19 22 26 6 7 Rule Learn 22 23 24 11 16 clearly outperforms SVM, PART, JRIP, and Bagged JRIP, and is slightly worse than Bagged PART.
 For the second experiment, the main goal is to investigate the gap between the PAC-Bayesian bound and the true er-ror as estimated by tenfold cross-validation and to com-pare our results with related approaches. Multi-class prob-lems require the application of the union bound on the PAC-Bayesian bounds for the p ensembles, so the result-ing bound is rather loose. We therefore focus on two-class problems. We apply the presented algorithm with the same parameters as above and a flat prior P to a selection of two-class problems taken from the UCI repository (Blake &amp; Merz, 1998). To the best of our knowledge, there are no comparable results on theoretical bounds for rule learn-ing systems in the literature. The closest approaches in the literature are SLIPPER (Cohen &amp; Singer, 1999), LRI (Weiss &amp; Indurkhya, 2000), and the Set Covering Machine (Marchand &amp; Shawe-Taylor, 2001; Sokolova et al., 2003; Marchand et al., 2003). SLIPPER and LRI are rule learning algorithms based on ensembles of individual rules instead of rule sets. Since they employ voting schemes, they are amenable to theoretical analysis and also would be able to abstain from predictions. However, standard approaches to bounding the error applied to SLIPPER and LRI give rather loose bounds. Like a rule learning system, the Set Cover-as concepts. However, unlike rule learners, it disjunctively joins data-dependent features such as generalized balls and half-spaces instead of conjunctions of literals. Marchard et al. derive a bound based on a compression scheme, that can be compared to the PAC-Bayesian bound. They report em-pirical and analytical results for a whole range of parameter settings. In table 3 we reproduce the values for two par-ticular settings: the  X  X onsistent X  columns give the results for the unparameterized version of the data-dependent ball SCM, which induces only consistent classifiers. The  X  X im-ple SCM X  is able to derive inconsistent classifiers, but the results are given for experiments that use only the best pa-rameter settings among an exhaustive scan of many values for each data set. Those values are thus much more opti-mistic than our results, which are based on default param-eter values that are fixed for all data sets. Nevertheless, the PAC-Bayesian bound is better in five out of seven cases, and the presented algorithm achieves a lower prediction er-ror in five out of the seven cases.
 We also performed preliminary experiments with abstain-ing ensembles of rule sets. To test the validity of this ap-proach empirically, we estimate the prediction error of the abstaining Bayes algorithm on the Haberman data set using tenfold cross-validation. We used the same parameters as before, but varied  X  between 0 and 2. Figure 1 shows the difference between bound and estimated error. As can be seen, the relative accuracy of the bound is optimal for val-ues of  X  near 1. Thus, the bound can in fact be improved on this dataset for a certain level of abstinence. This paper showed that the empirical error of a practical rule learning algorithm can be bounded theoretically by applying McAllester X  X  PAC-Bayesian theorem. We proved that the PAC-Bayesian bound can be further improved by allowing the model to abstain from making uncertain pre-dictions. A preliminary experiment also indicates empiri-cally the benefit of abstaining from uncertain predictions. Based on these theoretical considerations, we designed a new algorithm learning ensembles of rule sets. The per-formance of the algorithm on standard UCI datasets com-pares very favorably with state-of-the-art rule learning al-gorithms and their bagged variants. Experiments showed that the calculated bounds are reasonably close to the em-pirical error estimated in tenfold cross-validation. In most cases the ratio of the bound to the empirical error is smaller than for the Set Covering Machine, for which one of the tightest bounds is known. It should be noted that the bound (and with it the algorithm X  X  predictive accuracy) can be im-proved in various ways: one can (1) provide an informative prior instead of a flat prior, (2) use a matching noise model instead of the white noise default, (3) increase the ensem-ble size, or (4) allow for abstaining. Thus, the algorithm can be easily adopted to a particular setting in the presented framework. Moreover, it should be possible to extract com-prehensible consensus rules and statistics over frequently used features and feature combinations along the lines of Pfahringer et al.  X  X  work on ADTrees (Pfahringer et al., 2001). Overall, we hope that this work helps narrowing the gap between theory and practice for rule learning, one of the most important practical machine learning schemes.
