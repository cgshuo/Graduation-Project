 Collaborative prediction involves filling in missing entries of a user-item matrix to predict preferences of users based on their observed preferences. Most of existing models assume that the data is miss-ing at random (MAR), which is often violated in recommender sys-tems in practice. Incorrect assumption on missing data ignores the missing data mechanism, leading to biased inferences and predic-tion. In this paper we present a Bayesian binomial mixture model for collaborative prediction, where the generative process for data and missing data mechanism are jointly modeled to handle non-random missing data . Missing data mechanism is modeled by three factors, each of which is related to users, items, and rating values. Each factor is modeled by Bernoulli random variable, and the observation of rating value is determined by the Boolean OR operation of three binary variables. We develop computationally-efficient variational inference algorithms, where variational param-eters have closed-form update rules and the computational com-plexity depends on the number of observed ratings, instead of the size of the rating data matrix. We also discuss implementation is-sues on hyperparameter tuning and estimation based on empirical Bayes. Experiments on Yahoo! Music and MovieLens datasets confirm the useful behavior of our model by demonstrating that: (1) it outperforms state-of-the-art methods in yielding higher pre-dictive performance; (2) it finds meaningful solutions instead of undesirable boundary solutions; (3) it provides rating trend analy-sis on why ratings are observed.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; I.2.6 [ Artificial Intelligence ]: Parameter learning Collaborative filtering; non-random missing data; probabilistic mod-els; recommender systems; variational Bayesian inference
Collaborative prediction is a popular technique used in recom-mender systems, the task of which is to fill in missing entries of a user-item matrix given a small set of observed entries, to predict preferences of users based on their observed preferences. Suppose that we are given a set of observed entries, X  X  = { X ij  X  , 1  X  i  X  I, 1  X  j  X  J } , where X ij  X  { 1 ,...,V } repre-sents a value of rating by user i on item j , and  X  is the set of ( i,j ) pairs for which the corresponding X ij is observed. Then, collaborative prediction involves inferring X  X  c given X X
The real-world collaborative prediction problem is challenging computationally as well as statistically, since the user-item matrix is extremely large and sparse. For example, in the Yahoo! Mu-sic dataset [2], there are 1000990 users and 624961 items, but only 262810175 observed ratings are available, which constitutes 0.0042 percent of X . In addition to size and sparseness problems, the collaborative prediction is more than completing a matrix, be-cause of non-ignorable missing data mechanism , which is a main concern of this paper.

It follows from the theory of missing data that the mechanism for missing data are divided into three cases: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) [7]. MCAR is the most restrictive assumption, where the probability that X ij is observed does not depend on the data in any way. Most of theories developed for matrix comple-tion assume MCAR, where missing or non-missing is decided by coin-flips, regardless of index ( i,j ) and rating value [1]. However, the MCAR assumption is easily violated in collaborative predic-tion problems. For example, some users rate items more actively than others and some items are rated by many users while others are rarely rated. MAR is the case where missing or non-missing depends on the index ( i,j ) but does not depend on rating value X ij . More precisely, in the MAR assumption, the probability that a data point is observed is related to particular variables but is not related to the values of those variables. Most of previous methods for collaborative prediction have been developed, assuming MAR. They include theory on weighted trace norm regularization for non-uniform sampling [18, 3], probabilistic models such as URP [8] and RBM [17], and Bayesian matrix factorization (BMF) [5, 15, 16, 21, 14, 4].

When the data is MAR, the missing data mechanism can be ig-nored and parameter estimation or inferences are unbiased. How-ever there is a strong evidence that MAR is an incorrect assumption for collaborative prediction [11]. Intuitively, the MAR assumption is easily violated in recommendation system if, for instance: (1) users rate items they like more than ones they dislike; (2) users pro-vides extreme ratings, that is, they only provide feedback particu-larly good or bad items. In these examples, dependencies between the missing data values and observation process exist, hence the data is not MAR. When the MAR assumption does not hold, infer-ences are biased, leading to the degraded prediction performance.
An approach to dealing with MNAR data is to learn jointly a complete data model that explain how the data is generated, and a missing data model that explains the observation process for the data [7]. In this paper, we choose a binomial mixture model as the complete data model for the ordinal rating data matrix and propose a novel missing data model that explains the reason of observation as the function of three factors, each of which related with users, items, and rating values. Each factor is modelled by Bernoulli ran-dom variable, and the observation of rating value is determined by the Boolean OR operation of three binary variables. Because of the property of OR operation, one of three factors can override others, hence it naturally models the 80-20 rule, which commonly arise in the recommendation systems.

The combination of binomial mixture model and our novel miss-ing data model makes possible to develop computationally effi-cient variational inference algorithms, where variational parame-ters have closed-form update rules and the computational complex-ity depends on the number of observed ratings, instead the size of the rating data matrix. In fact for this reason, we select the bino-mial mixture model as the complete data model instead of widely used matrix factorization model. We also discuss implementation issues on hyperparameter tuning and estimation based on empirical Bayes.
 Experiments on Yahoo! Musing ratings for User Selected and Randomly Selected Songs dataset show that our model outperforms the state-of-the-art method for MNAR data, which is a combination of multinomial mixture model and CPT-v+ missing data model [11, 9, 10]. Our model finds meaningful solution instead of undesirable boundary solution (see Figure 2-(d)), if hyperparameters are care-fully estimated by empirical Bayes method. In addition to higher predictive performance on MNAR data, our model provides rating trend analysis on why ratings are observed. Preliminary experi-ments on Yahoo! Music and MovieLens datasets show that our model can capture the different rating trend between two domains: song and movie. Throughout the paper, we use notations described in Table 1. We introduce a companion matrix of response indicator R , where R ij = 1 if X ij is observed, and R ij = 0 if X ij is missing. Fol-lowing the missing data theory [7], we formulate the non-ignorable missing data mechanism as a two-step procedure. First a complete data model with parameter  X  , p ( X |  X ) , generates the complete data matrix X = X  X   X  X  X  c . Then a missing data model with parameter  X  , p ( R | X ,  X ) , determines which elements in X are observed. Hence, we can take a factorized joint distribution for X and R given  X  and  X  :
Most prior researches on collaborative prediction focus on the estimation of the complete data model p ( X |  X ) , and usually ig-nore the missing data model p ( R | X ,  X ) . Important exceptions are Marlin X  X  works, where the multinomial mixture (MM) model is combined with two different missing data models called CPT-v and Logit-vd [11, 9, 10]. The first, CPT-v is a simple missing data model where the probability of observing a rating depends only on the underlying rating value. This model can capture the intuition that a user X  X  preference for a particular item can influence whether the user rates that item. The CPT-v missing data model is param-Bin ( x | p,N ) p : success probability, Dir (  X  |  X ,K )  X  : concentration parameter, Beta ( x | a,b ) Beta distribution, a,b : shape parameters.
Bern ( x | p ) Bernoulli distribution, p : success probability. eterized by using a conditional probability table consisting of V Bernoulli parameters  X  = [  X  1  X  X  X   X  V ] (hence the name CPT-v). The parameter  X  v gives the probability that an item will be rated if its true rating value is v : p ( R ij = 1 | X ij = v ) =  X 
The CPT-v model makes the very strong assumption that a single value-based selection effect is responsible for generating all miss-ing data. The second, Logit-vd is more flexible missing data model because it allows the probability of observation to depend both on the underlying rating value and the identity of the item. The Logit-vd model specifies a logistic form for this relationship as: where  X  v models a non-random missing data effect that depend on the underlying rating value, and w j is a per-item bias. This parameterization is more suitable than simple factorizations, such as product of Bernoulli probabilities because (2) can model 80-20 rule which commonly arises in the recommendation system. In practice, some items are rated by many users, while others are rarely rated. The Logit-vd model can ac-count for this by setting to  X  j to a large value. The logistic com-bination rule (2) allows  X  j to override  X  v , and produces a high selection probability for these extremely popular items.
Compare to the widely used matrix factorization model, the MM model is less flexible as the complete data model. However this simplicity makes possible to develop computationally efficient learn-ing algorithm based on the expectation maximization (EM), when it is combined with CPT-v or Logit-vd missing data model. In ad-dition, experimental results on Yahoo! music dataset showed that the MM model with these missing data models significantly out-performs other collaborative prediction models based on MAR as-sumption.
Although Marlin X  X  works opened the door for collaborative pre-diction with non-random missing data and showed promising re-sults, several problems still remain unsolved. At first, when MM model and missing data model are jointly learned, the EM algo-rithm converges to undesirable boundary solution, where almost all of the missing rating data are predicted to the value 2, as shown in Fig. 2-(d). In fact the best performing model, referred to MM/CPT-v+ in [9], was obtained by an unnatural way, where only parameters for the MM model is learned by EM algorithm, while parameters for the CPT-v model is fixed to optimal values which is estimated by using a held-out dataset. Interestingly joint learning of complete data model and missing data model was con-verged to undesirable boundary solution even strong informative priors on  X  were given by using  X   X  [11, 9]. Secondly generalized EM algorithm for Logit-vd does not have close-form update rules, hence requires careful tuning of learning rate for gradient assent updates and a large number of iteration for convergence (more than 5000 iteration). In addition, although Logit-vd showed better pre-dictive performance than CPT-v [10], it is still significantly worse than CPT-v + [9].

In this paper, we thoroughly try to solve the aforementioned problems. As a part of solution, we choose the binomial mixture model for modelling rating data because it takes account of the or-dinal nature of rating values, which is ignored in the MM model. The idea of using the binomial distribution for modelling ordinal nature of rating values can also be found in binomial matrix factor-ization model [20], but it is based on the MAR assumption.
There are also several works which consider MNAR rating data in the widely used matrix factorization framework. Instead of ex-plicitly modelling missing data model, AllRank model optimizes a ranking-based metric that is robust to MNAR data [19]. The re-sulting method can be applied to recommendation, but not to rating prediction task.

Response aware probabilistic matrix factorization model [6] com-bined probabilistic matrix factorization with missing data model which is similar to [11, 10]. Although this model is based on pow-erful matrix factorization, it has several weak points: (1) there is no closed form update rules; (2) careful tuning of learning rate and other meta parameters is required, hence reproducibility is bad; (3) the computational complexity is high, it depends on data matrix size instead of the number of observed ratings. In our empirical study, reported RMSE of this model on the randomly selected Ya-hoo! music dataset is significantly worse than MM/CPT-v+.
Another variation of probabilistic matrix factorization for MNAR rating data is specialized to implicit (a.k.a binary or one-class) feed-back case [13], where Bayesian generative process for implicit rat-ing data is proposed. It models the like probability by interpreting the missing signal as a two-stage process: firstly, by modelling the odds of a user considering an item, and secondly, by eliciting a probability that the item will be view or liked. In this paper we model the explicit MNAR rating data, and it is harder than implicit one because common users usually rate on the good item but some-times will rate on the bad item to express their strong dissatisfac-tion.
We present our Bayesian binomial mixture model for collabora-tive prediction with non-random missing data, which is referred to as Bayesian-BM/OR in this paper. The graphical model for Bayesian-BM/OR is shown in Figure 1.
 Figure 1: A graphical model for binomial mixture with our non-random missing data model. Blank and hollow circles de-note unobserved and partially observed variables respectively. Note that if X ij is observed, then U ij ,M ij ,T ij are unobserved, and vice versa.
The generative process of the rating data matrix and missing data mechanism are as follows. 1. Choose the number of clusters K . 2. Choose mixing proportions  X   X  Dir (  X  |  X  0 ,K ) , 3. Choose parameters for binomial distributions 4. For each user i  X  X  1 ,...,I } 5. Choose per-user, per-item, and per-value parameters for miss-6. For each ( i,j )  X  X  1 ,...,I } X { 1 ,...,J }
We choose a binomial mixture model as the complete data model for the rating data matrix because (1) the mixture model can cap-ture the simple intuition that users form clusters according to their preferences for items; (2) the binomial distribution is suitable for modelling discrete, finite, and ordered rating values; (3) combined with our missing data model, it allows the efficient learning algo-rithm, of which time and space complexity depend on the number of observed rating instead the data matrix size.

We consider following conditions for the missing data model: (1) user activity, item popularity, value-based selection effect should be modeled; (2) one of these factors can override other factors such that the 80-20 rule can be well modeled; (3) combined with bi-nomial mixture model, the computationally efficient learning algo-rithm should be derived.

One possible solution will be a extending the Logit-vd model by simply adding a per-user bias  X  i : p ( R ij = 1 | X ij = v ) =  X  ijv = 1 However unlike CPT-v and Logit-vd, the parameterization (5) does not allow computationally efficient learning algorithm. For exam-ple, the complexity for computing a log-likelihood depends on the size of rating matrix IJ , not on the number of observed rating H : log p ( R , X  X  ) = log p ( R  X  , X  X  ) + log p ( R log p ( R  X  , X  X  ) = X where P kjv = p ( X ij = v | Z ki = 1) . Note that sum operations are involved with all entry ( i,j )  X   X   X   X  c .

Instead of the logistic combination rules, we present a novel missing data model which fulfils all above-mentioned conditions. We assume that for each entry ( i,j ) , the observation of rating value X ij is determined by the Boolean OR operation of three binary random variables U ij , M ij , and T ij , each of which is related with users, items, and rating values: R ij = U ij  X  M ij  X  T ij = 1  X  (1  X  U ij )(1  X  M ij )(1  X  T In addition, we assume that these binary random variables follow the Bernoulli distribution conditioned on X ij : More precisely speaking, U ij , M ij , and T ij are conditioned on the row index, column index, and value of X ij . With our missing data model, the probability of observation is computed by
Our missing data model naturally allows that one of three factors can override the rest, because of following inequality: A behaviour of probability of observation under various combina-tion of (  X  i , X  j , X  v ) is illustrated in Table 2.
Note that there is a interesting reverse observed/missing rela-tionship between X ij and ( U ij ,V ij ,R ij ) . If X ij R ij = 1 ), we only know that at least one of U ij ,M ij ,T hence they are latent variables. However if X ij is missing (i.e. R ij = 0 ), then U ij = M ij = T ij = 0 , consequently they are ob-served variables. Hence the set of of observed variables is defined by D = { X  X  , U  X  c , M  X  c , T  X  c } and the set of latent variable is defined by Z = {  X  ,  X  ,  X  ,  X  ,  X  , X  X  c , Z , U  X  , M  X 
We approximately compute posterior distributions over latent variables by maximizing a lower-bound on the marginal log-likelihood. The marginal log-likelihood log p ( D ) is given by where Jensen X  X  inequality was used to obtain the variational lower-bound F ( q ) and q ( Z ) is variational distribution . We assume that the variational distribution factorizes as q ( Z ) = q (  X  ) q (  X  ) q (  X  ) q (  X  ) q (  X  ) q ( X  X  c
Due to the space limitation, we omit the detailed derivation of variational inference algorithms and only present the final results in Table 3. But we emphasize that auxiliary latent variables U , M , T play key role for deriving closed-form update rules. Without them, we have to compute following expectation of log-likelihood  X  log p ( R ij = 1 | X ij = v,  X  ,  X  ,  X  )  X  =  X  log(1  X   X  which is intractable because it is no more conjugate to Beta distri-bution. But the free-form optimization can be easily solved with auxiliary latent variables. For example, optimal q (  X  i ) is given by log q (  X  i ) Taking the exponential on both sizes, we recognize that q (  X  Beta (  X  i | c i ,d i ) , where c i and d i are defined in Table 3.
Variational distributions Updating rules and sufficient statistics q (  X  ) = Dir (  X  |  X  )  X  k =  X  0 + P I i =1  X  Z ki  X  ,  X  log  X  q (  X  ) = Q k,j Beta (  X  kj | a kj ,b kj ) ,  X  log  X  kj  X  =  X  ( a kj )  X   X  ( a kj + a kj ) , ,  X  log(1  X   X  kj )  X  =  X  ( b kj )  X   X  ( a kj + b kj ) . q (  X  ) = Q I i =1 Beta (  X  i | c i ,d i ) q (  X  ) = Q I j =1 Beta (  X  j | e j ,f j ) q (  X  ) = Q V v =1 Beta (  X  v | g v ,h v ) ij = v )  X   X  v ,  X  log(1  X   X  v )  X  =  X  ( h v )  X   X  ( g v + h v  X  1 + v  X  log  X  kj  X  + ( V  X  v )  X  log(1  X   X  kj )  X  ,  X   X  ,  X  log  X  kj  X  + ( V  X  X ij )  X  log(1  X   X  kj )  X  ]  X   X  ) , ) , = v | Z ki = 1) q ( Z ki = 1) = P V v =1 ( v  X  1)  X  kjv  X   X  (0 , 0 , 0) q ( U  X  , M  X  , T  X  ) where
In Marlin X  X  works, hyperparameters for the complete data model and missing data model are set to non-informative values and do not updated [11, 10]. However we empirically observed that hy-perparameter estimation is crucial for finding meaningful solutions instead of undesirable boundary solutions. We estimate hyperpa- X  for symmetric Dirichlet distribution by empirical Bayes method, which maximizes the marginal log-likelihood of observed variables Since we can not compute the marginal log-likelihood exactly, an approximating approach is to maximize the variational lower bound (10). Unfortunately close-form updated rules can not be derived for these hyperparameters, hence numerical optimization methods are required. Empirically we observed that Newton-Raphson method do not work well for our problem since the resulting objective func-tion is not well-approximated by a quadratic. Instead we apply the method in [12], which estimates the Dirchlet mean and precision separately. Note that Beta distribution is special case of Dirchlet distribution ( K = 2 ).
We analyse the time and space complexity of variational infer-ence algorithms summarized in Table 3. We emphasize that pre-computing and caching intermediate factors are very important to implement computationally efficient learning algorithms. Table 4 summarizes the time and space complexity of our learning algo-rithms, of which complexity is depend on the number of observa-tions, instead of the rating data matrix size.

In the perspective of the space complexity, main computational burdens are come from the saving  X  X ij Z ki  X  and q ( X ij all ( i,j )  X   X  c . A required memory for saving them is and it is depends on the the data matrix size (  X  H IJ ) . Instead of saving  X  X ij Z ki  X  and q ( X ij = v ) , we save intermediate factors  X  , X  kj , X  kjv . This approach not only reduces the space complex-ity but also the time complexity. For example, in the update rule for q (  X  v ) , a naive implementation requires O ( IJ  X  H ) time complex-ity because of summing q ( X ij = v ) for all ( i,j )  X   X  it can be efficiently computed in O ( HK ) by where  X   X  k = P I i =1  X  ki is pre-computed factor. Using the similar idea, other updating rules also can be efficiently implemented.
We performed experiments on two datasets: Yahoo! Musing rat-ings for User Selected and Randomly Selected Songs, version 1.0 (Yahoo! music dataset) 2 and MovieLens dataset 3 .
Source code: http://mlg.postech.ac.kr/  X  karma13 http://webscope.sandbox.yahoo.com http://wwww.grouplens.org/datasets/movielens
Variational distributions Space Time
Yahoo! Music dataset : This dataset provides a unique op-portunity to test collaborative prediction methods that incorporate missing data models. The dataset consists of ratings collected dur-ing normal user interaction with Yahoo X  X  LaunchCast internet radio service, as well as ratings for items collected using an online sur-vey. The set of ratings collected through normal interaction with recommendation system are referred to as ratings for user-selected items and denoted by X u . There are 15400 users, 1000 songs, and 311704 ratings in X u . The set of ratings collected through survey are referred to as ratings for randomly selected items and denoted by X r . During a survey conducted by Yahoo! Research, exactly 10 songs are randomly selected and presented to each survey user. The survey users are enforced to rate on these randomly selected songs. In total 5400 users participated in this survey, hence X contains 54000 ratings.

Figure 2-(a) and 2-(b) show the marginal distribution of ratings for randomly selected items, X r , compared to the distribution of rating for user-selected items, X u . The two sets of ratings X and X u exhibit significantly different marginal statistics. The most remarkable feature of the ratings in X r is that they contain much fewer high ratings ( v = 5 ) compared to the ratings for X provides strong evidence on the violation of MAR condition in the Yahoo! ratings for user-selected items.

Our experimental protocol with the Yahoo! dataset is quite sim-ple. We train the model on the ratings for user-selected items X and test on the ratings for randomly selected items X r . We used both root mean squared error (RMSE) and mean absolute error (MAE) to measure the predictive performance.

MovieLens dataset: We used MovieLens-1M dataset which con-sist of 1000209 ratings with 6040 users and 3706 movies. Al-though MovieLens dataset only consists of ratings for user-selected movies, it is well worth comparing two trained models from differ-ent domains (songs and movies), because the missing data mech-anism for each domain can be different. The different behaviour of marginal statistics shown in Figure 2-(b) and 2-(c) support our assumption.
Firstly, we investigated on the benefit of binomial mixture model over multinomial mixture model. We combined binomial mixture model with CPT-v missing data model (referred to as BM/CPT-v). The number of cluster K is set to 10, and all hyperparameters for Beta distributions are set to 1 (i.e. uniform prior). Surprisingly BM/CPT-v model significantly outperformed the MM/CPT-v and it was comparable with MM/CPT-v+ as shown in Table 5.

However we observed that BM/CPT-v converge to a different kind of boundary solution. While MM/CPT-v found the boundary solution which predict almost all of the missing rating to the value 2 (see Figure 2-(d)), BM/CPT-v found the boundary solution which never predict value 5 for missing rating. The marginal statistics of Bayesian-BM/OR model respectively.
 Table 5: RMSE, MAE, and trained parameters for missing data model (  X  v ) of MM/CPT-v and BM/CPT-v on different hyper-parameter settings. Uniform denote uniform Beta prior ( g 0 = h 0 = 1) . In other case, g 0 / ( g 0 + h 0 ) is fixed to 0.02, and scale of ( g 0 + h 0 ) is controlled from H  X  10  X  3 to H  X  10 RMSE and MAE of MM/CPT-v + model are 0.989 and 0.727, respectively.
 In addition we also observed that missing data model parameter  X  was also converged to boundary solution as shown in Table 5. This is quite undesirable result, because  X  5 = 1 implies that, all of the rating with the highest value 5 is already observed, consequently it implied that there is no more item with rating value 5.

We performed a second set of experiments where informative prior is given for missing data model parameter  X  to suppress the over-estimated  X  5 . Based on the observation that the sparsity of the rating data matrix is about 2 percent, hyperparameters of Beta prior are set to g 0 = 0 . 02 S and h 0 = 0 . 98 S , such that mean of Beta prior is matched to 2 percent. S is the prior strength. We controlled the prior strength between H  X  10  X  3 and H  X  10  X  1 . We observed that as the prior strength is increased,  X  5 is suppressed well. Es-pecially in the case of S = H  X  10  X  1 , the prediction accuracy is good and trained missing data model parameters are similar to op-timal ones used in CPT-v+ (4). We computed the variational lower bound for all trained models to check that we can select a meaning-ful model by empirical Bayes method. Against our expectations, the variational lower bound is maximized when uniform priors are set to missing data model parameters such that boundary solution is learned.

Finally we trained our Bayesian-BM/OR model, which considers not only the value-based selection effect but also user activity and item popularity. All hyperparameters for Dirichlet and Beta priors are initialized to 1, and variational parameters are initialized as where kj are Gaussian random noise with standard deviation 0.01. Bayesian-BM/OR model showed best performing RMSE and MAE, which are 0.983 and 0.699 respectively. In addition the model did not converges to boundary solution. The marginal statistics of pre-dicted missing ratings are shown in Figure 2-(e).

We observed that Bayesian-BM/OR model can also be converged to boundary solution if hyperparameters are not optimized. In this case,  X  2 was converged to zero. From these results, we concluded that both three factors are equally important to learn the meaningful model from the MNAR rating data:
In addition to outperforming predictive performance on MNAR data, Bayesin-BM/OR model has another advantage which other models do not have. For observed rating X ij , by using variational posterior q ( U ij ,M ij ,T ij ) , our model can explain the reason of ob-servation with user, item, and rating value factor. It can be used in analysis and comparison of rating trend between different recom-mendation systems.

Table 6 shows the our preliminary rating trend analysis on two datasets: Yahoo! music and MovieLens. Each value in the table is computed by summing and normalizing q ( U ij = 1) , q ( M and q ( T ij = 1) for ( i,j )  X   X  v . For the ratings with value 5, the main reason of observation is rating value factor on both datasets, that means users rate on good items because they like them. How-ever for the ratings with lower values 1 or 2, we can observe the difference between two datasets. In both datasets user and item factor are larger than rating value factor, that means some active users rate on bad items or popular items are rated even though their rating is low. However the rating value factor for lower ratings on in MovieLens (0.044, 0.075) is extremely smaller than that of Yahoo! music (0.213, 0.161). We guess the reason of this pattern follow-ing way: watching the movie require more money and time than Table 6: Preliminary rating trend analysis on Yahoo! Music and MovienLens datasets.
 v User Item Value
All 0.266 0.390 0.421 1 0.377 0.416 0.213 2 0.303 0.540 0.161 3 0.177 0.511 0.320 4 0.084 0.424 0.506 5 0.035 0.180 0.832 listening the song in the internet radio, hence people may select the movie more conservative way than song.
We have presented a Bayesian binomial mixture model for col-laborative prediction, where the generative process for the data and missing data mechanism are jointly modeled to handle non-random missing data. Missing data mechanism was modeled by three fac-Each factor was modeled by Bernoulli random variable, and ob-servation of rating value was determined by the Boolean OR op-eration. Computationally efficient variational inference algorithms were presented, where variational parameters have closed-form up-date rules and the computational complexity depend on the num-ber of observed ratings, instead of the size of the rating data ma-trix. We also discussed implementation issues on hyperparameter tuning and estimation based on empirical Bayes. Finally, we pre-sented experimental results demonstrating that (1) binomial mix-ture model is more suitable than multinomial mixture model for modellng discrete, finite, and ordered rating values; (2) our model finds meaningful solutions instead of undesirable boundary solu-tions, if hyperparameters are estimated by empirical Bayes; (3) our model can capture different rating trend between domain(e.g. songs and movies).
 This work was supported by the IT R&amp;D Program of MSIP/IITP (14-824-09-014, Machine Learning Center), National Research Foun-dation (NRF) of Korea (NRF-2013R1A2A2A01067464), and Sam-sung Electronics Co., Ltd. [1] E. Cand X s and T. Tao. The power of convex relaxation: [2] G. Dror, N. Koenigstein, Y. Koren, and M. Weimer. The [3] Y.-D. Kim and S. Choi. Variational Bayesian view of [4] Y.-D. Kim and S. Choi. Scalable variational Bayesian matrix [5] Y. J. Lim and Y. W. Teh. Variational Bayesian approach to [6] G. Ling, H. Yang, M. R. Lyu, and I. King. Response aware [7] R. J. A. Little and D. B. Rubin. Statistical Analysis with [8] B. M. Marlin. Modeling user rating profiles for collaborative [9] B. M. Marlin. Missing Data Problems in Machine Learning . [10] B. M. Marlin and R. S. Zemel. Collaborative prediction and [11] B. M. Marlin, R. S. Zemel, S. T. Roweis, and M. Slaney. [12] T. Minka. Estimating a Dirichlet distribution. Technical [13] U. Paquet and N. Koenigstein. One-class collaborative [14] S. Park, Y.-D. Kim, and S. Choi. Hierarchical Bayesian [15] T. Raiko, A. Ilin, and J. Karhunen. Principal component [16] R. Salakhutdinov and A. Mnih. Bayesian probablistic matrix [17] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted [18] R. Salakhutdinov and N.Srebro. Collaborative filtering in a [19] H. Steck. Training and testing of recommender systems on [20] J. Wu. Binomial matrix factorization for discrete [21] J. Yoo and S. Choi. Bayesian matrix co-factorization:
