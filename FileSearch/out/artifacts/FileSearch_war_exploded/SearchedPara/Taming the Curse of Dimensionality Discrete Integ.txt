 Stefano Ermon, Carla P. Gomes { ermonste,gomes } @cs.cornell.edu Ashish Sabharwal ashish.sabharwal@us.ibm.com IBM Watson Research Center, Yorktown Heights, NY 10598, U.S.A. Bart Selman selman@cs.cornell.edu Computing integrals in very high dimensional spaces is a fundamental and largely unsolved problem of scien-tific computation (Dyer et al., 1991; Simonovits, 2003; Cai &amp; Chen, 2010), with numerous applications rang-ing from machine learning and statistics to biology and physics. As the volume grows exponentially in the dimensionality, the problem quickly becomes com-putationally intractable, a phenomenon traditionally known as the curse of dimensionality (Bellman, 1961). We revisit the problem of approximately computing discrete integrals, namely weighted sums over (ex-tremely large) sets of items. This problem encom-passes several important probabilistic inference tasks, such as computing marginals or normalization con-stants (partition function) in graphical models, which are in turn cornerstones for parameter and structure learning (Wainwright &amp; Jordan, 2008).
 There are two common approaches to approximate such large discrete sums: variational methods and sampling. Variational methods (Wainwright &amp; Jor-dan, 2008; Jordan et al., 1999), often inspired by sta-tistical physics, are very fast but do not provide qual-ity guarantees. Since sampling and counting can be re-duced to each other (Jerrum &amp; Sinclair, 1997), approx-imate techniques based on sampling are quite popular, but they suffer from similar issues because the num-ber of samples required to obtain a statistically reli-able estimate often grows exponentially in the problem size. Importance sampling based techniques such as SampleSearch (Gogate &amp; Dechter, 2011) provide lower bounds but without a tightness guarantee. Markov Chain Monte Carlo (MCMC) methods for sampling are asymptotically accurate, but guarantees for prac-tical applications exist only in a limited number of cases (fast mixing chains) (Jerrum &amp; Sinclair, 1997; Madras, 2002). They are therefore often used in a heuristic manner. In practice, their performance cru-cially depends on the choice of the proposal distribu-tions, which often must be domain-specific and expert-designed (Girolami &amp; Calderhead, 2011).
 We introduce a randomized scheme that computes with high probability (1  X   X  for any desired  X  &gt; 0) an approximately correct estimate (within a factor of 1 + of the true value for any desired &gt; 0) for gen-eral weighted sums defined over exponentially large sets of items, such as the set of all possible vari-able assignments in a discrete probabilistic graphical model. From a computational complexity perspective, the counting problem we consider is complete for the #P complexity class (Valiant, 1979), a set of problems encapsulating the entire Polynomial Hierarchy and be-lieved to be significantly harder than NP.
 The key idea is to reduce this #P problem to a small number (polynomial in the dimensionality) of instances of a (NP-hard) combinatorial optimization problem defined on the same space and subject to ran-domly generated  X  X arity X  constraints. The rationale behind this approach is that although combinatorial optimization is intractable in the worst case, it has witnessed great success in the past 50 years in fields such as Mixed Integer Programming (MIP) and propo-sitional Satisfiability Testing (SAT). Problems such as computing a Maximum a Posteriori (MAP) assign-ment, although NP-hard, can in practice often be ap-proximated or solved exactly fairly efficiently (Park, 2002; Sontag et al., 2008; Riedel, 2008). In fact, mod-ern solvers can exploit structure in real-world problems and prune large portions of the search space, often dramatically reducing the runtime. In contrast, in a #P counting problem such as computing a marginal probability, one needs to consider contributions of an exponentially large number of items.
 Our algorithm, called W eighted-I ntegrals-And-S ums-By-H ashing ( WISH ), relies on randomized hashing techniques to probabilistically  X  X venly cut X  a high di-mensional space. Such hashing was introduced by Valiant &amp; Vazirani (1986) to study the relationship between the number of solutions and the hardness of a combinatorial search. These techniques were also ap-plied by Gomes et al. (2006a) and Chakraborty et al. (2013) to uniformly sample solutions for the SAT prob-lem and to obtain bounds on their number (Gomes et al., 2006b). Our work is more general in that it can handle general weighted sums, such as the ones arising in probabilistic inference for graphical mod-els. Our work is also closely related to recent work by Hazan &amp; Jaakkola (2012), who obtain bounds on the partition function by taking suitable expectations of a combination of MAP queries over randomly per-turbed models. We improve upon this in two crucial aspects, namely, our estimate is a constant factor ap-proximation of the true partition function (while their bounds have no tightness guarantee), and we provide a concentration result showing that our bounds hold not just in expectation but with high probability with a polynomial number of MAP queries. Note that this is consistent with known complexity results regarding #P and BPP NP ; see Remark 1 below.
 We demonstrate the practical efficacy of the WISH algo-rithm in the context of computing the partition func-tion of random Clique-structured Ising models, Grid Ising models with known ground truth, and a challeng-ing combinatorial application (Sudoku puzzle) com-pletely out of reach of techniques such as Mean Field and Belief Propagation. We also consider the Model Selection problem in graphical models, specifically in the context of hand-written digit recognition. We show that our  X  X nytime X  and highly parallelizable algorithm can handle these problems at a level of accuracy and scale well beyond the current state of the art. Let  X  be a (large) set of items. Let w :  X   X  R + be a non-negative function that assigns a weight to each element of  X . We wish to (approximately) compute the total weight of the set, defined as the following discrete integral or  X  X artition function X  We assume w is given as input and that it can be compactly represented, for instance in a factored form as the product of conditional probabilities tables. Note however that our results are more general and do not rely on a factored representation.
 Assumption: We assume that we have access to an optimization oracle that can solve the following con-strained optimization problem where 1 {C} :  X   X  { 0 , 1 } is an indicator function for a compactly represented subset C  X   X , i.e., 1 {C} (  X  ) = 1 iff  X   X  C . For concreteness, we discuss our setup and assumptions in the context of probabilistic graphical models, which is our motivating application. 2.1. Inference in Graphical Models We consider a graphical model specified as a fac-tor graph with N = | V | discrete random variables x ,i  X  V where x i  X  X i . The global random vector x = { x s ,s  X  V } takes value in the cartesian product X = X 1  X X 2  X  X  X  X  X X N . We consider a probabil-ity distribution over x  X  X (called configurations ) p ( x ) = 1 Z Q  X   X  X   X   X  ( { x }  X  ) that factors into potentials or factors  X   X  : { x }  X  7 X  R + , where I is an index set and { x }  X   X  V a subset of variables the factor  X   X  de-pends on, and Z is a normalization constant known as the partition function .
 Given a graphical model, we let  X  = X be the set of all possible configurations (variable assignments). Define a weight function w : X  X  R + that assigns to each configuration a score proportional to its probability: w ( x ) = Q  X   X  X   X   X  ( { x }  X  ). Z may then be rewritten as Computing Z is typically intractable because it in-volves a sum over an exponential number of config-urations, and is often the most challenging inference task for many families of graphical models. Comput-ing Z is however needed for many inference and learn-ing tasks, such as evaluating the likelihood of data for a given model, computing marginal probabilities, and parameter estimation (Wainwright &amp; Jordan, 2008). In the context of graphical models inference, we as-sume to have access to an optimization oracle that can answer Maximum a Posteriori (MAP) queries, namely, solve the following constrained optimization problem that is, we can find the most likely state (and its weight) given some evidence C . This is a strong as-sumption because MAP inference is known to be an NP-hard problem in general. Notice however that computing Z is a #P-complete problem, a complex-ity class believed to be even harder than NP. We review some results on the construction and prop-erties of universal hash functions (cf. Vadhan, 2011; Goldreich, 2011). A reader already familiar with these results may skip to the next section.
 Definition 1. A family of functions H = { h : { 0 , 1 } n  X  { 0 , 1 } m } is pairwise independent if the fol-lowing two conditions hold when H is a function cho-sen uniformly at random from H . 1)  X  x  X  { 0 , 1 } the random variable H ( x ) is uniformly distributed in variables H ( x 1 ) and H ( x 2 ) are independent. A simple way to construct such a function is to think about the family H of all possible functions { 0 , 1 } n  X  { 0 , 1 } m . This is a family of not only pairwise indepen-dent but fully independent functions. However, each function requires m 2 n bits to be represented, and is thus impractical in the typical case where n is large. On the other hand, pairwise independent hash func-tions can be constructed and represented in a much more compact way as follows; see Appendix for a proof. Proposition 1. Let A  X  { 0 , 1 } m  X  n , b  X  { 0 , 1 } m The family H = { h A,b ( x ) : { 0 , 1 } n  X  { 0 , 1 } m } h
A,b ( x ) = Ax + b mod 2 is a family of pairwise inde-pendent hash functions.
 The space C = { x : h A,b ( x ) = p } has a nice geometric interpretation as the translated nullspace of the ran-dom matrix A , which is a finite dimensional vector space, with operations defined on the field F (2) (arith-metic modulo 2). We will refer to constraints of the form Ax = b mod 2 as parity constraints , as they can be rewritten in terms of logical XOR operations We start with the intuition behind our algorithm to ap-proximate the value of W called W eighted-I ntegrals-And-S ums-By-H ashing ( WISH ).
 Computing W as defined in Equation (1) is challeng-ing because the sum is defined over an exponentially large number of items, i.e., |  X  | = 2 n when there are n binary variables. Let us define the tail distribution of weights as G ( u ) , |{  X  | w (  X  )  X  u }| . Note that G is a non-increasing step function, changing values at no more than 2 n points. Then W may be rewritten as R vs. u curve. One way to approximate W is to (im-plicitly) divide this area A into either horizontal or vertical slices (see Figure 2), approximate the area in each slice, and sum up.
 Suppose we had an efficient procedure to estimate G ( u ) given any u . Then it is not hard to see that one could create enough slices by dividing up the x-axis, estimate G ( u ) at these points, and estimate the area A using quadrature. However, the natural way of doing this to any degree of accuracy would require a number of slices that grows at least logarithmically with the weight range on the x-axis, which is undesirable. Alternatively, one could split the y-axis, i.e., the G ( u ) value range [0 , 2 n ], at geometrically growing values Let b 0  X  b 1  X   X  X  X   X  b n be the weights of the configu-rations at the split points. In other words, b i is the 2 th quantile of the weight distribution. Unfortunately, despite the monotonicity of G ( u ), the area in the hor-izontal slice defined by each bin is difficult to bound, as b i and b i +1 could be arbitrarily far from each other. However, the area in the vertical slice defined by b and b i +1 must be bounded between 2 i ( b i  X  b i +1 ) and 2 i +1 ( b i  X  b i +1 ), i.e., within a factor of 2. Thus, sum-ming over the lower bound for all such slices and the left-most slice, the total area A must be within a factor Of course, we don X  X  know b i . But if we could approx-imate each b i within a factor of p , we would get a 2 p -approximation to the area A , i.e., to W . WISH provides an efficient way to realize this strategy, using a combination of randomized hash functions and an optimization oracle to approximate the b i values with high probability. Note that this method allows us to compute the partition function W (or the area A ) by estimating weights b i at n + 1 carefully chosen points, which is  X  X nly X  an optimization problem. The key insight to compute the b i values is as follows. Suppose we apply to configurations in  X  a randomly sampled pairwise independent hash function with 2 m buckets and use an optimization oracle to compute the weight w m of a heaviest configuration in a fixed (arbi-trary) bucket. If we repeat this process T times and consistently find that w m  X  w  X  , then we can infer by the properties of hashing that at least 2 m configura-tions (globally) are likely to have weight at least w  X  . By the same token, if there were in fact at least 2 m + c configurations of a heavier weight  X  w &gt; w  X  for some c &gt; 0, there is a good chance that the optimization or-acle will find w m  X   X  w and we would not underestimate the weight of the 2 m -th heaviest configuration. As we will see shortly, this process, using pairwise indepen-dent hash functions to keep variance low, allows us to estimate b i accurately with only T = O(ln n ) samples. The pseudocode of WISH is shown as Algorithm 1. It is parameterized by the weight function w , the dimen-Algorithm 1 WISH ( w :  X   X  R + ,n = log 2 |  X  | , X , X  ) for i = 0 ,  X  X  X  ,n do end for sionality n , a correctness parameter  X  &gt; 0, and a con-stant  X  &gt; 0. Notice that the algorithm requires solv-ing only  X ( n ln n ln 1 / X  ) optimization instances (MAP inference) to compute a sum defined over 2 n items. In the following section, we formally prove that the output is a constant factor approximation of W with probability at least 1  X   X  (probability over the choice of hash functions). Figure 1 shows the working of the al-gorithm. As more and more random parity constraints are added in the outer loop of the algorithm ( X  X ev-els X  increasing from 1 to n ), the configuration space is (pairwise-uniformly) thinned out and the optimization oracle selects the heaviest (in red) of the surviving con-figurations. The final output is a weighted sum over the median of T such modes obtained at each level. Remark 1. The parity constraints A X  = b mod 2 do not change the worst-case complexity of an NP-hard optimization problem. Our result is thus consistent with the fact that #P can be approximated in BPP NP , that is, one can approximately count the number of so-lutions with a randomized algorithm and a polynomial number of queries to an NP oracle (Goldreich, 2011). Remark 2. Although the parity constraints we im-pose are simple linear equations over a field, they can make the optimization harder. For instance, finding a configuration with the smallest Hamming weight sat-isfying a set of parity constraints is known to be NP-hard, i.e. equivalent to computing the minimum dis-tance of a parity code (Berlekamp et al., 1978; Vardy, 1997). On the other hand, most low density parity check codes can be solved extremely fast in practice using heuristic methods such as message passing. Remark 3. Each of the optimization instances can be solved independently, allowing natural massive paral-lelization . We will also discuss how the algorithm can be used in an anytime fashion, and the implications of obtaining suboptimal solutions. Since many configurations can have identical weight, it will help for the purposes of the analysis to fix, w.l.o.g., a weight-based ordering of the configurations, and a natural partition of the |  X  | = 2 n configurations into n + 1 bins that the ordering induces.
 Definition 2. Fix an ordering  X  i , 1  X  i  X  2 n , of the configurations in  X  such that for 1  X  j &lt; 2 n , w (  X  j w (  X  j +1 ). For i  X  X  0 , 1 ,  X  X  X  ,n } , define b i , w (  X  fine a special bin B , {  X  1 } and, for i  X  X  0 , 1 ,  X  X  X  ,n  X  Note that bin B i has precisely 2 i configurations. Fur-ther, for all  X   X  B i , it follows from the definition of the ordering that w (  X  )  X  [ b i +1 ,b i ]. This allows us to bound the sum of the weights of configurations in B i (the  X  X orizontal X  slices) between 2 i b i +1 and 2 i b i . 5.1. Estimating the Total Weight Our main theorem, whose proof relies on the two lem-mas below, is that Algorithm 1 provides a constant factor approximation to the partition function. The complete proof of the theorem and all lemmas may be found in the Appendix.
 Lemma 1. Let M i = Median( w 1 i ,  X  X  X  ,w T i ) be defined as in Algorithm 1 and b i as in Definition 2. Then, for any c  X  2 , there exists  X   X  ( c ) &gt; 0 such that for 0 &lt;  X   X   X   X  ( c ) , U Theorem 1. For any  X  &gt; 0 and positive constant  X   X  0 . 0042 , Algorithm 1 makes  X ( n ln n ln 1 / X  ) MAP queries and, with probability at least (1  X   X  ) , outputs a 16-approximation of W = P  X   X   X  w (  X  ) .
 Proof Sketch. It is clear from the pseudocode that it makes  X ( n ln n ln 1 / X  ) MAP queries. For accuracy analysis, we can write W as: Note that U  X  2 L because 2 L = 2 b 0 + P had access to the true values of all b i , we could ob-tain a 2-approximation to W . We do not know true b i values, but Lemma 1 shows that the M i values com-puted by Algorithm 1 are sufficiently close to b i with high probability. Specifically, applying Lemma 1 with T = log(1 / X  )  X  log n , we can show that with probabil-ity at least (1  X   X  ), the output of Algorithm 1 lies in [ L 0 ,U 0 ] as defined in Lemma 2. Observing that [ L,U ] is contained in [ L 0 ,U 0 ] and applying Lemma 2, we have a 2 2 c -approximation of W . Fixing c = 2 and noting that  X   X  (2)  X  0 . 0042 finishes the proof. 5.2. Estimating the Tail Distribution We can also estimate the entire tail distribution of the weights, defined as G ( u ) , |{  X  | w (  X  )  X  u }| . Theorem 2. Let M i be defined as in Algorithm 1, u  X  R + , and q ( u ) be the maximum i such that  X  j  X  { 0 ,  X  X  X  ,i } ,M j  X  u . Then, for any  X  &gt; 0 , with prob-ability  X  (1  X   X  ) , 2 q ( u ) is an 8-approximation of G ( u ) computed using O( n ln n ln 1 / X  ) MAP queries. While this is an interesting result in its own right, if the goal is to estimate the total weight W , then the scheme in Section 5.1, requiring a total of only  X ( n ln n ln 1 / X  ) MAP queries, is more efficient than first estimating the tail distribution for several values of u . 5.3. Improving the Approximation Factor Given a  X  -approximation algorithm such as Algorithm 1 and any &gt; 0, we can design a (1 + )-approximation algorithm with the following construction. Let ` = log 1+  X  . Define a new set of configurations  X  ` =  X   X   X   X  X  X  X  X   X , and a new weight function w 0 :  X  `  X  R as w 0 (  X  1 ,  X  X  X  , X  ` ) = w (  X  1 ) w (  X  2 )  X  X  X  w (  X  Proposition 2. Let c W be a  X  -approximation of P of P  X   X   X  w (  X  ) .
 To see why this holds, observe that W 0 = P c W  X   X W 0 , we obtain that c W 1 /` must be a  X  1 /` = 1 + approximation of W .
 Note that this construction requires running Algo-rithm 1 on an enlarged problem with ` times more vari-ables. Although the number of optimization queries grows polynomially with ` , increasing the number of variables might significantly increase the runtime. 5.4. Further Approximations When the instances defined in the inner loop are not solved to optimality, Algorithm 1 still provides approx-imate lower bounds on W with high probability. Theorem 3. Let optimization problems in Algorithm 1, i.e., Let f W be the output of Algorithm 1 with these subop-timal solutions. Then, for any  X  &gt; 0 , with probability at least 1  X   X  , f W 16  X  W .
 Further, if ability at least 1  X   X  , f W is a 16 L -approximation to W . The output is always an approximate lower bound, even if the optimization is stopped early. The lower bound is monotonically non-decreasing over time, and is guaranteed to eventually reach within a constant factor of W . We thus have an anytime algorithm. We implemented WISH using the open source solver ToulBar2 (Allouche et al., 2010) to solve the MAP in-ference problem. ToulBar2 is a complete solver (i.e., given enough time, it will find an optimal solution and provide an optimality certificate), and it was one of the winning algorithms in the UAI-2010 inference compe-tition. We augmented ToulBar2 with the IBM ILOG CPLEX CP Optimizer 12.3 based techniques borrowed from Gomes et al. (2007) to efficiently handle the ran-dom parity constraints. Specifically, the set of equa-tions Ax = b mod 2 are linear equations over the field F (2) and thus allow for efficient propagation and do-main filtering using Gaussian Elimination.
 For our experiments, we run WISH in parallel using a compute cluster with 642 cores. We assign each opti-mization instance in the inner loop to one core, and finally process the results when all optimization in-stances have been solved or have reached a timeout. For comparison, we consider Tree Reweighted Belief Propagation (Wainwright, 2003) which provides an up-per bound on Z , Mean Field (Wainwright &amp; Jordan, 2008) which provides a lower bound, and Loopy Belief Propagation (Murphy et al., 1999) which provides an estimate with no guarantees. We use the implementa-tions available in the LibDAI library (Mooij, 2010). 6.1. Provably Accurate Approximations For our first experiment, we consider the problem of computing the partition function, Z (cf. Eqn. (3)), of random Clique-structured Ising models on n binary variables x i  X  { 0 , 1 } for i  X  { 1 ,  X  X  X  ,n } . The inter-action between x i and x j is defined as  X  ij ( x i ,x exp(  X  w ij ) when x i 6 = x j , and 1 otherwise, where w is uniformly sampled from [0 ,w p | i  X  j | ] and w is a parameter set to 0 . 2. We further inject some struc-ture by introducing a closed chain of strong repulsive interactions uniformly sampled from [  X  10 w, 0]. We consider models with n ranging from 10 to 60. These models have treewidth n and can be solved exactly (by brute force) only up to about n = 25 variables. Figure 3 shows the results using various methods for varying problem size. We also computed ground truth for n  X  25 by brute force enumeration. While other methods start to diverge from the ground truth at around n = 25, our estimate, as predicted by Theo-rem 1, remains very accurate, visually overlapping in the plot. The actual estimation error is much smaller than the worst-case factor of 16 guaranteed by Theo-rem 1, as in practice over-and under-estimation errors tend to cancel out. For n &gt; 25 we don X  X  have ground truth, but other methods fall well outside the prov-able interval provided by WISH , reported as an error bar that is very small compared to the magnitude of errors made by the other methods.
 All optimization instances generated by WISH for n  X  60 were solved (in parallel) to optimality within a time-out of 8 hours, resulting in high confidence tight ap-proximations of the partition function. We are not aware of any other practical method that can provide such guarantees for counting problems of this size, i.e., a weighted sum defined over 2 60 items.
 6.2. Anytime Usage with Suboptimal Solutions Next, we investigate the quality of our results when not all of the optimization instances can be solved to opti-mality because of timeouts, so that the strong theoret-ical guarantees of Theorem 1 do not apply (although Theorem 3 still applies). We consider 10  X  10 binary Grid Ising models, for which ground truth can be com-puted using the junction tree method (Lauritzen &amp; Spiegelhalter, 1988). We use the same experimental setup as Hazan &amp; Jaakkola (2012), who also use ran-dom MAP queries to derive bounds (without a tight-ness guarantee) on the partition function. Specifically, we have n = 100 binary variables x i  X  { X  1 , 1 } with tive case, we draw w ij from [0 ,w ]; for the mixed case, from [  X  w,w ]. The  X  X ocal field X  is  X  i ( x i ) = exp( f where f i is sampled uniformly from [  X  f,f ], where f is a parameter with value 0.1 or 1.0.
 Figure 4 reports the estimation error for the log-partition function, when using a timeout of 15 min-utes. We see that WISH provides accurate estimates for a wide range of weights, often improving over all other methods. The slight performance drop of WISH for cou-pling strengths w  X  1 appears to occur because in that weight range the terms corresponding to i  X  n/ 2 par-ity constraints are the most significant in the output sum M 0 + P n  X  1 i =0 M i +1 2 i . Empirically, optimization in-stances with roughly n/ 2 parity constraints are often the hardest to solve, resulting in possibly a significant underestimation of the value of W = Z when a time-out occurs. We do not directly compare with the work of Hazan &amp; Jaakkola (2012) as we did not have access to their code. However, a visual look at their plots suggests that WISH would provide an improvement in accuracy, although with longer runtime. 6.3. Hard Combinatorial Structures An interesting and combinatorially challenging graph-ical model arises from Sudoku, which is a popular number-placement puzzle where the goal is to fill a 9  X  9 grid (see Figure 5) with digits from { 1 ,  X  X  X  , 9 } so that the entries in each row, column, and 3  X  3 block composing the grid, are all distinct. The puz-zle can be encoded as a graphical model with 81 dis-crete variables with domain { 1 ,  X  X  X  , 9 } , with potentials  X   X  ( { x }  X  ) = 1 if and only if all variables in { x }  X  different, and  X   X  I where I is an index set contain-ing the subsets of variables in each row, column, and block. This defines a uniform probability distribution over all valid complete Sudoku grids (a non-valid grid has probability zero), and the normalization constant Z s equals the total number of valid grids. It is known that Z s  X  6 . 671  X  10 21 . This number was computed exactly with a combination of computer enumeration and clever exploitation of symmetry properties (Fel-genhauer &amp; Jarvis, 2005). Here, we attempt to ap-proximately compute this number using the general-purpose scheme WISH . First, following Felgenhauer &amp; Jarvis (2005), we sim-plify the problem by fixing the first block as in Figure 5, obtaining a new problem over 72 variables whose normalization constant is Z 0 = Z s / 9!  X  2 54 . Next, since we are dealing with a feasibility rather than op-timization problem, we replace ToulBar2 with Cryp-toMiniSAT (Soos et al., 2009), a SAT solver designed for unweighted cryptographic problems and which na-tively supports parity constraints. We observed that WISH can consistently find solutions (60% of the times) after adding 52 random parity constraints, while for 53 constraints the success rate drops below 0 . 5, at 45%. Therefore M i = 1 in Algorithm 1 for i  X  52 and there should thus be at least 2 52  X  9!  X  1 . 634  X  10 21 solutions to the Sudoku puzzle. Although Theorem 1 cannot be applied due to timeouts for larger values of i , this estimate is clearly very close to the known true count. In contrast, the simple  X  X ocal reasoning X  done by vari-ational methods is not powerful enough to find even a single solution. Mean Field and Belief Propagation report an estimated solution count of exp(  X  237 . 921) and exp(  X  119 . 307), resp., on a relaxed problem where violating a constraint gives a penalty exp(  X  10) (simi-lar results are obtained using a wide range of weights to model hard constraints). A sophisticated adapa-tive MCMC approach tailored for (weighted) SAT in-stances (Ermon et al., 2011) reports 5 . 6822  X  10 21 so-lutions, with a runtime of about 45 minutes. 6.4. Model Selection Many inference and learning tasks require computing the normalization constant of graphical models. For instance, it is needed to evaluate the likelihood of ob-served data for a given model. This is necessary for Model Selection, i.e., to rank candidate models, or to trigger early stopping during training when the likeli-hood of a validation set starts to decrease, in order to avoid overfitting (Desjardins et al., 2011).
 We train Restricted Boltzmann Machines (RBM) (Hinton et al., 2006) using Contrastive Divergence (CD) (Welling &amp; Hinton, 2002; Carreira-Perpinan &amp; Hinton, 2005) on MNIST hand-written digits dataset. In an RBM there is a layer of n h hidden binary vari-ables h = h 1 ,  X  X  X  ,h n units v = v 1 ,  X  X  X  ,v n v . The joint probability distribu-tion is given by P ( h,v ) = 1 Z exp( b 0 v + c 0 h + h We use n h = 50 hidden units and n v = 196 visible units. We learn the parameters b,c,W using CD-k for k  X  { 1 , 10 , 15 } , where k denotes the number of Gibbs sampling steps used in the inference phase, with 15 training epochs and minibatches of size 20.
 Figure 6 depicts confabulations (samples generated with Gibbs sampling) from the three learned models. To evaluate the loglikelihood of the data and deter-mine which model is the best, one needs to compute Z . We use WISH to estimate this quantity, with a time-out of 10 minutes, and then rank the models according to the average loglikelihood of the data. The scores we spectively (larger scores means higher likelihood). In this case ToulBar2 was not able to prove optimality for all instances, so only Theorem 3 applies to these results. Although we do not have ground truth, it can be seen that the ranking of the models is consistent with what visually appears closer to a large collection of hand-written digits in Figure 6. Note that k = 1 is clearly not a good representative, because of the highly uneven distribution of digit occurrences. The ranking of WISH is also consistent with the fact that using more Gibbs sampling steps in the inference phase should provide better gradient estimates and therefore a bet-ter learned model. In contrast, Mean Field results in scores  X  35 . 47 ,  X  36 . 08 ,  X  36 . 84, resp., and would thus rank the models in reverse order of what is visually the most representative order. We introduced WISH , a randomized algorithm that, with high probability, gives a constant-factor approx-imation of a general discrete integral defined over an exponentially large set. WISH reduces the intractable counting problem to a small number of instances of a combinatorial optimization problem subject to parity constraints used as a hash function. In the context of graphical models, we showed how to approximately compute the normalization constant, or partition func-tion, using a small number of MAP queries. Using state-of-the-art combinatorial optimization tools, we are thus able to provide discrete integral or partition function estimates with approximation guarantees at a scale that could till now be handled only heuristi-cally. One advantage of our method is that it is mas-sively parallelizable, allowing it to easily benefit from the increasing availability of large compute clusters. Finally, it is an anytime algorithm which can also be stopped early to obtain empirically accurate estimates that provide lower bounds with a high probability. Acknowledgments: Research supported by NSF grants #0832782 and #1059284.

