 The choice of the loss function is critical in extreme multi-label learning where the objective is to annotate each data point with the most relevant subset of labels from an ex-tremely large label set. Unfortunately, existing loss func-tions, such as the Hamming loss, are unsuitable for learning, model selection, hyperparameter tuning and performance evaluation. This paper addresses the issue by developing propensity scored losses which: (a) prioritize predicting the few relevant labels over the large number of irrelevant ones; (b) do not erroneously treat missing labels as irrelevant but instead provide unbiased estimates of the true loss func-tion even when ground truth labels go missing under arbi-trary probabilistic label noise models; and (c) promote the accurate prediction of infrequently occurring, hard to pre-dict, but rewarding tail labels. Another contribution is the development of the PfastreXML algorithm (code available from [1]) which efficiently scales to large datasets with up to 9 million labels, 70 million points and 2 million dimensions and which gives significant improvements over the state-of-the-art.

This paper X  X  results also apply to tagging, recommenda-tion and ranking which are the motivating applications for extreme multi-label learning. They generalize previous at-tempts at deriving unbiased losses under the restrictive as-sumption that labels go missing uniformly at random from the ground truth. Furthermore, they provide a sound the-oretical justification for popular label weighting heuristics used to recommend rare items. Finally, they demonstrate that the proposed contributions align with real world ap-plications by achieving superior clickthrough rates on spon-sored search advertising in Bing.
Extreme multi-label learning addresses the problem of learning a classifier that can annotate a data point with the most relevant subset of labels from an extremely large label set. Note that multi-label learning is distinct from multi-class classification which aims to predict a single mutually exclusive label.

Extreme multi-label learning is an important research prob-lem as it has many applications in tagging, recommendation and ranking. For instance, there are more than a million labels (tags) on Wikipedia and one might wish to build an extreme multi-label classifier that tags a new article or web page with the subset of most relevant Wikipedia labels. Sim-ilarly, given a user X  X  buying or viewing history, one might wish to build an extreme multi-label classifier that recom-mends the subset of millions of items that the user might wish to buy or view next. In general, one can reformulate ranking and recommendation problems as extreme classifica-tion tasks by treating each item to be ranked/recommended as a separate label, learning an extreme multi-label classifier that maps a user X  X  feature vector to a set of labels, and then using the classifier to predict the subset of items that should be ranked/recommended to each user.

Extreme multi-label learning differs from traditional multi-label learning in a number of ways including the need for logarithmic time prediction, training at an extreme scale with millions of data points, features and labels, etc . Two aspects are germane to this paper. First, every data point has missing labels in its ground truth labelling since it is impossible for annotators to go through millions of labels and mark out the exact relevant subset. This has a fun-damental impact on training, validation and performance evaluation. Second, the notion of what constitutes a good prediction changes when one moves from traditional to ex-treme multi-label learning. In particular, due to relevant label sparsity, it is more important to accurately predict rel-evant labels than irrelevant ones. Furthermore, due to the power law distribution over labels, infrequently occurring tail labels have little training data and are harder to pre-dict than frequently occurring ones but might also be more informative and rewarding. As such, design choices made for traditional multi-label learning might not apply at the extreme scale.

One of the most critical design choices is that of the loss function. It determines whether the training algorithm learns a good solution, whether hyper-parameters are tuned appro-priately, influences model selection and, perhaps most im-portantly, ensures that performance evaluation on the test set is aligned with real world application requirements.
This paper argues that traditional multi-label loss func-tions are unsuitable for extreme multi-label learning even though they have been used extensively thus far. For in-stance, the popular Hamming loss [5, 7, 10, 11, 14, 19, 22, 40, 43, 48] does not prioritize predicting the few relevant la -bels over the millions of irrelevant ones, erroneously treats missing labels as irrelevant, treats all relevant labels as be-ing equally important and is biased due to missing ground truth. As a result, extreme multi-label models optimized and selected using traditional loss functions might perform poorly when deployed in real world applications.

The primary contribution of this paper is to develop loss functions suitable for extreme multi-label learning. It is ar-gued that losses which focus on ranking relevant labels as highly as possible are more suitable than the Hamming loss. Propensity scored variants of such losses, including preci-sion@k and nDCG@k, are developed and proved to give un-biased estimates of the true loss function even when ground truth labels go missing under arbitrary probabilistic label noise models. Furthermore, it is shown that the propensity models developed in this paper based on real world appli-cations naturally promote the accurate prediction of infre-quently occurring, difficult to predict, but rewarding tail labels. This addresses both the limitations of traditional multi-label loss functions as discussed in this paper. An-other contribution is the development of the PfastreXML al-gorithm that can scale to extreme multi-label datasets with up to 9 million labels, 70 million training points and 2 mil-lion dimensional features and achieves significant improve-ments over the state-of-the-art. The code for PfastreXML is available from [1].

This paper X  X  results generalize beyond extreme multi-label learning and are also relevant to tagging, recommendation and ranking. Previous attempts at developing unbiased loss functions in these areas have been limited to square loss [17], recall [38] and average discounted gain [26] under the restric-tive assumption that ground truth labels go missing uni-formly at random. Furthermore, the propensity models de-veloped in this paper present a sound theoretical justification for the popular label weighting heuristics [9, 12, 35, 39, 44, 47, 49, 51] used in the recommendation literature to promote the prediction of rare and novel items. Finally, the fact that higher clickthrough rates are achieved while ranking queries for sponsored search advertising in Bing demonstrates that the loss functions and algorithms proposed in this paper are better aligned with real world applications.
Extreme multi-label learning algorithms typically follow a tree [4, 34, 46] or an embedding based approach [5, 6, 7, 10, 11, 14, 18, 20, 22, 27, 32, 36, 43, 45, 48, 50]. While some algorithms have been proposed for training with missing la-bels [23, 40, 48] under restrictive settings, aspects such as hyper-parameter tuning, model selection and performance evaluation have not been addressed before. As such, the Hamming loss [5, 7, 10, 11, 14, 19, 22, 40, 43, 48] continues to be one of the most popular losses for extreme multi-label learning along with precision [4, 6, 18, 21, 22, 34, 45, 46] and the F-measure [5, 11, 15, 19, 20, 22, 40, 50]. On the other hand, unbiased estimators for recall [38], average discounted gain [26] and square loss [17] have been developed under the restrictive assumption that labels go missing uniformly at random from the ground truth. By contrast, this paper de-velops propensity scored variants of precision, nDCG and other loss functions and proves that they are unbiased even under general probabilistic label noise models.

Propensity scoring has been used to develop unbiased es-Figure 1: Plot showing the number of times each label occurs i n a dataset: 246201 and 452262 labels occur less than 5 times each in Wikipedia and Amazon respectively. Such labels are harder to predict than popular ones but might also be more informative and rewarding in certain applications. timators for observational data [37]. In machine learning, propensities have been used for bias correction in situations where the training and test data have been drawn from dif-ferent distributions [3]. Propensities have also been used for off policy evaluation, whereby feedback data from the inter-action logs of an existing system is used to evaluate a new system [8, 24, 25, 41, 42].

Label (item) weighting loss functions have been proposed to promote the accurate prediction of infrequently occur-ring labels (rare items) which might delight and surprise the user. For instance, denoting a label X  X  normalized frequency of occurrence by p l , [9, 44, 47, 49, 51] used the heuristic of weighting each label by  X  log p l whereas [12, 35, 39] rec-ommended a weight of p  X   X / (  X  +1) l where  X   X  0 was a user tunable parameter. This paper provides theoretical justifi-cation for such heuristics by showing how similar weights can arise from the proposed propensity models.
Consider evaluating the performance of an extreme multi-label algorithm for tagging Wikipedia articles by comput-ing a chosen loss function on the ground truth labels pro-vided by Wikipedia X  X  editors. To take a concrete exam-ple, Wikipedia X  X  editors annotated the article for the Di-vine Comedy with 15 labels such as  X 14th-century Chris-tian texts X ,  X  X pic poems in Italian X ,  X 1300 in Italy X , etc . Note that many relevant labels such as  X  X ante Alighieri X ,  X  X edieval philosophical literature X  and  X  X llegory X  are miss-ing since it is impossible for any annotator or expert to go through the entire list of Wikipedia labels and select all the relevant ones. Evaluating performance using the Hamming loss leads to the following issues which can be overcome by the proposed propensity scored nDCG@k and precision@k.
Relevant labels: Table 2 shows that the number of la-bels relevant to any given data point is far smaller than the number of irrelevant ones. Accurately predicting a rel-evant label is therefore more important than predicting an irrelevant one. For instance, predicting that  X  X pic poems in Italian X  is relevant to the Divine Comedy is more difficult, and informative, than predicting that  X  X aseball X  is irrele-vant. Similarly, it is more important to accurately predict relevant labels to fill the few slots available in typical recom-mendation applications than it is to predict irrelevant ones. Unfortunately, the Hamming loss charges the same penalty for misclassifying relevant and irrelevant labels. Precision@k and nDCG@k avoid this by promoting the prediction of rel-evant labels with high ranks.
Missing labels: T he Hamming loss would penalize an al-gorithm for predicting that the label  X  X ante Alighieri X  was relevant to the article for the Divine Comedy since the la-bel was missing from the ground truth. Section 4 addresses this issue by developing propensity scored variants of preci-sion@k and nDCG@k which provide unbiased estimates of the true loss as if computed on the complete ground truth without any missing labels.

Tail labels: Labels follow a power law distribution in extreme multi-label learning applications (see Figure 1). In-frequently occurring labels have little training data and are harder to predict than frequently occurring ones but might also be more informative and rewarding. This is particularly important on a dataset such as Wikipedia where 246201 la-bels occur in less than 5 articles each. For instance, little information is gained by predicting popular generic labels such as  X  X oems X  for the Divine Comedy article as compared to predicting relatively infrequent labels such as  X  X pic po-ems in Italian X  (which implies  X  X oems X  and more) or  X 14th-century Christian texts X . Similarly, in certain applications, there is little to be gained by recommending popular items since users might know about them already. Predicting rare items might be more desirable in these cases. While existing losses treat all labels as equal, Section 5 develops a propen-sity model that naturally promotes the accurate prediction of infrequent labels with high ranks.
This Section develops propensity scored variants of preci-sion@k, nDCG@k and other popular loss functions (see Ta-ble 1 for examples). It is proved that the proposed propen-sity scored losses computed on the observed labels provide unbiased estimates of the true loss function computed on the complete (but unobtainable) ground truth without any missing labels.

Label representation: Extreme multi-label learning deals with applications having an extremely large number of la-bels L where it is not possible for any annotator to select the exact relevant label subset for even a single data point. Let y  X  , y  X  { 0 , 1 } L denote the complete (but unobtainable) and observed (but with missing labels) ground truth label vectors for a given data point such that y  X  l = y l = 1 for observed relevant labels, y  X  l = 1 , y l = 0 for unobserved rele-Table 1: (a) presents unbiased propensity scored loss func-tions L ( y ,  X  y ) corresponding to precision@k and nDCG@k for an unrestricted probabilistic label noise model which is the focus of this paper. The unbiased losses in (b), including the Mean Reciprocal Rank (MRR) and the Average Discounted Gain (ADG), require either knowledge of 1  X  y  X  or that la-bels go missing with probability 1  X  g l / 1  X  y  X  with known g (except for the F-score). Note that  X  y has only k non-zero entries for precision@k, nDCG@k and recall@k and that r l represents the rank of label l in  X  y . vant labels and y  X  l = y l = 0 for irrelevant labels. Continuing with the example of the Divine Comedy from Section 3, y l = y l = 1 for the observed relevant label  X  X pic poems in Italian X , while y  X  l = 1 , y l = 0 for the relevant, but missing, label  X  X ante Alighieri X  and y  X  l = y l = 0 for the irrelevant label  X  X aseball X . Furthermore, it is assumed that the noise in the labelling process is one sided and that irrelevant la-bels are never marked as relevant. For instance, Wikipedia X  X  editors are not malicious and never allow an article to be tagged with an irrelevant label. Note that, even if this mild assumption was violated, it might be possible to hire anno-tators to weed out the irrelevant tags. Also note that, since y  X  is unavailable, this label representation does not assume that the position of missing labels is known unlike previous work [48]. Finally, let  X  y  X  { 0 , 1 } L denote an algorithm X  X  predicted label vector for a given data point.

Propensities: The propensity p il  X  P ( y il = 1 | y  X  il denotes the marginal probability of a relevant label l being observed for a data point i . No constraints have been placed on the propensities apart from the fact that label noise is one sided  X  i. e. P ( y il = 1 | y  X  il = 0) = 0. In particular, it is not assumed that labels go missing independently or uniformly at random. Note that, for notational convenience, the subscript i will be dropped from p il even though the propensity depends on both the label l and the data point i .
 Propensity scored loss functions: Let L  X  ( y  X  ,  X  y ) = P loss functions which decompose over individual labels l and are computed over the relevant labels alone ( { l | y  X  l L  X  represents the true loss function measuring the loss in-curred for predicting  X  y when the complete ground truth vector was y  X  . Training and performance evaluation us-ing L  X  is desirable but infeasible as y  X  is unavailable. The propensity scored variant of L  X  computed on the observed ground truth y is defined to be L ( y ,  X  y ) = P L l : y P that L can be a viable proxy for L  X  for training, model se-lection, hyperparameter tuning and performance evaluation.
Theorem 4.1. The loss function L ( y ,  X  y ) evaluated on the observed ground truth y is an unbiased estimator of the true loss function L  X  ( y  X  ,  X  y ) evaluated on the complete ground truth y  X  . Thus, E y [ L ( y ,  X  y )] = E y  X  [ L  X  ( y  X  and P ( y ) related through propensities p l and any fixed  X  y .
Proof. Please click here for the supplementary material containing the proof.

Theorem 4.1 covers loss functions which decompose over i ndividual labels such as precision@k and nDCG@k which are the primary focus of this paper. Unbiased estimators of recall, average discounted gain, mean reciprocal rank and other relevant non-decomposable loss functions can also be derived if it is assumed that P ( y  X  ) is a delta function imply-ing that each ground truth label is either definitely relevant or definitely irrelevant (with no uncertainty) to the data point being annotated.

Theorem 4.2. If P ( y  X  ) is a delta function then E y [ L ( y ,  X  y )] = E y  X  [ L  X  ( y  X  ,  X  y )] for non-decomposable loss functions of the Figure 2: Propensities p l a nd their corresponding weights w l on Wikipedia and Amazon. The estimated propensities compared to popular heuristics such as N  X   X  l and log( N/N
Proof. Please click here for the supplementary material containing the proof.

Note that Theorem 4.2 is useful only if g  X  ( y  X  ,  X  y ) can be evaluated even though y  X  is unknown. This is certainly possible in some applications. For instance, in the case of recall, g  X  ( y  X  ,  X  y ) = 1  X  y  X  counts how many labels were rel-evant in the complete ground truth. This can be readily estimated by counting the number of face detections ( 1  X  in a given image in name tagging applications on Face-book even though the name of each individual might not be known ( y  X  is unknown). Alternatively, if g  X  ( y  X  ,  X  y ) is un-known, the following corollary derives unbiased estimators when g  X  ( y  X  ,  X  y ) = g  X  ( y  X  ) and labels go missing with proba-bility 1  X  g l /g  X  ( y  X  ) with known g l . Note that this generalizes the results of [26, 38] which assume that labels go missing uniformly at random with constant g l = 1  X  y .

Corollary 4.2.1. If P ( y  X  ) is a delta function and labels are retained with propensities p l = g l /g  X  ( y  X  ) , then E = E y  X  [ L  X  ( y  X  ,  X  y )] for non-decomposable loss functions of the
P roof. Please click here for the supplementary material containing the proof.

The theorems so far prove that the propensity scored l osses are unbiased in expectation. In practice, one can only compute the propensity scored loss on the observed labels rather than in expectation over y . The following theorem shows that the bias induced by this point estimate depends on the average number of relevant labels rather than the to-tal number of labels and that it reduces as the number of points over which the loss is computed is increased.
Theorem 4.3. (Concentration bound) Let Y = { y i  X  { 0 , 1 } L } N i =1 be a set of N independent observed ground truth random variables. Then with probability at least 1  X   X  E w here  X  = max il 1 p is the maximum number of labels that can be relevant to a data point i in the complete ground truth.

Proof. Please click here for the supplementary material containing the proof.
 Finally, an unbiased estimator can also be derived for the H amming loss even though other loss functions, such as pre-cision@k and nDCG@k, might be preferable
Theorem 4.4. For any P ( y  X  ) and P ( y ) related through propensities p l and any fixed  X  y , E y [ L ( y ,  X  y )] = E where L ( y ,  X  y ) = P l 1 p timator of the Hamming loss L  X  ( y  X  ,  X  y ) = P l k y  X  concentration bound  X   X  L q 1 2 N l og (2/  X  ) where  X  = max
Proof. Please click here for the supplementary material containing the proof.
T he unbiased variants of precision@k, nDCG@k and other loss functions developed in Section 4 require that the marginal propensities of labels being retained is known. Unfortu-nately, propensities are generally unknown as y  X  is unavail-able due to the large label space. Based on empirical obser-vation, this Section proposes that the propensities might be modelled as a sigmoidal function of log N l where N l is the number of data points annotated with label l in the observed ground truth dataset of size N and A, B are application specific parameters and C = (log N  X  1)( B +1) In particular, propensities are estimated on Wikipedia and Amazon where meta-data is available for the task and shown to give a close fit to (1) (see Figure 2).

Tagging on Wikipedia ( A = 0 . 5 , B = 0 . 4 ): The marginal propensity of a label can be estimated as p l = N l /N  X  l N l and N  X  l are the number of times the label occurred in the observed and complete ground truth respectively. Es-timates of N  X  l can be obtained for Wikipedia by leveraging its hierarchy. It is assumed that if a label is relevant to a Wikipedia article then so are all its ancestor labels. For in-stance, the Divine Comedy article has been annotated with  X 1300 in Italy X  and should therefore have also been anno-tated with its ancestor  X 14th century in Italy X . Examining all Wikipedia articles revealed that 45 articles were anno-tated with a descendant of the label  X 14th century in Italy X  but only 10 were annotated with the label itself resulting in a propensity estimate of p l = 10 / (10 + 45) = 0 . 182. This procedure was carried out for all labels with more than 4 descendants for robust estimation. Labels with similar fre-quencies were binned together in an equiheight histogram with 20 labels per bin. Figure 2 plots the average propen-sity per bin as a function of label frequency on a log scale. As can be seen, the proposed sigmoid propensity model with parameters A = 0 . 5 and B = 0 . 4 is a close fit to the esti-mated propensities.
Product recommendation on Amazon ( A = 0 . 6 , B = 2 . 6 ): The item-to-item recommendation task on Amazon is to predict the subset of items (labels) that a user might buy along with a given item. In this case, N l represents the number of items that item l was bought along with in the observed dataset (across all transactions) while N  X  l resents the total number of items that item l could have been bought along with. While N l is known, N  X  l can be estimated by figuring out which items could have been sub-stituted in place of the ones that item l was bought along with. It has been argued that substitutable products can be inferred from the  X  X lso viewed X  items while complimentary items can be inferred from the  X  X lso bought X  items [28, 30]. Following this principle, N  X  l can be estimated as the total number of unique items viewed along with items that item l was  X  X lso bought X  along with. For robustness, propensities p = N l /N  X  l were estimated only for those items for which  X  X lso viewed X  information was available for each of the items that item l was  X  X lso bought X  with . Items with similar N were binned together in an equiheight histogram as in the case of Wikipedia. Figure 2 plots the average propensity per bin as a function of item frequency on a log scale. As can be seen, the proposed sigmoid propensity model with param-eters A = 0 . 6 and B = 2 . 6 is a close fit to the estimated propensities.

Recommending rare items: It is important to remove the popularity bias and recommend rare/novel items in many applications [9, 12, 35, 39, 44, 47, 49, 51]. A common heuris-tic is to weight each item inversely to its popularity and to assign weighted rewards for accurate recommendations. Weights have often been set in an ad hoc fashion as w l  X  N l [12, 35, 39] or w l  X  log( N/N l ) [9, 44, 47, 49, 51].
Such weights arise naturally as inverse propensities in the unbiased losses developed in this paper. As can be seen from (1) and Table 1, each label in the proposed unbiased losses has a weight given by which somewhat matches ( N l + B )  X  A and log( N/N l ) in dif-ferent ranges of N l (see Figure 2). This not only provides a sound theoretical justification of label weighting heuris-tics for recommending rare items but also leads to a more principled setting of the weights.

Other datasets: Propensity estimation might not be possible on datasets where meta information is not available. In such cases, it is recommended that A = (0 . 5 + 0 . 6) / 2 = 0 . 55 and B = (0 . 4 + 2 . 6) / 2 = 1 . 5 are set to their values averaged over Wikipedia and Amazon. This was verified to be reasonably close to the parameter settings on the Wiki10 dataset ( A = 0 . 55 , B = 0 . 1).
This Section develops the PfastreXML algorithm for ex-treme multi-label learning. PfastreXML optimizes propen-sity scored nDCG by leveraging FastXML [34] for nDCG optimization. PfastreXML then further extends FastXML to improve tail label prediction which is the most challeng-ing aspect of extreme multi-label learning. PfastreXML achieves this at scale by making key approximations which increase FastXML X  X  training time by just seconds while re-taining the prediction accuracy gains of the extension.
Classifier architecture: Propensity scored FastXML (PfastXML) shares the same architecture as FastXML [34] which learns an ensemble of trees during training. Trees are grown by recursively partitioning nodes starting at the root until each tree is fully grown. Nodes are split by learning a separating hyperplane which partitions training points be-tween a left and a right child. The FastXML hyperplane is learnt by optimizing nDCG such that each training point X  X  relevant labels are ranked as highly as possible in its parti-tion. Node partitioning terminates when a node contains less than a user specified number of points. Leaf nodes contain a probability distribution over labels generated by normalizing the frequency counts of all the training labels reaching the node.

Predictions are made in logarithmic time by passing a test point down each of the balanced trees in the ensemble. The test point is sent to an internal node X  X  left (right) child if it lies on the negative (positive) side of the separating hyperplane at that node. The label distributions of all the leaves containing the test point are aggregated in order to make a prediction as follows:
Propensity scored objective function: PfastXML im-proves upon FastXML by replacing the nDCG loss with its propensity scored variant which is unbiased and assigns higher rewards for accurate tail label predictions. Given a set of N training points at a node { ( x i , y i ) N i =1 tures x i  X  R D and observed ground truth label vectors y i  X  { 0 , 1 } L , PfastXML X  X  separating hyperplane w  X  at the node is given by the optimal solution of w. r. t. w  X  R D ,  X   X  { X  1 , +1 } L , r + , r  X   X   X (1 , L ) where L PSnDCG @ L ( r , y ) =  X  P l the training points present at the node being partitioned,  X  i  X  { X  1 , +1 } indicates whether point i was assigned to the negative or positive partition and r + and r  X  represent the predicted label rankings for the positive and negative partition respectively.

Optimization: Note that FastXML X  X  objective function can be recovered from PfastXML X  X  by substituting y p il = y /p il  X  i. e. by replacing each label y il with label y p objective function can therefore be optimized by FastXML X  X  iterative alternating optimization applied to y p il . In each it-eration, the algorithm fixes w and alternates between opti-mizing  X  and r  X  using efficient closed form solutions until a stationary point is reached according to Theorem 1 of [34]. Then  X  and r  X  are fixed and w is optimized by solving a standard l 1 regularized logistic regression binary classifica-tion problem using Liblinear [13].

Scale: PfastXML enjoys all the scaling properties of FastXML and is therefore one of the most efficient extreme multi-label learning algorithm for large scale problems. It could train o n WikiLSHTC-325K (325 K labels, 1 . 7 M training points and 1 . 6 M dimensions) and Ads-9M (9 M labels, 70 M train-ing points and 2 M dimensions) in less than 30 minutes and 17 hours respectively using a 16 core Intel Xeon 2.6 GHz server. In contrast, MLRF [4] required 4 and 42 hours on a thousand core cluster for training on these datasets and resulted in significantly lower prediction accuracies [34]. No other algorithm has been shown to scale to datasets of the size of Ads-9M to the best of our knowledge. Equally impor-tantly in terms of scaling, PfastXML X  X  predictions required less than 1.5 milliseconds per test point even at the largest scale which is critical for deployment in real world applica-tions.
Propensity scoring improves FastXML but tree classifiers are still prone to predicting tail labels with low probabili-ties as partitioning errors in the internal nodes dispropor-tionately reduce the support of tail labels in the leaf node distributions. PfastreXML addresses this limitation by re-ranking PfastXML X  X  predictions using classifiers designed specifically for tail labels. PfastreXML X  X  training and pre-diction routines are shown in Algorithms 2 and 3 of the supplementary material while code is available from [1].
Tail label classifiers: It is assumed that each label can be predicted independently based on a hyperspherical deci-sion boundary generated by Discriminative MLE estimation of the parameters {  X  l } on the observed training data { ( x i , y i ) N i =1 } with features x i  X  R D and observed ground truth label vectors y i  X  { 0 , 1 } L can be carried out as arg max {  X  l } Q N i =1 that P ( y il | y  X  il , x i ) = P ( y il | y  X  il ).
Optimization: Taking the log leads to L independent optimization problems Taking the gradient and equating it to zero yields  X   X  l = not a closed form solution since v il is a function of  X   X  hence one needs to apply an optimization technique, such as stochastic gradient descent, to obtain  X   X  l .

Approximation: Unfortunately, stochastic gradient de-scent is too expensive at the scale of Ads-9M. A simple, yet effective, approximation is therefore proposed which allowed training on Ads-9M in 18 minutes on a single core (all other datasets were trained in less than 20 seconds), led to sparse solutions taking up only 2.64 Gigabytes of RAM for Ads-9M and reduced prediction accuracy over the stochastic gradi-ent descent solution by 0.2% as measured on the smaller datasets. In particular, assuming that  X  2 k x i  X   X  l k 2 { 1 , .., N } , led to the simplification u il  X  y il yielding Thus, each  X   X  l turns out to be the mean of the training points for which the label was observed to be relevant. This implies that solutions preserving data sparsity can be effi-ciently computed for millions of tail labels as each of them has only a handful of training points.

Re-ranking: The final ranked list of labels is predicted by sorting a linear combination of (3) and (5) restricted to those l for which P pf ( y  X  l = 1 | x ) 6 = 0. Note that the re-ranking takes place in 0.13 milliseconds per point on Ads-9M so that the overall prediction time continues to be less than 1.5 milliseconds per test point.
Experiments were carried out on a synthetic dataset to show that the proposed propensity scored loss functions are unbiased and preferable for both training and performance evaluation. Experiments were also carried out on the largest benchmark datasets demonstrating that PfastreXML could achieve significantly higher prediction accuracies as com-pared to the state-of-the-art. Improvements in the click-through rates on Bing Ads indicated that the proposed loss functions and algorithms were better suited for real world applications.

Datasets: Experiments were carried out on extreme multi-label datasets including Ads-9M, Amazon-670K [6, 29], Wiki10-31K [6, 52],WikiLSHTC-325K [33, 34], AmazonCat-13K [29] and EUR-Lex [31]. The Ads-9M dataset is proprietary. All the other datasets are publically available and can be down-loaded from The Extreme Classification Repository [2]. The tasks include annotating Wikipedia articles with the subset of relevant 325 K Wikipedia tags, item-to-item recommen-dation on Amazon with 670 K items and ranking 9 M queries for sponsored search advertising on Bing. Table 2 lists the statistics of these datasets.

Baseline algorithms: PfastreXML was compared to a number of baseline extreme multi-label algorithms includ-ing FastXML [34] and SLEEC [6] which are the leading tree and embedding based approaches respectively. Other text for details. Figure best viewed under magnification. baseline algorithms include 1-vs-All [16], LEML [48], WSA-BIE [45], CPLST [10], CS [18], ML-CSSP [7] and LPSR [46]. A Popularity baseline was also included which predicted a constant ranking of the most frequently occurring labels in each dataset. Unfortunately, some of the algorithms did not scale beyond the EUR-Lex dataset and results are presented in Table 3.
 Implementations of SLEEC, FastXML, LEML and 1-vs-All were provided by the authors. The remaining algorithms were implemented by us taking care to ensure that the pub-lished results could be reproduced and were verified by the authors wherever possible.

Hyper-parameters: PfastreXML has 2 hyper-parameters  X ,  X  in addition to FastXML X  X  hyper-parameters. These were set to constants  X  = 0 . 8 ,  X  = 30 across all datasets. All of FastXML X  X  hyper-parameters were also set to con-stant default values across all datasets as was done in [34]. This helps significantly reduce training time by eliminating hyper-parameter tuning. The hyper-parameters for all the other algorithms were set using fine grained validation on each data set so as to achieve the highest possible predic-tion accuracy.

Evaluation metrics: Given a set of M test points, per-formance was evaluated using the unbiased propensity scored loss functions of Table 1 as G ( {  X  y i } ) =  X  1 M P M i = 1 Note that the gain G could be greater than 1 due to the propensities. Therefore, for greater interpretability, Table 3 reports 100  X  X  ( {  X  y i } ) / G ( { y i } ) for Precision@k and nDCG@k, referred to as Pk and Nk respectively, for k = 1 , 3 and 5. Coverage@k measuring the percentage of normalized unique labels present in the top k = 1 , 3 and 5 predictions made by an algorithm across all test points is also reported as Ck.
Simulations: It was assumed that the complete ground truth was available for the EUR-Lex dataset and missing labels were simulated according to the propensity curves shown in Figure 3a.

In the second experiment, FastXML was trained on miss-ing labels by optimizing nDCG while PfastXML was trained on the same set by optimizing propensity scored nDCG. Both methods were evaluated using nDCG@3 computed on the complete ground truth with no missing labels. As Fig-ure 3c shows, PfastXML consistently outperformed FastXML thereby indicating that propensity scoring could improve training. A related concern might be that PfastXML (or PfastreXML) might outperform FastXML just because it optimizes the loss function being used for evaluation while FastXML does not. This experiment demonstrates other-wise as PfastXML was trained using propensity scored nDCG but evaluated using standard nDCG. Results of similar ex-periments on the benchmark datasets are provided in the supplementary material.

Finally, Figure 3d demonstrates that training with in-correct propensities might be better than training with no propensities. In this experiment, labels were removed from the training set using a reference propensity curve depicted in bold in Figure 3a. FastXML and PfastXML were trained on this set and their performance evaluated using nDCG@3 computed on the complete test set without any missing la-bels. PfastXML was then trained on the very same training set but using incorrect propensities (also shown in Figure 3a with A 6 = 0 . 55). As can be seen, PfastXML trained on incor-rect propensities could outperform FastXML trained with-out propensities. This indicates that using even incorrect propensity estimates might be beneficial in certain situa-tions as compared to using no propensities.

Benchmark Results: Table 3 compares PfastreXML X  X  performance to that of state-of-the-art SLEEC, FastXML and other baseline algorithms using unbiased precision and nDCG. As can be seen, the proposed PfastXML and Pfas-treXML lead to significantly better prediction accuracies as compared to the state-of-the-art. PfastreXML X  X  improve-ments ranged from 3% on Ads-9M to more than 20% on AmazonCat-13K. Figure 4 shows that most of these im-provements were made for infrequently occurring tail labels. In addition, Table 4 shows that PfastreXML predicted a larger number of unique labels than SLEEC or FastXML further indicating that PfastreXML had better coverage in the tail.

PfastreXML also improves upon PfastXML X  X  prediction accuracy with negligible training and prediction overheads. For instance, PfastXML could train on WikiLSHTC-325K and Ads-9M in less than 30 minutes and 17 hours respec-tively using a 16 core Intel Xeon 2.6 GHz server. Pfas-treXML took an extra 12 seconds and 18 minutes for train-ing on these datasets using a single core. PfastreXML X  X  pre-dictions took an extra 0.13 milliseconds per test point over PfastXML X  X  and continued to be under 1.5 milliseconds.
Sponsored search on Bing: PfastreXML X  X  query rank-ings were also used to serve ads on the Bing search engine. PfastreXML was observed to give an improvement of sig-Table 3: The proposed PfastreXML and PfastXML algo-r ithms make significantly more accurate predictions as com-pared to state-of-the-art SLEEC, FastXML and other base-line algorithms. PfastreXML X  X  predictions are more accu-rate than PfastXML X  X  with negligible training and predic-tion overheads. Performance is evaluated according to the unbiased propensity scored Precision@k (Pk) and nDCG@k (Nk) for k = 1 , 3 and 5. (d) WikiLSHTC-325K N = 1 . 78 M, D = 1 . 62 M, L = 325 K nificantly more than 5% in the clickthrough rate over both F astXML and the highly specialized system in production Table 4: PfastreXML has more unique labels Ck in the top k = 1 , 3 and 5 predictions across all test points in a dataset as compared to SLEEC or FastXML indicating that it has better coverage of tail labels. (d) WikiLSHTC-325K N = 1 . 78 M, D = 1 . 62 M, L = 325 K (which was a large ensemble of many different ranking tech-n iques). Note that the production system was very good at predicting head queries with high ranks and that Pfas-treXML was rewarded for making only those predictions which could not be made by the production system. Ac-curately ranking tail queries highly was therefore critical in this case and PfastreXML was able to successfully serve ads which had never received clicks before. This helps verify that the propensity scored loss functions and proposed algorithm align with the requirements of real world applications. magnification
This paper developed loss functions suitable for extreme multi-label learning and long tail, missing label applications such as ranking, recommendation and tagging. Propensity scored variants of precision and nDCG were developed and proved to give unbiased estimates of the true loss function evaluated on the complete ground truth. No restrictions were placed on the propensities apart from the mild assump-tion that irrelevant labels were never marked as relevant. Furthermore, propensity models were developed based on real world applications and were shown to naturally pro-mote the accurate prediction of infrequently occurring tail labels. This provides a sound theoretical justification of pop-ular label weighting heuristics used to remove the popularity bias and recommend rare/novel items. The results also pro-vide a more principled setting of the weights as compared to previous heuristics.

This paper also developed the PfastreXML algorithm for optimizing propensity scored nDCG. PfastreXML was shown to make significantly more accurate predictions on all datasets as compared to the state-of-the-art. PfastreXML was demon-strated to be specially well suited to predicting tail labels which is the most challenging aspect of extreme multi-label learning. This helped PfastreXML achieve significantly higher clickthrough rates for sponsored search advertising on Bing as compared to the large ensemble of highly specialized rankers currently in production. In terms of scaling, PfastreXML could train on WikiLSHTC-325K and Ads-9M in less than 30 minutes and 17 hours respectively using a 16 core In-tel Xeon 2.6 GHz server. Finally, PfastreXML X  X  predictions were made in under 1.5 milliseconds per test point which is critical for deployment in real world applications. The code for PfastreXML is available from [1].
 We are grateful to Rahul Agrawal, Samy Bengio, Abhishek Kadian, Shrutendra Harsola, Purushottam Kar, Prateek Jain and Ambuj Tewari for discussions, feedback and help with experiments. Himanshu Jain is supported by a Google PhD Fellowship. Yashoteja Prabhu is supported by a TCS PhD Fellowship and MSR India travel grant. Manik Varma would like to thank the School of Information Technology at IIT Delhi where he holds an adjunct position.

