 Context-Aware Recommender Systems locally adapt to a specific contextual situation the rating prediction computed by a traditional context-free recommender. In this paper we present a novel semantic pre-filtering approach that can be tuned to the optimal level of contextualization by aggregating contextual situations that are similar to the target one. The similarities of contextual situations are derived from the available contextually tagged users X  ratings according to how similarly the contextual conditions influence the user X  X  rating behavior. We present an extensive evaluation of the performance of our pre-filtering approach on several data sets, showing that it outperforms state-of-the-art context-aware Matrix Factorization approaches. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  information filtering . Context-Aware Recommender Systems; Implicit Semantics ; Collaborative Filtering; Matrix Factorization Context-Aware Recommender Systems (CARSs) differ from traditional recommenders because they estimate the rating of a target user u for an item  X  , not only by using a data set of ratings (of users for items), but also exploiting the knowledge of the contextual situations under which the ratings were acquired and the contextual situation of the target user asking for a recommendation. In this paper we use the term contextual factor when referring to a specific type of contextual information (e.g. weather), and contextual condition when referring to a specific value of a contextual factor (e.g. sunny). The term contextual situation refers to a combination of contextual conditions that describe the context in which the user experienced the item. The main intuition supporting CARS techniques is that, when context matters, the ratings acquired in the target contextual situation should be more relevant for predicting what to recommend in that situation. As we will describe in the next section, in the last years several approaches for exploiting contextual knowledge have been proposed . We view CARS as locally adapted prediction models, i.e., adapted to a local contextual situation. Hence, as in other machine learning tasks, there is the need to judge to what extent the predictive model must fit the local situation. In other words, one must detect the optimal middle point between a global model based on all user preferences (i.e. personalization without context) and a local model just fitting the user preferences in a specific context. This means that one must find the optimal level of contextualization for a given rating data and contextual situation. For instance, a high degree of contextualization may not be appropriate when the contextual situation is too specific and not relevant, or when there are not enough ratings in that situation for generating reliable predictions. Actually, here we present an effective approach to accomplish this task: to find the right level of contextualization. In a recent work [5] we proposed a flexible pre-filtering approach that exploits the semantic similarities between conditions during rating aggregation in order to find the optimal level of contextualization. Our approach follows a two-step process: (1) we first estimate the similarity of apparently unrelated contextual conditions, by focusing on the ir effect on user X  X  rating b ehavior ; (2) we exploit these condition-to-condition similarities to estimate the situation-to-situation similarities which are then used to expand the set of ratings for a target contextual situation aggregating the ratings obtained in the most similar situations. In this work we improve the accuracy and computation efficiency of the proposed semantic pre-filtering approach by redefining its key component: the situation-to-situation similarity computation method. We show that our pre-filtering approach combined with standard Matrix Factorization ( MF ) is able to outperform, on several contextually tagged data sets, context-free MF (i.e. the global model) , exact pre-filtering MF (i.e. the local MF models based only on ratings acquired exactly in the recommendation  X  X  target situation), and state-of-the-art MF prediction models based on the contextual modeling paradigm. Furthermore, we provide a detailed performance analysis on the considered data sets using different contextualization levels; ranging from context-free MF (global model) to exact pre-filtering (perfect local models) . The remainder of this paper is organized as follows. Section 2 positions our work with respect to the state of the art. Section 3 presents our novel semantic pre-filtering approach to CARS. Sections 4 and 5 present the evaluation and its results, and Section 6 draws the main conclusions and describes the future work. CARS have been classified into three paradigms [1] : (1) pre-filtering, where context is used for selecting the relevant ratings before computing predictions with a traditional context-free model; (2) contextual post-filtering, where context is used to adjust predictions generated by a context-free model; and (3) contextual modeling, in which contextual information is directly incorporated in a context-free model as additional parameters. Exact p re-filtering rigidly implements the basic intuition behind CARS: when context matters, generate prediction using only the data acquired in the target contextual situation. Its main limitation is the lack of flexibility: it always uses the maximum level of contextualization, a perfect local model . In the literature, several solutions to this limitation have been proposed . The most popular one is generalized pre-filtering [1] that exploits the available hierarchical relations between contextual conditions when searching for the optimal contextual segment (i.e. aggregations of semantically-related situations) for the target contextual situation . However, this solution is computationally expensive and its performance depends on the quality of the context ontology that is often limited to general hierarchical relationships between conditions of the same contextual factor. Recent solutions have focused on integrating contextualization into a global MF model instead of building several local models. An example is item splitting [2] , a pre-filtering variant that identifies in a selective way the relevant contextual conditions for the rating prediction of each item. Here ratings X  filtering is selectively carried out item by item, for the most relevant contextual condition, and then a prediction model i s built using all the data. In the category of contextual modeling, tensor factorization [7] extends standard MF to an n dimensional model according to the number of contextual factors . Its main limitation is the computational complexity, being applicable only when the number of contextual factors (i.e. dimensions) is small . Th e authors present ed a more scalable approach in [4] , which consists of extending standard MF model with contextual baseline predictors modeling the relationships between conditions and items. However, it is effective only when the target contextual situations are described by one or few conditions. Furthermore, all these approaches do not exploit the potential similarities between conditions , and, as we will show in Section 5, this knowledge may be useful to improve the performance of CARS. Similarly to generalized pre-filtering, we exploit semantic relations between contextual situations to find the optimal level of contextualization for building the local models. But instead of using ontology-based semantic relations, our approach looks for the implicit semantics of contextual conditions that can be estimated from rating data. W e deem two contextual conditions similar if they influence the users X  ratings in a similar way . To compute these similarities from rating data, we adapted Latent Semantic Analysis (LSA) [6] to the task of reducing a context-by-item or context-by-user matrix where entries are real values that measure the influence of conditions on user X  X  rating behavior. We first measure the influence of a condition on the users  X  preferences as the aggregat ed deviation between the actual rating value when the condition holds, and a predicted context-free rating. This aggregation can be based on different perspectives . Here we propose two variants : user-based , in which we use a per-user aggregation; and item-based , where a per-item aggregation is used . If additional data ab out the items or the users are available , then other aggregations may be employed, such as those based on item types (e.g. movie genres) or users' groups (e.g. age). Let us denote with R ic the set of ratings, acquired in contextual user u in condition c . The importance weight of the condition c and item i (w ci ) or user u (w cu ) is calculated as the average of the deviations between the ratings r uic in R ic or R uc , respectively, and the conte xt-free baseline predictions b ui : where is used to reduce the estimated weights when R are small . We set the value of experimentally; in our experiments . In this work we used as baseline predictor the context-free model presented in [8 ] : , where is the overall rating average, is the bias associated to the user u , and the bias associated to the item i . We also tested more sophisticated predictors but no significant differences were observed, hence we decided to use a simple one . Then , we reduce the dimensionality of the condition-item or condition-user matrix M by using SVD. As a result, we obtain a l -dimensional vector of latent features that represents the implicit semantics of a condition c : T he optimal dimensionality l depends on data spars ity and was also identified experimentally (see Section 4 for more details about how we have set the model hyper-parameters and Table 3 where we show the optimal l values for the considered data sets). Assume now that there is a training set of rating data provided by users for items in various contextual situations S , and, a situation s contextual factors, c j = 0,1,. ..,z j are the contextual conditions for the j-th factor, and c j =0 means that the j-th condition is unknown . The semantic pre-filtering algorithm is a two-step process. Firstly, given a target contextual situation s* , for each possible contextual situation s in S , we measure how similar s is to the target situation s* , Sim(s , s*), by computing the cosine similarity between the vector representations of the two situations y s and y defined by one condition c , then , i.e., it is equal to the l-dimensional representation of the contextual condition into the reduced latent feature model representing it, as described in Eq. 3 . While if s is described by multiple conditions, we define y average of the vectors y c of its known conditions C s Secondly, all the ratings acquired in a contextual situation s where Sim(s , s *)  X  t (being t a similarity threshold) are then aggregated to the training set X s* , which is used for building the local rating prediction model for the target situation s* . Denoting with R set of ratings acquired in situation s , and with t a similarity threshold we have that: The similarity threshold t is a global parameter of the data set: the larger the threshold, the stronger the contextualization level of the local contextual models is, i.e., the local model of each target contextual situation is built aggregating less contextual situations more similar to the target one . When t= 1 the local mod els are those built by exact pre-filtering, and only the ratings acquired exactly in the target contextual situation are used. Conversely, when the threshold is set to -1 the local models are all equal and are built using all the rating data, i.e., they are not local anymore and coincide with the context-free model. The goal of our approach is to find the level of contextualization (i.e. a threshold between -1 and 1) that yields the best prediction accuracy. In our experiments all the predictive models are based on MF; they only differ in the set of ratings used for training. Note that i f there is more than one rating for a given user and item acquired in some contextual situations similar to the target one , these ratings are aggregated (average d) in order to generate a unique rating for user, item and target contextual situation. In order to evaluate our semantic pre-filtering technique we have considered several contextually tagged data sets with different characteristics (see Table 1). Music and Tourism data sets [3] contain ratings for items and users in contextual situations described by one condition only. They also contain ratings not tagged with a specific condition (context-free) . Comoda [10] and Adom [1] data sets contain at most one rating per user and movie in a contextual situation that is described by the conjunction of several conditions (e.g. summer , home and alone ). LibraryThing and MovieLens are data sets about books and movies, respectively, which contain ratings augmented with user tags (e.g. funny ), extracted from their respective websites contextual situation is described by the tags assigned by the users to the rated items: they provide a contextual clue of why the item is important for the user. But given the inherent noise of user-generated tag s, we only used those tags that have a statistically significant impa ct on the user X  X  rating behavior and have been used by a minimum number of users (5 users in MovieLens and 200 in LibraryThing). In particular, as significance test we used Pearson X  X  chi -squared that proved to be an effective method to identify relevant contextual information [10]. We selected the tags with significant dependency on the ratings (99% confidence). As suggested by [11] we measured the prediction accuracy of the considered models by conducting a per-user evaluation by randomly selecting, for each user, n ratings as test set (n=5 in LibraryThing and n=3 in the others) . The remaining ratings were used to acquire the implicit semantics of contextual conditions and train the MF models. W e then measured the Mean Absolute Error (MAE) of the predictions on the test items. All the reported results are averages of per-user evaluations, and the statistical significance of the differences between evaluated models has been calculated using the paired Wilcoxon sign rank test. We evaluated the performance of the two variants of the semantic pre-filtering described in Section 3 : using the item-based implicit semantics perspective according to Eq. 1 ( Pref-IB ), and using the user-based one according to Eq . 2 ( Pref-UB ). To build the local prediction models, with the training ratings identified by each pre-filtering approach, we used a standard MF approach that generates the rating estimation for the user u and item i as the sum of the user and item biases and the dot product between their corresponding vectors of latent features (p u ) and (q i ): For comparisons, we also evaluated two variants of the contextual modeling approach presented in [4] and also used in [10], which extends MF by adding baseline parameters that model the influence of the conditions on the items (CAMF-CI) or on the users (CAMF-CU). The parameters of the MF-based models are optimized using stochastic gradient descent. The numeric hyper-parameters of the models were optimized on a single validation se t by using the Nelder-Mead search algorithm [9] . See [www.librarything.com] and [www.movielens.org] (accessed August 5 th , 2013) Here we evaluate the performance of the two variants of the semantic pre-filtering approach ( Pref-IB and Pref-UB ) against the context-free model ( MF ), exact pre-filtering ( Exact ) and two contextual modeling variants ( CAMF-CI and CAMF-CU ). Table 2 shows that the proposed prediction models achieve the best accuracy in most of the data sets . We can observe that in the data sets where contextual situations are described by one or few conditions (viz. Tourism, Music and LibraryThing) the semantic pre-filtering performs similarly to the contextual modeling approach, especially in terms of MAE. However, in data sets where situations are defined by several conditions (viz. Comoda and Adom) our approach clearly outperforms it. Similar results were obtained for Normalized Discounted Cumulative Gain (N DCG) (not shown here for space reasons). As it can be observed in Table 3 (last two columns) the proposed semantic pre-filtering technique always outperforms MF and Exact . We can also observe that when the optimal level of contextualization is used (using the optimal threshold shown in the second data column), the average number of ratings increases by several factors with respect to the local models used by Exact (sub-column  X  X s. Exact  X  in  X  E xpansion X ), and considerably differs among data sets. In general, the proposed method produces the best results, compared with both Exact and MF , when the optimal level of contextualization causes a significant increase of the number of ratings that are used to build the local models. This occurs for Music, Tourism, Comoda and LibraryThing data sets where the local models built by our approach are on average 7.9, 7.2, 20 and 677 times larger than those built by Exact . We can also observe that the value of the similarity threshold t is not important, what matters most is the amount of ratings that must be aggregated. For instance, a threshold of 0.9 is much more restrictive in Adom compared to Comoda. This is related to the average similarity of contextual situations in a data set. In fact, in Ad om the average similarity between pairs of situations is small, 0.38, while in Comoda is much larger, 0.86; hence in Adom there are much less pairs of situations that are more similar than 0.9, which is the optimal threshold for both data sets. significant ly better (95% confidence) than context-free MF and results underlined are also significantly better than Exact
Table 3. MAE performance in optimal contextualization level We note that the effect of the situation-to-situation similarity threshold on the prediction accuracy is similar to the effect of the user-to-user similarity in user-based Collaborative Filtering (CF): using a lower similarity threshold yields to a larger neighbor set and it is well known that it is important to find the right level of locality (neighbor size) in user-based CF . Figure 1 illustrates the MAE and the level of contextualization in terms of ratings increase (bottom) as functions of the similarity threshold in the Comoda data set. The minimum MAE is obtained when the threshold is 0.9, which causes an aggregation of (on average) 730 situations and produces an increase of 20 times the average model size used by Exact . Compared to the all-pairs situation-to-situation similarity strategy presented in [5] , the one proposed here always performs better and has a reduced time complexity : its cost is at most O( n ) rather than O(n 2 ), n being the number of contextual factors. 
Figure 1 . MAE and level of contextualization as functions of W e have presented a semantic pre-filtering approach to CARS that can be tuned to find the right level of contextualization. This approach employs a global situation-to-situation similarity threshold that can be used to identify the ratings acquired in situations similar to the target one and that can be effectively used in the given target contextual situation to train the prediction model . The key component of our approach is a novel method for a ccurately measuring the similarity between contextual situations that is based on how similarly two different situations influence the users X  ratings . Therefore, our method does not depend on external knowledge sources as in generalized pre-filtering. The prediction accuracy of the proposed approach has been experimentally compared to state-of-the-art techniques on several contextually tagged data sets. We have shown that the proposed pre-filtering method outperforms bot h context-free MF and other CARS techniques based on MF as well. To further improve the proposed semantic pre-filtering technique we will investigate a variant able to fine tune the level of contextualization per contextual situation. We believe that in some data sets where the specificity and relevance of contextual situations considerably differ, this fined-grained tuning may produce even better prediction accuracy. The research described in this paper is partly supported by the SuperHub and the Citclops European projects (FP7-ICT-2011-7 , FP7-ENV-308469), and the Universitat Polit X cnica de Catalunya-BarcelonaTech (UPC) under an FPI-UPC grant. The opinions expressed in this paper are those of the authors and are not necessarily those of SuperHub or Citclops projects  X  partners. [1] Adomavicius, G. and Tuzhilin, A. 201 1. Context-aware [2] Baltrunas, L. and Ricci, F. 2009. Context-based splitting of [3] Baltrunas, L., Ludwig, B., Peer, S. and Ricci, F. 2012. [4] Baltrunas, L., Ludwig, B. and Ricci, F. 2011. Matrix [5] Codina, V., Ricci, F. and Ceccaroni, L. 2013. Exploiting the [6] Dumais, S. 2006. LSA and information retrieval: Getting [7] Karatzoglou, A., Amatriain, X., Baltrunas, L. and Olivier, N. [8] Koren, Y. and Bell, R. 2011. Advances in Collaborative [9] Nelder , A. and Mead , R. 1965. A simplex method for [10] Odi X  , A., Tkal X i X  , M., Tasi X  , J. and Ko X ir , A. 2013. [1 1] Shani, G. and Gunawardana, A. 2011. Evaluating 
