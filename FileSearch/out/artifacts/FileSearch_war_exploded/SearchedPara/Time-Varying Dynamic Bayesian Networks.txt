 Analysis of biological networks has led to numerous advances in understanding the organizational tems [1] and central nervous systems [2]. However, most such results are based on static networks , that is, networks with invariant topology over a given set of biological entities. A major challenge in systems biology is to understand and model, quantitatively, the dynamic topological and functional properties of biological networks. We refer to these time or condition specific biological circuitries as time-varying networks or structural non-stationary networks , which are ubiquitous in biological systems. For example ( i ) over the course of a cell cycle, there may exist multiple biological  X  X hemes X  that determine functions of each gene and their regulatory relations, and these  X  X hemes X  are dynamic and stochastic. As a result, the molecular networks at each time point are context-dependent and can undergo systematic rewiring rather than being invariant over time [3]. ( ii ) The emergence of a unified cognitive moment relies on the coordination of scattered mosaics of functionally special-ized brain regions. Neural assemblies, distributed local networks of neurons transiently linked by dynamic connections, enable the emergence of coherent behaviour and cognition [4].
 A key technical hurdle preventing us from an in-depth investigation of the mechanisms that drive temporal biological processes is the unavailability of serial snapshots of time-varying networks un-derlying biological processes. Current technology does not allow for experimentally determining a series of time specific networks for a realistic dynamic biological system. Usually, only time series measurements of the activities of the nodes can be made, such as microarray, EEG or fMRI. Our goal is to recover the latent time-varying networks underlying biological processes, with temporal resolution up to every single time point based on time series measurements of the nodal states. Re-cently, there has been a surge of interests along this direction [5, 6, 7, 8, 9, 10]. However, most existing approaches are computationally expensive, making large scale genome-wide reverse engi-neering nearly infeasible. Furthermore, these methods also lack formal statistical characterization of the estimation procedure. For instance, non-stationary dynamic Bayesian networks are introduced in [9], where the structures are learned via MCMC sampling; such approach is not likely to scale up to more than 1000 nodes and without a regularization term it is also prone to overfitting when the dimension of the data is high but the number of observations is small. More recent efforts have focused on efficient kernel reweighted or total-variation penalized sparse structure recovery meth-ods for undirected time-varying networks [10, 11, 12], which possess both attractive computational schemes and rigorous statistical consistency results. However, what has not been addressed so far is how to recover directed time-varying networks. Our current paper advances in this direction. More specifically, we propose time-varying dynamic Bayesian networks (TV-DBN) for modeling the directed time-evolving network structures underlying non-stationary biological time series. To make this problem statistically tractable, we rely on the assumption that the underlying network structures are sparse and vary smoothly across time. We propose a kernel reweighted ` 1 -regularized auto-regressive approach for learning this sequence of networks. Our approach has the following at-tractive properties: ( i ) The aggregation of observations from adjacent time points by kernel reweight-ing greatly alleviates the statistical problem of sample scarcity when the networks can change at each time point whereas only one or a few time series replicates are available. ( ii ) The problem of structural estimation for a TV-DBN decomposes into a collection of simpler and atomic struc-tural learning problems. We can choose from a battery of highly scalable ` 1 -regularized least-square solvers for learning each structure. ( iii ) We can formally characterize the conditions under which our estimation procedure is structurally consistent: as time series are sampled in increasing resolution, our algorithm can recover the true structure of the underlying TV-DBN with high probability. It is worth emphasizing that our approach is very different from earlier approaches, such as the structure learning algorithms for dynamic Bayesian networks [13], which learn time-homogeneous dynamic systems with fixed node dependencies, or approaches which start from an a priori static is that edges that are transient over a short period of time may be missed by the summary static network in the first place. Furthermore, our approach is also different from change point based al-gorithms [14, 8] which first segment time series and then fit an invariant structure to each segment. These approaches can only recover piece-wise stationary models rather than constantly varying net-works. In our experiments, we demonstrate the advantange of TV-DBNs using synthetic networks. We also apply TV-DBNs to real world datasets: a gene expression dataset measured during yeast cell cycle; and an EEG dataset recorded during a motor imagination task. In both cases, TV-DBNs reveal interesting time-varying causal structures of the underlying biological systems. We concern ourselves with stochastic processes in time or space domains, such as the dynamic con-trol of gene expression during cell cycle, or the sequential activation of brain areas during cognitive decision making, of which the state of a variable at one time point is determined by the states of a set of variables at previous time points. Models describing a stochastic temporal processes can be naturally represented as dynamic Bayesian networks (DBN) [15]. Taking the transcriptional regula-expression levels of p genes at time t , a stochastic dynamic process can be modeled by a  X  X irst-order Markovian transition model X  p ( X t | X t  X  1 ) , which defines the probabilistic distribution of gene ex-pressions at time t given those at time t  X  1 . Under this assumption, the likelihood of the observed expression levels of these genes over a time series of T steps can be expressed as: X individual genes, i.e. , Q i p ( X t i | X t  X  1  X  tion that takes multiple covariates (regulators) and produce a single response.
 A simple form of the transition model p ( X t | X t  X  1 ) in a DBN is a linear dynamics model : where A  X  R p  X  p is a matrix of coefficients relating the expressions at time t  X  1 to those of the next time point, and is a vector of isotropic zero mean Gaussian noise with variance  X  2 . In this case, the gate function that defines the conditional distribution p ( X t i | X t  X  1  X  univariate Gaussian, i.e. , p ( X t i | X t  X  1  X  the matrix A . This model is also known as an auto-regressive model.
 The major reason for favoring DBNs over standard Bayesian networks (BN) or undirected graph-ical models is its enhanced semantic interpretability. An edge in a BN does not necessarily imply causality due to the Markov equivalence of different edge configurations in the network [16]. In DBNs (of the type defined above), all directed edges only point from time t  X  1 to t , which bear a natural causal implication and are more likely to suggest regulatory relations. The auto-regressive model in (2) also offers an elegant formal framework for consistent estimation of the structures of DBNs; we can read off the edges between variables in X t  X  1 and X t by simply identifying the nonzero entries in the transition matrix A . For example, the non-zero entries of A i  X  represent the set of regulator X  X  i that directly lead to a response on X i .
 Contrary to the name of dynamic Bayesian networks may suggest, DBNs are time-invariant models and the underlying network structures do not change over time. That is, the dependencies between  X  X ynamic X  only means that the DBN can model dynamical systems. In the sequel, we will present a new formalism where the structures of DBNs are time-varying rather than invariant. We will focus on recovering the directed time-varying network structure (or the locations of non-problems studied in [11, 12], but in our case for auto-regressive models (and hence directed net-works). Structure estimation results in parse models for easy interpretation, but it is statistically more challenging than the value estimation problem. This is also different from estimating a non-stationary model in the conventional sense, where one interests in recovering the exact values of the varying coefficients [17, 18]. To make this distinction clear, we use the following 3 examples: Matrices B 1 and B 2 encode the same graph structure, since the locations of their non-zero entries are exactly same. Although B 1 is closer to B 3 than B 2 in terms of matrix values (eg. measured in Frobenius norm), they encodes very different graph strucutres.
 Formally, let graph G t = ( V , E t ) represents the conditional independence relations between the components of random vectors X t  X  1 and X t . The vertex set V is a common set of variables underlying X 1: T , i.e. , each node in V corresponds to a sequence of variables X 1: T i . The edge set E t  X  X   X V contains directed edges from components of X t  X  1 to those of X t ; an edge ( i,j ) 6 X  X  t Due to the time-varying nature of the networks, the transition model p t ( X t | X t  X  1 ) in (1) becomes time dependent. In the case of the auto-regressive DBN in (2), its time-varying extension becomes: and our goal is to estimate the non-zero entries in the sequence of time dependent transition matrices { A t } ( t = 1 ...T ). The directed edges E t := E t ( A t ) in network G t associated with each A t can be recovered via E t = ( i,j )  X  X   X V | i 6 = j, A t ij 6 = 0 . Note that if we follow the naive assumption that each temporal snapshot is a completely different network, the task of jointly estimating { A t } by maximizing the log-likelihood would be statistically impossible because the estimator would suffer from extremely high variance due to sample scarcity. Therefore, we make a statistically tractable yet realistic assumption that the underlying network structures are sparse and vary smoothly across time; and hence temporally adjacent networks are likely to share common edges than temporally distal networks. Overall, we have designed a procedure that decomposes the problem of estimating the time-varying networks along two orthogonal axes. The first axis is along the time, where we estimate the network for each time point separately by reweighting the observations accordingly; and the second axis is along the set of genes, where we estimate the neighborhood for each gene separately and then join these neighborhoods to form the overall network. One benefit of such decomposition is that the estimation problem is reduced to a set of atomic optimizations, one for each node i ( i = 1 ... |V| ) at each time point t  X  ( t  X  = 1 ...T ): where  X  is a parameter for the ` 1 -regularization term, which controls the number of non-zero en-observation from time t when we estimate the network at time t  X  . More specifically, it is defined as w the kernel bandwidth. We use a Gaussian RBF kernel, K h ( t ) = exp(  X  t 2 h ) , in our later experiments. Note that multiple measurements at the same time point are considered as i.i.d. observations and can be trivially handled by assigning them the same weights.
 The objective defined in (5) is essentially a weighted regression problem. The square loss function is due to the fact that we are fitting a linear model with uncorrelated Gaussian noise. Two other key components of the objective are: ( i ) a kernel reweighting scheme for aggregating observations across time; and ( ii ) an ` 1 -regularization for sparse structure estimation. The first component origi-nates from our assumption that the structural changes of the network vary smoothly across time. This assumption allows us to borrow information across time by reweighting the observations from dif-ferent time points and then treating them as if they were i.i.d. observations. Intuitively, the weighting should place more emphasis on observations at or near time point t  X  with weights becoming smaller as observations move further away from time point t  X  . The second component is to promote sparse structure and avoid model overfitting. This is also consistent with the biological observation that net-works underlying biological processes are parsimonious in structure. For example, a transcription factor only controls a small fraction of target genes at particular time point or under a specific con-dition [19]. It is well-known that ` 1 -regularized least square linear regression, has a parsimonious property, and exhibits model-selection consistency (i.e., recovers the set of true non-zero regression coefficients asymptotically) in noisy settings even when p T [20].
 Note that our procedure can also be easily extended to learn the structure of auto-regressive models of higher order D : X t = P D d =1 A t ( d )  X  X t  X  d + ,  X  N ( 0 , X  2 I ) . The change we need to make is to incorporate the higher order auto-regressive coefficients in the square loss function, i.e. , ( x i  X  P Estimating time-varying networks using the decomposition scheme above requires solving a collec-tion of optimization problems in (5). In a genome-wide reverse engineering task, there can be tens of thousands of genes and hundreds of time points, so one can easily have a million optimization problems. Therefore, it is essential to use an efficient algorithm for solving the atomic optimization problem in (5), which can be trivially parallelized for each genes at each time point. Instead of solving the form of the optimization problem in (5), we will push the weighting w  X  ( t ) into the square loss function by scaling the covariates and response variables by p w t  X  ( t ) , problem becomes a standard ` 1 -regularized least-square problem which can be solved via a bat-tery of highly scalable and specialized solvers, such as the shooting algorithm [21]. The shooting algorithm is a simple, straightforward and fast algorithm that iteratively solves a system of nonlin-dated by holding all other entries fixed. Overall, our procedure for estimating time-varying networks is summarized in Algorithm 1, which uses the shooting algorithm as the key building block (step Algorithm 1 : Procedure for Estimating Time-Varying DBN Input : Time series { x 1 ,..., x T } , regularization parameter  X  and kernel parameter h . Output : Time-varying networks { A 1 ,..., A T } . begin end 7-10). In step 5, we uses a warm start for each atomic optimization problem: since the networks In this section, we study the statistical consistency of the estimation procedure in Section 4. Our analysis is different from the consistency results presented by [11] on recovering time-varying undi-rected graphical models. Their analysis deals with Frobenius norm consistency which is a weaker result than the structural consistency we pursue here. Our structural consistency result for TV-DBNs estimation procedure follows the proof strategy of [20]; however, the analysis is complicated by two major factors. First, times series observations are very often non-i.i.d.  X  current observations may depend on past history. Second, we are modeling non-stationary processes, where we need to deal with the additional bias term that arises due to locally stationary approximation to non-stationarity. In the following, we state our assumptions and theorem, but leave the detailed proof of this theorem for a full version of the paper (a sketch of the proof can be found in the appendix). Theorem 1 Assume that the conditions below hold: Let the regularization parameter scale as  X  = O ( p (log p ) /Th ) , the minimum absolute non-zero To the best of our knowledge, this is the first practical method for structure learning of non-stationary DBNs. Thus we mainly compare with static DBN structure learning methods. The goal is to demon-strate the advantage of TV-DBNs for modeling time-varying structures of non-stationary processes which are ignored by traditional approaches. We conducted 3 experiments using synthetic data, gene expression data and EEG signals. In these experiments, TV-DBNs either better recover the underlying networks, or provide better explanatory power for the underlying biological processes. Synthetic Data In this experiment, we generate synthetic time series using a first order auto-regressive models with smoothly varying model structures. More specifically, we first generate random graph of node size p = 50 and average indegree of 2 (we have also experimented with p = 75 and 100 which provides similar results). We then evenly space these 8 anchor matrices, and interpolate a suitable number of intermediate matrices to match the number of observations T . Due to the interpolation, the average indegree of each node is around 4. With the sequence of We then study the behavior of TV-DBNs and static DBNs [22] in recovering the underlying varying networks as we increase the number of observations T . We also compare with a piecewise constant DBN that estimate a static network for each segment obtained from change point detection [14]. For the TV-DBN, we choose the bandwidth parameter h of the Gaussian kernel according to For all methods, we choose the regularization parameter such that the resulting networks has an average indegree of 4. We evaluate the performance using an F1 score, which is the har-monic mean of precision and recall scores in retrieving the true time-varying network edges. We can see that estimating a static DBN or a piecewise constant DBN does not provide a good estimation of the network structures (Figure 1). In contrast, the TV-DBN leads to a significantly higher F1 score, and its perfor-mance also benefit quickly from increasing the number of observations. Note that these results are not surpris-ing since time-varying networs simply fit better with the data generating process. As time-varying networks occur often in biological systems, we expect TV-DBNs will be useful for studying biological systems.
 Yeast Gene Regulatory Networks. In this experiment, we will reverse engineer the time varying gene regulatory networks from time series of gene expression measured across two yeast cell cycles. A yeast cell cycle is divided into four stages: S phase for DNA synthesis, M phase for mitosis, and G1 and G2 phase separating S and M phases. We use two time series (alpha30 and alpha38) from [23] which are technical replicates of each other with a sampling interval of 5 minutes and are common to both arrays. We choose the bandwidth parameter h such that the weighting decay to exp(  X  1) for half of a cell cycle, i.e. exp(  X  6 2 /h ) = exp(  X  1) . We choose the regularization parameter such that the sparsity of the networks are around 0 . 01 .
 During the cell cycle of yeasts, there exist multiple underlying  X  X hemes X  that determine the func-tionalities of each gene and their relationships to each other, and such themes are dynamical and stochastic. As a result, the gene regulatory networks at each time point are context-dependent and can undergo systematic rewiring, rather than being invariant over time. A summary of the estimated time-varying networks are visualized in Figure 2. We group genes according to 50 ontology groups. We can see that the most active groups of genes are related to background processes such as cy-toskeleton organization, enzyme regulator activity, ribosome activity. We can also spot transient interactions, for instance, between genes related to site of polarized growth and nucleolus (time point 18), and between genes related to ribosome and cellular homeostasis (time point 24). Note that, although gene expressions are measured across two cell cycles, the values do not necessarily exhibit periodic behavior. In fact, only a small fraction of yeast genes (less than 20%) has been reported to exhibit cycling behavior [23]. Figure 2: Interactions between gene ontological groups. The weight of an edge between two ontological Next we study genes sets that are related to specific stage of cell cycle where we expect to see periodic be-havior. In particular, we obtain gene sets known to be related to G1, S and S/G2 stage respectively. 1 We use interactivity, which is the total number of edges a group of genes is connected to, to describe the activity of each group of genes. Since the regulatory networks are directed, we can examine both indegree and out-degree separately for each gene sets. In Figure 3(a)(b)(c), the interactivities of these genes indeed exhibit periodic behavior which corresponds well with their supposed functions in cell cycles. We also plot the histogram of indegree and outdegree (averaged across time) for the time-varying networks in Figure 3(d). We find that the outdegrees approximately follow a scale free distribution with largest outdegree reaching 90. This corresponds well with the biological observation that there are a few genes (regulators) that regulate a lot of other genes. The indegree distribution is very dif-ferent from that of the outdegree, and it exhibits a clear peak between 5 and 6. This also corresponds well with biological observations that most genes are controlled only by a few regulators. To further assess the modeling power of the time-varying networks and its advantage over static network, we perform gene set enrichment studies. More specifically, we use three types of infor-mation to define the gene sets: transcription factor binding targets (TF), gene knockout signatures (Knockout), and gene ontology (Ontology) groups [24]. We partition the genes in the time varying networks at each time point into 50 groups using spectral clustering, and then test whether these groups are enriched with genes from any predefined gene sets. We use a max-statistic and a 99% confidence level for the test [25]. Table 1 indicates that time-varying networks are able to discover more functional groups as defined by the genes sets than static networks as commonly used in bio-logical literature. In the appendix, we also visualize the time spans of these active functional groups. It can be seen that many of them are dynamic and transient, and not captured by a static network. Brain Response to Visual Stimuli. In this experiment, we will explore the interactions between brain regions in response to visual stimuli using TV-DBNs. We use the EEG dataset from [26] where five healthy subjects (labeled  X  X a X ,  X  X l X ,  X  X v X ,  X  X w X  and  X  X y X  respectively) were required to imagine body part movement based on visual cues in order to generate EEG changes. We focus our Figure 3: (a) Genes specific to G1 phase are being regulated periodically; we can see that the average in-the visual cue is presented. We bandpass filter data at 8 X 12 Hz to obtain EEG alpha activity. We further normalize each EEG channel to zero mean and unit variance, and estimate the time-varying networks for all 5 subject using exactly the same regularization parameter and kernel bandwidth ( h s.t. exp(  X  (0 . 5) 2 /h ) = exp(  X  1) ). We tried a range of different regularization parameters, but obtained qualitatively similar results to Figure 4.
 What is particularly interesting in this dataset is that subject  X  X v X  is called BCI  X  X lliterate X ; he/she is unable to generate clear EEG changes during motor imagination. The estimated time-varying networks reveal that the brain interactions of subject  X  X v X  is particularly weak and the brain con-nectivity actually decreases as the experiment proceeds. In contrast, all other four subjects show an increased brain interaction as they engage in active imagination. Particularly, these increased inter-actions occur between visual and motor cortex. This dynamic coherence between visual and motor cortex corresponds nicely to the fact that subjects are consciously transforming visual stimuli into motor imaginations which involves the motor cortex. It seems that subject  X  X v X  fails to perform such integration due to the disruption of brain interactions. In this paper, we propose time-varying dynamic Bayesian networks (TV-DBN) for modeling the varying network structures underlying non-stationary biological time series. We have designed a simple and scalable kernel reweighted structural learning algorithm to make the learning possible. Given the rapid advances in data collection technologies for biological systems, we expect that com-plex, high-dimensional, and feature rich data from complex dynamic biological processes, such as cancer progression, immune responses, and developmental processes, will continue to grow. Thus, we believe our new method is a timely contribution that can narrow the gap between imminent methodological needs and the available data and offer deeper understanding of the mechanisms and processes underlying biological networks.
 Acknowledgments LS is supported by a Ray and Stephenie Lane Research Fellowship. EPX is supported
