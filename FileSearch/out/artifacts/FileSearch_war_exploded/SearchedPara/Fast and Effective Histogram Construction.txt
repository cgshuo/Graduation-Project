 Histogram construction or sequence segmentation is a ba-sic task with applications in database systems, informa-tion retrieval, and knowledge management. Its aim is to approximate a sequence by line segments. Unfortunately, the quadratic algorithm that derives an optimal histogram for Euclidean error lacks the desired scalability. Therefore, sophisticated approximation algorithms have been recently proposed, while several simple heuristics are used in prac-tice. Still, these solutions fail to resolve the efficiency-quality tradeoff in a satisfactory manner. In this paper we take a fresh view on the problem. We propose conceptually clear and scalable algorithms that efficiently derive high-quality histograms. We experimentally demonstrate that existing approximation schemes fail to deliver the desired efficiency and conventional heuristics do not fare well on the side of quality. On the other hand, our schemes match or exceed the quality of the former and the efficiency of the latter. F.2 [ Analysis of Algorithms and Complexity ]: Mis-cellaneous; H.3 [ Information Storage and Retrieval ]: Miscellaneous Algorithms, Experimentation, Performance
A histogram or segmentation aims to approximate a se-quence of values by piecewise-constant line segments that effectively capture the basic features of the underlying data. This form of approximation finds applications in database systems, as in the approximation of intermediate join results by a query optimizer [13, 30] and approximate query pro-cessing [33, 3]; the same form of approximation is used in  X  Work supported by Singapore MOE X  X  AcRF grants T12-0701-P01 and T1 251RES0807.
 knowledge management applications (as in decision-support systems [1, 17, 39]), in bio-informatics [37], and in informa-tion retrieval applications as time-series indexing and simi-larity search [4]. An overview of the area from a database perspective is provided in [14, 15].

In all cases, a segmentation algorithm is employed in order to divide a given data sequence into a given budget of con-secutive buckets or segments [12]. All values in a segment are approximated by a single representative. Both these repre-sentative values, and the bucket boundaries themselves, are chosen so as to achieve a low value for an error metric in the overall histogram approximation. A useful metric is the Eu-clidean error. Approximate answers to both point and range queries can be estimated with a histogram [11].
 A histogram optimal in terms of Euclidean error, called V -Optimal , is the optimal choice when approximating at-tribute value frequencies in a database for the estimation of the result size of simple equality join and selection queries [16]. This optimal segmentation is derived by an O ( n 2 dynamic-programming algorithm that recursively examines all possible solutions [2, 20]. Still, this algorithm is not scal-able enough to be used in practice [18]; database systems employ heuristics for histogram construction.

Recent research has revisited the histogram construction problem from the point of view of approximation algorithms. Guha et al. proposed a suite of approximation and stream-ing algorithms for histogram construction problems [10]. Of the algorithms proposed in [10], the AHistL - X  proves to be the best for offline approximate histogram construction. In a nutshell, AHistL - X  builds on the idea of approximating the error function itself, while pruning the computations of the V -Optimal algorithm. Likewise, Terzi and Tsaparas re-cently proposed DnS , an offline approximation scheme for sequence segmentation [38]. DnS divides the problem into subproblems, solves each of them optimally, and then uti-lizes V -Optimal to construct a global solution by merging the segments created in the partial solutions.

However, despite their theoretical elegance, the approxi-mation algorithms proposed in previous research do not al-ways resolve the tradeoff between time complexity and his-togram quality in a satisfactory manner. The running time of these algorithms can approach that of the quadratic-time V -Optimal solution. Still, the quality of segmentation they achieve can substantially deviate from the optimal. Further-more, previous research has not examined how the different approximation algorithms of [10] and [38] compare to each other in terms of efficiency and effectiveness.

In this paper, we propose a middle ground between the theoretical elegance of approximation algorithms on the one hand, and the simplicity, efficiency, and practicality of heuris-tics on the other. We develop segmentation algorithms that are both fast and effective, even though they do not pro-vide approximation guarantees with respect to the optimal solution. Still, our solutions have the scalability and the quality that allows them to be employed in practice, instead of the currently used heuristics, when dealing with the seg-mentation of very large data sets. We conduct the first, to our knowledge, experimental study of state-of-the-art opti-mal, approximation, and heuristic algorithms for histogram construction. This study demonstrates that our algorithms vastly outperform the guarantee-providing approximation schemes in terms of running time, while achieving compara-ble or superior approximation accuracy.
In this section we briefly present previous approaches to histogram construction or segmentation, with a Euclidean-error guarantee in mind. Given an n -size data vector D = d 0 ,d 1 ,...,d n  X  1 , the problem is to devise an approximate representation  X  D of D using at most B space units, so that the Euclidean error of the approximation is minimized. This Euclidean error is expressed as  X  d i denotes the approximate estimated value for d i .Anal-gorithm that aims to minimize L 2 in practice works on the sum-of-squared-errors (SSE) i |  X  d i  X  d i | 2 . Previous studies [20, 5, 23, 9, 36, 24, 25, 26, 21, 22] have generalized their re-sults into wider classes of maximum , distributive , Minkowski-distance , and relative-error metrics. Still, the Euclidean er-ror L 2 remains an important error metric for several appli-cations, such as database query optimization [16], context recognition [12], and time series mining [4].

Depending on the application domain, the same approx-imate representation of D is called a histogram ,a segmen-tation [38], a partitioning ,ora piecewise-constant approxi-mation [4]. Such a representation divides D into B n disjoint intervals [ b i ,e i ], where b i and e i are indices of the data items and 1  X  i  X  B ; these intervals are called buckets or segments . Each segment is attributed a single represen-tative value v i , which approximate all values d j under its scope, j  X  [ b i ,e i ]. A single bucket is described by the triplet s = { b i ,e i ,v i } .2 B  X  1 numbers suffice for a representation of B buckets, given that  X  i, 1 &lt;i  X  B, b i = e i  X  1 + 1, and the two endpoints are fixed.

For a given target error metric, the representative value v of a bucket that minimizes the resulting approximation error is straightforwardly defined as a function of the data values in the bucket. Thus, for the average absolute error the best v i is the median of the values in [ b i ,e i ] [38]; for the maximum absolute error it is the mean of the maximum and minimum value in [ b i ,e i ] [27]; an analysis of respective relative-error cases is offered in [11]. For the Euclidean error L 2 that concerns us, the optimal value of v i is the mean of values in [ b i ,e i ] [20]. The boundary positions e 1 ,...,e of a partitioning that achieves a minimum approximation error depend on the target error metric too. The task of a segmentation algorithm is to define boundary positions that achieve low approximation error for the error metric at hand.
The O n 2 B dynamic-programming (DP) algorithm that constructs an optimal segmentation under the L 2 metric is a special case of Bellman X  X  general line segmentation algo-rithm [2]. It was presented by Jagadish et al. [20] and optimized in terms of space-efficiency by Guha [8]. Its basic underlying observation is that the optimal b -segment his-togram of a data vector D can be recursively derived given the optimal ( b  X  1)-segmentations of all prefix vectors of D . Thus, the minimal sum-of-squared-errors (SSE) E ( i, b )of a b -bucket histogram of the prefix vector d 0 ,d 1 ,...,d recursively expressed as: where E ( j +1 ,i ) is the minimal SSE for the segment d j +1 ,...,d i . This error is easily computed in O (1) based on a few pre-computed quantities (sums of squares and squares of sums) for each prefix [20]. Thus, this algorithm requires a O ( nB ) tabulation of minimized error values E ( i, b ) along with the selected optimal last-bucket boundary positions j that correspond to those optimal error values. As noted by Guha, the space complexity is reducible to O ( n )bydis-carding the full O ( nB ) table; instead, only the two running columns of this table are stored. The middle bucket of each solution is kept track of; after the optimal error is estab-lished, the problem is divided in two half subproblems and the same algorithm is recursively re-run on them, until all boundary positions are set [8]. The runtime is significantly improved by a simple pruning step [20]; for given i and b , the loop over (decreasing) j that searches for the min value in Equation 2 is broken when E ( j +1 ,i ) (non-decreasing as j decreases) exceeds the running minimum value of E ( i, b ).
Unfortunately, the quadratic time complexity of V -Optimal renders it inapplicable in most real-world applications. Thus, several works have proposed approximation schemes for his-togram construction [6, 7, 10, 38].
Terzi and Tsaparas correctly observed that a quadratic algorithm is not an adequately fast solution for practical se-quence segmentation problems [38]. As an alternative to the V -Optimal algorithm, they suggested a sub-quadratic constant-factor approximation algorithm.

The basic idea behind this divide and segment ( DnS )al-gorithm is to divide the overall segmentation problem into smaller subproblems, solve those subproblems optimally, and then combine their solutions. The V -Optimal algorithm serves as a building block of DnS . The problem sequence is arbi-trarily partitioned into smaller subsequences. Each of those is optimally segmented using V -Optimal . Then, the derived segments are treated as the input elements themselves, and a segmentation (i.e., local merging) of them into B larger buckets is performed using the V -Optimal algorithm again. A thorough analysis of this algorithm demonstrates an ap-proximation factor 3 in relation to the optimal L 2 error, and a worst-case complexity of O ( n 4 / 3 B 5 / 3 ), assuming that the original sequence is partitioned into  X  = n B 2 / 3 equal-length segments in the first step.
Guha et al. have provided a collection of approximation algorithms for the histogram construction problem. Out of them, AHistL - X  is their algorithm of choice for offline his-togram construction [10].

The basic observation underlying the AHistL - X  algorithm is that the E ( j, b  X  1) function in Equation 2 is a non-decreasing function of j , while its counterpart function 1 ,i ) is a non-increasing function of j . Thus, instead of com-puting the entire tables of E ( j, b ) over all values of j , this non-decreasing function is approximated by a staircase his-togram representation -that is, a histogram in which the representative value for a segment is the highest (i.e., the rightmost) value in it. In effect, only a few representative values of E ( j, b  X  1), i.e., the end values of the staircase in-tervals, are used. Moreover, the segments of this staircase histogram themselves are selected so that the value of the E ( j, b ) function at the right-hand end of a segment is at most (1 +  X  ) times the value at the left-hand end, where  X  = 2 B . The recursive formulation of Equation 2 remains unchanged, with the difference that the variable j now only ranges over the endpoints of intervals in this staircase his-togram representation of E ( j, b ).

On top of this observation, the AHistL - X  algorithm adds the further insight that any E ( j, b ) value that exceeds the final SSE of the histogram under construction (or even an approximate version of that final error) cannot play a role in the eventual solution. Since E ( j, b ) form partial contri-butions to the aggregate SSE, only values smaller than the final error  X  contribute to the solution. Thus, if we knew the error  X  of the V -Optimal histogram in advance, then we could eschew the computation of any E ( j, b ) value that ex-ceeds it. Since  X  in not known in advance, we can still work with estimates of  X  in a binary-search fashion.
 The AHistL - X  algorithm combines the above two insights. In effect, the problem is decomposed in two parts. The inner part returns a histogram of error less than (1 + ) X , under the assumption that the given estimate  X  is correct, i.e., there exists a histogram of error  X ; the outer part searches for such a well-chosen value of  X . The result is an O ( n + B (log n +  X  2 ) log n )-time algorithm that computes an (1 + )-approximate B -bucket histogram.
Past research has also proposed several heuristics for his-togram construction. Some of these heuristics are relatively brute-force segmentations; this category includes methods such as the end -biased [16], equi -width [28], and equi -depth [32, 31] heuristics.
 A more elaborate heuristic is the MaxDiff [35] method. According to this method, the B  X  1 points of highest dif-ference between two consecutive values in the original data set are selected as the boundary points of a B -bucket his-togram. Its time complexity is O ( n log B ), i.e. the cost of inserting n items into a priority queue of B elements. Poos-ala and Ioannidis conducted an experimental study of several heuristics employed in database applications and concluded that the MaxDiff -histogram was  X  X robably the histogram of choice X . Matias et al. used this method as the conventional approach to histogram construction for selectivity estima-tion in database systems, in comparison to their alternative proposal of wavelet-based histograms [30].

Jagadish et al. [20] have suggested an one-dimensional variant of the multidimensional MHIST heuristic proposed by Poosala and Ioannidis [34]. This is a greedy heuristic that repeatedly selects and splits the bucket with the high-est SSE, making B splits. The same algorithm is mentioned by Terzi and Tsaparas by the name Top -Down [38]; a similar algorithm has been suggested in the context of multidimen-sional anonymization by LeFevre et al. [29]. Its worst-case time complexity is O ( B ( n +log B ), i.e., the cost to create aheapof B items while updating affected splitting costs at each step. In the pilot experimental study of [20], MaxDiff and MHIST turn out to be the heuristics of choice; it is ob-served that the former performs better on more spiked data, while the latter is more competitive on smoother data.
We observe that a gap exists in the research on segmen-tation. While several heuristics have been known for a long time, and more sophisticated approximation schemes have been recently proposed, it is unclear whether any them re-solves the tradeoff between efficiency and accuracy in a sat-isfactory manner. Besides, there has been no face-to-face comparison between them.

In particular, it is not clear whether the recently pro-posed approximation schemes can deliver the time-efficiency that will render them an attractive choice versus the near-linear simple heuristics. The V -Optimal DP algorithm fails to respond to this challenge; approximation schemes of time super-linear in n and/or B may still be problematic in terms of time-efficiency. Thus, there is still a need for an ap-proach that would provide both near-optimal quality and near-linear time complexity. In this paper, we provide algo-rithms that fill this gap.
Our aim is to develop segmentation algorithms that can achieve both good quality (i.e., low approximation error) and time efficiency (i.e., near-linear time complexity). We start out by presenting a simple greedy algorithm.
The rationale of our algorithm is to begin with an orig-inal, ad hoc segmentation S 0 and then greedily modify its boundary positions. S 0 can be randomly created, or it can be that of a simple heuristic, for example an equi -width his-togram [28]. The greedy boundary modification operations matter for the final result more than the choice of S 0 . These operations iteratively move one boundary from one position to another. The objective of these moves is reduce the total L 2 error. For that purpose, we maintain a min-heap H + of all running boundary positions b i ,1  X  i  X  B  X  1( b 0 and b
B are fixed) along with the potential error increase  X  + E that can be incurred if b i is removed. At each iteration, the boundary b  X  i of minimum  X  + E i is discarded.

Furthermore, we also maintain a max-heap H  X  of all run-ning segments s j ,1  X  j  X  B , where s j =[ b j  X  1 ,b j ], along with their potential error decrease  X   X  E j that can be ef-fected if s j is optimally split into two subsegments. At each iteration, the removed boundary is repositioned so as to split the segment of maximum  X   X  E j . Thus, at each iteration, the locally optimal boundary to remove is lifted, the locally op-timal segment to split is partitioned in two, and heaps are accordingly updated. This step is repeated until no further move that decreases the total error can be made. Figure 1 shows a pseudo-code for this GDY algorithm. The time complexity of GDY is O M n B + log B , where M stands for the number of moves, n B for the (average) cost of calculating the optimal split position for a newly created segment after each move, and log B for the overhead of selecting the best-choice boundary to remove and segment to split using the heaps, and updating them at each step.
We now strive to add some extra sophistication in the operation of this simple greedy heuristic. Our enhanced approach starts out from the observation that a dynamic-programming (DP) algorithm that aims to discover a good segmentation does not need to examine every possible bound-ary position per se; it can constrain itself to a limited set of candidate boundary positions that are deemed to be likely to participate in the optimal solution. Our objective is to identify a set of such candidate boundary positions, which we call samples .

Our approach is inspired from the methodologies of Terzi and Tsaparas [38] and Guha et al. [10], but also contains a departure from them. These authors also aim to eschew part of the DP computation without altogether discarding the DP itself. Still, their approaches are burdened by the need to provide provable approximation guarantees. This meticulousness hampers efficiency. We aim to preserve the quality advantage of such an approach, but avoid the effi-ciency overhead.
 In particular, AHistL - X  tries to carefully discard from the DP recursion those terms (i.e., candidate boundary posi-tions) whose error contribution can be approximated by that of their peers [10]. Likewise, DnS attempts to acquire a set of samples (i.e., candidate boundary positions) by run-ning the DP recursion itself in a small scale, within each of the  X  subsequences it creates [38], ending up with  X B samples. Thus, DP is applied in a two-level fashion, both at a micro-scale, within each of the  X  subsequeces, and at a macro-scale, among the derived boundaries themselves. While this approach allows for an error guarantee, it un-necessarily burdens what is anyway a suboptimal algorithm with super-linear time complexity. Likewise, in its effort to provide a bounded approximation of every error quan-tity that enters the problem, AHistL - X  ends up with an O ( B 3 (log n +  X  2 ) log n ) time complexity factor that risks exceeding even the runtime of V -Optimal itself in practice.
In a departure from the above approaches, we opt for a lighter version of eschewing part of the DP computation and selecting samples (i.e., candidate boundary positions). We simply run the fast GDY algorithm itself multiple times al-tering the initial input partitioning in each run, and collect the boundary positions it ends up with, eliminating dupli-cates. All boundary positions collected in this fashion are themselves participants in a B -segmentation of the input se-quence that GDY could not improve further; thus, they are reasonable candidates for an optimal B -segmentation. We emphasize that this approach to sample collection contrasts to the one followed in [38]; the samples in [38] do not them-selves participate in a global B -segmentation, but only in local B -segmentations of subsequences. Thus, even though that methodology allows for the computation of an elegant approximation guarantee, most of the samples it works with are unlikely to participate in an optimal B -segmentation. A similar observation holds for AHistL - X . This algorithm en-deavors to discard computations related to those boundary positions that have produced error upper-bounded by that of one of their neighbors in an examined subproblem; how-ever, it does not aim to work on boundary positions that are more likely to participate in the optimal solution per se.
In case B is less than until we collect up to O ( Optimal DP segmentation algorithm on this set of samples, in order to select a subset of B out of them that define a minimum-error histogram. Although this is a quadratic algorithm, it behaves as linear in n , taking advantage of the fact that the input size is bounded by O ( n ). Thus, dynamic programming is used to provide a high-quality histogram without sacrificing the linear time complexity of GDY . Figure 2 presents a pseudo-code for this GDY DP algorithm.

When B is larger than samples and looses its linear-in-n character, but still runs faster than both DnS and AHistL - X . Still, to deal with larger values of B , we propose a batch version of GDY DP .The idea is to process only divide the sorted sequence of collected samples into subse-quences of at most DP segmentations on each those; and we augment the results into a global B -segmentation. Our approach is reminiscent of the suggested piecewise application of a summarization scheme in [26]. However, it combines the batched or piece-wise processing approach with a sample selection mecha-nism, as in [38]. Thus, it contains the size of the problem in two ways: (i) it examines only selected samples instead of the whole data; and (ii) it processes the data in batches.
In more detail, we use an auxiliary segmentation A ,which is provided by a single run of GDY . For each group G of consecutive samples, we find two dividers l a and r a , among the boundaries in A , that enclose G . We then run V -Optimal on the set G , so as to produce as many boundaries between l and r a as the segmentation A has allocated in this interval. Thus, the number of boundaries in the derived segmentation is kept appropriate, but their positions are bettered within each improve on the boundary positions derived by GDY in A , but we do so in a more sophisticated manner than just greedily moving one boundary at a time. Thus, the quality of the segmentation is improved.

In each group G we are dealing with there are O ( I  X  B ) samples in total to deal with, where I the constant pre-selected number of GDY runs that produces them. Thus, the are O IB  X  n groups of hence the algorithm performs as many separate DP runs. Each run chooses a fraction of the performing an O them. Thus, the overall worst-case time complexity of the algorithm is O ( InB )= O ( nB ). Figure 3 depicts a pseudo-code for this batch-DP algorithm GDY BDP .

The rationale behind GDY BDP is that, when the num-ber of boundaries is large, selecting some breakpoints for them based on a simple GDY solution and then performing GDY DP within the breakpoints does not hamper the overall quality too much. On the contrary, it confers near-optimal quality and allows for near-linear time efficiency. As we shall see, the qual-ity achieved with GDY BDP is always very close to that of GDY DP , while GDY BDP is much faster on large B . GDY BDP also avoids a major loophole in the methodology of DnS . Namely, DnS forces each of the arbitrarily selected subsequences it works on to produce B samples, and then chooses from the total pool of samples. Thus, it does not pay attention to the actual form of the data. Cases where some data regions require much denser segmentation than other regions are not satisfactorily covered by DnS , but they are covered by GDY BDP . To our knowledge, no other his-togram construction algorithm scales well in both the input size n and the number of segments B , while producing, as we will see, near-optimal quality in terms of error.
This section presents our extensive experimental compar-ison of known approximation and heuristic algorithms and the solutions we have proposed. In particular, we have com-pared the following algorithms:
All algorithms were implemented with gcc 4.3.0, and ex-periments were run on a 2 Quad CPU Intel Core 2.4GHz machine with 4GB of main memory running a 64Bit version of Fedora 9. Table 1 summarizes the complexity require-ments of these algorithms.
Our quality assessment uses several real-world time series data sets. Some of these data are also used in [38]. Table 2 presents the original provenance 1 of the data. We have cre-ated aggregated versions of these data sets, i.e. concatenated the time series in them to create a united, longer sequence. In the following figures, we denote these aggregated versions by the appellation  X -a X  in the captioned data set names.
We first direct our attention to a comparison of quality, as measured by the Euclidean error achieved as a function of the available space budget B . In the following figures, the dotted line presents the position of 10% off the optimal error (always achieved by V -Optimal ), while the dash-dotted line is the position of 20% off the optimal.
Figure 4 presents the results with the aggregated balloon data set for a selected range of B = 120 ... 300. We ob-serve that all of DnS , AHistL - X  for =0 . 01, GDY DP , and GDY BDP achieve practically indistinguishable near-optimal error. There are four outliers. The performance of GDY lies reliably along the 10%-off-optimal line; this is the best-performing outlier. The second outlier is MHIST ; as expected, it does not produce histograms of near-optimal quality. Still, the variant of AHistL - X  for =10iseven worse. This result is significant from the point of view of the tradeoff between quality and time-efficiency that AHistL - X  achieves. We shall come back to it later. The MaxDiff heuristic had the worst performance.
Figure 5 shows the error results with the darwin data set for B =40 ... 200. The picture exhibits a pattern similar to the previous one. Still, with this easier to approximate data set, GDY approaches the quality of the other near-optimal algorithms. Furthermore, the quality of MHIST now fol-lows the 10%-off line. Now the MaxDiff heuristic achieves a smaller accuracy gap from the other contenders, but still has the worst quality. The low-quality version of AHistL - X  is again an outlier with unreliable performance. The per-formance with other values of was falling in between these two extremes. Naturally, the value of can be tuned so as to allow for quality that matches any of the other algorithms. We do not present these versions in order to preserve the readability of the graph.
In Figure 6 we present the results for the filtered version of the DJIA data set, for a range of B = 500 ... 1000. This version, also used in [9], contains the first 16384 closing val-ues of the Dow-Jones Industrial Average index from 1900 to 1993; a few negative values were removed. The perfor-mance evaluation with this data set follows the same pattern as before. In this case, neither MaxDiff nor the low-quality version of AHistL - X  are depicted, as they are outliers. On the other hand, the MHIST heuristic performs slightly better than it did in previous cases. Still, our GDY algorithm per-forms even better, while the performance of both GDY DP and GDY BDP is almost indistinguishable from that of both high-performing approximation algorithms in this scale.
Figure 7 depicts the quality results with the aggregated exrates data set for B =8 ... 128. This range of B at the lower values of the domain presents an interesting picture. Not only our advanced greedy techniques, but also the sim-ple GDY can achieve error very close to the optimal. So does the MHIST heuristic as well, which follows at a very close distance. Still, MaxDiff and the low-quality version of AHistL - X  remain low-performing outliers.
Next, we depict the Euclidean error results with the aggre-gated phone data set (Figure 8). The relative performance of different techniques appears clearly in this figure. GDY DP can almost match the optimal error at least as well as the tight-variant of AHistL - X  and DnS , while GDY BDP and GDY follow closely behind. MHIST does not perform as well as our algorithms, while the slack-version of AHistL - X  and MaxDiff are again poor performers.

So far we have presented quality results in terms of Eu-clidean error as a function of B , with the range of B zoomed in so as to allow the discernment of subtle differences. Still, we would also like to get a view of the larger picture: the shape of the E = f ( B ) function, indicating how the Eu-clidean error varies over a the full domain of practical B values. We do so with the synthetic data set. This data set appears highly periodic, but never exactly repeats itself.
The results are illustrated in Figure 9, plotting the error for a range of B =1 ... 8192 in a logarithmic x-axis. These results reconfirm our previous micro-scale observation at a larger scale. The performance of the approximation algo-rithms as well as all versions of our greedy approaches closely follow the optimal-error performance. MHIST follows them at a discernible distance. On the other hand, MaxDiff per-forms poorly, while the slack-version of AHistL - X  performs unreliably. The error function is not even monotonic for the low-quality AHistL - X ; this defect has also appeared in our earlier graphs.
Our hitherto results establish the practical performance of our greedy algorithms in relation to the optimal error. Still, we would like to gain a clear view at a very close level of resolution. Thus we measure the actual difference of the Euclidean error achieved with the tested algorithms from the optimal error. Figure 10 presents the results with the shuttle data set, for B = 150 ... 550. The dotted line presents the position of 0 . 1% off the optimal error (achieved by V -Optimal ), while the dash-dotted line traces the position of 1% off the optimal.

What was not clear before becomes apparent in this fig-ure. AHistL - X  matches the optimal quality, as its value predisposes it to do. The second-best performance is that GDY DP , while DnS and GDY BDP follow closely after. Our simple GDY algorithm does not achieve error as tightly close to the optimal, while the other heuristics are far-off, and do not fall in this figure. Still, it is remarkable that our heuris-tics can match and exceed the quality of DnS .
We elaborate on this line of comparison, presenting the difference from the optimal error with the aggregated winding data set (Figure 11). Now we use a logarithmic x-axis to present a wider range of B =16 ... 2048. We add a dash-dash line that denotes the position of 10% off the optimal. GDY DP exceeds the quality of DnS , while GDY BDP comes close. GDY is more far-off, while other heuristics are distant outliers, not seen in the figure. Thus, the injection of DP capacity into GDY indeed adds a sophistication that affords performance similar or superior to that of the guarantee-providing approximation schemes.
Now we turn our attention to the other side of the quality-efficiency tradeoff, that of runtime performance. To get a full picture of the runtime state of affairs, we measure runtime as a function of B for constant data set size n , for varying data set size n under constant segmentation size B , as well as for B linearly varying with n . Figure 12 plots the results with the filtered version of the DJIA data set, for B = 500 ... 1000, same as the one used for quality assessment in Figure 6. The time axis is log-arithmic. A comparison of Figures 6 and 12 reveals some interesting findings. If the quality-efficiency tradeoff were equitably resolved by all tested techniques, then we would expect the good quality performers to be bad runtime per-formers, and vice versa. Still, the presented picture does not follow such a pattern. On the contrary, some of the best quality performers are also among the best runtime perform-ers as well. That is, remarkably, our GDY DP and GDY BDP algorithms, which gave almost optimal quality results, also achieve satisfactorily low and scalable runtime. Moreover, GDY , which achieves next-to-optimal quality performance, is also one of the runtime champions, along with the lower-quality MHIST and the worst-quality MaxDiff heuristic. In contrast, it is clear that other high-quality performers such as AHistL - X  and DnS pay a high runtime price for the quality they deliver. The same holds for the V -Optimal algorithm itself. Furthermore, the high-quality variant of AHistL - X  takes runtime even higher than that of V -Optimal ; the loose-variant of AHistL - X , which does not perform well on quality, does not gain in runtime from this looseness, and eventually exceeds the runtime of V -Optimal too. The lines in the figure indicate the cubic O ( B 3 ) complexity factor of AHistL - X .
Our implementation of V -Optimal , as well as of all algo-rithms that employ its DP scheme, follows the simple prun-ing step suggested in [20] (see Section 2.1). It is not clear whether the experimental evaluations in [38] and [10] have used this step. Hence, our runtime results may diverge from those reported in these works. Figure 13: Runtime comparison vs. B : winding
Next we illustrate runtimes with the aggregated winding data set (Figure 13), which present the runtime side of the evaluation for which Figure 11 shows the quality side. Axes are logarithmic, while B =16 ... 2048. The emerging pic-ture follows a pattern similar to the previous figure. GDY DP and GDY BDP achieve a remarkably attractive resolution of the quality-efficiency tradeoff, while GDY is one of the effi-ciency champions without sacrificing quality as MHIST and MaxDiff do. The cubic growth of AHistL - X  is clear; both variants eventually exceed the runtime of V -Optimal (which employs pruning), while DnS also pays a high efficiency cost.
Figure 14 displays runtime results with the synthetic data set, i.e., the other side of the quality evaluation depicted in Figure 9, with B =1 ... 8192 on logarithmic axes. Growth trends are now more accentuated. The growth of DnS , aris-ing from its O ( B 5 / 3 ) runtime factor, is also apparent; thus, not only AHistL - X , but also DnS exceeds the runtime of V -Optimal for large enough B . GDY DP also assumes an unfavorable growth trend after the pivot point of B = (see Section 4.2). GDY BDP and GDY stand out as scalable algorithms that also achieve high quality. Figure 14: Runtime comparison vs. B : Synthetic
We also evaluate the scalability of different techniques with respect to data set size n . Figure 15 outlines the run-time results for constant B = 512, on different prefixes of the same synthetic data set. Both axes are logarithmic. In this case, the growth of AHistL - X  is not as severe as it was vs. B , but still stands out as the least scalable algorithm, sur-passing the runtime of V -Optimal and DnS . We surmise that the O ( B 3 log 2 n ) worst-case complexity factor of AHistL - X , arising from the burden of approximating the error function itself, works out its impact more saliently as n grows. On the other hand, the impact of pruning within the DP in V -Optimal and DnS allows these algorithms to scale better, although not adequately either. In order to illustrate the impact of pruning, we also include a version of V -Optimal without the pruning step in this experiment (labeled VOpt as opposed to VOpt2 in the figure). The non-pruning ver-sion exhibits not only higher runtime, but also more accen-tuated growth with n . The champions of scalability in this experiment are again our greedy algorithms, as well as the heuristics that perform poorly on the quality side.
Finally, we measure the runtime performance with the synthetic data set when n and B grow in parallel. Figure 16 plots the results on logarithmic axes, where B = n 32 .The unscalable growth of AHistL - X  is conspicuous; V -Optimal and DnS do not scale well either. GDY DP almost parallels the growth of the poorly scaling algorithms in this figure. On the other hand, the batch version, GDY BDP , presents affordable runtime growth in this experiment too; its growth is minimally affected by the growing B , as a comparison of Figures 15 and 16 indicates, and Figure 14 corroborates. Similar observations hold for GDY , MHIST and MaxDiff . Still, given their respective quality performance, GDY BDP emerges from our assessment as the algorithm of choice.
In the previous sections we have measured the perfor-mance of the examined algorithms in both quality and run-time, and we have inferred that some algorithms resolve this tradeoff in a more satisfactory manner than others. In par-ticular, we have argued that algorithms like AHistL - X  do not address this tradeoff in an attractive manner; that is, a benefit in quality comes at a high cost of runtime, while a re-duction of runtime requires a severe sacrifice in quality. Still, we would like to trace this tradeoff more concretely. In this section we plot both the quality and runtime performance of examined algorithms on the same graph. Figure 17: Tradeoff Delineation, B = 512 :DJIA Figure 17 traces the quality-efficiency tradeoff with the DJIA data set and B = 512. The runtime axis is logarith-mic, so that small runtime increases in the lower part of the y-axis are rendered noticeable. Single-version techniques are represented by a single dot. For AHistL - X , each variant for a different value of gets its own dot. Lower values of allow for higher accuracy at the price of extra runtime. Likewise, several variants of GDY DP and GDY BDP are presented, based on the number I of iterations of GDY that they use for collecting sample boundaries. More available samples enable these algorithms to achieve higher quality at the cost of efficiency. For I = 1 the histogram given by both these schemes is reduced to that of GDY , since one iteration pro-duces only B samples to choose from.
This figure reconfirms that AHistL - X  performs poorly at resolving the quality-efficiency tradeoff. An attempt to gain quality by lowering renders the runtime higher than that of V -Optimal ; an effort to improve time-efficiency by increasing is not effective in its objective, while it deteriorates quality. DnS dominates almost all versions of AHistL - X  in runtime and quality. In contrast, GDY DP and GDY BDP resolve the tradeoff attractively, while they can improve on quality by investing the extra time required by higher values of I .
Our experimental study has led to some remarkable find-ings. First, despite their elegance, approximation schemes for histogram construction do not achieve an attractive reso-lution of the tradeoff between efficiency and quality. Among the proposed schemes, DnS achieves a slightly better posi-tion in the tradeoff than AHistL - X . Still, both can be super-seded in time efficiency by V -Optimal , whom they are meant to approximate, due to their super-linear dependence on B . Secondly, we have determined that our enhanced heuristics, employing greedy local search in combination with a DP seg-mentation on sample boundaries, address the tradeoff more satisfactorily. Our best performing algorithm, GDY BDP , consistently achieves near-optimal quality in near-linear time.
In this paper we offer a fresh approach to histogram con-struction or sequence segmentation that addresses a criti-cal gap in related research. We develop segmentation algo-rithms that are both fast and effective in quality, as mea-sured in terms of Euclidean error. Our best-performing method is based on an application of greedy local search that generates sample boundaries. These sample boundaries are then used as input points for the dynamic-programming seg-mentation algorithm that selects an optimal subset among them. Moreover, in a divide-and-conquer fashion, this algo-rithm processes the candidates in batches , so that its time complexity is kept constrained. Besides, we have conducted the first , to our knowledge, experimental comparison of pro-posed heuristics and approximation schemes for sequence segmentation. This study shows that our mixed approach achieves near-optimal quality in near-linear runtime. Thus, it outperforms previously proposed heuristics in quality and recently suggested approximation schemes in time efficiency. In conclusion, our solutions provide a highly recommend-able choice for all areas where segmentation or histogram construction finds application. In the future we intend to extend our schemes to the multidimensional case.
