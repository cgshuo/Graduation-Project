 Classification is a well-established operation in text mining. Given a set of labels A and a set D A of training documents tagged with these labels, a classifier learns to assign labels to unlabeled test documents. Suppose we also had available a different set of labels B , together with a set of documents D
B marked with labels from B . If A and B have some se-mantic overlap, can the availability of D B help us build a better classifier for A , and vice versa? We answer this ques-tion in the affirmative by proposing cross-training : a new approach to semi-supervised learning in presence of multiple label sets. We give distributional and discriminative algo-rithms for cross-training and show, through extensive exper-iments, that cross-training can discover and exploit proba-bilistic relations between two taxonomies for more accurate classification.
 telligence ]: Learning; I.5.2 [ Pattern Recognition ]: De-sign Methodology -classifier design and evaluation Keywords: Semi-supervised multi-task learning, Document classification, EM, Support Vector Machines.
Document classification is a well-established area of text mining. A document classifier is first trained using docu-ments with preassigned labels or classes picked from a set of labels, which we call the taxonomy or catalog . Once the classifier is trained, it is offered test documents for which it must guess the best label/s. Depending on the application, the label may correspond to a broad topic (e.g., a topic in the Yahoo! directory), a product category, or a user X  X  per-sonal taste in books, CDs, or Web sites.

Support Vector Machines (SVMs) [8], nearest-neighbor classifiers [19], maximum entropy classifiers [14], and naive Bayes (NB) classifiers [13] are some of the commonly used document classifiers.

If all content creators and users agreed on a single cat-alog of universal labels, text classification could also help tag content with unambiguous semantic annotations. The Web, however, has evolved without central editorship. It is unclear if universal standards will emerge outside specific application segments, and even for those segments, there is a need to consolidate legacy data into content organized as per the agreed-upon standards. These standards, moreover, are far from static.

A few examples will illustrate the current scenario. An e-commerce site which consolidates catalogs of goods and services may want to organize them according to the (still evolving) codes being developed in cooperation between EC-CMA and UNSPSC (see http://www.eccma.org/unspsc/ ). Meanwhile, vendors may have their own custom/legacy codes which generally evolve over time.

As another example, consider Yahoo! and Dmoz. Both cover the Web and have evolved to similar taxonomies, but show non-trivial differences. Among other artifacts, taxon-omy inversion is rampant in the Regional categories; what is Reference.Education.Colleges_and_Universities.Asia. India in one may be Regional.Asia.India.Education in the other; in fact, they sometimes coexist in the same taxon-omy! Other relationships are also common: Dmoz.Recreation. Outdoors.Speleology overlaps Yahoo.Recreation.Outdoors. Caving , but there are important non-overlapping sub-topics.
Given that documents are inherently a conglomeration of concepts, we believe that mappings between content-based taxonomies will be complex , uncertain and noisy . There-fore, text searching, ranking, and mining tools must exploit any available relationships, even probabilistic ones, between diverse meta-data standards.
 Our contributions: We introduce a general semi-supervised learning framework called cross-training which can exploit knowledge about label assignments in one taxonomy B to make better inferences about label assignments in another taxonomy A . Cross-training generalizes several existing clas-sification algorithms, while also comparing favorably with their accuracy on a host of related applications. Apart from increased classification accuracy, the benefits include a bet-ter understanding of probabilistic relationships between tax-onomies, and more experience with encoding heterogeneous features for learning algorithms.

We propose two cross-training algorithms. One uses Ex-pectation Maximization (EM) [5]. The other uses Support Vector Machines [16, 7, 8]. Through a detailed experimen-tal study using real-life and semi-synthetic data from Yahoo! and Dmoz, we show that cross-training is decisively better than the best classifier we could induce on A or B alone. Neither of our approaches dominates the other across all data sets. Cross-training also compares favorably with one recent algorithm [1] for mapping taxonomies, as well as an earlier classification algorithm which could exploit a pool of unlabeled documents [15].

Cross-training is related to multi-task learning [18, 4], but quite different from co-training [3]. We will discuss these and other related work in  X  6. The connection between cross-training and supervised learning with discrete/categorical attributes is discussed in  X  7.
 Outline: We start with a formal definition of the cross-training problem in  X  2, and then present the two major classes of cross-training algorithms in  X  3 and  X  4. We conduct a detailed experimental evaluation of the accuracy of vari-ous algorithms in  X  5, review related work in  X  6, and make concluding remarks in  X  7.
Like most recent text classifiers, our system models each document as a bag (multi-set) of words. Term t occurs n ( d,t ) times in document d . In contrast with prior work, there are two sets of class labels A and B . A document can be associated with a pair of labels ( c A ,c B ) where c A  X  A and c
B  X  B . We will also denote these labels, for a specific document d , as c A ( d ) and c B ( d ).
 a flat set of class labels. We apologize for continuing to use the term  X  X axonomy X  which connotes hierarchical relations between concepts or topics. Generalizing our approach to that setting is left for future work.

Owing to the dichotomy of labels, training and testing processes must be defined more carefully than in standard document classification. There are two distinct learning sce-narios, which we discuss separately.
The mapping problem can arise in a e-commerce setting where catalogs of goods and services of one site need to be integrated with those of another site. A could represent the target taxonomy, and B the taxonomy used at the data source.

A training document has exactly one of c A ( d ) and c B ( d ) known; we call such documents half-labeled . The system trains on half-labeled documents. During deployment, a new document comes with exactly one of the labels known, and the system has to estimate the missing label.

For benchmarking, we use a test set which is fully-labeled , and hide each label in turn, comparing the hidden label with the system X  X  guess. The accuracy of the system is the frac-tion of documents that it assigns to the correct hidden label. Thus, mapping is a symmetric scenario.

D A (respectively, D B ) is the set of documents with A -labels (respectively, B -labels), and D A  X  D B and D B D A are the half-labeled instances available to an algorithm. D A  X  D B is used for testing the algorithm.
 Validation and tuning: As in earlier work [1], we will (some-times) assume that some fraction of fully-labeled data can be sampled and made available to the system to help it tune its parameters and validate its models. This is called the tune-set . Typically, the available set of fully-labeled documents is partitioned into a tuneset and a test set, the tuneset used to fine-tune the system X  X  models and parameters, and the test set used to evaluate the system. This split is done ran-domly, many times, and the average accuracy is reported. (The above discussion may hint that the tuneset ought to be a small portion of the fully-labeled data, but in earlier work [1] large tunesets have been used. We report experiments with both choices, to make a fair comparison.)
Several personal bookmark managers [11, 9] need to train document classifiers on bookmarks organized into personal topic directories (say, B ) with the intent of mapping sub-sequently visited pages to those topics. Because bookmark-ing and annotation takes effort, people bookmark far fewer pages than they visit.

Nigam et al. [15] showed that if training data is scarce, a pool of unlabeled documents can be used to induce more accurate classifiers. (We discuss their method in  X  3.1.2.) Unlabeled documents are plentiful and easy to collect. Ex-tending Nigam et al.  X  X  work, we note that plentiful labeled data is available as well, e.g., from Web topic taxonomies. The catch is that those taxonomies (say, A ) may differ sub-stantially from the target taxonomy X  X xactly the situation we are setting out to address. We wish to evaluate if the ad-ditional label data can be exploited to improve our accuracy further.
 Evaluation: As with mapping, training documents are half-labeled but one taxonomy, say A , has significantly more doc-uments than B and the goal is to improve the B classifier using A -labeled documents. In contrast, each test document d has only one label (say from B ) and even that is hidden from the system. The system must guess c B ( d ). Accuracy is defined as before. We call this the  X  X ero-label X  setting.
Distributional classifiers fit a generative model to the train-ing data, and use this model to predict labels for test cases. E.g., a naive Bayes (NB) classifier posits that a document is generated by first fixing a label by invoking a (typically multinomial) prior distribution on labels, and then creating the document by invoking a term (feature) distribution con-ditioned on the label just chosen. In the next subsection, we discuss two settings with this generative framework: com-pletely supervised and partially supervised learning of a sin-gle label. Then we propose our main algorithms for learning label-pairs. In NB classification for a single taxonomy with label set C , where c  X  C is the label, d is the test document, t occurs n ( d,t ) times in d ,  X  c is the fraction of documents tagged c (also called the prior probability of c ), and  X  c,t are multino-mial probability parameters [13, 1], estimated from training documents as where T is the vocabulary or feature set, D c is the set of training documents marked with label c , and 0 &lt;  X   X  1 is the Lidstone X  X  smoothing parameter [17] (  X  = 1 corresponds to the well-known Laplace X  X  smoothing). Having estimated model parameters from training data, the goal is to find the best class arg max c Pr( c ) Pr( d | c ) for test documents.
The NB classifier needs each training document to be marked with one label. Can we make use of additional docu-ments with no label information (such as the test documents themselves) or partial label information (e.g., that a docu-ment was generated from one among a restricted subset of labels)?
A classic approach to estimating distributions over miss-ing values is Expectation Maximization (EM) [5]. Nigam et al. [15] use EM to induce a document classifier starting from a few labeled and many unlabeled documents (Fig-ure 1). Because this algorithm is designed for only one label set, we will call it EM1D . 1: Use labeled documents to induce a naive Bayes 2: while model  X  has not stabilized to satisfaction do 3: set up new model parameters  X  0 4: collect contributions from labeled documents to  X  0 5: for each unlabeled document d do 6: E-step: calculate the class probabilities 7: M-step: if term t occurs n ( d,t ) times in d , let d 8: end for 9: Re-estimate new cluster model parameters  X  0 10:  X   X   X  0 11: end while Figure 1: Using standard EM ( X  X M1D X ) for semi-supervised learning of document labels.
We extend Nigam et al.  X  X  EM algorithm to EM2D by cre-ating a 2d grid of class labels taken from the product set C = A  X  B . We assume a standard mixture model [5] for document generation. First the label pair ( c A ,c B ) is picked with probability Pr( c A ,c B ), and then a conditional term dis-tribution Pr( d | c A ,c B ) is sampled to generate the document. Thus, We assume the term distribution to be multinomial, extend-ing parameters  X  c,t to  X  c A ,c B ,t . Likewise, parameters  X  express the prior probability of a document being generated from label-pair ( c A ,c B ).

Thus, each document belongs to exactly one cell of this grid 1 . However, in the mapping scenario (  X  2.1) each training document comes with exactly one label, which determines either the row or the column where the training document belongs, but not both. Thus, each document d identifies a subset C d  X  C to which it potentially belongs, and for  X  6 X  C d , we are given that Pr(  X  | d ) = 0. We force this con-straint in the E-step shown in Figure 1, limiting the con-tributions from a training document to its correct row or column, and scaling the E-variables to add up to 1 over the row or column.
The EM algorithm [5] guarantees only a locally optimum solution to the E and M variables. It is important to start the iterations from a reasonably good initial estimate of  X . In EM2D, we have two resources at our disposal to achieve good initialization.
 The first option is to train two naive Bayes classifiers on D
A  X  D B and D B  X  D A (see  X  2.1 for their definition), and derive guessed B -labels for D A  X  D B and guessed A -labels for D B  X  D A . The basic naive Bayes classifiers also help us (via random 70%/30% train/validate splits) to choose an initial number of features (in decreasing order of information gain) and an initial value of the Lidstone parameter  X  in equation (2). These initial steps are shown near the top of Figure 2.

The second option is to use the tuneset of fully-labeled documents to seed the initial  X  distribution. However, the tuneset is generally rather small. Using only the tuneset would generally fail to populate all the cells of the label grid adequately. It is probably best to use both options in the rare case that fully-labeled data is available.
Suppose a training document d has  X  = c A ( d ) known but c ( d ) unknown. Then P c standard multinomial model in equation (1), we can write This completes the specification of the E-step, although some care is required to preserve numerical precision. For the M-step, we set which is simply the expected fraction of documents occupy-ing label cell (  X , X  ). Likewise, we set This expression closely resembles equation (2), except that again, contributions to term counts are weighted by the probability of each document occupying label cell (  X , X  ), like in step 7 of Figure 1.
 Damping: In the EM2D setup, different documents have different quality and extent of label information. Tuneset documents plug into exactly one known (  X , X  ) slot and pre-sumably have the most reliable label information. Training documents have one label pinned by human input, which is assumed to be reliable, but the other label is not as reliable. In the zero-label setting (  X  2.2), test documents have neither label known, but may still help the classifier gain accuracy by participating in the EM iterations  X  X ompletely floating X  over the label grid.

In the update equations above, we have given one vote to each document. However, it is common [15] to use a damping factor L  X  1 to scale down the contribution of documents whose labels we consider less reliable. It is as though a fully-labeled document is worth one vote, but a singly labeled document is worth only L = 0 . 5, say. Thus, L can be thought of as an instance scaling mechanism like in boosting. It does not invalidate the theory of EM in any way. The best value of L can be set by cross-validation. Early stopping: The NB generative model is a very crude approximation to reality. Therefore, maximizing data like-lihood using EM may not improve classification accuracy in all cases. It is common to use a tuneset to stop EM itera-tions in case classification accuracy over (cross-) validation data is found to drop [15].
The half-label setting is simple. Given a test document d with c A =  X  known, we simply find Pr(  X ,c B | d ) for all candidates c B , and report the best. The zero-label setting gives us at least two distinct options: EM2D with guesses and EM2D with model aggregation.
 onomy B , we first apply an A -classifier to the test document. The guessed A -label now lets us deal with the zero-label test instance as if it were a mapping problem. Obviously, we should use the best possible A -classifier.
 ations are over, we use the final values of the E-variables to prepare a new classifier for target taxonomy B . More specif-ically, each training document d  X  D A  X  D B has associated E-values Pr( c B | d,c A ). Just like in EM, we let d  X  X ontribute X  its term counts in proportion to this probability to label c Documents in D B contribute fully to their respective labels in B . The resulting  X  X ggregated X  classifier for B is used to classify test instances.
If EM2D improves upon the accuracy of single-taxonomy learners, that could be attributed to multiple reasons. Let B be the target taxonomy in this discussion. The mapping of A -labeled documents to B may improve simply because of the extra documents in D A  X  D B , not because these doc-uments are A -labeled. Whether this is the case can be easily determined by calibrating EM2D against EM1D (run with B as target labels) with the documents in D A  X  D B thrown in as unlabeled documents.

Between EM1D and EM2D there are options which let us use the A -labels, but in ways simpler than EM2D. E.g., we can set up an EM1D instance for each  X   X  A . The B -labeled documents are shared across all such instances. A -labeled documents bearing the label  X  become unlabeled documents for the instance corresponding to  X  . The pseudo-code is shown in Figure 3. We call this Stratified-EM . 1: for each label  X   X  A do 2: train a B classifier  X   X  using EM1D with D B  X  D A 3: end for 4: for each test document labeled ( c A , ?) do 6: end for Figure 3: Stratified EM to exploit A -labels while classifying for B .

If EM2D beats both EM1D and Stratified-EM, we can conclude that the mutual  X  X orrections X  of term distributions in EM2D are somehow vital to its higher accuracy.
The classifiers discussed thus far aim to fit a class-conditional generative distribution Pr( d | c ) (or Pr( d | c A ,c Bayes rule to estimate Pr( c | d ) (or Pr( c A | d,c contrast, discriminative classifiers seek to directly fit a re-gression function from the document to scores for label(s).
In this section we will discuss cross-training using two dis-criminative classifiers. The first (new) approach uses Sup-port Vector Machines (SVMs), which have been reported to do well for text data [7]. The second (prior) approach [1] combines distributional and discriminative aspects. Linear SVMs: Suppose we are given a vector representation of n documents. Each vector has a component for each fea-ture (in our case, a term) which is proportional to the num-ber of times the term occurs in the document 2 . Document vectors are usually scaled to unit L 2 norm. Each document vector is associated with one of two labels, +1 or  X  1. The training data is thus { ( d i ,c i ) ,i = 1 ,...,n } ,c  X  X  X  1 , +1 } .
A linear SVM finds a vector w and a scalar constant b such that for all i , c i ( w  X  d i + b )  X  1, and k w k is minimized. This optimization corresponds to fitting the thickest possible slab between the positive ( c = +1) and negative ( c =  X  1) documents. In case the training samples are not linearly separable, it is possible to trade off the slab width for the number of misclassified training instances.

If the data has more than two labels, it is common to cre-ate an ensemble of yes/no SVMs, one for each label. During training, a document marked c is a positive example for the SVM associated with c , and a negative example for all other SVMs. This is called the  X  X ne-vs-rest X  ensemble approach. During testing, for a test document d , each SVM evaluates its regression function; the SVM corresponding to label c evaluates w c  X  d + b c . The label chosen is arg max c ( w (other policies can also be used).

Inducing a SVM classifier involves a complex, iterative numerical optimization. Several implementations of SVM are publicly available, including Sequential Minimum Opti-mization (SMO) [16, 7] and SVMlight [8].
 Cross-training SVMs: If A -labels are good predictors of B -labels, one way to enhance a purely text-based SVM learner for B is to allocate, over and above a column for each token in the training vocabulary, | A | extra columns, one for each label in A . A document d  X  D B  X  D A is submitted to a text-based SVM ensemble for A , called S ( A, 0), which gives it a score w c A  X  d + b c A for each class c A  X  A .
These scores can be inserted into the | A | new columns, either as-is, or after some simple transformation, such as taking the sign of the score, or converting the largest score to +1 and the rest to 0 or  X  1 (we use the latter option in our experiments), and scaling ordinary term attributes by a factor of f (0  X  f  X  1) and scaling these label attributes by a factor of 1  X  f . Document vectors are always scaled to unit L 2 norm.

The parameter f , which can be chosen through cross-validation on a tuneset, decides the relative importance of label and term attributes. We evaluated f from 0 to 1 in steps of 0 . 05 and set f = 0 . 95. These cross-trained SVMs are denoted by SVM-CT .

Documents in D B  X  D A thus get a new vector representa-tion with | T | + | A | columns where | T | is the number of term features. They also have a supervised B label. These are now used to train a new SVM ensemble S ( B, 1). The docu-ment tables and how they are used to train and test SVMs are shown in Figure 4. We can obviously repeat the process iteratively in a ping-pong manner, each classifier providing synthetic columns for the other. The complete pseudo-code is shown in Figure 5.
 Our experiments show that SVM-CT does outperform SVM, making effective use of the label attributes (although there is little improvement beyond the first ping-pong round). SVM-CT is also better than the distributional cross-training methods in about half the cases. SVM (which uses text alone), in turn, is much better than the baseline NB clas-sifier. Moreover, inspecting the components of w along the label dimensions derived by SVM-CT gives us some inter-esting insights into various kinds of mappings between the label sets A and B . We will return to these observations in  X  5. 1: Represent each document as a vector d in term space 2: Build one-vs-rest SVM classifiers S ( A, 0) and S ( B, 0) 3: for i = 1 , 2 ,... do 4: for each document d  X  D B  X  D A do 5: Apply S ( A,i  X  1) to d , getting a vector  X  A ( d ) of 6: Concatenate vectors d and  X  A ( d ) into a single 7: Add this vector into the training set for a 8: end for 9: Similarly, use S ( B,i  X  1) to get  X  B ( d ) and induce a 10: end for
Agrawal and Srikant (A&amp;S) [1] proposed a hybrid distri-butional/discriminative classification algorithm by enhanc-ing the prior estimation of NB (equation 1). Let the target label-set be C and the source label set be S (to be consis-tent with their notation). In the mapping setting, classi-fying document d entails finding arg max c Pr( c | d,s ), where the source label s  X  S is supplied and the target label c is sought.

Given d and s are fixed, arg max c Pr( c | d,s ) is equal to Pr( d | c ) , using the conditional independence assumption shown underlined (which is theoretically debatable, but seems to work in practice). All that remains is to propose parametric forms for Pr( c | s ) and Pr( d | c ). Pr( d | c ) is modeled exactly as in equation (1), i.e., Pr( d | c )  X  Q t  X  d  X  n ( d,t ) c,t estimated by a C -trained classifier which has no knowledge of S -labels. (This is the distributional/generative part.)
The key innovation of A&amp;S is to propose a parametric form for Pr( c | s ) depending on inter-label relations. Let N be the number of C -labeled documents in the training set for C . As in EM2D, A&amp;S use a C -trained classifier to guess classes of S -labeled documents; let G ( s,c ) be the number of documents with source label s that this classifier assigns to target label c . The overall score uses a tuning parameter R  X  0 and is given by log Pr( c | d,s ) = constant + log N c + R log G ( s,c ) + Note that (once the  X  c,t s are fixed) R is the only tunable parameter here. R = 0 coincides with standard NB on the master labels. Taking logs, we see that (like SVM) A&amp;S is also a linear discriminant learner. A&amp;S use a tuneset to set the best value of R , which can be chosen in two ways. Random sampling: A fraction (varying between 10% and 90% in our experiments) of the fully-labeled documents is sampled to create a tuneset. The remaining documents are used as the test set. We evaluate a range of choices for R  X  { 0 , 1 , 3 , 10 , 30 , 100 ,... } against the tuneset. The accuracy is averaged over dozens of such samples. Active learning: The system repeatedly samples the fully-labeled documents. For each sample d , it varies R to see if R makes any difference to the estimated C -label. If it does, d is placed in the tuneset; otherwise it is put in the test/calibration set. A&amp;S report that 5 X 10 actively chosen samples are adequate to pick a suitable R .
All our algorithms were coded in a few thousand lines of simple C++. A&amp;S, EM2D, and variants were run on (over 1GHz) Pentium3 servers with 1 X 3GB of RAM. The models fit easily in tens of megabytes of RAM. We scanned the documents sequentially and did not need to hold document vectors in memory. The SVM implementation we used did load document vectors into memory. A&amp;S, EM2D and its variants generally trained faster than SVM and SVM-CT.
We collected example URLs from Dmoz and Yahoo!. Their intersection has 110926 documents, less than 10% of either X  X  total size. This supports our claim that double-labeled doc-uments are hard to find. Like A&amp;S [1] we selected five data sets: Autos, Movies, Outdoors, Photo, and Software. However, their sub-topics and training examples were not available to us. Therefore, for each data set, in each of the two taxonomies, we picked immediate children as labels such that there were at least 10 URLs in common with a label of the other taxonomy. We then added in a few additional labels from each taxonomy. Finally we went back to the orig-inal Dmoz and Yahoo! sources to collect all URLs within the chosen label sets; some 70 X 80% of the fetches succeeded. Figure 6: Sizes of various document and label sets in our collected data. The first five benchmarks Autos to Software are used primarily for studying the half-labeled mapping scenario, and Bookmarks is used for studying the zero-label scenario.
 Figure 6 shows various properties of our main data sets. We felt uncomfortable about the small test sets, but A&amp;S re-ported intersections of similar small sizes, and we also found human labeling (based on page text alone) to systematically reject pages with relatively unreliable text, exactly those cases where A&amp;S and cross-training are likely to shine.
The Bookmarks data set was created mainly to study zero-labeled classification. We collected and inspected a dozen or so bookmark files published on the Web. We found it very common for bookmark authors to collect URLs into coherent topics. Usually, these topics had strong correspondence with one or few topics in Yahoo!/Dmoz. However, the number of URLs per topic was small (say 3 X 20), exactly the scenario we painted at the outset. We derived sample bookmark topics B from these bookmark collections, and populated them from Yahoo! ( A ) URLs, and removed them from D A .
We used naive Bayes (NB) classifier in the Rainbow pack-age [12]. We created two Rainbow classifiers, one for A labels using D A  X  D B , the other for B labels using D B  X  D A .
Apart from providing a strawman, NB runs are used to set the Lidstone parameters and the feature sets for A and B . Consider the classifier for A . We first created a random 70%/30% train-test split of D A  X  D B . Rainbow ingested the 70% training subset and listed features in decreasing order of information gain (w.r.t. the labels). In an outer loop, we chose from  X  A between 0.1 and 1 in steps of 0.1. In an inner loop, we chose a prefix T A of the feature list of size 10% through 90% in steps of 10% (similarly for B ). We then used the 30% validation data to pick the best values for  X  A , X  B ,T A ,T B . Finally, the NB baseline is obtained by subjecting the held-out D A  X  D B to these optimized Rainbow classifiers. Figure 7 shows various accuracy statistics. Figure 7: Naive Bayes baseline accuracy with op-timized choices of | T | , the number of features, and  X  , the smoothing parameter. A is Dmoz and B is Yahoo!. Percent accuracy is shown for 70/30 cross validation and the unseen D A  X  D B test set.

All other distributional cross-training algorithms used these optimized values of  X  and T . In particular, EM2D used T
A  X  T B as the feature set, and the average of  X  A and  X  B
Feature selection and the choice of  X  matters a great deal for most data sets. Given high-dimensional data like text, feature selection would likely be helpful for any learning method, but the benefit from tuning  X  is large mainly be-cause the naive Bayes model results in terrible estimates of the joint distribution, and any  X  X ix X  to the innumerable  X  s is likely to help. Whereas the two classifiers can each optimize T , T B ,  X  A and  X  B in an unconstrained manner, EM2D is stuck with a single feature set and a single value of  X  , which puts it at a disadvantage.
We used SVMlight [8] in one-vs-rest ensemble mode, with a linear kernel and default settings for all parameters. Documents were represented as unit vectors (  X  4.1). f was set to 0.95 as explained earlier in  X  4.1.

Figure 8 compares the accuracy of SVM and SVM-CT with the NB baseline. In most cases, SVM beats NB. This is consistent with folk wisdom that SVMs generally per-form better than NB on text classification tasks. More in-teresting is the observation that SVM-CT sometimes has higher accuracy than SVM, which shows that it is possible Figure 8: Comparative evaluation of NB, SVM and SVM-CT (cross-trained SVM). for SVM-CT to exploit additional information from label-derived columns.

We made two additional studies of SVM-CT. First, we checked that the average magnitude of w for ordinary term features was always lower than the average magnitude of w for label-derived features. Recall that | w t | is a measure of how strongly the feature t can influence the decision of the SVM, i.e., the sensitivity of the SVM to feature t .
Second, we tabulated the B (respectively, A ) labels cor-responding to the highest and lowest w values of various A (respectively, B ) classifiers. We wanted to observe the mappings learned between the classes in the two taxonomies using cross-trained SVMs 3 . During cross-training, the label information was transformed into a vector of 1 and  X  1 values as mentioned in  X  4.1. In addition, a new dimension called none-of-the-above (NOTA) was introduced, whose value was set to 1 when all label scores obtained from B (respectively, A) were negative and all label dimensions were set to  X  1. The purpose of NOTA is explained shortly.

The results are shown in Figure 9. We show some Dmoz (respectively, Yahoo!) class labels along with the Yahoo! (respectively, Dmoz) class labels which had the greatest pos-itive and negative influence in predicting the said Dmoz (re-spectively, Yahoo!) class. All positive couplings are very meaningful; some negative couplings are fairly intriguing too.

The Outdoors dataset for both taxonomies contains the class ScubaDiving which maps to its namesake in the other taxonomy with a large positive component along w . Such one-to-one mappings are symmetric and expected. Even when there is no direct one-to-one correspondence between the labels (or there is a containment relationship) such as between Yahoo.Movies.Genres.SciFi-Fantasy and Dmoz. Movies.Series.StarWars , SVM-CT seems capable of ex-tracting that information. On the other hand when the Dmoz.Software.Accounting class really has no relevant class in the Yahoo! taxonomy, the synthetic NOTA class indicates this by the high value of | w NOTA | .

The superclass Dmoz.Photo.Techniques&amp;Styles duly maps to finer categories in Yahoo.Photo , such as Pinhole Photog-raphy, 3D Photography, and Panoramic Photography. The Dmoz to Yahoo! mapping gives high positive weights to most of these child classes as seen in Figure 9. Such instruc-Figure 9: Dmoz and Yahoo topic mappings learned with cross-trained SVMs tive parent-child , or one-to-many mappings emerge in spite of our assumption of flat taxonomies.
 Another interesting mapping is from Yahoo.Software.OS. MSWindows to multiple high positive weights to classes in the Dmoz taxonomy. Here, the NOTA class can be inter-preted as clearly separating all the Windows related classes above it from the Unix related classes below it within Dmoz. Software . Figure 10 shows the accuracy of EM2D in comparison with NB. EM2D is significantly better than NB with a maximum gap of 30% for the Movies dataset and average gap of 10%. This is reassuring, but in this section we wish to analyze carefully why this is the case.
 Figure 10: EM2D vis-a-vis Stratified-EM1D, EM1D, and NB. For EM1D we used its best damping pa-rameter, L = 0 . 01 .
 There are two potential sources of information from D A  X  D
B which may improve the accuracy of classifying into B given d and c A . The first is simply the addition of a bunch of documents, even if they are not labeled with B -labels and even if we ignore their A -labels. If this were the only source of extra information, EM1D should be able to match EM2D, which is clearly not the case. Therefore, knowledge of A -labels of specific documents is vital. As we discussed in  X  3.3, A -labels can be used by Stratified-EM, which simply creates one instance of EM1D for each distinct label  X   X  A . Figure 10 also shows that with only two minor exceptions, EM2D beats both EM1D and Stratified-EM. This despite the fact that each EM1D has denser data, lowering the variance of the parameter estimates compared to EM2D. These measurements help us establish that
EM2D, like EM1D, finds locally optimum values of the total data likelihood. Hence the final accuracy is sensitive to the initial assignment of half-labeled data in the 2D label grid.

Given the baseline classifiers trained on A (using docu-ments in D A  X  D B ) and B (using documents in D B  X  D A ), it is natural to initialize EM2D by submitting documents in D
A  X  D B to the B -classifier and vice versa. We may use a  X  X ard X  assignment to the best guess, or a  X  X oft X  or frac-tional assignment based on the probabilities emitted by the baseline classifiers.

However, these are not the only options. How will EM2D behave if each document in D A  X  D B is assigned uniformly over each label in B ? In general, how sensitive is EM2D to perturbations and errors in the initial E-estimates?
To test EM2D X  X  resilience, we randomly picked a fraction q of documents (with A -labels, say) and replaced their guessed scores for B -labels with a uniform distribution smeared over all B -labels. The remaining fraction 1  X  q of documents are added to the EM2D system as before. Thus, q = 1 corresponds to full uniform assignment.

Obviously, the effect of smearing a fraction q depends on the accuracy of the guesses in the first place. Therefore we repeat the smearing experiments for varying level of start-ing guess accuracy. (We fake different guess accuracies by random flips in guesses. Note that these  X  X lips X  are distinct from the  X  X mear. X ) Figure 11: The effect on EM2D of smearing the ini-tial guesses of a fraction of half-labeled training doc-uments. The y-axis shows final EM2D accuracy.

In Figure 11 we show the change in accuracy with in-creasing fraction of smeared guesses on the Movie dataset, with two different settings of guess accuracy: the default as shown in Figure 7 and a second setting where 70% of the guesses have been pre-flipped to a random label (this results in guesses of very poor quality). These plots show that
All the data sets we collected have relatively balanced sizes of D A  X  D B and D B  X  D A . How well can EM2D do in highly unbalanced settings, especially w.r.t. the sparsely-populated taxonomy?
To answer this question, we (arbitrarily) picked B as the taxonomy to be decimated, and sampled D B  X  D A down to 300 documents. (Actually, we decimated to 200, 300, and 5% of the original. Results were similar.) D A  X  D B was left unchanged.

The small size of D B  X  D A led to a poor baseline B -classifier. Therefore, the guessed B -labels for the documents in D A  X  D B had a large error rate. Because information flow is bidirectional in EM2D, poor B guesses reduced overall accuracy. We propose three fixes for this problem: Figure 12: EM2D on a small sample of 300 docu-ments from D B  X  D A .
 Figure 12 shows that the accuracy gain of EM2D over NB in highly asymmetric settings can in fact be higher (av-erage 11.4%) than in more balanced data (average 10% in Figure 10), provided EM2D is initialized properly. Nigam et al.  X  X  experience [15] seems to corroborate that the gains from semi-supervised learning are larger when labeled data is limited. For our A&amp;S implementation, we fixed the feature set to T  X  T B as found by Rainbow, and also fixed the  X  parameter to one that gave the best accuracy for Rainbow for each of A and B prediction.

Making a fair comparison between A&amp;S and EM2D in-volves exposing to EM2D at least the fully-labeled tuneset that A&amp;S uses. In fact, it is very difficult to compare the active-learning version of A&amp;S with EM2D in a principled way, because A&amp;S inspects fully-labeled documents (not the labels, but the text) outside the tuneset as well. (EM2D is not designed for active learning.) Therefore, we focused on the randomly sampled tuneset paradigm only, because that could be used with both A&amp;S and EM2D.

In addition, given the large skew between half-labeled and fully-labeled populations, we used damping to re-scale them to the same effective size (see  X  3.2.2 for more details). Figure 13: Accuracy of the A&amp;S algorithm com-pared with EM2D for 10% and 90% tuneset (T) and A&amp;S active learning (AL).

In Figure 13 we present the accuracy of A&amp;S with 10% and 90% randomly sampled tuneset as well as a tuneset of size 10 picked by active learning (AL) from the entire test set of fully-labeled documents. Broadly, A&amp;S and EM2D are comparable, but EM2D edges over A&amp;S by a maximum of 20% and an average of 4% for the 10% tuneset and 2% for the 90% tuneset. When EM2D loses to A&amp;S, the gap is very small.
In this scenario, we are required to finally produce a classi-fier for B which does not depend on the test instances being labeled with A labels. In  X  3 we discussed two methods for deploying EM2D in this setting: EM2D-D, a model aggre-gation method, and EM2D-G which is essentially EM2D, except that the A -labels are supplied as guesses from an A -classifier.
 Figure 14 shows the accuracy for EM2D-D, EM2D-G, EM1D and NB, for various sizes of labeled training sets, and two choices of the damping factor L discussed at the end of  X  3. These numbers are for the Bookmark data set. The accuracy values were averaged over three random choices of the training set for each choice of training set size.
As the fraction of training data is increased, the benefit of semi-supervised learning reduces, which is obvious. The damping factor does essential damage control when there are many labeled documents, but can hurt when the labeled set is very small. These observations corroborate with earlier EM1D results by Nigam et al. [15].

Unlike Nigam et al. , EM1D could improve beyond NB only for the smallest training sets in our case. One possi-Figure 14: EM2D with guessing is the best methods for classifying zero-label documents. NB accuracy is shown only once for each size of the training set, because it does not change with L . ble reason is that the unlabeled Yahoo! dataset, from which EM1D adds instances, is significantly different, and has many more irrelevant classes, compared to the initial labeled data in our Bookmark dataset. Nigam et al.  X  X  experiments drew unlabeled and labeled documents from the same distribu-tion.

Finally, we were surprised to see that EM2D-G performed better than EM2D-D. Recall that EM2D-D is really a 1d classifier, which should reduce data sparsity and improve the reliability of its parameter estimates compared to EM2D. Despite this benefit, model aggregation appears to hurt. Even a noisy guess at the A -label, followed by a row-conditioned classification, outperforms the aggregated model.
In recent years, EM-like semi-supervised learning has been enhanced in several ways and applied to a number of set-tings.

Extending beyond EM1D, Liu et al. [10] and Yu et al. [20] consider the realistic situation where, apart from labeling only a few samples, the user is also unlikely to spend the effort to mark negative samples. Their EM-like algorithms can work on a set of positive examples P and a mixed pool of samples M which may contain both positive and negative instances.

Cross-training is related to multi-task learning or life-long learning, in which information (features, models, etc.) from one learning task is used for another. Thrun [18] discusses how to cluster learning tasks (not instances) and pick, for a given task, those other tasks that are likely to be related to the current one. Information from those tasks are then used to influence the distance function in a nearest-neighbor clas-sifier. Caruana [4] discusses how to use multi-task learning in neural networks, and Baxter [2] provides a PAC analysis. Cross-training is a two-task setting with no instance sub-mitted to more than one task. The similarity between tasks falls out naturally as we estimate  X   X , X  .

A recent approach to semi-supervised learning (which might appear superficially similar to cross-training) is co-training , proposed by Blum and Mitchell [3]. In co-training, too, there are two learners, but, unlike cross-training, the learn-ers have to use disjoint subsets of attributes, and assign la-bels from only one taxonomy. Each learner picks unlabeled training instances that it is most confident about classify-ing correctly, and makes it a labeled training instance for the other learner. Co-training and cross-training are quite different things: two label sets are central to our formula-tion, and our approach depends on modeling a single term distribution conditioned on a pair of labels.

Doan et al. [6] study a related problem of identifying map-ping between labels of two taxonomies (called ontologies in the paper). Their goal is to find for each label in one tax-onomy, the label most similar to it in the other taxonomy. In contrast, our goal is to assist classification in one cata-log without necessarily committing on a specific mapping relation with another catalog.
We have presented cross-training , a new technique for using sample documents from one taxonomy to improve classification tasks for another taxonomy. We have pre-sented two algorithms for cross-training: a probabilistic al-gorithm based on EM and a discriminative algorithm based on SVMs.

Extensive experiments with real-life Web data show that our approach definitely beats baseline classifiers in each tax-onomy, which is not very surprising. More reassuring are the observations that show our approach to compare favor-ably with the best existing approach, while providing a more sound foundation.

In principle, a sufficiently powerful supervised learner that can handle discrete categorical attributes can be used di-rectly for cross-training. In practice, specifically for text data, the large number of dimensions and the heterogeneity across term and label attributes pose challenges. Luckily, in cross-training, the label attribute can take only a few values. Therefore (in decision tree terms) we can first stratify the data on the label attribute and then build distributions for each label value.

Our work suggests several natural research directions. Might it be possible to improve EM2D using a better generative model whose E-scores are not as close to 0/1 as NB? Might tempering or annealing let EM2D reach better local optima? Are we estimating the number of clusters in EM properly? The  X  X orrect X  number of EM clusters may be as high as | A || B | (if the taxonomies are truly  X  X rthogonal X ), but is generally much smaller (perhaps even smaller than | A | and | B | for poorly separated labels). Can SVM-CT be improved further by designing better kernels? Is it an accident that neither of EM2D and SVM-CT dominates the other in ac-curacy? If not, can we predict which is likely to do better for a given problem? Can we design cross-trained classi-fiers which combine the strengths of the distributional and discriminative approaches? Finally, it would be useful to ex-tend the algorithm for real taxonomies (as against flat sets of classes).
 Acknowledgments: The research was supported in part by IBM Corporation, Tata Consultancy Services, and the Min-istry of Information Technology. We thank Kinshuk Jerath for collecting part of the data and for writing a prelimi-nary version of the A&amp;S algorithm; Ganesh Ramakrishnan, Deepa Paranjpe, and Roger Menezes for helping us clean the data; and an anonymous reviewer for pointing out ap-proaches for learning multiple tasks.
