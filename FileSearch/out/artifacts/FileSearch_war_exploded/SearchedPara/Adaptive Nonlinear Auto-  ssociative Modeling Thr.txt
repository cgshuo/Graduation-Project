 Much manifold learning literature has been published for discovering intrinsic infor-mation embedded in the high-dimensional space[1]. Two major algorithms (LLE and ISOMAP)[2, 3] are devoted to discover some intrinsic regularity underlying in the high-dimensional data. Also some algorithms based on manifold learning are proposed for supervised learning. However, most supervised manifold learning algorithms assume that data can be projected into the same subspace and recognition without considering the properties of concepts [4]. The disadvantage of the supervised manifold learning approach is that the separability of data would be impaired because data from different classes would be overlapped in the low-dimensional subspace [5]. Based on our obser-vation, we assume that data are projected different subspaces are more suitable than being projected a common subspace if data contain remarkably distinct concepts, for example, character and digit.
 cipal features, we propose adaptive nonlinear auto associative modeling (ANAM) for learning intrinsic features and recognition (Section 2). First, the low-dimensional sub-space of each class are attained with LLE algorithm. Second, based on the error rates of validation set, the parameters of each ANAM are adaptively obtained by computing minimum error rate of the validation set. Consequently, a ANAM-classifier is developed without LLE algorithm. The proposed ANAM will not lead to the overlapped data in the low-dimensional subspace and loss corresponding accuracy. Therefore, it partially overcomes the curse of dimensionality. Experiments (Section 3) on several character and digit databases show the advantages of the proposed ANAM algorithm. To establish the mapping and inverse mapping relationship of ANAM between the observed data and the corresponding low-dimensional one, locally linear embedding (LLE) algorithm [2] is first used to form the corresponding low-dimensional one Y ( Y  X  R d ) of the training set X ( X  X  R N ,N d ) in the paper. Then the data set ( X, Y ) is used for modelling the subsequently ANAM.
 data in both the embedded Euclidean space and the intrinsic one. Each sample in the observation space is a linearly weighted average of samples under neighbor constrain. Thus, we obtain the corresponding low-dimensional one Y of the original data X in the embedding space. And the completed set ( X, Y ) is used for the subsequent model of ANAMs.
 optimal mapping solution based on our experiments, in addition, it is used to avoid to calculate the parameters of inverse mapping matrices and mapping matrices of ANAM simultaneously.
 be estimated. In the proposed algorithm, we utilize mis-classified rate on the validation set to adjust model parameter to obtain the suboptimal model.
 one V d  X  R d with LLE mapping idea for avoiding the simultaneous computation of the mapping and inverse mapping matrices. After V d is obtained, the reconstruction procedure is then formulated with inverse mapping matrices of ANAM. On the basis of weierstrass approximation theorem, the inverse mapping formula in the i -th ANAM would be achieved with nonlinear polynomial function as follows: where v x ( i ) is a reconstructed sample through the i -th ANAM, n i is the number of samples used to construct the i -th ANAM, B ( i )= {  X  j ( i ) } is the N  X  n i weighted inverse mapping matrix or reconstruction matrix of the i -th ANAM, and v y ( i ) is the low-dimensional validation sample based on LLE algorithm. Without loss of generality, let the reconstruction kernel function be Gaussian kernel as: value in the proposed ANAM algorithm. Once the validation sample is auto-associated through different ANAMs, the similarity measure can be used for recognition as follows: Where L denotes the number of concepts. The geometrical explanation on Formula (3) is that sample is re-projected to the original space with the ANAM of same concept is closer to the original sample than these reconstructed samples through the ANAMs of different concepts.
 that the suboptimal parameter  X  2 optrec ( i ) can be adaptively obtained through searching some value which is related to the minimum recognition error rate of validation set. tion of validation set can be formulated as: Where A =  X  i is a d  X  n i weighted mapping matrix, k map ( x j ( i ) ,v x ( i )) denotes the similarity metric of data v x ( i ) with sample x j ( i ) as follows: And the reconstruction matrix is the same as Eq.(1). The only difference is that v x ( i ) , v ters  X  2 optmap ( i ) of mapping matrices is adaptively computed based on the error rate of validation set with fixed reconstruction parameters  X  2 optrec ( i ) .
 the weighted mapping matrix A ( i ) and the weighted inverse mapping matrix B ( i ) are calculated as follows: corresponding set in the original space with ANAM, and recognition is completed based on Eq. (3). Different from ANN Bourlard proposed[6], the proposed ANAN generalizes the model into high-dimensional nonlinear data and avoids the problem of convergence neural network often suffers. Experiments are carried out on four databases to evaluate the recognition ability of the proposed ANAM approach. Two sets are UCI character database[7] and OCR (opti-cal character recognition) database[8], and the other two sets are OPTDigits databases and PENDigits database from UCI repository[9, 10]. The details on the mentioned four databases and data partitions are illustrated in Table 1. It is noticeable that each dimen-sion in three datasets except OPTDigits Database was linearly scaled to [0,1] in this experiments. The former two databases are randomly partitioned three disjointing sets, that is, training set, validation set and test set. And the final results are the average of 10 repetitions. Meanwhile, the training set and validation set of the latter two databases are randomly partitioned disjointing sets, and test set has been separated in the original databases.
 different low-dimensional subspaces with LLE algorithm separately, validation set is used for searching the suboptimal parameters of ANAMs based on the error rate, and test set is used for evaluating the generalization performance of the proposed ANAM algorithm.
 generality, the neighbor parameter K is set to 50 for all the four databases. The ranges of and the size of each step is 10 0 . 5 so the optimal parameter can be adaptively searched. the recognition performance between the proposed NAMs and other known state-of-the-art algorithms, experimental results from [8] are cited.
 algorithm is comparable with other algorithm. For UCI letter and OPTDigits databases, the error rates of the proposed algorithm is lowest when comparing with other algorithms. For instances, in UCI character database, the error rates of NAMs is about 69.20% of the K-NN, 33.76% of the MLP. Furthermore, our proposed NAMs for the four databases using fewer features (10 dimensions) to model intrinsical feature spaces. of validation set for the error rates of test set (The experimental results also are the average of 10 runs). For example, the results in UCI and OCR database are illustrated as Figure 1. It can be seen that the error rate gradually decreases as the number of training sample increases. For example, when the number of training sample is equal to 400, the error rate and deviation are 6 . 07%  X  0 . 49% in the UCI Letter Database. In this paper, we propose ANAM for modeling different concepts and classifying samples which belong to remarkably distinct concepts. Unlike the other supervised manifold learning approaches, the advantages of the proposed ANAM is that it overcomes the curse of dimensionality without loss accuracy. And the discriminant information is implicity embodied because the parameters are determined based on the error rate of validation set. In the further study, we will consider how to combine cluster algorithm with ANAM for improving recognition rate.

