 Vector space models (VSMs) provide a powerful tool for representing word meanings and modeling the relations between them. While these models have demonstrated impressive success in capturing some aspects of word meaning (Landauer and Du-mais, 1997; Turney et al., 2010; Mikolov et al., 2013; Baroni et al., 2014; Levy et al., 2014), they generally fail to capture the fact that single word forms often have multiple meanings. This can lead to counterintuitive results X  X or example, it should be possible for the nearest word to rock to be stone in everyday usage, punk in discussions of music, and crack (cocaine) in discussions about drugs.

In a recent paper, Jauhar et al. (2015) introduce a method for  X  X etrofitting X  generic word vectors to create sense-specific vectors using the WordNet se-mantic lexicon (Miller, 1995). From WordNet, they create a graph structure comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar mean-ings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, un-like with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014).

In this paper, we observe that the crucial mean-ing relationships in the Jauhar et al. retrofitting process X  X he word sense graph X  X an be inferred based on another widely available resource: bilin-gual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense dis-tinctions sidesteps the persistent difficulty of identi-fying a single correct sense partitioning based on hu-man intuition, and avoids large investments in man-ual curation or annotation.

We use parallel text and word alignment to in-fer both word sense identities and inter-sense rela-tions required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec-tor space, using bilingual correspondences to define word senses. When evaluated on semantic judgment tasks, the vector spaces derived from this graph per-form comparably to and sometimes better than the WordNet-based space of Jauhar et al., indicating that parallel text is a viable alternative to WordNet for defining graph structure. Combining the output of parallel-data-based and WordNet-based retrofitted VSMs consistently improves performance, suggest-ing that the different sense graph methods make complementary contributions to this sense-specific retrofitting process. Retrofitting. The technique introduced by Jauhar et al. (2015) is based on what we will call a sense graph , which we formulate as follows. Nodes in the sense graph comprise the words w i in a vocabu-lary W together with the senses s ij for those words. Labeled, undirected edges include word-sense edges  X  w i ,s i,j  X  , which connect each word to all of its pos-sible senses, and sense-sense edges  X  s ij ,s i 0 j 0 beled with a meaning relationship r that holds be-tween the two senses.

Jauhar et al. use WordNet to define their sense graph. Synsets in the WordNet ontology define the sense nodes, a word-sense edge exists between any word and every synset to which it belongs, and WordNet X  X  synset-to-synset relations of synonymy, hypernymy, and hyponymy define the sense-sense edges. Figure 1 illustrates a fragment of a WordNet-based sense graph, suppressing edge labels.

Adopting Jauhar et al. X  X  notation, the original vec-tor space to be retrofitted is defined by the original word-form vectors  X  u i for each w i  X  W , and the goal is to infer a set V of sense-specific vectors v ij cor-responding to each sense s ij . Jauhar et al. use the sense graph to define a Markov network with vari-ables for all word vectors and sense vectors, within which each word X  X  vector  X  u i is connected to all of its sense vectors v ij , and the variables for sense vec-tors v ij and v i 0 j 0 are connected iff the corresponding senses are connected in the sense graph.

Retrofitting then consists in optimizing the fol-lowing objective, where  X  is a sense-agnostic weight, and  X  r are relation-specific weights for types of relations between senses:
The objective encourages similarity between a word X  X  vector and its senses X  vectors (first term), as well as similarity between the vectors for senses that are related in the sense graph (second term). Defining a sense graph from parallel text. Our key observation is that, although Jauhar et al. (2015) assume their sense graph to be an ontology, this graph can be based on any inventory of word-sense and sense-sense relationships. In particular, given a parallel corpus, we can follow the tradition of translation-as-sense-annotation: the senses of an En-glish word type can be defined by different possible translations of that word in another language.
Operationalizing this observation is straightfor-ward, given a word-aligned parallel corpus. If En-glish word form e i is aligned with Chinese word form c j , then e i ( c j ) is a sense of e i in the sense graph, and there is a word-sense edge  X  e i ,e i ( c j )  X  . Edges signifying a meaning relation are drawn be-tween sense nodes if those senses are defined by the same translation word. For instance, English senses swear (  X   X  ) and vow (  X   X  ) both arise via align-ment to  X   X  ( fashi ), so a sense-sense edge will be drawn between these two sense nodes. See Figure 2 for illustration. Tasks. We evaluate on both the synonym selection and word similarity rating tasks used by Jauhar et al. Synonym selection nicely demonstrates the advan-tages afforded by sense partitioning: if we believe that spin means  X  X ake up a story X , then we are not likely to perform well on a question in which the correct synonym is twirl . Word similarity rating, on the other hand, is a classic test of the extent to which vector representations simulate human intuitions of word relations in general.

For synonym selection, we follow Jauhar et al. in testing with ESL-50 (Turney, 2001), RD-300 (Jar-masz and Szpakowicz, 2004), and TOEFL-80 (Lan-dauer and Dumais, 1997), using maxSim for multi-sense models (Jauhar et al., 2015, eq. 9) to select the mirror Jauhar et al., testing with WS-353 (Finkel-stein et al., 2001), RG-65 (Rubenstein and Good-enough, 1965), MC-30 (Miller and Charles, 1991), and the designated test subset (1000 items) of MEN-3k (Bruni et al., 2014), using avgSim (Jauhar et al., 2015, eq. 8) as the similarity rating, and evaluating model ratings against human similarity ratings via Spearman X  X  rank correlation coefficient (  X  ) . 2 Initial word representations. We use the word2vec (Mikolov et al., 2013) skip-gram archi-tecture to train 80-dimensional word vectors (in keeping with Jauhar et al.), based on evidence that this model shows consistently strong performance on a wide array of tasks (Baroni et al., 2014; Levy et al., 2015). Training is on ukWaC (Ferraresi et al., Sense-graph construction from parallel text. To construct the sense graph per Section 2, we use  X  5.8M lines of segmented Chinese-English paral-lel text from the DARPA BOLT project and the Broadcast Conversation subset of the segmented Chinese-English parallel data in the OntoNotes cor-alignment with the Berkeley aligner (Liang et al., 2006). We filter out noisy alignments using the G-test statistic (Dunning, 1993), with a threshold se-lected during tuning on a development set.

We set  X  (see Equation 1) to 1.0. Each sense-sense edge  X  e i ( c j ) ,e i 0 ( c j )  X  has individual weight 0 &lt;  X  r  X  1 , computed by obtaining the G-test statistic for the alignment of e i with c j and for the alignment of e i 0 with c j , running these values through a logistic function, and averaging. Param-eters for these computations, as well as the G-test statistic threshold below which we filtered out noisy alignments, were selected during tuning on the de-velopment set.

Note that we have not currently incorporated spe-cial treatment for alignments of a single word to a multi-word phrase. This does create the possibil-ity of noisy or uninformative sense annotations (e.g., sense annotations corresponding to parts of aligned Chinese phrases) when such alignments are not fil-tered out by the G-test thresholding.
 Experimental conditions. We evaluate the fol-lowing experimental conditions: Skip-gram (SG) uses the un-retrofitted word2vec vectors, Word-Net (WN) retrofits using the WordNet-based sense graph, and Parallel Data (PD) retrofits using the sense graph built from parallel text. We also com-bine the two retrofitting approaches (PD-WN). For synonym selection, we compute maxSim over all sense pairs for WN and PD separately, and select the sense pair with the overall maximum cosine sim-ilarity across the two. For similarity rating, we explore two PD-WN combination approaches: for each word pair, we take the avgSim from each sep-arate model, and then we (a) take the average of the values given by the two models (avg), or (b) take the maximum value between the two models (max). Table 1 shows that combining our new method with Jauhar et al. X  X  WN retrofitting performs best on synonym selection across all datasets, and both retrofitted models consistently outperform the no-retrofitting model (SG). Error analysis on RD-87, the only set on which WN substantially outperforms PD, suggests that PD X  X  errors are driven by the large number of lower frequency items that characterize this dataset. Given that WordNet is a hand-curated lexicon while the parallel data mirrors actual us-age, it is not surprising that the latter suffers when it comes to low frequency items.

Error analysis also indicates that PD performs particularly well on the synonym task precisely when one would expect: when the probe and the correct answer have an alignment to the same Chi-nese word form, so that the corresponding sense vec-tors are extremely close in vector space. Occasion-ally, PD yields  X  X he wrong answer for the right rea-son X , choosing an option for which there is indeed a correct alignment that matches an alignment of the probe word. For instance, though the probe passage is intended to have the answer hallway , PD chooses ticket because both passage and ticket have a sense defined by alignment to the Chinese word  X   X  ( jip-iao ), meaning  X  X ir ticket X . Though this is a less fre-quent sense of passage , it is a reasonable one. Results on the similarity rating task (presented in Table 2) are less clearly interpretable, top perfor-mance being divided between the PD model and the combined models X  X ith the exception of WS-353. We note that WS-353 is a test set for which human raters were explicitly told to rate relatedness, rather than similarity, while the retrofitting process is in-tended to encourage similarity per se . If we exclude this set from consideration, we can observe that SG is outperformed by at least one sense-specific model
Note that as expected, the amount of training data has an impact on the quality of the alignments and of the sense graph. Retrofitting sense-specific embed-dings using only 300k sentence pairs, which repre-sent about 5% of the total training data, does not give clear benefit over word-form embeddings. Building on Jauhar et al. (2015), we have presented an alternative means of deriving information about senses and sense relations to build sense-specific vector space representations of words, making use of parallel text rather than a manually constructed ontology. We show that this is a viable alterna-tive, producing representations that perform on par with those retrofitted to sense graphs based on Word-
Based on these results, it would be interesting to evaluate further refinements of the sense graph: alignment-based senses could be clustered, or fur-ther filtered to reduce the impact of alignment noise; new edges could be added using other multilingual resources. Finally, it will be important to evaluate the effectiveness of the retrofitted word embeddings on extrinsic tasks that require disambiguating word meaning in context.
 The authors would like to thank Sujay Kumar Jauhar for sharing software and data and for helpful dis-cussion. Thanks also to Manaal Faruqui and Pe-ter Turney for help in acquiring evaluation datasets, to Amittai Axelrod for his assistance with data, and to the anonymous reviewers for valuable comments and suggestions. This work was supported in part by an NSF Graduate Research Fellowship under Grant No. DGE 1322106. Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the NSF.

