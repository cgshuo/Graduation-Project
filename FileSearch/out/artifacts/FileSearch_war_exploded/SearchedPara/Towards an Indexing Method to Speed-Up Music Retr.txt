 Computations in most music retrieval systems strongly de-pend on the size of data compared. We propose to enhance performances of a music retrieval system, namely a harmonic similarity evaluation method, by first indexing relevant parts of music pieces. The indexing algorithm represents each au-dio piece exclusively by its major repetition , using harmonic descriptions and string matching techniques. Evaluations are performed in the context of a state-of-the-art retrieval method, namely cover songs identification, and results high-light the success of our indexing system in keeping simi-lar results while yielding a substantial gain in computation time.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods Algorithms, Performance, Experimentation
With the growing presence of large collections of musical content, methods for facilitating efficient browsing become more and more useful. The research field of Music Informa-tion Retrieval (MIR) aims at analysing and comparing music pieces on musical criteria, such as harmony, timbre, rhythm etc. Most MIR comparison systems are based on a common scheme to estimate similarity on audio material. One or several audio features are chosen and computed on signals segmented in small parts, or frames, thus providing feature sequences representing pieces. For instance, harmonic in-formation is often used since it provides a good insight of the architecture of music pieces. Then, adapted algorithms, such as dynamic time warping or alignment techniques, are used to yield similarity scores between sequences. However, algorithms involved in such retrieval systems highly depend on the length of representative sequences. Harmonic se-quences, for instance, are usually relatively long and induce a high computational cost to be compared. Hence, many works over the last decades focused on the detection of the most salient parts in musical sequences in order to speed-up music retrieval and analysis (see for instance [6]).
In this study, we propose to reduce the size of musical sequences by selecting relevant parts in order to index data involved in retrieval tasks. Our indexing method uses har-monic representations analysed by string matching techniques to isolate specific parts. Then, our indexing process is eval-uated in the context of harmonic similarity from a data re-duction step preliminary to cover song identification. Audio pieces are represented by sequences using Harmonic Pitch Class Profiles (HPCP) [1], frequently used to describe harmonic information. The aim of the indexing step is to extract a specific part in each audio piece, namely the ma-jor repetition , that can roughly be described as the longest repeated part perceived in a piece. In western popular mu-sic, for instance, this repetition classically consists in the concatenation of a verse and a chorus. The motivation of such a selection is to reduce the audio representation to a summarized, less redundant version, that remains relevant to characterize properly the music piece. To isolate rele-vant parts, each harmonic sequence is analysed using string matching techniques by considering each HPCP feature as a symbol. We consider the polynomial algorithm proposed by Kannan et al. [2], that mainly relies on a local alignment algorithm [5] to detect approximate repetitions. In order to evaluate the alignment score of approximate repetitions, the latter technique requires scores for substituting each feature by another one and gap penalties. These scores are deter-mined according to the Optimal Transposition Index (OTI) method [4], which ensures a higher robustness to musical variations. The Pearson correlation coefficient is used as a similarity measure for OTI evaluations. For a detailed de-scription of the indexing process, please refer to [3].
The above process is evaluated as an indexing step for music retrieval, more specifically on harmonic content. One way to evaluate harmonic similarity retrieval systems is to identify cover songs, i.e. , every new recording, performance, or rendition of a previously recorded musical piece. In cover songs datasets, music pieces are divided into several classes, each one containing all the different covers of a track. Har-monic baselines of cover songs of a same piece are likely to remain very similar, even in presence of strong musical vari-ations. For our experiments, we focus on a robust cover song identification method, proposed by Serr` a et al. [4], and we compare its performances with and without the major repetition indexing step. white: from left to right, naive indexing processes I1, I2, I3. Table 1: Mean precision and computing durations for the retrieval system with each indexing method. MR stands for indexing by the major repetition .

We consider datasets chosen from personal music collec-tions that incorporate 7 classes A to G containing each be-tween 14 and 41 different covers songs (average of 32) of an original music piece, along with a  X  X onfusing songs X  dataset incorporating 1000 songs of the same genres and artists. Each class was constituted as widely as possible, containing for instance an acapella performance, an orchestral version or an electronic remix of the same popular song, Yesterday from The Beatles .
 Our method is compared to three naive indexing methods I1, I2 and I3. We denote by m the mean length of the indexed strings. I1 consists in indexing each sequence by its m first elements, I2 in indexing each sequence by its m first elements, where m denotes the length of the indexed string in the corresponding piece, and I3 in indexing each sequence by its m consecutive elements from the central position in the piece. Note that these three indexing methods as well as our method reduce data size with the same rate. On the evaluation database, the indexing algorithm reduces the size of sequences by k =2 . 96 on average on every cover classes. Since the local alignment techniques involved in [4] require | u | X | v | operations to compare two strings u and v ,the overall number of operations performed by local alignment computations in cover song identifications is divided by k 2 i.e. ,about8 . 58 in our case. Tab. 1 summarizes the durations elapsed for each task. In our experimental framework, cover identification lasts 324 minutes without any indexing step, and 41 minutes with our indexing step pre-computed (23 minutes for the pre-computation).

For each cover class, the recall and precision rates of the retrieval process within the first N retrieved songs are com-puted, where N denotes the number of covers for the cur-rent class. Note that in this particular context, precision and recall values are identical. Overall results, presented in Tab. 1 show that the retrieval system correctly identi-fies cover songs with an average precision of 74 . 4% with our indexing step and 78 . 7% without. Hence, the deletion of information seems to slightly affect the accuracy of the system, which highlights a limit of the indexing step pro-posed. However, naive indexing methods having the same data reduction rate yield 15 . 4% to 17 . 3% significantly lower results, which confirms the relevance, for this application, of the parts identified by our method. Fig. 1 presents the distribution of precision scores over 7 cover classes. For each class, all the naive indexing methods (white bars) provide results 5% to 25% worse than the major repetition indexing method (black bars). For instance, the C cover class (38 cover songs) points out close values with our indexing sys-tem (88%) and without it (93%), although naive methods yield much poorer scores (63%, 56% and 63% resp. for I1, I2 and I3). This distribution is similar for every classes, with highest precisions for the non-indexed retrieval and with our process applied and significantly lower precisions with naive indexing methods.
The proposed method allows isolating in music pieces rel-evant parts, namely major repetitions , using harmonic fea-tures combined to string matching techniques. Indexing songs with these specific part in a music retrieval system basedonharmonicsimilaritywastestedinacoversong identification task. Results show that the application of our indexing method as a preliminary step to the harmonic sim-ilarity evaluation produces significant results, close to the non-indexed approach and much better than naive indexing methods, which highlights the relevance of the algorithm for indexing music retrieval systems. Experiments additionally show a significant gain in computing time, which gives the indexing step a substantial interest for computation issues, above all on large audio databases.
