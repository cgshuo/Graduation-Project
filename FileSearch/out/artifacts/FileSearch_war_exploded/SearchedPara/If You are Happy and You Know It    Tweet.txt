 Extracting sentiment from Twitter data is one of the funda-mental problems in social media analytics. Twitter X  X  length constraint renders determining the positive/negative senti-ment of a tweet difficult, even for a human judge. In this work we present a general framework for per-tweet (in con-trast with batches of tweets) sentiment analysis which con-sists of: (1) extracting tweets about a desired target subject, (2) separating tweets with sentiment, and (3) setting apart positive from negative tweets. For each step, we study the performance of a number of classical and new machine learn-ing algorithms. We also show that the intrinsic sparsity of tweets allows performing classification in a low dimensional space, via random projections, without losing accuracy. In addition, we present weighted variants of all employed algo-rithms, exploiting the available labeling uncertainty, which further improve classification accuracy. Finally, we show that spatially aggregating our per-tweet classification results produces a very satisfactory outcome, making our approach a good candidate for batch tweet sentiment analysis. I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis Algorithms, Experimentation Twitter sentiment analysis, compressed learning, supervised learning, sparse modeling, Bayes classification, SVM.
Twitter, a micro-blogging service, is among the most per-vasive social media services. On a regular basis, its users willingly share their thoughts, preferences, and emotions, in the form of (up to) 140-characters length messages (a.k.a. tweets). Although the field of social media analytics for rich sources of information, like weblogs, is becoming mature, micro-blog analysis is in its initial stages. Recent studies [5, 6] have shown the predictive value of Twitter content in domains such as marketing, business, and politics.
Several data mining tasks can be defined for Twitter data, considering very diverse applications. Among them, senti-ment analysis [16] has increasingly gained attention. Senti-ment analysis tries to identify the positive or negative senti-ment of a corpus. Detecting major events based on tweets X  sentiments [1, 5, 6], and finding the pattern of temporal hap-piness and mood in human behavior [9, 11] are examples of applications of Twitter sentiment analysis.

In spite of the growing literature of Twitter sentiment analysis, there are several issues that limit its usage in prac-tice. Ignoring the objects, individuals, or products that a tweet is expressing emotion about, is a major gap between current state of the art approaches and practical applica-tions of Twitter sentiment analysis; in practice we are inter-ested in discovering people X  X  feelings about a certain prod-uct, topic, or in general a target [13]. There has been initial work [13] on target-dependent sentiment analysis. We name the process of separating tweets that are related to a target of interest target extraction .

In general, tweets do not always express sentiments. They may contain information, facts, or any kind of objective expressions. Thus, before actual sentiment analysis, polar tweets (i.e., those with sentiment) should be separated from neutral ones. Recently tweet analysis literature has also con-sidered neutral tweets in classification [1]. We refer to this step as sentiment extraction .

The three steps required for sentiment analysis are delin-eated in Figure 1. Target extraction distinguishes between tweets that are related to our topic of interest and unrelated ones [13]. Sentiment extraction separates tweets with emo-tional content. This is sometimes referred to as the polar-neutral classification problem [1, 3, 15]. Finally, sentiment classification sets apart positive from negative tweets.
To the best of our knowledge, the present work is the first attempt that addresses all the aforementioned tasks (Figure 1) together for sentiment analysis of single tweets. Spe-cially target extraction is a major step that is missing in related works [13]. Every step is formulated as 2-class clas-sification problem, and several supervised learning meth-ods were developed and evaluated. Unlike previous works t hat use sophisticated language features [1, 3] with heavy pre-processing, we show that high accuracy in each step is achievable by just using bag-of-words as the classification in-put. In addition, we show that this sparse high-dimensional data can be projected in a low-dimensional space, using ran-dom reconstructible projections, without losing performance accuracy. Besides single label per tweet, our database has multiple labels (soft label) for each tweet which enables us to perform confidence weighted classification as well.
Multiple studies try to circumvent the scarcity of per-tweet information by analyzing batches of tweets. These batches can be built using different criteria, such as spatial (location of senders), temporal (time of post), or by author. The most common methods for batch analysis are lexicon-based, which use pre-compiled lists of polar words as indi-cators of the sentiment type [5, 6, 9, 11]. We supplement our work with sentiment classification for spatially aggre-gated tweets and show that performing per-tweet sentiment analysis followed by aggregation, results in high accuracy.
The rest of this paper is organized as follows. Labeling, pre-processing, and classification algorithms are discussed in Section 2. In Section 3 we present experimental results and detailed comparisons of several methods. Finally, in Section 4 we provide concluding remarks.
In this section we explain how the data is labeled. Then we comment on the preprocessing procedure and discuss the method for compression of the sparse input vector. Finally we explain classification methods and their weighted vari-ants that are being used in the cascade classifier.
Supervised learning algorithms rely on the availability of labeled data. The use of specific words to label tweets is common. For example, Pak and Paroubek [15] use an emoti-con list as indicator for positive or negative content, and Bollen et al. [5] use the phrase  X  X  feel X  to label tweets as polar (i.e., expressing emotions). Other work, e.g., [3, 10], gather noisy labels from multiple sources, like unevaluated sentiment analysis tools, and then incorporate uncertainty into the classification algorithm.

In recent years, crowdsourcing has emerged as a cost-effective way of carrying out labor-intensive tasks. In this work, the process of data labeling is crowdsourced as a part of the Dialogue Earth Project ( www.dialogueearth.org ).
First gross filtering on a collection of tweets is performed and tweets that do not contain any of the indicator words (i.e., words that are associated to the target) are filtered out. The data is then hand labeled by several evaluators with 4 labels: positive, negative, neutral, and not related to the target topic. The disagreement between evaluators shows the inherent difficulty of the task at hand. An additional label is reserved for cases in which the evaluator cannot as-sign a tweet to any of the aforementioned classes. Tweets that majority of evaluators can not assign any label to are discarded from further consideration.

Let C be the set of all classes. For each tweet i , evalu-ator j chooses a class label e ij , that is a | C | dimensional vector in which one element equals 1 and all remaining el-ements are zero. By normalizing the sum of these vecto-rial labels for each tweet i we get our soft vector label as  X  i = ( P r j =1 e ij ) /r , where r is the number of evaluators.
Now we can work with two variants of the label set. The first one is soft labels contained in each  X  i , Y = {  X  ic [0 , 1] | i = 1 , . . . , n ; c = 1 , . . . , | C |} , where  X  confidence of label c for data point i and n is number of tweets. Alternatively we can consider hard labels derived from  X  i that is  X  Y = { y i = argmax c  X  C  X  ic | i = 1 , . . . , n } , and thus falling back into usual classification in which each data point has a single label, referred to here as dominant labels. Finally, we name two classes that we classify them in each step of cascade classifier C 1 and C 2 .
Since we use the bag-of-words model for representing the tweets, our preprocessing is very simple. We begin by ex-tracting the words, i.e., actual words and also numbers, user-names, emoticons, URLs, etc. Actual value/content of num-bers, usernames, and URLs is not important for us and thus we replace them by special generic identifiers. We do re-move re-tweet signs (RT), special characters (not contained in emoticons), and stop words. Note that we keep emotional stop words and negations [15] since they are crucial for sen-timent analysis. Hashtags are Twitter tags which are often a concatenation of words (e.g.,  X #loveThisWeather X ); when the words in a hashtag begin with an uppercase letters, we break it into separate words.

We spell check the words using three dictionaries: an En-glish dictionary, a Twitter dictionary which contains specific lingo, and an emoticon dictionary.

Finally, we perform another step which empirically proved to be effective in both speed and accuracy of classifications. Words that appeared in the cleaned database less than thrice are pruned. Also we remove words that are highly frequent and their frequencies in C 1 and C 2 are close. A word is highly frequent in a class if its frequency is more than 0.05 in the tweets of that class. Frequencies of a word in two classes are close if their difference is less that 0.2.
In the bag-of-words model a document is simply repre-sented as an unordered collection of words T . A predefined set of words W = { w i | i = 1 , . . . , d } is then used to build a d -dimensional feature vector v for each document T such that (  X  i = 1 , . . . , d ) v ( i ) = #( T, w i ), where #( T, w number of times word w i appears in T . Here d  X  10 4 but since the tweet X  X  length is limited, v is extremely sparse.
Although we have done experiments using the original high-dimensional bag-of-words vector v , we extend our ex-periments and also use low dimensional projection of it as input. Using random reconstructible projection, we project (compress) the sparse vector v to lower dimension and per-form classification in that domain. Projection to lower di-mensional space is also known as the hashing trick [18].
We use an m  X  d m atrix P ( m  X  d ) to create a compressed representation x = Pv of a feature vector v in such a way that m is as small as possible and v can be reconstructed from x . P is a random matrix, i.e., its entries p ij are sampled from i.i.d. random variables [12]. We build P by sampling its entries p ij from a Gaussian distribution N (0 , 1 /m ). The value of m is chosen such that m = O ( h log( d/h )) where h = max v  X  V k v k 0 (while d  X  10 4 , h  X  20).
 To conclude, the set of (one per-tweet) feature vectors V = { v i | i = 1 , . . . , n } is represented in the compressed domain by a set of vectors X = { x i | i = 1 , . . . , n } , where x
Several well-known supervised learning algorithms and their weighted variant have been used for classification of each step. We start by explaining the newest method which is based on dictionary learning [17] and subsequently dis-cuss weighted variant of Support Vector Machine(SVM) [4], K Nearest Neighbor(KNN) [4], and Na  X   X ve Bayes(NB) [4] briefly.
In sparse coding a signal is approximated with a linear combination of a few elements (atoms) of some (often) re-dundant basis. When these bases are learned from the data itself, they are usually called dictionaries [7].

Formally, we aim at learning a dictionary D  X  R m  X  k such that a training set of signals X = { x i  X  R m | i = 1 , . . . , n } (and later testing data from the same class) can be well represented by linearly combining a few of the basis vectors formed by the columns of D . This problem can be casted as the following optimization: which is convex with respect to  X  i s when D is fixed and viceversa. The optimization is then commonly solved by al-ternatively fixing one and minimizing over the other. Fixing D and solving for  X  i s is known as sparse coding and finding D for fixed  X  i s is referred to as dictionary learning . We use publicly available SPAMS library ( www.di.ens.fr/willow/ SPAMS ) for solving (1)
Sparse modeling has been previously employed for super-vised classification tasks [17]. Classification is often done by first learning, following the above optimization, a dic-tionary D c for each class c  X  C using only training data from the set { x i  X  X | y i = c } . Classification is then per-formed with testing data X test , assigning a label c  X  = f ( x ) to each x  X  X test where f ( x ) = argmin c  X  C  X  ( x , D  X  ( x , D c ) = min Dictionary Learning : For exploiting the possible available information in the non-binary confidence  X  ic introduced in section 2.1, we redefine the optimization (1) as: The closer  X  ic is to one, the more x i contributes to class c dictionary. We then solve (2) by alternating minimization on  X  i s and D c . The implementation is obtained by adding the weights to the SPAMS library.
 Na  X   X ve Bayes : A na  X   X ve Bayes classifier, assigns the maxi-mum a posteriori class to each test data point. Assuming conditional independence of the input vector X  X  features, clas-sification simplifies to y i = argmax c  X  C P ( c ) Q d j =1 where P ( c ) and P ( v ij | c )s are computed from their corre-sponding frequencies in the training data.

We incorporate the confidence weights in the na  X   X ve Bayes formulation. Methods for using soft labels in na  X   X ve Bayes have been proposed previously [14], here we present a sim-ple approach in which weights are used to compute P ( c ) = K Nearest Neighbor: In K Nearest Neighbor, class label is assigned to each test data point based on the labels of K closest training examples in the feature space. In order to use the weight information in KNN, instead of majority voting between the K nearest neighbor of v i , we add their K confidence vectors and pick the label with highest confi-Support Vector Machine: For including weights in SVM we follow [19], which introduced weighted SVM. The key idea is that we want SVM to classify point v i that has high confidence label (i.e., | w iC 1  X  w iC 2 | is near 1) correctly, but for points with low confidence, SVM can prefer margin max-imization to correct classification. Thus, for each data point we have different coefficient for slack variable that is propor-tional to the confidence of the data label. So the primal will change to
When the label set is binary, testing data X test = { x i 1 , . . . , n test } is accompanied by a label set  X  Y test argmax c  X  C  X  ic | i = 1 , . . . , n test } , and the per-sample loss function is1 [ f ( x i ) 6 = y i ] , where f is the mapping of input to output produced by classification algorithm and 1 [  X  ] is the indicator function. Therefore, the classification error is de-
I n our weighted framework, the testing label set takes the mentioned in Section 2.1, at each step of cascade classi-fier we perform 2-class classification to separate two dis-joint subsets of C namely C 1 and C 2 . So the weighted per-as l = argmax j  X  iC j , j = 1 , 2 . The higher the weight of a datum is, the more it costs to miss-classify it. Accordingly, we should redefine the error as the total loss over all data, normalized by total possible loss: P that the prior for weighted methods should also be computed accordingly.
For the main part of the experiments, we used three collec-tions of tweets. Two of them (DB1, DB2) are about weather and one is about gas price (GP), and they contains 4490, 8850, and 12770 tweets respectively.

Although in a natural scenario the first task is target ex-traction, because sentiment classification is at the center of attention in the literature, we start from it and subsequently discuss the other tasks in reverse order of Figure 1. We also u se sentiment classification to explain our parameters and their assigned values. All reported results are obtained using 10-fold cross validation.
In order to test the performance of the algorithms for sen-timent classification (third classification task of Figure 1), we only consider tweets which have an associated positive or negative sentiment. For unweighted experiments we con-sider only dominant labels and for weighted experiments we use the aggregated weights  X  iC 1 and  X  iC 2 , where here C and C 2 are representing negative and positive classes re-spectively.

For each algorithm, different parameter settings have been verified and results for best configurations are reported. Multi-nomial Na  X   X ve Bayes (MNB) outperformed other variants of NB in the original feature domain. K for KNN is set to 10. Linear kernel SVM performed better than other ker-nels. The main parameter in DL and WDL is the number of atoms in the dictionary. Our experiments showed that under-complete dictionaries (i.e., tall matrices) yield higher accuracy. We introduced ratio parameter to control the pro-portion of number of atoms to length of atoms. For all ex-periments we set ratio to 0.5. For detailed explanation of parameters setting refer to our technical report [2]. We consider two ways for modifying the input vectors. First, we can use their support (i.e., binary version) of the original word-count vector. Notice that tweets are them-selves near binary. Second, each word-count or binary vec-tors can be projected to a compressed domain using random projection. The result of each setting is presented for the sentiment classification in Table 1 just for DB2.

As it is clear from Table 1 classification in low dimensional space without major loss of classification accuracy (in com-parison with classification in v space) is possible. Recent results, [8], show theoretically that learning can be done in the compressed domain without significant loss in classifica-tion accuracy for support vector machine.

Based on the results of this step, we picked for each method the setting for input vectors which yields the best accuracy. NB and KNN best results are achieved with uncompressed binary vectors. Performance of both SVM and DL were increased using compressed binary vectors. Table 2 shows these results for all three databases.

We also compare our results with lexicon-based meth-ods. Google has recently provided an API (G-API) that classifies a tweet, as either neutral, positive, or negative ( t wittersentiment.appspot.com ) [10]. Linguistic Inquiry and Word Count (LIWC) is a text analysis software that was recently used by [11] for sentiment analysis of collec-tions of Twitter data ( www.liwc.net ).

Another lexicon based method (WAH) has been recently proposed by [9]. Following an extensive study of words X  sen-timent in [9], Dodds et al. generated a list of words with hap-piness score from 1 to 10. After eliminating neutral words (i.e., with score around 5), they compute the weighted aver-age happiness (WAH) of a batch of tweets.

G-API and LIWC perform 3-class classification (i.e., pos-itive vs. negative vs. neutral) and since their source code is not available we could not modify it for 2-class classifica-tion. Therefore we feed them only with positive and negative tweets and report the results. On the other hand WAH is working only for sentiment classification step. As expected, lexicon based algorithms are not suited for per-tweet tasks (Table 2).

Table 3 presents the results of the weighted variants for the weighted loss functions introduced in Section 2.4.3. Note that weighted priors of Table 3 is different from unweighted prior of Table 2. Here again WNB has the highest accuracy in almost all databases, but its margin with WSVM and WDL is reduced in comparison with the unweighted case.
Different algorithms have been recently applied to senti-ment extraction (second classification task of Figure 1), such as NB [15] and SVM [3]. All these approaches use rich fea-ture vectors, that incorporate higher-level grammatical or semantical knowledge of some form. However we show that we can separate polar and neutral tweets with high accuracy by using simple bag-of-words input vector.

As in the previous section, we assume an oracle that dis-cards tweets that are not related to the topic of interest be-fore sentiment detection step. We therefore use only tweets for which the dominant label is positive, negative, or neu-tral. Results are summarized in Table 4 and Table 5. In both cases NB (WNB) performance is the best.

We now turn to the target extraction in which we detect tweets belonging to the given topic of interest (first classi-fication task of Figure 1). Since the required label for the GP database is not available, we only report results for DB1 and DB2. Tables 6 and 7 show the results of target extrac-tion for unweighted and weighted algorithms respectively. Again NB (WNB) and DL (WDL) are the best performing methods.
A s mentioned in Section 1 sentiment analysis sometimes is being done on batches of tweets instead of single tweet [5, 6, 9, 11]. One common way of making batches is aggregating tweets based on the geographic location of the authors (e.g., state or county). We supplement our experiments with the spatially agglomerated results of the sentiment classification.
Figure 2 shows the results aggregated per state for the GP database using WDL and the ground truth map. Maps and the additional statistics provided in Table 8 show that the state mood is correctly recovered by WDL. Note that WDL outperforms LIWC (an off-the-shelf software for batch tweet processing) by substantial margin.
In this paper we presented a cascaded classifier framework for tweet sentiment analysis. We formulated the sentiment analysis as a cascade classifier with three sequential 2-class classification steps. In the first step, we separate tweets that are about the topic of interest, and then filter out tweets that do not contain any emotion. Finally we set apart positive from negative tweets.

Many previous work have tried to enrich the per-tweet information in various ways. Some added language level features [1, 3, 15] to improve input signal. The other used batches of tweets [5, 6, 9, 11] to compensate scarcity of per-tweet information. We showed that even with simple bag-of-words feature vector high accuracy is achievable for per-tweet classification tasks. We also showed that projecting the sparse feature vector into a lower-dimensional space is computationally beneficial and does not significantly affect the classification accuracy. Considering performance of clas-sification in compressed domain, tailoring methods to work in that domain is a possible direction of future works. The research was supported by NSF grants IIS-0812183, IIS-0916750, IIS-1029711, and NSF CAREER award IIS-0953274. GS and MT acknowledge partial support from DARPA, ONR, NSSEFF, ARO, NGA, and NSF. We thank K ent Cavender-Bares and dialogueearth team for providing the databases and Karl Otness for the preprocessing code.
