 The burgeoning amount of textual data in distributed sources combined with the obstacles involved in creating and maintaining central repositories motivates the need for effective distributed information extraction and mining techniques. Recently, as t he need to mine patterns across distributed databases has grown, Distributed Association Rule Mining (D -ARM) algorithms have been developed. These algorithms, however, assume that the databases are either horizontally or vertically distributed. In the spec ial case of databases populated from information extracted from textual data, existing D -ARM algorithms cannot discover rules based on higher -order associations between items in distributed textual documents that are neither vertically nor horizontally dis tributed, but rather a hybrid of the two. In this article we present D -HOTM, a framework for Distributed Higher Order Text Mining. D -HOTM is a hybrid approach that combines information extraction and distributed data mining. We employ a novel information e xtraction technique to extract meaningful entities from unstructured text in a distributed environment. The information extracted is stored in local databases and a mapping function is applied to identify globally unique keys. Based on the extracted inform ation, a novel distributed association rule mining algorithm is applied to discover higher -order associations between items (i.e., entities) in records fragmented across the distributed databases using the keys. Unlike existing algorithms, D -HOTM requires neither knowledge of a global schema nor that the distribution of data be horizontal or vertical. Evaluation methods are proposed to incorporate the performance of the mapping function into the traditional support metric used in ARM evaluation. An example application of the algorithm on distributed law enforcement data demonstrates the relevance of D -HOTM in the fight against terrorism.
 Distributed data mining, distributed association rule mining, knowledge discovery, artificial intelligence, machi ne learning, data mining, association rule mining, text mining, evaluation, privacy -preserving, terrorism , law enforcement, criminal justice The burgeoning amount of textual data in distributed sources combined with the obstacles involved in creating and maintaining central repositories motivates the need for effective distributed information extraction and mining techniques. One example of this is in the criminal justice domain. For instance, there are more than 1,260 police jurisdictions in the Commonwealth of Pennsylvania alone. As was made strikingly clear in the aftermath of the terrorist attack on September 11, different kinds of records on a given individual may exist in different databases  X  a type of data fragmentation. In fact, the United States Department of Homeland Security (DHS) recognizes that the proliferation of databases and schemas involving fragmented data poses a challenge to information sharing. In response, the DHS is promulgating a  X  X ystem of Systems X  approach that ack nowledges the infeasibility of creating a single massive centralized database [5]. Indeed, the picture that is emerging in the DHS is basically a three -tier structure with some overlap between tiers: local databases, state databases and federal databases. The state collects information from local jurisdictions to form a state -level centralized database, and likewise for the federal government. However, due to the sheer volume of data, constraints on system interoperability as well as legal restrictions on d ata sharing, not all information is passed from local jurisdictions to the state -level. Likewise, the federal level captures only a modicum of the data available in state -level databases. In essence, the resulting data sharing structure is pyramidal in nat ure. The more centralized the database, the less information that is shared. Given this reality, the DHS as noted has acknowledged that it is simply not feasible to keep an all -in -one federal database. As a result, the DHS is promoting a  X  X ystem of Systems  X  approach that is based initially on the creation of standards for interoperability and communication in areas where standards are currently lacking. Indeed, efforts are underway to establish standards in database schema integration (e.g., OWL [7], GJXDM [11], etc.). Nonetheless, even with the widespread acceptance of such standards, the ability to integrate schemas automatically is still an open research issue [10, 8, 15, 16]. A related issue is the fact that current algorithms for mining distributed data are capable of mining data (whether vertically or horizontally fragmented) only when the global schema across all databases is known [4, 6, 9, 1 4 , 1 8, 24 ]. In the case of information extracted from distributed textual data, however, no preexisting global schema (i.e., dictionary of terms) is available. This is due to the fact that the entities extracted may differ between textual documents at the same or different locations. In short, schemas of textual entities are highly fluid in nature. As a result, a f ixed global schema cannot be assumed and current algorithms that rely on the existence of such a schema cannot be employed. For example , existing privacy -preserving techniques do not function in the absence of knowledge of the global schema [24] .
 As noted , distributed Association Rule Mining (ARM) algorithms mine association rules from data in horizontally or vertically fragmented databases. It is our contention that the restriction to such database sources is unnecessary, and that useful rules can be mine d from diverse databases with different local schemas as long as records can be linked via, for example, a unique key such as SSN. Many interesting applications emerge if one considers this approach, which we term higher -order distributed association rule mining. Higher -order implies that rules may be inferred between items that do not occur in the same local database schema. In other words, rules can be inferred based on items (entities) that may never occur together in any record in any of the distribute d databases being mined.
 In this article, we propose a distributed higher -order text mining framework that requires neither the knowledge of the global schema nor schema integration as a precursor to mining rules. The framework, termed D -HOTM, extracts ent ities and discovers rules based on higher -order associations between entities in records linked by a common key. D -HOTM has two components: entity extraction and distributed association rule mining. The entity extraction is based on information extraction rules learned using a semi -supervised active learning algorithm detailed in [22]. The rules learned are applied to automatically extract entities from textual data that describe, for example, criminal modus operandi. The entities extracted are stored in lo cal relational databases, which are mined using the D -HOTM distributed association rule mining algorithm described in Section 3.
 The article is organized as follows. Section 2 summarizes the related work in parallel and distributed ARM. Section 3 describes our D -HOTM framework, and is followed in Section 4 by a discussion of issues in calculating support raised by the fragmented nature of distributed textual entity data. We draw conclusions and discuss future work in section 5. Association rule mining (ARM) discovers associations between items [1, 2]. Given two distinct sets of items, X and Y , we say Y is associated with X if the appearance of X implies the appearance of Y in the same context. ARM outputs a list of association rules of the f ormat X  X  Y , where X  X  Y has a predetermined support and confidence. Many ARM algorithms are based on the well -known Apriori [1] algorithm. In Apriori, rules are generated from itemsets, which in turn are formed by grouping items that co -occur in instances of data. The prototypical application of ARM is market -basket analysis in which items that are frequently purchased together are discovered in order to aid grocers in layout of items. Parallelism is an ideal way to scale ARM to large databases. There are two major approaches for using multiple processors: parallel ARM algorithms, in which all processors access shared memory, and distributed ARM algorithms, in which each processor accesses its own private memory and communication is accomplished via message pas sing. Most parallel and distributed ARM algorithms are based on a kernel that employs the Apriori algorithm [23 ]. Parallelism in both shared -memory and distributed memory ARM algorithms can be categorized as data -parallelism or task -parallelism [3, 23] . Da ta -parallelism logically partitions the database among processors. Each processor works on its local partition using the same computational model. Count distribution (CD) is a simple data -parallelism algorithm. Each processor generates the local candidate itemsets independently based on the local partition. Then the global counts are computed by sharing (or broadcasting) the local counts, and the global frequent candidate itemsets are generated . Data -parallelism exchanges only the counts among processors, w hich minimizes the communication cost. As a result, it seems ideal for use in a distributed environment.
 In task -parallelism, each processor performs different computations independently, yet all have access to the entire dataset. For example, the computat ion of candidate itemsets of different sizes might be distributed among processors in a parallel loop across itemsets. In this case, each processor generates global counts independently. As noted, this requires that each processor have access to the entire dataset. In a distributed environment, this can be accomplished by performing an initial  X  X opy -in X  operation of the dataset, albeit often at great cost.
 Distributed ARM algorithms discover rules from distributed databases. Fast Distributed Mining (FDM) is based on count distribution [6]. The advantage of FDM over CD is that it reduces the communication cost by sending the local frequent candidate itemsets to a polling site instead of broadcasting. Also based on CD, Ashrafi, et al. [4] propose the Optimized Distributed Association Mining (ODAM) algorithm which both reduces the size of the average transaction and reduces the number of message exchanges in order to achieve better performance. The transactions are reduced by deleting the non -frequent items from the itemsets and merging several transactions with the same itemsets into one record. As for the message exchange, instead of using broadcast as with CD or polling sites like FDM, ODAM just sends all local information to one site, called the receiver. The receiver then calculates the global frequent itemsets and sends them back.
 It is noteworthy that each of the parallel and distributed ARM algorithms discussed assume that the databases are horizontally distributed. This limits the applicability of these a lgorithms. To address this issue, distributed mining of vertically fragmented data has received a growing amount of attention, especially in the context of privacy preserving data mining. For example, Vaidya and Clifton [18] propose a privacy preserving as sociation rule mining algorithm for vertically distributed data. The authors use a vector to represent each (vertically fragmented) record in which the attributes in the schema are distributed amongst the different local sites. In essence, it is a method f or mining association rules when the (global) schema is distributed amongst multiple sites. As noted in the introduction, however, this approach requires a priori knowledge of the complete (global) schema by all sites, and is thus unsuitable for mining rul es from distributed data for which local schemas differ and the global schema is unknown. In fact, although these algorithms deal with fragmented data, they make the following three assumptions: (1) the databases are vertically fragmented; (2) there is a g lobal key to unambiguously link subsets of items, and no additional techniques are needed to identify which subsets should be linked; (3) the global database schema is known. In the following section, we present the D -HOTM framework which performs entity e xtraction and distributed higher -order association rule mining independent not only of these three constraints, but also independent of the aforementioned constraint that data be horizontally distributed. In this section, we present our Distributed Higher -Order Text Mining framework, which discovers rules based on higher -order associations between entities extracted from textual data. The first step in D -HOTM is to extract linguistic features, or entities, from textual documents. For example, law enforcement agencies generate numerous reports, many of them in narrative (unstructured) textual form. Much valuable information is contained in these reports. Unfortunately many agencies do not utilize these descriptiv e reports  X  they are generally filed away either in hardcopy form (e.g., printed or typed), or in outdated electronic formats. Information extraction techniques can however be employed to automatically identify and extract data from such descriptions and s tore it in fielded, relational form in databases. Once stored in relational form, the extracted information is useful in a variety of everyday applications such as search, retrieval and data mining. We have developed an algorithm that learns rules and ex tracts entities from unstructured textual data sources such as criminal modus operandi, physical descriptions of suspects, etc. Our algorithm discovers sequences of words and/or part -of -speech tags that, for a given entity, have high frequency in the label ed instances of the training data (true set) and low frequency in the unlabeled instances (false set). The formal definition of the class of rules discovered by our algorithm is given in [22], and each rule is termed a reduced regular expression (RRE). Our algorithm first discovers the most common element 1 of an RRE, termed the root of the RRE. The algorithm then extends the RRE in  X  X ND X ,  X  X AP X , and  X  X tart/End X  learning phases (Figure 1 from [22]).
 Our approach employs a covering algorithm. After an RRE is generated for a subset of the true set for a given entity, the algorithm removes all segments covered by the RRE from the true set. The remaining segments become a new true set an d the steps in Figure 1 repeat. The learning process ends when the number of segments left in the true set is less than or equal to a user -defined threshold [22]. In each iteration of Figure 1, a single RRE is generated. This RRE is considered a sub -rule o f the current entity. After all RREs have been discovered for the current entity (i.e., all instances labeled with the entity are covered), the system uses the  X  X R X  operator to combine the sub -rule RREs into a single rule that is also an RRE. Our results d emonstrate that our algorithm achieves excellent performance on features important in law enforcement and the fight against terrorism [22]. After applying the entity extraction algorithm to unstructured textual data, the it ems (i.e., entities) extracted populate databases local to each site that in turn become input to our distributed higher -order (DiHO) ARM algorithm. Each row in a given local database represents an object, which is for example a particular individual menti oned in an investigative report. In addition to the item (or items ) identifying the object such as a person X  X  name or SSN, each row also contains other items known to exist in the source document . It is clear that the distributed data cannot be horizontall y fragmented because there is no guarantee that every site will include the same set of items, and in the case where an object is not a document or report but a person, different distributed sites may also refer to the same object multiple times (e.g., inv estigative reports about different crimes committed by the same individual). On the other hand, the data is not vertically fragmented either, because there is no one -to -one mapping connecting records in the distributed databases. In addition, the (local)  X  schema X  for each individual document varies, and no clean division of all objects X  items into identical sets can be made as required for vertically fragmented data. As a result, the data is neither vertically nor horizontally fragmented, but is present in a form we term a hybrid fragmentation .
 The skeleton of our DiHO ARM algorithm is depicted in Figure 2. In step 1, the set of items used for linking records is selected. One requirement for this set is t hat the item (or combination of items) must uniquely identify objects. To understand the difference between an object and a record , consider the following example: given documents such as research papers, the combination of the two attributes title and aut hors might be selected as the subset used for record linkage because in general these two attributes together form a unique identifier for each document. On the other hand, given documents such as police investigative reports, SSN might be selected as the subset used for linkage because such reports are written about individuals, and individuals are uniquely identified by SSN. In the former case, title and authors together uniquely represent research paper objects, whereas in the latter case, SSN uniquely r epresents person objects. Of course, in the latter case there are also investigative report objects, but the point is that in the latter case linkage is done using person objects, not report objects. To distinguish the usage of object vs. record , in what f ollows the items in the subset used for linkage refer to objects , while a record consists of the collection of items (i.e., entities) extracted from a given document (e.g., a police report ). Naturally this implies that one or more of the items in a given r ecord uniquely represent the object used for record linkage . In step 2, a globally unique ID is assigned to each object and record, respectively . This step is discussed in more detail in subsection 3.2.1, and an example is given in section 3.3.
 Step 3 disc overs linkable records using the item(s) selected in step 1. For example, if a given suspect appears in a burglary record in Detroit, and the same suspect also appears in a mugging case in Philadelphia, and the SSN is chosen as the linking item, then those two distributed records are considered linkable. In a practical sense, linking these two records might reveal new information to the investigating police officer. This step is discussed in detail in subsections 3.2.2 and 3.2.3. A more extensive example is given in section 3.3. After determining which distributed records are linkable, entities are exchanged and records merged. Continuing with the same example, entities extracted from the record in Detroit are sent to Philadelphia, and vice versa. The two d istributed records about the same suspect are then treated as a new record which is stored in each local database. The final step is to apply a traditional association rule mining algorithm locally at each site to obta in the final association rules.
 In the following subsections, we take a closer look at step 2, the resolution of object identifiers, and step 3, the detection of linkable records. Our DiHO ARM algorithm requires that the items selected in step 1 of Figur e 2 uniquely identify objects. In other words, the item (or combination of items) should be globally unique and consistent. Unique means different objects have different identifiers while consistent implies that the records about the same object have the s ame identifiers. To understand the need for uniqueness, consider for example the online paper databases Axiom (Compendex X , INSPEC X ) and Citeseer. In many cases these two databases will have the same object (a reference to a scholarly research article) with different items. To illustrate, consider this simple example. Suppose the Citeseer database has a table : {ID, title, year, citenum} where ID is a unique ID assigned to the article (e.g., a DOI), title is the title of the publication, year is the year of p ublication, and citenum is the number of citations to the article. Likewise, suppose that the Axiom database has a table: {ID, title, year, code} where the first three fields are similar in meaning to those in Citeseer, and the fourth contains one of the c lassification codes assigned to the article by abstract and indexing personnel. Suppose an information scientist wishes to know the citation rate of articles containing certain Axiom classification codes. If we assume that the two databases use the same gl obal ID for articles, then in DiHO ARM each site will generate the local frequent 1 -itemset with the associated frequencies and exchange itemsets and frequencies. The 2 -itemset {code citenum} can then be calculated using the global ID to match records in t he two different databases  X  a higher -order association between code and citenum via the global ID.
 A problem arises, however, when there is no guaranteed unique global ID for objects. In this example, in the absence of a DOI or URN, there may be no failsa fe method to resolve object identity. In this particular case, however, author names can be concatenated with the title to form a reasonable approximation of a globally unique ID. As a general solution we employ an edit -distance algorithm to match IDs form ed in this way. Other special -purpose matching algorithms can be employed as well, such as that described in [19] for matching potentially falsified criminal suspect IDs. This does not guarantee, however, that for a given user -defined threshold, either the edit -distance or other special -purpose matching algorithms will predict matches correctly  X  both false positive and false negative mismatches are possible. Such mismatches will have an impact on both the support and confidence of the final rules. We deal with this issue further in section 4. The key step in enabling the discovery of higher -order associations is the detection of linkable records . T here are two cases to consider, the first of which is the case in which the item(s) selected to link records in step 1 of Figure 2 are also used to form the globally unique ID in step 2. For example, in the aforementioned Axiom and Citeseer example, each local record corresponds to a single document object. The combination of the title and author attributes serve both to link records from different sites and to uniquely identify objects. Thus, by comparing the global identifier (e.g., using an edit -distance function), linkable records can be discovered. It is worth noting that in this case, only 2 -order associations between items (i.e., when distinct items in two records are associated via a common item or items ) will be discovered during subsequent association rule mining.
 The second case that needs to be considered in detecting linkable records is more complex. In this case, record identifiers (such as DOIs for investigative reports) differ from the object identifiers used for linking. For example, suppose records are comprised of the entities extracted from report s that contain criminal suspect information such as SSN and modus operandi. Higher -order associations can be discovered by linking two such records through the SSN item. In this case, the linking item, SSN, uniquely identifies individuals, not records . This is different from the Citeseer/Axiom example in which the linking items were identical to the unique global identifier. The point in this example is that the item used to link records identifies a person object that is different from the investigative report record . I n addition, in this case a single investigative report record may contain multiple SSNs, and higher -order associations will not be limited only to 2 -order. In what follows we lay the theoretical framework for the detection of linkable records given these two scenarios. In order to address the issues regarding the detection of linkable records , it is first necessary to lay a theoretical foundation for reasoning about record linkage. As noted in section 2, the itemsets generated by Apriori are composed of items which co -occur in instances of data (i.e., in records). For example, for a given document record, title and author have particular values, and in that record these items are said to co -occur. Since th e items co -occur in the same record, the co -occurrence is termed 1 and is a direct link. Indirect links, on the other hand, involve more than one record and make use of a particular item to link records. For example, two different document records could be linked through a common author. Such links are termed higher -order . The DiHO ARM algorithm is designed to detect higher -order links between records. In the following, we first give a formal definition for such higher -order links. In the context o f detecting linkable records , we then prove that the maximum frequent itemsets generated using Apriori on the subset of items used to link records is sufficient to identify all linkable records . Definition 1 : Given two records R i and R j , let T= R T  X   X  . For any item a  X  T, we say R i and R j are 2 through a, or a 2 -linkable, denoted as R i  X  a R j . Theorem 1 : The relation between records that are a reflexive, symmetric and transitive.
 Proof : (1) Assume item a  X  R 1 . Then R 1  X  a R 1 Thus the relation is reflexive. (2) Assume R 1 and R 2 are 2 linkable, then R 1  X  a R 2 . Per Definition 1, R 1 item a, a  X  R 1 and a  X  R 2. But this implies R relation is symmetric. (3) Given that R 1  X  a R 2 and R for some item a, a  X  R 1 , a  X  R 2 , and a  X  R 3 . Per Definition 1, R R are a 2 -linkable, i.e., R 1  X  a R 3 . Thus, the relation is transitive. Without loss of generality, we simplify the discussion in what follows by dealing only with transiti ve links between records (i.e., we ignore reflexive and symmetric links). Furthermore, as will become evident, we allow each record to occur at most once in a given transitive path linking records.
 Definition 2 : Given n distinct records R 1 , R 2 , ..., R T = R i  X  R i+1 with T i  X   X  , let L be a list of items L = (a ..., a n -1 ) such that a i  X  T i . Then we say records R (k + 1) th -order linkable through L, where k is the number of distinct items in L. The (k+1) th -order link is denoted R R L a viable path .
 For example: R 1  X  a R 2  X  b R 3 is a 3 rd -order link because R are linked through two distinct items, a and b, and the viable path between R 1 and R 3 i s (a, b). On the other hand, R only a 2 nd -order link because R 1 and R 3 are linked through only a single item a, and the viable path in this case is (a, a).
 Theorem 2 : For any higher -order link between two records, there must exist at least one link which does not have repeated items in the viable path, or repeated records in the link. Proof : (1) Suppose we have a higher -order link which has two occurrences of an item b in the viable path as follows: Per Definition 1, we have b  X  R i , b  X  R i+1 , b  X  R clearly, R i  X  b R j+1 . Thus, the above higher -order link becomes a new link which has no repeated item in the viable path: (2) Suppose we ha ve a higher -order link where some record R the same as R j . Then, the general form of the higher -order link is: R the higher -order link can be rewritten as R 1  X  a1 ... R R j+1 ... R n . I.e., no repeated records occur in the higher -order link. When a given higher -order link has no repeated records or repeated items in the viable path, we term it a mi nimal higher -order link , and the corresponding viable path is termed a minimal viable path .
 Let the records supporting item a be denoted as R group on a . Similarly, R ab is a group on the pair of items ab. From theorem 1, the records in R a are a 2 -linkable to each other. For a given minimal viable path (a 1 , a 2 , ..., a n ), the corresponding minimal higher -order links can be written as R a1 -a1a2 R notation R a -ab , for example , means the group on item a minus the group on the pair of items ab. For instance, given a minimal viable which are derivable from the viable path (a, b). We term the set of such high er -order links a higher -order link cluster . To simplify, the higher order link cluster for a given minimal viable path (a a , ... a n ) is denoted R a1  X  a1 R a2  X  a2 ...... R an Definition 3 : The length of a higher -order link cluster is defined as the number of groups in the cluster. The distance between two groups is defined as the shortest of all possible higher -order links. For example, given the following two higher -order link clusters: The length of the fir st cluster is four while the length of the second is three. Suppose these two link clusters contain the only links between R a and R c , then from the link cluster R we see that the distance between R a and R c is three.
 Theorem 4 : Given a frequen t k -itemset X generated by Apriori for k  X  2, for any pair of items which are members of X , the distance between the groups on those items is at most three. Proof : Suppose we have two groups R b and R c , where items b,c  X  X. Per the Apriori algorithm, bc is a f requent itemset also. distance between the group on b and the group on c is three. If R  X  R bc = R c , then we will have R b -bc  X  b R bc distance of two between groups. The s ame case applies when R R bc  X  R c . If R b = R bc = R c , the distance between R one. Thus, we conclude that the distance between the group on b and the group on c is at most three.
 Theorem 5 : Given a frequent k -itemset A and j -itemset B (j,k  X  2) for which A is not a subset of B and B is not a subset of A, suppose there exists at least one item which is a member of A and B. Furthermore, suppose there are items a  X  A and b  X  B, then the distance between groups R a and R b is at most four.
 Proof : (1) I f item a  X  B, then we have a  X  B and b  X  B, and by theorem 4, the distance between groups R a and R b is at most three; (2) Let T be a set of items T={t | t  X  A and t  X  B}. Suppose a  X  A -T, b  X  B -T and c  X  T , then by theorem 4, the distance between groups R and R c is at mo st three, i.e., R a -ac  X  a R ac  X  c R between groups R b and R c is at most three also, i.e., R R c -bc . Thus, groups R a and R b can be linked through the viable path (a, c, b), yielding the higher -order link cluster R R cb  X  b R b -cb with distance at most four .
 As noted in the introduction to this subsection, our goal is to prove that the maximum frequent itemsets generated using Apriori on the subset of items used to link records is sufficient to identify all linkable re cords . Clearly, given any itemset A that is a subset of some itemset B , any pair of items in A will appear in B , thus any higher -order link cluster generated for a given pair of items in A will be the same as the cluster generated for the same items in B . As a result, we may now conclude based on theorem 4 that the 3 -order linkable records can be discovered using only the maximum frequent itemsets discovered by Apriori. Thus Apriori is applied in step 3 of Figure 2 to identify linkable records using the s ubset of items selected to form the global IDs. (Apriori will be used a second time (in step 6) to compute the final higher -order association rules at each distributed site as well.) In addition, links of even higher -order may be generated via connections between the maximum frequent itemsets discovered by Apriori in step 3. It seems intuitive to speculate that the higher the order of the link, the weaker the link. Although we are conducting experiments to explore this issue, t he question of where to stop h igher -order link detection is open at this point. In step 3 of our DiHO ARM algorithm in Figure 2, we speculatively limit our higher -order links to 4 th -order. The algorithm for completing step 3 to identify linkable records is shown below in Figure 3. An e xample application of this algorithm is given in the following section. Discover_Linkable_Records(level) For items selected in step 1 of Figure 2 Count locally, add global IDs (GIDs) into local GIDList If level == 2, exit // Per Definition 1 Generate frequent itemsets using Apriori For each maximum frequent itemset Generate the 3 rd -order link clusters // Per Theorem 4 If level = 3, exit For any maximum frequent k -itemset A and j -itemset B If A and B have one or more common items Generate the 4 th -order link clusters // Per Theorem 5 To further illustrate our algorithm, in this section we give a simple example. Consider a situation in the law enforcement domain where multiple investigative reports from different jurisdictions detail different crimes committed by the same per son. In this case, the criminal is the primary key (perhaps identified by name or SSN), and the various facts such as modus operandi that surround different crimes become the fragmented data items associated with the key. Let X  X  suppose that our goal is to learn association rules that link the type of crime committed by an individual with some aspect of the modus operandi used in committing the crime (e.g., the type of weapon used). This kind of association rule can be very useful in narrowing the list of po ssible suspects to question about new criminal incidents 2 . However, as noted earlier, we have no guarantee in this case that both the crime type and weapon used will be recorded in a given investigator X  X  record of an incident. This can result, for example, from incomplete (or inaccurate) testimony from witnesses. Thus D -HOTM is applied to discover associations between crime type and weapon used in multiple jurisdictions X  distributed databases.
 In Tables 1 and 2 below, we have depicted databases containing e ntities (i.e., items) extracted from 11 investigative police reports. Although not stored this way in the actual databases, to simplify the tables each column represent s a re cord while each row represents an entity . For example, the entities  X  X llen X  and  X  X  un X  were ex tracted from the first report, R 1, on Site 1. Tables 1 and 2 represent two databases at different (i.e., distributed) sites. In step 1 of the DiHO ARM algorithm in Figure 2, suppose that the suspect X  X  name is the item selected for linking records. Let us further suppose that each rec ord (i.e., entities extracted from an investigative report ) has been assigned a unique numerical ID as shown. In this case, however, the criminal suspect is the unique object, and the suspect X  X  name is used to link distributed records associated with each object. Hence, the items used to link records are {Allan, Jack, Carol, Diana, John, Bill}. Let the threshold for support be one. The next step is to discover the frequency of the itemsets from the local records associated with these items that are being us ed for linking. The globally frequent itemsets are obtained by exchanging the local information. The result is depicted in Table 3.
 Table 3. Local and Global ID Lists for Linkable Records Given an input level of three, step 3 in Figur e 2 can be completed using the algorithm depicted in Figure 3. The first step is to generate 2 nd -order links from 1 -itemsets. Since only one record supports {Bill}, and we do not allow the same record to appear more than once in the minimal higher -order li nks, no 2 links are generated for itemset {Bill}. However, the Apriori algorithm is applied using the remaining 1 -itemsets to generate all the frequent k -itemsets. In this example, only one 2 -itemset {Jack, Diana} is generated, which means that thi s is the only itemset capable of generating 3 rd -order link clusters (by theorem 4). As there are only two items in the 2 -itemset {Jack, Diana}, only a single higher -order link cluster exists; i.e., the link cluster between the group on Jack and the group o n Diana . As the group on Jack is not the same as the group on Diana , by theorem 4 the distance between R Jack and R Diana is three, and the groups are higher -order Using the GIDL ists to represent the groups on the items, we have {2,6,8} -{6} ~ Jack {6} ~ Diana {4,6} -{6}, or {2,8}~ The resulting higher -order links and link clusters are portrayed in Table 4.
 Itemset Higher -order lin k clusters Records involved Allen R 1  X  Allen R 7 {1, 7} Jack R 2  X  Jack R 6 {2, 6, 8} Carol R 3  X  Carol R 9 {3, 9} Diana R 4  X  Diana R 6 {4, 6} John R 5  X  Joh n R 11 {5, 11} Bill {10} Jack, Diana {R2, R 8}  X  Jack R 6  X  Diana R 4 {2, 8, 6, 4} This completes step 3 of the DiHO ARM algorithm in Figure 2. Next, step 4 of Figure 2 involves the exchange of the entities extracted from t he linkable records, and the subsequent generation of new, merged records based on the higher -order links discovered. At this point in the computation, each site has the same global information, which is depicted in Table 5.
 In steps 5 and 6 in Figure 2, the Apriori algorithm is applied again, this time to each local database. Since higher -order associations have been implicitly included in the new, merged records, both higher -order and the usual first -order associations will be included in the resulting rules generated by Apriori. For example,  X  X un  X  Robbery X  an d  X  X iana  X  Robbery X  are rules generated based on higher -order associations discovered by the DiHO ARM algorithm. These rules cannot be discovered by existing distributed association rule mining algorithms. The methods described in section 3.2.1 for resolving object identifiers lead to another challenge  X  evaluation. One of the most important metrics used in ARM is support , which is defined as the frequency of an itemset divided by the total number of instances . In distributed datab ases, the total number of instances can be calculated by counting the number of unique global object IDs. As noted in section 3.2.1, the function used to map local keys to a unique global identifier is not guaranteed to be 100% accurate. Different objects (and subsequently records ) can be incorrectly mapped to the same global ID; likewise, records that should be mapped to a single global ID can be mapped to different IDs. The error rate of the mapping function will thus influence both the support and confid ence metrics. It will not suffice to calculate support and confidence in the traditional way employed in existing ARM algorithms. The error rate of the mapping function must be considered in the calculation of these metrics. To our knowledge, no similar wo rk has been conducted that addresses this issue. In what follows, we present a novel evaluation analysis that incorporates an error rate into support. To simplify the presentation we use upper case letters to represent sets and lower case letters to repres ent single elements or sizes. In a realistic  X  X eal -world X  data mining scenario where databases reach terabytes in size, it is infeasible to obtain the true error rate of the mapping function. Hence, we must rely on an estimate of the error rate obtained f rom a sample of the data for which all errors have been manually identified. This sample data is termed a ground truth because, for the sample, the actual error rate is known. As a result, in the analysis that follows we estimate the true error rate as the upper bound of an assumed normally distributed observed error rate for a given confidence level Given a sample data set R ={ r 1 ,r 2 ,...r k } where r record, the ground truth data set G ={ g 1 , g 2 , ..., g R , where g i  X  R ,  X  g i =  X  and each element g i records which map to a single object. On the other hand, the ID mapping performed on R will result in a partition P ={ p p }, where p i  X  R ,  X  p i =  X  and each element p records which map to a si ngle object. This second mapping might incorrectly map two different records which in fact represent distinct objects into a single group, or conversely fail to map the records representing the same object into a single group.
 Figure 4 depicts a simple example that reveals the problem context. The sample data set is R ={1,2,3,4,5,6}, where records 1 and 2 represent the same object (e.g., have the same primary key), records 3 and 4 represent another object, and r ecords 5 and 6 represent a third object. Thus, the ground truth data set G has three elements which are {1,2}, {3,4} and {5,6}. Suppose the ID mapping function in this case also results in the three elements P ={{1,3},{2,4},{5,6}}. Clearly only one element in P is correct, {5,6}. The observed error rate is defined as the number of objects (i.e., objects in the ground truth data set) absent in the modeled data divided by the total number of objects (the size of the ground truth data set). In symbolic form the observed error rate f is: We also define the observed difference degree t X  as: where n X  is the size of the modeled data set and n is the size of the ground truth. Assuming that f is normally distribut ed, given a confidence level 1 -c , the probability that the normalized error rate is greater than z is: where e is the true population error rate. We use the upper bound of the confidence interval to estimate the true population error r ate e of the data set as follows: Similarly, the difference degree t is estimated in the same way. Having estimated both e and t based on sample data com pared to the ground truth, the e ffect of the ID mapping function on support ca n b e logically deduced as follows.
 Given a set of real application data, the modeled data set D X  can be obtained by applying the ID mapping function to the application data. The support rate for a given atom set on D X  is denoted as s X  . Suppose D is the g round truth for the application data, which is unknown. To estimate the true support error rate s , we must estimate both the size of the ground truth data set as well as the number of objects supported (i.e., the support number).
 Based on the definition of difference degree t , the size of the ground truth data set can be denoted as: m = m = | D | (2) Consider the modeled application data D X  as two parts: one part  X  denoted as T  X  containing all the correctly mapped objects; wh ile the other part  X  denoted as E  X  is an incorrect partition over a set of records that represents the incorrectly mapped objects. Thus: D X  = T + E , T  X  E =  X  The ground truth data can be represented in a similar way:
D = T + W , T  X  W =  X  where W is the correct partition for the same set of records partitioned in E .
 For the example shown in Figure 4, the modeled data set D X  has three objects: {1,2}, {3,4} and {5,6}, while the ground truth D= {{1,3},{2,4},{5,6}}. T ={{5,6}} because the mo deled data correctly group the two records 5 and 6 into one object, as in the ground truth. In this case, E= {{1,3},{2,4}} and W ={{1,2},{3,4}}. Clearly E and W are two different partitions of the same four records, and E  X  W =  X  .
 Theorem 6: Given a set X = { x 1 , x 2 , ..., x u } where X  X  E , | x 1; set Y = { y 1 , y 2 , ..., y v } where Y  X  W and  X  x y  X  Y , | y j |  X  2.
 partitions of the same data set,  X  x i  X  X where the same record r  X  x i . Give n that | x i | = 1  X  i , x i = { r : | x is correctly mapped and a violation of the original assumption. Let S X  be the subset supporting a given itemset in D X  (i.e., ' s = , | S X  |  X  1), and S be the supporting subset in D (i.e., s | | = ). Furthermore, let where S W is the correct partition of a subset of the data while S an incorrect partition. Let X  X  consider the f ollowing cases.
 Case 1. S E =  X  and S T = S X  S =  X  means that all records in the application data are correctly mapped by the mapping function. Since S W partitions the same data set as S E , S W =  X  . Thus, the true supporting data set can be derived as: As a result, the true support rate in this case is Case 2. S T =  X  and S E = S X  Assuming that the estimated error rate e is the true error rate for the ground truth data set D , we have: From equations (2) and (4), the size of E can be derived as: | E | = m X   X  | T | = m X   X  ( m  X  | W | ) = tm  X  ( m  X  (1 -e ) m ) From equ ations (2) and (3), the size of | S E | can be derived as: | S E | = s X  m X  = s X  t m A special situation can occur in which each of the elements in S contains a single record. The number of records in S equal the number of objects in S E , which is s X  X m. By Theorem 6, the size of each element in S W must be at least two. The largest possible size of any element in S W is the total number of records in S . This occurs when every record in S E represents the same object. Thus, we conclude that: 1  X  | S W |  X   X   X  2 / ' tm s (5) In the general case when elements in S E contain more than a single record, the bounds for S W are: The lower bound is achieved when the records contained in all the elements in S E represent the same object. Assuming that the remaining objects in E map to z objects in W , em -z objects in W correspond to the objects in S E . When S E equals E , z is 0. Thus the up per bound em is achieved.
 Combining equations (5) and (6), we conclude that: 1  X  | S W |  X  max( em ,  X   X  2 / ' tm s ) Since m is large, we assume that 1 1  X   X  becomes: Case 3. Based on the previous two cases, we can now explore the true error rate in the general situation where S X  = S T + S E , S T  X  T , S E  X  E . | S E | =  X   X  | S X  | =  X   X  s X  X  X  =  X   X  s X  X m = (  X   X  s X  ) tm. The true support rate s can thus be represented as: Obviously, s (  X  ) is a linear function for which the upper bound is reached when  X  =0 and the lower bound when  X  =1. Thus: s W W From equation (8), given that | S E | = (  X   X  s X  ) tm , Equation (9) can thus be expressed as: The lower bound for the support s on the ground truth implies that there is only one object supporting the itemset. This situation occurs when the records co ntained in all the elements in S represent a single object, which results in only one object in S This result serves to demonstrate the fact that the calculation of support and confidence in distributed association rule mining is non -trivial. Naturally, we speculate that the probability of this extreme lower bound occurring is not large. Nonetheless, it cannot be ruled out theoretically.
 Although this analysis represents to the best of our knowledge the first attempt to incorporate global ID mapping erro rs into the evaluation of distributed ARM, clearly there is much work that remains. Specifically, we have yet to deal with the impact of errors in the ID mapping on the confidence metric, as well as the impact on the resulting rules. The approach we have t aken, however, serves to blaze a trail for such future work. We have presented D -HOTM, a novel distributed higher -order text mining algorithm that mines hybrid distributed data. Our D -HOTM algorithm is a first step towards t ackling the difficult challenge posed by heterogeneous distributed databases that cannot be easily centralized. This is also the first work to address the complex issues surrounding the use of the traditional support metric in the context of distributed hi gher -order ARM. Even so, this is just the beginning of the research task at hand and much of real interest remains to be accomplished.
 In no particular order, we plan to address both theoretical and practical issues in areas such as further exploration of the utility of higher -order associations as well as identifier linkage, evaluation metrics and workload balancing. Although D -HOTM applies a key resolution method to identify globally unique IDs, currently the algorithm relies on the user to identify the d atabase fields to be used in generating IDs. This process could be partially automated given semantic mappings such as those supported by the recently released Web Ontology Language [7]. A related challenge is the need for further work in the adaptation of metrics for distributed mining of hybrid fragmented data. Our initial foray into this area serves primarily to highlight the need to address these issues in a rigorous manner within an overall framework, both theoretical and empirical, for evaluation. We have taken the first steps in creating the theoretical framework herein, and have also developed an empirical framework in the Text Mining Infrastructure (TMI), a software infrastructure for sequential, parallel and distributed textual data mining [13]. D -HOTM will be released initially in a Linux/MPI version packaged with the existing TMI at hddi.cse.lehigh.edu .
 One of the more interesting open problems that has not been addressed in any previous work that we are aware of in distributed ARM deals with the efficiency of computation. D -HOTM too as presented is just a skeleton in this regard  X  a great deal of work is needed to deal with workload balancing as well as optimization of both computation and communication. Unbalanced workloads coupled with the requirements of synchronization will heavily affect the efficiency of the algorithm. This can be mitigated in part by the fact that the D -HOTM can be used to mine association rules from the results returned from a sea rch (as opposed to mining rules from entire databases). For instance, in the Axiom/Citeseer example given earlier, a user may wish to find strong associations between Axiom X  X  classification codes and the number of citations on Citeseer for a particular set of documents returned by queries. Furthermore, from a different perspective, multilevel parallelism can be introduced to improve runtime performance: e.g., at each distributed database, a parallel ARM algorithm can be executed. This also leads to the need for time and space complexity analyses based on metrics such as isoefficiency [12].
 Foremost in our thoughts, however, is a plan to deploy D -HOTM in a law enforcement environment. We have done much work in preparation for such a deployment. In [22], we de tail our work in information extraction from narrative textual data sources. In [21] we describe the design of a system based on the theory developed in [22] that has recently been deployed in the Bethlehem, Pennsylvania Police Department. This system uses advanced information extraction techniques to mine modus operandi data from the narrati ve text of police investigative reports. The extracted data populates a relational database, thereby enabling straightforward modus operandi search. The next phase of our research will build on this system. It involves the development and deployment of D -HOTM in the Northampton County, Pennsylvania region. Northampton County, Pennsylvania has 32 independent police jurisdictions, none of which share modus operandi data o n a systematic basis. The County has, however, recently implemented a high -bandwidth networking infrastructure that supports secure communication amongst all 32 jurisdictions. It is our plan to deploy D -HOTM in this environment, with distributed higher -ord er modus operandi association rule mining and search as our first application. In this way we will take a step towards realizing the  X  X ystem of Systems X  envisioned by the US Department of Justice.
 There are many other practical examples of how distributed higher -order rules can be mined from multiple diverse databases. The common advantage of employing such technology is the promise of discovery of higher -order associations in data sources that cannot be easily centralized. Thus the area of distributed asso ciation rule mining in general, and higher -order distributed mining in particular, is an intriguing and promising area of research. Existing distributed ARM algorithms, however, assume that the distributed databases are either horizontally or vertically fr agmented. Furthermore, existing privacy -preserving techniques for distributed ARM do not function in the absence of knowledge of the global schema [24] . In this article, we have presented a novel framework, D -HOTM, which supports mining of higher -order rul es from distributed textual data in a hybrid fragmented form independent of the knowledge of the global schema . The authors wish to thank the referees as well as Lehigh University, the Pennsylvania State Police, the Lockheed -Martin Corp oration, the City of Bethlehem Police Department and the National Institute of Justice (NIJ) , US Department of Justice. This work was supported in part by NIJ grant number 2003 -IJ -CX -K003. Points of view in this document are those of the authors and do not necessarily represent the official position or policies of Lehigh University, the US Department of Justice, the Pennsylvania State Police or the Lockheed Martin Corporation.
 We are also grateful for the help of Tim Cunningham, other co -workers, family mem bers and friends. We also gratefully acknowledge the continuing help of our Lord and Savior, Yeshua the Messiah (Jesus the Christ) in our lives and work. Amen. [1] Agrawal R., Imielinske T., and Swami A. N. Mining [2] Agrawal R., Mannila H., Srikant R., Toivonen H., and Inkeri [3] Agrawal R. and Shafer J. C. Parallel Mining of Association [4] Ashrafi M. Z., Taniar D. and Smit h K. ODAM: an Optimized [5] Boyd D., Director of the Department of Homeland Security X  X  [6] Cheung D. W., Han J., Ng VAT., Fu AWE. and Fu Y. A Fast [7] Dean M. and Schreiber G. OWL Web Ontology Language [8] Draper D., Halevy A. Y., Weld D. S., The Nimble XML [9] Evfimievski A., Srikant R., Agrawal R. and Gehrke J. [10] Genesereth M. R., Keller A. M., and Duschka O. M. [11] GJXDM. Global Justice XML Data Model. [Online Article]. [12] Grama A., Gupta A. an d Kumar V. Isoefficiency function: a [13] Holzman, L.E., Fisher, T.A., Galitsky, L. M., Kontostathis, [14] McConnell S. and Skillicorn D. B. Building predictors from [15] Papakonstantinou Y. and Vassalos V. Architecture and [16] Rahm E., Bernstein P.A. A survey of approaches to [17] Schuster A. and Wolff R. Communication -Efficient [18] Vaidya J. and Clifton C. Privacy preserving association rule [19] Wang, G., Chen, H. and Atabakhsh H. Automaticially [20] Witten, I. and Frank, E. Data Mining. Morgan Kaufmann, [21] Wu, T. and Pott enger, W. M. A Software System for [22] Wu, T. and Pottenger, W. M. A Semi -Supervised Active [23] Zaki M. J. Parallel and Distributed Association Mining: A [24] Clifton, D., Doan, A.H., Elmagarmid, A., Kantarcioglu, M., BOUT THE AUTHORS : Shenzhi Li is a Ph.D. student at Lehigh University conducting research in distributed association rule mining.
 Tianhao Wu is a Ph.D. student at Lehigh Universi ty conducting research in reduction of knowledge engineering cost in information extraction.
 William M. Pottenger, Ph.D. , is an assistant professor at Lehigh University directing research in theories and algorithms for parallel and distributed text and dat a mining.

