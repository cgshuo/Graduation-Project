 A central goal of systems neuroscience is to identify the functional relationship between environ-mental stimuli and a neural response. Given an arbitrary stimulus we would like to predict the neural response as well as possible. In order to achieve this goal with limited amount of data, it is essential to combine the information in the data with prior knowledge about neural function. To this end, generalized linear models (GLMs) have proven to be particularly useful as they allow for flexible model architectures while still being tractable for estimation.
 The GLM neuron model consists of a linear filter, a static nonlinear transfer function and a Poisson spike generating mechanism. To determine the neural response to a given stimulus, the stimulus is first convolved with the linear filter (i.e. the receptive field of the neuron). Subsequently, the filter output is converted into an instantaneous firing rate via a static nonlinear transfer function, and finally spikes are generated from an inhomogeneous Poisson-process according to this firing rate. Note, however, that the GLM neuron model is not limited to describe neurons with Poisson firing statistics. Rather, it is possible to incorporate influences of its own spiking history on the neural response. That is, the firing rate is then determined by a combination of both the external stimulus and the spiking-history of the neuron. Thus, the model can account for typical effects such as refractory periods, bursting behavior or spike-frequency adaptation. Last but not least, the GLM neuron model can also be applied for populations of coupled neurons by making each neuron dependent not only on its own spiking activity but also on the spiking history of all the other neurons. In previous work (Pillow et al., 2005; Chornoboy et al., 1988; Okatan et al., 2005) it has been shown how point-estimates of the GLM-parameters can be obtained using maximum-likelihood (or maximum a posteriori (MAP)) techniques. Here, we extend this approach one step further by using Bayesian inference methods in order to obtain an approximation to the full posterior distribution, rather than point estimates. In particular, the posterior determines confidence intervals for every linear weight, which facilitates the interpretation of the model and its parameters. For example, if a weight describes the strength of coupling between two neurons, then we can use these confidence intervals to test whether this weight is significantly different from zero. In this way, we can readily distinguish statistical significant interactions between neurons from spurious couplings. Another application of the Bayesian GLM neuron model arises in the context of spike-triggered covariance analysis. Spike-triggered covariance basically employs a quadratic expansion of the external stimulus parameter space and is often used in order to determine the most informative subspace. By combining spike-triggered covariance analysis with the Bayesian GLM framework, we will present a new method for selecting the filters of this subspace.
 Feature selection in the GLM neuron model can be done by the assumption of a Laplace prior over the linear weights, which naturally leads to sparse posterior solutions. Consequently, all weights are equally strongly pushed to zero. This contrasts the Gaussian prior which pushes weights to zero proportional to their absolute value. In this sense, the Laplace prior can also be seen as an efficient regularizer, which is well suited for the situation when a large range of alternative explanations for the neural response shall be compared on the basis of limited data. As we do not perform gradient descent on the posterior, differentiability of the posterior is not required.
 The paper is organized as follows: In section 2, we describe the model, and the  X  X xpectation prop-agation X  algorithm (Minka, 2001; Opper &amp; Winther, 2000) used to find the approximate posterior distribution. In section 3, we estimate the receptive fields, spike-history effects and functional cou-plings of a small population of retinal ganglion cells. We demonstrate that for small training sets, the Laplace-prior leads to superior performance compared to a Gaussian-prior, which does not lead to sparse solutions. We use the confidence intervals to test whether the functional couplings between the neurons are significant.
 In section 4, we use the GLM neuron model to describe a complex cell response recorded in macaque primary visual cortex: After computing the spike-triggered covariance (STC) we determine the relevant stimulus subspace via feature selection in our model. In contrast to the usual approach, the selection of the subspace in our case becomes directly linked to an explicit neuron model which also takes into account the spike-history dependence of the spike generation. 2.1 Generalized Linear Models Let X t  X  R d ,t  X  [0 ,T ] denote a time-varying stimulus and D i = { t i,j } the spike-times of i = 1 ,...,n neurons. Here X t consists of the sensory input at time t and can include preceeding input frames as well. We assume that the stimulus can only change at distinct time points, but can be evaluated at continous time t . We would like to incorporate spike-history effects, couplings between neurons and dependence on nonlinear features of the stimulus. Therefore, we describe the effective input to a neuron via the following feature-map: where  X  sp represents the spike time history and  X  st the possibly nonlinear feature map for the stimulus. That is, the complete feature vector  X  contains possibly nonlinear features of the stimulus and the spike history of every neuron. Any feature which is causal in the sense that it does not depend on future events can be used. We model the spike history dependence by a set of small time windows [ t  X   X  l ,t  X   X  0 l ) in which occuring spikes are counted. words, for each neuron there is a set of windows l = 1 ,...,L with time-lags  X  l and width  X  l  X   X  0 l describing its spiking history. More precisely, the rate can only change if the stimulus changes or a spike leaves or enters one of these windows. Thus, we obtain a sequence of changepoints value  X  i,j . In the GLM neuron model setting the instantanious firing rate of neuron i is obtained by a linear filter of the feature map: where  X  is the nonlinear transfer function. Following general point process theory (Snyder &amp; Miller, 1991) and using the fact that the features stay constant between two changepoints we can write down the likelihood P ( D |{ w } ) = Q n i =1 L i ( w i ) , where each L i ( w i ) has the form The function  X  ( . ) in the second equation is defined to be one if and only if its argument equals zero. The sum therfore is 1 iff a spike of neuron i occurs at changepoint  X  t j . Note that the changepoints  X  t depend on the spikes and therefore, the process is not Poissonian, as it might be suggested by the functional form of the likelihood.
 As it has been shown in (Paninski, 2004), the likelihood is log-concave in w i if  X  i (  X  ) is both convex and log-concave. We are using the transfer function  X  i ( u ) = e u which, in particular, gives rise to a which grows only linearly (cf. Harris et al. (2003); Pillow et al. (2005)).
 While we require all rates  X  i ( t ) to be piecewise constant, it should be noted that we do not restrict ourselves to a uniform quantization of the time axis. In this way, we achieve an efficient architecture for which the density of change points automatically adapts to the speed with which the input signal is changing.
 The choice of the prior distribution can play a central role when coping with limited amount of data. We use a Laplace prior distribution over the weights in order to favor sparse solutions over those which explain the data equally well but require more weights different from zero (c.f. Tibshirani (1996)): w i  X  k as above. In our applications, we allowed the prior variance features to be different from the variance of the spike-history features. The posterior takes the form: where each  X  i,j individually instantiates a Generalized Linear Model (either corresponding to a likelihood factor or to a prior factor). As the posterior factorizes over neurons, we can perform our analysis for each neuron seperately. Therefore, for simplicity we drop the subscript i in the following.
 Our model does not assume or require any specific stimulus distribution. In particular, it is not limited to white noise stimuli or elliptically contoured distributions but it can be used without mod-ification for other stimulus distributions such as natural image sequences. Finally, this framework allows exact sampling of spike trains due to the piecewise constant rate. 2.2 Expectation Propagation As exact Bayesian inference is intractable in our model, we seek to find a good approximation to the full posterior. In our case all likelihood and prior factors are log-concave. Therefore, the posterior is unimodal and a Gaussian approximation is well suited. A frequently used technique for this purpose is the Laplace-approximation which computes a quadratic approximation to the log-posterior based on the Hessian around the maximum. For the Laplacian prior, however, this approach falls short since the distribution is not differentiable at zero. Instead, we employ the Expectation Propagation (EP) algorithm (Minka, 2001; Opper &amp; Winther, 2000). In this approximation technique, each factor (also called site )  X  j of the posterior is replaced by an unnormalised Gaussian: where the b j ,  X  j are called the site parameters . The approximation aims at minimizing the Kullback-The log-concavity of the model implies that all  X  j  X  0 , which supports the numerical stability of the EP algorithm. Some of the  X  j may even be 0 , as long as Q ( w ) is a (normalizable) Gaussian. An EP update at j consists of computing the Gaussian cavity distribution Q \ j  X  Q  X   X   X  1 j and the non-Gaussian tilted distribution  X  P  X  Q \ j  X  j , then updating b j ,  X  j such that the new Q 0 has the same mean and covariance as  X  P (moment matching). This is iterated in random order over the sites until convergence.
 We omit the detailed update schemes here and refer to (Seeger et al., 2007; Seeger, 2005). Conver-gence guarantees for EP applied to non-Gaussian log-concave models have not been shown so far. Nevertheless it is reported that at least in the log-concave case EP behaves stable (e.g., Rasmussen &amp; Williams (2006)), and we observe quick convergence in our case (  X  20 iterations over all sites are required). The model still contains hyperparameters, namely the prior variances 2  X  2 periment, these were determined via a standard crossvalidation procedure (80% training data, 10% validation, 10% test). We applied the GLM neuron model to multi-electrode recordings of three rabbit retinal ganglion cells. The stimulus consisted of 32767 frames each of which showing a random 16  X  16 checkerboard pattern with a refresh rate of 50 Hz (data provided by G. Zeck, see (Zeck et al., 2005)). First, in order to investigate the role of the Laplace prior, we trained a single cell GLM neu-ron model on datasets of different sizes with ei-ther a Laplace prior or a Gaussian prior. The models, which have the same number of param-eters, were compared by evaluating their nega-tive log-likelihood on an independent test set. As can be seen on the right the choice of prior be-comes less important for large training sets as the weights are sufficiently constrained by the data.
 For each training set size a separate crossvalida-tion was carried out. Errorbars were obtained by Fig. 1 shows the spatiotemporal receptive field of each neuron, as well as the filters describing the influence of spiking history and input from other cells. For conciseness, we only plot the filters for 80 and 120 ms time lags, but the fitted model included 60 and 140 ms time lags as well. The strongly positive weights on the diagonal of figure 1(c) for the spiking history can be interpreted as  X  X elf-excitation X . In this way, it is possible to model the bursting behavior exhibited by the cells in our recordings (see also Fig. 2). The strongly negative weights at small time lags represent re-fractory periods. The red lines correspond to 3 standard deviations of the posterior. The first neuron seems to elicit  X  X ursts X  at lower frequencies. Note the different scaling of the y-axis for diagonal and off-diagonal terms. By analyzing the coupling terms, we can see that there is significant interaction between cells 2 and 3, but not between any other pair of cells. As our prior assumption is that the couplings are 0, this interaction-term is not merely a consequence of our choice of prior. As a result of our crossvalidation it turns out that the prior variance for spike history weights should be set to very large values (  X  = 0.1, variance = 2 1  X  2 ) meaning that these are well determinated by the data. In contrast, prior variances for the stimulus weights should be more strongly biased towards zero (  X  = 150). Figure 1: (a): Stimulus dependence inferred by the GLM for the three neurons (columns) at different time lags (rows). 2 of 4 time lags are plotted (60, 140 ms not shown). (b): Spike-triggered average for the same neurons and time lags as in (a). (c): Causal dependencies between the three neurons. Each plot shows the value of the linear weight as a function of increasing time lag  X  l (in ms). Shown are posterior mean and three std. dev. (indicated in red). Different scaling of the y-axis is used for diagonal and off-diagonal plots.
 Table 1: Predictions performance of different models. Entries correspond to the correlation coef-ficient between the predicted rate of each model and spikes on a test set. Both rate and spikes are binned in 5 ms bins. The first GLM models neither connections nor self-feedback.
 Because of the regularization by the prior the spatio-temporal receptive fields are much smoother than the spike-triggered average ones, see Fig. 1(a). The receptive fields of the STA seems to be Figure 2: Predicted rate for the GLM neuron model with and without any spike history and the predicted rate for the STA for the same neurons as in the other plots. For the STA the linear response is rectified. Rate for the GLM with spike dependence is obtained by averaging over 1000 sampled spike-trains. Rates are rescaled to have the same standard deviation. more smeared out which might be due to the fact that it cannot model bursting behavior. The more conservative estimate of the sparse neuron model should increase the prediction performance. To verify this, we calculated the linear response from the spike-triggered average and the rate of our GLM neuron model. In order to have the same number of parameters we neglected all connections. As a model free performance measure we used the correlation coefficient between the spike trains and the rates (each are binned in 5 ms bins). For the GLM with couplings, rates were estimated by sampling 1000 spike trains with the posterior mean as linear weights. As our model explicitly includes the nonlinearity during fitting, the rate is more sharply peaked around the spikes, see Fig. 2. The prediction performance can be increased even further by modeling couplings between neurons as summarized in Tab. 1. Complex cells in primary visual cortex exhibit strongly nonlinear response properties which cannot be well described by a single linear filter, but rather requires a set of filters. A common approach for finding these filters is based on the covariance of the spike-triggered ensemble: Eigenvectors of eigenvalues that are much bigger (or smaller) than the eigenvalues of the whole stimulus ensemble indicate directions in stimulus space to which the cell is sensitive to. Usually, a statistical hypothesis test on the eigenvalue-spectrum is used to decide how many of the eigenvectors e i are needed to 1988). Here, we take a different approach: We use the confidence intervals of our GLM neuron model to determine the relevant dimensions within the subspace revealed by STC. We first apply STC to find the space spanned by a set of eigenvectors that is substantially larger than the expected dimensionality of the relevant subspace. Next, we fit a nonlinear function n i to the filter-outputs f equation (1) with (  X  st ) i ( X t ) = n i ( f i ( X t )) Figure 3: (a): 24 out of 40 Filters estimated by STC. The filters are ordered according to their log-ratio of their eigenvalue to the corresponding eigenvalue of the complete stimulus ensemble (from left to right). Highlighted filter are those with significant non-zero weights, red indicating excitatory and blue inhibitory filters. (b) Upper: Posterior mean +/-3 std. dev. Filter indices are ordered in the same way as in (a). Lower: Predicted rate on a test set for STC and for the GLM neuron model with spike history dependence on a test set.
 As the model is linear in the weights w i , we can use the GLM neuron model to fit these weights corresponding weight w i will automatically be set to zero by the model due to the sparsity prior. This provides an alternative, model-based method of determining the number of filters required to model the cell. The significance of each filter is not determined by a separate hypothesis test on the spectrum of the spike-triggered covariance, but rather by assessing its influence on the neural activity within the full model.
 As in the previous application, we can model the spike history effects with an additional feature vector  X  sp to take into account temporal dynamics of single neurons or couplings.
 Before applying our method to real data, we tested it on data generated from an artificial complex cell similar to the one in (Rust et al., 2005). On this simulated data we were able to recover the original filters. We then fitted this GLM neuron model to data recorded from a complex cell in primary visual cortex of an anesthetized macaque monkey (same data as in (Rust et al., 2005)). We first extracted 40 filters which eigenvalues were most different to their corresponding eigenvalues of the complete stimulus ensemble. Any nonlinear regression procedure could be used to fit a nonlinearity to each filter output. We used a simple quadratic regression technique. Having fixed the first nonlinearity we approximated the posterior as above. The resulting confidence intervals for the linear weights are plotted in Fig. 3(b). The filters with significant non-zero weights are highlighted in Fig. 3(a). Red indicates exitatory and blue inhibitory effects on the firing rate. Using 3 std. dev. confidence intervals 9 excitatory and 8 inhibitory filters turned out to be significant in our model. The number significant (Rust et al., 2005). The rank order of the linear weights is closely related but not identical to the order of eigenvalues, as can be seen in Fig. 3(b), top. We have shown how approximate Bayesian inference within the framework of generalized linear models can be used to address the problem of identifying relevant features of neural data. More precisely, the use of a sparsity prior favors sparse posterior solutions: non-zero weights are assigned only to those features which which are critical for explaining the data. Furthermore, the explicit uncertainty information obtained from the posterior distribution enables us to identify ranges of sta-tistical significance and therefore facilitates the interpretation of the solution. We used this technique to determine couplings between neurons in a multi-cell recording and demonstrated an increase in prediction performance due to regularization by the sparsity prior. Also, in the context of spike-triggered covariance analysis, we used our method to determine the relevant stimulus subspace within the space spanned by the eigenvectors. Our subspace selection method is directly linked to an explicit neuron model which also takes into account the spike-history dependence of the spike generation.
 Acknowledgements We would like to thank G  X  unther Zeck and Nicole Rust for generously providing their data and for useful discussions.
 Chornoboy, E., Schramm, L., &amp; Karr, A.(1988). Maximum likelihood identification of neural point process systems. Biological Cybernetics , 59 , 265-275.
 Harris, K., Csicsvari, J., Hirase, H., Dragoi, G., &amp; Buzsaki, G. (2003). Organization of cell assem-blies in the hippocampus. Nature , 424 (6948), 552 X 6.
 Minka, T. (2001). Expectation propagation for approximate Bayesian inference. Uncertainty in Artificial Intelligence , 17 , 362 X 369.
 Okatan, M., Wilson, M. A., &amp; Brown, E. N. (2005). Analyzing functional connectivity using a network likelihood model of ensemble neural spiking activity. Neural Computation , 17 , 1927-1961.
 Opper, M., &amp; Winther, O. (2000). Gaussian Processes for Classification: Mean-Field Algorithms. Neural Computation , 12 (11), 2655-2684.
 Paninski, L. (2004). Maximum likelihood estimation of cascade point-process neural encoding models. Network , 15 (4), 243 X 262.
 and decoding of retinal ganglion cell responses with a probabilistic spiking model. J Neurosci , 25 (47), 11003 X 11013.
 Rasmussen, C., &amp; Williams, C.(2006). Gaussian processes for machine learning . Springer. Rust, N., Schwartz, O., Movshon, J., &amp; Simoncelli, E.(2005). Spatiotemporal Elements of Macaque V1 Receptive Fields. Neuron , 46 (6), 945 X 956.
 Seeger, M. (2005). Expectation propagation for exponential families (Tech. Rep.). University of California at Berkeley. (See www.kyb.tuebingen.mpg.de/bs/people/seeger .) Seeger, M., Steinke, F., &amp; Tsuda, K. (2007). Bayesian inference and optimal design in the sparse linear model. AI and Statistics .
 Simoncelli, E., Paninski, L., Pillow, J., &amp; Schwartz, O.(2004). Characterization of neural responses with stochastic stimuli. In M. Gazzaniga (Ed.), (Vol. 3, pp. 327 X 338). MIT Press.
 Snyder, D., &amp; Miller, M. (1991). Random point processes in time and space. Springer Texts in Electrical Engineering.
 Steveninck, R., &amp; Bialek, W. (1988). Real-Time Performance of a Movement-Sensitive Neuron in the Blowfly Visual System: Coding and Information Transfer in Short Spike Sequences. Proceed-ings of the Royal Society of London. Series B, Biological Sciences , 234 (1277), 379 X 414. Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological) , 58 (1), 267 X 288.
 Touryan, J., Lau, B., &amp; Dan, Y.(2002). Isolation of Relevant Visual Features from Random Stimuli for Cortical Complex Cells. Journal of Neuroscience , 22 (24), 10811.
 Zeck, G. M., Xiao, Q., &amp; Masland, R. H. (2005). The spatial filtering properties of local edge detectors and brisk-sustained retinal ganglion cells. Eur J Neurosci , 22 (8), 2016-26.
