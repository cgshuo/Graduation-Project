 Kernel methods are broadly established as a useful way of constructing nonlinear algorithms from linear ones, by embedding points into higher dimensional reproducing kernel Hilbert spaces (RKHSs) [9]. A generalization of this idea is to embed probability distributions into RKHSs, giving us a linear method for dealing with higher order statistics [6, 12, 14]. More specifically, suppose we are given the set P of all Borel probability measures defined on the topological space M , and the RKHS ( H , k ) of functions on M with k as its reproducing kernel (r.k.). For P  X  P , denote by P k := in H as P k  X  H . The RKHS distance between two such mappings associated with P , Q  X  P is called the maximum mean discrepancy (MMD) [6, 14], and is written We say that k is characteristic [4, 14] if the mapping P 7 X  P k is injective, in which case (1) is zero if and only if P = Q , i.e.,  X  k is a metric on P . An immediate application of the MMD is to problems of comparing distributions based on finite samples: examples include tests of homogeneity [6], independence [7], and conditional independence [4]. In this application domain, the question of whether k is characteristic is key: without this property, the algorithms can fail through inability to distinguish between particular distributions.
 Characteristic kernels are important in binary classification: The problem of distinguishing dis-tributions is strongly related to binary classification: indeed, one would expect easily distinguishable distributions to be easily classifiable . 1 The link between these two problems is especially direct in the case of the MMD: in Section 2, we show that  X  k is the negative of the optimal risk (correspond-ing to a linear loss function) associated with the Parzen window classifier [9, 11] (also called kernel classification rule [3, Chapter 10]), where the Parzen window turns out to be k . We also show that  X  k is an upper bound on the margin of a hard-margin support vector machine (SVM). The impor-tance of using characteristic RKHSs is further underlined by this link: if the property does not hold, then there exist distributions that are unclassifiable in the RKHS H . We further strengthen this by showing that characteristic kernels are necessary (and sufficient under certain conditions) to achieve Bayes risk in the kernel-based classification algorithms.
 Characterization of characteristic kernels: Given the centrality of the characteristic property to both RKHS classification and RKHS distribution testing, we should take particular care in estab-lishing which kernels satisfy this requirement. Early results in this direction include [6], where k is shown to be characteristic on compact M if it is universal in the sense of Steinwart [15, Definition 4]; and [4, 5], which address the case of non-compact M , and show that k is characteristic if and only if H + R is dense in the Banach space of p -power ( p  X  1 ) integrable functions. The conditions in both these studies can be difficult to check and interpret, however, and the restriction of the first to compact M is limiting. In the case of translation invariant kernels, [14] proved the kernel to be characteristic if and only if the support of the Fourier transform of k is the entire R d , which is a much easier condition to verify. Similar sufficient conditions are obtained by [5] for translation invariant kernels on groups and semi-groups. In Section 3, we expand the class of characteristic kernels to include kernels that may or may not be translation invariant, with the introduction of a novel criterion: strictly positive definite kernels (see Definition 3) on M are characteristic. Choice of characteristic kernels: In expanding the families of allowable characteristic kernels, we have so far neglected the question of which characteristic kernel to choose. A practitioner asking by how much two samples differ does not want to receive a blizzard of answers for every conceivable kernel and bandwidth setting, but a single measure that satisfies some  X  X easonable X  notion of dis-tance across the family of kernels considered. Thus, in Section 4, we propose a generalization of the MMD, yielding a new distance measure between P and Q defined as which is the maximal RKHS distance between P and Q over a family, K of positive definite kernels. For example, K can be the family of Gaussian kernels on R d indexed by the bandwidth parameter. This distance measure is very natural in the light of our results on binary classification (in Section 2): most directly, this corresponds to the problem of learning the kernel by minimizing the risk of the associated Parzen-based classifier. As a less direct justification, we also increase the upper bound on the margin allowed for a hard margin SVM between the samples. To apply the generalized MMD in practice, we must ensure its empirical estimator is consistent. In our main result of Section 4, we provide an empirical estimate of  X  ( P , Q ) based on finite samples, and show that many popular kernels like the Gaussian, Laplacian, and the entire Mat  X  ern class on R d yield consistent estimates of  X  ( P , Q ) . The proof is based on bounding the Rademacher chaos complexity of K , which can be understood as the U-process equivalent of Rademacher complexity [2].
 Finally, in Section 5, we provide a simple experimental demonstration that the generalized MMD can be applied in practice to the problem of homogeneity testing. Specifically, we show that when two distributions differ on particular length scales, the kernel selected by the generalized MMD is appropriate to this difference, and the resulting hypothesis test outperforms the heuristic kernel choice employed in earlier studies [6]. The proofs of the results in Sections 2-4 are provided in the supplementary material. One of the most important applications of the maximum mean discrepancy is in nonparametric hy-pothesis testing [6, 7, 4], where the characteristic property of k is required to distinguish between probability measures. In the following, we show how MMD naturally appears in binary classifica-tion, with reference to the Parzen window classifier and hard-margin SVM. This motivates the need for characteristic k to guarantee that classes arising from different distributions can be classified by kernel-based algorithms.
 To this end, let us consider the binary classification problem with X being a M -valued random variable, Y being a { X  1 , +1 } -valued random variable and the product space, M  X { X  1 , +1 } , being endowed with an induced Borel probability measure  X  . A discriminant function, f is a real valued function L : { X  1 , +1 } X  R  X  R , the goal is to choose an f that minimizes the risk associated with L , with the optimal L -risk being defined as P ( X ) :=  X  ( X | Y = +1) , Q ( X ) :=  X  ( X | Y =  X  1) ,  X  :=  X  ( M, Y = +1) . Here, P and Q represent the class-conditional distributions and  X  is the prior distribution of class +1 . Now, we present the result that relates  X  k to the optimal risk associated with the Parzen window classifier. then which is the Parzen window classifier.
 Theorem 1 shows that  X  k is the negative of the optimal L -risk (where L is the linear loss as defined in Theorem 1) associated with the Parzen window classifier. Therefore, if k is not characteristic, which means  X  k ( P , Q ) = 0 for some P 6 = Q , then R L F since 0  X   X  k ( P , Q ) =  X  R L F the maximum risk is obtained only when P = Q . This motivates the importance of characteristic kernels in binary classification. In the following, we provide another result which provides a similar motivation for the importance of characteristic kernels in binary classification, wherein we relate  X  k to the margin of a hard-margin SVM.
 a training sample drawn i.i.d. from  X  . Assuming the training sample is separable, let f svm be the solution to the program, inf {k f k H : Y i f ( X i )  X  1 ,  X  i } , where H is an RKHS with measurable and bounded k . If k is characteristic, then where P m := 1 m represents the Dirac measure at x . Theorem 2 provides a bound on the margin of hard-margin SVM in terms of MMD. (5) shows that a smaller MMD between P m and Q n enforces a smaller margin (i.e., a less smooth classifier, f svm , where smoothness is measured as k f svm k H ). We can observe that the bound in (5) may be loose if for P m 6 = Q n and therefore the margin is zero, which means even unlike distributions can become inseparable in this feature representation.
 Another justification of using characteristic kernels in kernel-based classification algorithms can be provided by studying the conditions on H for which the Bayes risk is realized for all  X  . Steinwart and Christmann [16, Corollary 5.37] have showed that under certain conditions on L , the Bayes risk is achieved for all  X  if and only if H is dense in L p ( M,  X  ) for all  X  , where  X  =  X  P + (1  X   X  ) Q . Here, L p ( M,  X  ) represents the Banach space of p -power integrable functions, where p  X  [1 ,  X  ) is dependent on the loss function, L . Denseness of H in L p ( M,  X  ) implies H + R is dense L p ( M,  X  ) , which therefore yields that k is characteristic [4, 5]. On the other hand, if constant functions are achieve the Bayes risk. As an example, it can be shown that characteristic kernels are necessary (and sufficient if constant functions are in H ) for SVMs to achieve the Bayes risk [16, Example 5.40]. Therefore, the characteristic property of k is fundamental in kernel-based classification algorithms. Having showed how characteristic kernels play a role in kernel-based classification, in the following section, we provide a novel characterization for them. Q ,  X  P , Q  X  P . The following result provides a novel characterization for characteristic kernels, which shows that strictly pd kernels are characteristic to P . An advantage with this characterization is that it holds for any arbitrary topological space M unlike the earlier characterizations where a group structure on M is assumed [14, 5]. First, we define strictly pd kernels as follows. Definition 3 (Strictly positive definite kernels) . Let M be a topological space. A measurable and bounded kernel, k is said to be strictly positive definite if and only if for all finite non-zero signed Borel measures,  X  defined on M .
 Note that the above definition is not equivalent to the usual definition of strictly pd kernels that in-volves finite sums [16, Definition 4.15]. The above definition is a generalization of integrally strictly positive definite functions [17, Section 6]: which is the strictly positive definiteness of the integral operator given by the kernel. Definition 3 is stronger than the finite sum definition as [16, Theorem 4.62] shows a kernel that is strictly pd in the finite sum sense but not in the integral sense.
 Theorem 4 (Strictly pd kernels are characteristic) . If k is strictly positive definite on M , then k is characteristic to P .
 The proof idea is to derive necessary and sufficient conditions for a kernel not to be characteristic. We show that choosing k to be strictly pd violates these conditions and k is therefore characteristic to strictly pd kernel if k is strictly pd, where f : M  X  R is a bounded continuous function. Therefore, translation-variant strictly pd kernels can be obtained by choosing k to be a translation invariant strictly pd kernel. A simple example of a translation-variant kernel that is a strictly pd kernel on which is the same result that follows from the universality of  X  k [15, Section 3, Example 1]. The following result in [10], which is based on the usual definition of strictly pd kernels, can be obtained as a corollary to Theorem 4.
 y ,  X  i, j . Suppose k is strictly positive definite. Then  X  ,  X  j  X  R \{ 0 } X  X = Y .
 Suppose we choose  X  i = 1 m ,  X  i and  X  j = 1 n ,  X  j in Corollary 5. Then and characteristic). Then, by Corollary 5, the normal vector, w to the hyperplane in H passing through the origin is zero, i.e., the mean functions coincide (and are therefore not classifiable) if and only if X = Y . The discussion so far has been related to the characteristic property of k that makes  X  k a metric on P . We have seen that this characteristic property is of prime importance both in distribution testing, and to ensure classifiability of dissimilar distributions in the RKHS. We have not yet addressed how to choose among a selection/family of characteristic kernels, given a particular pair of distributions we wish to discriminate between. We introduce one approach to this problem in the present section. Let M = R d and k  X  ( x, y ) = exp(  X   X  k x  X  y k 2 2 ) ,  X   X  R + , where  X  represents the bandwidth parameter. { k  X  :  X   X  R + } is the family of Gaussian kernels and {  X  k of MMDs indexed by the kernel parameter,  X  . Note that k  X  is characteristic for any  X   X  R ++ and therefore  X  k number that defines the distance between P and Q . The question therefore to be addressed is how to choose appropriate  X  . The choice of  X  has important implications on the statistical aspect of  X  k Note that as  X   X  0 , k  X   X  1 and as  X   X   X  , k  X   X  0 a.e., which means  X  k or  X   X   X  for all P , Q  X  P (this behavior is also exhibited by k  X  ( x, y ) = exp(  X   X  k x  X  y k 1 ) and k small or sufficiently large  X  (depending on P and Q ) makes  X  k has to be chosen appropriately in applications to effectively distinguish between P and Q . Presently, the applications involving MMD set  X  heuristically [6, 7].
 To generalize the MMD to families of kernels, we propose the following modification to  X  k , which yields a pseudometric on P , Note that  X  is the maximal RKHS distance between P and Q over a family, K of positive definite kernels. It is easy to check that if any k  X  K is characteristic, then  X  is a metric on P . Examples for K  X  := { e  X   X  X  ( x,y ) , x, y  X  M :  X   X  R + } , where  X  : M  X  M  X  R is a negative definite kernel; K all finite nonnegative Borel measures,  X   X  on R + that are not concentrated at zero, etc. Section 2 between  X  k and the Parzen window classifier. Since the Parzen window classifier depends on the kernel, k , one can propose to learn the kernel like in support vector machines [8], wherein learning the kernel in a hard-margin SVM by maximizing its margin.
 At this point, we briefly discuss the issue of normalized vs. unnormalized kernel families, K in (6). We say a translation-invariant kernel, k on R d is normalized if constant independent of the kernel parameter), where k ( x, y ) =  X  ( x  X  y ) . K is a normalized kernel family if every kernel in K is normalized. If K is not normalized, we say it is unnormalized. For example, it is easy to see that K g and K l are unnormalized kernel families. Let us consider the shown that for any k  X  , k  X   X  K n g , 0 &lt;  X  &lt;  X  &lt;  X  , we have  X  k means,  X  ( P , Q ) =  X   X  similar result also holds for the normalized inverse-quadratic kernel family, { 2 )  X  1 , x, y  X  R :  X   X  [  X  0 ,  X  ) } . These examples show that the generalized MMD definition is usually not very useful if K is a normalized kernel family. In addition,  X  0 should be chosen beforehand, which is equivalent to heuristically setting the kernel parameter in  X  k . Note that  X  0 cannot be zero because in the limiting case of  X   X  0 , the kernels approach a Dirac distribution, which means the limiting kernel is not bounded and therefore the definition of MMD in (1) does not hold. So, in this work, we consider unnormalized kernel families to render the definition of generalized MMD in (6) useful. To use  X  in statistical applications where P and Q are known only through i.i.d. samples { X i } m i =1 [6, 12] have shown that  X  k ( P m , Q n ) is a statistical consistency of  X  ( P m , Q n ) is established in the following theorem, which uses tools from U-process theory [2, Chapters 3,5]. We begin with the following definition.
 Definition 6 (Rademacher chaos) . Let G be a class of functions on M  X  M and {  X  i } n i =1 be independent Rademacher random variables, i.e., Pr (  X  i = 1) = Pr (  X  i =  X  1) = 1 2 . The homogeneous Rademacher chaos process of order two with respect to {  X  i } n i =1 is defined as ity over G is defined as We now provide the main result of the present section.
 Theorem 7 (Consistency of  X  ( P m , Q n ) ) . Let every k  X  K be measurable and bounded with  X  := where Lemma 8 (Entropy bound) . For any K as in Theorem 7 with 0  X  K , there exists a universal constant C such that where D ( k 1 , k 2 ) = 1 m covering number of K with respect to the metric D .
 Assuming K to be a VC-subgraph class, the following result, as a corollary to Lemma 8 provides VC-subgraph class.
 Definition 9 (VC-subgraph class) . The subgraph of a function g : M  X  R is the subset of M  X  R VC-subgraph class, if the collection of all subgraphs of the functions in G forms a VC-class of sets (in M  X  R ).
 The VC-index (also called the VC-dimension) of a VC-subgraph class, G is the same as the pseudo-dimension of G . See [1, Definition 11.1] for details.
 Corollary 10 ( U m ( K ; { X i } ) for VC-subgraph, K ) . Suppose K is a VC-subgraph class with V ( K ) being the VC-index. Assume K satisfies the conditions in Theorem 7 and 0  X  K . Then for some universal constants C and C 1 .
 Using (10) in (8), we have |  X  ( P m , Q n )  X   X  ( P , Q ) | = O P , Q ( nel classes, K have V ( K ) &lt;  X  . [18, Lemma 12] showed that V ( K g ) = 1 (also see [19]) and U m ( K rbf )  X  C 2 U m ( K g ) , where C 2 &lt;  X  . It can be shown that V ( K  X  ) = 1 and V ( K l ) = 1 . All these classes satisfy the conditions of Theorem 7 and Corollary 10 and therefore provide consis-tent estimates of  X  ( P , Q ) for any P , Q  X  P . Examples of kernels on R d that are covered by these classes include the Gaussian, Laplacian, inverse multiquadratics, Mat  X  ern class etc. Other choices for K that are popular in machine learning are the linear combination of kernels, K lin := { k  X  = P Lemma 7] have shown that V ( K con )  X  V ( K lin )  X  l . Therefore, instead of using a class based on a fixed, parameterized kernel, one can also use a finite linear combination of kernels to compute  X  . So far, we have presented the metric property and statistical consistency (of the empirical estimator) of  X  . Now, the question is how do we compute  X  ( P m , Q n ) in practice. To show this, in the following, we present two examples.
 Example 11. Suppose K = K g . Then,  X  ( P m , Q n ) can be written as  X  2 ( P m , Q n ) = sup The optimum  X   X  can be obtained by solving (11) and  X  ( P m , Q n ) = k P m k  X   X   X  Q n k  X   X  k H Example 12. Suppose K = K con . Then,  X  ( P m , Q n ) becomes where we have replaced k by Similar examples can be provided for other K , where  X  ( P m , Q n ) can be computed by solving a semidefinite program ( K = K lin ) or by the constrained gradient descent ( K = K l , K rbf ). Finally, while the approach in (6) to generalizing  X  k is our focus in this paper, an alternative Bayesian strategy would be to define a non-negative finite measure  X  over K , and to average  X  k over that measure, i.e.,  X  ( P , Q ) := distinguished by  X  , but not vice-versa. In this sense,  X  is stronger than  X  . One further complication with the Bayesian approach is in defining a sensible  X  over K . Note that  X  k based on k 0 ) can be obtained by defining  X  ( k ) =  X  ( k  X  k 0 ) in  X  ( P , Q ) . In this section, we present a benchmark experiment that illustrates the generalized MMD proposed in Section 4 is preferred above the single kernel MMD where the kernel parameter is set heuristically. The experimental setup is as follows.
 Q respectively. It is easy to see that q differs from p at increasing frequencies with increasing  X  . Let k ( x, y ) = exp(  X  ( x  X  y ) 2 / X  ) . Now, the goal is that given random samples drawn i.i.d. from P and Q (with  X  fixed), we would like to test H 0 : P = Q vs. H 1 : P 6 = Q . The idea is that as  X  increases, it will be harder to distinguish between P and Q for a fixed sample size. Therefore, using this setup we can verify whether the adaptive bandwidth selection achieved by  X  (as the test statistic) helps to distinguish between P and Q at higher  X  compared to  X  k with a heuristic  X  . To this end, using if T mn  X  c mn , and H 1 otherwise. The problem therefore reduces to finding c mn . c mn is determined as the (1  X   X  ) quantile of the asymptotic distribution of T mn under H 0 , which therefore fixes the type-I error (the probability of rejecting H 0 when it is true) to  X  . The consistency of this test under  X  k (for any fixed  X  ) is proved in [6]. A similar result can be shown for  X  under some conditions on K . We skip the details here.
 In our experiments, we set m = n = 1000 ,  X  2 p = 10 and draw two sets of independent random samples from Q . The distribution of T mn is estimated by bootstrapping on these samples ( 250 boot-strap iterations are performed) and the associated 95 th quantile (we choose  X  = 0 . 05 ) is computed. Since the performance of the test is judged by its type-II error (the probability of accepting H 0 when H 1 is true), we draw a random sample, one each from P and Q and test whether P = Q . This process is repeated 300 times, and estimates of type-I and type-II errors are obtained for both  X  and  X  k . 14 different values for  X  are considered on a logarithmic scale of base 2 with exponents choice. 5 different choices for  X  are considered: ( 1 2 , 3 4 , 1 , 5 4 , 3 2 ) . Figure 1: (a) Type-I and Type-II errors (in % ) for  X  for varying  X  . (b,c) Type-I and type-II error (in which shows that its associated type-II error is very large at large  X  . (d) Box plot of log  X  grouped by  X  , where  X  is selected by  X  . (e) Box plot of the median distance between points (which is also a choice for  X  ), grouped by  X  . Refer to Section 5 for details.
 Figure 1(a) shows the estimated type-I and type-II errors using  X  as the test statistic for varying  X  . Note that the type-I error is close to its design value of 5% , while the type-II error is zero for all  X  , which means  X  distinguishes between P and Q for all perturbations. Figures 1(b,c) show the estimates of type-I and type-II errors using  X  k as the test statistic for different  X  and  X  . Figure 1(d) shows the box plot for log  X  , grouped by  X  , where  X  is the bandwidth selected by  X  . Figure 1(e) shows the box plot of the median distance between points (which is also a choice for  X  ), grouped by  X  . From Figures 1(c) and (e), it is easy to see that the median heuristic exhibits high type-II error for  X  = 3 2 , while  X  exhibits zero type-II error (from Figure 1(a)). Figure 1(c) also shows that heuristic choices of  X  can result in high type-II errors. It is intuitive to note that as  X  increases, (which means the characteristic function of Q differs from that of P at higher frequencies), a smaller  X  is needed to detect these changes. The advantage of using  X  is that it selects  X  in a distribution-dependent fashion and its behavior in the box plot shown in Figure 1(d) matches with the previously mentioned intuition about the behavior of  X  with respect to  X  . These results demonstrate the validity of using  X  as a distance measure in applications. In this work, we have shown how MMD appears in binary classification, and thus that characteristic kernels are important in kernel-based classification algorithms. We have broadened the class of characteristic RKHSs to include those induced by strictly positive definite kernels (with particular application to kernels on non-compact domains, and/or kernels that are not translation invariant). We have further provided a convergent generalization of MMD over families of kernel functions, which becomes necessary even in considering relatively simple families of kernels (such as the Gaussian kernels parameterized by their bandwidth). The usefulness of the generalized MMD is illustrated experimentally with a two-sample testing problem.
 Acknowledgments The authors thank anonymous reviewers for their constructive comments and especially the re-viewer who pointed out the connection between characteristic kernels and the achievability of Bayes risk. B. K. S. was supported by the MPI for Biological Cybernetics, National Science Founda-tion (grant DMS-MSPA 0625409), the Fair Isaac Corporation and the University of California MI-CRO program. A. G. was supported by grants DARPA IPTO FA8750-09-1-0141, ONR MURI N000140710747, and ARO MURI W911NF0810242.
