 We pres ent and em pirically analy ze a m achine-learning approach for detecting intrusions on indi vidual computers. Our Winnow-based algorithm continually monito rs user and sy stem behavior, recording such properties as the number of by tes transferred over the last 10 seconds, the programs that currently are running, and the load on the CPU. In all, hundreds of measurements are made and analy zed each second. Using this data, our algorithm creates a model that repres ents each particular com puter X  X  range of normal behavior. Parameters th at determ ine when an alarm should be raised, due to abnormal activity , are set on a per-computer basis, based on an anal ysis of training data. A major issue in intrusion-detection sy stem s is the need for very low false-alarm rates. Our empirical results suggest that it is possible to obtain high intrusion-detection ra tes (95%) and low fals e-alarm rates (less than one per day per co mputer), without  X  X tealing X  too many CP U cy cles (les s than 1%). W e als o report which system meas urem ents are the m ost valuable in term s of detecting intrusions. A surprisingly large number of different meas urem ents prove significantly useful. D.4.6 [Secu rity an d Protection ], I.2.6 [Artificial Intelligence] : Learning Algorithms, Experimentation, Security Intrusion detection, anomaly det ection, m achine learning, us er modeling, Windows 2000, feature selection, Winnow algorithm In an increas ingly com puterized a nd networked world, it is crucial to develop defenses against ma licious activity in inform ation systems. One promising appro ach is to develop computer algorithm s that detect when som eone is inappropriately intruding on the computer of another person. However, intrusion detection is a difficult problem to solve [3 ]. Sy stem performance cannot be adversely affected, false positiv es must be m inim ized, and intrusions must be caught (i.e., fals e negatives must be very low). The current state of the art in intrusion-detection systems is not good; false positives are m uch too high and successful detection is unfortunately too rare. We report on an approach where we have made significant advances toward creating an intrusion-detection system that requires few CP U cy cles (les s than 1%), produces few fals e alarm s (les s than one per da y), and detects most intrusions quickly (about 95% within five minutes). Intrusion-detection sy stems (IDS X  X ) can either (a) look for known attack patterns or (b) be  X  X daptive software X  that is smart enough to monitor and learn how the sy stem is supposed to work under normal operation versus how it works when misuse is occurring [9] . W e addres s approach (b) in this article. S pecifically , we are em pirically determ ining which sets of fine-grained system measurements are the most effective at distinguishing usage by the assigned user of a given com puter from misusage by others, who may well be  X  X nsiders X  [3; 11] within an organization. We have created a prototy pe anom aly -detection s ystem that computer running Windows 2000. Significant deviations from normal behavior indicate that an intrusion is likely occurring. For exam ple, if the probability that a specific com puter receives 10 Mbytes/sec during evenings is m eas ured to be very low, then when our monitoring program detects such a high transfer rate during evening hours, it can suggest that an intrusion may be occurring. The algorithm we have developed measures over two-hundred Windows 2000 properties every second, and creates about 1500  X  X eatures  X  out of them . During a m achine-learning  X  training X  phase, it learns how to weight these 1500 features in order to accurately characterize the particular behavior of each us er  X  each user gets his or her own set of feature weights. Following not it seem s like an intrusion is occurring. The weighted votes  X  X or X  and  X  X gainst X  an intrusion are compared, and if there is enough evidence, an alarm is raised. (Section 2 presents additional details about our IDS algorithm that are being glossed over at this point.) This ability to create statistical models of individual computer X  X  normal usage means that each computer X  X  unique characteristics serve a protective role. Similar to how each person X  X  antibodies can distinguish one X  X  own cells from invading organisms, these statistical-profile programs can, as they gather data during the normal operation of a computer, learn to distinguish  X  X elf X  behavior from  X  X oreign X  behavior. For instance, some people use Notepad to view small text files, while others prefer WordPad. Should someone leave their co mputer unattended and someone else try to inappropriately access their files, the individual differences between people X  X  com puter usage will mean that our statistical-modeling program will quickly recognize this illegal access. We evaluate the ability to detect computer misuse by collecting data from multiple employees of Shavlik Technologies, creating user profiles by analyzing  X  X raining X  subsets of this data, and then experimentally judging the accuracy of our approach by predicting whether or not data in  X  X esting sets X  is from the normal user of a given computer or from an intruder. The key hypothesis investigated is whether or not cr eating statistical models of user behavior can be used to accurately detect computer misuse. We focus our algorithmic development on methods that produce very low false-alarm rates, since a major reason system administrators ignore IDS systems is that they produce too many false alarms. Our empirical results suggest that it is possible to detect about 95% of the intrusions with less than one false alarm per (8 hr) day per user. It should be noted, though, that these results are based on our model of an  X  X nsider intruder, X  which assumes that when Insider Y uses User X  X  X  computer, that Y is not able to alter his or her behavior to explicitly mimic X  X  X  normal behavior. The training phase our approach can be computationally intensive due to some parameter tuning, but th is parameter tuning could be done on a central server or during the evenings when users are not at work. The CPU load of our IDS is negligible during ordinary operation; it requires less than 1% of the CPU cycles of a standard personal computer. Our approach is also robust to the fact that users X  normal behavior constantly changes over time. Our approach can also be used to detect abnormal behavior in computers operating as specialized HTTP, FTP, or email servers. Similarly, these techniques could be used to monitor, say, the behavior of autonomous intelligent software agents in order to detect rogue agents whose behavi or is not consistent with the normal range of agent behavior for a given family of tasks. However, the experiments reported herein only involve computers While we employ the word  X  X ser X  throughout this article, the reader should keep in mind that our approach applies equally well to the monitoring of servers and autonomous intelligent agents. All that would be needed to apply our approach to a different scenario would be to define a set of potentially distinctive properties to measure and to write code that measured these properties periodically. Previous empirical studies have i nvestigated the value of creating intrusion-detection systems by m onitoring properties of computer systems, an idea that goes back at least 20 years [2]. However, prior work has focused on Unix systems, whereas over 90% of the world X  X  computers run some vari ant of Microsoft Windows. In addition, prior studies have not looked at as large a collection of system measurements as we use. For example, Warrender et al. [13], Ghosh et al. [4], and Lane and Brodley [5] only look at Unix system calls, whereas Lee et al. [7] only look at audit data, mainly from the TCP program. Lazarevic et al. [6] provide a summary of some of the recent research on the application of data mining to network-based anomaly detection. In Section 2 we describe th e algorithm we developed that analyzes the Windows 2000 properties that we measure each second, creating a profile of normal usage for each user. Section 3 presents and discusses empirical studies that evaluate the strengths and weaknesses of th is algorithm, stressing it along various dimensions such as the am ount of data used for training. This section also lists which Windows 2000 properties end up with the highest weights in our weighted-voting scheme. Section 4 describes possible future follow-up research tasks, and Section 5 concludes this article. In this section we describe the algorithm that we developed. Our key finding is that a machine-learning algorithm called Winnow [8], a weighted-majority type of algorithm, works very well as the core component of an IDS. This algorithm operates by taking weighted votes from a pool of individual prediction methods, con tinually adjusting these weights in order to improve accuracy. In our case, the individual predictors are the Windows 2000 properties that we measure, where we look at the probability of obtaining the current value and comparing it to a threshold. That is, each individual measurement suggests that an intrusion may be occurring if: Each property we measure votes as to whether or not an intrusion is currently occurring. When the weighted sum of votes leads to the wrong prediction (intrusion vs. no intrusion), then the weights of all those properties that voted incorrectly are halved. Exponentially quickly, those properties that are not informative end up with very small weights. Besides leading to a highly accurate IDS, the Winnow algorith m allows us to see which Windows 2000 properties are the most useful for intrusion detection, namely those proper ties with the highest weights following training (as we shall see, when viewed across several users, a surprisingly high number of properties end up with high weights). Actually, rather than using Equation 1, we found [12] it slightly better to compare probabilities relative to those in general (i.e., computed over all our experimental subjects) and use: Prob(value | user X) / P(value | general public) &lt; r [Eq. 2] An alarm is sounded if this ratio is less than some constant, r . This way we look for relatively rare events for a specific user rather than rare events in gene ral (it may well make sense to use both Equations. 1 and 2, and in the one experiment [12] where we did so -using W = 1200sec in Table 1 X  X  algorithm -we increased the detection rate from 94.7% to 97.3% while still meeting our target of less than one false-alarm per day).
 The idea behind using the above ratio is that it focuses on feature values that are rare for this us er relative to their probability of occurrence in the general population. For example, feature values that are rare for User X but also occur rarely across the general population may not produce low ratios, while feature values that ratio distinguishes between  X  X are for User X and for other computer users as well X  and  X  X are for User X but not rare in general. X  We estimate prob( feature=value for the general population) by simply pooling all the training data from our experimental subjects, and then creating a disc rete probability distribution using ten bins, using the technique expl ained below. Doing this in a fielded system would be reasonabl e, since in our IDS design one requires a pool of users for th e training and tuning phases. Because our experimental setup only involves measurements from normal computer users, the use of our ratio of probabilities makes sense in our experiments, si nce it defines  X  X are for User X  X  relative to the baseline of other computer users operating normally. However, it is likely that the behavior of intruders, even insiders working at the same facility, may be quite different from normal computer usage (unfortunately we do not yet have such data to analyze). For example, an intruder might do something that is rare in gene ral, and hence Equation 2 above might not produce a value less than the setting for the threshold r . Before presenting our algorithm that calls as a subroutine the Winnow algorithm, we discuss how we make  X  X eatures X  out of the over two-hundred Windows properties that we measure. Technically, it is these 1500 or so features that do the weighted voting. Space limitations preclude describing here all of the 200+ properties measured. Appendix A of our full project report [12] lists and briefly describes all of the Windows 2000 properties that we measure; some relate to network activity, some to file accesses, and others to the CPU load of the programs currently running. Most come from Windows X  perfmon ( X  X erformance monitor X ) program. Several features we measure appear in Tables 3 and 4 of this article. For each of these measurements, we also derive additiona l measurements: Actual Value Measured Average of the Previous 10 Values Average of the Previous 100 Values Difference between Current Value and Previous Value Difference between Current Value and Average of Last 10 Difference between Current Value and Ave of Last 100 Difference between Average s of Previous 10 and Previous 100 As we discovered in our experime nts, these additional  X  X erived X  features play an important role; without them intrusion-detection rates are significantly lower. For the remainder of this article, we will use the term  X  X eature X  to refe r the combination of a measured Windows 2000 property and one of the seven above transformations. In other words, each Windows 2000 property that we measure produces seven features. (The first item in the above list is not actually a derived feature; it is the  X  X aw X  completeness.) Table 1 contains our main algorithm. We take a machine-learning [10] approach to creating an IDS, and as is typical we divide the learning process into three pha ses. First, we use some training data to create a model; here is wher e we make use of the Winnow algorithm (see Table 2), which we further explain below. Next, we use some more data, the tuning set , to  X  X une X  some additional parameters in our IDS. Finally, we evaluate how well our learned IDS works be measuring its performance on some testing data. We repeat this process for multiple users and report the average test-set performance in our experiments. The Windows 2000 properties that we measure are continuous-valued, and in Step 1b of Table 1 we first decide how to discretize each measurement into 10 bins; we then use these bins to create a discrete probability distribution for the values for this feature. Importantly, we do this discretization separately for each user, since this way we can accurately approximate each user X  X  probability distribution with our 10 bins. (We did not experiment with values other than 10 for th e number of bins. We chose 10 arbitrarily, though it does make sense that this number be small to reduce storage demands and to  X  X mooth X  our m easurements.) We always place the value 0.0 in its own bin, since it occurs so frequently. We then choose the  X  X ut points X  that define the remaining bins by fitting a sample of the measurements produced by each user to each of several standard probability distributions: uniform, Gaussian, and Erlang (for k ranging from 1 to 100). When k = 1 the Erlang is equivalent to the better known ( X  X ecaying X ) Exponential distribution, and as k increases the distribution looks more and more lik e a Gaussian. We then select the probability distribution that best fits the sample data (i.e., has the lowest root-mean-squared error), and create our 10 bins as follows:  X  For the uniform probability distribution, we uniformly divide  X  For the Gaussian probability distribution, we place the  X  For the Exponential probability distribution, we put half the  X  For the Erlang probability distribution, we execute a We did not investigate alternate design choices in our discretization process; we developed the above approach and then used it unchanged during our subsequent learning-algorithm development and evaluation. ____________________________________________________ Step 1: Initial Training Step 2: Parameter Tuning Step 3: Continual Operation ____________________________________________________ Most of our features turned out to be best modeled by Gaussians, with the Exponential distribution being the second most common selection. One final point a bout converting to a discrete probability distribution needs to be mentioned: for those Windows 2000 measurements that vary over orders of magnitude (e. g., bytes sent per second); we use the log of their values. After we have discretized our features, we simply count how often in the training data did a feature value fall into a given bin, thereby producing a probability distribution (after normalizing by the total number of counts). Following standard practice, we initialize all bins with a count of 1; this ensures that we will never estimate from our finite samples a probability of zero for any bin. We are now able to estimate the Prob(feature = measured value) that was mentioned earlier in Equations 1 and 2. ____________________________________________________ Step 1: Initialize User X  X  X  weights on each measured feature ( wgt f ) to 1. Step 2: For each training example do: ____________________________________________________ We next turn to discuss using these probabilities to learn models for distinguishing the normal user of a given computer from an intruder. Ideally we would use training data where some User X provided the examples of normal (i. e., non-intrusion) data and we had another sample of data measured during a wide range of intrusions on this user X  X  comput er. However, we do not have such data (this is a problem that plagues IDS research in general), and s o we us e what is a standard approach, nam ely we collect data from several users (in our cas e, 10), and we then simulate intrusions by replay ing User Y X  X  m eas urem ents on Us er X  X  X  com puter. W e say that a false alarm occurs when Us er Y  X  X  recent measurements are viewed as anomalous -th at is, suggestive of an intrusion -when replay ed on his or her own computer. A detected intrusion occurs when we view Us er Y  X  X  m eas urem ents as being anomalous when evaluated using X  X  X  feature discretization and feature weighting. (Notice that we need to us e X  X  X  discretization, com puter.) Figure 1 abstractly illustrates how we define false alarm s and detected intrus ions in our experimental setting. As m entioned, we use Table 2 X  s version of Littlestone X  X  Winnow algorithm [8] to choose weights on th e features we measure. This algorithm is quite sim ple, y et has im pressive theoretical properties and practical succes s on real-world tas ks, es pecially thos e that task. As a lready disc usse d, th is algorithm sums weighted votes  X  X or X  and  X  against X  the possibility that an intrusion is currently wrong, then all those features that voted for the wrong choice have their weights halved. We perform the Winnow algorithm for half drawn from this user X  X  measured behavior (the  X  against an intrusion X  examples) and half randomly drawn from some other user in the experiment (the  X  for an intrus ion X  exam ples ). In order to raise an alarm after the training phase (Step 1 in Table 1) has set the feature weights, our algorithm does not simply use the current weighted vote. Ins tead, the current weighted vote can rais e what we call a mini alarm , and we require that there be at actual alarm . In other words , our intrusion detector works as follows (Steps 2b and 2c in Table 1): As will be seen in Section 3, W needs to be on the order of 100 to get good detection rates with few false alarms. We choose the settings for our param eters on a per-user basis by evaluating performance on a set of tuning data  X  see Step 2 of Table 1. One significant advantage of a data-driven approach like ours is that we do not have to pre-select parameter values. Ins tead, the learning algorithm selects for each us er his or her personal set of parameter values, based on the performance of these parameters on a substantia l sample of  X  X uning set X  data. The only computationally demanding portion of our algorithm is the parameter-tuning phase, wh ich depends on how many param eter com binations are cons idered and on how much tuning data each com bination is evaluated. In a fielded s ystem , it m ight make sense to do this step on a central server or during the evenings. The other tasks of measuring features, computing weighted sums, and using Winnow to adjust weights can all be done very rapidly . Outside of the parameter tuning, Table 1 X  X  algorithm requires less than 1% of a desktop computer X  X  CPU cycles . Notice that even during the testing phase (e. g., Step 3 in Table 1), we find it necessary to still execute the Winnow algorithm , to adjus t the weights on the features after our algorithm decides whether or not an intrusion occurred. If we do not do this, we get too many fals e alarm s when the us er X  X  behavior switches, and the intrusion-detection rates drasti cally drops to 20% from about 95%. On the other hand continually adjusting weights means that if we miss an intrusion we will start learning the behavior of the intruder, which is a weakness of our approach (and a weakness of statistics-based approaches for in trusion detection in general). This also means that the empirical results reported in the next section should properly be interpreted as estimating the probability that we will detect an intruder after his or her first W seconds of activity. A subject for future work is to empirically evaluate how likely our approach will detect an intruder in the second (and successive) W seconds of activity , given we did not detect the intruder in the first W seconds. On the other hand, the fact that we continually are adjus ting the weights means that after alarm , our algorithm will adapt to the change in the user X  X  behavior. Obvious ly there is a delicate balance between adapting quickly to changes in the legitim ate user X  X  behavior, and thus reducing false alarms, and adapting too quickly to the activity of an intruder and thus thinking the intruder X  X  behavior is simply a change in the behavior of the normal user of the given computer and thereby users X  behavior is wide rangi ng and changing over time. The more cons istent a us er X  X  behavior is , and the m ore accurately we can capture his or her idiosy ncrasies, the better our approach will work. This section reports some expe rimental evaluation of our IDS algorithm. Additional experime nts are reported in detail in Shavlik and Shavlik [12] , with som e of their results m entioned in this article. We collected about 8 GB of data from 16 employ ees of Shavlik Technologies who volunteered to be experimental subjects. We only collected data between 9am and 5pm on weekday s. Of these 16 experimental subj ects, we use 10 during training (Steps 1 and 2 of Table 1); for each one, we train our IDS to recognize the differences in behavior of that user from the other 9 users . We call thes e 10 us ers  X  insiders  X  and view them as members of a small group of co-workers. The remaining 6 subjects, for whom we have a total of about 50 work day s of meas urem ents , serve as simulated  X  external X  intruders , i.e., us ers whose computer-usage behavior is not seen during training (including computing the denomin ator in Eq. 2)  X  these 6 experimental subjects are only used during the testing phase (Step 3 of Table 1) and are never used during the training and tuning phases. Hence, one expects that these 6  X  X utsiders X  would be harder to recognize as intruders on User X  X  X  com puter s ince their behavior is not observed while the IDS X  X  are still learning. Figure 2 shows, as a function of W (see Table 1) the detection and fals e-alarm rates for the s cenario where the training las ts 15 work day s (432,000 seconds), and the t uning, and testing periods each last 10 work day s (288,000 seconds). The train, tune, and test sets are temporally disjoint from one a nother. This scenario involves a five-week-long training process, but as presented in Shavlik and Shavlik [12] shorter training periods produce results nearly as good. these 10 experimental subjects is evaluated using the other 9 subjects as  X  X nsider intruders X  and the above-described 6  X  X utsider intruders, X  and the 10 resulting sets of false-alarm and detection rates are averaged to produce Figure 2. During the tuning phase of Table 2, the s pecified fals e-alarm rate of S tep 2e was set to 0; such a extrem e fals e-alarm rate could alway s be produced on the tuning set, though due to the fact we are able to explicitly fit our not result during the testing rate (as one expects). Over fitting (getting m uch higher accuracies on the tuning data than on the testing data due to having too ma ny  X  X egrees of freedom X  during the tuning phase) is arguably the key issue in machine learning and is central to adaptive IDS  X  X . As can be seen in Figure 2, for a wide range of window widths (from 1 to 20 minutes), the false-alarm rates are very low  X  alway s less than one per eight-hour work day per user -and the intrusion-detection rates are impressively high, nearly 95%. Interestingly , the detection rate fo r  X  X utsiders, X  whose behavior is never seen during training, is approximately the same as for  X  X nsiders. X  This suggests that our learning algorithm is doing a good job of learning what is characteristic about User X , rather than just exploiting idiosy ncratic differences between User X and the other nine  X  X nsiders. X  Based on Figure 2, 300 seconds is a reasonable setting for W in a fielded sy stem, and in most of th e subsequent experiments in this section us e that value. (It should be noted that going down to W = 60 sec in Figure 2 is not com pletely appropriate. S ome of the features we us e are averages of a given measurement over the last 100 seconds, as not use any exam ples where the us er X  X  com puter has not been turned on for at least 100 seconds. Hence, when we replay a 60-second window of activity from User Y on User X  X  X  com puter, there is some  X  X eakage X  of Us er Y  X  X  data going back 100 seconds. In a fielded sy stem, 40 seconds worth of the data would actually be from User X and 60 seconds from User Y . However, our experimental setup does not curre ntly support such  X  X ixing X  of user behavior. Should a fielded sy stem wish to use W =60 s ec, a simple solution would be to average over the last 60 seconds, rather than the last 100 seconds as done in our experiments. We do not expect the im pact of s uch a change to be s ignificant. The data point for W = 10 sec in Figure 2 only uses features that involve no more than the last 10 seconds of meas urements, as a reference point  X  the issue of using less or more than the last 100 seconds of measurements is visited in more depth in the next section.) ___________________________________________________ ___________________________________________________ One potentially confusing technical point needs to be clarified at this point. In an eight-hour work day , there are 480 sixty -second-wide, non-overlapping windows (i. e., W = 60) but only 48 six-hundred-second-wide ( W = 600) ones. So one false alarm per day for W = 60 sec corresponds to a false-alarm rate of 0.2%, whereas for W = 600 sec a false-alarm rate of 2.1% produces one false-alarm per day on average. The (lower) dotted line in F igure 2 shows the fals e-alarm rate that produces one fals e alarm per day per user. Although it cannot be seen in Figure 2, as W increas es call every second leads to too many false alarms [12] , so we use non-overlapping windows. Conversely , as W increas es an intruder is able to use someone else X  X  computer longer before being detected. To produce Figure 2 X  X  results, Table 2 X  X  tuning step considered 11 possible settings for threshold mini (0.8, 0.85, 0.90, 0.95, 0.97, 1.0, 1.03, 1.05, 1. 1, 1.15, and 1.2) and 26 for threshold 0.25, 0.5, 0.75, 0.1, 0. 125, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99, and 1.0), that is 11x26=286 different combinations of these two parameters. We did not experiment with different choices for the particular values and number of the candidate parameter settings, except we found it necessary to restrict threshold mini cas es in F igure 2 where W = 10 sec and W = 60 sec. Table 3 shows the highest-weighted features at the end of Figure 2 X  X  experiment, where the weights are averaged over all ten of our experimental subjects a nd over those values for W &gt; 10 used to create F igure 2; for each experim ental subject and setting for W , we normalize the weights so that they sum to 1, thus insuring that each configuration contributes equally. Remember that the weights are always changing, so th is table should be viewed as a representation  X  X napshot. X  (Appendix A of Shavlik and Shavlik [12] contains additional explanations of several of these features). ____________________________________________________ Table 3. Features with the 25 Highest Weights Averaged Across the Experiments that Produced Figure 2. Print Jobs, Average of Previous 100 Values (ranked #1) Print Jobs, Average of Previous 10 Values System Driver Total Bytes, Actual Value Measured Logon Total, Actual Value Measured Print Jobs, Actual Value Measured LSASS: Working Set, Average of Previous 100 Values Number of Semaphores, Average of Previous 100 Values Calc: Elapsed Time, Number of Semaphores, Actual Value Measured LSASS: Working Set, Average of Previous 10 Values CMD: Handle Count, CMD: Handle Count, Average of Previous 10 Values Write Bytes Cache/sec, Excel: Working Set, Number of Semaphores, Average of Previous 10 Values CMD: % Processor Time, LSASS: Working Set, Actual Value Measured System Driver Total Bytes, Average of Previous 100 Values CMD: % Processor Time, CMD: % Processor Time, System Driver Resident Bytes, Actual Value Measured Excel: Handle Count, Average of Previous 10 Values Errors Access Permissions, File Write Operations/sec, Average of Previous 100 Values System Driver Resident Bytes, Average of Previous 10 Values ____________________________________________________ ____________________________________________________ Table 4. The 25 Measurements with the Highest Number of Occurrences in the Top 10 Weights, Including Ties, in the Experiments that Produced Figure 2 (the numbers in parentheses are the percentages of Top 10 appearances) ____________________________________________________ Observe that a wide range of features appear in Table 3: some relate to network traffic, some measure file accesses, others refer to which programs are being used, while others relate to the for some features their average values over 100 seconds are important, whereas for others thei r instantaneous values matter, and for still others what is important is the change in the feature X  X  value. A weakness of Table 3 is that a measured Windows 2000 property that is important for only one or two subjects might not have a very high average weight. Table 4 provides a different way to see which features play important ro les. To produce this table we count how often each m easured property appears in the Top 10 weights (including ties, which ar e common) following training. Surprisingly , over half of the Windows 2000 properties we measure appear at least once in some Top 10 list! This supports our thesis that one should mon itor a large number of sy stem properties in order to best create a behavioral model that is well tailored to each individual com puter us er. Our project X  X  final report [12] display s longer and additional lists of the highest-weighted features, including those for one specific user. Most of the  X  derived X  calculations (s ee S ection 2.1) are used regularly in the highly weighted features , with the exception of  X  Difference from Previous Value , X  which appears in the Top 50 weighted features only about 1/20 th as often as the others., pres um ably becaus e it is too nois y of an estim ate and needs to be smoothed.  X  Difference between Current and Average of Last 10  X  is the m ost us ed, but the difference between the most used and the 6 -most used is only a factor of two. Tables 3 and 4 s how that the features that us e the las t N measurements of a Windows 2000 property play an important role. Figure 3 illustrates the perform ance of Table 1 X  X  algorithm when we restrict features to use at most the last 1, 10, 100, or 1000 measurements, respectively , of the Window 2000 properties that we m onitor. The Y -axis is the test-set detection rate and in all cas es the fals e-alarm rate m eets our goal of no m ore than one per 300 seconds; 15 day s of training data , 3 of tuning, and 3 of testing are us ed for each experim ental s ubject. Figure 3 s hows that there is an advantage in cons idering features that have longer  X  X istories. X  However, the cost of a longer his tory is that m ore data needs to be collected to define a feature value. That is, if histories can go back as far as 1000 seconds (a little over 15 m inutes), then it will take 1000 seconds after an intrusion until all of the feature values are due solely to the intruder X  X  behavior. It appears that lim iting features to at m ost the last 100 seconds of measur ements is a good choice. So far we have reported results average over our pool of 10 individual experimental subjects. Table 5 reports how often User Y was not detected when  X  X ntruding X  on User X  X  X  com puter. F or exam ple, the cell &lt;row=Use r4, column=Use r1&gt; say s that the probability of detection is 0.86 when User 1 operates on User 4 X  X  computer for 1200 seconds. (The rightmost column is the detection rate when outs iders operate on each ins ider X  X  com puter.) of 1200-sec intrusions do not sound alarms), one might expect that most of the individual pene tration rates would range from, say, 2% to 10%. However, the res ults are m uch m ore skewed. In most cas es, all (or nearly all) the attem pted intrusions are detected  X  the majority of cells in Table 5 contain 0 X  X  (in fact we report  X  X enetration X  rates rather than detection rates in this table becaus e otherwise all of the 100% X  X  woul d be visually overwhelming). But in s everal cas es a us er is frequently not detected when operating on another user X  X  computer. system one could run experime nts like these on some group of users, and then identify for which ones their computer behavior is sufficiently distinctive that Table 1 X  X  algorithm provides them effective protection. ____________________________________________________ Tab le 5. Percen tage (% ) of Times th at Us er Y Succes sfully Intr ude d on Use r X  X  X  Mach ine (using W = 1200 sec). The colu mn s ran ge over Y an d th e row s over X . Th e righ tmos t column (O) reports the rate of suc cessful intr usions by the se t of six outsiders. Cells w ith values less than 0.5% are left blank.
 Y X 1 7 23 41 15 2 5 1 35 2 3 26 5 4 14 5 2 4 4 6 1 20 7 45 75 2 8 4 9 3 2 10 9 60 16 1 43 6 25 3 meas urem ents in our cas e) are conditionally independent of each other given the category , and estim ates the probability of obtaining the current s et of m eas urem ents given each of the possible categories (intrusion versus normal behavior in our case). We applied the Na X ve Bay es algorithm in the sam e experim ental setup as us ed to evaluate Table 1 X  X  algorithm. However, the best results we have been able to obtain (for W = 1200 seconds) are a 59.2% detection rate with an average of 2.0 false alarms per day per user, which compares poorly to Table 1 X  X  algorithm X  X  results, in the identical s cenario, of a 93. 6% detection with an average of 0.3 false alarms per day per user. (In fact, we started out this project using the Na X ve Bayes algorithm, and then switched to our Winnow-based approach when we realized that Na X ve Bayes X  independence assumption was too se verely violated for us to create an effective anomaly detector.) We discuss a few possible extensions to the work reported above that have not yet been fully discussed. An obvious extension is to obtain and analyze data from a larger number of users, as well as data from a collection of server machines. And of course it would be greatly beneficial to have data gathered during actual intrusions, rather than simulati ng them by replaying one user X  X  measurements on another use r X  X  computer. Among other advantages, having data from a larger pool of experimental subjects would allow  X  X caling up X  issues to be addressed, statistically justified confiden ce intervals on results to be produced, and parameters to be better tuned (including many for which we have  X  X ard-wired in X  values in our current experiments). When we apply the Winnow algorithm during the training phase (Step 1 in Table 1), we get remarkable accuracies. For example, out of 3,000,000 seconds of exampl es (half that should be called an intrusion and half that shoul d not), we consistently obtain numbers on the order of only 150 missed intrusions and 25 false alarms, and that is from starting with all features weighted equally. Clearly the Winnow algorithm can quickly pick out what is characteristic about each user a nd can quickly adjust to changes in the user X  X  behavior. In fact , this rapid adaptation is also somewhat of a curse (as previously discussed in Section 2), since an intruder who is not immediatel y detected may soon be seen as the normal user of a given computer. This is why we look for N mini-alarms in the last W seconds before either sounding an alarm or calling the recent measurements normal and then applying Winnow to these measur ements; our assumption is that when the normal user changes behavior, only a few mini-alarms will occur, whereas for intruders the number of mini-alarms produced will exceed N . Nevertheless, we still feel that we are not close to fully exploiting the power of the Winnow algorithm on the intrusion-detection task. With more tinke ring and algorithmic variations, it seems possible to get closer to 99% detection rates with very few false alarms. In Section 2 X  X  Winnow-based algorithm we estimate the probability of the current value for a feature and then make a simple  X  X es-no X  call (see Eq. 1 and 2), regardless of how close the estimated probability is to the thre shold. However, it seems that an extremely low probability should have more impact than a value just below the threshold. In the often-successful Na X ve Bayes algorithm, for example, actual probabilities appear in the calculations, and it seems worthw hile to consider ways of somehow combining the weights of Winnow and the actual (rather than thresholded) probabilities. In our main algorithm (Table 1) we did not  X  X ondition X  the probabilities of any of the features we measured. Doing so might lead to more informative probabilities and, hence, better performance. For example, instead of simply considering Prob(File Write Operations/sec) , it might be more valuable to use Prob(File Write Operations/sec | MS Word is using most of the recent cycles) , where  X  X  X  is read  X  X iven. X  Similarly, one could use the Winnow algorithm to select good pairs of features. However these alternatives might be too computationally expensive unless domain expertise was somehow used to choose only a small subset of all the possible combinations. In none of the experiments of this article did we mix the behavior of the normal user of a computer and an intruder, though that is sets of Windows 2000 measurem ents in a semantically meaningful way (e. g., one cannot simply add the two values for each feature or, for example, CPU utilizations of 150% might result). However, with some thought it seems possible to devise a plausible way to mix normal and in truder behavior. An alternate approach would be to run our data-gathering software while someone is trying to intrude on a computer that is simultaneously being used by another person. In the results reported in Section 3, we tune parameters to get zero false alarms on the tuning data, and we found that on the testing data we were able to meet our goal of less than one false alarm per user per day (often we obtaine d test-set results more like one per week). If one wanted to obtain even fewer false alarms, then some new techniques would be needed, since our approach already is getting no false alarms on the tuning set. One solution we have explored is to tune th e parameters to zero false alarms, and then to increase the stringency of our parameters -e. g., require 120% of the number of mini-alarms as needed to get zero tuning-set false alarms. More evaluation of this and similar approaches is needed. We have also collected Windows 2000 event-log data from our set of 16 Shavlik Technologies employees. However we decided not to use that data in our e xperiments since it seems one would need to be using data from people actually trying to intrude on someone else X  X  computer for inte resting event-log data to be generated. Our approach for simulating  X  X ntruders X  does not result in then generation of meaningful event-log entries like failed logins. Another type of measurement that seems promising to monitor are the specific IP addresses involved in traffic to and from a given computer. Possibly interesting variables to compute include the number of different IP addr esses visited in the last N seconds, the number of  X  X irst time visited X  IP addresses in the last N seconds, and differences between incomi ng and outgoing IP addresses. A final possible future research topic is to extend the approaches in this article to local networks of computers, where the statistics of behavior across the set of computers is monitored. Some intrusion attempts that might not seem anomalous on any one computer may appear highly anomalous when looking at the behavior of a set of machines. Our approach to creating an effective intrusion-detection system (IDS) is to continually gather and analyze hundreds of fine-grained measurements about Wi ndows 2000. The hypothesis that we successfully tested is that a properly (and automatically) chosen set of measurements can provide a  X  X ingerprint X  that is unique to each user, serving to accurately recognize abnormal usage of a given computer. We also provide some insights into which system measurements play the most valuable roles in creating statistical profiles of users (Tables 3 and 4). Our experiments indicate that we ma y get high intrusion-detection rates and low false-alarm rates, without  X  X tealing X  too many CPU cycles. We believe it is of partic ular importance to have very low false-alarm rates; otherwise th e warnings from IDS will soon be disregarded. Specific key lessons learned ar e that it is valuable to: An anomaly-based IDS, such as th e one we present, should not be expected to play the sole intrusion-detection role, but such systems nicely complement IDS that look for known patterns of abuse. New misuse strategies will always be arising, and anomaly-based approaches provide an excellent opportunity to detect them even before the intern al details of the latest intrusion strategy are fully understood. We wish to thank the employees of Shavlik Technologies who volunteered to have data gathered on their personal computers. We also wish to thank Michael Skroch for encouraging us to undertake this project and Mi chael Fahland for programming support for the data-collection proce ss. Finally we also wish to thank the anonymous reviewers for their insightful comments. This research was supported by DARPA X  X  Insider Threat Active Profiling (ITAP) program within the ATIAS program. [1] R. Agarwal &amp; M. Joshi, PNrule: A New Framework for [2] J. Anderson, Computer Security Threat Monitoring and [3] DARPA, Research and Development Initiatives Focused on [4] A. Ghosh, A. Schwartzbard , &amp; M. Schatz, Learning Program [5] T. Lane &amp; C. Brodley, Approaches to Online Learning and [6] A. Lazarevic, L. Ertoz, A. Ozgur, J. Srivastava &amp; V. Kumar, [7] W. Lee, S.J. Stolfo, and K. Mok, A Data Mining Framework [8] N. Littlestone, Learning Quickl y When Irrelevant Attributes [9] T. Lunt, A Survey of In trusion Detection Techniques, [10] T. Mitchell, Machine Learning , McGraw-Hill. [11] P. Neumann, The Challenges of Insider Misuse , SRI [12] J. Shavlik &amp; M. Shavlik, Final Project Report for DARPA X  X  [13] C. Warrender, S. Forrest, &amp; B. Pearlmutter. Detecting 
