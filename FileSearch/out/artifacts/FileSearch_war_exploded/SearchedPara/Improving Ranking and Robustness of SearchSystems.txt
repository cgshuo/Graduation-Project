 During the last two years of its running in 2013 and 2014, the TREC Web track focused not only on measuring ad-hoc effectiveness, but also on measuring the risk involved when systems try to improve results over a baseline. In regard to measuring ad-hoc effectiveness, as in previous years, the track proceeded by using traditional effectiveness measures such as ERR and nDCG, as well as diversity measures such as ERR-IA and  X  -nDCG. As for measuring the robustness or risk-sensitivity of the sys-tems, the organizers of the track introduced a risk-sensitive utility measure that determines, for a set of queries, the average of the differences between the retrieval effectiveness of a given system and that of a baseline system. The goal is to determine and quantify the ability of each system to minimize the risk of providing results that are less effective than the baseline  X  s results, for any given query.
 Most research in Information Retrieval (IR) has focused on improving the average effectiveness of systems, using measures like the ones mentioned above. However, it is very often the case that the improved systems fare worse than the baseline on many of the queries, even though the average effectiveness score is higher than the baseline The concept of robust ranking appears therefore to be key, when it comes to remedying those cases. In this present work, robustness refers to the ability of the ranker to reduce and mitigate poor performance on individual queries while striving to improve the overall performance as well.
 popularity of documents with respect to a general topic  X  state-of-the-art ranking, improves the average effectiveness of the ranking while improving the robustness of the ranking. Both methods merely re-rank the documents that were retrieved by the baseline that is being considered, without ever adding any new document to the set of retrieved documents. Ad-hoc Information Retrieval (IR) has long focused on improving average overall effectiveness, without worrying much about reducing the probability of getting poorer results for individual queries. Popular state-of-the-art retrieval models that have been used in ad-hoc IR include language modeling [ 19 ] and Okapi BM25 [ 21 ]. The Markov Random Field model for term dependencies has been proposed by Metzler and Croft [ 18 ]. And so were other term proximity models such as those described by Buttcher et al. [ 4 ] and Tao and Zhai [ 23 ]. In recent years, learning to rank algorithms have been adopted [ 3 , 16 ], as well as learning to re-rank algorithms [ 15 ]. search and retrieval. Many of the diversity ranking approaches are inspired by the MMR algorithm introduced by Carbonnell and Goldstein [ 5 ]. The fundamental idea of MMR is to optimize for both document relevance and coverage of intents and/or aspects. Differences in implementations lie in how similarities are computed: Car-bonnell and Goldstein suggest using any similarity function such as cosine similarity. Zhai et al. advocate for modifying the language modeling framework to incorporate a model of relevance and redundancy [ 26 ]. Other researchers utilized the correlation between documents as a measure of their similarity in the pursuit of diversi risk minimization in document ranking [ 24 ]. Carterette and Chandar [ 6 ] introduced a greedy result set pruning wherein there are 2 steps: in the ments in decreasing order of their similarity to the query; and in the second step, they proceed to iteratively prune documents whose similarity to any previously selected document is greater than a certain threshold. They also use a set-based probabilistic model to maximize the likelihood of covering all the aspects [ 6 ]. Radlinski and Dumais exploited a commercial search engine to obtain aspects of queries, and proceeded to diversify the ranking using query-query reformulations [ 20 ]. Santos et al. utilize a query-driven approach wherein they explicitly account for aspects by using sub-queries to represent a query. They then estimate the relevance of each retrieved document to every identi fi ed sub-query, and the importance of each sub-query [50]. related to this paper is Wang et al.  X  s effort [ 25 ] to address robustness by proposing a principled learning to rank framework that optimizes for both effectiveness and robustness of a retrieval system. Essentially the authors proceeded by proposing a learning method that optimizes for both reward and risk with respect to a given baseline. While their approach consists in learning to control the tradeoff between effectiveness and robustness, our approach is a general re-ranking method that can be used on top of any retrieval model (e.g. language model, query expansion, learning to rank, data fusion, or Markov Random Field retrieval models) and that consists in re-arranging documents in the original ranking, in order to get a more effective and robust fi nal ranking.
 Another major contribution in the literature is that of Dincer et al. [ 13 ] that focused on uncovering biases inherent to the way TREC Web track evaluated systems prior to 2014. The authors argued that, given there were several various retrieval methods used by the track participants in building the systems (query expansion, learning to rank, etc.), comparing robustness using one single baseline that was created with a speci retrieval model creates inherent biases. That is, systems that build on top of retrieval models similar to the baseline will have an advantage over others, and systems that build on top of retrieval models very different from the baseline will be at great disadvantage. The authors proposed several ways to mitigate that issue including the use of mean within-topic system effectiveness as a baseline. In the present work, our focus is not to show that we can improve robustness with respect to one single speci baseline. Rather we focus on showing that, given any ranking obtained using a certain retrieval model, we can apply our method to improve the robustness as well as the overall effectiveness of the system. For that purpose, we used each and every run from TREC Web 2013 and 2014  X  as baselines  X  to empirically show that.
 Wang and Zhu [ 24 ] proposed a risk-aware ad-hoc retrieval model that utilized the correlation between documents as a measure of their similarity in the pursuit of diversi fi cation and risk minimization in document ranking. In a similar effort to establish a risk-aware framework for retrieval, Zhu et al. [ 27 ] proposed to model uncertainty and utilized a one-parameter loss function to model the level of risk acceptable by a user. Their loss function was applied to a language modeling frame-work. Our approach, however, is a general re-ranking approach that we apply to the ranking of any retrieval model, and that we show to improve average overall effec-tiveness as well as risk-sensitive measures. There are other efforts for ad-hoc risk-aware retrieval methods that focus on query expansion cases [ 12 ] and pseudo-relevance feedback cases [ 17 ].
 It is to be noted however that the term robustness as used in this paper differs from the sense it is given in other work like in Battacharjee  X  means ranking algorithm that is sensitive (or less vulnerable) to spams and noise in the training set [ 2 ]. In order to show that we can improve the ranked list for a given query by exploiting a pre-fetched list of documents sorted by decreasing probability of retrievability of a document, we proceed as follows: a. For obtaining pre-fetched lists of documents ranked by decreasing probability of retrievability, we propose to use the method described in Sect. 3.1 . b. In order to obtain a baseline ranked-list for each given query, we use every ranked c. Then at query run-time, we propose to use one of the two algorithms proposed in diversity measures and the robustness using risk-sensitive measures. 3.1 Estimating Document Retrievability Given a query, we want to estimate how likely a document is to be retrieved. That is, we want to estimate its retrievability with respect to the topic. We treat this as the popularity of the document with respect to that general topic.
 space of possible queries from which q is one sample. In our experiments, we obtain the sample of possible queries by using Bing and Yahoo! Suggestions. We submit a query to each service through their APIs and we obtain a list of suggested queries. for some query q. In our experiments, we use k = 100.
 document is retrieved for a query in that space is approximately the same as the probability that a document is retrieved for q: query space as: P ret q 0 ; D j  X  X  X  1 if document D appears in top-k ranking for q otherwise, then this equation is proportional to a data fusion method so-called Comb-CAT [ 1 ], which merges retrieval results based on the total number of times a document appears. We can think of  X  and use  X  this probability of retrieval of a document as a type of popularity score for the document with respect to the sample space. 3.2 Re-Ranking We propose two different methods for re-ranking the baseline.
 Method 1. Suppose a user provides a query q to our system. Given a pre-fetched ranked list in decreasing order of P ret q 0 ; D j  X  X   X  which we name MasterList documents pertaining to the same topic as q, we proceed as illustrated in Algorithm 1 (Fig. 1 ) and Fig. 2 .
 MasterList, at the same position where they appeared in the baseline ranked list (RL). The only documents to be shuf fl ed are the ones that:  X  appeared in both the MasterList and the baseline ranked list;  X  appeared in the ranked list of more than one possible query q recorded in the MasterList  X  will be ranked higher than the documents with lower RL but not in the MasterList, remain at their original rank.
 Method 2. Given a query q provided by the user to our system and a MasterList that contains a pre-fetched list of documents ranked in decreasing order of P ret q proceed as illustrated in Algorithm 2 (Fig. 3 ) and Fig. 4 .
 P ret Q ; D j  X  X   X  as recorded in the MasterList  X  will be ranked higher than the documents with lower P ret Q ; D j  X  X  in the fi nal ranked list. And, in this method unlike in Method 1  X  all the documents that are at the intersection of MasterList and the baseline RL will have precedence over all other documents in the baseline RL. That is, every document that appeared in the ranked list of more than one possible query q from the sample space Q , and that also appeared in the baseline RL, will be ranked before all other documents. 4.1 Data Each of the TREC Web track 2013 and 2014 datasets contains 50 queries [ 10 , 11 ]. Those queries were created after perusing candidate topics from query logs from commercial search engines. Some topics were faceted (with several possible subtopics), others were non-faceted with single intents, and a few others were ambiguous (queries with several intents). The task of the participants is to provide a diversi no more than 10000 documents per query for the 50 queries. Unlike in previous versions of the track that ran from 2009 to 2011 where the emphasis was on diversity ranking in addition to ad-hoc ranking, in these two versions, the emphasis was on risk-minimization and ad-hoc rankings. However the queries and relevance judgments were still suitable for diversity retrieval evaluations, and participants were also pro-vided with diversity-based results once the competition was over. In this paper, we use diversity measures to show how effective our method is, and we use risk-sensitive measures adopted by TREC Web track organizers to show robustness.
 retrieval models, we use many baseline ranked lists (RL). Speci submitted by TREC Web 2013 and 2014 participants, each RL is created by a different search system. For the 2013 TREC Web track, there were a total of 60 runs, while for 2014, there were a total of 30 ad-hoc runs and 12 risk-sensitive runs. All runs were supposed to be risk-sensitive, although some participants did not speci for risk-sensitivity [ 10 , 11 ].
 organizers, and include Indri, Terrier, Indri-with-spam-fi several baselines is to see how truly robust a system is with respect to various baselines, and mitigate the bias that gets introduced when using only one baseline (in such a case, there would be bias towards systems that are built on top of a ranker similar to the baseline) 4.2 Evaluation Measures We use four evaluation measures (two for diversity evaluation and two for traditional non-diversity evaluation). For non-diversity measure, we opted for using the two measures adopted by TREC Web track organizers: ERR and nDCG. nDCG rewards documents with high relevance grades and discounts the gains of documents that are ranked at lower positions [ 14 ]. ERR is de fi ned as the expected reciprocal length of time the document as well as the relevance of the documents shown above it. For diversity measures, we opted for using  X  -nDCG and ERR-IA.  X  -nDCG is an extension of nDCG that rewards novelty and diversity by penalizing redundancy and rewarding systems for including new subtopics [ 9 ]. Similarly, ERR-IA is an extension of ERR to compute the expectation of ERR over the different intents [ 7 ].
 measures proposed by TREC Web track organizers as well [ 11 ]. For each run, we create two new runs using the re-ranking methods Method 1 and Method 2 respec-tively. For each query of each new run, we compute the absolute difference ( between the effectiveness of the new run and that of the baseline provided by the track organizers  X  as mentioned in Sect. 4.1 , that can be either Indri or Terrier or Indri-with-spam-fi ltering. When the difference is positive the new run has a win over the baseline. When it is negative, it has a loss over the baseline, otherwise it is a tie. retrieval effectiveness R A (q) relative to the baseline  X  same query. We de fi ne the risk-sensitive utility measure U set of queries Q as: where Q + is the set of queries for which  X  (q) &gt; 0 and Q the  X  (q) &lt; 0.
 It is important to note that we did not need to apply the guideline given by [ 13 ]to mitigate bias by using the mean within-topic system effectiveness as a baseline. In fact, we focus on showing that, for most rankings obtained using speci we can apply our method to improve the robustness of the system as well as the overall effectiveness. And we do so by comparing the risk-sensitive measure between the original ranking and the baseline to the risk-sensitive measure between the new re-ranking and the baseline (Table 1 ).
 4.3 Results Effectiveness. Effectiveness results, given by diversity measures ERR-IA@20, show that our approach is very promising. The results for applying Methdod 2 on the 2013 runs show that, in most cases, there are large improvements. In fact, out of the 60 runs, only one of them saw a slight decrease in ERR-IA@20 using Method 2. The general trend, indeed, is that Method 2 performs well on both datasets 59 runs improved out of 60 for the 2013 dataset, 28 runs improved out of the 30 runs for 2014 dataset (ad-hoc runs category) and 9 runs improved out of the 12 runs for 2014 dataset (risk-sensitive runs category). It is worth noting that the runs from 2014 dataset that Method 2 failed to improve are all from the same participating group that performed the best. This could have to do with the set of documents retrieved for their runs. Results for  X  -nDCG@20 have similar trends as results for ERR-IA, as shown in Table 2 .
 Table 2 also shows results produced when applying Method 1. They are very close to the ones obtained using Method 2, albeit slightly lower  X  using ERR-IA@20, 55 results improved vs 58 for Method 2 on 2013 runs, 27 vs 28 on 2014 ad-hoc runs, and 7 vs 9 on 2014 risk-sensitive runs. However, the actual numbers for Method 1 effectiveness (not shown here due to space constraints) are much lower than the numbers for Method 2. Risk Analysis. Summary of risk-sensitive measures shown in Table 3 are evidence that our methods, overall, improve robustness as well. The effectiveness measure (R) used in the delta formula (  X  q =R A (q)  X  R BASE (q)) is ERR@20. We use U although an overwhelming number of runs witness an improvement of their robustness, there are far less improvement in risk-sensitive measure than there were for average effectiveness measures for the 2014 runs. For the 2013 dataset, the number of improved runs based on risk-sensitive measures is very close to the number improved using average effectiveness measures: with respect to Terrier, Method 1 improved robustness of 56 out of 60 runs and Method 2 improved 48 out of 60. But for the TREC Web 2014 dataset, the number is much lower, especially for the runs submitted to the risk-sensitive track (Table 4 ).
 There are also stark differences in risk-sensitive measures depending on whether Terrier, Indri or Indri-with-spam-fi ltering is being used. The least signi improvements  X  as well as the most decrease in risk-sensitive measures observed on the runs from the 2014 dataset. For instance, using Method 1, 10 out of the 12 runs submitted for the risk-sensitive track see a decrease in risk-sensitivity utility measure with respect to Terrier, and 18 out-of-the 30 for the runs submitted to the 2014 ad-hoc track with respect to Terrier. This is not very surprising since Indri Terrier  X  was used to obtain our pre-fetched list of documents sorted by popularity. Also, even though Method 1 re-ranking is more robust than the original ranking more often than Method 2, there are more cases where Method 2 is more robust than Method 1. In this paper, we proposed two re-ranking approaches based on exploiting document popularity across a topic, and show that these methods can help improve average overall effectiveness as well as robustness. Using the runs submitted to TREC Web track 2013 and 2014 as baselines, we show that, after our re-ranking, overall effec-tiveness gets improved in an overwhelming number of cases, and robustness gets improved in a large number of cases but fewer than for overall effectiveness. Our future efforts will focus on establishing a principled framework for better exploiting the popularity of documents as well as other features to improve robustness of systems.
