 Traditional ranking mainly focuses on one type of data source, and effective modeling still relies on a sufficiently large num-ber of labeled or supervised examples. However, in many real-world applications, in particular with the rapid growth of the Web 2.0, ranking over multiple interrelated (heteroge-neous) domains becomes a common situation, where in some domains we may have a large amount of training data while in some other domains we can only collect very little. One important question is:  X  X f there is not sufficient supervision in the domain of interest, how could one borrow labeled in-formation from a related but heterogenous domain to build an accurate model? X . This paper explores such an approach by bridging two heterogeneous domains via the latent space. We propose a regularized framework to simultaneously min-imize two loss functions corresponding to two related but different information sources, by mapping each domain onto a  X  X hared latent space X , capturing similar and transferable concepts. We solve this problem by optimizing the convex upper bound of the non-continuous loss function and derive its generalization bound. Experimental results on three dif-ferent genres of data sets demonstrate the effectiveness of the proposed approach.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models; H.2.8 [ Database applications ]: Data mining Algorithms, Experimentation Heterogeneous cross domain ranking, Transfer ranking, Learn-ing to rank
This work was done when the first and the last authors are visiting Tsinghua University.

Ranking over heterogeneous data sources is an important challenge for many applications. For example, to predict the users X  preference (rating score) based on product reviews, one may have much training data (rated reviews) of existing products, but little or no training data for a new product. In social networks, we may have a large amount of training data for movie recommendation, but very limited for recommend-ing friends or web communities. Thus, one basic question is how to make use of the labeled information from existing (source) domain(s) to build an accurate ranking model for the target domain.

Although, quite a few related studies have been conducted, for example, transfer learning [4, 12], domain adaptation [5, 6], multi-task learning [2, 7], learning to rank [9, 15], there are only a few theoretical studies on the heterogeneous cross-domain (HCD) ranking problem. The major difference be-tween the HCD ranking problem and learning to rank is that HCD ranking needs to consider how to borrow the preference order from the source domain (as the supervi-sion information) to the target domain for learning a better ranking model. The HCD ranking problem is also different from transfer learning whose goal is to transfer the knowl-edge from the source domain to the target domain to learn a classification model. In HCD, the knowledge we desire to transfer is the preference order between heterogeneous objects from the source domain, instead of their accurate ranking positions.
 Motivating Application
Figure 1 (a) shows an example of academic search. The objective is to learn functions that can rank different ob-jects for a given query. In Figure 1 (b), the example query is  X  X ata mining X . The training data (i.e. the labeled rank levels of objects) in some domains, e.g. rank levels of con-ferences, is relatively easy to obtain (e.g., from several on-line resources 2 ). However, obtaining the training data for some other domains, e.g. papers and authors, would be not obvious. Intuitively, we hope that an approach can take advantage of the available supervision information (labeled conferences) and the correlation between conferences and papers/authors in the academic network to help learn the ranking functions for papers and authors.
 Summaries
The challenges of heterogeneous cross-domain (HCD) rank-ing are as follows: Figure 1: Example of heterogeneous cross-domain ranking.
To address the above challenges, we propose a unified cross-domain ranking model, named HCDRank, to simul-taneously model the correlation between the source domain and the target domain, as well as learning the ranking func-tions. In particular, HCDRank uses a  X  X atent feature space X  defined over both the source and target domains to measure their correlation. Examples from both domains are mapped onto the new feature space via a projection matrix, where a common (sparse) feature space is discovered. HCDRank adopts a regularization method to simultaneously minimize two loss functions corresponding to the two domains, and su-pervision from the source domain is transferred to the tar-get domain via the discovered common feature space. An efficient algorithm has been developed and a generalization bound is discussed. Experimental results on three differ-ent types of data sets show the proposed approach performs better (+1.2%  X  +6.1% in terms of MAP) than the compar-ison baseline methods, in particular when the target domain has a very small number of labeled examples. The proposed framework is general, to allow us to utilize many different algorithms to learn the ranking function.
The heterogenous cross-domain (HCD) ranking problem can be formalized as follows. For clarity, Table 1 summarizes the notations.
 Input: Let X S  X  R d be the instance space of the source domain in which d is the number of features and Y S = { r number of rank levels in the source domain. The rank levels have: r S 1  X  r S 2  X   X  X  X   X  r S p , where  X  denotes the prefer-ence relationship. The labeled data in the source domain is denoted by L S = { ( q k S , ~x k S , ~y k S ) } n S k =1 q , ~x k S = { x k S are the corresponding labels where x k S and N k S is the total number of instances related to this query. Further, for the target domain, let X T  X  R d be the instance space and Y T = { r T 1 , r T 2 ,  X  X  X  , r T q } is the set of rank levels. There are two parts of data in the target do-main: S = { ( q k , ~x k ) } n k =1 represents the unlabeled test data in which x k i  X  X T and L T = { ( q k T , ~x k T , ~y k T the labeled data where x k T the labeled data L T is not necessary, which implies we may have no labeled target domain data.
 Learning task: In the HCD ranking problem, the transfer ranking task can be defined as: given limited number of labeled data L T , a large number of unlabeled data S from the target domain, and sufficiently labeled data L S from the source domain, the goal is to learn a ranking function f  X  predicting the rank levels of unlabeled data in the target domain.

There are several key issues: (1) the source and the target domains may have different feature distributions or different feature spaces (e.g., different types of objects); (2) the num-ber of rank levels in the two domains can be different; (3) the number of labeled training examples in different domains may be very unbalanced (e.g., a thousand vs a few).
In HCD ranking, we aim at transferring preference infor-mation from an interrelated (heterogeneous) source domain to the target domain. As the feature distributions and the objects X  types may be different across domains, the first chal-lenge we need to address is how to quantitatively measure the correlation between the different domains, which reflects what kind of information can be transferred across the do-mains. On the other hand, our ultimate goal is to obtain a higher ranking performance. Based on these considerations, we have two main ideas: first we assume there is a common (latent) space between the two domains. Examples (e.g., x ) from the two domains can be mapped onto the latent space through a transformation function  X  ( x ). Such a common la-tent space provides a potential way to quantify the correla-tion between the two domains. Second, in the target domain we aim to learn a ranking model that can minimize the error (loss) on the unlabeled test data while preserving the prefer-ence orders in the labeled training data. When transferring the supervision information from the source domain, we also desire to preserve its original preference order, equivalently minimizing the loss in the source domain. Therefore, we propose a general framework (HCDRank), in which we use a latent space to bridge the two domains (i.e., the source domain and the target domain) and define two loss func-tions respectively for the two domains. We further propose an efficient algorithm to optimize the two loss functions and learn the latent space simultaneously. Given the labeled training data from the target domain L tion f T which can correctly predict the preference relation-ships between instances for each query q k T , i.e. f T ( x f ranking function f T , we can predict the rank level of a new instance. To learn the ranking function, we can consider to minimize the following loss function: where I [  X  ] is the indicator function returning 1 when  X  is true and 0 otherwise; R ( f T , L T ) counts the number of mis-ranked pairs in the target domain;  X  is a parameter that controls the tradeoff between the empirical loss (the first term R ) and the penalty E (the second term) of the model complexity.

When transferring the supervision information from the source domain, we hope to preserve the preference order between instances from the source domain. For bridging instances from the two heterogeneous domains, we define a transformation function  X  : R d  X  R d 0 to map instances from both domains to a d 0 -dimensional common latent space. Then we can define a general objective function for the HCD ranking problem as follows: where J  X  ( f S , f T ) is a penalty for the complexity of the HCD ranking model,  X  is a tuning parameter that balances the empirical losses and the penalty, and C is a parameter to control the imbalance of labeled instances between the two domains.
 The problem now is to find the best parameters for f S , f and  X  , that minimize the objective function (Eq. 2). In the following section, we give an instantiation of the framework and present a preferred solution.
In HCDRank, we do not simply want to learn the ranking function f T , f S for the two domains but also learn the trans-formation function  X  . In addition, it is desirable to leave out features that are not important for transferring knowledge across domains and result in a sparse solution.
 Instantiation of the HCDRank framework. Without loss of generality, f T is assumed to be a linear function in the instance space: f T ( x ) =  X  w T , x  X  , where w T are parameters (feature weights) to be estimated from the training data and  X  X  X  indicates the inner product. By substituting it into Eq. 1 we have
The loss function R ( f T , L T ) is not continuous, so we just use Ranking SVM hinge loss to upper bound the number of mis-ranked pairs [8]. For easy explanation, we define the following notations: for each query ~q k T ( k = 1 ,  X  X  X  , n an instance pair x a T corresponding labels y a T
Then we can get a new training data set consisting of in-stance pairs in the target domain L 0 T = { ( x a T For the source domain, we can make the same assump-tion and use the parallel notations w S and L 0 S = { ( x x by optimizing the convex upper bound of the original loss as:
Now the problem is to define the transformation function and the penalty of the model complexity.
 Instantiation of the transformation function and the penalty. We use a d  X  d matrix U to describe the correlation between features. The inner product of examples are then defined as x &gt; i UU &gt; x j using the matrix. Such parameteriza-tion is equivalent to projecting every example x onto a latent space spanned by  X  : x  X  U &gt; x . With the transformation function, we can redefine the loss function, for example, by replacing the first term in Eq. 5 with:
As for the penalty J  X  ( w S , w T ) of the model complexity, we define it as a regularization term, specifically, a (2,1)-norm k W k 2 , 1 , for the parameters of the source and the target domains, where W = [ w S , w T ] is a d  X  2 matrix with the first column corresponding to w S and the second w T . The (2 , 1)-norm of W is defined as || W || 2 , 1 = the i -th row of W. The 2-norm regularizer on each row of W leads to a common feature set over the two domains and the 1-norm regularizer leads to a sparse solution. The (2,1)-norm regularizer thus offers a principled way to interpret the correlation between the two domains and also introduce useful sparsity effects. Finally, we can redefine the objective function as: where U &gt; U = I denotes an orthogonal constraint which makes the projection matrix U unique.
 Learning algorithm. Directly solving the objective func-tion (involving solving parameters w S , w T , U in Eq. 7) is intractable, as it is a non-convex problem. Fortunately, we can derive an equivalently convex formulation of the objec-tive function Eq. 7 as follows: (Derivation of the equivalence is given in the appendix.) where M = [  X  1 ,  X  2 ] = UW , D = U Diag ( || a i || 2 || W || the superscript  X + X  of D indicates the pseudoinverse of the matrix D . X is a p  X  q matrix, range of X is the span of columns of X which can be defined as range ( X ) = { x | Xz = x, for some z  X  R q } . The trace constraint of D is imposed because if D is set to  X  , the objective function will degener-ate to only minimize the empirical loss. The range constraint bounds the penalty term below and away from zero. The equivalence has been previously used for multi-task feature learning [2].

We can solve the equivalently convex problem with an iterative minimization algorithm, as outlined in Algorithm 1, and detailed as follows: Step 1. We use an iterative algorithm to optimize matrix M and D . First, in lines 2-4, we keep D fixed, and learn  X  1 and  X  2 (that is, matrix M ) from the labeled training data in two domains respectively. Second, in line 5, we up-date matrix D by the learnt matrix M . We run the above two steps iteratively until convergence or excess of the max-imal iteration number. Then in lines 7 and 8, we apply SVD decomposition [26] on the learnt intermedia matrix D , i.e. D = U  X  V &gt; ; then the matrix U is constructed by the eigenvectors corresponding to the first and second biggest eigenvalues of D .

Step 2. In line 9, we learn the weight vector of the target domain from all the labeled data of two domains in the latent space. In lines 10-12, we use the learnt w  X  T to predict ranking levels of new instances from the target domain.
 Complexity. The size of the two matrices to be optimized in HCDRank depends only on the feature number d , e.g., matrix D is d  X  d and W is d  X  2, and the complexity for SVD decomposition on matrix D is O( d 3 ).

Let N = n 1 + n 2 be the total number of instance pairs for training and s be the number of non-zero features. Using the cutting-plane algorithm[20], linear Ranking SVM train-ing has O( sN log(N)) time complexity. In our algorithm HCDRank, let T be the maximal iteration number, then the training of HCDRank has O( (2 T + 1)  X  sN log ( N ) + d time complexity.
First, let a domain be defined by two terms: the dis-tribution D on instance space X , and a ranking function f : X  X  { r 1 , r 2 ,  X  X  X  , r p } . Then source and target domains are denoted by  X  X  S , f S  X  and  X  X  T , f T  X  respectively. Let  X  and  X  T ( h ) denote the source and target risks. Correspond-ingly,  X   X  S ( h ) and  X   X  T ( h ) are the empirical risks.
An equivalent formulation for Eq. 7 is as follows: where z  X  0 and there is a one-to-one correspondence be-tween  X  and z [22].

In Eq. 9, the objective function is  X   X  S ( h ) + C  X   X  T parameter C  X  [0 ,  X  ). It is easy to prove that C is equiva-by replacing C with 1  X   X   X  and multiplying both sides of the equation by  X  , we can obtain the following equivalent ob-jective function which is a convex combination of empirical source and target risk: weighted risk respectively. Hereafter, we will analyze the objective function in the formulation of Eq. 10.
Theorem 1. Let H be a hypothesis space of VC-dimension d. Let U S and U T be unlabeled samples of size m 0 each, drawn from D S and D T respectively, and  X  d H4H is the em-pirical distance between them. Let L = L S beled samples of size m generated by drawing (1  X   X  ) m points from D S and  X m points from D T , labeling them according to f S and f T respectively. For each ranking function h with zero training risk, if  X  h  X  H is the empirical minimizer of  X   X  ( h ) on L , then with probability of at least 1  X   X  (over the choice of the samples)[5, 15] where  X  = min h  X  X   X  S ( h ) +  X  T ( h ) and  X  = n T n
The error bound is comprised of three components: the first one is the upper bound for the target risk using only the labeled data in the target domain; the second one cor-responds to the difference between the true and empirical weighted risks; the last one measures the distance between target risk and weighted risk. Due to space limitation, de-tails of the proof are given in the extended paper.
Our approach is general and can be applied to various data sets. We perform our experiments on three different genres of data sets: a homogeneous data set which consists of doc-uments from different domains; a heterogeneous data set which consists of three different types of objects; a hetero-geneous task data set which consists of two different ranking tasks. Evaluation measures. To quantitatively evaluate our method, we use P@n(Precision@n), MAP (mean average precision)[3] and NDCG (normalized discount cumulative gain) [17].
The precision of top n results for a query is measured by precision at n which is defined as follows:
Average precision is defined based on the P@n to measure the accuracy of ranking results for a given query.
MAP is then defined as the mean of all APs over test set and measures the mean precision of ranking results over all the queries. Different from MAP, NDCG gives high weights to the top ranked relevant documents. The NDCG score at position n is defined as follows: where r ( j ) is the rank of j -th document, and Z n is a nor-malization factor.
 Baseline methods. We compare the proposed ranking model HCDRank with three methods as listed in Table 2. Ranking SVM (RSVM) [15] is one of the state-of-the-art ranking algorithms for information retrieval. It is designed for ranking in one domain only. For fair comparison, we conduct two experiments with RSVM, one is to train the ranking model on the target domain L T only and the other (called RSVMt) is to train the ranking model by combin-ing the source domain and the target domain L S third comparison method is MTRSVM which is a multi-task feature learning approach using ranking SVM hinge loss[2].
All the experiments are carried out on a PC running Win-dows XP with Dual-Core AMD Athlon 64 X2 Processor(2 GHz) and 2 G RAM. We use SVM light [19] with linear ker-nel and default parameters to implement RSVM, RSVMt and the preference learning step of MTRSVM. The proposed ranking model HCDRank has been implemented using Mat-lab 7.1 and the maximal iteration number T is set to five. Also without special specification, we use the grid search to choose parameter C from { 2  X  6 , 2  X  5 , 2  X  4 , 2  X  3 , 2 2 , 2 3 , 2 4 , 2 5 } and the results reported in this paper are all averaged over 10 runs. Data Set. We use LETOR 2.0 [23] as the homogeneous data set, which is a data set for evaluating various algorithms for learning to rank. LETOR 2.0 is comprised of three sub data sets: TREC2003, TREC2004, and OHSUMED, with respectively 50, 75, and 106 queries. A set of query-document pairs are collected in each of the data sets. The TREC data is a collection from a topic distillation task which aims to find good entry points principally devoted to a given topic. The OHSUMED data is a collection of records from medical journals. In the OHSUMED data set, there are three rank levels, i.e. relevant  X  partially relevant  X  non-relevant, while in the TREC data set, there are two, i.e. relevant  X  non-relevant. In LETOR, all the features are highly abstract. In TREC, there are 44 features divided into four categories. In OHSUMED, there are 25 features falling into three categories. Table 3 summarizes the features in the LETOR data set. For example, for TREC data, there are 16 low-level content features (e.g. tf and idf), 13 high-level content features (e.g. BM25 and language model for IR), 7 hyperlink features (e.g. PageRank and HITS) and 8 hybrid features (e.g. hyperlink-based relevance propagation). Feature definition. To adapt to the cross-domain ranking scenario, we make slight revision to the LETOR data set. After revision, the whole data set and three sub data sets are correspondingly referred to as LETOR TR, TREC2003 TR, TREC2004 TR and OHSUMED TR. Specifically, we split each data set into two domains (source domain and target domain), according to the feature types. Table 4 lists statis-tics of the data sets in which the 3th column shows the details for features used in each domains of every data set by feature categories A-H in Table 3. We split features in this way in order to simulate some real applications. For ex-ample, the source domain of TREC2003 TR only contains feature categories A and B with queries 1-25 which corre-spond to features for document contents; while the target domain of TREC2003 TR consists of feature categories B, Table 4: Data characteristics of LETOR TR data set. #D/Q and #Dp/Q respectively denotes the av-C, D, E with queries 26-50 which may correspond to features in blogs. After this splitting, intuitively the features in two domains are quite different. In all experiments, we use the labeled related documents for queries from the source do-main as the training data L S , and randomly sample 20% of the queries and the related documents from the target domain as the training data L T of the target domain (that is, 5, 8 and 10 queries for TREC2003 TR, TREC2004 TR and OHSUMED TR respectively), while all the other data in the target domain are viewed as the unlabeled test set S .
For ease of implementation, in the experiments, we still define each instance with a vector of 44 dimensions (TREC) or 25 dimensions (OHSUMED). We set the values of features that are not defined in a domain as zero. For example, in the source domain of TREC2003 TR, only features of categories A and B are set with their actual values, the values of others (B, C, D, E) are set to zero. Similarly, in the target domain of TREC2003 TR, only features of categories B, C, D and E have their actual values and the others are set to zero. Results and analysis. Figure 2 and Table 6 show the results of the proposed and the comparison methods on the LETOR TR data sets. Generally, our approach achieves a higher performance and has a nice convergence property (converging after several iterations in most experiments). Specifically, we have the following observations: 1. Ranking accuracy. HCDRank performs much better 2. Effect of difference. We measure the difference of Table 5: Training time used on LETOR TR (S).
 3. Reason for performance. We conduct an analy-4. Training time. Finally, we compare the training time Data Set. The second data set is a heterogeneous academic data set, which contains 14 , 134 authors, 10 , 716 papers, and 1 , 434 conferences. The queries are 44 most frequent queried keywords (e.g.,  X  X ata mining X ,  X  X nformation retrieval X ) col-lected from the query log of the ArnetMiner 1 system[25]. Specifically, to obtain the ground truth for experts, for each query, the top 30 experts from Libra, Rexa and Arnetminer are collected respectively and pooled into a single list by re-moving the same or ambiguous ones [31]. Then, annotators provided human judgments in terms of how many publica-tions he/she has published, how many publications are re-lated to the given query, how many top conference papers http://www.arnetminer.org Table 6: MAP and NDCG performances for LETOR TR(Figure 2 in table form).
 he/she has published, what distinguished awards he/she has been awarded. There are four rank levels (3, 2, 1, and 0), which respectively represent definite relevance  X  relevance  X  marginal relevance  X  not relevance. To obtain the ground truth for conferences, the top 30 conferences from Libra and ArnetMiner are collected and three online resources 2 are mainly referenced for conference ranking.

In this experiment, we aim to answer the question: how can heterogeneous data be bridged for better ranking? We use the labeled data of one type of object (e.g., conferences) as the source domain and another type of object (e.g., au-thors) as the target domain. Thus, our goal is to transfer the conference ranking information for ranking authors. Feature definition. We use titles of all papers published in a conference to form a conference  X  X ocument X , and use titles of all papers written by an author as the author X  X   X  X ocument X . Thus we can define features for each object as listed in Table 7. For each  X  X ocument X , there are 10 low-level content features (e.g. L1 is term frequency(tf), L5 is inverse doc frequency(idf)) and 3 high-level content features (e.g. H1 and H2 are the original and log values of BM25 http://www.cs.ualberta.ca/~zaiane/htmldocs/ ConfRanking.html and http://www3.ntu.edu. sg/home/ASSourav/crank.htm and http://www. cs-conference-ranking.org/conferencerankings/ alltopics.html Table 7: Feature definitions for expertise search. score, H3 is the value of language model for IR). S1-S3 are special features for a conference which measure the number of years held and the total number of citations. S4-S7 are special features for an expert, for example, the year when his first paper has been published and the citation numbers of his all papers. Finally, we define 16 features (L1-L10, H1-H3 and S1-S3) for conference and 17 features for expert (L1-L10, H1-H3 and S4-S7).

We normalize the original feature vectors by query. Sup-i -th query, then for a feature x ( i ) j of document d ( i ) normalization, it will become Results and analysis. In this experiment, we use all the labeled conference data as the source domain, and the expert data as the target domain. In the target domain, we use one query with its corresponding documents as the labeled data and the rest as the unlabeled test data. The results reported below are averaged over all the queries. The parameter C is empirically set to 1.
 As for the baseline methods, besides RSVM,RSVMt and MTRSVM, we also compare the performance of our ap-proach with the results of two online academic search sys-tems: Libra.msra.cn and Rexa.info, which are mainly based on unsupervised learning algorithm, e.g., the language model [30]. Table 8 shows the results of different approaches, the main observations are as follows: 1. Ranking accuracy. Among all the approaches, our Table 8: Performances of different approaches for expert finding.
 2. Feature analysis. Figure 3 shows the final weight 3. Reason for performance. The key reason is that Figure 3: Feature correlation analysis in the source and the target domains. The red colored weights w
T are learnt by HCDRank; the blue and black ones (w
S and w T ) are learnt from the two domains sep-arately. The table lists top 10 features learnt from the academic data set for HCD ranking. Data set. The third experiment is for heterogeneous tasks, where we have two different ranking tasks: expert finding and best supervisor finding. The goal of expert finding is to find experts on a given topic (query), while best super-visor finding is about finding who are the best supervisors in a specific domain, which is useful for junior students to find  X  X ood X  supervisors in their interested fields. An expert can be a good supervisor, but not necessarily, thus the two tasks are related but different. The goal of this experiment is to evaluate whether the proposed approach can transfer knowledge to improve a different ranking task (best super-visor finding) using training data of an existing related het-erogeneous ranking task (expert finding). The demo for best supervisor finding is now online available 3 .

The evaluation data set for best supervisor finding is cre-ated by collecting the feedbacks from many researchers in related domains. The data set for best supervisor finding consists of 9 most frequent queries, and for each query, we choose the top ranked 50 researchers by ArnetMiner.org and another 50 researchers who start publishing papers only in recent years ( &gt; 2003, 91.6% of them are currently graduates or postdoctoral researchers). We send to each of the re-searchers an email, in which we list the top 50 researchers for each query, and ask for feedback on whether each candi-date is the best supervisor ( X  X es X ) or not ( X  X o X ), or X  X ot sure X . Participants can also add other best supervisors. Based on the feedbacks from the participants, we organized a list for evaluating best supervisor finding. We rated each candidate person by simply counting the number of X  X es X (+1) and X  X o X  (-1) from the received feedback, and averaged the rates over the number of the corresponding definite feedbacks ( X  X es X  and  X  X o X ). In this way, we created a relatively commonly accepted best supervisor list for each query.
 Feature definition. We define 21 common features for ex-pert finding and best supervisor finding (as shown in Table 9). Features L1-L10 and H1-H3 are scores calculated using language models, while features B1-B8 represent the exper-tise scores of an author from different aspects. B5-B7 are the same as S5-S7 in Table 7. In addition, we define another 32 special features for best supervisor finding. SumCo1-SumCo8 represent the overall expertise of his/her coauthors, and we average SumCo1-SumCo8 scores over the total num-ber of his/her coauthors, denoted by AvgCo1-AvgCo8. Sim-ilarly, we consider the summation and average of the ex-pertise of only his/her advisees through features SumStu1-SumStu8 and AvgStu1-AvgStu8. For SumStu1-SumStu8 and AvgStu1-AvgStu8, we need identify the adviser-advisee relationship between researchers. Refer to [28] for details. Results and analysis. In this experiment, for the source domain data, we use all the labeled data from the expert finding task, and for the target domain data, we uses two sampled queries with their corresponding documents from the best supervisor finding task as the labeled data, and the rest as the unlabeled test data. Table 10 shows the perfor-mance of best supervisor finding. We see that the proposed method performs better than the baseline methods of using RSVM, RSVMt, MTRSVM and the language model based method [30]. Also we can see that all supervised learning-to-rank methods can achieve higher ranking accuracy than the unsupervised ranking method (language model). http://bole.arnetminer.org Table 9: Features for expert finding and best super-visor finding.

Table 11 show the top 5 best supervisors/experts for two example queries. From that, we can see the traditional ex-pert finding algorithm is not appropriate for best supervisor finding task.
Considerable work has been conducted for supervised learn-ing to rank. The proposed approaches can be divided into three categories: pointwise approach, pairwise approach and listwise approach. In pointwise approaches, the ranking problem is aimed at predicting the rank level of an object. In pairwise approaches, the ranking problem can be reduced to a classification problem by comparing the rank levels of each instance pairs. Ranking SVM [15], RankBoost and RankNet [9] are three state-of-the-art algorithms in this category. In listwise approaches, the ranking problem is formulated to directly optimize some listwise performance measures of in-formation retrieval [27, 29].

Regarding the unavailability of a large amount of training data, there is also some work on ranking by semi-supervised learning and transductive learning. For example, Duh and Kirchhoff propose a framework for ranking problem in the transductive setting. They try to extract query-specific fea-tures in order to learn a query-specific ranking function [13]. Amini et al. propose a semi-supervised rankboost algorithm [1]. Hoi and Jin propose a semi-supervised ensemble rank-ing with a SVM-like formulation [16]. Chen and Lu et al. propose a tree based ranking adaptation algorithm, aiming to make use of the training data from an existing domain. Specifically, they first learn a regression tree in one domain and then adapted its structure to a new domain with only a few training data [10]. Up to our knowledge, this is the most similar work to ours. However, our problem setting and the proposed approach are different from theirs. We address the ranking adaptation problem in the heterogeneous data and our approach has a clear regularized formulation. Table 11: Example lists of expert finding verse best supervisor finding.

Another related work is transfer learning, which aims to transfer knowledge from a source domain to a related tar-get domain. Two fundamental issues in transfer learning are  X  X hat to transfer X  and  X  X hen to transfer X . Many ap-proaches have been proposed by reweighting instances in source domain for the use in target domain [12]. Gao et al. propose a locally weighted ensemble framework which can utilize different models for transferring labeled informa-tion from multiple training domains [14]. Also many works have been done based on new feature representation [18, 21]. For example, Argyriou et al. propose a method to learn a shared low-dimensional representation for multiple related tasks and the task functions simultaneously [2]. Raina et al. propose to use a large amount of unlabeled data in source domain to improve the performance on target domain in which there are only few labeled data. They don X  X  assume the two domains share the class labels or distributions [24]. Blitzer et al. proposed a structural correspondence learning approach to induce correspondences among features from source and target domains [6]. There are also other ap-proaches which transfer information by shared parameters [7] or relational knowledge. Transfer learning techniques are widely used in classification, regression, clustering and di-mensionality reduction problems.
We formally define the problem of heterogeneous cross do-main (HCD) ranking and address three challenges:(1) how to formalize the problem in a unified and principled frame-work even when objects X  types across domains are different; (2) how to transfer the knowledge of heterogeneous objects across domains; (3) how to preserve the preference relation-ships between instances across heterogeneous data sources. To address these, we propose a general regularized frame-work to discover a latent space for two domains and mini-mize two weighted ranking functions simultaneously in the latent space. We solve this problem by optimizing the con-vex upper bound of the non-continuous loss function and derive its generalization bound. Experimental results on three different genres of data sets show that the proposed ap-proach performs better (+1.2%  X  +6.1% in terms of MAP) than the comparison baseline methods.

There are several directions for future work. It would be interesting to develop new algorithms under the framework and to reduce the computing complexity for online applica-tion. Another issue is to extend the HCDRank framework to combine structural information for ranking. On the Web, there are many structural information such as hyperlinks and social relationships. How to incorporate such informa-tion into the HCDRank framework is an interesting problem. Another potential issue is to apply the proposed approach to other applications (e.g., recommendation, rating, and link prediction) to further validate its effectiveness.
Bo Wang and Songcan Chen are supported by NSFC (60773061) and NSF of Jiangsu (BK2008381). Jie Tang is supported by NSFC(60703059), National High-tech R&amp;D Program (No. 2009AA01Z138) and Chinese Young Faculty Research Fund (No. 20070003093).
We give a brief proof on the equivalence between Eq. 7 and Eq. 8. We follow the same structure as the proof of equation equivalence in [2]. For easy explanation, we denote the objective functions in Eq. 7 and Eq. 8 as E ( W, U ) and R ( M, D ) respectively.

Theorem 2. Problem of min {E ( W, U ) : U &gt; U = I } is equivalent to the problem min {R ( M, D ) : D  X  0 , trace ( D )  X  1 , range ( M )  X  range ( D ) } .
 Proof. The correspondence between the two problems is row of W, then k a i k 2 = k M &gt; u i k 2 . So Therefore, min M,D R ( M, D )  X  min W,U E ( W, U ). On the other side, let D = U Diag (  X  i ) U &gt; , then Hence, min M,D R ( M, D )  X  min W,U E ( W, U ). So the two problems are equivalent.
