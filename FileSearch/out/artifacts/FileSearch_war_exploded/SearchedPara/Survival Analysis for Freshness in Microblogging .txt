
Freshness of information in real-time search is central in social networks, news, blogs and micro-blogs. Nevertheless, there is not a clear experimental evidence that shows what principled approach effectively combines time and content.
We introduce a novel approach to model freshness using a survival analysis of relevance over time. In such models, freshness is measured by the tail probability of relevance over time. We also assume that the probability distributions for freshness are heavy-tailed. The heavy-tailed models of fresh-ness are shown to be highly effective on the micro-blogging test collection of TREC 2011. The improvements over the state-of-the-art time-based models are statistically signifi-cant or moderately significant.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation, Information Retrieval Combining Searches, Blog Search
We propose a real-time search conducted by a survival analysis of relevance leading to a new time-based retrieval model based on language modelling.

Up to now the time-based retrieval models have been tested only on existing TREC collections but using man-ually selected sets of time-sensitive topics, or on test collec-tions that were not publicly available. Although restricted to microblogging search, the new Tweets2011 test collec-tion of TREC, allows us to conduct a systematic study on the effectiveness of the existing time-based retrieval models. Therefore, we are now able to assess the state-of-art time-based models on a proper test setting, and also to compare them with a novel freshness-based language modeling ap-proach we introduce in this paper.
Time is of important value for users and modeling time for search can be helpful for relevance ranking purposes [1].
A first re-ranking technique based on time and relevance was introduced by Li and Croft [14] as an extension of the language model [13, 16]. They introduce a dependency of the prior distribution over documents from time by a relevance exponential decay factor, e  X   X   X   X  , where  X  is the elapsed time from the document publication to the most recent document publication time. However, Li and Croft find that values even slightly larger than the best performant one for the parameter  X  can significantly degrade performance.
Dakka et al. [5] define effective methods based on Bayes X  rule to process time sensitive queries. Starting from Dakka et al. decomposition of the query likelihood with both time and query evidence, Efron and Golovchinsky [9] propose sev-eral language modeling approaches for temporal re-ranking, assuming the independence of document content from pub-lication time. They apply the exponential distribution to assign time-based priors to documents and estimate the  X  parameter query-by-query. Arikan et al. [2] apply a dif-ferent extension of the language model considering explicit temporal expressions contained in the query.

Jones and Diaz [11] classify queries into temporal classes analyzing the top retrieved documents to elicit user feedback and to disambiguate queries. Similarly, Elsas and Dumais [10] propose a dynamic smoothing of the Dirichlet language model taking into account frequencies of document vocabu-lary across different periods of time. Zhang et al. [17] reward recent documents with respect to the oldest retrieved docu-ment through a difference measure between relevance scores. Dong et al. [6] apply a learning to rank approach restricted only to the topics that occur more often in recent documents. Different approaches of learning to rank process features ex-tracted from the micro-blogging data stream [7, 8]. The learning to rank approach is also used to model freshness and relevance using a weighted harmonic mean function which maps relevance and freshness scores to a single numerical score [4]. The difference between Twitter search and web search is discussed in [15].
To simulate a real time retrieval system, we assume that the query is submitted at time t q , and therefore only the documents of the collection that have the publication time t &lt; t q can be indexed and retrieved. We introduce a ran-dom variable T q , that observes the presence of a relevant document in a given instant of time. We then define the probability density function p ( T q =  X  ) of observing a rele-vant document exactly  X  instants of time before the query submission. Therefore, we use the reverse chronological or-der as principal timeline: Analogously, we define the tail probability (called the sur-vival function ): 1 For the sake of simplicity we denote p ( T q &gt;  X  ) the prob-ability of Formula (2). We generalize the language model retrieval approach by introducing the probability p ( d, T  X  | q ). According to Bayes Assuming the independence of T q from d we obtain: where: a) p ( d | q ) is the likelihood of the document content given the query; b) p ( T q &gt;  X  | q ) is the likelihood of observing relevant docu-ments after  X  . We denote the probability tail p ( T q &gt;  X  | q ) = S (  X  ). Note that the probability tail is inversely related to the cumulative P of the density p , that is S q (  X  ) = 1  X  p ( T  X  | q ) = 1  X  P (  X  | q ). The derived probabilistic model is thus a combination of relevance and time likelihoods with respect to the query q . Passing to the logarithmic transformation, Equation 3 is equivalent to: that can be further generalized with an utility function hav-ing a smoothing factor 0  X   X   X  1 which assigns different weights to the relevance and time log-likelihoods: Since probabilities are in the interval [0 , 1], scores are given by two negative functions on Formulas 4 and 5.

In the next section we motivate the use of the survival function to model freshness.
We have proposed the survival function S q (  X  ) as an esti-mate of the time likelihood according to the reverse chrono-logical order to model freshness: at time  X   X  0 the sys-tem provides the most recent retrieved information, while at large  X  system might select the relevant information at the origin time of the stream.

The survival function of a time-based retrieval model re-quires some parameters to set: the probability density p and its parameters, the unit time for  X  .
T he analysis is called survival, to predict how the fail-ure rate of an event would be after a given time, such as deaths for clinical treatments or occurrence of relevance for microblogging search.
 Figure 1: Given the relevance density function p o ver time  X  , survival function S q ( x ) is the cumulative distribution on the tail and provides the probability that no relevant documents appear in the interval [0 , x ] , i.e. the probability of observing the topmost fresh relevant document at least at x .

In order to choose the appropriate density function p , we first observe that a heavy-tailed distribution for modeling relevance backwards on the timeline is equivalent to assume that the rate of occurrence of relevant documents decreases slowly towards the past and becomes eventually null at the origin of the stream . The rate of occurrence (called the fail-ure rate in survival analysis) is defined as the ratio of the probability density at time  X  and the probability of the tail left after  X  , and only in fat-tailed distributions such rate is monotone decreasing.

There is not a general closed form to define all the fat-tailed distributions, but there exists the generalized Pareto distributions that defines a very large family of such dis-tributions. We here study a few more not included in the generalized Pareto, e.g. the log-normal and the exponential, which can be regarded as a limit case of the heavy-tailed distributions. A generalized Pareto distribution W can be mainly considered to be a linear combination of a power of the inverse of the Beta distribution [3], that is: where U = Y  X  1  X  1 and Y has the beta distribution with parameters  X  1 &gt; 0 and  X  2 &gt; 0. The log-logistic distri-bution and the Zipf-Mandelbrot distributions can be de-rived from this family. In general  X  2 is set to 1, so that W = W (  X ,  X ,  X ,  X  1 , 1). We now explicitly present some of the distributions for S q (  X  ) that are the most effective in our experimentation or theoretically important.

Exponential Assuming that T q follows an exponential distribution, its survival function is where  X  is a parameter.

Log-normal The random variable T follows the log nor-mal distribution if its logarithm Z = ln T follows the normal distribution. The formula for the survival function of the log-normal distribution is where  X  is the cumulative distribution function of the nor-L inearRank (  X  = 0 . 95) 0.3591 1.70% 10.50% (*) 8.44% (*) 7.94% (*) t-test at 95% confidence level ( 5%  X  p-value  X  10% ). mal distribution, whilst  X  and  X  are the mean and the stan-dard deviation of Z .

Log-Logistic distribution A distribution closely resem-bling the normal distribution is the log logistic distribution, but its survival function is mathematically more tractable than the log normal. T follows the log logistic distribution if its logarithm Z = ln T follows the logistic distribution. The survival time functions is where A = e  X   X  , with  X  and  X  the mean and the standard deviation of Z .

Zipf-Mandelbrot The survival time of the Zipf-Mandelbrot is where  X  and B are parameters to learn. Note that the Zipf-Mandlbrot and the Log-Logistic coincide when  X  = 0 and  X  = 1 and A = B .
We perform survival analysis on the retrieval set, that is removing as much as possible irrelevant documents from the stream. The ordering induced from the reverse chronological ordering of the document timestamps over the result set can be used as a new timeline for the retrieved set but with a different scale and smoothing time normalization. The order rank(  X , q ) becomes the new elapsed time of the document d from the submission query time, and at each unit time a pseudo relevant document is generated in the stream.
We may use both timestamps and ranks to define timelines for survival analysis, but we can anticipate that time ranking is the most effective between the two.
The experimentation is based on the Tweets2011 TREC corpus, made up of approximately 16 million messages sam-pled from the Twitter public timeline over a period of 2 weeks across January and February 2011. Together with the collection, 49 test queries were provided by NIST. In order to simulate a real-time system, we built a separate index for each query, filtering out all tweets published after the query time, as required by the TREC guidelines.
We used Equation 5 as retrieval model. In order to deter-mine optimal values for the parameters of each heavy-tailed survival distribution and baselines, as well as of the mixing parameter  X  , we applied the 3-fold cross validation using MAP as evaluation measure.

The relevance baseline, denoted by Dirichlet-BL, is ob-tained by retrieving documents through the Dirichlet Lan-guage Model.

Upon Dirichlet-BL we have built several extra baselines in order to compare time likelihood estimation with the state-of-the-art time based language models and re-ranking ap-proaches. We report the Li and Croft X  X  time-based baseline, TB1- X  , where  X  is the parameter of the exponential dis-tribution, and Efron and Golovchinsky X  X  time-based model, TSQL, that uses the maximum a posteriori to estimate the  X  parameter, which is a smoothed ratio of the number of doc-uments newer than d and the total number of documents in the collection. We also include the RRtopK baseline, that re-ranks the top K documents of Dirichlet-BL by recency only, and appends the rest with the Dirichlet-BL ordering. Finally, the baseline LinearRank is obtained by re-ranking documents of Dirichlet-BL according to a linear combination of relevance and time ranks, respectively rank r and rank with a smoothing factor  X  , that is  X   X  rank r +(1  X   X  )  X  rank suggested by Li and Croft [14]. Table 1 reports the compar-ison of the new time-based models with different baselines, showing the achieved MAP and the improvements with re-spect these baselines. The statistically significant results of the paired t-test are marked with a star.
 All the time-based baselines (TB1- X  , TSQL and Linear-Rank) show a lower or equal performance with respect to the Dirichlet-BL, the relevance-only baseline. These results are consistent with the considerations made by the authors, since their time-based models should be applied only to time-sensitive queries. For example, TB1- X  improves Dirichlet-BL for more than one half of the 49 queries of Tweets2011 (see Figure 2).

Only the new time-based models with heavy-tailed distri-butions show a statistically significant improvement (from 7.94% to 10.5%) over the only-relevance baseline, the Dirichlet-BL.
 Exponential distribution with Equation 5 obtains lower AP values with respect to other distributions, as reported in Table 1. Furthermore it seems to have similar perfor-mance, though slightly better, than its use within TB1- X  . This could depends on the  X  X o-aging X  X r  X  X emoryless X  prop-erty of the exponential distribution: if relevance is not ob-served in the interval of time which is close to the query time, then the probability of relevance backwards in the past re-mains the same. This property seems thus to be in contrast with the freshness assumption.

The paired t-test attests that the achieved performances by the new freshness-based model are statistically significant when compared with Dirichlet-BL, TSQL and LinearRank. Improvements over other time-based baselines are instead moderately significant (in between 5% and 10% of p-value). However, the percentage increase of MAP obtained by the LogLogistic time-based model over TB1- X  is much larger than that obtained with all other baselines, even through the t-test is only moderately significant. To give an ex-planation to this result, we compare their Average Preci-sions (AP) query-by-query, as reported in Figure 2. Their APs are close for several queries, whilst in all other cases LogLogistic-based model is more conservative than TB1- X  with respect to the relevance-only baseline Dirichlet-BL. In conclusion the Log Logistic time-based model is more robust than TB1- X  with respect the relevance-only baseline.
We have introduced a new freshness-based retrieval model for microblogging search that is more effective than the state-of-art time-based retrieval models. The novel aspects of the model are: the application of the survival function (the probability tail) of relevance over time as likelihood of the freshness of documents; the use of the heavy-tailed distri-butions to model freshness; the generalization of language modeling approach to combine freshness and relevance like-lihoods. Preliminary results show that using the ranks in-duced by the reverse chronological ordering over the result sets as timeline, seems more effective than using the times-tamps scale. Significance test show statistically significant or moderately significant improvements over other time-based baselines, and thus a larger number of test topics is required. [1] O. Alonso, M. Gertz, and R. Baeza-Yates. On the value [2] I. Arikan, S.J. Bedathur, and K. Berberich. Time will [3] Arnold, B. C. Pareto distributions . International [4] N. Dai, M. Shokouhi, and B.D. Davison. Learning to [5] W. Dakka, L. Gravano, and P. G. Ipeirotis. Answering [6] A. Dong, Y. Chang, Z. Zheng, G. Mishne, J. Bai, [7] A. Dong, R. Zhang, P. Kolari, J. Bai, F. Diaz, [8] Y. Duan, L. Jiang, T. Qin, M. Zhou, and H. Shum. An [9] M. Efron and G. Golovchinsky. Estimation methods for [10] J.L. Elsas and S.T. Dumais. Leveraging temporal [11] R. Jones and F. Diaz. Temporal profiles of queries. [12] J.P. Klein and M. L. Moeschberger. Survival analysis. [13] J. Lafferty and C. Zhai. Document Language Models, [14] X. Li and W. B. Croft. Time-based language models. [15] J. Teevan, D. Ramage, and M.R. Morris. [16] C. Zhai. Statistical language models for information [17] R. Zhang, Y. Chang, Z. Zheng, D. Metzler, and J. Nie.
