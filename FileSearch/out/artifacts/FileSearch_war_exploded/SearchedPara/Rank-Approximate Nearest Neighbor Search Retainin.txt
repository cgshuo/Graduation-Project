 In this paper, we address the problem of nearest-neighbor (N N) search in large datasets of high sis of the classes in its close neighborhood. Non-parametri c density estimation uses NN algorithms when the bandwidth at any point depends on the k t X  NN distance (NN kernel density estimation [2]). NN algorithms are present in and often the main cost of most no n-linear dimensionality reduction techniques (manifold learning [3, 4]) to obtain the neighbo rhood of every point which is then pre-served during the dimension reduction. NN search has extens ive applications in databases [5] and computer vision for image search Further applications abou nd in machine learning.
 Tree data structures such as kd -trees are used for efficient exact NN search but do not scale b etter than the na  X   X ve linear search in sufficiently high dimensions. Distance -approximate NN (DANN) search, introduced to increase the scalability of NN search , approximates the distance to the NN and any neighbor found within that distance is considered to be  X  good enough X . Numerous techniques assumptions.
 Although the DANN search places bounds on the numerical valu es of the distance to NN, in NN search, distances themselves are not essential; rather the order of the distances of the query to the points in the dataset captures the necessary and sufficient i nformation [6, 7]. For example, consider non-informative dimensions to each of the reference points produces higher dimensional datasets tance approximation, raising the dimension increases the n umber of points for which the distance to the query (i.e. the origin) satisfies the approximation cond ition. However, the ordering (and hence the ranks) of those distances remains the same. The proposed framework, rank-approximate nearest-neighbor (RANN) search, approximates the NN in its rank rather than in its distance, thereby making the approximation independent of the distance distributio n and only dependent on the ordering of the distances. DANN search and the challenges they face in high dimensions. Section 3 introduces the proposed approach and provides a practical algorithm using stratifie d sampling with a tree data structure to obtain a user-specified level of rank approximation in Eucli dean NN search. Section 4 reports the experiments comparing RANN with exact search and DANN. Fina lly, Section 5 concludes with discussion of the road ahead. The problem of NN search is formalized as the following: Problem. Given a dataset S  X  X of size N in a metric space ( X, d ) and a query q  X  X , efficiently find a point p  X  S such that 2.1 Exact Search The simplest approach of linear search over S to find the NN is easy to implement, but requires O ( N ) computations for a single NN query, making it unscalable for moderately large N . Hashing the dataset into buckets is an efficient technique, b ut scales only to very low dimensional X . Hence data structures are used to answer queries efficientl y. Binary spatial partitioning trees, metric d (commonly the Euclidean distance metric) to prune away parts of the dataset from the com-putation and answer queries in expected O (log N ) computations [9]. Non-binary cover trees [12] answer queries in theoretically bounded O (log N ) time using the same property under certain mild assumptions on the dataset.
 Finding NNs for O ( N ) queries would then require at least O ( N log N ) computations using the trees. The dual-tree algorithm [13] for NN search also build s a tree on the queries instead of going through them linearly, hence amortizing the cost of search o ver the queries. This algorithm shows orders of magnitude improvement in efficiency and is conject ured to be O ( N ) for answering O ( N ) queries using the cover trees [12]. 2.2 Nearest Neighbors in High Dimensions The frontier of research in NN methods is high dimensional pr oblems, stemming from common datasets like images and documents to microarray data. But h igh dimensional data poses an inherent problem for Euclidean NN search as described in the followin g theorem: Theorem 2.1. [8] Let C be a D -dimensional hypersphere with radius a . Let A and B be any two points chosen at random in C , the distributions of A and B being independent and uniform over the distribution of r is N ( a  X  2 , a 2 / 2 D ) .
 This implies that in high dimensions, the Euclidean distanc es between uniformly distributed points lie in a small range of continuous values. This hypothesizes that the tree based algorithms perform no better than linear search since these data structures wou ld be unable to employ sufficiently tight bounds in high dimensions. This turns out to be true in practi ce [14, 15, 16]. This prompted interest in approximation of the NN search problem. 2.3 Distance-Approximate Nearest Neighbors The problem of NN search is relaxed in the following form to ma ke it more scalable: Problem. Given a dataset S  X  X of size N in some metric space ( X, d ) and a query q  X  X , efficiently find any point p  X   X  S such that for a low value of  X   X  + with high probability.
 This approximation can be achieved with kd -trees, balls trees, and cover trees by modifying the search algorithm to prune more aggressively. This introduc es the allowed error while providing some speedup over the exact algorithm [12]. Another approac h modifies the tree data structures to trees [14]. These obtain significant speed up over the exact method s. The idea of approximately correct (satisfying Eq. 2) NN is further extended to a formulation wh ere the (1 + ) bound can be exceeded with a low probability , thus forming the PAC-NN search algorithms [17]. They provi de 1-2 orders of magnitude speedup in moderately large dataset s with suitable and .
 These methods are still unable to scale to high dimensions. H owever, they can be used in combina-tion with the assumption that high dimensional data actuall y lies on a lower dimensional subspace. There are a number of fast DANN methods that preprocess data w ith randomized projections to reduce dimensionality. Hybrid spill trees [14] build spill trees on the randomly projected data to obtain significant speedups. Locality sensitive hashing [18, 19] hashes the data into a lower dimen-sional buckets using hash functions which guarantee that  X  X  lose X  points are hashed into the same bucket with high probability and  X  X arther apart X  points are hashed into the same bucket with low probability. This method has significant improvements in ru nning times over traditional methods in high dimensional data and is shown to be highly scalable.
 However, the DANN methods assume that the distances are well behaved and not concentrated in a any distance approximation  X  0 . 01 will return an arbitrary point to a NN query. The exact tree-based algorithms failed to be efficient because many dataset s encountered in practice suffered the same concentration of pairwise distances. Using DANN in suc h a situation leads to the loss of the ordering information of the pairwise distances which is ess ential for NN search [6]. This is too large of a loss in accuracy for increased efficiency. In order to address this issue, we propose a model of approximation for NN search which preserves the inf ormation present in the ordering of this form of approximation. To approximate the NN rank, we formulate and relax NN search i n the following way: Problem. Given a dataset S  X  X of size N in a metric space ( X, d ) and a query q  X  X , let such that D r  X  S : d ( r, q ) = D (1) is the NN of q in S . The rank-approximation of NN search would then be to efficiently find a point p  X   X  S such that with high probability for a given value of  X   X  + .
 RANN search may use any order statistics of the population D , bounded above by the (1 + ) t X  sample order statistics bound on the order statistics of the whole set.
 Theorem 3.1. For a population of size N with Y values ordered as Y y replacement. For 1  X  t  X  N and 1  X  k  X  n , We may find a p  X   X  S satisfying Eq. 3 with high probability by sampling enough po ints { d from D such that for some 1  X  k  X  n , rank error bound , and a success probability Sample order statistic k = 1 minimizes the required number of samples; hence we substitu te the values of k = 1 and t = 1 + in Eq. 4 obtaining the following expression which can be comp uted in O ( ) time The required sample size n for a particular error with success probability is computed using binary search over the range (1 + , N ] . This makes RANN search O ( n ) (since now we only need to compute the first order statistics of a sample of size n ) giving O ( N/n ) speedup. 3.1 Stratified Sampling with a Tree For a required sample size of n , we randomly sample n points from S and compute the RANN for a be pruned away for the query q during the tree traversal. Hence we can ignore the random sam ples from the pruned part of the tree, saving us some more computat ion.
 Hence let S be in the form of a binary tree (say kd -tree) rooted at R points. Let the left and right child have N the population D is the set of distances of q to all the N points in R population D into D set of distances of q to all the N node R node, subsequently providing a lower bound on the number of s amples required from the unpruned part of the tree without violating Eq.5 Theorem 3.2. Let n tively by doing a stratified sampling on the population D of size N = N required for Eq.5 to hold in the population D for a given value of . Then Eq.5 holds for D with the same value of with the random samples of sizes n D respectively if n probability n/N of inclusion. For n and similarly n Since the ratio of the sample size to the population size is a c onstant = n/N , Theorem 3.2 is generalizable to any level of the tree. 3.2 The Algorithm The proposed algorithm introduces the intended approximat ion in the unpruned portion of the kd -tree since the pruned part does not add to the computation in t he exact tree based algorithms. The algorithm starts at the root of the tree. While searching for t he NN of a query q in a tree, most of the computation in the traversal involves computing the dis tance of the query q to any tree node R pruned. The computations of distance of q to points in the dataset S occurs only when q reaches (C
OMPUTE B RUTE NN subroutine in Fig.2). The traversal of the exact algorith m in the tree is illus-trated in Fig.1.
 To approximate the computation by sampling, traversal down the tree is stopped at a node which can be summarized with a small number of samples (below a certain threshold M AX S AMPLES ). This is illustrated in Fig.1. The value of M AX S AMPLES giving maximum speedup can be obtained by cross-validation. If a node is summarizable within the desired err or bounds (decided by the C AN A PPROX -IMATE subroutine in Fig.2), required number of points are sampled from such a node and the nearest neighbor candidate is computed from among them using linear search (C OMPUTE A PPROX NN sub-routine of Fig.2).
 Single Tree. The search algorithm is presented in Fig.2. The dataset S is stored as a binary tree rooted at R leaf node is reached (since the tree is rarely balanced), the exact NN candidate is computed. In case a non-leaf node cannot be approximated, the child node close r to the query is always traversed first. The following theorem proves the correctness of the algorit hm.
 Theorem 3.3. For a query q and a specified value of and , STR ANK A PPROX NN ( q, S, , ) computes a neighbor in S within (1 + ) rank with probability at least . Proof. By Eq.6, a query requires at least n samples from a dataset of size N to compute a neighbor within (1 + ) rank with a probability . Let = ( n/N ) . Let a node R contain  X  R  X  points. In the algorithm, sampling occurs when a base case of the recursion is reached. There are three base cases: Let the total number of points effectively sampled from S be n  X  . From the three base cases of the algorithm, it is confirmed that n  X   X  X  X   X  N  X  = n . Hence the algorithm computes a NN within (1+ ) rank with probability at least .
 Dual Tree. The single tree algorithm in Fig.2 can be extended to the dual tree algorithm in case of O ( N ) queries. The dual tree RANN algorithm (DTR ANK A PPROX NN ( T, S, , ) ) is given in Fig.2. The only difference is that for every query q  X  T , the minimum required amount of sampling is done and the random sampling is done separately for each of the queries. Even though the queries do not share samples from the reference set, when a query node of the query tree prunes a reference work-sharing is a key feature of all dual-tree algorithms [1 3]. A meaningful value for the rank error should be relative to the size of the reference dataset N . Hence for the experiments, the (1 + ) -RANN is modified to (1 +  X  "  X  N  X  ) -RANN where 1 . 0  X  for maximum speedup can be obtained by cross-validation, fo r practical purposes, any low value (  X  20-30) suffices well, and this is what is used in the experimen ts. 4.1 Comparisons with Exact Search The speedups of the exact dual-tree NN algorithm and the appr oximate tree-based algorithm over the linear search algorithm is computed and compared. Diffe rent levels of approximations ranging from 0.001% to 10% are used to show how the speedup increases w ith increase in approximation.
STR ANK A PPROX NN ( q, S, , )
STRANN ( q, R, ) Figure 2: Single tree (STR ANK A PPROX NN) and dual tree (DTR ANK A PPROX NN) algorithms and subroutines for RANN search for a query q (or a query set T ) in a dataset S with rank approximation and success probability . R l and R r are the closer and farther child respectively of R from the query q (or a query node Q ) Covertype dataset 600k  X  55, Phy dataset 150k  X  78)[21], MN IST handwritten digit recognition dataset (60k  X  784)[22] and the Isomap  X  X mages X  dataset (700  X  4096)[3] are used. The final dataset  X  X rand X  is a synthetic dataset of points uniform randomly sa mpled from a unit ball (1m  X  20). This dataset is used to show that even in the absence of a lower-dim ensional subspace, RANN is able to get significant speedups over exact methods for relatively l ow errors. For each dataset, the NN of point in the dataset is found in the approximate case. These r esults are summarized in Fig.3. datasets, the low values of approximation used in the experi ments are equivalent to zero rank error (which is the exact case), hence are equally efficient as the e xact algorithm. NN algorithm, and the subsequent(dark) bars are the speedup s of the approximate algorithm with increasing approximation. 4.2 Comparison with Distance-Approximate Search In the case of the different forms of approximation, the aver age rank errors and the maximum rank errors achieved in comparable retrieval times are consider ed for comparison. The rank errors are compared since any method with relatively lower rank error w ill obviously have relatively lower distance error. For DANN, Locality Sensitive Hashing (LSH) [19, 18] is used.
 Subsets of two datasets known to have a lower-dimensional em bedding are used for this experiment -Layout Histogram (10k  X  30)[21] and MN IST dataset (10k  X  784)[22]. The approximate NN of every point in the dataset is found with different levels of a pproximation for both the algorithms. The average rank error and maximum rank error is computed for each of the approximation levels. For our algorithm, we increased the rank error and observed a corresponding decrease in the retrieval time. LSH has three parameters. To obtain the best retrieval times with low rank error, we fixed one parameter and changed the other two to obtain a decrease in ru ntime and did this for many values of the first parameter. The results are summarized in Fig. 4 and F ig. 5.
 The results show that even in the presence of a lower-dimensi onal embedding of the data, the rank errors for a given retrieval time are comparable in both the a pproximate algorithms. The advantage of the rank-approximate algorithm is that the rank error can be directly controlled, whereas in LSH, a particular retrieval time. Another advantage of the tree-based algorithm for RANN is the fact that even though the maximum error is bounded only with a probabil ity, the actual maximum error is not much worse than the allowed maximum rank error since a tree is used. In the case of LSH, at times, the actual maximum rank error is extremely large, correspon ding to LSH returning points which are very far from being the NN. This makes the proposed algori thm for RANN much more stable than LSH for Euclidean NN search. Of course, the reported tim es highly depend on implementation details and optimization tricks, and should be considered c arefully. We have proposed a new form of approximate algorithm for unsc alable NN search instances by con-trolling the true error of NN search (i.e. the ranks). This al lows approximate NN search to retain meaning in high dimensional datasets even in the absence of a lower-dimensional embedding. The proposed algorithm for approximate Euclidean NN has been sh own to scale much better than the exact algorithm even for low levels of approximation even wh en the true dimension of the data is relatively high. When compared with the popular DANN method ( LSH), it is shown to be compara-bly efficient in terms of the average rank error even in the pre sence of a lower dimensional subspace of the data (a fact which is crucial for the performance of the distance-approximate method). More-the actual maximum error to be within a reasonable rank thres hold unlike the distance-approximate method.
 However, note that the proposed algorithm still benefits fro m the ability of the underlying tree data structure to bound distances. Therefore, our method is stil l not necessarily immune to the curse of dimensionality. Regardless, RANN provides a new paradigm f or NN search which is comparably efficient to the existing methods of distance-approximatio n and allows the user to directly control the true accuracy which is present in ordering of the neighbo rs. [2] B. W. Silverman. Density Estimation for Statistics and Data Analysis . Chapman &amp; Hall/CRC, [3] J. B. Tenenbaum, V. Silva, and J.C. Langford. A Global Geo metric Framework for Nonlinear [4] S. T. Roweis and L. K. Saul. Nonlinear Dimensionality Red uction by Locally Linear Embed-[5] A. N. Papadopoulos and Y. Manolopoulos. Nearest Neighbor Search: A Database Perspective . [6] N. Alon, M. B  X  adoiu, E. D. Demaine, M. Farach-Colton, and M. T. Hajiaghayi . Ordinal Em-[7] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. Whe n Is  X  X earest Neighbor X  Mean-[8] J. M. Hammersley. The Distribution of Distance in a Hyper sphere. Annals of Mathematical [9] J. H. Freidman, J. L. Bentley, and R. A. Finkel. An Algorit hm for Finding Best Matches in [10] S. M. Omohundro. Five Balltree Construction Algorithm s. Technical Report TR-89-063, [11] F. P. Preparata and M. I. Shamos. Computational Geometry: An Introduction . Springer, 1985. [12] A. Beygelzimer, S. Kakade, and J.C. Langford. Cover Tre es for Nearest Neighbor. Proceedings [13] A. G. Gray and A. W. Moore.  X  N -Body X  Problems in Statistical Learning. In NIPS , volume 4, [14] T. Liu, A. W. Moore, A. G. Gray, and K. Yang. An Investigat ion of Practical Approximate [15] L. Cayton. Fast Nearest Neighbor Retrieval for Bregman Divergences. Proceedings of the 25th [16] T. Liu, A. W. Moore, and A. G. Gray. Efficient Exact k-NN an d Nonparametric Classification [17] P. Ciaccia and M. Patella. PAC Nearest Neighbor Queries : Approximate and Controlled Search [18] A. Gionis, P. Indyk, and R. Motwani. Similarity Search i n High Dimensions via Hashing. [19] P. Indyk and R. Motwani. Approximate Nearest Neighbors : Towards Removing the Curse of [20] J. Sedransk and J. Meyer. Confidence Intervals for the Qu antiles of a Finite Population: Simple [21] C. L. Blake and C. J. Merz. UCI Machine Learning Reposito ry. http://archive.ics.uci.edu/ml/, [22] Y. LeCun. MN IST dataset, 2000. http://yann.lecun.com/exdb/mnist/.
