 Learning ranking functions has been a major issue in the data mining and machine learning communities and produced many appli cations in information retrieval [13, 10, 15,28,23]. Task of learning ranking is different from regression or classification in terms that the training data in ranking is relative ordering or partial orders . For instance, let  X  X  is preferred to B X  be denoted as  X  X  B X . For a dataset D = { x 1 , ..., x m } ,an example of partial orders is { ( x 4 x 2 x 3 x 5 ) , ( x 2 x 1 x 6 ) , ... } . Partial orders do not contain a class label nor r eal value assigned to each object, thus neither classification or regression methods can learn from partial orders.
 Among rank learning methods (e.g., ranking SVM and RankBoost [13, 10]), ranking SVM has been favorably applied to various applications in information retrieval [13, 15, 23, 28, 29]. Ranking SVM learns a ranking function F from partial orders such that F ( x i ) &gt;F ( x j ) for any x i x j . (We briefly discuss literatures related to ranking SVM in Section 2.)
This paper develops new methods for learning ranking functions. We first develop a 1-norm ranking SVM, a ranking SVM that is based on 1-norm objective function. (Standard ranking SVM is based on 2-norm objective function.) The 1-norm ranking SVM learns a function with much less support vectors than the standard SVM. Thereby, its testing time is much faster than 2-norm SVMs and provides better feature selection properties. (The function of 1-norm SVM is likely to utilize a less number of features by using a less number of support vectors [11].) Feature selection is also important in ranking. Ranking functions are relevance or preference functions in document or data retrieval. Identifying key features increases the interpretability of the function. Feature selection for nonlinear kernel is especially challenging, and the fewer the number of support vectors are, the more efficiently feature selection can be done [12, 20, 6, 31, 8].
Next, we propose another ranking machine, Ranking Vector SVM (RV-SVM) ,thatre-vises the 1-norm ranking SVM for fast training. The RV-SVM trains much faster than standard SVMs while not compromising the accu racy when the training set is relatively large. The key idea of RV-SVM is to express t he ranking function with  X  X anking vec-tors X  instead of support vectors. The support vectors in ranking SVMs are the pairwise difference vectors of the closest pairs [28, 29]. Thus, the training requires investigating every data pair as potential candidates of support vectors, and the number of data pairs are quadratic to the size of training set. O n the other hand, the ranking function of the RV-SVM utilizes each training data object instead of data pairs. Thus, the number of variables for optimization is substantially reduced in the RV-SVM.

We experimentally compared the performance of RV-SVM with the state-of-the-art rank learning method provided in SVM-light [2]. RV-SVM trains much faster than SVM-light for nonlinear funtions. RV-SVM uses a substantially less number of support vectors than SVM-light. Their accuracies are c omparable when the size of dataset is rela-tively large. Our implementation of RV-SVM is posted at http://iis.postech.ac.kr/rv-svm.
This paper is organized as follows. We first review related work and ranking SVM in Section 2 and 3. We present our methods, 1-norm ranking SVM and RV-SVM, in Section 4 and 5 respectively. We discuss our experiments in Section 6 and conclude our study. SVMs have been actively researched for cla ssification and regression. Conventional classifying SVMs are based on 2-norm objective functions where the objective func-tion minimizes 2-norm of the weight vector [25, 21, 5, 9, 14]. By minimizing 2-norm of the weight vector, SVMs maximize the margin, which corresponds to improving gen-eralization of classification functions [25]. Training in the SVMs becomes a quadratic programming (QP) problem, and fast training algorithms have been developed for the 2-norm SVMs such as sequential minimal optimization (SMO) [21], reduced SVM [17], and cutting plane algorithm [16].

Learning ranking functions (relevance or preference functions) has recently gained much attention, and many methodologies have been proposed especially based on the SVM [13, 7] and the Ensemble approaches [10, 26, 22]. Ranking SVM is one of the earliest methodologies showing reliable performance, and it has been applied to many practical applications for information retrieval [15, 23, 28, 29, 7]. Ranking SVM is now typically used as the baseline method for rank learning research.
The formulation of ranking SVM is extended from that of the classifying SVM by replacing the data vectors with pairwise differ ence vectors [13]. This  X  X imple X  extension changes the meaning of the margin maximization into maximizing closest data pairs in ranking, which corresponds to improving t he generalization of ranking [28].
This extension also makes it possible to implement the ranking SVM using the same decomposition algorithms (e.g., SMO [21]) of classifying SVMs [15]. However, due to the use of pairwise difference vectors in training, the training complexity of ranking SVMs is inherently more expensive by two asymptotic orders of magnitude (with respect to data size) than that of classifying SVMs. This critical disadvantage is unavoidable because the number of data pairs are quadra tic to the number of data objects. Joachims published cutting plane algorithms for cla ssifying and ranking SVMs but his methods are limited to linear (classifying or ranking) functions [16].

This is the first paper that introduces 1-norm ranking SVM and its extension that are not inherently confined by the disadvantage of the  X  X ouble order complexity problem X  discussed above. We overview ranking SVM in this section. The training set in the ranking SVM is indicating the ranking of x i , such that x i R x j when y i &lt;y j .Wesay x i R x j if a vector x i is ranked higher than x j in an order R . We assume for simplicity that R is strict ordering, which means that for all pairs x i and x j in R , either x i x j or x
The goal of ranking SVMs is to learn function F that satisfies F ( x i ) &gt;F ( x j ) for beyond the training set. F ( x i ) is a scoring function that returns a ranking score of x i . When F is a linear function ( = w  X  x ), learning F becomes computing a weight vector, w , such that, for most pairs { ( x i , x j ): y i &lt;y j  X  R } ,
Note that ranking SVMs train a function fro m pairwise orderings that are generated from R . When the number of training vectors is m in R , the total number of pairwise orderings that can be generated is m ( m  X  1) 2 = O ( m 2 ) .

This goal of ranking SVMs is expressed in OPT 1 (primal form) [13, 15]. OPT 1 suppresses || w || 2 , which corresponds to improve the generalization by maximizing the margin. P (= m ( m  X  1) 2 ) is the number of the pairwise orderings in R ,andthereare P constraints for Eq.(5). Eq.(5) are to force Eq.(2).  X  ij is a slack variable and C is a parameter controlling the tradeoff between the margin size and the amount of error. Note that this primal form is similar to that of classifying SVM except that the left side of Eq.(5) is w  X  x i in the classifying SVM.

Since the support vectors in the classifying SVM are the vectors x i that are closest to the decision boundary, the support vectors in the ranking SVM become the pairwise closest pairwise difference vectors in ranking according to F [28, 29]. By minimizing the norm of w , OPT 1 maximizes the ranking of the closest data pairs, that corresponds to improving the generalization of the function.

To illustrate, see Figure 1 showing two different functions F w four data vectors { x 1 , x 2 , x 3 , x 4 } onto w 1 and w 2 respectively in a two-dimensional space. Both w 1 and w 2 make the same ordering R for the four vectors, that is, x 1 x 2 x 3 x 4 . The distance between the closest two projections onto x 1 and x 2 are respectively  X  1 and  X  2 , which are formulated as 1 || w
Although the two weight vectors w 1 and w 2 make the same ordering, intuitivelly w 1 is likely to generalize better than w 2 because the distance of the closest vectors in w 1 ( =  X  1 ) is larger than that in w 2 ( =  X  2 ). OPT 1, by minimizing the norm of w , computes the function that maximize the distance between the closest two projections [28].
Similarily to the classifying SVM, this primal form can be tranformed to the dual form (OPT 2), on which the kernel trick can be applied in order to support nonlinear ranking functions [13].  X  ij in OPT 2 is a coefficient for a pairwise difference vector ( x (  X  m 4 ) times, thus solving OPT 2 takes O ( m 4 ) at least. Once  X  is computed, w can be written in terms of the pairwise difference vectors and their coefficients such that:
The ranking function F on a new vector z can be computed using the kernel function replacing the dot product as follows: The goal of 1-norm ranking SVM is the same as that of the standard ranking SVM, that well beyond the training set. In the 1-norm ranking SVM, we express Eq.(1) using the F of Eq.(11) as follows.
 Thus, the constraint of the 1-norm ranking SVM becomes Eq.(16).

The standard ranking SVM suppresses the weight w to improve the generalization performance. (The objective function (4) minimizes the norm of w .) The weight w can be also suppressed by suppressing the coefficient  X  since the weight is expressed by the sum of the coefficient times pairwise ranking difference vectors (Eq.(9)). (Mangasarian proves in [18] that suppressing the coefficient  X  corresponds to suppressing the weight w in the standard SVM.)
Thus, we have OPT 3 for the 1-norm ranking which suppresses  X  instead of w in the objective function (Eq.(15)). C is a user parameter controlling the tradeoff between the margin size and the amount of error,  X  ,and K is the kernel function, e.g., K ( a, b )= a  X  b for linear kernel and K ( a, b )= exp (  X   X  || a  X  b || 2 ) for RBF kernel.
Training in the 1-norm ranking SVM becomes a linear programming (LP) problem thus solvable by LP algorithms such as the Simplex and Interior Point method [18, 11, 19]. Just as the standard ranking SVM (OPT 2), K needs to be computed P 2 (  X  m 4 ) times, and there are P number of constraints (16) and  X  to compute. Once  X  is computed, F is computed using the same ranking function as the standard ranking SVM, i.e., Eq.(12).

The accuracies of 1-norm ranking SVM and standard ranking SVM are comparable, and both methods need to compute the kernel function O ( m 4 ) times. In practice, the training of the standard SVM is more efficient because fast decomposition algorithms have been developed such as sequential minimal optimization (SMO) [21] while the 1-norm ranking SVM uses common LP solvers.
 It is shown that 1-norm SVMs use much less support vectors that standard 2-norm in 1-norm SVMs [19, 11]. It is because, unlike the standard 2-norm SVM, the support vectors in the 1-norm SVM are not bounded to those close to the boundary in classifi-cation or the minimal ranking difference vectors in ranking. Thus, the testing involves much less kernel evaluations, and it is more robust when the training set contains noisy features [32]. Our experiments show that the 1-norm ranking SVM uses much less sup-port vectors in the function than the standard ranking SVM. Although the 1-norm ranking SVM has merits over the standard ranking SVM in terms of the testing efficiency and feature selection, its training complexity is very high w.r.t. the number of data points. In this section, we present Ranking Vector SVM (RV-SVM), which revises our 1-norm ranking SVM to reduce the training time substantially. The RV-SVM significantly reduces the number of variables in the optimization problem while not compromizing the accuracy. The key idea of RV-SVM is to express the rank-ing function with  X  X anking vectors X  instead of support vectors. The support vectors in ranking SVMs are chosen from pairwise difference vectors, and the number of pairwise difference vectors are quadratic to the size of training set. On the other hand, the ranking vectors are chosen from the training vectors, thus the number of variables to optimize is substantially reduced. To theoretically justify our approach, we first present the Representer Theorem. Theorem 1 (Representer Theorem [24]) Denote by  X  : [0 ,  X  )  X  X  a strictly monotonic increasing function, by X a set, and by c :( X X R 2 ) m  X  X  X  X  X  X  an arbitrary loss function. Then each minimizer F  X  X  of the regularized risk admits a representation of the form We omit the proof due to the space limit, but the proof of the theorem is presented in [24].

Note that, in the theorem, the loss function c is arbitrary allowing coupling between data points ( x i ,y i ) , and the regularizer  X  has to be monotonic.

Given such a loss function and regularizer, t he representer theorem states that al-though we might be trying to solve the optimization problem in an infinite-dimensional space H , containing linear combinati ons of kernels centered on arbitrary points of X , the solution lies in the span of m particular kernels  X  those centered on the training points [24].

Based on the theorem, we define our ranking function F as Eq.(19), which is based on the training points rather than arbitrary points (or pair wise difference vectors). Function (19) is similar to function (12) except that, unlike the latter using pairwise difference ( x i ) and their coefficients (  X  i ). With this function, Eq.(13) becomes the following.
Thus, we set our loss function c as follows.
The loss function utilizes couples of data points penalizing misranked pairs, that is, it returns higher values as the number of misranked pairs increases. Thus, the loss function is order sensitive, and it is an instance of the function class c in Eq.(18).
We set the regularizer  X  ( || f || H )= increasing. Let P is the number of pairs ( u, v )  X  R such that y u &lt;y v ,andlet  X  uv = 1
Then, our RV-SVM is formulated as the LP of OPT 4, and the solution of the op-Eq.(19)) as suggested in the representer theorem.

Just as the 1-norm ranking SVM, the RV-SVM suppresses  X  to improve the general-ization, and forces Eq.(21) by constraint (24). Note that there are only m number of  X  i in the RV-SVM. Thus, the kernel function is evaluated O ( m 3 ) times while the standard ranking SVM computes it O ( m 4 ) times.
 We evaluate the RV-SVM on synthetic datasets (Section 6.1) and a real-world dataset (Secton 6.2). We compare the RV-SVM with the state-of-the-art ranking SVM pro-vided in SVM-light. Our experiments show that the RV-SVM trains substantially faster than the SVM-light for nonlinear kernels wh ile their accuracies are comparable. More importantly, the number of ranking vectors in the RV-SVM is multiple orders of magni-tudes smaller than the number of support vectors in the SVM-light. We performed our experiments on a Windows XP Professional machine with a Pentium IV 2.8GHz and 1GB of RAM. We implemented the RV-SVM using C and used CPLEX 1 for the LP solver. Our source codes can be found at  X  X  ttp://iis.postech.ac. kr/rv-svm X  [30]. Evaluation metric: MAP (mean average precision) is used to measure the ranking quality when there are only two classes of ranking [27], and NDCG is used to evaluate ranking performance for IR applications when there are multiple levels of ranking [3, 4, 7, 26]. Kendall X  X   X  is used when there is a global ordering of data and the training data is a subset of it. Ranking SVMs as well as the RV-SVM minimize the amount of error or mis-ranking, which is corresponding to optimizing the Kendall X  X   X  [15,28]. Thus, we use the Kendall X  X   X  to compare their accuracy. 6.1 Experiments on Synthetic Dataset Below is the description of our experiments on synthetic datasets. 1. We randomly generated a training and a testing dataset D train and D test respec-2. We randomly generate a g lobal ranking function F  X  , by randomly generating the 3. We rank D train and D test according to F  X  , which forms the global ordering R  X  train 4. We train a function F from R  X  train , and test the accuracy of F on R  X  test .
We tuned the soft margin parameter C by trying C =10  X  5 , 10  X  5 , ..., 10 5 , and used the highest accuracy for comparison. For the linear and RBF functions, we used linear and RBF kernels accordingly. We repeat th is entire process 30 times to get the mean accuracy.
 Accuracy: Figure 2 compares the accuracies of the RV-SVM and the ranking SVM from the SVM-light. The ranking SVM outperforms RV-SVM when the size of data set is small, but their difference becomes tri vial as the size of data set increases. This phenomenon can be explained by the fact that when the training size is too small, the number of potential ranking vectors beco mes too small to draw an accurate ranking function whereas the number of potential support vectors is still large. However, as the size of training set increases, RV-SV M becomes as accurate as the ranking SVM because the number of potential ranking vectors becomes large as well. Training Time: Figure 3 compares the training time of the RV-SVM and the SVM-light. While the SVM light trains much faster than RV-SVM for linear kernel (SVM light is specially optimized for linear kernel.), the RV-SVM trains significantly faster than the SVM light for RBF kernel.
 Number of Support (or Ranking) Vectors: Figure 4 compares the number of support (or ranking) vectors used in the function of RV-SVM and the SVM-light. RV-SVM X  X  model uses a significantly smaller number of support vectors than the SVM-light. Sensitivity to noise: In this experiment, we compare the sensitivity of each method to noise. We insert noise by switching the orders of some data pairs in R  X  train .Wesetthe size of training set m train = 100 and the dimension n =5 .Afterwemake R  X  train from a random function F  X  , we randomly picked k vectors from the R  X  train and switched it with its adjacent vector in the o rdering to implant noise in the training set. Figure 5 shows the decrements of the accuracies as th e number of misorderings increases in the training set. Their accuracies are moderate ly decreasing as the noise increases in the training set, and their sensitivities to noise are comparable. 6.2 Experiment on Real Dataset In this section, we experiment using the OHSUMED dataset obtained from the LETOR, the site containing benchmark datasets for ranking [1]. OHSUMED is a collection of documents and queries on medicine, consis ting of 348,566 references and 106 queries. There are in total 16,140 query-document pairs upon which relevance judgements are made. In this dataset the relevance judgements have three levels:  X  X efinitely relevant X ,  X  X artially relevant X , and  X  X rrelevant X . The OHSUMED dataset in the LETOR extracts 25 features. We report our experiments on the first three queries and their documents. We compare the performance of RV-SVM and SVM-light on them. We tuned the parameters 3-fold cross validation with trying C and  X  =10  X  6 , 10  X  5 , ..., 10 6 for the linear and RBF kernels and compared the highest pe rformance. The training time is measured for training the model with the tuned param eters. We repeated the whole process three times and reported the mean values.

Table 1 shows the results. The accuracies of the SVM and RV-SVM are comparable overall; SVM shows a little high accuracy than RV-SVM for query 1, but for the other queries, their accuracy differences are not st atistically significant. More importantly, the number of ranking vectors in RV-SVM is s ignificantly smaller than that of support vectors in SVM. For example, for query 3, the RV-SVM having just one ranking vector outperformed the SVM with over 150 support vectors. The training time of RV-SVM is significantly shorter than that of SVM-light. This paper first develops the 1-norm ranking SVM which uses a much less support vectors than the standard 2-norm ranking SVM. We then develop the Ranking Vector SVM (RV-SVM) which uses as less support vectors as the 1-norm SVM and trains much faster than the standard 2-norm SVM. Our experiments show that, compared to the SVM-light, our RV-SVM shows comparabl e accuracy on relatively large datasets, trains much faster, and uses a substantially less support vectors.

