 With the increasing popularity of micro-blogging and social networking, the docu-ments collected from the web are becoming more and more condensed. Social media Twitter limits the length of each Tweet to 140 characters. A personal status message on Windows Live Messenger is restricted to 128 characters. Most of the categories in Yahoo! Answers have an average post length of less than 500 characters. Thus uses of short-texts are increasing and it becomes nece ssary to relook at the existing text min-document is bag of words in which a document is represented as a vector of words whose entries are non-zero if the corresponding terms appear in the document. When the number of terms in a text is very small, the vector is very sparse and hence do not have enough statistical information for distinguishing different documents. Thus, though there have been very large number of algorithms for document clustering and classification, these are not found to be suitable for short-text document set. Short-text documents require different approaches in data mining and study on special algo-rithms for short text documents has evolved as a major research area of data mining in recent years [2, 10, 14, 18-19]. 
Recently, an efficient algorithm for cluste ring of short-text document is proposed [18]. Since different terms have different importance in a given document, a term-weight is normally associated with every term. These weights are often derived from the frequency of terms within a document or a set of documents. Most widely used term weighting scheme is tfidf which is a simple and efficient representation. Howev-er, it eliminates some contextual information such as phrases, sequential patterns. Other widely used term weighting methods [3-4, 11, 13] include: mutual information [13], ATC, Okapi [4] and LTU[3]. In [18], weights are derived from well-known normalized cut method [17] and it is shown that efficiency of clustering can be sub-stantially improved. Hypotheses advocated in this work are that the proposed ncut-assigning weights to documents. A major shortcoming of this method is that spectral clustering with such term-weighting can be achieved by non-negative matrix factori-is an implicit assumption of pairwise similarity derived from XX T . Choice of similari-several similarity measures proposed in literature to achieve more accurate clustering. A question that naturally arises is whether the proposed method can be extended with other similarity measures. 
The objective of this paper is to study different weighting schemes (not limiting to term weighting) and different similarity measures (not limiting to Cosine similarity) in order to devise an efficient clustering of short-text collection of documents. A new clustering algorithm is proposed through rigorous experiments, it is demonstrated that the proposed method yields better clustering than the other existing methods. Major contributions of the present work are the followings. 
In Section 2, we discuss ncut -weighting scheme proposed in [18]. We also discuss the background of our extension. In Section 3, the technique of weighted similarity is introduced. Section 4 outlines our proposed clustering algorithm which makes use of non-negative matrix factorization of Hadamard product of similarities matrices. We consider two similarity measures in our study. Section 5 discusses the performance measures for cluster quality and Section 6 outlines our experimental results. A set of N documents with M distinct terms can be represented as a matrix  X   X   X  X   X  X  X   X   X   X   X  X  X  X  . The element  X   X  X  X  of matrix X can be determined by many ways such as term frequency ( tf ) or term frequency and inverse document frequency ( tfidf ). We assume  X   X  X  X  as binary and defined as follows. diagonal matrix with diagonal elements  X   X  X  X   X   X   X   X  X  X   X   X  X  X  X  . A ncut-term-weighting is introduced in [18] and each row of X (corresponding to each term) is assigned a weight to obtain a weighted matrix Y as  X   X  X   X  X / X  . X  It is proved that when S = XX T , spectral clustering of documents can be achieved through nonnegative factorization of Y . It is shown that ncut-term-weighting yields better clus-tering compared to other well-known term weighting such as tf and tfidf . Authors emphasize here that weighting of terms have advantage over document-weighting. The similarity matrix S is a n X  n pairwise symmetric matrix of documents, where S ij particularly those of the spectral variety rely on a suitable similarity measure to clus-ter data points. The pairwise similarity matrix S = XX T is un-normalized form of co-sine similarity and can also be viewed as standard inner-product linear Kernel matrix. It is worthwhile to examine the suitability of other kernel function for short-text clus-tering. Cosine Similarity Given two documents columns X i and X j , the cosine similarity as Gaussian Similarity (Kernel Similarity) Given two documents columns X i and X j , the Gaussian Kernel is defined as follows. where  X  is a scaling parameter to control the rapid reduction in  X   X  X  X  with the distance in applicability in feature space of higher dimension. One can view the concepts of term-weighting and document-weighting as assigning each individual element of S separately. A similarity measure has its own inherent characteristics of capturing a specific type of statistical property. To take advantage of multiple characteristics exhibited by different similarity measures, we propose a me-thod to assign weights to similarity measure of a pair of documents by another simi-larity measure. We call this as weighted similarity . Given two similarity measures S 1 and S 2 , weighted similarity is defined as S 1  X  S 2 where  X  is Hadamard Product or ele-ment-wise multiplication of two matrices. We think of non-negative factorization of Y = S 1  X  S 2 . 
NMF has been investigated by many researchers, but it has gained popularity through the works of Lee and Seung published in Nature and NIPS [7-8]. Based on the argument that the non negativity is important in human perception they proposed simple algorithms (often called the Lee-Seung algorithms) for finding nonnegative representations of nonnegative data and images. The basic NMF problem can be physical meanings in different applications. In clustering problems, V is the basis matrix, while U denotes the weight matrix. The nonnegative factorization can be re-formulated as trace maximization problem as follows. With the above formulation, it becomes easy to see that when Y is a Hadamard product of two symmetric matrices, the NMF of Y boils down to an NMF of S 1 weighted by S 2 . There are many interesting properties of factorization of Hadamard products which we intend to explore separately. Some authors have proposed weighted NMF [5] but conceptually our method of weighting the factors through Hadamard product is different from weighting the errors of Y as UV . Weighted NMF (WNMF) was first used to deal with mi ssing values in the distance matrix for predicting distances in large scale networks [5]. In this section we propose a new method of clustering of documents of short length. The method is generic in the sense that it makes use of Hadamard product of any two similarity matrix. But, in the present contex t, we consider only Gaussian Kernel and Inner product linear Kernel for short-text clustering. The motivation for selecting was shown that the weighting of one of similarity with another yields better results [15-16]. We also experimented with several combinations of similarity measures but we observe the combination of Gaussian kernel with inner product yields very satis-factory results. We term this algorithm as Kernel-Cosine with NMF (KC-NMF). 
Using term-document matrix X, we compute similarity matrices S 1 and S 2 corres-ponding to two different similarity measures. The Hadamard product of S 1 and S 2 , S 1  X  S 2 is the weighted similarity matrix. We first normalize this similarity matrix by scaling each element by square-root of the row sum of the matrix as follows. We use the alternating non-negative least-square (ANLS) algorithm [6] to find NMF of Y. The following iterative process is used as ANLS. Above steps are repeated till a predetermined number of iterations or when U and V become stationary. Then we normalize U and scale V accordingly as follows. It may be noted that any scaling of U is essentially a post multiplication of a diagonal matrix and corresponding scaling of V is pre-multiplication of the inverse of the di-agonal matrix. Hence, such normalization k eeps the factorization invariant. Finally document clustering is achieved by assigning the document to a cluster which corres-ponds to maximum entry in each column of V. In order to evaluate clustering quality in terms of high intra-cluster similarity and low and the predefined classes are C 1 , C 2 , ... and C k . The measures purity , NMI and ARI are defined as follows. Purity For a given cluster C  X  i , we determine the dominant class as the class C j with the high-est number of common elements. Purity denotes the ratio of sum of common elements in dominant class to the total number of elements. When element of same class symbolizes good clustering while value near to 0 represents bad cluster. Normalized Mutual Information (NMI) Normalized mutual information is defined as the measure that allows us to evaluate the quality of clustering against the number of clusters. It lies in between 0 to 1 Adjusted Random Index The Rand Index measures the accuracy of pair wise decision i.e., the ratio of pair of objects, both of which are located in the same cluster and same class or both in differ-ent cluster or different class. Adjusted Random Index is the corrected for chance ver-sion of Random index. Its value lies in [-1,1]. Higher value shows more resemblance between clustering result and label. In order to evaluate the efficiency of the proposed method, we carried out experi-ments with several benchmark datasets. Some datasets are not text datasets but as our proposed method is on short text, we try to use the short text concept to those non-text datasets by assuming their characteristic vector as a term vector in the document. We deals with the sparse problem but also it is easier to implement. We have also used the combination of normal and short text for our experiments. We report here the expe-rimental results for the following datasets. Iris dataset Fisher X  X  Iris data base [22] is perhaps the best known database to be found in the pat-tern recognition literature. The dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two; the latter are not linearly separable from each other. Pendigit dataset PENDIGIT dataset is one of UCI benchmark data repository [21]. We consider one easy instance, namely PENDIGIT 01 with 2286 instances and one hard instance, namely PENDIGIT 17 with 1557 instances. AGBlog dataset AGblog [1] is an undirected hyperlink ne twork mined from 1222 political blogs. It contains 2 clusters pertaining to the liberal and conservative divisions. 20Ng dataset The 20 Newsgroups collection is an archive of 1000 messages from each of 20 differ-ent Usenet newsgroups. We have selected different versions of this dataset such as 20NgA, 20NgB, 20NgC and 20NgD. Dataset A contains 100 documents of two groups-misc for sale and soc.religion.christian . Dataset B is an expanded dataset with 200 documents of each class. Dataset C has three classes obtained from dataset B by appending an additional class talk.politics.guns with 200 documents. Similarly dataset D has 4 classes obtained by appending yet another class rec.sport.baseball with 200 classes to dataset C. 
Each of these datasets were preprocessed by removing the stopwords, stemming, and removing very highly frequent words. Here high frequent word signifies, words which are more common in the corpus have high frequencies than the word which are less common. Applying certain thresholds to this list of words, words which comes under the thresholds are considered to be important and they are used as an feature for the documents. We compare the performance of our method with two techniques namely, Normalized-Cuts (NC-NMF) [18] and the Ng-Jordan-Weiss algorithm (NJW) [12]. Since the present work is a sort of extension of NC-NMF, we feel it mandatory to compare the performance of our algorithm with NC-NMF. NJW clustering is one of the most cited [9] clustering techniques for documents and we consider this as our benchmark for performance evaluation. For each data set, experiments were carried out with different values of k in the range [2, 15] in steps of 2. We experimented 50 times for each combination of the parameters. We take the best value among all com-binations. The experimental results comparing purity, NMI and ARI of three algo-rithms are summarized in Table 1. The last row of Table 1 gives the average value of these measures when average is taken over all datasets. It can be seen that except for Pendigit01 and for 20ngA, the proposed method yield better results. Datasets k KC-NMF NC-NMF NJW IRIS 3 0.9067 0.7337 0.6911 0.6733 0.7235 0.7779 0.7667 0.6083 0.797 PEN_DIGIT01 2 0.9987 0.9313 0.9619 1.000 1.000 1.000 1.000 1.000 1.00 PEN_DIGIT17 2 0.9724 0.4376 0.4113 0.7550 0.2066 0.6301 0.7550 0.2043 0.630 AGBLOG 2 0.9484 0.7955 0.7003 0.5205 0.0060 0.5006 0.5205 0.0006 0.500 20ngA 2 0.9600 0.7236 0.8081 0.9600 0.7594 0.9232 0.9600 0.7594 0.923 20ngB 2 0.9400 0.6829 0.7735 0.5050 0.0096 0.5001 0.5523 0.0842 0.505 20ngC 3 0.8050 0.3668 0.3384 0.6183 0.3295 0.6750 0.6317 0.3488 0.686 20ngD 4 0.8800 0.6746 0.7031 0.4750 0.2385 0.6312 0.5150 0.2959 0.682 k defines the predefined labels present in the data set . A detailed experimental comparison is carried out between KC-NMF and NC-NMF for each dataset and for each of the three measures, the performance of two algorithms are compared for different values of K. 
Figures 1-3 depicts three cluster measures for KC-NMF and NC-NMF for 20ngA dataset for different values of k. It can be seen that there is a noticeable improvement of performance of KC-NMF in terms of purity. Figures 4-6 give the similar results for 20ngB dataset. It can be seen that KC-NMF performs better for all values but im-provement over NC-NMF decreases for higher values of k. Figures 7-9 depict similar results for dataset 20ngC. It is interesting to note that for dataset 20ngD, Figures 10-12, there is a significant improvement in terms of all the three measures. Mixed per-formance is observed for AGBlog dataset (Figures 13-15). 
We carried out another set of experiment to analyze the efficacy of combining two similarity measures. In this experiment, we compare the clustering performance of S 1  X  S 2 with S 1 and S 2 individually. We use eight datasets for this and Table 2 summa-rizes the three external measures of clustering quality for KC-NMF with K-NMF (taking only kernel similarity measure) and C-NMF (taking only Cosine similarity benchmark datasets and hence, we conclude that by taking Hadamard product of two similarity measures we get better result of clustering than clustering obtained by taking individual similarity measure. 
Datasets k KC-NMF K-NMF C-NMF PEN_DIGIT01 2 0.9987 0.9313 0.9619 0.9983 0.5313 0.4194 0.9517 0.5941 0.7359 PEN_DIGIT17 2 0.9724 0.4376 0.4113 0.9531 0.3904 0.3276 0.8750 0.4230 0.5173 
AGBLOG 2 0.9484 0.7955 0.7003 0.7070 0.2423 0.1197 0.9422 0.3953 0.3975 
Average 0.9264 0.6682 0.6734 0.7093 0.4230 0.3706 0.8823 0.5094 0.5358 In this paper a new algorithm for clustering of short-texts is proposed and it is shown that the new algorithm yields better clustering for benchmark dataset. The new tech-nique makes use of Hadamard product of similarity matrices and it is viewed as weighted similarity. This improves the earlier technique of ncu t-term-weighting pro-posed recently. In our future work, we propose to investigate theoretical advantages of non-negative factorization of Hadamard product of two symmetric matrices. 
