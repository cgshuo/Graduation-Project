 Most cross-domain sentiment classification techniques consider a domain as a wh ole set of instances for training. However, many online shopping websites organize their data in terms of taxonomy. This paper takes Amazon shopping website as an example , and proposes a tree-structured domain representation scheme in which each node in the tree is encoded as a bit sequence to preserve its relationship with all the other nodes in the tree . To select an appropriate source node for training in the domain taxonomy, we propose a Taxonomy-Based Regression Model (TBRM) which predicts the accuracy loss from multiple source nodes to a target node using the tree-structured domain representation combined with domain similarity and domain complexity. The source node with the smallest accuracy loss is used to train a classifier which makes a prediction on the target node. The results show that our TBRM achieves better performance than the regression models without considering the taxonomy information. I.2.6 [ Arti ficial Intelligence ]: Learning Domain Adaptation; Sentiment Classification; Opinion Mining Due to large amount of users  X  experiences shared on the Web , mining opinions for some specific targets attracts much attention in recent years [1]. Sentiment classification extracts a set of reviews along with their polarities in some specific domain and predicts the polarity of an unseen review using a trained classifier. Domain dependency may drop the classification performance sharply when the distributions of the feature space in the source and the target domains are different. To reduce the performance loss caused by the distribution gap, approaches of tra ining a classi fier based on one or more source domain data and transferring the knowledge to the target domain have been proposed . The major challenging issues in cross-domain sentiment classification are two folds. On the one hand, the nature of human language carries plenty of domain-speci fic words in reviews . On the other hand, there are often suf ficient amount of labeled data in the source domain, but no or very limited amount of labeled data in the target domain. The cross-domain sentiment classification problem has been denote positive and negative sentiments respectively. With the taxonomy, the cross-domain means the source and the target nodes belong to distinct domain trees, and the cross-domain sentiment classification is defined as the task of learning a classifier using the labeled source domain data to make predicti ons on the unlabeled target domain data . We downloaded a collection of product reviews from three different domains in Amazon, i.e., electronics ( E ), books ( B ) and kitchen ( K ), and adopted a three-leveled tree structure for each domain 1 . It is easy to extend our method to the whole directory of Amazon shopping website. The root is in level one ( L1 ), with child nodes in level two ( L2 ), and the leaf nodes are in level three ( L3 ). Each domain tree is built from the leaf nodes. Each L3 node contains 1,000 positive reviews and 1,000 negative ones. For each node in L2 , we randomly extract 1,000 positive and 1,000 negative reviews from its child nodes. Similarly, we randomly fetch 1,000 positive and 1,000 negative reviews for the root from the leaf nodes . The positive and the negative reviews in each node are balanced. Each review instance in the source domain contains a review data written by some user and a sentiment value (i.e., number of stars) ranging from 1 to 5. Reviews with 1-2 stars are regarded as negative reviews while those with 4-5 stars are regarded as positive ones. T he number of nodes and the number of reviews in each level for each domain is illustrated in Table 1. This section provides a preliminary cross-domain sentiment analysis on the tree-structured dataset. We keep only unigrams with term frequency greater than or equal to 5 for the SVM classifier. L IBSVM 2 is adopted as our analyzing tool and its embedded parameter adjusting tool is used for parameter optimization. In order to show the gap of prediction accuracies from different source nodes to some target node, we construct six cross-domain B -E , where the former and the latter denote the source and the target domain trees, respectively. For each node in the target tree, we make predictions from all source nodes. Figure 1 shows the best and the worst prediction accuracies for E -K and B -K . The other four tasks have the similar gaps. Intuitively, kitchen domain is closer to electronic domain than to book domain. When the source-target pair is closer, the prediction performance is better. Thus, E-K_best is better than B-K_best and E-K_worst is better than B-K_worst in every target node. Moreover, the minimum differences between E-K_Best and E-K_Worst, and B-K_Best and B-K_Worst are larger than 7.1% and 7.7%, respectively. Even in the same domain, there are still distinctions among nodes. Thus, the choice of suitable source (sub)-domain is important. Besides the domain difference, we further examine the average prediction performance from SourceDomain-SourceLevel to TargetDomain-TargetLevel, and show the results in Table 2. If we Will be available at http://nlg.csie.ntu.edu.tw/sentiment/ http://www.csie.ntu.edu.tw/~cjlin/libsvm/ In this section, we describe our tree node encoding method and the use of the tree information to build the regression model. As mentioned in the introduction, when we have multiple source domains, an essential problem is how to select good source domain data. There are two domain properties introduced in [8] , namely domain similarity and domain complexity variance . Ponomareva and Thelwall effectively utilize the se two properties to assist predicting the accuracy loss on the target domain. Domain similarity measures similarity of data distributions for frequent words , while the domain complexity variance compares the tails of distributions . They used  X  2 distance as the measure of domain similarity and used the percentage of rare words (words that occur less than or equal to 3) as domain complexity. Their experiments demonstrated that these two properties have high correlation with the accuracy loss. However, under the taxonomy framework, it is still unknown whether their results can be further improved by adding the tree information feature to the regression model. The first step to add the tree information feature is to encode the taxonomy. There are many relationships in a tree, such as parent-child, siblings , grandparent-grandchild, and so on. In order to preserve the relationship among all the nodes in the tree, we consider an encoding scheme in which each node is represented as a bit sequence with length equal to the number of nodes in the tree . Figure 3 shows an example tree labeled with the node numbers . The tree node is encoded as follows. Firstly , all the bits in each sequence are initialized to zero except that the bit whose position (counting from left-to-right) equals to the node number is set to 1. Below is the result after the first step: Node 1: 10000000 Node 2: 01000000 Node 3: 00100000 Node 4: 00010000 Node 5: 00001000 Node 6: 00000100 Node 7: 00000010 Node 8: 00000001 Secondly, we add the ancestor relationship into the bit sequence. For each bit sequence, we update all the bits representing its ancestors  X  nodes to 1. For example, Node 4 has two ancestors, i.e., Node 2 and Node 1, so we update its 2 nd bit and 1 st bit to 1. Below is the result after this step: Node 1: 10000000 Node 2: 11000000 Node 3: 10100000 Node 4: 11010000 Node 5: 11001000 Node 6: 10100100 Node 7: 10100010 Node 8: 10100001 After the first two steps, the 1 st bit in all the bit sequences becomes 1, which indicates that Node 1 is the root. Node 2 and Node 3 have the 1-bit in the first position, and their second 1-bit s are in different positions. It means they are Node 1  X  s children. The bit sequences of Node 4 and Node 5 hold both the properties of Node 1 and Node 2 (Figure 4). Comparatively, Node 4 and Node 6, which share the same root but distinct parents (i.e., they own the first cousins relationship), only have the 1 st bit in common (Figure 4). By this scheme, we can make every bit sequence in the tree has its ancestors  X  properties and make the sibling relationship (e.g., Node 4 and Node 5) and first cousins relationship (e.g., Node 4 and Node 6) distinguishable. Each regression model uses the pair (SourceNode  X  TargetNode): AccuracyLoss as a training instance. We build a regression model to select one source node which has the smallest predicted accuracy drop and use this node as the source data to train a sentiment classifier to make predictions on the target node . The experimental results are shown in Table 3. Each column shows the average accuracies from source domains to all levels of target domain. The results show TBRM , which uses two baseline features and one tree information feature, is the best among the six cross-domain regression models . Note the performance of TBRM is even better than that of InDomain except the cases (( K , E )  X  B ). We further analyze the performance between (i) Chi-CompDrop vs. TBRM and (ii) 5Sim-2Comp vs. Tree-5Sim-2Comp, and find that the tree information is indeed a very helpful feature for selecting a good source node . However, OnlyTree cannot accurately predict a good source node. It might be because OnlyTree uses only the tree structure to predict the accuracy loss and the information is not enough to capture the source and target nodes  X  properties. Another interesting point is that the OnlyTree feature is particularly useful when the target domain is Book. In that case, the difference between source and target nodes is larger, and the performance of OnlyTree is closer to BestSource. Table 4 averages the results of Table 3 in the same level but different target domains. We can see that the proposed TBRM has the best performance in all the target levels. That demonstrates the idea of using the tree information to select the source node is very helpful under the taxonomy-based multiple source domains. This paper presents the use of the tree-structured domain taxonomy to deal with the cross-domain sentiment classification problem . The cross-domain sentiment analysis under taxonomy tree structure shows that the prediction performance is affected by the levels, the nodes, and the diversity of domains. According to the preliminary analysis, we propose the TBRM to select good source node. Two major contributions are summarized as follows: (1) we represent the domain by the tree-structured domain taxonomy , 
