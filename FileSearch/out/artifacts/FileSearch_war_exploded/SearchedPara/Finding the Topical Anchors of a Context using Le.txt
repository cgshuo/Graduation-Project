 Lexical cooccurrence in textual data is not uniformly ran-dom. The statistics inferred from the term-cooccurrence data enable us to model dependencies between terms as graphs, somewhat resembling the way semantic memory is organised in human beings. In this paper we look at cooccur-rence patterns to identify topical anchors of a given context. Topical anchors are those terms whose semantics represent the topic of the whole context. This work is based on com-puting a stationary distribution in the cooccurrence graph. Topical anchors were computed on a set of 100 contexts and were also evaluated by 86 volunteers and the results show that the algorithm correctly identifies the topical anchors around 62% of the time.
 Categories and Subject Descriptors: I.2.6 [Learning]: Knowledge Acquisition; I.2.7 [Natural Language Process-ing]: Text Analysis General Terms: Algorithms, Theory
While information retrieval (IR) is based on managing the lexical properties of terms, humans interacting with such systems are more interested in the semantic properties. For instance, the term car is lexically closer to terms like card or cat , while it is semantically closer to the term automo-bile . Underlying data structures in large-scale IR systems based on vector space models are efficient for finding lexi-cal similarity, but are not adequate for computing semantic similarity. On the other hand, a major source of knowledge about semantic relatedness without recourse to linguistics, is to observe cooccurrence patterns across terms.

This work addresses a specific problem of this nature  X  that of identifying words that are representative of a con-text. These words, called topical anchors , are words which are central to a topic and can best represent the topic in other contexts. For example, if a document has words like Roger Federer , Rafael Nadal and Wimbledon , it would be very useful if their association with Tennis can be mined, even if the word Tennis does not appear in the document. Tennis , here acts as the topical anchor for these set of words.
Humans are generally adept at identifying the context of a document or a discussion from very few words present in the context. For example, a human reading a question like,  X  X hat are the symptoms of type 2 diabetes and do oral insulin drugs have any side effects on the health of my pan-creas? X  , can immediately understand that the question is posed in the medical context and she can decide that the question does not belong to another context say, Computer Science . In this work we primarily focus on finding topical anchors like medicine given just a few words from a context to aid machine understanding. For example, they can be used in on-line forums to determine the topic of a discus-sion thread and classify the thread accordingly  X  even if the topical anchor itself never appears in the thread.

Traditionally, IR models based on vector spaces implicitly assume independence with regard to occurrences of words in a textual corpus, i.e., a word can occur in a document on any topic with equal probability. However, it had also been acknowledged [7] that the lexical cooccurrences are not uniformly random and that the cooccurrence dependencies are essential in improving the quality of results.

Cognitive sciences believe that the semantic memory in human beings is formed through a process of co-activations of concepts [2, 3]. They also hold the belief that meaning is not inherent in a word but rather evolves out of its usage [2, 4] with respect to other words and each word acts as a point of access to these other words 1 .

Based on these assumptions, we model the basic data structure for capturing semantic relatedness in the form of a cooccurrence graph. This work focuses on finding the topical anchors of a context defined in the cooccurrence graph.
Given a document corpus consisting of many documents, the problem we address is the following: Given a set of terms representing an unknown semantic context, find a term in the corpus that best labels the semantic context.

In order to answer this question, we in turn adopt the following hypothesis: Given a set of terms Q belonging to an (unknown) context C , the topical anchor is that term which has the highest probability of mention in any document or dialogue involving C .
Refer to Sahlgren X  X  work [6] for a philosophical argument on  X  X eaning is usage X  and thus cooccurrences can be used to extract semantics.
In this paper, we proceed to test this hypothesis based on a cooccurrence graph as the core data structure and reach-ability as the property of topical anchors.
 Cooccurrence Graph. The basic data structure for this work is a cooccurrence graph . This is an undirected graph where nodes represent terms in the corpus and edges repre-sent a cooccurrence count. Terms stored in the cooccurrence graph are noun phrases that are appropriately stemmed.
The cooccurrence graph G is a graph defined as, G = ( V,E,w ) where, V is the set of all nodes representing terms, E is the set of all edges representing cooccurrence and w : E  X  N is a weight indicating the cooccurrence count. When two words a and b cooccur in a document a node correspond-ing to each word is added to the cooccurrence graph, if such a node does not exist already. Also an edge between them e a,b is added if there is no edge, else the edge weight between the nodes is incremented.
 Semantic Context. A  X  X ontext X  in a cooccurrence graph is any subgraph containing terms and cooccurrences that can be used to describe a specific semantic universe. Given a cooccurrence graph it is not apparent what parts of the graph form semantic contexts. The first step towards iden-tifying a semantic context is from the query terms . Query terms are terms extracted from an information source like a discussion thread or a document whose topical anchor we wish to identify.

Given a term a in a cooccurrence graph G , the neighbour-hood of a denoted by N a is defined as the set of all nodes which share an edge with a . We then note that, when terms from the same semantic context are used as query words, there is an overlap in their neighbourhoods. Suppose we are given a set of query terms Q that represent terms from an unknown context C , we say that C is a non-trivial semantic context if \
When a set of query terms de-fine a non-trivial context, for the purposes of determining topical an-chors, we build a context subgraph whose node set V C is calculated as: This is schematically shown in Figure 1. The grey area represents the neighbourhood of query word q .
 Reachability and Centrality. As hypothesised earlier, a topical anchor of a context C is a term that is most likely to be mentioned in any document or dialogue involving terms in C . This hypothesis corresponds to calculating centrality of terms based on their reachability.

In the cooccurrence graph, a pair of cooccurring terms a and b are connected by an edge indicating the cooccurrence count. Using this, we can calculate the probability that b cooccurs with a as follows:
Here e a,b is the cooccurrence count of the pair of terms a and b . Given a context C , a topical anchor t is one such term which has the highest probability of cooccurring with all terms in the context. If a random walker were to walk the cooccurrence graph the topical anchor can be thought of as the term which is most reachable from other terms for the random walker. Random walking is a well-known technique on hyperlink graphs to compute an ordering of the nodes in the graph based on their reachability [1, 5].

For topical anchor computation, we start with a set of query terms Q , build a context subgraph C based on the terms in Q , and then use this subgraph to find the most reachable term.

A well-known algorithm for computing reachability over hyperlink graphs is PageRank [5]. An incremental equiva-lent of PageRank is the On-line Page Importance Computa-tion algorithm or OPIC [1]. In OPIC, each node is initially assigned a cash value c i . Then in an infinitely long process, nodes are randomly chosen, and their cash is uniformly dis-tributed among all the outgoing links. Each node i also keeps track of the history of cash flow h i through the node.
After every distribution, cash keeps getting accumulated at the target nodes till the node is picked for distribution. When distributing the cash, the current cash value is added to the cash flow history of the node.
 The random walk probabilities are then computed as:
The OPIC computation is said to have reached a fix-point when the ordering of nodes according to random walk prob-abilities stabilises.

Centrality computation for topical anchors is also based on the OPIC model, but with some significant changes. For topical anchors, we are given a set of query terms Q out of which we extract a subgraph C that is representative of our context. Also, cash is distributed according to the proba-bility of cooccurrence defined by  X  (equation 2) rather than with a uniform distribution.

In contrast to OPIC computations, for finding topical an-chors, we need to compute centrality of nodes only within a subgraph C and not across the entire cooccurrence graph.
There are two ways to address this: (a). Remove the subgraph C from the cooccurrence graph and run OPIC on C as if it were a closed world in itself, or (b). The proposed algorithm  X  Run OPIC on C , but allow random walkers to walk out of C into the larger graph, thus discarding them. In the former case, the centrality measured of nodes in C eventually converges to the principal eigen vector of the adjacency matrix of C . In the latter case, the OPIC compu-tation converges to some stationary distribution reflecting centrality that need not necessarily be the principal eigen vector.

In our experiments (section 3), we have found the latter option to be a better choice for cooccurrence graphs. This is due to the characteristic nature of cooccurrence as well as the way we build our initial context C . This is explained in detail in the following sub-section.
 Cash Leaking Random Walk. A characteristic feature of cooccurrence graphs over reasonably large corpora is its extremely small diameter. In addition, cooccurrence graphs contain a number of terms with very high degree. These could be commonly occurring nouns that are seen in al-most every document, or polysemic terms that have differ-ent meanings in different contexts. In our experiments over a cooccurrence graph created from a Wikipedia dump, we found that a term like United States has edges to 78,738 other nodes, in other words more than 11% of the nodes in our data set. In two hops from United States 695,862 nodes were reachable i.e., more than 98% of the graph was covered.
Another example would be a word like football which is very popular and is connected with many other words. Any game and any player has a very high probability of being in the context of football at least once and hence will share an edge, however insignificant, with football in the cooccur-rence graph. In fact, in our experiments, given a context around Roger Federer and Rafael Nadal , football was found to be more central to the context than tennis . The problem arises due to the differences in popularity of the two sports. Football occurs in a document corpus very rarely in the con-text of tennis but still due to its large number of edges, a tiny fraction of them will be large enough to act as a central node in the tennis context. The random walk should take into account not just the number of cooccurrence edges of a node in the context, but also the percentage of edges in the cooccurrence graph from a node which are in the subgraph. A word like football has a small percentage of edges coming into the tennis context and hence it should be penalised.
It might appear that a heuristic like TF-IDF would solve the problem but in our experiments (ref. section 3) we found that TF-IDF picks important words but words which are not the topical anchors.

To account for the elimination of nodes like football we propose a different method  X  that of a cash leaking random walk. The traditional random walk is usually performed on the whole graph G whereas a cash leaking random walk is run on a subset of the graph C . The nodes in the subgraph C can participate in two types of edges; edges which lead to nodes inside the subgraph ( E C ) and edges which lead to nodes outside the subgraph ( E C = E \ E C ).

A traditional random walk on the subgraph would ignore all edges other than E C as shown in equation 4. In a cash leaking random walk, these edges in E C are also consid-ered but, all the cash that flows from nodes in the subgraph ( V
C ) to nodes outside the subgraph ( V C ) is deleted from the system as given by equation 5, such that the system continuously loses cash and hence the name.
One of the implications of such a random walk is that the nodes which have a high percentage of cash flow into the subgraph, contribute more to the history of other nodes than the nodes which have a low percentage of cash flow into the subgraph. The percentage computation is significant because, the degree distribution between all nodes in the subgraph is not uniform, hence nodes that have a high degree into the subgraph tend to become important in a traditional random walk. In a cash leaking random walk their effect is normalised because the percentage of contribution into the subgraph is taken into account instead of the edge weights.
Technically, a topical anchor is the term which is most reachable from all the terms in a context. If a random walker starts from one of the nodes of the context subgraph in the cooccurrence graph and keeps visiting other nodes, the node which is visited most will be the topical anchor of the con-text. The history h i of node i corresponds to the frequency of node i being visited and it enforces an ordering of all nodes in the context subgraph. The m nodes with the high-est probabilities are then chosen as the topical anchors of the context where m is a user defined threshold. Data Set and Preprocessing. The experiments were run on a cooccurrence graph built using a dump of the English language version of the open source encyclopedia, Wikipedia obtained in December 2006. The dataset was cleaned by removing all the stub pages and in each of the pages the trivia sections, popular culture references, tables, info-boxes and general references were removed from the text. In the experiments, cooccurrence was measured between all noun phrases and not between all words.

Each section in a document is treated as a set of enti-ties and is added to the cooccurrence graph separately. If two words cooccur in three different sections in a document, then when the document is processed their cooccurrence is incremented by three. The number of times a word occurs in a section is completely ignored because the cooccurrence inside a section is taken to be either present or absent.
There were 81,253 documents in all, giving rise to a cooc-currence graph with 709,220 nodes and 19,686,591 cooccur-rence edges.

All the cleaned entries in Wikipedia were used to con-struct an offline cooccurrence graph. The on-line computa-tion takes the query words as input and uses them along with the cooccurrence graph to identify a neighbourhood for the query. Using these neighbourhoods the context subgraph is built. A cash leaking random walk along with history ranking yields the topical anchors for the query.
 Experimental Setup. A set of 100 human generated con-text queries were chosen and 86 volunteers were asked to write down at most three topical anchors for each of the queries. The answers exhibited a considerable amount of synonyms and spelling errors. We cleaned the answers given, by dividing them into various semantic buckets. For exam-ple, with regard to monarchy the words empire , dynasty , kingdom , and rule are only minor differences in perception and hence were put in the same bucket. Words like marine , ocean and sea were put in their semantic bucket but words like film and Hollywood though related were kept apart. Af-ter mapping words into semantic buckets, the semantic buck-ets were termed as the topical anchors that the users chose. These user generated topical anchors were used to evaluate the topical anchors generated by the algorithm.

In the experiment, the validity of a cash leaking random walk was tested against a conventional random walk like OPIC [1] and with a baseline like TF-IDF. The topical an-chors generated using all the algorithms are compared using the user feedback.

For the experiment, we partitioned the topical anchors given by the users into confidence intervals based on the percentage of users endorsing a topical anchor. A confidence interval from x to x + c is a bucket where x % to ( x + c )% of users agree that the terms in the bucket are topical anchors for a query. For example, if 95% of the users answer com-puter for the query , X  CPU, hard disk, monitor, mouse  X  then we put computer into the 90-100 confidence interval. The confidence intervals are in steps of 10 starting from 40-50 and going up to 90-100. The user generated topical anchors which are not in any of the confidence intervals above 40 are ignored because of the lack of adequate support. The distribution of user generated topical anchors based on the confidence interval to which they belong is shown in figure 2 marked in gray.

With a confidence cut-off at 40, there are a total of 156 topical anchors, which are in confidence intervals of 40 and above, across all the 100 chosen queries. Some queries like,  X  summer, winter, spring, autumn  X  have just one anchor sea-son and some others like  X  Volt, Watt, Ohm, Tesla  X  have several user given anchors like unit, electricity, physics .
In the experiments, hit-rate is defined as the number of anchors picked by an algorithm in a confidence interval out of the 156 assigned and it is used as the measure of compar-ison. Formally, hit-rate HR a = P Q | t a ( Q )  X  t u ( Q ) | , where t is the set of topical anchors picked by the algorithm for a query Q and t u are the set of topical anchors picked by a user for the same query.

We evaluated the hit-rate of three different algorithms for computing the random walks. First we used a variant of TF-IDF to identify the most important nodes in a subgraph. As there is no notion of a document in the cooccurrence graph we had to use an interpretation of TF-IDF which is similar in spirit. Term Frequency (TF) of a node for a context was defined as the sum of all the edge weights of edges to nodes in the context . Inverse of Document Frequency (IDF) of a node was defined as the log of the ratio between the sum of all the edge weights of edges from all the nodes in the context to the sum of all of edge weights of edges from the given node. The product of TF and IDF was used as the score of a node as shown in equation 6.

TF ( i,C )  X  IDF ( i,C ) = X
Note: In equations 6 e a,b = 0 if there is no edge between a and b .
 Figure 2: Comparison between TF-IDF and random walks with and without cash leakage
We also compared two random walks based on OPIC, one in which the context C is considered a closed world graph with no edges to the other nodes G \ C in the graph and another in which the edges to outside nodes ( G \ C ) are considered as the edges through which the cash leaks out of C . The former is called opic and the latter cl where cl stands for cash leakage. For each of the three algorithms we computed the hit-rate for comparison. Each algorithm was executed in three different variants based on the number of topical anchors they choose. The hit-rate of tfidf 1 was computed where the 1 stands for picking only one topical anchor per query. Then the hit-rates of tfidf 3 and tfidf 10 were also computed where the number of topical anchors generated by the algorithms are 3 and 10 respectively. The same procedure was repeated with opic 1 , opic 3 and opic 10 and cl 1 , cl 3 and cl 10 .

On the whole, there were 3 different algorithms computed 3 times with varying number of topical anchors on each of the 100 test queries. The results were plotted with the con-fidence intervals for the topical anchors on the horizontal axis and the hit-rate in the vertical axis and are presented in figure 2.

The plot in figure 2 compares the number of user gener-ated topical anchors with the hit-rates of all the three algo-rithms. The results show that while tfidf 10 and opic 10 had an overall hit-rate of only 40 and 56 respectively, i.e., tfidf 10 could only pick 40 correct topical anchors and the random walk cl 1 performed much better with a hit-rate of 63. To emphasise, the cash leaking random walks, cl 3 and cl 10 had hit-rates of 97 and 137 respectively.

In summary, we found that a cash leaking random walk performed much better than a OPIC like random walk with-out any cash leakage or a baseline heuristic like TF-IDF.
It had been known that semantic relationships between terms can be modelled to a certain extent using the cooc-currences between them. These cooccurrences can be inter-preted as the probability of mention of a term in another term X  X  context. Using this interpretation we hypothesised that a topical anchor is term which is most central to a con-text. In this paper, we formally defined what we mean by a context and how a random walk on the context would lead to the topical anchors of the context. To this effect, we pro-posed cash leaking random walk, which is designed to run on subgraphs of dense graphs like the cooccurrence graph. Through a user evaluation we corroborated our hypothesis.
