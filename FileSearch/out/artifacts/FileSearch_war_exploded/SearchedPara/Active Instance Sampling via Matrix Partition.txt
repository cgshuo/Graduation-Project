 Active learning is well-motivated in many supervised learn ing scenarios where unlabeled instances are abundant and easy to retrieve but labels are difficult, ti me-consuming, or expensive to obtain. For example, it is easy to gather large amounts of unlabeled d ocuments or images from the Inter-net, whereas labeling them requires manual effort from expe rienced human annotators. Randomly tive sampling) methods have been adopted to control the labe ling process in many areas of machine learning. Given a large pool of unlabeled instances, active learning provides a way to iteratively select the most informative unlabeled instances X  X he querie s X  X rom the pool to label.
 Many researchers have addressed the active learning proble m in various ways [13]. Most have fo-cused on selecting a single most informative unlabeled inst ance to query each time. The ultimate goal for most such approaches is to select instances that cou ld lead to a classifier with low gener-alization error. Towards this, a few variants of a mutual inf ormation criterion have been employed instance to maximize the increase of mutual information and the mutual information, respectively, between the selected set of instances and the remainder base d on Gaussian process models. The approach proposed in [5] seeks the instance whose optimisti c label provides maximum mutual in-formation about the labels of the remaining unlabeled insta nces. The mutual information measure exploits the clustering information contained in the unlab eled data in an optimistic way. stance being labeled. When the learning task is sufficiently c omplex, the retraining process between queries can become very slow. This may make highly interacti ve learning inefficient or imprac-different labeling workstations at the same time on a networ k, a single instance selection system can make wasteful use of the resource. Thus, a batch-mode act ive learning strategy that selects multiple instances each time is more appropriate under thes e circumstances. The challenge in batch-mode active learning is how to properly assemble the optimal query batch. Simply using a single instance selection strategy to select a batch of queries in e ach iteration does not work well, since Most of these approaches use greedy heuristics to ensure the overall informativeness of the batch by taking both the individual informativeness and the diversi ty of the selected instances into account. Schohn and Cohn [12] select instances according to their pro ximity to the dividing hyperplane for a linear SVM. Brinker [2] considers an approach for SVMs that explicitly takes the diversity of the selected instances into account, in addition to individual informativeness. Xu et al. [14] propose a representative sampling approach for SVM active learning, which also incorporates a diversity mea-regression. Hoi et al. [9] propose a novel batch-mode active learning scheme on SVMs that exploits labeled and unlabeled examples, and then is used to effectiv ely identify the informative and diverse instances via a min-max framework. Instead of using heurist ic measures, Guo and Schuurmans [6] tempt to construct the most informative batch directly. Ove rall, these batch-mode active learning approaches all make batch selection decisions directly bas ed on the classifiers employed. In this paper, we propose a novel batch-mode active learning approach that makes query selection decisions independent of the classification model employed . The idea is to select a batch of queries in each iteration by maximizing a general mutual information measure between the labeled instances and the unlabeled instances. By employing a Gaussian proces s framework, this mutual information maximization problem can be further formulated as a matrix p artition problem. Although the matrix optimization problem and then a good local solution can be ob tained by exploiting an effective local optimization. The local optimization method we use is devel oped by combining a local lineariza-backtracking line search. Unlike most active learning meth ods studied in the literature, our query selection method does not require knowledge of the employed classifier. Our empirical studies show that the proposed batch-mode active learning approach can a chieve superior or comparable perfor-mance to discriminative batch-mode active learning method s that have been optimized on specific classifiers.
 The remainder of the paper is organized as follows. Section 2 provides preliminaries on Gaussian processes. Section 3 introduces the proposed matrix partit ion approach for batch-mode active learn-ing. Empirical studies are presented in Section 4, and Secti on 5 concludes this work. A Gaussian process is a generalization of the Gaussian proba bility distribution. Although Gaussian machine learning community during the past decade [11]. In t his section, we provide an overview of Gaussian processes and some of their important properties w hich we will exploit later to construct our active learning approach. 2.1 Multivariate Gaussian Distribution The Gaussian, also known as the normal distribution, is a wid ely used model for the distribution of continuous variables. In the case of multiple random vari ables, the joint multivariate Gaussian distribution for a d  X  1 vector x is given in the form where  X  is a d -dimensional mean vector,  X  is a d  X  d covariance matrix, and |  X  | denotes the determinant of  X  . When d = 1 , we obtain the standard one-variable Gaussian distributio n. 2.2 Gaussian Processes A Gaussian process is a generalization of a multivariate Gau ssian distribution over a finite vector space to a function space of infinite dimension. Given a set of instances X = [ x &gt; a data modeling function f (  X  ) can be viewed as a single sample from a Gaussian distribution with a mean function  X  (  X  ) , and a covariance function C (  X  ,  X  ) . In particular,  X  ( x the function variable f ( x functions f at point x of functions f which can be written in the form where  X  ( x ) is the mean function,  X  is defined using the covariance function C , and Z denotes the normalization factor. One typical choice for the covarianc e function C is a symmetric positive-definite kernel function K , e.g. a Gaussian kernel One important property of Gaussian processes is that for eve ry finite set (or subset) of instances X with indices Q , the joint distribution over the corresponding random func tion variables f is a multivariate Gaussian distribution with a mean vector  X   X  Here Z = (2  X  ) q/ 2 |  X   X  (  X  ) = 0 . Nevertheless, it is irrelevant in this paper. Given a small set of labeled instances { ( x our task is to iteratively select the most informative set of b instances from U and add them into conduct instance selective sampling using a maximum mutual information strategy which can then be formulated into a matrix partition problem. 3.1 Maximum Mutual Information Instance Selection mance on unseen test data, it makes sense to select instances that can produce a labeled set that is most informative about the unseen test instances. Apparent ly it is not possible to access the unseen test data. Nevertheless, in active learning setting, we hav e a large number of unlabeled instances available that come from the same distribution as the future test instances. Thus we can select in-instead. We propose to use a mutual information criterion to measure the informativeness of the labeled set L over the unlabeled set U where X tively, H (  X  ) denotes the entropy term.
 Both the mutual information measure and the entropy measure are defined on probability distribu-tions [3]. We thus employ a Gaussian process framework (intr oduced in the previous section) to model the joint probability distribution over all the insta nces. We first associate each instance x with a random variable f represented using the joint multivariate Gaussian distrib ution over variables f (2). Thus the entropy term H ( X where m is the number of variables, i.e., the size of Q ;  X  X information criterion in (3) can be rewritten as Note that for a given data set, the overall number of instance s does not change during the active in each iteration. Thus the set V and the entropy term H ( f selection. Based on this observation, our maximum mutual in formation instance selection strategy can be formulated as where L 0 = L  X  Q and U 0 = U \ Q . This also suggests the mutual information criterion depen ds only on the covariance matrices computed using the kernel functi ons over the instances. Our maximum mutual information strategy attempts to select the batch of b instances from the unlabeled set U to 3.2 Matrix Partition Let  X  be the covariance matrix over all the instances indexed by V = L  X  U = L 0  X  U 0 . Then the covariance matrices  X  generality, we assume the instances are arranged in the orde r of [ U, L ] , such that The instance selection problem formulated in (6) selects a s ubset of b instances indexed by Q from U and moves them into the labeled set L . This problem is actually equivalent to partitioning matrix  X  into submatrices  X  the actual matrix partition is conduct on covariance matrix  X  where 1 denotes a vector of all 1 entries. We let M  X  the last b rows of M , such that Obviously M where O According to (8) we then have Finally, the maximum mutual information problem given in (6 ) can be equivalently formulated into the following matrix partition problem After solving this problem to obtain an optimal M  X  , the instance selection can be determined from the last b rows of M  X  , i.e., M  X  However, the optimization problem (11) is an NP-hard combin atorial optimization problem over an integer matrix M . To facilitate a convenient optimization procedure, we rel ax the integer optimiza-tion problem (11) into the following upper bound optimizati on problem in
X . However, the quadratic matrix function X = B  X  B &gt; is matrix convex given the matrix  X  develop an efficient local optimization technique to solve f or a reasonable local solution instead. 3.3 First-order Local Optimization linear inequality and equality constraints (13). Here u is the number of unlabeled instances, and we typically assume it is a large number. Therefore a second-order optimization approach will be space demanding. We develop a first-order local maximizatio n algorithm to conduct optimization, which combines a gradient direction finding method with a str aightforward backtracking line search technique. This local optimization algorithm produced pro mising results in our experiments. which gives the following results Note here we use notations in the matlab format where [ X ] submatrix of X formed by entries between the i th to the j th rows and the m th to the n th columns. tion regarding the constraints. This leads to a convex linea r optimization The gradient direction for the ( k + 1) th iteration can be determined as Algorithm 1 Matrix Partition Input: l : the number of labeled instances; u the number of unlabeled instances; Output: M  X 
Initialize k = 0 , N oChange = f alse . repeat until N oChange is true or maximum iteration number is reached.
 Algorithm 2 Heuristic Greedy Rounding Procedure Input: b, M  X  (0 , 1) b  X  u for b &lt; u .
 Output: c M , Q .

Initialize Let Q =  X  , set c M as a b  X  u matrix with all 0 entries. for k = 1 to b do end for the linear constraints in (13), and leads to an objective val ue no worse than before. In our implementation, the constrained linear optimizatio n (18) can be efficiently solved using an optimization software package CPLEX. When the number of unla beled instances, u , is large, com-or underflow. Instead of computing the log-determinant dire ctly, we choose to compute it in an alternative efficient way. The key idea is based on the mathem atical fact that the determinant of a triangular matrix equals the product of its diagonal elemen ts. Hence, the matrix X  X  log-determinant is equal to the sum of their logarithm values. By keeping all c omputations in log-scale, the problem of underflow/overflow caused by product of many numbers can be effectively circumvented. For positive definite matrices, such as the matrices we have, one can use Cholesky factorization to first produce a triangular matrix and then compute the log-determ inant of the original matrix using the logarithms of the diagonal values of the triangular matrix. The computation of log-determinants or matrix inverse in our algorithm are all conducted on matrice s assumed to be positive definite. How-ever, in order to increase the robustness of the algorithm an d avoid numerical problems, we can add an additional  X I term to the matrices to guarantee the positive definite prope rty. Here  X  is a very small value and I is an identity matrix.
 However, this M  X  contains continuous values. In order to determine which set of b instances to d this procedure, we focused on rounding the last b rows, M  X  instances for labeling. The procedure is described in Algor ithm 2, which returns the indices of the selected b instances as well. To investigate the empirical performance of the proposed ba tch-mode active learning algorithm, we conducted two sets of experiments on a few UCI datasets and th e 20 newsgroups dataset. Note the proposed active learning method is in general independent o f the specific classification model em-evaluate the informativeness of the selected labeled insta nces. We compared the proposed approach, denoted as Matrix , with three discriminative batch-mode active learning met hods proposed in the selection [8]; Discriminative , a discriminative optimization approach based on logistic regression classifiers [6]. We have also compared our approach to one tra nsductive experimental design method which is formulated from regression problems and whose inst ance selection process is independent of evaluation classification models [15]. We used the sequen tial design code downloaded from the authors X  webpage and denote this method as Design .
 First, we conducted experiments on seven UCI datasets. We co nsider a hard case of active learning, where we start active learning from only a few labeled instan ces. In each experiment, we start with two randomly selected labeled instances, one in each class. We then randomly select 2/3 of the algorithm repeatedly select b instances to label each time and evaluate the produced class ifier on testing data after each new labeling, with maximum 110 instances to select in total. The experiments were repeated 20 times. In Table 1, we report the experimenta l results with b = 10 , comparing the proposed Matrix algorithm with each of the three batch-mode alternatives. W ith b = 10 , there are at each evaluation point to compare the performance of each p air of algorithms. The  X  X in% X  denotes the percentage of evaluation points where the Matrix algorithm outperforms the specified algorithm points where the specified algorithm outperforms the Matrix algorithm. The  X  X verall X  nevertheless show the comparison results using a single 2-sided paired t-test on all 220 results. These results show that the proposed active learning method, Matrix , overperformed svmD , Fisher and Design on a tie with Design on flare. Matrix is mostly tied with Discriminative on all data sets, with a slight pointwise win on crx and a slight overall lose on german. Although Matrix and Discriminative demonstrated similar performance, the proposed Matrix is more efficient regarding running time on relatively big data sets. The comparison in running times ov er 20 repeats are reported in Table 2. Table 1: Comparison of the active learning algorithms on UCI data with batch size = 10. These results are based on 2-sided paired t-test at the level of p &lt; 0.05.
 cleve 63.6 0 win 45.5 0 win 0 0 tie 90.9 0 win crx 27.3 0 win 9.1 0 win 9.1 0 tie 90.9 0 win flare 54.5 0 win 100.0 0 win 0 0 tie 36.4 9.1 tie german 81.8 0 win 9.1 0 win 0 0 lose 72.7 0 win heart 63.6 0 win 36.4 0 win 0 0 tie 100.0 0 win hepatitis 100.0 0 win 33.3 0 tie 0 0 tie 0 0 tie pima 0 0 lose 100.0 0 win 0 0 tie 81.8 0 win Table 3: Comparison of the active learning algorithms on New sgroup data with batch size = 20. These results are based on 2-sided paired t-test at the level of p &lt; 0.05.
 Autos 86.7 0 win 20.0 6.6 tie 73.3 6.6 win 80.0 6.7 win Hardware 100.0 0 win 0 0 tie 13.3 0 win 86.7 0 win Sport 86.7 6.6 win 20.0 13.3 tie 46.7 0 win 80.0 6.7 win Next we conducted experiments on 20 newsgroups dataset for d ocument categorization. We build uments); (2) Hardware: comp.sys.ibm.pc.hardware (979 doc uments) vs. comp.sys.mac.hardware (958 documents); (3) Sport: rec.sport.baseball (991 docum ents) vs. rec.sport.hockey (997 docu-ments). Each document is first minimally processed into a  X  X f .idf X  vector. We then select the top ered task. In each experiment, we start with four randomly se lected labeled instances, two in each class. We then randomly select 1000 instances (500 from each class) from the remaining ones as the the experimental results with b = 20 averaged over 20 times repetitions. There are 300 / 20 = 15 evaluation points in this case.
 experiments on UCI datasets. This substantially increases the searching space of instance selection. One consequence in our experiments is that the Discriminative algorithm becomes very slow. Thus we were not able to produce comparison results for this algor ithm. The proposed Matrix method was affected as well. However, we coped with this problem usi ng a subsampling assisted method, additional constraints on M this subset of 400 instances are all set to 0. For the experime nts, we chose the 400 instances as the ones with top entropy terms under the current classificat ion model. The same subsampling was used for the method Design as well. Table 3 shows the comparison results on the three doc ument categorization tasks, comparing Matrix to svmD , Fisher , Design and a baseline random selection, Random . These results show the proposed Matrix outperformed svmD , Design and Random . It tied with Fisher regarding overall measure, but had a slight win regarding pointwise measure. These empirical results suggest that selecting unlabeled i nstances independent of the classification model using the proposed matrix partition method can achiev e reasonable performance, which is better than a transductive experimental design method and c omparable to the discriminative batch-mode active learning approaches. However, our approach can offer certain conveniences in some circumstances where one does not know the classification mod el to be employed for classification. In this paper, we propose a novel batch-mode active learning approach that makes query selection decisions independent of the classification model employed . The proposed approach is based on a general maximum mutual information principle. It is formulated as a matrix partition optimizat ion problem under a Gaussian process framework. To tackle the fo rmulated combinatorial optimization problem, we developed an effective local optimization tech nique. Our empirical studies show the proposed flexible batch-mode active learning approach can a chieve comparable or superior perfor-mance to discriminative batch-mode active learning method s that have been optimized on specific data by exploiting different kernel functions. [1] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. [2] K. Brinker. Incorporating diversity in active learning with support vector machines. In Pro-[3] T. Cover and J. Thomas. Elements of Information Theory . John Wiley &amp; sons, 1991. [4] C. Guestrin, A. Krause, and A. Singh. Near-optimal senso r placements in Gaussian processes. [5] Y. Guo and R. Greiner. Optimistic active learning using m utual information. In Proceedings [6] Y. Guo and D. Schuurmans. Discriminative batch mode acti ve learning. In Proceedings of [7] S. Hoi, R. Jin, and M. Lyu. Large-scale text categorizati on by batch mode active learning. In [8] S. Hoi, R. Jin, J. Zhu, and M. Lyu. Batch mode active learni ng and its application to medical [9] S. Hoi, R. Jin, J. Zhu, and M. Lyu. Semi-supervised SVM bat ch mode active learning for [10] A. Krause, C. Guestrin, A. Gupta, and J. Kleinberg. Near -optimal sensor placements: Max-[11] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning . MIT Press, 2006. [12] G. Schohn and D. Cohn. Less is more: Active learning with support vector machines. In [13] B. Settles. Active learning literature survey. Comput er Sciences Technical Report 1648, Uni-[14] Z. Xu, K. Yu, V. Tresp, X. Xu, and J. Wang. Representative sampling for text classification
