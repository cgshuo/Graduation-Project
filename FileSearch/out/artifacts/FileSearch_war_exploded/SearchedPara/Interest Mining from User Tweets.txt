 We build a system to extract user interests from Twitter messages. Specifically, we extract interest candidates us-ing linguistic patterns and rank them using four different keyphrase ranking techniques: TFIDF, TextRank, LDA-TextRank, and Relevance-Interestingness-Rank (RI-Rank). We also explore the complementary relation between TFIDF and TextRank in ranking interest candidates. Top ranked interests are evaluated with user feedback gathered from an online survey. The results show that TFIDF and TextRank are both suitable for extracting user interests from tweets. Moreover, the combination of TFIDF and TextRank consis-tently yields the highest user positive feedback.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  text analysis ; H.3.3 [ Information Storage and Re-trieval ]: Information Search and Retrieval X  clustering, in-formation filtering keyword/keyphrase extraction; keyword/keyphrase ranking; topic modeling; data processing; social networks; Twitter Social networks have developed drastically in recent years. This has drawn increased interest in social media analysis from the community. Among these are works analyzing tweets from Twitter for use in several useful applications. Sakaki et. al. [5] extracted real-time events from Twitter by applying  X  X ocial sensors X . Zhao et. al. [7] studied the spreading of news via Twitter versus a traditional medium like the New York Times. They discovered that Twitter is informatively rich in entity-oriented topics, and it actively broadcasts important world news. While many works focus on detecting global insights by analyzing Twitter data as a whole, studies on mining of the individuality of Twitter users via their tweets is not exhaustive. However, we believe that tweets that belong to a single user are worth studying even if they are not representative of an entire group or community. For example, individual tweets can be used to automatically build a profile for a user or to suggest, in Twit-ter lingo, followers and followees [2]. In addition, learning personal interests from one X  X  tweets can be useful for some commercial applications such as personalized advertisement, or potential customer analysis.

Wu et. al. [6] is one of the pioneering works that ex-tracts interests from user tweets using keyword extraction. The authors use two standard keyword ranking techniques: TFIDF and TextRank [4], a variance of PageRank for text data. The performance of these two methods are compa-rable. Their experiments show that TextRank outperforms TFIDF when deciding top-5 keywords. However, TFIDF performs slightly better than TextRank in determining a user X  X  top-10 keywords.

In this paper, we first extend the work of Wu et. al. [6] by adapting more recent techniques in keyphrase extrac-tion to the problem. Specifically, we consider two additional ranking techniques for social textual data. Both of these techniques rely on extracting topics from data. Liu et. al. [3] proposes ranking keyphrases per topics. Zhao et. al. [7] uses topical information to compute the relevance and inter-estingness of a phrase. Second, we study the complementary relation between the two common ranking metrics: TFIDF and TextRank. The result is evaluated with respect to the user feedback.

The rest of the paper is organized as follows. We describe the general architecture of the system and its components in section 2. This section includes three subsections describ-ing the three main steps: tweet pre-processing, candidate generation, and candidate ranking. We then present our ap-proach to evaluate the extraction task, which is subjective, in section 3. Finally, we draw conclusions from our work and discuss future work in the last section.
We model a user X  X  tweets similar to sentences of a docu-ment in an author-topic notion. As such, the interest extrac-tion problem for a Twitter user becomes equivalent to the keyphrase extraction problem for a document, given a set of documents. In this paper, an interest of a user is defined as a multi-word phrase extracted from his tweets. In general, we follow two standard steps for extracting keyphrases from text: candidate generation and candidate ranking.
Figure 1 describes the workflow of the extraction task in three stages: pre-processing, keyword candidate extrac-tion, and keyword ranking. First, we consider all the pre-processing used in [6]. Second, we propose using the Tweet NLP Tagger 1 with our normalization strategies to better recognize interests in noisy tweet data. In other words, we describe interests using linguistic information. Finally, ex-tracted candidates are ranked with multiple ranking meth-ods. In the following subsections we introduce each compo-nent in detail.
Since tweets are short and contain a lot of noise, pre-processing is an important component in almost every Twit-ter mining system. We consider textual variances, emoti-cons, slang, shorthand, and hyperlinks as noise in tweets. We handle this noise in four steps.

First, we clean all non-English tweets. We employ a tri-gram language model trained by a combination of clean and noisy English data. This model computes a language prob-ability score for every tweet. Because language probabil-ity depends on the length of a tweet, to be fair we set a limit on the number of words. The limit is six words. We use SRILM 2 to build the trigram language model. Given a tweet, we compute the language probability. If the tweet has more than six words, its language probability is the highest score possible for any six consecutive words. We empiri-cally chose -30 for the language model log-probability as a threshold to detect English tweets.

Slang, shorthand, and emoticons are widely used in short messages. However, interests are rarely seen in these text items. In the second pre-processing step we discard all of these items using dictionaries rather than normalizing.
Third, linguistic variances in tweets are normalized. We propose using a morphology mapping dictionary instead of a stemming algorithm used in [6]. This is needed to avoid creating noise in later processing stages, which make use of linguistic tags to describe interest candidates. For example, a standard stemmer would change the word  X  X nteresting X  (adjective) into  X  X nterest, X  (noun) and this will create ambi-guity if we need to distinguish nouns and adjectives in our linguistic patterns. In our system, we use the derivational morphology dictionary 3 .

Finally, we annotate every word in a tweet with a part-of-speech (POS) tag. This is used in the next phase to find interest candidates. As mentioned earlier, we decided to use http://www.ark.cs.cmu.edu/TweetNLP/ http://www.speech.sri.com/projects/srilm http://www-formal.stanford.edu/jsierra/cs1931-project/ morphological-db.lisp the TweetNLP Tagger for this annotation task as it specifi-cally handles noise in tweets.
After the pre-processing steps, every word separated by spaces is assigned a POS tag that describes its grammatical information. We use linguistic patterns to describe interest candidates due to their flexibility.

Unlike [6], our interest candidates can be either single words or multi-word phrases. In addition to noun-phrase, we also consider verb-phrase to be interest candidates. For instance, Math-related tweets in our dataset usually contain verbs like  X  X olve, X  and  X  X rove X . Since common verbs are usu-ally noise, we filter out all verbs found in the top 100 most common English verbs 4 . Here are the regular expression patterns that we used to describe a user X  X  interests:
The output of the candidate generation phase is a set of all interest candidates to be ranked. We introduce five different ranking methods in the following subsection.
We study a series of keyphrase ranking metrics in order to single out the ideal one for ranking interests. We consider five different ranking methods. The first two are used in [6]: TFIDF and TextRank. It is shown in this work that they both perform comparably on Twitter data, even though Tex-tRank outperforms TFIDF in processing regular text input. We then propose a novel way to combine TFIDF and Tex-tRank as we think they complement each other. Specifically, we propose a weighted version of TextRank using TFIDF scores as initial weights. We name this metric TI-TextRank (TF-IDF-TextRank). Finally, we also explore whether top-ical information can be used to rank phrases. We consider two recent related metrics from [3] and [7].
TFIDF (term-frequency inverse-document-frequency) is a statistical metric to rank words in a set of documents. A word is ranked higher if it appears more frequent in that particular document (higher TF) and less frequent in the whole corpus (higher IDF or lower DF). In our system, we use the author-topic notion that represents tweets of a user as a single document. Let U = { u 1 ,u 2 ,...,u |U| } be the set of Twitter users. The TFIDF score for each interest candidate p for user u is calculated using the following formulas: http://www.world-english.org/100verbs.htm Where TF( p,u ) returns the frequency of candidate p in doc-ument u ; |U| is the number of users and | u : p  X  u and u  X  X | is the number of users having candidate p .
TextRank is a graph-based algorithm that ranks textual entities proposed by Mihalcea et. al. [4]. It is essentially a variant of Google X  X  PageRank algorithm. One key differ-ence when compared to PageRank is that the graph contains weights between nodes (textual entities to rank) as a natu-ral feature of text. In our system, the textual entities are a set of interest candidates. Let G = ( V,E ) be the graph that represents the relation between candidates in a docu-ment. Let V = { p 1 ,p 2 ,...,p | V | } be the set of nodes, and E = { ( p j ,p i ) : 1  X  i,j  X  | V |} the set of links from p p with weight e ( p j ,p i ). The TextRank score TR( p i ,u ) for each candidate of user u is computed as:
TR( p i ,u ) = X
Where  X  is the damping factor that ranges between 0 and 1 and O ( p j ) is the summation of all edge weights starting from p j as follows: O ( p j ) = P k :( p didates are linked together in a graph if they both appear in one tweet. The weight e between two candidates is the number of times they co-occur in the same tweet. The value of TR( p i ,u ) for each candidate p i is obtained through an iterative process until results convergence. The graph con-tains the candidates of a Twitter user. Because the graph has a simple structure, the calculation usually converges in 20 iterations.
TrustRank, a PageRank extension, is widely used in liter-ature for its ability to sensitively detect spamming activities in search engines[1]. The key idea of TrustRank is to cus-tomize the second factor, (1  X   X  ) 1 | V | , in regular PageRank into a biased value. This is based on the assumption that a trust score is only given to  X  X rusted pages. X  It is empir-ically shown that TFIDF and TextRank perform similarly when ranking keywords [6]. However, we observe that the features that these methods use to rank keywords are differ-ent. TFIDF only considers the frequency of words in both the local document and in the whole corpus. On the other hand, TextRank only uses the intrinsic textual structure of a document. We propose to use TFIDF as a trust score to bias the TextRank algorithm. This is similar to the adjust-ment factor used in TrustRank. Specifically, we modify the original TextRank formula as follows: TI-TR( p i ,u ) =
Similar to TextRank, we implement an iterative algorithm to compute the ranking of candidates.
TextRank is considered a state-of-the-art method for un-supervised keyphrase extraction. Yet, this method ranks keyphrases in a document using local features. This could pose difficulties in finding representative keyphrases for a group of users. Although extracting keyphrases for each user is our main concern, we also want to learn if additional information could help to rank keyphrases better. For ex-ample, with the tweet:  X  X he Art of War in Fitness for iPad on Sale, X  there are two candidates:  X  X itness X  and  X  X Pad. X  For an Apple fan, the most important keyword would be  X  X Pad. X  A relevant keyword could also be  X  X itness X  if the given user is concerned about health and wellness. In this case, it would be helpful to take the most related topic of a user into consideration. To address this issue, [3] pro-posed a two stage Topical PageRank ranking algorithm. In the first stage topic decomposition is performed on the cor-pus to generate the two probabilistic matrices: documents-topics and topics-words. In the second stage a topic sensi-tive PageRank (LDA-TR) is applied to each document to extract keyphrases. Formally, the score for each keyword w in a topic z is computed as:
R z ( w i ) =  X  X
Where Z = { z 1 ,z 2 ,...,z | Z | } is the set of topics. With specific consideration for Twitter data, [7] proposed a novel keyphrase ranking approach, which considers retweeting ac-tivity. They defined a metric to evaluate relevance and in-terestingness of a keyphrase p in a topic (RI), based on the relative frequency of its individual words in a topic C z in the entire corpus C .
 R
Where R and T are the set of retweets and tweets respec-tively; l avg ( p ) is the average number of tweets that p belongs to;  X  and  X  are empirically set to 0.01 and 500 as suggested in [7].
We downloaded our sample user tweets from Twitter us-ing the Twitter corpus tools from TREC 2011 Microblog Dataset 5 . The data set was sampled from 1/23/2011 to 2/3/2011. The download process for the first day of data took 5 days. Given that we wanted to process the entire data set (17 days), we had to come up with a different strategy. Realizing that many of the Twitter users are non-English language users, we decided to download only English language users. We used the SRI Language Modeling tool to filter non-English tweets as described in section 2. We http://trec.nist.gov/data/tweets/ trained a trigram language model, and estimated the prob-ability that a message X  X  text was English. This allowed us to select 11,550 English language users having at least five English tweets, from the original 164,871 user set. By re-ducing the number of candidate users, we also reduced the total number of Twitter messages we had to download, from 16,161,812 to 200,038. This is only a little over 1% of the original dataset.

We used the Stanford Topic Modeling Toolbox 6 with 1000 iterations of Gibbs sampling to generate 30 topics. A sim-ple English tokenizer was used to select topic words based on frequency. To reduce the bias in running topic modeling, the first 100 most frequent words were removed from considera-tion as suggested by the tool X  X  author. We also removed all hash-tags and user-tags that appeared in the tweets to pre-vent biasing the results toward these tags which are usually noisy.
We evaluate the performance by conducting an online sur-vey. We chose a sample of 11 English language Twitter users, with tweets spanning a diverse set of topics. For each user and their set of messages, we extracted the top 11 keywords using the five ranking methods described in this paper. In order to remove bias in the evaluation, we merged the three sets of top 11 keywords by creating the union set of the top ranked keywords. The evaluators were presented with the set of Twitter messages for each user and the correspond-ing union set of top keywords. They were asked to select all the keywords which they believed were relevant or could be an interest/concern to the given user X  X  tweets. With the evaluation results we then mapped all chosen keywords to the corresponding extracted keyphrases of all ranking ap-proaches. We assigned a score based on whether or not a keyword was chosen, and how high the particular keyword was ranked by the ranking method.
After conducting the evaluation survey we extracted the results. We were interested in knowing how each ranking approach performed in computing a user X  X  top-k keyphrases that best describe that user X  X  interests. k was randomly cho-sen as 5, 8 and 11. Table 1 is divided into 3 columns showing 3 result sets for the top-5, top-8, and top-11 keyphrases. For each ranking method row and result set column we compute a pair of numbers based on the method X  X  performance in identifying the top-k keyphrases. The first number is the mean-average-precision (MAP) and the second number is the precision.

According to the results, our proposed TI-TextRank rank-ing method consistently shows the best performance. The standalone versions of TextRank and TFIDF score relatively close in second and third place after TI-TextRank. The re-sults confirm our intuition that the features of TFIDF and TextRank are complimentary to each other. The additional  X  X onfidence X  factor allows TI-TextRank to select more rele-vant keywords. In addition, this result again supports the conclusion in [6] that TextRank and TFIDF have similar performance for Twitter data. In our experiment TFIDF actually outperforms TextRank.

We initially expected that LDA-TextRank and RI-Rank would at least yield comparable performance to TFIDF or http://nlp.stanford.edu/software/tmt/tmt-0.4/ TextRank, but they both performed poorly. If a candidate keyphrase in a document does not share words with the key-words of the topics, it may be ranked lower than it should. A similar explanation is used for RI-Rank. Although the first factor based on retweet behavior sounds reasonable, the sec-ond factor relies heavily on the frequency of words in that phrase.
 Method Top-5 Top-8 Top-11 TFIDF 0.652/0.520 0.652/0.470 0.603/0.444 TextRank 0.650/0.442 0.616/0.402 0.582/0.377 TI-TextRank 0.726/0.545 0.679/0.518 0.661/0.452 LDA-TextRank 0.464/0.243 0.424/0.225 0.389/0.245
RI-Rank 0.551/0.412 0.536/0.395 0.530/0.390
We presented an empirical study of extraction methods for the task of automatic interest profiling of Twitter users. The evaluation results show that TFIDF and TextRank are both reliable in ranking candidate keyphrases extracted for each user. Moreover, the resulting performance of TI-TextRank strongly supports our preliminary belief that these two meth-ods are complimentary.

We believe that using topic-modeling to extract Twitter user interests is an approach worth exploring further in fu-ture work. In addition, considering more document local features will also help enhance the work presented here. [1] Z. Gy  X  ongyi, H. Garcia-Molina, and J. O. Pedersen. [2] J. Hannon, M. Bennett, and B. Smyth. Recommending [3] Z. Liu, W. Huang, Y. Zheng, and M. Sun. Automatic [4] R. Mihalcea and P. Tarau. Textrank: Bringing order [5] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [6] W. Wu, B. Zhang, and M. Ostendorf. Automatic [7] W. X. Zhao, J. Jiang, J. He, Y. Song, P. Achananuparp,
