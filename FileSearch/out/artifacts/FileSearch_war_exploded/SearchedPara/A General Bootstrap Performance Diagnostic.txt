 As datasets become larger, more complex, and more avail-able to diverse groups of analysts, it would be quite useful to be able to automatically and generically assess the qual-ity of estimates, much as we are able to automatically train and evaluate predictive models such as classifiers. However, despite the fundamental importance of estimator quality as-sessment in data analysis, this task has eluded highly auto-matic solutions. While the bootstrap provides perhaps the most promising step in this direction, its level of automa-tion is limited by the difficulty of evaluating its finite sample performance and even its asymptotic consistency. Thus, we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap X  X  outputs, determining whether or not the bootstrap is per-forming satisfactorily when applied to a given dataset and estimator. We show that our proposed diagnostic is effective via an extensive empirical evaluation on a variety of estima-tors and simulated and real datasets, including a real-world query workload from Conviva, Inc. involving 1.7TB of data (i.e., approximately 0.5 billion data points).
 G.3 [ Probability and Statistics ]: nonparametric statis-tics, statistical computing bootstrap; performance; diagnostic; estimator quality as-sessment
Modern datasets are growing rapidly in size and are in-creasingly subjected to diverse, rapidly evolving sets of complex and exploratory queries, often crafted by non-statisticians. These developments render generic applica-bility and automation of data analysis methodology par-ticularly desirable, both to allow the statistician to work more efficiently and to allow the non-statistician to cor-rectly and effectively utilize more sophisticated inferential techniques. For example, the development of generic tech-niques for training classifiers and evaluating their general-ization ability has allowed this methodology to spread well beyond the boundaries of the machine learning and statistics research community, to great practical benefit. More gen-erally, estimation techniques for a variety of settings have been rendered generically usable. However, except in some restricted settings, the fundamental inferential problem of assessing the quality of estimates based upon finite data has eluded a highly automated solution.

Assessment of an estimate X  X  quality X  X or example, its vari-ability (e.g., in the form of a confidence region), its bias, or its risk X  X s essential to both its interpretation and use. In-deed, such quality assessments underlie a variety of core sta-tistical tasks, such as calibrated inference regarding param-eter values, bias correction, and hypothesis testing. Beyond simply enabling other statistical methodology, however, esti-mator quality assessments can also have more direct utility, whether by improving human interpretation of inferential outputs or by allowing more efficient management of data collection and processing resources. For instance, we might seek to collect or process only as much data as is required to yield estimates of some desired quality, thereby avoiding the cost (e.g., in time or money) of collecting or processing more data than is necessary. Such an approach in fact con-stitutes an active line of work in research on large database systems, which seeks to answer queries on massive datasets quickly by only applying them to subsamples of the total available data [1, 15]. The result of applying a query to only a subsample is in fact an estimate of the query X  X  output if applied to the full dataset, and effective implementation of a system using this technique requires an automatic ability to accurately assess the quality of such estimates for generic queries.

In recent decades, the bootstrap [7, 9] has emerged as a powerful and widely used means of assessing estimator quality, with its popularity due in no small part to its rela-tively generic applicability. Unlike classical methods X  X hich have generally relied upon analytic asymptotic approxima-tions requiring deep analysis of specific classes of estimators in specific settings [17] X  X he bootstrap can be straightfor-wardly applied, via a simple computational mechanism, to a broad range of estimators. Since its inception, theoretical work has shown that the bootstrap is broadly consistent [3, 10, 20] and can be higher-order correct [11]. As a result, the bootstrap (and its various relatives and extensions) pro-vides perhaps the most promising avenue for obtaining a generically applicable, automated estimator quality assess-ment capability.

Unfortunately, however, while the bootstrap is relatively automatic in comparison to its classical predecessors, it re-mains far from being truly automatically usable, as eval-uating and ensuring its accuracy is often a challenge even for experts in the methodology. Indeed, like any inferential procedure, despite its excellent theoretical properties and frequently excellent empirical performance, the bootstrap is not infallible. For example, it may fail to be consistent in particular settings (i.e., for particular pairs of estimators and data generating distributions) [19, 4]. While theoreti-cal conditions yielding consistency are well known, they can be non-trivial to verify analytically and provide little useful guidance in the absence of manual analysis. Furthermore, even if consistent, the bootstrap may exhibit poor perfor-mance on finite samples.

Thus, it would be quite advantageous to have some means of diagnosing poor performance or failure of the bootstrap in an automatic, data-driven fashion, without requiring signif-icant manual analysis. That is, we would like a diagnostic procedure which is analogous to the manner in which we evaluate performance in the setting of supervised learning (e.g., classification), in which we directly and empirically evaluate generalization error (e.g., via a held-out validation set or cross-validation). Unfortunately, prior work on boot-strap diagnostics (see [5] for a comprehensive survey) does not provide a satisfactory solution, as existing diagnostic methods target only specific bootstrap failure modes, are often brittle or difficult to apply, and generally lack sub-stantive empirical evaluations. For example, a theoretical result of Beran regarding bootstrap asymptotics has been proposed as the basis of a diagnostic for bootstrap inconsis-tency [2]; however, it is unclear how to reliably construct and interpret the diagnostic plots required by this proposal, and the limited existing empirical evaluation reveals it to be of questionable practical utility [5]. Other work has sought to diagnose bootstrap failure specifically due to incorrect stan-dardization of the quantity being bootstrapped (which could occur if an estimator X  X  convergence rate is unknown or incor-rectly determined), use of an incorrect resampling model (if, for example, the data has a correlation structure that is not fully known a priori), or violation of an assumption of piv-otality of the quantity being bootstrapped [5]. Additionally, jackknife-after-bootstrap and bootstrap-after-bootstrap cal-culations have been proposed as a means of evaluating the stability of the bootstrap X  X  outputs [8, 5]; while such proce-dures can be useful data analysis tools, their utility as the basis of a diagnostic remains limited, as, among other things, it is unclear whether they will behave correctly in settings where the bootstrap is inconsistent.

In contrast to prior work, we present here a general boot-strap performance diagnostic which does not target any par-ticular bootstrap failure mode but rather directly and auto-matically determines whether or not the bootstrap is per-forming satisfactorily (i.e., providing sufficiently accurate outputs) when applied to a given dataset and estimator. The key difficulty in evaluating the accuracy of the bootstrap X  X  (or any estimator quality assessment procedure X  X ) outputs is the lack of ready availability of even approximate com-parisons to ground truth estimate quality. While compar-isons to ground truth labels are readily obtained in the case of supervised learning via use of a held-out validation set or cross-validation, comparing to ground truth in the con-text of estimator quality assessment requires access to the (unknown) sampling distribution of the estimator in ques-tion. We surmount this difficulty by constructing a proxy to ground truth for various small sample sizes (smaller than that of our full observed dataset) and comparing the boot-strap X  X  outputs to this proxy, requiring that they converge to the ground truth proxy as the sample size is increased. This approach is enabled by the increasing availability of large datasets and more powerful computational resources. We show via an extensive empirical evaluation, on a variety of estimators and simulated and real data, that the resulting diagnostic is effective in determining X  X ully automatically X  whether or not the bootstrap is performing satisfactorily in a given setting.

In Section 2, we formalize our statistical setting and nota-tion. We introduce our diagnostic in full detail in Section 3. Sections 4 and 5 present the results of our evaluations on simulated and real data, respectively. Finally, we conclude in Section 6.
We assume that we observe n data points D = ( X 1 ,...,X n ) sampled i.i.d. from some unknown distribu-tion P ; let P n = n  X  1 P n i =1  X  X i be the empirical distribution of the observed data. Based upon this dataset, we form an estimate  X   X  ( D ) of some parameter  X  ( P ) of P ; note that, un-like  X  ( P ),  X   X  ( D ) is a random quantity due to its dependence on the data D . We then seek to form an assessment  X  ( P,n ) of the quality of the estimate  X   X  ( D ), which consists of a sum-mary of the distribution Q n of some quantity u ( D ,P ). Our choice of summary and form for u depends upon our infer-ential goals and our knowledge of the properties of  X   X  . For instance,  X  ( P,n ) might compute an interquantile range for u ( D ,P ) =  X   X  ( D ), the expectation of u ( D ,P ) =  X  (i.e., the bias), or a confidence interval based on the distri-bution of u ( D ,P ) = n 1 / 2 (  X   X  ( D )  X   X  ( P )). Unfortunately, we cannot compute  X  ( P,n ) directly because P and Q n are un-known, and so we must resort to estimating  X  ( P,n ) based upon a single observed dataset D .

The bootstrap addresses this problem by estimating the unknown  X  ( P,n ) via the plug-in approximation  X  ( P n ,n ). Al-though computing  X  ( P n ,n ) exactly is typically intractable, we can obtain an accurate approximation using a simple Monte Carlo procedure: repeatedly form simulated datasets D  X  of size n by sampling n points i.i.d. from P n , compute u ( D  X  , P n ) for each simulated dataset, form the empirical dis-tribution Q n of the computed values of u , and return the desired summary of this distribution. We overload nota-tion somewhat by referring to this final bootstrap output as  X  (
Q n ,n ), allowing  X  to take as its first argument either a data generating distribution or a distribution of u values.
For ease of exposition, we assume below that  X  is real-valued, though the proposed methodology can be straight-forwardly generalized (e.g., to contexts in which  X  produces elements of a vector space).
We frame the task of evaluating whether or not the boot-strap is performing satisfactorily in a given setting as a de-cision problem: for a given estimator, data generating dis-tribution P , and dataset size n , is the bootstrap X  X  output sufficiently likely to be sufficiently near the ground truth value  X  ( P,n )? This formulation avoids the difficulty of pro-ducing uniformly precise quantifications of the bootstrap X  X  accuracy by requiring only that a decision be rendered based upon some definition of  X  X ufficiently likely X  and  X  X ufficiently near the ground truth. X  Nonetheless, in developing a diag-nostic procedure to address this decision problem, we face the key difficulties of determining the distribution of the bootstrap X  X  outputs on datasets of size n and of obtaining even an approximation to the ground truth value against which to evaluate this distribution.

Ideally, we might approximate  X  ( P,n ) for a given value of n by observing many independent datasets, each of size n . For each dataset, we would compute the corresponding value of u , and the resulting collection of u values would approxi-mate the distribution Q n , which would in turn yield a direct approximation of the ground truth value  X  ( P,n ). Further-more, we could approximate the distribution of bootstrap outputs by simply running the bootstrap on each dataset of size n . Unfortunately, however, in practice we only observe a single set of n data points, rendering this approach an unachievable ideal.

To surmount this difficulty, our diagnostic (Algorithm 1) executes this ideal procedure for dataset sizes smaller than n . That is, for a given p  X  N and b  X  b n/p c , we randomly sample p disjoint subsets of the observed dataset D , each of size b . For each subset, we compute the value of u ; the re-sulting collection of u values approximates the distribution Q , in turn yielding a direct approximation of  X  ( P,b ), the ground truth value for the smaller dataset size b . Addition-ally, we run the bootstrap on each of the p subsets of size b , and comparing the distribution of the resulting p bootstrap outputs to our ground truth approximation, we can deter-mine whether or not the bootstrap performs acceptably well at sample size b .

It then remains to use this ability to evaluate the boot-strap X  X  performance at smaller sample sizes to determine whether or not it is performing satisfactorily at the full sam-ple size n . To that end, we evaluate the bootstrap X  X  perfor-mance at multiple smaller sample sizes to determine whether or not the distribution of its outputs is in fact converging to the ground truth as the sample size increases, thereby allow-ing us to generalize our conclusions regarding performance from smaller to larger sample sizes. Indeed, determining whether or not the bootstrap is performing satisfactorily for a single smaller sample size b alone is inadequate for our purposes, as the bootstrap X  X  performance may degrade as sample size increases, so that it fails at sample size n despite appearing to perform sufficiently well at smaller sample size b . Conversely, the bootstrap may exhibit mediocre perfor-mance for small sample sizes but improve as it is applied to more data.

Thus, our diagnostic compares the distribution of boot-strap outputs to the ground truth approximation for an in-creasing sequence of sample sizes b 1 ,...,b k , with b k subsamples of each of these sizes are constructed and pro-cessed in the outer for loop of Algorithm 1. In order to conclude that the bootstrap is performing satisfactorily at sample size n , the diagnostic requires that the distribution of its outputs converges monotonically to the ground truth approximation for all of the smaller sample sizes b 1 ,...,b Convergence is assessed based on absolute relative devia-tion of the mean of the bootstrap outputs from the ground truth approximation (which must decrease with increasing sample size) and size of the standard deviation of the boot-strap outputs relative to the ground truth approximation (which must also decrease with increasing sample size). In Algorithm 1, this convergence assessment is performed by conditions (1) and (2). As a practical matter, these con-ditions do not require continuing decreases in the absolute relative mean deviation  X  i or relative standard deviation  X  i when these quantities are below some threshold (given by c 1 and c 2 , respectively) due to inevitable stochastic er-ror in their estimation: when these quantities are sufficiently small, stochastic error due to the fact that we have only used p subsamples prevents reliable determination of whether or not decreases are in fact occurring. We have found that c = c 2 = 0 . 2 is a reasonable choice of the relevant thresh-olds. Figures 1 and 2 highlight the use of conditions (1) and (2) in both positive and negative settings for the boot-strap.

Progressive convergence of the bootstrap X  X  outputs to the ground truth is not alone sufficient, however; although the bootstrap X  X  performance may be improving as sample size increases, a particular value of n may not be sufficiently large to yield satisfactory performance. Therefore, beyond the convergence assessment discussed above, we must also determine whether or not the bootstrap is in fact perform-ing sufficiently well for the user X  X  purposes at sample size n . We define  X  X ufficiently well X  as meaning that with prob-ability at least  X   X  [0 , 1], the output of the bootstrap when run on a dataset of size n will have absolute relative devia-tion from ground truth of at most c 3 (the absolute relative deviation of a quantity  X  from a quantity  X  o is defined as |  X   X   X  o | / |  X  o | ); the constants  X  and c 3 are specified by the user of the diagnostic procedure based on the user X  X  inferen-tial goals. Because we can only directly evaluate the boot-strap X  X  performance at smaller sample sizes (and not at the full sample size n ), we take a conservative approach, mo-tivated by the assumption that a false positive (incorrectly concluding that the bootstrap is performing satisfactorily) is substantially less desirable than a false negative. In particu-lar, as embodied in condition (3) of Algorithm 1, we require that the bootstrap is performing sufficiently well under the aforementioned definition at the sample size b k . Satisfying this condition, in conjunction with satisfying the preceding conditions indicating continuing convergence to the ground truth, is taken to imply that the bootstrap will continue to perform satisfactorily when applied to the full sample size n (in fact, the bootstrap X  X  performance at sample size n will likely exceed that implied by  X  and c 3 due to the diagnostic X  X  conservatism).

It is worth noting that this diagnostic procedure reposes on the availability in modern data analysis of both sub-stantial quantities of data and substantial computational resources. For example, with p = 100 (an empirically ef-fective choice), using b k = 1 , 000 or b k = 10 , 000 requires n  X  10 5 or n  X  10 6 , respectively. Fortuitously, datasets of such sizes are now commonplace. Regarding its computa-tional requirements, our procedure benefits from the modern shift toward parallel and distributed computing, as the vast Algorithm 1: Bootstrap Performance Diagnostic
Input : D = ( X 1 ,...,X n ): observed data for i  X  1 to k do end return true if all of the following hold, and false otherwise:  X  c 1 ,  X  i = 1 ,...,k, (1)  X  c ,  X  i = 1 ,...,k, (2) majority of the required computation occurs in the inner for loop of Algorithm 1, the iterations of which are inde-pendent and individually process only small data subsets. Additionally, we have sought to reduce the procedure X  X  com-putational costs by using an identical number of subsamples p for each subsample size b 1 ,...,b k ; one could presumably improve statistical performance by using larger numbers of subsamples for smaller subsample sizes.

The guidelines given in Algorithm 1 for setting the di-agnostic procedure X  X  hyperparameters are motivated by the procedure X  X  structure and have proven to be empirically ef-fective. We recommend exponential spacing of the b 1 ,...,b to help ensure that reliable comparisons of bootstrap per-formance can be made across adjacent sample sizes b i and b i +1 . However, by construction, setting the b 1 ,...,b k too close together should primarily cause an increase in the false negative rate (the probability that the diagnostic incor-rectly concludes that the bootstrap is not performing satis-factorily), rather than a less desirable increase in the false positive rate. Similarly, setting c 1 or c 2 to be too low should also primarily result in an increase in the false negative rate. Regarding c 3 and  X  , these hyperparameters should be deter-mined by the user X  X  bootstrap performance desiderata. We nonetheless expect that fairly lenient settings of c 3  X  X uch as c 3 = 0 . 5, which corresponds to allowing the bootstrap to deviate from ground truth by up to 50% X  X o be reasonable in many cases. This expectation stems from the fact that the actual or targeted quality of estimators on fairly large datasets is frequently high, leading to estimator quality as-sessments, such as interquantile ranges, which are small in absolute value; in these cases, it follows that a seemingly large relative error in bootstrap outputs (e.g., 50%) corre-sponds to a small absolute error.

As we demonstrate via an extensive empirical evaluation on both synthetic and real data in the following sections, our proposed bootstrap performance diagnostic is quite effective, with false positive rates that are generally extremely low or zero and false negative rates that generally approach zero as the subsample sizes b 1 ,...,b k are increased. Of course, like any inferential procedure, our procedure does have some un-avoidable limitations, such as in cases where the data gen-erating distribution has very fine-grained adverse features which cannot be reliably observed in datasets of size b k discuss these issues further below.
We first evaluate the diagnostic X  X  effectiveness on data generated from a variety of different synthetic distributions ground truth approximations for sample sizes b 1 ,...,b 3 . The right plot shows the absolute relative deviation sample size. deviations) and ground truth approximations for sample sizes b outputs for each sample size. paired with a variety of different estimators. Using simu-lated data here allows direct knowledge of the ground truth value  X  ( P,n ), and by selecting different synthetic distribu-tions, we can design settings that pose different challenges to the diagnostic procedure. For each distribution-estimator pair and sample size n considered, we perform multiple in-dependent runs of the diagnostic on independently gener-ated datasets of size n to compute the Diagnostic True Rate (DTR), the probability that the diagnostic outputs true in that setting. We then evaluate this DTR against the boot-strap X  X  actual performance on datasets of size n ; because the underlying data generating distributions here are known, we can also compare to known theoretical expectations of boot-strap consistency.

More precisely, we consider the following data gener-ating distributions: Normal(0 , 1), Uniform(0 , 10), Stu-dentT(1.5), StudentT(3), Cauchy(0 , 1), 0.95Normal(0 , 1) + 0.05Cauchy(0 , 1), and 0.99Normal(0 , 1) + 0.01Cauchy(10 4 , 1). In our plots, we denote these dis-tributions using the following abbreviations: Normal, Uniform, StuT(1.5), StuT(3), Cauchy, Mixture1, and Mixture2. We also consider the following estimators  X  (abbreviations, if any, are given in parentheses): mean, median (med), variance (var), standard deviation (std), sample maximum (max), and 95th percentile (perc). The estimator quality assessment  X  in all experiments computes the interquantile range between the 0.025 and 0.975 quan-tiles of the distribution of u ( D ,P ) =  X   X  ( D ). For all runs of the bootstrap, we use between 200 and 500 resamples, with the precise number of resamples determined by the adaptive hyperparameter selection procedure given by [13]. All runs of the diagnostic use the hyperparameter guidelines given in Algorithm 1: p = 100 ,k = 3 ,b i = b n/ ( p 2 k  X  i 0 . 2 ,c 2 = 0 . 2 ,c 3 = 0 . 5 , X  = 0 . 95. We consider sample sizes n = 10 5 and n = 10 6 .

For each distribution-estimator pair and sample size n , we first compute the ground truth value  X  ( P,n ) as the in-terquantile range of the u values for 5,000 independently generated datasets of size n . We also approximate the dis-tribution of bootstrap outputs on datasets of size n by run-ning the bootstrap on 100 independently generated datasets of this size. Whether or not this distribution of bootstrap of size n = 10 6 . outputs satisfies the performance criterion defined by c 3 that is, whether or not the  X  quantile of the absolute relative deviation of bootstrap outputs from  X  ( P,n ) is less than or equal to c 3  X  X etermines the ground truth conclusion regard-ing whether or not the bootstrap is performing satisfactorily in a given setting. To actually evaluate the diagnostic X  X  ef-fectiveness, we then run it on 100 independently generated datasets of size n and estimate the DTR as the fraction of these datasets for which the diagnostic returns true . If the ground truth computations deemed the bootstrap to be performing satisfactorily in a given setting, then the DTR would ideally be 1, and otherwise it would ideally be 0.
Figure 3 presents our results for all distribution-estimator pairs and both sample sizes n considered. In these plots, dark blue indicates cases in which the ground truth com-putations on datasets of size n deemed the bootstrap to be performing satisfactorily and the bootstrap is expected the-oretically to be consistent (i.e., the DTR should ideally be 1); red indicates cases in which neither of these statements is true (i.e., the DTR should ideally be 0); and light pur-ple indicates cases in which the ground truth computations on datasets of size n deemed the bootstrap to be performing satisfactorily but the bootstrap is not expected theoretically to be consistent (i.e., the DTR should ideally be 1). As seen in the lefthand and middle plots (which show DTRs for n = 10 5 and n = 10 6 , respectively), our proposed diagnostic performs quite well across a range of data gen-erating distributions and estimators, and its performance improves as it is provided with more data. For the smaller sample size n = 10 5 , in the dark blue and light purple cases, the DTR is generally markedly greater than 0.5; further-more, when the sample size is increased to n = 10 DTRs in all of the dark blue and light purple cases increase to become uniformly near 1, indicating low false negative rates (i.e., the diagnostic nearly always deems the bootstrap to be performing satisfactorily when it is indeed perform-ing satisfactorily). In the red cases, for both sample sizes, the DTR is nearly always zero, indicating that false positive rates are nearly always zero (i.e., the diagnostic only rarely deems the bootstrap to be performing satisfactorily when it is in fact not performing satisfactorily). Mixture2-var and Mixture2-std with n = 10 6 provide the only exceptions to this result, which is unsurprising given that Mixture2 was specifically designed to include a small heavy-tailed com-ponent which is problematic for the bootstrap but cannot be reliably detected at the smaller sample sizes b nonetheless, even in these cases, the righthand plot indicates that the ground truth computations very nearly deemed the bootstrap to be performing satisfactorily. Interestingly, the bootstrap X  X  finite sample performance for the settings con-sidered nearly always agrees with theoretical expectations regarding consistency; disagreement occurs only when Mix-ture2 is paired with the estimators mean, var, or std, which is again unsurprising given the properties of Mixture2.
We next evaluate the diagnostic X  X  effectiveness on real datasets obtained from Conviva, Inc. [6], which are routinely subjected to analysis by practitioners. Our first set of exper-iments pairs this real data with the (synthetic) estimators considered in the previous section; we then consider a larger dataset paired with a set of 268 production SQL queries. We present here the results of experiments on three real datasets obtained from [6], which describe different at-tributes of large numbers of video streams viewed by Inter-net users. These datasets are routinely subjected to a variety of different analyses by practitioners and are the subject of ongoing efforts to improve the computational efficiency of database systems by processing only data subsamples and quantifying the resulting estimation error [1].

We designate the three (scalar-valued) datasets as fol-lows, with their sizes (i.e., numbers of constituent data points) given in parentheses: Conviva1 (30,470,092), Con-viva2 (1,111,798,565), and Conviva3 (2,952,651,449). His-tograms of the three datasets are given in Figure 4; note that the datasets are heavily skewed and also contain large numbers of repeated values. Due to privacy considerations, we are unable to provide the precise values and correspond-ing frequencies represented in the data, but the histograms nonetheless convey the shapes of the datasets X  empirical dis-tributions.

To circumvent the fact that ground truth values for indi-vidual real datasets cannot be obtained, we do not directly apply our diagnostic to these three datasets. Rather, we treat the empirical distribution of each dataset as an under-lying data generating distribution which is used to generate the datasets used in our experiments. With this setup, our experiments on these real datasets proceed identically to the experiments in Section 4 above, but now with data sampled from the aforementioned empirical distributions rather than from synthetic distributions.
 Figure 5 presents the results of our experiments on the Conviva data. The color scheme used in these plots is iden-tical to that in Figure 3, with the addition of magenta, which indicates cases in which the ground truth computations on datasets of size n deemed the bootstrap to not be performing satisfactorily but the bootstrap is expected theoretically to be consistent (i.e., the DTR should ideally be 0). Given that the data generating distributions used in these experiments all have finite support, the bootstrap is expected theoreti-cally to be consistent for all estimators considered except the sample maximum. However, as seen in the righthand plot of Figure 5, the bootstrap X  X  finite sample performance is often quite poor even in cases where consistency is expected; in this regard (as well as in other ways), the real data setting of this section differs substantially from the synthetic data setting considered in Section 4 above.

The lefthand and middle plots of Figure 5 demonstrate that our diagnostic procedure again performs quite well. In-deed, the DTR is again nearly always zero (or is quite small if positive) in the red and magenta cases, indicating false positive rates that are nearly always zero. The dark blue cases generally have DTRs markedly greater than 0.5 for n = 10 5 (lefthand plot), with DTRs in these cases generally increasing to become nearly 1 for n = 10 6 , indicating low false negative rates; no light purple cases occur for the real data. Beyond these broad conclusions, it is worth noting that the Conviva2-max, Conviva2-perc, and Conviva3-med settings exhibit rather surprising behavior relative to our other results, in that the diagnostic X  X  performance seems to degrade when the sample size is increased. We believe that this behavior is related to the particularly high redundancy (i.e., degree of repetition of values) in Conviva2 and Con-viva3, and it will be the subject of future work. frequencies on a log scale.
 We finally evaluate the diagnostic X  X  effectiveness on a larger real dataset and accompanying real-world analytical work-load derived from a SQL-based ad-hoc querying system at Conviva, Inc. [6]. This dataset is 1.7TB in size and contains approximately 0.5 billion records extracted from access logs describing video streams viewed by Internet users during a thirty day time span. We treat each record, which has 104 attributes (such as video genre, web browser type, request response time, etc.), as a data point.

This data is routinely processed by data analysts, who is-sue SQL queries which are run over the dataset to compute quantities of interest. For example, a typical query might fil-ter the underlying data based on various attributes and then compute a quantity derived from other attributes (such as those given by Conviva1, Conviva2, and Conviva3 above). This computed quantity might correspond to a standard es-timator (e.g., the sample mean or percentile), or it might be the result of an arbitrary computation performed by a User-Defined Function (UDF). We study a set of 268 queries selected randomly from the set of obtainable queries which were issued in a production setting against our dataset; 113 out of these 268 queries include UDFs which may perform arbitrary computations rather than simply using standard SQL operators.

In our experiments, we treat each query as an esti-mator which seeks to compute some real-valued popula-tion quantity; our estimator quality assessment  X  is again the interquantile range used in our previous experiments. To obtain ground truth conclusions regarding bootstrap performance, we randomly split the full dataset into dis-joint chunks, each of size 5GB (approximately 2.5 million records), and compute both the point estimate and the boot-strap output for each chunk. We also run the diagnostic for each query; as in the experiments in preceding sections, we use the hyperparameter guidelines given in Algorithm 1 (i.e,. p = 100 ,k = 3 ,c 1 = 0 . 2 ,c 2 = 0 . 2 ,c 3 = 0 . 5 , X  = 0 . 95), with the exception of now using b i = 10 5 / 2 k  X  i due to the large quantity of available data.

Recall that a query typically filters its input data on one or more attributes before computing its output. To address cases in which queries are highly selective and hence effec-tively operate on severely reduced quantities of data, we do not consider the 17 out of our 268 queries which filter out more than 99% of the data in any of the diagnostic subsam-ples; one might consider the diagnostic as simply not being applicable to such queries because the available subsamples are not sufficiently large after the queries apply their filters.
Of the remaining 251 queries, the diagnostic deemed the bootstrap to be performing satisfactorily on 224, with 9 false negatives and 7 false positives. Thus, on this real dataset with accompanying query workload, the diagnostic exhibited low false negative and false positive rates of 3.6% and 2.8%, respectively.
We have presented a general diagnostic procedure which permits automatic determination of whether or not the boot-strap is performing satisfactorily when applied to a given dataset and estimator; we have demonstrated the effective-ness of our procedure via an empirical evaluation on a va-riety of estimators and simulated and real data. A number of avenues of potential future work remain. For example, it would be interesting to apply our diagnostic procedure to other estimator quality assessment methods [4, 17, 13] and to devise extensions of the diagnostic which are suitable for variants of the bootstrap designed to handle non-i.i.d. data [9, 12, 14, 16, 18]. Additionally, it should be possible to characterize theoretically the consistency of our diagnostic procedure, showing that its false positive and false negative rates approach zero as b 1 ,...,b k ,p  X  X  X  and c 1 ,c 2  X  0, un-der some assumptions (e.g., monotonicity of the bootstrap X  X  convergence to ground truth in cases where it is performing satisfactorily). It would be interesting to make such a result precise.
This research was supported in part by NSF CISE Ex-peditions award CCF-1139158 and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, Blue Goji, Cisco, Clearstory Data, Cloud-era, Ericsson, Facebook, General Electric, Hortonworks, Huawei, Intel, Microsoft, NetApp, Oracle, Quanta, Sam-sung, Splunk, VMware, and Yahoo!. Ameet Talwalkar was supported by NSF award No. 1122732. [1] S. Agarwal, A. Panda, B. Mozafari, S. Madden, and [2] R. Beran. Diagnosing bootstrap success. Annals of the [3] P. J. Bickel and D. A. Freedman. Some asymptotic [4] P. J. Bickel, F. Gotze, and W. van Zwet. Resampling [5] A. J. Canty, A. C. Davison, D. V. Hinkley, and [6] Conviva, Inc. http://www.conviva.com, November [7] B. Efron. Bootstrap methods: Another look at the [8] B. Efron. Jackknife-after-bootstrap standard errors [9] B. Efron and R. Tibshirani. An Introduction to the [10] E. Gin  X e and J. Zinn. Bootstrapping general empirical [11] P. Hall. The Bootstrap and Edgeworth Expansion . [12] P. Hall and E. Mammen. On general resampling [13] A. Kleiner, A. Talwalkar, P. Sarkar, and M. I. Jordan. [14] H. R. Kunsch. The jacknife and the bootstrap for [15] N. Laptev, K. Zeng, and C. Zaniolo. Early accurate [16] R. Y. Liu and K. Singh. Moving blocks jackknife and [17] D. Politis, J. Romano, and M. Wolf. Subsampling . [18] D. N. Politis and J. P. Romano. The stationary [19] H. Putter and W. R. van Zwet. Resampling: [20] A. W. van der Vaart and J. A. Wellner. Weak
