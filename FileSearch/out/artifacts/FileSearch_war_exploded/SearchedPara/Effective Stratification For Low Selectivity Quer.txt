 We study the problem of estimating the result of an aggregation query with low selectivity when a data source only supports limited data accesses . Existing stratified sampling techniques cannot be applied to such a problem since either it is very hard, if not impos-sible, to gather certain critical statistics from such a data source, or more importantly, the selective attribute of the query may not be queriable on the data source. In such cases, we need an effective mechanism to stratify the data and form homogeneous strata with respect to the selective attribute of the query, despite not being able to query the data source with the selective attribute.

This paper presents and evaluates a stratification method for this problem utilizing a queriable auxiliary attribute . The breaking points for the stratification are computed based on a novel Bayesian Adaptive Harmony Search algorithm. This method derives from the existing Harmony search method, but includes novel objective function , and introduces a technique for dynamically adapting key parameters of this method. Our experiments show that the esti-mation accuracy achieved using our method is consistently higher than 95% even for 0.01% selectivity query, even when there is only a low correlation between the auxiliary attribute and the selective attribute. Furthermore, our method achieves at least a five fold re-duction in estimation error over three other methods, for the same sampling cost.
 H.2 [ Database Management ]: Systems Algorithms Deep Web, Query Processing, Sampling Methods
We study the problem of estimating the result of an aggregation query with low selectivity over a data source that only allows lim-ited access to data , i.e, is limited with respect to the kind of queries it supports. By low selectivity, we refer to the queries with a selec-tion predicate that is matched by only a very small fraction of the data. Data sources with limited access to data have been studied extensively in the context of data integration and query optimiza-tion [28, 6]. More recent examples of such data sources are the deep web data sources. With the rapid growth of the size and pop-ularity of the data disseminated over the World Wide Web, a large portion of structured web data resides in the deep web . To access data in the deep web, a user must issue queries through the input interfaces of deep web data sources. Then, answers are returned to the user as dynamically generated HTML pages.

Deep web data sources often have limited data accessibility, from two different aspects. First, it is very hard, if not impossible, to obtain certain critical population statistics on deep web data. This is because many deep web data sources limit the number of queries a particular IP address can issue, or the number of data records can be returned, to protect their data from being completely downloaded, or to disallow a denial-of-service attack. Second, more importantly, deep web data can only be accessed through query interface made available on the data source X  X  web page. Therefore, certain attribute of a query may not be directly queriable over the input interface of the deep web data source.

In this paper, we propose solutions to make online aggregation [17] feasible even for low selectivity queries over deep web data sources with limited data accessibility. Answering database queries by sam-pling has been extensively studied in the literature [18, 27, 5, 3, 4], including recent work on sampling methods and estimators for low selectivity queries [20]. However, these approaches are not directly applicable to a database that offers limited data access. Similarly, sampling deep web data sources has been a topic of recent inter-est [13, 11, 12]. However, sampling for low selectivity queries has not been studied in this work.

To further illustrate our problem, let us consider the following example. Suppose we have a data set as shown in Table 1. In this data set, we have 3 attributes and 10 records. We are considering the query Q 1 ,whichis
The selection predicate of Q 1 only accepts data records from Ta-ble 1 with the value of attribute B larger than 15. From Table 1, we observe that there are only 2 records that can be accepted by the selection predicate of Q 1 . The natural way of preparing to han-dle such low selectivity query is to use stratification [29]. Specifi-cally, we can partition the entire data set into multiple sub-groups, called strata , such that the values of the selective attribute(s) of the query (attribute B in Table 1) are homogenous within each stratum. This objective is best achieved when the variability of the partition-ing attribute(s) , in our case the selective attribute(s), is minimized within each stratum, and maximized across different strata. For answering the query shown above, we could have two strata, i.e., the last two records (in bold) in one stratum, and the rest eight records in the other. After stratification, we know that the query X  X  selection predicate will favor certain strata. Then, those favored strata can be heavily sampled so as to obtain a better estimation for the query. More specifically, if a specified number of samples, n , are to be drawn from the data set, the number of samples selected from each stratum can be determined by existing sampling alloca-tion methods, such as Neyman allocation [29] or Bayes-Neyman allocation [20].

Therefore, the critical problem is to find an appropriate strat-ification , i.e., to find multiple breaking points of the partitioning attribute on which the stratification is performed. In the literature, various methods have been proposed for this purpose, including clustering based stratification [29] and outlier indexing [3]. The above methods require knowledge of either the distribution or sum-mary statistics of the partitioning attribute. While this information can be easily obtained for a relational database, this is not the case for a deep web source where the partitioning attribute is not directly queriable.

In statistical sampling, if the partitioning attribute is unknown or difficult to obtain, stratification can be performed based on an auxiliary attribute, which is closely correlated with the partitioning attribute [8]. Examples of methods which use a closely correlated auxiliary attribute include Dalenius and Hodges X  X  method [9, 10], Ekman X  X  rule [14], and Gunning and Horgan X  X  geometric regres-sion method [16]. However, all of the above methods have rigid requirements on the auxiliary attribute. For example, the auxiliary attribute and the partitioning attributes should be highly correlated and the variance of the auxiliary attribute is equal to the variance of the partitioning attribute [8], or the auxiliary attribute should be uniformly distributed within each stratum [14, 16]. In actual deep web data sources, these requirements are rarely satisfied. In our evaluation (Section 5), we actually show that the above methods do not achieve good estimation results for low selectivity queries on real data sets. In this paper, we have developed a Bayesian Adaptive Harmony Search Stratification algorithm for a non-queriable selective attribute based on an auxiliary attribute. We consider aggregation queries over deep web sources in the following format:
Here, f 1 () is any numerical function, f 2 () is a selection pred-icate with low selectivity ,and r.T arget is the target attribute on which the aggregation is performed. If r.Sel is directly queriable on the input query form of the deep web data source, the query can be answered trivially by specifying the selection predicate directly on the input query form. However, when r.Sel is not directly queri-able on the input query form, because of the limited query interface of a deep web data source, we require novel techniques.

A specific motivating example from the biology domain is as follows. Suppose we are interested in Single Nucleotide Poly-morphisms (SNPs), which are promising for explaining the genetic contribution to certain rare diseases [1]. Biologists want to obtain the average frequency of the SNPs related with a certain type of rare breast cancer . In this example, the target attribute is SNP fre-quency and the selective attribute is SNP function . SeattleSNP is a widely used deep web data source for searching SNP related information. The input interface of SeattleSNP isshowninFig-ure 1. As we can see from this figure, the SNP data can only be queried by SNP identifier and the chromosome range of a SNP. This limited query interfaces prohibits us from directly querying the selective attribute SNP function and performing stratification.
In our proposed approach, we take certain directly queriable at-tribute on the input query form, the input attribute r.Input ,asthe auxiliary attribute . We have developed an optimization to the re-cently proposed meta-heuristic algorithm, the harmony search al-gorithm [25], that we refer to as the Bayesian Adaptive Harmony Search . This algorithm finds robust stratification plans for the non-queriable selective attribute based on the auxiliary attribute. The stratification thus obtained can accurately reflects the distribution of the hidden selective attribute . In the above motivating example, we could use chromosome range as the auxiliary attribute to do stratification since there will be some correlation between the posi-tion of the chromosome on which a SNP is located and the function of the SNP.

Overall, our contributions are summarized as follows. 1. We have developed a novel stratification method to stratify a hidden selective attribute based on an auxiliary attribute. The strat-ification obtained from our algorithm accurately reflects the distri-bution of the hidden selective attribute even when the correlation between the auxiliary attribute and the selective attribute is weak. 2. Besides applying the idea of Harmony search to the stratification for low selectivity queries on the deep web, an important contri-bution of our work is an approach for automatically adapting the values of the parameters in the original harmony search algorithm based on the bayesian approach. 3. We experimentally show that the estimation accuracy, obtained with the same sampling cost, based on the stratification generated from our method is better than three existing methods by at least a factor of 5. Put another way, we incur much lower sampling cost to obtain the same level of accuracy. Furthermore, the estimation accuracy we obtained is consistently higher than 95% even for ex-tremely low selectivity query (0.01% selectivity).
In survey sampling, if the attribute we are interested in takes on different mean values in different sub-populations ,wemaybe able to obtain more precise estimates to answer a given query by taking a stratified sample [29]. Stratified sampling is widely used for answering low selectivity queries [5, 3, 4, 20].

The first step in stratified sampling is stratification , i.e., to parti-tion the data set R into k disjoint strata, with R i representing the i th stratum. Then, we have R = R 1  X  R 2  X  ...  X  R k . The attribute on which stratification is performed is refered to as the partitioning attribute . Specifically, suppose the partitioning attribute x has the range [ x 0 ,x k ] , then we need to find k  X  1 breaking points within the range [ x 0 ,x k ] , such that x 0 &lt;x 1 &lt;x 2 &lt; ... &lt; x In this way, the data records whose attribute x fall into the range [ x  X  1 ,x i ] are in stratum R i . Formally, we could consider a stratifi-cation of the partitioning attribute to be a vector of breaking points , for example, V ector ( x )=[ x 0 ,x 1 ,...,x k ] .

As a specific example, we consider a datset that captures sales of used cars. Suppose we are interested in estimating the average price of used cars with mileage less than 50000. We could stratify the data set on the attribute Mileage . A possible stratification could be as follows: [0 , 10000] , [10000 , 40000] , [40000 , 100000] , and [100000 , 300000] . In this example, the breaking points are mileage 10000, 40000, and 100000.

When stratification is performed, the size of the stratified sample we need to choose from each stratum R i can be determined by a sample allocation method , whihc will need to meet the constraint P n i = n ,where n i is the number of samples we drawn from stratum R i ,and n is a pre-defined total number of samples.
In sampling a hidden data source, i.e. a data source supporting only a limited access, for a low selectively query, we also need determine the breaking points of the partitioning attribute for strat-ification. The key challenge we face is that the selective attribute of the query we are interested in, may not be directly queriable. As a result, stratification problem becomes significantly harder. The only option is to stratify or partition attribute with a queriable or an auxiliary attribute . Returning to the used car sale example, target attribute we are interested in is Price , and the the selective at-tribute is Mileage . Suppose Mileage cannot be queried on, and the input query form of the data source only provides direct access to the attribute Year of Car . Therefore, we need to take Year of Car as the auxiliary attribute , and create strata by choosing break points on these values. In choosing these break points, the key consideration is to be able to efficiently find sufficient number of useful samples , i.e. samples that meet the selection predicate based on Mileage , for the given low selectivity query.
The harmony search (HS) algorithm is a meta-heuristic algo-rithm that was developed in 2005 [25]. Harmony search is a phenomenon-mimicking algorithm inspired by the improvisation process of mu-sicians. In the HS algorithm, each musician (decision variable) plays (generates) a note (a value) for finding the best harmony (global optimum). Harmony search algorithm had been very suc-cessful in a wide variety of optimization problems [15, 25], and offers several advantages with respect to traditional optimization techniques [26].

Harmony search algorithm tries to find a vector X that optimizes (minimizes or maximizes) a certain objective function. The HS algorithm consists of the following five steps [25].
 Step 1: Initialize the optimization problem and algorithm parame-ters. The optimization problem is specified as where f ( X ) is the objective function; x i is a decision variable in the vector X , X i is the set of possible values (range) for each de-cision variable x i ,and N is the number of decision variables. To optimize the objective function, we need to specify the following harmony search parameters: the harmony memory size (the num-ber of solution vectors in the harmony memory), harmony mem-ory consideration rate ( HMCR ) and pitch adjusting rate ( PAR ). The HMCR and PAR parameters both range from 0 to 1 and determine the generation of a new harmony vector. HMCR in-dicates our confidence on the existing solutions in the current har-mony memory. If HMCR is set to be close to 1 , it shows that we have high confidence that the existing solutions are close to the optimal one. However, if HMCR is set to be close to 0 , it means that we have low confidence on the existing solutions, and we are intend to obtain new solutions outside the current harmony mem-ory. PAR indicates the likelihood we assign a random shift to the value of a decision variable. A large PAR indicates that we try to aggressively change the value of a decision variable, and a small PAR suggests otherwise.
 Step 2: Initialize the harmony memory: The harmony memory HM is a M  X  N matrix, where N is the number of decision vari-ables and M is the size of the harmony memory. In other words, the harmony memory contains M randomly initialized solution vectors of the objective function. An example of initialized harmony mem-ory is shown as follows.
 Step 3: Improvise a new harmony from the harmony memory: A new harmony vector, x =[ x 1 ,x 2 ,...,x N ] is generated in a ran-domized fashion based on the value of the HMCR and PAR pa-rameters. Specifically, the value of the design variable x chosen in two steps.

In the first step, we consider the harmony memory considera-tion rate, i.e., we choose an existing harmony or we generate a new harmony from an appropriate domain. Suppose we have a biased coin with head probability of HMCR , the value of x i is gener-ated based on either of the following two outcomes of tossing the coin. If the head of the coin is up, the value of x i should be chosen from any value of the i th decision variable in the current harmony memory. If the tail of the coin is up, the value of x i should be chosen from any possible value within the range of the i th variable, which is X i .

In the second step, for each x i , we determine whether it should be pitch-adjusted by a random distance by considering the PAR parameter. Similarly, the value of x i can be either shifted by a random distance with the probability PAR , (i.e. not shifted with the probability 1  X  PAR ).

Overall, the rule for generating a new value for x i is summarized as follows ( w/p stands for with probability ). Step 4: Update harmony memory: If the new harmony vector is better than the worst harmony vector in the current harmony mem-ory (in terms of the objective function value), the worst harmony vector in the harmony memory is replaced by the new harmony vector.
 Step 5: Termination: The above iterative procedure is terminated until a termination condition is satisfied, for example, when the total number of iterations exceeds a threshold or when we achieve convergence in the objective function.

Our proposed Bayesian adaptive harmony search stratification algorithm derives from Harmony search. However, we had to ad-dress the following challenges. First, we need to map the stratifica-tion problem on a hidden data source to the harmony search model. Second, we had to develop an objective function that would be ap-propriate for low selectivity queries. Third, since execution time is critical while executing a query, we developed a Bayesian frame-work to dynamically update HMCR and PAR parameters, and help the search converge faster. Details of these are presented in the next 2 sections.
In this section, we give an overview of our entire algorithm. Our approach comprises of the following three steps. First, we map the stratification problem to the harmony search model. Second, we apply the harmony search algorithm to find the best stratification. To enable this step, we have developed a novel Bayesian adaptive framework to adjust the two parameters in the original harmony search algorithm, which are the harmony memory consideration rate and the pitch adjusting rate. The Bayesian adaptation method is detailed in Section 4. Finally, given the stratification obtained from the previous step, we apply a sample allocation method we have developed to allocate samples into each stratum.
To estimate the answer of a low selectivity query accurately, we need to find the best stratification of the selective attribute, from the view-point of most accurately reflecting its distribution and being able to efficiently obtain sufficient number of samples that match the selection predicate. Let us suppose a selective attribute has a distribution as illustrated in Figure 2(a). A good stratification of this selective attribute is illustrated by the vertical reference lines in the Figure 2(a). Suppose a low selectivity query asks for data with the values in the range between 1.2 and 1.8. Using the shown stratification, a sample allocation method could obtain good esti-mation result by heavily drawing samples from the last stratum, ( [1 . 2 , 2 . 1] ).

As we have described earlier, our focus is on cases where the selective attribute is not accessible through the input interface of the deep web data source. In such a case, we use an attribute available in the input query form as the auxiliary attribute . Unlike many existing methods in statistics, we cannot assume that there be a strong correlation between the auxiliary attribute and the selective attribute, since this assumption is not likely to hold on real data sources. In Figure 2(b), the solid line shows the distribution of the selective attribute, and the dotted line shows the distribution of an auxiliary attribute. In this example, the correlation between them is 0.35. By applying our approach, we are trying to find a stratification of the auxiliary attribute (shown as the vertical lines in Figure 2(b)), which is sufficiently close to the stratification we could obtain for the selective attribute.

Harmony search can be viewed as a guided search process, which can stratify the auxiliary attribute in a way that we can efficiently obtain samples meeting the selection predicate, and accurately es-timate the aggregation function. In Section 2, we had described that the harmony memory consists of a list of decision vectors. In our specific problem scenario, we consider each stratification of the auxiliary attribute as a decision vector. In other words, each decision variable in our case is a breaking point value in the range of the auxiliary attribute. For example, the stratification shown in Figure 2(b), [0 . 25 , 1 . 05 , 1 . 25] , is a decision vector in our harmony memory. In statistics literature, Cochran [7] concluded that in strat-ified sampling, there is typically not much further reduction in vari-ance from having more than 6 strata. Based on this observation, in our implementation, the size of the decision vector is set to be 6, though clearly, our approach can apply in other cases as well. Figure 2: Example: Auxiliary Attribute Based Stratification: (a) Stratification for Selective Attribute, (b) Auxiliary Attribute Based Stratification
At the beginning, the initial harmony memory is randomly gen-erated. Suppose there are M decision vectors in the harmony mem-ory, i.e., the size of the harmony memory is M . Let the range of the auxiliary attribute be X . Then, each decision vector is generated in the following three steps. First, we generate a random number, de-noted as k , between 1 and 6 using a uniform distribution. This is the number of strata represented by this decision vector. Second, we need to generate k  X  1 breaking points. Therefore, we generate k  X  1 random numbers within the range of the auxiliary attribute, which is X , using a uniform distribution. Each such generated number is a breaking point. Third, we form a decision vector of size 6 by ranking the k  X  1 breaking points in an increasing or-der and randomly inserting 6  X  k number of  X  .The  X  indicates an empty position in the stratification.

The above procedure is repeated M times to initialize the har-mony memory. An example of a harmony memory in our approach is shown as follows.

The above harmony memory shows 5 stratification candidates (decision vectors). The first decision vector corresponds to the where X is the range of the auxiliary attribute.
Given the initial harmony memory, our stratification search al-gorithm works as follows. 1. For each initial stratification candidate in the harmony memory, compute its cost score . The method for computing this metric will be described later. A stratification candidate with a lower cost score is preferred. 2. Generate a new stratification candidate according to the origi-nal harmony search algorithm described in Section 2.2. However, a key distinctive aspect of our work is that in the search algorithm, the two harmony search parameters, HMCR and PAR , are adapted according to our Bayesian framework. This method will be de-scribed later in Section 4. 3. For each newly generated stratification candidate, compute its cost score, and update harmony memory using the newly generated stratification candidate. 4. Repeat the above procedure until either of the following two termination conditions is satisfied: 1). the number of iteration ex-ceeds a pre-defined threshold, or 2). the average accuracy of the query obtained using the best stratification candidate in the current harmony memory is lower than a pre-defined threshold.

The method listed above clearly depends on the calculation of a cost or objective function , which evaluates the quality of the strat-ification. We now describe our method for evaluating the qual-ity of stratification. More specifically, given a stratification vector x =[ x 1 ,x 2 ,...,x k ] on an auxiliary attribute, our goal is to calcu-late the fitness of the stratification with respect to the distribution of the selective attribute.

In the traditional statistical methods, one prefers a stratification vector if the selective attribute of the data records in each stratum is homogenous. To test this condition, we could draw a random pilot sample of equal size from each stratum of the stratification, and compute the summation of the sample variance of the selective attribute of the pilot sample.

However, only considering the sum of variance condition is not sufficient for justifying a good stratification in our scenario, since the auxiliary attribute may not be highly correlated with the selec-tive attribute. Thus, only using sum of variance may not ensure that we can efficiently obtain samples where the selection predi-cate will be true. Instead, we need to consider another measure in the objective function, which we call the precision of a stratifi-cation. The precision of a stratification is defined as the percent-age of the pilot sample (from current stratification) that satisfies the low selective predicate specified in the given query. The pre-cision of the i th stratum in a stratification is formally defined as sample from the i th stratum. Recall that f 2 is the selection predi-cate in the query. Stratification that involves a higher precision can help us in finding more data records satisfying the low selectivity predicate.

Formally, given a stratification x , we draw a random pilot sam-ple from each stratum R i , and compute two measurements. The first one is  X  2 i , which is the sample variance, and the second one is pre i , which is the precision. Then, the objective function for the stratification x is defined as
Clearly, the above approach involves a significant sampling over-head during stratification itself. However, the stratification ob-tained allows us to obtain samples matching the selection predicate more effectively. Our experiments show that despite this overhead, we can obtain the same number of samples matching the selection predicate with a lower sampling cost.
The problem of sample allocation is to determine the number of samples n i that need to be drawn from each stratum R i to conduct the final estimation for the query. Suppose the total number of samples used to answer a query is pre-defined as n .Then sample allocation must satisfy the constraint widely used sample allocation method is Neyman Allocation [29]. The Neyman allocation states that the variance of the estimator is minimized when sample size n i is proportional to the size of the stratum, N i , and to the sample variance of the selective attribute in the stratum,  X  2 i .Thatis,
We can see that Neyman allocation only captures one measure-ment of our stratification objective function, the sample variance, but not the second measurement, the precision. Based on the de-scription in Section 3.2, a stratum should be sampled more heav-ily if one of the following two conditions hold: 1) The values of the selective attribute in the stratum is more heterogenous, i.e., the sample variance of the selective attribute from the stratum is larger, and 2) The stratum covers more data records that satisfy the query X  X  selective predicate, i.e., the stratum has a higher precision.
To capture the above two measurements, we propose a novel sample allocation method. In our method, the sample size n proportional to the sample variance of the selective attribute in the stratum, and as well as the precision of the stratum. Formally,
From the description of harmony search in Section 2, we know that two parameters are very important for the performance of the harmony search algorithm, since they enable new, probably better, harmony vectors into the harmony memory. These two parame-ters are harmony memory consideration rate ( HMCR ) and pitch adjusting rate (PAR) . As we explained earlier, HMCR indicates the probability that a new harmony vector is generated using the current solution candidates in the harmony memory, and PAR in-dicates the probability that we shift the values of a newly generated harmony vector by a random distance.

In the original harmony search algorithm [25], these two param-eters are fixed during the entire process, though their initial values are chosen carefully. In applying harmony search idea to query processing, efficiency (rate of convergence) is a critical issue. We observe that the values of these two parameters should be dynam-ically adapted to achieve faster convergence. Specifically, at the beginning of the algorithm, we want to bring in more new harmony vectors into the harmony memory, as the randomly generated ini-tial harmony vectors are less likely to be good. Thus, a small value of HMCR and a high value of PAR is preferred. As the algo-rithm progresses, the existing harmony vectors may be quite close to the global optima. If we still update the harmony memory ag-gressively, we may diverge from the optima. As a result, we may want HMCR to become high and PAR to be low.

Figure 3 illustrates the adaptation pattern of HMCR parame-ter on a dataset. In Figure 3, the x-axis represents the value of Figure 3: Example: adaptation Pattern of HMCR Parameter HMCR , and the y-axis shows the percentage of newly generated harmony vectors having better objective function scores than the worst harmony vector in the current harmony memory. We show results for four phases of the execution of the algorithm, denoted as phase 1 (beginning) through phase 4 (end). During this exper-iment, the PAR is not changed. From Figure 3 we observe that, during the phase 1 the highest fraction of newly generated harmony vectors are better when HMCR =0 . 15 . During the phase 2, the peak occurs when HMCR is around 0.55, at phase 3, HMCR around 0.7 gives the best results, and at phase 4 (near the end of the harmony search algorithm), the best HMCR value for generating better harmony vector is about 0.85. The PAR parameter has a similar trend as the HMCR parameter.

While the above experiment demonstrates the benefit of increas-ing HMCR during the execution of the algorithm, the desired adaptation pattern of the two parameters could vary depending upon the dataset and the query. Thus, in our approach, the adaptation pat-tern at each iteration is inferred from the statistics of the harmony vectors we obtained from the previous iteration of the algorithm. We now describe this method.
 Bayesian Adaptation Overview: The Bayesian approach is a method which could let us estimate some unknown parameters (posterior distribution) based on prior knowledge or beliefs (prior distribu-tion) [2]. In the Bayesian approach, based on a list of observed data y , we want to estimate an unknown parameter  X  . The unknown pa-rameter is usually in the form of a probability distribution f ( y We could suppose  X  has a prior distribution  X  (  X  |  X  ) ,where  X  is a vector of hyperparameters . Inference concerning  X  is then based on its posterior distribution, given by
In our specific scenario, at each iteration of the harmony search algorithm, the data we can observe is the harmony parameter values ( HMCR and PAR ) that generate better stratification candidates. The variable we want to estimate is the adaptation pattern of the these parameters.

Our Bayesian adaptive algorithm works as follows. We assume the adaptation patterns of the two harmony search parameters HMCR and PAR follow the probability distribution f HMCR ( HMCR and f PAR ( PAR |  X  ) . The parameter  X  determines the adaptation pattern of the harmony parameters.  X  is unknown and needs to be estimated. Next, we represent our belief in  X  as a prior probability distribution.

In our method, we take a pilot sample from all possible val-ues of the harmony parameters HMCR and PAR , and select the ones, denoted as HMCR opt and PAR opt , which generate the best stratification candidate with respect to the current harmony mem-ory. HMCR opt and PAR opt is our observed data. Based on our observed data HMCR opt and PAR opt , we update the value of the unknown parameter  X  , i.e., compute the posterior distribution . Finally, suppose the estimated value of the unknown parameter  X  is  X   X  , then the new adapted harmony search parameters HMCR and PAR for the next iteration of the algorithm could be com-puted from the probability distribution f HMCR ( HMCR |  X  f PAR ( PAR |  X   X  ) .

We next describe our Bayesian adaptive approach for estimat-ing HMCR in detail. The adaptation for PAR parameter can be performed similarly.
 Determining Probability Function f HMCR ( HMCR |  X  ) : To ap-ply Bayesian approach, we need to find a probability distribution which could reflect the pattern of the change of HMCR during the entire procedure of harmony search. Learning from Figure 3, we know that the change of HMCR has the following features. First, HMCR ranges from 0 to 1, and second, HMCR starts with small value and then increase along with the iterative procedure of harmony search. In statistics, the Beta distribution best mimics the above two features. The Beta distribution is a family of continuous probability distributions defined on the interval (0 , 1) parameter-ized by two positive shape parameters, typically denoted by  X  and  X  . Figure 4 shows the probability density function of Beta distribu-tion with fixed  X  =5 and varying  X  from 2 to 30.

Therefore, we use a Beta distribution with fixed  X  value to repre-sent the probability function f HMCR () . The change of the parame-ter  X  of the Beta distribution reflects the change of HMCR during the iteration of the harmony search algorithm. Thus, the parameter  X  of the Beta distribution is the unknown parameter  X  we need to estimate in our Bayesian adaptive method, and we use  X  to repre-sent the first parameter of a Beta distribution. Thus, to summarize, the probability function f HMCR ( HMCR |  X  ) is Determining Prior Distribution and Bayesian Updates for  X  : In Bayesian analysis, the value of  X  is estimated after the values of HMCR are observed. The prior distribution of  X  expresses one X  X  belief about  X  before the data is observed. In other words, in our problem, the prior distribution indicates our belief on the value of  X  even before the execution of the harmony search algorithm. We know that, for a Beta distribution with fixed  X  parameter, the shape of the Beta distribution is determined by the relation between the values of  X  and  X  .If  X &lt; X  , the peak of the Beta distribution occurs before HMCR =0 . 5 ,andif  X &gt; X  , the peak of the Beta distribu-tion occurs after HMCR =0 . 5 . Furthermore, from Figure 3, we know that, roughly, HMCR changes from small values (smaller than 0.5) to large values (larger than 0.5) during the execution of the harmony search algorithm. This suggests that a reasonable prior belief for  X  will generate small HMCR values. Formally, the prior distribution of  X  is then:
Now, for the parameter HMCR , we have the probability func-tion f HMCR ( HMCR |  X  ) and the prior distribution  X  (  X  ) . The pos-terior estimation for the unknown parameter  X  could be computed using the following expression [2]
In this section, we evaluate our Bayesian adaptive harmony search stratification algorithm. First, we examine the performance of our algorithm using datasets with different correlation between the aux-iliary and the selective attributes, as well as using queries with dif-ferent selectivity. Second, we compare the Bayesian adaptive ver-sion of our algorithm with a non-adaptive version of the algorithm. Finally, we compare our method with three other existing sampling methods. Here, we focus on two aspects: total sampling cost for obtaining the same number of samples meeting the selection pred-icate, and the accuracy achieved with the same sampling cost.
We have used 7 datasets, including five synthetic datasets and two real data sets.
 Synthetic Data sets: These were generated using MINITAB 2 statistical software package. We generated five datasets. As men-tioned in Section 2, our method performs stratification on the selec-tive attribute based on an auxiliary attribute, and therefore, the cor-relation between the auxiliary attribute and the selective attribute is important for our algorithm. These five synthetic datasets have dif-ferent correlation values, i.e., 1, 0.85, 0.7, 0.5, and 0.3, respectively. Each synthetic dataset has 100,000 data records and 3 attributes, which are used as the auxiliary attribute, the selective attribute, and the target attribute, respectively.
 US Census Dataset: The Census dataset, denoted as IBQ , com-prises of the 2002 US Economic Census data on Wholesale Trade Product Lines listed by the Kind of Business. This dataset can be downloaded at American FactFinder 2 . We have 6 attributes and 24984 data records. We use profit as the target attribute, sale as the selective attribute, and number of establishments as the auxiliary attribute. An example low selectivity query that we could issue on IBQ dataset is  X  X ind the average profit of all kind of business whose sale amount during the year is smaller than $10,000. X  Suppose sale is not directly queriable, we need to an-swer the query based on the stratification generated for the of establishments attribute. The correlation between the aux-iliary attribute ( number of establishments ) and the selective attribute ( sale ) is 0.7.
 Yahoo! Auto Data set: The Yahoo! Auto dataset, denoted as Auto , comprises of the data crawled from http://autos.yahoo.com/ . Particularly, it includes the data on used cars from any model lo-cated within 50 miles of a zipcode address. This yields a dataset with 4000 data records. We consider price as the target attribute, mileage as the selective attribute, and year as the auxiliary at-tribute. An example low selectivity query that we could issue on Auto is  X  X ind the average price of the used cars with mileage smaller than 5000 miles. X  The correlation between the auxiliary attribute ( year ) and the selective attribute ( mileage )is 0 . 56 . http://www.Minitab.com http://factfinder.census.gov/ Figure 5: Evaluating HarmonyAdp on Different Datasets and Queries on Synthetic Data Figure 6: Evaluating HarmonyAdp on Different Datasets and Queries on Real World Data
In this experiment, we evaluate the performance of our approach on datasets with different correlation and queries with different se-lectivity. We first briefly describe how our experiment is conducted. For each query, we first find the stratification of the dataset using our method. Then, we draw a sequence of 5 independent random stratified samples using the sample allocation method proposed in Section 3.3. The query result is estimated independently using each stratified sample set. Finally, the average of the 5 independent esti-mation results are reports as the final estimation. All queries con-sidered in our experiments are low selectivity queries with an ag-gregation function that averages the target attribute.

Throughout our experiment, we use Absolute Error Rate (AER) as our evaluation metric. AER reflects the accuracy of an estimator. For a variable with true value  X  and estimated value  X   X  ,theAERof the estimator  X   X  is AER (  X   X  )= |  X   X   X   X  |  X  .

Figure 5 shows the result on the 5 synthetic datasets, and Fig-ure 6 shows the result on the 2 real world datasets. In Figure 5, each sub-figure shows the result on a dataset with different data correlation ranging from 1 to 0.3. The x-axis represents the num-ber of iterations performed in the HarmonyAdp algorithm, and the y-axis is the AER of the query result estimation obtained. In this experiment, we consider three query selectivity values, which are 1%, 0.1%, and 0.01%.

From Figure 5, we have the following observations. First, as ex-pected, with the increase in the number of iterations, the accuracy of the estimation results becomes higher for all correlation cases and all query selectivity values. When the number of iterations is around 40, corresponding to about 2000 samples (about 2% of dataset size), the estimation error rate for 1% selectivity query is below 5% for all cases, and even for 0.01% selectivity queries, the error rate is below 15%. This shows that our method HarmonyAdp is very appropriate for answering low selectivity queries using lim-ited number of samples (i.e. only 2% of total size of the data). Sec-ond, for all data correlation cases, while the estimation accuracy decreases between the 1% and 0.01% selectivity queries, the dif-ference between the error rate of 1% and 0.01% selectivity query is not significant. This shows that our approach is robust with respect to different query selectivity levels. Third, we observe that the error rates for queries of all selectivity levels do not vary too much across datasets with different data correlation. Even for a very low corre-lation, for example 0.3, the error rates achieved when the number of iteration is 40 range from 5% to 18% for different query selec-tivity. This shows that our method is also robust in terms of data correlation.

In Figure 6, the left sub-figure shows the result on Auto dataset, and the right sub-figure shows the result on IBQ dataset. Here, we consider three query selectivity levels 2%, 1%, and 0.5%. Here, we have similar observations as in Figure 5. For the Auto dataset, when the number of iteration is about 20 (7% of total data size), the error rates are always below 13% for all selectivity levels, and the error rates are about 8% for the 2% selectivity query. For the IBQ dataset, when the number of iteration is about 40 (8% of total data size), the error rates are always below 16% for all selectivity levels, and the error rates are about 10% for 2% selectivity query. The correlation of the IBQ date set is about 0.5, which is not very high. This further shows that our approach works quite well on real a dataset with relatively modest correlation.
In this experiment, we compare the performance of our Bayesian adaptive version of the algorithm, HarmonyAdp, with a non-adaptive version of the algorithm. In the non-adaptive version of the algo-rithm, denoted as SmartFixed, the MHCR and PAR parameters are fixed to 0.8 and 0.5 respectively. These two parameter values are suggested (and used) in the original harmony search algorithm publication [25].

Figure 7 shows the results on all combinations of correlations and query selectivity values. From all the cases shown in Figure 7, we observe that, when the number of iteration is small, the error rate of the adaptive version is lower than the non-adaptive version, especially for queries with extreme low selectivity (such as 0.01%). However, with the increase of the number of iterations, the perfor-mance of the non-adaptive version becomes close to the adaptive version. This is an expected result, since the goal of the adaptive version is to achieve a good stratification faster.

Recall that we need extra samples to perform parameter adapta-tion, i.e. there is an additional cost for HarmonyAdp over Smart-Fixed in every iteration. Thus, we also examine the reduction in the total sampling cost achieved while meeting the same accuracy. We observe that the estimation accuracy of HarmonyAdp achieved at 10 iterations is very close to the estimation accuracy of SmartFixed achieved at 20 iterations in our experiments. For the synthetic data sets, the sample size is 50 in each iteration. The number of addi-tional samples used in parameter adaptation is 340. Thus, the total sampling cost for 20 iterations for SmartFixed is 1000, whereas, for HarmonyAdp, it is 840. Thus, through the adaptive method we have introduced, there is a 16% reduction in the sampling cost to achieve the same quality of estimation. Similar results are also observed for the two real datasets.
We now compare our method with three other methods. They are Leaps and Bounds stratification method [16] (denoted as L&amp;B), Dalenius and Hodges stratification method [9, 10] (denoted as D&amp;H), Figure 7: Comparing Adaptive and Non-Adaptive Algorithm Figure 8: Four Methods Comparison on Total Sample Size on Synthetic Data and No stratification (Random Sampling) method (denoted as Ran-dom). The L&amp;B and D&amp;H methods are classical optimal stratifi-cation methods from statistics literature and are both based on an auxiliary attribute.
 Leaps and Bounds (L&amp;B) : This method assumes that the auxil-iary attribute x is strongly correlated with y . Besides the corre-lation assumption, the L&amp;B method is based on two more impor-tant assumptions, which are uniform distribution within stratum, and equal coefficients of variance in each stratum of the auxiliary attribute. While these three assumptions lead to a much simpler method, which does not require iteration. According to L&amp;B, a L-stratum stratification on an auxiliary attribute is determined by the following stratum boundaries Dalenius and Hodges (D&amp;H) : As in the L&amp;B method, the basic assumption of the D&amp;H method is the existence of strong correla-tion between the auxiliary attribute and the unknown attribute. Fur-thermore, the D&amp;H method also critically depends on the assump-tion that the distribution of the auxiliary attribute in each stratum is approximately uniform.
 No Stratification (Random) : This is a very simple method which does not stratify the dataset. Instead, we just select a random sam-ple from the entire dataset and perform the estimation.

We conduct two experiments to compare our method HarmonyAdp with the these three methods. The first experiment is as follows. To answer low selectivity queries on the deep web, we will obviously prefer a method that can be more effective in obtaining records sat-isfying the selection predicate (refered to as useful samples ). This Figure 9: Four Methods Comparison on Total Sample Size on Real World Data is because the number of useful samples directly impacts the ac-curacy of the estimation, and because total querying cost over the deep web needs to be minimized. Therefore, in this experiment, we compare the total number of samples that need to be selected (i.e., the sampling cost involved), to obtain the same number of useful samples , across the four methods. For HarmonyAdp, the total number of samples includes the samples used in training our model and query estimation. Our goal is to show that HarmonyAdp is a useful method despite the sampling involved while training our model.

Figure 8 shows the results on synthetic data. Here, the number of useful samples obtained is fixed at 25. The Y-axis in Figure 8 is in logarithmic scale. We have the following observations. First, for simpler queries (10% selectivity), all four methods use nearly the same number of total samples to obtain 25 useful samples. Second, for more challenging queries (selectivity smaller than 10%), our al-gorithm only needs about one-tenth of the total number of samples to achieve the same number of useful samples. Third, with reduc-ing selectivity of the queries, L&amp;B, D&amp;H, and the Random method have a linear increase in the total number of samples needed. How-ever, the increase in total number of required samples is very small for our method. This shows that our stratification method is very effective for selecting samples satisfying the low selectivity clause. Figure 9 shows the results on real world data. For the Auto and IBQ dataset, the number of useful samples is fixed to be 10 in the experiment. We observe a similar pattern, i.e., our method always uses the least number of total samples to obtain a specified number of useful samples. Finally, in this experiment, we also compared the AER values achieved by the four methods when the number of useful samples is the same. As expected, all four methods ob-tain similar accuracy when aggregation is performed over the same number of useful samples.

In the second experiment, we compare the estimation accuracy obtained from the four methods when the total sample size is fixed to be the same for the four methods. For our method, the total sam-ple size includes the samples used in training the harmony search model and the samples used in estimating query results. However, once the search model has been trained, we obtain more useful samples from fewer total number of samples. Figure 10 shows the results on synthetic datasets. Here, the sample size is fixed to be 2000 for all four methods (2% of total data size). Specifically, for our HarmonyAdp method, among the 2000 samples, 1500 are used in training, and 500 are used in query estimation. For other three methods, all 2000 samples are used in query estimation. We have the following observations. First, when the query selectivity is 10%, all of the methods achieve good results, across different cor-relation levels. Second, with a decrease in query selectivity, L&amp;B, D&amp;H and Random methods degrade severely, but our method only Figure 10: Four Methods Comparison on AER on Synthetic Data Figure 11: Four Methods Comparison on AER on Real World Data incurs a small reduction in accuracy. For 0.1% selectivity queries, HarmonyAdp outperforms other three method by a factor of 5 on the average. For 0.01% selectivity queries, the error rate of our method is consistently lower than 18%. Figure 11 shows the re-sults on two real world datasets. For the Auto dataset, the sam-ple size is fixed to be 300 (7% of the entire data) for all methods. Among these 300 samples, 50 samples are used for query estima-tion in HarmonyAdp. For the IBQ dataset, the sample size is fixed to be 2500 (10% of the entire data), and among the 2500 samples, 500 samples are used for query estimation in HarmonyAdp. From Figure 11, we can observe a similar pattern as in Figure 10. It may be argued that comparison of our method with L&amp;B and D&amp;H methods is not a very fair one, since the latter two meth-ods involve assumptions which did not hold true on these datasets. Similarly, random sampling is a very simple method, and it is not surprising that it does not do well on low selectivity queries. The primary conclusion, however, that we will like to make from our ex-periments is as follows. HarmonyAdp is the first effective method for answering low selectivity queries on data sources that allow only limited data accesses, and where the queriable attribute is not necessarily highly correlated with the selectivity attribute. Our ex-periments have demonstrated the robustness of our method when selectivity and/or correlation are decreased.
We now compare our work with existing work on related topics, including optimal stratification, answering queries over the deep web, answering low selectivity queries, and adaptive harmony search. Optimal Stratification: The method of choosing the best bound-aries that make strata internally homogenous is known as optimum stratification. Ideally, stratification should be done using the survey variable y . However, in many cases, y is unknown, and as a result, many stratification methods have been proposed to perform strati-fication based on an auxiliary variable x . The well-known cumula-tive root frequency method of stratum construction (D&amp;H method), derived by Dalenius and Hodges [9, 10], depends critically on the assumption that the distribution of x in each stratum is approximate uniform. To further simplify the computation of the D&amp;H method, Ekman [14] and Gunning and Horgan [16] have both proposed sev-eral other approximation methods. These methods are based on the Dalenius and Hodges X  X  assumption, and even add more assump-tions on the auxiliary variable x . As we have experimentally shown and analyzed in Section 5.4, these existing methods are not appro-priate for our problem because of the following two reasons. First, when the correlation between the x and y is weak, the assump-tions in these methods do not hold true. Second, these methods are not designed for low selectivity queries. There are stratifica-tion methods proposed for data following certain distribution, such as uniform and right triangular distributions [22], exponential [23] and triangular distribution [24]. However, in our problem, we need to handle dataset with an arbitrary and/or unknown distribution. Answering Database (Low Selectivity) Queries Using Sampling: Answering database queries by sampling has been extensively stud-ied in the literature [18, 27]. Chaudhuri et al [5, 3, 4] have con-ducted extensive studies on approximate aggregation queries an-swering using stratification. The approaches include partitioning the database into fundamental regions [4, 5] and generating outlier indexing [3]. The above approaches cannot be applied in the deep web scenario, as they need to know critical statistics or distribution about dataset. With support for only limited data accesses, as is the case on the deep web, this is not possible. Joshi et al designed sampling methods and estimators for low selectivity queries [20] using Bayesian-Neyman Allocation. The key difference between our work and their approach is that they only focus on allocating samples into stratum, whereas we also consider determining opti-mum stratification.
 Answering Queries Over the Deep Web: The prominent work on deep web sampling and query answering includes online aggrega-tion [17, 19] and hidden web sampler developed by Dasgupta et al. [13, 11]. In online aggregation, approximate answers for aggre-gation queries are generated and further refined when all data has been processed. Jermaine et al [19] proposed a scalable online ag-gregation method within the DBO engine. The hidden web sampler focuses on generating true random samples from the deep web. Re-cently, Dasgupta et al [12] developed a unbiased estimator for size and other aggregates over hidden web data sources. Their approach could use a small number of queries to produce unbiased estimates with a small variance. Joshi et al designed sampling methods and estimators for subset-based queries [21]. The key distinct aspect of our work is that we focus on handling low selectivity aggregation queries.
In this paper, we have studied the problem of estimating the re-sult of an aggregation query with low selectivity on data sources that only support limited data access. The key challenge we have addressed is of stratification when the selective attribute is not di-rectly queriable. Thus, we need to perform stratification on a que-riable attribute, which may not necessarily be highly correlated with the selectivity attribute. We have developed a guided search method, which is motivated by the recently developed Harmony search model, to stratify the queriable attribute in a way that useful samples for a low selectivity query can be effectively obtained.
We have evaluated our method using both synthetic and real datasets. We show that the query estimation result using the strat-ification generated from our approach have a high accuracy (about 95%) for low selectivity queries, while querying only 2% of the data. The accuracy achieved using our method is higher than 85% even for extremely low selectivity queries (selectivity 0.01%). Fur-thermore, our method is robust even when the correlation between the auxiliary attribute and the selective attribute is relatively low.
