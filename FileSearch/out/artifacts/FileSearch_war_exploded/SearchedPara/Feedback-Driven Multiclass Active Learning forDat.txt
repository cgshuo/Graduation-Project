 Active learning is a promising way to efficiently build up training sets with minimal supervision. Most existing meth-ods consider the learning problem in a pool-based setting. However, in a lot of real-world learning tasks, such as crowd-sourcing, the unlabeled samples, arrive sequentially in the form of continuous rapid streams. Thus, preparing a pool of unlabeled data for active learning is impractical. More-over, performing exhaustive search in a data pool is expen-sive, and therefore unsuitable for supporting on-the-fly in-teractive learning in large scale data. In this paper, we present a systematic framework for stream-based multi-class active learning. Following the reinforcement learning frame-work, we propose a feedback-driven active learning approach by adaptively combining different criteria in a time-varying manner. Our method is able to balance exploration and ex-ploitation during the learning process. Extensive evaluation on various benchmark and real-world datasets demonstrates the superiority of our framework over existing methods. H.2.8 [ Information Systems ]: Database Application X  data mining ; I.5.2 [ Pattern Recognition ]: Design Methodol-ogy X  classifier design and evaluation Algorithms, Experimentation Active Learning, Stream Data Mining, Reinforcement Learn-ing, Adaptive Criteria With the massive amount of data produced by various sources, from sensor networks to social network, substantial efforts have been devoted to efficiently collecting labeled data. Ac-tive learning methods [21] provide a way to automatically pinpoint informative examples for which labels should be requested, thereby reducing labeling cost without sacrific-ing accuracy in the model. Recent results have shown that active selection can benefit object detection, image/video classification, machine translation systems etc.
 Currently, active learning models primarily focus on pool-based setting, where each query selection is made via ex-haustively searching in a fixed pool of unlabeled data. Per-forming exhaustive search in the pool is expensive and time-consuming for the tasks requiring on-the-fly interactive learn-ing from unbounded streams or large scale data. Stream-based setting is preferred in this context as it is capable of making immediate query decision without the need of ac-cessing the data pool. On the other hand, there are a lot of real-world learning tasks with crowdsourcing, and systems such as Amazon Mechanical Turk (MTurk) 1 or LabelMe 2 provide access to multiple distributed annotators. For ex-ample, [23] presented an approach for live learning of ob-ject detectors, in which the system autonomously refines its models by actively requesting crowdsourced annotations on images crawled from the Web. In [1], Vamshi et al . pro-posed a paradigm where active learning and crowdsourc-ing come together to enable automatic translation for low-resource language pairs. [25] developed an online system to obtain cost-effective labels of images. In such cases, the un-labeled samples arrive sequentially and the learner cannot store or re-process all the instances due to constraints such as memory limitation. Preparing a pool of unlabeled data in active learning is impractical and a stream-based approach for data processing is required.
 The overall framework of stream-based active learning with its differences from the pool based active learning scenario is showed in Figure 1. In stream-based learning, a learner receives one sample at a time and has to determine whether or not to select the instance to be labeled by general anno-tators or crowdsourced labelers. There are lots of strategies have been proposed for stream-based active learning [14, 11, 27, 11]. Generally, most of these methods consider to choose the informative instances based on a single criterion. Using a ht tps://www.mturk.com/mturk/welcome http://labelme.csail.mit.edu/Release3.0/ single criterion would limit the performance of active learn-in g, which is known as exploitation-exploration dilemma [16, 18, 13]. The problem is more prominent in the stream-based scenario in which the subset of data chosen for labeling can hardly represent the original distribution of data. Consequently, methods have been proposed to address this problem. [9] tried to minimize the unbiasedness in the sam-pling process by designing optimal instrumental distribu-tions. But this method relies on heuristic weighting and lim-its to binary classification scenarios. [10] and [17] proposed different active learning methods for anomaly detection by combining different criteria. However, the proposed com-binations are fixed ones. According to [13], fixed weighted combination of different criteria would not work well across all the datasets and learning stages. We argue that the com-bination should be in a time-varying manner.
 Unlike the method proposed in [9] that tried to minimize the variance to control the bias of stream data, we propose a reinforcement learning framework to learn the optimal strategy during the labeling process and use the feedback from the classifier to guide the selecting. Our active learner updates the weights of exploration and exploiting criteria in subsequent rounds based on the feedback the model re-ceived. The premise behind this adaptive weighting scheme is to favor the criterion that is more likely to return a queried sample that brings most influence to the current model. In this fashion, we manage to make a trade-off not only between exploration and exploitation but also between different cri-teria in a time-varying manner. Our contributions in this paper are as follows: 1. We formulate a stream-based multiclass active learning 2. We propose a reinforcement active learning strategy 3. We compare the proposed strategy with baselines on The rest of the article is organized as follows. In Section 2, we discuss several closely related previous research works and highlight the differences and contributions of our work. In Section 3, we describe our feedback driven active learn-ing approach, which adaptively combine exploration and ex-ploitation criteria. It is essential for on-the-fly interactive labeling for on-line learning. The evaluated experiments us-ing several benchmark datasets are detailed in Section 4. Figure 1: Stream-based active learning vs pool-based active le arning.
 Section 5 concludes the article with a summary and some possible future directions. Recently, there is an unprecedented increase in the amount of publicly available data from various sources such as social networks and mobile phone users [5, 26]. This surge of data has not been accompanied by a complementary increase in annotation [7]. To reduce human supervision in classifier learning, researchers have begun to explore novel ways to collect labeled data. One of the promising research direction is active learning.
 While most of existing methods only consider the pool-based setting [23, 6, 25], in this paper, we focus on active learn-ing for sequential data. Compared to pool-based learning, the stream-based learning is more efficient without expen-sive search in the data pool. Yet it may encounter several difficulties such as imbalance data distribution and new class discovering. There are some stream-based approaches have been developed, most of which are based on a single query criterion. [14, 11] introduced an uncertainty criterion based on the QBC algorithm, in which an ensemble of committee members are maintained. [27] proposed a classifier ensemble based active learning framework for stream data. They split the data stream into chunks and used the minimal variance strategy to select samples.
 Active learning with a single criterion would reduce the per-formance, which is known as exploitation-exploration dilemma [16, 13]. A pure exploitative criterion only focuses on regions that are difficult to learn and will lead to sampling bias. In particular, this problem is more prominent for sequential data where some rare classes would be overlooked since the learner lacks complete knowledge on the underlying data distribution. In contrast, a pure explorative criterion covers the entire data space but needs too many iterations before a good decision boundary is found. There are attempts in combining multi-criteria for active learning. For example, [24] proposed active learning strategies for streaming data that explicitly handle concept drift. which are based on un-certainty, dynamic allocation of labeling efforts over time and randomization of the search space. In [10], Dan et al. proposed an active learning method for anomaly detection by combining likelihood and uncertainty criterion; [17] also exploited active learning for anomaly detection with a fixed combination of different criteria. However, the proposed combinations are unsatisfactory as they are not adaptive ones. Non-adaptive methods can not apply the right crite-rion at different phases of learning, e.g. the active learner may waste effort refining the boundary before discovering the right classes, or vice versa. Chu et al. [9] considered the unbiasedness property in the sampling process, and designed optimal instrumental distributions to minimize the variance in the stochastic process. However, their approach requires a heuristic parameter estimation and limits to binary clas-sification, which is not easy to be extended to a multiclass setting.
 To balance the exploitation and exploration, researchers have developed many approaches to reduce the sampling bias. Generally, they formulated the active learning in a reinforce-ment learning manner and used the feedback from the clas-sifier to guide the sampling process. The method proposed by Baram et al. in [3] took an ensemble containing two ac-tive learning algorithms by a novel maximum entropy semi-supervised criterion. [15] presented an online algorithm that effectively combines an ensemble of active learners based on the classification entropy maximization (CEM) score. In [12], Donmes et al. proposed a dynamical method called DUAL, where the strategy selection parameters are adap-tively updated based on estimated future residual error re-duction after each actively sampled point. [19] addressed this problem by randomly choosing criterion between explo-ration and exploitation at each round, and then receiving feedback measured by the change induced in the learned classifier. [13] modeled active learning as a feedback driven Markov decision process (MDP) that can change over time, and found a successful strategy for each individual data set. However, these methods are limited to the pool-based set-ting, which are infeasible to be directly applied to stream-based environments.
 Our proposed algorithm also addresses the problem by adap-tively weighting of difference criteria based on the feedback from the classifier model. Compared with the pool-based approaches, our method can work in stream-based environ-ments, which is computationally more efficient. In stream-based active learning, we are given a small set of labeled instances  X  = { ( x 1 , y 1 ) , ..., ( x l , y set of unlabeled input stream { x t , x t +1 , ... } X  X  . At each time step t , an unlabeled instance x t is observed from U . Active learning process proceeds by iteratively: (1) training a classifier f t on the labeled pool  X  , (2) using query function Q ( f t ,  X , x t ) to determine whether to query the label y discard x t on each iteration, and (3) updating the model f and  X  with x t and y t , if x t is not discarded. The goal of active learning is to choose instances  X  X isely X  to achieve low classification error of classifier f t using as few labels as possible.
 We want to classify the observed instance x = (x 1 , ..., x dimensionality D into one of the C classes c  X  (1 , ..., C ). We assume there are separate multinomial distributions p ( x on each x i for each class label. The classification task can be considered as Bayesian classification by assuming that conditional are independence among the distributions of the input attributes ( x 1 , ..., x D ). The classifier is quantified by a parameter set  X  specifying the conditional probability dis-tributions. Specifically, we use  X  x tor of parameters for the multinomial p( x i | y). The condi-tional probability p( x | y = c) can be obtained via the formula p( x | y = c) = Q D i=1 p( x i | y = c) for a class c . Given p( x and p(y), posterior conditional distribution p(y | x ) can be computed via Bayes rules. A class y  X  that best explains x is given as follows: y  X  = arg max Incremental Learning To make the stream-based active learning well suited for real-time applications, we use con-jugate prior to facilitate efficient Bayesian learning. The conjugate prior of a multinomial distribution with parame-ters  X  x as follows: where  X  x We now describe our algorithm for active learning. As men-tioned in Section 1, active learning framework with a single criterion can not work well across all the datasets as well as all the learning stages. Two criteria are important for ac-tive learning in streams. Exploitation criterion is designed to select labeled instances that are near the current decision boundary, and the exploration criterion searches for exam-ples that are far from the labeled points. We first discuss different criteria for exploitation and exploration separately. Then, we will bridge this gap by proposing a query strat-egy which combines the scores from the exploration and ex-ploitation criteria. In each step, the query decision is made based on the combined query score. The goal of exploiting sampling is to label the instances near the decision boundary to refine the boundary. Our exploit-ing criterion is a reformulated from of the existing query by Committee algorithm [14, 22]. It first generates several com-mittee members corresponding to hypotheses h = h i of the hypotheses space H t , where each hypothesis is trained with a subset of training data. Then, each committee member is allowed to vote on the labels of query candidates to find the most uncertain sample. In this study, we formulate a novel uncertainty score as follows: at first, a class disagreement score is computed over all possible class labels: where i 6 = j. The top two classes that return highest s y = k are identified as c 1 and c 2 . Following the idea from mar-gin sampling [20], the uncertainty score can be computed by using top two disagreement scores. We define the final un certainty score as: U( x i )  X  [0 , 1]. If U( x i ) of an instance is closer to 1, it is more likely to be queried.
 Generating Committees In a Bayesian Naive Bayes set-ting with multinomial conditional probability distributions, generating committees can be done by sampling new pa-rameters from the posterior Dirichlet distribution of classi-fiers [14]. It has been proven that parameters of a Dirichlet distribution can be generated from a Gamma distribution. Assuming we sample  X   X  x bution Dir(  X   X  x the Gamma distribution,  X   X  x ter of a committee member can then be estimated as: where  X  is a weight added to compensate data sparseness, i.e. to prevent zero probabilities for infrequently occurring values x ij . Based on [2], we empirically set  X  to be 0.2. The goal of exploration is to either search the inhabit dense regions of the input space or find points which are poorly captured by the current model. In the stream-based set-ting, previously selected instances are inaccessible, so we introduce a sampling criterion that compares the likelihood against current distribution modeled by the classifier. The intuition behind likelihood sampling is that points of low likelihood are not well captured by the current model, and may reflect an as yet unseen space. The likelihood sampling strategy finds a class y that maximizes the likelihood and requests the label for the sample The likelihood score L( x i ) lies within [0,1]. If L( x i to 1, x i is more likely to be queried. A combination of two criteria as well as a time-varying trade-off between exploration and exploitation is the key ingre-dient to improve active learning. Our framework aims to combine exploration and exploitation with a time-varying parameter  X  (t), 0  X   X  (t)  X  1, where t  X  X  1 , ..., T } with T the maximum number of queried labels. We integrate  X  (t) in the active learning framework so that there is always a mixture of two criteria. Consequently, the final active learn-ing framework is of the following form: where x t is the sample received at time stamp t . Following [19], we consider the active learning sequence as a process that is optimized by learning a strategy from the feedback. The parameter  X  ( t ) is guided by the change of the classifier feedback and updated according to a reward function of the classifier update:  X  (t) is used to guide the selection between exploration and exploitation with reward function r (t) and  X  (t)  X  [  X , 1  X   X  ].  X  is a parameter that upper-and lower-bounds the value of  X  (t). Parameter  X  is the learning rate that controls the in-fluence of the reward. We use coarse values in parameters setting without optimization:  X  = 0 . 5 for a slow learning rate, and  X  = 0 . 1 for the minimal weight. Reward func-tion r ( t ) is given by the change of the previous hypothesis.  X  exp(r (t) ) &gt; 1 corresponds to larger values of  X  (t), i.e. pos-itive feedback from the model, while  X  exp(r (t) ) &lt; 1 corre-sponds to negative feedback.
 The first improvement in our work is to measure the distance between two distributions from the classifier models p t  X  and p t+1  X  ( x ). We employ the KL-divergence, which is given as KL(  X  |  X   X  ) = P x p t  X  ( x )ln p t  X  ( x ) sifier f t and an updated classifier f t +1 , the KL-divergence between their distributions can be decomposed as: where  X  and  X   X  represent sets of parameters of classifier f f t +1 , respectively. A symmetric KL-divergence  X  KL(  X  k computed as follows: Second, we proposed a more general rescaling for the reward function r (t) . By setting s (t) =  X  KL(  X  k  X   X  ), the function r can be obtained as follows: where 1  X  i  X  t. This reward function r (t) is rescaled from Eq. 11 to get feedback according to Eq. 8. Despite the fact that mathematically, r (t)  X  [0 , 1], from the experiments we find that r (t) is always in the interval [ 2 5 , 1] . In each iteration, the query decision is typically determined by a query score Q( x t ) derived from the query criterion Q. The query score will be compared against a threshold Q th Specifically, if Q( x t )  X  Q th , query is made; otherwise x discarded. Algorithm 1 summaries the process of the pro-posed active learning framework.
 The proposed framework has several parameters while the probability threshold Q th is the most important. We do not tune our parameters to match the test datasets. Following the works in [17, 10], we set the Q th to be 0.5. The sensitivity analysis of Q th is discussed in subsection 4.4. The time complexity of our algorithm obviously depends on the active learners used as subroutines (exploitation and ex-ploration). For each round of active learning, our algorithm takes constant time to update the reward function, then we add the linear time in | S | to update r t (Eq. 8). Specifi-cally, the expected time complexity of our algorithm  X  a  X rs in the current round is O ( T lik + T qbc + | S | ), where T time to run likelihood sampling, and T qbc is the time to run Algorithm 1 Fe edback-driven stream-based active learning 1: Input: (i) A set of data stream U = { x t , x t +1 , } ; (ii) a 2: Parameters: (i) A probability threshold Q th ; (ii) learn-3: Init: (i) Set S 0 =  X  ; (ii) train an initial classifier f 4: For t = 1 , 2 , 5: Receiving x t from U ; 6: Compute the U( x t )(Eq. 4) given instance x t ; 7: Compute the L( x t )(Eq. 6) given instance x t ; 8: Compute the Q( x t )(Eq. 7)given instance x t ; 9: if Q( x t ) &gt; Q th then 10: Request y t and update S t = S t  X  1  X  ( x t , y t ); 11: Updating classifier f t using S t ; 12: Updating query criterion weight  X  (t) based on 13: else 15: End query by the proposed qbc sampling. The time complex-it y of qbc is much larger than that of likelihood sampling due to its heuristic search. Overall, the proposed algorithm has an upper bound of O (2  X  T qbc ). Compared with the pool-based methods, our proposed stream-based approach is more computationally efficient since it need not to search all the candidate data in the pool. We evaluate the proposed method on three benchmark datasets from UCI repository: Thyroid, Pageblocks, and Ecoli, with simulated stream-based active learning setting. These datasets were selected because they contained multiple classes in nat-urally unbalanced proportions. In addition, we also include two product review datasets: Camera and TV. The two datasets were collected using Amazon API from two sub-categories: camera SLR and HD TV. Each dataset contains thousands of review sentences describing the product fea-tures, such as  X  X ppearance X ,  X  X icture quality X  and  X  X ervice X . For each sentence, we asked five different annotators from MTurk to label and we use majority voting to determine the final label for each sentence. Furthermore, we require that each data received the same label by a minimum of three annotators, thereby providing more certainty in the acquired label. Details of the five datasets are shown in Ta-ble 1, where N is the number of instances, d is the number of dimensions; C is the number of classes, S % and L % are proportions of smallest and largest classes, respectively. We applied similar preprocessing steps described in [15] on these five sets.
 We compare the proposed method against the following ex-isting stream-based active learning methods: low-likelihood: Low-likelihood criterion, which is de-scribed in Section 3.2.2. qbc-entropy: Query-by-Committee approach with vote entropy measure, which was used in [14]. The number of committee members is set to be three. minimal-variance: Minimal variance method was pro-posed in [27] and is modified for our stream-based setting. low-lik+qbc: A multi-criteria active learning method [17] that combines Query-by-Committee and low-likelihood. Dif-ferent criteria are balanced through constant weights. Ac-cording to [13], the weights are set to be 0.5 to achieve the best performance.
 Each dataset is randomly partitioned into training/test sets with size ratio 3:7. Before the active selection, some num-ber of labeled samples are given to initialize the classifiers. We assume that a learner can not reuse past samples in a strict stream-based learning setting and can not retrieve any discarded samples to the data stream. For performance comparison, we use: (1) accuracy (Acc), the number of sam-ples correctly classified divided by the total number of test samples, (2) AUC, the area under the receiver operating characteristic curve. In this study, all experimental results are averaged over 25 runs. Cross-validation is useful for error estimation with low bias [ ? ]. In all the testing experiments, two-fold and ten-fold cross-validation are used. In this section, we show results for all the active learning methods on five datasets. Overall accuracy and AUC af-ter max(10 C , 100) iterations ( C is total number of classes), are shown in Table 2 and 3, with two-fold and ten-fold cross-validation respectively. We make the following ob-servations: (1) the proposed method is always better than the other baseline methods on different datasets; (2) ex-ploitation criterion (qbc-entropy) works better than explo-ration criterion (low-likelihood) on Ecoli and TV datasets but has similar performance on the other datasets; (3) the low-lik+qbc sampling method does not achieve better per-formance on most datasets compared with simple exploita-tion or exploration criterion, which indicates using the fixed combination method will not improve the performance; (4) the minimal-variance does not work well on most of the datasets. Its performance is even not better than the low-likelihood or qbc-entropy method; (5) it is notable that in the two datasets: Camera and TV, the proposed feedback-driven sampling shows more significant gains compared to other datasets, which indicates its high effectiveness for the crowdsourced real data.
 We also plot the test results as the various methods learn each additional sample selected in every active learning step. Figure 2 shows AUC performances during different learning stages on the five datasets. As we can see, the proposed feedback driven sampling maintains the best performance all the time. The low-likelihood method achieves higher classification performance than the qbc-entropy method at t Fi gure 3: The average exploitation and exploration weights for different number of queried instances on two datasets. the early stage, due to its ability to rapidly discover new classes. But after about 50-100 iterations, with no new classes left to be discovered, the qbc-entropy criterion starts to outperform the low-likelihood method. Some improve-ments are observed on the Camera and TV datasets by us-ing the weighted low-lik+qbc method. Nevertheless, due to the difficulties tuning the fixed weighted parameters, it gets poor performance on the other datasets (Thyroid, Page-blocks and Ecoli). Again, the minimal-variance only works well on Ecoil.
 Figure 3 illustrates the average exploitation and exploration weights for different number of query instances on Ecoli and Camera datasets within 100 iterations. The exploration cri-terion leads to higher weight at the early stage while the exploitation criterion obtains greater reward and dominates after a certain number of iterations. The is because the ex-ploration criterion can help discover new classes at an early stage while the exploitation criterion can help to refine the classification boundary later. We first measure the efficiency of our implementation on three experimental datasets: Thyroid, Ecoli and Camera. Our C++ implementation runs on a dual-core 3.3 GHz ma-chine with 8G memory. We compare it with the running times of other three stream-based methods: low-likelihood, qbc-entropy and minimal-variance. We also compare it with three pool-based methods: the naive entropy-based method, the comb algorithm proposed in [3], and the exploration al-gorithm proposed in [19]. In each iteration, the pool-based methods search all the data in the pool to select one sample for labeling. Table 6 summaries the results. On Thyroid, Ecoli and Camera datasets, the feeback-driven sampling re-quires 0.28, 0.22 and 0.23 seconds to make a query deci-sion separately. Although the complexity of our algorithm is higher than low-likelihood and qbc-entropy, our method achieves comparable running time with the two basic meth-ods. Compared with the minimal-variance method, our pro-posed method is more efficient. The pool-based methods, on the contrast, take much more time to run a query on each dataset. In datasets with thousands of instances (such as Thyroid), comb and exploration algorithms require ap-proximately 6-7 minutes to make a decision, which shows the pool-based methods are clearly infeasible given large Table 6: Running time (seconds) comparison between pool-based algorithms and stream-based algorithms on three datasets.
 fe edback-driven sampling 0.28 0.22 0.23 Fi gure 4: Sensitivity analysis of Q th on Pageblocks and TV datasets. datasets. We also compare the performances of our feedback-driven sampling method with the three pool-based approaches: entropy, comb and exploration. The experiments are con-ducted on three experimental datasets: Thyroid, Ecoli and Camera with different ratio of training data labeled. Tables 4 and 5 show the accuracy and AUC with four approaches after 10% or 30% of the training data are labeled. Our proposed method can achieve comparable performance with the three pool-based methods. When 30% of the training data are labeled, our feedback-driven sampling has a better performance than the entropy method, and nearly the same performance as the comb and exploration methods. We mentioned in subsection 3.3 that the choice of Q th is very important to the framework performance. We now study how the choice of the threshold score Q th affects the per-formance. With fixed active learning settings, we vary Q th to evaluate the robustness of our method after 20% training data are labeled. The test is only performed on Pageblocks and TV datasets, with Q th varying from 0.3 to 0.8. As shown in Figure 4a, as Q th increases from 0.3 to 0.8, the perfor-mances of both accuracy and AUC on Pageblocks data have a peak when Q th is around 0.5 then it starts falling off. It is also concluded that the accuracy and AUC remain almost the same with Q th in the range from 0.45 to 0.55. Simi-lar trends can be observed in Figure 4b, where Q th ranges from 0.3 to 0.9. These experimental results indicate that the empirical choice of Q th = 0 . 5 is reasonable. la beled data.
 la beled data.
 In many active learning applications, it is necessary to make immediate query decisions without accessing a data pool. In this work, we presented a general framework for efficiently learning from stream data, and proposed a reinforcement ac-tive learning algorithm that can adaptively combine differ-ent criteria over time based on the KL divergence measured from classifier change.
 In addition, by introducing a conjugate prior distribution for efficient incremental learning, our approach is well suited for real-time applications. Experimental results on five real-world datasets showed the superiority of the proposed method in both of the classification performance and the computa-tional efficiency.
 Our proposed framework is applicable to address numerous increasingly common and important contemporary tasks re-quiring on-the-fly interactive learning from unbounded streams and large scale data. It is relevant for applications like robotics where data is incrementally generated [8, 4], or web applications where processing the entire corpus may be pro-hibitively expensive.
 There are several avenues for future work arising from this work. First, the proposed framework can be applied to other types of data (image or video) and a more natural setting of some practical problems in some data mining and computer vision sub-fileds. We would like to further investigate the in-terplay between exploration and exploitation criteria in both the theoretical and practical sense. Since crowdsourcing is a fascinating application for active learning, we will elaborate more on how active learning can deal with the challenges of this application. Finally we will explore potential exten-sion such as active learning from multiple noisy oracles or combining active learning with reinforcement learning in the stream-based setting. This work is supported in part by the following grants: NSF awards CCF-0833131, CNS-0830927, IIS-0905205, CCF-0938 000, CCF-1029166, and OCI-1144061; DOE awards DE-FG02-08ER25848, DE-SC0001283, DE-SC0005309, DESC0 005340, and DESC0007456; AFOSR award FA9550-12-1-0458. We would like to thank Kunpeng Zhang, Yusheng Xie and Wei-keng Liao for their useful comments and insightful suggestions. [1] V. Ambati, S. Vogel, and J. G. Carbonell. Active [2] S. Argamon-Engelson and I. Dagan. Committee-based [3] Y. Baram, R. El-Yaniv, and K. Luz. Online choice of [4] S. Chen, T. Zhang, C. Zhang, and Y. Cheng. A [5] Y. Cheng, Y. Xie, K. Zhang, A. Agrawal, and [6] Y. Cheng, K. Zhang, Y. Xie, A. Agrawal, and [7] Y. Cheng, K. Zhang, Y. Xie, A. Agrawal, W.-k. Liao, [8] Y. Cheng, T. Zhang, and S. Chen. Fast person-specific [9] W. Chu, M. Zinkevich, L. Li, A. Thomas, and [10] A. M. Dan Pelleg. Active learning for anomaly and [11] S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis [12] P. Donmez, J. G. Carbonell, and P. N. Bennett. Dual [13] S. Ebert, M. Fritz, and B. Schiele. Ralf: A reinforced [14] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. [15] T. M. Hospedales, S. Gong, and T. Xiang. Finding [16] S.-J. Huang, R. Jin, and Z.-H. Zhou. Active learning [17] J. K. Jack W. Stokes, John C. Platt and M. Shilman. [18] H. T. Nguyen and A. Smeulders. Active learning using [19] T. Osugi, D. Kun, and S. Scott. Balancing exploration [20] T. Scheffer, C. Decomain, and S. Wrobel. Active [21] B. Settles. Active learning literature survey. Technical [22] H. S. Seung, M. Opper, and H. Sompolinsky. Query by [23] S. Vijayanarasimhan and K. Grauman. Large-scale [24] I.  X  Zliobait  X e, A. Bifet, B. Pfahringer, and G. Holmes. [25] P. Welinder and P. Perona. Online crowdsourcing: [26] K. Zhang, Y. Cheng, W.-k. Liao, and A. Choudhary. [27] X. Zhu, P. Zhang, X. Lin, and Y. Shi. Active learning
