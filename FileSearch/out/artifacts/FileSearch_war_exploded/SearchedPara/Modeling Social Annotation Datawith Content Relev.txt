 Recently there has been great interest in social annotations, also called collaborative tagging or folksonomy, created by users freely annotating objects such as web pages [7], photographs [9], blog posts [23], videos [26], music [19], and scientific papers [5]. Delicious [7], which is a social bookmarking service, and Flickr [9], which is an online photo sharing service, are two representative social annotation services, and they have succeeded in collecting huge numbers of annotations. Since users can attach annotations freely in social annotation services, the annotations include those that do not describe the semantics of the content, and are, therefore, not content-related [10]. For example, annotations such as  X  X ikon X  or  X  X anon X  in a social photo service often represent the name of the manufacturer of the camera with which the photographs were taken, or annotations such as  X 2008 X  or  X  X ovember X  indicate when they were taken. Other examples of content-unrelated annotations include those designed to remind the annotator such as  X  X oread X , those identifying qualities such as  X  X reat X , and those identifying ownership.
 Content-unrelated annotations can often constitute noise if used for training samples in machine learning tasks, such as automatic text classification and image recognition. Although the perfor-mance of a classifier can generally be improved by increasing the number of training samples, noisy training samples have a detrimental effect on the classifier. We can improve classifier performance if we can employ huge amounts of social annotation data from which the content-unrelated annota-tions have been filtered out. Content-unrelated annotations may also constitute noise in information retrieval. For example, a user may wish to retrieve a photograph of a Nikon camera rather than a photograph taken by a Nikon camera.
 In this paper, we propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated data. A number of methods for automatic annotation have been proposed [1, 2, 8, 16, 17]. However, they implicitly assume that all annotations are related to content, and to the best of our knowledge, no attempt has been made to extract content-related annotations automatically. The extraction of content-related annotations can improve performance of machine learning and information retrieval tasks. The proposed model can also be used for the automatic generation of content-related annotations.
 The proposed model is a generative model for content and annotations. It first generates content, and then generates the annotations. We assume that each annotation is associated with a latent variable that indicates whether it is related to the content or not, and the annotation originates either from the topics that generated the content or from a content-unrelated general distribution depending on the latent variable. The inference can be achieved based on collapsed Gibbs sampling. Intuitively speaking, this approach considers an annotation to be content-related when it is almost always at-tached to objects in a specific topic. As regards real social annotation data, the annotations are not explicitly labeled as content related/unrelated. The proposed model is an unsupervised model, and so can extract content-related annotations without content relevance labels.
 The proposed method is based on topic models. A topic model is a hierarchical probabilistic model, in which a document is modeled as a mixture of topics, and where a topic is modeled as a proba-bility distribution over words. Topic models are successfully used for a wide variety of applications including information retrieval [3, 13], collaborative filtering [14], and visualization [15] as well as for modeling annotated data [2].
 The proposed method is an extension of the correspondence latent Dirichlet allocation (Corr-LDA) [2], which is a generative topic model for contents and annotations. Since Corr-LDA assumes that all annotations are related to the content, it cannot be used for separating content-related an-notations from content-unrelated ones. A topic model with a background distribution [4] assumes that words are generated either from a topic-specific distribution or from a corpus-wide background distribution. Although this is a generative model for documents without annotations, the proposed model is related to the model in the sense that data may be generated from a topic-unrelated distri-bution depending on a latent variable.
 In the rest of this paper, we assume that the given data are annotated document data, in which the content of each document is represented by words appearing in the document, and each document has both content-related and content-unrelated annotations. The proposed model is applicable to a wide range of discrete data with annotations. These include annotated image data, where each image is represented with visual words [6], and annotated movie data, where each movie is represented by user ratings. Suppose that, we have a set of D documents, and each document consists of a pair of words and in Table 1. Figure 1: Graphical model representation of the proposed topic model with content relevance. The proposed topic model first generates the content, and then generates the annotations. The gen-erative process for the content is the same as basic topic models, such as latent Dirichlet allocation (LDA) [3]. Each document has topic proportions d that are sampled from a Dirichlet distribution. For each of the N d words in the document, a topic z dn is chosen from the topic proportions, and then word w dn is generated from a topic-specific multinomial distribution z dn . In the generative pro-cess for annotations, each annotation is assessed as to whether it is related to the content or not. In particular, each annotation is associated with a latent variable r dm with value r dm = 0 if annotation t dm is not related to the content; r dm = 1 otherwise. If the annotation is not related to the content, r dm = 0 , annotation t dm is sampled from general topic-unrelated multinomial distribution 0 . If the annotation is related to the content, r dm = 1 , annotation t dm is sampled from topic-specific multinomial distribution c dm , where c dm is the topic for the annotation. Topic c dm is sampled uniform randomly from topics z d = f z dn g N d n =1 that have previously generated the content. This means that topic c dm is generated from a multinomial distribution, in which P ( c dm = k ) = N kd N where N kd is the number of words that are assigned to topic k in the d th document.
 In summary, the proposed model assumes the following generative process for a set of annotated documents f ( w d ; t d ) g D d =1 , where , and are Dirichlet distribution parameters, and is a beta distribution parameter. Fig-ure 1 shows a graphical model representation of the proposed model, where shaded and unshaded nodes indicate observed and latent variables, respectively.
 As with Corr-LDA, the proposed model first generates the content and then generates the annotations by modeling the conditional distribution of latent topics for annotations given the topics for the content. Therefore, it achieves a comprehensive fit of the joint distribution of content and annotations and finds superior conditional distributions of annotations given content [2].
 The joint distribution on words, annotations, topics for words, topics for annotations, and relevance given parameters is described as follows: f r f to multinomial distributions. The first term on the right hand side of (1) is calculated by P ( Z j ) =  X  P ( Z j ) = term is given as follows, P ( W j Z ; ) = of times word w has been assigned to topic k , and N k = follows, P ( T j C ; R ; ) = indicates irrelevant to the content. M k 0 t is the number of times annotation t has been identified as content-unrelated if k 0 = 0 , or as content-related topic k 0 if k 0 6 = 0 , and M k 0 = The Bernoulli parameter can also be integrated out because we use a beta distribution for the prior, which is conjugate to a Bernoulli distribution. The fourth term is given as follows, P ( R j ) = unrelated annotations. The fifth term is given as follows, P ( C j Z ) = M kd is the number of annotations that are assigned to topic k in the d th document.
 The inference of the latent topics Z given content W and annotations T can be efficiently computed using collapsed Gibbs sampling [11]. Given the current state of all but one variable, z j , where j = ( d;n ) , the assignment of a latent topic to the n th word in the d th document is sampled from, where n j represents the count when excluding the n th word in the d th document. Given the current the m th annotation in the d th document is estimated as follows, The assignment of a topic to a content-unrelated annotation is estimated as follows, and the assignment of a topic to a content-related annotation is estimated as follows, The parameters , , , and can be estimated by maximizing the joint distribution (1) by the fixed-point iteration method described in [21]. 3.1 Synthetic content-unrelated annotations We evaluated the proposed method quantitatively by using labeled text data from the 20 Newsgroups corpus [18] and adding synthetic content-unrelated annotations. The corpus contains about 20,000 articles categorized into 20 discussion groups. We considered these 20 categories as content-related annotations, and we also randomly attached dummy categories to training samples as content-unrelated annotations. We created two types of training data, 20News1 and 20News2, where the former was used for evaluating the proposed method when analyzing data with different numbers of content-unrelated annotations per document, and the latter was used with different numbers of unique content-unrelated annotations. Specifically, in the 20News1 data, the unique number of content-unrelated annotations was set at ten, and the number of content-unrelated annotations per document was set at f 1 ; ; 10 g . In the 20News2 data, the unique number of content-unrelated annotations was set at f 1 ; ; 10 g , and the number of content-unrelated annotations per document was set at one. We omitted stop-words and words that occurred only once. The vocabulary size was 52,647. We sampled 100 documents from each of the 20 categories, for a total of 2,000 documents. We used 10 % of the samples as test data.
 We compared the proposed method with MaxEnt and Corr-LDA. MaxEnt represents a maximum entropy model [22] that estimates the probability distribution that maximizes entropy under the constraints imposed by the given data. MaxEnt is a discriminative classifier and achieves high per-formance as regards text classification. In MaxEnt, the hyper-parameter that maximizes the perfor-mance was chosen from f 10 3 ; 10 2 ; 10 1 ; 1 g , and the input word count vector was normalized so that the sum of the elements was one. Corr-LDA [2] is a topic model for words and annotations that does not take the relevance to content into consideration. For the proposed method and Corr-LDA, we set the number of latent topics, K , to 20, and estimated latent topics and parameters by using collapsed Gibbs sampling and the fixed-point iteration method, respectively.
 We evaluated the predictive performance of each method using the perplexity of held-out content-related annotations given the content. A lower perplexity represents higher predictive performance. In the proposed method, we calculated the probability of content-related annotation t in the d th document given the training samples as follows, P ( t j d; D ) point estimate of the topic proportions for annotations, and ^ kt = M kt + M annotation multinomial distribution. Note that no content-unrelated annotations were attached to the test samples. The average perplexities and standard deviations over ten experiments on the 20News1 and 20News2 data are shown in Figure 2 (a). In all cases, when content-unrelated annotations were included, the proposed method achieved the lowest perplexity, indicating that it can appropriately predict content-related annotations. Although the perplexity achieved by MaxEnt was slightly lower than that of the proposed method without content-unrelated annotations, the performance of MaxEnt deteriorated greatly when even one content-unrelated annotation was attached. Since MaxEnt is a supervised classifier, it considers all attached annotations to be content-related even if they are not. Therefore, its perplexity is significantly high when there are fewer content-related annotations per document than unrelated annotations as with the 20News1 data. In contrast, since the proposed method considers the relevance to the content for each annotation, it always offered low perplexity even if the number of content-unrelated annotations was increased. The perplexity achieved by Corr-LDA was high because it does not consider the relevance to the content as in MaxEnt. We evaluated the performance in terms of extracting content-related annotations. We considered ex-traction as a binary classification problem, in which each annotation is classified as either content-related or content-unrelated. As the evaluation measurement, we used F-measure, which is the harmonic mean of precision and recall. We compared the proposed method to a baseline method in which the annotations are considered to be content-related if any of the words in the annotations appear in the document. In particular, when the category name is  X  X omp.graphics X , if  X  X omputer X  or  X  X raphics X  appears in the document, it is considered to be content-related. We assume that the base-line method knows that content-unrelated annotations do not appear in any document. Therefore, the precision of the baseline method is always one, because the number of false positive samples is zero. Note that this baseline method does not support image data, because words in the annotations never appear in the content. F-measures for the 20News1 and 20News2 data are shown in Fig-ure 2 (b). A higher F-measure represents higher classification performance. The proposed method achieved high F-measures with a wide range of ratios of content-unrelated annotations. All of the F-measures achieved by the proposed method exceeded 0.89, and the F-measure without unrelated annotations was one. This result implies that it can flexibly handle cases with different ratios of content-unrelated annotations. The F-measures achieved by the baseline method were low because annotations might be related to the content even if the annotations did not appear in the document. On the other hand, the proposed method considers that annotations are related to the content when the topic, or latent semantics, of the content and the topic of the annotations are similar even if the annotations did not appear in the document. Figure 2: (a) Perplexities of the held-out content-related annotations, (b) F-measures of content relevance, and (c) Estimated content-related annotation ratios in 20News data.
 Figure 2 (c) shows the content-related annotation ratios as estimated by the following equation, ^ = M M 0 + M +2 , with the proposed method. The estimated ratios are about the same as the true ratios. 3.2 Social annotations We analyzed the following three sets of real social annotation data taken from two social bookmark-ing services and a photo sharing service, namely Hatena, Delicious, and Flickr.
 From the Hatena data, we used web pages and their annotations in Hatena::Bookmark [12], which is a social bookmarking service in Japan, that were collected using a similar method to that used in [25, 27]. Specifically, first, we obtained a list of URLs of popular bookmarks for October 2008. We then obtained a list of users who had bookmarked the URLs in the list. Next, we obtained a new list of URLs that had been bookmarked by the users. By iterating the above process, we collected a set of web pages and their annotations. We omitted stop-words and words and annotations that occurred in fewer than ten documents. We omitted documents with fewer than ten unique words and also omitted those without annotations. The numbers of documents, unique words, and unique annotations were 39,132, 8,885, and 43,667, respectively. From the Delicious data, we used web pages and their annotations [7] that were collected using the same method used for the Hatena data. The numbers of documents, unique words, and unique annotations were 65,528, 30,274, and 21,454, respectively. From the Flickr data, we used photographs and their annotations Flickr [9] that were collected in November 2008 using the same method used for the Hatena data. We transformed photo images into visual words by using scale-invariant feature transformation (SIFT) [20] and k-means as described in [6]. We omitted annotations that were attached to fewer than ten images. The numbers of images, unique visual words, and unique annotations were 12,711, 200, and 2,197, respectively. For the experiments, we used 5,000 documents that were randomly sampled from each data set. Figure 3 (a)(b)(c) shows the average perplexities over ten experiments and their standard deviation for held-out annotations in the three real social annotation data sets with different numbers of topics. Figure 3 (d) shows the result with the Patent data as an example of data without content unrelated annotations. The Patent data consist of patents published in Japan from January to March in 2004, to which International Patent Classification (IPC) codes were attached by experts according to their content. The numbers of documents, unique words, and unique annotations (IPC codes) were 9,557, Figure 3: Perplexities of held-out annotations with different numbers of topics in social annotation data (a)(b)(c), and in data without content unrelated annotations (d). Figure 4: Examples of content-related annotations in the Delicious data extracted by the proposed method. Each row shows annotations attached to a document; content-unrelated annotations are shaded. 104,621, and 6,117, respectively. With the Patent data, the perplexities of the proposed method and Corr-LDA were almost the same. On the other hand, with the real social annotation data, the proposed method achieved much lower perplexities than Corr-LDA. This result implies that it is important to consider relevance to the content when analyzing noisy social annotation data. The perplexity of Corr-LDA with social annotation data gets worse as the number of topics increases because Corr-LDA overfits noisy content-unrelated annotations.
 The upper half of each table in Table 2 shows probable content-unrelated annotations in the leftmost column, and probable annotations for some topics, which were estimated with the proposed method using 50 topics. The lower half in (a) and (b) shows probable words in the content for each topic. With the Hatena data, we translated Japanese words into English, and we omitted words that had the same translated meaning in a topic. For content-unrelated annotations, words that seemed to be irrelevant to the content were extracted, such as  X  X oread X ,  X  X ater X ,  X * X ,  X ? X ,  X  X mported X ,  X 2008 X ,  X  X ikon X , and  X  X annon X . Each topic has characteristic annotations and words, for example, Topic1 in the Hatena data is about programming, Topic2 is about games, and Topic3 is about economics. Figure 4 shows some examples of the extraction of content-related annotations. Table 2: The ten most probable content-unrelated annotations (leftmost column), and the ten most probable annotations for some topics (other columns), estimated with the proposed method using 50 topics. Each column represents one topic. The lower half in (a) and (b) shows probable words in the content. We have proposed a topic model for extracting content-related annotations from noisy annotated data. We have confirmed experimentally that the proposed method can extract content-related anno-tations appropriately, and can be used for analyzing social annotation data. In future work, we will determine the number of topics automatically by extending the proposed model to a nonparamet-ric Bayesian model such as the Dirichlet process mixture model [24]. Since the proposed method is, theoretically, applicable to various kinds of annotation data, we will confirm this in additional experiments.
