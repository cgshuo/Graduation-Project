 There is growing interest in applying natural lan-guage processing (NLP) techniques to historical texts (Piotrowski, 2012), with applications in infor-mation retrieval (Dougherty, 2010; Jurish, 2011), linguistics (Baron et al., 2009; Rayson et al., 2007), and the digital humanities (Hendrickx et al., 2011; Muralidharan and Hearst, 2013; Pettersson and Nivre, 2011). However, these texts differ from con-temporary training corpora in a number of linguistic respects, including the lexicon (Giusti et al., 2007), morphology (Borin and Forsberg, 2008), and syn-tax (Eumeridou et al., 2004). This imposes signif-icant challenges for modern NLP tools: for exam-ple, the accuracy of the CLAWS part-of-speech Tag-ger (Garside and Smith, 1997) drops from 97% on the British National Corpus to 82% on Early Mod-ern English texts (Rayson et al., 2007). There are two main approaches that could improve the accu-racy of NLP systems on historical texts: normaliza-tion and domain adaptation.
 Normalization Spelling normalization (also called canonicalization) involves mapping histor-ical spellings to their canonical forms in modern languages, thus bridging the gap between contem-porary training corpora and target historical texts. Figure 1 shows one historical sentence and its normalization by VARD (Baron and Rayson, 2008). Rayson et al. (2007) report an increase of about 3% accuracy on adaptation of POS tagging from Modern English texts to Early Modern English texts if the target texts were automatically normalized by the VARD system. However, normalization is not always a well-de fi ned problem (Eisenstein, 2013), and it does not address the full range of linguistic changes over time, such as unknown words, morphological differences, and changes in the meanings of words (Kulkarni et al., 2015). In the example above, the word  X  X yottours X  is not successfully normalized to  X  X ioters X ; the syntax is comprehensible to contemporary English speakers, but usages such as  X  X ild disposed X  and  X  X rew unto X  are suf fi ciently unusual as to pose problems for NLP systems trained on contemporary texts.
 Domain adaptation A more generic machine learning approach is to apply unsupervised domain adaptation techniques, which transform the repre-sentations of the training and target texts to be more similar, typically using feature co-occurrence statis-tics (Blitzer et al., 2006; Ben-David et al., 2010). It is natural to think of historical texts as a dis-tinct domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be signi fi cantly improved by domain adaption. However, we are unaware of prior work that em-pirically evaluates the ef fi cacy of this approach on Early Modern English texts. Furthermore, histor-ical texts are often associated with multiple meta-data attributes (e.g., author, genre, and epoch), each of which may in fl uence the text X  X  linguistic prop-erties. Multi-domain adaptation (Mansour et al., 2009) and multi-attribute domain adaptation (Joshi et al., 2013; Yang and Eisenstein, 2015) can poten-tially exploit these metadata attributes to obtain fur-ther improvements.

This paper presents the fi rst comprehensive em-pirical comparison of effectiveness of these ap-proaches for part-of-speech tagging on historical texts. We focus on the two historical treebanks of the Penn Corpora of Historical English  X  the Penn Parsed Corpus of Modern British English (Kroch et al., 2010, PPCMBE) and the Penn-Helsinki Parsed Corpus of Early Modern English (Kroch et al., 2004, PPCEME). These datasets enable a range of analy-ses, which isolate the key issues in dealing with his-torical corpora:  X  In one set of analyses, we focus on the  X  In another set of analyses, we train on the Penn  X  We show that F EMA , a domain adaptation algo- X  We compare the impact of normalization with  X  Error analysis shows that the improvements ob-The Penn Corpora of Historical English consist of the Penn-Helsinki Parsed Corpus of Middle English, second edition (Kroch et al., 2010, PPCME2), the Penn-Helsinki Parsed Corpus of Early Modern En-glish (Kroch et al., 2004, PPCEME), and the Penn Parsed Corpus of Modern British English (Kroch and Taylor, 2000, PPCMBE). The corpora are an-notated with part-of-speech tags and syntactic pars-ing trees in an annotation style similar to that of the Penn Treebank. In this work, we focus on POS tag-The Penn Parsed Corpus of Modern British En-glish The PPCMBE is a syntactically annotated corpus of text, containing roughly one million word tokens from documents written in the period 1700-1914. It is divided into three 70-year time periods according to the composition date of the works. Ta-ble 1 shows the statistics of the corpus by time pe-text from a variety of genres, such as Bible, Drama, Fiction, and Letters.
 The Penn-Helsinki Parsed Corpus of Early Mod-ern English The PPCEME is a collection of text samples from the Helsinki Corpus (Rissanen et al., 1993), as well as two supplements mainly consisting of text material by the same authors and from the same editions as the material in the Helsinki Cor-pus. The corpus contains nearly two million words from texts in the period from 1500 until 1710, and it is divided into three 70-year time periods similar to the PPCMBE corpus. The statistics of the corpus by time period is summarized in Table 2. The PPCEME consists of text from the same eighteen genres as the PPCMBE.
 Penn Treebank Release 3 The Penn Tree-bank (Marcus et al., 1993) is the de facto stan-dard syntactically annotated corpus for English, which is used to train software such as Stanford CoreNLP (Manning et al., 2014). When using this dataset for supervised training, we follow Toutanova et al. (2003) and use WSJ sections 0-18 for training, and sections 19-21 for tuning. When applying un-supervised domain adaptation, we use all WSJ sec-tions, together with texts from the PPCMBE and the PPCEME.
 Tagsets The Penn Corpora of Historical English (PCHE) use a tagset that differs from the Penn Tree-bank, mainly in the direction of greater speci fi city. Auxiliary verbs  X  X o X ,  X  X ave X , and  X  X e X  all have their own tags, as do words like  X  X ne X  and  X  X lse X , due to their changing syntactic function over time. Over-all, there are 83 tags in the PPCEME, and 81 in the PPCMBE, as compared with 45 in the PTB. Further-more, the tags in the PCHE tagset are allowed to join constituent morphemes in compounds, yielding complex tags such as PRO+N (e.g.,  X  X imself X ) and ADJ+NS (e.g.,  X  X entlemen X ).

To measure the tagging accuracy of PTB-trained taggers on the historical texts, we follow Moon and Baldridge (2007), who de fi ne a set of deterministic mappings from the PCHE tags to the PTB tagset. For simplicity, we fi rst convert each complex tag to the simple form by only considering the fi rst simple tag component (e.g., PRO+N to PRO and ADJ+NS to ADJ ). This has little effect on the tagging per-formance, as the complex tags cover only slightly more than 1% of the tokens in the PCHE treebanks. Among the 83 tags, 74 mappings to the correspond-ing PTB tags are obtained from Moon and Baldridge (2007). We did our best to convert the other tags ac-cording to the tag description. The complete list of mappings is published in Appendix A. In typical usage scenarios, the user wants to tag some historical text but has no labeled data in the target domain (e.g., Muralidharan and Hearst, 2013). This best fi ts the paradigm of unsupervised domain adaptation, when labeled data from the source do-main (e.g., the PTB) is combined with unlabeled data from the target domain. Representational dif-ferences between source and target domains can be a major source of errors in domain adaptation (Ben-David et al., 2010), and so several representation learning approaches have been proposed.

The most straightforward approach is to replace lexical features with word representations , such as Brown clusters (Brown et al., 1992; Lin et al., 2012) or word embeddings (Turian et al., 2010), such as word2vec (Mikolov et al., 2013). Lexical features can then be replaced or augmented with the result-ing word representations. This can assist in domain adaptation by linking out-of-vocabulary words to in-vocabulary words with similar distributional proper-ties.

Word representations are suitable for adapting lexical features, but a more general solution is to adapt the entire feature representation. One such method is Structural Correspondence Learn-ing (Blitzer et al., 2006, SCL ). In SCL, we create ar-ti fi cial binary classi fi cation problems for thousands of cross-domain  X  X ivot X  features, and then use the weights from the resulting classi fi ers to project the instances into a new dense representation. We also consider a recently-published approach called Fea-ture Embedding ( F EMA ), which achieves the state-of-the-art results on several POS tagging adaptation tasks (Yang and Eisenstein, 2015). The intuition of F EMA is similar to SCL and other prior work: it relies on co-occurrence statistics to link features across domains. Speci fi cally, F EMA exploits the ten-dency of many NLP tasks to divide features into templates, and induces feature embeddings by us-ing the features in each template to predict the active features in all other templates  X  just as the skipgram model learns word embeddings to predict neighbor-ing words. The resulting embeddings can be substi-tuted for the  X  X ne-hot X  representation of each fea-ture template, resulting in a dense, low-dimensional representation of each instance.

A further advantage of F EMA is that it can per-form multi-attribute domain adaptation, enabling it to exploit the many metadata attributes (e.g., year, genre, and author) that are often associated with his-torical texts. This is done by accounting for the speci fi c impact of each domain attribute on the fea-ture predictors, and then building a domain-neutral representation from the common substructure that is shared across all domain attributes. In the experi-ments that follow, we use genre and epoch as domain attributes. We evaluate these unsupervised domain adaptation approaches on part-of-speech tagging for historical English (the PPCMBE and the PPCEME), in two settings: (1) temporal adaptation within each indi-vidual corpus, where we train POS taggers on the most modern data in the corpus and test on increas-ingly distant datasets; (2) adaptation of English POS tagging from modern news text to historical texts. The fi rst setting focuses on temporal differences, and eliminates other factors that may impair tagging per-formance, such as different annotation schemes and text genres. The second setting is the standard and well-studied evaluation scenario for POS tagging, where we train on the Wall Street Journal (WSJ) text from the PTB and test on historical texts. In addi-tion, we evaluate the effectiveness of the VARD nor-malization tool (Baron and Rayson, 2008) for im-proving POS tagging performance on the PPCEME corpus. 4.1 Experimental Settings The datasets used in the experiments are described in  X  2. All the hyperparameters are tuned on devel-opment data in the source domain. In the case where there is no speci fi c development dataset (adaptation within the historical corpora), we randomly sample 10% sentences from the training datasets for hyper-parameter tuning. 4.1.1 Baseline systems
We include two baseline systems for POS tag-ging: a classi fi cation-based support vector machine (SVM) tagger and a bidirectional maximum en-tropy Markov model (MEMM) tagger. Specif-ically, we use the L 2 -regularized L 2 -loss SVM implementation in the scikit-learn package (Pe-dregosa et al., 2011) and L 2 -regularized bidirec-tional MEMM implementation provided by Stanford CoreNLP (Toutanova et al., 2003; Manning et al., 2014).

Following Yang and Eisenstein (2015), we apply the feature templates de fi ned by Ratnaparkhi (1996) to extract the basic features for all taggers. There are three broad types of templates: fi ve lexical feature templates, eight af fi x feature templates, and three or-thographic feature templates. 4.1.2 Domain adaptation systems
We consider the unsupervised domain adaptation methods described in  X  3: structural correspondence F
EMA , which we train in both the single embedding mode (F EMA -single), where metadata attributes are ignored, and in multi-attribute mode (F EMA -attribute), where metadata attributes are used. The domain adaptation models are trained on the union of the (unlabeled) source and target datasets. This ensures that there are no out-of-vocabulary items for the word or feature embeddings.

Following Yang and Eisenstein (2015), we do not learn feature embeddings for the three orthographic feature templates: as each orthographic feature tem-plate represents only a binary value, it is unneces-sary to replace it with a much longer numerical vec-tor. The learned representations are then concate-nated with the basic surface features to form the aug-mented representations. For computational reasons, the domain adaptation systems are all based on the SVM tagger, as pilot studies showed that Viterbi tag-ging offers minimal improvements. 4.1.3 Parameter tuning
We choose the SVM regularization parameter by lowing Blitzer et al. (2006), we consider pivot fea-tures that appear more than 50 times in all the do-mains for SCL. We empirically fi x the number of singular vectors of the projection matrix K to 25, and also employ feature normalization and rescal-ing, as these settings yield best performance in prior work. The number of Brown clusters is chosen from the range { 50 , 100 , 200 , 400 } . For F EMA and word2vec, we choose embedding sizes from the range { 50 , 100 , 200 , 300 } and fi x the numbers of negative samples to 15. The window size for train-ing word embeddings is set as 5. Finally, we adopt the same regularization penalty for all the attribute-speci fi c embeddings of F EMA , which is selected ters were tuned on development data in the source domain. We train the Stanford MEMM tagger using the default con fi guration fi le. 4.2 Temporal Adaptation In the temporal adaptation setting, we work within each corpus, training on the most recent section, and evaluating on the two earlier sections. For PPCMBE, the source domain is the period from 1840 to 1914; for PPCEME, the source domain is the period from 1640 to 1710. All earlier texts are treated as target domains. We transform the tags to the PTB tagset for evaluation, so that results can be compared with the next experiment, in which the PTB is used for supervision.
 Settings We randomly sample 10% sentences from the training data as the development data for optimizing hyperparameters, and then retrain the models on the full training data using the best pa-rameters. For F EMA , we consider domain attributes for 70-year temporal periods and genres, resulting in a total of 21 attributes for each corpus. The numbers of pivot features used in SCL are 4400 and 5048 for the PPCMBE and the PPCEME respectively. The best number of Brown clusters is 200, and the best embedding sizes are 200 and 100 for word2vec and F Results As shown in Table 3, accuracies are signi fi cantly improved by domain adaptation, es-pecially for the PPCEME. English spelling had become mostly uniform and stable since around 1700 (Baron et al., 2009), which may explain why improvements on the PPCMBE are relatively mod-est, especially in the 1770-1839 epoch. Among the two baseline systems, MEMM performs slightly better than SVM, showing a small bene fi t to struc-tured prediction. Among the domain adaptation al-gorithms, F EMA clearly outperforms SCL, Brown clustering and word2vec, with an averaged increase of about 0.5% and 1.5% accuracies on the PPCMBE and the PPCEME test sets respectively. The meta-data attribute information boosts performance by a small but consistent margin, 0.1-0.2% on average. 4.3 Adaptation from the Penn Treebank Newspaper text is the primary data source for training modern NLP systems. For example, most  X  X ff-the-shelf X  English POS taggers (e.g., the Stanford Tagger (Toutanova et al., 2003), SVM-Tool (Gim  X  enez and Marquez, 2004), and CRFTag-ger (Phan, 2006)) are trained on the WSJ por-tion of the Penn Treebank, which is composed of professionally-written news text from 1989. This motivates this evaluation scenario, in which we train the tagger on the Penn Treebank WSJ data and ap-ply it to historical English texts, using all sentences of the PPCMBE and PPCEME for testing.
 Settings The feature representations are trained on the union of the PTB and the PPCEME. The domain attributes for F EMA are set to include the three cor-pora themselves (PTB, PPCMBE, and PPCEME), and the genre attributes in the historical corpora. Note that all sentences in the Penn Treebank WSJ data belong to the same genre (news). For SCL, we use the same threshold of 50 occurrences for pivot features, and include 8089 features that pass this threshold. PTB WSJ sections 19-21 are used for parameter tuning: we fi nd that the best number of Brown clusters is 200, and the optimum embedding sizes are 200 and 100 for word2vec and F EMA . Spelling normalization Spelling variants lead to a high percentage of out-of-vocabulary (OOV) tokens in historical texts, which poses problems for POS tagging. We normalize the PPCEME sentences us-ing VARD (Baron and Rayson, 2008), a widely used spelling normalization tool that has been proven to improve performance on POS tagging (Rayson et al., 2007) and syntactic parsing (Schneider et al., 2014). VARD is designed speci fi cally for Early Modern English spelling variation, and additional labeled data and training are required for other forms of spelling variation, which we do not con-sider here. Following Schneider et al. (2014), we utilize VARD X  X  auto-normalization function with a 50% normalization threshold, achieving a balance between precision and recall. At this threshold, a total of 12% ( 236298 / 1961157 ) of the tokens in the Results As shown in Table 4, this task is consid-erably more dif fi cult, with even the best systems achieving accuracies that are nearly 15% worse than in-domain training. Nonetheless, domain adapta-tion can help: F EMA improves performance by 1.3% on the PPCMBE data, and by 3.8% on the unnor-malized PPCEME data. Spelling normalization also helps, improving the baseline systems by more than 2.5%. The combination of spelling normalization and domain adaptation gives an overall improve-ment in accuracy from 74.2% to 79.1%. The relative error reduction is lower than in the temporal adap-tation setting: only 19% at best, versus 30% error reduction in temporal adaptation. This is because there are now at least two sources of error  X  lan-guage change and tagset mismatch  X  and unsuper-vised domain adaptation cannot address mismatches in the tag set. As expected, the Early Modern English dataset (PPCEME) is considerably more challenging than the Modern British English dataset (PPCMBE): the
Target Normalized PPCMBE No 81.12 81.35 81.66 81.65 81.75 82.34 82.46 (7%) PPCEME No 74.15 74.34 75.89 76.04 75.85 77.77 77.92 (15%) PPCEME Yes 76.73 76.87 77.61 77.65 77.76 78.85 79.05 ( 19%  X  ) baseline accuracy is 7% worse on the PPCEME than the PPCMBE. However, the PPCEME is also more amenable to domain adaptation, with F EMA offering considerably larger improvements. One reason is that the PPCEME has many more out-of-vocabulary (OOV) tokens: 23%, versus 9.2% in the PPCMBE. Both domain adaptation and normalization help to address this speci fi c issue, and they yield further im-provements when used in combination. This section offers further insights on the sources of errors and possibilities for improvement on the PPCEME data. 5.1 Feature Ablation Table 5 presents the results of feature ablation ex-periments for the non-adapted SVM tagger. Word context features are important for obtaining good accuracies on both IV and OOV tokens. Af fi x fea-tures, particularly suf fi x features, are crucial for the OOV tokens. The orthographic features are shown to be nearly irrelevant, as long as af fi x features are present. Overall, the high percentage of OOV to-kens can be a major source of errors, as the tag-ging accuracy on OOV tokens is below 50% in our best baseline system. Note that these results are for a classi fi cation-based tagger; while the Viterbi-based MEMM tagger performs only marginally bet-ter overall (  X  0 . 2% improvement), it is possible that its error distribution might be different due to the ad-vantages of structured prediction. 5.2 Error Analysis The accuracy on out-of-vocabulary (OOV) tokens is generally low, and spelling variation is a major source of OOV tokens. For instance,  X  X e X  and  X  X hy X , the older forms of  X  X he X  and  X  X our X , are often incor-rectly tagged as NN and JJ in the PPCEME. In gen-eral, the per-tag accuracies are roughly correlated with the percentages of OOV tokens. Some excep-tions including VB , NNP and NNS , where the af fi x features can be very useful for tagging OOV tokens.
That said, the cross-domain accuracy on in-vocabulary (IV) tokens is also low, at roughly 80% when adapting from the PTB to the PPCEME. A major source of error here is the mismatch in an-notation schemes between the two datasets, which is only partially addressed by a deterministic tag map-ping. Table 6 presents the SVM accuracy per tag, and the most common error correspondingly. Most of the errors shown in the table are owing to different annotations of the same token in the two corpora.
One major cause of errors is in misalignments of punctuations and their POS tags. For example, in the PPCEME, 16.6% of commas are labeled as . (sentence-fi nal punctuation), and 12.3% periods are labeled as , (sentence-internal punctuation); these punctuations are less ambiguous in the PTB. The historical corpora lack special tags for colons and ellipses, which are present in the PTB. In contrast to the PTB, there is no distinction between opening quotation mark and closing quotation mark in the PPCEME. Moon and Baldridge (2007) avoid these dif fi culties by mapping all the punctuation tokens to a single tag. We did not follow their setting be-cause it would lead to a signi fi cant change of test data. However, it should be noted that these  X  X r-rors X  are not particularly meaningful for linguistic analysis, and could easily be addressed by heuristic post-processing.

The tagging performance is also impaired by the different annotations of many common words. For example, in the PTB, more than 99.9% of token  X  X o X  are labeled as TO , but in the PCHE this word can also be labeled as IN , distinguishing the in fi nitive marker from the preposition. The words  X  X ll X ,  X  X ny X  and  X  X v-ery X  are annotated as quanti fi ers in the PCHE; this tag is mapped to JJ , but these speci fi c words are all labeled as DT in the PTB. A simple remapping from Q to DT leads to an increase of 0.78% base-line accuracy; it is possible that other changes to the tag mappings of Moon and Baldridge (2007) might yield further improvements, but a more systematic approach would be outside the bounds of unsuper-vised domain adaptation. 5.3 Improvements from Normalization As shown above, the tagging accuracy decreases from 81.7% on IV tokens to 49.0% on OOV tokens. Spelling normalization helps to increase the accu-racy by transforming OOV tokens to IV tokens. Af-ter normalization, the OOV rate for the PPCEME falls from 23.0% to 13.5%, corresponding to a re-duction of 41.5% OOV tokens. Normalization is not perfectly accurate, and the tagging performance for IV tokens drops slightly to 81.2% on IV tokens. But due to the dramatic decrease in the number of OOV tokens, normalization improves the overall accuracy by more than 2.5%. We also observe performance drops on tagging OOV tokens after normalization (49.0% to 48.1%), which suggests that the remain-ing unnormalized OOV tokens are the tough cases for both normalization and POS tagging. 5.4 Improvements from Domain Adaptation As presented in Table 7, the tagging accuracies are increased on both IV and OOV tokens with the do-main adaptation methods. Compared against the baseline tagger, F EMA -attribute achieves an abso-lute improvement of 14% in accuracy on OOV to-kens. SCL performs slightly better than Brown clus-tering and word2vec on IV tokens, but worse on OOV tokens. By incorporating metadata attributes, F
EMA -attribute performs better than F EMA -single on OOV tokens, though the accuracies on IV to-kens are similar. Interestingly, the venerable method of Brown clustering (slightly) outperforms both word2vec and SCL.

We further study the relationship between do-main adaptation and spelling normalization by look-ing into the errors corrected by both approaches. Domain adaptation yields larger improvements than spelling normalization on both IV and OOV tokens, although as noted above, the approaches are some-what complementary. The results show that among the 60,928 error tokens corrected by VARD, 60% are also corrected by F EMA -attribute, while the remain-ing 40% would be left uncorrected by the domain adaptation technique. Conversely, among the errors corrected by F EMA -attribute, 38% are also corrected by VARD, while the remaining 62% would be left uncorrected. The overlap of reduced errors is be-cause both approaches exploit similar sources of in-formation, including af fi xes and local word contexts. Domain adaptation Early work on domain adap-tation focuses on supervised setting, in which some amount of labeled instances are available in the target domain (Jiang and Zhai, 2007; Daum  X  e III, 2007; Finkel and Manning, 2009). Unsupervised domain adaptation is more challenging but attrac-tive in many applications, and several representation learning methods have been proposed for address-ing this problem. Structural Correspondence Learn-ing (Blitzer et al., 2006, SCL) and marginalized denoising autoencoders (Chen et al., 2012, mDA) seek cross-domain representations that are useful to predict a subset of features in the original in-stances, called pivot features. Schnabel and Sch  X  utze (2014) directly induce distributional representations for POS tagging based on local left and right neigh-bors of the token. More recent work trains cross-domain representations with neural networks, with additional objectives such as minimizing errors in the source domain and maximizing domain confu-sion loss (Ganin and Lempitsky, 2015; Tzeng et al., 2015). We show the Feature Embedding model, which is speci fi cally designed for NLP problems with feature templates (Yang and Eisenstein, 2015), achieves strong performance on historical adapta-tion tasks.
 Historical texts Historical texts differ from mod-ern texts in spellings, syntax and semantics, pos-ing signi fi cant challenges for standard NLP systems, which are usually trained with modern news text. Numerous resources have been created for over-coming the dif fi culties, including syntactically an-notated corpora (Kroch et al., 2004; Kroch et al., 2010; Galves and Faria, 2010) and spelling normal-ization tools (Giusti et al., 2007; Baron and Rayson, 2008). Most previous work focuses on normaliza-tion, which can signi fi cantly increase tagging ac-curacy on historical English (Rayson et al., 2007) and German (Scheible et al., 2011). Similar im-provements have been obtained for syntactic pars-ing (Schneider et al., 2014). Domain adaptation of-fers an alternative approach which is more generic  X  for example, it can be applied to any corpus with-out requiring the design of a set of normalization rules. As shown above, when normalization is pos-sible, it can be combined with domain adaptation to yield better performance than that obtained by either approach alone. Syntactic analysis is a key fi rst step towards pro-cessing historical texts, but it is confounded by changes in spelling and usage over time. We empir-ically evaluate several unsupervised domain adapta-tion approaches for POS tagging of historical En-glish texts. We fi nd that domain adaptation meth-ods signi fi cantly improve the tagger performance on two historical English treebanks, with relative error reductions of 30% in the temporal adaptation set-ting. F EMA outperforms other domain adaptation approaches, showing the importance of adapting the entire feature vector, rather than simply using word embeddings. Normalization and domain adaptation combine to yield even better performance, with a to-tal of 5% raw accuracy improvement over a baseline classi fi er in the most dif fi cult setting. Error anal-ysis reveals that tagset mismatch is the most com-mon source of errors for in-vocabulary words. We hope that our work encourages further research on domain adaptation for historical texts and provides useful baselines in these efforts.
 Acknowledgments This research was supported by National Science Foundation award 1349837, and by the National Institutes of Health under award number R01GM112697-01. We thank the reviewers for their helpful feedback.

