 Location-based services have become widely available on mobile devices. Existing methods employ a pull model or user-initiated model, where a user issues a query to a serv-er which replies with location-aware answers. To provide users with instant replies, a push model or server-initiated model is becoming an inevitable computing model in the next-generation location-based services. In the push mod-el, subscribers register spatio-textual subscriptions to cap-ture their interests, and publishers post spatio-textual mes-sages. This calls for a high-performance location-aware pub-lish/subscribe system to deliver publishers X  messages to rel-evant subscribers.

In this paper, we address the research challenges that arise in designing a location-aware publish/subscribe system. We propose an R -tree based index structure by integrating tex-tual descriptions into R -tree nodes. We devise efficient fil-tering algorithms and develop effective pruning techniques to improve filtering efficiency. Experimental results show that our method achieves high performance. For example, our method can filter 500 tweets in a second for 10 million registered subscriptions on a commodity computer.
 H.2.8 [ Database Applications ]: Spatial databases Location-aware Publish/Subscribe, Filtering Algorithm
Location-based services (LBS), thanks to global position-ing systems (GPS) wired into smart phones, have recently attracted significant attention from both industrial and aca-demic communities. Many LBS services such as Foursquare (http://foursquare.com) have been widely accepted since they can provide users with location-aware experiences.
Existing LBS systems employ a pull model or user-initiated model [13, 7], where a user issues a query to a server which responds with location-aware answers. For example, if a mo-bile user wants to find a seafood restaurant nearby, she can issue a query with keywords  X  seafood restaurant  X  to an LBS system, which returns relevant answers based on the user X  X  location and keywords.

To provide users with instant replies, a push model or server-initiated model is becoming an inevitable comput-ing model in next-generation location-based services. In the push model, subscribers register spatio-textual subscriptions to capture their interests, and publishers post spatio-textual messages. This calls for a high-performance location-aware publish/subscribe system to deliver messages to relevant subscribers. This new computing model brings new user experiences to mobile users, and can help users retrieve in-formation without explicitly issuing a query.

There are many real-world applications using location-aware publish/subscribe services. The first one is Groupon. In a Groupon system, subscribers are Groupon customers and messages are Groupon messages. Groupon customer-s register their interests with locations and keywords (e.g.,  X  iphone4s  X  at New York ). For each Groupon message (e.g.,  X  iphone4s AT &amp; T package  X  X t Manhattan ), the system provider sends the message to the customers who may be potentially interested in the message by evaluating the spatial proximi-ty and textual relevancy between subscriptions and the mes-sage. The second one is location-aware AdSense, which ex-tends traditional AdSense (http://www.google.com/adsense) to support location-aware services, where the subscribers are advertisers and the publishers are mobile users. The ad-vertisers register their location-based advertisements (e.g.,  X  seafood  X  at Manhattan ) in the system. The system push-es relevant advertisements to mobile users based on their locations and contents they are browsing (e.g., webpages). The third one is tweet delivery. Market analysts want to receive feedback of their products in a specific area from Twitter. In this case, the subscribers are market analyst-s and the messages are tweets. Market analysts register their interests (e.g.,  X  ipad2  X  at LA ). For each tweet (e.g.,  X  ipad2 is expensive  X  at LA Airport ), the system pushes the tweet to relevant analysts whose spatio-textual subscriptions match the tweet.

One big challenge in a publish/subscribe system is the high performance. A publish/subscribe system should sup-port tens of millions of subscribers and deliver messages to relevant subscribers in milliseconds. Since messages and subscriptions contain both location information and textual description, it is rather costly to deliver messages to relevant subscribers. This calls for an efficient filtering technique to support location-aware publish/subscribe services.
To address the challenge, we propose a token-based R -t ree index structure (called R t -tree ) by integrating each R -tree node with a set of tokens selected from subscriptions. Using the R t -tree , we develop a filter-and-verification framework to efficiently deliver a message. To reduce the number of token-s associated with R t -tree nodes, we select some high-quality representative tokens from subscriptions and associate them with R t -tree nodes. This technique not only reduces index sizes but also improves the performance. We propose effi-cient filtering algorithms and develop pruning techniques to achieve high performance. Experiments on large, real data sets show that our method achieves high performance. To summarize, we make the following contributions. (1) We introduce a new computing model for LBS and formal-ize the location-aware publish/subscribe problem. (2) We propose a novel index structure, the R t -tree , by integrating high-quality representative tokens selected from subscrip-tions into the R -tree nodes. (3) Using our proposed indexes, we develop efficient filtering algorithms and develop several effective pruning techniques to improve the efficiency.
The rest of this paper is organized as follows. We for-malize the problem in Section 2. In Section 3, we propose the R t -tree index. We propose a representative token based method in Section 4. We devise an efficient filtering algorith-m without a verification step in Section 5. Experiments are provided in Section 6. We review related works in Section 7 and conclude the paper in Section 8.
In a location-aware publish/subscribe system, subscribers register subscriptions to capture their interests. A subscrip-tion s includes a textual description s.T and spatial informa-tion s.R , denoted by s = ( T, R ). The spatial information is used to capture a subscriber X  X  most interested region. In this paper we use the well-known minimum bounding rectangle (MBR) to denote a region s.R . The textual description is used to capture a subscriber X  X  content-based interests, de-noted by a set of tokens s.T = { t 1 , t 2 ,  X  X  X  , t | s.T |
A message m posted by a publisher also contains a textual description m.T and spatial information m.R , denoted by m = ( T, R ), which respectively have the same meaning as those of subscriptions. Note that the spatial information m.R of a message can be a point, e.g., mobile user X  X  location. If the spatial information of a message is a point, we call it point message ; otherwise we call it range message .
Let S = { s 1 , s 2 ,  X  X  X  , s |S| } denote the set of all subscrip-tions. Given a subscription s i  X  S and a message m , a location-aware publish/subscribe system delivers the mes-sage m to s i ( s i is called an answer of m ), if they satisfy (1) Spatial Constraint : Message m and subscription s i have spatial overlap (i.e., s i .R  X  m.R 6 =  X  ) and; (2) Textual Constraint : All tokens in subscription s i are contained in message m (i.e., s i .T  X  m.T ).

In this paper, for textual constraint we consider the con-junctive semantics, that is any token in a subscription needs to be contained in the message. Our method can also sup-port disjunctive semantics by decomposing a subscription into several small subscriptions. For example, we can de-compose a subscription with tokens  X ( iphone4s or ipad2 ) and AT &amp; T  X  to two subscriptions with tokens  X  iphone4s and AT &amp; T  X  and  X  ipad2 and AT &amp; T  X . For the spatial constrain-s s s s s s s s s s s s Figure 1: An example of subscriptions and messages t , we consider the case that a message and a subscription have spatial overlap. In the future work, we want to study (1) range queries (e.g., price between 100 and 200); and (2) ranking queries: finding the subscriptions with similar-ity to the message larger than a given threshold (or top-k subscriptions), by considering both textual relevancy and s-patial proximity. Based on these notations, we formalize the location-aware publish/subscribe problem as below. Definition 1 (Location-aware Publish/Subscribe).
 Given a set of subscriptions S = { s 1 , s 2 ,  X  X  X  , s |S| message m , a location-aware publish/subscribe system deliv-ers m to s i  X  X  if s i .R  X  m.R 6 =  X  and s i .T  X  m.T .
Example 1. Consider the 12 subscriptions and 2 mes-sages in Figure 1. For point message m p = ( { iphone4s , R 12 ) is an answer. s 10 = ( { 32GB , AT &amp; T , iphone4s } , R not an answer as it has a token  X  32GB  X  which does not ap-it has no spatial overlap with m p . The answers of m p are s 11 and s 12 . For a range message m r = ( { iphone4s , ipad2 , AT &amp; T , 64GB } , R m ), its answers are s 8 , s 11 , and s Keyword-first method : It first uses existing content-based publish/subscribe techniques to generate the candidates that satisfy the textual constraint, e.g, using inverted lists [25]. Then it verifies the candidates to check whether they satis-fy the spatial constraint. Obviously this method generates large numbers of candidates and leads to low performance. Spatial-first method : Different from the keyword-first method, it first generates the location-based candidates that satisfy the spatial constraint, using existing methods, e.g., segment tree or R -tree [22]. Then it filters candidates which do not satisfy the textual constraint. This method also generates huge numbers of candidates and has poor performance. Spatial keyword search based method : There are several studies on spatial keyword search by using a pull model [30, 13, 7, 3]. In this model, the underlying data are a set of ob-jects with locations and keywords. A user submits a spatial keyword query, and the system returns top-k relevant ob-jects by considering spatial and textual proximity between the query and objects. They incorporate keywords (e.g., sig-nature files [13] or inverted lists [7]) into R-tree nodes. We can extend these approaches to support our application by traversing tree based indexes and pruning tree nodes using textual and spatial information. Experimental results (Sec-tion 6) show that our method outperforms these approaches.
We first propose an index structure in Section 3.1 and then devise efficient algorithms in Section 3.2.
R -tree is a well-known index structure to index spatial data, which is a balanced search tree where all leaf nodes are at the same level. As the standard R -tree has no textual pruning power, we propose a token-based R -tree , called R tree , by integrating tokens of subscriptions into R -tree nodes.
Like R -tree , R t -tree is also a balanced search tree. Each leaf node contains between b and B data entries, where each entry is a subscription. Each internal node has between b and B node entries. Each entry is a triple h Child , MBR , To-kenSet i , where Child is a pointer to its child node, MBR is the minimum bounding rectangle of all entries within this child, and TokenSet is a set of tokens selected from subscrip-tions (or a pointer to the token set). A leaf node X  X  token set is the union of tokens of all subscriptions within this node and an internal node X  X  token set is the union of token sets of all entries within this node. As an entry corresponds to a node, for simplicity a node is mentioned interchangeably with its corresponding entry if the context is clear.
Example 2. Figure 2 shows an R t -tree for the subscrip-tions in Figure 1. Nodes n 4 , n 5 , n 6 , n 7 are leaf nodes. Each leaf node contains three subscriptions. For instance, n 4 contains three subscriptions s 1 , s 2 , s 3 . Node n 2 has two 64GB } ) and ( n 5 , R 14 , { iphone4s , verizon } ). The first entry points to its child n 4 with MBR R 13 and the token set is the union of the subscriptions of entries in its children, i.e., s .T  X  s 2 .T  X  s 3 .T . The second entry points to its child n with MBR R 14 and the token set is s 4 .T  X  s 5 .T  X  s 6 .T .
Suppose the height of the R t -tree is H and the average number of tokens in subscriptions is S avg . Each token of a subscription is stored at most H times 1 . Thus the token-set complexity is O ( H  X  S avg  X  |S| ). There are at most the root also has b child nodes). The space complexity of a node is O ( B ) for storing MBRs and child pointers. Thus the overall space complexity is We discuss how to use the R t -tree to filter a message. We want to prune unnecessary nodes and only visit a small number of  X  X ivotal X  nodes, where a node is a pivotal node if there exist answers in its leaf descendants. Thus when traversing the R t -tree , we only need to visit the pivotal nodes . However an R t -tree node may have large numbers of leaf descendants and it is expensive to check whether a node is a pivotal node. Based on this observation, we propose a filter-and-verification framework. In the filter step, we find a set of candidate nodes which is a superset of pivotal nodes. In the verification step we verify the subscriptions in the leaf candidate nodes generated in the filter step. Next we introduce two filters. Filters: Given a node n , let n.R denote its MBR and n.T denote its token set which can be obtained from the corre-sponding entry in its parent node. We prune node n , if (1) MBR Filter : n.R  X  m.R =  X  . It invalidates spatial con-straint, as any subscription under node n (i.e., subscriptions in n  X  X  leaf descendants) has no overlap with m ; or (2) Token Filter : n.T  X  m.T =  X  . It invalidates the textual constraint. The reason is that any subscription under node n must contain a token in n.T which does not appear in m.T , thus the subscription does not satisfy the textual constraint.
The nodes that are not pruned by the MBR filter and token filter are called candidate nodes . The subscriptions in the leaf candidate nodes are candidate answers .

For MBR-filter, to check whether n.R  X  m.R =  X  , we examine whether n.R has a vertex contained in m.R . For token filter, to check whether n.T  X  m.T =  X  , we use the hash table of n  X  X  TokenSet to do the checking as follows. For each token in m.T , if it is contained in the hash table, n.T  X  m.T 6 =  X  ; otherwise we check the next token in m.T . Verification: For each subscription s on leaf candidate n-odes, we check whether s.R  X  m.R 6 =  X  and s.T  X  m.T . If yes, s is an answer. To check s.T  X  m.T , we first sort the tokens in m.T (e.g., document frequency order, and we will discuss different sorting strategies in Section 4.2). Then we use each token in s.T to do a binary search in m.T . Algorithm: Given a message m , we traverse the R t -tree in pre-order. From the root node, we scan each of its entries, e.g., node n . If node n satisfies one of the two filters, i.e., n.R  X  m.R =  X  or n.T  X  m.T =  X  , we prune node n ; otherwise we visit n  X  X  children and repeat the above steps. Iteratively, we can find all leaf candidate nodes.

Example 3. Consider the R t -tree in Figure 2 and a mes-sage m = ( { ipad2 , AT &amp; T , 32GB , 64GB } , R m ), where R dashed MBR in Figure 1. The root node has two entries: node n 2 and node n 3 . Node n 2 is not pruned by the MBR filter as it has overlap with m.R . n 2 is not pruned by the token filter as it contains  X  ipad2  X  which appears in message m . Thus node n 2 is a candidate node. Node n 2 has two entries: n 4 and n 5 . As node n 4 has no overlap with m.R , it is pruned by the MBR filter. Node n 5 has spatial over-lap with m.R and cannot be pruned by the MBR filter. As n  X  X  token set { iphone4s , verizon } has no common token with m.T , n 5 is pruned by the token filter. For node n 3 access its first entry n 6 . As n 6 is not pruned by the MBR filter and token filter, it is a leaf candidate node. We verify subscriptions s 7 , s 8 , s 9 in n 6 . We prune s 7 as it has no spa-tial overlap with m.R . We prune s 8 and s 9 as they have a token  X  iphone4s  X  which does not appear in m.T . Similarly n 7 is also a leaf candidate node and we verify s 10 , s 11 n . We prune s 10 and s 11 as their tokens are not contained in m.T and get an answer s 12 .

Notice that an algorithm should satisfy (1) Completeness: any subscription satisfying the spatial constraint and textual constraint must be found by the algorithm; and (2) Correct-ness: any subscription found by the algorithm must satisfy the two constraints. We prove that the R t -tree based algo-rithm satisfies the two properties as stated in Theorem 1.
Theorem 1. The R t -tree based algorithm satisfies com-pleteness and correctness.

Proof. We omit the proofs due to space constraints.
In this section, we propose an effective technique to reduce the number of tokens associated with each node, which not only reduces index sizes but also improves performance.
Different from TokenSets on R t -tree nodes, we select high-quality representative tokens and use representative-token sets to replace TokenSets . For each subscription, we select a single token as its representative token. For each leaf node, its representative-token set is the set of representative tokens of subscriptions within this node. For each internal node, its representative-token set is the union of representative-token sets of its child nodes. To differentiate this method from R -tree , we call it R t + -tree .

We have an observation that if the representative-token set of a node has no common token with a message, we can prune the node as stated in Lemma 1. This is because any subscription under this node cannot satisfy textual con-straint as it contains tokens which are not in the message.
Lemma 1. Given an R t + -tree node n and a message m , if n  X  X  representative-token set has no common token with m.T , any subscription under node n cannot be an answer of m .
Example 4. Consider the subscriptions in Figure 1. We construct an R t + -tree as shown in Figure 3(a). For subscrip-tions s 1 , s 2 , s 3 , we respectively select representative tokens  X  iphone4s  X ,  X  iphone4s  X ,  X  ipad2  X . Thus the representative-token set of node n 4 is { iphone4s , ipad2 } . For s 4 , s select representative tokens X  iphone4s  X . Thus the representative-token set of node n 5 is { iphone4s } . The representative-token set of n 2 is { iphone4s , ipad2 } which is the union of the representative tokens of its child nodes n 4 and n For a message m =( { AT &amp; T , 32GB , 64GB } , R m ), R prune node n 2 as its token set shares a token  X  AT &amp; T  X  with m (Figure 2). Instead R t + -tree can prune n 2 as its representative-token set shares no common token with m .

There are at most |S| representative tokens and each rep-resentative token is stored at most H times. Thus the com-plexity of representative-token sets is O ( H X |S| ). The R t + -tree has the same MBR size with the R t -tree . Thus the space complexity of the R t + -tree is
We still adopt the filter-and-verification framework in Sec-tion 3.2 to filter a message. Theorem 2 shows correctness and completeness of the R t + -tree based algorithm.
Theorem 2. The R t + -tree based algorithm satisfies com-pleteness and correctness.
 Comparison of R t -tree and R t + -tree : Based on the space complexity, the R t + -tree involves much smaller index sizes than the R t -tree . More interestingly, we prove that the R tree has larger pruning power than the R t -tree . That is if the R t -tree prunes a node, then the R t + -tree must also prune the node as stated in Lemma 2.

Lemma 2. Let C R t and C R t + respectively denote the can-didate node sets of the R t -tree based method and the R t + -tree based method, we have C R t  X  X  R t + .

Example 5. Recall the R t -tree in Figure 2. Consider a message m =( { AT &amp; T , 32GB , 64GB } , R m ). Node n mon tokens with m , thus the R t -tree cannot prune node n However all subscriptions under n 3 contain tokens X  iphone4s  X  or  X  ipad2  X  which do not appear in m . Thus n 3 can be pruned. In the R t + -tree , the representative-token set of n is { iphone4s , ipad2 } which has no common token with m , thus the R t + -tree can prune node n 3 .
Given a subscription, there are multiple ways to select a representative token from this subscription. We introduce several methods to select high-quality representative tokens. Random selection: A naive method is to randomly select a representative token for each subscription. This method does not utilize the token distribution and can be optimized. The df -based method: Intuitively the smaller sizes of representative-token sets, the larger pruning power, and the smaller number of candidates. Thus it is important to reduce the sizes of the representative-token sets. To this end, we select representative tokens based on the df order as follows. We first select the token with the largest df and take it as the representative token of subscriptions that contain the token. Then we remove all such subscriptions and select the token with the largest df in the remainder subscriptions. Iteratively we generate representative token.
 The idf -based method: A token with higher frequency usually has larger probability appearing in a message. If we add such a token into the token set, the token set has larger probability sharing tokens with the message, thus the token set has lower pruning power. As the df order selects the representative tokens with the highest frequencies, the df based method may lead to low pruning power. To address this issue, for each subscription, we select the token with the largest idf as its representative token.

Example 6. Consider the subscriptions in Figure 1. If we use the df order, the R t + -tree is shown in Figure 3(a). As  X  iphone4s  X  has the largest df (its df is 9) and it ap-pears in s 1 , s 2 , s 4 , s 5 , s 6 , s 8 , s 9 , s 10 , s as their representative tokens. Then in the remainder sub-lect  X  ipad2  X  as their representative tokens. For node n subscriptions under n 4 are s 1 , s 2 , s 3 , thus its representative-token set is { iphone4s , ipad2 } . On the R t + -tree there are 23 representative tokens and on the R t -tree there are 59 to-kens. Obviously the df order can reduce token-set sizes. Figure 3(b) shows the R t + -tree with representative token-s selected by the idf order. We respectively select  X  32GB , verizon , 64GB  X  for subscriptions s 1 , s 2 , s 3 with the largest idf . The representative-token set of node n 2 is the union of these three representative tokens. Consider message m = prune node n 5 as its representative-token set { iphone4s } shares a common token with message m . However the id-f order can prune node n 5 as its representative-token set { verizon } shares no common token with message m . Locality-aware method: Tokens usually have different frequencies in different locations. For example, in Figure 1 token  X  verizon  X  has a large frequency in node n 5 ( R 14 low frequency in nodes n 6 ( R 15 ) and n 7 ( R 16 ). We should consider the locality-aware token distribution to count token frequencies. To address this issue, we partition the whole re-gion into several grids and count the frequencies of tokens for each grid. Thus we can capture the locality-aware fre-quencies. Then for each subscription, we first sort its tokens based on by their frequencies in the grid where the sub-scription locates in and then use the df -based method or the idf -based method to select representative tokens.
In this section, we propose selecting multiple representa-tive tokens and devise an efficient filtering algorithm, which directly finds answers and avoids the verification step.
Like the R t + -tree , we also associate each node with a representative-token set. Different from the R t + -tree , for subscription s , we do not repeatedly use a single token for all ancestors of its corresponding leaf node. Instead we selec-t different representative tokens for different ancestor nodes and each ancestor node contains a token of s . If s has larg-er than H tokens, we insert its last | s | X  X  +1 tokens into the leaf node. If s has smaller than H tokens, only the an-cestors in the first | s | levels contain a token. Formally, let s.T [ i ] denote the i -th token in s and we assign s.T [ i ] to it-s ancestor at the i -th level. For each node n on the i -th level, its representative-token set is the set of i -th tokens of subscriptions under node n , i.e.,  X  s  X  X  n s.T [ i ], where S the set of subscriptions under node n . Let n.T denote the representative-token set of node n . For each token t  X  n.T , we build an inverted list I n [ t ] of subscriptions in S i -th token is t , i.e., I n [ t ]= { s | s  X  X  n and s.T [ i ]= t } .
Example 7. Figure 4 shows the R t ++ -tree for subscrip-s .T [3] =  X  iphone4s  X . Consider the first entry at the first level (the root node), i.e., node n 2 . Node n 2 contains six subscriptions s 1 , s 2 ,  X  X  X  , s 6 , thus the subscription set under node n 2 is S n 2 = { s 1 ,  X  X  X  s 6 } . s 1 .T [1] =  X  32GB  X , s  X  verizon  X , s 3 .T [1] =  X  64GB  X , s 4 .T [1] =  X  verizon  X , s  X  verizon  X , and s 6 .T [1] =  X  verizon  X . The representative-token set of node n 2 is n 2 .T = s 1 .T [1]  X  s 2 .T [1]  X   X  X  X   X  s .T [1] = { verizon , 32GB , 64GB } . The inverted list of X  verizon  X  in node n 2 is I n 2 [ verizon ] = { s 2 , s 4 , s 5 , s 6 ode n 5 , it contains three subscriptions, s 4 , s 5 , s 6 tokens of these subscriptions are  X  iphone4s  X . Thus the to-ken set of n 5 is { iphone4s } and the inverted list of iphone4s in node n 5 is { s 4 , s 5 , s 6 } .
 The R t ++ -tree has the following good property.
Property 1. For any subscription s , s appears exactly | s | times on the inverted lists in the R t ++ -tree . For any i &lt; | s | , s appears exactly i times in the first i levels.
For example, consider subscription s 8 with tokens { AT &amp; T , ipad2 , iphone4s } . s 8 appears three times in the R t ++ -tree , i.e., the inverted list of AT &amp; T in node n 1 at level 1, the inverted list of ipad2 in node n 3 at level 2, and the inverted list of iphone4s in node n 6 at level 3. Obviously s 8 appears once in the first level and twice in the first two levels.
The R t ++ -tree has the same MBR size with the R t -tree but has much smaller token-set size. We first analyze the token-set size. For each token of a subscription, it appears in exact-ly one R t ++ -tree node. Thus the total size of representative-token sets is O ( |S avg | X |S| ). Then we consider inverted-list size. Each subscription s appears in exactly | s.T | inverted lists, and the total inverted-list size is O ( |S avg | X |S| ). Obvi-ously the size of token sets and inverted lists is proportional to the data size. Plus MBR sizes, the overall space is
Consider a message m . We traverse the R t ++ -tree from the root. For a node n in the i -th level, if n.R  X  m.R =  X  , we prune node n as the message invalidates the spatial con-straint; otherwise for each token t  X  n.T  X  m.T , we retrieve the corresponding inverted list I n [ t ]. For each subscription s  X  X  n [ t ], we count its occurrence number in the first i levels (i.e., the number of inverted lists of ancestors of node n that contain the subscription), denoted by cand s . We will discuss how to compute cand s later. Here based on cand s , we have the following observations.
 Case 1 -cand s &lt; i : In this case, subscription s appears smaller than i times in the first i levels. We can prove that s is not an answer as formalized in Lemma 3. The reason is as follows. As s appears in the i -th level, it contains at least i tokens. For each node on the path from the root to node n , s must have a token in the node. If cand s &lt; i , s must have a token which does not appear in m . Thus s invalidates the textual constraint and cannot be an answer.

Lemma 3. Consider a message m and a node n at the i -th level. For any subscription s on the inverted lists of node n , if cand s &lt; i , s cannot be an answer of m .
For example, consider message m = ( { ipad2 , 64GB } , R m and node n 3 at the second level in Figure 4. For subscription s , we have cand s 7 = 1. We can deduce that s 7 cannot be an answer of the message. The main reason is as follows. As s 7 has 2 tokens, it must appear twice in the first two levels and its occurrence number should be two. However its occurrence number cand s 7 is 1, thus s 7 must contain a token which does not appear in m and s 7 is not an answer. Case 2 -cand s = i : We consider the following subcases. Case 2.1 -cand s = i = | s | : In this case, if s.R  X  m.R 6 =  X  , s must be an answer of m as stated in Lemma 4. The basic idea is as follows. Each token of s appears in the R t ++ once. As cand s = | s | , all tokens in s are contained in m , thus s satisfies the textual constraint. As s.R  X  m.R 6 =  X  , s satisfies the spatial constraint. Thus s must be an answer.
Lemma 4. Consider a message m and a node n at the i -th level. For any subscription s on the inverted lists of node n , if cand s = | s | and s.R  X  m.R 6 =  X  , s must be an answer.
For example, consider the message m r = ( { iphone4s , ipad2 , AT &amp; T , 64GB } , R m ) and node n 6 at the third level in Figure 4. For subscription s 8 , we have cand s 8 = 3 and s  X  m r .R 6 =  X  . We can deduce that s 8 must be an answer of m r . The reason is as follows. First s 8 has 3 tokens and cand s 8 = 3. That is all of its tokens must be contained in the message. Thus s 8 satisfies the token constraint. Second, as s 8  X  m r .R 6 =  X  , s 8 satisfies the MBR constraint. Case 2.2 -cand s = i &lt; | s | : In this case, if n is an internal node, there may exist answers under this node, and we need to visit n  X  X  children and repeat the above steps. On the contrary, if n is a leaf node, s cannot be an answer. This is because s only has i &lt; | s | tokens that appear in message m (i.e., it has at least one token which does not appear in m ). Case 3 -cand s &gt; i : In this case n must be a leaf node. This is because if n is an internal node, cand s  X  i since each internal node contains at most one token of s . If cand s Computing cand s : To efficiently compute the occurrence number cand s , we use a hash map M to maintain the oc-currence numbers. Given a message m and a node n , if n.R  X  m.R 6 =  X  , we use the hash-based method (Section 3.2) to compute m.T  X  n.T . For each token t  X  m.T  X  n.T , we access its inverted list I n [ t ]. For each subscription s in I we increase M [ s ] by 1. Notice that as s may be in multiple inverted lists on a leaf node, when computing its occurrence number, we consider all such lists. Obviously cand s = M [ s ]. The R t ++ -tree based Algorithm: Based on the above analysis, we devise an efficient algorithm. We still traverse the R t ++ -tree from the root in pre-order. Given a node n , for each token t  X  n.T  X  m.T , we retrieve the corresponding inverted list I n [ t ]. For each subscription s  X  X  n [ t ], we count its occurrence number cand s . If cand s = | s | and s.R  X  m.R 6 =  X  , s is an answer and added into the result set. If there exists a subscription s such that cand s = i &lt; | s | and n is not a leaf node, there may exist answers under node n . We access the node and repeat the above steps; otherwise if there has no such subscription, we prune node n . The main reason is as follows. First, all subscriptions with no smaller than i tokens under node n cannot be an answer. Second, for subscriptions with smaller than i tokens, if they are answers, they are added as results when accessing n  X  X  ancestors.
Figure 5 illustrates the pseudo-code. R t ++ -Tree first constructs an R t ++ -tree with root r (line 1) and initializes Algorithm 1 : R t + + -Tree ( S , m ) Input : S : A subscription set; m : A message Output : R : Answers of m
Build an R t ++ -tree with root r ;
Initialize a hash map M ;
R t ++ -Tree -Prune ( r, m, R , M ) ; Function R t + + -Tree -Prune ( r , m , R , M ) Input : r : An R t ++ -tree node; m : A message
Output : R : Answers of m visitFlag = false ; for each entry n in node r do a hash map M (line 2). Then it calls function R t ++ -Tree -Prune to filter message m . R t ++ -Tree -Prune first scans each entry (node n ) from the root. If n does not satisfy spatial constraint (line 3), R t ++ -Tree -Prune prunes the node (line 3); otherwise R t ++ -Tree -Prune computes the intersection of its representative-token set and m.T (line 4). Then for each token t in the intersection, R t ++ -Tree -Prune accesses its inverted list I n [ t ] and for each subscription s on I [ t ], it increases M [ s ] by 1 to count its occurrence num-ber (line 6). If M [ s ] = i &lt; | s | and n is not a leaf node, R t ++ -Tree -Prune visits n  X  X  children (To avoid repeatedly visiting n  X  X  children, we first set visitFlag as true in line 8 and then if visitFlag is true , we visit such children in line 12). If M [ s ] = | s | and s.R  X  m.R 6 =  X  , s is an answer and added into the result set (line 10).

Example 8. Consider the R t ++ -tree in Figure 4 and the message m r = ( { iphone4s , ipad2 , AT &amp; T , 64GB } , R ure 1. We first access the root. The first entry, i.e., node n , satisfies the spatial constraint. Thus we compute the intersection of the representative-token set of node n 2 and m r .T . Here we get { 64GB } and the corresponding inverted list I n 2 [ 64GB ] = { s 3 } . Thus cand s 3 = 1 and s 3 date. Next we access node n 2 . We prune its first entry, node n , based on the spatial constraint. For its second entry, n we get the intersection of the representative-token set of n-ode n 5 and m r .T , i.e., { iphone4s } . For each subscription in its inverted list I n 5 [ iphone4s ] = { s 4 , s 5 , s currence number is 1, which is smaller than the node level (i.e., 2). Thus the three subscriptions are not candidates. As there is no candidate, we prune node n 5 .

Theorem 3. The R t ++ -tree based algorithm satisfies com-pleteness and correctness.
In this section we report experimental results. We com-pared with state-of-the-art method IRTree [7]. We extended IRTree to support our problem as discussed in Section 2.2. We used two datasets. The first one was a real dataset Twitter . We collected 60 million tweets from May 2012 to August 2012, in which 13 million tweets had locations. We selected 10 million tweets with region information (denot-ed by polygon in the twitter dataset) as subscriptions and used the others as messages. Each subscription contained an MBR and had 1-5 tokens selected from the tweets. The average token number of subscriptions was 3. The token distribution follows a Zipf X  X  law.

We generated four groups of messages and each group c ontained 10,000 messages. (1) Short Point Messages: Each message contained 6-20 tokens and had a point location. (2) Long Point Mes-sages: Each message contained 100-1000 tokens and had a point location. (3) Short Range Messages: Each mes-sage contained 6-20 tokens and had an MBR region. (4) Long Range Messages: Each message contained 100-1000 tokens and had an MBR region.

We also used a synthetic dataset by combing Point of In-terests (POIs) in USA and publications in DBLP. The USA dataset contained 17 million POIs and DBLP had 1.5 million publications. We generated MBRs from the POIs by select-ing a POI as the center and extending a random width and height. Each subscription was generated by selecting an M-BR and 1-5 tokens from DBLP. Each message was generated by selecting an MBR and a publication. We also generated four groups of messages. Table 1 summarized datasets (the subscription length is the number of tokens).

All the algorithms were implemented in C++. All the experiments were run on a Windows Server 2008 machine with an Intel Core E5410 2.33GHz CPU and 16 GB memory. In the experiments we set b = 25 and B = 50.
We evaluated different sorting strategies: random , df , idf , and locality-aware (We generated 10,000 grids and used the idf order) on the R t + -tree as discussed in Section 4.2. The R t + -tree sizes for the four methods were respectively 0.76 G-B, 0.72 GB, 0.79 GB, and 0.83 GB. This is because the df order shared many tokens in upper-level nodes and the idf order and the locality-aware method shared few tokens. Fig-ure 6 shows the results. We can see that idf outperformed df which in turns was better than random . This is because idf can prune many unnecessary nodes as infrequent token-s were on the upper-level nodes which had low probability to be contained in messages. df reduced token-set sizes by sharing many common tokens, thus df outperformed ran-dom . The locality-aware method was better than df and idf as it used the locality-aware token distributions. (c) S hort Range Messages (c) S hort Range Messages
We evaluated R t -tree with different token sets, i.e., R tree with token sets, R t + -tree with representative tokens, R t ++ -tree with multiple representative tokens. For R t + and R t ++ -tree , we used the locality-aware method. Figure 7 shows the results. We can see that R t ++ -tree outperformed R t + -tree which was better than R t -tree . This is because R t ++ -tree had larger pruning power and did not involve an expensive verification step. R t + -tree reduced the token-set sizes and decreased the number of candidates against R t -tree , thus it achieved higher performance than R t -tree . For example, in Figure 7(d), for messages with 1000 tokens, R t -tree took 50 milliseconds, and R t + -tree decreased the time to 28 milliseconds, and R t ++ -tree further reduced the time to 12 milliseconds. For short messages, e.g., tweets, in Fig-ures 7(a) and 7(c), R t ++ -tree only took 1-2 milliseconds.
We compared our best method R t ++ -tree with existing approaches, the keyword-first method [25], the spatial-first method [22], and state-of-the-art spatial keyword search method IRTree [7] as discussed in Section 2.2. All algorithms em-ployed an in-memory setting. Figures 8 and 9 show the results on the Twitter and USA datasets respectively.
An observation is that the spatial-first method outperformed the keyword-first method for point messages (Figures 8(a), 8(b), 9(a), 9(b)). The reason is that the spatial-first method efficiently found candidate nodes using spatial index struc-tures while the keyword-first method had no spatial pruning power. Another observation is that the spatial-first method had lower performance than the keyword-first method for range messages (Figures 8(c), 8(d), 9(c), 9(d))), as the spatial-first method had no textual pruning power.

In addition, notice that IRTree also achieved low perfor-mance and was even worse than the spatial-first method . There are two main reasons. First, it associated each R -tree node with a rather large inverted index and it was very ex-pensive to traverse the R -tree by using the large inverted in-dex. Second, it was designed for spatial keyword search and had to access larger numbers of unnecessary nodes. Thus IRTree was inefficient for the filtering problem.

Our R t ++ -tree based algorithm always achieved the high-est performance for any types of messages. This is because R t ++ -tree seamlessly integrated the spatial and textual in-formation and had large pruning power.
We evaluated the scalability of the R t ++ -tree based algo-rithm by varying the numbers of subscriptions. Figure 10 shows the results. We can see that our method scaled very well, and with the increase of the numbers of subscriptions, the elapsed time increased sublinearly. This is because even if the number of subscriptions increased, our indexes still pruned large numbers of unnecessary subscriptions. Figure 10: Scalability of R t ++ -tree on Twitter dataset
Although there are some studies on location-aware pub-lish/subscribe systems from a network perspective (e.g., rout-ing messages) [5, 8, 10], to our knowledge there is no study on this problem focusing on performance and scalability. Spatial Keyword Search: Recently there are many stud-ies on spatial keyword search [30, 6, 15, 13, 27, 7, 28, 26, 3, 23, 4, 21, 20, 17, 18, 29, 16, 12, 19]. The first problem is knn based keyword search, which, given a location and a set of keywords, finds top-k nearest neighbors by considering the distance and textual relevancy. Felipe et al. [13] integrated signature files and R-tree. Cong et al. [7] combined inverted files and R-tree. The second problem is region based key-word search, which, given a region and a keyword query, finds the relevant objects in this region. Zhou et al. [30] discussed several strategies to combine R-tree and inverted indexes. Hariharan et al. [15] integrated inverted lists in-to R-tree nodes. The third problem is collective keyword search, which, given a set of keywords, finds a set of close objects that match the keywords. Zhang et al. [27, 28] inte-grated keyword bitmap and MBR into R-tree nodes to find the closest objects.

Obviously the above problems substantially differ from our location-aware publish/subscribe problem, since they use a pull model and we employ a push model. (c) S hort Range Messages (c) S hort Range Messages Publish/Subscribe Services: Foltz and Dumais [14] s-tudied the information filtering problem from the IR per-spective. Fabret et al. [11] and Aguilera et al. [1] studied the publish/subscribe problem from the database perspec-tive. Yan and Garcia-Molina studied keyword-based filtering based on a binary model [25] and a vector space model [24]. There are some studies on XML filtering [2, 9]. Existing pub-lish/subscribe methods only consider textual descriptions while we consider both textual and spatial information. We study the location-aware publish/subscribe problem. We propose an effective index structure R t -tree by integrat-ing textual description into R -tree nodes. We develop a filter-and-verification framework, and devise efficient filtering al-gorithms. We propose reducing the number of tokens associ-ated with each node which not only reduces index sizes but improves performance. We also devise an efficient algorith-m which directly finds answers and avoids the verification step. Experimental results on real data sets show that our method achieves high performance and good scalability.
