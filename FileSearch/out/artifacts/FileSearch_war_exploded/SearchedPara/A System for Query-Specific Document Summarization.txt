 There has been a great amount of work on query-independent summarization of documents. Howe ver, due to the success of Web search engines query-specific document summarization (query result snippets) has beco me an important problem, which has received little attention. We present a method to create query-specific summaries by identifying the most query-relevant fragments and combining them us ing the semantic associations within the document. In particular, we first add structure to the documents in the preprocessing stage and convert them to document graphs . Then, the best summaries are computed by calculating the top spanning trees on the document graphs. We present and experimentally evaluate efficient algorithms that support computing summaries in in teractive time. Furthermore, the quality of our summarization method is compared to current approaches using a user survey. H.3.3 [ Information Search and Retrieval ]: Search process Algorithms, Performance, Experimentation. query-specific summarization, keyword search, Steiner tree problem, user survey As the number of documents availa ble on users X  desktops and the Internet increases, so does the need to provide high-quality summaries in order to allow the user to quickly locate the desired summarization is the snippets ge nerated by Web search engines for each query result, which assist users in further exploring individual results. The Information Retrieval (IR) community has largely viewed text documents as linear sequences of words for the purpose of summarization (with some exceptions as explained in Section 2). Although this model has proven quite successful in efficiently answering keyword queries, it is clearly not optimal since it ignores the inherent structure in documents. independent and follow one of the following two extreme approaches: Either they simply extract relevant passages viewing the document as an unstructured se t of passages, or they employ Natural Language Processing tec hniques. The former approach ignores the structural information of documents while the latter is too expensive for large datasets (e.g., the Web) and sensitive to the writing style of the documents. of a graph, to text documents in order to allow effective query-specific summarization. That is, we view a document as a set of interconnected text fragments (p assages). We focus on keyword queries since keyword search is the most popular information discovery method on documents, because of its power and ease of use. Our technique has the follo wing key steps: First, at the preprocessing stage, we add structure to every document (explained later), which can then be viewed as a labeled, weighted graph, called the document graph . Then, at query time, given a set of keywords, we perform keyword proximity search on the document graphs to discover how th e keywords are associated in the document graphs. For each document its summary is the minimum spanning tree on the corresponding document graph that contains all the keywords (or equivalent based on a thesaurus). the document and split it into text fragments using a delimiter (e.g., the new line character). Each text fragments becomes a node in the document graph. A weighted edge is added to the document graph between two nodes if they either correspond to adjacent the weight of an edge denotes the degree of the relationship. There are many possible ways to define the degree of the relationship between tw o text fragments. share common words (not stop words) and the degree of relationship is calculated by an adaptation of traditional IR term weighting formulas. We also consider a thesaurus to enhance the word matching capability of the system. To avoid dealing with a highly interconnected graph, which would lead to slower execution times and higher maintena nce cost, we only add edges Project partly supported by NSF grant IIS-0534530. with weight above a threshold. Al so notice that the edge weights are query-independent, so they can be precomputed. Example 1. Figure 2 shows the document graph for the document of Figure.1. The document is first split to text fragments v0...v16, which correspond to its paragraphs (other delimiters are possible as we explain below). Notice that the edge between nodes v8 and v7 has the highest weight because there are many infrequent (and hence with higher idf value) words that are common between them like  X  X onoghue X  and  X  X rainGate X . Figure 2. Document Graph for the document in Figure 1. document is processed as follows to create the best query-specific summary. First, each node of the document graph is assigned a score according to the relevance of the corresponding text fragment to the query. To do so we employ traditional IR ranking functions. Notice that a full-text index is used to accelerate this step. Then, we execute our keyword proximity algorithms, which are inspired by the techniques developed in previous work on proximity search on graphs [7], where approximation algorithms are presented for the Group Stei ner Tree problem (which is equivalent 1 to the proximity search problem). The best summary is the top-ranked spanning tree that contains all the keywords. The ranking considers both the node and the edge weights (which are query-dependent and independent respectively). Notice that the problem can be easily modified to allow summaries that do not contain all keywords, although this case is not further discussed in this paper. Example 1 (cont X  X ). Table 1 shows the top-ranked spanning trees for the document graph of Figure 2 for the query  X  X rain chip research X . The values shown a bove the nodes in Table 1 indicate the node scores with respect to the query. The scores of the spanning trees are a function of their node and edge scores, as explained in Section 4. Notice th at all results contain all query keywords. The top result is the best summary of the document of Figure 1 (the keywords of the query are shown in bold) for this minimum possible number of nodes and the edge that connects the two nodes is strong. even though it has fewer nodes. The reason is that the nodes of Result #4 are connected through very commonly occurring words like  X  X omputer X  and  X  X rain X  whereas in Result #3 they are connected through infrequent words like  X  X riehs X . Notice that to compute the frequency of a keywor d we consider all documents of the corpus. Table 1. Top-5 summaries for query  X  X rain Chip Research X  1 67.74 2 84.77 3 87.64 4 103.77 5 167.41 related work. Section 3 formally defines the problem, while Section 4 explains how we add st ructure to documents. Section 5 presents the various algorithms for efficient summary computation using the document gr aphs. Sections 6 and 7 present the quality and performance experiments respectively. Section 8 describes the developed prototype. Finally Section 9 discusses our conclusions and future work. 
A large corpus of work has focused on generating query-independent summaries [6,3, 16,5]. The OCELOT system [6] provides the summary of a web pa ge by selecting and arranging the most (query-independent)  X  X  mportant X  words of the page. OCELOT uses probabilistic models to guide the selection and ordering of words into a summary. Amitay and Paris [3] propose a new fully automatic pseudo-su mmarization technique for Web http://duc.nist.gov/ pages, where the anchor text of hyperlinked pages is used to construct summaries. This approach is unique since it ignores the actual content of a document. [5] uses lexical chains for text summarization. In particular, they use Wordnet to create all lexical chains and choose the str ongest ones as a summary of the document. Understanding Conference [11] (a large scale summarization evaluation effort sponsored by th e United States government), and the Text Summarization Challenge [14] are extraction based. Extraction-based automatic text summarization systems extract parts of original documents and output the results as summaries [10,12,16,20,25]. Other systems ba sed on information extraction [35,47] and discourse analysis [30,42] also exist but they are not yet usable for general-domain summarization. However these works do not exploit the inherent structure of the document and mostly focus on query-independent summaries. In this work (a preliminary version appears in [44]) we also show the semantic connections between the extracted fragments, which improve the quality as shown in Section 6. summaries for documents, which cannot scale to large corpora like the Web and are limited to the writing style of the page authors. [47] and Goldstein et al. [16] create query-dependent summaries using a sentence extraction model in which the documents (web pages) are broken up into their component sentences and scored according to factors such as their position, the words they contain, and the proportion of query terms they contain. A number of the highest-scoring sentences are then chosen as the summary. Lin [27] compresses the sentences to achieve better summaries. [1,19,38,39] select the best passa ge of a document as its summary. However, these works ignore po ssible semantic connections between the sentences or the possibility that linking a relevant set of text fragments will provide a better summary. Radev et al. [36] provide a technique for multi-document summarization used to cluster the results of a web keyword query. However, their clustering and summarization t echniques are query-independent in contrast to our work. [13, 32] provide a technique to rank sentences based on their similar ity with other sentences across multiple documents and then provide a summary with the top ranked sentences. However, thei r methods are query-independent in contrast to our work. extract query-independent rankings for the fragments, for the purpose of improving the performance of web search and also to facilitate web mining and accessibility. Cai et al. [9] partition a web page into blocks using th e vision-based page segmentation algorithm. Based on block-level link analysis, they proposed two new algorithms, Block Level Page Rank and Block Level HITS to extract authoritative parts of a page. Lee et al. [26] discuss a Web block classification algorithm af ter Web page division into algorithms for block importance. MSN Search, and so on) generate query-specific snippets of the returned results. Although their algorithms are not published, we observed that they simply extract some of the query keywords and their surrounding words. Recently, some of these companies made functionality on a user X  X  desktop [17,33]. We include these snippets in our user study of Section 6. In creating the document graph and computing the node weights, we adopt ranking principles fro m the Information Retrieval community. Various methods for weighting terms have been developed [40]. The most widely used are the Okapi (Equation 1) and the pivoted normalization wei ghting, which are based on the tf-idf principle. (between 0 X 1000) are constants In the second stage of our appr oach, when the document graphs are already created and a query arrives, the system searches the document graphs for sub-trees th at contain all query keywords. This problem has been studied by the database and graph-algorithms communities. In particular, recent work [15,7,2,22,21, 24,18] has addressed the problem of free-form keyword search on structured and semi-structured data. These works follow various techniques to overcome the NP-completeness of the Group Steiner problem, to which the ke yword proximity search problems can be reduced. runtime cost. BANKS [7] views the database as a graph and proposes algorithms to approximate the Group Steiner Tree problem. We consider and experime ntally evaluate modifications of these algorithms in this work. XRANK [18] works on XML trees, which simplifies the problem. Li et al. [28] tackle the problem of proximity search on th e Web, which is viewed as a graph of hyperlinked pages. They use of the concept of information unit , which can be viewed as a logical Web document consisting of multiple physical pages. [2,22,21] perform keyword search on relational databases a nd exploit the schema properties to achieve efficient execution. adding structure to unstructured da ta from a completely different angle: how to define a schema to describe a labeled graph (e.g., an XML document). Let D={d 1 ,d 2 ,,...,d n } be a set of documents d occurrences of w in d . Inverse document frequency idf(w) is the inverse of the number of documents containing term w in them. result of the keyword query, which is not the focus of this work, is a list of documents from D ranked according to their relevance to Q . A key component in our work is the document graph G(V,E) of a document d , which is defined as follows: Definition 1 (Document Graph). The document graph G(V,E) of a document d is defined as follows: where the associations between text fragments of d are depicted. Example 1 explains a possible document graph for the document of Figure 1. Notice that there are many ways to define the document graph for a document. In this work we follow a semantic approach where a delimiter is chosen to create text fragments, and edges are added wh en the text fragments contain common (or equivalent) words as we explain in Section 4. document graph may be weighed according to a variety of reasons, both query-dependent a nd independent. For example, authority flow techniques [4] on the document graph can be employed to assign both query-dep endent and independent scores. In this work (see Section 4) we consider query-dependent (resp. independent) weights for the nodes (resp. edges). Definition 2 (Minimal Total Spanning Tree). Given a document graph G(V,E) , a minimal total spanning tree of G with respect to a keyword query Q={w 1 ,...,w m } is a sub-tree T of G that is both: respect to a keyword query Q={w 1 ,...,w m } , is a minimal total spanning tree of G for Q . Problem 1 (Summarization). Given a document d  X  D and its summary, i.e., the minimum score minimal total spanning tree. semantics, that is, require all keywords to be in the summary. Another alternative is OR semantics where not all keywords are required to be in the summary. OR semantics are useful in the following scenarios: (a) the keywords are rare and hence no document contains all of them, so to summarize the query result we need OR semantics and (b) in orde r to have more compact summaries we may choose to not display the less important keywords. In this paper we only present our results on AND semantics due to space limitations. Our techniques and algorithms can be extended to generate summaries for OR semantics, by relaxing the totality constraint on summary spanning trees. that we do not allow any nodes not containing any keyword as leaf nodes in the summary tree. However, nodes with no keywords can be internal nodes. For example, in the second result between nodes v0 and v4 which contain the keywords. function based on the weights of the nodes and edges of T . The scoring function used in this work is presented in Section 4. Example 1 (cont X  X ). For the document of Figure 1 and the keyword query  X  X rain Chip Research X , the top summary is shown in Figure 3. Figure 3. Top summary of the document of Figure 1 for query contain internal text fragments with no query keywords, which are called Steiner nodes. The reason we include such nodes is to achieve semantic coherence in the generated summaries, which increases the user satisfaction as we show in Section 6. If brevity is the top priority then Steiner nodes can be omitted. As we explain below, there are many ways to create and assign weights to a document graph. In this section we present the specific approach we follow to create a document graph. In parameters (explained below), we construct a document graph G(V,E) . Notice that Q is only used in assigning weights to the be computed before queries arrive. precomputation stage to create the document graph: Example 1 (cont X  X ) The new-line character was used to parse the document of Figure 1 into 17 text fragments v0,...,v16 . (text fragments), for each pair of nodes u,v we compute the association degree between them, that is, the score (weight) fragments t(u) , t(v) respectively is: where tf(d,w) is the number of occurrences of w in d, idf(w) is the inverse of the number of documents containing w , and size(d) is the size of the document (in words). words are ignored. Furthermore, we use thesaurus and stemmer (we rely on Oracle interMedia as e xplained in Section 7) to match words that are equivalent. The sum is divided by the sum of the lengths of the text fragments in the same way as the document length ( dl ) is used in traditional IR formulas. Notice that Equation 2 is an adaptation of traditiona l IR formulas for a pair of documents. possible, like the cosine document distance, which however have similar effect as the tf  X  idf method that we employ. In future versions of our system we plan to also use Wordnet and Latent Semantic Indexing techniques to improve the quality of the edge weights, which is challenging on the performance level since our system is interactive. independent part of the document graph creation. Next, when a query Q arrives, the nodes in V are assigned query-dependent weights according to their relevance to Q . In particular, we assign NScore(v) defined by the Okapi formula (Equation 1). In order to accelerate this step of assigning node scores we built a full-text index on the set D of documents that efficiently allows locating the nodes that contain the query keywords and also calculate the query-dependent score. The details of this index are out of the scope of this paper. Given the document graph G and a query Q, a summary (subtree of G) T is assigned a score Score(T) by combining the scores of the nodes v  X  T and the edges e  X  T. In particular Equation 3 computes the summary score . 
Score(T)
Brain chip offers hope for paralyzed. journal Nature in 2002 consisted of attaching an implant to a monkey X  X  brain that enabled it to play a simple pinball computer game remotely. where a and b are constants (we use a =1 and b =0.5), EScore(e) is node v using Equation 1. should degrade (increase) sin ce larger trees denote looser semantic connections [23,2,22,7]. This is the reason we take the sum of the inverse of the edge sc ores in Equation 3. Furthermore, (decrease). Hence, we take the inverse of the sum of the node scores. size of the summary (in number of edges) versus the amount of relevant information contai ned. In particular, higher a values boost the score of smaller and tightly connected summaries, whereas higher b values benefit summaries with more relevant content (i.e., containing nodes with high score with respect to the query). Notice that a and b can also be viewed as adjusting parameters for the query-independe nt and dependent parts of the scoring function respectively. Example 1 (cont X  X ). The top summary T for the document of Figure 1 with document graph s hown in Figure 2 is shown in Figure 4. T has a single edge e (v0,v10) with score determined by the common word  X  X rain X  between v0 and v10 . Also, the scores of nodes v0 , v10 are computed using Equation 1, for the query  X  X rain chip research X . This section tackles the probl em of how; given the document graph G of a document d for a query Q , to compute the top only present algorithms for AND semantics. Notice that the problem of finding the top summary (total minimal spanning tree) is very similar to the Group Stei ner Tree problem [37], which is known to be NP-complete . Our problem is slightly more complex since the groups of nodes are not disjoint, in contrast to the Group Steiner Problem, which is defined as follows: G , and l is a weight function which maps each edge e non-negative real number; and given a family R={R disjoint groups of vertices, where R i is a subset of V , the problem is to find a minimum-cost tree T which contains at least one vertex from each group R i . Since the weights of the graph are non-negative, the solution is a tree-structure. minimal spanning trees are re quested, in our summarization problem we typically care for only the single top summary, that is, the top-1 total minimal spanning tree. This allows more efficient algorithms as we explain below. Notice that the presented algorithms, which can be viewed as approximations of the Group Steiner Tree problem, can be divided along two dimensions. First, we have multi-result and top-1 algorithms, which compute a set of summaries or a single summary for a document and query pair resp ectively. Second, we have enumeration and expanding algor ithms, which follow different execution approaches as explained below.
 Precomputation. In order to boost the performance of the algorithms, we precompute and st ore the following information: This algorithm returns a ranked list of summaries for a document and a query. In particular, it returns a summary for each possible combination of nodes that contain the keywords. all combinations of nodes in G that are minimal (no node is redundant) and total (contain all keywords in Q ). Then, for each combination we create a complete graph G c (called closure graph ) that contains all nodes in the combination and all-pairs edges between them with weight equal to their distance (taken by the precomputed all-pairs shortest paths). Then, we calculate all possible spanning trees in G c , and compute their scores using Equation 3. Then, for the top spanning tree we insert the Steiner nodes and trim redundant nodes to make it minimal. Then its accurate score is computed and added to the results list. Finally, the results are ranked and displayed. Example 2. Consider the document graph in Figure 2 and the query  X  X rain chip research X . The nodes that contain the keywords are v0, v1, v3, v4, v10, v11, and v15 . We then find all minimal and total node combinations, which are { v0, v10} , { v15, v0}, {v0, v3}, MultiResultEnumeration (document graph G , query Q ) 1. Results  X  X  X  ; /*stores summaries*/ 3. Find all minimal combinati ons of nodes that when taken 4. For each minimal node combination C do { 5. Create closure graph G c that contains only the nodes in C ; 6. Find all possible spanning trees S of G c ; 7. Calculate the score of each spanning tree in S using Eq. 3; 8. Pick the spanning tree T with the minimum score; 9. Replace the edges u~v in T with their precomputed shortest 10. Trim T to make it a minimal total spanning tree; 11. Calculate the score of T using Equation 3 and add T to 12. Sort and output summaries in Results ; } { v4, v0}, and so on. For each combination we create a closure graph. For example, the closure graph for the second combination is v15~v0 with edge weight 0.096 (which is the length of the shortest path from v15 to v0 ). We then find all possible spanning trees of this graph, which is just v15~v0, for the above closure graph. Then, we replace the edge between v15 and v0 with the shortest path between them, which is v15~v14~v7~v0. This tree is already minimal and hence we output this result along with its score. The Steiner nodes in this result are v14 and v7 , which don X  X  have any keywords in them but are used to relate the other 2 nodes, v15 and v0 . The Top-1 enumeration algorithm returns only one summary per document, for a query. The reason we created top-1 variants for both the enumeration and the expandi ng search (Sections 5.3, 5.4) is that typically the user only requests a single summary for a document, as in the case of snippets in Web search engine results. result enumeration algorithm, because it only adds the Steiner nodes (line 9 in Figure 4) for a single spanning tree. In particular, this algorithm finds the top spanning tree among all node combinations and then substitutes the Steiner nodes, while the multi-result algorithm finds the top spanning tree and substitutes the Steiner nodes for each node combination. The pseudo-code for this algorithm has the following difference with respect to Figure 4: Lines 8-11 are moved outside the for-loop, that is, the for-loops ends at line 7. Example 2 (cont X  X ). For the document graph in Figure 2 and query  X  X rain chip research X , this algorithm goes through the same steps as in the case of enumeration algorithm. It computes all node combinations as explained in the previous example. The only difference is that this al gorithm first finds the minimum-score spanning tree v1~v3 with edge weight 0.03 (which is the length of the shortest path from v1 to v3 ) among all spanning trees of all node combinations, and then replaces that edge with the recomputes the score and displays it as the summary of the document. The basic idea behind this algorithm (inspired by the algorithm in BANKS [7]) is that we start from the nodes that contain the query keywords and progressively expa nd them in parallel until we find all minimal total spanning trees. The advantage of this algorithm compared to the enumeration algorithms is that we do not need to repeat the processing for all comb inations of nodes, which may be too many if the document is larg e and contains many occurrences of the query keywords. text index) all the nodes that ma tch some keywords in the query and starts expanding them incr ementally, the best (maximum-score) edge at a time. We call the subgraph created from each keyword node v expanding area of v . Notice that, in contrast to BANKS, we use the precomputed all-pairs shortest paths data to efficiently grow the expanding area. That is, we only consider the edges that are contained in a s hortest path from the current node v to any other node u that contains additional query keywords than v . When two or more expanding areas meet we check for possible new summaries. If a summary is f ound, it is trimmed to become minimal and its score is calculate d using Equation 3. The parallel expansion of the expanding areas terminated when for each combination of nodes that contai ns all keywords, their expanding areas have met. Example 3. For the document graph in Figure 2 and the query  X  X rain chip research X , the keyword nodes are v0, v1, v3, v4, v10, v11, and v15 . We grow the expanding area of v0 to v0~v10, which is the first precomputed single source shortest path of source v0 and check for possible summaries. v0~v10 is total as well as minimal and hence we add it to the set of results. We grow each expanding area using its precomputed shortest paths. Then we v11~v10 and once we expand v11 we have another summary v11~v10 that is total and minimal. We keep doing this until the expanding areas of all the keywords nodes have been met and hence we can X  X  have any more possible summaries and hence we terminate. Figure 5. Multi-Result Expanding Search Algorithm This algorithm differs from the multi-result expanding search algorithm in that it stops expanding the expanding areas once the first summary is produced. Intu itively this greedy approach produces a high-quality summary, as the trees produced first have smaller sizes, which implies smaller scores (Equation 3). The pseudo code for the Top-1 expanding search algorithm differs from the multi-result variant in that, once it finds a summary in exits the loop. So we have an extra line in Figure 5:  X  7a. break;  X . 
MultiResultExpandingSearch(document graph G , query Q ) 1. Results  X  X  X  ; /*stores summaries*/ 2. Find all nodes N={N 1 ,...,N m } that contain the keywords in 3. Repeat until the expanding areas of all combinations of 4. For each node v in N do { 5. Add to the expanding area of v the maximum-score 6. Check for new results (summaries) T ; /*i.e., trees that 7. Trim summaries T to become minimal; 9. Sort and output summaries in Results ; Example 3 (cont X  X ). For the document graph in Figure 2 and the query  X  X rain chip research X , this algorithm goes through the same steps as its multi-result variant, but stops expanding once it finds the first summary, which is v10~v11 as explained in the previous example. conducted two surveys. The subjects of the survey are fifteen students (of all levels and various majors) of FIU, who were not involved in the project. In this survey the users were asked to evaluate the summaries based on their quality and size (a longer summary carries more informati on but is less desirable).Each participant was asked to compare the summaries and rank them, assigning a score of 1 to 5, according to their quality for the corresponding query. A rank of 5 (1) represents the summary that is most (least) descriptive. The dataset used in this survey consists of ten documents and two queries taken from the DUC 2005 dataset 3 , as shown in Table 2. We compare our summaries with DUC Peer summaries for quality. DUC peers are human and automatic summaries used in quality evaluation. We compared our summaries against the DUC peers with highest linguistic quality. Unfortunately, most of the summaries in the DUC datasets are query-independent and the few query-dependent ones are multi-document. Hence, in order to compare our work to that of DUC we used the following method to extract single-document summaries from query-dependent multi-document summaries for a set of ten documents over two topics. The sentences that have been extracted from a document d to construct the multi-document summary are viewed as d  X  X  single-document summary for the query/topic. Notice that the DUC summaries are created by extracting whole sentences from documents. Table 2. Average summary ratings for DUC topics Query 1 ( International Organized FT941-3237 2.33 4.66 FT921-7786 4.00 2.50 FT944-8297 2.50 3.33 FT922-190 2.00 4.00 FT931-3563 2.83 3.00 FT921-937 2.00 4.33 FT943-16477 4.00 4.17 FT922-13353 2.83 4.17 FT943-16238 3.67 3.67 FT921-74 2.33 3.67 
The results of the survey prove the superiority of our approach, as shown in Table 2. Our method of combining extracted sentences using semantic connections in the form of Steiner trees leads to higher user satisfaction than the traditional sentence extraction methods. In particular, the Steiner sentences in http://duc.nist.gov/ summaries provide coherency in the aggregation of the keyword-containing-sentences. The dataset used in this survey consists of two news documents taken from the technology section of cnn.com. The participants were asked to evaluate the quality of the summaries of the two documents with respect to five queries. We chose queries where keywords appear both close and far from each other. For each query-document pair, three summaries are displayed corresponding to (a) the result of the Top-1 expanding search algorithm, (b) Google Deskt op X  X  summary, and (c) MSN Desktop X  X  summary. Summaries (b ) and (c) were created by indexing the two documents in our desktop and then submitting the five queries to the Desktop engines. The summaries are the snippets output for these documents . In order to compare apples to apples, we chose queries for which the length of the summaries fair to compare summaries of di fferent lengths as some people favor conciseness while others the amount of information. 3, which we found to produce higher-quality summaries. Notice that by increasing the value of constant a , we favor short results, while by increasing constant b we favor longer and more informative results. Hence, by setting a to 1 and b to 0.5 we favor shorter summaries, which have si milar size to the ones produced by Google and MSN Desktop. This makes their comparison fairer. Table 3. Average summary ratings for documents D1 and D2 Table 4. Queries used for documents D1 and D2
The results of the survey, which prove the superiority of our approach, are shown in Table 3. Notice that Google and MSN Desktop systems do not always include all keywords in the summary when they are more than two and have big distances between them. In contrast, our approach always finds a meaningful way to connect them. To evaluate the performance of our approach we used a dataset of 200 news documents taken from the technology section of cnn.com. We used a PC with Pentium 4 2.44GHz processor and 256MB of RAM running Windows XP. The algorithms were implemented in Java. To build the full-text index we used Oracle interMedia [34] and stored the documents in the database. JDBC was used to connect to the database system. Section 5 for summarizing keywor d queries of various lengths. The execution times consist of two parts: (a) the computation of the scores of the nodes of the document graph (remember that this is query-specific and cannot be precomputed), and (b) the generation of the top summaries (minimal total spanning trees) in the document graph. The first part is handled by Oracle interMedia and the average times for a single document for various-length queries are shown in Table 5. Table 5. Average times to calculate node weights (a) Multi-Result algorithms (b) Top-1 algorithms four algorithms and the results are shown in Figures 6 (a) and (b). In particular, Figure 6 (a) compares the performance of the Multi-Result algorithms, whereas Figure 6 (b) the Top-1 algorithms. We observe that the expanding search algorithms are faster than the enumeration ones, especially for long queries. Also, notice that there is only a slight difference in the performance of the Top-1 and the Multi-Result algorithms, because the document graphs are relatively small and hence there is no big difference between computing one or more summaries. algorithms to BANKS, since our Multi-Result algorithms are adaptations of the BANKS algorith ms to our problem, which is different as we explain in Section 5. the algorithms. In particular, we measure (Table 6) the average rank of the summary of the Top-1 algorithms in the list of summaries created by the Multi-Result algorithms. For example, if the summary of the Top-1 algorithm appears as the third summary of the Multi-Result algorithm, then the rank is 3. We observe that the Top-1 expandi ng algorithm better approximates the corresponding Multi-Result algorithm X  X  results.
 Table 6. Average ranks of Top-1 Algorithms with respect to Multi-Result algorithms Top-1 Expanding Search In this work we presented a structure-based technique to create query-specific summaries for text doc uments. In particular, we first create the document graph of a doc ument to represent the hidden semantic structure of the document and then perform keyword proximity search on this graph. We show with a user survey that our approach performs better than other state of the art approaches. Furthermore, we show the feas ibility of our approach with a performance evaluation . between documents of the dataset. For example, exploit hyperlinks in providing summarization on the Web. Furthermore, we are investigating how the document graph can be used to rank documents with respect to keywor d queries. Finally, we plan to work on more elaborate techniques to split a document to text fragments and assign weights on th e edges of the document graph. [1] J.Abracos and G. Pereira-Lope s. Statistical methods for [2] S. Agrawal, S. Chaudhuri, a nd G. Das. DBXplorer: A [3] E. Amitay, C. Paris: Automatically Summarizing Web Sites [4] A. Balmin, V. Hristidis, Y. Papakonstantinou: Authority-[5] R. Barzilay and M. Elhadad: Us ing lexical chains for text [6] A. L. Berger and V. O. Mittal, OCELOT: A System for [7] G. Bhalotia, C. Nakhe, A. Hulgeri, S. Chakrabarti and [8] P. Buneman, S. Davidson, M. Fernandez, D. Suciu "Adding [9] D. Cai, X. He, J.Wen, W.Ma : Block-level Link Analysis. [10] H.H. Chen, J.J. Kuo, a nd T.C. Su: Clustering and [11] Document Understanding Conference http://duc.nist.gov, [12] H.P. Edmundson: New Methods in Automatic Abstracting. [13] G. Erkan and D.R. Radev. Le xrank: Graph-based centrality [14] T. Fukusima and M. Okum ura: Text Summarization [15] R. Goldman, N. Shivakumar, S. Venkatasubramanian, H. [16] J. Goldstein, M. Kantrow itz, V. Mittal, J. Carbonell: [17] Google Desktop search http://desktop.google.com/ [18] L. Guo, F. Shao, C. Botev, and J. Shanmugasundaram. [19] M.A. Hearst . Using categories to provide context for full-[20] E. Hovy and C.Y. Lin: The automated acquisition of topic [21] V. Hristidis, L. Gravano, Y. Papakonstantinou: Efficient IR-[22] V. Hristidis, Y. Papakonsta ntinou: DISCOVER: Keyword [23] V. Hristidis, Y. Papakonstan tinou, A. Balmin: Keyword [24] V. Kacholia, S. Pandit, S. Ch akrabarti, S. Sudarshan, R. [25] J. Kupiec, J. Pederson, and F. Chen: A Trainable Document [26] C.H. Lee, M.Y. Kan, S. La i: Stylistic and Lexical Co-[27] C.Y. Lin: Improving Summa rization Performance by [28] W.S. Li, K. S. Candan, Q. Vu, and D. Agrawal: Retrieving [29] C. Y. Lin and E. Hovy. Identifying topics by position. In [30] D. Marcu. Discourse trees are good indicators of importance [31] D. Marcu. The rhetorical pars ing of natural language texts. [32] R. Mihalcea, P. Tarau, TextRank: Bringing Order into [33] MSN Desktop search http://toolbar.msn.com/ [34] Oracle interMedia [35] D.R. Radev and K.R. McKe own: Generating Natural [36] D.R.Radev, W. Fan, Z. Zhang: WebInEssence: A [37] P. W. G. Reich. Beyond Steiner X  X  Problem: A VLSI [38] G. Salton, A. Singhal, C. Buckle y, M. Mitra. Automatic text [39] G. Salton , A. Singhal , M. Mitra, and C. Buckley: [40] A. Singhal: Modern Information Retrieval: A Brief [41] R. Song, H. Liu, J. We n, W. Ma: Learning Block [42] T. Strzalkowski, G. Stein, J. Wang, and B, Wise. A Robust [43] A. Tombros, M. Sanderson. Advantages of Query Biased [44] R. Varadarajan, V Hristidis: Structure-Based Query-Specific [45] R. W. White, I. Ruthven and J. M. Jose: Finding Relevant [46] M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce, and K. [47] K. Zechner. Fast generation of abstracts from general 
