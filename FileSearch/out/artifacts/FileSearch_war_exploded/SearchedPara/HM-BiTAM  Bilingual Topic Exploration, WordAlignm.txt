 not they are from the same document-pair. Consequently, tra nslation models are learned only at sentence-pair level, and document contexts  X  essential fac tors for translating documents  X  are gen-erally overlooked. Indeed, translating documents differs considerably from translating a group of unrelated sentences. A sentence, when taken out of the conte xt from the document, is generally more ambiguous and less informative for translation. One should avoid destroying a coherent document by simply translating it into a group of sentences which are i ndifferent to each other and detached from the context.
 Developments in statistics, genetics, and machine learnin g have shown that latent semantic aspects of complex data can often be captured by a model known as the statistical admixture (or mixed model. In the context of SMT, each parallel document-pair is treated as one such object. Depending on the chosen modeling granularity, all sentence-pairs or w ord-pairs in a document-pair correspond to the basic elements constituting the object, and the mixtu re from which the elements are sampled can correspond to a collection of translation lexicons and m onolingual word frequencies based on in population genetics [6] and text modeling [1, 4].
 Recently, a Bilingual Topic-AdMixture ( BiTAM ) model was proposed to capture the topical aspects of SMT [10]; word-pairs from a parallel document-pair follo w the same weighted mixtures of trans-lation lexicons, inferred for the given document-context. The BiTAMs generalize over IBM Model-target X , under document-specific topical assignment. To in corporate such constituents, we integrate the strengths of both HMM and BiTAM, and propose a Hidden Mark ov Bilingual Topic-AdMixture model, or HM-BiTAM, for word alignment to leverage both loca lity constraints and topical context underlying parallel document-pairs.
 In the HM-BiTAM framework, one can estimate topic-specific w ord-to-word translation lexicons (lexical mappings), as well as the monolingual topic-speci fic word-frequencies for both languages, based on parallel document-pairs. The resulting model offe rs a principled way of inferring optimal translation from a given source language in a context-depen dent fashion. We report an extensive empirical analysis of HM-BiTAM, in comparison with related methods. We show our model X  X  ef-fectiveness on the word-alignment task; we also demonstrat e two application aspects which were untouched in [10]: the utility of HM-BiTAM for bilingual topic explora tion, and its application for improving translation qualities. An SMT system can be formulated as a noisy-channel model [2]: where a translation corresponds to searching for the target sentence e  X  which explains the source model. In this paper, we generalize P ( f | e ) with topic-admixture models.
 An HMM implements the  X  X roximity-bias X  assumption  X  that wo rds  X  X lose-in-source X  are aligned to words  X  X lose-in-target X , which is effective for improvi ng word alignment accuracies, especially for linguistically close language-pairs [8]. Following [8 ], to model word-to-word translation, we introduce the mapping j  X  a e in position i = a generated by an HMM state defined as [ e considered to have a dependency on the previous alignment a alignment between e  X  e where p ( a English sentences, respectively. The transition model enf orces the proximity-bias. An additional pseudo word  X  X ULL X  is used at the beginning of English senten ces for HMM to start with. The HMM implemented in GIZA++ [5] is used as our baseline, which i ncludes refinements such as special treatment of a jump to a NULL word. A graphical model r epresentation for such an HMM is illustrated in Figure 1 (a).
 known , and indeed they serve as the key information for defining doc ument-specific topic weights underlying aligned sentence-pairs or word-pairs . To simplify the outline, the topics here are sam-pled at sentence-pair level; topics sampled at word-pair le vel can be easily derived following the sentence-pairs ( e 3.1 Generative Scheme of HM-BiTAM Given a conjugate prior Dirichlet(  X  ), the topic-weight vector (hereafter, TWV),  X  document-pair ( F of a typical document-pair ( F , E ) , a collection of topic-specific translation lexicons be B  X { B where B given topic indexed by z ; the topic-specific monolingual model  X   X {  X  LDA-style monolingual unigrams. The sentence-pairs { f mixture of topics. Specifically (as illustrated also in Fig. 1 (b)): Under an HM-BiTAM model, each sentence-pair consists of a mi xture of latent bilingual topics; each topic is associated with a distribution over bilingual word-pairs. Each word f is generated by two hidden factors: a latent topic z drawn from a document-specific distribution over K topics, and the English word e identified by the hidden alignment variable a . 3.2 Extracting Bilingual Topics from HM-BiTAM similar semantic meanings. This assumption is captured in o ur model. Shown in Figure 1(b), both specific topic-weight vector.
 Although there is an inherent asymmetry in the bilingual top ic representation in HM-BiTAM (that topic representations of the foreign language via a margina lization over hidden word alignment. For example, the frequency (i.e., unigram) of foreign word f As a result, HM-BiTAM can actually be used as a bilingual topi c explorer in the LDA-style and beyond. Given paired documents, it can extract the represen tations of each topic in both languages using, e.g., LDA), as well as the lexical mappings under each topics, based on a maximal likelihood or Bayesian principle. In Section 5.2, we demonstrate outco mes of this application. We expect that, under the HM-BiTAM model, because bilingual statistics from word alignment a and sharper , which give rise to a more parsimonious and unambiguous tran slation model. We sketch a generalized mean-field approximation scheme for inferring latent variables in HM-BiTAM, and a variational EM algorithm for estimating model p arameters. 4.1 Variational Inference Under HM-BiTAM, the complete likelihood of a document-pair ( F , E ) can be expressed as follows: where P ( ~a | T )= ment jumps; P ( F | ~a, ~z, E , B )= topic-dependent unigram as used in LDA. Apparently, exact i nference under this model is infeasible as noted in earlier models related to, but simpler than, this one [10].
 tively, determined by some variational parameters that correspond to the expected sufficient statis-tics of the dependent variables of each factor [9].
 derivation, and directly give the fixed-point equations bel ow: where 1 (  X  ,  X  ) denotes an indicator function, and  X (  X  ) represents the digamma function. The vector  X   X  topic weights for each sentence-pair ( f icon probabilities, topical distributions of monolingual English language model, and the smoothing factors from the topic prior.
 Equation (7) gives the approximate posterior probability f or alignment between the j -th word in f represent the messages corresponding to the forward and the backward passes in HMM; The third strengths of individual topic-specific lexicons; and the la st term provides further smoothing from monolingual topic-specific aspects.
 Inference of optimum word-alignment One of the translation model X  X  goals is to infer the op-timum word alignment: a  X  = arg max Thus, extracting the optimum alignment amounts to applying an Viterbi algorithm on q ( ~a | ~  X  ) . 4.2 Variational EM for parameter estimation To estimate the HM-BiTAM parameters, which include the Diri chlet hyperparameter  X  , the transition matrix T , the topic-specific monolingual English unigram { ~  X  translation lexicon { B subsection, and optimizing the parameters with respect to t he variational likelihood (the M-step ). Here are the update equations for the M-step:
For updating Dirichlet hyperparameter  X  , which is a corpora-level parameter, we resort to gradient In this section, we investigate three main aspects of the HM-BiTAM model, including word align-ment, bilingual topic exploration, and machine translatio n.
 The training data is a collection of parallel document-pairs , with document boundaries explicitly given. As shown in Table 1, our training corpora are general n ewswire, covering topics mainly about 95 document-pairs, with 627 manually-aligned sentence-pairs and 14,769 alignment-links in total, from TIDES X 01 dryrun data. Word segmentations and tokeniza tions were fixed manually for optimal sentence length of 40.67 words. The long sentences introduce more ambiguities for al ignment tasks. ten documents from TIDES X 04 MT-evaluation are used as the un seen test data. BLEU scores are reported to evaluate translation quality with HM-BiTAM mod els. 5.1 Empirical Validation Word Alignment Accuracy We trained HM-BiATMs with ten topics using parallel corpora of sizes ranging from 6M to 22.6M words; we used the F-measure, t he harmonic mean of precision and recall, to evaluate word-alignment accuracy. Followin g the same logics for all BiTAMs in [10], we choose HM-BiTAM in which topics are sampled at word-pair l evel over sentence-pair level. The baseline IBM models were trained using a 1 8 h 5 4 3 scheme 2 . Refined alignments are obtained from both directions of baseline models in the same way as describ ed in [5].
 Figure 2 shows the alignment accuracies of HM-BiTAM, in comp arison with that of the baseline-HMM, the baseline BiTAM, and the IBM Model-4. Overall, HM-Bi TAM gives significantly better F-measures over HMM, with absolute margins of 7.56%, 5.72% a nd 6.91% on training sizes of Figure 2: Alignment accuracy (F-measure) of differ-6 M, 11 M and 22.6 M words, respectively. In HM-BiTAM, two fact ors contribute to narrowing the same as the baseline-HMM, implementing the  X  X roximity-bias X . Whereas the emission lexical weights are inferred using document contexts. The topic-sp ecific translation lexicons are sharper and smaller than the global one used in HMM. Thus the improvem ents of HM-BiTAM over HMM essentially resulted from the extended topic-admixture le xicons. Not surprisingly, HM-BiTAM also outperforms the baseline-BiTAM significantly, because BiT AM captures only the topical aspects and ignores the proximity bias.
 Notably, HM-BiTAM also outperforms IBM Model-4 by a margin o f 3.43%, 3.64% and 2.73%,re-spectively. Overall, with 22.6 M words, HM-BiTAM outperfor ms HMM, BiTAM, IBM-4 signifi-distortion submodels on top of HMM, which further narrows th e word-alignment choices. However, context as in HM-BiTAM. In a way, HM-BiTAM wins over IBM-4 by l everaging topic models that capture the document context.
 Likelihood on Training and Unseen Documents Figure 3 shows comparisons of the likelihoods of document-pairs in the training set under HM-BiTAM with th ose under IBM Model-4 or HMM. Each point in the figure represents one document-pair; the y -coordinate corresponds to the negative log-likelihood under HM-BiTAM, and the x -coordinate gives the counterparts under IBM Model-4 or HMM. Overall the likelihoods under HM-BiTAM are significa ntly better than those under HMM and IBM Model-4, revealing the better modeling power of HM-B iTAM.
 We also applied HM-BiTAM to ten document-pairs selected fro m MT04, which were not included in the training. These document-pairs contain long sentences and diverse topics. As shown in Table 2, the likelihoods of HM-BiTAM on these unseen data dominates s ignificantly over that of HMM, BiTAM, and IBM Models in every case, confirming that HM-BiTAM indeed offers a better fit and generalizability for the bilingual document-pairs.

Table 2: Likelihoods of unseen documents under HM-BiTAMs, in compar ison with competing models. 5.2 Application 1: Bilingual Topic Extraction Monolingual topics: HM-BiTAM facilitates inference of the latent LDA-style rep resentations of topics [1] in both English and the foreign language (i.e., Ch inese) from a given bilingual corpora. from HM-BiTAM parameters  X  . As discussed in  X  3.2, even though the topic-specific distributions of words in the Chinese corpora are not directly encoded in HM -BiTAM, one can marginalize over alignments of the parallel data to synthesize them based on t he monolingual English topics and the topic-specific lexical mapping from English to Chinese.
 Figure 4 shows five topics, in both English and Chinese, learn ed via HM-BiTAM. The top-ranked frequent words in each topic exhibit coherent semantic mean ings; and there are also consistencies between the word semantics under the same topic indexes acro ss languages. Under HM-BiTAM, the two respective monolingual word-distributions for the same topic are statistically coupled due to sharing of the same topic for each sentence-pair in the two languages. Whereas if one merely apply LDA to the corpora in each language separately, such co upling can not be exploited. This coupling enforces consistency between the topics across la nguages. However, like general clustering algorithms, topics in HM-BiTAM, are not necessarily to pres ent obvious semantic labels. Topic-Specific Lexicon Mapping: Table 3 shows two examples of topic-specific lexicon mapping learned by HM-BiTAM. Given a topic assignment, a word usuall y has much less translation candi-topic-specific lexicons emphasize different aspects of tra nslating the same source words, which can not be captured by the IBM models or HMM. This effect can be obs erved from Table 3. 5.3 Application 2: Machine Translation The parallelism of topic-assignment between languages modeled by HM-BiTAM , as shown in  X  3.2 and exemplified in Fig. 4, enables a natural way of improving t ranslation by exploiting semantic consistency and contextual coherency more explicitly and a ggressively. Under HM-BiTAM, given a source document D source word, P ( e | f, D according to the topic weights p ( z | D We used p ( e | f, D trained with 250 M words. We kept all other parameters the sam e as those used in the baseline. Then decoding of the unseen ten MT04 documents in Table 2 was carri ed out. Table 4 shows the performance of our in-house Hiero system (f ollowing [3]), the state-of-the-art Gale-baseline (with a better BLEU score), and our HM-BiTAM m odel, on the NIST MT04 test set. If we know the ground truth of translation to infer the to pic-weights, improvement is from 32 . 78 to 34 . 17 BLEU points. With topical inference from HM-BiTAM using mon olingual source document, improved N-gram precisions in the translation we re observed from 1-gram to 4-gram. potentially more ambiguities for translations than the hig her order ngrams, because the later ones encode already contextual information. The overall BLEU sc ore improvement of HM-BiTAM over p = 0 . 043 . We presented a novel framework, HM-BiTAM, for exploring bil ingual topics, and generalizing over traditional HMM for improved word-alignment accuracies an d translation quality. A variational in-ference and learning procedure was developed for efficient t raining and application in translation. We demonstrated significant improvement of word-alignment accuracy over a number of existing systems, and the interesting capability of HM-BiTAM to simu ltaneously extract coherent monolin-gual topics from both languages. We also report encouraging improvement of translation quality HM-BiTAM remains a purely autonomously trained system. Fut ure work also includes extensions with more structures for word-alignment such as noun phrase chunking.

