 a tree bank or a corpus of unannotated sentences. It has been conjectured in (Wetherell, 1980) that these methods always provide probabilistic context-free grammars with the consistency property. A first re-sult in this direction was presented in (Chaudhuri et al., 1983), by showing that a probabilistic context-free grammar estimated by maximizing the likeli-hood of a sample of parse trees is always consistent.
In later work by (S  X  anchez and Bened  X   X , 1997) and (Chi and Geman, 1998), the result was in-dependently extended to expectation maximization, which is an unsupervised method exploited to es-timate probabilistic context-free grammars by find-ing local maxima of the likelihood of a sample of unannotated sentences. The proof in (S  X  anchez and Bened  X   X , 1997) makes use of spectral analysis of ex-pectation matrices, while the proof in (Chi and Ge-man, 1998) is based on a simpler counting argument. Both these proofs assume restrictions on the un-derlying context-free grammars. More specifically, in (Chi and Geman, 1998) empty rules and unary rules are not allowed, thus excluding infinite ambi-guity, that is, the possibility that some string in the input sample has an infinite number of derivations in the grammar. The treatment of general form context-free grammars has been an open problem so far.
In this paper we consider several estimation meth-ods for probabilistic context-free grammars, and we show that the resulting grammars have the consis-tency property. Our proofs are applicable under the most general conditions, and our results also include the expectation maximization method, thus solving the open problem discussed above. We use an alternative proof technique with respect to pre-there is a one-to-one correspondence between com-plete derivations and parse trees for strings in L ( G ) .
For X  X  V and  X   X  V  X  , we write f ( X,  X  ) to denote the number of occurrences of X in  X  . For ( A  X   X  )  X  R and a derivation d , f ( A  X   X , d ) denotes the number of occurrences of A  X   X  in d . We let f ( A, d ) = P  X  f ( A  X   X , d ) .

A probabilistic CFG (PCFG) is a pair G = ( G, p G ) , where G is a CFG and p G is a function from R to real numbers in the interval [0 , 1] . We say that G is proper if, for every A  X  N , we have Function p G can be used to assign probabilities to derivations of the underlying CFG G , in the follow-ing way. For d =  X  1  X  X  X   X  m  X  R  X  , m  X  0 , we define Note that p G (  X  ) = 1 . The probability of a string w  X   X   X  is defined as A PCFG is consistent if Consistency implies that the PCFG defines a proba-bility distribution over both sets D ( G ) and L ( G ) . If a PCFG is proper, then consistency means that no probability mass is lost in derivations of infinite length. All PCFGs in this paper are implicitly as-sumed to be proper, unless otherwise stated. In this section we give a brief overview of some esti-mation methods for PCFGs. These methods will be later investigated to show that they always provide consistent PCFGs.

In natural language processing applications, esti-mation of a PCFG is usually carried out on the ba-sis of a tree bank, which in this paper we assume to be a sample , that is, a finite multiset, of complete derivations. Let D be such a sample, and let D be where E p denotes an expectation computed under distribution p , and p G ( d | w ) is the probability of derivation d conditioned by sentence w (so that p
G ( d | w ) &gt; 0 only if y ( d ) = w ). The solution to the above system is not unique, because of the non-linearity. Furthermore, each solution of (9) identi-fies a point where the curve in (8) has partial deriva-tives of zero, but this does not necessarily corre-spond to a local maximum, let alone an absolute maximum. (A point with partial derivatives of zero that is not a local maximum could be a local min-imum or even a so-called saddle point.) In prac-tice, this system is typically solved by means of an iterative algorithm called inside/outside (Charniak, 1993), which implements the expectation maximiza-tion (EM) method (Dempster et al., 1977). Starting with an initial function p G that probabilistically ex-tends G , a so-called growth transformation is com-puted, defined as p G ( A  X   X  ) = Following (Baum, 1972), one can show that p
G ( C )  X  p G ( C ) . Thus, by iterating the growth trans-formation above, we are guaranteed to reach a local maximum for (8), or possibly a saddle point. We refer to this as the unsupervised MLE method. We now discuss a third estimation method for PCFGs, which was proposed in (Corazza and Satta, 2006). This method can be viewed as a general-ization of the supervised MLE method to probabil-ity distributions defined over infinite sets of com-plete derivations. Let D be an infinite set of com-plete derivations using nonterminal symbols in N , start symbol S  X  N and terminal symbols in  X  . We assume that the set of rules that are observed in D is drawn from some finite set R . Let p D be a probability distribution defined over D , that is, a function from set D to interval [0 , 1] such that Consider the CFG G = ( N,  X , R, S ) . Note that D  X  D ( G ) . We wish to extend G to some PCFG G = ( G, p G ) in such a way that p D is approxi-mated by p G (viewed as a distribution over complete derivations) as well as possible according to some criterion. One possible criterion is minimization of This is the supervised MLE estimator in (7). This re-minds us of the well-known fact that maximizing the likelihood of a (finite) sample through a PCFG dis-tribution amounts to minimizing the cross-entropy between the empirical distribution of the sample and the PCFG distribution itself. In this section we recall a renormalization technique for PCFGs that was used before in (Abney et al., 1999), (Chi, 1999) and (Nederhof and Satta, 2003) for different purposes, and is exploited in the next section to prove our main results. In the remainder of this section, we assume a fixed, not necessarily proper PCFG G = ( G, p G ) , with G = ( N,  X , S, R ) . We define the renormalization of G as the PCFG R ( G ) = ( G, p R ) with p R specified by It is not difficult to see that R ( G ) is a proper PCFG. We now show an important property of R ( G ) , dis-cussed before in (Nederhof and Satta, 2003) in the context of so-called weighted context-free gram-mars.
 Lemma 1 For each derivation d with A d  X  w , A  X  N and w  X   X   X  , we have Proof. The proof is by induction on the length of d , written | d | . If | d | = 1 we must have d = ( A  X  w ) , and thus p R ( d ) = p R ( A  X  w ) . In this case, the statement of the lemma directly follows from (15).
Assume now | d | &gt; 1 and let  X  = ( A  X   X  ) be the first rule used in d . Note that there must be at least one nonterminal symbol in  X  . We can A i  X  N , 1  X  i  X  q , and u j  X   X   X  , 0  X  j  X  q . In words, A 1 , . . . , A q are all of the occur-rences of nonterminals in  X  , as they appear from left to right. Consequently, we can write d in the form d =  X   X  d 1  X  X  X  d q for some derivations d i , 1  X  i  X  q , with A i d i  X  w i , | d i | X  1 and with In this section we prove the main results of this paper, namely that all of the estimation methods discussed in Section 3 always provide consistent PCFGs. We start with a technical lemma, central to our results, showing that a PCFG that minimizes the cross-entropy with a distribution over any set of derivations must be consistent.
 Lemma 2 Let G = ( G, p G ) be a proper PCFG and let p D be a probability distribution defined over some set D  X  D ( G ) . If G minimizes function H ( p D || p G ) , then G is consistent.
 Proof. Let G = ( N,  X , S, R ) , and assume that G is not consistent. We establish a contradiction. Since G is not consistent, we must have P d,w p G ( S d  X  w ) &lt; 1 . Let then R ( G ) = ( G, p R ) be the renormalization of G , defined as in (15). For any derivation S d  X  w , w  X   X   X  , with d in D , we can use Lemma 1 and write In words, every complete derivation d in D has a probability in R ( G ) that is strictly greater than in G . But this means H ( p D || p R ) &lt; H ( p D || p G ) , against our hypothesis. Therefore, G is consistent and p G is a probability distribution over set D ( G ) . Thus function H ( p D || p G ) can be interpreted as the cross-entropy. (Observe that in the statement of the lemma we have avoided the term  X  X ross-entropy X , since cross-entropies are only defined for probability distributions.)
Lemma 2 directly implies that the cross-entropy minimization method in (12) always provides a con-sistent PCFG, since it minimizes cross-entropy for a distribution defined over a subset of D ( G ) . We have already seen in Section 3 that the supervised MLE method is a special case of the cross-entropy min-imization method. Thus we can also conclude that a PCFG trained with the supervised MLE method is estimator (10) already discussed in Section 3. In fact, for each w  X  L ( G ) and d  X  D ( G ) , we have p
G ( d | w ) = result, namely that a general form PCFG obtained at any iteration of the EM method for the unsupervised MLE is always consistent. In this paper we have investigated a number of methods for the empirical estimation of probabilis-tic context-free grammars, and have shown that the resulting grammars have the so-called consistency property. This property guarantees that all the prob-ability mass of the grammar is used for the finite strings it derives. Thus if the grammar is used in combination with other probabilistic models, as for instance in a speech processing system, consistency allows us to combine or compare scores from differ-ent modules in a sound way.

To obtain our results, we have used a novel proof technique that exploits an already known construc-tion for the renormalization of probabilistic context-free grammars. Our proof technique seems more intuitive than arguments previously used in the lit-erature to prove the consistency property, based on counting arguments or on spectral analysis. It is not difficult to see that our proof technique can also be used with probabilistic rewriting formalisms whose underlying derivations can be characterized by means of context-free rewriting. This is for instance the case with probabilistic tree-adjoining grammars (Schabes, 1992; Sarkar, 1998), for which consistency results have not yet been shown in the literature.
 In order to make this paper self-contained, we sketch a proof of the claim in Section 3 that the estimator in (12) minimizes the cross entropy in (11). A full proof appears in (Corazza and Satta, 2006).
Let D , p D and G = ( N,  X , R, S ) be defined as in Section 3. We want to find a proper PCFG G = ( G, p G ) such that the cross-entropy H ( p D || p G ) is minimal. We use Lagrange multipliers  X  A for each A  X  N and define the form We sum over all strings  X  such that ( A  X   X  )  X  R , deriving From each equation  X   X   X  X  P  X  p G ( A  X   X  ) = 1 for each A  X  N (our original constraints). Combining this with (28) we obtain Replacing (29) into (27) we obtain, for every rule ( A  X   X  )  X  R , This is the estimator introduced in Section 3.
