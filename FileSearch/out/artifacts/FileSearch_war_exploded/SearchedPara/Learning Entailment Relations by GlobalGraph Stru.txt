 Tel Aviv University Bar-Ilan University Bar-Ilan University inference. In this article we propose a global inference algorithm that learns such entailment rules. First, we define a graph structure over predicates that represents entailment relations as of edges, formulating the optimization problem as an Integer Linear Program. The algorithm is rules between predicates that co-occur with this concept. Results show that our global algorithm improves performance over baseline algorithms by more than 10%. 1. Introduction
The Textual Entailment (TE) paradigm is a generic framework for applied semantic inference. The objective of TE is to recognize whether a target textual meaning can be inferred from another given text. For example, a question answering system has to recognize that alcohol affects blood pressure is inferred from the text alcohol reduces blood pressure to answer the question What affects blood pressure? In the TE framework, entailment is defined as a directional relationship between pairs of text expressions, denoted by T , the entailing text, and H , the entailed hypothesis. The text T is said to entail the hypothesis H if, typically, a human reading T would infer that H is most likely true (Dagan et al. 2009).
 entailment rules  X  X ules that specify a directional inference relation between two text fragments (when the rule is bidirectional this is known as paraphrasing ). A common type of text fragment is a proposition , which is a simple natural language expression that contains a predicate and arguments (such as alcohol affects blood pressure ), where the predicate denotes some semantic relation between the concepts that are expressed by the arguments. One important type of entailment rule specifies entailment between propositional templates , that is, propositions where the arguments are possibly re-placed by variables. A rule corresponding to the aforementioned example may be X reduce blood pressure  X  X affect blood pressure . Because facts and knowledge are mostly led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).
 however, that there are interactions between rules. A prominent phenomenon is that entailment is inherently a transitive relation, and thus the rules X the rule X  X  Z . 1 In this article we take advantage of these global interactions to improve entailment rule learning.
 an entailment graph that models entailment relations between propositional templates (Section 3). Next, we motivate and discuss a specific type of entailment graph, termed a focused entailment graph , where a target concept instantiates one of the arguments of all propositional templates. For example, a focused entailment graph about the target concept nausea might specify the entailment relations between propositional templates like X induce nausea , X prevent nausea ,and nausea is a symptom of X .
 to learn the entailment relations, which comprise the edges of focused entailment maximizes that function given scores provided by a local entailment classifier and a global transitivity constraint. The optimization problem is formulated as an Integer
Linear Program (ILP) and is solved with an ILP solver, which leads to an optimal solution with respect to the global function. In Section 5 we demonstrate that this algorithm outperforms by 12 X 13% methods that utilize only local information as well as methods that employ a greedy optimization algorithm (Snow, Jurafsky, and Ng 2006) rather than an ILP solver.
 components. First, we perform manual comparison between our algorithm and the baselines and analyze the reasons for the improvement in performance (Sections 5.3.1 and 5.3.2). Then, we analyze the errors made by the algorithm against manually pre-pared gold-standard graphs and compare them to the baselines (Section 5.4). Last, we perform a series of experiments in which we investigate the local entailment classifier and specifically experiment with various sets of features (Section 6). We conclude and suggest future research directions in Section 7.
 substantially expanding upon it. From a theoretical point of view, we reformulate the two ILPs previously introduced by incorporating a prior. We show a theoretical relation between the two ILPs and prove that the optimization problem tackled is NP-hard.
From an empirical point of view, we conduct many new experiments that examine analysis of the algorithm is performed and an extensive survey of previous work is provided. 74 2. Background
In this section we survey methods proposed in past literature for learning entailment rules between predicates. First, we discuss local methods that assess entailment given a pair of predicates, and then global methods that perform inference over a larger set of predicates. 2.1 Local Learning
Three types of information have primarily been utilized in the past to learn entailment rules between predicates: lexicographic methods, distributional similarity methods, and pattern-based methods.
 formation about semantic relations between lexical items. WordNet (Fellbaum 1998b), by far the most widely used resource, specifies relations such as hyponymy, synonymy, derivation ,and entailment that can be used for semantic inference (Budanitsky and
Hirst 2006). For example, if WordNet specifies that reduce is a hyponym of affect , then one can infer that X reduces Y  X  X affects Y . WordNet has also been exploited to automatically generate a training set for a hyponym classifier (Snow, Jurafsky, and Ng 2004), and we make a similar use of WordNet in Section 4.1.
 but not for more complex expressions. For example, WordNet does not cover a complex predicate such as X causes a reduction in Y . Another drawback of WordNet is that it only supplies semantic relations between lexical items, but does not provide any information on how to map arguments of predicates. For example, WordNet specifies that there is an entailment relation between the predicates pay and buy , but does not describe the way in which arguments are mapped: if X pays Y for Z then XbuysZfromY .Thus, using WordNet directly to derive entailment rules between predicates is possible only for semantic relations such as hyponymy and synonymy, where arguments typically preserve their syntactic positions on both sides of the rule.
 1998) is a dictionary that provides the mapping of arguments between verbs and their nominalizations and has been utilized to derive predicative entailment rules (Meyers et al. 2004; Szpektor and Dagan 2009). FrameNet (Baker, Fillmore, and Lowe 1998) is a lexicographic resource that is arranged around  X  X rames X : Each frame corresponds to an event and includes information on the predicates and arguments relevant for that specific event supplemented with annotated examples that specify argument positions.
Consequently, FrameNet was also used to derive entailment rules between predicates (Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional man-ually constructed resources for predicates include PropBank (Kingsbury, Palmer, and Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000).
 lexicographic resources tend to have limited coverage. Distributional similarity algo-rithms employ  X  X he distributional hypothesis X  (Harris 1954) and predict a semantic relation between two predicates by comparing the arguments with which they occur. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009;
Schoenmackers et al. 2010), which differ in terms of the specifics of the ways in which predicates are represented, the features that are extracted, and the function used to com-pute feature vector similarity. Next, we elaborate on some of the prominent methods. criterion. A predicate is represented by a binary template , which is a dependency path between two arguments of a predicate where the arguments are replaced by variables.
Note that in a dependency tree, a path between two arguments must pass through their common predicate. Also note that if a predicate has more than two arguments, then it is represented by more than one binary template, where each template corresponds to contains a predicate and three arguments, and therefore is represented by the following three binary templates: X subj  X   X   X  buys obj  X   X  Y , X obj  X   X  prep  X   X   X  for pcomp  X  n  X   X  X  X  X  X  X   X  Y .
 which are the words that instantiate the arguments X and Y , respectively, in a large corpus. Given a template t and its feature set for the X variable F weighted by the pointwise mutual information between the template and the feature: w over the corpus. Given two templates u and v ,the Lin measure (Lin 1998a) is computed for the variable X in the following manner:
The measure is computed analogously for the variable Y and the final distributional similarity score, termed DIRT , is the geometric average of the scores for the two variables:
If DIRT ( u , v ) is high, this means that the templates u and v share many  X  X nformative X  arguments and so it is possible that u  X  v . Note, however, that the DIRT similarity measure computes a symmetric score, which is appropriate for modeling synonymy but not entailment, an inherently directional relation.
 similarity measure. In their work, Szpektor and Dagan chose to represent predicates with unary templates , which are identical to binary templates, only they contain a pred-icate and a single argument, such as: X subj  X   X   X  buys . Szpektor and Dagan explain that unary templates are more expressive than binary templates, and that some predicates can only be encoded using unary templates. They propose that if for two unary templates u then relatively many of the features of u should be covered by the features of v .This is captured by the asymmetric Cover measure suggested by Weeds and Weir (2003) (we omit the subscript x from F u x and F v x because in their setting there is only one argument):
The final directional score, termed BInc (Balanced Inclusion), is the geometric average of the Lin measure and the Cover measure: 76 each argument separately, effectively decoupling the arguments from one another. It is clear, however, that although this alleviates sparsity problems, it disregards an impor-tant piece of information, namely, the co-occurrence of arguments. For example, if one looks at the following propositions: coffee increases blood pressure , coffee decreases fatigue , wine decreases blood pressure , wine increases fatigue , one can notice that the predicates occur with similar arguments and might mistakenly infer that decrease looking at pairs of arguments reveals that the predicates do not share a single pair of arguments.
 estimates the probability that two predicates are synonymous (synonymy is simply bidirectional entailment) by comparing pairs of arguments. They represent predicates and arguments as strings and compute for every predicate a feature vector that counts that number of times it occurs with any ordered pair of words as arguments. Their main modeling decision is to assume that two predicates are synonymous if the number of pairs of arguments they share is maximal. An earlier work by Szpektor et al. (2004) also tried to learn entailment rules between predicates by using pairs of arguments as features. They utilized an algorithm that learns new rules by searching for distributional similarity information on the Web for candidate predicates.
 tifying the existence of semantic similarity between predicates, they are often unable to discern the exact type of semantic similarity and specifically determine whether it is entailment. Pattern-based methods are used to automatically extract pairs of predicates for a specific semantic relation. Pattern-based methods identify a semantic relation between two predicates by observing that they co-occur in specific patterns in sentences.
For example, from the single proposition He scared and even startled me one might infer that startle is semantically stronger than scare and thus startle
Pantel (2004) manually constructed a few dozen patterns and learned semantic relations between predicates by looking for these patterns on the Web. For example, the pattern
X and even Y implies that Y is stronger than X , and the pattern to X and then Y indicates that Y follows X . The main disadvantage of pattern-based methods is that they are based on the co-occurrence of two predicates in a single sentence in a specific pattern. These events are quite rare and require working on a very large corpus, or preferably, the Web. between nouns, and there has been some work on automatically learning patterns for nouns (Snow, Jurafsky, and Ng 2004). Although these methods can be expanded for predicates, we are unaware of any attempt to automatically learn patterns that describe semantic relations between predicates (as opposed to the manually constructed patterns suggested by Chklovski and Pantel [2004]). 2.2 Global Learning
It is natural to describe entailment relations between predicates (or language expres-sions in general) by a graph. Nodes represent predicates, and edges represent entail-ment between nodes. Nevertheless, using a graph for global learning of all entailment relations within a set of predicates, rather then between pairs of predicates, has attracted little attention. Recently, Szpektor and Dagan (2009) presented the resource Argument-mapped WordNet, providing entailment relations for predicates in WordNet. This re-source was built on top of WordNet and augments it with mapping of arguments for predicates using NomLex (Macleod et al. 1998) and a corpus-based resource (Szpektor and Dagan 2008). Their resource makes simple use of WordNet X  X  global graph structure:
New rules are suggested by transitively chaining graph edges, and then verified using distributional similarity measures. Effectively, this is equivalent to using the intersection of the set of rules derived by this transitive chaining and the set of rules in a distribu-tional similarity knowledge base.
 taxonomy induction, although it involves learning the hyponymy relation between nouns, which is a special case of entailment, rather than learning entailment between predicates. We provide here a brief review of a simplified form of this algorithm. the hyponymy relation between them. The notation H uv  X  T means that the noun u is a hyponym of the noun v in T . They define D to be the set of observed data over all pairs of words, and define D uv  X  D to be the observed evidence we have in the data for the event
H the posterior probability of the event H uv  X  T , given the data. Their goal is to find the taxonomy that maximizes the likelihood of the data, that is, to find
Using some independence assumptions and Bayes rule, the likelihood P ( D expressed:
Crucially, they demand that the taxonomy learned respects the constraint that hy-ponymy is a transitive relation. To ensure that, they propose the following greedy algorithm: At each step they go over all pairs of words ( u , v ) that are not in the taxonomy, and try to add the single hyponymy relation H uv . Then, they calculate the set of relations
S uv that H uv will add to the taxonomy due to the transitivity constraint (all of the relations H uw , where w is a hypernym of v in the taxonomy). Last, they choose to add that set of relations S uv that maximizes P ( D | T ) out of all the possible candidates.
This iterative process stops when P ( D | T ) starts dropping. Their implementation of the algorithm uses a hyponym classifier presented in an earlier work (Snow, Jurafsky, and Ng 2004) as a model for P ( H uv  X  T | D uv )andasingle sparsity parameter k = this article we tackle a similar problem of learning a transitive relation, but we use linear programming (Vanderbei 2008) to solve the optimization problem. 2.3 Linear Programming
A Linear Program (LP) is an optimization problem where a linear objective function is minimized (or maximized) under linear constraints. where c  X  R d is a coefficient vector, and A  X  R n  X  R d and b
In short, we wish to find the optimal assignment for the d variables in the vector x ,such 78 that all n linear constraints specified by the matrix A and the vector b are satisfied by this assignment. If the variables are forced to be integers, the problem is termed an Integer
Linear Program (ILP). ILP has attracted considerable attention recently in several fields of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and
Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor X  X hild nodes in small graphs of three nodes. 3. Entailment Graph
In this section we define a structure termed the entailment graph that describes the entailment relations between propositional templates (Section 3.1), and a specific type of entailment graph, termed the focused entailment graph , that concentrates on entail-ment relations that are relevant for some pre-defined target concept (Section 3.2). 3.1 Entailment Graph: Definition and Properties
The nodes of an entailment graph are propositional templates . A propositional tem-plate is a binary template 2 where at least one of the two arguments is a variable whereas the second may be instantiated. In addition, the sense of the predicate is specified (ac-cording to some sense inventory, such as WordNet) and so each sense of a polysemous predicate corresponds to a separate template (and a separate graph node). For example,
X subj  X   X   X  treats#1 obj  X   X  Y and X subj  X   X   X  treats#2 first and second sense of the predicate treat , respectively. An edge ( u , v ) represents the fact that template u entails template v . Note that the entailment relation transcends hyponymy/troponomy. For example, the template X is diagnosed with asthma entails the template X suffers from asthma , although one is not a hyponym of the other. An example of an entailment graph is given in Figure 1.
 example, X buys Y  X  X acquires Y and X acquires Y  X  X learns Y ,but X buys Y X learns
Y . This violation occurs because the predicate acquire has two distinct senses in the two templates, but this distinction is lost when senses are not specified.
 nodes entail each other. For example, in Figure 1 the nodes X-related-to-nausea and X-associated-with-nausea form a strongly connected component. Moreover, if we merge every strongly connected component to a single node, the graph becomes a Directed
Acyclic Graph (DAG), and a hierarchy of predicates can be obtained. 3.2 Focused Entailment Graphs
In this article we concentrate on learning a type of entailment graph, termed the focused entailment graph. Given a target concept, such as nausea , a focused entailment graph describes the entailment relations between propositional templates for which the target concept is one of the arguments (see Figure 1). Learning such entailment rules in real time for a target concept is useful in scenarios such as information retrieval and question answering, where a user specifies a query about the target concept. The need for such rules has been also motivated by Clark et al. (2007), who investigated what types of knowledge are needed to identify entailment in the context of the RTE challenge, and found that often rules that are specific to a certain concept are required. Another example for a semantic inference algorithm that is utilized in real time is provided by
Do and Roth (2010), who recently described a system that, given two terms, determines the taxonomic relation between them on the fly. Last, we have recently suggested an application that uses focused entailment graphs to present information about a target concept according to a hierarchy of entailment (Berant, Dagan, and Goldberger 2010). concept that instantiates the propositional template usually disambiguates the predicate and hence the problem of predicate ambiguity is greatly reduced. Thus, we do not employ any form of disambiguation in this article, but assume that every node in a focused entailment graph has a single sense (we further discuss this assumption when describing the experimental setting in Section 5.1), which allows us to utilize transitivity constraints.
 constraints is the notion of probabilistic entailment . Whereas troponomy rules (Fellbaum 1998a) such as X walks  X  X moves can be perceived as being almost always correct, rules such as X coughs  X  Xissick might only be true with some probability.
Consequently, chaining a few probabilistic rules such as A might not guarantee the correctness of A  X  D . Because in focused entailment graphs the number of nodes and diameter 4 are quite small (for example, in the data set we 80 present in Section 5 the maximal number of nodes is 26, the average number of nodes is 22.04, the maximal diameter is 5, and the average diameter is 2.44), we do not find this to be a problem in our experiments in practice.
 tion 4.2). Because the number of nodes in focused entailment graphs is rather small, a standard ILP solver is able to quickly reach the optimal solution.
 focused entailment graphs. However, we believe that it is suitable for any entailment graph whose properties are similar to those of focused entailment graphs. For brevity, from now on the term entailment graph will stand for focused entailment graph . 4. Learning Entailment Graph Edges
In this section we present an algorithm that, given the set of propositional templates constituting the nodes of an entailment graph, learns its edges (i.e., the entailment relations between all pairs of nodes). The algorithm comprises two steps (described in
Sections 4.1 and 4.2): In the first step we use a large corpus and a lexicographic resource (WordNet) to train a generic entailment classifier that given any pair of propositional templates estimates the likelihood that one template entails the other. This generic step is performed only once, and is independent of the specific nodes of the target entailment graph whose edges we want to learn. In the second step we learn on the fly the edges of a specific target graph: Given the graph nodes, we use a global optimization approach that determines the set of edges that maximizes the probability (or score) of the entire graph. The global graph decision is determined by the given edge probabilities (or scores) supplied by the entailment classifier and by the graph constraints (transitivity and others). 4.1 Training an Entailment Classifier
We describe a procedure for learning a generic entailment classifier, which can be used to estimate the entailment likelihood for any given pair of templates. The classifier is constructed based on a corpus and a lexicographic resource (WordNet) using the following four steps: (1) Extract a large set of propositional templates from the corpus. (2) Use WordNet to automatically generate a training set of pairs of (3) Represent each training set example with a feature vector of various (4) Train a classifier over the training set. (Lin 1998b) and use the Minipar representation to extract all binary templates from every parse tree, employing the procedure described by Lin and Pantel (2001), which considers all dependency paths between every pair of nouns in the parse tree. We also apply over the extracted paths the syntactic normalization procedure described by Szpektor and Dagan (2007), which includes transforming passive forms into active forms and removal of conjunctions, appositions, and abbreviations. In addition, we use a simple heuristic to filter out templates that probably do not include a predicate: We omit  X  X ni-directional X  templates where the root of template has a single child, such as therapy prep  X   X   X  in p relation, such as in the template nausea vrel  X   X   X  characterized the Minipar passive label vrel . 5 Last, the arguments are replaced by variables, resulting in propositional templates such as X subj  X   X   X  affect obj  X   X  the template after replacing the arguments by variables are termed predicate words . set of positive (entailing) and negative (non-entailing) template pairs. Let T be the set of propositional templates extracted from the corpus. For each t and a single predicate word w , we extract from WordNet the set H of direct hypernyms (distance of one in WordNet) and synonyms of w . For every h template t j from t i by replacing w with h .If t j  X  T , we consider ( t example. Negative examples are generated analogously, only considering direct co-hyponyms of w , which are direct hyponyms of direct hypernyms of w that are not synonymous to w . It has been shown in past work that in most cases co-hyponym terms do not entail one another (Mirkin, Dagan, and Gefet 2006). A few examples for positive and negative training examples are given in Table 1.

Snow, Jurafsky, and Ng (2004) for training a noun hypernym classifier. It differs in some important aspects, however: First, Snow, Jurafsky, and Ng consider a positive example to be any Wordnet hypernym, irrespective of the distance, whereas we look only at direct hypernyms. This is because predicates are mainly verbs and precision drops quickly when looking at verb hypernyms in WordNet at a longer distance. Second,
Snow, Jurafsky, and Ng generate negative examples by looking at any two nouns where one is not the hypernym of the other. In the spirit of  X  X ontrastive estimation X  (Smith and
Eisner 2005), we prefer to generate negative examples that are  X  X ard, X  that is, negative examples that, although not entailing, are still semantically similar to positive examples and thus focus the classifier  X  X  attention on determining the boundary of the entailment class. Last, we use a balanced number of positive and negative examples, because classifiers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse, Khoshgoftaar, and Napolitano 2007; Nikulin 2008). input template pair ( t 1 , t 2 ) determines whether t 1 entails t a template pair by a feature vector where each coordinate is a different distributional similarity score for the pair of templates. The different distributional similarity scores 82 are obtained by utilizing various distributional similarity algorithms that differ in one or more of their characteristics. In this way we hope to combine the various methods proposed in the past for measuring distributional similarity. The distributional similar-ity algorithms we employ vary in one or more of the following dimensions: the way the predicate is represented, the way the features are represented, and the function used to measure similarity between the feature representations of the two templates. tree structures. However, some distributional similarity algorithms measure similarity between binary templates directly (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat,
Pantel, and Hovy 2007; Yates and Etzioni 2009), whereas others decompose binary templates into two unary templates, estimate similarity between two pairs of unary templates, and combine the two scores into a single score (Szpektor and Dagan 2008). instantiated the argument variables in a corpus. Two representations that are used in our experiments are derived from an ontology that maps natural language phrases to semantic identifiers (see Section 5). Another variant occurs when using binary tem-plates: a template may be represented by a pair of feature vectors, one for each variable as in the DIRT algorithm (Lin and Pantel 2001), or by a single vector, where features represent pairs of instantiations (Szpektor et al. 2004; Yates and Etzioni 2009). The former variant reduces sparsity problems, whereas Yates and Etzioni showed the latter is more informative and performs favorably on their data.
 and Pantel 2001) similarity measure, and the directional BInc (Szpektor and Dagan 2008) similarity measure, reviewed in Section 2. Thus, information about the direction of entailment is provided by the BInc measure.
 all possible combinations of the aforementioned dimensions. These scores are then used
Section 5.) This is reminiscent of Connor and Roth (2007), who used the output of unsu-pervised classifiers as features for a supervised classifier in a verb disambiguation task. the training set: margin classifiers (such as SVM) and probabilistic classifiers. Given a pair of templates ( u , v ) and their feature vector F uv
I uv the event that u entails v . A margin classifier estimates a score S
I uv = 1, which indicates the positive or negative distance of the feature vector F the separating hyperplane. A probabilistic classifier provides the posterior probability
P uv = P ( I uv = 1 4.2 Global Learning of Edges
In this step we get a set of propositional templates as input, and we would like to learn all of the entailment relations between these propositional templates. For every pair of templates we can compute the distributional similarity features and get a score from optimal graph X  X hat is, the best set of edges over the propositional templates. Thus, in this scenario the input is the nodes of the graph and the output are the edges. topologies. Because we seek a global solution under transitivity and other constraints, ILP is a natural choice, enabling the use of state-of-the-art ILP optimization packages.
Given a set of nodes V and a weighting function f : V entailment classifier in our case), we want to learn the directed graph G = ( V , E ), where
E = { ( u , v ) | I uv = 1 } , by solving the following ILP over the variables I
Manning (2008) in a coreference resolution task, except that the edges of our graph are directed. The constraints in Equations (10) and (11) state that for a few node pairs, defined by the sets A yes and A no , respectively, we have prior knowledge that one node does or does not entail the other node. Note that if ( u , v ) there must be no path in the graph from u to v , which rules out additional edge combi-nations. We elaborate on how the sets A yes and A no are computed in our experiments in
Section 5. Altogether, this Integer Linear Program contains O ( constraints, and can be solved using state-of-the-art optimization packages. it as a decision problem in the following manner: Given V , f , and a threshold k ,we wish to know if there is a set of edges E that respects transitivity and
Yannakakis (1978) has shown that the simpler problem of finding in a graph G = ( V , E ) a subset of edges A  X  E that respects transitivity and we can conclude that our optimization problem is also NP-hard by the trivial poly-nomial reduction defining the function f that assigns the score 0 for node pairs ( u , v ) / and the score 1 for node pairs ( u , v )  X  E . Because the decision problem is NP-hard, it is clear that the corresponding maximization problem is also NP-hard. Thus, obtaining a solution using ILP is quite reasonable and in our experiments also proves to be efficient (Section 5).
 the type of entailment classifier we prefer to train. 4.2.1 Score-Based Weighting Function. In this case, we assume that we choose to train a margin entailment classifier estimating the score S uv predicts entailment, and a negative score otherwise) and define f
This gives rise to the following objective function:
The term  X   X | E | is a regularization term reflecting the fact that edges are sparse. Intu-itively, this means that we would like to insert into the graph only edges with a score 84
S uv &gt; X  , or in other words to  X  X ush X  the separating hyperplane towards the positive half space by  X  . Note that the constant  X  is a parameter that needs to be estimated and we discuss ways of estimating it in Section 5.2. 4.2.2 Probabilistic Weighting Function. In this case, we assume that we choose to train a probabilistic entailment classifier. Recall that I whether u entails v ,that F uv is the feature vector for the pair of templates u and v ,andde-fine F to be the set of feature vectors for all pairs of templates in the graph. The classifier estimates the posterior probability of an edge given its features: P and we would like to look for the graph G that maximizes the posterior probability
P ( G | F ). In Appendix A we specify some simplifying independence assumptions under which this graph maximizes the following linear objective function:  X 
G prob = argmax estimated in some manner. Thus, the weighting function is defined by f similar: Both contain a weighted sum over the edges and a regularization component reflecting the sparsity of the graph. Next, we show that we can provide a probabilistic interpretation for our score-based function (under certain conditions), which will allow us to use a margin classifier and interpret its output probabilistically. 4.2.3 Probabilistic Interpretation of Score-Based Weighting Function. We would like to use the score S uv , which is bounded in (  X  ,  X  X  X  ), and derive from it a probability P that end we project S uv onto (0, 1) using the sigmoid function, and define P following manner: sigmoid function:
Therefore, when we derive P uv from S uv with the sigmoid function, we can rewrite  X 
G prob as: regularization term  X  is related to the edge prior odds ratio by:  X  = features (such as a linear-kernel SVM), that is, S uv feature values and  X  i denotes feature weights. In this case, the projected probability acquires the standard form of a logistic classifier:
Hence, we can train the weights  X  i using a margin classifier and interpret the output of the classifier probabilistically, as we do with a logistic classifier. In our experiments in Section 5 we indeed use a linear-kernel SVM to train the weights  X  can interchangeably interpret the resulting ILP as either score-based or probabilistic optimization. 4.2.4 Comparison to Snow, Jurafsky, and Ng (2006). Our work resembles Snow, Jurafsky, and Ng X  X  work in that both try to learn graph edges given a transitivity constraint. There are two key differences in the model and in the optimization algorithm, however. First, they employ a greedy optimization algorithm that incrementally adds hyponyms to a large taxonomy (WordNet), whereas we simultaneously learn all edges using a global optimization method, which is more sound and powerful theoretically, and leads to the optimal solution. Second, Snow, Jurafsky, and Ng X  X  model attempts to determine the graph that maximizes the likelihood P ( F | G ) and not the posterior P ( G their objective function as an ILP we get a formulation that is almost identical to ours, only containing the inverse prior odds ratio log 1  X  =  X  log  X  rather than the prior odds ratio as the regularization term (cf. Section 2):
This difference is insignificant when  X   X  1, or when  X  is tuned empirically for optimal performance on a development set. If, however,  X  is statistically estimated, this might cause unwarranted results: Their model will favor dense graphs when the prior odds high (  X &gt; 1or P ( I uv = 1) &gt; 0 . 5), which is counterintuitive. Our model does not suffer from this shortcoming because it optimizes the posterior rather than the likelihood. In
Section 5 we show that our algorithm significantly outperforms the algorithm presented by Snow, Jurafsky, and Ng. 5. Experimental Evaluation
This section presents an evaluation and analysis of our algorithm. 5.1 Experimental Setting
A health-care corpus of 632MB was harvested from the Web and parsed using the Mini-par parser (Lin 1998b). The corpus contains 2,307,585 sentences and almost 50 million 86 word tokens. We used the Unified Medical Language System (UMLS) medical concepts in the corpus. The UMLS is a database that maps natural language phrases to over one million concept identifiers in the health-care domain (termed
CUIs ). We annotated all nouns and noun phrases that are in the UMLS with their (possibly multiple) CUIs. We now provide the details of training an entailment classifier as explained in Section 4.1.
 medical concepts, that is, annotated with a CUI (  X  50,000 templates). This was done to increase the likelihood that the extracted templates are related to the health-care domain and reduce problems of ambiguity.
 we used were different distributional similarity scores for the pair of templates, as summarized in Table 2. Twelve distributional similarity measures were computed over the health-care corpus using the aforementioned variations (Section 4.1), where two feature representations were considered: in the UMLS each natural language phrase may be mapped not to a single CUI, but to a tuple of CUIs. Therefore, in the first representation, each feature vector coordinate counts the number of times a tuple of
CUIs was mapped to the term instantiating the template argument, and in the second representation it counts the number of times each single CUI was one of the CUIs mapped to the term instantiating the template argument. In addition, we obtained the three distributional similarity measures learned by Szpektor and Dagan (2008), over the
RCV1 corpus, 7 as detailed in Table 2. Thus, each pair of templates is represented by a total of 16 distributional similarity scores.
Net and the procedure described in Section 4.1, and trained the entailment classifier with SVMperf (Joachims 2005). We use the trained classifier to obtain estimates for P and S uv , given that the score-based and probabilistic scoring functions are equivalent (cf. Section 4.2.3).
 standard entailment graphs. First, 23 medical target concepts, representing typical top-the most frequent concepts in the health-care corpus. The 23 target concepts are: alcohol , concept, we wish to learn a focused entailment graph (cf. Figure 1). Thus, the nodes of each graph were defined by extracting all propositional templates in which the corre-sponding target concept instantiated an argument at least K ( = 3) times in the health-care corpus (average number of graph nodes = 22.04, std = 3.66, max = 26, min = 13). and constructed the gold standard of graph edges using a Web interface. We gave an oral explanation of the annotation process to each student, and the first two graphs annotated by every student were considered part of the annotator training phase and were discarded. The annotators were able to select every propositional template and observe all of the instantiations of that template in our health-care corpus. For example, selecting the template X helps with nausea might show the propositions relaxation helps with nausea , acupuncture helps with nausea ,and Nabilone helps with nausea . The concept of entailment was explained under the framework of TE (Dagan et al. 2009), that is, the template t 1 entails the template t 2 if given that the instantiation of t is true then the instantiation of t 2 with the same concept is most likely true. target concept disambiguates the propositional templates in focused entailment graphs. templates such as X treats asthma , annotators were unclear whether X is a type of doctor or a type of drug. The annotators were instructed in such cases to select the template, read the instantiations of the template in the corpus, and choose the sense that is most prevalent in the corpus. This instruction was applicable to all cases of ambiguity. nizing TE (RTE) practice (Bentivogli et al. 2009), after initial annotation the two students met for a reconciliation phase. They worked to reach an agreement on differences and corrected their graphs. Inter-annotator agreement was calculated using the kappa statis-tic (Siegel and Castellan 1988) both before (  X  = 0 . 59) and after (  X  = 0 . 9) reconciliation. Each learned graph was evaluated against the two reconciliated graphs.
 possible edges, of which 882 on average were included by the annotators (averaging over the two gold-standard annotations for each graph). The concept graphs were randomly split into a development set (11 concepts) and a test set (12 concepts). ficiently solves the model without imposing integer restrictions branch-and-bound method to find an optimal integer solution. We note that in the 88 experiments reported in this article the optimal solution without integer restrictions was already integer. Thus, although in general our optimization problem is NP-hard, in our experiments we were able to reach an optimal solution for the input graphs very efficiently (we note that in some scenarios not reported in this article the optimal solution was not integer and so an integer solution is not guaranteed a priori). strong evidence that edges are not in the graph. This is done in the following scenarios (examples given in Table 3): (1) When two templates u and v are identical except for a pair of words w u and w v ,and w u is an antonym of w distance  X  2 in WordNet. (2) When two nodes u and v are transitive  X  X pposites, X  that is, if u = X subj  X   X   X  w obj  X   X  Y and v = X obj  X   X  w subj  X   X   X  some transitive verbs that express a reciprocal activity, such as X marries Y , but usually reciprocal events are not expressed using a transitive verb structure.

This is done in a single scenario (see Table 3), which is specific to the output of Minipar: when two templates differ by a single edge and the first is of the type X graphs. We note that we tried to use WordNet relations such as hypernym and synonym as  X  X ositive X  hard constraints (using the constraint I uv performance because the precision of WordNet was not high enough.
 measure evaluates the graph edges directly, and the second measure is motivated by semantic inference applications that utilize the rules in the graph. The first measure is simply the F 1 of the set of learned edges compared to the set of gold-standard edges.
In the second measure we take the set of learned rules and infer new propositions by applying the rules over all propositions extracted from the health-care corpus. We apply example, given the corpus proposition relaxation reduces nausea and the edges X reduces nausea  X  X helps with nausea and X helps with nausea  X  X related to nausea , we eval-uate the set { relaxation reduces nausea, relaxation helps with nausea, relaxation related to learned graphs when compared to the set of propositions inferred by the gold-standard graphs. For both measures the final score of an algorithm is a macro-average F the 24 gold-standard test-set graphs (two gold-standard graphs for each of the 12 test concepts).
 standard desktop. 5.2 Evaluated Algorithms
ILP solver. For each of the 16 distributional similarity measures (Table 2) and for each template t , we computed a list of templates most similar to t (or entailing t for directional measures). Then, for each measure we learned graphs by inserting an edge ( u , v ), when u is in the top K templates most similar to v . The parameter K can be optimized either on the automatically generated training set (from WordNet) or on the manually annotated development set. We also learned graphs using WordNet: We inserted an edge ( u , v ) when u and v differ by a single word w u and w v , respectively, and w or synonym of w v . Next, we describe algorithms that utilize the entailment classifier. tion to find maximum a posteriori graphs. Therefore, we compare it to the following three variants: (1) ILP-Local : An algorithm that uses only local information. This is looks for the maximum a posteriori graphs but only employs the greedy optimization procedure as described by Snow, Jurafsky, and Ng (2006). (3) ILP-Global-Likelihood : An ILP formulation where we look for the maximum likelihood graphs, as described by Snow, Jurafsky, and Ng (cf. Section 4.2).
 the edge prior odds ratio,  X  (or  X  ), is estimated: (1)  X  = 1(  X  = 0), which means that no prior is used. (2) Tuning  X  and using the value that maximizes performance over the development set. (3) Estimating  X  using maximum likelihood over the development set, which results in  X   X  0 . 1(  X   X  2 . 3), corresponding to the edge density P ( I added all edges inferred by transitivity. This was done because we assume that the rules learned are to be used in the context of an inference or entailment system. Because such systems usually perform chaining of entailment rules (Raina, Ng, and Manning 2005;
Bar-Haim et al. 2007; Harmeling 2009), we conduct this chaining as well. Nevertheless, we also measured performance when edges inferred by transitivity are not added: We once again chose the edge prior value that maximizes F and obtained macro-average recall/precision/ F 1 of 51.5/34.9/38.3. This performance is comparable to the macro-average recall/precision/ F 1 of 44.5/45.3/38.1 we report next in Table 4. 5.3 Experimental Results and Analysis In this section we present experimental results and analysis that show that the
ILP-Global algorithm improves performance over baselines, specifically in terms of precision.
 shows our main result when the parameters  X  and K are optimized to maximize per-formance over the development set. Notice that the algorithm ILP-Global-Likelihood is omitted, because when optimizing  X  over the development set it conflates with
ILP-Global. The rows Local 1 and Local 2 present the best algorithms that use a single distributional similarity resource. Local 1 and Local 90
Results with prior estimated on the development set, that is  X  = 0 . 1, which is equivalent to  X  = 2 . 3.

Global improves performance by at least 13%, and significantly outperforms all local methods, as well as the greedy optimization algorithm both on the edges F (p &lt; 0.05) and on the propositions F 1 measure (p &lt; 0.01). parameters  X  and K : A uniform prior ( P uv = 0 . 5) is assumed for algorithms that use the entailment classifier, and the automatically generated training set is employed to estimate K . Again ILP-Global-Likelihood is omitted in the absence of a prior. ILP-Global outperforms all other methods in this scenario as well, although by a smaller margin however, that local algorithms are more vulnerable to this phenomenon. This makes sense because in local algorithms eliminating the prior adds edges that in turn add more edges due to the constraint of transitivity and so recall dramatically rises at the expense of precision. Global algorithms are not as prone to this effect because they refrain from adding edges that eventually lead to the addition of many unwarranted edges. the highest precision and lowest recall. The low recall exemplifies how the entailment relations given by the gold-standard annotators transcend much beyond simple lexical relations that appear in WordNet: Many of the gold-standard entailment relations are missing from WordNet or involve multi-word phrases that do not appear in WordNet at all.
 value (44.1%) is far from perfect. This illustrates that hierarchies of predicates are quite 92 ambiguous and thus using WordNet directly yields relatively low precision. WordNet is vulnerable to such ambiguity because it is a generic domain-independent resource, whereas our algorithm learns from a domain-specific corpus. For example, the words have and cause are synonyms according to one of the senses in WordNet and so the erroneous rule X have asthma  X  X cause asthma is learned using WordNet. Another example is the rule X follows chemotherapy  X  X takes chemotherapy , which is incorrectly inferred because follow is a hyponym of take according to one of WordNet X  X  senses ( she followed the feminist movement ). Due to these mistakes made by WordNet, the precision achieved by our automatically trained ILP-Global algorithm when tuning parameters on the development set (Table 4) is higher than that of WordNet.
 over the development set (by computing the edge density over all the development set graphs), and not tuned empirically with grid search. This allows for a comparison between our algorithm that maximizes the a posteriori probability and Snow, Jurafsky, and Ng X  X  (2006) algorithm that maximizes the likelihood. The gold-standard graphs are quite sparse (  X   X  0 . 1); therefore, as explained in Section 4.2.4, the effect of the prior is substantial. ILP-Global and Greedy-Global learn sparse graphs with high precision and low recall, whereas ILP-Global-Likelihood and Greedy-Global-Likelihood learn dense graphs with high recall but very low precision. Overall, optimizing the a posteriori probability is substantially better than optimizing likelihood, but still leads to a large degradation in performance. This can be explained because our algorithm is not purely probabilistic: The learned graphs are the product of mixing a probabilistic objective function with non-probabilistic constraints. Thus, plugging the estimated prior into this model results in performance that is far from optimal. In future work, we will examine a purely probabilistic approach that will allow us to reach good performance when estimating  X  directly. Nevertheless, currently optimal results are achieved when the prior  X  is tuned empirically.

Local, obtained by varying the prior parameter,  X  . The figure clearly demonstrates the advantage of using global information and ILP. ILP-Global is better than Greedy-Global and ILP-Local in almost every point of the recall X  X recision curve, regardless of the exact value of the prior parameter. Last, we present for completeness in Table 7 the results of ILP-Global for all concepts in the test set.
 sures. The main conclusion we can derive from this table is that the best distributional similarity measures are those that represent templates using pairs of argument instan-tiations rather than each argument separately. A similar result was found by Yates and
Etzioni (2009), who described the RESOLVER paraphrase learning system and have representation that utilizes pairs of arguments comparing to DIRT, which computes a separate score for each argument.
 comparison trying to analyze the importance of using global information in graph learning (Section 5.3.1), as well as the contribution of using ILP rather than a greedy optimization procedure (Section 5.3.2). We note that the analysis presented in both sec-tions is for the results obtained when optimizing parameters over the development set. 5.3.1 Global vs. Local Information. We looked at all edges in the test-set graphs where
ILP-Global and ILP-Local disagree and checked which algorithm was correct. Table 9 presents the result. The main advantage of using ILP-Global is that it avoids inserting wrong edges into the graph. This is because ILP-Local adds any edge ( u , v )suchthat
P uv crosses a certain threshold, disregarding edges that will be consequently added due to transitivity (recall that for local algorithms we add edges inferred by transitivity, cf.
Section 5.2). ILP-Global will avoid such edges of high probability if it results in inserting many low probability edges. This results in an improvement in precision, as exhibited by Table 4.
 and illustrate qualitatively how global considerations improve precision. In Figure 3, we witness that the single erroneous edge X results in diarrhea inserted by the local algorithm because P uv is high, effectively bridges two strongly connected components and induces a total of 12 wrong edges (all edges from the upper component to the lower component), whereas ILP-Global refrains from inserting this edge. Figure 4 depicts an even more complex scenario. First, ILP-Local induces a strongly connected component of five nodes for the predicates control , treat , stop , 94 reduce ,and prevent , whereas ILP-Global splits this strongly connected component into two, which although not perfect, is more compatible with the gold-standard graphs.
In addition, ILP-Local inserts four erroneous edges that connect two components of size 4 and 5, which results in adding eventually 30 wrong edges. On the other hand,
ILP-Global is aware of the consequences of adding these four seemingly good edges, and prefers to omit them from the learned graph, leading to much higher precision. prior is  X  = 0 . 45 in ILP-Global but  X  = 1 . 5 in ILP-Local. Thus, any edge ( u , v )suchthat graph, but will have negative weight in ILP-Local and will be rejected. The reason is that in a local setting, reducing false positives is handled only by applying a large penalty for every wrong edge, whereas in a global setting wrong edges can be rejected because they induce more  X  X ad X  edges. Overall, this leads to an improved recall in ILP-Global.
This also explains why ILP-Local is severely harmed when no prior is used at all, as shown in Table 5.
 the edges in 7 graphs with an average advantage of 11.7 points, ILP-Local achieves better F 1 over the edges in 4 graphs with an average advantage of 3.0 points, and one performance is equal. 5.3.2 Greedy vs. Non-Greedy Optimization. We would like to understand how using an ILP solver improves performance compared with a greedy optimization procedure.
Table 4 demonstrates that ILP-Global and Greedy-Global reach a similar level of re-call, although ILP-Global achieves far better precision. Again, we investigated edges for which the two algorithms disagree and checked which one was correct. Table 10 demonstrates that the higher precision is because ILP-Global avoids inserting wrong edges into the graph.

Global. Parts A1 X  X 3 show the progression of Greedy-Global, which is an incremental algorithm, for a fragment of the headache graph. In part A1 the learning algorithm still separates the nodes X prevents headache and X reduces headache from the nodes X causes headache and X results in headache (nodes surrounded by a bold oval shape constitute a strongly connected component). After two iterations, however, the four nodes are joined into a single strongly connected component, which is an error in principle connected component can no longer be untied. Thus, in A3 we observe that in future iterations the strongly connected component expands further and many more wrong edges are inserted into the graph. On the other hand, in B we see that ILP-Global takes into consideration the global interaction between the four nodes and other nodes of the graph, and decides to split this strongly connected component in two, which improves the precision of ILP-Global. Second, note that in A3 the nodes Associate X with headache and Associate headache with X are erroneously isolated. This is because connecting them to the strongly connected component that contains six nodes will add many edges with 96 low probability and so this is avoided by Greedy-Global. Because in ILP-Global the strongly connected component was split in two, it is possible to connect these two nodes to some of the other nodes and raise the recall of ILP-Global. Thus, we see that greedy optimization might get stuck in local maxima and consequently suffer in terms of both precision and recall.
 the edges in 9 graphs with an average advantage of 10.0 points, Greedy-Global achieves better F 1 over the edges in 2 graphs with an average advantage of 1.5 points, and in one case performance is equal. 5.4 Error Analysis
In this section, we compare the results of ILP-Global with the gold-standard graphs and perform error analysis. Error analysis was performed by comparing the 12 graphs learned by ILP-Global to the corresponding 12 gold-standard graphs (randomly sam-pling from the two available gold-standard graphs), and manually examining all edges for which the two disagree. We found that the number of false positives and false negatives is almost equal: 282 edges were learned by ILP-Global but are not in the gold-standard graphs (false positive) and 287 edges were in the gold-standard graphs but were not learned by ILP-Global (false negatives). that the majority of mistakes are misclassifications of the entailment classifier. For 73.5% of the false negatives the classifier  X  X  probability was P struggles to distinguish between positive and negative examples. Figure 6 illustrates over all node pairs in the 12 test-set graphs. Close to 80% of the scores are in the range 0.45 X 0.5, most of which are simply node pairs for which all distributional similarity features are zero. Although in the great majority of such node pairs ( t does not entail t 2 , there are also some cases where t 1 current feature representation is not rich enough, and in the next section we explore a larger feature set.
 tives are pairs of predicates that are semantically related, that is, 18% of false positives are templates that are hyponyms of a common predicate (co-hyponym error), and 15.1% of false positives are pairs where we err in the direction of entailment (direction error). For example ILP-Global learns that place X in mouth  X  remove X from mouth , which is a 98 co-hyponym error, and also that X affects lungs  X  X damages lungs , which is a direction error because entailment holds in the other direction. This illustrates the infamous between two predicates.
 false negatives one of the two templates contained a  X  X ong X  predicate, that is a predicate composed of more than one content word, such as Ingestion of X causes injury to Y .This statistics for complex predicates. In addition, 26.8% of false negatives were manually analyzed as  X  X enerality errors. X  An example is the edge HPV strain causes X
HPV with X that is in the gold-standard graph but was missed by ILP-Global. Indeed, this edge falls within the definition of textual entailment and is correct: For example, if some HPV strain causes cervical cancer then cervical cancer is associated with HPV.
Because the entailed template is much more general than the entailing template, how-ever, they are not instantiated by similar arguments in the corpus and distributional similarity features fail to capture their semantic similarity. Last, we note that in 20.9% of the false negatives, there was some string overlap between the entailing and entailed templates, for example in X controls asthma symptoms  X  X controls asthma . In the next section we experiment with a feature that is based on string similarity.

ILP-Local or Greedy-Global are correct. An illustrating example for such a case is shown in Figure 7. Looking at ILP-Local we see that the entailment classifier correctly classifies the edges X triggers asthma  X  X causes asthma and X causes asthma
X with asthma , but misclassifies X triggers asthma  X  Associate X with asthma . Because this configuration violates a transitivity constraint, ILP-Global must make a global decision whether to add the edge X triggers asthma  X  Associate X with asthma or to omit one of the correct edges. The optimal global decision in this case causes a mistake with respect to the gold standard. More generally, a common phenomenon of ILP-Global is that it splits components that are connected in ILP-Local, for example, in Figures 3 and 4. ILP-
Global splits the components in a way that is optimal according to the scores of the local entailment classifier, but these are not always accurate according to the gold standard. correct. ILP-Global mistakenly learns entailment rules from the templates Associate X with headache and Associate headache with X to the templates X causes headache and
X results in headache , whereas Greedy-Global isolates the templates Associate X with headache and Associate headache with X in a separate component. This happens because of the greedy nature of Greedy-Global. Notice that in step A2 the templates X causes headache and X results in headache are already included (erroneously) in a connected component with the templates X prevents headache and X reduces headache . Thus, adding the rules from Associate X with headache and Associate headache with X to X causes headache and X results in headache wouldalsoaddtherulesto X reduces headache and X prevents headache and the Greedy-Global avoids that. ILP-Global does not have that problem: It simply chooses the optimal choice according to the entailment classifier, which splits the connected component presented in A2. Thus, once again we see that mistakes made by
ILP-Global are often due to the inaccuracies of the scores given by the local entailment classifier. 6. Local Classifier Extensions
The error analysis in Section 5.4 exemplified that most errors are the result of misclassi-fications made by the local entailment classifier. In this section, we investigate the local entailment classifier component, focusing on the set of features used for classification.
We first present an experimental setting in which we consider a wider set of features, then we present the results of the experiment, and last we perform feature analysis and draw conclusions. 6.1 Feature Set and Experimental Setting
In previous sections we employed a distant supervision framework: We generated training examples automatically with WordNet, and represented each example with distributional similarity features. Distant supervision comes with a price, however X  X t prevents us from utilizing all sources of information. For example, looking at the pair of gold-standard templates X manages asthma and X improves asthma management , one can exploit the fact that management is a derivation of manage to improve the estimation of entailment. The automatically generated training set was generated by looking at Word-
Net X  X  hypernym, synonym, and co-hyponyms relations, however, and hence no such examples appear in the training set, rendering this type of feature useless. Moreover, one cannot use WordNet X  X  hypernym, synonym, and co-hyponym relations as features because the generated training set is highly biased X  X ll positive training examples are either hypernyms or synonyms and all negative examples are co-hyponyms.
 ing the biases that occur due to distant supervision. Therefore, we use the 23 manually annotated gold-standard graphs for both training and testing, in a cross-validation utility of various features in a setting where the training set and test set are sampled from the same underlying distribution, without the aforementioned biases. 100 orthogonal to the one given by distributional similarity. Therefore, we turn to existing knowledge resources that were created using both manual and automatic methods, entailment prediction: 1. WordNet: contains manually annotated relations such as hypernymy, 2. VerbOcean 11 (Chklovski and Pantel 2004): contains verb relations such as 3. CATVAR 12 (Habash and Dorr 2003): contains word derivations such as 4. FRED 13 (Ben Aharon, Szpektor, and Dagan 2010): contains entailment 5. NomLex 14 (Macleod et al. 1998): contains English nominalizations 6. BAP 15 (Kotlerman et al. 2010): contains directional distributional standard examples (resulting in a total of 32 features). The first 15 features were gen-erated by the aforementioned knowledge bases. The last feature measures the edit distance between templates: Given a pair of templates ( t in each template and derive a pair of strings ( s 1 , s 2 string edit-distance (Cohen, Ravikumar, and Fienberg 2003) between s divide the score by | s 1 | + | s 2 | for normalization.
 which the feature value is non-zero (out of the examples generated from the 23 gold-standard graphs). A salient property of many of the new features is that they are sparse:
The four antonymy features as well as the Derivation, Entailment, Nomlex, and FRED features occur in very few examples in our data set, which might make training with these features difficult.
 maximally exploit the manually annotated gold standard for training. For each of the test-set graphs, we train over all development and test-set graphs except for the one that is left out, 16 after tuning the algorithm X  X  parameters and test. Parameter tuning is done by cross-validation over the development set, tuning to maximize the F of learned edges (the development and test set are described in Section 5). Graphs are always learned with the LP-Global algorithm.
 therefore we run the experiment both with and without the new features. In addi-tion, we would like to test whether using different classifiers affects performance.
Therefore, we run the experiments with a linear-kernel SVM, a square-kernel SVM, a Gaussian-kernel SVM, logistic regression, and naive Bayes. We use the SVMPerf package (Joachims 2005) to train the SVM classifiers and the Weka package (Hall et al. 2009) for logistic regression and naive Bayes. 6.2 Experiment Results
Table 13 describes the macro-average recall, precision, and F and without the new features on the development set and test set. Using all features is denoted by X all , and using the original features is denoted by X 102 mance. Whereas on the development set the new features add 1.2 X 1.5 F performance when using SVM on the development set, it is masked by the variance added in the process of parameter tuning. In general, including the new features does not yield substantial differences in performance.
 sifiers. Using the more complex square and Gaussian kernels does not seem justified, however, as the differences between the various kernels are negligible. Therefore, in our analysis we will use a linear kernel SVM classifier.
 vision, the results we get are slightly lower than those presented in Section 5. This is probably due to the fact that our manually annotated data set is rather small. Nev-ertheless, this shows that the quality of the distant supervision training set generated automatically from WordNet is reasonable.
 stand the reasons for the negative result obtained. 6.3 Feature Analysis
We saw that the new features slightly improved performance for SVM classifiers on the development set, although no clear improvement was witnessed on the test set.
To further check whether the new features carry useful information we measured the set graph). Using the new features improved the average training set accuracy from 71.6 to 72.3. More importantly, it improved performance consistently in all 12 training certain amount of information, but this information is too sparse to affect the overall performance of the algorithm. In addition, notice that the absolute accuracy on the training set is low X 72.3. This shows that separating entailment from non-entailment using the current set of features is challenging.
 an ablation test over the features by omitting each one of them and re-training the classifier Linear all . In Table 14, the columns ablation F the difference in performance from the Linear all classifier, which scored 40.3 F
Results show that there is no  X  X ad X  feature that deteriorates performance. For almost all features ablation causes a decrease in performance, although this decrease is relatively small. There are only four features for which ablation decreases performance by more than one point: three distributional similarity features, but also the new hypernym feature.
 boolean features. The column Feature type indicates whether we expect a feature to indicate entailment or non-entailment and the columns Prec. and Recall specify the 104 precision and recall of that feature. For example, the feature FRED is a positive feature that we expect to support entailment, and indeed 77.8% of the gold-standard examples positive examples, however. Similarly, VO ant. is a negative feature that we expect to support non-entailment, and indeed 96% of the gold-standard examples for which it is well over the proportion of positive examples in the gold standard, which is about 10% (except for the Entailment feature whose precision is only 15%). For the negative features it seems that the precision of VerbOcean features is very high (though they are sparse), and the precision of WordNet antonyms and co-hyponyms is lower. Looking at the recall we can see that the coverage of the boolean features is low.
 feature. For each feature we train a linear kernel SVM, tune the sparsity parameter on trained on sparse features yield low performance.
 similarity features. There are three distributional similarity features that achieve F more than 30 points, and all three represent features using pairs of argument instan-tiations rather than treat each argument separately, as we have already witnessed in Section 5.
 measure, and features that are pairs of CUI tuples, is the best feature both in terms of the ablation test and when it is used as a single feature for the classifier. The result obtained by this feature is only 3.3 points lower than that obtained when using the entire feature set. We believe this is for two reasons: First, the 16 distributional similarity features are correlated with one another and thus using all of them does not boost performance substantially. For example, the Pearson correlation coefficients between the features h-b-B-pCt, h-b-B-Ct, h-b-L-pCt, h-b-L-Ct, h-u-B-Ct, and h-u-L-Ct (all utilize CUI tuples) and h-b-B-pC, h-b-B-C, h-b-L-pC, h-b-L-C, h-u-B-C, and h-u-L-C (all use CUIs), respectively, are over 0.9. The second reason for gaining only 3.3 points by the remaining features is that, as discussed, the new set of features is relatively sparse.
 7. Conclusions and Future Work
This article presented a global optimization algorithm for learning entailment rules between predicates, represented as propositional templates. Most previous work on learning entailment rules between predicates focused on local learning methods, which consider each pair of predicates in isolation. To the best of our knowledge, this is the most comprehensive attempt to date to exploit global interactions between predicates for improving the set of learned entailment rules.
 graph under a global transitivity constraint. Two objective functions were defined for the optimization procedure, one score-based and the other probabilistic, and we have shown that under certain conditions (specified in Appendix A) the score-based function can be interpreted probabilistically. This allowed us to use both margin as well as probabilistic classifiers for the underlying entailment classifier. We solved the optimiza-tion problem using Integer Linear Programming, which provides an optimal solution (compared to the greedy algorithm suggested by Snow, Jurafsky, and Ng [2006]), and demonstrated empirically that this method outperforms local algorithms as well as a state-of-the-art greedy optimization algorithm on the graph learning task. We also analyzed quantitatively and qualitatively the reasons for the improved performance of our global algorithm and performed detailed error analysis. Last, we experimented with various entailment classifiers that utilize different sets of features from many knowledge bases.
 the local entailment classifier needs to be improved. We believe that the most promising direction for improving the local classifier is to use methods that look for co-occurrence of predicates in sentences or documents on the Web, because these methods excel at identifying specific semantic relations. It is also possible to use other sources of infor-mation such as lexicographic resources, although this probably will require a learning scheme that is robust to the relatively low coverage of these resources. Increasing the size of the training corpus is also an important direction for improving the entailment classifier.
 that are larger by a few orders of magnitude than the focused entailment graphs dealt with in this article. This will introduce a challenge to our current optimization algorithm due to complexity issues, as our ILP contains O ( | V | 3 require careful handling of predicate ambiguity, which interferes with the transitivity of entailment and will become a pertinent issue in large graphs. Some first steps in this direction have already been carried out (Berant, Dagan, and Goldberger 2011). ment relation. We would like to model more types of edges in the graph, representing additional semantic relations such as co-hyponymy, and to explicitly describe the inter-actions between the various types of edges, aiming to further improve the quality of the learned entailment rules. 106 proposed by Berant, Dagan, and Goldberger (2010), we believe that these hierarchies can be useful not only in the context of semantic inference applications, but also in the field of faceted search and hierarchical text exploration (Stoica, Hearst, and Richardson 2007).
Figure 8 exemplifies how a set of propositions can be presented to a user according to the hierarchy of predicates shown in Figure 1. In the field of faceted search, information is presented using a number of hierarchies, corresponding to different facets or dimen-sions of the data. One can easily use the hierarchy of predicates learned by our algorithm as an additional facet in the context of a text-exploration application. In future work, we intend to implement this application and perform user experiments to test whether adding this hierarchy facilitates exploration of textual information.
 Appendix A: Derivation of the Probabilistic Objective Function
In this section we provide a full derivation for the probabilistic objective function given in Section 4.2.2. Given two nodes u and v from a set of nodes V , we denote by
I uv = 1 the event that u entails v ,by F uv the feature vector representing the ordered pair ( u , v ), and by F the set of feature vectors over all ordered pairs of nodes, that is,
F =  X  u = v F uv . We wish to learn a set of edges E , such that the posterior probability P ( G is maximized, where G = ( V , E ). We assume that we have a  X  X ocal X  model estimating the edge posterior probability P uv = P ( I uv = 1 | F uv ). Because this model was trained over a balanced training set, the prior for the event that u entails v under the model is uniform: P ( I uv = 1) = P ( I uv = 0) = 1 2 . Using Bayes X  X  rule we get:
P ( I are following Snow, Jurafsky, and Ng [2006]): vectors given the graph. Assumption A.4 states that the features F are generated by a distribution depending only on whether entailment holds for ( u , v ).
Last, Assumption A.5 states that edges are independent and the prior probability of a graph is a product of the prior probabilities of the edges. Using these assumptions and equations A.1 and A.2, we can now express the posterior P ( G because the model was trained over a balanced training set. Generally, however, this is not the case, and thus we introduce an edge prior into the model when formulating the global objective function. Now, we can formulate P(G | F) as a linear function: G = argmax 108 respect to the graph and denote the prior odds ratio by  X  = final formulation described in Section 4.2.2.
 Acknowledgments References 110
