 This paper investigates the temporal cluster hypothesis : in search tasks where time plays an important role, do relevant documents tend to cluster together in time? We explore this question in the context of tweet search and temporal feed-back: starting with an initial set of results from a baseline retrieval model, we estimate the temporal density of relevant documents, which is then used for result reranking. Our contributions lie in a method to characterize this temporal density function using kernel density estimation, with and without human relevance judgments, and an approach to in-tegrating this information into a standard retrieval model. Experiments on TREC datasets confirm that our tempo-ral feedback formulation improves search effectiveness, thus providing support for our hypothesis. Our approach out-performs both a standard baseline and previous temporal retrieval models. Temporal feedback improves over standard lexical feedback (with and without human judgments), illus-trating that temporal relevance signals exist independently of document content.
 Categories and Subject Descriptors : H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval X  Relevance feedback Keywords: temporal clustering; cluster hypothesis; rele-vance feedback
Twitter has become an indispensable communications plat-form through which hundreds of millions of users around the world witness breaking news events. They can participate in the global conversation in real time, 140 characters at a time. To access relevant content in microblogs, people often turn to search. And naturally, time plays an important role in tweet search. We seek to improve access to microblog information by building better search systems.

From a theoretical perspective, this work formulates and explores the temporal cluster hypothesis , stated as follows: in search tasks where time plays an important role, do relevant documents tend to cluster together in time? This parallels the  X  X lassic X  cluster hypothesis [7], which is the observation that relevant documents tend to share similar content (i.e., cluster in document space). In the same way that the ef-fectiveness of content-based relevance feedback techniques affirms the classic cluster hypothesis, the effectiveness of temporal relevance feedback techniques, which we explore in this paper, can be considered evidence supporting the temporal cluster hypothesis.

Our formulation of the tweet search problem follows a user scenario that underpins the recent Microblog evaluations at the Text Retrieval Conference (TREC): at time t , a user expresses an information need in the form of a query Q . The system X  X  task is to return topically-relevant documents (tweets) posted before the query time. Since the tempo-ral distribution of relevant tweets for an information need is usually non-uniform, it is important for retrieval systems to model the temporal characteristics of the query, retrieved documents, and the collection as a whole. This insight, shared by many researchers [8, 3, 5, 4, 2, 20], provides the starting point for our study.

In this paper, we propose a family of techniques for tweet search that integrates temporal signals with  X  X lassic X  lexi-cal (i.e., content-based) approaches. We adopt a feedback framework where temporal features are extracted from R , the initial list of documents retrieved by a standard query-likelihood approach, and then used to rerank R to produce a final ranked list. Let us suppose that each document D i has an associated timestamp T i : the core contribution of our work lies in novel techniques (1) to estimate f ( T | Q ), the temporal density of relevance (i.e., for a particular infor-mation need, where we would expect relevant documents to occur in time), and (2) to integrate this signal with standard lexical features in a log-linear model.

We propose two ways to estimate f , implicit temporal feed-back and explicit temporal feedback . Both methods rely on a simple non-parametric approach to estimating a distribu-tion from data, kernel density estimation (KDE). The im-plicit/explicit distinction is analogous to the difference be-tween pseudo-and true relevance feedback based on docu-ment content. Experiments using the TREC 2011 and 2012 Microblog test collections reveal three main findings: 1. Our temporal feedback approach (implicit as well as 2. Because the effectiveness gain of our approach is addi-3. With only a few human relevance judgments (a negli-
Note that although this work only studies tweet search, there is no prima facie reason why our proposed techniques would not generalize to other domains and retrieval tasks with a strong temporal component (e.g., news search). Sub-stantial interest in social media today justifies a study fo-cused only on tweets, and we leave such generalizations to future work.
Various papers have previously observed that the tem-poral distribution of relevant documents for an information need is rarely uniform [8, 3, 5, 4, 2, 20]. There is general con-sensus in the IR community that effective retrieval systems need to model the temporal characteristics of the query, re-trieved documents, and the collection as a whole. We take this as a non-controversial starting point.

Retrieval models that incorporate temporal evidence have been explored in early work by Li and Croft [12], who pro-posed temporal extensions to language models. This thread has been subsequently extended by others [4, 2], and pro-vides the basis for our formal model. Jones and Diaz [8] explored the temporal profile of queries, classifying queries as atemporal, temporally ambiguous, and temporally un-ambiguous. They show that the distribution of retrieved documents can provide an additional source of evidence to improve rankings. Other attempts at incorporating tempo-ral signals in ranking include [3, 5].

An important difference between the cited papers and our own work lies in the envisioned role of the user. Most previ-ous work relies on automatic methods for inferring temporal signals, such as the distribution of retrieved documents or term statistics time series. Our proposed implicit tempo-ral feedback approach can be viewed as an extension of this line of work, but the inclusion of explicit temporal feedback in our study does imply a different direction. We assume an active role for the end user, which we believe is plausi-ble for a class of sophisticated searchers (e.g., journalists or historians), and empirically demonstrate that even a small amount of user-supplied temporal  X  X ints X  can significantly improve result quality. Thus, the contribution of this paper is both technical (the methods outlined in Section 4) and conceptual (making the role of temporal evidence a type of user-directed relevance feedback).

Beyond search ranking, researchers have explored related problems that benefit from modeling temporal signals. Ex-amples include query log analysis (mining similar web queries by examining query volume over time [28]), behavior pre-diction (by modeling the temporal dynamics of user activ-ities [22]), time-sensitive query auto-completion [24], and real-time query suggestion in the context of Twitter [18].
Finally, we are also aware of researchers who have at-tempted to characterize and quantify temporal change of web pages [1] as well as the  X  X hurn X  of queries on Twit-ter [16]. Although these cited works explore fundamentally different issues than the focus of our study, they demon-strate that temporal modeling is important from a variety of perspectives.
The context for our study is the recent Microblog tracks at TREC [19, 27]. The 2011 and 2012 evaluations used the Tweets2011 corpus, 1 which consists of an approximately 1% sample (after some spam removal) of tweets from January 23, 2011 to February 7, 2011 (inclusive), totaling approx-imately 16 million tweets. Major events that took place within this time frame include the massive democracy demon-strations in Egypt as well as the Super Bowl in the United States. There are 49 topics for TREC 2011 and 60 top-ics for TREC 2012. Each topic consists of a query and an associated timestamp, which indicates when the query was issued. Using a standard pooling strategy, NIST assessors evaluated a total of 114K tweets and assigned one of three judgments to each:  X  X ot relevant X ,  X  X elevant X , and  X  X ighly relevant X . For the purpose of our experiments, we consid-ered both  X  X elevant X  and  X  X ighly relevant X  tweets relevant.
The premise of this work is that the temporal distribution of relevant tweets is not uniform, and that a retrieval model should take this signal into account. More precisely, we seek to estimate f ( T | Q ), the temporal density of relevance X  where in time we would expect relevant documents for a query to show up. To confirm our intuitions, we began by creating simple visualizations that characterize the distri-bution of relevant documents for TREC Microblog topics from 2011 and 2012 [15]. The results of three topics are shown in Figure 1: in each timeline, the query time is an-chored to the right edge; the x -axis shows time prior to the query time, in days. Dots show tweets that were retrieved by participating teams and evaluated by assessors (i.e., the pools): green dots are relevant, red dots are highly relevant. The vertical position of the dots has no meaning; jitter is added only to prevent overlap. The underlying blue bars show the distribution of relevant and highly-relevant tweets as a histogram. Due to space limitations, only three top-ics are shown here, but these timelines are representative of the shapes of the distributions we see across all topics. For topic 29  X  X lobal warming and weather X , relevant tweets are distributed relatively evenly from a temporal perspective; for topic 30  X  X eith Olbermann new job X , with one exception all relevant tweets are very close to the query time; and for topic 37  X  X iffords recovery X , most relevant tweets are clus-tered in two temporal intervals that occur several days prior to the query time.

These visualizations confirm our intuition that the tem-poral distribution of relevant tweets is highly non-uniform X  which means that any retrieval model that does not take into account document timestamps is potentially  X  X issing out X  on an important relevance signal. Furthermore, the tempo-ral distributions appear to be query specific, which means that one-size-fits-all strategies are unlikely to be effective for all topics. For example, we accept that topic 30 would benefit from recency priors, but it is unclear how the same http://twittertools.cc/ the relevant and highly-relevant documents. technique could help topic 37, whose relevant documents are mostly concentrated a week before the query time.

Finally, note that these visualizations are created post hoc , i.e., after the assessment process has completed, so it is not immediately obvious what features are available at query time . In this paper, we propose techniques that exactly address this issue X  X sing no user input and a limited amount of user input.
In this section, we present a formal model for integrating traditional (i.e., lexical) models of relevance with temporal relevance signals X  X pecifically, f ( T | Q ), the temporal density of relevance that we will estimate from an initial list of re-trieved documents. Details about the estimation procedure are described in the next section. By way of comparison, we discuss alternative formulations that attempt to integrate temporal information into standard retrieval models.
We use as a starting point the query-likelihood approach in the language modeling framework [21], where documents are ranked on where P ( Q | D ) is the likelihood that the language model that generated document D would also generate the text of query Q , and P ( D ) is a prior distribution over documents. Recency Priors. One of the simplest way to let time in-fluence the ranking model was given by Li and Croft [12], who proposed a document prior that favors recently pub-lished documents. If T D is the timestamp associated with document D , they propose modeling P ( D ) in Eq. (1) via an exponential distribution: where  X   X  0 is the rate parameter of the exponential distri-bution. We refer to this as a recency prior and refer to runs that use it as  X  X ecency X .
 Independent Evidence. Though previous studies have shown that recency priors increase overall effectiveness across a set of topics, by definition they are query-independent. This could be problematic insofar as the dependencies be-tween time and relevance vary from query to query [8]. Fig-ure 1 clearly shows that this is the case: we would expect recency priors to be effective for topic 30, but such tech-niques are not likely to be effective for information needs represented by topic 37, where the relevant documents are not clustered close to the query time.

Dakka et al. [2] proposed a query-specific way to com-bine lexical and temporal evidence in the language model-ing framework by separating the lexical and temporal signals into two components: W D , the words in the document and T
D , the document X  X  timestamp. This leads to the following derivation: where the last step follows from Eq. (4) if we assume inde-pendence between lexical and temporal information. The re-sulting formula is identical to the standard query-likelihood model, but with the addition of the probability of observing a time T D given the query Q .
 Dakka et al. proposed several ways to estimate P ( T D | Q ). In our experimental analyses (see Section 5) we use one of their methods, the moving window (WIN) approach, as a point of comparison to our own techniques, so we describe it here. With WIN, documents retrieved for Q are allocated among b bins according to their timestamps. For each bin b , we count n ( b t ), the number of retrieved documents in b . Next, bin counts are smoothed by averaging x bins into the past and x bins into the future (where x is the window width). Let n ( b tx ) be the average number of documents in the 2 x bins surrounding b t and b t itself. Finally, bins are arranged in decreasing order of n ( b tx ). The quantity P ( T D | Q ) depends on the bin associated with T D . If T in the n th ordered bin, then P ( T D | Q ) =  X  ( n, X  ) where  X  is an exponential distribution with rate parameter  X  . Log-Linear Temporal Integration. Following Dakka et al., we take the view that two distinct distributions arise during retrieval that we wish to integrate into a single rank-ing. There is P ( R | W D ,Q ), the word-based (i.e., lexical) probability of relevance given Q . We also have P ( R | T which is the probability of relevance to Q given temporal considerations.
Defining the word-based distribution is a well-studied prob-lem. Lafferty and Zhai [10] have argued that P ( R | W D not substantially different from the standard query-likelihood estimate. Accepting that view, we may assume that where, by assuming term independence and a multinomial language model, we have: for the language model  X  D , where c ( Q ) is the number of terms in the query. Using Bayesian updating with a Dirich-let prior parameterized by the real vector  X P ( w | C ), we have the estimator: where P ( w | C ) is the term probability given the language model of the entire corpus, and c ( w,D ) is the count of term w in document D .

Now consider P ( R | T D ,Q ), the probability of the relevance of document D to Q given temporal information. To com-bine the temporal and lexical evidence, we assume a log-linear model. For a parameter  X   X  [0 , 1], we have where Z  X  is a normalization constant. Since Z  X  does not depend on D for ranking, we can ignore it. We estimate  X  from a set of training topics by finding the value that maximizes mean average precision (MAP); see Section 5 for more details.

The resulting log-linear retrieval model is equivalent to ranking documents based on Eq. (10):
A log-linear interpolation (in the sense of Klakow [9]) is appealing because it allows us to combine temporal and lex-ical evidence multiplicatively, as in Eq. (5); time becomes simply another feature in our ranking model. A log-linear approach allows us to express the strength of the temporal evidence explicitly via the interpolation parameter  X  . This is in contrast to most previous work, where the influence of temporal information is controlled indirectly, by param-eterizing a distribution such as an exponential to optimize retrieval metrics. Lexical and temporal evidence may differ inherently in importance, but this should not be controlled via the temporal model itself. The log-linear combination provides the advantage that the relative importance of lexi-cal and temporal evidence is controlled independently from the way we capture the temporal relevance information.
Summarizing, log-linear models provide a flexible means to combine heterogeneous evidence and integrate arbitrary features. As a final remark, a different way to understand our approach is to think of it as a very simple linear learning-to-rank model [17] with only two features.
The theoretical motivation for this work is what we call the temporal cluster hypothesis : in search tasks where time plays an important role (such as tweet search), we hypoth-esize that relevant documents tend to cluster together in time, and that this property can be exploited to improve search effectiveness. Just as van Rijsbergen X  X   X  X lassic X  clus-ter hypothesis suggests that documents relevant to a query Q will form clusters in a term space, we argue that docu-ments relevant to a query will form clusters along a timeline. Analysis shown in Figure 1 suggests that this is indeed the case. Note that although this intuition is implicit in most prior work in temporal IR, to our knowledge we are the first to explicate such a hypothesis as an underlying principle of how time impacts retrieval.

More formally, we define P ( R | T D ,Q ) in Eq. (10) as the distribution of documents relevant to Q over time. That is, we assume that there is a density f Q over the time span of the corpus, such that f Q is large for times where rele-vant documents are likely to appear and small during times where we are unlikely to find relevant documents. Intu-itively, we want to promote documents whose timestamps coincide with large values of f Q , i.e., temporal regions where relevant documents  X  X luster together X . This section focuses on the problem of estimating f Q .
To estimate f Q , we take advantage of kernel density es-timation (KDE), which is a non-parametric method to ap-proximate a density by analyzing data generated from that density. Let { x 1 ,x 2 ,...,x n } be an i.i.d. sample drawn from some distribution with an unknown density f . We are inter-ested in estimating the shape of this function f . Its kernel density estimator is: where K (  X  ) is the kernel X  X  symmetric but not necessarily positive function that integrates to one X  X nd h &gt; 0 is a smoothing parameter called the bandwidth. Though many kernel functions are viable, we use the common Gaussian distribution, such that: where N is the normal density. We chose the Gaussian ker-nel for two reasons. First, as shown below, it gives a ready plug-in value for the optimal bandwidth h . Second, experi-mentally we found that the choice of kernels has almost no effect on the effectiveness of our methods.
 A kernel density estimate is very similar to a histogram. However, KDE requires no binning of data, offloading the bias/variance tradeoff to the choice of bandwidth, which has well-defined methods of selection. One key advantage in using KDE versus histograms for estimating f is KDE X  X  ability to handle weighted observations naturally. If we have {  X  1 , X  2 ,..., X  n } , a vector of non-negative weights on our ob-served X  X  X  such that P  X  i = 1, then is also a proper density:  X  f  X  is similar to  X  f , except that we allocate different weights to the kernels. As noted by Hall and Turlach [6],  X  i can be interpreted as the probability Density Figure 2: Kernel density estimates for the topic MB01. Each curve corresponds to a different band-width selection method. associated with x i . Unless otherwise specified, in this paper, the phrase kernel density estimate refers to Eq. (13). Bandwidth Selection. If we choose a Gaussian kernel, as we do here, then as Silverman [25] has shown, the optimal bandwidth is: where  X   X  is the sample standard deviation. It is important to note that the choice of a kernel function is mainly a matter of convenience, carrying with it no implications of the under-lying parametric forms of the data. We select the Gaussian due to its wide use and its ready definition of an optimal bandwidth. We refer to this bandwidth as RT because it is often called Silverman X  X  R ule of T humb ( RT ).

The notion of the  X  X ptimal X  bandwidth has seen much attention in the statistical literature. Many state-of-the-art bandwidth selection approaches are based on some type of cross validation. For instance, a common approach is to minimize the mean integrated squared error (MISE): where E is the expectation. It can be shown that an asymp-totically correct approximation MISE can be calculated an-alytically, yielding a simple algorithm for bandwidth selec-tion that can be optimized via c ross v alidation [26]; we refer to this method as CV . Related methods based on penalized log-likelihood have been proposed and evaluated by Sheather and Jones [23], which we refer to as SJ .

Figure 2 shows different kernel density estimates for doc-ument timestamps of relevant documents for TREC Mi-croblog topic MB01. A mode at day 14 is clearly visible under all bandwidth selection methods. Overall, the band-widths given by the three methods yield nearly identical den-sities. We found that in general, the bandwidth selection method X  X mong those mentioned here X  X ade little differ-ence in effectiveness. Because of its wide adoption in current statistical practice, for the remainder of this paper, we rely on the SJ estimate.
KDE, via Eq. (13), presents a simple framework for weight-ing observations (document timestamps) during density es-timation. The intuition behind the weight  X  i for document D i is that this quantity corresponds to our prior belief that the corresponding timestamp T i was truly generated by f Q However, this approach leaves the matter of defining these weights unspecified. In this section we propose four alterna-tive weighting schemes: Uniform Weights. The simplest approach to weighting for density estimation is simply to give all documents in the initial retrieval equal weight during estimation. Thus for all D i  X  R , we have  X  u i = 1 | R | , where | R | is the number of doc-uments retrieved. We call this approach uniform weighting . Score-Based Weights. The simplicity of uniform weight-ing ignores the information that we have from the score of each document D i with respect to its lexical similarity to Q , expressed by Eq. (8). Thus, we define a second weighting method, score-based weights , where document weights are proportional to their language model-derived probabilities of relevance: Rank-Based Weights. A reasonable objection to score-based weights is their reliance on lexical similarity, which we are ostensibly measuring in tandem with temporal probabil-ities. In other words,  X  s i is tied to the retrieval scores of the initial run, while in theory, f Q ( T i ) should be independent of any retrieval model. To remove this coupling, we can make an assumption common in lexical feedback settings. Given an initial lexical ranking R of documents against Q , we can assume that documents near the front of R have a higher probability of relevance than documents ranked lower in R . While traditional pseudo-relevance feedback requires us to choose a hard cutoff of putatively relevant documents k in the context of weighting, we can be less restrictive. Thus, we define rank-based weights via an exponential distribution: where  X  &gt; 0 is the rate parameter of the exponential density and r i is the rank of document D i in R .

Though we could leave  X  as a tuneable parameter, a sim-pler way to approach rank-based weights is to use the max-imum likelihood estimate. If R contains n documents, the MLE of  X  is simply 1  X  r , where  X  r is the mean of the ranks 1 , 2 ,...,n . This is the approach we use in Eq. (17) and in our implementation of rank-based weights.
 True Feedback-Based Weights. All of the weighting methods we have discussed so far assume no knowledge of which documents in R are actually relevant to Q . But if some sort of user interaction gives us true (i.e., human) rel-evance judgments, it makes sense that we take advantage of the corresponding timestamps to influence  X  f Q .

If we know that document D i is relevant (e.g., based on user input), it makes sense to give that document ex-tra weight. For k relevant documents, D r 1 ,D r 2 ,...,D it is sensible that they be given the same weight (absent graded relevance judgments). Thus, given true relevance Table 1: Tuning parameters for the temporal re-trieval models. Separate values for each parameter were estimated from training topics for runs with no lexical relevance feedback and runs with lexical feedback.
 Method Parameter Recency Rate parameter of the exponential prior
WIN Bin size; window width; rate parameter for
KDE log-linear mixing parameter from Eq. (10) judgments, we define the true feedback-based weights with respect to the score-based approach as follows: The final weight z s i is arrived at after renormalization: Note that true feedback-based weights can also be computed with respect to document ranks, per Eq. (17), to produce z in a similar manner.

To eliminate the need to tune one more parameter, in our experiments we simply set c = 1 (arbitrarily). Because the value one is almost always much larger than P ( Q | D i ), this scheme amplifies the influence of relevant documents.
Experiments were performed on the Tweets2011 corpus using test collections from the Microblog tracks at TREC 2011 and 2012 (described in Section 2.2). Relevance judg-ments for the test collections were made on a 3-point scale ( X  X ot relevant X ,  X  X elevant X ,  X  X ighly relevant X ), but in this work we ignored the different degrees of relevance and use both higher grades as  X  X elevant X .

During collection preparation, we eliminated all retweets since they are by definition not relevant according to the as-sessment guidelines. No stemming was used, and no stoplist was applied at index time. However, a Twitter-specific sto-plist was used when estimating relevance models to reduce the dominance of common terms such as  X  X T X  and  X  X ttp X  during query expansion. All three temporal retrieval mod-els required parameter tuning, which is described in Table 1. Parameters were trained with respect to mean average pre-cision on even-numbered topics (54 total); odd-numbered topics were used for testing (55 total). We report mean average precision (MAP) and precision at rank 30, which was the primary metric used in the TREC 2011 Microblog evaluation. In our experiments, the statistical significance of effectiveness differences were determined using one-sided paired t -tests; results are reported using the symbols shown in Table 2.

All experiments were performed with the Indri search en-gine. 2 The baseline condition (QL) uses the standard query likelihood approach with Dirichlet smoothing (  X  = 2500), http://www.lemurproject.org/indri/ Table 2: Symbols indicating statistically significant change for data reporting.

Symbol Description Table 3: Effectiveness measures on held-out test data (odd-numbered topics). Results show mean av-erage precision (MAP) and precision at 30 (P30).
 retrieving no more than 1000 results per topic. All tem-poral retrieval models were implemented by reranking the originally returned documents. This means that they can be applied even if we do not have direct access to the entire document collection, as is the case with the  X  X valuation as a service X  approach implemented in the TREC 2013 Microblog track [14, 13].
Table 3 compares the effectiveness of our KDE approach using the three different weighting schemes described in Sec-tion 4.2. Our techniques were compared against the follow-ing methods:
The results suggest that our KDE approach improves re-trieval effectiveness significantly over a purely lexical base-line (QL) and at least one of the previously published tem-poral models (WIN). However, it unclear whether any of the weighting schemes for KDE systematically yields better results than the others. As we might expect, non-uniform weights appear to boost effectiveness over an estimate that does not take advantage of lexical evidence. However, it is unclear whether score-based or rank-based weighting is more effective. Lacking strong evidence one way or the other, the remainder of this paper relies on the score-based weights, as this approach eliminates an extra free parameter.
Figure 3 plots per-topic differences in average precision obtained by each temporally-informed method versus the baseline QL run (data in the KDE panel uses score-based weights). The magnitude of change is query-dependent, sug-gesting that query-specific analyses are important. Consider topic 37  X  X iffords recovery X , which we examined in the vi-sualization in Figure 1: effectiveness decreased with recency priors, which is expected since our visualization does not show any relevant tweets near the query time. In contrast, both the WIN and our KDE approach increase effectiveness, although our approach beats the WIN approach.

On the other hand, for topic 103  X  X ea Party Caucus X , all temporally-informed techniques are less effective than the QL baseline, with the kernel method incurring the biggest decline. This case is interesting insofar as relevance for the query could have both temporal and non-temporal aspects. The Tea Party political movement has an ongoing presence on the American political stage, but its influence is punctu-ated by time-bound news stories. On the whole, however, improvements from our KDE technique over the QL baseline are consistently higher than the other methods (Table 3). Aside from topic 103, even when KDE hurts effectiveness, its effect is usually smaller than either recency priors or WIN.
Given the results shown in Table 3, a natural question is: what is the upper bound on improvements obtainable by using f Q , the density of actual relevant documents? To an-swer this question, we created an oracle condition, where f was estimated using all known relevant documents in the test collection, according to the true feedback-based weights method discussed in Section 4.2 (using the score-based ap-proach z s i ). As discussed, we arbitrarily set c = 1 in Eq. (19) without any tuning. Note that this experimental condition still relies on KDE, so it caps the effectiveness upper bound for this particular estimation technique.
 Results of the oracle condition are shown in the last row of Table 3. As we would expect, the oracle run outperforms all Figure 4: Effectiveness of KDE given an increasing number of user-supplied relevance judgments. others at statistically significant levels. But the gulf between the oracle and more realistic runs is smaller than the table suggests. Figure 4 compares the results of our KDE-based temporal feedback to the oracle condition. In the figure, hor-izontal lines indicate the effectiveness of various techniques in terms of MAP: baseline query likelihood is shown in red, KDE with score-based weights in blue, and the oracle in green (i.e., estimating f Q with all human relevance judg-ments). With the black dots, we simulate the effectiveness of an interactive retrieval system whereby users can provide relevance judgments. We emphasize that the x -axis denotes total judgments , not the number of positive judgments . This is important because, for many queries, the top k (say, ten) retrieved documents contained no relevant examples. For these queries, since we do not take advantage of negative judgments, the average precision in the feedback condition is unchanged from simple,  X  X nsupervised X  KDE. Note that ef-fectiveness is computed over all topics, even for those where the addition of human relevance judgments made no impact.
Nevertheless, a clear trend emerges from Figure 4. Not surprisingly, obtaining more relevance judgments increases the effectiveness of our density estimates. But what is sur-prising is the rate of improvement: sixteen judgments al-low us to reach roughly oracle effectiveness. Furthermore, a modest number of judgments (e.g., five) advances the KDE approach almost halfway to oracle effectiveness.
Results from the previous section suggest that our for-mulation of temporal feedback based on kernel density es-timation is effective, compared to both a query-likelihood baseline and two other temporal retrieval models. How-ever, these results do not answer a related question: is the improvement that we see due to a signal that is different from information gleaned from document content? Perhaps traditional (lexical) relevance feedback implicitly captures whatever signal we obtain from temporal feedback. In other words, is the temporal cluster hypothesis distinct from the classic cluster hypothesis?
In this section, we experimentally show that the answer is yes . Effectiveness improvements from temporal feedback are additive with improvements from lexical feedback, which shows that the temporal signal we are exploiting exists in-dependently of document content.

In our experiments, we supplemented a standard lexical feedback method with different temporal retrieval models, in the context of pseudo-relevance feedback and simulated  X  X rue X  relevance feedback. In both cases, the lexical feedback method is Lavrenko and Croft X  X  relevance model RM3 [11]: For clarity, we review the definition of relevance models: where q i is the i th query term in a query that is n words long. The relevance model P ( w | R Q ) for Q is simply a weighted average of the terms in all documents, where the weights are the query likelihood scores. In the RM3 variant, the quantity in Eq. (20) is interpolated with the observed query according to a mixing parameter  X  . We report results with  X  = 0 . 5, the Indri default. 3
When using feedback with temporal information, we fol-lowed this sequence: 1. Retrieve initial set of documents. 2. Apply the temporal retrieval model (Recency, WIN, or 3. From the top k reranked documents, estimate feed-
Training  X  led to inter-system comparisons very similar to those reported here. Thus, we chose to fix  X  at the Indri default during experimentation.
 Table 4: Retrieval effectiveness in the context of lexical pseudo-relevance feedback, with RM3 as the baseline. Each temporal retrieval model augments lexical feedback via re-ranking both before and after estimating relevance models.
 4. Run retrieval with feedback model. 5. Rerank final results using the same temporal model. The results of integrating temporal retrieval models with RM3 are shown in Table 4. We see that neither recency pri-ors nor the WIN variant yields much in addition to simply performing pseudo-relevance feedback. Our KDE approach, on the other hand, is significantly more effective than RM3. This result is reinforced by Figure 5, where we plot per-topic average precision differences between each temporal model and RM3. Aside from the obvious differences in magnitude of change obtained by our KDE method versus Recency or WIN, it is also notable that our KDE method X  X  effectiveness with pseudo-relevance feedback is  X  X afer X  than KDE rerank-ing without pseudo-relevance feedback, as in Figure 3. In each case, although the number of topics that our KDE method either helped or hurt is the same (32 helped and 21 hurt), the magnitude of decline in cases where KDE hurt is smaller when using lexical feedback.

As a final summative evaluation and to deepen our un-derstanding of the interaction between temporal and lexical signals during feedback, we tested effectiveness in the pres-ence of true (i.e., human) relevance judgments, as opposed to the pseudo-relevance feedback methods described above. Results in Figure 4 suggest that temporal feedback by KDE benefits immensely from human relevance judgments, but those results were on a query-likelihood baseline without lexical feedback. We would like to examine the effect of introducing RM3 into the experimental setup.

Methodologically, the experiments with true feedback mir-rored the pseudo-relevance feedback runs above, except that relevance information was injected into a run for documents that had been judged by NIST assessors as relevant among the top five documents retrieved (these judgments are in-troduced in step two and used in all subsequent steps). In other words, this true feedback condition modeled the case where the user judges the first five tweets retrieved by a model. Relevance models were estimated only from explic-itly judged documents. If no relevant tweets appeared in a run X  X  top five results, no lexical feedback was performed.
Table 5 summarizes the true feedback results in terms of residual effectiveness measures, i.e., each measure was cal-culated after removing the documents ranked in the top five during the initial run since the (simulated) user had already  X  X ead X  them. In this case, we see that both Recency and WIN do not significantly improve over RM3, whereas the gains exhibited by our KDE approach are statistically sig-nificant. Note that in all cases, the effectiveness metrics were computed over all topics, even those that did not con-Table 5: Results obtained using five human rele-vance judgments. Measures are residuals (omitting results seen during feedback).
 tain a relevant tweet in the top five initial results. This is a conservative evaluation approach and quantifies effective-ness improvements over a broad range of topics, not just those which are particularly beneficial for our technique. Implications of the Temporal Cluster Hypothesis. The temporal cluster hypothesis frames temporal retrieval as a matter of estimating proximity along a single dimension X  in this case, via the density of f ( T | Q ), the temporal density of relevance. This amounts to treating time as simply an-other feature in a ranking method. This approach is simple and has intuitive appeal, not to mention many options for practical implementation in the learning to rank framework.
Beyond what we have explored here, there are other ways in which a notion of proximity could inform an assessment of similarity as a feature for ranking. For example, we might in-form a query classifier of the (dis-)similarity of f Q from some null distribution f 0 (such as the distribution of the entire collection). Another way to think about this is as follows: a broader interpretation of the classic cluster hypothesis is that documents with similar content profiles are likely rele-vant to the same information need. We could replace  X  X on-tent profile X  with  X  X emporal profile X  as a more general formu-lation of the temporal cluster hypothesis X  X imeline proxim-ity is merely one example of a temporal profile, but there are other temporal signals that could be valuable for ranking. Model Generalizations. Although our work is couched in the broader context of the temporal cluster hypothesis, in this paper we focused on improving the effectiveness of tweet search as a concrete application. This choice was motivated by the temporal nature of tweet search and the substantial interest in social media by researchers today.

However, there is no reason to think that the methods we proposed would not generalize to other timestamped docu-ment collections. Thus, an interesting avenue of future work is to extend our analysis to other domains such as collections of news articles. In particular, two factors make this exten-sion appealing: 1. Diverse temporal nature of queries 2. Diversity of collection timespans.
 With respect to the first point, relevance in tweet search is, almost by definition, temporally conditioned. Moreover, the semantics of this temporality often lends itself to a simple promotion of recent information. But in other collections, it is likely that we would find test queries that have quite different temporal dynamics, including topics where time is not a helpful signal. A core problem that remains to be ad-dressed (not only in our work, but in the literature at large) is how best to approach the varied nature of temporality that bears on relevance in heterogeneous retrieval settings.
The second point speaks to the fact that the Tweets2011 corpus spans a relatively small window of time X  X oughly two weeks. Many of the TREC news collections, on the other hand, span months or years. How temporally-informed re-trieval methods generalize to longer time horizons is an im-portant, open question.
On both of these counts, we suspect that the flexibility of the kernel-based approach will be an asset. Since the bandwidth selection methods for KDE depend on observable features of the data (i.e., timestamps) themselves, it seems likely that longer windows will not pose a problem for our general approach. This is in contrast to the use of recency priors, where the exponential rate parameter and the unit used to represent time are tightly coupled. Likewise, band-width selection allows temporal influence to shrink in the face of evidence that the density f Q lacks any pronounced modes. In theory this should allow KDE-informed ranking to scale its influence when faced with non-temporal queries.
For much of the history of information retrieval, research-ers have treated queries and document collections as mostly static. Recently, however, there is a growing recognition that time plays an important role in many aspects of search X  X n this paper, we have explored temporal feedback using ker-nel density estimation for tweet search and demonstrated its effectiveness under a variety of experimental conditions. Although the specific application domain is independently interesting, we hope that our formulation of the temporal cluster hypothesis will be a more lasting contribution, in providing a general principle for future explorations in tem-poral retrieval.
This work was supported in part by the U.S. National Sci-ence Foundation under Grant Nos. 1144034, 1217279, and 1218043. Any opinions, findings, conclusions, or recommen-dations expressed are those of the authors and do not nec-essarily reflect the views of the sponsor. Additional support was provided by the Dutch National Institute for Mathe-matics and Computer Science (CWI). [1] E. Adar, J. Teevan, S. T. Dumais, and J. L. Elsas. The [2] W. Dakka, L. Gravano, and P. G. Ipeirotis. Answering [3] M. Efron. Linear time series models for term [4] M. Efron and G. Golovchinsky. Estimation methods [5] J. L. Elsas and S. T. Dumais. Leveraging temporal [6] P. Hall and B. A. Turlach. Reducing bias in curve [7] N. Jardine and C. J. van Rijsbergen. The use of [8] R. Jones and F. Diaz. Temporal profiles of queries. [9] D. Klakow. Log-linear interpolation of language [10] J. Lafferty and C. Zhai. Probabilistic relevance models [11] V. Lavrenko and W. B. Croft. Relevance based [12] X. Li and W. B. Croft. Time-based language models. [13] J. Lin and M. Efron. Evaluation as a service for [14] J. Lin and M. Efron. Overview of the TREC-2013 [15] J. Lin and M. Efron. Temporal relevance profiles for [16] J. Lin and G. Mishne. A study of  X  X hurn X  in tweets [17] D. Metzler and W. B. Croft. Linear feature-based [18] G. Mishne, J. Dalton, Z. Li, A. Sharma, and J. Lin. [19] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. [20] M.-H. Peetz and M. de Rijke. Cognitive temporal [21] J. M. Ponte and W. Croft. A language modeling [22] K. Radinsky, K. Svore, S. Dumais, J. Teevan, [23] S. J. Sheather and M. C. Jones. A reliable data-based [24] M. Shokouhi and K. Radinsky. Time-sensitive query [25] B. W. Silverman. Density Estimation for Statistics and [26] J. S. Simonoff. Smoothing Methods in Statistics . [27] I. Soboroff, D. McCullough, J. Lin, C. Macdonald, [28] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos.
