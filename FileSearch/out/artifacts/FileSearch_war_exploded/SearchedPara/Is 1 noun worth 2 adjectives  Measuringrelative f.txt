 1. Introduction
Information systems that order documents rarely incorporate all the information that is available about the documents during the sorting process. When implementing information systems, decisions are made about which linguistic components and media characteristics should be incorporated into the system and which may be omitted. The purpose of this work is to develop a tool that can be used to determine the relative utility of different linguistic and non-linguistic components when ordering media, allowing system designers and managers to select for inclusion those features that have the greatest expected effectiveness. For example, on the average, is a single noun worth the same as two adjectives, or is a hyperlink worth the same as a 3 term index phrase when documents are ordered. We refer here to the information that the ordering systems need to most efficiently sort documents, given quality, time, space, and other constraints; we are not referring to the information that the user wishes to receive.

Features that may be used for ordering documents may be categorized as linguistic or non-linguistic. State-ments in a natural language are composed of a variety of features, on several different levels. While it may be desirable to understand language as a whole ( Aronoff &amp; Fudeman, 2005, p. 11 ), processing of natural language at the present time requires the processing of text as a set of parts, since scholars remain unable to fully under-stand many aspects of natural language. Understanding the relative merits of these different features of natural language is essential to deciding which features to address using computational linguistics when developing document ordering systems. A measure is provided that easily allows us to determine the relative merits of incorporating specific linguistic features into natural language based systems.

Feature topicality in systems using media ordering can exist due to a specific meaning attached to a single term or small number of terms functioning as a phrase ( Church &amp; Hanks, 1990; Fagan, 1989; Schlobinski &amp;
Schutze-Coburn, 1992 ). Similarly, larger grammatical units, such as sentences or paragraphs, or entire docu-ments, may represent topic and topical-comments ( Davison, 1984; Greisdorf &amp; O X  X onnor, 2003; Jacobs, 2001;
Shi, 2000 ). The topic may also be inferred from individual terms, phrases, or larger text fragments, which pro-vide a context for inferring topicality. Individual terms have different meanings or senses associated with the various parts of the term, e.g., psychiatry :( psyche = mind, iatreia = healing), or the common root run for run, runs, running ,and ran . Non-linguistic features that may be used in document ordering include characteristics such as citations and hyperlinks, the presence of specific images on a book X  X  cover, or whether an index occurs in a book. What is the relative degree to which these linguistic and non-linguistic characteristics represent top-icality and predict the utility of a document to a system user? Term roots may carry one or more meanings or topics, and the addition of contextual or supporting information, such as suffixes, part-of-speech tags, and larger contexts can contribute to the topicality, therefore improving document ordering ( Bossong, 1989; Clement &amp; Sharp, 2003; Losee, 2001 )
How does one measure how many of one feature or type of feature is equivalent in ordering power to another chosen feature or type of feature? The Relative Feature Utility may be used to empirically analyze the ordering effects of term stemming, the length of natural language phrases, the effect of using different part-of-speech labels, and various information retrieval or filtering assumptions. 2. Ordering performance as a utility measure
The performance of systems incorporating characteristics may be studied by measuring how documents are ordered given different sets of characteristics. How does the ordering performance vary when incorporating just nouns or just adjectives? How would one word queries perform when they are only nouns or only adjec-tives? Determining the relative performance of two different characteristics used in ordering allows one to make decisions about which types of characteristics should be incorporated into a specific system. While ordering performance measures take many forms ( Losee, 2000 ), e.g. precision, recall, average search length, or any of a number of measures of ordering performance, a linear measure that can be directly interpreted as an occurrence of characteristic X produces performance at the same level as n occurrences of characteristic
Y can be particularly valuable when making decisions about whether characteristic X or characteristic Y should be used, along with their associated costs and benefits.

Precision is the probability that a document in the set of retrieved documents is relevant and is used as an effectiveness measure. While precision may be computed and reported over a range of recall levels, where recall is defined as the percent of relevant documents that have been retrieved, precision is commonly used as a mea-sure of the quality of the retrieved set at a specific point in the ordered list of documents retrieved from a search engine. Search engines that produce initial output screens of 10 documents might have the precision computed after 10 documents have been retrieved, denoted as P
The goal of this work is different than the goal of those who developed measures such as precision and recall. This work presents a means by which one can make a clear statement that one characteristic, for exam-ple, is two or five times as useful as another feature. The Relative Feature Utility is computed based on an analytic model of ordering performance that begins with the Average Search Length (ASL). The ASL is the average position of relevant documents in an ordered list of documents, with the positions for N docu-ments ranging from 1 to N , with an ASL of 1 being a single relevant document at the front of the ordered list of documents, and an ASL of N being the worst, with a single relevant document occurring at the end of the ordered list of documents. The discussion below is for single term queries. For N documents, each doc-ument with or without the single query term, where the Q value represents the probability that the ordering function is optimal, N is the number of docu-ments in the database, and A is a normed average search length which scales from 0 to 1, with 0 being best-case performance and 1 being worst-case performance. The performance measure A is the probability that a randomly selected document is located before the average position of relevant documents and is computed as
A  X  X  1 p  X  t  X  = 2, where p is the probability that a relevant document has the feature in question and t is the unconditional probability that a document has the feature in question. The non-approximating forms of this equation may be used for small document sets ( Losee, 1998 ). The  X  Q A  X  X  1 Q  X  X  1 A  X  component has a value between 0 and 1, and the value is converted to the Average Search Length by Eq. (1) . This average nor-malized position of a relevant document after ordering occurs is referred to as the Normalized Average Search Length (NASL):
The core of this equation may be computed by first measuring the ASL empirically and then algebraically solving for other variables, such as Q and A . The availability of the probabilistic interpretation of A allows us to compute the ASL based upon parameters, rather than just upon experimental data, and to develop ana-lytic models of ordering performance ( Losee, 1998 ).

To illustrate the computation of NASL, consider the situation where Q is understood to be 1 (less than perfect rankings are discussed in Losee (1998) &amp; Losee &amp; Paris (1999) ) and A is computed from the fact that half of the documents have the query term and three quarters of the relevant documents have the query term.
In this case A  X  X  1 3 = 4  X  1 = 2  X  = 2  X  3 = 8. NASL is then computed as 1  X  3/8 + (1 1)(1 3/8) = 3/8. This is clearly better than a random NASL of 1/2 but is only 1/4 of the distance from random to perfection (NASL = 0). ASL is similarly computed, with the primary difference being the multiplication by the number of documents. In the case of 1000 documents, for example, ASL would be 1000  X  .375 = 375, that is, the expected position of a relevant document would be at the 375th ordered document. 3. Relationships between normalized ASL measures: two of these perform as well as one of those
Can the discussion of the relative merits of a linguistic characteristic be simplified so that one could say, for example, that two part-of-speech tags are  X  X  X orth X  X  the same as a single term suffix or a single hyperlink, or that two adjectives are worth as much as a single noun when trying to identify topicality and topic-based relevance?
One can relate probabilistic distributions through use of the Kullback X  X eibler information gain measure ( Kullback, 1959; Losee, 1990 ), which examines the similarity between two probabilistic distributions. Log-odds discrimination measures ( Bishop, 1995; Duda, Hart, &amp; Stork, 2001 ) may also be used to measure the number of bits of information that are added by using features in a discrimination task. The ordering based
Relative Feature Utility measure is somewhat different than these, given its focus on ordering, its linearity, and its ease of interpretation.

The probability that a document in the top half of the ordered list of documents is ranked ahead of the average position of the relevant documents is denoted as W . This can be obtained by multiplying A by 2 and then truncating anything over 1, thus W  X  min  X  2 A ; 1  X  . When the query term is a positive discriminator, A will always be less than 1/2 and W  X  2 A .

Given probability W , one may compute the number of independent occurrences of type 1 that produce the same performance as the number of independent occurrences of type 2, using W
Feature Utility (RFU), is the multiplier representing the number of occurrences of type 2 that produce the same ordering performance as a type 1 occurrence, e.g., how many adjectives produce the same ordering per-formance (e.g., A ) as a single noun? One can solve for M as
Consider a situation where a test value has A 1  X  .35 (thus W
A 2  X  .45 (thus W 2 = .9). The Relative Feature Utility is computed as M  X  log  X  .7  X  = log  X  .9  X  X  3 : 39. This may be interpreted as 3.39 occurrences of a type 2 event producing the same level of ordering performance as an occurrence of a type 1 event. This may be used to compare anything that contributes to ordering, such as term roots, term stems, part-of-speech tags, citations, and annotations, as well as the characteristics of phrases, documents, or entire libraries or distributed systems. The RFU may be used to study relationships between feature characteristics, such as the number of subject headings assigned to documents compared to the length and information content of individual subject headings ( Losee, 2004 ).

Using the Relative Feature Utility allows one to make claims that 3 of feature x produces the same perfor-mance as 1 occurrence of feature y . System designers may make choices given this knowledge, considering how expensive in terms of cost, system speed, and storage, of whether to use features x or y or both. Knowing that the precision or recall or average search length is doubled or halved when using a specific type of feature pro-vides ordinal information that performance has improved or decreased, but the magnitude of change may be difficult to use in practical situations. However, given the knowledge that 3 of something produces the same performance as 2 of something else provides one with specific information that can be used in calculating trade-offs and decision making. For example, one might decide to use something that performs twice as well if it took less than twice the processing time. While some measures can be shown to be metrics under some circumstances ( Shaw, 1986 ), the work here produces a linear measure (i.e., ratio data) with all the accompa-nying simplicity missing from non-linear measures.

Using this technique, both types and tokens , general categories and specific instances, can be analyzed. A claim could be made that using a specific occurrence of a term increases ordering performance more than 3 occurrences of a different term type or of 3 specific terms x , y ,and z . Similarly, one may choose to work with entire classes, such as all nouns or all hyperlinks.

When using M as a measure of Relative Feature Utility, one may be faced with determining when a result should be treated as significant. While it has been pointed out that many information retrieval applications do not meet the assumptions made by many commonly used statistical significance tests ( Van Rijsbergen, 1979 ), the significance of studies has been studied through the use of the t -test, sign, and Wilcoxon tests ( Hull, 1993;
Sanderson &amp; Zobel, 2005 ), as well as through the use of the Kolmogorov Smirnov test ( Moon, 1993 ). Other studies have suggested that a better focus is not only on whether two results are significantly different, but instead one should consider whether the results are significantly better than one would expect through random ordering ( Shaw, Burgin, &amp; Howell, 1997b ). More ad hoc rules have suggested that for precision, a difference of 5% may be considered significant and a difference of 10% in precision may be considered very significant ( Sparck Jones &amp; Bates, 1977 ). Following this model, one may suggest that improvements in M of 5% (for a feature being used as compared to not being used) with a large database should be considered significant, that is, an M value of 1.05 should be considered significant and important, although this is rather arbitrary and is based primarily on experience with the data described in this paper. We note that for the purposes of this study, which attempts to describe the Relative Feature Utility and illustrate its application, the numbers provided here are meant to show roughly the range of numbers that might be expected using this form of analysis. 4. The Nyltiac system and databases
Tests were conducted to help us understand this Relative Feature Utility model and to allow us to make empirical observations. The Nyltiac system ( http://Nyltiac.com ), a retrieval system on the web that runs in a user X  X  browser software, was used for most of the document ordering tasks and data manipulation tasks.
Nyltiac has been used as a classroom instruction tool for several semesters and it is assumed that most of the  X  X  X ugs X  X  in major routines have been eliminated. The research version contains routines that are not avail-able to students, while the student version that is available on the web site only contains routines that the author routinely uses in an introductory Information Retrieval course.

There are a number of ranking algorithms that could be used in ordering terms or documents. Coordina-tion Level Matching (CLM) ranks documents by the number of term types in a document that are also in the query. For this study, the Coordination Level Matching-Frequency (CLMF) method is used, which is similar to the popular TF-IDF measure that is widely used by search engine term weightings except that CLMF treats all terms as having the same weight, avoiding the introduction of bias toward or against certain terms or clas-ses of terms based on their relative frequency of occurrence. Terms, phrases, and documents are assigned a weight computed from the term frequency for terms that occur both in the document surrogate and in the query. In the case of single term queries, the document orderings consistent with CLMF and the TF-IDF rankings are identical.

Given a query { x , y } and two documents A :{ v , x , x , y } and B :{ v , y }, we find that the CLM value for A is 2 (1  X  X  X oint X  X  each for the presence of x and y ) and for B the CLM value is 1 (for y ). As an example of CLMF, using the query and documents above, the weights for A become 3 (for x , x , and y ) and for B the CLMF remains 1 (for y ).

In many circumstances, one may wish to analyze the effects of using a single term or a variant of a term. For many of the analyses below, the queries are broken up into individual terms or into small sets of one, two, three, or four sequential terms extracted from the original queries. In these cases, the relevance judgments for the query-document-relevance triples are derived from the full queries. When using these triples, where the queries are each single terms extracted from the original natural language text query, these queries are referred to as single term queries . There will be as many single term queries as there were original terms in all the original natural language queries. A query with three terms with n documents would originally have n triples; if the three term query were broken up into individual single term queries, each of which would have its own triples, 3 n triples would exist, n from each single term query. If a database had an average of 10 terms per query, 82 documents, and 35 natural language queries, using this single term model would produce 35  X  10 = 350 single term queries, and this, times 82 triples for each query, would produce 350  X  82 = 28,700 single term triples.

Three databases were used for this study. The ADI463 database is the smallest database, with 463 single term queries and 82 document abstracts, representing papers presented at an American Documentation Insti-tute Conference ( Salton &amp; Lesk, 1968 ). The database from which this is derived, the ADI database, is an older database and has been often used in retrieval experiments. The ADI463 database has 37,996 query-document relevance judgments (triples). The second database, the MED559 database, contains 559 single term queries and 1033 documents, and is derived from the MED1033 database ( Kwok, 1990; Shaw, Burgin, &amp; Howell, 1997a ). The MED559 database has 577,477 query-document relevance judgments (triples). The third data-base, the CF683 database, is the largest, with 683 single term queries, and is derived from the first 50 queries from the CF database. All 1239 available documents are used ( Shaw, Wood, Wood, &amp; Tibbo, 1991 ). The
CF683 database has 846,237 query-document relevance judgments (triples). All these test databases have rele-vance judgments, normally based on a notion of topical relevance ( Greisdorf &amp; O X  X onnor, 2003 ), providing binary relevance relationships between each document and query ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ).
We believe that having tens of thousands of query-document relevance judgments for one database and hundreds of thousands of query-document relevance judgments for the others is adequate for illustrating sit-uations in which the Relative Feature Utility model may be applied. For example, having several hundred sin-gle term queries and hundreds of thousands of query-relevance judgments concerning nouns provides a useful snapshot of the underlying parameters of nouns in natural language. If this study were examining far less com-mon linguistic expressions, then using larger databases would likely be necessary, but because of the large number of nouns, adjectives, and verbs that exist in each of the databases, we feel that general trends can be observed that are roughly generalizable. If one wished to gain a much more precise picture of the param-eters of natural language, numerous databases representing the different contexts and sublanguages being used among humans (e.g., children X  X  speech, children X  X  writing, parents X  speech to children, parents X  speech to spouses, etc.) would probably be more valuable than using large single-domain databases, such as some of the popular large databases that are based on documents licensed from either a single or a very small number of organizations.

Using these databases also allows us to suggest possible differences between different scientific disciplines and their terminology ( Latour &amp; Woolgar, 1986; Lodahl &amp; Gordon, 1972; Losee, 1995; Pierce, 1992 ). The
ADI463 database may be understood as a social science database, with more ambiguous and less precise ter-minology than one would find in the harder sciences, as exemplified by the MED559 and CF683 databases.
The MED559 database has an additional difference in that the original natural language  X  X  X ueries X  X  are state-ments of topic, phrases indicating what is desired. This is different than the true English language questions that serve as the basis for the single term queries in the CF683 and ADI463 databases. 4.1. Linguistic processing
Suffixes were removed on some occasions from query and document text with the Porter stemmer ( Porter, 1980 ). A widely used stemmer, the Porter algorithm has been used to remove suffixes in a number of lan-guages. Stemmers may be used to find the basic morphological units representing concepts in natural language ( Aronoff &amp; Fudeman, 2005; Fox &amp; Fox, 2002; Kurz &amp; Stoffel, 2002 ). The use of the Porter stemmer is meant as an example of a popular stemmer, and is not meant to imply that this is the best stemmer for all situations.
Stemming algorithms bring together similar forms of terms ( Cleverdon, 1967; Salton &amp; Lesk, 1968 ) and have been analyzed using a number of techniques. Most frequently a retrieval performance measure is used to evaluate the relative performance of different stemming methods ( Hull, 1996 ). This is somewhat similar to the measures of document ordering as an indicator of algorithm and feature quality. Other methods may measure the degree of stemmed term assignment to manually produced groups or meanings ( Paice, 1996; Savoy, 1993 ), considering qualitatively cases where too much of the core term is removed, as well as cases where too little of the suffix is removed.
 Terms were part-of-speech (POS) tagged through use of the Brill Part-of-Speech tagger ( Brill, 1994 ).
Although it does not provide perfect tagging, it is an established and well-understood tagger. The parts-of-speech produced by the Brill Tagger are consistent with the University of Pennsylvania Treebank. Examples of the part-of-speech tags (compared to other tag sets) are provided in ( Manning &amp; Schutze, 1999, p. 141 X 2 ).
Tagger performance may be measured by direct comparison with accurate manually assigned POS tags and by the algorithm X  X  relative speed ( Murata, Ma, &amp; Isahara, 2002; Padro &amp; Marquez, 1998 ), as well as by using retrieval performance.
 The numbers of unique terms in queries varies from text with part-of-speech tags to text without such tags.
For example, there are 204 unique query tokens in the ADI463 queries when untagged, but when POS tags are added, there are 231 unique query tokens. Terms such as automated and coding occur only once in the list of unique untagged tokens but can each be part-of-speech tagged in two different ways and these terms thus occur twice in the list of unique query tokens, once with each of two POS tags.

Ordering performance is computed by Nyltiac in terms of A values approximated by the observed NASL values, as well as a number of other performance characteristics. Using the A (NASL) values, the M values for empirical data may be computed. 5. Term stems and suffixes
Individual terms, their components, and part-of-speech tags provide information that contributes to the ordering of documents. By studying the ordering performance using individual terms, unmodified or modified, with suffixes and part-of-speech tags, one can determine the relative contribution of different natural language processing options or combinations. Table 1 shows the performance results for a system using different term options when ordering documents in the three databases.

Term roots represent the most basic semantic unit in many instances, although those who know the etymol-ogy of terms may recognize syllables in terms as carrying specific meanings from their language of origin.
When language is produced, prefixes and suffixes are often added to represent features such as tense, gender, person, number, etc. Native speakers of English are aware (consciously or unconsciously) that the suffix able happy , happily ) represents an adverb, and that the suffix s can represents several things, such as person (e.g., I walk , she walks ) or plurality (e.g., toy , toys ). The roots may represent aspects of a topic, and matching the occurrences of these roots in queries and documents should produce better than random document ordering.

The Porter algorithm for stemming ( Porter, 1980 ) provides an effective method of isolating stems. The algo-rithm moves through 5 steps in the removal of suffixes. Porter tested the algorithm on 10,000 terms and found that roughly 1/3 were reduced by Step 1. After Step 4 has been applied, roughly 2/3 of the terms had been reduced.

Table 1 contains data showing that the performance improves as more stemming occurs. Interestingly, removing stems beyond Porter level 1 makes a great deal of difference for the ADI463 database but little for the other two databases. As the database size increases and as one moves into the harder sciences, the rela-tive utility of stemming appears to decrease. This may be due to the much better initial performance achieved with no stemming for the MED559 and CF683 databases, probably due to the superior topic carrying ability of the precise terminology in the sciences (e.g., MED559 and CF683 databases) as compared to the more ambiguous terminology in the social sciences (e.g., the ADI463 database).

Analyzing the Relative Feature Utility (RFU) for the extremes in this table may provide insights. One may examine the within-database changes by looking at the M values, noting their general improvement as one moves down the table. Examining between-database M values allows the characteristics of the databases to be further studied and understood. The M value for MED559 (compared to ADI463) using Eq. (2) is stemming, with the comparable values for CF683 compared to ADI463 being 1.91 and 1.52. Clearly, the base-line, full term performance is better for the harder sciences, with the topical statements in MED559 having better performance than the questions in CF683. When full stemming is used, the harder sciences still perform better than the social sciences, but the degree of increase (RFU) is lower than the increase for full terms as one moves from softer to harder sciences. 6. The utility of part-of-speech knowledge The above techniques also may be used to analyze the Relative Feature Utility of various parts-of-speech.
This work considers terms as tagged consistent with the University of Pennsylvania Treebank, using terms tagged by the Brill Tagger with the presence of the letters VB in a tag as representing a verb, NN as indicating a noun, RB as indicating an adverb, and JJ as indicating an adjective. In some analyses, terms labeled as nouns or adjectives are lumped together in an attempt to capture noun phrases, which may be the primary linguistic structure that captures topicality.

Part-of-speech tags, attached to a specific term, provides valuable information that can help disambiguate terms, allowing a system to determine which meaning of a term is represented by the term X  X  presence ( Fox,
Nutter, Ahlswede, Evens, &amp; Markowitz, 1988; Justeson &amp; Katz, 1995; Losee, 2001; Rittman et al., 2004; Wilks &amp; Stevenson, 1998 ). When the term bank occurs in a document and a query, having the term labeled as a noun in the query and meaning a river bank and labeled as a verb in the document meaning to bank a plane, the
POS tagging allows the non-match to exclude documents with the term whose sense does not match the query sense. This is an imperfect form of disambiguation, as queries addressing financial matters (e.g., bank as a noun) will match with documents containing bank as a noun that address the inward tilt of an airplane X  X  path.
An examination of Table 2 suggests that nouns and adjectives contributed the most to ordering tasks of those terms and parts-of-speech tested. This is consistent with the notion that nouns and noun phrases repre-sent concrete and abstract  X  X  X hings X  X  that might be the subject of a query and thus have the best potential to contribute to discriminating between relevant and non-relevant terms. Verbs and adverbs consistently assist little in the ordering process. Stemming, as shown in Tables 1 and 2 , contributes a small amount to ordering performance. It is clear, at the same time, that stemming does not provide as much of an ordering improve-ment as does part-of-speech tagging.

In the top half of Table 2 , nouns (unstemmed) have M values of approximately 2 (2.26, 1.56, and 1.88) when compared to all the parts-of-speech, that is, a noun has approximately twice the ordering performance of a ran-domly selected term of any part-of-speech. One finds a similar strength for nouns in the bottom half of Table 2 , showing stemmed terms. The M value comparing the improvement made when changing from individual adjectives to individual nouns has the set of M values for the three databases of log  X  2 0 : 4407  X  = for stemmed terms. These values were computed from the A values in Table 2 . For unstemmed terms, the M values, similarly computed, are 1.72, 0.58, and 1.28. Clearly nouns are more effective than all terms at ordering for ADI463 and for CF683, with adjectives contributing more to ordering with MED559.

The MED559 database is based on queries which are not English language questions but are instead phrases indicating the topic of the query. In these rich statements, adjectives are not merely supportive but in fact are main topic carriers. The first 10 adjectival occurrences in the CF683 database queries are physical, 10 adjectival occurrences in the MED559 database queries are crystalline , cerebrospinal , partial , bronchial , fatty , placental , normal , fatty , ventricular and septal . The adjectives in MED559 are more subject-bearing than adjectives in CF683, leading to the significant difference between the M values for MED559 and the other databases.

Comparing nouns and verbs shows that nouns contribute much more to ordering than do verbs. The M values for unstemmed nouns (improvement over unstemmed verb performance) is log  X  2 0 : 4425  X  = log  X  2 the ADI463, MED559, and CF683 databases, respectively. These values were computed from the A values in Table 2 . Nouns carry more topicality for ordering in the harder sciences than in the social sciences, and thus the M values are larger for the MED559 and CF683 databases than for the ADI463 database. When com-paring the MED559 and CF683 databases, the MED559 database expresses much of its topicality through its adjectives, with the nouns in the CF683 database carrying a much greater degree of topicality than is found in the MED559 database. Nouns seem to be better indicators of topicality in the harder sciences than in the social sciences.
 7. Phrases and groups of terms
Queries and document terms were treated above as single term queries. In this section, sequences of n terms are considered as phrases of size n . Given a phrase, this work matches by using the bag-of-words model, where terms matching between query and document are matched in any order. Thus, { a , b , c } would perfectly match with { c , b , a } and { a , c , b }. Each possible phrase of size n that can be derived from the queries as a set of successive terms is used to produce new query-document-relevance relationships. With phrases of size 2, there queries for a given database in Table 3 decreases as the phrase size increases because there is usually 1 fewer phrase of size n in a given query than there is of size n 1.

The bag-of-words model is used here rather than trying to match sequences because it was found that as sequences increase in length, very few exact matches occur. Using the full bag-of-words model with sequential phrases results in much better full and partial matching, thus capturing the topicality inherent in phrases ( Haas &amp; Losee, 1994; Jacquemin, 1996; Losee, 1994a, 1996 ).

The data shown in Table 3 provides two sets of M values for each database, representing different starting points for the computation of the base value for M . The data shows that larger sets of terms contribute more to ordering performance than do smaller sets ( Losee, 2004 ). As more features are made available, more infor-mation is available to allow us to discriminate between documents of different topical-relevance categories.
One can approximate the study of phrases that may be thought of as noun phrases by examining phrases containing only nouns and adjectives. Ordering with only nouns and adjectives performs better than with the full untagged terms. Table 3 shows comparable phrase sizes being superior when they are noun phrases (nouns and adjectives) compared to using no part-of-speech tags. Stemming adds little beyond what is already obtained with nouns and adjectives, suggesting that someone designing a system might gain significantly by limiting phrases to noun phrases but would gain much less by stemming.

Beginning with the top of Table 3 , data suggests that ordering for MED559 and CF683 databases is supe-rior to performance with ADI463 databases. The M values for MED559 and CF683 databases for single full term phrases, compared to an M = 1 for the ADI463 database, may be computed as 2:26 and 1:91, respec-tively. Considering the 4 term phrases with only nouns and adjectives, data about MED559 and CF683 suggests that the M values, based on the ADI463 values, may be computed as 1.42 and 1.32, respectively, and for stemmed nouns and adjectives are 1.29 and 1.31.

This decrease in M values is probably due in part to the much stronger topic-carrying ablity of the full terms in the harder sciences. The social science database carries less topicality initially and thus has greater room for improvement (and does improve more) as one adds part-of speech information and stemming. 8. Large groups of terms: documents
One can extend the power of phrases to use all terms in a query when matching queries and documents for retrieval purposes. Table 4 shows results for more traditional information retrieval experimentation. The M values for MED559 and CF683 clearly take a leap when scanning down the table when one moves to stemmed nouns, which clearly outperform other modifications or options that are used. The upper-bounds values show another leap, as one would expect as one moves from empirical data to upper limits. Note that the upper bounds ranking is determined by ranking documents by their retrospective expected precision values ( Losee, 1994b ), first retrieving the set of documents with the same characteristics, vis-a ` -vis the query, which has the highest precision, then the set of documents with the same characteristics as the query with the second highest precision, and so forth.
 Several other weighting systems are used in this table. TF-IDF represents term frequency multiplied by the
Inverse Document Frequency (IDF) weight. Because the CLMF weight used here is similar to TF-IDF but with the IDF weight held constant, the ordering performance values for CMLF and TF-IDF values are sim-ilar. The Binary Independent (BI) and Two Poisson Independent (TPI) methods ( Lewis, 1998; Losee, 1988 ) use full knowledge (omniscience) or retrospective parameter estimation and use the initial parameters of 1, 2 (a prior beta distribution with parameters 1, 2 that could be used with relevance feedback) for both relevant and non-relevant documents ( Losee, 1988 ). 9. Discussion and conclusion
When information carrying units are ordered by their probability of topical relevance, the performance shown in the ordering, and the use of features when ordering, can be measured. These values, in turn, may be compared so that a given feature may be said to be n times as helpful as another feature using the Relative
Feature Utility (RFU) measure, a linear measure of relative performance. System designers and users who have the choice of which features a system should use may then make a rational, cost-effective decision about which features to include by computing the M or RFU value. Features may be ordered and the better and most cost effective features used. Clearly, a low M feature that occurs frequently may be a better choice than a higher M valued feature that occurs rarely; however the rare but high M valued feature may be the better choice in situations where there is significant cost associated with using each feature occurrence, possibly due to storage and processing costs.

The empirical results presented above help us to understand how terms and their features have utility in the ordering of documents, as do other features in documents. Stemming terms consistently resulted in improved performance, as did the addition of part-of-speech information about nouns and adjectives. We have also examined how longer phrases resulted in better ordering performance. Table 2 shows precisely how much more topically powerful nouns are than most other parts-of-speech. Using this knowledge would allow a nat-ural language engineer to decide how much more useful a noun would be than an adjective when considering the cost effectiveness of incorporating specific features in document ordering systems or when designing meta-data system, for example.

Are two adjectives as useful as a noun when applied to ordering tasks? This work has provided a measure that can be used to answer this form of question. More specifically, we have suggested that the answer is close to yes for a social science database ( M = 1.7) but is clearly no with the medical databases. Further studies will need to be conducted to make more definitive statements about the relationships between different linguistic and non-linguistic features across a range of different environments.
 References
