 Efficient or g anization, retrieval and convenient browsin g of m u ltimedia content are an attractive application today. A lar g e proportion of m u ltimedia doc u ments involve speech s u ch as news broadcasts, meetin g s, interviews, technical presentations, movies and lect u res. With the increasin g importance of h u man-machine interaction, speech, as the most nat u ral way of h u man comm u nication, has become the core of many Nat-u ral Lan gu a g e P rocessin g (NL P ) applications. The retrieval of m u ltimedia doc u ments (SDR) or alternatively, speec h retrie va l . Research in SDR is concerned with the re-presentation of spoken a u dio in video and/or a u dio doc u ments u sin g speech reco g ni-tion techniq u es, for application in information retrieval (IR). The g oal in SDR is to g ain access to the information that is  X  X ncoded X  in the speech by  X  X ecodin g  X  the representation of s u ch doc u ments. Nowdays, findin g for s u ch spoken doc u ments will be an inte g ral part of the browsin g interface, facilitatin g search, indexin g and retriev-al. Witho u t do u bt, speech retrieval has g enerated a lot of interest lately. Strictly speakin g , in this area, the foc u s of SDR is not on spoken doc u ments b u t on the spoken a u dio contained in m u ltimedia doc u ments. A lot of u sef u l information can be fo u nd in the spoken a u dio contained in a m u ltimedia doc u ment and by deployin g the state-of-the-art in c u rrent speech reco g nition technolo g y, a considerable part of information can s u ccessf u lly be recovered. m u ch work on SDR has been done in recent years. In 2000 the iss u e of a u tomatic speech reco g nition (ASR) for spoken doc u ment retrieval was declared sol v ed for the broadcast news domain. Many collections however, are not in this domain and ASR for these collections may contain spontaneo u s speech yielded specific new challen g es to SDR. Transcripts of speech obtained thro ug h ASR contain reco g nition errors, which can be as hi g h as 40% for spontaneo u s speech [1]. Unlike written lan gu a g es, speech disfl u encies s u ch as lexicalized or non-lexicalized pa u ses, repetitions, and false starts. This has impact in several nat u ral lan gu a g e processin g tasks s u ch as part-of-speech ta gg in g , a u dio se g mentation, capitalization, p u nct u ation, s u mmarization, speech translation, etc. Th u s, disfl u encies which can be removed in order to retrieve the ori g inally intended fl u ent u tterance. Disfl u ency removal makes sentences shorter, less ill-formed and th u s facilitates the downstream processin g by nat u ral lan gu a g e u nderstandin g components s u ch as machine translation or s u mmarization. Initial re-s u lts on disfl u ency remover into a speech-to-speech translation system show very promisin g res u lts. Altho ug h the ASR system has to acco u nt for all the disfl u ent cate-editin g expressions, insertions, and complex seq u ences), the foc u s of the present freq u ent disfl u ent types prod u ced, where it is a vocalized pa u se that is u s u ally u sed by speakers to prevent interr u ption from others while plannin g their u tterances [1]. In many cases of ASR, filled pa u se is commonly g ro u ped with word elon g ations as they have similar aco u stical feat u res [2,3,4]. mana g in g filled pa u ses in spontaneo u s speech were done by u sin g two approaches. The first approach is by u sin g a F P corp u s to g ive prior knowled g e to the speech re-co g nizer [5]. On the other hand, the second approach detects and deletes F P s prior to same class, therefore F P detection is less a problem. The second approach detects and removes F P u sin g aco u stical feat u res before speech reco g nition is done. This ap-proach poses a problem as both elon g ation and F P have stable formant freq u encies, flat pitch, constant ener g y and lon g er d u ration [7,8]. Therefore, elon g ation is conf u sed as F P and conseq u ently removed. s u ch as Hidden Markov Model has been u sed and has shown promisin g res u lts [7]. Gar g and Ward attempted F P and elon g ation classification for Mandarin lan gu a g e and achieved 80.6% precision and 92.59 recall rates. In ASR, well-known classifiers s u ch as Hidden Markov Model (HMM), Ga u ssian Mixt u re Model (GMM), S u pport Vector Machine (SVM) and Artificial Ne u ral Network (ANN) have been u tilized. Amon g these classifiers, ANN is proven to be the most efficient in speech reco g nition [9]. The g eneralization ability of ANN allows the hidden part of the pop u lation to be u n-derstood even if the sample data contains noisy information [10]. ANN has also been u sed for disfl u ency detection [5]. However, F P and elon g ation are g ro u ped as disfl u -ency and no distinct classification is made. In this research, we present and compared two trainin g al g orithms of ANN to classify elon g ation and F P in spontaneo u s speech. overview of the data collection. The details of the methods and implementation are described in Section 3. Section 4 describes the experimental and performance eval u a-tion of the speech reco g nizer and retrieval. In Section 5, the experimental res u lts are reported and disc u ssed. Finally, the concl u sion is drawn in Section 6. The speech data u sed in this research is g athered from han s a rd doc u ments of Malay-sian P arliament  X  s debate sessions of 2008. It comprises Malay lan gu a g e spontaneo u s speeches spoken by male and female speakers of Malay, Chinese and Indian ethnics. terr u ptions, and vario u s speakin g style (low, medi u m and hi g h intonation). A total of 1348 sentences are selected from the speech data for o u r experiment. There are 148 sentences contained F P s alone which is defined as F P d a t a set and the remainin g sen-tences (1200) contained both F P s and elon g ated words and defined as F P Elo d a t a set , when all 1328 sentences were annotated man u ally. F u rther analysis shows that 440 F P s and 1,241 normal words of different d u ration and m u lti-speakers are then g a-thered from these sentences. We s u bseq u ently extracted 129 elon g ated words from F P Elo dataset and define it as ELO_dataset for testin g p u rposes. In En g lish lan gu a g e, ELO is described as the extension at the end of the u tterances as a replacement of F P [11]. Malay words are a gg l u tinative alphabetic-syllabic that are based on fo u r distinct syllable str u ct u res, i.e. V, VC, CV and CVC [12]. Based on o u r data analysis, we described o u r ELO as the last syllable of an u tterance. The selection of the elon g ated data is based on the most common u ttered words in ELO_dataset. The example of words is tab u lated in Table 1. The g eneral idea towards this work is to classified two spontaneo u s speech disfl u en-experiments. Both datasets are divided in a trainin g set, a testin g set and a develop-ment test set for cross-validation. Since only little data is available, the entire test set and ELO dataset were u sed for testin g and t u nin g the system and facilitate the remov-filterin g are s u bseq u ently applied to the datasets. ly removed, th u s affect the semantic contents and meanin g of the sentences. Havin g g enerated the transcripts for the speech corp u s, an information retrieval en g ine index-es the transcripts for retrieval. In order to improve speech retrieval process of spoken filled pa u se and the elon g ation words into syllables by u sin g Voice Activity Detection (VAD) method. The aim is to determine exact representation of both data before aco u stical feat u res are extracted. Bo u ndary detection of F P and elon g ated words, Fi g . 1 below. 3.1 FP and ELO Boundary Detection In ASR, there is a need to process the u tterance consistin g of speech, silence and other back g ro u nd noise. The detection of the presence of speech embedded in the vario u s types of non-speech events and back g ro u nd noise is called end point detection, speech detection or voice activity detection (VAD). In o u r research, we inte g rate Vol u me and First Order Differences (VFOD) for voice activity detection. The key parameter in vol u me-based VAD is a vol u me threshold, Vol T h r comp u ted as follows: where, Vol ma x and Vol m i n is the maxim u m and minim u m vol u me vector of the speech and V c is the coefficient that is set to 10 19 . Vol u me vector is f u rther calc u lated by u sin g the followin g form u la. is the n u mber of frames. FOD is based on first-order differences of a g iven si g nal as characteristics in time domain and is defined as: Th u s, it can be confirmed that the se g mented of last syllable of normal words can be conf u sed as F P if it reached more than 200ms d u ration [2]. 3.2 Classifier For classification, we u se the noisy-channel approach, a concept which is borrowed can only observe the noisy, i.e. the F P disfl u ent strin g which is above than 200ms machine translation the fl u ent strin g is associated with the tar g et lan gu a g e, the disfl u -translation of the disfl u ent strin g into a fl u ent one as adopted from [14]. In Eq u ation (4), the problem is expressed in mathematical terms and reform u lated u sin g Bayes r u le. C denotes the fl u ent strin g , N the disfl u ent strin g . modeled. The probability can be decomposed as in Eq u ation (5), which is simplified from translation model [4]. which can be deleted in to obtain is the probability that word of len g th of the disfl u ent sentence . Each of the probabilities is finally composed of wei g hted s u m over two models, where (M1) models the position of F P and (M2) models the position of ELO as shown in Eq u ation (6). factor for model . Usin g Eq u ation (4) u ntil Eq u ation (6) will transform the res u lt into ne g ative lo g space, the search criterion becomes: gu a g e model over the translation model. While all probabilities can be learned from the trainin g data, the wei g htin g factors and have to be determined separately. Iterative g radient descent proced u re which maximizes the avera g e probability 
Startin g with a set of initial parameters ( , , for each pair new pa-rameters val u es are calc u lated u sin g the followin g u pdates r u les: previo u s u pdate directions to the c u rrent u pdate direction and th u s infl u ences the ef-avera g e val u e of of previo u s and the c u rrent epoch (one epoch is one com-rameter set is taken from the epoch, in which the avera g e val u e of over the pairs of development test set is minimal. For the known-item speech retrieval eval u ation, a set of 1348 sentences from han s a rd doc um e n ts were collected and man u ally transcribed on the word level. Man u ally se g mented hand annotated temporal F P s bo u ndaries were g iven three types of F P s ( u hm, eer and aaa) and were f u rther interpreted as q u ery aimin g at the retrieval of the respective sentences. The retrieval task was to find for every q u ery, the tar g et sentences which incl u de F P s and elo g ated words. For eval u atin g speech reco g nition performance we u sed the standard word error rate (WER) as o u r metric over man u al hand annotated transcription. words in the a u tomatic transcription, as the n u mber of s u bstit u ted words in the a u tomatic transcription, as the n u mber of words from the reference deleted in the a u tomatic transcription, as the n u mber of words inserted in the a u tomatic tran-scription not appearin g in the reference, and as the n u mber of correctly reco g -nized words. The word error rate is defined as: as the w ord reco gn itio n r a te , 
On the other hand, we u sed precision and recall eval u ation meas u res with respect to man u al transcription to eval u ate the retrieval performance. P recision and recall can be calc u lated for each word of the g iven sentences. To calc u late the meas u res instead over the entire vocab u lary, we may take either the m icro-av er ag e or the ma -cro-av er ag e [11]. The micro-avera g e (denoted by s u bscript ) wei g hs each individ u -al information u nit (word occ u rrence) eq u ally, as: (vocab u lary word) eq u ally: where is the s u bset of words in that are present in the reference transcrip-tion, is the s u bset of word in occ u rrin g in the a u tomatic transcription and where in all cases the s u mmation is only over terms where the correspondin g word-based meas u res are defined. edit framework of the WER is the way in which word s u bstit u tion errors are handled. In o u r proposed framework there are f u ndamentally only two types of errors, inser-tions (false alarms) and deletions (false rejections). We view a s u bstit u tion error as a constr u ct describin g the case when these co-occ u r. In terms of information content, a s u bstit u tion error represents both a loss of relevant information as well the retrieval of erroneo u s information, and th u s is considered as both a de letion and an insertion er-ror. While the  X  X ommon sense" view in ASR considers that co u ntin g s u bstit u tions information content of the words in the context of an end application. Of co u rse, it is feasible that the s u bstit u tion of one partic u lar word by another may be allowable for a g iven application as it inc u rs no cost in terms of system u sability. S u ch cases can be catered for in the information retrieval framework by applyin g a text normalization process (e. g . stemmin g , synonym-matchin g , homophone-matchin g ) prior to calc u lat-in g eval u ation meas u res, with this in mind, relatin g the information retrieval frame-work to standard error types enco u ntered in speech reco g nition, the word reco g nition rate is g iven by (from Eq u ation 12): word reco g nition rate can be ne g ative and that interpretin g val u es depends on the can be considered that WRR as the micro-avera g ed recall penalized by incl u din g insertion errors in the n u merator. In the information retrieval perspective presented here, there is no basis or clear interpretation for s u ch a meas u re. The micro-avera g ed recall can be written as, which is eq u ivalent to the word correct rate (WCR), and the micro-avera g ed preci-sion can be expressed as, 
From this we see that the WRR is essentially eq u ivalent to the WCR (recall) pena-lized to also incl u de insertion errors. A more consistent way of eval u atin g the rate of insertion errors is to instead define the correspondin g precision meas u re, and u se principled combinations, s u ch as the F-meas u re [15], whenever a sin g le meas u re is req u ired. This section compares the res u lts obtained from u sin g the g radient descent proced u re below. achieves the same as the hand t u ned parameter set. These res u lts are very enco u ra g in g since the hand optimization was performed on the test data and therefore defines a kind of g olden standard. These res u lts hold for both datasets, and indicate that the g radient descent proced u re g eneralizes well. We concl u de that the g radient descent proced u re is an appropriate method for rapid system development that makes hand man u al and comp u tational effort and expertise, since the whole system has to be r u n for several parameter combinations and the res u lts have to be eval u ated caref u lly, in order to find a g ood parameter set. experiments on applyin g F P Elo dataset to F P s removal system of reco g nized speech. We investi g ated the followin g trainin g and cross-validation set u ps: (S1) Trainin g on man u al transcriptions, cross-validation on reco g nizer o u tp u t, (S2) trainin g and cross-validation on reco g nizer o u tp u t, (S3) trainin g and cross-validation on man u al transcriptions, and (S4) trainin g and cross-validation on a combination of man u al transcriptions and reco g nizer o u tp u t. P arameters for each set u p were optimized u sin g the g radient descent proced u re. The F P and ELO in reco g nized speech were annotated minimal editin g distance. Res u lts of the experiments for the different set u ps are g iven in below. reco g nition o u tp u t rather than man u al transcriptions. The lower recall may res u lt from F u rthermore, d u e to reco g nition errors, seq u ences which are ta gg ed as repetitions no lon g er exist as seq u ences of repeated words. The lower precision can be explained by the fact that wron g ly reco g nized words appear to be F P s in their context altho ug h the ori g inal word is fl u ent. improve performance on noisy test data. The decrease of recall in the other set u ps is Usin g (S2) for trainin g , d u e to reco g nition errors these words occ u r less freq u ently in contexts in which they are deleted. P recision increases for experiments with set u p (S2) beca u se a system trained on reco g nized speech can cope better with the problem of ill-formed and u n g rammatical sentences in a test set that is based on reco g nized speech as well. Therefore, a smaller n u mber of false positives are prod u ced, since some ill-formed constr u ctions are tolerated. The res u lts prod u ced with set u p (S3) are almost as g ood as the res u lts prod u ced with set u p (S1). This indicates that trainin g on model wei g hts a cross-validation set based on reco g nized speech seems more appro-priate. The combination of man u ally transcribed and reco g nized speech in (S4) does not improve the res u lts. This means that a simple combination does not profit from the g ains seen in (S2) and (S3). In this paper we presented two approaches to a u tomatically detected and removed F P s in transcription of spoken doc u ments retrieval system. We implemented a g radient descent method to a u tomatically optimizin g the parameter wei g hts. The res u ltin g system is as g ood as the g olden standard which was set by hand optimizin g the para-meters on the test data. These res u lts are very enco u ra g in g since they allow for a rap-id deployment of the disfl u ency removal system in new domains or lan gu a g es. Second, we extended o u r experiments to reco g nizer o u tp u t. We achieved best res u lts when we trained the models on man u ally transcribed data and optimized the model wei g hts on reco g nizer o u tp u t data. Acknowledgements. D u e acknowled g ement is accorded to the the Research Man-a g ement Instit u te (RMI), Universiti Teknolo g i MARA for the f u ndin g received thro ug h the Cl u ster Grant, 600-RMI/DANA 5/3/CG (5/2012).

