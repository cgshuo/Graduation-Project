 In recent years, sentiment analysis, a lso known as opinion mining, has been a hot research area and has attracted much attention from many fields (e.g., data mining, information retrieval and information security). The task of sentiment analysis is to classify a subjective text as positive or negative according to the sentiment expressing in it. Sentiment word always plays a decisive role in sen-timent classification. In this paper, we concentrate on this task of sentiment word identification (SWI). Most existing automatic approaches for sentiment word identification are either dictiona ry-based or corpus-based. Approaches in both classes have in common that they typ ically exploit a set of labeled words called seed words. By calculating seman tic similarities between seed words and candidate words, the orientation of each candidate word is derived. However, those seed words are always manually se lected. This selection process is very subjective. Any missing key words may l ead to poor performance. Therefore, those seed words based approaches have low robustness.

To tackle this problem, Yu et al. [19] and Liang et al. [9] have proposed two solutions without seed words. They respectively utilize sentiment matching and sentiment consistency for modeling. Th ey assume that subjective corpora fit the two sentiment phenomena but have not proved it. In fact, the hypotheses is an ideal condition. Real-worl d corpora can not fit them well.

Inspired by the two models, we explore to combine the two sentiment con-textual factors for SWI in this paper. We firstly investigate whether sentiment matching and sentiment consistency exist on real-world corpora. We find that the two phenomena are complementary sentiment contextual factors in real-world corpora. Then, we discuss how the sentiment contextual information could be modeled and utilized for SWI. Finally, we conduct extensive experiments to ver-ify the proposed models. Even inspired by [19,9], this paper is different from them. The main contributions of this paper can be summarized as follows: -This paper is the first to verify the existences of sentiment matching and -We propose three novel matrix factorization based models to automatically -We propose a new similarity function from both co-occurring and semantic -We conduct extensive empirical studies to compare our models with other The rest of this paper is organized as follows. We introduce the motivation in Section 2. Section 3 defines our effective s entiment word identification models. We present our experimental setup and results in Section 4, while describing the related work in Section 5. Finally conclusion appears in Section 6. In this section, we introduce our motivation exploiting sentiment contextual relations for SWI. At first, we would like to demonstrate the existence and sig-nificance of sentiment contextual factors (including sentiment matching and sen-timent consistency) on three real datasets which will be introduced in Section 4.1. We classify the sentiment words into positive and negative according MPQA 1 .
In order to describe senti ment matching, we calculate the percentage of senti-ment words that match to the documents X  polarities and define sentiment match-ing ratio as where M is the document number in the corpus, d i is the i th document and c j is the j th word in the vocabulary list with size N . I ij is the indicator function whether polarities of d i and c j are the same.

We define the sentiment consistency ratio of c j as
We plot part of the pairs ( sm j , sc j ) as points w.r.t. sentiment matching and sentiment consistency in Fig.1. It ca n be observed that (1) only a very few of sentiment words and documents absolutely fit to sentiment matching, (2) more than half of sentiment words have sm j  X  0 . 5 (3) positive sentiment words have higher sentiment consistency ratios than negative sentiment words and they are linearly separable, (4) sentiment match ing and sentiment consistency are two cover the vast majority of words, and (5) IMDB matches the two phenomena best, then the DV D corpus, and the Movie corpus last.

The first three observations show that ei ther sentiment matching or sentiment consistency exists in subjective corpora. Both of them can be modeled to address SWI problem. However, only one of them can not cover most of the words because sm j  X  0 . 5or sc j  X  0 . 5 can only cover slightly more than half words. Observation (4) provides the possibility of combining them for modeling. But there are still One possible reason is that the pattern  X  X egation word + negative word X  is used to express positive sentiment while  X  X egation word + positive word X  is used to express negative sentiment. Negat ion word (e.g., not, never, don X  X , can X  X ) could reverse the polarity of word. Ther efore, it is necessary to do the negation preprocessing by concatenating first word after the negation word that should not be a stop word. For example,  X  not a good idea X  becomes to  X  not good idea X  after negation handling. Observation (5) shows that different corpora differ in matching the two sentiment phenomena. Here, we make a prediction that the more matching the higher sentime nt word identification accuracy. 3.1 Problem Formulation The M documents and N words in the corpus are represented in an M  X  N document-word matrix W =[ w 1 ,w 2 ,  X  X  X  ,w N ], in which each row corresponds to a document and each column corresponds to a word. w ij denotes the weight of c j to d i . The larger value of w ij , the more important role word c j plays in the document d i . WEED[19] uses normalized TF-IDF to claculate it. However, TF-IDF does not consider any sentiment information. It might not truly represent the importance of a word from sentiment view. To overcome it, we define w ij as collection frequencies of c j in positive and negative corpora, and IDF j is the inverse document frequency of c j . l i  X  X  +1 ,  X  1 } ,if d i is a positive document, then l i = +1; otherwise l i =  X  1. | d i | is the length of d i .
Let U be an M  X  K document-feature matrix in which each column u k cor-representation of document d i in the latent feature space. Suppose that there are M + positive documents and M  X  negative documents in the corpus where M + + M  X  = M .So U can be split into 2 sub-matrices U =[ U + ,U  X  ] T ac-cording to documents X  sentiments where U + =[ u 1 , u 2 ,  X  X  X  , u M + ] T and U  X  = [ u v n stands for the representation of word c n in the latent feature space.
For each word c n , we calculate its sentiment polarity value by where y n &gt; 0 indicates c n is a positive word while y n &lt; 0 indicates c n is a negative word. 3.2 Modeling Content This paper considers sentiment word identification as a matrix factorization problem. More precisely, the document-word matrix W is approximated as UV . It amounts to solving the following optimization problem: where || X || 2 F denotes the Frobenius norm,  X , X  &gt; 0 and the last two regularization terms are used to avoid overfitting. In fact, W is a sparse matrix. We only need to factorize the nonzero entries. Hence, we change Equation 5 to
Low rank matrix factorization models, such as singular value decomposition (SVD) and non-negative matrix factorization (NMF), are always utilized to solve this problem.
 3.3 Modeling Sentiment Matching In this section, we discuss how to model sentiment matching phenomenon for SWI. Yu et al. [19] believe that document and its most component sentiment words share the same sentiments, It be true in the situation that a document only express one sentiment. Obviously, in real-world subjective documents, there might be more than one sentiments expressed in them. Based on this intuition, we hold that, for each document, the average of the sums of all its contained words X  polarity values is closed to its polarity value. We formulate it as the following optimization problem where x  X  X  + ,  X  X  ,if l i =1then x = +; else x =  X  . Then the objective function exploiting sentiment matching can be formulated as where  X &gt; 0. The second and the third terms are sentiment matching regular-ization terms with respect to U + and U  X  .Wecallthismodel SM .Inthispaper, we use the gradient search method to solve the problem. 3.4 Modeling Sentiment Consistency with Average-Based Term According to sentiment consistency, two f requently co-occurring sentiment words aremorelikelytohavethesamepolari ty than those of two randomly selected words. So let X  X  further assume that, c i polarity should be close to the expectation polarity value of all the co-occurring words. Based on it, we propose another regularization term to impose constraint on sentiment consistency where R ij  X  X  0 , 1 } ,if c i and c j have once co-occurred then R ij =1;otherwise R ij = 0. Similarity function Sim ( c i ,c j ) allows to treat c i co-occurring words differently. CONR [9] uses PMI to capture the similarity. PMI considers co-occurring frequency as the main measure. Factually, even some words seldom co-occur, they might similar from seman tic perspective. For example,  X  X reat X  and  X  X reatly X  hold the same meaning although they may seldom co-occur. PMI fails to capture those semantic information. To solve this problem, we present a novel similarity between c i and c j defined as original corpus and cos is a semantic similarity function. We use cosine similarity with word vector trained by Word2Vec 2 to calculate semantic similarity between c and c j . Word2Vec is a neural network implementation that learns distributed and meaningful representation for word s. Each word is represented by a fixed-length vector. Word2Vec is the state-of-the-art word vector model. It can enable words with similar semantic properties close to each other in the latent space.  X   X  c i is the vector representation of c i by Word2Vec.

Then, the optimization formulation, which integrates sentiment matching and average-based consistency term into the learning process, could be defined as where  X  1 is a positive regularization parameter. We name this model SMC 1 . 3.5 Modeling Sentiment Consistency with Individual-Based Term SMC 1 imposes a consistency regularization term to constrain sentiments among words. However, this approach is insensitive to those words that convey diverse sentiments but appear in the same document. It will cause information loss prob-lem, which will result in inaccurate predicting of y . Hence, we impose constraints between one word and its co-occurring words individually
Under this scenario, the task of sentiment word identification can be mathe-matically formulated as solving the following optimization problem where  X  2 &gt; 0. For convenience sake, this model is called SMC 2 . 4.1 Experiment Setup Datasets. We evaluate our method on the three real-world corpora: the Internet Movie Database 3 (IMDB), Movie review dataset collected by Pang et al.[13] and DVD reviews from NLP&amp;CC2013 4 . The related data settings are the same as for [9].
 Evaluation Metrics. Following [19,9], we consider the precision of the top K sentiment words in our experiments and we use P @ K and MAP @ K to evaluate the top-K ranked words. More precisely, P 1 @ K is used to evaluate the ability of recognizing sentiment words from non-sentiment words while P 2 @ K assesses the performance of classifying sentiment words as positive or negative words. MAP 1 @ K and MAP 2 @ K consider the order for evaluating the ranked words. Concrete definitions can be found in [9]. 4.2 Parameter Settings Here we focus on parameter settings. In our experiments, we set dimension K = 10. Next, we investigate the performa nces when the parameters change. Tradeoff Parameters: The parameters  X  ,  X  1 and  X  2 in our models play the role of learning rates and adjusting the strengths of different constraint terms in the objective functions as shown in Equation (8), (11) and (13). The impacts of  X  and  X  1 generally share the similar trend as the impact of  X  2 . Hence we only illustrate the results of  X  2 for SMC 2 here due to the space limitation. In the extreme case, if we use a very small value of  X  2 , only content term is employed. On the other side, if we employ a very large value of  X  2 , the sentiment consistency term will dominate the learning processes. As a result, we conduct experiments with  X  2 ranging from 0.01 to 0.1. Fig.2 shows the impacts of  X  2 on P 2 @ K .We observe that the values of  X  2 impact the identification results significantly. From Fig.2, we can find that when  X  2 =0 . 04, SMC 2 obtains the best performance on IMDB dataset. Hence, we choose this parameter in our experiments. Similarly, we choose the parameter  X  2 =0 . 07 for Movie and DVD dataset. Although they may be not the best ones, the following experiments demonstrate they are adequate. Number of Iterations: Fig.3 shows the impacts of iteration number on P 2 @100 in our model. From the results, we can see that no matter using which dataset, as iteration number increases, the P 2 @100 values increase gradually at first. With further increasing, P 2 @100 finally turns to a relatively stable value. In order to reach converged results with an acceptable time cost, it is better to run more than 30 iterations in the following experiments.
 4.3 Comparative Analysis We use SO-PMI [17], WEED [19], SVD [4], NMF [8] and CONR[9] as baselines for comparisons with our models. As mentioned above, WEED and CONR are the-state-of-art methods for SWI. SO -PMI needs seed words, we randomly select 20% words for it following [9]. Other models do not need seed words.
P 1 @ K and P 2 @ K results of the methods on the three datasets are respectively reported in Table 1 and Table 2 with 30 iterations. As a further comparison, Fig.4 shows the MAP @ K results. By comparing the results of different methods, we can draw the following observations: (1)Both SMC 1 and SMC 2 can generate better results than the state-of-the-art algorithms. It demonstrates that sentiment matching and sentiment consis-tency can be applied as two complement ary sentiment contextual factors. (2) SMC 1 could achieve reasonable results but do not perform as well as SMC 2 which shows that individual-based term is generally more accurate in capturing sentiment words X  distributions in the corpus. (3)Compared to WEED, SM achieves consistently better performance on three datasets. It indicates that w ij could better describe the importance from word to document from the view of sentiment than TF-IDF. (4) SM obtains better performance than SVD and NMF shows that sentiment matching could be modeled and optimized for SWI. (5) SMC 1 performs better than CONR from which we can seen that word vectors make a contribution to similarities among words. (6)As the size of K increases, both P @ K and MAP @ K of all methods falls accordingly. It indicates that all methods can rank the most probable sentiment words in the front of the word list. (7)Models exploiting sentiment contextu al factors generate t he best results on IMDB, followed by DVD and then Movie. It proves the prediction that the iden-tification accuracy is proportional to the matching degree of the two sentiment phenomena as discussed in Section 3. 4.4 Case Study We conduct a case study on IMDB dataset to demonstrate the effectiveness of our proposed model in this section. Table 3 shows top-8 positive and negative words ranked by each method where the bold words are the ones with correct polarity values. SO-PMI obviously yields the worst results. Our three models perform comparable or better than WEED and CONR. Especially SMC 1 and SMC 2  X  positive words and negative words are absolutely correct. Sentiment word identification is a very common theme in sentiment analysis research. A number of approaches have been proposed to address this problem [14] [16] [18]. Most of them are either dictionary-based or corpus-based. Dictionary-Based Approaches: There is a common assumption that senti-ment words hold the same sentiment polarities with their synonyms and different polarities with their antonyms. With this assumption, lots of approaches [14] [6] [10] which rely on rules and lexical databases such as WordNet [12] are proposed. In [7] and [2], negation/conjunction/disjunction rules and manually created syn-tactic dependency rules are used to automatically derive morphological and syn-onymy/antonymy relationships from WordNet. However, these dictionary-based approaches can not identify domain dependent sentiment words.
 Corpus-Based Approaches: Corpus-based approaches investigate the seman-tic relatedness between unlabeled candidate words and seed words, and then obtain sentiment polarities of unlabeled words according to these relations. [3] and [17] calculate similarity measures, such as pointwise mutual information (PMI) and latent semantic analysis (LSA), to obtain the polarities of words. A further corpus-based approach [14] [5] is to infer the semantic relatedness of words by exploiting linguistic clues. In these approaches, negation and conjunc-tion rules (e.g.,  X  X ot X ,  X  X nd X  or  X  X ut X ) are exploited. However, those approaches are sensitive to prior knowledge. Any missing rules or seed words may lead to low identification accuracy.
 Recently, several methods [19,9] without seed words have been proposed for SWI. In [19], SWI has been tackled via an optimization model exploiting senti-ment matching while Liang et al. [9] converts it to a matrix factorization problem exploiting sentiment consistency. Senti ment matching assumes that polarities of the document and its most component sentiment words are the same [19]. Sen-timent consistency considers that two co-occurring sentiment words are more likely to share the same sentiments [9]. The two models are the state-of-the-art methods without seed words. However, they only propose and utilize the two assumptions but do not investigate wheth er they exist in real-world corpora. Besides, they only respectively use one factor for modeling. Could the two sen-timent contextual factors be combined f or the learning process? In this paper, we give an affirmative answer. In this paper, we address the problem of sentiment word identification. We present three matrix factorization based models for SWI. Our models predict the polarities of words by exploiting the phenomena of sentiment matching and sentiment consistency. Experiments on three real datasets show that our models outperform the state-of-the-art methods without seed words.
 Acknowledgments. This work was supported by Strategic Priority Research Program of Chinese Academy of Sciences (XDA06030600) and National Nature Science Foundation of China (No.61202226).

