 Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Ju-rafsky, 2004). Figure 1 shows an example in Chi-nese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels.

Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suf-fer from three major problems. Firstly, their per-formances are heavily dependent on feature engi-Figure 1: A sentence with semantic roles labeled from CPB. neering, which needs domain knowledge and la-borious work of feature extraction and selection. Secondly, although sophisticated features are de-signed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific anno-tated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent semantic meaning, can alleviate this problem. However, traditional meth-ods cannot relate distinct annotation schemas and introduce heterogeneous resource with ease.

Concerning these problems, in this paper, we propose bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem of Chinese SRL. Our approach makes the following contributions:  X  We formulate Chinese SRL with bidirection- X  Compared with previous work that relied on  X  The framework of our model makes the intro-Following previous work, we regard Chinese S-RL as a task of sequence labeling, which assigns a label for each word in the sequence. To iden-tify the boundary information of semantic roles, we adopt the IOBES tagging schema for the la-bels as shown in Figure 1. For sequence labeling, it is important to capture dependencies in the se-quence, especially for the problem of SRL, where the semantic role label for a word not only relies on its local information, but also is determined by long-range dependencies from other words. The advantage of RNN is the ability to better capture the contextual information, which is beneficial to capture dependencies in SRL. Moreover, we en-rich the basic RNN model with bidirectional LST-M RNN, which can model bidirectional and long-range dependencies simultaneously. 2.1 Model Architecture The architecture of our approach is illustrated in Figure 2. Given a sentence, we first get repre-sentation for each word to be labeled. Then af-ter a nonlinear transformation, bidirectional LST-M RNN layer is designed to combine the local in-formation of a word and its contextual information from both directions. With a nonlinear layer to form more complex features, a linear output lay-er follows. For each word to be labeled, there is an output vector, whose each dimension is a score corresponding to a kind of semantic role label. 2.2 Word Representation Word representation captures the features locally embedded around the word. The features used in our work are: the current word, the current POS tag, the predicate, left and right words, left and right POS tags, distance to the predicate. Note that these features are all basic information about the word, hence we alleviate the heavy job of feature engineering. All these features are introduced by embeddings. After concatenation, we get the word representation feature vector.

To get more complex features, we adopt a non-linear transformation: where x t is the word representation of the t -th word, W 1  X  R n 1  X  n 0 , n 0 is the length of word rep-resentation, f is an activation function and we use tanh in our experiments, N is the number of words to be labeled in the sequence. 2.3 Bidirectional LSTM RNN Representation z t only captures the local informa-tion. Here we adopt RNN to capture contextual information. Traditional RNN has the problem of vanishing or exploding gradients, which means the long-term dependencies can hardly be modeled. LSTM is designed to mitigate this problem.

At each word position t , the LSTM RNN com-putes six internal vectors: e C , g i , g f , g o , C t and h for the memory cell, which is a structure used in LSTM to store information. e C computes the can-didate value for the state of the memory cell: The activations of the memory cell X  X  input gate, forget gate and output gate are defined as: where j stands for i , f or o .  X  is taken sigmoid in experiments. Then we can compute C t , the mem-ory cell X  X  new state at position t : where indicates elementwise vector multiplica-tion. With the new state of the memory cell, we can compute the value of output state h t : h t contains the information not only from local representation z t , but also from previous output state h t  X  1 , hence can capture dependencies in a sentence. Because the dependencies forward and backward are both important to label seman-tic roles, we extend LSTM with bidirectional ap-proach, resulting in:
Further, a nonlinear transformation follows: where W 2  X  R n 3  X  n 2 , n 2 is the dimension of a t . 2.4 Output Representation For each word to be labeled, we adopt linear trans-formation to get the output vector o t : W 3  X  R n 4  X  n 3 , where n 4 is the number of seman-tic role labels in IOBES tagging schema. There-fore, the resulting vector o t for the t -th word is of length n 4 , and each dimension corresponds to the score of a certain semantic role label. 2.5 Training Criteria Because there are dependencies among word la-bels in a sentence, isolated training approach which independently considers each word will be inappropriate. Therefore, we adopt sentence tag approach, in which we encourage the correct path of tags, while discouraging all other valid paths. Given all our training examples: is the corresponding N i (the number of words to be labeled) dimension vector, which indicates the correct path of tags, and y ( i ) t = k means the t -th word has the k -th semantic role label. The score where  X  is an ensemble of all the parameters in the network.

The log likelihood with a single sample is then: where y 0 ranges from all the valid paths of tags.
The full log likelihood of the whole training cor-pus is as follows:
To compute the network parameter  X  , we maxi-mize the log likelihood J (  X  ) using stochastic gra-dient ascent in the experiments. 2.6 Introducing Heterogeneous Resource A single annotated corpus with semantic role la-bels is often limited in its scalability. Heteroge-neous resource in our work is defined as another dataset annotated with semantic roles, which also provides predicate-argument structure annotation, but uses very different semantic role labels and an-notation schema. However, in spite of these differ-ences, the latent semantic meaning may be highly correlated. Therefore, the introduction of hetero-geneous data can alleviate the problem of scalabil-ity with a single annotated corpus.

Traditional approaches hardly concern the ex-istence of heterogeneous resource and are diffi-cult to relate different annotation schemas, but in the framework of our model, heterogeneous data can be introduced in a relatively convenient way. Specifically, we learn a bidirectional LSTM RN-N model based on heterogeneous data, then with the fine-tuned word embeddings we initialize the model on our experimental dataset. The princi-ple behind is that the words almost convey the same semantic meaning albeit in distinct annota-tion schemas. The introduction of heterogenous resource in this way is efficient and can lead to performance improvement on our experiment. We conduct experiments to compare our model with previous landmark methods on the bench-mark dataset CPB for Chinese SRL. The result reveals that even with our basic model, which does not resort to other resources, our approach can significantly outperform all of the competitors. Moreover, we enrich our work with introducing heterogenous resource to make a further improve-ment on performance. And the result also shows the influence of heterogeneous resource is more evident than the standard method of pre-training for word embeddings. 3.1 Experimental Setting To facilitate comparison with previous work, we conduct experiments on the standard benchmark as previous work (Xue, 2008; Sun et al., 2009), which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid. The test set includes 72 files, which are chtb 001.fid to chtb 040.fid, and chtb 900.fid to chtb 931.fid. mantic role labels and annotation schema, which is designed by ourselves for other projects, as hetero-geneous resource. This labeled dataset has 17,308 annotated sentences, and the semantic roles con-cerned are like  X  X gent X  and  X  X atient X , resulting in 21 kinds of types, which are all distinct from the semantic roles defined in CPB. We use the devel-opment set of CPB for model selection, and the hyper parameter setting of our model is reported in Table 1. 3.2 Chinese SRL Performance Table 2 summarizes our SRL performance com-pared to previous landmark results. The work of Collobert and Weston (2008) was conducted on English SRL, we implement their approach on CP-Table 2: Results comparison on CPB dataset.
 B for comparison. As indicated by this table, our approach significantly outperforms previous state-of-the-art methods even with all parameters ran-domly initialized, that is without introducing other resources. This result can prove the ability of our model to capture useful dependencies for Chinese SRL with minimal feature engineering.

Further, we conduct experiments with the intro-duction of heterogenous resource. Previous work found that the performance can be improved by pre-training the word embeddings on large unla-beled data and using the obtained embeddings to make initialization. With the result in Table 2, it is true that these pre-trained word embeddings have a good effect on our performance (we use pre-training). However, as shown in Table 2, com-pared to standard pre-training, the influence of het-erogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improve-ment. The information provided by standard pre-training with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Ju-rafsky (2004) did the preliminary work on Chi-nese SRL without any large semantically annotat-ed corpus and produced promising results. Af-ter CPB (Xue and Palmer, 2003) was built, X-ue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chi-nese SRL. Ding and Chang (2009) established a word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their ap-proach achieved competitive performance on En-glish SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can only weak-ly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well alle-viated.

Our model is based on recurrent neural network (RNN), which uses iterative function loops to store contextual information. To remedy the problem of vanishing and exploding gradients when train-ing the standard RNN, Hochreiter and Schmidhu-ber (1997) proposed long-short-term memory (L-STM), which has been shown capable of storing and accessing information over very long time s-pans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-the-art results in many NLP problems such as lan-guage modeling (Mikolov et al., 2010) and ma-chine translation (Sutskever et al., 2014; Bah-danau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to im-prove strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed L-STM RNN approach for English Semantic Role Labeling, which shared similar idea with our mod-el. However, the features used and the network ar-chitecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RN-N model. In our approach, the bidirectional and long-range dependencies in a sentence, which are important for Chinese SRL, can be well modeled. And with the framework of deep neural network, the heavy job of feature engineering is much alle-viated. Moreover, our model makes the introduc-tion of heterogenous data, which can alleviate the problem of scalability with a single annotated cor-pus, more convenient. Experiments show that our approach achieves much better results than previ-ous work, and the introduction of heterogenous re-source can make further improvement on perfor-mance.
 This research is supported by National Key Basic Research Program of China (No.2014CB340504) and National Natural Science Foundation of China (No.61375074,61273318). The contact authors of this paper are Baobao Chang and Zhifang Sui.
