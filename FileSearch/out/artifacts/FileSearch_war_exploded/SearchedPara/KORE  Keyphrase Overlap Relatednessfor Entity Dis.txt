 Measuring the semantic relatedness between two entities is the basis for numerous tasks in IR, NLP, and Web-based knowledge extraction. This paper focuses on disambiguat-ing names in a Web or text document by jointly mapping all names onto semantically related entities registered in a knowledge base. To this end, we have developed a novel notion of semantic relatedness between two entities repre-sented as sets of weighted (multi-word) keyphrases, with consideration of partially overlapping phrases. This measure improves the quality of prior link-based models, and also eliminates the need for (usually Wikipedia-centric) explicit interlinkage between entities. Thus, our method is more ver-satile and can cope with long-tail and newly emerging enti-ties that have few or no links associated with them. For effi-ciency, we have developed approximation techniques based on min-hash sketches and locality-sensitive hashing. Our experiments on semantic relatedness and on named entity disambiguation demonstrate the superiority of our method compared to state-of-the-art baselines.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Semantic Relatedness, Entity Relatedness, Entity Disam-biguation, Locality-Sensitive Hashing
Semantic relatedness measures between entities like peo-ple, places, songs, companies, etc. are a fundamental asset for many applications dealing with natural language process-ing (NLP), text mining, or Web analytics [4]. Its usefulness has been shown in a wide variety of tasks: named entity dis-ambiguation [23, 19, 16], general word sense disambiguation [31, 24, 26], query expansion for information retrieval [18, 25], Web-based information extraction [1], knowledge base population [9, 30], and more.

In this paper, we focus on semantic relatedness of enti-ties and its role in named entity disambiguation (NED). Consider the terms  X  X ash X  and  X  X ackson X  as an example. When trying to measure the relatedness between these sur-face forms, we face high ambiguity and would assess them as weakly related only. Most possible meanings of  X  X ash X  and  X  X ackson X , such as movies entitled  X  X ash X  and people or cities named  X  X ackson X , have nothing in common. At the entity level, however, once we identify the surface form  X  X ash X  with the singer Johnny Cash and  X  X ackson X  with a song he performed, Jackson (song) , the relatedness measure should be very high. Wikipedia and derived knowledge bases like DBpedia, YAGO, or Freebase provide us with millions of uniquely identifiable entities, and this enables computing relatedness measures at the entity level instead of ambiguous words or phrases. Conversely, entity relatedness is key for disambiguating names that occur in surface text. In our ex-ample, the entity names in a sentence like  X  X he audience got wild when Cash performed Jackson. X  can be correctly dis-ambiguated because only the singer-song combination, out of thousands of other pairs of entities named  X  X ash X  and  X  X ackson X , yields a semantically coherent interpretation.
State-of-the-art measures for entity relatedness that per-form well in NED and other tasks are based on the extensive link structure of Wikipedia. Most notably, the Milne-Witten measure considers the overlap of the incoming links to two entity articles as a notion of their relatedness [22]. This is a great asset for the 3 million entities known to Wikipedia, but it is a limitation for other entities that do not (yet) have a Wikipedia page. For example, in the sentence  X  X he au-dience was haunted when Cave performed Hallelujah. X , the correct meaning of  X  X allelujah X  is neither the H X ndel chorus nor the Leonard Cohen composition, but a totally different song by the Australian singer Nick Cave. That song has a Web page in the music portal last.fm but there is no Wi-kipedia article for the song. Link-based relatedness has no chance to capture the tight connection between Nick Cave and his song. This limitation already applies to the long tail of not so prominent Wikipedia entities whose articles have only few incoming links. In these cases, the Milne-Witten measure does not properly capture the semantic relatedness between entities. Our goal in this paper is to overcome this limitation of Wikipedia-link-based relatedness measures for named enti-ties.
A relatedness measure for link-poor and out-of-Wikipedia entities needs to tap into information that is more widely available and can be gathered with reasonable effort. In this paper, we use entity-specific keyphrases to this end [21, 5]. Keyphrases can be mined from any text describing an entity: people not popular enough for Wikipedia have per-sonal homepages, small companies have websites, singers and songs are discussed in online communities like last.fm or YouTube, and so on. For our difficult example, the last.fm page on Nick Cave X  X  song contains keyphrases like  X  X o More Shall We Part X ,  X  X ustralian singer X ,  X  X arren Ellis X ,  X  X erie cello X ,  X  X ick Cave and the Bad Seeds X , etc. These exhibit significant overlap with phrases contained in the Wikipedia article about Nick Cave (or, alternatively, his Web page), thus giving cues for the high relatedness. We propose a new notion of entity relatedness, coined KORE , based on the overlap of two sets of keyphrases.

While the idea for this approach may seem obvious, it entails a number of technical difficulties, addressed in this paper: Keyphrases and Weights: Multi-word keyphrases are more informative than single-token keywords and thus pre-ferrable for characterizing entities. On the other hand, this makes it less clear which word sequences should be consid-ered as salient keyphrases, and how we could associate them with weights that can be computed in an effective and effi-cient manner. If we simply used keywords, we could use precomputed idf -style weights; the generalization to key-phrases is not obvious.
 Partial Matches: When comparing the keyphrases of two entities, the chance of finding exact matches drops with the length of multi-word phrases. For example, we may not find the phrases  X  X ustralian singer X  and  X  X ick Cave and the Bad Seeds X , associated with the song Hallelujah, in this exact form in the keyphrase-set of Nick Cave, where we may in-stead have  X  X ustralian male singer X  and  X  X ave X  X  Bad Seeds X . Therefore, a viable notion of keyphrase-based overlap mea-sure must consider partial matches between phrases as well. Efficient Computation: In NED, we are interested in the relatedness of all entity pairs in the candidate space. For n potentially relevant entities, a straightforward approach would require n article with say 10 mentions of ambiguous entity names, each of which has 10 possible meanings, we obtain 100 can-didate entities and would need to compute the relatedness for 5000 pairs. This is too expensive for most online ap-plications, and totally infeasible for longer input texts or mentions with higher degrees of ambiguity.
Our approach addresses the above challenges in the follow-ing way. We extract particular kinds of noun phrases from entity-specific pages, and compute weights for both entire phrases and their constituent words. We use MI (mutual information) for phrases, contrasting the entity page with the entire Wikipedia corpus (or the Web), and idf (inverse document frequency) for single words, based on the Wiki-pedia corpus. We define the KORE measure in a two-stage manner, thus allowing for partial matches between keyphra-ses: 1) two keyphrases are paired up if there is a partial match of at least one word, with word-level weights influ-encing the matching score; 2) a weighted Jaccard coefficient captures the overlap between the keyphrase-sets of two en-tities, where the scores of the partially matching keyphrase pairs are aggregated and the phrase-level weights are consid-ered. Finally, to solve the efficiency challenge, we use a two-level approximation technique: keyphrases, as sequences of words, are approximated by min-hash sketches, and these sketches are organized by locality-sensitive hashing (LSH). This way, for a given input set of entity pairs, we compute their relatedness only if their keyphrase representations are mapped to at least one common LSH bucket.

The novel contributions made in this paper are as fol-lows:  X  KORE improves the quality of assessing entity relatedness,  X  KORE is integrated into a joint-inference NED method,  X  KORE can be efficiently computed and avoids the quadra- X  Our improvements in the quality of semantic related-
The classical approaches for quantifying the relatedness of words (common nouns, verbs, etc.) make use of the Word-Net thesaurus, by measuring the token overlap of the glosses describing the concepts (Lesk) or by means of paths in the concept taxonomy or other lexical relations (Resnik, Wu &amp; Palmer). Budanitsky and Hirst [4] give an overview of all these measures.

More recently, Gabrilovich and Markovitch [12] and Ra-dinsky et al. [28] have exploited Wikipedia (and Zesch et al. Wiktionary [33]) to compute word relatedness. These meth-ods represent words by vectors of the concepts they are used to describe, i. e. a word is represented by all the articles it occurs in. Then they use vector-space measures (e.g., co-sine similarity) for relatedness. A powerful variant of this approach, proposed by Hassan and Mihalcea [14], is to rep-resent words as vectors of co-occurring Wikipedia entities. However, the method crucially depends on the availability of a rich link structure like that of Wikipedia.
Work on semantic relatedness between entities (people, places, songs, companies, products, etc.) has started with the advent of large knowledge bases, largely derived from Wikipedia. This created huge opportunities for specifically dealing with named entities (as opposed to general words). The emerging need for lifting Web search and advertising to the entity level created a mega-application as well.
Ponzetto and Strube [27] applied previously WordNet-based measures to Wikipedia, showing that some work bet-ter on the larger though less rigorous collection of entities and classes. Milne and Witten [22] used the link structure in Wikipedia to model entity relatedness. Links within Wiki-pedia refer to canonical entities; thus, the source-target pairs of links can be used as the basis for a relatedness measure. Building on the co-occurrence-based information-theoretic model of [20, 6], the Milne-Witten relatedness measure, MW for short, for two entities e and f with sets of incoming Wi-kipedia links I is the total number of articles in Wikipedia):
MW ( e, f ) = 1  X  log ( max {| I e | , | I f |} )  X  log ( | I So far, the MW measure has achieved the best results on quantifying entity relatedness.
NED is the task of mapping surface names of entities (peo-ple, places, etc.) in text documents onto canonicalized en-tities registered in a knowledge base like DBPedia [2], Free-base.com, or YAGO [32], this way disambiguating the entity mentions. Classical approaches were based on comparing the text similarity of the context that surrounds a mention against textual descriptions of candidate entities. This can be cast into IR-style bag-of-words or language-model com-parisons, using words, bigrams, noun phrases, and other lin-guistic features. The mentions in a given text are processed one at a time.

Cucerzan [8] was the first to recognize the potential of joint inference by mapping all named entities in a text si-multaneously, aiming to enforce coherence among the chosen entities as expressed by their pair-wise semantic relatedness. Milne &amp; Witten [23], Kulkarni et al. [19], Dredze et al. [9], Ferragina &amp; Scaiella [10], and Hoffart et al. [16] extended and improved this line of methodology, using combinatorial or statistical-learning algorithms for joint inference. The best performing of these NED algorithms utilize the link-based MW measure as defined in Equation (1). None of the methods can robustly cope with long-tail entities that have only few or no Wikipedia links.

NED is a specific case of the more general task of word sense disambiguation (WSD) where all words in a text should be mapped onto canonicalized denotations: entities for pro-per nouns (i.e., names) and WordNet senses (so-called syn-sets) for common nouns and other words or phrases. This task can benefit from semantic relatedness as well. However, we do not consider general WSD in this paper. Navigli [24] surveys state-of-the-art WSD methods. Another task re-lated to but different from NED is cross-document corefer-ence resolution (e.g., [30]). Here, mentions are grouped into equivalence classes, but there is no explicit mapping onto entities in a knowledge base.
All measures of semantic relatedness between entities de-scribed in this section are based on (multi-word) key phrases and their constituting (single-word) key words . When we do not need to distinguish between (multi-word) key phrases and (single-word) key words we speak of key terms .
Example. The keyphrase English rock guitarist is represented by the set of keywords { English , rock , guitarist } .
We gathered keyterms from the Wikipedia pages of each entity: link anchors of both internal and external links, titles of citations , and names of categories . The choice of Wiki-pedia is merely for convenience; keyterms can also be mined from other textual descriptions of entities, such as home-pages of singers or scientists, or pages about songs in online communities. In these cases, we can use similar heuristics like focusing on link anchors, headings, etc., or alternatively extract all noun phrases. The latter will pick up noise, but our weighting scheme, described next, will keep the noise level low.

All keyterms are weighed by a combination of Inverse Doc-ument Frequency (idf), which is global and entity-indepen-dent, and Mutual Information (MI), which depends on the co-occurrence of the keyterms with entities.

Idf weights capture a global notion of how important a keyterm is. The weights are computed using the standard formula, log case the total number of entities in the knowledge base) and df is the number of entities that have the phrase among their keyphrases (for keyphrase idf) or have at least one keyphrase that contains the keyword (for keyword idf).

MI weights capture the notion of how important a key-term is with respect to a given entity. It is based on the rel-ative entropy between the events of an entity-keyterm pair occurring individually or jointly. For its computation, we define the superdocument of an entity to be the union of its keyphrases with the keyphrases of all entities linking to it. The joint occurrence of a keyterm and an entity is the oc-currence of the keyterm in the superdocument. We use this definition in a normalized variant of MI, which is computed as follows: where H ( E ) and H ( T ) are the marginal entropies of the entity and term, respectively, and H ( E, T ) is the joint en-tropy. We compute the  X  weights for all entity-keyphrase and entity-keyword pairs. Note that all weights are with respect to the keyphrase space, not the original texts they were mined from.
If no links are available upon which the entity relatedness can be computed, a baseline method is as follows. We com-pare two entities ( e, f ) represented by keyterm sets T ( e ) T ( f ) by casting the keyterms into a weighted vector. This works for both keyphrases and keywords . If the terms are phrasal in nature, they can either be treated as a sin-gle unit in the vector or tokenized before constructing the vector. In our experiments, the vectors are filled with the MI weights of the keyterms. The relatedness between these vectors can be calculated using cosine similarity. Let ( e, f ) denote a pair of entities with keyterm vectors V respectively. The keyterm relatedness is:
If keywords are derived from keyphrases by tokenization, their weights should take the phrase weights into account. We simply multiply the weights of the words with the aver-age weight of the phrases from which the words are taken. Concepts are often represented as phrases (at least in the English language, but to a large extent also in other lan-guages); so relatedness measures should consider multi-word keyphrases instead of single keywords only. However, when entities are represented by keyphrases, it is crucial to con-sider partial matches rather than focusing solely on exact-match comparisons. For example,  X  X nglish rock guitarist X  should be more similar to  X  X nglish guitarist X  than to  X  X er-man President X . To solve this issue, the relatedness measure needs to match keyphrases based on overlap and needs to take into account both keyphrase and keyword weights. Let ( e, f ) denote a pair of entities with keyphrase sets P e = { p 1 , p 2 , . . . } phrase is identified with the set of constituent terms p { w 1 , w 2 , . . . }  X  ( w ) with respect to the entity e .

In the following, let ( p, q )  X  P phrases. This allows us to introduce a measure of phrase overlap (PO) for the pair ( p, q ) , where its constituent words are weighted with respect to the entities ( e, f ) , given by the weighted Jaccard similarity of the keywords:
We use PO to measure the keyphrase overlap relatedness (
KORE ) of a pair of entities ( e, f ) based on the sets of their associated phrases P spirit of weighted Jaccard similarity while at the same time allowing keyphrases to contribute to the measure based on their overlap with all keyphrases of the other entity. Key-phrases are weighted with respect to the entities; recall that these weights  X  We define the keyphrase overlap relatedness measure: KORE ( e, f ) =
The factor PO ( p, q ) is squared to penalize phrases which do not fully overlap. Our experiments have shown that us-ing MI weights for keyphrases (  X  keywords (  X  phrase overlap PO with the lesser weight of the two phrases that match. The denominator sums up all the keyphrase weights of each entity to normalize the numerator. Notice that we do not normalize by maximum possible intersec-tion, which would be the sum over the full cartesian prod-uct P ization would unduly penalize popular entities with a larger keyphrase set, as the Cartesian product grows much faster than the intersection.
Computing similarities between a set of n objects is an important step in many applications, not only in entity dis-ambiguation, but also in clustering or coreference resolution (record linkage). The KORE measure is relevant for all these tasks, assuming that the input data is a keyphrase set and partial matching improves the quality of the similarity mea-sure.

The naive approach is to compute all n ities. This quickly becomes a bottleneck for large n . Even if the task is parallelizable, overcoming the O ( n 2 ) complexity is necessary to achieve good scalability, especially for inter-active tasks such as on-the-fly NED (e.g., for news streams) or high-throughput tasks such as NED on an entire corpus (e.g., one day X  X  social-media postings).

Precomputing the similarities is prohibitive with regard to both space and time. Today X  X  knowledge bases contain millions of entities. The quadratic space and time com-plexity would lead to more than 10 12 pairs. Even if we merely needed a single byte to store a pair X  X  relatedness value, we would consume Terabytes of storage, and poten-tially much more. This is not practically viable. So the goal is to avoid the quadratic effect and devise appropriate pre-processing that facilitates fact relatedness computations on the fly. To this end, we have developed hash-based ap-proximation techniques, using min-hash sketches [3] and the method of locality-sensitive hashing (LSH), invented by In-dyk et al. [17, 13]. LSH has been used in numerous applica-tions, including clustering in the context of NLP tasks (e.g., [29]). To the best of our knowledge, no prior work has con-sidered such hashing techniques for entity relatedness and NED tasks.
Entities are represented as sets of (weighted) keyphrases, which in turn are represented as sets of (weighted) key-words. Thus, keyphrases should not be compared atomi-cally.  X  X resident of the United States X  should be more sim-ilar to  X  X nited States President X  than to  X  X erman Presi-dent X .

If we want to use LSH to speed up the computation, par-tial matches among phrases must be taken into consider-ation. As a solution, we propose the following two-stage hashing scheme: 1. Bucketize all highly similar keyphrases. Entities are 2. Group entities together that have sufficiently high over-In the following, we explain the two steps in more detail. Figure 1 gives a pictorial overview of our technique.
The goal of this step is to group keyphrases that are near duplicates in the sense that their Jaccard similarity is very high. As a compact representation of the words in a key-phrase, we first compute min-hash sketches for each phrase. e Entities This form of sketches allows us to approximate (as an un-biased estimator) the Jaccard similarity between sets (of words, in our case). To avoid the pair-wise comparison of all keyphrases, we apply LSH on the min-hash sketches of the phrases.

In detail, we first represent a keyphrase by a set of words and assign each word a globally unique id. For each keyphra-se (the average length in our knowledge base is 2.5 words) we sample 4 times by min-hashing, thus representing each keyphrase by a vector of length 4. For LSH, we divide the vectors in two bands of length two, and combine the two ids in each band by summing up their ids, losing the order among them. Each keyphrase is finally represented by the two bucket ids (one per band) it was hashed to, combining near duplicate keyphrases. Note that we do not perform this stage-one hashing to reduce the dimensionality of the keyphrase space, but to capture the notion of partial over-lapping keyphrases in KORE and to improve the second stage of grouping entities.
While the first stage above is precomputed for all entities and their keyphrases (with linear complexity), the second stage, described now, is executed at task run-time with a set of entities as input  X  for NED these are all candidate en-tities in a document. The algorithm first retrieves the sets of min-hash sketches for all input entities, pre-computed from the phrase-bucket ids output by the previous stage. These ids are now the input to a second stage of LSH. Algorith-mically, this is fairly analogous to the first-stage technique, but the inputs are different from the first stage.
The exact KORE measure between two input entities is com-puted only if they share at least one hash bucket. Otherwise, we assume the entity relatedness is sufficiently low to con-sider the entities as unrelated. For the 3 million entities in our knowledge base we can fit the sketches for KORE main memory, merely requiring about 2 GBytes.
 For KORE has nearly the same quality as KORE , we partition the sketched phrase-bucket id vectors into 200 bands of size 1. For KORE a really fast approximation of KORE which degrades the ap-proximation quality a bit, we use 1,000 bands of size 2, again combining the sketched ids by summing them up be-fore hashing. KORE the actual computation is executed between all somewhat related entities, but filters out noise. The speed-up is not so high, but the quality is close to exact KORE . Sometimes, quality is even improved as noisy candidates are removed. KORE LSH-F is geared towards higher precision with bands of size two, allowing LSH to prune even more entity pairs, speeding up the subsequent computation of the semantic relatedness due to fewer comparisons.

As we create the LSH hashtables dynamically during run-time for a given input set of entities, using more bands of larger size means that we need longer min-hash sketches  X  for KORE time for constructing the hashtables. However, in our ex-periments the time difference in creating the candidate pairs using LSH for KORE runtime is improved, especially for large sets of entities.
Existing datasets to evaluate semantic relatedness quan-tify the relatedness between surface forms of words, not be-tween entities registered in a knowledge base, which makes these datasets unsuitable for our task. We have created a new dataset to evaluate the quality of different measures for the relatedness between entities. A conceivable way to cre-ate such data would try to determine numeric scores for the degrees of relatedness (e.g., [11]). However, for most tasks a good ranking among the candidates is important; numeric scores are rarely needed. Moreover, semantic relatedness is difficult to judge by humans in an absolute manner. For these reasons, we settled for relative ranking judgments and use a crowdsourcing platform (Crowdflower) to average out the subjectivity of such judgments.

We selected a set of 20 entities from YAGO2 [15] from 4 different domains: IT companies, Hollywood celebrities, video games, and television series. For each of the 20 seed entities we selected 20 candidates from the set of entities linked to by the seed X  X  Wikipedia article. Using entities from Wikipedia articles allows us to capture the candidates in the context which relates them to their seed entity. We tried to Seed Related Entity (Rank) Apple Inc. Steve Jobs (1), Steve Wozniak (2) ... Johnny Depp Pirates of the Caribbean (1), Jack GTA IV Niko Bellic (1), Liberty City (2) ... New The Sopranos Tony Soprano (1), David Chase (2) ...
Chuck Norris Chuck Norris facts (1), Aaron Norris Table 2: Example seed entities and gold-standard ranks of related entities select the candidates in such a way that their semantic rela-tedness to the seed entity should be clearly distinguishable among each other, and included highly related as well as only remotely related entities in the set of candidates. Example. For the entity Chuck Norris we have United States Air Force and Chun Kuk Do as candidates, described by the following context:  X  X fter serving in the United States Air Force , he began his rise to fame as a martial artist and has since founded his own school, Chun Kuk Do  X . This con-textual information is given to the human judges, together with links to the Wikipedia articles for more detailed infor-mation. The crowdsourcing worker is then asked to rank United States Air Force versus Chun Kuk Do in terms of their relatedness to Chuck Norris . This included a  X  X hey are about the same X  option.

The gold-standard ranking of the 20 candidate entities per seed is created as follows:  X  All possible comparisons of the 20 candidates with re- X  10 of the 190 comparison pairs are created as gold stan- X  For of the remaining 180 comparison pairs, 5 distinct  X  All comparisons are aggregated by Crowdflower into a  X  The 20 candidate entities are then ranked by these con-
The final output is a set of 20 ranked lists consisting of 20 entities each, against which we compare the rankings generated by the semantic relatedness measures. For the previous example, Chun Kuk Do was ranked 5 and United States Air Force 10, reflecting the stronger relatedness of a person to a school of martial arts initiated by him than to an organization he worked for. More examples of entity pairs with their crowdsourcing-derived ranks are given in Table 2.
All seed entities were chosen to be among the most popu-lar individuals in their respective domain ( Apple as IT com-pany, Johnny Depp as Hollywood celebrity, Grand Theft Auto IV (GTA IV) as video game, and The Sopranos as television series). We also added Chuck Norris consisting only of Chuck Norris (a singleton set). The dataset, which contains a total of 441 entities (20 candidates for 21 seeds), is available at http://www.mpi-inf.mpg.de/yago-naga/aida .
Experimental results for this dataset are given in Ta-ble 1. The numbers are the Spearman correlation between the gold-standard ranking and the automatically generated rankings by the relatedness measures.
 The MW measure is by Milne and Witten (Equation (1)). KWCS is cosine similarity between the IDF-weighted key-word vectors derived from the (MI-weighted) keyphrases, KPCS is the cosine similarity on MI-weighted keyphrase vectors. KORE is the keyphrase overlap relatedness, with two configurations of LSH approximations. All measures are de-tailed in Section 3.

We see that all the measures using keyphrases  X  and do not depend on links  X  work better than the link-based MW, with KPCS performing best. The difference becomes even more obvious when we average the Spearman correlation not over all entities but only over those with less than 500 incoming links in Wikipedia (relatively  X  X ink-poor X  entities)  X  here KORE works best. The small size of the dataset did not allow meaningful significance tests, though. We emphasize the point that even without links, the keyphrase-based measures perform at least as good as the MW measure. So we no longer depend on the rich Wikipedia link structure without losing quality, and we are well geared for dealing with truly long-tail entities that have very few or no links at all.
For the NED experiments with different relatedness mea-sures, we modified the AIDA framework [16] to incorporate the KORE measure. Figure 2 illustrates how AIDA represents the mentions in a text document and their entity candidates as a graph.

Mention-entity edges have weights that combine a popu-larity-based prior (for a name denoting a specific entity) and the contextual similarity between a text window around the mention and a set of keyphrases for the candidate entity. Entity-entity edges have weights based on semantic related-ness measures. AIDA uses Milne and Witten X  X  measure of semantic relatedness MW (Equation (1)). This method has outperformed all other coherence-aware alternatives in the experiments of [16]. AIDA does not consider keyphrases for entity-entity coherence. We conducted experiments on three datasets: CoNLL-YAGO . The CoNLL-YAGO data set which is orig-inally used in [16]. It is based on the CoNLL 2003 dataset, which consists of 1,393 newswire articles with an average article length of 216 words. In each article, all mentions are annotated and mapped to the correct entity in YAGO2. All numbers are from runs on the original 231 withheld test documents.
 KORE50 . We hand-crafted 50 difficult test sentences from five domains: celebrities, music, business, sports, and pol-itics. The sentences were formulated according to a set of criteria:  X  Short context: on average, only 14 words per sentence.  X  High density of entity mentions: 148 mentions in 50 sen- X  Highly ambiguous mentions: 631 candidate entities per  X  Sentences contain long-tail entities with very few incom-
Examples are persons referred to by their first name only instead of last name or full name, or football clubs and play-ers that are not well known outside their own home states. It is available at http://www.mpi-inf.mpg.de/yago-naga/aida/ . WP . This dataset is a prepared slice of Wikipedia with sim-ilar characteristics as KORE50. It contains all articles in categories ending with  X  X eavy metal musical groups X . Each article is split into sentences, and only sentences contain-ing at least 3 named entities as link-anchor texts are kept. The link targets of these mentions provides us with ground-truth mappings. As a stress test, to make disambiguation more difficult, we replaced all occurrences of person names with the family name only (e. g.  X  X ohnny Cash X  replaced by  X  X ash X ). In the same vein, we disabled the popularity-based prior for name-entity pairs for this dataset in all methods under comparison. We gathered 2,019 sentences this way, with an average length of 52 words. (The automatic sen-tence boundary detection did not always work properly, so some output segments were longer than expected). Each sentence has an average of 5 mentions with 64 candidates.
We measured NED accuracy as the fraction of mentions that were correctly disambiguated, ignoring all mentions that do not have a candidate entity at all, following the evaluation methodology of [16]. The results are shown in Ta-ble 3: micro-averaged numbers aggregate over all mentions, macro-averaged numbers aggregate over all documents of a dataset. We also broke down the per-mention results by the number of Wikipedia inlinks of the mention X  X  true en-tity. Link-averaged numbers are the macro-average over the groups of mentions with the same number of inlinks.
Overall, the KORE -based NED performed about as well as the original AIDA method, which uses the MW mea-sure based on the rich link structure of Wikipedia. MW performs better on the CoNLL-YAGO dataset, KORE per-forms better on the KORE50 and WP datasets. A paired t-test shows that there is no significant difference between MW and KORE on CoNLL-YAGO or KORE50 for macro-average accuracy; however, KORE is significantly better on the WP dataset ( p -value &lt; 0.01 in a paired t-test). With link-averaged accuracy, KORE and KORE nificantly better than MW on KORE50 ( p -value &lt; 0.01). On the WP dataset, Table 3 additionally gives the average accuracy for mentions whose true entities have few links, to emphasize the accuracy of KORE for long-tail entities.
Figure 3 shows a detailed comparison of MW and the KORE variations (using LSH) on the KORE50 dataset. The accu-racy at each point x on the x -axis is the average accuracy of all entities with up to x links. As expected, KORE works a lot better for really link-poor entities, because it builds on keyphrases instead of links. With more links, the perfor-mance of KORE is still significantly better, but the difference between KORE and MW becomes smaller.

Performing well on link-poor entities is really important considering the fact that entities with  X  500 incoming links make up more than 90% of Wikipedia.
To evaluate the efficiency, we compared the running time of AIDA with the link-based MW measure against AIDA with the KORE measure, both in its exact form and with the LSH approximation for speed-up. We tested on the full CoNLL-YAGO collection consisting of 1,393 newswire arti-cles (216 words on average) with a total of 27,820 mentions marked with an asterisk) (we exclude mentions with no gold standard entity). On average, there are 25 mentions of named entities per arti-cle, and 73 entity candidates per mention. AIDA computes coherence weights only between candidate entities that are candidates for at least two different mentions. If they share only one mention, they are mutually exclusive candidates anyway. Thus, the number of comparisons (and the run-time) is still in the order of n 2 , but it does not increase monotonically with the number of candidate entities for a document. Figure 4 shows the documents on the x -axis and the measured run-times for computing the NED on a docu-ment on the y -axis. The documents on the x axis are sorted in ascending order by the respective number of candidate entities ( X  X argest X  documents are thus rightmost).
We observe that the exact version of KORE is already much faster than MW. This is because MW is based on links, and some popular entities have a large number of incoming links (sometimes above 100,000, e.g., for places). In such cases, the intersection of bitvectors that AIDA uses to represent inlinks is very time-consuming. The number of keyphrases of popular entities does not nearly grow at the same rate, as it is bounded by the length of the document describing the entity. In contrast, incoming links can grow independently of the document size.

Table 4 gives detailed figures for the average run-time, its standard deviation, and the 90%-quantile of the run-time distribution (over the documents), and the same measures also for the number of entity-pair relatedness comparisons. KORE and MW perform the same number of comparisons, but the run-times differ significantly. The accelerated variants of
KORE reduce the number of comparisons by a large margin and reduce the run-time further. In particular, the KORE variant reduces the number of comparisons and the average run-time by an order of magnitude. For the 90%-quantile, which is relevant for being fast on the hardest cases, KORE gains almost a factor of 20.
The keyphrase overlap relatedness measure KORE provides a high-quality notion of semantic relatedness of entities. In our experiments, KORE consistently performed at least as good as the best state-of-the-art methods, and often sig-nificantly better. Most importantly, KORE eliminates the re-liance on explicit linkage between entities, and is thus geared for dealing with long-tail entities with few or no links. The approximate computation by the two-stage hashing method speeds up KORE and makes it suitable for online tasks that require interactive responses. These techniques also boost the throughput of KORE computations when applied in batch mode, e.g., for annotating an entire corpus (news, blogs, etc.) with NED mappings. Our future work will consider ap-
Running time ( ) Computations (millions) plications of these techniques in the growing space of Linked Open Data and for social media on the Web.
We would like to thank Edwin Lewis-Kelham for his help with the WP dataset, and also Mohamed Amir Yosef and Marc Spaniol for their contributions to the KORE50 dataset. [1] E. Alfonseca, M. Pasca, and E. Robledo-Arnuncio. [2] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, [3] A. Z. Broder, M. Charikar, A. M. Frieze, and [4] A. Budanitsky and G. Hirst. Evaluating [5] K. Chakrabarti, S. Chaudhuri, T. Cheng, and D. Xin. [6] R. L. Cilibrasi and P. M. Vitanyi. The Google [7] D. Coppersmith, L. K. Fleischer, and A. Rurda. [8] S. Cucerzan. Large-Scale Named Entity [9] M. Dredze, P. McNamee, D. Rao, A. Gerber, and [10] P. Ferragina and U. Scaiella. TAGME: On-The-Fly [11] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, [12] E. Gabrilovich and S. Markovitch. Computing [13] A. Gionis, P. Indyk, and R. Motwani. Similarity [14] S. Hassan and R. Mihalcea. Semantic Relatedness [15] J. Hoffart, F. M. Suchanek, K. Berberich, and [16] J. Hoffart, M. A. Yosef, I. Bordino, H. F X rstenau, [17] P. Indyk and R. Motwani. Approximate Nearest [18] A. Kotov and C. Zhai. Tapping into Knowledge Base [19] S. Kulkarni, A. Singh, G. Ramakrishnan, and [20] M. Li, X. Chen, X. Li, B. Ma, and P. M. B. Vit X nyi. [21] R. Mihalcea and P. Tarau. TextRank: Bringing Order [22] D. Milne and I. H. Witten. An Effective, Low-Cost [23] D. Milne and I. H. Witten. Learning to Link with [24] R. Navigli. Word Sense Disambiguation: A survey. [25] P. Pantel and A. Fuxman. Jigs and Lures: Associating [26] S. P. Ponzetto and R. Navigli. Knowledge-Rich Word [27] S. P. Ponzetto and M. Strube. Knowledge Derived [28] K. Radinsky, E. Agichtein, E. Gabrilovich, and [29] D. Ravichandran, P. Pantel, and E. Hovy.
 [30] S. Singh, A. Subramanya, F. C. N. Pereira, and [31] R. Sinha and R. Mihalcea. Unsupervised Graph-based [32] F. M. Suchanek, G. Kasneci, and G. Weikum. YAGO: [33] T. Zesch, C. M X ller, and I. Gurevych. Using
