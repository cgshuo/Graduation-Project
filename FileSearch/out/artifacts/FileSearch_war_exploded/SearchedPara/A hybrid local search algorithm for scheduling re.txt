 1. Introduction
The standard job shop scheduling problem (JSSP) has been widely adopted as a model in the research of combinatorial optimization algorithms. There are several objective functions to be considered in theoretical investigations, of which the most frequently studied is the criterion of makespan (a.k.a. maximum 2008 , 2009 ). However, due-date-related performances are becom-ing increasingly important for manufacturing firms nowadays, because timely delivery of goods is closely related with service reputation (a source of sustainable profit). For this reason, the objective functions based on due dates are much more mean-ingful and relevant for practical scheduling than the makespan.
The total weighted tardiness (TWT) criterion, which we will consider in this paper, is one such objective.
 Although some purely theoretical research has been done on JSSP with the objective of minimizing TWT (abbreviated as TWT-there are two aspects where the theoretical model significantly differs from real-world production environments. 1. The specification of due dates : In theoretical TWT-JSSP models, the due date of each job is an exogenously determined factor, which must be treated as an input to the algorithm used for solving the TWT-JSSP ( Zhou et al., 2009 ). However, in order to cope with the fluctuating market demand under the back-ground of economic crisis, most modern companies are adopt-ing the make-to-order (MTO) production mode in place of make-to-stock. In MTO, the factory will not manufacture or assemble the final products until it has received official orders from its customers. Actually, when a customer places an order, the firm must  X  X  X uote X  X  a due date (sometimes referring to a lead time) together with a price for this order. Of course, if the customer is not satisfied with the initial due date (price) quotation, there may arise several rounds of negotiations between the firm and the customer until consensus is reached, or otherwise, the customer may simply turn to another supplier and this firm will lose the business. In other words, for real-world manufacturing firms, due dates are not solely determined by the outside customers but ultimately set according to the will of both the manufacturer and the customers. Sometimes, the role of the company itself is even greater if the company maintains a monopoly position in the industry. Finally, the point is that, due dates should be regarded as decision variables rather than unchangeable input when we consider real-world scheduling. assumed to be independent of one another, and each job can have mutually unrelated property values (esp. in randomly generated test instances). In real MTO environments, an order that the firm receives usually includes a number of (different) products ordered at the same time. In other words, a set of jobs arrive simultaneously at the firm and require to be processed as a whole. For the convenience of the customer, it is generally demanded that the set of jobs should be delivered on the same day (i.e., they will have the same due date in the scheduling system). In addition, these jobs may have identical values for other properties, e.g., the weight which describes the impor-tance level of the job to the manufacturer (identical weights because they all come from the same customer). Such a set of jobs is called a batch in terms of scheduling. The production scheduling process must guarantee that the completion times of a batch of jobs should not be far apart from each other, or else there will arise inventory holding costs for the jobs finished considerably earlier than the others in the same batch.
Therefore, the existence of job batches requires some special treatment in the design of a real-world scheduling algorithm.
From the above point (1), we see that due date quotation is actually a very important decision process on the part of the manufacturer ( Lucas and Moses, 2005 ). The due dates for most orders are initially promised and then fixed by MTO firms themselves rather than the external customers. In this process, the due date setter has to make a trade-off between delivery Y  X  uz  X  ug  X  ull  X  u, 2009 ):
If the firm quotes a lead time which is notably shorter than that of its competitors, then it will have more chance of obtaining the order. But at last it may probably fail to deliver on time because the tight due date cannot be achieved by the production scheduling system, and consequently, the firm X  X  goodwill might be damaged.

On the other hand, if the firm quotes a long lead time, it can keep its timely delivery reputation with a higher level of confidence. But it may have already lost the order in fierce market competition.

So, the process of setting the due date can be defined as an optimization problem.

For the optimization of theoretical job shop schedules, the only contributing factor is the quality of the scheduling algorithm, since the fixed due dates cannot be adjusted. Exactly for this reason, scheduling has become a remarkably important research field ever since the 1950s ( Akers, 1956 ), and is currently one of the hottest topics in the operational research society ( Jain and
Meeran, 1999 ; Minella et al., 2008 ). However, in real-world scheduling, due dates are controllable decision variables, which certainly complicates the optimization task but also provides opportunity for enhancing the overall performance of the manu-facturing system. Therefore, in practical scheduling, the quality of due date setting and the quality of scheduling simultaneously determine the performance of manufacturing. Previous research has shown that due date optimization is not independent of scheduling. Actually, if the due dates have not been set in a rational way, the final scheduling performance will be unsatisfac-tory, no matter how superior the scheduling algorithm is ( Vig and
Dooley, 1991 ). For this reason, we will consider the two decision problems integrally in one optimization model.

From the above point (2), we see that it is necessary to incorporate the batch information into the scheduling algorithm. Especially, two aspects have to be considered:
A mechanism has to be designed to synchronize the produc-tion of a batch (the aim is to make the completion time of each job in the same batch close enough). To this end, we have devised a new dispatching rule for practical scheduling, which attempts to make each job keep in step with the production progress of the other jobs in the same batch.

Although the jobs have been grouped into batches and a batch of jobs must have the same due date setting, each job is still independently processed on the production line. Therefore, we must decide when to consider a batch of jobs together and when to examine each job separately. In this paper, we propose a two-stage algorithm, where the first phase optimizes the batch-wise due dates in a rough manner while the second phase fine-tunes the production schedule in a job-by-job mode.
In this research, we consider a firm (the actual prototype is a large-scale speed reducer manufacturer in Pingyao, Shanxi Pro-vince of China) which has accepted a number of orders, each consisting of many jobs. The due dates for some of the jobs have already been assigned, possibly due to the fact that such orders have strategic importance to the firm, and therefore, their due dates have been set according to the customers X  wishes and thus cannot be modified. For the remaining jobs, the firm X  X  scheduler (scheduling system) has to decide their due dates based on comprehensive information about the current production status of the workshops. In sum, the integrated scheduler must handle due date assignment and job shop scheduling at the same time.
The output of the scheduling system includes the optimized due date values for each batch of unassigned jobs and an optimized processing sequence of relevant jobs for each machine.
The contribution of the paper is twofold. First, it presents an optimization model for characterizing the features of real-world job shop scheduling. Pending due dates and batch information are also taken into consideration. Second, it proposes a local search-based a special type of genetic algorithm which belongs to estimation of distribution algorithms, and the second-stage algorithm is a local search procedure based on systematic parameter perturbation.
The rest of this paper is organized as follows. Section 2 presents a brief introduction to job shop scheduling meta-heur-istics and the basic theories on estimation of distribution algo-rithms. Section 3 defines the integrated due date assignment and job shop scheduling problem. Section 4 describes the proposed optimization approach in detail. Section 5 presents the computa-tional results on the real running data of the Pingyao speed reducer factory and some related discussions. Finally, Section 6 concludes the paper. 2. Literature review 2.1. The job shop scheduling problem and total weighted tardiness objective
In a standard JSSP instance, a set of n jobs J  X f J j g n processed on a set of m machines M  X f M h g m h  X  1 under the following basic assumptions: (i) There is no machine breakdown. (ii) No preemption of operations is allowed. (iii) The transporta-tion time between different machines and the setup time between different jobs can be neglected. (iv) Each machine can process at most one job at a time. (v) Each job may be processed by at most one machine at a time.

Each job has a fixed processing route which traverses all the machines in a predetermined order (the manufacturing process of a job on a machine is called an operation). The duration time of each operation is fixed and known. In the theoretical model, a preset due date d j and a preset weight w j are given for each job.
Due dates are the preferred latest finishing time of each job, and completion of the job after this specific time will result in losses such as a worsened reputation in customers. Weights reflect the importance level of the orders from different customers, larger values suggesting higher strategic importance. If we use C denote the completion time of job j , the minimization objective of  X  L max  X  max n j  X  1 f C j d j g X  , total weighted tardiness  X  TWT  X 
P  X  U  X 
Up till now, most research on JSSP has been focused on the makespan criterion. However, due-date-related performances are becoming more significant in the customer-centered market envir-onment nowadays. In this sense, the total weighted tardiness of a firm more accurately than makespan. Meanwhile, from the theoretical perspective, the total weighted tardiness is more diffi-cult to optimize than makespan. According to the concepts of computational complexity ( Pinedo, 2008 ), minimizing C max a special case of minimizing TWT , which means the complexity of solving JSSP with total weighted tardiness objective is much greater than that of solving JSSP with makespan. Despite its importance,
The only exact solution methodology is the branch-and-bound algorithm proposed by Singer and Pinedo (1998) . 2.2. A brief introduction to local search methods for JSSP
Local search (LS) is a generic methodology for searching the solution space. The underlying idea of LS is quite straightforward: a given solution may be improved by a consecutive series of small modifications. For each solution v in the solution space V , the neighborhood function N defines the neighborhood of v . In other words, N  X  v  X  is the set of neighbor solutions which can be reached from v . In general, a local search algorithm defines a walk in V such that each visited solution is a neighbor of the previously visited solution ( Vaessens et al., 1996 ).

The quality of a local search algorithm is directly affected by two factors, i.e., the definition of the neighborhood function and the method of selecting the next solution from the neighborhood.
As for the first issue, there is usually a trade-off in the size of neighborhood. A large neighborhood may include high-quality solutions, but finding such a good solution requires more compu-tational time. A small neighborhood is easy to perform an exhaustive search, but the improvement by each neighborhood move may be too trivial. In fact, the definition of neighborhood is rather problem-dependent and objective-dependent. So even for
JSSP, the neighborhood structure for minimizing makespan  X  C should be somewhat different from the neighborhood used for minimizing tardiness. The quality of neighborhood is critical for such difficult combinatorial optimization problems as JSSP and thus this topic attracts consistent research interest.
As for the second issue, different criteria exist for selecting the next visited solution. The simplest of all is the hill-climbing approach which always selects an improving solution from the current neighborhood. However, such a strategy makes the search process converge to local optima, which usually have poor quality. Alternatively, recent meta-heuristics such as simulated annealing (SA), tabu search (TS) and genetic algorithms (GAs) use special mechanisms in order not to get trapped by local optima.
GAs were inspired by the principles of natural selection and population evolution. GAs can be viewed as a special case of local search methods. Different from those serial local search algo-rithms (like SA and TS), GAs generate a new solution from two previous solutions (rather than one immediately previous solu-tion), and maintain a population of solutions (rather than one solution) in the searching process. So GAs are said to have an inherent parallel property. In fact, GAs are perhaps the maturest meta-heuristic for both continuous function optimization and combinatorial optimization. A vast number of papers dealing with the application of GAs to JSSP exist in the literature. Cheng and Gen (1996 , 1999) provide a systematic survey on this topic. 2.3. Basics of estimation of distribution algorithms (EDA)
Recently, there has been a growing interest in the evolutionary algorithms that explore the search space by building and utilizing probabilistic models of high-quality solutions. Indeed, these algorithms use the following two steps to replace the conven-tional crossover and mutation operators in GA: (1) Build a probabilistic model of the selected promising (2) Sample the built model to produce a new generation of The evolutionary algorithms based on such a principle are referred to as estimation of distribution algorithms (EDAs) or probabilistic model-building genetic algorithms (PMBGAs). The major steps of a PMBGA implementation are listed as follows, where GN is the maximum number of generations.
 Step 1: Set the generation index g  X  0. Initialize the population of Step 2: Select a subset S of promising individuals from P  X  g  X  . Step 3: Establish a probabilistic model M which somehow Step 4: Generate a set N of new individuals by sampling M .
The PMBGA is especially useful for the complex optimization problems in which the decision variables are correlated. For such problems, the realized value of a certain decision variable can produce an impact on the optimal value for another decision variable. Therefore, if these variables are optimized in a separate way (or one by one), traditional GA will be very likely to converge to local optimum. PMBGA improves traditional GA by modeling the relationship between decision variables. Clearly, the most crucial element in the design of PMBGA is the type of the adopted probabilistic model (in Step 3), which directly affects the algo-rithm X  X  capability of producing high-quality offspring solutions. In the artificial intelligence community, the commonly adopted graphical model for characterizing the relationship between a set of discrete variables is the Bayesian networks ( Heckerman et al., 1995 ). A Bayesian network is a directed acyclic graph. A node in the Bayesian network indicates a variable under investi-gation (each variable may actually correspond to a coding gene for possible solutions in PMBGA), and an arc indicates the probabil-istic causal relationship between the two nodes connected by it. The direction of the arc implies that the variable corresponding to the head node of the arc is conditioned by the variable corre-sponding to the tail node. An example is given in Fig. 1 . network in Fig. 1 have the following joint distribution function: P  X  x 1 , x 2 , ... , x 6  X  X  P  X  x 1  X  P  X  x 2 j x 1  X  P  X  x 3 j x denotes Pr  X  X i  X  x i  X  and P  X  x i j x j  X  denotes Pr  X  X holds because a pair of variables which are not directly connected by any arc are assumed to be mutually independent. From this example, it is evident that Bayesian networks can reduce the required storage space for describing the joint distribution of random variables. If X 1 , ... , X 6 are all binary variables, i.e., x can factor the joint distribution into conditional distributions by utilizing the existing relationship between the variables. Hence, according to the right-hand side of Eq. (1), only 2  X  4  X  4  X  8  X  4  X  4  X  26 values are necessary for describing their joint distribution.

In general, the joint probabilistic distribution of an n -variate x  X  X  x there exists a directed arc pointing from node X j to X i a parent of X i . Regarding the example in Fig. 1 , X 2 and X of X .)
A detailed introduction to the PMBGA can be found in Pelikan et al. (2002) . Some important advances and interesting applica-tions of EDA are covered in Larran  X  aga and Lozano (2002) and
Lozano et al. (2006) . Successes in utilizing EDA to solve schedul-ing problems have been reported in Tsutsui and Miki (2002) and
Jarboui et al. (2009) (for flow shop scheduling). 3. Problem description
The newly defined model incorporates due date assignment (DDA) and TWT-JSSP into one optimization problem. At the beginning, there are a number of jobs waiting to be scheduled.
This set J of unscheduled jobs can be further divided into two subsets according to their due date quotation status: the jobs with already designated due dates belong to subset J 1 , while the jobs whose due dates are to be determined constitute subset J (so J  X  J 1 [ J 2  X  . Furthermore, the jobs within J 2 are grouped into totally G batches, i.e., J 2  X  B 1 [ B 2 [[ B G , and each batch
B contains b g jobs, i.e., B g  X f j g , 1 , j g , 2 , ... , j tion model for the integrated scheduling problem can be math-ematically characterized as follows (where x  X  max f x , 0 g X  : min Z  X  P s : t : 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :
The meaning of the notations used in the above formulation are listed below: Decision variables in the optimization problem:
J s i (corresponding to scheduling ): the starting time of opera-
J d g (corresponding to DDA ): a decision variable indicating Order/job related information: J O  X  j  X  : the set of job j  X  X  operations.
 J p i : the processing time of operation i .

J C j : job j  X  X  completion time, which should equal the finish-
J  X  X   X  i 1 -i 2  X  A A  X  J  X   X  X  denotes the fact that operations i
J  X  X   X  i 1 2 i 2  X  A E  X  J  X   X  X  denotes the fact that operations i Parameters set by the manufacturer:
J w g : the weight of each job in batch g , which reflects the
J o g : a penalty coefficient which measures the potential cost
J v Z 1: a constant which depicts the growing pattern of the Parameters set by the customers:
J d j (for j A J 1  X  : the predetermined due date for job j in J
J b g : the expected due date of batch g (expectation of the J d max g : the maximum value for d g accepted by the customer.
According to the definition of the objective function Z in Eq. (2), tardiness costs are incurred when the completion time of a job is later than its due date, no matter whether the due date is preset outside the discussed problem range (i.e., d j ) or determined within this optimization process (i.e., d g  X  . Moreover, for each unassigned batch of jobs, if we set a due date which is within the time range  X  0 , b g (which exceeds the customer X  X  expectation on due date is large enough there will arise DDA-related penalties.
Constraint (a) defines the completion time of each job, which is equal to the finishing time of its last operation. Constraint (b) guarantees the precedence relations between any two operations from the same job. Constraint (c) ensures that each machine can process at most one job at a time (without overlap in processing
The major differences between the problem studied in this paper and the ones in other existing works can be stated as follows: (1) The problem discussed in this paper integrates the optimiza-tion of operation sequences together with the optimization of due date settings. By contrast, in most existing literature on
DDA methods, the sequencing of operations is not an optimi-zation target but is directly achieved by simple dispatching rules, and thus, optimality cannot be ensured. (2) Our model includes a portion of jobs with already fixed due dates. Thus, this optimization model may be applied to a dynamic rescheduling environment, where the jobs with 4. The algorithm design
The proposed solution framework for the integrated optimiza-tion problem consists of two functional modules: (1) The PMBGA-based algorithm which implements coarse-gran-(2) The parameter perturbation (PP) based algorithm for fine-4.1. The DDA-PMBGA algorithm
In DDA, the due date setting for one job may have an impact on the optimal due date setting for another job. For example, if some important jobs have been assigned with tight due dates, then the reasonable due date quotation for other jobs should be postponed to certain extent (because the overall production capacity of the system not mutually independent. Based on this observation and the fact that
PMBGA is excellent in modeling the relationship between the variables to be optimized, we use PMBGA in this stage for roughly optimizing the batch-wise due dates. In PMBGA, the due dates for different batches are optimized not in a separate manner but based on their potential interaction.

Before the proposed optimization process, the due date for each batch in J 2 is factored as d  X  k g t TP g :  X  3  X 
In this expression, TP g  X  P j A B adjustment during the rough optimization of due dates. With such factorization, the target that DDA-PMBGA attempts to optimize is necessary to care about the absolute scale of job sizes. Meanwhile, such factorization also discretizes the continuous d g values such that the resulting k g on which PMBGA works are all integers (since Bayesian networks can only handle discrete variables). In the following, we will describe the key elements of DDA-
PMBGA. 4.1.1. Encoding
The coding length for each individual in DDA-PMBGA is l  X  G because G due date coefficients have to be optimized. Each coding digit is an integer which records the value of k g for batch g in J
Based on the integral encoding scheme and the factorization of d by Eq. (3), as well as the requirement that  X  X  d g r d max domain for k g is t TP g  X  X  k () This also reflects the coarse-granularity feature of the DDA-PMBGA optimization process because only the values at these discrete points controlled by the fixed step size t are considered in space for k g , resulting in more accurate due date setting but longer computational time. So a trade-off must be obtained with respect to t . 4.1.2. Selection
In each iteration, we first sort all the individuals in the current population according to their fitness and then select the best e %of individuals to form the set S , which will subsequently be used to build the Bayesian network and produce a new generation of individuals. 4.1.3. The network structure
A Bayesian network has to be built to describe the probabilistic S . Each individuals can be characterized by a directed acyclic network as shown in Fig. 2 . In this network, a node N g , k  X 
A f 1 , ... , G g , and for each g , k A f k min g , ... , k max that the due date coefficient k g has been set as k for batch g . The directed arc from node N g , k to node N g  X  1 , k 0 represents the dependency between the two nodes, so it characterizes the possible influence of setting k g  X  k for batch g on the setting of from a certain node in the first row to a certain node in the G -th row can completely describe an individual in the population (because a directed path records an instantiation of all the k values). 4.1.4. The probabilistic model
Since we adopt a fixed network structure in DDA-PMBGA, building the Bayesian network is equivalent to determining the values of all the conditional probabilities according to the selected solution set S . After that, new individuals will be produced by iteratively sampling these probabilistic distributions, expecting to obtain high-quality offsprings.

Given a number of individuals (i.e., the training set S ), an estimate of the conditional probabilities can be obtained simply by counting the frequencies of occurrence. Here we provide a concrete example to illustrate the calculation process. For a DDA-PMBGA optimization instance with G  X  3 and t  X  1, let us further individuals are displayed on a network as previously defined. The weight of each arc (the number placed on the arc) indicates the occurring frequency of this arc (counted for all the individuals in k  X  3, k 2  X  1 and k 3  X  2), then the path  X  X  N 1 , 3 -N 2 , 1
Bayesian network is used to record this individual, and conse-quently, the weights (counted frequencies) of the arcs N 1 , 3 the final network, the sum of the weights of all the incoming arcs of a certain node should be equal to the sum of the weights of all the outgoing arcs of the same node. This is because each individual corresponds to a complete path from the first row to the last row.

By using frequency as an approximation for probability, the relevant (conditional) probabilities should be calculated as fol-lows:
P  X  N
P  X  N
P  X  N
P  X  N
P  X  N
P  X  N
P  X  N
P  X  N
According to the above calculation method, a connection can never be re-discovered in the DDA-PMBGA if the corresponding conditional probability is zero (e.g., from N 2 , 2 to N 3 , 1 come this drawback, we can set the minimum count to 1. Taking
N as an example, the conditional probabilities for the outgoing arcs will then become
P  X  N
P  X  N ability is small. 4.1.5. The sampling process
The sampling process for generating a new individual begins from the root nodes of the Bayesian network and gradually instantiates the whole network by selecting the outgoing arc at each node based on the calculated conditional probabilities. 4.1.6. Decoding
First, we calculate the due date of each batch in J 2 using the f k g g  X  1 values indicated by the current individual, i.e., d  X  k g t TP g .

Then, considering the characteristics of the objective function and the requirement of real-world scheduling, we devise two dispatching rules that can be used to obtain a feasible schedule in relation to the current solution. (I) The weighted minimum slack rule : The slack time for job j A J 1 is defined as y  X  d which reflects the processing urgency level of the considered job (smaller y j implies higher urgency of job j ). The conventional minimum slack (MS) rule always selects the job with the shortest slack time to be processed when several jobs are waiting in the machine X  X  buffer. Here we define the weighted minimum slack (WMS) rule. In WMS, the priority index for job j A J evaluated as  X  X  1  X  w j  X  X  2 y j  X  , where w j A  X  0 , 1 is the normalized weight of job j , and y the normalized slack time of job j , and normalization means x x
A f y , w g .

Likewise, we can define a WMS priority index for a batch of jobs in J 2 . First, the slack time of batch g is defined as y  X  B  X  . The lower bound of makespan can be obtained in a number of ways. Here we use the single machine relaxation to calculate the lower bound, which is a computationally fast method:
Step 2: Let LB C max  X  B g  X  X  max f C max  X  1  X  , ... , C
Consequently, the priority index for batch g ( B g J 2  X  is similarly defined by  X  X  1  X  w g  X  X  2 y g  X  , where w g A  X  0 , 1 is the normalized weight of batch g , and y is the normalized slack time of batch g , and here normalization means x g  X  X  x g x min  X  =  X  x max x min  X  , x min  X  min G job in B g has the same WMS priority index, which is equal to r (II) The batch synchronization rule : In order to ensure that the jobs within a batch are completed closely in time, we define the batch synchronization (BS) rule.

First, the production progress rate of job j A B g at the current time t is defined as
PR  X  t  X  X  where f i denotes the finishing time of operation i . So the index PR represents the proportion of completed operations to all opera-tions in job j .

Meanwhile, we denote by RO j  X  t  X  the number of remaining (unscheduled) operations of job j A B g at the current time t . RO provides a different measurement for the unfinished workload than  X  1 PR j  X  t  X  X  . If the number of remaining operations is large (even if each remaining operation requires very short processing time), there will be more uncertainties in the following manu-facturing process (because each unscheduled operation will participate a competition with other candidate operations). So, larger RO j  X  t  X  values indicate higher production urgency level.
When two or more jobs from the same batch are competing for machine processing, it is beneficial to select the job whose production progress has lagged behind its counterparts, or the job which has a larger number of subsequent operations. The motivation is to synchronize the completion times of the jobs in a batch so as to minimize inventory costs. To achieve this, we define the priority index for the BS rule as r 0  X  X  1  X  RO j  X  X  2 PR j  X  , values of RO j and PR j , and here normalization means x x
A f RO , PR g . (III) Application of WMS and BS : In order to evaluate the fitness of an individual (i.e., a rough DDA policy), we try to obtain a complete and reasonable schedule under such a due date setting.
In the process of constructing the schedule, WMS and BS are applied in a hierarchical manner. When a number of jobs are ready for processing, we first evaluate their priority values under each job from the same batch in J 2  X  , and identify the largest one (noted as r max  X  .If r max corresponds to a single job j select job j as the next job to be processed. Otherwise if r shared value for several jobs from a certain batch, then evaluate the BS priority values for each of the corresponding jobs and only one job from that batch, of course we do not need to calculate the BS priority). In this way, we can produce a feasible schedule and obtain each job X  X  completion time C j . 4.1.7. Evaluation
Once decoding is done, the completion time of each job ( C known and this is used to evaluate the first item of Z (i.e.,
P
It is noticeable that, after the schedule has been fixed, the due date of job j  X  j A J 2  X  could then be modified in order to further minimize the last two items of the objective Z (i.e., P date is noted as ^ d . The optimal due date adjustment policy (i.e., obtaining ^ d g for each batch in the decoding process) is described as follows.

We now consider only one batch of jobs. The subscript  X  X  g  X  X  can be omitted in this part because the following procedure will be
Recall that b jobs belong to this batch, and they have the same non-penalty due date b , the same penalty coefficient o and the same weight w . The objective is to determine a common due date ^ d for these jobs such that the following cost value is minimized: Z  X  z b  X  d  X  X  w can set ^ d  X  b under this situation. In the following, we will assume that there exist at least one C i which satisfy C i a sequence C  X  b L , ... , C  X  2 , C  X  1 , b , C  X  1 , C b 4 0  X  . That is to say, there are b L jobs completed before the expectation due date b and b R jobs completed after b . Thus, the piecewise function z b  X  d  X  can be reformulated on each of the intervals  X  C  X  i 1 , C  X  i  X  i  X  b L , ... , b R  X  with C
When i A f b L , ... , 0 g , z b  X  d  X  can be rewritten for interval  X  C z  X  d  X  X  w function with a negative slope  X  b R i  X  w which increases with i . (of tangent). Now that z b  X  d  X  is downward sloping when d r b and can only be found in  X  b , C  X  b R .

When i A f 1 , ... , b R g , z b  X  d  X  can be rewritten for interval  X  C z  X  d  X  X  w By differentiating z b  X  i  X  d  X  : d z d d  X  X  b and assuming d z b  X  i = d d  X  0, we get  X  b  X   X  b Note that the d %  X  i may or may not lie within the domain of z to find the global minimum of z b  X  d  X  .
 Step 1: Initialize i  X  1.
 Step 4: If d %  X  i 4 C  X  i , then set i  X  i  X  1 and go to Step 2.
Step 5: If d %  X  i o C  X  i 1 , then the global optimum is C
The algorithm output is definitely the global minimum of z b  X  d  X  because d %  X  i decreases with i . If for a certain i 0 , d % 8 i ^ d is a  X  X  X orner X  X  solution (Step 5), which is depicted in Fig. 4 (b).
A special case is v  X  1, and each segment of z b  X  d  X  is a linear function (see Fig. 5 ). On interval  X  C  X  i 1 , C  X  i  X  i be written as z  X  d  X  X  X  b o  X  b R i  X  1  X  w d  X  w
Hence, the slope of this line segment is k  X  i  X  b o  X  b R above minimization algorithm should be correspondingly mod-ified as follows.
 Step 1: Initialize i  X  1.

Step 2: If i 4 b R , then return ^ d  X  C  X  b R . Otherwise, calculate Step 3: If k  X  i  X  0, then return ^ d  X  C  X  i .
 Step 4: If k  X  i o 0, then set i  X  i  X  1 and go to Step 2. Step 5: If k  X  i 4 0, then return ^ d  X  C  X  i 1 .

Finally, after ^ d g is obtained for each batch using the above procedure, the objective value Z could ultimately be calculated (substituting the d g in Z with ^ d g  X  . Z is directly taken as the evaluation of the current individual, and a smaller Z suggests higher solution quality. 4.2. The OSA-PP algorithm
After obtaining a rough due date value for each unassigned batch with DDA-PMBGA, we use a fine-granularity optimization procedure (called OSA-PP) to fine-tune the final solutions in an attempt to further improve the solution quality. The input of OSA-
PP is a set of k g  X  g  X  1 , ... , G  X  values as output by DDA-PMBGA, which indicate that the due date for each job in batch g has been set as d g  X  k g t TP g . On such a basis, OSA-PP conducts the search for a better schedule of all the operations that belong to J .
The fundamental idea underlying OSA-PP is that, by means of perturbing the due date values of some jobs, the production schedule obtained under a given (fixed) due-date-related dis-patching rule will change, which can be used as a local search mechanism. Furthermore, if the directions of successive perturba-tions are not completely random but determined partly based on the knowledge gained from previous experience, then this method can have satisfactory searching performance with respect to both solution quality and computational time ( Agarwal et al., 2006 ).

In order to guarantee the effectiveness of fine-tuning, we allow the due dates of the jobs in one batch to be perturbed indepen-dently. Before OSA-PP begins, the due date of each job j A initially inherited from its belonging batch g  X  j  X  , i.e., d after we exert perturbations on each d j  X  j A J 2  X  independently, each job in j A J 2 will have separate and distinct WMS indices r one batch). Perturbing the due date of each job in one batch separately is a main feature of OSA-PP against DDA-PMBGA. This slight change brings a higher degree of freedom in search directions so it can provide a reasonably large neighborhood for local search.

Below is the detailed procedure of such a local search algorithm  X  OSA-PP, which relies on random perturbations to explore the neighborhood and involves a parameter learning process similar to that of artificial neural networks for guiding the search direction.

Step 1: Initialize the iteration index: u  X  1. Let k  X  u  X 
Step 2: Evaluate the (perturbed) due dates in the current itera-
Step 3: Apply the WMS  X  BS rule 3 (in which the due date of each Step 4: If Z u is the best objective value achieved so far, then set Step 5: If u  X  u max , go to Step 10.

Step 6: If u 4 1 and the best objective value has been improved,
Step 7: If u 4 Q and Z has not been improved during the most
Step 8: Generate a random number x from the uniform distribu-Step 9: Let u  X  u  X  1 and go to Step 2.

Step 10: Output the results: the best schedule S n , and its corre-
The parameters of OSA-PP include: u max : the total iteration number; l : the reinforcement step size;
Q : the allowed number of iterations without any improvement; a A  X  0 , 1 : the amplitude of random perturbation.

In OSA-PP, Step 6 applies a reinforcement strategy when the current perturbation direction is beneficial for improving the overall objective value. Step 7 is a backtracking policy which restores the solution to the best-so-far when the latest Q pertur-bations do not result in any improvement. Step 8 imposes a random perturbation on the current solution to explore its neighborhood region. In Steps 6 and 8, the reinforced or perturbed new k j values should be bounded by  X  k min g  X  j  X  , k max 5. Computational results and analysis 5.1. Experimental setup
To test the effectiveness of the proposed two-stage optimization algorithm (referred to as TSO below), experiments are carried out for real-world instances from the Ping yao speed-reducer factory located in Shanxi, China. All related algorithms have been implemented in Visual C  X  X  7 and tested on an Intel Core2 Duo 2.2 GHz/2 GB RAM/
Windows XP platform. Next, we will introduce the parameter setting for TSO and the characteristics of the scheduling instances. 5.1.1. Algorithm related information
The parameters of the proposed TSO have been determined by empirical studies and are listed as follows: Parameters of DDA-PMBGA: J The population size PS  X  50.

J The generation number GN  X  500 (maximum), or deter-J The step size t  X  0.15.
 J The percentage of individuals selected for building the Parameters of OSA-PP:
J u max  X  1000 (maximum), or determined by the externally J l  X  3.
 J Q  X  15.
 J a  X  0 : 3.
 Other parameters: J The time allocation between DDA-PMBGA and OSA-PP:
Meanwhile, for comparative purposes, we use two other algorithms to give the baseline for performance evaluation (the schedule builder in these two comparative algorithms is based on the WMS rule only, and ties are broken randomly):
GLS (a hybrid genetic local search algorithm proposed by Essafi et al. (2008) for TWT-JSSP). As Section 4.1.7 shows, when the production schedule (i.e., operation sequence) is fixed, the optimal due date settings ^ d g can be thought of as a function of the schedule. So the GLS adopted here only searches in the space of operation sequences (i.e., each solution in GLS is a sequence of all the operations). The optimal due date for each batch is determined in the evaluation stage before calculating the ultimate objective value.

TS/SA (a nested combination of tabu search and simulated annealing proposed by Zhang et al. (2008) for JSSP). The TS/SA method used here has been modified as follows in order to better suit our problem. The outer iteration (TS) performs a search for the
G due date settings of each batch in J 2 while the inner iteration (SA) optimizes the operation sequence under every set of due dates chosen by TS. 5.1.2. The real-world test instances
These production instances are extracted from a large-scale speed-reducer manufacturing enterprise in China. The factory manufactures many types of speed-reducers to be used in various industries layout and routes. The required processing time of each operation ranges from 0.5 to 150 min (with satisfactory precision). The number of schedulable machine cells is around 42 (according to the type of jobs to be processed). To give only a small example, the manufactur-ing route of a certain part type is illustrated in Fig. 6 .Fromthis example, some additional characteristics can be found in the real manufacturing data: (1) There exist reentrant jobs on the production line. For example, the  X  X  X ough turning X  X  and  X  X  X inish turning X  X  are performed on the same machine (ID: 101). This feature will not affect the functioning of our algorithm since we have used a rule-not need to traverse all the machines to complete processing. Hence, thetotalnumberofoperationsintheseinstancesislessthan n m . (3) Another common feature in real-world scheduling scenarios is thatthereexistquiteanumberofide ntical jobs. Therefore, although the number of jobs to be scheduled in each instance is large (compared to theoretical benchmar k instances), the complexity for optimization is still within the affordable range.

In the investigated speed-redu cer factory, the planning staff (under the general management department) have to make both DDA and job sequencing decisions when new orders arrive. Consistent with our model, a  X  X  X atch X  X  of jobs correspond-ing to the same order have to be considered together. The test instances used in this section are extracted from the orders accepted by this factory during different months. The charac-teristics of these real-world instances are described as follows: v  X  2, which means the penalty cost grows as a square of  X  d of losing orders increases acceleratedly with the promised lead time.

The maximum acceptable due date d max g and the expectation due date b g for each job batch g are all set according to the customers X  preferences (which have been recorded in the process of order placement).

The weight  X  w g  X  and the due date penalty coefficient  X  batch are measured by three levels: w g  X  o g  X  9formost important customers (accounting for about 30% of orders for the studied reducer factory); w g  X  o g  X  5 for ordinary customers (accounting for about 55% of total orders); w g  X  o g  X  0 : 5forthe customers who hold a passive position in the supply chain (accounting for about 15% of total orders).

The allowed computational time fo r the three algorithms to solve each instance is 300 s. The aim of imposing the same time limit is to make the subsequent comparisons meaningful and reliable. 5.2. Computational results
In this paper, we consider 10 test instances which are extracted from the investigated company X  X  ERP and MES systems.
For each instance, all the algorithms have been executed for 20 consecutive times independently. Then, the best result obtained from the 20 independent runs for each instance is displayed in the  X  X  X est X  X  column. Likewise, the worst result and the mean of the 20 output values for each instance are displayed in the  X  X  X orst X  X  and the  X  X  X ean X  X  column, respectively. Since absolute objective values are unimportant for comparative purposes, the above-mentioned data have been converted into relative numbers, taking the best result achieved by TSO as reference (i.e., the conversion is simply  X  X  X he obtained actual value/the best objective value from TSO X  X ). The results are displayed in Table 1 .

In this paper, we have introduced a new dispatching rule named BS which is intended to synchronize the production of jobs in a batch. In order to examine the effectiveness of this rule, we define the following coordination index, which is calculated for the final production schedule output by TSO:
CI  X  1 G where C g  X  X  1 = b g  X  P j A B jobs in batch g . Clearly, smaller CI suggests a higher level of coordination in the processing of a batch of jobs, and thus a lower level of product inventory.

We also calculate the CI index for the two comparative algorithms and list the results on the 10 instances in Table 2 .
For each instance, the reported CI (in minutes) is the average value based on 20 independent runs just as the above. In order to provide more accurate information, we use the
Mann X  X hitney U test to compare the computational results. One group of tests are performed between TSO and GLS, and another group of tests are performed between TSO and TS/SA. Because each algorithm is run for 20 times independently, we have n  X  n 2  X  20 in the Mann X  X hitney U test. The null hypothesis is that there is no difference between the results of the two compared algorithms. Therefore, if the obtained U (the lesser of
U and U 2 ) is below 127, the null hypothesis can be rejected at the 5% level. Furthermore, if U o 105, the null can be rejected at the 1% level. The values of U are listed in Table 3 (for comparison of objective values) and Table 4 (for comparison of the coordination degree). Based on the statistical tests, we can conclude that TSO is significantly more effective than the two comparative methods.
Besides the fact that TSO outperforms GLS and TS/SA on all instances as revealed by the computational results, the following comments can be made as an attempt to explain the behavior of the algorithms. (2) In the TS/SA algorithm, the outer-layer TS is used to optimize (3) For the tested instances, we also find that larger improvement (4) The comparison of CI verifies the efficacy of the proposed BS 5.3. Experiment on the algorithm parameters
Just like many other local search-based algorithms, the pro-posed DDA-PMBGA and OSA-PP also involve several parameters which must be specified before the optimization process. In order to observe the influence of TSO parameters on the final solution quality, we select five important factors for experiment: T %: the proportion of computational time devoted to DDA-
PMBGA. e %: the proportion of individuals selected for building Bayesian networks. a : the perturbation amplitude in OSA-PP. t : the step size of DDA-PMBGA. l : the reinforcement factor.

The optimization results (mean value of 20 independent runs) for a selected larger-scale instance (No. R-7) under different settings of the five parameters are shown in Figs. 7(a) X (e) .
In these experiments, when one of the parameters is being tested, all of the other parameters will be kept constant at their original values (as listed in Section 5.1.1). When we experiment on the parameters a and l of the second-stage optimizer (i.e., OSA-PP), we use the same output solution of DDA-PMBGA as the input of OSA-PP so as to make the results comparable. In Fig. 7 ,thedataarealso mean objective value in 20 runs when the currently tested parameter parameter within the considered value range Y .

The following remarks can be made concerning these results (each item corresponds to the discussion on one parameter): (1) When over 60% of the total computational time is spent on OSA-
PP, the overall optimization effect is not satisfactory. One obvious reason is that the searching ability of OSA-PP is too weak to perform a global exploration of the solution space. However, it definitely improves the solution output by DDA-PMBGA because, according to Fig. 7 (a), the results also worsen on the other extreme when DDA-PMBGA takes up 100% of the available computational time and thus OSA-PP is not used. Overall, the result is most desirable when the time allocation for DDA-PMBGA is 70 X 80%. At this ratio, DDA-PMBGA X  X  advantage in extensive exploration and OSA-PP X  X  advantage in intensive exploitation can be beneficially combined. (2) The number of solutions sele cted for building the Bayesian network also affects the solution quality. When more than 60% of the population are selected for S , the optimization performance is poor. This is because such a high proportion of individuals cannot represent the really  X  X  X romising X  X  solutions in the popula-tion. The overall trend displayed by Fig. 7 (b) is that a smaller S will generally result in better optimization quality. It is recom-mended to adopt the best 20 X 40% of the population for Bayesian model building at each generation. A proportion below 10% will be quite unacceptable, because too few samples may lead to (3) OSA-PP relies on random perturbations to search the neighbor-(4) The parameter t affects only DDA-PMBGA but not OSA-PP (5) Reinforcement is a key step in OSA-PP, the basic idea for 6. Conclusion
Real-world scheduling problems differ from theoretical pro-blems in several aspects. In this paper, we mainly focus on two features, that is, the pending due dates and the influence of job batches. We integrate due date assignment with job shop sche-duling and define a new optimization model. To solve the problem, a two-stage algorithm is proposed, in which the first stage performs a rough optimization for the batch-wise due dates while the second stage performs a local search by perturbing the due date of each job separately. Meanwhile, a dispatching rule is proposed to make the completion times of the jobs in a batch close to each other in order to reduce product inventory. Applica-tions to a real-world mechanical factory verified the efficacy of the proposed approach. The impact of parameter settings is also discussed.

Future research will be conducted for incorporating more of real-world factors, for example, the effect of random events in the production line. From the operations management perspective, the occurring frequencies of such events as machine breakdown and defective products (requiring rework) can be regarded as controllable decision variables. It would be interesting to consider such factors in the scheduling model and construct a simulation optimization problem, which will be another great step closer to real-world applications.
 Acknowledgment The paper is supported by the National Natural Science Foundation of China (Nos. 60874071, 71001048). The authors would like to thank the anonymous referees for their detailed comments which help to improve the quality of this paper. References
