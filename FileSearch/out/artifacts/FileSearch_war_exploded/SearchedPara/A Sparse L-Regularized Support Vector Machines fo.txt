 Mining large-scale unlabeled data has received a great attention in recent years. Such this is not the true way of handling large-scale natural language learning problems. generates dense models where most feature weights are small but non-zero. The situa-like Chinese word segmentations. Maintaining such high-dense feature weights is not easy for common processors. In addition, the denser the model, the slower the testing it achieves. 
In this paper, we present a cutting-weight algorithm for L 2 -SVM for sequential la-sparse by disregarding a set of weak features. The classic feature selection approaches effectiveness, we compare our method on three well-known benchmark corpora, namely, CoNLL-2000 syntactic chunking [14], SIGHAN Chinese word segmentation change in accuracy. fier W (hypothesis) b x W y +  X  = the modified finite Newton L 2 -SVM solves: off between margin size and training error. 
Following [7], the function of L 2 -SVM is a strictly convex, quadratic, and first or-der differentiable function and has a unique solution. The solution of L 2 -SVM objec-tive function is to minimize for: Gradient (CG) [2] scheme refer to as CGLS. order derivable, and can be directly optimized in primal form. By following this line, nal objective function of (1) can be re-written as: The optimization problem is similar to solve: limited to a zero weight vector and can be further disregarded from the objective func-tion, then the objective function becomes: degree of approximation. 
One important property of L 2 -norm regularization is that it pushes a value less and W W w in W 2 , it satisfies: Similarly for each feature weight w i in W 1 , it satisfies:  X  sentative) or irrelevant (non-representative) feature values. 
Obviously the parameter  X  determines the trade-off between the model sparsity and rithm. The general optimization technique is the same as the modified finite Newton technique could also be applied to the other gradient descent-based linear SVM opti-mization methods. For example, line 6-8 in Fig. 1 can be replaced by introducing the dual-coordinate descent algorithms [4]. Li ne 12 can be replaced by verifying the dif-ference between maximum and minimum projected gradient (Property 3 of Theorem 2 in [4]). 3.1 Speed-Up Local Classifiers come large, the matrix will be unmanageable in practice. 
To solve it, we further introduce the indexing idea from the Information Retrieval file. For each dimension, we store the set of non-zero feature weight with as postings feature. example. On the contrary, the testing time complexity of the sparse multiclass repre-average number of relevant items per feature and m m avg  X  . 
The use of indexing file to SVM is not new. For example, [9] introduced a similar idea to manage large number of support vectors generated from the polynomial kernel SVM. 1. Initialize 0 W ; 2. iter := 0; 3. While (! converged) { 4. Set up (3) using 6. Solve (3) with CG methods and obtain k W  X  ; 9. Update weight vector 13. stop; 14. iter := iter +1; 15. } 4.  X  : a threshold parameter that is used to control the goodness of one iteration; 6. While (! converged) { 7. Best _ Acc := Inner_acc ; 11. Train SVM model by the feature set F ; 12. Evaluate the accuracy Acc i with the trained model; 15. Inner_acc := Acc i ; 16. } 17. } 19. converged := true ; 20. } 3.2 Search the Optimal Feature Set over the threshold  X  . chunking, three Chinese word segmentation tasks that derived from SIGHAN-3, and the Chinese Dependency parsing [16]. Table 1 shows the statistics of those datasets. 
By following most literatures, we adopted the IOB2 with forward direction chunk-global information (AV feature) and extending the boundary information can improve settings of the Chinese dependency parsing were set the same as previous work [16]. 
To validate the proposed idea, we re-implemented SVM-MFN ( L 2 -norm with modified finite Newton method) [7] and LibLinear-L 2 [4]. To handle multiclass prob-used feature set. All of the experiments were performed under the E6300 OC 3.2 GHz with 4GB RAM under the Server 2003 32bit environment. 4.1 Results size implies the memory requirement in run-time. The larger number the model is, the the objective function (eq. (3)) is reduced. 
We also run another experiment to compare with the  X  X inal cutting X  method. This scale down the epsilon to 0.05 since there will be no accuracy difference when epsilon while resulting slightly worse result than f-cut (93.96 v.s. 93.90).

Note that we should carefully select suitable value for parameter  X  . The input fea-usually yield much smaller model size, better training/testing time cost. (  X  =0.0001) as default parameter value for our method. racy, the L 2 -regularized groups such as SVM-MFN and Liblinear( L 2 ) yield more supe-as accurate as the L 2 -regularized groups. Also, its testing time is not fast. 
By applying the proposed cutting-weight algorithm to SVM-MFN and Liblinear ( L resulting almost no change in accuracy. As compared to SVM-MFN, our cutting-testing time costs while keeping the same accuracy. 
In this paper, we do not successfully conduct all experiments for SVM-light since the training time is not human-tolerable. For example, it costs more than one week to train one class with the MSRA and CityU dataset. That means it takes at least 6 weeks to train one Chinese word segmentor. show that our method achieves better accuracy on three well-known datasets, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation, and Chi-reduced the model size and also reaches slightly faster training time and at least 20% improvement in runtime efficiency. 
