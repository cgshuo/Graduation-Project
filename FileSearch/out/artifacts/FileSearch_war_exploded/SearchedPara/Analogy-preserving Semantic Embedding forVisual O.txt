 Sung Ju Hwang sjhwang@cs.utexas.edu Kristen Grauman grauman@cs.utexas.edu Fei Sha feisha@usc.edu Discriminative approaches to object categorization have shown much success in recent years. However, as the community shifts its focus towards fine-grained and large-scale recognition problems, the traditional view of object categories as isolated low-level visual patterns is restrictive.
 Therefore, researchers have begun to explore how semantic relationships between categories might in-form a purely discriminative approach. By view-ing the classes as interrelated entities in some la-tent semantic space, the goal is not only to im-prove the ultimate recognition accuracy, but also to yield models that make semantically reasonable er-rors. Recent work makes use of semantic knowl-edge that is proprietary (i.e., attribute-based) or inclusive (i.e., taxonomy-based) ( Zweig &amp; Weinshall , 2007 ; Lampert et al. , 2009 ; Wang &amp; Mori , 2010 ; Fergus et al. , 2010 ; Zhao et al. , 2011 ; Hwang et al. , 2011b ; a ). For example, one approach is to require that semantically related categories select a common set of features ( Zhao et al. , 2011 ); another is to use mid-level semantic attributes to regularize object repre-sentations ( Lampert et al. , 2009 ; Wang &amp; Mori , 2010 ; Hwang et al. , 2011b ). However, those methods focus on each individual class X  X  relationships and properties;l thus they are limited to pairwise semantic structures. Moving beyond per-class semantic relatedness, our goal is to exploit higher-order relationships jointly in-volving multiple classes. Specifically, we propose to model analogies between classes in the form  X  p is to q , as r is to s  X  (or, in shorthand, p : q = r : s ). An anal-ogy encodes the relational similarity between two pairs of semantic concepts. By augmenting labeled data in-stances with a set of semantic analogies during train-ing, we aim to enrich the learned representation and thereby improve generalization. Analogies can be de-fined with almost arbitrary abstraction, ranging from  X  X s-a X  relationships ( dog : canine = cat : feline ), to contextual dependencies ( fish : water = bird : sky ). To examine analogies most likely to benefit vi-sual learning, we restrict our focus to analogical pro-portions ( Miclet et al. , 2008 ) X  X nalogies between pairs of concrete objects in the same semantic universe and with similar abstraction level.
 Before sketching our approach, we first motivate why this form of analogy should offer new information to a learning algorithm. As any standardized test-taker knows, analogies are used to gauge both vocabulary skills and reasoning ability. Notably, the pairs of en-tities involved in an analogy need not share proper-ties. For example, in the analogy planet : sun = electron : nucleus , the planet and electron do not have anything in common; rather, the relational similarity ( orbiter and center ) is what makes us rec-ognize the two pairs as parallel in meaning ( Gentner , 1983 ). Furthermore, the common difference exhibited by the two pairs in an analogy may encapsulate a com-bination of multiple properties X  X nd that combination need not have a succinct semantic name. For exam-ple, in the analogy leopard : cat = wolf : dog , the common difference relating the two pairs entails mul-tiple low-level concepts; in both, the first class lives in the wild , has fangs , and is more aggressive , etc. Thus, to master analogies, one must not only estimate the similarity of words, but also infer the abstract rela-tionships implied by their pairings.
 Accordingly, we expect analogies to benefit a feature learning algorithm in ways that semantic distance con-straints alone cannot. Whereas existing methods in-ject only  X  X ocabulary skills X  by requiring that seman-tically related instances be close and semantically un-related ones be far, our method will also inject  X  X ea-soning ability X  by requiring that the common differ-ences implied by analogies be reflected in the learned semantic feature space. Often, the higher-order con-straints may connect quite distant sets of categories. The analogies can thus facilitate a form of transfer from class pairs that are more easily discriminated in the original feature space to analogous class pairs that are not. For example, suppose leopard and cat are often confused in the visual space because the training set consists of only close-up images, whereas dog and wolf are easily separable due to their dis-tinct backgrounds. Enforcing the analogy constraint leopard : cat = wolf : dog could make the sep-aration in the first pair clearer, by aligning it with the same hypothetical semantic axis of differences ( wild/fanged/aggressive ) shared by the second (more distinctive) pair.
 We propose an Analogy-preserving Semantic Embed-ding (ASE), which embeds features discriminatively with analogies-based structural regularization. Given a set of analogies involving various object categories, we translate each one into a geometric constraint called an analogical parallelogram . This constraint states that the difference between the first pair of categories should be the same as the that between the second pair, where each category is represented by a (learned) prototype vector in some hypothetical semantic space. See Figure 1 . We represent the constraints as a novel regularizer that augments a large-margin label embed-ding. Consequently, we obtain an embedding where examples with the same label are mutually close (and far from differently labeled points) and analogical par-allelograms have nearly parallel sides.
 Our learned embedding can be used for recognition, automatic analogy completion, visualization, and po-tentially other tasks. To use it for recognition, we project a novel image into the learned space, and pre-dict its label based on the nearest category prototype. We further show how to automatically discover and prioritize useful analogies, which is valuable to concen-trate on constraints that are influential for recognition. Compared to traditional large-margin label embed-dings ( Weinberger &amp; Chapelle , 2009 ; Bengio et al. , 2010 ), our approach preserves a new form of rela-tional similarity. While the prior methods also map to a space where semantic similarities are preserved, they risk learning spurious associations between fea-tures and labels. Our analogy-induced regularizer mit-igates such adverse effects by constraining the hypoth-esis space with structural relations between category pairs, yielding robust models with better generaliza-tion. Even constraints not in the axes of visual prop-erties can be helpful, as they shift the focus from brittle incidental correlations to higher-order semantic ties. Analogical logic and learning Several findings from cognitive science and AI provide background for our approach. Gentner et al. ( Gentner , 1983 ) study analogies in light of human cognition. They define an analogy as a relational similarity over two pairs of en-tities, and contrast it with the more superficial similar-ity defined by attributes. Based on this intuition, they suggest a conceptual structural mapping engine that enables analogical reasoning ( Gentner &amp; Markman , 1997 ). Recognizing that such generic analogies require high-level logical reasoning that may be problematic for an automated prediction system, Miclet et al. sug-gest focusing on the analogical dissimilarity between entities in the same semantic universe ( Miclet et al. , 2008 ). They exploit analogical dissimilarity to do direct logical inference when one of the entities is unknown. Our work focuses on similarly scoped analogies X  X he semantic universe of object categories. In contrast to their logical inference model, however, we propose geometric constraints to enforce analogical proportions in a learned embedding.
 While our main idea is to use analogies in an embed-ding, we also show how to automatically discover cat-egories that have analogical relationships using their attribute descriptions. In this respect, there is a con-nection to structural transfer learning work that dis-covers mappings between domains ( Mihalkova et al. , 2007 ; Wang &amp; Yang , 2011 ). However, while that work aims to associate distinct source and target domains (e.g., computer viruses and human viruses), we aim to detect parallel associations within the same domain, and then use those pairings to constrain feature learn-ing. In graphics, inferring the filter relating two input images allows the automatic creation of  X  X mage analo-gies X  ( Hertzmann et al. , 2001 ); we deal with analogies on visual data, but our idea of using them to regular-ize the representation is different and original. Semantics in recognition Recent research ex-plores how external semantic knowledge can ben-efit visual recognition, e.g., ( Zweig &amp; Weinshall , 2007 ; Lampert et al. , 2009 ; Wang &amp; Mori , 2010 ; Fergus et al. , 2010 ; Hwang et al. , 2011b ; a ; Zhao et al. , 2011 ). There, the semantics originate from taxonomies or attribute memberships, limiting what can be cap-tured to proprietary or inclusive relations. To our knowledge, our work is the first to exploit analogi-cal relations in learning an object recognition model, opening up the potential advantages discussed above. Embedding and manifold learning Most existing embedding methods aim to preserve the distances be-tween data points, either globally ( Duda et al. , 2001 ) or locally ( Roweis &amp; Saul , 2000 ; Weinberger &amp; Saul , 2006 ). Label embeddings learned for object or docu-ment categorization also aim to preserve distances, but with further constraints to promote the discriminabil-ity of labeled classes ( Weinberger &amp; Chapelle , 2009 ). Recent embedding methods preserve not only the ge-ometry of local neighborhoods, but also higher-order properties like category clusters ( Shieh et al. , 2011 ) or graph structure ( Shaw &amp; Jebara , 2009 ). We also aim to preserve more far-reaching structures. However, our method is distinct in that it enforces the relative dis-tances between semantically related pairs of instances. We assume a labeled dataset D = { ( x i , y i ) } N n =1 , where x i  X  R D stands for the i -th D -dimensional feature vec-tor and y i  X  X  the corresponding class label, which in our primary application of interest will correspond to an object category ( panda , leopard , etc.) We fur-ther assume that we have access to a set of analogies A = {  X  1 ,  X  2 , ,  X  A } . The analogies are derived ei-ther directly from human input or with an automatic discovery procedure we propose below.
 The goal of our learning algorithm is to embed both the data features and the class labels in a low-dimensional space R M with M &lt; D , while minimiz-ing misclassification errors. In what follows, we will denote the embedding of the feature vector x i by z i , and the embedding of the class c  X  X  by u c . In this  X  X emantic space X , we want to ensure instances from the same class stay close to each other and to their class label X  X  location u c . Moreover, and most impor-tantly, we would like the placement of the class labels to reflect their analogy-based relationships . To this end, our approach addresses two crucial chal-lenges: i) how can we encode an analogy between class labels via their coordinates in the learned semantic space ? and ii) how can we automatically discover anal-ogy relationships among a large number of categories ? 3.1. Encoding analogies For each class c  X  X  , u c  X  R M denotes its coordinates in the M -dimensional semantic space. Each u c can be thought of as a prototype for the category; we will explain how the prototypes are optimized jointly with the data projection matrix W in Sec. 3.3 .
 An analogy involves four categories, and we rep-resent the relationship with an ordered quadruplet ( p, q, r, s )  X  X  X Y X Y X Y . As we focus on ana-logical proportions ( Miclet et al. , 2008 ), the difference between p and q is equated with the difference between r and s . Moreover, the difference between p and r also is equated with the difference between q and s . Analogical proportions naturally induce geometric constraints among the embeddings of the four cate-gories in the semantic space. In particular, the geom-etry is characterized by a parallelogram; we will show how to exploit this structure in our learning algorithm. Analogy parallelogram We use the vector shift ( u q  X  u p ) to represent the difference between the two categories q and p in the semantic space. Note that this difference is directed, that is, u q  X  u p 6 = u p  X  u The analogical proportion implied by ( p, q, r, s ) is thus encoded by the following pair of equalities: u q  X  u p = u s  X  u r , and u r  X  u p = u s  X  u q . (1) These constraints form a parallelogram in which each vertex is a category, as illustrated in Fig. 2 . Convex regularizer There are several ways of en-forcing the analogical proportion constraints in eq. ( 1 ). A natural choice is to exploit the parallel property of opposing sides. Specifically, the normalized inner products between opposing sides are the cosine of their intersection degree, which should be 1 if perfectly par-allel. Concretely, for an analogy  X  = ( p, q, r, s ), the resulting parallelogram  X  X core X  would be defined as While intuitive, maximizing the parallelogram score (or equivalently, minimizing its negative) is compu-tationally inconvenient, since it is not convex in the embeddings u . Thus, we use a relaxed version and compare the sides only in their lengths . Specifically, our regularizer is defined as where  X  1 and  X  2 are two scaling constants used to pre-vent either pair of sides from dominating the other. We simply estimate them as the mean distances between data instances from different classes.
 R (  X  ) is convex in the embedding coordinates. More-over, it is straightforward to kernelize as it depends only on the distances (and thus inner products). 3.2. Automatic discovery of analogies Human knowledge is a natural source for harvesting analogy relationships among categories. However, it is likely expensive to completely rely on human as-sessment to acquire a sufficient number of analogies for training. To address this issue, we use auxiliary semantic knowledge to identify candidate analogies. In the context of visual object recognition, visual attributes are an appealing form of auxiliary se-mantic knowledge ( Lampert et al. , 2009 ). Attributes are binary predicates shared among certain visual categories X  X or example, the category panda has the  X  X rue X  value for the spotted attribute and the  X  X alse X  value for the orange attribute. Supposing we have access to attribute descriptions stating the typical at-tribute values for each category, we can automatically discover plausible analogies.
 We next define two strategies to do so. The first is in-dependent of the data instances, while the second ex-ploits the instances to emphasize analogies more likely to lend discriminative information.
 Attribute-based analogy discovery Our first strategy is to view attributes as a proxy to the embed-ding coordinates of the visual categories in the seman-tic space we are trying to learn. In the attribute space, each category is encoded with a binary vector, with bits set to one for attributes the class does possess, and bits set to zero for attributes the class does not possess. Note that this is a class-level description X  X e have one binary vector per object class.
 Imagine that we enumerate all quadruplets of visual categories. For each quadruplet  X  , we compute its par-allelogram score according to eq. ( 2 ), using the cate-gories X  attribute vectors as coordinates. We then select top-scoring quadruplets as our candidate analogies. Pragmatically, we can only score a subset of all pos-sible analogies for a large number of visual categories. Thus, to ensure good coverage, for each randomly se-lected pivot category p , we select at most K triplets of other categories, where K is far fewer than the to-tal number of possible ones. We also remove equiva-lent analogies. For example, ( p, q, r, s ) is equivalent to ( p, r, q, s ) or other shift-invariant forms. We will use the highest-scoring analogies to augment the class-labeled data when learning the embedding. We stress that while we discover analogies based on parallelogram scores computed in the space of at-tribute descriptions, we regularize the learned em-bedding according to parallelogram scores computed in the learned embedding coordinates (cf. Sec. 3.3 ). Thus, external semantics drive the  X  X raining X  analo-gies, which in turn mold our learned semantic space. Discriminative analogy discovery The process de-scribed thus far has two possible issues. First, it does not take the data instances into consideration. While our goal is to find a joint embedding space for both data instances and category labels, analogies inferred purely from attributes do not necessarily align the data and mid-level representations X  X hey might even lead to conflicting embedding preferences! Secondly, being fully unsupervised, this procedure need not discover analogies directly useful to our classification task. In particular, the extracted candidate analogies are not indicative of whether two categories are easily distin-guishable or confused.
 We address both issues with an intuitive and empiri-cally very effective heuristic. Mindful of our goal (de-scribed in the introduction) of improving discrimina-tion for confusable categories by leveraging analogy relationships connecting those confusing categories to easily distinguishable categories, we first use baseline classifiers to estimate the pairwise confusability be-tween categories. This step can be achieved easily with any off-the-shelf multi-way classifier and visual features computed from the training instances. The confusability between two categories p and q is defined in terms of the resulting misclassification error: where  X  p  X  q is the rate of misclassifying instances from the category p as the category q , and likewise for  X  q  X  p Our next step is to refine the candidate analogies gen-erated above by finding those with unbalanced confus-ability . Specifically, for each analogy  X  = ( p, q, r, s ), we compute its discrimination potential: This score attains its maximum when C pq and C rs are drastically different X  X hat is, if one is 0 and the other is 1. We use this score to re-rank the K candidate analogies generated for each category p . Intuitively, we seek the quadruplet where one pair of categories is easily distinguishable (based on the image data) while the other pair is difficult to differentiate. Precisely by enforcing their analogy relationship, we expect the easy pair to assist discrimination for the difficult one. To summarize, our automatic discovery of analogies is a two-phase strategy. We first use an auxiliary seman-tic space to identify a set of candidate analogies where the four categories are highly likely to form a parallel-ogram. Then, we analyze misclassification error pat-terns of these categories and use the scoring function in eq. ( 4 ) to determine the potential of each analogy in improving classification performance. We describe next how to use the highest-scoring analogies to learn the joint embedding of both features and categories. 3.3. Discriminative learning of the ASE Next we explain how we regularize a discriminative embedding to account for the analogies.
 Large margin-based discrimination We aim to learn a projection matrix W  X  R M  X  D to map each data instance (image example) x i into the semantic space, giving its M -dimensional coordinates z i = W x i . 1 The ideal projection matrix W should make z i close to its corresponding label X  X  embedding u y all other labels X  embeddings ( Weinberger &amp; Chapelle , 2009 ) 2 . Specifically, we enforce the large margin con-straint for every training instance, k W x i  X  u y where  X  ic  X  0 is a slack variable for satisfying the separation by the margin of 1.
 Regularization To jointly embed both features and class labels, we regularize so that the class labels in the analogy set A form parallelograms as much as possible. The regularizer is given by which is the weighted sum of the regularization de-fined in eq. ( 3 ) for each analogy  X  a . If using the  X  X aw X  attribute-based analogies, the weight  X  a = S (  X  a ), thus enforcing stricter regularization for category quadru-plets whose structure is closer to a  X  X erfect X  anal-ogy. If using discriminatively discovered analogies, the weight is instead  X  a = P (  X  a ), thus prioritizing those that are more discriminative.
 Additionally, we also constrain the parameters W and all u c with their Frobenius norms: k W k 2 F and label embeddings, we constrain them to be close to to zeroes. Or, the class label embeddings could be computed from auxiliary information, for example, the multi-dimensional embedding of class labels where the dissimilarities between labels are measured with tree distances from a taxonomy ( Weinberger &amp; Chapelle , 2009 ) or attributes. We consider both in the results. 3.4. Numerical optimization Our learning problem is thus cast as the following op-timization problem: min subject to both the large margin constraints in eq. ( 5 ) and non-negativity constraints on the slack variables  X  . The regularization coefficients  X  , , and  X  are determined via cross-validation.
 The optimization is nonconvex due to the quadratically-formed large margin constraints. We have developed two methods for solving it. Our first method uses stochastic (sub)gradient descent, where we update W and u c according to their sub-gradients computed on a subset of instances. Despite its simplicity, this method works well in practice and scales better to problems with many categories. We also consider a convex relaxation analogous to the procedure in ( Weinberger &amp; Chapelle , 2009 ). Briefly, in eq. ( 7 ), we hold { u c } fixed first and solve W in closed-form, W = U Q where the matrix U is com-posed of { u c } as column vectors. The matrix Q de-pends only on x i and is constant with respect to U or W . Substituting the solution of W into both the objective function eq. ( 7 ) and the large margin con-straints eq. ( 5 ), we can reformulate the optimization in terms of U T U . In particular, the original non-convex large margin constraints in U can be relaxed into con-vex if we reparameterize U T U as a positive semidef-inite matrix V . We then solve V and recover the solutions U and W , respectively. For cases where D is much larger than the number of categories, we expect this variant to optimize faster. We validate three aspects: i) the effectiveness of our analogy discovery approach; ii) recognition accuracy when incorporating discovered analogies in learning embeddings; and iii)  X  X ill in the blank X  X  X  Graduate Record Examination (GRE)-style prediction task of filling in the category that would form a valid analogy. Datasets and implementation details We use three datasets created from two public image datasets: Animals with Attributes (AWA), which contains 50 animal classes ( Lampert et al. , 2009 ) and ImageNet, which contains general object categories ( Deng et al. , 2009 ). They were chosen due to their available at-tribute descriptions and their challenging diverse con-tent. From AWA, we create two datasets: AWA-10 of 6 , 180 images from 10 classes ( Lampert et al. , 2009 ), and the complete 50-class AWA-50 of 30 , 475 im-ages. From ImageNet, we use the 50-class ImageNet-50 with annotated attributes ( Russakovsky &amp; Fei-Fei , 2010 ), totaling 70 , 380 images.
 We use the features provided by the authors, which consist of SIFT and other texture and color descrip-tors. We use PCA to reduce the feature dimensional-ity to D = 150 for efficient computation. Additionally, we augment ImageNet-50 with attribute labels for col-ors, material, habitat, and behaviors (e.g., big , round , feline ), yielding 39 and 85 binary attributes for Ima-geNet and AWA, respectively. We fix K = 10 , 000. We use the convex relaxation, since the dimensionality is much greater than the number of classes; accordingly, the semantic space dimensionality M equals the num-ber of categories (10 or 50). 4.1. Automatic discovery of analogies In real-world settings, acquiring all analogies from manual input may be costly and impractical. Thus, we first examine the analogies discovered by our method (Sec. 3.2 ), which assumes only that attribute-labeled object classes are available.
 Figure 3 displays several examples for AWA-50 and ImageNet-50. Most analogies are intuitive to un-derstand. For example, in the second row of col-lie:dalmatian = lion:leopard , the categories col-lie and lion are both furry and brown, while the cate-gories dalmatian and leopard are both spotted and lean. We also see that the analogies can be largely visual (e.g., the third row), an upshot of the many visually relevant attributes offered with the datasets. 4.2. Visual recognition with ASE We compare the classification performance of our Analogy-preserving Semantic Embedding (ASE) to the following baselines, all of which lack analogies: (1) SVM-RBF : Multiclass SVM with RBF kernel. (2) Large margin embedding (LME) : The exist-ing technique of ( Weinberger &amp; Chapelle , 2009 ) with-out the taxonomy prior regularizer, which is also a special case of our approach where we disable both the attributes prior and analogy regularizers by set-ting  X  = 0 and  X  = 0 in eq. ( 7 ). For this baseline, the class label embeddings are constrained only to satisfy the large margin separation criterion of eq. ( 5 ); (3) Large margin embedding with attributes prior (LME prior ) : This baseline adds the prior regu-larizer to LME, where we adjust  X  for eq. ( 7 ) via cross-validation. In particular, we use the multi-dimensional scaling (MDS) embedding of class labels where the pairwise dissimilarity is the Euclidean distance be-tween the attribute vectors of two classes. The con-trast between LME and LME prior reveals how use-ful attributes as auxiliary semantic information are in yielding discriminative embeddings, separating out the All embedding methods classify novel images accord-ing to the nearest category u c in the embedding space. For our method, we include two variants, differentiated only by how the analogies are discovered, cf. Sec. 3.2 . In ASE-A, the analogies are derived solely from at-tributes, aiming to preserve parallelograms as much as possible. In ASE-C, the analogies are derived from the discrimination-based discovery, aiming to use dis-tinct categories to assist confusable categories. The confusability among categories is measured using the baseline LME classifier on the validation set. In our experiments, all hyperparameters (regulariza-tion coefficients, kernel function parameters) are tuned via cross-validation. We use 30 examples per class for both training and testing, and use another 30 images as a validation set to learn the parameters. We report the average results over 5 such random splits. How do analogies affect recognition accuracy? We first validate our method on multiclass classifica-tion. Since the analogies help preserve the intrinsic se-mantic structure among objects, we expect the learned space to show better generalization power, and hence improved object categorization.
 Table 1 shows the results. 4 We report the optimal number of analogies selected from preliminary experi-ments, though the results were in general insensitive to the number of analogies. On all three datasets, we ob-serve clear improvement using our analogy-preserving embedding variants over both LME variants.
 We see that the difference in accuracy for LME and from LME to ASE. This suggests that using attribute distances alone as a prior to constrain embeddings (as the prior and the analogy constraints work together, leading to a noticeable improvement.
 Which types of analogies should we use? We also observe that our ASE-C variant outperforms ASE-A. This coincides with our intuition that the analogies would be much more helpful for discrimination if a pair of easily confusable categories can leverage a pair of easily distinguishable categories.
 Detailed analysis supports this intuition even more strongly. Figure 4 compares the amount of reduc-tion in confusability among the 10 classes of AWA-(right). We observe that for ASE-A, the improvement is made on pairs that are not included in the analogies; in contrast, for ASE-C, the improvements are mostly made on pairs that are included in analogies. This noticeable correlation between the category pairs se-lected for analogies, and the pairs whose confusion is reduced (for ASE-C) suggests that our consideration of the pairwise confusion is indeed the reason ASE-C outperforms ASE-A, whose analogies do not care about the data distribution.
 Figure 5 shows projections of AWA-50 categories to a 2 D space using ASE-C. We see that the quadrilaterals formed by the four categories involved in each analogy do indeed show distinct parallelogram shape. 4.3. Completing a visual analogy Finally, we subject our method to a GRE test. Given p : q = r :?, how well can our method fill in the blank, based on its representation of the three other classes? In this analogical reasoning task, which is performed by virtually every graduate school appli-cant, the learning algorithm is given a set of complete analogies A train . Then it is given a disjoint test set of analogies A test , each of which has its fourth category missing. No analogies overlap in ( p, q, r ) between the two sets. To fill in the blank with ASE or LME, we simply rank each category according to its parallelo-gram score when its u c is used as the fourth category. The more parallelogram-like, the more it appears to be the right answer. The ground truth answer is the class maximizing the parallelogram score according to the auxiliary attribute ground truth.
 Our hypothesis is that by learning to discriminate cat-egories in conjunction with preserving the analogy con-straints in A train , the learned semantic embedding will generalize well to complete the novel analogies, with-out resorting to auxiliary information.
 Table 2 strongly supports our hypothesis. We report the prediction accuracy averaged over 5 random trials, where we take the classes with the top k parallelogram scores as guesses. ASE-A achieves the best accuracy, followed by ASE-C. They both outperform the LME methods, which lack analogical constraints. On AWA-10, we predict the right completion in the first guess ( k = 1) 64% of the time. There is clearly room for improvement, though, as accuracy decreases substan-tially for all methods on the larger 50-class datasets. Table 3 shows example completed analogies for AWA-50. Compared to LME, ASE selects more intuitive classes to fill in the missing values. Our work introduces a semantic embedding for visual data that preserves structural similarities in the form of analogies. In addition to formulating a novel reg-ularizer suitable for our goal, we also explore ways to systematically discover plausible analogies from aux-iliary attribute information. Our method improves recognition accuracy over an existing  X  X istance-only X  embedding approach, thanks to its ability to preserve higher-order structures and facilitate transfer between easier and harder pairs of objects. Beyond bene-fitting recognition, we show it also allows analogy completion X  X  high-level reasoning task. We next plan to explore more general forms of analogies, such as pairs of subgraphs containing multiple categories. Acknowledgements Research is supported in part by NSF IIS-1065390 (KG) and NSF IIS-1065243 (FS).
