 Information retrieval in Arabic has often pivoted on contemporary texts, for obvious reasons: electronic availability, usefulness of information, analogy with work done in other linguistic domains. However, Classical texts are much more important in con-temporary Arabic culture than in most Western countries, as witnessed by the large diffusion of websites which make Middle Ages books available not only to scholars, but also -and most important -to laymen interested in such texts. life and deeds of the Prophet th at altogether constitute the sunna , or Islamic Tradition (see Section 2). These texts do not have only a historical importance, they are the cornerstone of Muslim law and a favored reading of most Muslims around the world, and their presence in contemporary written Arabic is widespread. 
Notwithstanding their importance, Classical texts have not been -at least to the best of our knowledge -the subject of any scholar research project as far as informa-resources such as Wensinck's concordances [ 1].
 Their structure, which couples a text with a preceding chain of transmitters that as-sures the validity of the tradition, or isn  X  d , is already (if informally) organized in such a way that readers are able to detect info rmation with a relatively small amount of ambiguity. Yet, notwithstanding the importance of relations among transmitters in rather haphazard way, by recurring to traditional resources such as prosopographical repertories and by evaluating transmission relations in a mostly impressionistic way. 
The same is true of the lexical and grammatical content of traditions: in most cases, interpreters analyze each  X  ad  X  X  on its own merit, making few, if any, recur to cross-textual regularities and collocations. 
Our research project aims to devise methods and algorithm to extract as much in-formation as possible from such texts in an automatic way. The subject matters on which we started working are the automatic segmentation of isn  X  d and narrative text (or matn : see Section 3), the reconstruction of chains of transmitters through graphs, the creation of (semi-)automatic lexical concordances and the prospective develop-ment of a grammar suitable to (semi-)automatically interpret texts and to build seman-tic representation which can further be employed in inference (by modeling a classical method used by Islamic law scholars). Preliminary results of a morphological analyzer and lemmatizer (see Section 4) are discussed (see Section 5). shorter or longer narratives on the life and deeds of the Prophet Mu  X  ammad (571-632) that report what he said or did, or of his tacit approval of something said or done and by itself define what is considered good, by providing details to regulate all as-pects of life in this world and to prepare people for the beyond, clarifying the Koranic shades;  X  ad  X  X  texts constitute the sunna , lit.  X  X ay of life X , or Islamic Tradition, that in Muslim culture is considered second in auth ority only to the Koran: other sources of have generally a lower rank. 1
 X  ad  X  X  s structure is a sequence of binary elements: a text of the narrative, matn , with a preceding chain of transmitters ( isn  X  d , literally  X  X upport X ), that have transmit-ted the narrative, and that assures the validity of the tradition, following one another until the first one who saw or heard Mu  X  ammad. 2
For the selection of input data computational and linguistics criteria were privi-leged, rather than philological ones. Among the canonical collection of  X  ad  X  X  s, it has been chosen an on-line edition of the collection known as  X  a  X  X  X  Al-Bu  X   X  r  X  , compiled vocalization, allow a wide range of investigations without any needing of manual intervention or preparation. The text has been processed as is, and a systematic con-trol of orthographical and philological coherence has been postponed as not relevant at this stage of project X  X  architecture X  X  implementation. The automatic segmentation is a process that assigns segment boundaries to get dis-crete objects from a non-discrete continuum [ 4]. This approach aims to avoid or at least to limit drastically the supervised intervention, which is rather resource-consuming in time and human involvement, especially considering large amount of data [ 4-6]. 3.1  X  ad  X  X  's Segmentation: Pairing Explicit and Implicit Information semi-structured texts in (pseudo-)natural languages, such as semi-formal texts in de-scriptions of mathematics. Wolska and Kruijff-Korbayov X  [ 7] approached the analysis and formalization of symbolisms and formulas used in mathematics manuals, and extract textual and meta-textual information; (iii) conceives a set of rules based on these patterns in order to automatically translate extensive verbal expression in math X  X  formulas and vice-versa. This study and others [ 8], point out that segmentation could through an analysis of regularities and recurrences, a global structure to the text itself as well. This structure could be seen as governed by a sort of contextual  X  X rammar of rules X , which also controls connections between content's information and its textual organization. that bound, define and sometimes nest different kind of content [ 9-10]. The text X  X  conti-nuum could therefore read as formed by two levels, the first one containing information and the other which assumes, beside its textual value, a meta-textual function which or-ganize and define the first level. This seem to show, although in a linear way, somehow a similar structure of that employed in databases, in which records contain information defined by fields. The parallelism with mathematics and information science is far from shows that there is not a general agreement about the original value, meaning and transla-tion of these  X  X unctional expressions X  [ 11-12-13] and a complete set of them has not been jet fully defined. However, they could be undoubtedly considered as provided with some another; (ii) they specify the authority and typology of transmission; (iii) sometimes they show the  X  X irection X  of the transmission. An automatic recognition of these elements in meanings. 3.2 Extraction and Organization: The HadExtractor Program In order to test the above-mentioned models of segmenting and structuring texts, a specific program named HadExtractor (HE) has been designed to deal with  X  ad  X  X  corpora, aiming to: (i) read the full collection and identify single  X  ad  X  X  s; (ii) segment together with relative supplementary information (position, typology and direction of transmission). HE was written and implemented in Python [ 14]. At the present HE has designed as rather close system, as specifically requires as input  X  ad  X  X  texts only and not jet other Classical Arabic textual structures. 
Direct processing of Arabic script in programming languages is possible in theory among strings in different characters X  system s. The original Arabic script has been therefore converted by using a set of characters based on Buckwalter transliteration system [ 15] and modified by us in order to fit to Python and regular expressions X  con-straints on special reserved characters. This transliteration uses ASCII characters only and substitutes usual diacritics employed academically in Latin characters with capital letters and, where needed, supplying with non-alphabetic characters. We implemented the conversion by employing a specific progra m that allows back transliteration to Arabic at every stage and processes toge ther either vocalized and not vocalized strings [ 16]. The core of HE is based on Regular Expressions Syntax, conceived in the 1950s by Kleene as tool of automata theory to describe formal languages and developed after-wards by Thompson to be used in programming languages [ 17]. A regular expression (regex) consists of a formally structured text string for describing complex search patterns, which could be applied to other strings in order to find for matches. The complexity of constants and operators employed allows regexs to be powerful and deeply expressive instruments to retrieve textual segments [ 18]. HE has been built mainly on three regexs: the first one identifies single  X  ad  X  X  , the second one separates isn  X  d from matn , the last one catches the transmitters X  names. between isn  X  d and matn , through a regex that models a pattern containing as variable the above-mentioned  X  X unctional expressions X  and some look-back and look-forwards operators that verify the context to detect effectively the  X  X unctional X  value of the expression as, obviously, the same word could recur in other contexts, for example inside matn without any particular meta-textual role. Once that all isn  X  d s are ob-tained, another regex, working in similar way but referring to a larger list of  X  X unc-tional expressions X , extract all transmitters X  names pairing them with the corres pondent  X  X unctional expression X . Concerning the extracted matn s, they were paired with a digitalized English translation [ 19-20], which was processed by a tailored version of HE. 
Once HE has implemented all regex routines and related tasks, it produces as out-put an XML file, in which all extracted information is organized, automatically tagged and nested. The following lines show an excerpt from output XML file, related to a single  X  ad  X  X  (Arabic script is in Buckwalter modified transliteration): 3.3 Representation: Transmitters X  Chains and Graphs Once HE has been applied to the  X  ad  X  X  corpus, a large amount of automatically ex-tracted information was available for further investigations, most of them dealing with extracted matn (see Section 4). Focusing instead on the isn  X  d , a smart example of representation of information about transmitters is given here below, with the aim to focus on objects' relationships rather then objects themselves. 
We structured accordingly the features of extracted isn  X  d 's information in the fol-lowing categories: (i) name of transmitter; (ii) its position in the single  X  ad  X  X  transmis-sion chain, which starts usually from the collector and arrives to the Prophet Mu  X  ammad); (iii) the typology of transmission (see Section 3.1). These categories were read as pertaining to a simple model in which  X  X bjects X  have different kinds of  X  X elationships X  among them. This model is undoubtedly similar to the ones coming from the graph theory, in which a graph is defined as a structure of nodes and edges to model relations among objects from a given collection [ 21]. These nodes and edges are drawn on a bi-dimensional grid through specific algorithms, in order to graphically visualize the above-mentioned relationships [ 22]. It was therefore clear that the graph theory could be usefully applied to our data in order to try a graphical representations of transmitters X  chains. This kind of representation aims to offer a sophisticate and quantitative-based instrument in a field of research traditionally cha-racterized by analogical an d human-based approaches [ 23] [ 11]. 
On the basis of fundamental literature on graph drawing [24-25] we have con-ceived and implemented in Python another specific program, named ChainViewer. This application, by using existing Python libraries for graph drawing: (i) gets all information about transmitters stored in the XML file containing previously extracted relationships among each transmitter and the previous/next one to edges(i.e. the chains), the typology of transmission to edges' types; (iii) through an algorithm is able to generate graphs for single chains or joins together multiple chains in the same graph (in this case if a transmitter's name appears twice or more is shown once but with multiple edges). 
At the current stage of development, ChainViewer works well with limited number time, in order to automatically gather in a unique graph all the transmitters of an  X  ad  X  X  collection together with the complete set of their transmission's relationships. This task obviously presents new problems to deal with, namely: (i) the needing of a semi-automatic instrument able to disambiguate homonyms, unify various inflected forms of the same name, identify nicknames and aliases; (ii) a specifically designed draw-ing's algorithm that could deal with thousands of nodes and edges, and dynamically represent them with expansion/compression tools. 4.1 The Original AraMorph Implementation As a starting point for the implementation of the text analysis module of the SALAH project, the morphological analyzer and lemmatizer AraMorph (AM) by Tim Buckwalter model, its high performance level even in unsupervised environment and the easiness of its maintaining and extending. 3
Opposite to a long-standing linguistic and computer science tradition which em-phasized the need for complex, multi-layered,  X  X eep X  morphological components in order to analyze properly Arabic texts and to account for the apparent lack of linearity of many Arabic morphemes  X  X ee examples and discussion in [ 26][ 28-30],  X  AM chooses to treat Arabic words (in the rather naive, but computationally efficient sense of  X  X ny sequence of characters separated by spaces X ) as elements linearly decompos-able in three sub-elements: a prefix, a stem, and a suffix, the stem being the only ne-cessary sub-element (in fact, zero prefixes and suffixes are postulated, sometimes adding some grammatical information to the stem according to the time-honoured morphological concept of  X  X ero morpheme X ). 
This simple account is straightforwardly implemented in the (possibly) simplest way, by feeding the system with three lookup lists of, respectively, (a) prefixes, (b) stems and (c) suffixes, together with three compatibility tables between, respectively, (d) prefixed and stems, (e) prefixes and suffixes and (f) stems and suffixes. Entries in the lookup lists are made up of four fields: (i) unvocalized and (ii) vocalized forms of the morpheme, (iii) grammatical category and (iv) English gloss; compatibility tables just list couples of compatible morphemes, all other combinations being incompa-tibles. Supplementary pieces of information, not employed in the analysis proper but potentially useful for glossing the texts (root and lemma for a group of morphemes), are provided in the stem lookup list in the form of pseudo-comments. 
The analysis, both in the original Buckwalter model (implemented in Perl) and in the Java implementation by the AM project, is performed through a brute-force search of every possible decomposition of words into prefixes, stems and suffixes, by look-ing up for prefixes from 0 to 4 letters long, stems from 1 letter upwards, and suffixes from 0 to 6 letters long. Only the unvoca lized form of words is taken into account (short vowels and other diacritics are stripped before looking up): candidate prefix-stem-suffix decompositions are first matched against the first fields of the respective lookup lists and discarded if any of the elements is missing, then the grammatical categories of the surviving combinations are matched against the compatibility tables and discarded if any of the combination is not present. As a result, each word of the (ii) unambiguous or (iii) ambiguous if, respectively, one ore more analyses are licensed. 
This model, whose beauty lies in its very simplicity, is a good starting point for a successful morphological analysis, but does not fits our needs for a plurality of rea-sons. First, while the emphasis on the unvocalized form of the word is relatively justi-fied for the ideal text genre targeted by Buckwalter  X  X ewspaper texts and other Modern Standard Arabic (MSA) non-literary texts that largely comprise the LDC Arabic corpus X  it is far from ideal for other types of texts, first of all fully vocalized consonantal skeleton of a text reduces ambiguity, and thus a system that, like the orig-inal AM model, deliberately chooses to igno re this information accepts to live with a higher degree of (morphological) ambiguity and automatically passes a number of wrong analyses that would instead be ruled out by taking into account diacritics present in the text. 
The second weak point in the original model lies in the fact that the lookup lists and the relative compatibility tables were built from a sample of the text corpora Buckwalter worked on: again, only morphemes attested in a subset of MSA texts and their combinations are included in the lookup lists and the combination tables, which unavoidably brings to reject or analyze wrongly many words attested in other textual types. 
A third weakness in the original AM implementation, which is linked to the pre-vious one, lies in the lack of any stylistic or chronological information in the lookup lists. This way, many morphemes that are virtually exclusive of MSA texts  X  X or instance, a not negligible number of transliterated foreign named entities which can-not be found in Classical texts and which are relatively rare in modern literary texts as well X  are included in the lists (and more ore less properly vocalized  X  X oreign proper names are never vocalized in real-world Arab ic texts X  in order to respect the field positives in the analysis of some textual genres. 4.2 Modifications to the Algorithm In order to overcome the weaknesses listed in the previous section, a number of mod-ifications to the original AM algorithm were devised that tackle the single problems detected above; the new algorithm has been dubbed  X  X evised AraMorph X  (RAM). The first modification is about the token identification mechanism: instead than dis-carding vocalization, our revised lemmatizer uses it to reduce the number of false positives by taking into account all the vowels present in the text. The comparison phase is less trivial than it might seem, since it must proceed on a three-stage level: (i) the token is segmented in consonants and diacritics (where everything between two characters marked as a consonant is a diacritic); (ii) consonants must match exactly  X  in fact some qualifications are orders which take into account current practice, e.g. an alif with hamza above or below matches a simple alif (which the original AM ac-counts for pragmatically, but rather inefficiently, by multiplying entries), and some more are required to reflect idiosyncrasies in the  X  ad  X  X  orthography; X  (iii) diacritics present in the token must not contradict the full vocalized form in the lexicon (that is, e.g., missing vowels are ok, but a vowel cannot match a different one). 4 To tackle the second weak point, namely the partial and unbalanced coverage of Arabic lexicon in the original AM implementation, a file with additional stems au-tomatically extracted from Anthony Salmon X  Arabic-English dictionary [ 31] (a work from the end of the 19th century encoded in TEI-compliant XML format within the Perseus project) was added to the system. Moreover, an analysis of most frequent types of unrecognized tokens allowed to add a limited, but important, number of additional lists of prefixes and suffixes together with the relative combination tables. The single more important addition was the set of prefixes, suffixes and combinatory rules for verb imperatives, a category entirely missing from the original AM imple-mentation  X  X erhaps on purpose, since Arabic imperatives are morphologically complex and quite rare in newspaper texts X  and relatively frequent in  X  ad  X  X  texts, given the abundance of prescriptions and performative contexts in the latter. 
To reduce the third problem detected, namely the genre and style indistinctness in the AM lexicon, we experimented with automatically remove items that are likely to correspond to contemporary foreign named entities, especially proper names and by exploiting a suggestion by Tim Buckwalter himself that in most cases a gloss starting with an uppercase letter is a named entities in 99% of cases; we after per-form a full-text search for each word in Salmon X  X  X  dictionary and retain only words found there. This way, we are likely to exclude most foreign contemporary named entities by retaining Arabic proper names and place-names which can be found in Classical texts and which are often (but unfortunately not always) included by Salmon X . Results of both HE and RAM have been submitted to standard practice of evaluation testing section has been held relatively small in consideration of the homogeneity of the corpus and the necessity to manually annotate the test sentences. At the present stage of development, the results obtained through both modules are brilliant but quite faceted. The total number of processed  X  ad  X  X  was 7305, and the segmentation pro-duced outputs for 7135 of them, showing an effectiveness X  rate of 97.7%. A manual screening of the testing  X  ad  X  X  sample showed a rate of 7.7% incorrect, of which 6.8% are false negatives and 0.9% false positives. segmentation testing of segmented data
The accuracy is going to be improved mainly by refining the above-mentioned op-erators, and secondly by raising human co ntrol on outputs whereas automatic recogni-tion is still impeded. 
As to RAM, the system was applied only on the effectively segmented matn text output by HE. We obtained a corpus that gathers only matn  X  X  section and consists of 382,700 words. Then we applied the original AM analyzer to get a preliminary system output; the results were then compared to the output of the RAM analyzer with different parameter settings. recognition unanalyzed 10.36% 12 .55% 7.23% 8.12% univocal 29.45% 58 .98% 62.52% 67.79% testing of segmented data error rate 60.54% 32.77% 27.6 5% 24.58% precision 64.90% 74 .57% 81.47% 83.37% recall 74.56% 92.6 6% 90.88% 92.05% F measure 69.40% 82 .64% 85.92% 87.50% 
The RAM system with vocalization fares far better than the original AM in un-ivocal token recognition, even if the rate of unanalyzed token is slight higher. In fact, the result is equivocal, since AM gets a better result at the price of a higher number of false positives (which RAM discriminates through vocalization). Both HE and RAM can be seen as starting points for future research. HE can be extended and generalized to other domains within Classical Arabic culture where texts are arranged according to semi-formal criteria: gen ealogical repertories, specialized dictiona-ries, definition lexica (as opposed to lexical encyclopedia). RAM can be further extend to cope with a larger domain of textual genres, especially if coupled with some reasonably well performing system of Arabic Named Entities recognition. As showed by the flow-systems of text analysis and information retrieval. 
