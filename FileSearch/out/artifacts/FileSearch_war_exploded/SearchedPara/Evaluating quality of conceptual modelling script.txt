 1. Introduction
Conceptual modelling plays an important role in information systems development (ISD) projects. A con-ceptual modelling script 1 is developed before designing and implementing an information system (IS). In order to perform its required functions, the IS requires general knowledge about its domain and the functions it has to perform, and this knowledge is captured in the conceptual modelling script [45] . The conceptual modelling script thus documents the common understanding that (future) system users, analysts and designers have about the domain and the functions imposed on the IS [45] . It is used as a communication, analysis and doc-umentation tool for domain knowledge and IS requirements, and it provides input for the system design pro-cess [62,66] .

Empirical studies point out that the quality of the conceptual modelling script affects the quality of the IS [20] . The script captures the requirements for the IS and is used to communicate these requirements between the ISD project members and stakeholders. Empirical studies show that more than half of the errors, which occur during system development, are exactly due to requirements errors [39] . Therefore the importance of high-quality conceptual modelling scripts cannot be overemphasized. Assuring the quality of conceptual mod-elling scripts leads to the early detection and correction of errors in ISD projects and is a necessary step for ensuring the quality of the final system.

The awareness of the importance of conceptual modelling script quality generated a large number of pro-posals of quality evaluation frameworks [39] . A first observation to make is that, despite this proliferation of quality frameworks, generally agreed quality measures still have to be developed [39] . Secondly, most of the research has focused on the use of the conceptual modelling script as a representational tool and/or as a tool for communication between analysts and developers, whereas the quality of the script as a supporting tool for user-analyst communication has largely been ignored [62] . With [40] as a notable exception, the quality of the conceptual modelling script has not been expressed explicitly from the point of view of the stakeholder who has provided input to the modelling process (i.e. during requirements elicitation) and who is asked to validate the script that the analyst has developed based upon this (and possibly other) input. Such script  X  X alidators X  (hereafter referred to as script users ) would include future IS end-users (though not necessarily all of them) and other domain experts that are consulted but who may not necessarily become IS end-users, but would not include system designers or other types of developers. Although system designers use conceptual modelling scripts as an input to the design process, they are not in the position to validate the scripts because the scripts are not based on developers X  requirements and views of the domain. Hence system developers will not fall within the category of script users as considered here.

As conceptual modelling scripts are used in the requirements discovery and validation stages of ISD, an interesting and relevant perspective on conceptual modelling script quality is the user X  X  perception of script usability. However, to date, research has proposed no instrument to evaluate how satisfied a user is with a conceptual modelling script that is intended to express his view of the domain and requirements that must be met by the IS.

The current study addresses this need by developing an evaluation framework that consists of a theoretical model explaining the relevant dimensions of conceptual modelling script quality for script users and a prac-tical instrument to measure these quality dimensions. The proposed user evaluation model combines different variables used in previous research and presents a measurement framework for evaluating the success or qual-ity of a conceptual modelling script from the user point of view. The measurement of conceptual modelling script quality is independent of the constructs proposed by particular conceptual modelling grammars. What-ever grammar is applied, practitioners can use the framework to assess and compare the quality of scripts that are generated using that grammar to find out which one is better. Two or more grammars might also be used to generate alternative  X  X nformationally equivalent X  scripts that can be evaluated to determine which one better supports the task at hand [66] . The proposed evaluation framework can thus also be employed by researchers in empirical studies that compare the relative effectiveness of competing grammars via the measurement of scripts generated using these grammars. Even with the same grammar, different methods can be employed that result in different scripts, and the measurement of these scripts may inform researchers about the relative effec-tiveness of the used methods. However, the proposed model and measurement instrument does not intend to directly measure grammar or method quality as it only applies to conceptual modelling scripts.
The remainder of the paper is as follows. Section 2 elaborates on the framework of Lindland et al. [36] , which provides a comprehensive and multi-faceted definition of quality in a conceptual modelling context.
This section also introduces the models of DeLone and McLean [14] and Seddon [52] for evaluating the effec-tiveness of information systems as implemented in organizations. We argue in Section 3 that these IS success models provide a suitable framework to evaluate the quality of a conceptual modelling script from the user perspective. In particular, we link Lindland et al. X  X  conceptual model quality framework with Seddon X  X  IS suc-cess model to determine appropriate dimensions for evaluating the quality of conceptual modelling scripts from the user point of view.

In Section 4 , we investigate the relationships between the different dimensions of conceptual model quality and generate testable, causal research hypotheses about these relationships. Since the relationships in Seddon X  X 
IS success model have been validated using theories from social psychology (in particular, the Theory of Rea-soned Action (TRA) and the Theory of Planned Behaviour (TPB) [50] ), we adopt the same ideas to support the hypothesized relationships in our model.

In Section 5 , we describe the design of two experiments that aim at testing the proposed user evaluation model. This section also presents the measurement instrument that can be used in conjunction with the model.
Next, Section 6 presents the analysis of the collected data and evaluates the explanatory and predictive power of the model. To end the paper, we conclude in Section 7 with a discussion of the contributions so far, the limitations of the current study, and an outline of future research directions. 2. Background 2.1. Evaluating quality of conceptual modelling scripts
Lindland et al. [36] were the first to articulate a systematic framework to help understanding quality in the context of conceptual modelling. Previous attempts merely resulted in lists of unstructured, imprecise and often overlapping quality properties [40] . Although alternative frameworks have been developed after Lind-land et al. X  X  proposal (e.g., Kesh [31] , Schu  X  tte and Rotthowe [51] , Moody and Shanks [40] ), the Lindland et al. framework is the only one that has both a theoretical basis and has been empirically validated (see Moody et al. [41,42] ).
 The theoretical basis for Lindland et al. X  X  framework is grounded in linguistics and semiotics. According to
Lindland et al. [36] modelling is essentially making statements in some language. Fig. 1 depicts the main ele-ments of the framework:  X  Language: all the statements that are allowed according to the syntax of the grammar used.  X  Domain: all possible statements that would be relevant and correct for describing the problem domain.  X  Model: the set of statements actually made (i.e. the script).  X  User Interpretation: the set of statements that readers think the script contains.

The framework distinguishes three types of script quality based on the correspondence between different sets of statements: Syntactic quality (i.e. correspondence Model  X  Language) describes how well the script adheres to the rules of the grammar. Semantic quality (i.e. correspondence Model  X  Domain) describes how well the script reflects the reality modelled. Finally, pragmatic quality (i.e. correspondence Model  X  User Interpretation) captures how well the script is understood by its users.

Lindland et al. X  X  framework suggests that a systematic evaluation of quality considers a script X  X  syntax, semantics and pragmatics. In practice, syntactic quality issues in conceptual modelling seem to be well con-trolled [48] and can be objectively measured [35] . Therefore, the main evaluation effort would be directed towards semantic and pragmatic quality. 2.1.1. Semantic quality
The semantic quality of a conceptual modelling script is difficult to evaluate directly as it is hard (and per-haps even impossible) to know reality, externalize this knowledge (which would mean building another script) and agree upon it. When evaluating semantic quality, users can only refer to their perception of reality, which is obtained through observation and internalization. Which filter is put upon reality by our observations pos-sibly depends on many factors such as previously acquired knowledge, perceptual psychology effects, cognitive abilities, and ontological and epistemological standpoints taken.

Krogstie et al. [34] extended the Lindland et al. framework with a fourth quality type, namely perceived semantic quality ( Fig. 2 ). 3 This type of quality is defined as the correspondence between the information that users think the script contains (User Interpretation) and the information that users think the script should contain, based upon their knowledge of the problem domain (Domain Knowledge). Perceived semantic qual-ity serves as an operational surrogate of semantic quality as it does not require verifying the correspondence between Model and Domain, but between the knowledge that users have of Model (i.e. User Interpretation) and Domain (i.e. Domain Knowledge).

Perceived semantic quality should be easier to measure than semantic quality. Nevertheless, the user per-ception of semantic quality has barely been investigated. The only study we found was Dunn and Grabski [18] , who compared the perceived semantic quality of conceptual models of accounting systems that were based on different accounting paradigms. The other studies on semantic quality that we identified have chosen other substitutes than Domain Knowledge for Domain. Examples include studies employing meta-model analysis [28] , comparison against reference models [47] and ontological analysis [24] , although the latter type of analysis is primarily aimed at evaluating grammar quality. In particular the Bunge X  X and X  X eber (BWW) ontology [63 X 65] has often been used as a  X  X eference theory X  for the real-world. A BWW analysis of a concep-tual modelling grammar looks for ontological deficiencies (i.e. cases of construct excess, redundancy, overload and deficit) that indicate (i) lack of ontological clarity or (ii) ontological incompleteness. This means that scripts constructed with the grammar (i) may contain elements that cannot be mapped to domain concepts or where the mappings are ambiguous (e.g. synonyms or homonyms), or (ii) may not contain representations for all relevant domain concepts.

Using an ontologically clear and complete grammar does not guarantee the quality of the generated scripts (i.e. poor scripts can still be made), but they do make it easier to create good scripts. BWW analyses of con-ceptual modelling grammars therefore allow making predictions about the semantic quality of the scripts gen-erated by these grammars. Empirical studies testing ontological propositions (e.g., Burton-Jones and Weber [8,9] , Gemino and Wand [25] ) have investigated effects on conceptual modelling script comprehension (i.e., pragmatic quality) or on the acquisition of domain understanding, but have not directly measured the user X  X  evaluation of how faithful the script reflects the problem domain. We postulate that it is the perception of semantic quality, rather than a theoretically predicted semantic quality, that will determine whether a user will evaluate a conceptual modelling script as usable for communicating his view of the domain and the require-ments that the IS must meet. Therefore an empirical approach that recognizes possible differences in the sub-jective perception of semantic quality is needed to complement the more objective, theoretically-oriented evaluation approaches. 2.1.2. Pragmatic quality
Several measures and instruments have been proposed for evaluating pragmatic quality. For instance, when comparing alternative conceptual modelling grammars or methods, resultant scripts have been compared with respect to how well they are understood by users. In empirical studies, comprehension task performance has been proposed as an operational definition of pragmatic quality. Such tasks should only test whether users comprehend the explicit semantics expressed in a conceptual modelling script [46] . Measures used for compre-hension task performance include task accuracy, completion time, and normalized accuracy (i.e. accuracy divided by completion time) (e.g. [32,60,4] ).

In some empirical studies problem solving tasks have been used as an instrument to measure understanding (e.g. [4,25,7] ). The answer to a problem solving question is not literally present in a conceptual modelling script, but requires the problem solver to reason about the domain that is represented in the script. In these studies a conceptual modelling script is seen as a tool to learn about a domain. If the a priori domain knowl-edge of the script user is low then the performance on a problem solving task is a measure of how well the conceptual modelling script has helped the user to understand that domain. Although the performance on problem solving tasks is a measure of domain understanding rather than script understanding (i.e. pragmatic quality as defined in Lindland et al. [36] and Krogstie et al. [34] ), it does relate to the effectiveness of concep-tual modelling script use. In the most recent SEQUAL version of Krogstie et al. X  X  framework, the pragmatic quality of a conceptual modelling script has been redefined as the model X  X  ability to facilitate learning and action [35] . Hence, while not directly measuring script understanding, according to this new view problem solving task performance may be an indicator of the pragmatic quality of conceptual modelling scripts if they are used to increase the knowledge of their users.

We are not aware of any studies that have directly measured how well users thought they understood a con-ceptual modelling script. Some studies (e.g. [9,25,7] ) have measured, apart from task performance, the user X  X  perception of how easy or difficult it was to understand a script. In the broader Task-Technology-Fit (TTF) research context, it has been shown that technology users X  perception of task performance is related to their evaluation of the technology used [27] . Perceptions of usability (i.e. the ability to succeed in a task), including perceptions of both efficiency and effectiveness, have also been measured in experimental research on concep-tual modelling. Such measures include perceived value [32] , perceived ease of use [32,17,19] and user satisfac-tion [17,19] measures. Especially the perceived ease of understanding measure [9,25,7] already mentioned can be seen as an indirect user evaluation of pragmatic quality. 2.2. User evaluation of conceptual modelling script quality based on IS success models
For information systems, several evaluation frameworks that recognize subjective user perceptions have been proposed in the literature. A well-known framework is the DeLone and McLean [14] model of IS success.
We contend that the basic structure of IS success models (such as the DeLone and McLean model) provides a suitable basis for a user evaluations based model of conceptual modelling script quality because the IS effec-tiveness or success dimensions in these models can also be applied to conceptual modelling scripts. For instance, Seddon et al. [55] argue that IS success models can be applied to different types of  X  X ystems X . They showed that IS effectiveness measures are suitable for evaluating any aspect of an ISD methodology. Since conceptual modelling is a technique used during systems development for discovering and validating require-ments, IS success variables can be used to evaluate conceptual modelling scripts (as a kind of preliminary ver-sion of the system). Furthermore, given the impact that conceptual modelling script quality has on the quality of the system that is based on the script, it is logical and justified to link research on conceptual modelling quality to IS effectiveness research.

Based on similar arguments, Sedera et al. [56] founded their framework for measuring the success of process modelling activities in organizations on the IS success model of DeLone and McLean. Additionally, they referred to the theory of Planned Organizational Change [26,68] where the acceptance and use of a new system is considered to be dependent on the quality of the implementation process. In other words, the evaluation of the ISD process and its intermediate artefacts is equally important and similar to the evaluation of the actual resulting system.

Based on these thoughts, we adopted the IS success model of DeLone and McLean as the starting point for identifying the relevant user perceived quality dimensions of a conceptual modelling script. Using the commu-nications research of Shannon and Weaver [58] , DeLone and McLean (henceforth D&amp;M) identified six major interrelated dimensions of IS Success ( Fig. 3 ): System Quality, Information Quality, Use, User Satisfaction,
Individual Impact and Organizational Impact. The D&amp;M model specifies that System Quality and Informa-tion Quality affect both Use and User Satisfaction, which are direct antecedents of Individual Impact. This will in turn have an impact on organizational performance. Furthermore, D&amp;M expect a two-way causal relation-ship between Use and User Satisfaction, which signifies that Use causes User Satisfaction and vice versa.
Empirical tests of the D&amp;M model mostly supported the model and its relationships and contributed to a better understanding of IS success and its dimensions [15] . The greatest challenge to the D&amp;M model comes from Seddon [52] in terms of his re-specified model of IS success. Seddon criticizes the D&amp;M model on com-bining causal and process explanations of IS success, which leads to confusing interpretations of the Use con-struct (i.e. causal: Use as a proxy for Benefits From Use; process: Use as the predicted variable Future IS Use and as a necessary process for outcomes such as User Satisfaction, Individual Impact and Organizational
Impact). Consequently, Seddon proposed a re-specified model of IS success where the original D&amp;M model is split into two variance sub-models (one model to assess IS success and one model to predict use as a behav-iour). In the sub-model for IS success ( Fig. 4 ), Seddon retains System Quality and Information Quality and claims their causal impact to two frequently used perceptual and satisfaction outcomes measuring the benefits of system use: Perceived Usefulness and User Satisfaction. Additionally, it is postulated that Perceived Use-fulness influences User Satisfaction. Besides these user evaluations of the net benefits of system use, other net benefits to individuals, organizations and society are represented in the model by means of the Net Benefits construct. These net benefits are supposed to have a direct causal connection with the perceptual and satisfac-tion outcomes. Finally, observe the feedback loop through the variance sub-model of future system use. The main ideas here are that the user satisfaction with a system shapes the user expectation about the benefits of continued or future system use and that the user observations of and experiences with the consequences of this use will determine the perceptions of IS success in the future.

In the following section we will show that Seddon X  X  model provides appropriate quality dimensions for the user evaluation of conceptual modelling scripts. We will argue that Seddon X  X  Information Quality construct corresponds to Krogstie et al. X  X  notion of perceived semantic quality, that the System Quality construct can be approximated by the script X  X  perceived ease of understanding (and thus indirectly relates to Lindland et al. X  X  pragmatic quality type), and that Perceived Usefulness and User Satisfaction correspond to perceptions of a script X  X  usability and the users X  overall quality evaluation. 3. Conceptual modelling script quality dimensions from the user point of view
Seddon X  X  re-specified D&amp;M IS success model is used to guide the development of our user evaluation model for conceptual modelling script quality. We argue that there are clear parallels between the perceptual and satisfaction constructs of the re-specified D&amp;M model and perceptual conceptual modelling script quality and satisfaction constructs. The model we propose is shown in Fig. 5 . In this section we justify our choice of user conceptual modelling script quality dimensions (i.e. the model constructs). In the following section we hypothesize and theoretically underpin a number of causal links between these dimensions (i.e. the model relationships). 3.1. Perceived semantic quality as information quality
Information Quality (IQ) refers to the quality of the information the system produces and to the degree this information output matches the needs of the users in terms of accuracy, reliability, relevance, completeness and precision of information [2,67] . So, users assess the value of the information with respect to desired char-acteristics such as accuracy, meaningfulness, completeness, timeliness, etc. These information quality attri-butes have been extensively studied in IS research [15] . Nelson et al. [44] remark that the evaluation of the different information characteristics from a user X  X  perspective is fairly subjective and relative to the specific context and task at hand.

According to DeLone and McLean [14] , IQ represents a success measure at the semantic level of informa-tion in Shannon and Weaver X  X  communication theory. The semantic level concerns how well the information conveys the intended meaning. Similar to an information system, the purpose of a conceptual modelling script is to assist people in performing particular tasks. Users therefore evaluate how well the script serves this stated purpose. If users see the script as a vehicle for communicating system requirements to the analyst, and from there on to the system developers, they will evaluate the script with respect to how well it expresses their view of the domain and the requirements that must be met by the system. In other words, they will evaluate the script taking into account the quality (i.e. accuracy, relevance, completeness, etc.) of the script X  X  contents, meaning the information conveyed by the script.

In terms of the Krogstie et al. [34] framework of conceptual modelling script quality, the IQ of a script thus corresponds to its perceived semantic quality. Users will perceive the semantic quality of the script (in the way they understand it) as how valid and complete it is with respect to their perception of the problem domain.
Validity means that all information conveyed by the script is correct and relevant to the problem whereas com-pleteness entails that the script contains all information about the domain that is considered correct and rel-evant [36] . Consequently, quality properties for conceptual modelling scripts mapped by Lindland et al. [36] onto their semantic quality construct include correctness, completeness and consistency.

These parallels justify the reformulation of the IQ construct by Krogstie et al. X  X  [34] Perceived Semantic Qual-ity (PSQ) construct when applying Seddon X  X  re-specified D&amp;M IS success model to conceptual modelling scripts. 3.2. Perceived ease of understanding as system quality
System Quality (SQ) considers desired characteristics of the system that produces the information output such as the consistency of the user interface, ease of use, documentation, etc. [52] . The perception of system quality is formed through interaction with the system when users complete a specific task. Of course, not all
SQ dimensions are immediately applicable to conceptual modelling scripts as  X  X ystems X  (e.g. accessibility, response time, reliability). However, SQ has several times been represented and measured by  X  X ase of use X , which refers to the  X  X ser friendliness X  of the system [50,53] . Ease of use is defined as  X  X  X he degree to which a person believes that using a particular system would be free of effort X  X  [13, p. 320] .
 This consideration makes it possible to also apply this success dimension in a conceptual modelling context.
When users  X  X nteract X  with a conceptual modelling script, they will evaluate how well the script serves its pur-pose in terms of containing the right information (i.e., semantic quality) but also in terms of how easy it is to read this information and interpret it correctly. Hence, analogous to the above definition of Davis, we can define the ease of using a conceptual modelling script as the degree to which a person believes that using a conceptual modelling script for understanding the problem domain and IS requirements would be free of men-tal effort. We will refer to this belief as the Perceived Ease Of Understanding (PEOU) a conceptual modelling script and use it to specialize the SQ construct of Seddon X  X  re-specified D&amp;M model. As mentioned in the pre-vious section, PEOU (sometimes also called Perceived Ease of Interpretation [25] ) has been used in experimen-tal research on conceptual modelling as an indirect user evaluation of pragmatic quality. 3.3. Perceived Usefulness (PU)
Although Use, one of the six categories of IS success in D&amp;M X  X  original model, has often been reported as the objective dependent variable in IS success research studies, we agree with Seddon [52] that the degree to which a system is used is not an appropriate measure of IS Success. Use does not necessarily mean success just as non-use is not equal to failure. Also in our conceptual modelling context, Use seems not appropriate as a success dimension. Consequently, Seddon [52] replaced Use by the more relevant measure Perceived Useful-ness (PU) since it is not use but the benefits from use that determine whether an IS is successful. Seddon defines PU as  X  X  X he degree to which a person believes that using a particular system has enhanced his or her job performance X  X  (adapted from [13, p. 320] ). Users evaluate the IS with respect to its usefulness after using the IS for completion of a certain task; in other words they make an ex post assessment of PU. Usefulness appears to be also a valuable evaluation aspect for conceptual modelling scripts (e.g. Kim and
March [32] ). After interacting with the conceptual modelling script (e.g. during requirements validation activ-ities), users will make judgments about how effective the script is in expressing and communicating their view of the domain and the IS requirements. Hence, users will evaluate the usability of the script [62] , which will amongst others shape their overall quality judgment. PU is the third construct in our user evaluation model for conceptual modelling scripts. 3.4. User Satisfaction (US)
User Satisfaction (US) continues to be the most commonly used success measure and is regarded as the most general measure of IS success [53] . The extensive US research has lead to the development of reliable and validated multi-attribute satisfaction measures with varying meanings of the underlying US construct.
Consequently, we first need to clarify the meaning of US when adopting this success dimension and related measures in a conceptual modelling context.

Seddon [52, p. 246] defines US as  X  X  X  subjective evaluation of the various consequences evaluated on a pleas-ant X  X npleasant continuum X  X . US as defined by Seddon is concerned with users X  overall level of satisfaction and does not directly refer to causal antecedents like IQ and SQ. Consistent with the definition of Seddon [52] ,
Gelderman [23, p. 12] defines US as  X  X  X he extent to which information requirements are met X  X . Similarly, Ives et al. [30, p. 785] defined  X  X ser information satisfaction X  as  X  X  X he extent to which users believe the information system available to them meets their information requirements X  X . For the context of our study, we adopt this generic view of US. In our opinion, a general evaluation of the quality of a conceptual modelling script can thus be measured in terms of how satisfied users are with the script with respect to its purpose.
A measure to assess overall US and effectiveness directly has been constructed by Seddon and Yip [54] .An adaptation of the US measure of Seddon and Yip [54] has been employed in empirical comparisons of con-ceptual modelling scripts [17,19] . As such, it seems justified to include US in a conceptual modelling script evaluation framework from a user X  X  point of view. 4. Relationships between the proposed quality dimensions
Until now we have drawn upon IS success research to identify candidate conceptual modelling script qual-ity dimensions from the user X  X  point of view. The meaning of the four perceptual and satisfaction constructs in
Seddon X  X  re-specified D&amp;M IS success model has been clarified for an application context that requires user evaluation of conceptual modelling scripts. This lead to the selection of four related but distinct evaluation model variables: Perceived Semantic Quality (PSQ), Perceived Ease Of Understanding (PEOU), Perceived Usefulness (PU) and User Satisfaction (US).

A second research goal is to theorize and test the hypothesized causal relationships between the selected model variables. The original D&amp;M model and Seddon X  X  re-specified D&amp;M model propose SQ and IQ as cau-sal antecedents of US. Additionally, Seddon states a direct impact of IQ and SQ on PU (which is not a var-iable in the D&amp;M model). Further, PU is supposed to affect US. Previous research has found empirical support for these relationships [50,53] . Given the parallels between the constructs in Seddon X  X  model and our proposed evaluation model, we can directly adopt these five relationships in our user evaluation model for conceptual modelling script quality. However, to make our hypotheses stronger we will also theoretically underpin the proposed model relationships. In the first sub-section we present a theoretical basis for the model relationships in Seddon X  X  and our model. In the second sub-section we argue why we believe these relation-ships are plausible in the context of user evaluation of conceptual modelling scripts.
 4.1. Theoretical underpinning of the relationships between user evaluation dimensions
A theoretical basis for hypothesizing causal relationships between the four user evaluation dimensions can be found in Fishbein and Ajzen X  X  [21] theory of reasoned action (TRA) and the subsequent theory of planned behaviour (TPB) [1] . TRA suggests that a person X  X  behaviour is determined by behavioural intention which, in turn, is influenced by a person X  X  attitude. A person X  X  attitude towards a concept requiring or involving some human action can be defined as  X  X  X  person X  X  general feeling of favourableness or unfavourableness for the concept X  X  [21, p. 6] . An attitude can thus be seen as a person X  X  overall evaluation of the concept.
According to TRA, external stimuli will influence a person X  X  attitude indirectly by influencing his or her sali-ent beliefs about performing a specific behaviour. TRA considered  X  X ubjective norms X  or the perceived opin-ions of  X  X ignificant others X  about the behaviour as a second determinant of a person X  X  intention to act. TPB added perceived behavioural control as a third factor that contributes to the formation of behavioural inten-tion. Perceived behavioural control refers to a person X  X  perceptions of his ability to perform a given behav-iour. Just as behavioural beliefs form a favourable or unfavourable attitude toward the behaviour, perceived subjective norm and perceived behavioural control will result from normative and control beliefs, respectively [1] .

TRA has been, and continues to be, one of the most influential theories in social psychology. It has been applied and proven successful to explain and predict people X  X  behaviour in several domains, including the IS discipline. The Technology Acceptance Model (TAM) developed by Davis [12] to explain why users accept or reject information technology is based on TRA. The intention to use an information technology, which deter-mines whether one will actually use the technology, was hypothesized and empirically proven to be a function of two pervasive beliefs: perceived ease of use and perceived usefulness. Furthermore ease of use also indirectly affects usage intention through its effect on usefulness. Similarly as TRA and TBP, TAM considers three cat-egories of variables: beliefs about using the system, attitudes about using the system and usage intention/ behaviour [50] .

This underlying categorization is useful when reasoning about possible relationships between the four user evaluation dimensions. The validity of the relationships in Seddon X  X  model has been investigated by Rai et al. [50] by treating the constructs at the higher level of beliefs, attitudes, and intention/behaviour in accordance to the TRA, TPB and TAM. Seddon considers three classes of variables: measures of information and system quality, general measures of the net benefits of IS use and behavioural measures with respect to IS use.
According to Rai et al. [50] Seddon X  X  measures of information and system quality (i.e. IQ and SQ) represent beliefs about the system, while the general measures of the net benefits of IS use (i.e. PU and US) represent attitudes towards the use of the system. Consistent with TRA/TPB the IQ and SQ beliefs are hypothesized to contribute to the formation of the PU and US attitudes. In other words, the user X  X  overall evaluation of the usability of the system is likely to be influenced by his beliefs about information and system quality.
The third category of variables, consisting of behavioural measures, is found in the behavioural sub-model reformulated and represented in our user evaluations based quality model for conceptual modelling scripts. Table 1 summarizes the belief X  X ttitude X  X ntention/behaviour categorization for all these models and shows how the four constructs of our user evaluation model fit this categorization. Note that this categorization was already indicated in Fig. 5 . 4.2. Proposed relationships between the conceptual modelling script quality dimensions
Adopting the ideas expressed in TRA, TPB and TAM, we propose five relationships between the concep-tual modelling script quality dimensions (see Fig. 5 ). The model proposes that changes in a user X  X  beliefs about the quality of a conceptual modelling script (PSQ and PEOU) will cause changes in that person X  X  overall eval-uation of the usability of the script (PU and US).

Just as SQ and IQ are independent of each other and based on their specific meaning, we argue that for conceptual modelling scripts PSQ and PEOU are not related. These two evaluation aspects, which represent specific beliefs about the quality of the conceptual modelling script, are likely to have independent impacts on
US. A conceptual modelling script that users perceive as easy to understand will make the user more  X  X atisfied X  about the script X  X  ability to communicate the user view of the domain and the IS requirements. Previous research has also suggested that higher perceived semantic quality may result in higher user satisfaction [18] . In Seddon X  X  model, US is additionally impacted by PU which represents an attitudinal measure of net benefits. Contrary to the TAM construct of PU which is future oriented and as such represents beliefs about future use, the PU construct in Seddon X  X  model and our model relates to attitudes formed by perceptions from past use. So, if after interacting with the conceptual modelling script users judge the script to be usable for expressing and communicating their view of the domain and the IS requirements, they are also likely to be satisfied with the script.

These considerations form the basis for the causal relationships 1, 2 and 3 in Fig. 5 , which we formulate here as testable hypotheses:  X  H1: Increases in PEOU will cause increases in US.  X  H2: Increases in PSQ will cause increases in US.  X  H3: Increases in PU will cause increases in US.

Furthermore, we hypothesize that PU, as an attitudinal measure of net benefits, is influenced by beliefs about the quality of the conceptual modelling script as captured by PSQ and PEOU. If a user is confronted with two conceptual modelling scripts that he believes are informational equivalent [59] , then he is likely to find the script that is easier to understand more useful. We therefore hypothesize that PEOU has an impact on PU.

Apart from evaluating the pragmatic quality of a conceptual modelling script, users also form a perception believe that the script is invalid and/or incorrect with respect to the problem domain, they are likely to develop a less favourable feeling of usability of the script. Hence also an impact of PSQ on PU is hypothesized.
These hypothesized relationships are again consistent with the beliefs-attitudes link of TRA/TPB. Of course, user perceptions of usefulness may be affected by other factors than PSQ and PEOU (e.g. perceived value of the conceptual modelling grammar [32] ) but including these two relationships allows investigating and quantifying to what extent perceived usefulness is affected by these perceived script qualities. Conse-quently, we formulate the last two causal model relationships:  X  H4: Increases in PSQ will cause increases in PU.  X  H5: Increases in PEOU will cause increases in PU. 5. Research method
To empirically test the hypothesized relationships we conducted two experiments. The first experiment (E1) took place in November 2004 while the second experiment (E2) was conducted in November 2005. Since both experiments are similar we will present the design of these experiments together but will comment on any pos-sible differences between them.
 5.1. Participants
The two experiments required participants to evaluate a number of domain models (in Entity-Relationship (ER) diagram format) for an example commercial company. The experimental participants were 187 (E1) and 124 (E2) business students enrolled in a junior-level Management Information Systems (MIS) course. The groups of business students participating in the studies cannot be considered as representative for the entire target population (i.e. IS users or domain experts in their capacity of conceptual modelling script user). Given their limited experience with information systems and ISD projects, these groups of students approximate samples of novice users, rather than experienced users. Student participants form a homogeneous group with respect to their educational background and working experience. Using students instead of practitioners allows easier control of human factors (e.g. training, technical modelling skills, task-related experience) that are likely to impact user attitudes [62] and thus confound the impact of the exogenous constructs (PSQ and
PEOU) on the endogenous constructs (PU and US). Moreover, the experimental task does not require high levels of industrial experience, so given the recommendations of Basili et al. [3] experiments with students are justified.
 Conceptual modelling is a key module of the MIS course from which the study participants were drawn.
Acquiring modelling skills and in particular being able to understand and validate scripts developed by ana-lysts, is essential for future business professionals. Conceptual modelling scripts capture domain knowledge and IS requirements that have been elicited by analysts and future IS end-users and domain experts must be able to validate the scripts developed by the analysts. Understanding what is modelled in a script is a first step in this validation process [32] .

Apart from studying the semantics of the ER conceptual modelling grammar, students were shown exam-ples of and learned to read ER diagrams of various domains (e.g. university personnel management, hospital operations) with the purpose of understanding the domain information conveyed by the diagrams. The sub-sequent course module exercises required students to analyze ER diagrams by answering comprehension ques-tions. These questions were chosen to mimic real-life situations where business professionals are confronted with tasks requiring a correct interpretation of the reality shown in the diagrams. 5.2. Experimental materials
The experiments were conducted after the conceptual modelling module of the course. In the experiments, each participant received one of several alternative ER diagrams depicting a structural view of some part of the business. The participants in the first experiment worked with a conceptual modelling script of an inte-grated sales and acquisition process of a fictitious company selling surf-boards, while the script in the second experiment represented the hiring, engagement, and paying of consulting services by some (not further iden-tified) company. It is important to note that the scripts used were domain models rather than behavioural models. They showed the business domain concepts (e.g. agents, resources, events) involved in a business pro-cess and the properties of these concepts (attributes and relationships), but not the sequencing, synchroniza-tion, etc. of the different process steps.

In the first experiment, two distinct representations of the buying/selling process were used representing a spread in semantic quality. The script of inferior semantic quality had some shortcomings as opposed to the script of high semantic quality with respect to various semantic quality properties as identified in Lindland et al. [36] (i.e. there were cases of incompleteness, incorrectness, irrelevancy and inconsistency). In the second experiment, the two scripts of the consulting services process were informational equivalent, but we expected them not to be computational equivalent [59] for our experiment participants. One script was centred around an instantiation of the generic transaction pattern portrayed in the Resource Event Agent (REA) Template [37] that the students had studied in class. The other script was derived from this REA-based script by infor-mation-preserving transformations involving mainly the physical repositioning of the entities and relation-ships on the diagram layout. These transformations aimed at creating a script in which the REA Template was no longer easily and quickly recognizable to REA trained users. The second script might therefore be per-ceived as less readable by people that are trained in REA modelling as it does not match their frame of ref-erence. The conceptual modelling scripts used as experimental objects in the two experiments are included in
Appendices A and B . The labels on the E1 diagram elements (i.e. entity names, relationship names, attribute names) are in Dutch, whereas they are in English on the E2 diagrams.

The experimental task involved evaluating the conceptual modelling script received. Such evaluation requires the participants to form a judgment about the script X  X  quality and therefore they had to interact with the script. Prior to evaluating the script, students performed a comprehension and validation task in which they were presented a series of questions requiring them to retrieve information from the script and verify it against a textual scenario describing the process. As the script was the only information source available for answering the questions, participants were  X  X orced X  to make an effort to understand the script. 5.3. Measures
In order to evaluate the PSQ, PEOU, PU and US constructs, measures are needed. A literature search revealed the existence of validated multi-item measures for all four constructs. Table 2 shows the measurement instrument that we composed to be used in conjunction with our evaluation model. For each item statement a 7-point Likert scale with response options ranging from  X  X trongly disagree X  to  X  X trongly agree X  was offered.
Because the experimental objects were domain models of business processes, some of the item statements refer to  X  X  X rocess X  X  or  X  X  X usiness process X  X . This wording needs to be changed if the instrument is applied to other kinds of conceptual modelling scripts. For the rest the instrument can be reused as it is.

The measures for PEOU, PU and US have their origin in IS research on IT adoption, IS effectiveness and user satisfaction, where they have been frequently used. The PEOU items were proposed by Gemino and Wand [25] and are adapted from Moore and Benbasat X  X  [43] perceived ease of use measure for IT innovations.
Further, Moody X  X  [38] PU measure is used which is itself based on Davis X  [13] PU measure for the TAM. To measure US, Seddon and Yip X  X  [54] overall User Information Satisfaction measure was chosen. This measure has been reworded by Dunn and Grabski [19] for measuring satisfaction with conceptual modelling script use, and it is this version that is used in our instrument.

A measure for PSQ was developed by Poels et al. [49] . Candidate measurement items were generated from a definition of semantic quality derived from two theory-based conceptual modelling quality frameworks, i.e. the semiotics-based quality definitions of Lindland et al. [36] and the BWW ontology-based quality definitions of Shanks et al. [57] . Lindland et al. [36] distinguish two orthogonal dimensions of semantic quality, validity and completeness (confer supra), and a limited number of sub-dimensions that can mathematically be expressed in terms of validity, completeness or both (e.g. consistency is subsumed by validity and complete-ness; correctness, relevancy and minimality are subsumed by validity). The four quality criteria that Shanks et al. [57] propose for conceptual modelling scripts are loosely derived from the BWW classification of onto-logical deficiencies of conceptual modelling grammars: a script should be correct (no construct excess), conflict-free (no construct overload), complete (no construct deficit) and contain no redundancies (no con-struct redundancy).

The proposed items were validated and refined in two empirical studies until their convergent and divergent validity was deemed satisfactory. Items with undesirable psychometric properties were removed to purify the measure. The remaining five items of the PSQ measure are included in Table 2 . 5.4. Operational procedures
The two experiments were organized as class room exercises. The students of the MIS course were informed beforehand that the exercise was part of a research study and that additional data in the form of question-naires would be collected. However, no information was given with respect to the research question that would be tested (to avoid experimenter bias).

When students entered class, they received a sheet containing instructions and asking for their name. Next, they were given the script and the list of problems they had to solve. After finishing the comprehension/val-idation task, they were asked to complete the PEOU X  X SQ X  X U X  X S questionnaire.

Participation was strictly voluntary. In order to increase the motivation to participate (and perform well), the students were told that a similar exercise (meaning the prior comprehension/validation task) could be part of the final course exam and feedback on student performance was promised. Furthermore, in E1 optional course credits could be earned, while in E2 four prizes (i-Pod Shuffles and Nanos) were distributed to the best performers. With hindsight this incentive schema might not be considered ethical for the E2 students because of the (expected) difference in computational equivalence between the two scripts. For E1 there was no prob-lem as the correct answer to a same question for the two scripts could be different because of differences in information content. Anyway we do not expect response bias for the experimental task proper (i.e. expressing an opinion on the 16 PEOU, PSQ, PU and US item statements) because students were only evaluated with respect to their performance on the comprehension/validation exercise. 6. Research findings
To analyze the collected PEOU X  X SQ X  X U X  X S data we need a technique that tests the reliability and validity of the employed measurement instrument (e.g. Confirmatory Factor Analysis) and that tests the hypothesized causal relationships between the model constructs (e.g. Ordinary Least Square Regression). Structural Equa-tion Modelling (SEM) is a data analysis methodology that combines these psychometric and econometric per-spectives. Partial Least Squares (PLS) is one such technique within the class of SEM-based techniques, that allows testing the psychometric properties of the instruments used to measure unobservable variables (i.e. mea-surement models ), as well as estimating hypothesized structural models of relationships among multiple model variables [10] . Although PLS provides no overall measures of a model fit (as other, covariance-based SEM techniques like LISREL do), it emphasizes the prediction of variation and causality. PLS thus identifies the variance explained in the data and the relationships between model constructs.

Compared to covariance-based SEM, PLS is more flexible with respect to distributional characteristics of the data and minimum sample size [10] . PLS does for instance not require the data to be normally distributed, which makes it an attractive technique to analyze ordinal data obtained from Likert scales. The minimum sample size for a PLS study should be equal to the larger of the following: ten times the scale with the largest number of formative indicators (confer infra) or ten times the largest number of structural paths directed at a particular construct in the structural model [10] . The measure with the largest number of formative indicators is PSQ (5 items). The construct with the largest number of structural paths directed towards it is US (three relationships). Hence, the minimum sample size is 50, which is exceeded in both experiments. For covari-ance-based SEM a minimum sample size of 200 is usually recommended, which would not be met by either experiment.

However, our main motivation for choosing PLS is that it can handle both reflective (effect) and formative (causal) indicators [16] . The predominant perspective in SEM is reflective measurement where we think of indicators as effects. That is the underlying concept is thought to affect the indicators [5] . This means that a change in the latent variable will be reflected in a change in all indicators. An alternative measurement per-spective is based on the use of formative (causal) indicators, and involves the construction of an index rather than a scale. Here, it are the indicators that are assumed to cause an emergent variable [6] . Since the variable is now formed by its indicators, a change in the emergent variable is not necessarily accompanied by a change in all of its indicators; rather if any one of the indicators changes, then the emergent variable would also change.
In our measurement model, the five PSQ items in the measure of Poels et al. [49] should be conceptualized as formative (causal) indicators since PSQ results from the assessment of specific elements that all contribute in their own way to the semantic quality of a script. The five PSQ items reflect different semantic quality aspects: correctness (PSQ 1 ), realism (PSQ 2 ), consistency (PSQ (PSQ 5 ). These measure items were derived from the conceptual definition of semantic quality in the framework of Lindland et al. [36] and from the BWW ontology-based semantic quality (called faithfulness) definition of
Shanks et al. [57] . In both frameworks different semantic quality dimensions have independent impacts on (or cause) the higher-order construct of semantic quality. For instance, a conceptual modelling script can be com-plete with respect to the problem domain, and at the same time contain incorrect or irrelevant facts. Also, if an improvement in the correctness of the script is perceived by the script users, then PSQ will be higher, even if the completeness of the script has not changed. 6.1. Assessing reliability and validity
Although PLS estimates both item loadings and structural paths simultaneously, we follow a two-step approach in evaluating PLS models [29] . In this first sub-section, the measurement model is evaluated to assess reliability and validity. In the following sub-section, the hypothesized structural model of relationships is tested (hypotheses testing). 6.1.1. Reflective indicators
In our measurement model, PEOU, PU and US are, as in previous research, operationalized as latent vari-ables. In PLS the adequacy of the measurement model with respect to reflective indicators (i.e. the items for
PEOU, PU and US) is assessed by examining the individual item reliabilities and by evaluating the convergent and discriminant validity of the construct measures [10] .

First, individual item reliabilities were assessed by examining the loadings of all items on their respective constructs ( Table 3 ). Only items with loadings of at least 0.60 are considered reliable for measuring the con-struct for which they are intended and should be retained in the final measurement model [10] . All items of
PEOU, PU and US demonstrate a good level of reliability, except for PEOU the threshold.
 Next, the convergent validity of the different construct measures was examined by computing the Internal Composite Reliability (ICR) internal consistency measure, which is the PLS equivalent of Cronbach X  X  alpha.
In this study the ICR value of every construct measure in the final measurement model was higher than 0.7, the suggested value by Fornell and Larcker [22] for measures to be deemed reliable (see also Table 3 ).
Additionally, convergent validity was investigated through the Average Variance Extracted (AVE) mea-sure. AVE is defined as the average variance shared between a construct and its items. Fornell and Larcker [22] stated that AVE should be higher than 0.5, meaning that at least 50 percent of measurement variance is captured by the construct. The AVE of all the constructs in the measurement model was above 0.5 (see again Table 3 ).

Finally, to complete the psychometric assessment of our measurement model discriminant validity was examined. Discriminant validity refers to the extent to which the items proposed to measure a given construct differ from the items intended to measure other constructs in the same model. First, a cross-loading check indi-cated that all items loaded higher on the construct they were supposed to measure than on any other construct.
Second, besides a simple cross-loading check, a more formal AVE test was used for discriminant validity assessment as suggested by Fornell and Larcker [22] . This test requires that the correlation between any two constructs is smaller than the square root AVE of each construct, meaning that the variance shared between any two constructs is less than the measurement variance captured by each construct separately.
The results of this discriminant validity analysis revealed that there was no correlation between any two latent constructs larger than or even equal to the square root AVE of these two constructs. Consequently, discrim-inant validity was supported and confidence was gained that all reflective constructs in the model were indeed measuring different concepts. 6.1.2. Formative indicators
Because of the formative structure of the PSQ construct, traditional validity assessments can not be used [16] . Observed correlations among the items may not be meaningful [16] and as a consequence, assessment of internal consistency and convergent validity become irrelevant [10,29] .

The PSQ measure can be considered as valid if the PSQ indicator coefficients are significantly different from zero [16] . Formative indicator coefficients are comparable to regression coefficients when the construct is seen as the dependent variable and the items as the independent variables. Indicator coefficients which are not sig-nificantly different from zero contain information perceived redundant by the respondents, or have high cor-relations with other indicators or are perceived irrelevant for the particular construct and should therefore be excluded from the model [16] .

PLS analysis indicates that not all PSQ indicators have a coefficient significantly different from zero ( p &lt; 0.05). These indicators should be deleted from the model before the structural model can be tested. The weights and significance of the formative items are presented in Table 4 .

In the analysis of the first experiment, the PSQ 4 ( t = 0.44) and PSQ cantly related to the perceived semantic quality construct. On the other hand, the PLS analysis indicates that
PSQ judgement is caused by PSQ 2 ( t = 3.51), PSQ 3 ( t = 4.24) and PSQ are retained to test the hypotheses.

The construct validity assessment of the PSQ construct in the second experiment largely supports the above findings. Here, again some PSQ indicators must be removed before the testing of the structural model can start. As in the previous assessment, not all indicator coefficients are significantly different from zero ity. Furthermore, also PSQ 1 ( t = 0.49) will be removed when evaluating the structural model. The other two
PSQ elements, PSQ 2 ( t = 5.04) and PSQ 3 ( t = 1.97), are relevant formative indicators of perceived semantic quality. 6.2. Hypothesis testing
After the validity assessment of the four constructs, the structural model was tested to assess the hypoth-esized relationships in the proposed user evaluations based quality model (see Fig. 5 ). The model is assessed in
PLS using bootstrapping to test the statistical significance of the path coefficients between the different con-structs [10] . Variance explained ( R 2 ) and the sign and significance of path coefficients are used to evaluate the structural model. Figs. 6 and 7 presents the structural path diagrams for the two conducted experiments with the statistically significant path coefficients and the total variance explained in the constructs ( R The meaning of the path coefficients is analogous to regression coefficients. For example, a path coefficient of
B means that if the value of the independent variable increases with 1, then this increase causes the value of the dependent variable to increase with B .
 An examination of the results reveals that the variance explained in the endogenous constructs (PU and US) ranges from 0.42 to 0.69. Further, all the paths were statistically significant at the 0.05 level or below and as such confirm the hypothesized relationships between the four user evaluation dimensions for concep-tual modelling scripts. The results indicate that PSQ, as was hypothesized, had a significant direct effect on US. We also found support for the relationship between PSQ and PU, though this effect was not as strong as with US. PEOU seems more important in explaining PU since there was a highly significant and strong effect of
PEOU on PU in both experiments. Together PSQ and PEOU explained 42% (E1) and 47% (E2) of the var-iance of the PU variable. Finally, we could also confirm the direct effects of PEOU on US and of PU on US which were more or less of equal importance. Together with PSQ, these variables were able to explain 45% (E1) and 69% (E2) of the variance of the US variable. 7. Conclusions
By adapting Seddon X  X  information system success model to a conceptual modelling context, we developed a user evaluation model for conceptual modelling scripts that combines four quality variables: Perceived Seman-tic Quality (PSQ), Perceived Ease Of Understanding (PEOU), Perceived Usefulness (PU), and User Satisfac-tion (US). The resulting model has a theoretical basis (the TRA/TPB theories for explaining human behaviour) and is strongly influenced by previous conceptual and empirical research on evaluating informa-tion technology (TAM), information systems (the DeLone and McLean Model) and conceptual modelling scripts (semiotics-based and BWW ontology-based quality frameworks). Together with a proposed PEOU X 
PSQ X  X U X  X S measurement instrument, it provides a practical evaluation framework that combines concep-tual modelling script quality variables related to perceptions of pragmatic quality, semantic quality and usabil-ity, as well as satisfaction outcomes. In two experiments we tested the measurement instrument and empirically corroborated the theorized and hypothesized relationships between these quality dimensions.
We gained evidence that the perceived semantic quality (PSQ) of a script impacts the user X  X  perception of use-fulness (PU) and his satisfaction with the script (US). The user X  X  overall satisfaction with a conceptual mod-elling script (US) is further proven to be influenced by perceived usefulness (PU) and perceptions related to the script X  X  pragmatic quality (PEOU). Finally, consistent with TAM experiences, perceived usefulness (PU) is also determined by the perceived ease of understanding the script (PEOU).

The research contributions of this paper are twofold: first, a new quality model and measurement instru-ment for conceptual modelling scripts has been developed that is solely based on user evaluations. Second, we demonstrated relationships between different quality perceptions, in particular between semantic and prag-matic quality perceptions, the overall user evaluation of usability, and satisfaction. These contributions have both research and practical significance. From a research point of view, the evaluation model and instrument can serve in studies investigating the mechanisms that lead to successful conceptual modelling applications.
Such studies may provide insight in how to develop better conceptual modelling scripts, as well as how to make sure that script users perceive quality improvements. Our evaluation framework may therefore supple-ment other (and maybe more objective) research instruments, that are not directly aimed at evaluating user perceptions of conceptual modelling script quality.

When applied in conceptual modelling practice, the proposed evaluation framework can be used to evaluate and compare the quality of alternative conceptual modelling scripts or successive versions of the same script (or parts/views of the script). As our research shows, user perceptions of conceptual modelling script quality are an important determinant of the users X  satisfaction with the script. Obtaining user satisfaction is impor-tant, given the conceptual modelling script X  X  role as a communication vehicle for information system require-ments between analysts and future system end-users or domain experts. Users are unlikely to sign-off a script for further use in system development if they are not satisfied with how the script represents their view of the domain and system requirements. As our research shows, user perceptions of the faithfulness, ease of under-standing and usefulness of the conceptual modelling script determine large part of user satisfaction (up to 70% in the second experiment). For a given conceptual modelling script, these perceptions and the degree of sat-isfaction can be measured using our questionnaire, and a decision to further improve the script can be made based on the measurements obtained. After the improvement actions, our instrument can be applied again to verifying whether script users perceive the intended effects of quality improvements actions.

Our research can be extended and improved in a number of ways. First, we demonstrated direct links between PSQ on the one hand and PU and US on the other hand. In our model, and based on the IS success literature, we did not hypothesize a link between PSQ and PEOU. Alternative models, which include such a relationship, can be developed and tested. A post-hoc analysis of our data set seems to indicate a significant relation between PSQ and PEOU (i.e. E1: structural path coefficient = 0.43, p &lt; 0.001; E2: structural path coefficient = 0.47, p &lt; 0.01). It is thus plausible that the perception of semantic quality also affects the user perception of ease of understanding. If users believe that the model is invalid and/or incorrect with respect to the problem domain, they are likely to develop a less favourable perception of the model X  X  ease of under-standing. But also, according to Lindland et al. [36] , improvements in pragmatic quality can improve semantic quality by making it easier to detect (and subsequently correct) invalidity and incompleteness. Further theo-retical and empirical research is therefore required to explore the direction and significance of the relationship between these two types of quality.

Second, we would welcome research that applies, further validates, and refines our measurement instru-ment. Apart from a minor problem with one of the PEOU items (with limited effect on the convergent and discriminant validity of the PEOU measure), the proposed PEOU, PU and US items seem to be reliable indi-cators of the corresponding perceptual and satisfaction constructs. Some of the formative indicators of PSQ (e.g. perceptions of relevancy and completeness) were however not significant in our experiments. Future research should clarify whether this lack of statistical significance was due to problems with the experimental materials (e.g. some deliberately injected script deficiencies went unnoticed, causing little variance in the response data), ambiguities in the formulation of the items statements, or the underlying semantic quality dimensions.

Third, although the conducted experiments support the hypothesized relationships in the user evaluations based quality model for conceptual modelling scripts, it is clear that the proposed model is only preliminary and needs further testing, validation and possibly reformulation to draw final conclusions and enhance its external validity. Both experiments were classroom experiments employing students as participants, small-scale business domain models (in Entity-Relationship diagram format) as study objects, and relatively simple tasks to be performed with the diagrams as stimuli to the formation of user beliefs and attitudes. Future research might employ other tasks and scripts and take the form of field studies (with practitioners instead of students) so the model is tested in different settings. Maybe practitioners can better judge perceived seman-tic quality aspects such as a script X  X  relevance and completeness, which would avoid the problems with some of the PSQ formative indicators observed in the current study. As controlled experimentation would be hard to achieve in practical scenarios employing big, realistic scripts, the design of such studies is challenging and would require careful evaluation of validity threats.

A recent practitioner survey in Australia has shown that the degree to which conceptual modelling scripts facilitate effective communication between analysts and key stakeholders is the most important factor in influ-encing the continued use of conceptual modelling in organizations [11] . Of special interest would therefore be a study that compares the conceptual modelling script quality perception, in particular the perception of the script X  X  semantic quality, of the analyst (who developed the script) and user (who provided the input to develop the script). Such studies can inform us on the effectiveness of conceptual modelling scripts in user-ana-lyst communication, i.e. they would help answering the question whether conceptual modelling scripts contrib-ute to a shared understanding of the domain and system requirements. Our framework could be used as a research tool in such studies, but as the measurement instrument was tailored to script users (i.e. the items assume a user perspective) further research is necessary to adapt it to script developers.

Finally, and in combination with the previous suggestion, the current research model, including only per-ceptual and attitudinal measures, could be extended with other variables of interest requiring more objective conceptual or performance-based measures of pragmatic and semantic quality. We can think about semantic expressiveness measures such as proposed in Si-Sa X   X  d et al. [61] and Patig [47] and the comprehension measures that have been previously employed in experimental research on conceptual modelling. Including these vari-ables would allow investigating the relationships between objective and subjective script quality. Furthermore, including variables related to the  X  X et benefits X  (as in Seddon X  X  model) of using a good conceptual modelling script in systems development, such as systems meeting better the user requirements, would make it possible to corroborate or falsify the universally assumed relationship between conceptual modelling script quality and information system quality, and would provide the basis for a comprehensive model of conceptual modelling success.
 Appendix A. Experimental objects in first experiment (E1) Appendix B. Experimental objects in second experiment (E2) See Figs. B.1 and B.2 .

References
