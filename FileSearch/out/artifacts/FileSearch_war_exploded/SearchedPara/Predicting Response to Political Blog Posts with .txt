 Web logging (blogging) and its social impact have recently attracted considerable public and scientific interest. One use of blogs is as a community dis-cussion forum, especially for political discussion and debate. Blogging has arguably opened a new channel for huge numbers of people to express their views with unprecedented speed and to unprece-dented audiences. Their collective behavior in the blogosphere has already been noted in the Ameri-can political arena (Adamic and Glance, 2005). In this paper we attempt to deliver a framework useful for analyzing text in blogs quantitatively as well as qualitatively. Better blog text analysis could lead to better automated recommendation, organization, ex-traction, and retrieval systems, and might facilitate data-driven research in the social sciences.
Apart from the potential social utility of text pro-cessing for this domain, we believe blog data is wor-thy of scientific study in its own right. The sponta-neous, reactive, and informal nature of the language in this domain seems to defy conventional analytical approaches in NLP such as supervised text classifi-cation (Mullen and Malouf, 2006), yet the data are rich in argumentative, topical, and temporal struc-ture that can perhaps be modeled computationally. We are especially interested in the semi-causal struc-ture of blog discussions, in which a post  X  X pawns X  comments (or fails to do so), which meander among topics and asides and show the personality of the participants and the community.

Our approach is to develop probabilistic mod-els for the generation of blog posts and comments jointly within a blog site. The model is an extension of Latent Dirichlet Allocation (Blei et al., 2003). Unsupervised topic models can be applied to collec-tions of unannotated documents, requiring very lit-tle corpus engineering. They can be easily adapted to new problems by altering the graphical model, then applying standard probabilistic inference algo-rithms. Different models can be compared to ex-plore the ramifications of different hypotheses about the data. For example, we will explore whether the contents of posts a user has commented on in the past and the words she has used can help predict which posts she will respond to in the future.
The paper is organized as follows. In  X  2 we re-view prior work on topic modeling for document collections and studies of social media like political blogs. We then provide a qualitative characterization of political blogs, highlighting some of the features we believe a computational model should capture and discuss our new corpus of political blogs (  X  3). We present several different candidate topic models that aim to capture these ideas in  X  4.  X  5 shows our empirical evaluation on a new comment prediction task and a qualitative analysis of the models learned. Network analysis, including citation analysis, has been applied to document collections on the Web (Cohn and Hofmann, 2001). Adamic and Glance (2005) applied network analysis to the political bl-ogosphere. The study modeled the large, complex structure of the political blogosphere as a network of hyperlinks among the blog sites, demonstrated the viability of link structure for information discovery, though their analysis of text content was less exten-sive. In contrast, the text seems to be of interest to social scientists studying blogs as an artifact of the political process. Although attempts to quanti-tatively analyze the contents of political texts have been made, results from classical, supervised text classification experiments are mixed (Mullen and Malouf, 2006; Malouf and Mullen, 2007). Also, a consensus on useful, reliable annotation or catego-rization schemes for political texts, at any level of granularity, has yet to emerge.

Meanwhile, latent topic modeling has become a widely used unsupervised text analysis tool. The ba-sic aim of those models is to discover recurring pat-terns of  X  X opics X  within a text collection. LDA was introduced by Blei et al. (2003) and has been espe-cially popular because it can be understood as a gen-erative model and because it discovers understand-able topics in many scenarios (Steyvers and Grif-fiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collec-tions, notably for community discovery in social net-works (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collec-tion of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse.

A notable trend in the recent research is to aug-ment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more general framework by Blei and McAuliffe (2008). Steyvers et al. (2004) and Rosen-Zvi et al. (2004) first ex-tended LDA to explicitly model the influence of au-thorship , applying the model to a collection of aca-demic papers from CiteSeer. The model combined the ideas from the mixture model proposed by Mc-Callum (1999) and LDA. In this model, an abstract notion  X  X uthor X  is associated with a distribution over topics. Another approach to the same document col-lection based on LDA was used for citation network analysis. Erosheva et al. (2004), following Cohn and Hofmann (2001), defined a generative process not only for each word in the text, but also its citation to other documents in the collection, thereby cap-turing the notion of relations between the document into one generative process. Nallapati and Cohen (2008) introduced the Link-PLSA-LDA model, in which the contents of the citing document and the  X  X nfluences X  on the document (its citations to exist-ing literature), as well as the contents of the cited documents, are modeled together. They further ap-plied the Link-PLSA-LDA model to a blog corpus to analyze its cross citation structure via hyperlinks.
In this work, we aim to model the data within blog conversations, focusing on comments left by a blog community in response to a blogger X  X  post. We discuss next the dataset used in our experiments. 3.1 Corpus We have collected blog posts and comments from 40 blog sites focusing on American politics during the period November 2007 to October 2008, con-temporaneous with the presidential elections. The discussions on these blogs focus on American poli-tics, and many themes appear: the Democratic and Republican candidates, speculation about the results of various state contests, and various aspects of international and (more commonly) domestic poli-tics. The sites were selected to have a variety of political leanings. From this pool we chose five blogs which accumulated a large number of posts during this period: Carpetbagger (CB), 1 Daily Kos (DK), 2 Matthew Yglesias (MY), 3 Red State (RS), 4 and Right Wing News (RWN). 5 CB and MY ceased as independent bloggers in August 2008. 6 Because our focus in this paper is on blog posts and their comments, we discard posts on which no one com-mented within six days. We also remove posts with too few words: specifically, we retain a post only if it has at least five words in the main entry, and at least five words in the comment section. All posts are represented as text only (images, hyper-links, and other non-text contents are ignored). To standardize the texts, we remove from the text 670 commonly used stop words, non-alphabet symbols including punctuation marks, and strings consisting of only symbols and digits. We also discard infre-quent words from our dataset: for each word in a post X  X  main entry, we kept it only if it appears at least one more time in some main entry. We ap-ply the same word pruning to the comment section as well. The corpus size and the vocabulary size of the five datasets are listed in Table 1. In addition, each user X  X  handle is replaced with a unique inte-ger. The dataset is available for download at http: //www.ark.cs.cmu.edu/blog-data . 3.2 Qualitative Properties of Blogs We believe that readers X  reactions to blog posts are an integral part of blogging activity. Often com-ments are much more substantial and informative than the post. While circumspective articles limit themselves to allusions or oblique references, read-ers X  comments may point to heart of the matter more boldly. Opinions are expressed more blatantly in comments. Comments may help a human (or au-tomated) reader to understand the post more clearly when the main text is too terse, stylized, or technical.
Although the main entry and its comments are certainly related and at least partially address similar topics, they are markedly different in several ways. First of all, their vocabulary is noticeably different. Comments are more casual, conversational, and full of jargon. They are less carefully edited and there-fore contain more misspellings and typographical er-rors. There is more diversity among comments than within the single-author post, both in style of writing and in what commenters like to talk about. Depend-ing on the subjects covered in a blog post, different types of people are inspired to respond. We believe that analyzing a piece of text based on the reaction it causes among those who read it is a fascinating problem for NLP.

Blog sites are also quite distinctive from each other. Their language, discussion topics, and col-lective political orientations vary greatly. Their vol-umes also vary; multi-author sites (such as DK, RS) may consistently produce over twenty posts per day, while single-author sites (such as MY, CB) may have a day with only one post. Single author sites also tend to have a much smaller vocabulary and range of interests. The sites are also culturally different in commenting styles; some sites are full of short interjections, while others have longer, more analyt-ical comments. On some sites, users appear to be close-knit, while others have high turnover.
In the next section, we describe how we apply topic models to political blogs, and how these prob-abilistic models can put to use to make predictions. The first model we consider is LinkLDA , which is analogous to the model of Erosheva et al. (2004), though the variables are given different meanings here. 7 The graphical model is depicted in Fig. 1 (left). As in LDA and its many variants, this model postulates a set of latent  X  X opic X  variables, where each topic k corresponds to a multinomial distribu-tion  X  k over the vocabulary. In addition to gener-ating the words in the post from its topic mixture, this model also generates a bag of users who respond to the post, according to a distribution  X  over users given topics. In this model, the topic distribution  X  is all that determines the text content of the post and which users will respond to the post.

LinkLDA models which users are likely to re-spond to a post, but it does not model what they will write. Our new model, CommentLDA , gen-erates the contents of the comments (see Fig. 1, right). In order to capture the differences in lan-guage style between posts and comments, however, we use a different conditional distribution over com-ment words given topics,  X  0 . The post text, comment text, and commenter distributions are all interdepen-dent through the (latent) topic distribution  X  , and a topic k is defined by:  X 
A multinomial distribution  X  k over post words;  X 
A multinomial distribution  X  0 words; and  X 
A multinomial distribution  X  k over blog com-menters who might react to posts on the topic.
Formally, LinkLDA and CommentLDA generate blog data as follows: For each blog post (1 to D ): 1. Choose a distribution  X  over topics according 2. For i from 1 to N i (the length of the post): 3. For j from 1 to M i (the length of the comments 4.1 Variations on Counting Users As described, CommentLDA associates each com-ment word token with an independent author. In both LinkLDA and CommentLDA, this  X  counting by verbosity  X  will force  X  to give higher probabil-ity to users who write longer comments with more words. We consider two alternative ways to count comments, applicable to both LinkLDA and Com-mentLDA. These both involve a change to step 3 in the generative process.
 Counting by response (replaces step 3): For j from 1 to U i (the number of users who respond to the post): (a) and (b) as before. (c) (CommentLDA only) For ` from 1 to ` i,j (the number of words in u j  X  X  comments), choose w 0 ment word distribution  X  0 comments by a user into a single bag of words on a single topic. 8 Counting by comments (replaces step 3): For j from 1 to C i (the number of comments on the post): (a) and (b) as before. (c) (CommentLDA only) For ` from 1 to ` i,j (the number of words in comment j ), choose w 0 distribution  X  0 topic, a user, and a bag of words.

The three variations X  X ounting users by ver-bosity, response, or comments X  X orrespond to dif-ferent ways of thinking about topics in political blog discourse. Counting by verbosity will let garrulous users define the topics. Counting by response is more democratic, letting every user who responds to a blog post get an equal vote in determining what the post is about, no matter how much that user says. Counting by comments gives more say to users who engage in the conversation repeatedly . 4.2 Implementation We train our model using empirical Bayesian esti-mation. Specifically, we fix  X  = 0 . 1 , and we learn the values of word distributions  X  and  X  0 and user distribution  X  by maximizing the likelihood of the training data: els.) This requires an inference step that marginal-izes out the latent variables,  X  , z , and z 0 , for which we use Gibbs sampling as implemented by the Hier-archical Bayes Compiler (Daum  X  e, 2007). The Gibbs sampling inference algorithm for LDA was first in-troduced by Griffiths and Steyvers (2004) and has since been used widely. We adopt a typical NLP  X  X rain-and-test X  strategy that learns the model parameters on a training dataset consisting of a collection of blog posts and their commenters and comments, then considers an un-seen test dataset from a later time period. Many kinds of predictions might be made about the test set and then evaluated against the true comment re-sponse. For example, the likelihood of a user to comment on the post, given knowledge of  X  can be estimated as: 9 p ( u | w N
The latter is in a sense a  X  X uessing game, X  a pre-diction on who is going to comment on a new blog post. A similar task was used by Nallapati and Co-hen (2008) for assessing the performance of Link-PLSA-LDA: they predicted the presence or absence of citation links between documents. We report the performance on this prediction task using our six blog topic models (LinkLDA and CommentLDA, with three counting variations each).

Our aim is to explore and compare the effective-ness of the different models in discovering topics that are useful for a practical task. We also give a qualitative analysis of topics learned. 5.1 Comment Prediction For each political blog, we trained the three varia-tions each of LinkLDA and CommentLDA. Model parameters  X  ,  X  , and (in CommentLDA)  X  0 were learned by maximizing likelihood, with Gibbs sam-pling for inference, as described in  X  4.2. The num-ber of topics K was fixed at 15.

A simple baseline method makes a post-independent prediction that ranks users by their comment frequency. Since blogs often have a  X  X ore constituency X  of users who post frequently, this is a strong baseline. We also compared to a Na  X   X ve Bayes classifier (with word counts in the post X  X  main en-try as features). To perform the prediction task with our models, we took the following steps. First, we removed the comment section (both the words and the authorship information) from the test data set. Then, we ran a Gibbs sampler with the partial data, fixing the model parameters to their learned values and the blog post words to their observed values. This gives a posterior topic mixture for each post (  X  in the above equations). 10 We then computed each user X  X  comment prediction score for each post as in Eq. 2. Users are ordered by their posterior probabil-ities. Note that these posteriors have different mean-ings for different variations:  X 
When counting by verbosity, the value is the prob-ability that the next (or any) comment word will be generated by the user, given the blog post.  X 
When counting by response, the value is the prob-ability that the user will respond at all , given the blog post. (Intuitively, this approach best matches the task at hand.)  X 
When counting by comments, the value is the probability that the next (or any) comment will be generated by the user, given the blog post.
We compare our commenter ranking-by-likelihood with the actual commenters in the test set. We report in Tab. 2 the precision (macro-averaged across posts) of our predictions at various cut-offs ( n ). The oracle column is the precision where it is equal to the recall, equivalent to the situation when the true number of commenters is known. (The performance of random guessing is well below 1% for all sites at cut-off points shown.)  X  X req. X  and  X  X B X  refer to our baseline methods.  X  X ink X  refers to LinkLDA and  X  X om X  to CommentLDA. The suffixes denote the counting methods: verbosity ( X -v X ), response ( X -r X ), and comments ( X -c X ). Recall that we considered only the comments by the users seen at least once in the training set, so perfect precision, as well as recall, is impossible when new users comment on a post; the Max row shows the maximum performance possible given the set of commenters recognizable from the training data.
Our results suggest that, if asked to guess 5 peo-ple who would comment on a new post given some site history, we will get 25 X 37% of them right, de-pending on the site, given the content of a new post.
We achieved some improvement over both the baseline and Na  X   X ve Bayes for some cut-offs on three of the five sites, though the gains were very small for and RS and CB. LinkLDA usually works slightly better than CommentLDA, except for MY, where CommentLDA is stronger, and RS, where Com-mentLDA is extremely poor. Differences in com-menting style are likely to blame: MY has relatively long comments in comparison to RS, as well as DK. MY is the only site where CommentLDA variations consistently outperformed LinkLDA variations, as well as Na  X   X ve Bayes classifiers. This suggests that sites with more terse comments may be too sparse to support a rich model like CommentLDA.

In general, counting by response works best, though counting by comments is a close rival in some cases. We observe that counting by response tends to help LinkLDA, which is ignorant of the word contents of the comment, more than it helps CommentLDA. Varying the counting method can bring as much as 10% performance gain.

Each of the models we have tested makes differ-ent assumptions about the behavior of commenters. Our results suggest that commenters on different sites behave differently, so that the same modeling assumptions cannot be made universally. In future work, we hope to permit blog-specific properties to be automatically discovered during learning, so that, for example, the comment words can be ex-ploited when they are helpful but assumed indepen-dent when they are not. Of course, improved per-formance might also be obtained with more topics, richer priors over topic distributions, or models that take into account other cues, such as the time of the post, pages it links to, etc. It is also possible that bet-ter performance will come from more sophisticated supervised models that do not use topics. 5.2 Qualitative Evaluation Aside from prediction tasks such as above, the model parameters by themselves can be informative.  X  defines which words are likely to occur in the post body for a given topic.  X  0 tells which words are likely to appear in the collective response to a partic-ular topic. Similarity or divergence of the two dis-tributions can tell us about differences in language used by bloggers and their readers.  X  expresses users X  topic preferences. A pair or group of par-ticipants may be seen as  X  X ike-minded X  if they have similar topic preferences (perhaps useful in collabo-rative filtering).

Following previous work on LDA and its exten-sions, we show words most strongly associated with a few topics, arguing that some coherent clusters have been discovered. Table 3 shows topics discov-ered in MY using CommentLDA (counting by com-ments). This is the blog site where our models most consistently outperformed the Na  X   X ve Bayes classi-fiers and LinkLDA, therefore we believe the model was a good fit for this dataset.

Since the site is concentrated on American pol-itics, many of the topics look alike. Table 3 shows the most probable words in the posts, comments, and both together for five hand-picked topics that were relatively transparent. The probabilistic scores of those words are computed with the scoring method suggested by Blei and Lafferty (in press).

The model clustered words into topics pertain-ing to religion and domestic policy (first and last topics in Table 3) quite reasonably. Some of the religion-related words make sense in light of cur-rent affairs. 11 Some words in the comment sec-tion are slightly off-topic from the issue of religion, such as dawkins 12 or wright , 13 but are relevant in the context of real-world events. Notice those words rank highly only in the comment section, showing differences between discussion in the post and the comments. This is also noticeable, for example, in the  X  X rimary X  topic (second in Table 3), where the Republican primary receives more discussion in the main post, and in the  X  X raq war X  and  X  X nergy X  top-ics, where bloggers discuss strategy and commenters focus on the tangible ( oil , taxes , prices , weapons ).
While our topic-modeling approach achieves mixed results on the prediction task, we believe it holds promise as a way to understand and summa-rize the data. Without CommentLDA, we would not be able to easily see the differences noted above in blogger and commenter language. In future work, we plan to explore models with weaker indepen-dence assumptions among users, among blog posts over time, and even across blogs. This line of re-search will permit a more nuanced understanding of language in the blogosphere and in political dis-course more generally. In this paper we applied several probabilistic topic models to discourse within political blogs. We in-troduced a novel comment prediction task to assess these models in an objective evaluation with possi-ble practical applications. The results show that pre-dicting political discourse behavior is challenging, in part because of considerable variation in user be-havior across different blog sites. Our results show that using topic modeling, we can begin to make rea-sonable predictions as well as qualitative discoveries about language in blogs.

