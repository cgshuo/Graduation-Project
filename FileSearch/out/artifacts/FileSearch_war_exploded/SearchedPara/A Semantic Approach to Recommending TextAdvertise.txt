 In recent years, more and more images have been upload-ed and published on the Web. Along with text Web pages, images have been becoming important media to place rele-vant advertisements. Visual contextual advertising, a young research area, refers to finding relevant text advertisements for a target image without any textual information (e.g., tags). There are two existing approaches, advertisement search based on image annotation, and more recently, adver-tisement matching based on feature translation between im-ages and texts. However, the state of the art fails to achieve satisfactory results due to the fact that recommended adver-tisements are syntactically matched but semantically mis-matched. In this paper, we propose a semantic approach to improving the performance of visual contextual advertis-ing. More specifically, we exploit a large high-quality image knowledge base (ImageNet) and a widely-used text knowl-edge base (Wikipedia) to build a bridge between target im-ages and advertisements. The image-advertisement match is built by mapping images and advertisements into the re-spective knowledge bases and then finding semantic matches between the two knowledge bases. The experimental results show that semantic match outperforms syntactic match sig-nificantly using test images from Flickr. We also show that our approach gives a large improvement of 16.4% on the pre-cision of the top 10 matches over previous work, with more semantically relevant advertisements recommended. H.3.3 [ Information Search and Retrieval ]: Search pro-cess Algorithms, Experimentation Visual Contextual Advertising, Semantic Matching, Cross-media Mining
Nowadays, Web pages no longer contain just textual in-formation. Instead, more and more images have been up-loaded and published on the web. For instances, social Web sites like Facebook 1 and Flickr 2 have billions of photo album pages with little text. Compared with the traditional textual Web pages, images become the main contents of these Web pages. Thus, traditional contextual advertising approaches cannot be directly applied to Web pages dominated by im-ages because of the lack of textual information. Therefore, understanding the contents or topics of images and then rec-ommending relevant advertisements based on these images becomes a challenging problem interesting to both academia and industry.

Visual contextual advertising (see Figure 1) refers to find-ing the most relevant advertisements for a target image with-out textual information such as tags. It can be regarded as a special case of contextual advertising where images be-come the context for recommending advertisements. While it is a young branch of contextual advertising, it is more challenging than advertising on textual Web pages because it requires techniques such as computer vision and cross-media transfer learning . In other words, visual contextual advertising aims at semantic m atching between two hetero-geneous features spaces (i.e., image feature space and text feature space).
 Figure 1: An example of visual contextual advertis-ing.

Image annotation [4, 11] is one approach to visual con-textual advertising. Intuitively, given a target image, text annotations are extracted based on a model trained by la-beled images. Then these annotations are used to search for relevant advertisements, similar to keyword search in tra-ditional contextual advertising. However, since it is time consuming and error prone to obtain high-quality labeled images, the quality of annotations cannot be guaranteed, which leads to poor recommendation performance. On the other hand, since the match process is performed between two heterogeneous feature spaces (i.e., images and text), het-http://www.facebook.com http://www.flickr.com erogeneous transfer learning [30, 9] can be adapted to the image-advertisement match. The state-of-the-art algorithm for visual contextual advertising is ViCAD [8]. It first builds a bridge between the image feature space and text feature space through a feature translation model. Then it uses a method based on a language model to estimate the rele-vance of each candidate advertisement to the target image. While ViCAD is reported to outperform annotation-based approaches, the advertising precision is still not satisfactory as to be used in real world applications.

With a careful investigation of the performance of previ-ous work, we find that the major weakness of ViCAD as well as the annotation-based approaches comes from mismatch-es between image tags and text advertisements due to their shortness, ambiguity, and variety. Figure 2 presents some examples which indicate the syntactic mismatch in these approaches. Detailed explanations are as follows. Problem 1 Problem 2 Figure 2: Two main problems in current approaches. These two problems also occur frequently in traditional Web page contextual advertising. To overcome the syntactic mismatch problem, people use a broad match [12, 15] which finds the semantic relation between different keywords or phrases. In order to optimize the semantic relevance, sever-al semantic approaches to contextual advertising have been proposed [5, 23]. In this paper, we follow this promising di-rection and propose a semantic approach to tackle the prob-lems of visual contextual advertising. More specifically, we map the target image to some nodes of interlinked knowl-edge bases instead of to pure text features. Compared with pure text features, knowledge base nodes have their cont-ext and relationships with other relevant nodes, which helps solve the two problems. Using the precision@10 measure on the Flickr test dataset with 230 images, our approach out-performs the syntactic matching approaches by up to 16.4 percent.

To sum up, the contributions of this paper are threefold.
The rest of this paper is organized as follows. In section 2, we discuss related work. In section 3, we present our seman-tic approach to visual contextual advertising. In section 4, we describe the experiments and analyze the results. Final-ly, in section 5, we conclude this paper and discuss future work.
Contextual advertising refers to placing relevant advertise-ments on third-party Web pages. The publisher and search engine will share the revenue once any advertisement on their pages is clicked. Studies [28] have shown that the rel-evance of the advertisements to the content of target pages makes a large difference at the click-through rate. Therefore, the work of matching target Web pages and advertisements is the key point of contextual advertising [23, 6].
Keyword-based approaches [29, 31] are widely used in con-textual advertising. They first extract the keywords from a target Web page and then use these keywords to retrieve relevant advertisements just like sponsored search (anoth-er kind of Web advertising). However, due to the vagaries of keyword extraction and the lack of content in advertise-ments, keyword-based approaches always lead to irrelevant advertisements. Besides keyword-based approaches, the au-thors in [5] imported semantic information to enhance the matching work. They classified both pages and advertise-ments into a common taxonomy and combined the keyword-based approach with taxonomy matching to rank the adver-tisements. Moreover, Pak et al. [23] proposed an ESA [14] based approach which makes use of Wikipedia as the knowl-edge base to improve the performance of contextual adver-tising. However, they only chose one thousand entities and no link information was used. This work has much room to improve. On the side of efficiency, since analyzing the entire page content is costly and new or dynamically created Web pages cannot be processed to match the advertisements ah-ead of time, the authors in [2] proposed a summary-based approach to enhance the efficiency of contextual advertising with an ignorable decrease on effectiveness.
Besides textual content, there are more and more multi-media elements such as images, audio, and video on Web pages. These elements, as pieces of information, are often important and illustrate the topic of a Web page. Mining on these multimedia elements has got considerable attention from both academia and industry. In particular, data min-ing across different media has become a promising research direction. IJCAI 2009 held a workshop focusing on cross-media information access and mining [1]. Recently, some applications using cross-media mining technologies were de-veloped. Chao et al. [7] proposed Tu n e S e n s or ,asemantic-driven service to recommend background music for Web pho-to albums. In contextual video advertising, the systems VideoSense [22] and vADeo [24] have been built based on video content analysis.

Regarding contextual advertising on Web images, image annotation approaches [4, 11] can be leveraged. However, image annotation is not specifically designed for recommend-ing advertisements. The authors of ImageSense [21] first proposed to match advertising with images. But ImageSense mainly used surrounding text for advertisement match while visual relevance acted as a complement to that information. To the best of our knowledge, ViCAD [8] is the only work trying to match advertisements for a target image without any textual information. In ViCAD , the authors built an image-text feature mapping using a graphical model and a language model. Then, the conditional probability of any advertisement for a target image was determined. ViCAD is a very relevant work and will be compared in our experi-ments.
In the field of contextual advertising, besides the direct syntactic page-ad matching, there are two major frame-works for matching the target Web page and advertisement. In the page-keyword-ad framework [31], advertising key-words are extracted from the target Web page and then ad-vertisements are matched with the keywords. In the page-taxonomy-ad framework [5], pages and advertisements are mapped to the same taxonomic structure and the semantic similarities are calculated using the mapping on the taxo-nomic hierarchy of pages and advertisements. Besides, the traditional syntactic matching is also combined into this framework.

For visual contextual advertising, the traditional image annotation approach is just like the page-keyword-ad ap-proach and ViCAD corresponds to the syntactic matching. To the best of our knowledge, there is no previous work in visual contextual advertising using any semantic approach. In this section, we propose a semantic approach to visual contextual advertising, with the goal of improving the per-formance of the advertisement precision.
First we formally define the problem of visual contextu-al advertising. Let T = { t 1 ,t 2 ,...,t m } be the text fea-ture space, where t i is a text feature and m is the size of the text feature space. Let A be the advertisement space and each advertisement a  X  X  is represented by a text fea-feature t k in a . Similarly, we denote image feature space V = { v 1 ,v 2 ,...,v n } ,where v i is an image feature and n is the size of image feature space. The image space is denoted as
I . And each image  X   X  X  is represented by an image fea-image feature v k in  X  . In addition, the text knowledge base is denoted as O t = { O t ,E t } ,where O t = { ot 1 ,ot 2 isthenodesetand E t = { ( ot i ,ot j ) } istheedgeset. Al-so the image knowledge base is defined as O v = { O v ,E v where O v = { ov 1 ,ov 2 ,...,ov  X  } and E v = { ( ov i ,ov a given image  X   X  X  , the objective is to find the function r (  X , a ): I X A  X  R that accurately estimates the relevance of any candidate advertisement a to  X  .
In this subsection, we discuss the framework of semantic visual contextual advertising. As mentioned in Section 1, we first map the image and advertisement onto some nod-es of interlinked knowledge bases. Since the feature spaces of image and text are heterogeneous, the image and text knowledge bases are always different. However, just like the image-text occurrence data, we can still find a way to match the nodes on the two knowledge bases 3 .Wepropose a framework for semantic matching of images and advertise-ments by building links between nodes of the image and text knowledge bases.
In this framework, first, images and advertisements are mapped to nodes in the image and text knowledge bases, respectively. Then the matching between the nodes of inter-linked image and text knowledge bases is processed. With the help of semantic link information in the knowledge bases, syntactic mismatches between the image features and text features can be reduced. Therefore, given a target image  X  , the task of finding the best match advertisement can be written as where with Here  X  and  X  are the functions mapping text instances to nodes in the text knowledge base and image instances to nodes in the image knowledge base, respectively. Each mapped node is assigned a weight to express its relevance to the image or advertisement. M is a cross-knowledge base matching function for the two sets of weighted nodes on the combined structure of image and text knowledge bases. To sum up, our framework can be depicted as Figure 3.
For ontology engineering, one of the most important pro-cesses is to find the match between two ontologies. Figure 3: Framework of semantic matching between the image and advertisement.

The specific implementation of the functions in Equation 1 depends on the knowledge used, which will be discussed in detail later.
In this section, we introduce the specific knowledge bases we use in this framework and the bridging knowledge data between the nodes of two knowledge bases.

Text Knowledge Base: Wikipedia. We use Wikipedia as the text knowledge base O t in our framework. Wikipedia is a user-contributed online encyclopedia. It contains numer-ous entities with a formatted article description and inter-links to other relevant entities. Each entity article is written or revised by Web users so as to lead a comprehensive des-cription of the entity. In addition, the interlinks between each two entities serve as auxiliary information and further explanation, which indicates their semantic relatedness. In sum, Wikipedia is a large-scale qualified knowledge base: so far in March 2012, it has more than 3.9 million articles written in English, with 19.67 edits for each article 4 .
Image Knowledge Base: ImageNet/WordNet. We choose ImageNet [10] as the image knowledge base O v in our framework. ImageNet is an image database organized according to WordNet [13]. WordNet is composed of synsets, each of which is described by several synonyms. The edges linking two synsets provides the semantic relation between them. The kinds of edges include: antonym , hypernym , in-stance hyponym , part meronym , derivationally related form , member of this domain , and so on. We regard syn-sets as concept nodes. The hypernym edges are used to con-struct a node hierarchy. Currently, there are 14.2 million im-ages and 21.8 thousand nodes indexed in ImageNet 5 .Each node is assigned 1000 images on average. Images of each con-cept are human-annotated and have high quality. Therefore, using ImageNet, each node ov i  X  X  v is represented as a set of images.

Bridging Knowledge: YAGO. We connect Wikipedia nodes and ImageNet/WordNet nodes using YAGO [27]. Each Wikipedia node is labeled with types in YAGO X  X  taxonomy, which is built on the topology of WordNet. Thus we can obtain a list of WordNet nodes for each Wikipedia node. For example, Wikipeida::Aristotle has the type of Word-Net::Person , WordNet::Scientist ,etc. http://en.wikipedia.org/wiki/Special:Statistics http://image-net.org/about-stats
In this section we introduce the definition of the functions in our framework, based on the image and text knowledge bases discussed in Section 3.3.
Given a candidate advertisement in bag-of-words form,  X  maps a to the relevant nodes on O t .Here O t represents the set of Wikipedia entities each with an article description. Because advertisement content is in short-text and diver-sely written, it is usually difficult to directly find Wikipedia entity names in advertisement content. For this reason, we make use of explicit semantic analysis (ESA) [14] to find the most relevant Wikipedia entities for each candidate adver-tisement.

Here each mapped node weight  X  ot in Equation 3 is de-fined by the ESA association strength. A widely used choice [26] is to select tfidf weighting where tfidf ot ( t ) is the product of the frequency and inverse document frequency of t in the article of ot , tf a ( t )isthe frequency of a word or phrase t in the advertisement dataset. Particularly, top-3 weighted nodes are selected.
Given a target image  X  = { v 1  X  ,v 2  X  ,...,v n  X  } ,  X  maps  X  to the relevant nodes on O v . Different from mapping advertise-ment content to nodes of Wikipedia as  X  , the image map-ping function  X  is closer to multi-label classification. Each node in ImageNet has about 1000 image instances; these are used as the training data and the target image is regarded as test data. Specifically, we use a node-level centroid based similarity function  X  (  X , ov ) to obtain the closest k node set C k to the target image  X  . Specifically,  X  can be implemented as cosine similarity after the process of principle compo-nent analysis (PCA) [18]. Moreover, since ImageNet has a hierarchical structure, we can implement a hierarchical cen-troid algorithm which leverages the ancestor information in the similarity calculation. Finally, the weight  X  ov for the mapped node ov is defined by the (normalized) similarity between  X  and ov ,calculatedas where A ( ov ) denotes the set of ancestors of ov and  X  is the weight assigned to each ancestor node; these are combined with a geometric mean. With leveraged ancestor informa-tion,  X  is less likely to map  X  to irrelevant nodes. Partic-ularly, we set k =7and  X  =0 . 6 in our experiment, after preliminary parameter tuning.
Above we have elaborated the text and image mapping functions which map the advertisements and images to Wiki-pedia and ImageNet/WordNet. In addition, these two knowl-edge bases could be bridged via YAGO (Section 3.3). Thus we can regard them as a combined knowledge base. Now we introduce the matching function M between the two disjoint sets of weighted nodes on the combined knowledge base. Here we uniformly use o i to represent ov i and ot i nce the two knowledge bases have been combined. We also define the mapped node sets of image and advertisement as O  X  and O a respectively. We discuss two implementations of the cross-knowledge base matching function M .

LOD Description Overlap (LODDO). This is an ap-proach proposed in [32] for evaluating named entity semantic relatedness on linked open data (LOD). The authors pro-pose to regard the neighborhood of an entity o in LOD as its description  X  ( o ), defined as the set of entities linked to o . And the similarity between entity o i and o j is defined as the description overlap between  X  ( o i )and  X  ( o j ).
Since Wikipedia and ImageNet/WordNet are also mem-bers of LOD, this approach can seamlessly be adapted to our matching function. The matching function between the target image  X  and a candidate advertisement a can be cal-culated as the weighted average of the similarity of each image-advertisement entity pair.

M (  X  (  X  ) , X  ( a )) =
Hierarchy-based Matching. Taxonomy-based seman-tic matching has been used in contextual advertising [5]. As has been mentioned in Section 3.3, YAGO does pro-vide a shared taxonomy between Wikipedia and WordNet. Thus we can map the nodes in both knowledge bases to a common taxonomy hierarchy, where we can implemen-t hierarchy-based matching. Matching function M can be written as
M (  X  (  X  ) , X  ( a )) = where LCA ( o i ,o j ) means the maximal path length from o and o j to their least common ancestor [5].

In the experiment, we will compare the above two cross-knowledge base matching functions to explore how to pro-vide cross-media semantic matching appropriately.
So far we have introduced our framework of semantic matching between an image  X  andanadvertisement a .Now the practical task is to retrieve and rank the relevant adver-tisements for a given image  X  ,. Since expansion and matching of graph structures are involved in our matching algorithm, it is very inefficient to traverse the advertisement dataset to perform a match between each advertisement and the tar-get image. Here we propose the algorithm flow to efficiently solve the problem (see Figure 4).
 In an offline process, we pre-calculate a set of relevant Wikipedia nodes for each advertisement a using ESA. Thus we can build an inverse advertisement index for each node, like the document index to each keyword in a search engine. For the online process, with a target image  X  as input, first we use image mapping function  X  to get k ImageNet nodes  X  (  X  ). Then we link mapped ImageNet nodes to Wikipedia nodes via YAGO. With the advertisement index above, we can retrieve the indexed advertisements for each linked Wiki-pedia node, which lead to the candidate advertisement list L . For each advertisement a in L , calculate the similari-ty with  X  via ontology matching function M . Finally, rank the candidate advertisement list in descending order by the similarity score, get the top N advertisements as the output.
For the image mapping process, each image-centroid sim-ilarity function  X  takes O ( |V| ). Thus the image mapping is as in Section 3.1. In practice, the image feature number |V| is much larger than log | O v | .Thusthecomplexityofthe image mapping function is O ( | O v | X |V| ).

For the matching process, let n a be the maximum number of advertisements one Wikipedia node could retrieve, D be the maximum out-degree of WordNet nodes in YAGO. Thus the maximum number of candidate advertisements is k  X   X  n a  X  D . For the LODDO matching function, the complexity is O ( k  X   X  k a  X  N  X  log N  X  ), where N  X  is the description size. For the hierarchy-based matching function, the complexity is O ( k  X   X  k a  X  d ), where d is the depth of the YAGO taxonomy hierarchy, k  X  and k a are the maximum number of mapped nodes for images and advertisements respectively. Thus the complexities of the matching processes are O ( n a  X  k 2  X  N  X  log N  X  )and O ( n a  X  k D  X 
N  X  log N  X  )and d are not large numbers ( d&lt; 15 and N 30). Uniformly, we use c to denote the upper bound of these two numbers and the matching complexity is O ( n a  X  k 2  X 
To sum up, the overall complexity of the online algorithm is O ( | O v | X |V| + n a  X  k 2  X   X  k a  X  c ).

In our experiment, the average real run time for each test case is 0.751 seconds on a machine with an Intel(R) Core-2(TM) Quad Q8400 CPU with 2 cores at 2.6GHz and 2GB memory. Furthermore, the efficiency can be further improved with the optimization such as parallelization in the image ontology match process and advertisement index pruning.
In this section, we introduce the datasets, compare algori-thms and evaluation measures, and finally report and discuss our experimental results.
The textual advertisements can be crawled from a main-stream commercial search engine. Specifically, we use AOL query log [16] as query set and then crawl the delivered advertisements on the search engine result page (SERP) for each query during March 2011. Specifically, there are 9,954,130 queries in the AOL dataset, where 1,118,729 queries attract at least one advertisement. As a result, we collect 1,607,688 unique advertisements.

For each advertisement, we crawl its title, creative, and display URL, as has been shown in Figure 1.
As has been discussed in Section 3.2, there are text and image knowledge bases (Wikipedia and ImageNet/WordNet) and bridging data (YAGO).
 Wikipedia -We obtained the Wikipedia dump of Jan.5, 2012. We selected the Wikipedia articles representing con-crete concepts using heuristics similar to [14], resulting in a collection of 1,521,080 concept nodes. We use Lucene 6 to build the ESA index from articles describing the concepts. ImageNet/WordNet -For WordNet structure, we down-load WordNet 3.0 7 and remove the edges with negative se-mantics ( Antonym ). The knowledge base contains 117,659 nodes and 377,592 edges, where 97,666 are Hypernym edges.
For the image data, we take the 1,000 ImageNet synsets released on April 30, 2010 which contain 2,522,812 images. Each image in this dataset has SIFT features extracted and 1000-clustered bag of words. To investigate whether the size of the image knowledge base is large enough to provide relevant advertisements, we will drive an experiment about the performance against the number of ImageNet nodes in Section 4.4.2.
 YAGO -To connect Wikipedia and ImageNet/WordNet, we take YAGO dataset of type_star in version yago2core-20120109 . On average, each Wikipedia concept is mapped to 25.2 WordNet concepts. In all, 4,564 WordNet concepts have at least one corresponding Wikipedia concept 8 .
In our experiment, we use a Flickr image set as our target dataset. This dataset contains 521 thousand images crawled from Flickr during 2010. Considering the large effort of hu-man judgement, we randomly selected 230 images as the target images for testing 9 .

The data preprocessing is the same as ImageNet. First we detect the interesting points for each image using SIFT descriptors [19]. Then we cluster 1,000 categories (same as [25]) for all interesting points to obtain a codebook, which turns out to be the image feature space and each image can be represented by image-bag-of-words. These image features are used in the similarity function  X  (  X , ov ).
Since there are few methods for visual contextual advertis-ing except ViCAD , we compared all the methods that work [8]. The algorithms are listed below.

Annotation + Search ( AS ). First, the target image is annotated [20]. Then advertisements are retrieved and ranked by a search process using the annotations as query. http://lucene.apache.org/ http://wordnet.princeton.edu/wordnet/download/
Although the ratio of involved WordNet concepts is not high, these concepts are usually the representative category labels, which have links to most of WordNet Concepts.
As a reference, 200 test images were selected in the exper-iment of previous work [8].
 The search engine is built based on Lucene. This work is just like the keyword-based methods used in traditional contex-tual advertising.

Annotation + Expansion + Search ( ASEx ). One in-tuitive approach to adding semantic matching into the tradi-tional AS approach is to expand the extracted annotations using a semantic knowledge base and then search the ad-vertisements with the expanded query set. Specifically, we implement ASEx similar to the work [17].

ViCAD . The heterogeneous transfer learning based ViCAD proposed in [8] has been discussed in Section 2.2.
ImageAdSense. This is our approach and the algori-thm has been discussed in Section 3. In order to compare different matching functions, we implement LODDO and the hierarchy-based matching function mentioned in Sec-tion 3.4.3, denoted as iAdSense-LODDO and iAdSense-Tree . In order to investigate the impact of a cross-knowledge-base matching function, here we add an algorithm iAdSense-OneLayer , which only has ImageNet/WordNet. The map-ping of advertisements to WordNet nodes is based on syn-tactic match.
The input of the experiment is a target image  X  and the output is k advertisements for  X  . As the basis of the eval-uation work, we invited six college students to judge the relevance of each image-advertisement pair as below. Each image-advertisement pair has at least two human judges. Then, we averaged the scores for each image-advertisement pair. Then we evaluated the performance of the algorithms using P @ n as the evaluation measure. Precision at position n ( P @ n ) is defined to be the fraction of the top-n retrieved advertisements that are relevant [3].
 In Equation 9,  X  i denotes the average rate score for the pair of the target Web page and the i th recommended advertise-ments. Since we cannot evaluate every image-advertisement pair, there is no good measure to evaluate the recall of each approach.
In the first part of the experiment, we judge the overall recommendation performance of the compared algorithms on test dataset. For the 230 test images, each algorithm recommends 10 top ranked advertisements. We use the eval-uation measure P @ n (see Section 4.3) for the recommenda-tion performance. The result for six algorithms is provided in Figure 5.

From Figure 5 we can have the following observation. (i) Three iAdSense -algorithms provide much better perfor-mance than AS , ASEx and ViCAD . The absolute improve-ment of P@10 of iAdSense-LODDO is 16.4% and 20.7%, com-pared with ASEx and ViCAD respectively, which verifies the Figure 5: Figure representation of P @ n results of all compared algorithms on test dataset. impact of semantic matching. (ii) In the comparison among these three iAdSense -algorithms, iAdSense-LODDO performs the best. This indicates that the semantic relatedness ap-proach LODDO is well adapted to our framework. iAdSense-Tr e e has a little lower precision. This is because only hierar-chy edges are used in iAdSense-Tree , while iAdSense-LODDO makes use of all edges of each node to provide a more com-prehensive semantic description. iAdSense-OneLayer is not as good as others with two layers. This indicates the neces-sity of semantic text mapping. Syntactically mapping adver-tisement content to its words in WordNet will import much ambiguity since each word always occurs in several Word-Net synsets. (iii) ViCAD outperforms AS but is not as good as iAdSense algorithms. The reason ViCAD is not as good as iAdSense -algorithms is the frequent noise in the tags of training images, which reduces the accuracy of cross-domain feature transferring. In addition, ViCAD is also a syntactic match approach and has the same problems as AS .
To sum up, the above comparison shows that iAdSense is more effective than previous approaches. Figure 6: P @10 of iAdSense against the number of randomly selected ImageNet nodes.
As mentioned in Section 4.1.2, we should investigate the recommendation performance of iAdSense against the num-ber of ImageNet nodes and check whether it is enough to take the 1,000 ImageNet synsets released with SIFT fea-tures. Specifically, we vary the number of randomly se-lected ImageNet nodes from 50 to 1,000 with a step of 50. Then we evaluate the performance of iAdSense-LODDO in the same way as above 10 . The result is shown in Figure 6.
Due to the huge human labeling effort, the test set here is a subset of the test dataset.
 Figure 7: Advertisements recommended by the com-pared algorithms on one case.

From Figure 6 we can see that (i) as the number of nodes increases, the P @10 performance of iAdSense improves and its real run-time increases. (ii) The precision curve has a sigmoid-shaped trend: P @10 fluctuates without an obvious increase when number of nodes varies from 50 to 350; in the range of [350, 700], P @10 increases rapidly; after 700, P @10 fluctuates around 0.475. (iii) The real run-time curve has a stable increase rate against the number of ImageNet nodes. This is because the image mapping process is im-plemented as a hierarchical centroid algorithm, a memory-based approach, so more ImageNet synsets will surely bring an efficiency decrease. To sum up, 1,000 ImageNet size is a suitable scale for iAdSense considering both effectiveness and efficiency.
Here we demonstrate a case that makes a difference among the compared algorithms. Figure 7 provides some adver-tisements recommended by the four algorithms for a target image about a gorilla. From the results we can find AS rec-ommends an irrelevant advertisement. For ASEx ,thereis a topic drift between the target image and advertisement, which is caused by annotation expansion. ViCAD recom-mends a syntactic match advertisement. However, squirrel here refers to a brand name instead of a kind of animal, which is a case of semantic mismatch. iAdSense-LODDO recommends a suitable advertisement, where Gorilla in the advertisement refers to the animal in the target image.
Finally, we provide more cases of the results of seman-tic visual contextual advertising with respect to the test dataset. In Figure 8, there are two advertisements listed on the right of each target image. These advertisements are recommended by algorithm iAdSense-LODDO . More demon-strations are presented on the Web site of ApexLab 11 .
We investigate the current work on visual contextual ad-vertising and point out the problems of semantic mismatch despite a syntactic match between image and advertisement content. In order to solve these problems, we proposed a semantic approach named iAdSense with the help of text and image knowledge bases. In the experiment, iAdSense provides an improvement of 16.4% over the previous ap-proaches, with more semantically relevant advertisements recommended.

Infuturework,wewillexploreotherknowledgebasesto help in this framework. For example, we can use a more com-
Online demo. http://iadsense.apexlab.org mercially relevant knowledge base to explore a better adver-tisement mapping. Moreover, we will work on the appli-cation of visual contextual advertising to E-commerce such as Taobao 12 . The input will be a product image and some relevant products will be recommended. In this topic, more specific image features will be selected and more information can be obtained from the product pages. http://www.taobao.com
