 In this paper, we address the problem of learning when some cases are fully labeled while other cases are only partially labeled, in the form of partial labels. Partial labels are re p-resented as a set of possible labels for each training exampl e, one of which is the correct label. We introduce a discrimina-tive learning approach that incorporates partial label inf or-mation into the conventional margin-based learning frame-work. The partial label learning problem is formulated as a convex quadratic optimization minimizing the L2-norm reg-ularized empirical risk using hinge loss. We also present an efficient algorithm for classification in the presence of par-tial labels. Experiments with different data sets show that partial label information improves the performance of clas si-fication when there is traditional fully-labeled data, and a lso yields reasonable performance in the absence of any fully la -beled data.
 I.5 [ Pattern Recognition ]: Design Methodology Algorithms, Design Partial Labels, Support Vectors
Partially labeled training data such as pairwise constrain ts have been shown to improve performance in both supervised [20, 21, 10, 11, 12, 14, 15, 18, 6] and semi-supervised [17, 3, 13, 19, 7, 2, 4] learning. While labeled data is usually ex-pensive, time consuming to collect, and sometimes requires human domain experts to annotate, partially labeled data is often relatively easier to obtain. Much attention in the machine learning community has been focused on integrat-ing partially labeled data that are complementary to the fully labeled training data into existing learning framewo rk. However, in previous research partially labeled informati on is usually presented in the form of pairwise constraints whi ch indicate whether a pair of examples belongs to the same class or not. In [20], the authors showed significant per-formance improvement in video object classification using a modified logistic regression algorithm which can learn the decision boundary with labeled data as well as additional pairwise constraints. Moreover, in [21] the authors pro-posed a discriminative method which can effectively utilize pairwise constraints to construct a sign-insensitive cons is-tent estimator with respect to the optimal linear boundary.
In this paper, we investigate the usefulness of a different partially labeled information, Partial Labels 1 . Partial labels are presented as a set of possible labels for each training ex -ample, one of which is the correct label. Unlike fully labele d data that would require users to have prior knowledge or experience with a data set, partial labels relatively requi re often less effort from users. For example, in the task of predicting nationality based on facial images, it is relati vely easier for users to determine if a face belongs to a group of countries such as Asian countries, African countries or Western countries than to identify the exact nationality.
In addition to using partially labeled data to improve the performance of classifiers, unlabeled data is the main fo-cus of semi-supervised learning [22]. In this setting, a sma ll amount of labeled data is augmented with a large amount of unlabeled data is used to learn better classifiers. Note that unlabeled data may not always help. For example, Cozman et al [8] showed that unlabeled data can degrade classifica-tion performance even in situations where additional label ed data would increase the performance. Partially labeled dat a is a perfect tradeoff between fully labeled data and unlabele d data. We will show that partially labeled data in form of partial labels helps producing better classifiers without t oo much labeling annotation from users.

In this work, we propose a discriminative learning ap-proach which incorporates partial label information into t he conventional margin-based learning framework. First, we review the margin-based learning framework for the multi-class classification problem [9]. Then we extend the learnin g framework to include partial label information. In our ex-periment with a variety of data sets, partial labels not only
Here we want to make a distinction between partially la-beled data and partial labels. Partially labeled data indi-cates only partial information about examples is given in-stead of the actual correct labels. Both pairwise constrain ts and partial labels are subcategories of partially labeled d ata. improve the performance of classification when there is tra-ditional fully-labeled data, but also yields reasonable pe rfor-mance in the absence of any fully-labeled data. The paper is structured as follow: in section 2, we describe in detail the novel partial label classification algorithm; in sectio n 3 we review related work on supervised and semi-supervised learning with partially labeled data; the experimental res ults and conclusion are given in section 4 and 5, respectively.
In this section, we start with the margin-based multiclass classification problem. Then, we show how partial label in-formation fits into the margin-based discriminative learni ng framework.
In the supervised setting, a learning algorithm typically takes a set of labeled training examples, L = { ( x 1 , y ( x n , y n ) } as input, where x i  X  X and the corresponding label y i belongs to a finite set of classes denoted as Y . The goal of classification is to form a hypothesis h : X 7 X  Y which maps an input x  X  X to an output y  X  Y . Many machine learning algorithms is formulated to minimize the regularized empirical risk where  X ( ) is a convex and monotonically increasing function which serves as a regularizer with a regularization constan t example x i measuring the amount of inconsistency between the correct label y i and the predicted label arising from using the weight parameter w .

Consider a mapping  X  : X  X  Y 7 X  F which projects each example-label pair ( x, y )  X  X  X  Y to  X ( x, y ) in a new space F , is defined as where I ( ) is the indicator function. We can obtain the multiclass-SVM proposed by [9] by considering the situatio n where we use the L2-norm regularization, and the loss function l ( x i , y i , w ) is set to the hinge loss, Figure 1: Illustration of how the relative positions of the scores associated with example-label pairs w T  X ( x i , ) change from before training to after train-ing for a fully labeled example.
 Specifically, the multiclass-SVM learns a weight vector w and slack variables  X  via the following quadratic optimiza-tion problem:
Optimization Problem I: Multiclass-SVM subject to:  X  ( x i , y i )  X  L : w T  X ( x i , y i )  X  max After we have learned w and  X  , the classification of a test example x is done by
In this margin-based learning framework, we observed that for a training examples ( x i , y i )  X  L the score associated with the correct label y i , w T  X ( x i , y i ), is greater than the score associated with any other labels y i 6 = y i , w T  X ( x i least the amount, 1  X   X  i . In Figure 1, we demonstrate how the relative positions of the scores associated with exampl e-label pairs, w T  X ( x i , ), change from before training to after training for a fully labeled example, ( x i , y i ).
In this section, we address the problem of learning when there are additional partially labeled data, in the form of partial labels, augmented with fully labeled data. Partial labels are presented as a set of possible labels for each trai n-ing example, one of which is the correct label. Let PL = data, where x i  X  X and the corresponding set of possible labels Y i  X  Y , one of which is the correct label.
The partial label learning problem is also formulated to minimize the regularized empirical risk as shown in Equa-tion (1), where the loss function L ( w ) is the addition of the empirical loss due to the fully labeled data and the partial label data. Formally, the loss function can be expressed as, In addition to utilizing the same L2-norm regularization an d the hinge loss for the fully labeled data, we use the followin g hinge loss, l ( x i , Y i , w ), for the partial label data: max The justification of using the hinge loss for the partial labe l data is that for a partial label training example ( x i , y PL the maximum score associated with the partial labels y  X  Y i , is greater than the maximum score associated with any other labels y i 6 X  Y i , by at least the amount, 1  X   X  i . In Figure 2, we demon-strate how the relative positions of the scores associated w ith example-label pairs, w T  X ( x i , ), change from before training to after training for a partial label example, ( x i , Y i Figure 2: Illustration of how the relative positions of the scores associated with example-label pairs w
T  X ( x i , ) change from before training to after train-ing for a partial label example.

In this learning setting, the average size of the partial labels, of the partial label data indicates the amount of labeled in-formation given to the learning algorithm. In the limit, if | Y | = 1 then we have the conventional supervised learning framework where each training example is given the correct label. Moreover, if | Y i | = |Y| then we obtain the semi-supervised learning framework where there is additional un -labeled data augmented with the fully labeled data. We will show later in the experiment section, how the classification performance changes in according to the variation of the siz e of the partial labels.

Formally, the partial label SVM classification (PL-SVM) learns a weight vector w and slack variables  X  and  X  via the following quadratic optimization problem:
Optimization Problem II: PartialLabel-SVM subject to:  X  ( x i , y i )  X  L :  X  ( x i , Y i )  X  PL : The classification of test examples are done in the same man-ner as for the multiclass-SVM classification.

In order to solve the partial label SVM classification, we apply the partial label Pegasos (PL-Pegasos), a extended version of the Pegasos algorithm proposed by [16]. The PL-Pegasos is a simple and effective iterative algorithm for sol v-ing the above QP and does not require transforming to the dual formulation. The algorithm alternates between gradi-ent descent steps and projection steps. In each iteration, t he algorithm first computes a set of labeled examples A L  X  L and a set of partially labeled examples A PL  X  PL that con-tain violated examples. Then the weight vector w is updated according to the violated sets A L and A PL . In the projec-tion step, the weight vector w is projected to the sphere of radius 1 / Algorithm 1.

In order to used the kernel trick , as pointed out in [16], we set w 1 = 0 then w t can be written as Hence, we can incorporate the usage of kernel when com-puting inner product operations, i.e.: In our experiments, we use the polynomial kernel, where the polynomial kernel degree d is chosen from the set { 1 , 2 , 3 , 4 , 5 } . Algorithm 1 : Partial Label SVM Classification (PL-SVM)
Input: L -the labeled data, PL -the partial label data
Initialize: Choose w 1 such that k w 1 k  X  1 / for t = 1 to T do ff , y ) &lt; 1 , y i )] + X end for
The efficiency and guaranteed performance of PL-SVM in solving the quadratic optimization problem is shown by the following theorem: Theorem 1. Let then the number of iterations for Algorithm 1 to achieve a solution of accuracy  X  &gt; 0 is  X  O ( R 2 / (  X  X  )) . The proof of Theorem 1 is omitted since it is similar to the one given in [16].
In supervised learning, partially labeled data in the form of pairwise constraints have been shown to improve the per-formance of classifiers. In [21, 20], the authors proposed a discriminative learning framework which can simultane-ously learn the fully labeled data and pairwise constraints . In addition, the pairwise constraint information is also us ed to learn metric learning algorithms [10, 11, 12, 14, 15, 18, 6]. Metric learning algorithms first learn a Mahalanobis dis -tance metric and then apply distance-based classifier such as K-nearest neighbor to the transformed data.

In semi-supervised learning, partially labeled data in the form of pairwise constraints is used as users X  feedback to guide the clustering process [17, 3, 13, 19, 7, 2, 4]. In particular, CKmeans [17] is a semi-supervised variant of Kmeans. The objective function of CKmeans is reformu-lated to incorporate the cost incurred by violating any pair -wise constraints specified by the user. In addition, [4] uti-lized both metric learning and pairwise constraints in the clustering process. In MPCKmeans (metric learning and constraints Kmeans), a separate weight matrix for each clus -ter is learned to minimize the distance between must-link instances and maximize the distance between cannot-link instances. Hence, the objective function of MPCKmeans minimizes cluster dispersion under the learned metrics whi le reducing constraint violations. However, most existing al go-rithms may get stuck at local-optimal solutions for the clus -tering problem with pairwise constraints as users X  feedbac k.
We evaluate our proposed algorithm (PL-SVM) on six data set from the UCI repository [1] and the LIBSVM data [5]. A summary of the data sets is given in Table 1.
In our experiments, we compare the classification perfor-mance of the PL-SVM algorithm which utilizes the partial label information against the regular SVM which ignores the partial labels. As a upper bound for the performance of the PL-SVM algorithm, we train a regular SVM using the fully labeled data and the partial label data where the true labels are revealed to the algorithm. (We refer to this training procedure as SVM All.) For all the algorithms, we set the parameter values as follows:
In Figures 6 and 7, we plot the classification performance of SVM, SVM All and PL-SVM (one for each value of the partial label size) versus the size of the partial label data at different sizes of the fully labeled training data for six data sets. To summarize the information, Figure 3 shows the same information but averaging across the six data sets. For all six data sets, we observe that the performance of the PL-SVM is between the performance of SVM and SVM All. This behavior is what we should expect since partial label information helps to significantly improve the performance of PL-SVM over SVM which does not use this information; and fully labeled data should still provide more discrimina -tive information to the SVM All than partial labels could to the PL-SVM. We also notice the expected learning curve for PL-SVM as the size of the partial label data is varied. For a fixed fully labeled training size, as we increase the amount of the partial label data the performance of PL-SVM is also increasing. Moreover, we also observed the inverse relatio n between the amount of performance improvement of PL-SVM over SVM and the size of the partial labels. For fixed sizes of the fully labeled data and the partial label data, as we increase the size of the partial labels the performance of PL-SVM is decreasing. This behavior is expected since the larger the size of the partial labels the less the discrimina tive power of each partial label example.
In this paper, we address the problem of learning when some cases are fully labeled while other cases are only par-tially labeled, in the form of partial labels. Partial label s are represented as a set of possible labels for each training ex-ample, one of which is the correct label. We formulate the partial label learning problem as a convex quadratic opti-mization minimizing the L2-norm regularized empirical ris k using hinge loss and present an efficient algorithm for clas-sification in the presence of partial labels. pendigits. and usps.
This work was supported by NSF CAREER Grant # 0347318. We would like to thank anonymous reviewers for valuable comments and suggestions. [1] A. Asuncion and D. Newman. UCI machine learning [2] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. [3] M. Bilenko, S. Basu, and R. J. Mooney.
 [4] M. Bilenko, S. Basu, and R. J. Mooney. Integrating [5] C.-C. Chang and C.-J. Lin. Libsvm data. [6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a [7] D. Cohn, R. Caruana, and A. McCallum.
 [8] F. Cozman, I. Cohen, and M. Cirelo. Semi-supervised [9] K. Crammer and Y. Singer. On the algorithmic [10] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. [11] A. Globerson and S. Roweis. Metric learning by [12] J. Goldberger, S. Roweis, G. Hinton, and [13] D. Klein, S. D. Kamvar, and C. D. Manning. From [14] M. Schultz and T. Joachims. Learning a distance [15] S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online [16] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [17] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. [18] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance [19] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. [20] R. Yan, J. Zhang, J. Yang, and A. G. Hauptmann. A [21] J. Zhang and R. Yan. On the value of pairwise [22] X. Zhu. Semi-supervised learning literature survey.
