 Question Answering System (QAS) is a form information retrieval that tries to produce an exact answer given a natural language question. Despite its natural task to find a single answer, a QAS needs supporting textual context from one or more document collections, in the size of a sentence, a passage, a paragraph or even the whole document [1]. This is one of the reasons why question answering also considered as a challenging field in information retrieval. 
Most typical QAS pipeline architectures consist of four main components, i.e.: question analyzer, query formulation, information retrieval and answer validation. types, which will be used as the expected answer type during the answer validation phase. In the query formulation component, a question will be formulated into a specific keyword-based query, for instance by using bag-of-words (BOW) approach after removing stopwords, or by using WordNet for term expansion [2]. In the information retrieval component, usually by using third-party search engines, such as: answer validation component needs to valid ate whether a retrieved answer candidate reflects some information need, with respect to the expected answer type, and produce results [3, 4, 5]. In this context, the performance of an underlying information retrieval system is important to retrieve relevant passages. 
Recent works in information retrieval strategies that are specific to the question answering (QA) task are mostly focused on: linguistic and semantic constraints [4, 6], relevance feedback [7], semantic role labeling [8] or by topic indexing [9]. Despite these recent approaches, performing QA pa ssage retrieval in a more conventional information retrieval way, i.e. by using textual features consisting of appropriate question terms, could be preferable if importa nt search terms are already stated in the question. Recently, a new approach has been developed that focuses in the relational data between existing questions answer pairs [10]. By assuming that answers are relating unseen questions to their appropriate answers. 
Inspired by this work, we study whether a question and its answer can be related using low cost language features in a QA passage retrieval scenario. We argue that in a QA passage retrieval context, low cost language features, such as n -gram, can actually contribute some positive influence to represent the information need that also appear in other passages, which have some analogical or related features. Table 1 gives an example of such a case, the words in bold show the overlapping words between the question and the answer, underlined words show the overlapping words between both question answer pairs, and the text surrounded by  X  X  ] X  are the exact answers. 
In Table 1, seems that the two questions have different question types. The first question could be classified into 'COUNTRY' question type and the second one into 'LOCATION' question type. But if we consider the QA pairs as a relation, the answers of both question are directing into something in common, i.e. a kind of named-entity, by using the question word 'WHICH' , either it is about  X  X ocation of an event X  or  X  X ocation of a section in a regulation document X . In this way, we could define an analogy as measure of similarity between structures of related objects. As stated in [2, 3], most typical QA architectures consider questions and answers as independent elements. The consequence of this kind of architecture is that question type and the related expected answer type cannot be learned in a single learning mechanism framework. To compensate for the independence of a question and its answer pair, we exclude the question type component in our approach, and use a single learning mechanism framework to learn the relations of a question and its answer pair. We propose to use the related features of a question and its answer as a means to recognize the information need of a question, which at the same time could also give an indication of how a question should be answered. The related features are learnt from a collection of question answer pairs, in which the answer is given in the form of a passage. The complete approach can be seen in Fig. 1. 
In our approach, we assume that all words that appear in a question are important and have influences during the information retrieval phase. Each passage candidate and its related question will be compared to a set of question answer pairs which relation has been learnt by using an analogical learning mechanism. This comparison value will produce a new score that shows how information need from a passage candidate is related to the set of learnt question answer pairs, which also depends on the feature set during the analogical learning . We hypothesized that it is theoretically possible to exchange those related features or to enhance the original question, and form the final passage answer. Bayesian Analogical Reasoning (BAR) was originally introduced in [11, 12], which basic idea is to learn model parameters and priors from related objects, and update it during the comparison process of a query to obtain marginal probability that relates the query with the objects that have been learnt. 
Assume there is a space of unseen functions . If two objects, a question Q and an answer A are members of a set S , which are related by an unknown function f(Q,A) = 1 , what needs to be quantified is how similar the function f(Q,A) is need a set of priors that will be used to integrate them over the function space. 
Suppose for each pair , there exists a feature vector: point of link representation on the feature space  X  . 
This feature space mapping computes a K-dimensional vector of features of the question answer pairs, which is hoped to have a relevant link prediction between the objects in the pairs. The feature vector X ij , for each pair of question and answer consists of the same number of features, and thus we can define a measure as the link representation between such pair. In this case we use the cosine distance. 
If there is an unseen label L ij , with as a predicted indicator of the vector , which models the presence or absence of interaction between objects, and could be learnt by performing the logistic regression model: where logistic(x) is defined as: .

The priors are learnt by using: where is the Maximum Likelihood Estimator (MLE) of , N(m, v) is a normal of mean m and variance V . Matrix links that exist in the trained set. 
During the retrieval process of linked pair s, a query is compared by the functions for links prediction by marginalizing over the parameters of the functions. If we have L process indicating the order of predicted links between the query and the related objects that has been learnt, and is compute as follows: function. To evaluate the influence of analogical reasoning in our question answering scenario, we use Indri [13, 14], to retrieve the top-5 passage candidates of the testing question set, in a bag-of-words approach. We evaluate the performance in terms of accuracy at the top-1 answer against the gold standard. The accuracy is computed according to the formula: where tp is the number of  X  X rue X  answer at the top-1, and fp is the number of  X  X rong X  answer. 
We use the question answer pairs from ResPubliQA 2009 paragraph selection gold standard as our training set, and ResPubliQA 2010 as our testing data. To maintain the question type X  X  equality between the two question collection sets, we use the: Definition (95 questions), Factoid (139), Reason-Purpose (187), and Procedure (79) question types during the experiments, and exclude the Opinion and Other question types. In total, we have 500 question answer pairs from ResPubliQA 2009 collection, and 133 questions from ResPubliQA 2010, which consist of: Definition (32 questions), Factoid (35), Reason-Purpose (33), and Procedure (33) . The document collections that were used during this study are JRC-ACQUIS and EUROPARL [15]. We created an index that was based on paragraph segmentation with Indri indexing tools. In total, we have about 1.5 million paragraphs indexed that were considered as documents. Indri is a search engine that is specially designed for passage retrieval, thus will be fitted to the retrieval task in this study [14]. The works in [15] showed that paragraph selection is a challenging task, and one of the successful methods is to improve paragraph retrieval by using overlapping uni-and bigram occurrences X  as contextual information. This is the main motivation that we explored the following textual feature sets during the experiments: 
Finally, we decomposed the feature sets into SVD 25-dimension, as the main feature dataset, in order to reduce the word features dimensionality. 
During the passage retrieval phase, we made two types of query enhancement. The first one is to add overlapping non-stopwords word occurrences, which appear in the top-5 retrieved analogous question answer pairs, to the original question, as a kind of query expansion method. The second one is to use the stopwords that appear in the best analogous question answer pair, to complete the stopwords that was removed from the original question. Each query is considered as a BOW model. To evaluate our expansion method, we also run some experiments of the original question by using Indri pseudo-relevance feedback, which were set to some configuration of smoothing parameters and weights for the original query [14, 16]. We present our results in the following aspects: the accuracy performance of the query expansion strategies, the comparison of the performance with respect to Indri relevance feedback, and the influence of the retrieved analogous pairs to the passage retrieval ranking performance. 5.1 Performance of Query Expansion The result of the re-run scenario by using overlapping of non-stopwords which occur in the top-5 of analogical pairs is given in Table 2. The best accuracy score was achieved by the  X  X igram occurrences by using stemmer and TF-IDF weighting completed with named-entity X  feature set, i.e. 0.31. For our example in Table 1, by using the  X  X igram-stem-TFIDF-ne X  feature set, the question will be enriched by some other non-stopwords term(s) that occur in the top-5 analogical pairs, as follows :  X  X n which country will the 2010 FIFA World Cup be held + European X  . 
The low accuracy performance is mainly influenced by out-of-topic terms. Such cases are mostly occurred when the analogical pair s come from different topic or when the semantics of the question and answer are too far to be captured in the analogical model. For example the question:  X  X hat is maladministration? X  (Q#6-2010: Definition type) , the analogical model only considered the word  X  X hat is X  , as related important features, and thus fail to enrich the query with something that is related to  X  maladministration X  . 
To create another view of retrieval performance, a running result of the original questions that were expanded using WordNet is included in Table 2. This expansion strategy is performed for every verb and noun from the original questions [2, 17]. The result is mostly below the accuracy of our approach. The original questions were mostly enriched with out-of-topic terms which decreased the retrieval accuracy. For our example in Table 1, the query would be expanded as follows:  X  X n which (country OR commonwealth OR state OR land OR na tion OR "res publica" OR "body politic") will the 2010 FIFA World Cup be (held OR maintained OR kept) X  . 
Table 3 shows the result of the re-run scen ario by using the stopwords that appear in the best analogous question answer pair. Again, the retrieval result for the WordNet query expansion is also included, which performance is lower than our approach. The best performed feature set is the  X  bigram occurrences by using stemmer and TF-IDF weighting X , with 0.34 accuracy. For our example in Table 1, the question, by using the  X  X igram occurrences by using stemmer and TF-IDF weighting X , will be reformulated as the following bag-of-words query:  X  X ountry FIFA World Cup held + What is the of A top side to of which is X  . 
The stopwords exchanged have the same failure analysis as the non-stopwords enhancement. In overall the stopwords exchanged performance is slightly better in terms of accuracy than the non-stopwords expansion. 5.2 Indri Pseudo-relevance Feedback To evaluate the performance of our best feature set expansion approach, we compare our results to the Indri pseudo-relevance feedback of the original questions, with various parameter settings. The first parameter setting is regarding the document smoothing to overcome data sparseness problem [14]. We use Dirichlet smoothing, and experimenting Indri feedback smoothing (fbMu = 0.0 (default), and 0.5) , the query word weighting (fbOrigWeight = 0.5, 0.8 and 1.0), the number of terms for the feedback (fbTerms=10) , and the number of documents for the feedback (fbDocs=5) . The comparison is presented in Table 4. 
From Table 4, we can observe that the accuracy of our expansion approach (0.31) is quite similar to the accuracy of Indri pseudo relevance feedback (0.32). This indicates that the expanded terms of the analogical question answer pairs can maintain the information need of the original query. Further analysis on the top-5 retrieval results, in terms of Mean Reciprocal Rank (MRR) performance; give us promising results for answer validation strategy, which is beyond this study. 5.3 Question Type and Retrieval Performance Issues Table 5 gives a number of analogous pairs examples, relating to the question type, of the  X  X igram-TFIDF-NE X  feature set in Table 2. The  X  X uestion type X  classification accuracy characterized by its wide scope of questions and documents coverage in parliamentary domains. On the other hand, the BAR framework assumes that the feature space should general decomposable as similarities between only the textual features in the question part, but also the presence or absence of the features in the answer part of related pair. feature sets in Table 2 and Table 3. The Reason-Purpose question type always has the best accuracy, and Definition question type always has the lowest one. terms, such as:  X  X n order X  ,  X  X rder to X  or  X  X bjective to' in the Reason-Purpose type, could provide us much better expanded terms, in contrast to the relations of quite specific terms in the Definition question types, such as:  X  X efine as X  , or  X  X he meaning . retrieval ranking performance. 
The result presented in Table 7 indicates that a simple question has in fact more more complex question with numerous term occurrences X  in the answer part, has the tendency to be more related to their analogous pair, and hence could achieved a better retrieval performance, as in the Reason-Purpose type. In general we conclude that the predicted analogical relation between question answer overall passage accuracy in this study is much below the best performance of the ResPubliQA 2010 baseline [15], which is 0.73. It seems that the feature sets which were explored during the experiments are not enough to bridge the semantic gap between question and answer pairs. The choice of feature set is a crucial step in our study, which give significant influence to the retrieval results. In our study the best performed feature set is  X  X igram occurrences by using stemmer and TF-IDF weighting completed with named-entity X  for the query expansion approach, and  X  X igram occurrences by using stemmer and TF-IDF weighting X  for the stopwords exchanged approach (c.f. section 5.1). 
Considering that we cannot always have all possibilities of question answer pairs during the training, it might valuable to aggregate patterns from n -most analogous question answer pairs, as recurring patterns, would seem to specify an indicative feature of the information need. Such automatic pattern generation strategy will be useful to expose question type analysis and its expected answer type in a question answering system. To address these issues we plan to conduct study in feature selection mechanism to be fitted in the analogical model as our future work. 
