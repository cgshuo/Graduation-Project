 In this paper, we show that PLSI and NMF optimize the same objective function, although PLSI and NMF are different algo-rithms as verified by experiments. In addition, we also propose a new hybrid method that runs PLSI and NMF alternatively to achieve better solutions.
 I.2 [ Artificial Intelligence ]: Learning; I.5.3 [ Pattern Recog-nition ]: Clustering Algorithms, Experimentation, Theory NMF, PLSI, Equivalence, Chi-square
Non-negative Matrix Factorization (NMF) [1, 5] and Prob-abilistic Latent Semantic Indexing (PLSI) [4] have been suc-cessfully applied to document clustering recently. Despite sig-nificant research on both NMF and PLSI, few attempts have been made to establish the connections between them while highlighting their differences in the clustering framework. Gaussier and Goutte [3] made the first connection between NMF and PLSI, by showing that the local fixed point solutions of the iterative procedures of PLSI and NMF are the same. Their proof is, however, incorrect. NMF and PLSI are different al-gorithms. They converge to different solutions even if they start from same initial conditions.

In this paper, we show that PLSI and NMF optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoret-ical basis for a new hybrid method that runs PLSI and NMF al-ternatively, combining different advantages of PLSI and NMF and thus achieving better optimal solutions. Extensive exper-iments on 5 real-life datasets show the relation between NMF and PLSI. Our results indicate the hybrid method lead to sig-nificant improvements over NMF-only or PLSI-only methods. Suppose we have n documents and m words (terms). Let F =( F ij ) be the word-to-document matrix: F ij = F ( w i is the frequency of word w i in document d j .

In this paper, we re-scale the term frequency F ij by F ij F ij /T w , where T w = With this stochastic normalization, occurrence probability p ( w i ,d j )= F ij .
 The general form of NMF is where the matrices C =( C ik ) ,H =( H jk ) are nonnegative matrices. They are determined by minimizing
PLSI maximize the likelihood where P ( w i ,d j ) is the factorized (i.e., parameterized or ap-proximated ) joint occurrence probability where the probability factors follow the normalization of prob-abilities Proposition 1: Objective function of PLSI is identical to the objective function of NMF, i.e., J PLSI =  X  J NMF + constant by setting ( CH T ) ij = P ( w i ,d j ) .
 Proposition 2 . Column normalized NMF of Eq.(1) is equiva-lent to the probability factorization of Eq.(4), i.e., ( CH C
S P ( z k ) . Thus ( CH T ) ij = P ( w i ,d j ) in detailed factoriza-tions as in Eq.(4).
 The proofs of Propositions 1 and 2 can be found in [2]. From Propositions 1 and 2, we have Theorem 1: PLSI and NMF are equivalent.

In [3], the authors attempted to prove that NMF and PLSI converge to the same solution (fixed point). Their proof is in-correct. NMF and PLSI are different computational algorithms optimizing the same objective function. In our experiments, starting with the same initial starting C 0 ,H 0 ,NMFandPLSI always converge to different solutions.
 NMF and  X  2 -statistic . J NMF of Eq.(2) has a complicated ex-pression. We give a better understanding by relating it to the  X  2 statistics [2]. Here F ij is the data and ( CH T ) model fit. Assuming | ( CH T ) ij  X  F ij | /F ij is small, we have
Experiment Setup . We compare the clustering performance of each method on 5 real-life datasets. Table 1 summarizes the characteristics of the datasets [6].

Agreements Between NMF and PLSI . For a given cluster-ing solution, as a measure of pairwise matches, we introduce a clustering matrix R =( r ij ) ,where r ij =1 if x i , x j tered into the same cluster; r ij =0 otherwise. To compare NMF and PLSI solutions, we compute the relative difference The computed results are listed in line A of Table 2. These results are the average of 10 different runs, each beginning with different random initial starts in K-means. The results show that the differences between NMF and PLSI are quite substantial for WebKB (24%), and ranges between 1% to 8% in general cases. Function J NMF defines a surface in the multi-dimensional space. Because this global objective function is not a convex function, there are in general a very large num-ber of local minima in the high p -dimensional space. Our ex-perimental results suggest that starting with the same initial smoothed K-means solution, NMF and PLSI converge to dif-ferent local minima. In many cases, NMF and PLSI converge to nearby local minima; In other cases they converge to not-so-nearby minima.
We have seen that NMF and PLSI optimize the same ob-jective function, but their different detailed algorithms con-verge to different local minima. An interesting question arises. Starting from a local minimum of NMF, could we jump out the local minimum by running the PLSI algorithm? Strictly speaking, if an algorithm makes an infinitesimal step, it will Table 2: Dis-agreements between NMF and PLSI. All 3 type experiments begin with the same smoothed K-means. (A) Smoothed K-means to NMF. Smoothed K-means to PLSI. (B) Smoothed K-means to NMF to PLSI. (C) Smoothed K-means to PLSI to NMF. not jump out of a local minimum (we ignore the situation that the minimum could be saddle points). But PLSI algorithm is a finite-step algorithm, so it is possible to jump out of a local minimum. NMF is also a finite-step algorithm.

Interesting enough, experiment results indicate we can jump out of local minima consistently. Starting with converged solu-tion using NMF, we run PLSI until convergence. The obtained accuracy is listed in Line B of Table 2. The accuracy are im-proved significantly compared with the NMF-only solutions (in Line A). Similarly, starting with converged solution using PLSI, we run NMF until convergence. The accuracy are also improved significantly compared with the PLSI-only solutions (in Line C). Based on the ability of NMF for jumping out of local minima of PLSI and vise versa, we propose a hybrid al-gorithm that alternatively runs NMF and PLSI, with the goal of successive jumping out local minima and therefore converging to a better minimum. The hybrid algorithm consists of 2 steps (1) K-means and smooth. (2) Iterate till converge: (2A) Run NMF to converge. and (2B) Run PLSI to converge. We run the hybrid algorithm on all 5 datasets. The results are listed in Table 3. We use accuracy as performance measure [2]. We observe that: (1) NMF/PLSI always improves upon K-means. (2) Hybrid always improves upon NMF and PLSI, and the im-provements are significant.
 Table 3: Clustering Accuracy. (A) K-means. (B) NMF-only. (C) PLSI-only. (D) Hybrid.

