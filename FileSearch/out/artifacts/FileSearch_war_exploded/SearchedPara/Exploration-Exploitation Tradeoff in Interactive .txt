 We study an interesting optimization problem in interactive feedback that aims at optimizing the tradeoff between pre-senting search results with the highest immediate utility to a user (but not necessarily most useful for collecting feedback information) and presenting search results with the best po-tential for collecting useful feedback information (but not necessarily the most useful documents from a user X  X  perspec-tive). Optimizing such an exploration-exploitation tradeoff is key to the optimization of the overall utility of relevance feedback to a user in the entire session of relevance feed-back. We frame this tradeoff as a problem of optimizing the diversification of search results. We propose a machine learning approach to adaptively optimizing the diversifica-tion of search results for each query so as to optimize the overall utility in an entire session. Experiment results show that the proposed learning approach can effectively opti-mize the exploration-exploitation tradeoff and outperforms the traditional relevance feedback approach which only does exploitation without exploration.
 Categories and Subject Descriptor : H.3.3 [Information Search and Retrieval]: Retrieval models General Terms: Algorithms, Experimentation Keywords: Interactive Retrieval Models, Feedback, Diver-sification, User Modeling, Learning
Relevance feedback has proven to be very effective for im-proving retrieval performance [5]. The idea is to have a user to provide relevance judgments on some initial search results shown to the user, a retrieval system can then use the col-lected relevance judgments as feedback information to better infer the user X  X  information need and improve the ranking of the unseen results for the user.

The relevance judgments are assumed to be collected on some top-ranked documents that would be shown to a user as initial results. The judgments collected in this way may not necessarily be the most useful judgments for relevance feedback. As an extreme case, if the top documents all have identical contents, the judgments on all these documents would be clearly redundant, and would not be so useful for relevance feedback as if we collect the same number of judg-ments on a more diversified set of documents. This shows a deficiency of the traditional (standard) relevance feedback strategy: it does not attempt to obtain the most useful judg-ments.

This observation has motivated some researchers to study active feedback methods (e.g., [6, 8]), where the goal is to choose documents for relevance feedback so that the sys-tem can learn most from the feedback information. These methods choose a set of diverse documents for judgments, since judgments on diverse documents can be expected to be more informative and thus helpful for the system to learn the user X  X  information need. However, presenting diverse docu-ments to users means that we generally do not present the search results in the order of relevance. Thus, the utility of the presented documents to the user is generally lower than that of presenting the top-ranked documents. This shows that active feedback has a different deficiency: it does not attempt to optimize the utility of the presented documents to the user.

These observations illustrate an interesting dilemma in in-teractive relevance feedback: On the one hand, we want to present the top-ranked documents to a user so that the pre-sented documents would be most useful for the user in this interaction cycle 1 as in standard relevance feedback which can be regarded as emphasizing exploitation of all the exist-ing information about the user X  X  information need. However, such a strategy does not generate the most useful judgments for feedback. On the other hand, if we diversify the results (i.e., emphasizing exploration ) in order to obtain more use-ful judgments, which would bring more utility to the user in the next interaction cycle, we would risk on decreasing the utility to the user in the current interaction cycle.
Clearly, in an interactive retrieval system, what matters toauseristhe overall utility of relevance feedback, which means that we need to optimize this exploration-exploration tradeoff so as to optimize the overall utility over an inter-action session 2 which includes both the current interaction cycle and the next interaction cycle after feedback. To the best of our knowledge, no existing work has addressed the problem of optimizing the exploration-exploitation tradeoff for relevance feedback.

In this paper, we frame the exploration-exploitation trade-off problem as optimizing a diversity parameter which con-trols the diversity of the results on the second page. Since more diversified results would mean more exploration and thus less exploitation, while less diversified would mean more exploitation and less exploration, we essentially convert this tradeoff problem to one of optimizing this diversity param-eter. We propose a machine learning approach to adap-tively optimizing the diversification of search results for each query so as to optimize the overall utility in an entire ses-sion. Experiment results on three representative retrieval test collections show that the proposed learning approach can effectively optimize the exploration-exploitation trade-off and outperforms the traditional relevance feedback ap-proach which only does exploitation without exploration.
Given a query Q and a document collection C ,weconsider an interactive retrieval system that presents k (e.g., k = 10) documents on each result page to a user. We consider the following interactive process: The system first uses the query to retrieve k documents P 1 =( d 11 , ..., d 1 k ) and present them on the first page. The user is then assumed to click on any relevant documents on this page, leading to a set of relevance judgments on the first page, J 1 . We assume that the user would continue viewing the second page of search results, and at least some results on the third page. We will focus on studying relevance feedback that happens after the user finishes viewing all the results on the second page.
We would like to optimize the diversification of the results on the second page P 2 =( d 21 , ..., d 2 k ) so that the relevance feedback that happens just before the user views the results on the third page would deliver the maximum overall utility on both the second page and the third page. (The utility on the first page has already been fixed in this setup, thus it is irrelevant for the optimization.)
A common way to generate diversified results is a greedy algorithm called Maximal Marginal Relevance (MMR) [1] which models both topical relevance and redundancy of doc-uments. With this strategy, after we have already selected documents d 21 ,...,d 2 ,i  X  1 for the second page, document d would be selected to maximize the following score: where Q is the query, S R ( d 2 i ,Q ) is a relevance-based scor-ing function (i.e., regular retrieval function). S novelty novelty value of d 2 i w.r.t. { d 21 ,...,d 2 ,i  X  1 } ,and  X  is the diversification parameter that controls the degree of diversification, or equivalently, the amount of exploration.
The results on the third page P 3 =( d 31 , ..., d 3 k )wouldbe generated using relevance feedback on the judgments col-lected on both first and second pages. Clearly, the utility of this third page would be affected by the usefulness of the collected judgments J 2 which further depend on the di-versification on the second page. Our goal is to optimize the diversification coefficient, i.e.,  X  , to maximize the over-all utility of the results on the second page and the third page, i.e., to optimize the exploration-exploitation tradeoff. Formally, we assume that there is a function A that can map aquery Q and its corresponding relevance judgments J 1 on the first page to the optimal diversification coefficient, i.e.,  X  = A ( Q, J 1 ).
To optimize the diversification parameter  X  , we propose a learning method to  X  X earn when to diversify the results X  by adaptively learning this coefficient for each query, i.e., adaptive coefficient which we now present in detail.
The features are computed based on only the first-page results, P 1 , and the judgments on it, J 1 . Because at the time of optimizing the diversification on the second page, we only have this much information. We combine these features to learn the function A ( Q, J 1 ) using the past queries as training data. The learned function A ( Q, J 1 ) can then be used for future queries to predict new  X  .

F rel and F nonRel are the set of relevant and non-relevant documents in J 1 , respectively. c ( w,Q ) is the count of term w in query Q. | Q | = w  X  Q c ( w,Q ) is the total number of words in query Q . p ( w |  X  Q )= c ( w,Q ) | Q | is the query language model. p ( w |C ), p ( w |  X  F rel )and p ( w |  X  F nonRel tion language model, language model of relevant documents and non-relevant documents on the first-page results, respec-tively.
 Query Length: | Q | = w  X  Q c ( w,Q ).
 Query Distribution: Each term is associated with an in-verse document frequency which describes the informative amount that a term in the query carries. It is defined as fol-maximum and minimum idf s among the terms w in query Q , respectively.
 Query Clarity: Query clarity has been shown to predict query difficulty [2]. When a query is difficult, it might mean that it has different interpretations, so in order to complete the picture of all aspects of the query, we need to provide a diverse set of documents. (1) According to definition [2], the clarity of a query is the Kullback-Leibler divergence of the query model from the collection model. For our case, the query model is es-timated from the relevant documents in F rel which is de-fined as follows: QClar 1 = w  X  F c ( w,F rel )isthenumberofword w in F rel . (2) To avoid the expensive computation of query clarity, we use the simplified clarity score proposed in [3] as a compa-rable pre-retrieval performance predictor. It is calculated as follows: QClar 2 = w  X  Q p ( w |  X  Q ) log 2 p ( w |  X  Q ) Query Entropy: If query entropy is high, it means that the query covers broad topics, as a result, we do not need to diversify the results. We define two query entropies: (1) QEnt 1= w  X  Q  X  p ( w |  X  Q ) log 2 p ( w |  X  Q ). (2) Since a query is often short, we compute another query entropy score based on the relevant documents in F rel as fol-lows: QEnt 2= w  X  F Number of Relevant documents: If the query has high precision , we do not need to diversify the results; as a result we consider the number of relevant documents in the first-page results as a feature: num = | F rel | .
 Virtual Mean Average Precision: Another feature to capture high precision is to calculate Average precision for the first-page results: VirMAP = d  X  F r is the rank of document d and prec ( r d )istheprecisionof top r d documents.
 Separation of Relevant and Non-Relevant Documents: If the query is clear enough (relevant and non-relevant docu-ments are separated) we do not need to diversify the results. As a result, we define two following features: (1) Jensen-Shannon Divergence (JSD) (2) Cosine Similarity: We denote the Term Frequency tively. The Cosine Similarity is then defined as: CosSim = Diversification (Div): Intuitively, if the baseline results are already diversified, we do not need to do more diversifi-cation; one measure to capture that is as follows: We cluster the top 30 results from KL-divergence retrieval to 5 clusters using K-Means algorithm. We then consider the ratio of the size of the first largest cluster to the second largest cluster. If this ratio is small, it means that we have already formed multiple clusters and the results are already diversified, so we do not need to diversify more.

Learning Algorithm : We use logistic regression for our learning algorithm since it can take any input value and outputs a value between zero and one which is what we want. The logistic regression model is of the form: f ( z )= 1+exp(  X  z ) ,where z is a set of features and f ( z ) is the prob-ability of an outcome. f ( z ) is used to predict the novelty coefficient, i.e.,  X  . z is of the form z =  X  w  X  x where  X  x is a vector of the features and  X  w is the learned weight associated with each feature.
Our goal is to optimize the utility to a user over all the three pages P 1 , P 2 ,and P 3 . We thus need to define how we measure the utility over all these pages.

Since the first-page results is the same for all the methods, we only consider the utility on the second and third-page results. The user is assumed to see all the results on the second page, thus we define the utility on the second page as the number of relevant documents on the second page, denoted as U ( P 2 )= REL ( P 2 ).

For the third page, since the user is only assumed to see some results, the utility is defined as the expected num-ber of relevant documents that the user would see on the third page: U ( P 3 )= 10 j =1 s j j i =1  X  ( i ), where s probability that the user would stop reading after seeing the top j documents and  X  ( i )is1ifthe i -th document is relevant and is zero otherwise.

If we assume the uniform stopping probability, i.e., s j is uniform, we would model the impatient user and the util-ity on the third page with some algebraic transformation is reduced to: U ( P 3 )= 10 n =1 (11  X  n ) 10 . X  ( n ), where n is the ranking of the document. For example, if a document is ranked first on the third page, the expected utility for that document would be one since n =1.

The total utility on both the second page and the third page is thus U ( P 2 )+ U ( P 3 ). We used three standard TREC data sets in our study: TREC2004 robust track (with 249 queries), WT2G (with 50 queries) and AP88-90 (with 149 queries) which represent different genre of text collections. We used Porter stemming and did stop word removal on all these collections.
We used Lemur toolkit to index document collections and retrieved a document list for each query using KL-divergence retrieval model [4] to return 10 result documents to get some limited information about relevant documents and extract features that we need for our learning algorithm. We then based on the extracted features decide if we need to diversify the results on the second page and then feedback is used to improve the ranking of documents on the third page based on the user judgments on the first two pages.

We compare the proposed adaptive diversification method (referred to as AdaptDivFB) with two standard baseline, i.e., NoFeedback and RegularRelFB by measuring the total utility over all three pages. Since the first page is the same for all the methods, we actually only computed the utility over the second and third pages. While the first-page re-sults are similar for all these four methods, the second-page results for different methods are different. For NoFeedback and RegularRelFB, we return 10 results using KL-divergence retrieval model [4]. For AdaptDivFB, we used MixAvg-MMR [9] with the novelty coefficient dynamically set for each individual query using the proposed learning method. Then for the third page, we perform the two-component mixture model [10] for positive feedback and the MultiNeg Strategy [7] for negative feedback in case there is not any relevant documents on the first-page results.

To train our proposed methods, we did 5-fold cross valida-tion. We set the parameters to their optimal values gained from training data, i.e., we set the Dirichlet prior smoothing to its optimal value for each training data using NoFeed-back method, and set the feedback coefficient to its optimal value using RegularRelFB. We fixed feedback term count to 100 and mixture noise parameter [10] to 0.9 and  X  in nega-tive feedback [7] to 0.1. We then use the optimal parameters gained from training data to measure the performance of test queries. To get the optimal novelty parameter for training queries for AdaptDivFB, we try different novelty coefficient  X   X  X  0 , 0 . 1 , 0 . 2 ,..., 1 } using MixAvg-MMR [9] on training data sets and then we choose the optimal one for each query.
We also included another baseline called FixedDivFB which differs from AdaptDivFB in that the novelty coefficient is simply set to a fixed coefficient learned over the same train-ing set as used by AdaptDivFB. Thus FixedDivFB also at-tempts to optimize the exploitation-exploration tradeoff, but with no adaptation to each query, and comparing AdaptDi-vFB with FixedDivFB would allow us to see whether learn-ing an adaptive coefficient is more effective than learning a fixed coefficient.
Our hypothesis is that in order to achieve optimal retrieval performance and outperform relevance feedback, we need to optimize based on the total user utility . In order to test our hypothesis, we compare the two methods that optimize the total utility on both the second and the third pages (i.e., FixedDivFB and AdaptDivFB) with the two baseline meth-ods (i.e., NoFeedback and RegularRelFB). Table 1 shows the results across different TREC data sets. The results in-dicate that method RegularRelFB is better than NoFeed-back as we expected since feedback outperforms the ba-sic retrieval model using KL-divergence. Method FixedDi-vFB outperforms RegularRelFB and AdaptDivFB outper-forms both FixedDivFB and RegularRelFB methods since it uses different novelty coefficient for each query. Statis-tical Significant tests using Wilcoxon singed-rank test in-deed indicate that the improvements of both FixedDivFB and AdaptDivFB methods over baseline RegularRelFB are statistically significant. Thus, by optimizing exploration-exploitation tradeoff, both FixedDivFB and AdaptDivFB outperform the regular relevance feedback method, i.e., Reg-ularRelFB which ignores exploration. AdaptDivFB outper-forms FixedDivFB method which indicates that the optimal Exploration-Exploitation tradeoff is query dependent.
Our hypothesize is that to achieve optimal overall utility on a session, we should train with a  X  X imilar X  objective func-tion (i.e., a similar utility measure on the training data). Thus, we expect training to optimize utility on both sec-ond and third page should lead to better performance than training to optimize either one alone. In particular, optimiz-ing the second-page  X  only  X  would lead to over-exploitation (ending up with lower third-page utility), while optimizing third-page  X  only  X  would be the opposite.

In order to see if this hypothesizes is true, we conduct two experiments: (1) optimizing the second-page utility only (exploitation) (2) optimizing the third-page utility only (ex-ploration). The results are shown in table 2 (we only show the results for AdaptDivFB method, similar patterns can also be seen for FixedDivFB method.). These results show clearly the need for optimizing the tradeoff between exploita-tion and exploration (i.e., trained over the sum of utilities on the second and third pages), and neither pure exploita-tion, nor pure exploration can maximize the total utility even though they maximize the utility on the second page and that on the third page, respectively.
In this paper, we studied how to optimize relevance feed-back to maximize the total utility over an entire interaction session. In particular, we studied the issue of exploration-exploitation tradeoff in interactive feedback. We framed this tradeoff as a problem of optimizing the diversification of search results. We proposed a learning method to adaptively optimizing the diversification of search results for each query. Experiment results on three representative TREC data sets indicate that our proposed learning method is effective for optimizing such a tradeoff and outperform the traditional relevance feedback.

This work is a first step in addressing the new problem of optimizing the exploration-exploitation tradeoff in interac-tive feedback. There are many interesting directions to fur-ther explore. For example, There are many different ways of diversification which can be compared. Also, understand-ing how the learned knowledge could be transferred across different data sets is another interesting line of future work.
This paper is based upon work supported in part by an Al-fred P. Sloan Research Fellowship, an AFOSR MURI Grant FA9550-08-1-0265, and by the National Science Foundation under grants IIS-0713581 and CNS-0834709. The first au-thor is supported by Sohaib and Sara Abbasi Fellowship and Yahoo! Key Scientific Challenge Award. Any opinions, findings, conclusions, or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors. [1] J. Carbonell and J. Goldstein. The use of mmr, [2] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [3] B. He and I. Ounis. Inferring query performance using [4] J. Laffetry and C. Zhai. Document language models, [5] J. Rocchio. Relevance feedback in information [6] X. Shen and C. Zhai. Active feedback in ad hoc [7] X. Wang, H. Fang, and C. Zhai. A study of methods [8] Z. Xu, R. Akella, and Y. Zhang. Incorporating [9] C. Zhai, W. Cohen, and J. Lafferty. Beyond [10] C. Zhai and J. Lafferty. Model-based feedback in the
