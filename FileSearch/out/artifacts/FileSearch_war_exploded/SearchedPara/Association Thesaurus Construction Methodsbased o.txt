 Wikipedia, a huge scale Web based encyclopedia, attracts great attention as an invaluable corpus for knowledge extrac-tion because it has various impressive characteristics such as a huge number of articles, live updates, a dense link struc-ture, brief anchor texts and URL identification for concepts. We have already proved that we can use Wikipedia to con-struct a huge scale accurate association thesaurus. The as-sociation thesaurus we constructed covers almost 1.3 million concepts and its accuracy is proved in detailed experiments. However, we still need scalable methods to analyze the huge number of Web pages and hyperlinks among articles in the Web based encyclopedia.

In this paper, we propose a scalable method for construct-ing an association thesaurus from Wikipedia based on link co -occurrences . Link co-occurrence analysis is more scal-able than link structure analysis because it is a one-pass process. We also propose integration method of tfidf and link co-occurrence analysis. Experimental results show that both our proposed methods are more accurate and scalable than conventional methods. Furthermore, the integration of tfidf achieved higher accuracy than using only link co-occurrences.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Thesauruses ; H.2.8 [ Database Man-agement ]: Database Applications X  Data mining, Statisti-cal databases Algorithms, Experimentation, Measurement, Performance Data mining, link co-occurrence, thesaurus, Web mining, Wikipedia mining
A thesaurus is a kind of dictionary that defines a set of concepts and relations among these concepts. It can be clas-sified into two types. One is the  X  X elation thesaurus X , a the-saurus that defines explicit relationships (such as  X  X s-a X  and  X  X art-of X ). The other is the  X  X ssociation thesaurus X , a the-saurus that enables users to extract associated words of a specific word. An association thesaurus can be represented as a weighted-graph with concepts being its nodes and the relatedness between these nodes being represented by edges. The structure of an association thesaurus is not hierarchi-cal like the relation thesauru s but a network structure. The effectiveness of a thesaurus is widely proved by various re-search areas such as natural language processing (NLP) and information retrieval (IR). For example in the IR research area, keyword matching based Web search engines usually cannot find Web pages that do not exactly match the query keywords. Thus, researches have been conducted on query expansion that identifies words related to a query. Query ex-pansion enables users to find related Web pages even if they do not contain the query keywords. In the research area of information retrieval, thesauri are often used for expanding queries.

Nevertheless, automated thesaurus dictionary construc-tion (esp. machine-understandable) is one of the most dif-ficult issues. Of course, the simplest way to construct a thesaurus is human-effort. Thousands of contributors have spent much time to construct high quality thesaurus dictio-naries in the past. However, since it is difficult to maintain such huge scale thesauri, they do not support new concepts in most cases. Therefore, a large number of studies have been made on automated thesaurus construction based on NLP. However, issues due to complexity of natural language, for instance the ambiguous/synonym term problems still re-main on NLP. We still need an effective method to construct a high-quality thesaurus automatically avoiding these prob-lems.
We noticed that Wikipedia 1 , one of the biggest collabo-rative Wiki-based encyclopedia, is a promising corpus for thesaurus construction. Recently, Wikipedia has become dramatically popular among Internet users as the WWW evolves. It covers concepts of various fields such as Culture, Arts, Geography, History, Science, Sports or Games. Its ma-jor characteristic is that it allows users to submit or mod-ify articles freely and quickly via Web browser. Wikipedia contains more than 1.3 million articles (Sept. 2006, English only) and is becoming larger day by day, while general ency-clopedias contain just several hundred thousands of articles at most. According to statistics of Nature, Wikipedia is about as accurate as Britannica 2 , an encyclopedia created by many specialists[9]. In addition, the greatest difference of Web-based encyclopedias compared to general electronic dictionaries is that articles are connected with each other by hyperlinks, and these hyperlinks compose a dense link structure.

We have been focusing on these special characteristics of Wikipedia and have conducted a number of studies on Wikipedia Mining[15, 16], Further, several researches have already proved the importance and effectiveness of Wikipedia Mining[8, 14, 18, 22]. In our previous work, we proved that it is possible to construct a large scale accurate association thesaurus from Wikipedia by mining its link structure. By using the constructed thesaurus, it is possible to construct a query expansion method which covers wide-range terms.
Our proposed method, named pfibf 3 , computes related-ness among terms (concepts) by analyzing the link struc-ture of Wikipedia articles. pfibf analyzes the relatedness by counting the number of n -hop length paths, and a sparse ma-trix compression method achieved reasonable performance in terms of scalability. However, the number of Wikipedia articles is growing dramatically, thus it will be difficult to analyze the whole Wikipedia in the future due to the scal-ability. This is the reason why we seek alternative methods having more scalability.

In this paper, we propose a scalable association thesaurus construction method based on link co-occurrence. Our ap-proach analyzes statistical information of links in the whole Wikipedia to extract the features of a particular concept.
A thesaurus is a data structure that defines the semantic relatedness among words[21]. The simplest way to construct a thesaurus is human effort. WordNet[13] is a notable ex-ample developed by a lot of contributors. WordNet contains over eighty thousand nouns and ninety thousand hierarchi-cal relations. However, it is obvious that thesaurus construc-tion by human-effort is very time consuming. Thousands of contributors have spent much time to construct high qual-ity thesaurus dictionaries in the past and we should respect their contribution, but still it is difficult to maintain such huge thesauri, thus traditional thesauri usually do not sup-port new concepts. To solve this problem, a large number of studies have been conducted on automated thesaurus con-struction.

In this section, we introduce a number of studies for au-tomated thesaurus construction related to our research. http://www.wikipedia.org/ (visited on June 3, 2008) http://store.britannica.com/(visited on June 3, 2008)
Themethodnamewas lfibf in the past and was renamed to pfibf
In the field of thesaurus construction based on natural language processing (NLP), a large number of automatic or semi-automatic thesaurus construction methods such as word co-occurrence based methods, word filtering based meth-ods and word clustering methods have been proposed over time[3, 5, 19, 21, 23]. However issues due to complexity of natural language, such as the ambiguous/synonym term problems, still remain on NLP.

Additionally, NLP has the disadvantage of morphologi-cal analysis. NLP based thesaurus construction methods require to split sentences into morphemes (the minimum linguistic unit) and perform POS tagging as preprocessing. Brill X  X  Tagger[1], the Stanford NLP tools [10] and OpenNLP [17] are famous tools for morphological analysis, tagging of word classes and parsing, but sometimes make errors caused by insufficient consideration of word meanings (e.g. ambigu-ous, synonym and unknown words).
One of the most notable differences between an ordinary text corpus and a Web corpus is the existence of hyper-links. Hyperlinks do not just provide a navigational function among pages, but have more valuable information. There are two type of links;  X  X orward links X  and  X  X ackward links X  (See Figure 1). A  X  X orward link X  is an outgoing hyperlink from a Web page, an incoming link to a Web page is called  X  X ackward link X . Web structure mining researches such as Google X  X  PageRank[12] and Kleinberg X  X  HITS[11], empha-size the importance of backward links in order to extract objective and trustful data.

The  X  X nchor text X  is also a valuable information. A anchor text is the text part of a link that is shown in the Web page. It is often closely related to the content of the linked page, thus it can be used for various purposes such as Web page categorization[2].

By analyzing information on hyperlinks, we can extract various information such as topic locality[6], site topology, and summary information. Topic locality is the law that Web pages which are sharing the same links have more topi-cally similar contents than pages which are not sharing links.
In recent years, thesaurus construction methods based on Web mining are getting much attention. Chen et al.[4] have proposed an approach to automatically constructing domain-specific thesauri by hyperlink structure analysis. How-ever, this approach is confronted by two difficulties; the NLP problem and the scalability. This algorithm still uses NLP tools to segment the anchor text. Moreover, if we apply this algorithm to very large Web based dictionaries, the analysis becomes very time consuming.
Wikipedia has become an invaluable corpus for knowl-edge extraction because of its unique characteristics. In this subsection, we describe two important characteristics of Wikipedia; the dense link structure and concept identifi-cation by URL, and introduce some researches on Wikipedia mining.

First of all, Wikipedia has a very dense link structure.  X  X ense X  means that it has a lot of  X  X nner links, X  links from pages in Wikipedia to other pages in Wikipedia. This means that articles are strongly connected by hyperlinks and there is no doubt that it is possible to extract important knowledge by analyzing the link structure. Let us show some results of Wikipedia link structure analysis. Figure 2 shows the dis-tribution of backward links plotted on a linear scale (left) and on a logarithmic scale (right). It has Zipf distribution, containing a few nodes that have a very high degree and many with low degree of backward links. Only 196 pages have more than 10,000 backward links per page, 3,198 pages have more than 1,000 backward links per page, and 67,515 pages have more than 100 backward links per page. To-tally 49,980,910 forward links (excluding redirect links) were extracted from 1,686,960 pages (excluding redirect, image, category pages). This means a page in Wikipedia has 29.62 forward links on average. Further, 2,531 pages have more than 500 forward links per page and 94,932 pages have more than 100 forward links per page. It can be concluded from the statistics shown above, that Wikipedia has a very dense link structure. We believe that Wikipedia has topic locality and the connectivities among articles are much stronger than on ordinary Web sites because of the dense link structure. The dense link structure shows us the potential of Wikipedia mining and we believe that it is possible to extract valuable knowledge by analyzing its link structure.

Second, concept identification based on URL is also a key feature on Wikipedia. Ordinary (electric) dictionaries have indexes, but several concepts are put into one index in most cases. This means that ambiguous terms are listed in one article. The problem is that this is human readable, but not machine understandable. For example, if a sentence  X  X olden delicious is a kind of apple X  exists in an article in a dictionary, humans can immediately understand that  X  X p-ple X  describes a fruit. However, it is difficult to analyze for a machine because  X  X pple X  is an ambiguous term and there is no identification information for it. To make this sen-tence machine understandable, we need some identifier. On Wikipedia, almost every concept (article/page) has its own URL as an identifier. In other words, each concept can be identified by its URL individually. This means that it is possible to analyze term relations avoiding ambiguous term problems or context problems.

As described above, Wikipedia has many advantages as a corpus for thesaurus construction. In previous studies, there have been several researches for constructing thesauri based on Wikipedia analysis; based on tfidf , pfibf etc. We briefly introduce them in the following. tfidf [19] is a weighting method that is often used to ex-tract important keywords from a document. This method uses two measurements, tf (Term Frequency) and idf (Inverse Document Frequency), and treats the products of tf and idf as the importance level of each word in the document. tf is simply the number of appearances of a term in the docu-ment. df describes the number of documents containing the term. In short, the importance of a term basically increases with the term frequency of the term in the document, and decreases with the inversed document frequency of the term in the whole collection of documents, since idf worksasfilter for common terms.
 Gabrilovich et al. has applied tfidf to Wikipedia[8]. In Wikipedia, a page corresponds to a concept(word) and hy-perlinks clearly represent semantic associations to other con-cepts. Therefore, extracting associations between words is achieved by extracting important hyperlinks in the page by means of tfidf . The importance of each hyperlink in a doc-ument can be defined as follows: where tf ( l, d ) is the number of appearance of hyperlinks in the article d , df ( l ) is the number of articles containing the hyperlink l ,and N is the number of articles.

Then we can create vector space model[20] based vectors for each concept. In these vectors, a dimension represents a hyperlink and an element represents the importance of a correspondent hyperlink. The relatedness between two concepts can be calculated by comparing their vectors using correlation metrics such as cosine metrics.

This method has high scalability because it requires hy-perlinks of only one article, i.e. local information, to extract the vector representing one concept. This characteristic, however, decreases the accuracy when the analyzed article is unreliable or the number of hyperlinks regarding the article is not enough. pfibf (Path Frequency -Inversed Backward link Frequency), an association thesaurus construction method which we pro-posed, is a link structure mining method optimized for Wikipedia mining. The relativity between any pair of articles ( v i assumed to be strongly affected by the following two factors:
The relativity is strong if there are many paths (sharing of many intermediate articles) between two articles. In ad-dition, the relativity is affected by the path length. Specifi-cally, if the articles are placed closely together in the concept graph and sharing hyperlinks to articles, the relativity is es-timated to be higher than that of further ones.

The number of backward links of articles is also estimated as a factor of relativity because general / popular articles have a lot of backward links and these articles easily have high relativity to many articles. Therefore, we must consider the inversed backward link frequency ibf in addition to the two factors above.
 Therefore, if all paths from v i to v j are given as follows: The pfibf between them can be expressed as follows: where d () denotes a function that increases in value accord-ing to the length of path t k . A monotonically increasing function such as a logarithmic function can be used for d (). N denotes the total number of articles and bf ( v j ) denotes the number of backward links of v j . This means that a page that shares hyperlinks with a specific page but not with other pages, has a high pfibf . pfibf achieved reasonable performance to get accurate results but it is computationally expensive because analysis of the total n -hop hyperlink structure is required to compute the relatedness between two words. WikiRelate[22] is one of the pioneers in this research area. The algorithm finds the shortest path between categories which the concepts belong to in a category tree. As a mea-surement method for two given concepts, it works well. How-ever, it is impossible to extract all related terms for all con-cepts because we have to search all combinations of category pairs of all concept pairs (1.3 million  X  1.3 million).
As mentioned in section 2.3, current methods face two problems; accuracy and scalability. To solve these problems, we propose an efficient association thesaurus construction method based on link co-occurrence analysis. The advan-tage of link co-occurrence analysis is that not local informa-tion such as tfidf but global information is used, i.e. infor-mation in all Wikipedia articles, hence, the analysis result is not subject to the quality of particular articles. More specif-ically, tfidf uses the information in only the particular arti-cle for representing the characteristics of this article whereas in co-occurrence analysis, the links in the whole Wikipedia are analyzed to detect the characteristics of a particular ar-ticle. Furthermore, since the computational complexity of the co-occurrence analysis is linear against an amount of data, no large number of computational resources like pfibf is needed.

Now, we define the meaning of links more explicitly. Links having the same reference URL are defined as the same links even if the anchor texts are different and the links are placed in different articles. For example, let us assume that there are two hyperlinks having the anchor texts  X  X S X  and  X  X i-crosoft X  respectively but referring to the same article. In this case, we assume that these two links are same links (links to the article  X  X icrosoft X )
In the following, we describe our approach in detail.
In short, our proposed method is a relatedness measure-ment method that computes the relatedness among links (i.e. concepts / articles) based on link co-occurrence analy-sis. Since basically each Wikipedia article has its own URL and corresponds to one concept, computing the relatedness between two links is equal to computing the relatedness be-tween two concepts (words). Two links co-occur if they ap-pear in an article within some distance of each other. Typi-cally, the distance is a window of k links. The window limits the co-occurrence analysis are a to an arbitrary range. The reason why we set a window is because the number of co-occurring combinations becomes too huge when the number of links in an article is large. Qualitatively, the fact that two links often occur close to each other is more likely to be significant than the fact that they occur in the same article, especially if the number of links in an article is large.
Figure 3 shows the overview of the co-occurrence frequency counting. This example shows the case of the window size 3. The capital letters in the figure express hyperlinks. The concrete process flow is as follows: 1. A list of links is extracted from an article, in order of 2. The window moves through the list one by one. 3. The co-occurrence frequency is counted within each This process is performed in all Wikipedia articles, and the co-occurrence frequency of each link pair is calculated by adding up all co-occurrences.

In the research area of term co-occurrence analysis, there are traditionally four methods to compute co-occurrence be-tween two terms from the co-occurrence frequency: where f x and f y are the numbers of windows in which x and y occur respectively. f xy is the number of times terms x and y co-occur in a window. In our proposed method, we apply all of these methods.

The co-occurrence between two links calculated by these methods is called first-order (direct) co-occurrence, i.e. links can only be similar if they co-occur with each other directly. However, Sch  X  utze el at. pointed out the accuracy problem of first-order co-occurrence and they proposed the analysis of second-order co-occurrence[21], i.e. links sharing the same neighbors. It is true that there are sometimes associated (related) link pairs which do not co-occur directly due to insufficient corpus size. Second-order co-occurrence can dis-cover the relatedness if a link pair has a similar co-occurrence pattern even if they do not co-occur directly.

In our preliminary experiment, using a few hundreds of link pairs, for fifty percent of all link pairs the relatedness could not be calculated by first-order co-occurrence due to the limited corpus size. This indicates that fifty percent of these link pairs never co-occur directly. In contrast, second-order co-occurrence could compute the relatedness of all link pairs. For instance, we instinctively know that the two words,  X  X PEC X  and  X  X il X , have high relatedness. How-ever, their relatedness cannot be measured by first-order co-occurrence, whereas second-order co-occurrence showed that these two words have comparatively high relatedness. For this reason, we decided that second-order co-occurrence is more suitable for computing the relatedness of a link pair than first-order co-occurrence.

In order to compute the relatedness based on second-order co-occurrence, we create a vector of each link, named link vector. A link vector represents the pattern of first-order co-occurrences. v i , which is the link vector of link i , is defined by the following formula: where l ij is a first-order co-occurrence between link i and j .
After creating link vectors, we can compute the related-ness of a link pair using the cosine metric defined by the following formula:
The link vectors include co-occurrence information as sta-tistical information extracted from the whole Wikipedia anal-ysis result, but not the information of a particular article. Global statistical information such as co-occurrence is less subject to the quality of a particular article, whereas it is possible that important links are lost as keywords in the ar-ticle describing the particular concept. In contrast, tfidf is characterized by using local information such as information in each particular article.

Therefore, we propose an integration method of link co-occurrence analysis and tfidf . To integrate these two meth-ods, we combine a vector representing concepts based on tfidf with a link vector for considering description (links) in each article. Since both vectors based on link co-occurrence analysis or tfidf are represented by high dimensional spaces by all links, these vectors are added to each other. In ad-dition, these vectors are normalized before combining. The combined vector cv is defined as follows: where lv is the link vector and tv is the tfidf vector, and  X  is a weighting parameter between 0 and 1. In this work, we set  X  to 0.5.
To evaluate the advantages of our approach, we have con-ducted two experiments. In this section, we describe these experiments and discuss the results.
First of all, we constructed four thesauri from Wikipedia using the four methods mentioned in this paper; tfidf , pfibf (2-hop), link co-occurrence analysis (proposed method) and link co-occurrence analysis + tfidf (proposed method), in order to evaluate the performance.

We used the English edition of Wikipedia as of September 2006. Upon removing small, overly specific concepts (those having fewer than 5 incoming or outgoing links) and special articles (top page, category pages, etc.), about 820 thousand articles and 40 million links were left.

In the next step, we conducted experiments to evaluate the constructed thesauri. In the first experiment, we evalu-ated the total analysis time to construct each thesaurus. In the second experiment, the accuracy was evaluated by a test collection. This test collection is described in the following.
In our experiment, we use the  X  X ordSimilarity-353 test collection X  X 7] for evaluating the accuracy of the association thesauri. The test collection has often been used in previous relatedness measurement researches[22, 8]. It contains 353 word pairs and these pairs have been judged by 13-16 testers to produce a single relatedness score. For each method, we calculated the relatedness for all word pairs in the test col-lection. Then, we compared the extracted relatedness with the human judgments using the Spearman rank-order cor-relation coefficient.

The Spearman rank-order correlation coefficient has a high value, if the order of two value sets is similar to each other. It has the value 1 as the max correlation when the order is completely the same, whereas it has the value  X  1asthe negative correlation when the orders are reverse.
The  X  X ordSimilarity-353 Test Collection X  contains word pairs and it is not considered about ambiguous term prob-lem. Since a Wikipedia article has its own URL for word dis-ambiguation and the concepts in thesauri constructed from Wikipedia have already been disambiguated, the words in the test collection must be mapped to concepts in Wikipedia. In this work, we made mappings between terms in the test collection and concepts in thesauri constructed from Wikipedia by the following process. 1. For each word w in the test collection, links having that 2. However, in step 1, there is a possibility that a word is 3. Finally, we exclude word pairs with only 500 or less
As a result of this process, 100 term pairs were left as the test collection.
In this section, we present the experimental results (anal-ysis time and accuracy) and discuss them. The experimental environment is shown in Table 1.
Figure 4 shows the total analysis time for each method and window size. In link co-occurrence analysis (LCA), one of the proposed methods, the total analysis time increased along with the expansion of the window size from 2 and 5. However, the increment was linear.

In comparison with tfidf , the link co-occurrence analysis took approximately 0.8 times as much time for the window size 2 and approximately 2.5 times as much time for window size 5. The analysis time of pfibf was dramatically longer than the link co-occurrence analysis. Concretely speaking, pfibf took approximately 390 times as much time as the link co-occurrence analysis for the window size 2 and for the win-dow size 5, pfibf still took approximately 120 times as much Table 2: Thesaurus accuracy: Proposed methods time. This indicates clearly that the link co-occurrence anal-ysis has an advantage in terms of the analysis time compared with pfibf . The reason is the characteristic of pfibf ,which recursively analyzes the link structure of all links in n -hop range. Even if we use the approximate strategies described in [16] for pfibf , we will need a large amount of compu-tational power. By contrast, since the link co-occurrence analysis does not analyze links recursively, it can keep down the computational cost.

Besides, the analysis time of the integration method of link co-occurrence analysis and tfidf was nearly equal to the sum of the analysis time of link co-occurrence analysis and tfidf .

Next, we describe the link vectors generated by the ex-periment. The number of vectors and their dimension is conceptually equivalent to 816,463, which is the number of Wikipedia articles used in the experiment. However, since these vectors are sparse vectors that contain many zero ele-ments, zero length vectors and zero elements of the vectors were removed on a computer, and so the analysis on a real-istic computer resource was made possible.

As for the quantity of data after the compression, the number of link vectors without the zero length vectors was 782,417 in the case of window size 2 and the number of di-mensions of the vectors without zero elements was 62,830 at the maximum and approximately 33 on average. Likewise, in the case of window size 5, the number of link vectors was 782,417 and the number of dimensions of the vectors was 162,431 at the maximum and approximately 114 on aver-age.
Table 2 shows the accuracy of thesauri constructed based on the two proposed methods, the link co-occurrence anal-ysis method and the integration method with tfidf .The table includes the results of four methods for calculating the first-order co-occurrence described in section 3.1 on window sizes from 2 to 5. Table 3 shows the accuracy of thesauri constructed based on other methods for comparison, namely tfidf and pfibf .

First, according to the result s of link co-occurrence anal-ysis (Table 2), the window size influenced the accuracy of a method. We realized that a small window size led to higher accuracy than a large window size, thus the window size 2 achieved the best score. This indicates that analyzing the co-occurring links side by side is best to achieve high ac-curacy while analyzing on the co-occurrence of far link pair leads to a decrease in accuracy. Moreover, in the comparison of the difference of the co-occurrence calculation methods, Cosine led to the highest accuracy in all window sizes, and the difference of the accuracy in other methods was limited. Accordingly, the combination of the window size 2 and the Cosine method brought the most accurate results.

Furthermore, in comparison with tfidf in Table 3, the link co-occurrence analysis per formed more accurately than tfidf for all window sizes and calculation methods. The rea-son is that tfidf based methods extract the feature vector that represents the characteristics of an article (a concept) by using only links in that article, whereas link co-occurrence analysis uses co-occurring link pairs in all Wikipedia arti-cles for generating the feature vector. Since each article is edited by a limited number of users, it is not homogeneous on the number of links and the reliability. Therefore, the feature vector from links of each article is not necessarily common sense, but possibly contains biased information. As opposed to that, co-occurrence analysis methods use statisti-cal information of the whole collection of Wikipedia articles which means that it is not easily affected by the quality of a particular article. For this reason, we believe that link co-occurrence analysis can produce more accurate association relations than that of tfidf .

Likewise, in the comparison with pfibf in Table 3, link co-occurrence analysis achieved a lower accuracy than pfibf . However, 0 . 65, the maximum accuracy achieved by link co-occurrence analysis, is very close to 0.68, the accuracy of pfibf . Moreover, considering that pfibf took 390 times as much time as link co-occurrence analysis, as mentioned in the previous section, link co-occurrence analysis brought about almost as high accuracy as pfibf at a much lower analysis time.

Furthermore, the integration method of link co-occurrence analysis with tfidf (Table 2) was more accurate than only link co-occurrence analysis in most cases. The integration method showed significant improvement especially in Dice coefficient. Compared with tfidf , the integration method was more accurate, moreover, it achieved an at least equal accuracy to pfibf . It is quite likely that this is due to the dif-ference of characteristics between co-occurrence and tfidf . As we mentioned before, link co-occurrence analysis deals with global information, i.e. statistical information in the whole collection of Wikipedia articles. It conducts related-ness from average uses of terms regardless of the quality of particular articles. In contrast, tfidf conducts relatedness from comparing features of an article based on the descrip-tion of the article. Although the accuracy of tfidf is influ-enced by the quality of a particular article, information of Accuracy Figure 5: Transition of accuracy change (five times moving average) tfidf is important as information representing the feature of the article directly. It is assumed that this information was used for interpolating feature information based on link co-occurrence analysis in cases where a concept was completely represented by statistical information of link co-occurrence analysis. Even if a feature vector based on tfidf includes noisy information which is unsuitable for representing the particular article, combining it with a feature vector based on link co-occurrence analysis, noisy information can be re-duced in many cases because there are very few possibilities that each dimension of noisy information of tfidf and that of link co-occurrence analysis correspond in a huge dimen-sional vector. On the other hand, it is highly probable that a dimension of important information corresponds in both methods; consequently, it is estimated that its dimension was emphasized. Apart from that, feature information ex-tracted by tfidf is profitable for interpolating a feature if enough statistical information is not obtained by link co-occurrence analysis due to too few backward links.
In this section, we discuss whether the experimental re-sults about the accuracy are valid as an evaluation of the whole thesaurus. Since our experiment about the accuracy was conducted by using the test collection, only a part of concept pairs in a thesaurus was evaluated. Though it is re-ally impossible to evaluate all concept pairs of a thesaurus, we need to verify whether the evaluation using the test col-lection is valid for the whole thesaurus.

For looking into the validity of the evaluation, we con-ducted the following processes 50 times: 1. We randomly extract 80 sets (word pairs) from 100 2. The accuracy of the thesaurus is computed with the Figure 5 shows the result with five times moving average in order of the experiment. The reason why the moving average is used is that the purpose is not to analyze each single value but to check whether the evaluation corresponds to that of the whole thesaurus by showing that the tendency of the evaluation result in any subset of the test collection is similar to the experiment in this work. First of all, the link co-occurrence analysis was more accurate than tfidf at any time. This is strong evidence to support the validity of the result of the experiment.

Likewise, in comparison with link co-occurrence analysis only, the integration with tfidf was more accurate at any time. This result was also tantamount to the result of the experiment. Finally, compared with pfibf , there were two cases where the integration method was more accurate or less accurate than pfibf . This indicates that although these two methods produce similar accuracy, an error of the accu-racy occurs by a concept.

The above results show that the experimental accuracy results are valid for not only a part of the thesaurus but for the whole thesaurus.
Table 4-6 shows examples of an association thesaurus con-structed by link co-occurrence analysis. We present exam-ples in three categories,  X  X ompany X (Table 4),  X  X roduct X (Table 5) and  X  X ther categories X (Table 6). The top 15 associated terms of each concept are listed in order of their relatedness. The examples show that link co-occurrence analysis can ex-tract associated terms which relate to the terms intuitively.
In this paper, we proposed a scalable approach called link co-occurrence analysis to construct an association the-saurus from Wikipedia. Our experiment confirmed that both proposed methods have significant capability to con-struct a more accurate association thesaurus than tfidf and that they are more scalable than pfibf in spite of their high accuracy close to that of pfibf . In particular, as a method for calculating co-occurrence, Cosine achieved high-est accuracy. Furthermore, c ompared with normal link co-occurrence analysis, the integration method with tfidf showed remarkable improvements. Wikipedia has huge coverage of concepts, and yet, Wikipedia continues to increase in num-ber of links and concepts. Therefore, we expect that our constructed association thesaurus will have an even larger coverage and be more accurate in the future.

In future work, to further improve the accuracy of our as-sociation thesaurus, we are planning to develop a method considering specific structures of Wikipedia articles such as sections, paragraphs or lists, and improve completeness and information amount by expanding the concept of co-occurrence, e.g. using co-occurrences of backward links or forward links. For the integration method, though we deterministically set the weighting parameter  X  in this work , we plan to deter-mine a more suitable value by using other strategies such as machine learning. By above ideas and more improvements, we also would like to improve the accuracy of a concept of few numbers of backward links. Additionally, to make our experiment more distinct, we are planning to conduct an evaluation for each of general terms or domain specific terms and compare the performance of our thesaurus based query expansion system to a state-of-the-art query expan-sion system.

Furthermore, the association thesaurus construction is just a first step in our whole project. Our next goal is another project called  X  X ikipedia Ontology, X  a huge scale Web-based ontology which is extracted by Wikipedia Mining. The pur-pose of this project is to extract not only term associations but also term relations such as  X  X s-a X  or  X  X art-of. X  Our asso-ciation thesaurus can be utilized to improve the accuracy of relation extraction, since the association thesaurus has the capability to detect important (strongly related) sentences for particular concept. Information on our other projects and the constructed association thesauri are available on our Web site.
The application of our proposed method is not limited to Wikipedia, but also for other Wiki-based systems. It is gratifying that Wiki-based knowledge management in en-terprise environments, so-called Enterprise Wiki, have be-come popular in recent days. From this reason, extracting organization-specific association thesaurus by applying our method is a remarkable research direction and has potential to support enterprise activities. This research was supported in part by Grant-in-Aid for Scientific Research (C)(20500093) and for Scientific Research on Priority Areas (18049050), and by the Microsoft Research IJARC Core Project. [1] E. Brill. A simple rule-based part of speech tagger. In [2] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [3] H. Chen, T. Yim, D. Fye, and B. R. Schatz.
 [4] Z. Chen, S. Liu, L. Wenyin, G. Pu, and W.-Y. Ma. [5] C. J. Crouch. A cluster-based approach to thesaurus [6] B. D. Davison. Topical locality in the web. In [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, [8] E. Gabrilovich and S. Markovitch. Computing [9] J. Giles. Internet encyclopaedias go head to head. [10] D. Klein and C. D. Manning. Accurate unlexicalized [11] J. M. Kleinberg. Authoritative sources in a [12] P. Lawrence, B. Sergey, M. Rajeev, and W. Terry. The [13] G. A. Miller. Wordnet: A lexical database for english. [14] D. Milne, O. Medelyan, and I. H. Witten. Mining [15] K. Nakayama, T. Hara, and S. Nishio. A thesaurus [16] K. Nakayama, T. Hara, and S. Nishio. Wikipedia [17] OpenNLP forum. Opennlp, 2006. [18] M. Ruiz-Casado, E. Alfonseca, and P. Castells. [19] G. Salton and M. McGill. Introduction to Modern [20] G. Salton, A. Wong, and C. S. Yang. A vector space [21] H. Sch  X  utze and J. O. Pedersen. A cooccurrence-based [22] M. Strube and S. P. Ponzetto. Wikirelate! computing [23] Y. H. Tseng. Automatic thesaurus generation for
