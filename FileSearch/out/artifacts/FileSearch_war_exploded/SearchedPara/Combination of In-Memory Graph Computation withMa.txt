 In many applications such as web data manag ement and social network, we always meet many massive graphs. But scalability is a problem for traditional in-memory graph al-gorithms. A promising platform for massive graph algorithm is Hadoop [1] which pro-vides MAPREDUCE [2] computation model for distributed computing. In this paper we take PageRank [3] as an example to explore how to combine in-memory graph al-gorithm with MapReduce. To get a high scalability system many researchers focus on efficiently parallelizing PageRank on Map Reduce. But there are many challenges to implement the pageRank algorithm on MapReduce, such as data partition, reduce the communication cost. Moreover, as a typical graph processing algorithm, techniques de-veloped to improve the efficiency of the P ageRank algorithm can be easily extended to resolve many other graph-based algorithms.

Previous implementations of the PageRank algorithm on MapReduce ignore the char-acteristic of locality in distributed systems which is very important to reduce the I/O and network costs. Broadly speaking, in a distributed network without shared mem-ory, processes cooperate by exchanging me ssages. Since sending messages to far away nodes is expensive, computation should be based on local information as much as pos-sible. Both HaLoop [4] and PEGASUS [5] provide implementations of PageRank on MapReduce. But the computation process are implemented by two rounds of MapRe-duce jobs for one PageRank iteration which cause extra I/O costs. Previous approaches are edge-centric and produce a large amount of internal results which lead to high Shuf-fle Bytes for Hadoop job. In this paper, we explore the locality property and propose a new subgraph-centric method for fast PageRank computation only using one MapRe-duce job for one PageRank iteration by the adoption of map cache and graph partition-ing strategies. We especially consider decr easing the communication cost in the cluster and improving the local throughput of the PageRank algorithm for distributed systems. The contributions of this paper are as follows: 1. We propose a method to adapt MapReduce for graph applications. We treat a sub-2. We design Locality Iterative-PageRank algorithm, LI-PageRank for short, which is PageRank algorithm. A performance analysis of our methods is presented in Section 3. We discuss related work in Section 4 and conclude the study in section 5. First we look how PageRank was implemented on Hadoop and HaLoop [4] previously. As Figure 1(a) shows, L is the link table which is invariant across iterations with the format of source, destination . R i is the rank table at iteration i with the format of vertex,rank .Take R 0 in Figure 1(b) for example. There are two MapReduce jobs in the loop body of naive PageRank implementation on Hadoop. The first job is to join rank table R i and linkage table L and populate ranks; the second one is to aggregate ranks on each destination vertex. After one iteration, R i is updated to R i +1 . 2.1 Locality of PageRank Computation on MapReduce Here we extend the locality concept of PageRank computation on MapReduce by con-sidering the computation is executed mostly in the main memory by map function to reduce the disc or network costs by pro cessing messages, which are the output of map functions and input of reduce functions. 2.2 LI-PageRank Algorithm The main idea of LI-PageRank algorithm is to reduce the communication among the Hadoop cluster. We have two advantages over previous methods. First, we need only one MapReduce job for an iteration of PageRank computation by using map cache to load previous PageRank values from HDFS; Secondly, we combine the in-memory PageRank computation with MapReduce by using subgraph as processing unit.

The PageRank value of vertex u at iteration i , denoted as R i ( u ) , is dependent on vertexes pointing into u at partition j . Different from previous PageRank computation methods on MapReduce, we compute locally for a subgraph in a map function and sum up the intermediate values f or each iteration. As in Equation 1, we divide the graph into k subgraphs and compute the latter part of Equation 1 by k map functions. Previous PageRank implementation on MapReduce processes an edge in a map function while our method processes a subgraph in a map function. Our method changes the granularity for map function to reduce Shuffle Bytes . Our method does not change the PageRank algorithm itself so the results are the same as the original one.
 For each partition, map function produces PageRank value for vertex v .The mapper (Here mapper means the map task) outputs all the partial results as &lt;vertex,rank&gt; pairs to reducer to do the sum operation. We utilize mapper cache to cache the PageR-ank values of previous iteration by the setup API provided by Hadoop . For a subgraph, the map function outputs PageRank values contributed by the vertexes in the subgraph for all the vertexes, which are linked by other vertexes in this subgraph. It is much smaller comparing to previous method which outputs PageRank values as many as the number of the edges in the subgraph. In the reduce function, we sum up the partial results together to get the final PageRank for every vertex. There are four key steps for our method: (1) graph pa rtitioning, (2) mapping phase as well as cache update, (3) reducing phase as well as message grouping, and (4) convergence checking. 2.3 Graph Partitioning Our partitioning strategy is based on adjacent list of graphs. According to Equation 1 , the less common destination vertexes the subgraphs have, the less aggregation work need be done. We call a vertex as destination vertex if there is an edge pointing to it. We use the combination of horizontal partitioning and vertical partitioning to divide a graph into subgraphs. Horizontal partitioni ng aims to decrease the amount of PageRank values contained in mapper cache. Vertical partitioning aims to decrease the amount of output records of reducer ( reduce task) output. Combination of Vertical and horizontal aim to keep the subgraphs with a pproximate-equal amount of edges. For the graph which has more vertexes than the threshold, horizontal partitioning is needed to divide the graph into groups. Each group contains vertexes with all their neighbors. 3.1 Dataset and Experiments Environment We use two real-world social network datasets in this paper. The first dataset is the well-known LiveJournal dataset (4847571 vertexes, 68993773 edges). The second is Facebook dataset(957359 vertexes, 161933115 edges). These two datasets have been converted into digital format. We use a cluster with 8 working data nodes. Each node has 48TB of storage and 40GB RAM. This cluster is installed with Hadoop 0.20.2 and HaLoop. 3.2 Evaluation We run LI-PageRank, naive PageRank on Hadoop and PageRank on HaLoop using Live-Journal dataset and Facebook dataset respectively. Both runtime and Shuffled Bytes are compared, and results are shown in Figure 3 -6. As the figures show, for a 10-iteration job, LI-PageRank reduces the runtime t o 39% compared with HaLoop and 31% com-pared with naive PageRank on Hadoop for facebook dataset. For LiveJournal dataset LI-PageRank reduces the runtime to 42% compared with HaLoop and 24% compared with naive PageRank on Hadoop. It can be seen that with the increase of the amount of edges, LI-PageRank is more efficient compared to other methods. Another measure is Shuffled Bytes. Shuffled Bytes means the amount of data shuffled from map phase to reduce phase. It is sorted by Hadoop according to the key comparator of the output record. Because the sort operation is quite time-consuming, we can improve the per-formance if we can reduce the Shuffled Bytes. HaLoop reduces the shuffled bytes of invariant table L compared to naive PageRank on Hadoop. Compared with PageRank on HaLoop, LI-PageRank reduces the shuffled bytes of both invariant table L and vari-ant table R for join step. For the aggregate step, HaLoop can X  X  reduce shuffled bytes because there isn X  X  invariant table. LI-Pag eRank can reduce shuffled bytes by reducing the intermediate results. Link Analysis has been a popular and broadly used Web mining technique. Various link analysis shemes have been proposed. The most popular technique is Google X  X  PageRank [3]. PageRank is a famous algorithm and many new ranking schemes have emerged which are on basis of it. In recent years, researchers focus on parallel PageR-ank approaches. MapReduce computing paradigm has been a standard of distributed massive data processing platform. [6] gives a method to compute Personalized PageR-ank on MapReduce. [7] proposes a new approach for aggregated PageRank compu-tation via distributed randomized algorithms. Hadoop as an open-source version of Map/Reduce [2] implementation gains its popul arity by high efficiency, scalability and fault tolerance. HaLoop [4] is an extension of Hadoop, which can support iterative data processing. Twister [8] is a stream-based Map/Reduce framework which can sup-port multi-iteration applications too. Another famous system based on Map/Reduce is HOP [9], which is a modified version of Hadoop, and allows data to be pipelined between tasks and between jobs. Google proposes Pregel [10] for large-scale graph pro-cessing. Like Hadoop, distribution-related details are hidden behind an abstract API. Pregel provides a vertex-centric appro ach for large-scale graph applications. This paper is concerned with improving the performance of the PageRank calcula-tion using MapReduce (based on both Hadoop and HaLoop). The optimization of our method focuses on decreasing communication costs in the cluster by exploiting the locality of the PageRank algorithm for dist ributed system. We propose LI-PageRank algorithm to adapt hadoop to PageRank computing from a locality point of view. We will study community-based partitioning for LI-PageRank deeply in the future.
