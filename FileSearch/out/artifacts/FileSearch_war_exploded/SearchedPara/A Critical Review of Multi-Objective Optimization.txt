 This paper addresses the problem of how to evaluate the quality of a model built from the data in a multi-objective optimization scenario, where two or more quality criteria must b e simultaneously optimized. A typical example is a s cenario where one wants to maximize both the accuracy and the sim plicity of a classification model or a candidate attribute subse t in attribute selection. One reviews three very different approac hes to cope with this problem, namely: (a) transforming the ori ginal multi-objective problem into a single-objective problem b y using a weighted formula; (b) the lexicographical approach, where the objectives are ranked in order of priority; and (c) the Pareto approach, which consists of finding as many non-dom inated solutions as possible and returning the set of non-dominated solutions to the user. One also presents a critical review of the case for and against each of these approaches. The general conclusions are that the weighted formula approach  X  which is by far the most used in the data mining literature  X  i s to a large extent an ad-hoc approach for multi-objective optimization , whereas the lexicographic and the Pareto approach are more prin cipled approaches, and therefore deserve more attention fr om the data mining community. Multi-objective optimization, lexicographic approac h, Pareto dominance, Classification. A crucial issue in data mining is how to evaluate t he quality of a candidate model  X  e.g., a classification model such as a rule set or a decision tree. This paper addresses an important aspect of this issue, which is how to evaluate a model X  X  quality b y taking into account multiple quality criteria (objectives) to b e optimized. In this case the quality of a model can be represented by a n-dimensional vector, where n is the number of qualit y criteria to be optimized, rather than by a single scalar number. M ulti-objective problems are very common in a number of different d ata mining tasks and problems. As typical examples, let us men tion two very generic scenarios, which are broad enough to refer to a large number of data mining projects. The first scenario involves predictive tasks. Examp les of predictive data mining tasks are classification (wh ere the attribute to be predicted is categorical), regression (where the attribute to be predicted is continuous) and dependence modellin g (where there are several categorical attributes to be pred icted). Here we focus on the classification task, which is in gener al the most studied in the data mining literature, but the argu ments discussed here also hold for other predictive tasks. The knowledge discovered by a data mining algorithm should be not only accurate but also comprehensible to the us er [Fayyad et al. 1996]. Hence, there are a number of classificat ion projects where the goal is to maximize both the predictive a ccuracy of the classification model and the comprehensibility (sim plicity) of the classification model, in order to obtain a model ea sier to be interpreted by the user. This is the case particula rly in the context of decision tree and rule induction algorithms, whi ch lend themselves naturally to the discovery of knowledge in a high-level representation, which can in principle be int erpreted by the user  X  as long as the  X  X omplexity X  of the model, ty pically measured by the size of the model, is relatively sm all. This raises the question of how to evaluate the trade-off betwe en the accuracy of a model and its size. Indeed, a common approach in the literature consist s of reporting the values of both the predictive accuracy and the simplicity (size) of different models produced in a set of experiment s [Weiss et al. 2003], [Chisholm &amp; Tadepalli 2002], [Weiss &amp; Indurk hya 2000], [Li et al. 2002]. When reporting these kinds of res ults, accuracy and simplicity are usually analyzed separately from each other, and it is often the case that  X  X odel A is more accu rate than model B, but model B is simpler than model A X . For instan ce, which one is a  X  X etter X  classification model: an easily-inter pretable decision tree with a dozen nodes and accuracy rate (on the t est set) of 92% or a large, non-interpretable decision tree with hu ndreds of nodes and accuracy rate of 95%? The answer depends on the problem at hand and the preference of the user. The second scenario involves attribute selection. T his is one of the most studied data preprocessing tasks of the kn owledge discovery process, where the goal is to select, out of all original attributes, a subset of attributes that are relevan t for the target data mining task [Guyon &amp; Elisseeff 2003]. Again, for th e sake of simplicity we focus on attribute selection for the classification task, which is the kind of attribute selection most investigated in the data mining literature, but the arguments discu ssed here also hold in general for other kinds of data mining task s, particularly predictive tasks. In general it is accepted that, in the context of d ata mining, an attribute selection method should select an attribu te subset that not only maximizes the accuracy of the classificati on model, but also minimizes the number of selected attributes (t o save memory space, speed up the classification algorithm, etc.) and/or the size of the classification model built from the selected attributes (to improve the simplicity/interpretability of the mode l). Again, a common approach in the literature consists of reporting not only the predictive accuracy of the model built from the selected attributes, but also the number of selecte d attributes and/or the size of the classification model built f rom the selected attributes [Yu &amp; Liu 2003], [Kim et al. 2000], [Koh avi &amp; John 1998], [Cherkauer &amp; Shavlik 1996]. It is often the case that  X  X ttribute subset A leads to a more accurate model than attribute subset B, but B has a smaller cardinality (number o f attributes) than A X . This introduces the difficult problem of a nalyzing the trade-off between maximizing a model X  X  accuracy and minimizing the number of selected attributes, which is analogous to the above mentioned trade-off between a model X  X  accuracy and its size. We emphasize that the previous two generic scenario s are just a sample of the many kinds of scenario where a data m ining problem involves the simultaneous optimization of t wo or more criteria (objectives). One could easily add to the list scenarios with several other criteria to be optimized. For in stance, one could  X  X ecompose X  the accuracy criterion into the precisi on and recall criteria, as it is usual in the information retriev al literature [Baeza-Yates &amp; Ribeiro-Neto 1999]. One could measure rule surprisingness (unexpectedness) separately from acc uracy and simplicity [Liu et al. 1997], [Freitas 1998]. (The motivation for this is clear by considering the following hypothet ical rule: IF (patient is pregnant) THEN (gender is female). This rule is very accurate and very simple, but it gets a very low ma rk for surprisingness.) One could consider the two objecti ves of maximizing accuracy and minimizing the cost of the attributes used by the classifier [Turney 1995]. A comprehensi ve discussion about all kinds of multi-objective data mining scen arios would take a far larger space than the available space, b ut the above list is, hopefully, enough to show that multi-objective optimization problems are quite common in data mining. The remainder of this paper is organized as follows . Section 2 discusses three general approaches to cope with mul ti-objective problems. Section 3 discusses arguments for and aga inst each of those three approaches. Section 4 discusses the rel ationship between multi-objective optimization and ROC graphs . Finally, section 5 concludes the paper. In the data mining literature, by far the most used approach to cope with a multi-objective problem consists of tra nsforming it into a single-objective problem. This is typically done by assigning a numerical weight to each objective (eva luation criterion) and then combining the values of the wei ghted criteria into a single value by either adding or multiplying all the weighted criteria. That is, the quality Q of a give n candidate model is typically given by one of the two kinds of formula: Q = c 1 W1  X  c 2 W2  X  . . .  X  c n Wn (2) where w i , i=1,...n, denotes the weight assigned to criteria c is the number of evaluation criteria. Let us mentio n some examples of this approach in the context of the two multi-objective scenarios discussed in the Introduction. The first scenario involves rule induction algorith ms for classification, where it is common to evaluate the quality of a candidate rule by measuring two or more criteria. A n example is: an instance of the general formula structure (1) co mbining completeness and consistency into a single measure of rule quality. This formula and its variations are used i n several rule induction algorithms [Bruha &amp; Tkadlec 2003], [Furnk ranz &amp; Flach 2003]. Another example is: an instance of the general formula structure (2) us ed in [Kaufmann &amp; Michalski 1999] to produce one of the e valuation criteria used in a lexicographic approach for rule evaluation (to be discussed in section 3.2). The second scenario involves attribute selection a lgorithms for classification, where it is also common to evaluate the quality of a candidate attribute subset by measuring two or more criteria. An example is: This formula was used by [Cherkauer &amp; Shavlik 1996] to measure the quality of a candidate attribute subset in an a ttribute selection method following the wrapper approach, where Acc (a ccuracy) was measured by the validation-set accuracy of a de cision tree built with the selected attributes, S was the decis ion tree size and F was the number of attributes (features) in the ca ndidate attribute subset. Other examples of attribute selection metho ds following the wrapper approach and combining two or more attr ibute subset quality criteria in a weighted formula can be found in [Liu &amp; Motoda 1998]. The basic idea of this approach is to assign differ ent priorities to different objectives, and then focus on optimizing the objectives in their order of priority. Hence, when two or more candidate models are compared with each other to choose the b est one, the first thing to do is to compare their performance m easure for the highest-priority objective. If one candidate model is significantly better than the other with respect to that objectiv e, the former is chosen. Otherwise the performance measure of the tw o candidate models is compared with respect to the second objec tive. Again, if one candidate model is significantly better than the other with respect to that objective, the former is chosen, ot herwise the performance measure of the two candidate models is compared with respect to the third criterion. The process is repeated until one finds a clear winner or until one has used all the criteria. In the latter case, if there was no clear winner, one can simply select the model optimizing the highest-priority objective . A well-known algorithm using this approach is the A Q18 rule induction algorithm [Kaufmann &amp; Michalski 1999] and its variants. AQ18 uses a  X  X exicographic evaluation fun ctional (LEF) X  defined by a sequence of pairs &lt;(c 1 ,t 1 ), (c where each c i , i=1,...,n, represents the value of a performance criterion for a given candidate model, and each t represents the tolerance associated with c i . This tolerance is specified by a threshold indicating the maximum val ue that the performance criterion c i for a given candidate model is allowed to deviate from the value of c i for the best current candidate model. More precisely, let M 1 and M 2 be two candidate models being compared, and assume, without loss of generality, t hat M better value of c i than M 2 . If the difference between M and M 2  X  X  c i value is greater than t i , then M considered better than M 2 , without the need to check lower-priority objectives. Otherwise the difference betwe en M is not considered significant, and in order to sele ct the best candidate model one has to use the remaining lower-priority objectives. In [Kaufmann &amp; Michalski 1999] this app roach is used with a LEF where a predictive accuracy-related measure (a combination of completeness and consistency gain) i s used as the highest-priority criterion (c 1 ), and rule description simplicity as the next criterion (c 2 ); but of course other criteria could be used. The basic idea of the Pareto approach is that, inst ead of transforming a multi-objective problem into a singl e-objective problem and then solving it by using a single-objec tive search method, one should use a multi-objective algorithm to solve the original multi-objective problem. Intuitively, this approach makes sense. One should adapt the algorithm to the proble m being solved, rather than the other way around. In any ca se, this intuition needs to be presented in more formal term s, which is done in the following. Let us start with a definition of Pareto dominance. A solution s said to dominate (in the Pareto sense) a solution s 2 if and only if s is strictly better than s 2 with respect to at least one of the criteria (objectives) being optimized and s 1 is not worse than s respect to all the criteria being optimized. Mathem atically, assuming  X  without loss of generality  X  that all cr iteria c are to be maximized, a solution s 1 dominates a solution s only if  X  c i such that s 1 (c i ) &gt; s 2 (c i ) and  X  c where s 1 (c i ) denotes the quality of solution s criteria c i and k is the number of criteria being optimized. A solution s i is said to be non-dominated if and only if there i s no solution s j that dominates s i . Note that the Pareto approach never mixes different criteria into a single formula  X  al l criteria are treated separately. The concept of Pareto dominance is illustrated in F igure 1, where the two objectives to be maximized are the accuracy and the simplicity of a classification model. In the figure , model A is dominated by model B and by model D; model C is dom inated by model D; and model E is dominated by model F. Model s B, D and F are non-dominated solutions. They form the so-cal led Pareto front. Once Pareto dominance has been defined, the next st ep is to understand the crucial differences between a multi-objective algorithm based on Pareto dominance and a single-ob jective algorithm. There are two related crucial difference s. First, there is a difference in the kind of output expected from ea ch of these two kinds of algorithm. A multi-objective algorithm sho uld return to the user a set of non-dominated solutions, rather than just a single solution as in a single-objective algorithm. Second, and related to the first difference, the se arch performed by a multi-objective algorithm should explore a con siderably wider area of the search space and keep track of al l non-dominated solutions found so far, in order to find as many solutions in the Pareto front as possible. Of cours e, this makes a Pareto-based multi-objective optimization algorithm more complex than its single-objective counterpart. Some Pareto-based multi-objective optimization data mining algorithms are discussed in [Kim et al. 2000], [Bhattacharyya 2000], [Pappa et al. 2002]. This approach, discussed in section 2.1, has the ad vantage of conceptual simplicity and easy of use, which probab ly explains its popularity. However, it has several drawbacks, whic h are discussed in the following. The most obvious problem with weighted formulae suc h as formulas (1) and (2) is that, in general, the setti ng of the weights in these formulas is ad-hoc, based either on a some what vague intuition of the user about the relative importance of different quality criteria or in trial and error experimentat ion with different weight values. In other words, each weight seems a  X  X agical number X , which often is justified in the literature with suitably vague sentences such as  X  X he values of these weight s were empirically determined X . Another problem with weights is that, once a formul a with precise values of weights has been defined and given to a d ata mining algorithm, the data mining algorithm will be effect ively trying to find the best model for that particular setting of weights, missing the opportunity to find other models that might be actually more interesting to the user, representing a better trad e-off between different quality criteria. In particular, weighted formulas involving a linear combination of different quality criteria have the limitation that they cannot find solutions in a non-convex region of the Pareto front. This point will be disc ussed in section 3.3. For now, to see the limitation of linear combinatio ns of different criteria, consider a hypothetical scenario where we have to select the best candidate to the position of data miner in a company, taking into account two criteria c 1 and c 2  X  say, their amount of knowledge about machine learning and statistics, me asured by a test and/or a detailed technical interview. Suppose the first candidate X  X  scores are c 1 = 9.5 and c 2 = 5; the second candidate X  X  scores are c 1 = 7 and c 2 = 7; and the third candidate X  X  scores are c = 5 and c 2 = 9.5. The choice of the  X  X est X  candidate should depend, of course, on the relative importance assig ned to the two criteria by the employer. It is interesting to note , however, that although it is trivial to think of weights for crit eria c would make the first or third candidate the winner, it is actually impossible to choose weights for c 1 and c 2 so that the second candidate would be the winner  X  assuming that the w eighted formula is a linear combination of weights, such as formula (1). Intuitively, however, the second candidate might be the favorite candidate of many employers, since she/he is the on ly one to have a good knowledge about both machine learning and st atistics. Of course, it is possible to make the second candid ate the winner. One has to use a non-linear combination of criteria , e.g. the formula c 1  X  c 2  X  an instance of the generic formula (2), where th e exponent weights are set to 1. However, in this exa mple one would have chosen the formula c 1  X  c 2 a posteriori , i.e., after one has learned that the user would prefer the second c andidate, rather than the other two candidates. If one had asked a s earch algorithm to find the best candidate without this a posterior i knowledge about the candidates, one could easily have provide d the algorithm with a formula involving a linear combina tion of weights (which are more often used in the literatur e than non-linear formulas), and in this case one would miss t he opportunity of finding a candidate such as the second above can didate. In other words, it is hard for the user to define the best setting of weights a priori , without knowing the results of the research. This problem is particularly serious when the weigh ted formula involves a summation/subtraction (rather than a multiplication/division) of terms representing diff erent quality criteria, such as formulas (1) and (5). Different m odel-quality criteria often have very different scales in their units of measurement. For instance, accuracy and classificat ion model-size (e.g. the number of decision tree nodes) are m easured in very different scales. This problem can be dealt with by normalizing the different quality criteria so that they refer t o the same scale. This approach is well-known in the literature and a t first glance it is a very satisfactory approach. There is, however, a subtle problem associated with normalization that is rarel y discussed in the literature. In essence, the problem is that in general there are several different ways of normalizing a quality mea sure, and the decision about which normalization procedure should be applied tends to be ad-hoc. This problem can also be unders tood in the context of inductive biases. An inductive bias is a ny criterion (explicit or implicit), except consistency with the data, used to favour one hypothesis over another [Mitchell 1980], [Mitchell 1997]. A normalization procedure is a source of ind uctive bias, in the sense that it tends to favour one hypothesis ov er others and it is not based on consistency with the data  X  assuming that a predictor attribute is normalized without taking in to account the class values, which is usually the case. It is well -known that an inductive bias has a domain-dependent effectiveness , so that any inductive bias will be suitable for some applicatio n domains and unsuitable for others. To see an example of the subtle problem associated with normalization, consider the previously-mentioned wo rk of [Cherkauer &amp; Shavlik 1996] on attribute selection, where the quality of a candidate attribute subset was evaluat ed by formula (5). In that formula, the value of S (decision tree size) was normalized by dividing the tree size by the number of training examples the decision tree was built from. Consider now an alternative normalization (not used in the paper): dividing the current tree size by the size of the largest tree a mong all the trees generated by the search so far. Both normalization procedures produce a value in the range 0..1, as desired, but the two procedures might produce quite different values of S to be used in formula (5)  X  which should influence the choice of the weight values in formula (5). Which of them is the  X  X est X  normalization? One cannot mathematically prove that one of them is always the best, in the same sense that one cannot mathematica lly prove that an inductive bias is always the best. Hence, ideall y the choice of the  X  X est X  normalization procedure should involve b ackground knowledge about the application domain and/or trial and error experimentation with different normalization proced ures and weight values. This tends to be an ad-hoc approach, rather than a principled approach. This is another subtle problem associated with the weighted-formula approach, which is often ignored in the lit erature. Before discussing this problem in the context of measuring the quality of a model, let us explain the core of the problem by using some simple examples and common-sense arguments. Common sense tells us that non-commensurable criter ia should not be added/subtracted to/from each other in a for mula. For instance, if the salary of a customer is US$ 50,000 and the number of dependents (e.g. children) of the custome r is 5, it does not make sense to add 50,000 + 5. This would produc e a meaningless quantity. Note that the problem here is not only the fact that the two attributes being added have very different scales. This problem can be solved by normalizing both attr ibutes into a range 0..1, say by dividing the salary and the numb er of dependents by the maximum values of these attribute s among the customers in the database. This problem was discuss ed in the previous item. In this item we are interested in an other problem. Even after performing normalization, we still have the problem that salary and number of dependents are non-commen surable criteria. In other words, they measure very differe nt attributes of a customer, and the addition/substraction of these va lues in a weighted formula does not make any sense at all, re gardless of normalization. It would produce a quantity that wou ld be meaningless to the user, which would go against a b asic goal of data mining, namely that discovered knowledge shoul d be ultimately understandable to the user [Fayyad et al . 1996]. At first glance it could be argued that this is a p roblem only if the different criteria are added/subtracted, but not if they are multiplied/divided. Indeed, in some cases the multiplication/division of different criteria produ ces perfectly meaningful attributes. In the previous example, one can divide the salary of a customer by her/his number of dependent s, which intuitively would produce a meaningful indicator if we were trying to classify customers into, say,  X  X ood credi t X  and  X  X ad credit X  customers. However, even when using a formu la involving multiplication/division, the produced quantity may not be meaningful to the user in many cases. For instance, dividing (or multiplying) salary by age does not appear to produ ce a very meaningful indicator to a user. Let us now turn to the problem of mixing non-commen surable criteria in a weighted formula evaluating a candida te model. In particular, let us consider the problem of mixing a ccuracy and comprehensibility (simplicity) measures into the sa me formula, since these are probably the two model-quality crit eria most used in data mining. Clearly, accuracy and comprehensibi lity are two very different, non-commensurable criteria to evalu ate the quality of a model. Actually, comprehensibility is an inher ently subjective, user-dependent criterion. Even if we re place the semantic notion of comprehensibility by a syntactic measure of simplicity such as model size, as it is usually don e when evaluating  X  X omprehensibility X , the resulting measu re of simplicity is still non-commensurable with a measur e of accuracy. The crucial problem is not that these two criteria have different units of measurement (which can be, to some extent,  X  X easonably solved X  by normalization, as discussed earlier), bu t rather that they represent very different aspects of a model X  X  quality. In principle, it does not make sense to add/subtract a ccuracy and simplicity, and the meaningfulness of an indicator multiplying/dividing these two quality criteria is questionable. A more meaningful approach is to recognize that accur acy and simplicity are two very different quality criteria, and treat them separately, without mixing them in the same formula . The lexicographic approach has one important advant age over the weighted-formula approach: the former avoids the pr oblem of mixing non-commensurable criteria in the same formu la. Indeed, the lexicographic approach treats each of the crite ria separately, recognizing that each criterion measures a differen t aspect of quality of a candidate solution. As a result, the l exicographic approach avoids the three drawbacks associated with the weighted-formula approach discussed in section 3.1  X  namely the  X  X agic number X  problem, the problem of mixing diffe rent units of measurement and the problem of mixing  X  X pples an d oranges X . In addition, although the lexicographic approach is somewhat more complex than the weighted-formula approach, th e former can still be considered conceptually simple and eas y to use. In particular, the lexicographic approach is considera bly simpler and easier to use than the Pareto approach. As discussed earlier, the lexicographic approach us ually requires one to specify a tolerance threshold for each crite rion. It is not trivial how to specify these thresholds in a princi pled manner. A commonplace approach is to use a statistics-oriente d procedure, e.g. standard deviation-based thresholds, which all ow us to reject a null hypothesis of insignificant difference betwe en two objective values with a certain degree of confidenc e. Although this approach is statistically sound, it should be recalled that it still requires the specification of one parameter, the degree of confidence. This specification still has a certain degree of arbitrariness, since any  X  X igh enough X  value such a s 95% or 99% could be used. Of course one can always ask the use r to specify the thresholds or any other parameter, but this int roduces some arbitrariness and subjectiveness in the lexicograph ic approach  X  analogous to the usually arbitrary, subjective spec ification of weights for different criteria in the weighted form ula approach. It could be argued that we do not need Pareto-based multi-objective optimization because a data mining algori thm based on the conventional weighted formula approach can be u sed to analyse the trade-offs associated with different cr iteria to be optimized. One simply has to run the algorithm mult iple times, with a different set of weights in each run. Howeve r, this argument does not seem very convincing, for the rea sons discussed in the next item. First, it should be noted that multiple runs of a s ingle-objective optimization algorithm is an ad-hoc approach, since there is no principled, mathematically-sound method to decide w hich weights should be used in each run, nor to decide h ow many runs of the algorithm should be performed. Second, this is an inefficient approach [Deb 2001], [Corne et al. 2003 ], because each run will be effectively ignoring the candidate solutions evaluated by the previous runs of the algorithm, so that later runs can spend a considerable time re-evaluating some so lutions that had already been evaluated by earlier runs. Third, this is an ineffective approach. There is no mechanism to enfo rce the desired property that the solutions discovered by t he different runs should be as spread as possible along the Pareto fr ont. In addition, no matter how many times we run a conventional weig hted-formula algorithm with a linear weighted formula (w hich is the most used kind of weighted formula), it will never find a non-dominated solution in a non-convex region of the Pa reto front, such as the solutions indicated by the black circle in Figure 2. The figure assumes  X  without loss of generality  X  that both objectives are to be minimized. (See also Section 3.1, first  X  Argument against X  heading.) It could be argued that, in problems where the obje ctives to be optimized are a model X  X  accuracy and size, we do no t need Pareto-based multi-objective optimization, because we already have the Minimum Description Length Principle. This principle is often used to favour the discovery of knowledge tha t is both accurate and simple [Tuzhilin 2002], [Quinlan &amp; Riv est 1989], [Fayyad &amp; Irani 1993]. In essence, this principle r ecommends that, given a set of competing hypotheses (predicti ve models for a given data set), one should choose as the  X  X est X  hy pothesis the one that minimizes the sum of two terms, namely: (a ) the length of the hypothesis; and (b) the length of the data g iven the hypothesis, i.e., the length of the data when encod ed using the hypothesis as a predictor for the data. The second term represents the length of the encoding of the data instances th at are  X  X xceptions X  to the hypothesis. One characteristic of this principle is that both terms (a) and (b) are measured in bits . At first glance, this has the nice characteristic that two terms tha t seemed non-commensurable (accuracy and size of the hypothesis) have been transformed into a common unit of measurement, and have therefore become  X  X ommensurable X . Although the Minimum Description Length Principle s eems an elegant solution for obtaining commensurability bet ween accuracy and simplicity, it introduces another prob lem, as discussed in the next item. The previously-mentioned commensurability is artifi cially obtained at the following price: the MDL principle introduces the problem of how to encode a hypothesis and its data exceptions into bits of information. For any hypothesis space with a reasonably large size, there will be a large number of different ways of encoding hypotheses and exceptions in bits of information. Finding a  X  X ood X  encoding scheme, amon g so many possible encoding schemes, is usually a very diffic ult task  X  see e.g. [Quinlan &amp; Rivest 1989]  X  and the value of the previously mentioned terms (a) and (b) is entirely dependent o n the choice of encoding scheme. Actually, one cannot say that one encoding is superior to others in general, because each encodin g is associated with an inductive bias, and it is well-known that t he effectiveness of any inductive bias is application dependent. Hen ce, in general the choice of the encoding to be used is difficult and typically done manually , either taking into account background knowledge or in a more ad-hoc fashion involving trial and err or  X  a situation that is somewhat analogous to the manual choice of the weights for each objective in the weighted-formula approach . Pareto-based optimization avoids the problems assoc iated with the choice of a good encoding, since it treats the model-quality criteria of accuracy and size as two separate quali ty measures that are never mixed into the same formula, respecting t he natural non-commensurability of these two quality measures.
 In addition, note that the Pareto approach is more generic than the Minimum Description Length principle, since the lat ter is used only to cope with accuracy and simplicity, whereas the Pareto approach can cope with any kind of non-commensurabl e model-quality criteria. A possible criticism of the Pareto approach is that in this approach the data mining algorithm returns a set of non-domi nated solutions, whereas in practice the user will often use a single solution. How can one choose the  X  X est X  non-dominat ed solution, out of all non-dominated solutions? This seems a di fficult problem associated with the Pareto approach. There are at least two possible answers to the ques tion posed by the previous item. The first answer to the problem of choosing a single  X  X est X  solution, which is the answer most co mmonly found in the multi-objective optimization literature, it that it is up to the user to choose the best solution, by taking into ac count her/his background knowledge and preferences. At first glan ce, this might seem a not very satisfactory answer, because it introduces some subjectivity into the choice of the best solut ion. However, this problem is actually less serious than it looks like at first glance, and this criticism can be rebutted by two p oints. First, one should recall that data mining is just o ne step of a broader knowledge discovery process, and this proce ss is highly interactive [Fayyad et al. 1996], [Brachman &amp; Anand 1996] because participation of the user in the process is essential to improve the chance that discovered knowledge will b e actually useful for the user. Second, and this is a subtle p oint that is often missed by critics of the Pareto approach, the conve ntional approach for coping with multi-objective problems  X  viz., using a weighted formula  X  is also associated with an impor tant subjective decision, namely the choice of the weight values fo r each of the different criteria. With respect to the latter point, the difference be tween the weighted formula approach and the Pareto approach c an be summarized by the two flowcharts in Figures 3(a) an d 3(b). This Figure clearly shows that in both approaches the us er must make a subjective decision. The difference is that in the weighted-formula approach the user has to make the subjective decisi on about the weight values a priori , before the data mining algorithm is run. Intuitively, this is a very uninformed decision, be cause at this point the user does not have any computational resu lt to support her/him in the task of analysing the trade-offs ass ociated with the different criteria. By contrast, in the Pareto appr oach the user makes a subjective choice about the best classifier a posteriori , after she/he has seen the several non-dominated sol utions returned by the data mining algorithm. Those solutions cover a wide range of different trade-offs between the criteria being optimized, so that in principle the user is now in a much better position to analyze the trade-offs associated with the differen t criteria and choose the best solution by taking into account her /his preferences. In any case, there is also a second answer to the p roblem of how to choose the  X  X est X  solution out of all the non-do minated solutions. One can actually violate the principle o f returning all non-dominated solutions and return only the  X  X est X  non-dominated solution found by the algorithm, accordin g to a data-driven heuristic used by the algorithm. One possibi lity is as follows. Although all non-dominated solutions, by d efinition, have in common the characteristic of not being domi nated by any other solution, they are still different from each other with respect to the number of solutions that they dominate. That is, different non-dominated solutions will dominate a different n umber of (dominated) solutions. This suggests the use of a q uality measure that can be used as a  X  X ie-breaking criterion X  to s elect the  X  X est X  non-dominated solution, out of all non-dominated so lutions. In essence, one can choose the non-dominated solution that dominates the largest number of (dominated) solutio ns among all solutions generated by the algorithm [Deb 2001]. It should be noted that this approach is a signific ant departure from the  X  X onventional X  Pareto approach represented by Figure 3(b). At first glance, one might criticize this app roach as an unnecessarily complex way of implementing a single-objective optimization algorithm, since just a single  X  X ptima l X  solution is returned anyway. However, recall that in this appro ach the algorithm is still performing a multi-objective sea rch, looking for a diverse set of non-dominated solutions spread acr oss the Pareto front. The wider exploration of the search space co uld still be very beneficial, by finding a non-dominated solutio n that could not be explored by a conventional single-objective algorithm using, say, the weighted formula approach. ROC graphs are an increasingly popular way of analy zing the performance of a classifier, and they are particula rly useful to choose the best classifier under different scenario s of class distribution and misclassification costs [Provost &amp; Fawcett 1997], [Ting 2002]. On a ROC graph, the false positive rat e (FPR) is plotted on the X axis and the true positive rate (T PR) is plotted on the Y axis. The performance of classifiers can then be visualized on this graph, as follows. Some classifiers produce a binary output  X  a positive or negative class. These classi fiers are represented by points in the ROC graph. Other class ifiers produce a numeric output, to which a threshold is applied i n order to determine if the predicted class is positive or neg ative. These classifiers are represented by curves in the ROC gr aph  X  corresponding to a continuous series of (FPR, TPR) pairs as the threshold values are varied. A ROC graph is illustr ated in Figure 4. In a ROC graph, the ideal performance correspond s to the upper-left point (0,1), and the strategy of randoml y guessing the class of an example corresponds to the line y = x, shown as the dashed line in the figure. Let us consider first the case of binary classifier s. A given point A in the ROC graph is better than another point B if A is to the northwest of B, that is, if A is better than B with respect to at least one of the two criteria (lower FPR, higher TPR) and A is not worse than B with respect to any of the two criteri a. If these conditions hold, then A dominates B, i.e., A is bet ter than B across all class and misclassification cost distrib utions. An example is shown in Figure 4, where classifier A do minates classifier B. Note that this is precisely the defin ition of Pareto dominance, i.e., a given point A in the ROC graph i s better than another point B across all class and misclassificat ion cost distributions if and only if A dominates B in the P areto sense. In the case of numeric classifiers, a given classifier A is better than another classifier B only when the entire curve of A is to the northwest of B. This is clearly a generalization of Pareto dominance from individual points to continuous curv es. In addition, a classifier is potentially optimal if an d only if it lies on the northwest boundary of the convex hull [Provost &amp; Fawcett 1997]. Note that this boundary is precisely the Par eto front in the context of a multi-objective optimization problem  X  involving the minimization of FPR and the maximization of TPR. As mentioned earlier, it is clear that if a classif ier A dominates a classifier B then A is a better classifier. However , it is often the case that one classifier obtains better performance than another only in a limited range of the ROC graph, correspon ding to a limited range of class and misclassification cost d istributions. This is what happens, for instance, with classifier s A and C in Figure 4. Both are non-dominated classifiers, lying in the  X  X areto front X  of the ROC graph. In this case, it has been suggested that one should choose the classifier that has the highe st value of the area under the ROC curve (AUC) [Ling &amp; Zhang 2002], [Bradley 1997]. AUC is a single measure summarizing the perf ormance of a classifier across the entire range of class distr ibution and misclassification costs. However, since the AUC is just a global measure summarizing performance across a wide range of scenarios, it will lead to the choice of a suboptim al classifier in many scenarios [Provost &amp; Fawcett 1997]. Hence, as pointed out by Provost &amp; Fawcett, it is important to discover c lassifiers that are spread across the northwest boundary of the con vex hull, which corresponds to discover classifiers along the Pareto front. This is clearly an argument supporting multi-object ive optimization algorithms, whose goal is precisely to discover solutions spread across the Pareto front. To summarize, ROC Graph-based analysis of classifier performance can be regarded as a particular case of the more general principle of Pareto dominance . This paper has presented a critical review of three different approaches for coping with multi-objective problems in data mining, namely: (a) the  X  X onventional X  approach of transforming a multi-objective problem into a single-objective o ne via a weighted formula; (b) the lexicographical approach; and (c) the Pareto approach. The weighted formula approach is b y far the most popular approach in the data mining literature . However, a careful analysis of this approach has revealed its drawbacks. One of these drawbacks is that it mixes different non-c ommensurable model-quality criteria into the same formula. In pa rticular, this has the disadvantage of producing model-quality mea sures that are not very meaningful to the user, going against the principle that in data mining discovered knowledge should be not only accurate but also comprehensible to the user [Fayya d et al. 1996]. This and other drawbacks of the weighted-formula ap proach are avoided by the lexicographic approach and the Paret o approach. The latter two approaches are more principled appro aches for coping with multi-objective optimization problems i n data mining. With respect to the influence of the user in the re sult returned by the data mining algorithm, the lexicographic approa ch can be considered as an intermediary approach between the weighted-formula and the Pareto approach, as follows. As dis cussed earlier, in the weighted-formula approach the user has the f ull responsibility for specifying the weights (defining the relative importance of each of the objectives to be optimize d) a priori, before she/he has knowledge about the solution cand idates to be explored by the algorithm. By contrast, in the Pare to approach the user does not need to specify any weight nor any ot her form of assigning different priority to different objective s. The algorithm will search for all non-dominated solutions, implic itly considering that all objectives have  X  X he same priority X , and o nly after the set of non-dominated solutions is returned by the algor ithm the user will have to choose one particular solution, a post eriori. Finally, the lexicographic approach allows the user to assig n different priorities to different objectives in a kind of qualitative fashion, i.e., the user just has to say, for instance,  X  X bje ctive A is more important than objective B X , without having to spec ify the precise quantitative value of weights for objectives A and B. Hence, the user X  X  task becomes considerably simpler. In any case, the lexicographic approach shares with the weighted-formula formula the characteristic that a single so lution is returned to the user. This has the advantage of sim plicity but the disadvantage of missed opportunities (i.e., the use r misses the opportunity of analysing the trade-off of different non-dominated solutions) discussed earlier. The Pareto approach is sometimes criticized as bein g  X  X nnecessarily complex X . However, in the discussion presented in section 3.3 several arguments against the Pareto ap proach were found wanting and were rebutted by counter-argument s. It is true that the Pareto approach is considerably more compl ex (in terms of designing and running the data mining algorithm) than the other two approaches. However, this disadvantage se ems compensated by its several advantages  X  in particul ar, avoiding multiple runs of the algorithm, avoiding ad-hoc spe cification of parameters and returning to the user a very informa tive set of non-dominated solutions. The Pareto approach X  X  property of returning a set o f non-dominated solutions not only offers the user a rich source of information about the trade-offs between different model-quality criteria, but also has other potential applications in data mining. In particular, the diversity of solutions in the Paret o front naturally lends itself to the use of those solutions in the c reation of an ensemble of models (e.g., classifiers). Several tec hniques for generating an ensemble of models rely on a kind of randomization  X  e.g. random subsets of data. By contrast, the Par eto approach offers the interesting alternative of performing an explicit search for diverse, non-dominated models, and all the non-dominated discovered models can be immediately used to compos e a diverse ensemble of models. Of course, multi-objective optimization is not a panacea. There are scenarios where a problem that is, at first gla nce, a multi-objective problem can be effectively casted as a si ngle-objective problem. An example is the  X  X wo-objective X  problem where a company wants to identify customers satisfying two criteria, viz.: a) having a high probability of churning, and b) re presenting a high revenue for the company. At first glance this might look like a two-objective scenario. However, in this scenario the two objectives are very related, and they can be effect ively transformed into a single objective by multiplying them, which will compute the expected revenue loss due to churn ing. However, it should be noted that in these scenarios , although a multi-objective optimization algorithm might not be necessary, it might still offer some benefits associated with the fact that its search considers a diverse set solutions. For insta nce, [Bhattacharyya 2000] addressed a problem where a ce llular-phone provider wanted to identify churners satisfying the two above-mentioned criteria. The author developed a Pareto-b ased multi-objective optimization method to solve the two-obje ctive optimization problem, obtaining good results  X  outp erforming more conventional single-objective algorithms. In any case, we emphasize that, similarly to severa l other techniques in data mining, the effectiveness of dif ferent multi-objective optimization approaches strongly depends on the application domain. For instance, recall that the l exicographic approach requires the user to specify a priority or dering for the objectives. This can be natural and desirable in so me applications, but may be unnatural or undesirable in other applic ations. In addition, the computational complexity associated w ith the Pareto approach might make it cumbersome in some applicati ons. As a general conclusion of the discussion presented in this paper, both the lexicographic approach and the Pareto appr oach are more principled approaches to cope with multi-objective data mining problems than the conventional weighted-formula app roach. Hence, the former two approaches deserve more atten tion from the data mining community. In particular, much more work is needed to compare these three approaches, both empi rically (in a large number of different data sets and different s cenarios) and theoretically, since projects comparing two or more multi-objective approaches are rare in the data mining li terature. [Baeza-Yates &amp; Ribeiro-Neto 1999] R. Baeza-Yates an d B. [Bhattacharyya 2000] S. Bhattacharyya. Evolutionary algorithms [Bradley 1997] A.P. Bradley. The use of the ROC cur ve in the [Brachman &amp; Anand 1996] R.J. Brachman and T. Anand. The [Bruha &amp; Tkadlec 2003] I. Bruha and J. Tkadlec. Rul e quality for [Cherkauer &amp; Shavlik 1996] K.J. Cherkauer and J.W. Shavlik. [Chisholm &amp; Tadepalli 2002] M. Chisholm and P. Tade palli. [Corne et al. 2003] D. Corne, K. Deb and P.J. Flemi ng. The good [Deb 2001] K. Deb. Multi-Objective Optimization Using [Fayyad &amp; Irani 1993] U.M. Fayyad and K.B. Irani. M ulti-[Fayyad et al. 1996] U.M. Fayyad, G. Piatetsky-Shap iro and P. [Freitas 1998] A.A. Freitas. On objective measures of rule [Furnkranz &amp; Flach 2003] J. Furnkranz and P.A. Flac h. An [Guyon &amp; Elisseeff 2003] I. Guyon and A. Elisseeff. An [Kaufmann &amp; Michalski 1999] K.A. Kaufmann and R.S. [Kim et al. 2000] Y. Kim, W.N. Street and F. Mencze r. Feature [Kohavi &amp; John 1998] R. Kohavi and G.H. John. The w rapper [Li et al. 2002] J. Li, R. Topor, H. Shen. Construc t robust rule sets [Ling &amp; Zhang 2002] C.L. Ling and H. Zhang. Toward Bayesian [Liu &amp; Motoda 1998] H. Liu and H. Motoda (Eds.) Feature [Liu et al. 1997] B. Liu, W. Hsu and S. Chen. Using general [Mitchell 1980] T.M. Mitchell. The need for biases in learning [Mitchell 1997] T.M. Mitchell. Machine Learning . McGraw-Hill, [Pappa et al. 2002] G.L. Pappa, A.A. Freitas and C. A.A. [Provost &amp; Fawcett 1997] F. Provost and T. Fawcett. Analysis [Quinlan &amp; Rivest 1989] J.R. Quinlan and R.L. Rives t. Inferring [Ting 2002] K.M. Ting. Issues in classifier evaluat ion using [Turney 1995] P. D. Turney. Cost-sensitive classifi cation: [Tuzhilin 2002] A. Tuzhilin. Minimum Description Le ngth. In: [Weiss &amp; Indurkhya 2000] S.M. Weiss and N. Indurkhy a. [Weiss et al. 2003] S.M. Weiss, S.J. Buckley, S. Ka poor and S. [Yu &amp; Liu 2003] L. Yu and H. Liu. Efficiently handl ing feature Dr. Alex Freitas earned a Ph.D. in Computer Science from the University of Essex, UK, in 1997. He is currently a Lecturer at the University of Kent, UK. His publications includ e two authored books about data mining, several invited b ook chapters and more than 70 refereed papers published in journ als or conferences. His main research interests are data m ining, biologically-inspired algorithms and bioinformatics .

