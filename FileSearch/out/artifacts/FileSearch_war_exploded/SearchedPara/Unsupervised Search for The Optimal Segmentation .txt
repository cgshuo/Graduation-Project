 In statistical machine translation (SMT), words are normally considered as the building blocks of translation models. However, especially for mor-phologically complex languages such as Finnish, Turkish, Czech, Arabic etc., it has been shown that using sub-lexical units obtained after morpho-logical preprocessing can improve the machine translation performance over a word-based sys-tem (Habash and Sadat, 2006; Oflazer and Durgar El-Kahlout, 2007; Bisazza and Federico, 2009). However, the effect of segmentation on transla-tion performance is indirect and difficult to isolate (Lopez and Resnik, 2006).

The challenge in designing a sub-lexical SMT system is the decision of what segmentation to use. Linguistic morphological analysis is intuitive, but it is language-dependent and could be highly am-biguous. Furthermore, it is not necessarily opti-mal in that (i) manually engineered segmentation schemes can outperform a straightforward linguis-tic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Dur-gar El-Kahlout and Oflazer, 2006).

A SMT system designer has to decide what segmentation is optimal for the translation task at hand. Existing solutions to this problem are predominantly heuristic, language-dependent, and as such are not easily portable to other lan-guages. Another point to consider is that the op-timal degree of segmentation might decrease as the amount of training data increases (Lee, 2004; Habash and Sadat, 2006). This brings into ques-tion: For the particular language pair and training corpus at hand, what is the optimal (level of) sub-word segmentation? Therefore, it is desirable to learn the optimal segmentation in an unsupervised manner.

In this work, we extend the method of Creutz and Lagus (2007) so as to maximize the transla-tion posterior in unsupervised segmentation. The learning process is tailored to the particular SMT task via the same parallel corpus that is used in training the statistical translation models. Most works in SMT-oriented segmentation are su-pervised in that they consist of manual experimen-tation to choose the best among a set of segmen-tation schemes, and are language(pair)-dependent. For Arabic, Sadat and Habash (2006) present sev-eral morphological preprocessing schemes that en-tail varying degrees of decomposition and com-pare the resulting translation performances in an Arabic-to-English task. Shen et al. (2007) use a subset of the morphology and apply only a few simple rules in segmenting words. Durgar El-Kahlout and Oflazer (2006) tackle this problem when translating from English to Turkish, an ag-glutinative language. They use a morphologi-cal analyzer and disambiguation to arrive at mor-phemes as tokens. However, training the trans-lation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selec-tive morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkish-to-English translation setting.

Unsupervised segmentation by itself has gar-nered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the par-allel training corpora in translation between Dan-ish, Finnish, and Swedish, but without a consistent improvement in results.

Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only mono-lingual information in its objective function. It is conceivable that we can achieve a better segmenta-tion for translation by considering not one but both sides of the parallel corpus. A posssible choice is the post-segmentation alignment accuracy. How-ever, Elming et al. (2009) show that optimizing segmentation with respect to alignment error rate (AER) does not improve and even degrades ma-chine translation performance. Snyder and Barzi-lay (2008) use bilingual information but the seg-mentation is learned independently from transla-tion modeling.
 In Chang et al. (2008), the granularity of the Chinese word segmentation is optimized by train-ing SMT systems for several values of a granular-ity bias parameter and it is found that the value that maximizes translation performance (as measured by BLEU) is different than the value that maxi-mizes segmentation accuracy (as measured by pre-cision and recall).

One motivation in morphological preprocess-ing before translation modeling is  X  X orphology matching X  as in Lee (2004) and in the scheme  X  X N X  of Habash and Sadat (2006). In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained seg-mentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes us-ing alignments with a part-of-speech tagged En-glish corpus. But this method is not completely unsupervised since it requires external linguistic resources in initializing the segmentation with the output of a morphological analyzer and disam-biguator. Talbot and Osborne (2006) tackle a spe-cial case of morphology matching by identifying redundant distinctions in the morphology of one language compared to another. Maximizing translation performance directly would require SMT training and decoding for each segmentation hypothesis considered, which is computationally infeasible. So we make some conditional independence assumptions using a generative model and decompose the posterior probability P ( M f | e, f ) . In this notation e and f denote the two sides of a parallel corpus and M f denotes the segmentation model hypothesized for f . Our approach is an extension of Morfessor (Creutz and Lagus, 2007) so as to include the translation model probability in its cost calcula-tion. Specifically, the segmentation model takes into account the likelihood of both sides of the parallel corpus while searching for the optimal segmentation. The joint likelihood is decomposed into a prior, a monolingual likelihood, and a translation likelihood, as shown in Eq. 1.

P ( e, f, M f ) = P ( M f ) P ( f | M f ) P ( e | f, M f )
Assuming conditional independence between e and M f given f , the maximum a posteriori (MAP) objective can be written as:
The role of the bilingual component P ( e | f ) in Eq. 2 can be motivated with a simple exam-ple as follows. Consider an occurrence of two phrase pairs in a Turkish-English parallel corpus and the two hypothesized sets of segmentations for the Turkish phrases as in Table 1. Without ac-cess to the English side of the corpus, a monolin-gual segmenter can quite possibly score Seg. #1 Turkish phrase: anahtar anahtar X m English phrase: key my key higher than Seg. #2 (e.g., due to the high fre-quency of the observed morph  X +m X ). On the other hand, a bilingual segmenter is expected to assign a higher alignment probability P ( e | f ) to Seg. #2 than Seg. #1, because of the aligned words key || anahtar, therefore ranking Seg. #2 higher.
The two monolingual components of Eq. 2 are computed as in Creutz and Lagus (2007). To sum-marize briefly, the prior P ( M f ) is assumed to only depend on the frequencies and lengths of the indi-vidual morphs, which are also assumed to be in-dependent. The monolingual likelihood P ( f | M f ) is computed as the product of morph probabilities estimated from their frequencies in the corpus.
To compute the bilingual (translation) likeli-hood P ( e | f ) , we use IBM Model 1 (Brown et al., 1993). Let an aligned sentence pair be rep-resented by ( s e , s f ) , which consists of word se-quences s e = e 1 , ..., e l and s f = f 1 , ..., f m . Us-ing a purely notational switch of the corpus labels from here on to be consistent with the SMT lit-erature, where the derivations are in the form of P ( f | e ) , the desired translation probability is given by the expression: The sentence length probability distribution P ( m | e ) is assumed to be Poisson with the ex-pected sentence length equal to m . 3.1 Incremental computation of Model-1 During search, the translation likelihood P ( e | f ) needs to be calculated according to Eq. 3 for every hypothesized segmentation.

To compute Eq. 3, we need to have at hand the individual morph translation probabilities t ( f j | e i ) . These can be estimated using the EM algorithm given by (Brown, 1993), which is guaranteed to converge to a global maximum of the likelihood for Model 1. However, running the EM algorithm to optimization for each considered segmentation model can be computationally expensive, and can result in overtraining. Therefore, in this work we used the likelihood computed after the first EM iteration, which also has the nice property that P ( f | e ) can be computed incrementally from one segmentation hypothesis to the next.

The incremental updates are derived from the equations for the count collection and probability estimation steps of the EM algorithm as follows. In the count collection step, in the first iteration, we need to compute the fractional counts c ( f j | e i ) (Brown et al., 1993): where (# f j ) and (# e i ) denote the number of occur-rences of f j in s f and e i in s e , respectively.
Let f k denote the word hypothesized to be seg-mented. Let the resulting two sub-words be f p and f , any of which may or may not previously exist in the vocabulary. Then, according to Eq. (4), as a result of the segmentation no update is needed for c ( f j | e i ) for j = 1 . . . N , j 6 = p, q , i = 1 . . . M (note that f k no longer exists); and the necessary updates  X  c ( f j | e i ) for c ( f j | e i ) , where j = p, q ; i = 1 . . . M are given by:
Note that Eq. (5) is nothing but the previous count value for the segmented word, c ( f k | e i ) . So, all needed in the count collection step is to copy the set of values c ( f k | e i ) to c ( f p | e i ) and c ( f adding if they already exist.

Then in the probability estimation step, the nor-malization is performed including the newly added fractional counts. 3.2 Parallelization of search In an iteration of the algorithm, all words are pro-cessed in random order, computing for each word the posterior probability of the generative model after each possible binary segmentation (splitting) of the word. If the highest-scoring split increases the posterior probability compared to not splitting, that split is accepted (for all occurrences of the word) and the resulting sub-words are explored re-cursively for further segmentations. The process is repeated until an iteration no more results in a sig-nificant increase in the posterior probability.
The search algorithm of Morfessor is a greedy algorithm where the costs of the next search points Figure 1: BLEU scores obtained with different segmentation methods. Multiple data points for a system correspond to different random orders in processing the data (Creutz and Lagus, 2007). are affected by the decision in the current step. This leads to a sequential search and does not lend itself to parallelization.

We propose a slightly modified search proce-dure, where the segmentation decisions are stored but not applied until the end of an iteration. In this way, the cost calculations (which is the most time-consuming component) can all be performed independently and in parallel. Since the model is not updated at every decision, the search path can differ from that in the sequential greedy search and hence result in different segmentations. We performed in vivo testing of the segmenta-tion algorithm on the Turkish side of a Turkish-to-English task. We compared the segmenta-tions produced by Morfessor, Morfessor modi-fied for parallel search (Morfessor-p), and Mor-fessor with bilingual cost (Morfessor-bi) against the word-based performance. We used the ATR Basic Travel Expression Corpus (BTEC) (Kikui et al., 2006), which contains travel conversa-tion sentences similar to those in phrase-books for tourists traveling abroad. The training cor-pus contained 19,972 sentences with average sen-tence length 5.6 and 7.7 words for Turkish and English, respectively. The test corpus consisted of 1,512 sentences with 16 reference translations. We used GIZA++ (Och and Ney, 2003) for post-segmentation token alignments and the Moses toolkit (Koehn et al., 2007) with default param-eters for phrase-based translation model genera-tion and decoding. Target language models were Figure 2: Cost-BLEU plots of Morfessor and Morfessor-bi. Correlation coefficients are -0.005 and -0.279, respectively. trained on the English side of the training cor-pus using the SRILM toolkit (Stolcke, 2002). The BLEU metric (Papineni et al., 2002) was used for translation evaluation.

Figure 1 compares the translation performance obtained using the described segmentation meth-ods. All segmentation methods generally im-prove the translation performance (Morfessor and Morfessor-p) compared to the word-based models. However, Morfessor-bi, which utilizes both sides of the parallel corpus in segmenting, does not con-vincingly outperform the monolingual methods.
In order to investigate whether the proposed bilingual segmentation cost correlates any better than the monolingual segmentation cost of Mor-fessor, we show several cost-BLEU pairs obtained from the final and intermediate segmentations of Morfessor and Morfessor-bi in Fig. 2. The cor-relation coefficients show that the proposed bilin-gual metric is somewhat predictive of the trans-lation performance as measured by BLEU, while the monolingual Morfessor cost metric has almost no correlation. Yet, the strong noise in the BLEU scores (vertical variation in Fig. 2) diminishes the effect of this correlation, which explains the incon-sistency of the results in Fig. 1. Indeed, in our ex-periments even though the total cost kept decreas-ing at each iteration of the search algorithm, the BLEU scores obtained by those intermediate seg-mentations fluctuated without any consistent im-provement.

Table 2 displays sample segmentations pro-duced by both the monolingual and bilingual seg-mentation algorithms. We can observe that uti-lizing the English side of the corpus enabled
Count Morfessor Morfessor-bi English Gloss Morfessor-bi: (i) to consistently identify the root word  X  X nahtar X  (top portion), and (ii) to match the English plural word form  X  X ames X  with the Turk-ish plural word form  X  X yunlar X  (bottom portion). Monolingual Morfessor is unaware of the target segmentation, and hence it is up to the subsequent translation model training to learn that  X  X yun X  is sometimes translated as  X  X ame X  and sometimes as  X  X ames X  in the segmented training corpus. We have presented a method for determining opti-mal sub-word translation units automatically from a parallel corpus. We have also showed a method of incrementally computing the first iteration pa-rameters of IBM Model-1 between segmentation hypotheses. Being language-independent, the pro-posed algorithm can be added as a one-time pre-processing step prior to training in a SMT system without requiring any additional data/linguistic re-sources. The initial experiments presented here show that the translation units learned by the proposed algorithm improves on the word-based baseline in both translation directions.

One avenue for future work is to relax some of the several independence assumptions made in the generative model. For example, independence of consecutive morphs could be relaxed by an HMM model for transitions between morphs (Creutz and Lagus, 2007). Other future work includes optimiz-ing the segmentation of both sides of the corpus and experimenting with other language pairs.
It is also possible that the probability distribu-tions are not discriminative enough to outweigh the model prior tendencies since the translation probabilities are estimated only crudely (single it-eration of Model-1 EM algorithm). A possible candidate solution would be to weigh the transla-tion likelihood more in calculating the overall cost. In fact, this idea could be generalized into a log-linear modeling (e.g., (Poon et al., 2009)) of the various components of the joint corpus likelihood and possibly other features.

Finally, integration of sub-word segmentation with the phrasal lexicon learning process in SMT is desireable (e.g., translation-driven segmenta-tion in Wu (1997)). Hierarchical models (Chiang, 2007) could cover this gap and provide a means to seamlessly integrate sub-word segmentation with statistical machine translation.
 The authors would like to thank Murat Sarac  X lar for valuable discussions and guidance in this work, and the anonymous reviewers for very useful com-ments and suggestions. Murat Sarac  X lar is sup-ported by the T  X  UBA-GEB  X  IP award.

 In statistical machine translation (SMT), words are normally considered as the building blocks of translation models. However, especially for mor-phologically complex languages such as Finnish, Turkish, Czech, Arabic etc., it has been shown that using sub-lexical units obtained after morpho-logical preprocessing can improve the machine translation performance over a word-based sys-tem (Habash and Sadat, 2006; Oflazer and Durgar El-Kahlout, 2007; Bisazza and Federico, 2009). However, the effect of segmentation on transla-tion performance is indirect and difficult to isolate (Lopez and Resnik, 2006).

The challenge in designing a sub-lexical SMT system is the decision of what segmentation to use. Linguistic morphological analysis is intuitive, but it is language-dependent and could be highly am-biguous. Furthermore, it is not necessarily opti-mal in that (i) manually engineered segmentation schemes can outperform a straightforward linguis-tic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Dur-gar El-Kahlout and Oflazer, 2006).

A SMT system designer has to decide what segmentation is optimal for the translation task at hand. Existing solutions to this problem are predominantly heuristic, language-dependent, and as such are not easily portable to other lan-guages. Another point to consider is that the op-timal degree of segmentation might decrease as the amount of training data increases (Lee, 2004; Habash and Sadat, 2006). This brings into ques-tion: For the particular language pair and training corpus at hand, what is the optimal (level of) sub-word segmentation? Therefore, it is desirable to learn the optimal segmentation in an unsupervised manner.

In this work, we extend the method of Creutz and Lagus (2007) so as to maximize the transla-tion posterior in unsupervised segmentation. The learning process is tailored to the particular SMT task via the same parallel corpus that is used in training the statistical translation models. Most works in SMT-oriented segmentation are su-pervised in that they consist of manual experimen-tation to choose the best among a set of segmen-tation schemes, and are language(pair)-dependent. For Arabic, Sadat and Habash (2006) present sev-eral morphological preprocessing schemes that en-tail varying degrees of decomposition and com-pare the resulting translation performances in an Arabic-to-English task. Shen et al. (2007) use a subset of the morphology and apply only a few simple rules in segmenting words. Durgar El-Kahlout and Oflazer (2006) tackle this problem when translating from English to Turkish, an ag-glutinative language. They use a morphologi-cal analyzer and disambiguation to arrive at mor-phemes as tokens. However, training the trans-lation models with morphemes actually degrades the translation performance. They outperform the word-based baseline only after some selec-tive morpheme grouping. Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkish-to-English translation setting.

Unsupervised segmentation by itself has gar-nered considerable attention in the computational linguistics literature (Poon et al., 2009; Snyder and Barzilay, 2008; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Brent, 1999). However, few works report their performance in a translation task. Virpioja et al. (2007) used Morfessor (Creutz and Lagus, 2007) to segment both sides of the par-allel training corpora in translation between Dan-ish, Finnish, and Swedish, but without a consistent improvement in results.

Morfessor, which gives state of the art results in many tests (Kurimo et al., 2009), uses only mono-lingual information in its objective function. It is conceivable that we can achieve a better segmenta-tion for translation by considering not one but both sides of the parallel corpus. A posssible choice is the post-segmentation alignment accuracy. How-ever, Elming et al. (2009) show that optimizing segmentation with respect to alignment error rate (AER) does not improve and even degrades ma-chine translation performance. Snyder and Barzi-lay (2008) use bilingual information but the seg-mentation is learned independently from transla-tion modeling.
 In Chang et al. (2008), the granularity of the Chinese word segmentation is optimized by train-ing SMT systems for several values of a granular-ity bias parameter and it is found that the value that maximizes translation performance (as measured by BLEU) is different than the value that maxi-mizes segmentation accuracy (as measured by pre-cision and recall).

One motivation in morphological preprocess-ing before translation modeling is  X  X orphology matching X  as in Lee (2004) and in the scheme  X  X N X  of Habash and Sadat (2006). In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained seg-mentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes us-ing alignments with a part-of-speech tagged En-glish corpus. But this method is not completely unsupervised since it requires external linguistic resources in initializing the segmentation with the output of a morphological analyzer and disam-biguator. Talbot and Osborne (2006) tackle a spe-cial case of morphology matching by identifying redundant distinctions in the morphology of one language compared to another. Maximizing translation performance directly would require SMT training and decoding for each segmentation hypothesis considered, which is computationally infeasible. So we make some conditional independence assumptions using a generative model and decompose the posterior probability P ( M f | e, f ) . In this notation e and f denote the two sides of a parallel corpus and M f denotes the segmentation model hypothesized for f . Our approach is an extension of Morfessor (Creutz and Lagus, 2007) so as to include the translation model probability in its cost calcula-tion. Specifically, the segmentation model takes into account the likelihood of both sides of the parallel corpus while searching for the optimal segmentation. The joint likelihood is decomposed into a prior, a monolingual likelihood, and a translation likelihood, as shown in Eq. 1.

P ( e, f, M f ) = P ( M f ) P ( f | M f ) P ( e | f, M f )
Assuming conditional independence between e and M f given f , the maximum a posteriori (MAP) objective can be written as:
The role of the bilingual component P ( e | f ) in Eq. 2 can be motivated with a simple exam-ple as follows. Consider an occurrence of two phrase pairs in a Turkish-English parallel corpus and the two hypothesized sets of segmentations for the Turkish phrases as in Table 1. Without ac-cess to the English side of the corpus, a monolin-gual segmenter can quite possibly score Seg. #1 Turkish phrase: anahtar anahtar X m English phrase: key my key higher than Seg. #2 (e.g., due to the high fre-quency of the observed morph  X +m X ). On the other hand, a bilingual segmenter is expected to assign a higher alignment probability P ( e | f ) to Seg. #2 than Seg. #1, because of the aligned words key || anahtar, therefore ranking Seg. #2 higher.
The two monolingual components of Eq. 2 are computed as in Creutz and Lagus (2007). To sum-marize briefly, the prior P ( M f ) is assumed to only depend on the frequencies and lengths of the indi-vidual morphs, which are also assumed to be in-dependent. The monolingual likelihood P ( f | M f ) is computed as the product of morph probabilities estimated from their frequencies in the corpus.
To compute the bilingual (translation) likeli-hood P ( e | f ) , we use IBM Model 1 (Brown et al., 1993). Let an aligned sentence pair be rep-resented by ( s e , s f ) , which consists of word se-quences s e = e 1 , ..., e l and s f = f 1 , ..., f m . Us-ing a purely notational switch of the corpus labels from here on to be consistent with the SMT lit-erature, where the derivations are in the form of P ( f | e ) , the desired translation probability is given by the expression: The sentence length probability distribution P ( m | e ) is assumed to be Poisson with the ex-pected sentence length equal to m . 3.1 Incremental computation of Model-1 During search, the translation likelihood P ( e | f ) needs to be calculated according to Eq. 3 for every hypothesized segmentation.

To compute Eq. 3, we need to have at hand the individual morph translation probabilities t ( f j | e i ) . These can be estimated using the EM algorithm given by (Brown, 1993), which is guaranteed to converge to a global maximum of the likelihood for Model 1. However, running the EM algorithm to optimization for each considered segmentation model can be computationally expensive, and can result in overtraining. Therefore, in this work we used the likelihood computed after the first EM iteration, which also has the nice property that P ( f | e ) can be computed incrementally from one segmentation hypothesis to the next.

The incremental updates are derived from the equations for the count collection and probability estimation steps of the EM algorithm as follows. In the count collection step, in the first iteration, we need to compute the fractional counts c ( f j | e i ) (Brown et al., 1993): where (# f j ) and (# e i ) denote the number of occur-rences of f j in s f and e i in s e , respectively.
Let f k denote the word hypothesized to be seg-mented. Let the resulting two sub-words be f p and f , any of which may or may not previously exist in the vocabulary. Then, according to Eq. (4), as a result of the segmentation no update is needed for c ( f j | e i ) for j = 1 . . . N , j 6 = p, q , i = 1 . . . M (note that f k no longer exists); and the necessary i = 1 . . . M are given by:
Note that Eq. (5) is nothing but the previous count value for the segmented word, c ( f k | e i ) . So, all needed in the count collection step is to copy the set of values c ( f k | e i ) to c ( f p | e i ) and c ( f adding if they already exist.

Then in the probability estimation step, the nor-malization is performed including the newly added fractional counts. 3.2 Parallelization of search In an iteration of the algorithm, all words are pro-cessed in random order, computing for each word the posterior probability of the generative model after each possible binary segmentation (splitting) of the word. If the highest-scoring split increases the posterior probability compared to not splitting, that split is accepted (for all occurrences of the word) and the resulting sub-words are explored re-cursively for further segmentations. The process is repeated until an iteration no more results in a sig-nificant increase in the posterior probability.
The search algorithm of Morfessor is a greedy algorithm where the costs of the next search points Figure 1: BLEU scores obtained with different segmentation methods. Multiple data points for a system correspond to different random orders in processing the data (Creutz and Lagus, 2007). are affected by the decision in the current step. This leads to a sequential search and does not lend itself to parallelization.

We propose a slightly modified search proce-dure, where the segmentation decisions are stored but not applied until the end of an iteration. In this way, the cost calculations (which is the most time-consuming component) can all be performed independently and in parallel. Since the model is not updated at every decision, the search path can differ from that in the sequential greedy search and hence result in different segmentations. We performed in vivo testing of the segmenta-tion algorithm on the Turkish side of a Turkish-to-English task. We compared the segmenta-tions produced by Morfessor, Morfessor modi-fied for parallel search (Morfessor-p), and Mor-fessor with bilingual cost (Morfessor-bi) against the word-based performance. We used the ATR Basic Travel Expression Corpus (BTEC) (Kikui et al., 2006), which contains travel conversa-tion sentences similar to those in phrase-books for tourists traveling abroad. The training cor-pus contained 19,972 sentences with average sen-tence length 5.6 and 7.7 words for Turkish and English, respectively. The test corpus consisted of 1,512 sentences with 16 reference translations. We used GIZA++ (Och and Ney, 2003) for post-segmentation token alignments and the Moses toolkit (Koehn et al., 2007) with default param-eters for phrase-based translation model genera-tion and decoding. Target language models were Figure 2: Cost-BLEU plots of Morfessor and Morfessor-bi. Correlation coefficients are -0.005 and -0.279, respectively. trained on the English side of the training cor-pus using the SRILM toolkit (Stolcke, 2002). The BLEU metric (Papineni et al., 2002) was used for translation evaluation.

Figure 1 compares the translation performance obtained using the described segmentation meth-ods. All segmentation methods generally im-prove the translation performance (Morfessor and Morfessor-p) compared to the word-based models. However, Morfessor-bi, which utilizes both sides of the parallel corpus in segmenting, does not con-vincingly outperform the monolingual methods.
In order to investigate whether the proposed bilingual segmentation cost correlates any better than the monolingual segmentation cost of Mor-fessor, we show several cost-BLEU pairs obtained from the final and intermediate segmentations of Morfessor and Morfessor-bi in Fig. 2. The cor-relation coefficients show that the proposed bilin-gual metric is somewhat predictive of the trans-lation performance as measured by BLEU, while the monolingual Morfessor cost metric has almost no correlation. Yet, the strong noise in the BLEU scores (vertical variation in Fig. 2) diminishes the effect of this correlation, which explains the incon-sistency of the results in Fig. 1. Indeed, in our ex-periments even though the total cost kept decreas-ing at each iteration of the search algorithm, the BLEU scores obtained by those intermediate seg-mentations fluctuated without any consistent im-provement.

Table 2 displays sample segmentations pro-duced by both the monolingual and bilingual seg-mentation algorithms. We can observe that uti-lizing the English side of the corpus enabled
Count Morfessor Morfessor-bi English Gloss (the) key ( ACC. ); 2 his/her key your key; 2 of (the) key your key ( ACC. ); 2 his/her key ( ACC. ) (the) games ( ACC. ); 2 his/her games; 3 their game(s) of (the) games; 2 your games Morfessor-bi: (i) to consistently identify the root word  X  X nahtar X  (top portion), and (ii) to match the English plural word form  X  X ames X  with the Turk-ish plural word form  X  X yunlar X  (bottom portion). Monolingual Morfessor is unaware of the target segmentation, and hence it is up to the subsequent translation model training to learn that  X  X yun X  is sometimes translated as  X  X ame X  and sometimes as  X  X ames X  in the segmented training corpus. We have presented a method for determining opti-mal sub-word translation units automatically from a parallel corpus. We have also showed a method of incrementally computing the first iteration pa-rameters of IBM Model-1 between segmentation hypotheses. Being language-independent, the pro-posed algorithm can be added as a one-time pre-processing step prior to training in a SMT system without requiring any additional data/linguistic re-sources. The initial experiments presented here show that the translation units learned by the proposed algorithm improves on the word-based baseline in both translation directions.

One avenue for future work is to relax some of the several independence assumptions made in the generative model. For example, independence of consecutive morphs could be relaxed by an HMM model for transitions between morphs (Creutz and Lagus, 2007). Other future work includes optimiz-ing the segmentation of both sides of the corpus and experimenting with other language pairs.
It is also possible that the probability distribu-tions are not discriminative enough to outweigh the model prior tendencies since the translation probabilities are estimated only crudely (single it-eration of Model-1 EM algorithm). A possible candidate solution would be to weigh the transla-tion likelihood more in calculating the overall cost. In fact, this idea could be generalized into a log-linear modeling (e.g., (Poon et al., 2009)) of the various components of the joint corpus likelihood and possibly other features.

Finally, integration of sub-word segmentation with the phrasal lexicon learning process in SMT is desireable (e.g., translation-driven segmenta-tion in Wu (1997)). Hierarchical models (Chiang, 2007) could cover this gap and provide a means to seamlessly integrate sub-word segmentation with statistical machine translation.
 The authors would like to thank Murat Sarac  X lar for valuable discussions and guidance in this work, and the anonymous reviewers for very useful com-ments and suggestions. Murat Sarac  X lar is sup-ported by the T  X  UBA-GEB  X  IP award.

