 Automated text categorization (TC) has been widely investigated since the early days of artificial intelligence. According to the arriving mode of documents, TC can be divided into online TC and batch TC. According to the number of predef ined categories, TC includes binary TC and multi-category TC. For instance, email spam filtering is an online binary TC application and web document classifying is normally a batch multi-category TC application. The binary TC is a special case of the multi-category TC and a batch TC can be regarded as a series of onlin e classifications, so this paper addresses the online multi-category TC problem. algorithms. The power law of word frequency in a set of text documents, a famous random distribution phenomenon, was discovered for a long time. How to use the power law to reduce the space-time complexity of st atistical TC algorith ms is a significant research problem. In statistical TC algorithms, token frequency is a very effective feature. If we only use token frequency features in a closed text collections, the feature with once occurrence will never be used, and according to the power law, we can easily remove these useless long tail features fo r lower space-time costs. But in an online situation, we meet a puzzle of open featur e space. The ubiquitous power law may bring an opportunity to propose a novel statistical TC algorithm for the efficient online multi-category TC problem. works about TC. In section 3, we investigate the power law of token frequency both in an email collection and a web document collec tion, and analyze the potential uselessness rate. In section 4, we propose a random sampling ensemble Bayesian (RSEB) algorithm. In section 5, the experiment and result are described. At last, in section 6, the conclusion and further work are given. Recently, statistical TC algorithms have been widely used in TC ap plications [1]. Email spam filtering is defined as an online supervised binary TC problem, which is simulated as an immediate full feedback task (IFFT ) in the TREC spam track. Web document classifying is normally defined as a batch multi-category TC problem, which is simulated as a 12-category Chinese web document classifying task (WDCT) [2]. For instance: 1) based on the vector space model (VSM), the onlin e Bayesian algorithm uses the joint probabilities of words and categories to estimate the probabilities of categories for a given document; 2) the rela xed online support vector machines (SVMs) algorithm [3] relaxes the maximum margin requirement and produces nearly equivalent online fusion of dynamic Markov compression (DMC) and logistic regression on character 4-gram algorithm [4] is the winner on the IFFT in the TR EC 2007 spam track. web document classifying. For instance: 1) the k-nearest neighbor (kNN) TC algorithm decides a document acco rding to the k neares t neighbor document categories; 2) the centroid TC algorithm [5] is based on the assumption that a given document should be assigned a particular category if the similarity of this document to the centroid of its true category is the largest; and 3) the winnow algorithm [6] uses a multiplicative weight-update scheme that allows it to perform much better when many dimensions are irrelevant. crucial to the classification performance. Pr evious research shows that the multi-field structured feature of email documents supports the divide-and-conquer strategy, and can be used to improve the classification performance [7]. This multi-field learning (MFL) framework will bring the statistical, computational and representational advantages like that of ensemble learning methods [8]. Previous research also shows that the token phenomenon in many text documents. high overall performance of supervised lear ning, without more claiming their low space-time complexity. However, in practice th e algorithm is space-tim e-cost-sensitive for many real-world large-scale applications. For instance, specified in the TREC spam track, the space-time limitation (total 1 GB RAM and 2 sec/email) is still unpractical and horrible in a real large-scale email system, where large-scale emails will form a round-the-clock data stream and there will be more than thousands of emails arriving during 2 seconds. Especially, it is unreasonable to require an industrial TC algorithm with a time-consuming training or updating: such a requ irement defeats previous complex statistical algorithms, and motivates us to expl ore a space-time-efficient TC algorithm. 3.1 Corpora The email documents corpus is the TREC07p collection, firstly designed as a public corpus for TREC 2007 spam track, which contains total 75,419 emails (25,220 hams and 50,199 spams). Each email document is stored as a plain text file, and email text is unaltered except for a few systematic substitutions of names. 14,150 documents and is organized in two hierarchies. The first hierarchy contains 12 use TanCorp-12. TREC07p corpus contains multi-language, although the main language is English. The TanCorp corpus is a Chinese text documents collection. 3.2 Token Frequency Distribution In order to re-examine token frequency distribution, we calculate the number of tokens. According to the widely-used VSM, a text docu ment is normally represented as a feature word-level k-gram token m odel can achieve promising results [10]. But different granularities determine the total number of text features. Here, we consider four overlapping word-level k-gram token models (1-gram, 2-gram, 3-gram, 4-gram) to represent tokens.
 the number of each token occurrence in the TREC07p collection. Fig. 1 shows the token frequency as the function of the token X  X  rank with the word-level 4-gram token model. that the frequency distribution of the word-l evel 4-gram token ap proximately follows a power law. occurrence in the TanCorp collection. Fig. 2 shows the token frequency as the function of the token X  X  rank with the word-level 2-gr am token model. The trendline also shows a power law distribution. distribution in the TREC07p collection and the 2-gram token frequency distribution in the TanCorp collection follow the power law, but the others k-gram token frequency distribution also follow the power law. Table 1 shows the detailed trendline (y = a x + b ) coefficients a and b .
 power law in the multilingual email documents, the Chinese web documents, and the field sub-documents of email [11]. The ubiquitous power law indicates that the method to remove those useless features for lower space-time costs. 3.3 Potential Useless Feature performance. The iteration, the cross-validati on and the multi-pass scans are all effective methods to the text feature selection. But these methods bring the high space-time complexity. If we can detect and remove those useless features, we will save more time and space. However, what is the us eless feature and how to find it? potential useless feature. As an extreme instan ce, if a token feature occurs only once all number of token features. Here, we only consider the word-level 4-gram token in the TREC07p collection and the word-level 2-gram token in the TanCorp collection. occurs once and twice in the related documents set. The N (*) denotes the total number of ( N (1)+ N (2))/ N (*). Table 2 shows that the uselessne ss rate in the TREC07p collection is between 63% and 82%, and the uselessness ra te in the TanCorp collection is between TREC07p corpus [11]. If we can get the whole text documents before TC predicting, we TC application faces an open text space pr oblem, and we can not foreknow a token there are lots of useless token features. Supported by the priori and ubiquitous power law, this paper proposes a random sampling method to remove these useless token features at the time of online training. Th e range of uselessness rate indicates the theoretical tolerant range of training feature loss rate. 4.1 Token Level Memory In this paper, the object categories of the online multi-category TC problem are represented as a set in the form ( C ={C i } , i=1, 2, ..., n), and a document D is represented as a sequence of tokens in the form ( D =T 1 T 2 ...T j ...). Here, we use the overlapping word-level k-gram model to define a token. The token frequency within historical labeled classification information and must be stored effectively. The token level memory (TLM) is a data structure to store the token frequency information of labeled documents, from which we can conveniently calculate the Bayesian conditional probability P (C i |T j ) for the object category C i and the token T j . We straightforwardly combine the Bayesian conditional probabilities of tokens and choose the category of the biggest probability as the document X  X  final category prediction. tables. The table entry of the DF index is a key-value pair &lt; key C , value &gt;, where each key C denotes the i th category and each value DF(C i ) denotes the total number of documents integers. memory-based algorithms, such as kNN, st ore document-level labeled examples. This index structure has a native compressible property of raw texts. Each incremental updating or retrieving of index has a constant time complexity. The power law can help us to remove lots of long tail tokens through random sampling learning. 4.2 Random Sampling Learning Supported by the TLM, the RSEB algorithm takes the online supervised training process as an incremental updating process of indexes, and takes the online predicting process as a retrieving process of indexes. supervised training process. The random sampling idea is based on the assumption that some tokens selected randomly according to equiprobability trend to be higher frequency tokens. If only the relative frequency features are concerned among tokens, we can use partial tokens of a labeled document to update the TLM after random sampling. As a result, lots of long tail tokens will be onlin e removed, and the relative frequency will not change among tokens. We define the random sampling rate R rs as the ratio of the number of tokens added into the TLM to the total number of tokens of each labeled document, which is a real number ( R rs  X  [0, 1]).
 token X  X  rank, and the vertical-axis (y-axis) indicates the token frequency. If R rs =1, all the tokens of a labeled document will be added into the TLM at the time of online training. While if R rs &lt;1, there will be some tokens absent in the TLM. Along the online incremental updating, these above two cases w ill form two power law curves in Fig. 4, where the shadow range denotes removed tokens. These two power law curves also frequency among tokens. Of course, if the random sampling rate approximates zero, the classification ability of the TLM will also be damaged. However, what is the optimal random sampling rate? Theoretically, a promising random sampling rate is the ( R rs =1-R gives an approximate heuristic, such as the 20|80 rule of the R rs | R u . algorithm. When a new labeled document arrives, the online training procedure only needs to add the document X  X  tokens into th e TLM. This procedure firstly analyzes the document text and extracts tokens based on an overlapping word-level k-gram model, and then randomly samples the tokens based on a preset random sampling rate, and finally updates the token frequency or adds a new index entry to the TLM according to the tokens after the random sampling. 4.3 Ensemble Bayesian Predicting The Bayesian conditional probability predicting is a very classical method. According to each observed token of a document, the Ba yesian method can ob tain an array of probabilities, reflecting the likelihood that the classified document belongs to each category. The ensemble method uses arithme tical average to combine the multi-array of the maximal probability in the final array is predicted as the document X  X  category. algorithm. When a new document arriving, the online predicting procedure is triggered: 1) the procedure also analy zes the document text and extracts tokens based on an overlapping word-level k-gram model; 2) the procedure retrieves the current TLM and probability; 3) the procedure a ssumes that each token X  X  contribution is equivalent to the final probabilities array and uses the arithmetical average method to calculate a final ensemble probabilities array; and 4) the procedure chooses the maximal probability in the final ensemble probabilities array, and outputs the document X  X  category predication and this maximal probability. 4.4 Space-Time Complexity The RSEB algorithm mainly makes up of the online training and the online predicting procedures, whose space-time complexity depends on the TLM storage space and the loops in the two procedures. property of index files [12] and the random-sampling-based compressible property at the time of online incremental updating. Hash list structure, prevailingly employed in information retrieval, has a lower compress ion ratio of raw texts. Though the training documents will mount in the wake of the increasing of online feedbacks, the TLM storage space will only increase slowly. The native compressible property of index files tokens, and not limited to the total number of training documents. The random-sampling-based compressible property of TLM is caused by the power law of token frequency distribution and the only requirement of relative frequency. The random-sampling-based feature selection can cut the long tail useless features in the online situation. The above two compressible properties make that the online labeled document stream can be incrementally space-efficiently stored. according to hash functions. The online tr aining procedure is lazy, requiring no retraining when a new labeled document added. Fig. 5 shows that the time cost of per updating is only proportional to the total number of tokens in the document. Except the loop (see (4) of Fig. 5) acco rding to the number of tokens , there are no time-consuming operations. The major time cost of the online predicting procedure is related to the number of categories. The straightforward calc ulating makes that the time complexity is acceptable in the practical online application. 5.1 Implementation We implement an email spam filter ( esf ) and a web document classifier ( wdc ) according pre-processing, such as stemming, stop word elimination, etc. classifiers within the seven-field MFL frame work, five natural fields (Header, From, ToCcBcc, Subject, and Body) and two artifici al fields (H.IP, H.EmailBox), and each field classifier is an implementation of the RSEB algorithm with binary categories. In each field classifier, the overlapping word -level 4-gram token model is applied. implementation of the RSEB algorithm with 12 categories in Chinese texts. In order to extract word-level tokens, we bu ild a Chinese segmenter in the wdc classifier. 5.2 Task and Evaluation We run an IFFT of email spam filtering and a WDCT of 12-category Chinese web document classifying to evaluate the performance of the RSEB algorithm. ROCA, the area above the recei ver operating characteristic (ROC) curve percentage, where 0 is optimal, to evaluate the filter X  X  performance. We compare the esf to the bogo TREC spam track. The bogo filter is a classical implementation of online Bayesian algorithm, the tftS3F filter is based on the relaxed online SVMs algorithm and has gained several best results in the TREC 2007 spam track, and the wat3 filter is based on the online fusion of DMC and logistic regressi on algorithm, which is the winner on the IFFT in the TREC 2007 spam track. In this experiment, we use the TREC07p corpus, the TREC spam filter evaluation toolkit, and the associated evaluation methodology. experiments by evenly splitting the TanCorp-12 dataset into three parts and use two parts for training and the remaining third for testing. We perform the training-testing Here reports classical MacroF1 and MicroF1 measures. We run the wdc classifier on the 12-category Chinese WDCT, and compare the results of the wdc classifier as well as to that of the kNN classifier, the centroid classifier, and the winnow classifier. 2.80 GHz Pentium D CPU. 5.3 Results and Discussions There are four experiments. On the email spam filtering, the experiment A tries to evaluate that the RSEB algorithm is time -efficient and can achieve the best overall performance, and the experiment B wants to verify that the TLM data structure has the random-sampling-based compressible property and the proposed random sampling method is space-efficient. On the web document classifying, the experiment C tries to evaluate the effectiveness of the RSEB algor ithm, and the experiment D wants to verify the random-sampling-based compressible property of the TLM data structure in the multi-category situation. can complete filtering task in high speed (2,834 sec), whose overall performance 1-ROCA is comparable to the best wat3 filter X  X  (0.0055) among the participators at the TREC 2007 spam filtering evaluation. The time and 1-ROCA performance of the esf filter exceed the bogo  X  X  and the tftS3F  X  X  more. from the 90% down to the 10%. The esf filter repeatedly runs 30 times for each random sampling rate, and here reports the mean performance among the 30 results for each random sampling rate. The detailed random sampling rate ( R rs ), final indexed token processed tokens during the filtering task. The space is the number of tokens in the final TLM storage. the 100% down to the 30%, which indicates if we randomly remove up to 70% tokens at the time of online training, the 1-ROCA will not be influenced obviously. On average of the 30 results, there are four 1-ROCA values exceed the best one (0.0055). Table 4 shows that the final indexed token compressing rate approximates a direct ratio of the random sampling rate, which proves that random-sampling-based token feature selection according to the theoretical uselessness rate heuristic between 63% and 82% is effective in the online situation. and sets its random sampling rate R rs =1. Through evenly splitting the TanCorp-12 dataset, we make the three-fold cross va lidation. The mean MacroF1 and the mean MicroF1 are showed in Table 5, where the resu lts of other four classifiers are cited from existing researches [2]. The results show that the wdc classifier can complete classifying task in high MacroF1 (0.8696) and high MicroF1 (0.9126), w hose performance exceeds MacroF1 (0.9172) and MicroF1 (0.9483). R from the 90% down to the 10%. The wdc classifier repeatedly runs 30 times for each random sampling rate, and here reports the mean performance among the 30 results for each random sampling rate. The detailed random sampling rate ( R rs ), training indexed token compressing rate ( R tc ), and performances are show ed in Table 6. Where, the R tc is a posteriori value after the training, and is defined as the ratio of the token number in the average of the 30 results, Tabl e 6 shows that the training indexed token compressing rate approximates a direct ratio of the random sampling rate, which proves that random-sampling-based token feature selection accord ing to the theoretical uselessness rate heuristic between 63% and 79% is also ef fective in the multi-category situation. This paper investigates the power law distribution and proposes a RSEB algorithm for the online multi-category TC problem. The experimental results show that the RSEB algorithm can obtain a comparable performan ce compared with other advanced machine learning TC algorithms in email spam filte ring application and Chinese web document performance at greatly reduced space-time co mplexity, which can satisfy the restriction of limited space and time for many practical large-scale applications. concept extends from email spam to inst ant messaging spam, short message service spam, and so on. A web document may belong to the hierarchical category or may have multiple category labels. Further research will concern the short message TC problem, the multi-hierarchy TC problem, and the multi-category multi-label TC problem. According to the ubiquitous power law, the RSEB algorithm is more general and can be easily transferred to other TC applications. 
