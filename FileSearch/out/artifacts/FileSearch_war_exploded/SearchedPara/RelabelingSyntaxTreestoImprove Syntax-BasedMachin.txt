 Recent work in statistical machine translation (MT) has sought to overcome the limitations of phrase-based models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ne y, 2004) by making use of syntactic information. Syntax-based MT of-fers the potential adv antages of enforcing syntax-moti vated constraints in translation and capturing long-distance/non-conti guo us dependencies. Some approaches have used syntax at the core (W u, 1997; Alsha wi et al., 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner , 2003; Hearne and Way, 2003; Melamed, 2004) while others have inte grated syn-tax into existing phrase-based frame works (Xia and McCord, 2004; Chiang, 2005; Collins et al., 2005; Quirk et al., 2005).

In this work, we emplo y a syntax-based model that applies a series of tree/string (xRS) rules (Gal-ley et al., 2004; Graehl and Knight, 2004) to a source language string to produce a tar get language phrase structure tree. Figure 1 exemplies the translation process, which is called a derivation , from Chinese into English. The source string to translate ( replaces the Chinese word (shaded) with the English NP-C police . Rule 2  X  then builds a VP over as the NP-C the gunman by rule 3  X  . Finally , rule 4  X  combines the sequence of NP-C VP . into an S , denot-ing a complete tree. The yield of this tree gives the tar get translation: the gunman was killed by police .
The Penn English Treebank (PTB) (Marcus et al., 1993) is our source of syntactic information, lar gely due to the availability of reliable parsers. It is not clear , howe ver, whether this resource is suitable, as is, for the task of MT . In this paper , we argue that the overly-general tagset of the PTB is problematic for MT because it fails to capture important grammati-cal distinctions that are critical in translation. As a solution, we propose methods of relabeling the syn-tax trees that effecti vely impro ve translation quality .
Consider the deri vation in Figure 2. The output translation has two salient errors: determiner/noun number disagreement (* this Turkish positions ) and auxiliary/v erb tense disagreement (* has demon-str ate ). The rst problem arises because the DT tag, which does not distinguish between singular and plural determiners, allo ws singular this to be used with plural NNS positions . In the second problem, the VP-C tag fails to communicate that it is headed by the base verb ( VB ) demonstr ate , which should pre-vent it from being used with the auxiliary VBZ has . Information-poor tags lik e DT and VP-C can be rela-beled to encourage more uent translations, which is the thrust of this paper . Figure 1: A deri vation from a Chinese sentence to an English tree.

Section 2 describes our data and experimental procedure. Section 3 explores dif ferent relabeling approaches and their impact on translation qual-ity. Section 4 reports a substantial impro vement in BLEU achie ved by combining the most effecti ve re-labeling methods. Section 5 concludes. Our training data consists of 164M+167M words of parallel Chinese/English text. The English half was parsed with a reimplementation of Collins' Model 2 (Collins, 1999) and the two halv es were word-aligned using GIZA++ (Och and Ne y, 2000). These three components  X  Chinese strings, English parse trees, and their word alignments  X  were inputs to our experimental procedure, which involv ed ve steps: (1) tree relabeling, (2) rule extraction, (3) de-coding, (4)
This paper focuses on step 1, in which the orig-inal English parse trees are transformed by one or more relabeling strate gies. Step 2 involv es extract-ing minimal xRS rules (Galle y et al., 2004) from the set of string/tree/alignment s triplets. These rules are then used in a CKY -type parser -decoder to trans-late the 878-sentence 2002 NIST MT evaluation test set (step 3). In step 4, the output 2,500-sentence best list is rerank ed using an trained on 800M words of English news text. In the nal step, we score our translations with 4-gram BLEU (Papineni et al., 2002).

Separately for each relabeling method, we ran these ve steps and compared the resulting BLEU score with that of a baseline system with no re-labeling. To determine if a BLEU score increase or decrease is meaningful, we calculate statistical signicance at 95% using paired bootstrap resam-pling (K oehn, 2004; Zhang et al., 2004) on 1,000 samples.

Figure 3 sho ws the results from each relabel-ing experiment. The second column indicates the change in the number of unique rules from the base-line number of 16.7M rules. The third column gives the BLEU score along with an indication whether it is a statistically signicant increase ( s ), a statisti-cally signicant decrease ( t ), or neither ( ? ) over the baseline BLEU score. Figure 3: For each relabeling method and variant, the impact on ruleset size and BLEU score over the baseline. The small tagset of the PTB has the adv antage of being simple to annotate and to parse. On the other hand, this can lead to tags that are overly generic. Klein and Manning (2003) discuss this as a prob-lem in parsing and demonstrate that annotating ad-ditional information onto the PTB tags leads to im-pro ved parsing performance. We similarly propose methods of relabeling PTB trees that notably im-pro ve MT quality . In the next two subsections, we explore relabeling strate gies that fall under two cate-gories introduced by Klein and Manning  X  internal annotation and external annotation. 3.1 Inter nal Annotation Internal annotation reveals information about a node and its descendants to its surrounding nodes (ancestors, sisters, and other relati ves) that is other -wise hidden. This is paramount in MT because the contents of a node must be understood before the node can be reliably translated and positioned in a sentence. Here we discuss two such strate gies: lexi-
Figure 4: Rules before and after lexicalization. calization and tag annotation. 3.1.1 Lexicalization
Man y state-of-the-art statistical parsers incor -porate lexicalization to effecti vely capture word-specic beha vior , which has pro ved helpful in our system as well. We generalize lexicalization to al-low a lexical item (terminal word) to be annotated onto any ancestor label, not only its parent.
Let us revisit the determiner/noun number dis-agreement problem in Figure 2 (* this Turkish po-sitions ). If we lexicalize all DT s in the parse trees, the problematic DT is relabeled more specically as DT_this , as seen in rule 2 0  X  in Figure 4. This also produces rules lik e 4 0  X  , where both the determiner and the noun are plural (notice the DT_these ), and , where both are singular . With such a ruleset, 2 0  X  could only combine with 4 00  X  , not 4 0  X  , enforcing the grammatical output this Turkish position .

We explored ve lexicalization strate gies, each tar geting a dif ferent grammatical cate gory . A com-mon translation mistak e was the improper choice of prepositions, e.g., responsibility to attac ks . Le xical-izing prepositions pro ved to be the most effecti ve lexicalization method ( LEX _ PREP ). We annotated a preposition onto both its parent ( IN or TO ) and its grandparent ( PP ) since the generic PP tag was often at fault. We tried lexicalizing all prepositions (vari-ant 1), the top 15 most common prepositions (variant 2), and the top 5 most common (variant 3). All gave statistically signicant BLEU impro vements, espe-cially variant 2.

The second strate gy was DT lexicalization (
LEX _ DT ), which we encountered pre viously in Fig-ure 4. This addresses two features of Chinese that are problematic in translation to English: the infre-quent use of articles and the lack of overt number in-dicators on nouns. We lexicalized these determiners: the , a , an , this , that , these , or those , and grouped to-gether those with similar grammatical distrib utions ( a / an , this / that , and these / those ). Variant 1 included all the determiners mentioned abo ve and variant 2 was restricted to the and a / an to focus only on arti-cles. The second slightly impro ved on the rst.
The third type was auxiliary lexicalization (
LEX _ AU X ), in which all forms of the verb be are annotated with _be , and similarly with do and have . The PTB purposely eliminated such distinc-tions; here we seek to reco ver them. Ho we ver, auxiliaries and verbs function very dif ferently and thus cannot be treated identically . Klein and Man-ning (2003) mak e a similar proposal but omit do . Variants 1, 2, and 3, lexicalize have , be , and do , re-specti vely . The third variant slightly outperformed the other variants, including variant 4, which com-bines all three.
 The last two methods are dra wn directly from Klein and Manning (2003). In CC lexicalization (
LEX _ CC ), both but and &amp; are lexicalized since these two conjunctions are distrib uted very dif fer -ently compared to other conjunctions. Though help-ful in parsing, it pro ved detrimental in our system. In % lexicalization ( LEX _%), the percent sign (%) is given its own PCT tag rather than its typical NN tag, which gave a statistically signicant BLEU increase. 3.1.2 Tag Annotation
In addition to propagating up a terminal word, we can also propagate up a nonterminal, which we call tag annotation . This partitions a grammatical cat-egory into more specic subcate gories, but not as ne-grained as lexicalization. For example, a VP headed by a VBG can be tag-annotated as VP_VBG to represent a progressi ve verb phrase.

Let us once again return to Figure 2 to address the auxiliary/v erb tense disagreement error (* has demonstr ate ). The auxiliary has expects a VP-C , per -mitting the bare verb phrase demonstr ate to be incor -rectly used. Ho we ver, if we tag-annotate all VP-C s, rule 6  X  would be relabeled as VP-C_VB in rule 6 0  X  and rule 7  X  as 7 0  X  in Figure 5. Rule 6 0  X  can no longer
Figure 5: Rules before and after tag annotation. join with 7 0  X  , while the variant rule 6 00  X  can, which produces the grammatical result has demonstr ated .
We noticed man y wrong verb tense choices, e.g., gerunds and participles used as main sentence verbs. We resolv ed this by tag-annotating every VP and VP-with its head verb ( TAG _ VP ). Note that we group VBZ and VBP together since the y have very similar grammatical distrib utions and dif fer only by number . This strate gy gave a healthy BLEU impro vement. 3.2 Exter nal Annotation In addition to passing information from inside a node to the outside, we can pass information from the external environment into the node through ex-ternal annotation . This allo ws us to mak e transla-tion decisions based on the conte xt in which a word or phrase is found. In this subsection, we look at three such methods: sisterhood annotation, parent annotation, and complement annotation. 3.2.1 Sisterhood Annotation
The single most effecti ve relabeling scheme we tried was sisterhood annotation . We annotate each nonterminal with #L if it has any sisters to the left, #R if any to the right, #LR if on both sides, and noth-ing if it has no sisters. This distinguishes between words that tend to fall on the left or right border of a constituent (often head words, lik e NN#L in an NP or IN#R in a PP ), in the middle of a constituent (of-ten modiers, lik e JJ#LR in an NP ), or by themselv es Figure 6: A bad translation xable by sisterhood or parent annotation. (often particles and pronouns, lik e RP and PRP ). In our outputs, we frequently nd words used in posi-tions where the y should be disallo wed or disf avored.
Figure 6 presents a deri vation that leads to the ungrammatical output * deeply love she . The sub-ject pronoun she is incorrectly preferred over the ob-ject form her because the most popular NP-C trans-lation for is she . We can sidestep this mistak e through sisterhood-annotatio n, which yields the re-labeled rules 3 0  X  and 4 0  X  in Figure 7. Rule 4 0  X  ex-pects an NP-C on the right border of the constituent (
NP-C#L ). Since she never occurs in this position in the PTB, it should never be sisterhood-annota ted as an NP-C#L . It does occur with sisters to the right, which gives the NP-C#R rule 3 0  X  . The object NP-C her , on the other hand, is frequently rightmost in a constituent, which is reected in the NP-C#L rule 3 00  X  . Using this rule with rule 4 0  X  gives the desired result deeply love her .

We experimented with four sisterhood annotation ( SISTERHOOD ) variants of decreasing comple xity . The rst was described abo ve, which includes right-most ( #L ), leftmost ( #R ), middle ( #LR ), and alone (no annotation). Variant 2 omitted #LR , variant 3 kept only #LR , and variant 4 only annotated nodes with-out sisters. Variants 1 and 2 produced the lar gest gains from relabeling: 1.27 and 0.85 BLEU points, respecti vely .
 Figure 7: Rules before and after sisterhood annota-tion.

Figure 8: Rules before and after parent annotation. 3.2.2 Parent Annotation
Another common relabeling method in parsing is par ent annotation (Johnson, 1998), in which a node is annotated with its parent' s label. Typically , this is done only to nonterminals, but Klein and Man-ning (2003) found that annotating preterminals as well was highly effecti ve. It seemed lik ely that such conte xtual information could also benet MT .
Let us tackle the bad output from Figure 6 with parent annotation. In Figure 8, rule 4  X  is relabeled as rule 4 0  X  and expects an NP-C X VP , i.e., an NP-C with a VP parent. In the PTB, we observ e that the NP-C she never has a VP parent, while her does. In fact, the most popular parent for the NP-C her is VP , while the most popular parent for she is S . Rule 3  X  is relabeled as the NP-C X S rule 3 0  X  and her is expressed as the NP-C X VP rule 3 00  X  . Only rule 3 00  X  can partner with rule which produces the correct output deeply love her .
We tested three variants of parent annota-tion ( PARENT ): (1) all nonterminals are parent-annotated, (2) only S nodes are parent-annotated, and (3) all nonterminals are parent-and grandparent-annotated (the annotation of a node' s parent' s par -ent). The rst and third variants yielded the lar gest ruleset sizes of all relabeling methods. The second variant was restricted only to S to capture the dif-ference between top-le vel clauses ( S X T OP ) and em-bedded clauses (lik e S X S-C ). Unfortunately , all three variants turned out to be harmful in terms of BLEU. 3.2.3 Complement Annotation
In addition to a node' s parent, we can also anno-tate a node' s complement. This captures the fact that words have a preference of taking certain comple-ments over others. For instance, 96% of cases where the IN of tak es one complement in the PTB, it tak es NP-C . On the other hand, although never tak es NP-C but tak es S-C 99% of the time.

Consider the deri vation in Figure 9 that results in the bad output * postponed out May 6 . The IN out is incorrectly allo wed despite the fact that it almost never tak es an NP-C complement (0.6% of cases in the PTB). A way to restrict this is to annotate the 's complement. Complement-annotated versions of rules 2  X  and 3  X  are given in Figure 10. Rule  X  is relabeled as the IN/PP-C rule 2 0  X  since PP-C is the most common complement for out (99% of the time). Since rule 3 00  X  expects an IN/NP-C , rule 2 0  X  is disqualied. The preposition from (rule 2 00  X  ), on the other hand, frequently tak es NP-C as complement (82% of the time). Combining rule 2 00  X  with rule 3 0  X  ensures the correct output postponed from May 6 .
Complement-annotating all IN tags with their complement if the y had one and only one comple-ment ( COMP _ IN ) gave a signicant BLEU impro ve-ment with only a modest increase in ruleset size. 3.3 Remo val of Parser Annotations Man y parsers, though trained on the PTB, do not preserv e the original tagset. The y may omit func-tion tags (lik e -TMP ), indices, and null/gap elements or add annotations to increase parsing accurac y and pro vide useful grammatical information. It is not obvious whether these modications are helpful for MT , so we explore the effects of remo ving them.
The statistical parser we used mak es three re-labelings: (1) base NP s are relabeled as NPB , (2) argument nonterminals are suf x ed with -C , and (3) subjectless sentences are relabeled from S to SG . We tried remo ving each annotation indi vidually ( signicantly dropped the BLEU score. This leads us to conclude these parser additions are helpful in MT . Figure 9: A bad translation xable by complement annotation.
 Figure 10: Rules before and after complement anno-tation. To maximize the benet of relabeling, we incorpo-rated ve of the most promising relabeling strate gies into one additi ve system: LEX _%, LEX _ DT variant Figure 11: Relabelings in the additi ve system and their indi vidual/cumulati ve effects over the baseline. 2, variant 1. These relabelings contrib uted to a 2.3 ab-solute (11.6% relati ve) BLEU point increase over the baseline, with a score of 22.38. Figure 11 lists these relabelings in the order the y were added. We have demonstrated that relabeling syntax trees for use in syntax-based machine translation can sig-nicantly boost translation performance. It is na X v e to assume that linguistic resources can be immedi-ately useful out of the box, in our case, the Penn Treebank for MT . Rather , we tar geted features of the PTB tagset that impair translatability and proposed relabeling strate gies to overcome these weaknesses. Man y of our ideas effecti vely raised the BLEU score over a baseline system without relabeling. Finally , we demonstrated through an additi ve system that re-labelings can be combined together to achie ve an even greater impro vement in translation quality . This research was supported in part by NSF grant IIS-0428020. We would lik e to thank Gre g Lang-mead, Daniel Marcu, and Wei Wang for helpful comments. This paper describes work conducted while the rst author was at the Uni versity of South-ern California/Information Sciences Institute.
