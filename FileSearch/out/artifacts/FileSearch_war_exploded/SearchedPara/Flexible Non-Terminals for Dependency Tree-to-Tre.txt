 Translation is most commonly performed by splitting an input sentence into manageable parts, translating these segments, then arrang-ing them in an appropriate order. The first two steps have roughly the same difficulty for close and distant language pairs, however the reorder-ing step is considerably more challenging for lan-guage pairs with dissimilar syntax. We need to be able to make linguistic generalizations, such as learning to translate between SVO and SOV clauses and converting post-modifying preposi-tional and pre-modifying postpositional phrases (Quirk et al., 2005). Such generalizations often require syntactically motivated long-distance re-ordering.

The first approaches to reordering were based on linear distortion (Koehn et al., 2003), which models the probability of swapping pairs of phrases over some given distance. The linear distance is the only parameter, ignoring any contextual information, however this model has been shown to work well for string-to-string translation. Linear reordering was improved with lexical distortion (Tillmann, 2004), which characterizes reordering in terms of type (mono-tone, swap, or discontinuous) as opposed to dis-tance. This approach however is prone to spar-sity problems, in particular for distant language pairs.

In order to improve upon linear string-based approaches, syntax-based approaches have also been proposed. Tree-to-string translation has been the most popular syntax-based paradigm in recent years, which is reflected by a number of reordering approaches considering source-only syntax (Liu et al., 2006; Neubig, 2013). One particularly interesting approach is to project source dependency parses to the target side and then learn a probability model for reordering children using features such as source and target head words (Quirk et al., 2005).

While tree-to-tree translation (Graehl and Knight, 2004; Cowan and Collins, 2006; Chiang, 2010) has been somewhat less popular than tree-to-string translation, we believe there are many benefits of considering target-side syntax. In particular, reordering can be defined naturally with non-terminals in the target-side grammar. This is relatively simple when the target struc-ture of rules is restricted to  X  X ell-formed X  depen-dencies (Shen et al., 2008), however in this pa-per we consider more general rules with flexible non-terminal insertion positions. Dependency tree-to-tree translation begins with the extraction of translation rules from a bilin-gual corpus that has been parsed and word aligned. Figure 1 shows an example of three rules that can be extracted from aligned and parsed sentence pairs. In this paper we consider rules similar to previous work on tree-to-tree de-pendency MT (Richardson et al., 2014).

The simplest type of rule, containing only ter-minal symbols, can be extracted trivially from aligned subtrees (see rules 2 and 3 in Figure 1). Non-terminals can be added to rules (see rule 1 in Figure 1) by omitting aligned subtrees and replacing on each side with non-terminal sym-bols. We can naturally express phrase reorder-ing as the source/target-side non-terminals are aligned.

Decoding is performed by combining these rules to form a complete translation, as shown in Figure 2. We are able to translate part of the sentence with non-ambiguous reordering ( X  X ead a magazine X ), as we can insert  X   X  X  X   X  a maga-zine X  into the rule  X  [X] X  X  X  X  X   X  read [X] X .

We cannot however decide clearly where to insert the rule  X   X  X  X   X  yesterday X  as there is no matching non-terminal in the rule containing its parent in the input sentence ( X   X  X  X  X   X ). We use the term floating to describe words such as  X  X es-terday X  in this example, i.e. for an input subtree matched to the source side of a rule, children of the input root that are not contained in the source side of the rule as terminals and cannot be inserted using fixed-position non-terminals in the rule.

Previous work deals with this problem by ei-ther using simple glue rules (Chiang, 2005) or limiting rules in a way to avoid isolated float-ing children (Shen et al., 2008). For example, it is possible to disallow the first rule in Figure 1 when translating a sentence such as that in Fig-ure 2 with uncovered children (in this case the word  X  X esterday X ). This method greatly reduces the expressiveness and flexibility of translation rules.

In our generalized model, we allow any num-ber of terminals and non-terminals and permit arbitrarily many floating children in each rule. To our knowledge this is the first study to take this more comprehensive approach.

Note that in the case of constituency-based tree-to-tree translation it is possible to binarize the input tree and therefore gluing floating chil-dren becomes simpler, as we only have to choose between pre-insertion and post-insertion. In the dependency case it is in general much more dif-ficult because we must order an arbitrarily large group of children sharing a common head. In this paper we propose flexible non-terminals in order to create generalized tree-to-tree trans-lation rules that can overcome the problems de-scribed in the previous section. Rather than fixed insertion positions for child nodes, we in-stead consider multiple possible insertion posi-tions and give features to each position. These are stored in a compact representation allowing for efficient decoding.

We define flexible non-terminals as non-terminals with multiple possible insertion posi-tions and associated features. During decoding we select the most promising insertion position for each non-terminal. 3.1 Rule Augmentation As is standard practice in phrase-based SMT, before translation we filter translation rules to those relevant to the input sentence. At this time, for each accepted rule we check the input sentence for floating children, and flexible non-terminals are added for each floating child.
We allow all insertion positions between the children (along with their descendants) of the target-side head for each floating child, includ-ing insertion before the first child and after the last child. We do not allow insertion positions between deeper descendants of the head to avoid non-projective dependencies. See Figure 3 for an example of allowed/disallowed positions.

Features are then set for each insertion po-sition and these are used to determine the best insertion position during decoding (see Sec-tion 3.2). Figure 4 shows an example of the pro-posed rule augmentation. 3.2 Features In previous work reordering is mostly decided by the combination of a standard distortion model and language model to score possible insertion positions. We instead consider the following four features and combine them during decoding to find the most appropriate insertion positions for floating children. All features are real numbers between 0 and 1. 3.2.1 Insertion Position Features
We first define a set of features to estimate the likelihood of each insertion position for some given non-terminal. The features for inserting the translation f of a source phrase into the target-side e of a rule at insertion position i are defined as follows, for surface forms ( S ) and POS tags ( P ):  X  Reordering probability:  X  Marginalized over target-side:  X  Marginalized over source-side:
The probabilities P ( i | X ) are calculated by counting insertions of X in each position i across the whole training corpus (aligned and parsed bitext). The exact formula is given below, for position i ( X is one of { f } , { e } or { f, e } ):
Instead of applying smoothing, in order to re-duce sparsity issues we use both the full proba-bility P ( i | f, e ) and also probabilities marginal-ized over the source/target phrases. We also consider both probabilities trained on surface forms ( S ) and POS tags ( P ).

While traditional models use linear distance for i , this is impractical for long-distance re-ordering. Instead we restrict insertion types i to one of the following 6 types: first-pre-child, mid-pre-child, final-pre-child, first-post-child, mid-post-child, and final-post-child. These corre-spond to the first (first), last (final) or central (mid) children on the left (pre) or right (post) side of the parent word. We found this was more effective than using either linear distance or a binary (pre/post) position type. 3.2.2 Relative Position Feature
We also consider a relative position, or  X  X wap-ping X  feature, inspired by the swap operation of classic lexical distortion (Tillmann, 2004).
Let T be the children of the root word of the target-side of a rule. We also include in T a pseudo-token M splitting the left and right chil-dren of the target-side root to differentiate be-tween pre-insertion and post-insertion.

We first learn a model describing the proba-bility of the translation of input phrase I ap-pearing to the left ( P L ( I, t ) ) or right ( P R ( I, t ) of word t in the target-side of a translation rule. The probabilities are calculated by counting oc-currences of I being translated to the left/right sides of t over the aligned and parsed training bitext.

The relative position feature is calculated by considering the relative position of the transla-tion of I with all the target-side root children T . For each insertion position i , let T i,L be the t  X  T to the left of position i and T i,R the t  X  T to the right of position i . Then we have:
P ( i | I, T ) = 3.2.3 Left/Right Attachment Preference
We also set an attachment direction prefer-ence feature for each rule, specifying whether we prefer to insert the rule as a left child or right child of the root of a parent rule.

The attachment preference is determined by the position of the target-side of the rule in the target-side of the parallel sentence from which it was extracted. For example, in Figure 1 the rule  X   X  X  X   X  yesterday X  was extracted from a par-allel sentence in which  X  X esterday X  was a right-side child of its head ( X  X aw X ), so we set the at-tachment preference to  X  X ight X . In cases when we cannot determine the attachment preference (for example  X  X ead X  in the first rule in Figure 1), because it is the sentence root), we arbitrarily choose  X  X ight X . 3.2.4 Unambiguous Insertion Preference
In cases where we have a single unambiguous insertion position for a non-terminal (e.g. [X] in Figure 4), we set an additional binary feature to the value 1 (otherwise 0) to specify that this position is unambiguous. We found that a large positive weight is almost always given to this fea-ture, which is to be expected as we would prefer to use fixed non-terminals if possible. We set all features related to insertion position choice to the maximum value (1). 3.3 Decoding The flexible non-terminals that we are propos-ing can lead to some interesting challenges when it comes to decoding. A naive approach is to expand each translation rule containing flexible non-terminals into a set of  X  X imple X  rules with fixed non-terminals, and then apply classic de-coding with cube-pruning.
However, this can be quite inefficient in prac-tice. Due to the combinatorial aspect, a single rule can expand into a very large number of sim-ple rules. It is common for our translation rules to have more than four flexible non-terminals, each with more than four possible insertion posi-tions. Such rules will already generate hundreds of simple rules. In the most extreme cases, we may encounter rules having more than ten flex-ible non-terminals, leading to the generation of many millions of simple rules. This explosion of rules can lead to impractical decoding time and memory usage.

It is therefore important to make use of the compact encoding of many simple rules provided by the concept of flexible non-terminals in the decoding process itself. We use the decoding approach of right-hand lattices (Cromi X res and Kurohashi, 2014), an efficient way of encoding many simple rules. The idea is to encode the translation rules into a lattice form, then use this lattice to decode efficiently without the need to expand the flexible non-terminals explicitly.
Figure 5 shows how the concept of flexible non-terminals can be efficiently encoded into lat-tice form. The top half shows a target-side tree translation rule with flexible non-terminals X1, X2, X3 and X4 allowed to be inserted at any position that is a child of the word  X  X  X , with the constraint that X1 comes before X2 and that X2 comes before X3. X5 is another flexible non-terminal that will be a child of the word  X  X  X . The lower half shows a lattice compactly encoding all the possible combinations of non-terminal posi-tions. Each path from the top-left to the bottom right in this lattice represents a choice for the insertion positions of the non-terminals. For ex-ample, the path marked with a dotted line rep-resents the flattened sequence  X  X  c X1 X2 a X3 X4 d e f X5 g X . The lattice form has only 48 edges, while an explicit enumeration of all com-binations of insertion positions for the flexible non-terminals would force the decoder to con-sider 8 C 4  X  3  X  12 = 2520 edges.

The insertion position features described above are added to the edges of the lattice. They are combined alongside the standard set of fea-tures, such as word penalty and language model Train 3M 3M 676K 676K Dev 1790 1790 2123 2123 Test 1812 1812 2171 2171 score, using a standard log-linear model. The weights for the reordering features are tuned to-gether with the standard features. 4.1 Data and Settings We performed translation experiments on four distant language pairs, Japanese X  X nglish (JA X  EN), English X  X apanese (EN X  X A), Japanese X  Chinese (JA X  X H) and Chinese X  X apanese (ZH X  JA), from the Asian Scientific Paper Excerpt Corpus (ASPEC) 1 . The data was split into training, development and test folds as shown in Table 1.

Our experiments were conducted using a state-of-the-art dependency tree-to-tree frame-work KyotoEBMT (Richardson et al., 2014). 77.11  X  29.42  X  80.44  X  35.37  X  81.33  X  76.85  X  29.48  X  80.43  X  35.57  X  81.79  X  77.01  X  29.64  X  80.65  X  35.71  X  82.05  X  76.93  X  29.78  X  80.51  X  35.81  X  81.95  X  Experiments were performed with the default settings by adding the proposed non-terminal reordering features to the rules extracted with the baseline system. We used lattice-based de-coding (Cromi X res and Kurohashi, 2014) to sup-port multiple non-terminal insertion positions and default tuning using, k -best MIRA (Cherry and Foster, 2012). Dependency parsing was performed with: KNP (Kawahara and Kuro-hashi, 2006) (Japanese), SKP (Shen et al., 2012) (Chinese), NLParser (Charniak and Johnson, 2005) (English, converted to dependencies with hand-written rules). Alignment was performed with Nile (Riesa et al., 2011) and we used a 5-gram language model with modified Kneser-Ney smoothing built with KenLM (Heafield, 2011). 4.2 Evaluation As our baseline ( X  X aseline X ), we used the de-fault tree-to-tree settings and features of Ky-otoEBMT, allowing only fixed-position non-terminals. We dealt with floating children not covered by any other rules by adding glue rules similar to those in hierarchical SMT (Chiang, 2005), joining floating children to the rightmost slots in the target-side parent. For reference, we also show results using Moses (Koehn et al., 2007) with default settings and distortion limit set to 20 ( X  X oses X ).

The proposed system ( X  X lexible X ) adds flex-ible non-terminals with multiple insertion po-sitions, however we do not yet add the inser-tion choice features. This means that the in-sertion positions are in practice chosen by the language model. Note that we do not get a substantial hit in performance by adding the flexible non-terminals because of their compact lattice representation. The systems  X +Pref X ,  X +Pref+Ins X  and  X +Pref+Ins+Rel X  show the re-sults of adding insertion choice position features (left/right preference, insertion position choice, relative position choice).

We give translation scores measured in BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010), which is designed to reflect quality of translation word order more effectively than BLEU. The translation evaluation is shown in Table 2. The experimental results showed a significantly positive improvement in terms of both BLEU and RIBES over the baseline tree-to-tree system. The baseline system uses fixed non-terminals and is competitive with the most popular string-to-string system (Moses).

The extensions of the proposed model (adding a variety of features) also all showed signifi-cant improvement over the baseline, and ap-proximately half of the extended settings per-formed significantly better than the core pro-posed model. It is unclear however which of the extended settings is the most effective for all language pairs. There are a number of fac-tors such as parse quality, corpus size and out-of-vocabulary occurrence that could affect the potential value of these features. Furthermore, Japanese is strongly left-branching (head-final), so the left/right preference distinction is likely to be less useful than for English and Chinese, which contain both left-branching and right-branching structures.

Compared to the baseline, the flexible non-terminals gave around a 1.2 X 1.9 BLEU improve-ment at the cost of only a 30% increase in de-coding time (approximately 2.04 vs. 2.66 sec-onds per sentence). This is made possible by the compact non-terminal representation com-bined with lattice decoding. 5.1 Non-Terminal Matching Analysis We found that roughly half of all our trans-lation rules were augmented with flexible non-terminals, with one flexible non-terminal added per rule on average. This led to roughly half of non-terminals having flexible insertion posi-tions. The decoder chose to use ambiguous in-sertion positions between 30% X 60% of the time (depending on language pair), allowing for many more new translation hypotheses than the base-line system. For detailed results, see Table 3. 5.2 Translation Examples The following translation is an example of an im-provement achieved by using the proposed flex-ible non-terminals. There were multiple word order errors in the baseline translation that im-peded understanding, and these have all been corrected.  X  Input:  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  Reference: The pressure difference and  X  Baseline: We have measured the pressure  X  Proposed: The pressure difference and
There are also cases where the proposed model decreases translation quality. In the ex-ample below, the proposed system output was selected by the decoder since it had a higher language model score than the baseline output, despite having incorrect word order. The in-correct translation was made available by the increased flexibility of the proposed model, and selected because the LM feature had a higher impact than the insertion position features.  X  Input:  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  Reference: The characteristics of R5 ver- X  Baseline: The R5 version of this software  X  Proposed: This software design docu-In this paper we have proposed flexible non-terminals for dependency tree-to-tree transla-tion. We plan to continue working on feature design for insertion position choice, and in the future would like to consider using neural net-works for learning these features. We believe that it is important to continue to explore ap-proaches that exploit more general target-side syntax, faithful to the tree-to-tree translation paradigm.

Flexible non-terminals allow multiple inser-tion positions to be expressed compactly and selected with features based on both source and target syntax. We have shown that a significant improvement in BLEU and RIBES scores can be gained by using the proposed model to in-crease the generality of dependency tree-to-tree translation rules.
 We would like to thank the anonymous reviewers for their feedback.
