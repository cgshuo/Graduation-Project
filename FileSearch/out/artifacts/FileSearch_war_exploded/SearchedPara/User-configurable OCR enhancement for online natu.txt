 ORIGINAL PAPER Andy Downton  X  Jingyu He  X  Simon Lucas Abstract The creation of structured digital libraries from paper-based archives is an area of growing demand in many scientific and cultural fields, and is not satis-fied either by off-the-shelf OCR or commercial form-processing systems. This paper describes and evaluates a configurable archive construction system, which inte-grates document image pre-processing and analysis with text post-processing tools and a standard OCR package to meet digital archiving requirements. The prototype system is currently being used in conjunction with the UK Natural History Museum to help convert more than 500,000 cards of Lepidoptera (Butterflies and Moths) and Coleoptera (Beetles) to searchable digital archives. Evaluation results covering different aspects of the sys-tem from card scanning to overall word recognition rates for different database fields are summarised for two datasets comprising over 5,000 cards selected from different parts of these archives. First-pass end-to-end word recognition rates of 70 X 90% are reported for key data fields, subject to availability of suitable electronic dictionaries. Further validation and correction is sup-ported through web-editing of the online digital archive. Keywords Document analysis  X  Digital archive  X  OCR 1 Introduction Digital archive construction from historic paper archives is a major image analysis application of interest both for cultural and scientific purposes. For example, 17 papers out of 53 presented at the most recent Document Anal-ysis Systems workshop were concerned with analysis of historical documents [1].

Archivedocuments areoftenstoredinwell-structured taxonomies (e.g. libraries, scientific specimen indexes and censuses) where the structure extends across the index as well as within the layout of each document. Documents are recorded using text which challenges off-the-shelf OCR [2], not only because of poor quality and/or decayed typescript or handwriting, but also be-cause standard commercial OCR systems cannot infer the data structure inherent within the records without human guidance. Similarly, although some commercial form-processing OCR systems exist, these normally pro-cess fixed-format pre-defined forms designed for OCR usingbackgrounddrop-out colours and/or tabular guide-lines to maximise performance, rather than arbitrary pre-existing document archives. 1.1 System concept To address archive applications, the user-configurable archive document processing system described in this paper integrates image analysis and text post-processing tools with a configurable commercial OCR package, to generate text content that can be fed direct into a target online database. User configurability is essen-tial to allow the system to be re-targeted to process different archives, and also variable layouts within the same archive. The pattern recognition aspects of the system (ranging from colour segmentation, to document structure classification, to stamp identification and re-moval) are uniformly implemented using a fuzzy classi-fication scheme which is parameterised within the user interface. OCR performance is optimised using config-urable user dictionaries linked to semantically labelled text fields identified using document image analysis. The raw output of the OCR system corresponding to each labelled sub-field of the document image is then post-processed using a regular expression engine to convert it into the required database format.

Our system has been developed in conjunction with the UK Natural History Museum (NHM), and hence has largely been evaluated on their archive data, although it is also intended to be more widely appli-cable to other structured document archives. The work reported here has mainly worked with the index to world species of Butterflies and Moths (Lepidoptera), which contains 290,886 index cards, and more recently the index of Beetles (Coleoptera), of similar size. 1.2 NHM card archives In addition to 68 million biological specimens, The NHM houses global index card archives of taxonomic data for many important groups of organisms extant and ex-tinct. These card archives represent a comprehensive inventory of their scientific names and associated biblio-graphical data, for which no published global catalogue currently exists. Archives at the NHM are recorded in card indexes, which contain bibliographical data and other information for one scientific name on each card, laid out in a standardised format (Fig. 1) for each archive. However, different archives may be subject to very different recording conventions. Information is usually type-written, but a minority of cards are entirely hand-written and hand-written annotations are common.
Cards are ordered within the index: first, according to higher classification (superfamily, family, subfamily, tribe); second, alphabetically by genus; third, alphabeti-cally within each genus by species; and fourth, alphabet-ically within each species by subspecies (hence the card sequence implies several database fields which are not explicitly included on every card 1 ). Cards with names that are no longer in use (e.g. synonyms of current species names) are arranged alphabetically by scientific name following the card for the currently valid name.
The importance of the NHM Lepidoptera card index is demonstrated by the fact that taxonomic catalogues for several groups of Lepidoptera have been produced largely based on data from it (e.g. for Noctuidae [3], Geometridae [4], and the Butterflies &amp; Moths of the World: Generic Names &amp; their Type-species , providing a full list of genus names online [5]). More generally, access to such historic data is now increasingly required to support worldwide research in areas such as biodiver-sity and climate change, providing a strong motivation to digitise paper archives. 1.3 Card scanning Card archives are scanned using a SEAC Banche RDS-6000 bank cheque scanner (Fig. 2), modified by the addition of a customised software interface which allows configuration of the scanning process, to build a large im-age archive from a series of batch scans which may take place over days or weeks. Using this system, over 0.5 million Lepidoptera (Moths and Butterflies) and Cole-optera (Beetles) cards have so far been scanned at the NHM. The scanner has the capability to scan both sides of a card simultaneously in colour and/or monochrome at 200 pixels/in. resolution at a rate of about 1 card/s, and stores the resulting images in JPEG format. 2 It is also able to print a reference file number on the back of each card for cross-checking against the electronic file archive. 1.4 Paper structure Subsequent sections of this paper describe each part of the overall archive construction system and then eval-uate its end-to-end system performance. Section 2 pro-vides a system overview. Section 3 describes alternative pre-processing algorithms used to binarize the scanned card images, Section 4 then describes the document im-age analysis method used to extract and semantically label each independent text block in an archive card image. Results from this stage are fed to the commer-cial OCR system, with performance for each separate semantic field optimised by the use of field-specific dic-tionaries. Section 5 explains how text is post-processed using regular expression matching, so that it can be fed directly into an NHM database, accessible on the Inter-net [6].

Becausedifferent cardarchives recordvaryingseman-tic information with different layouts, user configuration of the system is required before commencing analysis of an archive, to define the card template layout(s), identify dictionaries to be used with the OCR system, and spec-ify any text post-processing required to interface the system to a target database. The system is configured using fuzzy templates specified using a graphical user interface, and described in detail in Section 6. Section 7 briefly describes the Access database and Web browser to which the output of the system interfaces.
Evaluation is required not only of each individual component of the system, but also of its end-to-end per-formance from card scanning to database input. This is presented in Section 8, followed by a brief discussion of other issues in Section 9 and conclusions in Section 10. 2 System overview The overall system (Fig. 3) consists of four main compo-nents, pre-processing, document analysis, OCR (using a commercial OCR engine) and post-processing, imple-mented in a mixture of Tcl/tk and C++ on top of an existing document analysis system framework [7]. Pre-processing reads the original JPEG archive document images, and converts them from either colour or grey level into binary for semantic labelling purposes. Doc-ument analysis then segments and semantically labels important text fields. The output normally consists of labeled text field colour or grey-level sub-images in the system X  X  internal image format (PNM). However, pre-processing can also be recalled after Document Anal-ysis so that the labelled text field sub-images can be converted into binary under system control rather than relying on the OCR X  X  internal binarization algorithm. The OCR system recognises labelled images text-field by text-field and converts them into raw text, which is further processed by regular expression post-processing to meet final database input requirements. The compo-nents are integrated into a complete archive batch pro-cessing system with the user interface shown in Fig. 4. 3 Document image pre-processing Pre-processing includes two independent operations, binarization and colour segmentation, either or both of which can be applied to input images. Five alternative algorithms have been implemented within the system for binarization from grey-level images: global thres-holding, Niblack X  X  algorithm [8], Sauvola X  X  algorithm [9], adaptive Niblack, and adaptive Sauvola. The first three of these are standard algorithms, while the last two are novel developments reported in [10]. A comparative performance evaluation of all these algorithms, and also the internal binarization algorithm used by the OCR system, established that the best OCR performance for the NHM archive dataset was achieved by our adaptive Niblack algorithm. All subsequent system evaluation re-sults reported in this paper therefore use this segmenta-tion method.

Colour segmentation [11] is used to separate an input colour image into monochrome colour layers accord-ing to a predefined colour map (a collection of colour clusters, which represent the distinct colours found in the document). Each colour layer may be associated with one or more distinct document fields (e.g. a docu-ment stamp, which may need to be removed, see [12]). After colour cluster identification, all the foreground colour layers can be projected together to generate a background-free image, which can then be binarized by mapping all foreground colours to black (and optionally eroding to reduce stroke thickness). In our evaluation of binarization methods [10], we compared this colour classification route to binarization with the grey-level binarization methods described above, but found no improvement in overall system performance. However, colour segmentation is still a useful component of our system for background artefact removal and to allow identification of specific colour layers where they repre-sent a particular semantic component of the document, e.g. a stamp or genus name. 4 Document image analysis and OCR 4.1 Document image analysis (DIA) The format of archive index cards consists of several independent blocks of text, and each block contains one or more logically related text fields. Blocks retain a fairly consistent mutual layout over a complete archive, but the layout of text fields within each block is not strictly fixed. Nor are there any tabular guidelines defining fixed block boundaries. The X X  X  cuts algorithm [13] is there-fore an appropriate segmentation algorithm for this class of document image structure. Pixel smearing [14], with a threshold sufficient to join adjacent text characters but not adjacent horizontal words or vertical lines, is first applied as a pre-processing non-linear low-pass filter to each archive card image. The X X  X  cuts algorithm then extracts and stores the contents of each index card into a hierarchical tree structure (the so-called X X  X  tree), consisting of text blocks, lines and words.

In addition to segmentation, DIA labels each seg-mented region (as shown in Fig. 1), in this case based upon a template layout pre-registered during system configuration for the NHM archive format. Labelled im-age fields allow the OCR system to be configured with field-specific dictionaries, and raw text output from the OCR to be fed to the correct database field. They also make it possible to automatically remove any redundant field from the document image if desired (replacing it with average background pixels from the surrounding region). In fact, the only redundant foreground in Fig. 1 is the  X  X riginal Spelling ...  X  stamp, which may appear anywhere on the image with any orientation, but with a fixed overall format. As part of our archive process-ing system, a special tool was developed to remove such stamps, based upon fuzzy matching of global features of the stamp [12]. The features used are relative corner angles, distances and font sizes of the outer boundary of the stamp. 4.2 OCR and word recognition The OCR used in the proposed system is a commer-cial product, Abbyy FineReader 6.0, which is currently regarded as one of the best available solutions for print and type-writer text recognition. Since it is designed for stand-alone use, it includes its own internal image pro-cessing (e.g. binarization) integrated with OCR, but can also accept pre-processed images in a variety of different image formats including binary. We used it as middle-ware working in combination with the other components of our system. For example, Fig. 5 shows texts on the card that have been extracted and labelled into five classes of images, Index , Species , Author , Reference and Location . To recognise the class Author , a specific name dictionary (provided by NHM) can be added to the default OCR English dictionary. When the input class is changed, a different specific dictionary is used.

A few other OCR settings also need to vary accord-ing to the image field being processed. As most text is typewritten, the default print type is normally set as  X  X ypewriter X  and layout detect as  X  X utodetect lay-out X . However, in this application, the year of publica-tion within the Reference class is a specific searchable database field, and hence needs to be extracted from the remainder of the reference using regular expression post-processing. Initial evaluation showed that the OCR system performs poorly on numeric fields with the print type set to  X  X ypewriter X , but somewhat better when it is set as  X  X utodetect X .

The OCR output of the Abbyy middleware is raw text which is saved into separate text files for each text sub-image of the overall card image, since all sub-images of a single field (e.g. species name) are processed as a single batch with the same user-specified dictionary and other settings. The subsequent post-processing stage then reassembles all the different raw texts derived from a single card image into a set of labelled text fields suit-able for database insertion. 5 Post-processing The purpose of text post-processing is to generate database-oriented text strings for input to the online database from the raw text output by the OCR. For example, the NHM online database only needs the year of publication in the recognized reference to be stored as a search key; other text in the reference can be ig-nored. Another example is the author name, where the database requires complete author names, but abbre-viated author names (terminated with a full stop) are frequently found in the original images (e.g. the au-thor  X  X arren X  may be abbreviated to  X  X arr. X ). The corresponding complete author names need to be retrieved and substituted for each abbreviation in the online database.

Tcl regular expressions specified within another part of the system X  X  user interface (Fig. 6) are the main tool for the manipulation of raw OCR text output. For exam-ple the regular expression to parse the published year from a reference is expressed by regexp { ( [ 1 ][ 7  X  9 ][ 0  X  9 ][ 0  X  9 ] ) } $ referenceyear where regexp is the Tcl regular expression command (assumed by the graphical user interface), {([1][7 X 9][0 X  9][0 X 9])} is the parsing pattern (which searches for a year between 1700 and 1999, prefixed by a space), $ref-erence represents the reference raw text generated by OCR (fed to the parser automatically and displayed in the source text window), and year contains the four matched digits output to the  X  X ear X  database field (dis-played in parsed text window). Similarly, for author abbreviations, a regular expression is applied to the au-thor name raw OCR source text field to find any pattern terminated by a full stop: regexp {[  X  \ . ]+} $ authorpattern Based on the detected pattern , another regular expres-sion is then used to search the specific Author dictionary to find and substitute the matching full Author name. Similar dictionary techniques, based on edit distance, are used to detect and correct limited errors in text fields which otherwise match one of the database fields.
Finally, the post-processing combines the separate text files generated by the OCR process, augmented with the results of regular expression matching, into a single file suitable for populating the online database. 6 Fuzzy configuration 6.1 Fuzzy inference When a card image is examined, humans will tend to de-scribe the text within the image in terms of approximate positions rather than exact coordinates. These positions can be defined by introducing some  X  X uzzy X  terms, e.g. top, bottom, right, left, which are in fact our natural language. Fuzzy logic is a technique developed to mimic this kind of human thinking, and our system design is based on fuzzy logic so that people who are not ex-perts on document analysis can still achieve a realistic understanding of the document image analysis processes which the system carries out. The overall archive docu-ment analysis system comprises three parts, fuzzy tem-plate creation, user configuration and batch processing. 6.2 Fuzzy template creation Initially, a set of default fuzzy zones is defined on an example card image from the archive to be processed. For example, Fig. 7, shows a card image that has been equally divided into 4  X  3 = 12 fuzzy zones (any number of zones can be defined both horizontally and vertically, but 4  X  3 was found to be sufficient and optimal for the archives evaluated). Vertically, the card image is divided into four rows, which are denoted using the fuzzy terms, Top , Upper , Lower , and Bottom . Horizontally, the image is divided into three columns, which are denoted with fuzzy terms, Left , Middle and Right . Any point T(x,y) on the image can then be described as (Top, Left) or (Bottom, Middle) etc. The default initial zones and their denotation are configurable by the curator according to the application.

Secondly, the text fields to be recognised are labelled using the system interface (see Fig. 4) by selecting them using a rubber-banding technique and labelling each boxed area. The default fuzzy zone boundaries are then automatically adjusted to maximise vertical and hori-zontal gaps between labelled text fields, as shown in Fig. 8.
Fuzzy membership functions fitted to the adjusted zone boundaries can be then set up as shown in Fig. 8. Vertically, four membershipfunctions overlapeachother and correspond respectively to the four fuzzy terms previously defined. Horizontally, there are a total of 12 membership functions corresponding respectively to three fuzzy terms on each of four rows. The overlap be-tween adjacent fuzzy terms varies with each document image, and depends on the gap in horizontal or vertical position between adjacent text components.

Finally, usingthetemplatemembershipfunctions, typ-ical fuzzy  X  X f X  X hen X  rules can be deduced for each text field, for example:  X   X  X f T(y) is from Upper to Upper and T(x) is from  X   X  X f T(y) is from Lower-middle to Lower-middle and  X   X  X f T(y) is from Bottom to Bottom and T(x) is from
If a particular archive contains more than one typical card layout, then further templates for additional lay-outs can be defined by repeating the steps above, and the best fuzzy matching template for each card image will be chosen when the system is run in batch card pro-cessing mode.

Each labelled text field has a corresponding fuzzy rule. Therefore, a total of five fuzzy rules can be obtained for the template of Fig. 8. 6.3 User configuration The fuzzy rules are changeable using a template configu-ration interface as shown in Fig. 9. For example, the rule for Location can be changed to  X  X f T(y) is from Bottom to Bottom and T(x) is from Left to Middle, then T(x,y) is Location X  to allow for images that may have a longer location name. The interface also provides four heuristic inputs, Type , Align , Sequence and Quantity . Type defines text components into three levels, word, line and block, consistent with the X X  X  tree document image analysis. Align gives four directions, west, east, north and south, for text components to align to (the default is none). Sequence indicates the sequence of a text component in a text field. Quantity provides the number of text components in a text field. These inputs are used to make the classification more accurate and precise, since the fuzzy rules can only define an approximate area for text fields. By combining fuzzy rules with heuristic inputs, the complete rule for the Species text field of the template can for example be written as  X  X ased on word level, if T(y) is from Upper to Upper and T(x) is from Left to Left, then the first word of T(x,y) is the Species X . 6.4 Batch processing Batches of card images are processed using the registered template(s) which include template member-ship function and fuzzy rules. The template member-ship functions are readjusted locally image by image, by measuring the vertical and horizontal gaps between text components. Based on the local membership functions, the text components on each image are then classified in terms of the fuzzy rules. 7 Archive card database and Web interface 7.1 LepIndex archive card database The card images and associated taxonomic data are man-aged by the NHM using an MS Access relational data-base consisting of seven linked tables, 12 lookup tables and 18 additional tables, plus 32 queries and 27 forms; it also includes more than 10,000 lines of Visual Basic code. The seven linked tables form the main part of the database and contain a total of 135 fields (fields are included for all data that might be present on the index cards, as well as for fields not on the cards but indirectly inferred from card sequence). These tables are linked by the unique card reference number assigned to each card image (and printed on its back) when the images were created.

The structure of the database and the layout of the front-end were constructed to meet specific taxonomic requirements. It incorporates, therefore, specialist knowledge of taxonomic protocols and demanded a thorough assessment of the structure and function of existing taxonomic databases. The main purpose of the database is to enable quick visual comparison of the type-or hand-written data on the card images with data generated by OCR analysis of these images and to al-low these data to be edited. The database was designed to provide an electronic substitute for the card index it replaces, and is now being made available to other taxonomists in the NHM Entomology Department via the local intranet. A Web interface for the database has also been developed (Sect. 7.2).

The main database form (Fig. 10) allows users quickly to find a card image, and the associated data, using a vari-ety of search options (e.g. a drill-down search by higher classification and a  X  X imple search X , with or without wild-cards, for any taxon name). Authorised users are able to edit, delete and create new records. They can also  X  X ove X  records, singly or in batches, to new relative posi-tions within the record sequence (e.g. in cases where the user wishes to transfer a species name from one genus to another). All changes made to data in the database are recorded in a set of archive tables. These tables store the old and the new field values, the name of the user, and the date and time of the change. Deleted records are also archived, and the user name, date and time are recorded. Users can validate information in all except memo fields, by placing the cursor in the appropriate field and dou-ble clicking the left hand mouse button. The value cur-rently stored in the field, plus the user name, date and time are recorded. If a field containing validated data is double clicked subsequently, then the validated data is displayed on a pop-up form and the user is given the option of deleting the stored validation information or overwriting it with a new validation record. Any of the card images (colour front or back, or grey-level front) can be viewed separately by selecting the appropriate button, and clicking on the magnifier button presents a full size image (around 1,000  X  600 pixels) for more detailed inspection. 7.2 LepIndex Web browser interface A Web interface for the Access database has also been developed and is publicly accessible (see Fig. 11) [6]. Users are able to search for records using a variety of search systems (e.g. a simple search by scientific name, or an advanced search using a combination of a number of different search terms). The results page displaying a record is laid out in a similar way to the main form of the Access database (i.e. Fig. 10), except that related groups of fields (e.g. the fields which comprise a ref-erence) are concatenated to aid readability. Although users cannot add entire new records, they are able to edit existing ones. If they do so, the user X  X  details and sug-gested changes are sent to the NHM server and stored in Access tables until the administrator of the system decides whether or not to include the changes in the master Access database. The Web interface operates using copies of the tables from the master Access data-base and these are updated periodically. 8 System evaluation 8.1 Card scanning The 290,886 cards in the full Lepidoptera index were scanned using the modified cheque scanner in a total of 61 person days X  X n average of about 10 cards/min (compared with the raw read rate of 60 cards/min) due to the overhead of collecting, transferring and returning cards from file drawers to the small 40-card hopper of the scanner (larger hoppers are available for commer-cial systems, but at increased cost). The full archive of JPEG images requires around 30 Gb of storage, with three images (front and back colour images, and front grey-scale) stored for each card. Individual JPEG card images are typically around 30 kbytes in size, though they can be considerably further compressed without significant subjective distortion using newer image cod-ing standards such as JPEG2000 or DjVu [15]. However, since both initial capture and web browser display use JPEG as the default image coding standard, we have not explored other options further as yet.

Subjective assessment of images by NHM staff, dis-played both through the Access database and LepIn-dex online [6] have indicated that the current image resolution of 200 d.p.i. is more than adequate for cross-checking or validating data on-screen, although it falls a little short of the recommended resolution of 300 d.p.i. for optimising OCR performance [16]. 8.2 Evaluation datasets The system was evaluated on two sets of sample cards. One set of 4,435 cards was randomly chosen from the Pyraloidea dataset of 27,578 archive cards, for which full truth data was independently available from the NHM. A second set of 10,000 cards was processed from the Cur-culionidae subset of the Coleoptera archive, as part of the overall system trials. No truth data was initially avail-able for Curculionidae, so a random subset of 994 cards from this dataset was manually truthed by the authors (the remaining unvalidated data output of the system was returned for curatorial evaluation). The Curculion-idae test set uses different card layouts and dictionaries from the Pyraloidea testset and therefore provides an independent dataset for validating system performance, and also for estimating whether sufficient user (re)con-figurability has been allowed in the system design. 8.3 Overall evaluation method For both datasets, the text fields extracted from each archive card for evaluation were: genus/species name, author name and the date sub-field within the refer-ence, since these fields are currently indexed in LepIn-dex [6]. Electronic dictionaries (not always complete) are also available for genus names, species names and author names. Evaluation was carried out using pre-processing and document analysis to generate three sets of binary text sub-images (genus/species name, author name and reference), which were binarized using our adaptive Niblack algorithm. The text sub-images were then fed into the Abbyy OCR for recognition class-by-class using respective class dictionaries, and the results were saved into three sets of text files.

The three sets of text files were parsed and merged into three single text files by post-processing with respec-tive regular expressions for database input. The results produced by the system were then compared with the word-level truth data for the corresponding database fields. In the word-level evaluation, if any unmatched character was found, the whole text field (which could contain one or more words) was considered incorrect as shown in Fig. 12, where the german  X  was unmatched, because it is not included in the OCR character set.
The same evaluation methodology was used for both evaluation datasets, except that only a single archive card template was required for the first dataset (Pyro-loidea cards), whereas three different templates were required for the second dataset (Circulionidae cards), becausethreedifferent cardlayouts weredetectedwithin this dataset (Fig. 13).

Evaluation of the pre-processing performance was carried out using only the genus/species name image field, whereas evaluation of overall system end-to-end performance also used the author name and reference sub-images. 8.4 Pre-processing results After optimising parameters using a small independent dataset of 335 images, recognition results for eight differ-ent thresholding methods were compared as shown in Table 1. The methods were: default Abbyy OCR thres-holding (as a baseline), global thresholding, Niblack X  X  and Sauvola X  X  algorithms, our own adaptive versions of Niblack X  X  and Sauvola X  X  algorithms [10], and Niblack X  X  and Sauvola X  X  algorithms applied to images with back-ground removed using our colour segmentation method. Table 2 gives an example of a binarized word image using each binarization method, and the corresponding OCR results.

From analysis of the results, we concluded that the best system performance was achieved by Niblack X  X  and our adaptive Niblack X  X  algorithms, both of which per-formed better than the default thresholding included in the Abbyy system. Although previous research [9] has claimed that Sauvola X  X  algorithm has superior perfor-mance to Niblack X  X , our experimental work with archive documents suggests this is not always true. In particular, backgrounds on archive documents vary considerably between images (Fig. 14), and performance of Niblack X  X  algorithm was found to be relatively insensitive to its parameter settings, whereas Sauvola X  X  algorithm was quite sensitive. Hence, even with the benefit of the adap-tiveparameterisationmethodwhichweintroduced, over-all performance across the full set of 4,435 images was still better using Niblack X  X  algorithm and our adaptive variant. 8.5 End-to-end word recognition results for the first Analysis of errors in the first dataset (Table 3) shows that 15% of overall errors occurred when document image analysis wrongly extracted or labelled text fields, and 13% resulted from incorrect truthing (e.g. see Fig. 15) including abbreviations. The remaining 72% of errors were generated by the OCR system, often caused by touching typewritten characters. 16.4% of errors were subsequently corrected by text post-processing, due to dictionary correction or expansion of abbreviations.
Since author recognition was carried out with an incomplete Author dictionary, the word recognition rate for this field is lower than for Species/Genus, where a full dictionary was available. The poorer result for Year was mainly caused by the OCR, which was less accu-rate in recognising digits than characters (nearly 89% of total errors for Year were caused by OCR errors com-pared with the average of 72%). Another cause of poor performance is that quite a few years are handwritten (Fig. 16). 8.6 End-to-end word recognition results for second The second test set of 994 cards were randomly cho-sen from the Curculionidae dataset of 10,000 archive cards. Three different layout formats were encountered as shown in Fig. 13. In this evaluation, three templates corresponding to these formats were registered for Doc-ument Analysis.

Table 4 shows that 99% of cards X  formats were identi-fied correctly, and hence subsequently analysed with the correct template. Type (a) was by far the most common template, occurring in more than 88% of the sampled cards. Therefore, we carried out the same evaluation as in Sect. 8.4 using just the 881 cards of type (a).
The text fields extracted from each archive card for evaluation in the second evaluation dataset were genus name, species name and author name. On the image (Fig. 13a), the top block is Genus and the second (ref-erence) block contains both Species and Author data. Most of the time, Species is the first the word in the block. Author, in most cases, is located in the middle of the block, and terminated with a comma. Its initial letter is always capitalized. Suitable regular expressions are used to search for these fields embedded within the  X  X aw X  OCR output for the reference sub-image.
Table 5 summarises the evaluation results for the sec-ond test set. 8.1% of overall errors occurred when doc-ument image analysis wrongly extracted or labelled text fields, and the remaining 91.9% of errors were gener-ated by the OCR system, often caused by touching type-written characters and complex surroundings. 12.6% of errors were corrected by the text post-processing stage. As species recognition was carried out with an incom-plete Species dictionary, the word recognition rate for this field is poorer than the other two. Also, both species and author fields have more complex adjacent text sur-rounding than genus which is a separate sub-image. This also causes a lower recognition rate for both species and author in comparison with genus. 8.7 Effect of removing the document analysis stage A final stage of evaluation attempted to address the question of whether improved performance could be obtained by dispensing with the image analysis stage of the system, and effectively treating the whole card image as flat text, as a standard OCR system would. In this case, in addition to processing the raw text output of the OCR to extract the sub-texts required for feed-ing into the database, the regular expression processing also parses the text, labelling the database field to which each sub-text is applied, by means of dictionary match-ing. The result is a simpler system, but with some effect on word recognition performance for several reasons:  X  The use of dictionaries to identify specific database  X  Regular expression matching using a dictionary will  X  Processing the complete image in one pass through
The system was set up to process the first test set of 4,435 images again using a configuration where all images were processed directly by the OCR system, with modified regular expression post-processing to extract and label all sub-texts from the raw OCR output for each complete image.

Results obtained were: Species/genus name 88.7% word recognition rate (compared with 89.2% from Table 3); Author name 77.5% (79.8% from Table 3); and Year 62.5% (73.5% from Table 3). The very small differ-ence in performance for Species/genus names is proba-bly due to the slight difference in performance between the Abbyy thresholding algorithm and the Adaptive Ni-black algorithm used in the full system. The 2.3% differ-ence in performance for Author names is indicative of the effect of the author dictionary being incomplete. Finally, the much larger difference in performance for Year (11%) results from the non-optimal parameterisa-tion of the Abbyy OCR for this field. Since the full card image is processed by the OCR in a single pass, it was processed with the  X  X ypewriter X  text setting (which was optimal for species/genus and author names), instead of  X  X utodetect X  (which gave improved performance for numerals). 9 Discussion of results The aim of this work was to develop a system-level ap-proach to online historic archive conversion, by integrating a configurable set of document image analysis and recognition components into a complete user-reconfigurable system. Work on this system is continuing, and a copy is currently under user evaluation at the NHM. While it can not be claimed that any of the individual system components exhibit great novelty, the overall project approach of designing a reconfigurable system to allow users to configure aspects of document analysis on a batch-by-batch basis (a  X  X ottage industry X  rather than  X  X roduction line X  approach) is unusual, but we suggest necessary, to handle the unique variability and physical distribution throughout the world that is characteristic of historic document archives. The authors are aware of very few other published evaluated imple-mentations of end-to-end systems of this type. The value of this systems-level approach is in clarifying architec-tural issues and performance sensitivities which cannot be addressed by component-level studies X  X he breadth-first document image analysis strategy recently advo-cated by Baird et al. [17].

As might be expected, the completion of this first implementation and evaluation phase has highlighted many areas of possible improvement for the system. For example, it is evident from the current evaluation that a high proportion of the current word recognition errors result from touching characters being mis-segmented by the Abbyy OCR system (which nevertheless per-forms significantly better than other commercial OCR systems we evaluated). Since most of the text is fixed-pitch Courier typewriter font, a significant performance improvement might be achieved by pre-segmentation of characters using projection from word boundaries or Fourier transforms of histograms, prior to OCR, as has already been suggested for second world war type-written documents [18]. Another promising approach is segmentation free OCR, where a classifier is convolved over the word image, thereby considering all possible segmentations [21, 22]. While prototype implementa-tions of these methods have been shown to significantly improve accuracy on these archive cards, more work is needed to tune the system for practical operation [21], or to make it run at reasonable speed [22].

Improved performance could also be achieved by upgrading to the latest version of Abbyy X  X  OCR sys-tem (currently version 7.1), which trial evaluations have shown improves on the results reported here. However, to adopt this version would require repeating all the evaluation results reported in this paper, to maintain consistency. Since adopting Finereader 6.0 for the trial however, Abby have also recently announced other rel-evant developments, including support for old Euro-pean languages [19], and Abbyy Flexicapture studio [20]. Flexicapture may be the first commercial attempt to provide support for variable-format business form processing, and adopts several of the same approaches described in this paper, including GUI-based form tem-plating using form image samples, flexible hierarchical document image descriptions for initial DIA, and regu-lar expression post-processing to extract key text fields from larger text strings.

Initial user evaluation at the NHM has also indicated that, although the current system is usable, and repro-duces the word recognition performance described here with other datasets, it is far from efficient, due to a vari-ety of sub-optimal user interface characteristics that are typical of a research and development prototype. For example, at present the system is restricted to process-ing moderate batches of a few hundred card images at a time, due to restrictions on the number of file han-dles simultaneously open. Such issues are not funda-mental performance limitations, but nevertheless serve to highlight the need for considerable further user inter-face optimization before the present system could be considered to be fully industrialized. A more complete comparative evaluation of the system in terms of both evaluation performance and time could in principle be carried out using a CAVIAR-style methodology [23], except that the VIADOCS system is only intended for use by taxonomic experts with significant computational experience. Such subjects are rare and widely geograph-ically distributed, making statistically valid conclusions difficult to achieve. 10 Conclusions This paper has described and evaluated a complete end-to-end system for archive document acquisition, analysis and recognition for digital libraries. The system is user-configurable through a fuzzy configuration user inter-face to handle different archive layout formats. It also integrates sub-systems to perform document image pre-processing, document image analysis for semantic label-ling, and text post-processing using regular expressions, with a standard off-the-shelf OCR system. The over-all system performance is encouraging: basic word-level OCR exceeds the raw OCR of the embedded commer-cial OCR system we used, and subsequent text post-processing corrects a significant proportion of residual errors, as well as allowing direct insertion of the recogni-sed text into appropriate online database fields. Database field recognition rates of up to 90% have been observed during evaluation, and such performance should be reliably met or exceeded with foreseeable improvements to the preprocessing and consistent avail-ability of complete field dictionaries.
 References Author Biography
