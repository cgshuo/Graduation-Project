 Mark McCartin-Lim markml@cs.umass.edu Andrew McGregor mcgregor@cs.umass.edu Rui Wang ruiwang@cs.umass.edu Spatial partition trees are data structures that hierar-chically subdivide a set of data points with the goal that resulting partitions contain  X  X imilar X  points. A ubiquitous example is the k -d tree, which is widely used in many unsupervised learning methods includ-ing classification, regression, nearest-neighbor finding, and vector quantization.
 At level L of the tree, the data has been partitioned into 2 L different classes. Every time we subdivide a set of points, we do so with the hope of reducing the the average distance between points in the same sub-sets as much as possible. The goal is similar to that in k -means clustering , but unfortunately, k -means clus-tering is an NP-hard problem even when k = 2 (Das-gupta, 2008).
 We know that k -d trees are vulnerable to the so-called curse of dimensionality  X  if the dimensionality of our data is D , then we may have to traverse O ( D ) levels to halve the average diameter. This is shown to be true even when the intrinsic dimensionality of the data is low (Verma et al., 2009). Thus, k -d tree are not well suited to applications that involve high-dimensional data, because to produce a good classifier, we would need to branch our tree O (2 D ) times. Similarly, it has been shown that using a k -d tree to do nearest-neighbor queries in high-dimensional data is not much more efficient than scanning every element (Lee &amp; Wong, 1977).
 Recently, random-projection (RP) trees (Dasgupta &amp; Freund, 2008) and PCA trees (Verma et al., 2009) have been proposed as alternatives to k -d trees that adapt to intrinsic dimensionality. RP trees and PCA trees differ from k -d trees in that their splitting planes are not restricted to being axis-aligned. Thus, unlike k -d trees, they can adapt to the covariance of the data (see Figure 2).
 Of the two choices, RP trees are appealing because they can be computed very efficiently. Each subdivi-sion in an RP tree is determined by a randomly-chosen hyperplane. PCA trees are significantly more expen-sive to compute, because the normal of each hyper-plane is chosen by doing principal component analysis (PCA) on the data to find the principal direction. This requires one to compute the covariance matrix and perform eigendecomposition, or to do singular value decomposition (SVD). However, PCA trees perform significantly better than RP trees at reducing the aver-age diameter, as demonstrated in (Verma et al., 2009). APD Trees. We propose a new spatial partition tree, the approximate principal direction tree or APD trees, which generalize the idea behind RP trees, but perform almost as well as PCA trees when it comes to reducing average diameter with respect to intrinsic dimensionality.
 When choosing the normal of the hyperplane, APD trees start with random vectors like RP trees, but then apply a small number of power-method iterations (Burden &amp; Faires, 2010) to these vectors. The power method is often used in data intensive applications to approximate principal eigenvectors (e.g., Google X  X  PageRank (Wills, 2007)) and for spectral clustering (Lin &amp; Cohen, 2010). However, it is important to note that in our application we are not simply concerned with getting a good approximation of the principal eigenvector. Rather, we are finding a hyperplane that will yield a good subdivision. This is important be-cause the power method can be slow to converge when the first principal component has the same variance as the second principal component. However, this is not a concern for us since either component (or some linear combination thereof) would suffice for a good split. We show strong performance guarantees with only one or two power-method iterations. See Figure 1 for an empirical illustration. Furthermore, when the intrin-sic dimension of a dataset is d , we prove that O (log d ) power-iterations are sufficient to produce trees that re-duce average diameter at the same rate as PCA trees. Again, this is true even when the principal direction is not dominant in the data.
 Outline. In Section 2, we present the necessary def-initions including that of local covariance dimension (Dasgupta &amp; Freund, 2008) and what it means for a tree to adapt to intrinsic dimensionality. In Section 3, we presents the algorithm for building APD trees. Then, in Section 4, we prove that it adapts to intrinsic dimensionality with the average diameter converging at a similar rate to PCA trees. This is further demon-strated by our experimental results (Section 5), which also show that APD tree is much faster than the stan-dard PCA tree algorithm. Finally, we also present a GPU-based implementation, which is even faster. 2.1. Average Diameter For a given set of points S = { x 1 ,...,x n }  X  R D we measure their similarity in terms of the average dis-tance between points in the set. We will use the fol-lowing notation: Definition 2.1 (Average diameter of a set of points) . or equivalently 1 where mean( S ) = P x  X  S x/ | S | .
 We are interested in partitioning S as { S 1 ,S 2 } such that the average diameter of S 1 and S 2 is small. Definition 2.2 (Average diameter over two sets) . 2.2. Local Covariance Dimension Several possible definitions of intrinsic dimension are discussed in (Verma et al., 2009). The one they use in their analysis of RP trees and PCA trees, local covari-ance dimension is based upon a statistical representa-tion of the data, and is thus well-suited to modeling data from machine learning problems. We will analysis APD trees using the same definition.
 Recall that the covariance matrix C  X  R D  X  D of a set of points S = { x 1 ,...,x n }  X  R D with mean( S ) = 0, can be written as C = X T X where X  X  R n  X  D is the matrix whose i -th row equals x i . The idea behind local covariance dimension is as follows. The eigen-vectors of the covariance matrix form an orthonormal basis for the data. If a small number of the eigenvec-tors describe almost all the variance in the data, then we know that the data is well represented by the sub-space spanned by those eigenvectors. We can think of the dimension of this subspace as being the intrinsic dimension of the data.
 Definition 2.3 (Local Covariance Dimension) . Let  X  1  X   X  2  X  ...  X   X  D be the eigenvalues of the co-variance matrix of S = { x 1 ,...,x n }  X  R D . We say S has local covariance dimension ( d, X  ) for d  X  D and 0 &lt;  X  &lt; 1 when Notice that the local covariance dimension is also parametrized by  X  , which corresponds to how closely the subspace represents the data.
 We say a method for partitioning S adapts to the intrinsic dimension of S if the resulting partition { S is independent of D , the extrinsic dimension. A spatial partition tree is determined by the rule used to partition the points at each node of the tree. Be-fore we discuss the partitioning rule used in APD trees, we review a general template for possible rules. This template is presented in Algorithm 1. If we assume S corresponds to the set of points assigned to a par-ticular node in the tree, then it is obvious that this meta-algorithm can be applied recursively to produce a balanced binary tree.
 The tree produced by this meta-algorithm is a hybrid of a BSP tree and a sphere tree (Devroye et al., 1996). If S is considered to contain  X  X utliers X , we use a sphere to partition points that are close to the center of the data away from those that are not. We discuss this case further in Section 3.2. Otherwise, we use a hyper-plane to partition the data. The choice of this hyper-Algorithm 1 Tree construction meta-algorithm if S has outliers then else end if
S 2 := S \ S 1 return { S 1 ,S 2 } Algorithm 2 APD splitting rule p := a random vector s  X  R D for 1 ...t do end for return p plane depends upon a splitting rule , which determines the normal for the hyperplane. Two existing splitting rules are the RP and PCA rules: 1. RP rule: p is chosen uniformly at random from 2. PCA rule: p is the principal eigenvector of C . As we mentioned earlier, the downside of the PCA splitting rule is that computing p either requires one to compute the covariance matrix and perform eigen-decomposition, or to do singular value decomposition on the data. Both are computationally intensive tasks. On the other hand, applying the RP splitting rule is computationally trivial. However, this splitting rule does not achieve as good accuracy as the PCA split-ting rule. 3.1. The New Splitting Rule The new splitting rule we propose allows us to achieve similar accuracy to that achieved by the PCA rule without the computational overhead. The rule is based on the Power Method (Burden &amp; Faires, 2010), a well-known technique for approximating eigenvectors. See Algorithm 2. The technique translates nicely into a parallel algorithm we can implement on the GPU, as we will see in Section 5.
 Theorem 3.1. For i  X  [ D ] , let p i  X  R D and  X  i &gt; 0 be the (normalized) eigenvectors and eigenvalues of the covariance matrix C . If s = P D i =1  X  i p i is the initial vector used in the APD splitting rule, then the vector returned by the splitting rule is Proof. The result follows from the observation that P Below are two properties of the APD splitting rule: Generalization of RP and PCA: As t increases, p will converge to the principal eigenvector of the co-variance matrix, i.e., the vector we would get if we were to use the PD splitting rule. On the other hand, when t = 0, p is a random unit vector in R D , and thus equivalent to if we were using the RP splitting rule. So intuitively, using the APD splitting rule gives us a trade-off between the RP splitting rule and the PCA splitting rule.
 Doesn X  X  require convergence of power method: It is important to emphasize that the idea behind ap-plying the power method in our setting is not to ap-proximate the principal eigenvector per se. We need the splitting rule to be computed quickly and will pri-marily be interested in the case when t = 1 or 2. In this case, it is unlikely that the vector returned is similar to the principal eigenvector. For example, the power method converges slowly when the first few principal values are very close to each other. However, this is not an issue in our setting since a few iterations will still ensure that the direction of p has high variance. 3.2. Fast outlier detection The tree construction meta-algorithm has a special case for they S contains outliers , i.e., when there av-erage distance between points is significantly less than the maximum distance between two points. This is important, because even if we were to find a good hy-perplane to split our data, the average diameter of the resulting partitions will still be influenced by the out-liers.
 Following (Dasgupta &amp; Freund, 2008; Verma et al., 2009), we say that S has outliers if the maximum de-pends on the relative size of the average diameter and the maximum diameter: Definition 3.1 (Outliers) . For an user-definable pa-rameter c &gt; 0 , we say S contains outliers if where  X ( S ) = max x,y  X  S k x  X  y k .
 It was shown in (Dasgupta &amp; Freund, 2008) that in the case S has outliers according to this definition, the meta-algorithm (Algorithm 1) still guarantees a constant reduction in average diameter.
 Proposition 3.1. Suppose  X  2 ( S ) &gt; c  X  2 a ( S ) , so that S is split into { S 1 ,S 2 } as described in Algorithm 1. Then the following holds: Unfortunately, it is computationally expensive to use Definition 3.1 to determine if there are outliers, be-cause calculating  X  2 ( S ) involves comparing O ( | S | distances. Instead we proposed a simple variant. For an arbitrary point a  X  S , let D ( S ) = max x  X  S k x  X  a k . It can easily be shown that  X ( S ) / 2  X  D ( S )  X   X ( S ). Definition 3.2 (Outlier heuristic) . S has outliers if 3.3. Comparison between splitting rules We can now state our main theoretical result for the APD splitting rule and contrast it with the analogous results for RP and and PCA trees. Henceforth, we as-sume that there are no outliers, i.e.,  X  2 ( S ) &lt; c  X  The following results were shown in (Dasgupta &amp; Fre-und, 2008) and (Verma et al., 2009).
 Proposition 3.2. There exist constants c 1 ,c 2  X  (0 , 1) such that if S has local covariance dimension ( d,c 1 ) : 1. RP rule: If p is chosen uniformly at random 2. PCA rule: p is the principal eigenvector then, These results show that RP trees and PCA trees both adapt to intrinsic dimensionality, since the squared av-erage diameter of nodes in the tree is decreasing as a function of d , the local covariance dimension of the data, rather than its extrinsic dimension D .
 In the next section we will prove that a similar diam-eter reduction guarantee holds for APD trees: Theorem 3.2 (Main Result) . For c 1 , X   X  (0 , 1) , there exists constant 3 c 2  X  (0 , 1) such that if 1. S has local covariance dimension ( d,c 1 ) and 2. p  X  R D is returned by the APD splitting rule with then with probability 1  X   X  , where k = 1  X  Note that for t = O (log d ), this gives i.e., the same improvement as that achieved for PCA trees. However, even for smaller t , the bound only has a weak dependence on d . To prove Theorem 3.2 we will need to analyze the quantity For a fixed vector p , observe that V ( S,p ) corresponds to the variance of x  X  p when x is drawn uniformly at random from S . Intuitively, it makes sense that a good splitting vector is one for which V ( S,p ) is large. Specifically, if we can prove a lower bound for V ( S,p ) when p is chosen according to the APD splitting rule, then we can appeal to the following variant of a propo-sition from (Verma et al., 2009).
 Proposition 4.1. There exist constants 0 &lt; c 1 ,c 2 &lt; 1 with the following property. Suppose  X  2 ( S )  X  jection vector p . If S has local covariance dimension ( d,c 1 ) , then: where k = 1  X  To lower bound V ( S,p ) we prove the following se-quence of lemmas that relate V ( S,p ) to the eigen-vectors p 1 ,...,p D and corresponding eigenvalues  X  ,..., X  D of the covariance matrix C . Recall that  X  1  X   X  2  X  ...  X   X  D and that the eigenvalues are non-negative since C is positive semi-definite. Lemma 4.1. For any q = P D i =1  X  i p i , Proof.

V ( S,q ) = where the last equality follows because the eigenvec-tors are orthonormal.
 Lemma 4.1 and Theorem 3.1 imply the next lemma. Lemma 4.2. Let p be the vector computed by the APD rule after t iterations where s = P i  X  i p i is the initial vector. Then We now analyze the distribution of the  X  i coefficients to prove the next lemma.
 Lemma 4.3. For any  X  &lt; 1 and  X  1 ,..., X  D  X  0 , there exists c  X  &gt; 0 with if the direction of s = P i  X  i p i is chosen uniformly. Proof. Let {  X  i } i  X  [ D ] be independently distributed  X  random variables with one degree of freedom. Then, since the direction of the vector  X   X  1 ,..., X  D  X  is chosen uniformly at random in the APD rule, we know that where  X  i =  X  i / X  1 so that 1 =  X  1  X   X  2  X   X  X  X   X   X  D . This follows because a) a random point on the unit sphere can be sampled by choosing each coefficient ac-cording to the standard normal distribution and then renormalizing (Muller, 1958); and b) the square of a variable with standard normal distribution has the  X  2 -distribution with one degree of freedom.
 The lemma follows by the union bound if we can show that there exist constants c 1 ,c 2 &gt; 0 such that: since then Eq. 1 holds with c  X  = c 2 /c 1 .
 For the first inequality, note that E [  X  i ] = 1 (expecta-tion of  X  2 distribution) and hence Therefore Eq. 2 follows from an application of the Markov inequality with c 1 = 2 / X  .
 For the second inequality there are two cases. First suppose that P D i =1  X  2 t +1 i  X  16 / X  . Using the inverse CDF of the  X  2 distribution, we can compute  X  where P [  X  1  X   X  ] =  X / 2. Then note that, where the second inequality (  X  1  X   X  ) holds with prob-ability at least 1  X   X / 2.
 Alternatively, suppose that P D i =1  X  2 t +1 i  X  16 / X  . Then, appealing to the Chebyshev inequality given that we conclude P This establishes Eq. 3 with c 2 = min(  X  X / 16 , 1 / 2). Lemma 4.4. For S = { x 1 ,...,x n }  X  R D with local covariance dimension ( d, X  ) with mean( S ) = 0 . From Definition 2.3, it follows that P d j =1  X  (1  X   X  ) P D j =1  X  j and therefore  X  P d j =1  X  j  X  (1  X   X  ) P D j = d +1  X  j . And hence, it follows that: Theorem 4.1. Let p be the vector computed by the APD rule after t  X  1 iterations. Then with probability 1  X   X  , there exists a constant c  X  &gt; 0 such that
V ( S,p )  X   X  1 c  X  (1  X   X  ) Note this implies V ( S,p )  X   X  1 c  X  (1  X   X  )( d  X  1) Proof. Let  X  = 1 / (2 t ). By Lemmas 4.2, 4.3, and 4.4, if we set  X  i =  X  2 t i / X  2 t 1 , we get
V ( S,p ) c (1  X   X  ) where the third last inequality follows from the con-vexity of the function x 1+1 / (2 t ) and the second last inequality follows by setting  X  = P d j =2  X  i . By analyzing the derivatives, it can be verified that has a unique minimum at u = (2 t ) 2 t ( d  X  1) Substituting in this value establishes the theorem. We compare the quality of APD trees to that of RP trees and PCA trees by measuring the vector quantiza-tion error (VQ-error). In vector quantization, the goal is to map all vectors (or points) in a given data set to a small number of representative vectors (or points). This can be done with a spatial partitioning tree  X  the points belonging to each partition are represented by the average of the points in that partition. Following this, the VQ-error is defined as the average squared representation error. Specifically, if S 1 ,S 2 ,...,S 2 ` the sets of points associated with the leaves of a tree T of depth ` , then V Q T ( S ) = .
 We try to closely replicate the experiments done in (Freund et al., 2008), using the same kind of datasets and the same parameters. We ran our experiments on a synthetic dataset and the MNIST test dataset. Additionally we use a protein homology dataset from the KDD Cup 2004 data mining competition.
 As in (Freund et al., 2008), the synthetic dataset con-sists of 10,000 points, each a 1,000-d vector and gener-ated as follows: choose a peak value p uniformly ran-domly from [0 , 1], and then generates the coordinates of the point from the normal distribution N ( p, 1). The MNIST test dataset is a set of 10,000 images of hand-written digits, each of which has been normalized to 28  X  28 pixels, and is thus a 784-d vector. The pro-tein homology dataset consists of pairs of proteins that have been tested for homology. It contains 285,409 data points, each of which is a 74-d vector.
 For RP trees and APD trees, we perform 15 runs of each experiment, and calculated the average VQ-error. For PCA trees, we only needed to run each experiment once since there is no randomization in that algorithm. The results are shown in Figure 3. Each curve plots the decay of average VQ-errors as the depth of the tree increases. For APD trees, we show results of applying 1, 2, and 3 power iterations. RP tree is equivalent to applying no power iteration. The quality differences between APD trees and PCA trees are small, while the differences of RP trees with them are much more visible. Note that using 1 power iteration in APD trees already provides very good quality and gives the the biggest drop in VQ-error from RP trees.
 GPU Implementation of APD Trees. Our APD tree algorithm is well-suited for parallel computation on modern GPUs. Since it only relies on basic matrix operations, most GPU-based linear algebra packages can be applied directly. Our implementation is writ-ten in MATLAB using the Jacket GPU library (Ac-celerEyes). The main tree building uses a standard recursive subdivision algorithm controlled by the host CPU, while the tree splitting algorithm (Algorithm 1) is accelerated on the GPU. Experimental results are presented in Figure 1 and 4. These results were ob-tained on a PC with NVIDIA 470 GTX graphics card (1.2GB GPU memory) and an Intel Core-i7 3.0GHz CPU with 8 hyperthreads.
 Figure 1 lists the computation time using the GPU and CPU implementations. The timing is averaged over 15 runs per test. The GPU implementation gen-erally achieves 5  X  12  X  speedup compared to the CPU version, which is also written in MATLAB and multi-threaded. This performance gain is mainly due to the acceleration of matrix-matrix and matrix-vector mul-tiplications, which can easily exploit the GPU X  X  par-allel computation power. Note that the speedup for the KDDCup04 dataset is moderate, mainly because of the small vector size. We generally obtain higher performance gain on higher dimensional datasets, be-cause they can better utilize available GPU resources. For comparison we have also included a CPU PCA tree implementation using MATLAB X  X  svds routine. As seen from the table, the CPU PCA trees are 4  X  6 times slower than the CPU APD trees, which are in turn 5  X  12 times slower than the GPU APD trees. We did not include a GPU PCA tree because the Jacket library only provides a full svd routine but not svds . As a result, the GPU PCA tree is only moderately faster than the CPU counterpart. This shows that our APD tree algorithm is well-suited for exploiting the GPU, due to its simplicity, at the same time providing comparable quality to PCA trees.
 Figure 4 plots the computation times for synthetic data sets containing different numbers of points. Each point is a 512-d vector generated using the same algo-rithm before. We plot the CPU and GPU timing for each data set and with 0 to 4 power iterations. From this plot we observe that the cost incurred by each additional APD iteration is generally quite small. We presented the APD-tree, a new spatial data struc-ture for high-dimensional data. APD-trees use a small number of power iterations to achieve computational efficiency (comparable to RP trees) and high quality (comparable to PCA trees). The approach is insensi-tive to the convergence properties of the power method and is well-suited for GPU computation.
 This work is supported in part by NSF grants CCF-0953754 and CCF-1025120.

