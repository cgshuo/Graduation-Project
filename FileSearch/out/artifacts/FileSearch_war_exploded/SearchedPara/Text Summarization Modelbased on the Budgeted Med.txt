 We propose a multi-document generic summarization model based on the budgeted median problem. Our model selects sentences to generate a summary so that every sentence in the document cluster can be assigned to and be represented by a sentence in the summary as much as possible. The advantage of this model is that it covers the entire relevant part of the document cluster through sentence assignment and can incorporate asymmetric relations between sentences such as textual entailment.
 I.2 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Experimentation text summarization, median problems
Generic text summarization is the task of generating a short and concise document, or a summary, that describes the content of a document or multiple documents [8]. One well-studied approach to this task generates a summary by selecting some sentences from given documents. We focus on this extractive approach, since it has an advantage that the grammaticality is guaranteed.

A summary will be considered good, if the summary rep-resents the whole content of the document cluster. This de-mand will be met if every sentence in the document cluster is assigned to a selected sentence and the former can be in-ferred from the latter. In the example in Figure 1,  X  the man is a football fan  X  can be inferred from  X  the man likes football and baseball  X . In this case, the former sentence need not be selected in the summary, if the latter is selected. We thus Figure 1: Sentences in a document cluster. The starting node of each arrow infers the target node. generate a summary by selecting the sentences that provide the best selection and assignment. Given sentences in Fig-ure 1, we would like to select the two shaded nodes (i.e.,  X  the man likes football and baseball  X  and  X  his wife bought a book and a magazine  X ), since these two sentences infer all the other sentences. This summarization model can be formal-ized by the budgeted median problem, which is an integer linear programming problem.

A possible criticism to this idea of summarization by sen-tence selection and assignment is that a document cluster would contain many irrelevant sentences and one cannot expect that all the sentences should be inferred from the summary, especially if the summary length is limited. We manage this problem by incorporating the benefit of each sentence being included in a summary. If the benefit of a sentence is low, this sentence will be practically ignored and does not affect the resultant summary.

An advantage of the model is that it covers the relevant part of the document cluster through sentence assignment and can incorporate inter-sentential asymmetric relations such as textual entailment.
Nomoto and Matsumoto [10] performed clustering on sen-tences and used the obtained sentence clusters to generate a summary. MEAD [11] used document clustering (not sen-tence clustering) to obtain salient words of each cluster and then used these words to select sentences. CLASSY [2], which performed best in DUC X 04 in terms of ROUGE-1 score, scored sentences with the sum of tf  X  idf scores of words. They also incorporated sentence compression based on heuris-tic rules. Filatova and Hatzivassiloglou [5] represented each sentence with a set of conceptual units (e.g., words) and for-malized the extractive summarization as a maximum cov-erage problem that aims at covering as many conceptual units as possible by selecting some sentences. Takamura and Okumura [15] used the same model and solved the problem as an integer linear programming problem. They also ex-tended the model so that it takes into account the relevancy of sentences to the topic of the document cluster. Since their report is the latest one on the generic text summariza-tion, we will use it for comparison. McDonald [9] formalized text summarization as a knapsack problem which determines whether each sentence is packed in the knapsack or not.
Facility location problems [3] are applicable to practical is-sues of determining, for example, hospital location or school location, where the facility is desired to be accessible to the customers in the area. The budgeted median problem is a fa-cility location problem which has the cardinality constraints in the knapsack form. We consider here that customer loca-tions are also potential facility sites.

We will propose a multi-document summarization model based on the budgeted median problem. Documents are split beforehand into sentences D = { s 1 ,  X  X  X  , s j D j select some sentences from D to generate a summary.
We would like to generate a summary such that every sen-tence in the document cluster is assigned to and represented by one selected sentence as much as possible.

Let us denote by e ij the extent to which s j is inferred by s i . We call the score inter-sentential coefficient . If we denote by z ij the variable which becomes 1 if s j is assigned to s i , otherwise 0, then the score of this whole summary is going to be
We next have to impose the cardinality constraint on this maximization so that we can obtain a summary of length K or shorter, measured, for example, by the number of words or bytes. Let x i denote a variable which becomes 1 if sen-tence s i is selected, 0 otherwise. Let c i denote the length of sentence s i . The cardinality constraint is then represented as
P cannot expect that every sentence in the document cluster is perfectly inferred by the summary. However, at least we are going to maximize the inference relations of the contents so that we can obtain a good summary.

We formalize text summarization as follows: (1) guarantees that any sentence to which another sentence is assigned is in a summary. (2) is the cardinality constraint. (3) guarantees that every sentence is assigned to a sentence, and (4) means that any selected sentence is assigned to it-self. The integrality constraint on z ij (6) is automatically satisfied in the problem above.

This maximization problem can be regarded as a bud-geted median problem [12], which is an NP-hard problem. Although this summarization model is intractable in general, if the problem size is not so large, we can still find the opti-mal solution by means of the branch-and-bound method [6], which we rely on throughout this paper. We explain how to calculate inter-sentential coefficients. Although the inference relation required for summarization is similar to the entailment relation, it is also supposed to in-clude other relations such as exemplification. For example, if one sentence gives merely an example of what is stated in another sentence, then the former sentence might not be needed in the summary. Our model provides a framework in which such relations are well utilized. However, since the techniques for identifying such relations are still under devel-opment, we use baseline methods for entailment recognition.
Inter-sentential coefficient e ij indicates the extent to which sentence s i infers sentence s j . We use the score based on word coverage used as a baseline method by Rus et al. [13]: where s i is regarded as the set of the words contained in the sentence, and therefore s i  X  s j represents the intersection of s and s j . We should notice that e asy ,ij is asymmetric with respect to i and j , as suggested by the subscript asy (for asymmetric). We also try the symmetric version of e asy ,ij
The proposed method attempts to cover the entire doc-ument cluster as much as possible by assigning every sen-tence to one of the selected sentences. However, we cannot necessarily expect that all the sentences are inferred from the summary, because there can be irrelevant sentences and the summary length is limited. There are also some sen-tences that are more important than the other, and hence need to be covered more thoroughly. We manage this prob-lem by incorporating benefit b j which is given by sentence s . If the benefit of a sentence is low, this sentence will be practically ignored and does not affect the resultant sum-mary. We use this benefit to define a new inter-sentential coefficient: e 0 asy ,ij = b j e asy ,ij . This new coefficient e indicates the benefit that can be obtained by assigning s to s i . It becomes large when the original benefit b j is large and s j is sufficiently entailed by s i . Similarly, we also define e more. Benefit b j is defined as follows: where pos ( s j ) indicates the position of sentence s j , which ranges from 1 to the number of sentences in the document. We use the inverse of the sentence position because it is known that leading sentences tend to be important in the summarization of news articles, which we use in the later experiments. P ter, and cos( and the entire document cluster are supposed to be important in summarization. Parameter p controls the trade-off between the two terms.
The proposed method is somewhat similar to methods based on sentence clustering [10] in the sense that both methods generate some sets of sentences. However, there is a big difference between these two methods. While the methods based on sentence clustering generate sets of simi-lar sentences, the proposed method attempts to generate the sentence sets, each of which has one selected sentence and contains sentences entailed by the selected sentence. Two sentences in a set in the proposed method might not be sim-ilar to each other. One advantage of the proposed method is that asymmetric relations between sentences such as textual entailment can be incorporated in a natural manner.
Filatova and Hatzivassiloglou [5] represented each sen-tence with a set of conceptual units (e.g., words) and formal-ized the extractive summarization as a maximum coverage problem that aims at covering as many conceptual units as possible by selecting some sentences. Their model has the re-striction that each sentence has to be decomposed into con-ceptual units. The question of what should be used as con-ceptual units has not been clearly answered yet. Our model is free from such a restriction and can incorporate various inter-sentential relations. One advantage of Filatova X  X  model is in computational time; their model has O (max {| D | , N decision variables and our model has O ( | D | 2 ) decision vari-ables, where | D | denotes the number of sentences in the document cluster, and N denotes the number of conceptual units. Another advantage of Filatova X  X  model is that it can express the situation where two sentences infer one sentence. Our current model cannot handle such a situation. However, our model can be extended in a straightforward way so that it can handle those situations (Section 5).

McDonald [9] formalized text summarization as a knap-sack problem which determines whether each sentence is packed in the knapsack or not. In order not to select similar sentences, they added to the objective function the negative sum of similarities between sentences in the summary. Al-though their model can incorporate various similarities be-tween sentences, their model does not directly emulate the coverage of the sentences in the document cluster.
We conducted experiments on task 2 of DUC X 04 [4], be-cause it is the latest DUC dataset on generic text summa-rization. 50 document clusters, each of which consists of 10 documents, are given. One summary is to be generated for each cluster. Following the official experimental setting of DUC, we set the target length to 665 bytes.

In the calculation of e ij , each sentence is represented as a set of words. We use stems of content words (nouns, verbs, Table 1: ROUGE-1 score and computational time of each model. Underlined scores are significantly different from that of peer 65 in statistical test. and adjectives) that are not stopwords. In the calculation of b j , each sentence is represented as a bag-of-words vec-tor whose elements are stems of content words as above. ROUGE version 1.5.5 [7] was used for evaluation. 1 We focus on ROUGE-1, because it has been usually used for evaluation on DUC X 04 in other papers, although ROUGE-2 and ROUGE-SU4 are more often used on newer non-generic datasets. Wilcoxon signed rank test for paired samples with significance level 0 . 03 was used for the significance test of the difference in ROUGE-1. We used the branch-and-bound methods implemented in ILOG CPLEX version 11.1 to solve integer linear programming problems. We tested 4 models: e
Experimental results are shown in Table 1 with three columns: ROUGE-1 measured without stopwords ( X  X ith-out X ), ROUGE-1 measured with stopwords ( X  X ith X ), the av-erage computational time in seconds for one document clus-ter. In addition to the result of p = 0, we report the result of the predicted p determined on DUC X 03 as development data (tuned on ROUGE-1 without stopwords), and the optimal p determined on DUC X 04.

In the experiment, the models with asymmetric inter-sentential coefficients generally outperformed the models with symmetric inter-sentential coefficients. We also successfully predicted p . e 0 asy ,ij and e 0 sym ,ij , which contain the benefit b , outperform respectively e asy ,ij and e sym ,ij , which do not contain the benefit. It means the inference alone will not yield a good summary.

For comparison, we added ROUGE-1 scores of other meth-ods in Table 1. We selected three methods: CLASSY ( peer 65), which performed best in DUC X 04 in terms of ROUGE-1, MaxCov, which is based on the maximum coverage prob-lem, MaxCov-Rel, which is a latest generic text summariza-tion model and is a variant of MaxCov. These methods were mentioned in Section 2. For CLASSY, we obtained the DUC official results. For MaxCov and MaxCov-Rel, we used the results obtained through the branch-and-bound method by Takamura and Okumura [15], in order to remove the search Options are -n 4 -m -2 4 -u -f A -p 0.5 -b 665 -t 0 -d -s. In case of evaluation with stopwords, -s was removed. error and directly compare the models. Conceptual units in MaxCov and MaxCov-Rel are stems of content words as those used in our method. In ROUGE-1 evaluation, our model with e 0 asy ,ij with the predicted or the optimal p sig-nificantly outperformed peer 65, Even without position in-formation p = 0, e 0 asy ,ij yielded a better result than peer 65, although the difference was not statistically significant. This means that the proposed method should work well on non-newspaper datasets, in which the sentence position is not going to be a strong information for summary generation.
The proposed method still requires much computational time; for example, e 0 asy ,ij requires nearly 90 seconds for gen-erating one summary. For applications that require fast summarization, we will need to incorporate efficient approx-imation techniques [14].

Figure 2 displays the change in ROUGE-1 score of e 0 asy ,ij when p changes. The curve reaches its peak at around p = 0 . 5. ROUGE-1 score degrades when p is large. It means that the sentence position alone is not sufficient. Figure 2: p and ROUGE-1 score of our model with the asymmetric inter-sentential coefficient.
 We give an example of sentence assignment we obtained. To the selected sentence X  X eltsin has a long history of health problems, including a heart bypass surgery two years ago X , the following sentence was assigned:  X  X eltsin suffered from heart disease during the 1996 presidential election and had a heart attack, followed by multiple bypass surgery, in the months after his victory X .  X  One-to-two assignment : in some situations, one sen-tence might be better represented by two sentences, than by one sentence. We can extend our model to handle such a 1-to-2 assignment. We introduce a new decision variable z ijk , which becomes 1 when sentence s k is assigned to sen-tence pair s i and s j , 0 otherwise. We also set the objective function to to which the pair of sentences s i and s j infers s k . We can also extend our model so that it can handle 1-to-many as-signment, or even many-to-many assignment.  X  Other relations between sentences : we used baseline methods for recognizing textual entailment. Our model is expected to perform better with a support of more sophisti-cated entailment engines [1]. We can use any inter-sentence relations, such as kernel functions between sentences [16].  X  Efficient algorithms : the proposed method requires much computational time. We need to incorporate efficient ap-proximation techniques [14].
We proposed a novel text summarization model based on the budgeted median problem. The proposed model covers the entire document cluster through sentence assignment, since in our model every sentence is represented by one of the selected sentences as much as possible. An advantage of our method is that it can incorporate asymmetric relations between sentences in a natural manner. [1] R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, [2] J. M. Conroy, J. D. Schlesinger, J. Goldstein, and [3] Z. Drezner and H. W. Hamacher, editors. Facility [4] Document Understanding Conference. HLT/NAACL [5] E. Filatova and V. Hatzivassiloglou. A formal model [6] J. Hromkovi X c. Algorithmics for Hard Problems . [7] C. Lin. ROUGE: a package for automatic evaluation [8] I. Mani. Automatic Summarization . John Benjamins [9] R. McDonald. A study of global inference algorithms [10] T. Nomoto and Y. Matsumoto. A new approach to [11] D. R. Radev, H. Jing, M. gorzata Sty  X s, and D. Tam. [12] P. Rojeski and C. S. ReVelle. Central facilities [13] V. Rus, A. Graesser, P. M. McCarthy, and K.-I. Lin. [14] D. B. Shmoys. Approximation algorithms for facility [15] H. Takamura and M. Okumura. Text summarization [16] D. Zelenko, C. Aone, and A. Richardella. Kernel
