 Charu C. Aggarwal  X  Yao Li  X  Philip S. Yu Abstract Many applications such as social networks, recommendation systems, email com-munication patterns, and other collaborative applications are built on top of graph infrastruc-tures. The data stored on such networks may contain personal information about individuals and may therefore be sensitive from a privacy point of view. Therefore, a natural solution is to remove identifying information from the nodes and perturb the graph structure, so that re-identification becomes difficult. Typical graphs encountered in real applications are sparse . In this paper, we will show that sparse graphs have certain theoretical properties which make them susceptible to re-identification attacks. We design a systematic way to exploit these theoretical properties in order to construct re-identification signatures , which are also known as characteristic vectors . These signatures have the property that they are extremely robust to perturbations, especially for sparse graphs. We use these signatures in order to create an effective attack algorithm. We supplement our theoretical results with experimental tests using a number of algorithms on real data sets. These results confirm that even low levels of anonymization require perturbation levels which are significant enough to result in a massive loss of utility. Our experimental results also show that the true anonymization level of graphs is much lower than is implied by measures such as k -anonymity. Thus, the results of this paper establish that the problem of graph anonymization has fundamental theoretical barriers which prevent a fully effective solution.
 Keywords Graphs  X  Anonymization  X  Privacy 1 Introduction The problem of privacy-preserving data mining has gained importance in recent years because of increasing concerns about compromising private information [ 3 , 4 , 9 , 14 , 15 , 17 , 18 ]. Detailed surveys on privacy may be found in [ 2 ]. In many graph and social network applications, it may sometimes be desirable to release a node de-identified network for data mining and analysis. This may however not be sufficient protection, if the identities on the nodes can be attacked with the use of background information about network structure. Many social network sites such as Facebook and Twitter allow the determination of partial knowl-edge about portions of the underlying network. Similarly, the structure of many organizational networks is often partially or fully known. This background information about the network structure can be used in order to determine the identities of the de-identified nodes. It was observed in [ 6 ] that pure de-identification is not sufficient for effective privacy preservation.
A natural approach for privacy preservation is to perturb the structure of the network in order to prevent re-identification. A lot of research has recently been devoted to the problem of graph anonymization [ 8 , 16 , 22  X  24 ]. These methods use techniques such as duplicating, removing, switching, or grouping vertices or edges in input graphs. Some of the techniques [ 13 ] rely on completely random edge additions and deletions, whereas others [ 16 ] use more carefully designed anonymization methods. However, such approaches are not completely immune to privacy attacks. A number of techniques [ 12 ] have been proposed in the literature in order to attack anonymized social networks. Some examples of such techniques are as follows:  X  (1)Degree-basedattacks: Inthiscase,weconstructthedegreesignatures ofdifferentorders which are targeted around a given node. The signature of the i th order is represented by a set and is denoted by Q i ( x ) .Theset Q 0 ( x ) represents the label of node x ,theset Q 1 ( x ) is the degree of node x ,theset Q 2 ( x ) is the multi-set of each neighbor X  X  degree, and so on. This signature provides a characterization of each node and can be used in order to re-identify the nodes. While this characterization may not always be unique, it may provide a good re-identification in many cases.  X  (2) Subgraph attacks: In this case, we assume that the structures of particular subgraphs around a target node are known a priori. This is used for re-identification.  X  (3) Distance attacks: We assume that the distances to a pre-defined set of nodes are known a priori. This creates a unique signature for re-identification purposes.

Many of the proposed attacks in the literature are ad-hoc and tend to depend upon specific structural characteristics of the graphs. These techniques do not try to construct a system-atic way to characterize the overall linkage behavior of the underlying data, or provide a theoretical understanding of why a particular attack should be successful. In this paper, our goal is to create such a systematic characterization of the linkage structure of the graph. We provide a theoretical understanding of its effectiveness and use the corresponding linkage characterization in order to create a re-identification attack algorithm. The re-identification algorithm uses the aggregate covariance behavior of the network linkage structure. We will see that this attack measure is far more robust to edge perturbations as compared to typical utility measures such as the distances between nodes. This is essentially because our attack measure is based on robust statistical quantifications which depend upon the aggregate net-work structure , whereas typical utility measures such as distances are highly sensitive to the behavior of a few edges. Thus, utility is degraded much faster than privacy is achieved. Thus, the results of this paper establish that the linkage behavior of graphs contains robust statis-tical information which is hard to hide with the use of structural anonymization techniques. This establishes the significant challenges in the problem of graph anonymization.
In most cases, social network graphs such as Facebook are not fully known because of a variety of constraints such as scalability. Typically, only a small portion of the social network is known in most cases. Therefore, we assume the identities and structure of only a small subset of the nodes in the original network graph. This is the only background knowledge available and reflects only partial information about the underlying graph. For the case of the anonymized graph, no assumptions are made about knowledge of the identities of the nodes. Typically, the anonymized network graph is released with other sensitive information, and this can be inferred if the identities of the nodes are inferred. Therefore, it is crucial to retain the anonymity of these nodes in order to protect this sensitive information. It should be pointed out that the results in this paper are designed for the edge perturbation model. This model is the most commonly used one in social networking anonymization. In future work, we will also examine the generalizability of these results to other models.

This paper is organized as follows. In Sect. 2 , we will show that the linkage behavior of typical networks is robust to perturbations. In Sect. 3 , we leverage this statistical robustness in order to create a characteristic vector or signature for each node and use it to create a re-identification attack. Section 4 provides the experimental results. Section 5 contains the conclusions and summary. 1.1 Related work and contributions The problem of privacy-preserving data mining has been studied extensively in recent years [ 2 ] because of increasing concerns about the security of personal information stored about individuals. One of the earliest methods for privacy-preserving data mining was that of perturbation [ 3 , 4 ]. While the perturbation method is focused on attribute value privacy ,it is not designed for the issues of anonymity and re-identification . For this purpose, group-based anonymization [ 17 , 18 ] is particularly useful. One such technique is the method of context of multi-dimensional data. Recently, these techniques have also been extended to graph anonymization [ 8 , 12 , 16 , 24 ].

A number of attack techniques have also been designed for graph data. For example, the methods in [ 20 , 21 ] discusses methods for reconstructing randomized graphs. However, this approach is focused on link re-construction rather than the anonymity issues associated with node re-identification .Theworkin[ 12 ] discusses some ad-hoc attack techniques for specific kinds of graphs, but does not try to develop a theoretical framework for why networks should be hard to anonymize. In this paper, we will show that networks in real scenarios have a number of natural characteristics which make them susceptible to attacks. We use these char-acteristics in order to create a re-identification attack. This re-identification attack continues to retain its effectiveness, even when the perturbation is sufficient to significantly destroy the utility in the underlying graph, as measured by the intra-node distances. Furthermore, the effective anonymization levels reached are much lower than those implied by the privacy parameters. Thus, the results of this paper theoretically and experimentally demonstrate the fundamental challenges in the problem of graph anonymization.

This paper is an extended journal version of an earlier conference paper [ 1 ]. Compared to the original paper, this version contains significantly detailed exposition and discussion, with proofs of theoretical results. More data sets have been added to the experimentation, and trade-offs are explored in terms of how privacy and utility relate to one another. Furthermore, experiments for a random edge deletion algorithm have been added to the paper. 2 Network linkage signatures We will begin by introducing some notations and definitions. We assume that the graph G is denoted by ( V , E ) ,where V is the set of vertices and E is the set of edges. For simplicity, we assume that the edges in the graph are directed, though a similar analysis also applies to the case of undirected networks. We assume that the total number of vertices in the network is denoted by | V |= N . One property of large graphs from real-world scenarios is that they are typically sparse . By sparsity, we refer to the fact that the number of edges in the network is much smaller than the total number of possible edges. We will see that such natural structural properties make graphs fundamentally harder to anonymize. It is assumed that a set S of labels in the original graph G is available. The anonymized graph labels that are released by G = ( V , E ) . The node re-identification problem is defined as follows: Definition 2.1 ( Node Re-identification Problem )Givenagraph G = ( V , E ) with labeled set S , and an anonymized graph G = ( V , E ) , determine the matching set of S in the anonymized graph G . The matching is based on the unique identity of the actor representing that node.

We will use a careful analysis of the correlations in the linkage structure of the nodes in order to construct our attack algorithm. Unlike degree information, this is second-order correlation information about the linkage structure, which is particularly robust to random edge perturbations in real-world networks. We will illustrate this robustness with the help of some theoretical results. First, we will define the concept of linkage covariance between a given pair of nodes. For a given node i ,let  X  X i represent the random 0 X 1 variable, which takes on the value 1, if node i is linked by a directed edge to any particular (potentially adjacent) node and 0 otherwise. Thus, we have instantiations of this random variable for all possible (potentially) adjacent nodes j , and the corresponding instantiation is denoted by x ij .The value of x ij is 1, if a directed edge does indeed exist from node i to node j . We denote the linkage covariance between nodes p and q by LinkCo v( p , q ) and define it as follows: Definition 2.2 The linkage covariance between nodes p and q is denoted by LinkCo v( p , q ) and is equal to the covariance between the random variables  X  X p and  X  X q . Therefore, we have The last equation expands the formula for covariance by using the data instantiations asso-ciated with the edges in the underlying network.

Next, we examine the effect of the addition of edges to the link covariance. Let us assume that the node p has n p outgoing edges, and the node q has n q outgoing edges. We denote the number of edges from p and q which point to the same node by m pq . Let us also assume that an edge between any pair of nodes ( p , q ) is added with probability f a . We note that f a is typically a small quantity, because most real networks are extremely sparse, and the number of edges that are added are also likely to be smaller than the number of edges in the original network. The latter is usually necessary in order to preserve the utility of the underlying network. Then, we can show the following result: Lemma 2.1 Let L be the estimated value of the link covariance between nodes p and q after the addition of edges with probability f a . Then, the expected value of the estimated link covariance L is related to the true link covariance LinkCo v( p , q ) by the following relationship: f  X  ( n p / N + n q / N  X  2  X  m pq / N ) + f 2 by the sum of the following items: (2) The increase in value because of nodes which do not have an edge incident from both p and q . Note that the fraction of values which do not have an edge from either p or q is given by ( 1  X  n p / N  X  n q / N + m pq / N ) . Therefore, the expected increase because of approximation is true for real-world graphs in which n p and n q are much smaller than N . (3) The increase in value because of nodes which have exactly one edge incident from either p or q . This value is given by f a  X  ( n p / N + n q / N  X  2  X  m pq / N ) .
Similarly, the expected value E [  X  X p ] X  E [  X  X q ] after the addition of new edges is given by ( n p / N + f a )  X  ( n q / N + f a ) . Therefore, by substituting the above expressions, we can derive that the new link covariance E [ L ] after addition of edge is given by: The result follows.

We note that the value of f a is a fraction, which is less than one. Since the link covariance between a pair of nodes changes by only 2  X  f a  X  m pq / N , this suggests that the change in link covariance is dependent on the value of m pq / N .Thevalueof m pq is the number of nodes to which both nodes p and q have a connecting edge. This value is smaller than the degree of both the nodes p and q and is especially very small for sparse graphs. For sparse graphs in which the value of N is much larger than the value of m pq , the value of m pq / N becomes extremely small. This also means that the overall change in the link covariance is extremely small for sparse graphs. In the experimental section, we will show that the link covariance measure between nodes changes very little from edge perturbations, as compared to (utility-driven) measures such as distances between nodes. This suggests that it is possible to systematically leverage the linkage covariance in the large graph in order to create a node re-identification mechanism which is robust to edge perturbations.

The robustness of the link covariance measure is dependent on the fact that it is dependent on the aggregate statistical behavior of all nodes. We can further increase the robustness of this measure by using the pairwise information about the link covariances in a more coordinated way. Specifically, for a given node, we can define a vector of link covariances to the other nodes in the graph in both the de-identified network and the original network. The key is to define a mapping of nodes from one network to the other, such that the ordered vectors created by this mapping are very similar to one another. Since each link covariance value is only slightly affected by the randomization process, it follows that the vector of link covariances is also not affected significantly. We will use this vector in order to create a characteristic vector for each node. This characteristic vector is a representation of the relationship of the linkage behavior of a node with all other nodes in the graph. The small variation in the link covariance will ensure that the characteristic vector is quite robust to the anonymization process. Before discussing the characteristic vector in detail, we will first show that the result discussed above is also true for edge deletion, though the proof technique is slightly different. This is because anonymization techniques use both edge additions and deletions for the transformation process.
 Lemma 2.2 Let L be the estimated value of the link covariance between nodes p and q after the deletion of edges with probability f d . Then, the expected value of the estimated link covariance L is related to the true link covariance LinkCo v( p , q ) as follows: f items: (2) The decrease in value because of deletion of one of the 2  X  m pq edges incident on the nodes which have edges incident from both p and q . Ignoring terms of the order of f 2 d ,this value is 2  X  f d  X  m pq .

Therefore, the value after edge deletion is given by m pq  X  ( 1  X  2  X  f d ) . Similarly, the ( 1  X  2  X  f d ) .
 We note that the deletion probability f d has a different interpretation than the addition prob-ability f a , but the value of f d also needs to be small enough to retain the basic structure of the overall graph. This is because for large values of f d , the basic structure of the underlying graph may be lost. Furthermore, we note that the final expected value of the link covariance after edge deletion continues to be proportional to the initial value. We will see that this is also very helpful for creating a re-identification attack.

Since the link covariances for a given node do not change very easily, they can be used to define a signature or characteristic vector for that node. The characteristic vectors for the nodes in the background knowledge graph and the de-identified graph can be matched with one another in order to create a re-identification attack. There are several ways of defining this signature or characteristic vector, depending upon the following factors:  X  Whenthemappingbetweenthetwographsarecompletelyunknown,wecancreateavector of link covariances, which are sorted in decreasing values. This creates a monotonically decreasing vector.  X  When the mapping between the two graphs is approximately known for all nodes, we can create a sort order of nodes in the two graphs corresponding to this mapping. The vector is defined with respect to this sort order. This provides more accurate results, when we match the signatures between the two graphs.  X  In some cases, an approximate mapping is known for some of the nodes, but not others.
In such cases, we use a sort order on the nodes for which the mapping is known and use a sort order on the magnitudes of the link covariances in order to define the remaining part of the signature. All of the above definitions are quite useful, because we often start off with no mapping between the nodes and then partially define the mappings. Correspondingly, we will define different kinds of vectors which will be required during different phases of the matching process. We will use an iterative matching approach, in which the first iteration uses one kind of the characteristic vector with no information about the mapping, and later iterations use other forms of the characteristic vector with partial information about the approximate matching.

When we do not even have any approximate information about the matching between the two graphs, then the most natural form of the characteristic vector of node p is defined as the ranked link characteristic vector . This is defined as the characteristic vector obtained by ordering the nodes in decreasing order of the link covariance with respect to node p .This is defined with respect to an unordered node set S , and the ranking is imposed based on the behavior of the link covariance of p to other nodes. The ability to use an unordered set allows us to create a signature for any node in each of the graphs and match the corresponding signatures.
 Definition 2.3 The ranked link characteristic vector RC ( p , S ) for a node p and unordered node set S ={ i 1 ... i s } is defined as the s -component vector ( LinkCo v( p , j 1 )... LinkCo v( p , j s )) , where the ordering j 1 ... j s is defined in order to ensure that LinkCo v ( p , j i )  X  LinkCo v( p , j i + 1 ) . Ties are broken using the lexicographic ordering of the node identifiers.

In other words, the node set S is a priori unordered , but we impose the rank ordering of the nodes in S in decreasing link covariance with respect to node p . We use this to construct a smooth monotonically decreasing curve representing the variation in this link covariance. It is evident that the shape of this curve could vary quite significantly with the underlying node. This shape is the characteristic of that node and is used for the matching process. We denote the ranked link covariance by RC ( p , S ) . The vector RC (  X  ) can be computed without any knowledge about the mapping, and this is useful in the initial stages of the re-identification process, when no information about the matching of the nodes is known. Therefore, a natural strategy at the initial stages of re-identification is to use an ordering which is specific to each node and is based on a ranking of the magnitude of the link covariance. Next, we will define the concept of a characteristic link vector for a given node, with respect to a known ordering. The characteristic vector is defined with respect to a node p ,andan ordering S o of the node set S . Therefore, S o is considered a sequence of nodes, representing an ordering of the nodes p and ordered vector S o is denoted by C ( p , S o ) .
 Definition 2.4 The characteristic vector C ( p , S o ) for a node p and ordered set S o = { i 1 ... i s } is defined as the s -component vector ( LinkCo v( p , i 1 )... LinkCo v( p , i s )) .
The above vector is useful in situations in which a mapping between all the nodes of the two graphs is approximately known. We note that the concepts of C ( p , S o ) and RC ( p , S ) are two extreme ways of creating the characteristic vectors corresponding to whether no mapping is known or (an approximate version of) the complete mapping is known. In cases in which the mapping of only a subset S of the nodes is known, it is also possible to create a partially ranked characteristic vector PRC ( p , S o , V ) with respect to the ordered set S o and the unordered set V  X  S o ,where V is the vertex set of the entire graph. This is essentially a combination of the two cases above in which we use the known portions of the ordering in the subset S o and use a sort order on the link covariance for the other nodes. Definition 2.5 Let V denote the vertex set of the entire graph, and S o be an ordered sub-set of V . Then, we define the partially ranked characteristic vector PRC ( p , S o , V ) by concatenating C ( p , S o ) and RC ( p , V  X  S o ) . In other words, we have PRC ( p , S o , V ) = (
C ( p , S o ), RC ( p , V  X  S o )) .
 We will see that the above definition is particularly useful, when the mapping information about a subset of the nodes is known. All of the vectors above can be normalized by dividing by their L 2 modulus. We call such vectors as the unit characteristic vectors. Throughout the next section, we will utilize the unit characteristic vectors in order to perform re-identification attacks. 3 Re-identification attack For the base graph G , in which the identities are available, we assume that background information is available in terms of the links of these nodes. While the entire network is not available, it is reasonable to assume that in many graphs and social networks, small localities around specific nodes are known. This is of course dependent upon the network which is being attacked. For example, a social network such as Twitter has an open API, and the entire linkage structure of the network can be known. Therefore, the release of any sensitive information along with the de-anonymized graph can be highly revealing, because a lot of network structure is available in order to determine the node identities. On the other hand, in social networks such as Facebook , it is possible to query most nodes of the graph in order to know the corresponding linkages. However, the site has built-in protections to prevent the automated crawl of the whole network. Therefore, it is reasonable to assume that only a subset of the network can be manually explored.

We assume that for a given set S of nodes, its links, and the links 1 of its neighbors are known. Thus, all nodes up to a distance of two from specifically targeted nodes are known. Thisleveloflocalexplorationisoftenpossibleinmanyreal-worldgraphsandsocialnetworks. The goal of the re-identification algorithm is to determine a node set S from the de-identified graph , so that a one-to-one correspondence can be created from nodes in S to nodes in S .This leads to disclosure of the identities of the nodes in S , and therefore, the sensitive information associated with nodes or edges can be breached. This one-to-one matching can be defined by picking an ordering of nodes in S , which matches the corresponding ordered nodes in S . We define the goodness of the re-identification as the link similarity between S and S .The link similarity between S and S is defined as the dot product between the corresponding characteristic vectors: Definition 3.1 The link similarity between the ordered set of (labeled) nodes S from the publicly available graph and the set of nodes S in the de-identified graph is defined as the dot products between their normalized partially ranked characteristic vectors .

We note that this dot product always lies in the range ( 0 , 1 ) because of the normalization process. The higher the dot product between the characteristic vectors, the greater the link similarity between the corresponding pair of nodes. We make the following observation: Observation 3.1 Ifwehaveapairofidenticalgraphs G and G (withoutedgerandomization) and we correctly match a set of nodes S in G to a set of nodes S , then the dot product between the corresponding characteristic vectors is 1.
Our problem of determining the matching from the publicly available portions of the graph to the anonymized graph is as follows: Problem 1 Determine an ordered set S in the de-identified and perturbed graph G ,so that the dot product of its characteristic vector with the characteristic vector of the ordered (identified) set S from the base (publicly available) graph G is maximized.

The above problem is hard to solve even in the case when the edges are not perturbed, becauseitisessentiallyageneralizedversionofthematchingproblemingraphs.Thisproblem is well known to be NP-hard.
 Lemma 3.1 The graph re-identification problem is NP-hard.
 Proof Sketch: Note that the graph re-identification problem is a generalized version of the graph matching problem under isomorphism. This problem is well known to be NP-hard [ 11 ] and can be easily reduced to the re-identification problem, by setting S to the entire set of nodes.

Therefore, we will design an effective heuristic for the problem. We will show that this approach is usually sufficient to re-identify most of the nodes in the labeled set. In order to design the re-identification algorithm, we will make a number of approximation assump-tions based on the behavior of the algorithm. First, we note that most real-world graphs are extremely sparse and a very small fraction of the nodes have large degrees. For example, a typical social network such as Facebook may have millions of nodes, but most nodes have links to less than a hundred nodes. In general, we make the assumption that the average degree of a node is significantly less than network. This makes it easy to perform the following approximation for the link covariance. As before, we assume that  X  X i represents the random 0 X 1 variable, which characterizes the linkage of node i to node j . The instantiations of this random variable to the different adjacent nodes j are denoted by x ij .
 Approximation 1 The linkage covariance between nodes p and q is denoted by LinkCo v ( p , q ) can be approximated as the expectation of the product of random variables  X  X p and  X  X q . Therefore, we have The important point to show here is that the characteristic vector encodes the relative behavior of the link covariances, and this relative behavior is unaffected by this approximation. In order to elaborate further, we express the link covariance as follows: In order to show that the relative behavior of the link covariances is unaffected by this approximation, we have to show that nonzero values of the second term are significantly smaller in magnitude than nonzero values of the first term. Note that the least nonzero value ofthefirsttermis1 / N , which corresponds to the case that p and q share exactly one link. The  X  X ypical X  value of the second term is approximately d 2 / N 2 ,where d is the average degree of the nodes. Since d &lt;&lt; shape of the characteristic vector is likely to be well estimated with the use of this first-order approximation.

A second property of this approximation is that it allows the computation of the charac-teristic vector for a node with the use of a small amount of local information around that node . This holds true for all forms of the characteristic vector (ranked with respect to a full or partial ordering of nodes). Specifically, we make the following claim: Observation 3.2 Any of the characteristic vectors for a node p can be approximated with the use of only the link information of node p and its immediate neighbors.
 This is because any other node q which has a nonzero value of E [  X  X p  X   X  X q ] mustlinktoat least one node which is a neighbor of node p . Therefore, by examining all the inlinks of the neighbors of p and determining the number m pq of neighbors of p which contain an inlink of all nodes which do not share a common neighbor with node p is set to 0.

Thus, if the local link structure of a subset of nodes S is known, then it can be used in order to construct the (ordered, partially ordered, or unordered) characteristic vector. Of course, in order to construct the re-identification, we need to determine two things:  X  For the node set S  X  V in G , we need to determine the corresponding set S  X  V in G .  X  X orthesets S and S , we need to construct a one-to-one matching of the nodes, so that We will transform the problem to a bipartite graph matching problem. Let G = ( V , E ) and G = ( V , E ) be the original and de-identified graphs, respectively. It is assumed that we have local information about a subset S of nodes of the original graph G .Wecreatea bipartite graph H = ( V  X  V , D ) , where one partition of the bipartite graph contains nodes corresponding to V of G , whereas the other partition contains nodes corresponding to V of G . The edge set D contains an edge from each node of S  X  V to every node of V .The weight of an edge between i  X  V and j  X  V is (initially) defined as the dot product of the normalized characteristic vectors for nodes i and j with respect to the entire vertex sets in the two graphs. In other words, we use the dot product of RC ( i , V ) and RC ( j , V ) .Intheevent that the graphs do not have the same number of vertices, we append zeros at the end of the smaller of the two characteristic vectors, so that a dot product can be performed. For a given set S  X  V ,a bipartite matching is defined as a set of node-disjoint edges, such that there is one edge from a node in S to a node in V . These defines the one-to-one mapping between the nodes of the two graphs. Clearly, the determination of the maximum weight one-to-one matching from the subset of nodes S to the nodes in V determines a matching of the nodes which maximizes the similarity between the corresponding nodes. Polynomial algorithms for this problem are discussed in [ 5 ].

The first matching provides a correspondence of nodes in S and S . This can be used to which is ranked by link covariance size, since we have no a priori information about the mapping. We can then use the matching generated from this first phase in order to further improve the quality of the assignment by using information about the approximate mapping which is generated. We achieve this by using partially ranked characteristic vectors for the set S and S which have already been matched. Then, we create partially ranked characteristic vectors with this ordering on S and S using the first matching and re-define the weights as the dot products on the (normalized) partially ranked characteristic vectors. As in the previous case, we handle inequality in the sizes of the two graphs by adding zeros to the ranked part of the characteristic vector. We solve the matching problem again with re-defined weights, in order to re-define the matching between S and S . We repeat the iterative process of re-defining weights from matchings and then matchings from weights. In practice, it was found that by using this approach for a small number of iterations (typically 3), we were able to reach convergence in which the objective function did not improve significantly in successive iterations. An overall description of the attack algorithm is provided in Fig. 1 .
Next, we discuss the time complexity of the one-to-one matching problem. The bi-partite graph contains O ( n 2 ) edges, where n is the total number of nodes in the original graph. For a graph containing n nodes and m edges, most efficient algorithms can be implemented within O ( time is O ( n 2 . 5 ) . 4 Experimental results The effectiveness of a privacy-preservation technique depends upon it ability to retain util-ity after the privacy-transformation process. On the other hand, if the process of privacy-transformation results in too much perturbation, so as to create a destruction of utility, then the privacy-preservation process becomes self-defeating. One of the challenges in the case of graph mining is that typical utility-based measures such as distances are brittle and far more sensitive to edge perturbations as compared to the more robust link covariance measure . In this section, we will show that a much larger percentage of the nodes can be typically identified, than is implied by measures such as k -anonymity. We will also demonstrate the brittleness of graph-distance measures in comparison with the graph privacy quantifications. Therefore, we will show this comparison between different privacy and utility measures, which are defined as follows: (1) Distance perturbation (Utility Measure): This measure tests the brittleness of distance measures in the graph with increasing perturbation. Distance perturbation is a good utility metric, because may natural mining properties of the graph such as unsupervised clustering or supervised classification are dependent on distances between nodes. Therefore, the per-turbation of this metric also affects other key-mining properties. For this purpose, we sample k pairs of nodes and determine the distances between these pairs both in the original graph and in the perturbed graph. We compute the standard deviation of these k different distance values. We compute the number of node pairs for which the change in distances between node pairs from the original to the perturbed graph is greater than one standard deviation of the original distances between the node pairs. We note that lower values of the distance perturbation are more desirable because it implies a better preservation of utility. (2) Link Covariance Perturbation (Privacy Measure): We sample the same pairs of nodes as above and calculate the link covariances between them. As in the previous case, we calculate the standard deviation of the link covariance of the different node pairs. As in the previous case, we compute the number of node pairs for which this change is greater than one standard deviation. Thus, in this case, lower values of the link covariance perturbation imply that privacy can be attacked, because it means that the link covariance-based attack signatures are robust to the underlying anonymization process. (3) Node Re-identification Rate (Privacy Measure): A second and more explicit measure is the node re-identification rate . This is the percentage of nodes in the labeled set which are accurately identified with the use of the matching algorithm based on the characteristic vector. This is a more direct measure of the anonymization level and will complement our other privacy measure based on the link covariance perturbation. This can also be used to measure the true effectiveness of a k -anonymization algorithm. For example, for a k -anonymization algorithm, the re-identification rate should be no larger than 1 / k . We will show that the re-identification rates are significantly larger than the expected re-identification rates of k -anonymization algorithms.

By varying the input perturbation level for a given privacy algorithm, both the distance perturbation (utility) and link covariance perturbation (re-identification ability) will vary as well. One of our goals is to show that the privacy measure continues to be quite robust as compared to the distance-based utility measure, and therefore, the re-identification behavior continues to be quite robust as well. 4.1 Data sets We used three different data sets in order to test the effectiveness: (1) Power Grid Graph: This graph 2 consists of 4940 vertices and 6594 edges in a power grid network. Each vertex stands for a generator/transformer and substation and the edge between pairs of them represent the power line in the network. This data set was also used in [ 13 ]. (2) Co-author Graph: This data set 3 is a collection of bibliographies of scientific literature in computer science from various sources, covering most aspects of computer science. It contains 7955 vertices and 10055 edges. An edge will exist between a pair of authors who co-authored a paper. This data set was also used in [ 13 ]. (3) DIP (Database of Interacting Proteins): The DIP database 4 records experimentally deter-mined interactions between proteins. The graph includes 19928 vertices and 82406 edges.
In all cases, we assumed that we have background knowledge about the linkage structure of only a very small percentage of nodes in the underlying graph . This corresponds to the 2-hop neighborhood of the graph. In the case of the Power Grid and Co-Author graphs, we assume that information about the linkage structure of only 1 % of the nodes is known. For the case of the (slightly larger) DIP graph, we assume information about the linkage structure of only 0 . 5 % of the nodes. Furthermore, only the labels of these nodes are known. 4.2 Results We tested two different algorithms for its resistance to privacy attacks: a random edge addition X  X eletion algorithm [ 13 ], and a degree-based k -anonymization algorithm with the use of the greedy swap method [ 16 ]. First, we present the relative robustness of the utility and privacy measures with the use of the random edge addition and deletion algorithm [ 13 ]. In Fig. 2 a, c, and e, we have illustrated the percentage of node pairs for which the distance values (utility measure) and the link covariance measure (privacy measure) are perturbed by more than one standard deviation. The figures are for the case of power grid , Co-Author and DIP data sets, respectively. In each case, the X -axis contains the percentage of edges added or deleted, and the Y -axis contains the percentage of node pairs which are perturbed by more than one standard deviation. It is evident that in each case, an increasing perturbation level greatly increases the percentage of node pairs for which the utility perturbation is greater than a standard deviation. For example, for the case of the Power Grid data set (Fig. 2 a), the distances between 56 . 87 % of the node pairs are perturbed by more than a standard deviation. On the other hand, link covariances between only 0 . 03 % of the node pairs are perturbed by more than a standard deviation, when 7 % of edges are added or deleted in the original graph. Thus, the attack signatures that are constructed from link covariance measures are much more robust to the perturbation process. The results for the Co-Author and the DIP data sets are quite similar and are illustrated in Fig. 2 c and e, respectively. In this case, when 7 % of the edges are added or deleted, the distances between 21 . 57 and 38 . 2 % of the node pairs are, respectively, perturbed by more than one standard deviation. On the other hand, the link covariance measures are not affected much by the perturbation process. Later, we will use more direct measures such as the re-identification rate in order to show that privacy continues to be compromised in the presence of considerable distance perturbations.
We also tested the trade-offs for the case of the second k -anonymity-based algorithm [ 16 ]. We illustrated the results for the Power Grid , Co-Author ,and DIP data sets in Fig. 2 b, d, and f, respectively. In this case, the Y -axis illustrates the privacy and utility measures as in the case above. On the other hand, the X -axis is different. In this case, the X -axis illustrates the value of k for the k -anonymity level. The anonymity level was varied between 5 and 30. The percentage of distance pairs significantly perturbed increase monotonically with increasing anonymity. In the case of the Power Grid data set illustrated in Fig. 2 b, about 60 % of the distances between node pairs were perturbed by more than a standard deviation at an anonymity level of 30. Thus, distances between a majority of the node pairs are affected significantly by the perturbation process. This implies that distance-based utility may be seriously compromised at this level of anonymity. On the other hand, the link covariance measures for privacy are not affected too significantly. In fact, only 0 . 02 % of the nodes show a significant change (more than one standard deviation) in the link covariance measure with increasing perturbation. This is because of the greater statistical robustness of our technique for characterizing the underlying structural behavior. Similar results are obtained for the Co-Author and DIP data sets illustrated in Fig. 2 d and f, respectively. As in the case of the Power Grid data set, the distance perturbation increased monotonically with anonymity level. In the case of the Co-Author data set, 20 . 76 % of the nodes were perturbed significantly at an anonymity level of 30, whereas in the case of the DIP data set, about 49 % of the nodes were perturbed significantly at an anonymity level of 30.

All of the above figures show that the link covariance measure is extremely robust to input data perturbation, as compared to the distance measures. Therefore, it is also interesting to explore more direct measures of the privacy, which test the percentage of nodes which can be re-identified with the use of an attack algorithm. As in the previous case, we also plot the distance-based utility measures along with the re-identification level. We note that these distance-based utility curves are essentially replicated from Fig. 2 a X  X . One issue with showing utility and privacy on the same plot in this case is that the two quantities correspond to different units of reference. For example, the privacy corresponds to the percentage of nodes re-identified, whereas the utility corresponds to the percentage of node pairs for which the distance changes by more than a standard deviation. Thus, we have used two different Y-axes ,aleft Y -axis, which corresponds to the re-identification rate (or percentage of nodes accurately re-identified by the privacy attack), and a right Y -axis which corresponds to the percentage of node pairs for which distances are perturbed by more than a standard deviation. Each figure contains two plots (for utility and privacy), and the corresponding axes are used by each of the plots.
 The results for the random edge addition X  X eletion algorithm for the case of the Power Grid , Co-Author ,and DIP data sets are illustrated in Fig. 3 a, c, and e, respectively. In each case, we have illustrated the privacy and utility figures. While the variation of utility perturbation with input perturbation was different for each data set, it is more interesting and meaningful to examine the data-specific input perturbation level at which re-identification rate was greater than 30 %. This corresponds to an average anonymity level of only 3.33. It is instructive to note that at this very high re-identification rate (or low anonymity level), more than 55 %, 20 and 30 % of the distance pairs were perturbed significantly in the Power Grid, Co-Author and DIP graphs, respectively. In the case of the Power Grid data set, when the input data perturbation was reduced, so that the re-identification rate was increased to 50 % (average anonymity of only 2), more than 50 % of the distance pairs were still perturbed by more than a standard deviation. Thus, even in the case of such a low average anonymity, a large fraction of the distance pairs are significantly perturbed. This makes it hard to achieve a significant level of anonymity and utility at the same time.

We also tested the results for the k -anonymity-based algorithm [ 16 ]inFig. 3 b, d, and f, respectively. One of the key observations was that the re-identification rate was much higher , essentially implies that a significant compromise of utility is still not sufficient to retain the privacy implied by the k -anonymization approach.

Finally, we tested the impact of the approximation of Eq. 1 . Note that the approximation only reduces the re-identification rate, though it makes the algorithms faster, and does not require knowledge about the degrees of the nodes. Nevertheless, we also tested how the algorithms would work if the approximation was not used and exact algorithms were used. The results for both anonymization algorithms for the PowerGrid graph are illustrated in only by a small amount. Therefore, the use of the approximation is a simple and practical alternative to the exact version of the algorithm. 5 Conclusions In this paper, we examined the problem of graph anonymization and proposed a technique for mounting re-identification attacks on the nodes. We presented theoretical results which suggest that such attacks are likely to be particularly robust to perturbation in the case of real-world graphs. We also present experimental results which confirm that such characterizations of the graph linkage structure are much more robust to perturbation than distance-based utility measures. Our results show that the typical re-identification rates from such attack methods are also much greater than is implicitly implied by k -anonymization techniques. Thus, the results of this paper establish the fundamental challenges in the problem of graph anonymization.

It would also be interesting, in future work, to extend the results of this paper to the problem of anonymization with the use of clustering [ 19 ]. These methods use node contractions for the anonymization process, and therefore, it would be interesting to explore how the covariance-based matching process can be explored for such scenarios. In such a case, the matching process would need to be applied with portions of the graph which are retained.
 References
