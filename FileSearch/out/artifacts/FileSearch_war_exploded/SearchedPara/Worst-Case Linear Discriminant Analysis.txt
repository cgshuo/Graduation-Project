 With the development of advanced data collection techniques, large quantities of high-dimensional data are commonly available in many applications. While high-dimensional data can bring us more information, processing and storing such data poses many challenges. From the machine learning perspective, we need a very large number of training data points to learn an accurate model due to the so-called  X  X urse of dimensionality X . To alleviate these problems, one common approach is to perform dimensionality reduction on the data. An assumption underlying many dimensionality reduction techniques is that the most useful information in many high-dimensional datasets resides in a low-dimensional latent space. Principal component analysis (PCA) [8] and linear discriminant analysis (LDA) [7] are two classical dimensionality reduction methods that are still widely used in many applications. PCA, as an unsupervised linear dimensionality reduction method, finds a low-dimensional subspace that preserves as much of the data variance as possible. On the other hand, LDA is a supervised linear dimensionality reduction method which seeks to find a low-dimensional subspace that keeps data points from different classes far apart and those from the same class as close as possible.
 LDA. We then establish that conventional LDA seeks to maximize the average pairwise distance between class means and minimize the average within-class pairwise distance over all classes. Note To put this thinking into practice, we incorporate a worst-case view to define a new between-class scatter measure as the minimum of the pairwise distances between class means, and a new within-class scatter measure as the maximum of the within-class pairwise distances over all classes. Based on the new scatter measures, we propose a novel dimensionality reduction method called worst-case linear discriminant analysis (WLDA). WLDA solves an optimization problem which simultaneously maximizes the worst-case between-class scatter measure and minimizes the worst-case within-class e.g., below 100, we propose to relax the optimization problem and formulate it as a metric learning problem. In case both the number of training data points and the number of features are large, we propose a greedy approach based on the constrained concave-convex procedure (CCCP) [24, 18] to analyze a special case of WLDA to show its relationship with conventional LDA. We will report experiments conducted on several benchmark datasets. dimensionality reduction by finding a transformation matrix W  X   X   X   X   X  . 2.1 Objective Function We first briefly review the conventional LDA. The between-class scatter matrix and within-class scatter matrix are defined as where  X m  X  = 1  X  within-class scatter measure are defined as maximizes the ratio  X   X  / X   X  as the optimality criterion.
 By using the fact that  X m = 1  X  P  X   X  =1  X   X   X m  X  , we can rewrite S  X  as
According to this and the definition of the within-class scatter measure, we can see that LDA tries to maximize the average pairwise distance between class means {  X m  X  } and minimize the average within-class pairwise distance over all classes. Instead of taking this average-case view, our WLDA model adopts a worst-case view which arguably is more suitable for classification applications. We define the sample covariance matrix for the  X  th class  X   X  as
Unlike LDA which uses the average of the distances between each class mean and the sample mean as the between-class scatter measure, here we use the minimum of the pairwise distances between class means as the between-class scatter measure:
Also, we define the new within-class scatter measure as which is the maximum of the average within-class pairwise distances. measure to the within-class scatter measure: where I  X  denotes the  X   X   X  identity matrix. The orthonormality constraint in problem (4) is widely of W and eliminate the redundancy among all columns of W . 2.2 Optimization Procedure Since problem (4) is not easy to optimize with respect to W , we resort to formulate this dimen-sionality reduction problem as a metric learning problem [22, 21, 4]. We define a new variable  X  = WW  X  which can be used to define a metric. Then we express  X   X  and  X   X  in terms of  X  as due to a property of the matrix trace that tr( AB ) = tr( BA ) for any matrices A and B with proper sizes. The orthonormality constraint in problem (4) is non-convex with respect to W and cannot be expressed in terms of  X  .
 We define a set  X   X  as
Apparently  X   X   X   X  . It has been shown in [16] that the convex hull of  X   X  can be precisely expressed as a convex set  X   X  given by where 0 denotes the zero vector or matrix of appropriate size and A  X  B means that the matrix B  X  A is positive semidefinite. Each element in  X   X  is referred to as an extreme point of  X   X  . Since  X   X  consists of all convex combinations of the elements in  X   X  ,  X   X  is the smallest convex set that contains  X   X  , and hence  X   X   X  X  X   X  . Then problem (4) can be relaxed as  X  = {  X   X  tr(  X  ) =  X , 0  X   X   X  I  X  } . Table 1 shows an iterative algorithm for solving problem (5). problem
According to [3], we know that max  X  n tr S  X   X  o is a convex function because it is the maximum of is a convex optimization problem. We introduce new variables  X  and  X  to simplify problem (6) as
Note that problem (7) is a semidefinite programming (SDP) problem [19] which can be solved using eigenvectors of  X   X  . In the following, we will prove the convergence of the algorithm in Table 1. Theorem 1 For the algorithm in Table 1, we have  X  (  X  (  X  ) )  X   X  (  X  (  X   X  1) ) .  X   X  (  X  (  X  ) )  X   X  (  X  (  X   X  1) ) = 0 . This means which implies that  X  (  X  (  X  ) )  X   X  (  X  (  X   X  1) ) .  X  Theorem 2 For any  X   X   X  , we have 0  X   X  (  X  )  X  2 tr ( S  X  ) P  X  value of S  X  .
 Proof: It is obvious that  X  (  X  )  X  0 . The numerator of  X  (  X  ) can be upper-bounded as
Moreover, the denominator of  X  (  X  ) can be lower-bounded as constraints  X  on  X  . By utilizing Eqs. (8) and (9), we can reach the conclusion.  X  From Theorem 2, we can see that  X  (  X  ) is bounded and our method is non-decreasing. So our method can achieve a local optimum when converged. 2.3 Optimization in Dual Form In the previous subsection, we need to solve the SDP problem in problem (7). However, SDP is not scalable to high dimensionality  X  . In many real-world applications to which dimensionality reduction is applied, the number of data points  X  is much smaller than the dimensionality  X  . Under such circumstances, speedup can be obtained by solving the dual form of problem (4) instead. is the data matrix and A  X   X   X   X   X  . Then problem (4) can be formulated as where K = X  X  X is the linear kernel matrix. Here we assume that K is positive definite since the variable B = K 1 2 A and problem (10) can be reformulated as
Note that problem (11) is almost the same as problem (4) and so we can use the same relaxation technique above to solve problem (11). In the relaxed problem, the variable  X   X  = BB  X  used to the dual form facilitates kernel extension of our method. 2.4 Alternative Optimization Procedure In case the number of training data points  X  and the dimensionality  X  are both large, the above optimization procedures will be infeasible. Here we introduce yet another optimization procedure based on a greedy approach to solve problem (4) when both  X  and  X  are large.
 We find the first column of W by solving problem (4) where W is a vector, then find the second column of W by assuming the first column is fixed, and so on. This procedure consists of  X  steps. In the  X  th step, we assume that the first  X   X  1 columns of W have been obtained and we find the  X  th column according to problem (4). We use W  X   X  1 to denote the matrix in which the first  X   X  1 columns are already known and the constraint in problem (4) becomes exist. So in the  X  th step, we need to solve the following problem constraint of problem (12), we relax the constraint on w  X  as w  X   X  w  X   X  1 to make it convex. problem (12), which is the difference of two convex functions, is also non-convex. We rewrite the objective function as with respect to  X  and  X  according to [3]. Then we can use the constrained concave-convex pro-CCCP, we replace the non-convex parts of the objective function and the second constraint with following problem 4  X  , i.e., where  X  X  X  X  2 denotes the 2-norm of a vector, we can reformulate problem (13) into a second-order cone programming (SOCP) problem [12] which is more efficient than SDP: 2.5 Analysis It is well known that in binary classification problems when both classes are normally distributed solution. We will show here that this property still holds for WLDA.
 The objective function for WLDA in a binary classification problem is formulated as
Here, similar to conventional LDA, the reduced dimensionality  X  is set to 1. When the two classes of conventional LDA since w  X  S 1 w = w  X  S 2 w for any w and w is the solution of conventional LDA. 1 So WLDA also gives the same Bayes optimal solution as conventional LDA.
 Since problem (16) is to maximize a convex function, it is not a convex problem. We can still use CCCP to optimize problem (16). In the (  X  + 1) th iteration of CCCP, we need to solve the following problem
The Lagrangian is given by where  X   X  0 and  X   X  0 . We calculate the gradients of  X  with respect to w and set them to 0 to obtain
From this, we can see that when the algorithm converges, the optimal w  X  satisfies
This is similar to the following property of the optimal solution in conventional LDA
However, in our method,  X   X  and  X   X  are not fixed but learned from the following dual problem where  X  = (  X m 1  X   X m 2 )  X  w (  X  ) is just the scaled optimality criterion of conventional LDA when we assume the within-class scatter and S 2 as the within-class scatter matrix to maximize the optimality criterion of conventional LDA while controlling the complexity of the within-class scatter matrix as reflected by the second and third terms of the objective function in problem (18). In [11], Li et al. proposed a maximum margin criterion for dimensionality reduction by changing the optimization problem of conventional LDA to: max W tr W  X  ( S  X   X  S  X  ) W . The objective function has a physical meaning similar to that of LDA which favors a large between-class scatter measure and a small within-class scatter measure. However, similar to LDA, the maximum margin sures. Kocsor et al. [10] proposed another maximum margin criterion for dimensionality reduction. decision function in SVM as one direction in the transformation matrix W .
 In [9], Kim et al. proposed a robust LDA algorithm to deal with data uncertainty in classification applications by formulating the problem as a convex problem. However, in many applications, it is handle binary classification problems but not more general multi-class problems. The orthogonality constraint on the transformation matrix W has been widely used by dimension-ality reduction methods, such as Foley-Sammon LDA (FSLDA) [6, 5] and orthogonal LDA [23]. The orthogonality constraint can help to eliminate the redundant information in W . This has been shown to be effective for dimensionality reduction. In this section, we evaluate WLDA empirically on some benchmark datasets and compare WLDA with several related methods, including conventional LDA, trace-ratio LDA [20], FSLDA [6, 5], and MarginLDA [11]. For fair comparison with conventional LDA, we set the reduced dimensionality of each method compared to  X   X  1 where  X  is the number of classes in the dataset. After dimen-of the optimization procedure follows this strategy: when the number of features  X  or the number depending on which one is smaller; otherwise, we use the greedy method in Section 2.4. 4.1 Experiments on UCI Datasets 70% to form the training set and the rest for the test set. We perform 10 random splits and report shown in bold. We can see that WLDA gives the best result for most datasets. For some datasets, e.g., balance-scale and hayes-roth, even though WLDA is not the best, the difference between it and the effectiveness of WLDA. 4.2 Experiments on Face and Object Datasets Dimensionality reduction methods have been widely used for face and object recognition applica-Table 2: Average classification errors on the UCI datasets. Here tr-LDA denotes the trace-ratio LDA [20].
 of the ambient image space. Fisherface (based on LDA) [2] is one representative dimensionality re-database, COIL [15], in our experiments. In the AR face database, 2,600 images of 100 persons (50 men and 50 women) are used. Before the experiment, each image is converted to gray scale and normalized to a size of 33  X  24 pixels. The ORL face database contains 400 face images of 40 persons, each having 10 images. Each image is preprocessed to a size of 28  X  23 pixels. In our experiment, we choose the frontal pose from the PIE database with varying lighting and illumina-tion conditions. There are about 49 images for each subject. Before the experiment, we resize each image to a resolution of 32  X  32 pixels. The COIL database contains 1,440 grayscale images with black background for 20 objects with each object having 72 different images.
 4 images of a person or object in the database to form the training set and the remaining images across the 10 trials in Table 3. From the result, we can see that WLDA is comparable to or even better than the other methods compared.
 Table 3: Average classification errors on the face and object datasets. Here tr-LDA denotes the trace-ratio LDA [20].
 In this paper, we have presented a new supervised dimensionality reduction method by exploiting the worst-case view instead of average-case view in the formulation. One interesting direction of our future work is to extend WLDA to handle tensors for 2D or higher-order data. Moreover, we will investigate the semi-supervised extension of WLDA to exploit the useful information contained in the unlabeled data available in some applications.
 Council of Hong Kong.
