 Malcolm J A Strens mjstrens@QinetiQ.com United Kingdom.
 Mac hine learning metho ds often generate sampling problems that must be solv ed ecien tly. This is par-ticularly true of Bayesian inference approac hes to `ex-plaining' observ ed data using compact mo dels. Exam-ples of suc h mo dels are decision trees, Bayesian net-works, and sets of logic statemen ts. In this pap er the focus is on the discrete-v alued parts of these mo dels, rather than any con tinuous parameters that must also be inferred. The discrete-v alued parts migh t be struc-tural information about the architecture of the mo del (dep endency arcs in a Bayesian net, for example) or assignmen ts of boolean-v alued states in the mo del. In di eren t domains this information migh t be called the structur e , hidden states or the chromosome . Through-out this pap er, a medical diagnosis problem is used as an example: the hidden state de nes the presence or absence of a set of diseases in the patien t. The requiremen t is often not just to nd the the maxi-mum likeliho od structure, but instead to assign a prob-abilit y to every possible structure that could explain a set of observ ations. This pro vides a much richer source of information for subsequen t stages of pro cess-ing (e.g. decision-making or data fusion). When the space of structures is very large or in nite, it becomes necessary to sample rather than enumerate this space. 1.1. Sampling for Bayesian Inference Let B be the set of unique bit-strings that describ e possible structures. For simplicit y, assume that their lengths are all equal to some constan t m . There-fore the set can be indexed using a natural num ber 1 i 2 m . In Bayesian inference, a mo del P ( D j b ) is assumed for observ ations D given structure b 2 B . The observ ations in the medical application are the ndings of a series of tests, indicating the presence or absence of disease symptoms. There is also a prior dis-tribution P ( b ) for the likeliho od of eac h structure: this assigns low probabilities to rare diseases, for example. Then the posterior distribution on b is given by Bayes rule: If the observ ations ( D ) are given, this becomes a target function D ( b ). The maximum a posteriori solution for b is then given by ^ b arg max b D ( b ). This solution can be found by discrete optimization techniques suc h as genetic algorithms, random searc h, tree searc hes, etc. However, it is often the case that ^ b is of little use for decision-making or subsequen t pro cessing: the most-lik ely cause of the observ ations is not necessarily the correct one.
 Supp ose that the Bayesian inference task is to inter-pret observ ations in a decision-making system. The decision u will lead to a set of outcomes or payo s given by a function R ( b;u ). Then the optimal deci-sion is given by: If the exp ectation cannot be obtained by exhaustiv e evaluation of ( b ) over b 2 B , then an estimate can be obtained by sampling N indep enden t hypotheses H f b (1) ;:::b ( N ) g from ( b ) and evaluating the average: Alternativ ely, assume that Bayesian inference is to be used to pro vide information from a single source of data (e.g. a sensor) that will later be fused with other sources, before attempting to infer the hidden state. Again, the optimal solution can be appro ximated using sets of samples. If eac h source k yields a sample set H k of size N , an estimate for the probabilit y of some hidden state b is given by: where count ( k;b ) is the num ber of occurrences of b within H k . Genetic algorithms (Holland, 1975) are introduced here to iden tify a source of prop osal mec hanisms that can be used in the sampling algorithm. The GA searc hes through the set B of xed-size binary strings (\chromosomes"), to maximize some \ tness" func-tion 1 f ( b ). At any given time, the state of the GA is a population of chromosomes ( b 1 ;:::;b N ).The popula-tion is mo di ed by creating new prop osals in batc hes. The next generation (also of size N ) is then generated from the new prop osals (and the existing population) according to a selection mec hanism whic h prefers t-ter individuals. The genetic algorithm may be run for a xed num ber of generations, or until some condition on the average population tness (or its rate of change) is met.
 Eac h prop osal is obtained by selecting two paren t chro-mosomes and applying crossover to create a new chro-mosome. An example is standard 2-p oint crosso ver: where b i and b j are the paren t chromosomes and the o set n and length s are chosen uniformly from f 1 ;:::;m g . Eac h prop osal is then sub ject to muta-tion: where is some small constan t mutation probabilit y. While crosso ver allo ws t substrings to gro w in fre-quency within the population, mutation enables the population to move out of local minima.
 This pap er focusses on xed size chromosomes of bi-nary digits, but the broader eld of evolutionary algo-rithms addresses problems in whic h the chromosome is a vector of real num bers 2 , or has variable size (e.g. genetic programming).
 Evolutionary algorithms can bene t from estimating the structure of the curren t population, in order to obtain new prop osals that are predicted to have a high tness. Population-based incremen tal learning (PBIL) (Baluja &amp; Caruana, 1995) uses an adaptiv e probabilit y vector (learn t on-line from the population) to con trol the likeliho od of selecting 0 or 1 in eac h string position. Higher-order estimation metho ds, for example the Bayesian net work used in the Bayesian Optimization Algorithm (BO A), can capture multi-dimensional structure in the existing population (Pe-likan et al., 1999). We will adapt prop osals to the high-order structure of the curren t population by us-ing the di er enc e between two mem bers of the popu-lation, com bined with a third, to obtain a prop osal 3 . Therefore our approac h may have the bene ts of these \estimation of distribution" approac hes while avoiding the pro cess of parametric mo del-tting. It can oper-ate with arbitrarily complex (e.g. multi-mo dal) pop-ulations whereas estimation-based approac hes will be limited by the chosen class of mo dels. Mark ov Chain Mon te Carlo (MCMC) is a means of sampling hypotheses from some target densit y ( b ) that is kno wn up to some normalizing constan t 4 Z . MCMC can be interpreted as a form of imp ortance sampling in whic h the prop osal distribution depends on the curren t state of the chain (e.g. by making a random change). Eac h prop osal is accepted or rejected according to the usual imp ortance sampling rule. Sev-eral MCMC chains can be run in parallel, to obtain evolutionary or \population-based" metho ds that ap-pear similar in structure to a genetic algorithm but perform sampling rather than optimization. 3.1. Mark ov Chain Monte Carlo MCMC works by constructing a Mark ov chain over B that has as its invarian t distribution. A Mark ov chain on B is de ned by a transition matrix T of size j B jj B j whic h determines the time-ev olution of a state variable X t : A necessary condition is that the vector of probabilities with elemen ts i = ( b i ) be a xed point of the transition function ( = T ). An appropriate choice is given by: Another requiremen t is ergo dicit y of the Mark ov chain: there must be a nite path with non-zero transition probabilit y from every state to every other state. Sam-pling from is achiev ed by sim ulating the Mark ov chain, and outputting its states at irregular interv als of mean duration . If eac h interv al is sucien tly large, successiv e states will be indep enden t. Indep en-dence is not a requiremen t for evaluating exp ectations suc h as the decision evaluation E [ R ( b;u )], but chains whic h mix (achiev e indep endence) more rapidly re-quire shorter sequences to pro vide accurate estimates. To design an e ectiv e MCMC sampler requires an ap-propriate proposal mec hanism. The prop osal mec h-anism, together with an acceptance rule de ne the transition function of the Mark ov chain. Speci cally , a prop osal distribution P ( b 0 j b ) is used to generate a prop osal b 0 for the next state of the chain, given the curren t state is X t = b . The prop osal distribution is tanc e rule decides whether the next state of the chain ( X t +1 ) will be b 0 or b . The Metrop olis acceptance rule (Metrop olis et al., 1953) is given by: For a symmetric prop osal distribution, the Metrop o-lis acceptance rule ensures that the transition ma-trix has as a xed point. The more general Metrop olis-Hastings rule can be used to accoun t for non-symmetric prop osal distributions. Most MCMC metho ds (including those that use the Metrop olis-Hastings rule) have a transition function that satis-es the reversibility prop erty (also called \detailed bal-3.2. The Mutation Prop osal A simple mutation prop osal is sucien t to meet the above requiremen ts. Mutation constructs b 0 by ran-domly inverting eac h bit in b according to some prob-abilit y . Therefore: where ( b;b 0 ) is the num ber of bits that di er between the strings b and b 0 and m is the size of the strings. Symmetry of the mutation prop osal densit y follo ws from observing that ( b;b 0 ) = ( b 0 ;b ) for all ( b;b 0 ). The resulting chain is ergo dic because P ( b 0 ;b ) m for all ( b 0 ;b ).
 The key to e ectiv e sampling performance is to design the prop osal distribution to minimize mixing time. Where possible, prior kno wledge about the problem should be incorp orated into the prop osals. For ex-ample, if b represen ts the structure of a decision tree, natural prop osals would be common tree operations suc h as splits, merges and rotations. 3.3. Evolutionary Monte Carlo It is possible to run multiple MCMC sampling chains in parallel. One bene t is that eac h chain can be started with a di eren t (random) state, and so reason-able samples may be obtained even when the chains have not \mixed". However, even when running mul-tiple chains, a \burn-in" time must be allo wed for eac h chain to move from its random starting state to a high-probabilit y region of the target distribution. There-fore, for any given problem there is some ( nite) op-timal num ber of chains N . However, there is another poten tial adv antage of running multiple MCMC sam-pling chains: inter action . At any one time, the set of chains pro vide a population that is spread across dif-feren t regions of the target distribution. It is reason-able to exp ect that the population as a whole con tains useful information about the direction in whic h any particular population mem ber could explore to nd regions of higher probabilit y. That is, the prop osal dis-tribution for one population mem ber can exploit the information con tained in the others. The evolutionary Mon te Carlo (EMC) metho d was introduced recen tly (Liang &amp; Wong, 2001). Muc h previous work in the evo-lutionary computation eld prop osed algorithms with stochastic acceptance of prop osals that could be used for robust optimization, but were not valid MCMC sampling algorithms (Lozano et al., 1999; Mahfoud &amp; Goldb erg, 1995).
 The algorithm describ ed here di ers signi can tly from EMC. We do not run eac h sampling chain at a di eren t temp erature (parallel temp ering; see Gey er (1992)). Strens et al. (2002) sho wed that a suitable prop osal operator capable of adapting to the shap e of the pop-ulation enabled very e ectiv e sampling in real spaces at a single temp erature. This \di eren tial evolution sampler" (DES) simpli es the implemen tation greatly , and allo w outputs to be tak en from any one of the sam-pling chains. It also pro vides a closer resem blance to con ventional evolutionary algorithms because all the individuals have the same tness criterion. Here, we attempt an analogous approac h in the discrete-state case, where we must nd a similarly adaptiv e prop osal operator that acts on bit-strings.
 When using a population it is again imp ortan t to have a prop osal distribution of a form that ensures valid sampling. Let ( b 1 ;:::;b N ) be the join t state of the whole population, dra wn from the pro duct-space B N . We construct a Mark ov chain on this space that has as its invarian t distribution: Sampling from is achiev ed by sampling from , then returning any one of the mem bers f b i : 1 i N g .
 Prop osals in this new Mark ov chain are de ned in terms of the join t population state . However, con-sider rstly a prop osal that would alter only one mem-ber ( b i ) of the population (with i chosen at random). A transition matrix T satisfying the invariance and er-godicit y requiremen ts for sampling from also ensures the population's invarian t distribution is . Therefore any com bination of prop osal distribution and accep-tance rule that is valid for a single chain sampler can be applied to a randomly chosen mem ber of the pop-ulation. Furthermore, in constructing suc h a prop osal the states of all other population mem bers f b j : j 6 = i g can be exploited. 3.4. A New Prop osal We now introduce a new prop osal of this kind: where i , j , k are mutually unique population mem bers. The sym bol indicates bit-wise exclusiv e or: where l indexes the individual bits of the strings b and b 0 . The prop osal distribution is symmetrical because b 0 ( b j b k ) regenerates b i .
 Wh y should this \xor" prop osal be useful? A similar prop osal mec hanism was introduced recen tly for sam-pling in con tinuous state spaces (Strens et al., 2002). In that case the prop osal was of the form: where x i ;x j ;x k are real-v alued state vectors and F is a scalar constan t. With F = 1 the e ect is to generate a prop osal x 0 that di ers from its paren t x in the same way that x j di ers from x k (in terms of vector arith-metic). This type of prop osal allo ws the population to explore non-isotropic structures suc h as ridges in the target function, because the vector di erences are typ-ically aligned with the direction of the ridge. The aim here is the same, but to obtain a prop osal mec hanism that is applicable to binary vectors; therefore the \+" and \-" are replaced by . Observ e that: Reading ( A B ) as \the di erence between A and B " the e ect is to generate a prop osal b 0 that di ers from its paren t b in the same way that b j di ers from b k . We exp ect this to pro vide a prop osal densit y that adapts to typical variabilit y of the population: an exp erimen-tal evaluation will be performed to determine whether this is true. 3.5. The Crosso ver Prop osal Unlik e the mutation and xor prop osals introduced above, the crosso ver operation that is often used in genetic algorithms must be adapted carefully to meet the requiremen ts for population-based MCMC prop os-als. Crosso ver works by selecting two paren ts b i and b j from the population and constructing a child that consists of some genetic material from eac h. For ex-ample: where \choose" selects randomly between its two in-puts according to some xed probabilit y. If the pro-posal is accepted then b i , b j or some other mem ber of the population is replaced (overwritten) by b 0 .To ensure rev ersibilit y of the population chain, it is nec-essary for the prop osal to select two paren t strings and replace them both with child strings, preserving every bit of eac h paren t within one of the children. There-fore if child 2 receiv es bit b i;l , child 1 must receiv e b Supp ose that a crosso ver prop osal generates children ( c 1 ;c 2 ) from paren ts ( b i ;b j ). Then the Metrop olis rule on the join t population state has acceptance probabil-ity: If the prop osal is accepted, both paren ts are replaced; otherwise there is no change to the population. The chain is rev ersible because a prop osal that regenerates the paren ts is equally likely to be chosen at the next step. Note that the crosso ver prop osal requires two evaluations of the target distribution (at c 1 and c 2 ; the values at b i and b j will already be stored). This additional computational cost is tak en into accoun t in determining whether crosso ver can be bene cial in a sampling con text. An interesting prop erty of this form of crosso ver is that there is a special form of where ( c 1 ) ( c 2 ) = ( b i ) ( b j ) alw ays. This is true if the bits of b are indep enden t in terms of their in uence on . (i.e. ( b ) is of the form Q l f l ( b l ).) In this case, crosso ver prop osals will alw ays be accepted because they cannot increase or decrease the join t probabilit y of the population, ( ). The 3 types of prop osal (mutation, crosso ver and xor) can be used within a population-based MCMC metho d, and can together meet the requiremen ts for valid sampling. Giv en an initial population of ran-dom bit-strings, and the value of for eac h, prop osals are rep eatedly accepted/rejected until the budget of function evaluations is exhausted. The prop osal type is selected according to the probabilities ( p M , p C and p X ), then the follo wing steps are applied: Mutation 1. Select a random paren t b i and apply mutation to 2. Accept or reject b 0 using the Metrop olis rule, re-Crossover 1. Select two mutually exclusiv e paren ts ( b i , b j ) uni-2. Accept or reject both children together using the Xor proposal 1. Select three mutually exclusiv e paren ts ( b i , b j and 2. Accept or reject b 0 using the Metrop olis rule, re-To ensure that the Mark ov chain is ergo dic requires that p M &gt; 0. Otherwise the only constrain t is p M + p
C + p X = 1. Table 1 sho ws the choices used in the exp erimen tal evaluation. These are chosen to balance the budget of evaluations of equally between the selected prop osal types.
 A random mem ber of the population can be pro vided as output on every time step. Although these outputs are highly correlated from one step to the next, the resulting exp ectation (or any other statistics computed from the samples) will be asymptotically unbiased. In order to demonstrate the e ectiv eness of the new sampling algorithm, we introduce an application prob-lem from medical diagnostics that o ers a dicult discrete sampling problem. A comparison of sam-pling performance for di eren t prop osal com binations is then presen ted. The e ect of population size is also evaluated. 5.1. The QMR-DT architecture Consider the problem of diagnosing whic h diseases are presen t in a patien t, given a set of measuremen ts (\ ndings"). Eac h patien t may have zero, one or man y diseases. The QMR-DT net work architecture (Figure 1) speci es a statistical mo del for the relationship be-tween diseases and ndings, that can be calibrated using recorded data (Sh we et al., 1991; Jaakk ola &amp; Jordan, 1999). The mo del is of the form: f is the binary value of nding i . q i 0 is the leak prob-ability for that nding. If q i 0 is close to 1 then the nding is common even when no disease is presen t. q il is the asso ciation between disease l and nding i . If q is large then the nding i often results from disease l ; if q il = 0 disease l does not in uence nding i and the corresp onding arc can be omitted from the net work diagram.
 In order to perform inference, a prior P ( b ) must also be speci ed. The simplest approac h is to assume that the diseases are indep enden t: where p l is the prior probabilit y for disease l . Random instances of this architecture were generated according to: 1. Num ber of diseases m = 20 2. Num ber of ndings n = 80 3. Disease prior p l U [0 ; 0 : 5] 4. Leak probabilit y q i 0 U [0 ; 1] 5. Asso ciation between disease l and nding i : 40 suc h instances were generated, and for eac h a sam-pling test problem was created as follo ws. Firstly , a plausible hidden disease state b truth was created by sampling from the disease prior. Then a set of nd-ings were sampled according to P ( f j b ) for that in-stance. The sampling problem is to obtain the dis-tribution of possible diseases given the ndings; i.e. to perform a diagnosis. Therefore the target distribution is ( b ) P ( b j f ). For any give prop osal b it is possible to evaluate ( b ) up to an unkno wn constan t (equal to easily computed in the architecture. 5.2. Measuring Sampling Accuracy Samples are generated by applying the algorithm de-scrib ed in section 4, and outputting a random popula-tion mem ber after every step. (One step corresp onds to one evaluation of the target distribution.) For eac h disease l the empirical prop ortion of samples for whic h b l = 1 is computed by exhaustiv e evaluation. This gives a vector of probabilities 5 . The true value of l P ( b l = 1 j f ) was also computed by exhaustiv e evaluation of the 2 m disease com binations. In an ef-fectiv e sampler, we should nd that ! . A measure of the di erence between these vectors is given by: This symmetrical measure is zero when = and positiv e otherwise. This measure is much more infor-mativ e than the full KL-div ergence (between the true probabilit y distribution of diseases and the set of sam-ples) because the latter is in nite if any state that has non-zero probabilit y is not presen t in the set of sam-ples.
 Figure 2 sho ws the sampling accuracy over 100,000 steps for eac h of the prop osal com binations in Table 1. The standar d errors indicated by error bars were computed from the set of performance measures ob-tained over the 40 random problem instances. There are signi can t di erences in performance between the three exp erimen ts. The crosso ver operator mak es per-formance worse rather than better, whic h suggests that the extra cost (two target distribution evalua-tions) out weighs any bene t. In con trast, the xor prop osal mec hanism sho ws a signi can t impro vemen t (reducing error by 38% at 1024 steps). In this eval-uation we have limited the size of the state space to 2 20 in order to be able to evaluate it exhaustiv ely (to validate performance). Greater performance impro ve-men ts migh t be possible on larger and more complex spaces in whic h the \local searc h" performed by mu-tation is inadequate. (Con versely , the rejection rate within the Metrop olis rule for the xor-prop osal is likely to rise as dimensionalit y gro ws, poten tially reducing performance.) Figure 3 sho ws sampling performance over a much longer perio d (log-log scale). We observ e that perfor-mance con tinues to impro ve at the same rate. Giv en enough time, the performance of the mutation mec h-anism approac hes that of the xor-op erator. This sug-gests that both populations have become adequately spread across the high probabilit y regions of the tar-get densit y and are essen tially in equilibrium. The \exhaustiv e" performance line indicates the result of systematically visiting every state (in a random or-der). This veri es that the problem is non-trivial and that the sampling approac hes o ers some bene t over a \brute-force" metho d. The error asso ciated with this metho d only becomes zero after 2 20 (ab out a million) steps. Exhaustiv e evaluation rapidly becomes infeasi-ble as the dimensionalit y of the state space increases. Figure 4 sho ws the e ect of population size on perfor-mance of the xor-prop osal sampler. Population size is imp ortan t until about 10000 steps, with a mo der-ate population size (12) giving the best performance. This probably o ers the best trade-o between div er-sity (num ber of individuals) and the rate at whic h eac h state in the chain is being updated. After this initial \exploration" phase, the performance of all the pop-ulation sizes becomes similar, although larger popula-tions become sligh tly better. This is probably because the larger populations pro vide more consisten t cover-age of the whole target distribution. We have tak en prop osal operators that would nor-mally be used for evolutionary optimization and ap-plied them to accelerate sampling. However sim ulated annealing (SA) (Neal, 1998) is a means of using a sam-pler to perform global optimization. It works by sam-pling from T =Z T while gradually reducing the tem-perature parameter T . ( Z T is the normalizing con-stan t P b T ( b ).) As T decreases towards zero, the distribution becomes \sharp er"; i.e. probabilit y mass moves towards the global maxim um (or maxima) of . Therefore sim ulated annealing allo ws us to obtain a global optimizer from the new sampling metho d. This is particularly useful in dicult integer programming or SAT (satis abilit y) problems where the aim may be to nd any solution rather than every solution. For example, we tested the SA varian t of the sampling algorithm on the \8-queens" problem. The aim is to place 8 queen chess pieces on an 8-b y-8 board so that none share a common rank, le of diagonal. By enco d-ing the position of eac h in its le as a 3-digit binary num ber, a 24-bit optimization problem was obtained. The target densit y was ( b ) / exp ( hits ( b ) =T ) where hits ( b ) was the num ber of constrain ts brok en in board con guration b . 2 16 sampling steps were sucien t for the SA algorithm to nd a solution (with probabilit y &gt; 0 : 99). This was using a population size of 24, and a uniform linear reduction in temp erature from 1 to 0. (All three prop osal options gave this level of per-formance.) We have dev elop ed an evolutionary Mon te Carlo algo-rithm for sampling in discrete spaces. The population is not only used as a means to obtain indep enden t sampling chains: it is exploited by a prop osal mec h-anism that uses relational information (between ex-isting population mem bers) to move about the state space more e ectiv ely. Our exp erimen tal evaluation sho wed a signi can t impro vemen t in sampling perfor-mance for a represen tativ e mac hine learning problem. The algorithm has a simple implemen tation compared with existing metho ds for exploiting population struc-ture (\estimation of distribution" approac hes suc h as BO A); but a comparativ e evaluation should be per-formed.
 We also sho wed that sim ulated annealing can mak e use of the sampler to obtain a global optimizer. This has an adv antage over con ventional genetic algorithms in that the rate of cooling can be used to con trol the trade-o between the risk of becoming trapp ed in a local minim um and the total computational cost. The capabilit y to sample from discrete spaces has wide applicabilit y in mac hine learning, particularly in the con vergence between statistical and sym bolic learning. Often it will be necessary to be able to work with probabilit y distributions over strings of sym bols and graphical mo del structures. The metho d describ ed here works solely with binary strings of xed length, complemen ting previous work that focussed on real-valued states (Strens et al., 2002), but it could be naturally extended to work with mixed real/discrete state-spaces within the EMC framew ork, and to spaces of variable dimension using rev ersible jump MCMC metho ds (Green, 1995).
 The general purp ose prop osal mec hanisms we have de-scrib ed may work well for the test problems presen ted, but better performance is likely to be achiev ed by mak-ing use of domain kno wledge. If the problem is to nd a graphical structure that best explains some data, standard operations for working with that structure (e.g. node and arc deletion/insertion in a graph) can be mo di ed to act as suitable prop osal mec hanisms. This researc h was funded by the UK Ministry of De-fence Corp orate Researc h Program.
 Baluja, S., &amp; Caruana, R. (1995). Remo ving the ge-netics from the standard genetic algorithm. Proceed-ings of the Twelth International Confer enc e on Ma-chine Learning (pp. 38{46). San Mateo, CA: Morgan Kaufmann Publishers.
 Gey er, C. (1992). Practical Mark ov chain Mon te Carlo (with discussion). Statistic al Scienc e , 7 , 473{511. Green, P. J. (1995). Rev ersible jump mark ov chain mon te carlo computation and bayesian mo del deter-mination. Biometrika , 82 , 711{732.
 Holland, J. H. (1975). Adaptation in natur al and ar-ti c al systems . MI: Univ ersit y of Mic higan Press. Jaakk ola, T., &amp; Jordan, M. I. (1999). Variational prob-abilistic inference and the QMR-DT net work. Jour-nal of Arti cial Intel ligenc e Research , 10 , 291{322. Liang, F., &amp; Wong, W. H. (2001). Real-parameter evo-lutionary Mon te Carlo with applications to Bayesian mixture mo dels. Journal of the Americ an Statistic al Asso ciation , 96 , 653.
 Lozano, J. A., Larra ~ naga, P., Gra ~ na, M., &amp; Albizuri,
F. X. (1999). Genetic algorithms: bridging the con-vergence gap. The oretic al Computer Scienc e , 229 , 11{22.
 Mahfoud, S. W., &amp; Goldb erg, D. E. (1995). Parallel recom binativ e sim ulated annealing: a genetic algo-rithm. Par allel Computing , 21 , 1{28.
 Metrop olis, N., Rosen bluth, A. W., Rosen bluth, M. N.,
Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing mac hines. Journal of Chemic al Physics , 21 , 1087{1092.
 Neal, R. (1998). Anne aled imp ortanc e sampling (Tech-nical Rep ort 9805 (revised)). Departmen t of Statis-tics, Univ ersit y of Toron to.
 Pelik an, M., Goldb erg, D. E., &amp; Can tu-P az, E. (1999).
BO A: The bayesian optimization algorithm. Pro-ceedings of the Genetic and Evolutionary Computa-tion Confer enc e (pp. 525{532). Orlando, Florida, USA: Morgan Kaufmann.
 Shwe, M., Middleton, B., Hec kerman, D., Henrion, M., Horvitz, E., Lehmann, H., &amp; Cooper, G. (1991). Probabilistic diagnosis using a reform ulation of the
INTERNIST-1/QMR kno wledge base: Part I. The probabilistic mo del and inference algorithms. SIAM Journal on Computing , 30 , 241{250.
 Storn, R., &amp; Price, K. (1995). Di er ential evolution -a simple and ecient adaptive scheme for glob al op-timization over continuous spaces (Technical Rep ort TR-95-012). Berk eley , CA.
 Strens, M., Bernhardt, M., &amp; Everett, N. (2002).
Mark ov chain mon te carlo sampling using direct searc h optimization. Proceedings of the Ninete enth International Confer enc e on Machine Learning . San
Francisco: Morgan Kaufmann.
