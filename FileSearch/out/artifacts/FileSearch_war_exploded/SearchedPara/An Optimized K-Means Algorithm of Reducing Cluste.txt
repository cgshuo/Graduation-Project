 Recently, the most popular document clustering method is K-Means[4]. It has such advantages as straightforward processing, easily programming, and the relative low computational complexity. However, due to the high-dimension and sparseness properties of documents, especially the same influence to means from all new objects, (means) randomly, for each remaining object, the similarity between the objects with the each cluster is computed. The process will result in intra-dissimilarity. The intra-the similarity is measured by computing distance between an object (document) with serious that the documents in one cluster are totally unrelated. 
SOM (Self Organizing Map) is a method to reduce the dimension of data and clustering of high-dimensional objects. The scalar factor is an important part of SOM. For reducing the intra-dissimilarity of clusters , we reference to the scalar factor from SOM. During the assignment step in K-Means, the coming document imposes the mean of one cluster it belongs to, some influence making the mean shift towards the coming document, but the influence will decrease with the coming of documents. 
We use BOW (Bag of Word) to represent a document by performing the following remaining terms are stemmed using Porter Stemmer 2 . Then a term space T is between two documents d 1 , d 2  X  D is computed using the cosine of their term vector. K-Means Algorithm is the most popular document clustering algorithm, standard K-Means algorithm for document clustering works as follows. Algorithm1: Standard K-Means Algorithm for Document Clustering Input: The number of clusters k and a document set with n objects Output: A set of k clusters that minimize the criterion function 
Method: 1) randomly choose k objects as the initial cluster centers; 2) repeat 4) update the cluster means; 5) until meeting a given criterion function; 
According to the steps of standard K-Means algorithm, step 3)~4) will bring on We think that this is due to during assigning a new object to a cluster, the mean of the cluster not change with the new object. 4.1 SOM Algorithm SOM consists of sample data and Map. It first assigns a weight for every neuron in a given Map randomly, and the weight has the same dimensions with the sample. Then mapped to the same neuron for clustering. The algorithm is shown as follows. Algorithm2: SOM Algorithm Input: Map and samples Output: trained map 
Method: 1) initialize Map so that every neuron in Map is assigned an initial weight; 2) For i =1 to n ( n is a parameter of given iteration) 3) Select a sample randomly; 4) search best matching unit (BMU) with the sample in Map; 5) adjust BMU and its neighborhood neurons X  weight; 6) i ++; 7) End for; 
The key is Step 5), which concerns the learning process of neural-network, and the process will result in BUM more similar with its neighborhood unit. 4.2 Optimized K-Means Algorithm It has been mentioned in section 3 that the K-Means might result in the intra-dimension and sparseness properties of documents, especially the influence of new documents. Generally, one document corpus includes many words, and the vector representation of the each document includes at least hundreds of words. But the similarity between two documents might be determined by only 10 percents of the vector or even less. In step 3) of Algor ithm1, each document is assigned to one cluster based on its similarity to each cluster mean, the similarity between them lying on only a little part of vectors, which is the key of generating intra-dissimilarity. 
In SOM, the coming sample vector finds its best matching unit (BMU) and then imposes a learning function on neighborhoods of the BMU. The leaning function changes the value of neighborhoods based on BMU, making them shift towards BMU. This step is called Scale Neighbors . By this means, the SOM makes sure that the samples captured by one neuron are of the most similar. Inspired by Scale Neighbors in SOM, we improve the step 3) in Algorithm1. Different from keeping the mean unchanged, with the documents assigned to the clusters, the corresponding cluster mean is influenced by the coming document, making the mean shift towards the coming document. But if each of the coming documents imposes the same influence on the mean, the new mean can not reduce the the influence of orderly coming documents, which is defined in formula (1). 
Where x is the order of document assigned to certain cluster. Formula (1) illustrates influences mean the most, and the influence decreases with the document assigned to the cluster orderly. 
The assignment step in K-Means with scal ar factor (optimized K-Means algorithm) is depicted in Algorithm3. Algorithm3: Assignment step in K-Means with scalar factor Output: a set of k clusters C= { C 1 , C 2 , ... , C k } 
Method: 1) repeat 3) if d in LC i then  X  i =  X  i + d ; //modify the mean value 4) else d  X  D ; //put d back to D 5) until no documents that are assigned to the same cluster; 6) repeat 7) d  X  C i ; //assign d to cluster C i 8)  X  i =  X  i + d ; //modify the mean 9) until D=null ; 10) LC=C ; //save the clustering result 
Here step 3), 4) and step 8) use scalar factor and means of clusters control strategy. We select 20NG (http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups) as testing corpus and construct 5 datasets from the corpus. A document is selected only if it includes at least 100 words. All the stopwords are removed, and other words are documents, Dataset2 includes 5 classes and 200 documents, Dataset3 includes 10 classes and 500 documents, Dataset4 includes 10 classes and 1000 documents, and Dataset5 includes 15 classes and 1500 documents. In the datasets, every document document belongs to and the cluster which the document is grouped into. Comparing the class with the cluster, we can evaluate the quality of the algorithms. 
We apply the standard K-Means algorithm and optimized K-Mean algorithm (with cluster centers are given manually by randomly selecting one text from each class. result is. The latter is used to evaluate the correlation among objects in a cluster, and Means algorithms on a computer with 1GHz CPU and 128M memory. Measure , Entropy and time cost, respectively. 
Obviously, the optimized K-Means algorithm has better F-Measure and Entropy than the standard K-Means algorithm from Fig.1 and Fig.2. Because of introducing extent thereby quality of document clustering is improved. 
From Fig.3, the time cost of the two K-Means algorithms has not obvious difference, especially when number of documents becomes more. There are two reasons. Firstly, in optimized K-Means algorithm, for each newly assigned document, it requires k more times similarity computation. This process increases the time cost. Secondly, the influence of scalar factor makes that the constringency is quickened. So in the mass, the time cost has not too much change. In dimension reduction and quality improvement of document clustering, reference[6] proposed an efficient two-level neural network based document clustering Organizing Map into the process of clustering without special dimension reduction. In reducing intra-dissimilarity of cluster, reference[8] analyzed the reasons of skew generation and proposed a skew prevention method ESPClust for document clustering. Our idea is similar with the idea of the reference[8]. However, the method of reference[8] was based on model for implementing, our method is using scalar factor from Self-Organizing Map. 
In the using Self-Organizing Map idea in document clustering, reference[6] mentioned in above paragraph is an example of using SOM to reduce dimensions. Reference[11] described a method of clustering document using a series of 1-dimensional SOM arranged hierarchically to provide an intuitive tree structure representing document clusters. Reference[12] presented an algorithm of based on Tree-Structured Growing Self-Organizing Feature Map for document clustering. These techniques all improved the quality of document clustering to some extent and gave us many good suggestions although different from our method in using SOM. Moreover, there are many good algorithms in document clustering self [13,10,1]. These techniques including setting k in K-Means, introducing new data structure, identifying important feature, and so on, will provide more improvement ideas for our research. We analyze that the reason of intra-dissimilarity of using K-Means algorithm in document clustering are because of high-dimension and sparseness properties of documents. To solve this problem, we reference to Self-Organizing Map (SOM) and introduce the scalar factor into K-Means, the most popular algorithm of document step. From experiments, this method improves the performance of K-Means in dealing with intra-dissimilarity through calculating F-Measure and Entropy . work, we will compare our algorithm with the related work mentioned in Section 6 to test the quality of our algorithm. 
