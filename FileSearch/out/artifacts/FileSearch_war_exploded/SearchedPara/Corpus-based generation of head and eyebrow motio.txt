 Mary Ellen Foster  X  Jon Oberlander Abstract Humans are known to use a wide range of non-verbal behaviour while speaking. Generating naturalistic embodied speech for an artificial agent is therefore an application where techniques that draw directly on recorded human motions can be helpful. We present a system that uses corpus-based selection strategies to specify the head and eyebrow motion of an animated talking head. We first describe how a domain-specific corpus of facial displays was recorded and annotated, and outline the regularities that were found in the data. We then present two different methods of selecting motions for the talking head based on the corpus data: one that chooses the majority option in all cases, and one that makes a weighted choice among all of the options. We compare these methods to each other in two ways: through cross-validation against the corpus, and by asking human judges to rate the output. The results of the two evaluation studies differ: the cross-validation study favoured the majority strategy, while the human judges preferred schedules gene-rated using weighted choice. The judges in the second study also showed a preference for the original corpus data over the output of either of the generation strategies.
 Keywords Data-driven generation Embodied conversational agents Evaluation of generated output Multimodal corpora 1 Introduction It has long been documented that the verbal and non-verbal components of embodied speech are tightly linked. For example, Ekman ( 1979 ) noted that eyebrow raises  X  X  X ppear to coincide with primary voice stress, or more simply with a word that is spoken more loudly. X  X  Similarly, Graf et al. ( 2002 ) found that in their corpus of facial recordings,  X  X  X ises of eyebrows are often placed at prosodic events, sometimes with head nods, at other times without. X  X  However, while correlations have been found between facial displays and prosodic events, this is not a strict rule: in normal embodied speech, many pitch accents and other prosodic events are unaccompanied by facial displays, while other facial displays occur with no obviously related prosodic event. Other factors including information structure, syntactic structure, and affective and pragmatic context can also influence a speaker X  X  non-verbal behaviour.

Since there are so many factors that can influence the non-verbal behaviours that accompany speech, specifying appropriate multimodal behaviour for an artificial embodied agent is a complex task X  X nd one where models derived from recorded human data can be helpful. In this project, we select the head and eyebrow motions of a synthetic talking head in a multimodal dialogue system based on the recorded and annotated behaviour of a speaker reading a script of similar sentences. We implement two different selection techniques, majority choice and weighted choice, and compare them using two methods: by computing a range of automated corpus similarity measures, and by gathering the opinions of human judges.

By building and evaluating models of multimodal human behaviour based on manual annotation of a video corpus of a human speaker, the work described in this paper contributes to the growing body of knowledge about multimodal behaviour. The corpus is used to generate the behaviours of an embodied conversational agent, and also to evaluate those behaviours. The conclusions concerning the relative utility of cross-validation and human evaluation for generation contribute to the emerging consensus that the latter is absolutely essential. 2 Background This study builds on work in three areas: generating non-verbal behaviour for embodied conversational agents, building and using multimodal corpora, and using corpora in generation systems. In this section, we summarise the main techniques and issues in each of these areas. 2.1 Embodied conversational agents An embodied conversational agent (ECA) is a computer interface that is represented as a human body, and that uses its face and body in a human-like way in conversation with the user (Cassell et al. 2000 ). The main benefit of an ECA as a user-interface device is that it allows users to interact with a computer in the most natural possible setting: face-to-face conversation. However, to take full advantage of this benefit, the conversational agent must produce high-quality output, both verbal and non-verbal. Non-verbal behaviour has two main aspects: motions such as beat gestures and emphatic facial displays that correspond directly to the structure of the speech, and other behaviours such as emotional facial expressions that are related to the pragmatic context.

An ECA system generally uses the recorded behaviour of humans in conver-sational situations to choose the motions of the agent. There are two main implementation strategies. In some cases, the recorded behaviours are analysed by hand and rules are created to make the selection; in others, models based directly on the recorded data are used the decision process. The performative facial displays for the Greta agent (de Carolis et al. 2002 ), for example, were selected using the former technique: rules to map from emotional states to facial displays were derived from the literature on facial expressions of emotion. Similarly, Cassell et al. ( 2001a ) selected gestures and facial expressions for the REA agent using heuristics derived from studies of typical North American non-verbal displays. An implementation of this sort tends to produce averaged behaviours from a range of speakers, but does not include specific personality and stylistic effects, and tends to draw from a small range of alternative behaviours.

In contrast, the non-verbal behaviour of other ECAs is selected using models built directly from the data; such systems are able to produce more naturalistic output than a rule-based system, and can also easily model a single individual. Stone et al. ( 2004 ), for example, used motion capture to record an actor performing scripted output in the domain of a computer game. They segmented the recordings into coherent phrases and annotated them with the relevant semantic and pragmatic information, and then combined the segments at run-time to produce performance specifications to be played back on an embodied agent. Similarly, Mana and Pianesi ( 2006 ) captured the facial motions of an actor speaking nonsense words in a range of emotional contexts, modelled the behaviour using a hidden Markov model, and then used the model to specify MPEG-4 animation commands for a talking head.
Both of the above systems used corpora of human non-verbal behaviour built using automated motion capture; this requires specialised hardware and software. An alternative strategy is to use manual annotation to create the corpus. Annotating a video corpus can be less technically demanding than capturing and directly re-using real motions, especially when the corpus and the number of features under consideration are small. For example, Cassell et al. ( 2001b ) used this technique to choose posture shifts for the REA agent based on the annotated behaviours of speakers describing a house and giving directions. More recently, Kipp ( 2004 ) used a similar technique to generate agent gestures based on annotated recordings of skilled public speakers. 2.2 Multimodal corpora A multimodal corpus is an annotated collection of coordinated content on communication channels such as speech, gaze, hand gesture, and body language, and is generally based on recorded human behaviour. At the moment, multimodal corpora are primarily employed in descriptive tasks such as analysis and summarisation (Martin et al. 2006 ); however, they have also been used as resources for making decisions when generating output. In particular, the data in such a corpus can be useful for selecting the behaviour of an embodied agent, as in several applications described in the preceding section.

If a multimodal corpus is to be used for generation, the annotated data must correspond to the inputs and outputs that will be used in the system. This imposes additional requirements on the corpus that do not exist if the primary purpose is analysis. First, the pragmatic context under which each item of the corpus was created must be known; that is, the corpus must include all contextual information that the generator might use to choose among alternatives in a given situation. Also, the content on the different channels must be linked to each other so that the generator can produce properly coordinated output. In some cases, the common strategy of annotating each modality on a separate channel and leaving the links implicit in the temporal information is adequate; however, the temporal relationship among communicative modalities can be complex (cf. McNeill ( 2000 )), so explicit links may be necessary. Finally, the annotated content in the corpus must be described at a level that is appropriate for specifying the output of the target generation system. This level can vary widely: for example, among the data-driven ECA systems mentioned in the preceding section, Kipp ( 2004 ) described non-verbal behaviour using a gesture grammar, Mana and Pianesi ( 2006 ) used facial animation parameters (FAPs), while Stone et al. ( 2004 ) used the captured motions directly.
The necessary correspondence between a multimodal corpus and a generation system can be achieved in several ways. When recording the data, one possibility is to create situations in which the necessary pragmatic context is known in advance so that it does not need to be annotated; this was done, for example, by Stone et al. ( 2004 ) and Cassell et al. ( 2001b ). It is also possible to annotate existing recordings to add the contextual information, as was done by Kipp ( 2004 ). To obtain compatible input and output specifications and cross-modal links, in most cases the generation system and the annotation scheme are defined in parallel. It is also possible to design a generation system to use the representations found in an existing corpus, but this is not a common strategy. 2.3 Corpora in generation The increasing availability of large textual corpora has led to increased use of data-driven techniques in many areas of language processing. Researchers in natural-language generation (NLG) have now also begun to make use of such techniques (cf. Belz and Varges ( 2005 )). Modern data-driven NLG systems make use of textual corpora in two ways: on the one hand, corpus data can act as a resource for decision-making at all levels of the generation process, from content determination to lexical choice; on the other hand, the data can also be used to help evaluate the output.
One of the first generation systems to exploit corpus data directly in its decision-making process was Nitrogen (Langkilde and Knight 1998 ). Nitrogen works in two stages: first, it maps its semantic inputs into word lattices, and then it uses n -grams derived from text corpora to search the lattice to find the best-scoring realisations. The successor system HALogen (Langkilde-Geary 2002 ) adds a fuller treatment of syntax and makes other modifications to permit broader coverage and finer control over the output. Among more recent systems, the OpenCCG surface realiser (White 2006 ) uses a chart-based realisation algorithm that ranks edges using n -gram precision scores based on a corpus of target outputs, while many of the ECA systems described in Sect. 2.1 use data-driven techniques to select non-verbal behaviours.
 Corpora have also been used as resources for evaluating the generated output. Although the predictions of metrics based on corpus similarity do not always correspond with the preferences of users (cf. Belz and Reiter ( 2006 )), they do provide a fast and often useful form of evaluation. Bangalore et al. ( 2000 ) evaluated the F ERGUS realisation module using a number of metrics that compared the output of their system directly to the corpus that it was trained on, using either the surface strings or the syntactic trees. They found that the metrics corresponded well with human judgements of word-ordering quality. The current shared-task evaluation campaign for NLG (Belz et al. 2007 ) includes corpus-based evaluation of generated referring expressions. 3 Building a corpus of non-verbal behaviour As in many of the systems described in Sect. 2.1 , the goal of the current implementation is to generate naturalistic behaviour for an embodied agent using a model drawn from a corpus of recorded human behaviour. In this section, we describe how the corpus was constructed, recorded, and annotated, and also discuss how it responds to the requirements for a generation corpus.

The implementation is based on the output-generation components (Foster et al. 2005 ) of the COMIC multimodal dialogue system, which adds a multimodal talking-head interface to a CAD-style system for redesigning bathrooms. We concentrate on the turns where the system describes and compares options for tiling the room, as those are the turns with the most interesting and varied content. An example sentence from this phase of the system is the following description, tailored to the current user X  X  likes and dislikes, of two features of a set of tiles: (1) Although it X  X  in the family style, the tiles are by Alessi Tiles. 3.1 Recording The script for the recording consisted of 444 sentences created by the full COMIC output planner, which uses the OpenCCG surface realiser (White 2006 ) to create texts including prosodic specifications for the speech synthesiser and incorporates information from the dialogue history and a model of the user X  X  likes and dislikes. Every node in the OpenCCG derivation tree for each sentence in the script was initially labelled with all of the available syntactic and pragmatic information from the output planner, including the following features:  X  The user-preference evaluation of the object being described (positive or  X  Whether the fact being presented was previously mentioned in the discourse ( as  X  Whether the fact is explicitly compared or contrasted with a feature of the  X  Whether the node is in the first clause of a two-clause sentence, in the second  X  The surface string associated with the node;  X  The surface string, with words replaced by semantic classes or stems drawn  X  Any pitch accents specified by the text planner.

Figure 1 illustrates the labelled OpenCCG derivation tree for a sample sentence, where the indentation reflects the derivation structure. Every node in the first half of this sentence is associated with a negative user-preference evaluation and is in the first clause of the sentence, while every node in the second half is linked to a positive evaluation and is in the second clause. The figure also shows the pitch accents selected by the output planner according to Steedman X  X  ( 2000 ) theory of information structure and intonation.

We recorded a single amateur actor reading all sentences, which were presented one at a time; the presentation included both the textual content (with accented words indicated) and the intended pragmatic context. Each sentence was displayed in a large font on a laptop computer directly in front of the speaker, with the camera positioned directly above the laptop to ensure that the speaker was looking towards the camera at all times. The speaker was instructed to read each sentence out loud as expressively as possible into the camera. 3.2 Annotation Once the video was recorded, it was split into individual clips corresponding to each sentence using Anvil (Kipp 2004 ). We then annotated the speaker X  X  facial displays in each clip, considering five types of motion: eyebrow raising or lowering; eye narrowing; head nodding (up or down); head leaning (left or right); and head turning (left or right). This set of displays was chosen based on the emphatic facial displays documented in the literature, the capabilities of the target talking head, and the actual behaviour of the speaker during the recording session. Figure 2 shows some typical displays.
 Each display was attached to the span of words that it coincided with temporally. If a single node in the derivation tree exactly covered all of the words spanned by a display, then the annotation was placed on that node; if the words did not coincide with a single node, it was attached to the set of nodes that did cover the necessary words. For example, in the derivation shown in Fig. 1 , the sequence the family style is associated with a single node, so a motion that started and stopped at the same time as that sequence would be attached to the single node. On the other hand, if there were a motion on the tiles are , it would be attached to both the the tiles node and the are node. Any number of displays could be attached to each node.

The annotation tool allowed the coder to play back a recorded sentence at full speed or slowed down, and to associate any combination of displays with any node or set of nodes in the OpenCCG derivation tree of the sentence. The tool also allowed a proposed annotation sequence to be played back on a synthetic talking head to verify that it was faithful to the actual motions. Figure 3 shows a screenshot of the annotation tool in use on the sentence from Fig. 1 . In the screenshot, a left turn has been attached to the entire sentence (i.e., the root node), while a series of nods is associated with single leaf nodes in the first half of the sentence. The coder has already attached a brow raise to the word are in the second half and is in the process of adding a downward nod to the same word.

The output of the annotation tool is an XML document including the original labelled OpenCCG derivation tree of each sentence, with each node additionally labelled with a (possibly empty) set of facial displays. Figure 4 shows an excerpt from the annotated version of the sentence from Fig. 1 . This document includes the full set of features from the original tree. Every node specifies the string generated by the subtree that it spans, both in its surface form  X  sf  X  and with semantic-class and stem replacement  X  sc  X  : The nodes also have contextual features, indicated by italics in the figure: every node in the second subtree has um  X   X  X  X  00 and fs  X   X  X  X  00 (i.e., a positive evaluation in the second clause), while the accented are also has ac  X   X  X  X  00 : This output tree also includes the facial displays added by the coder in Fig. 3 , highlighted in the figure by underlining: a left lean  X  lean  X   X  X  X eft 00  X  attached to the root node and a downward nod  X  nod  X   X  X  X own 00  X  accompanied by a brow raise  X  brow  X   X  X  X p 00  X  on are near the end. 3.3 Reliability of annotation Several measures were taken to ensure that the annotation process was reliable. As the first step, two independent coders each separately processed the same set of 20 sentences, using a draft of the annotation scheme. The coders discussed the differences in the outputs and agreed on a final scheme, which one of those coders then used to process the entire set of 444 sentences. As a further test of reliability, an additional coder was trained on the annotation scheme and processed 286 sentences (approximately 65% of the corpus).

To assess the degree of agreement between these two coders, we used a version of the b agreement coefficient proposed by Artstein and Poesio ( 2005 ). b is designed separate probability distribution for each coder. Weighted coefficients like b permit degrees of agreement to be measured, so that partial agreement is penalised less severely than total disagreement. Like other weighted coefficients, b is based on the ratio between the observed and expected disagreement on the corpus.

To use this coefficient, we must define a measure that computes the distance between two proposed annotations. We use a measure similar to that proposed by Passonneau ( 2004 ) for measuring agreement on set-valued annotations. The full details of the computation are included in Foster ( 2007 ); here, we give an informal description.

For each display proposed by each coder on a sentence S , we search for a corresponding display proposed by the other coder X  X ne with the same value (e.g., a brow raise) and covering a similar span of nodes. If both proposals cover the same nodes, that indicates no disagreement (0); if one display covers a strict subset of the nodes covered by the other, that indicates minor disagreement  X  1 3  X  ; if the nodes covered by the two proposals overlap, that is a more major disagreement  X  2 3  X  ; and if no corresponding display can be found, there is total disagreement (1). The total observed disagreement D o ( S ) is the sum of the disagreement level for each display proposed by each coder on sentence S .

The expected disagreement D e ( S ) on a sentence S is based on the length of the sentence. We first use the corpus counts to compute the probability of each coder assigning each possible facial display to word spans of all possible lengths. We then use these probabilities to estimate the likelihood of the two coders assigning identical, super/subset, overlapping, or disjoint annotations to the sentence, for each possible display. The total expected disagreement for the sentence is the sum of these probabilities across all displays, using the same weights as above.

The overall observed disagreement in the corpus D o is the arithmetic mean of the disagreement on each sentence; similarly, the overall expected disagreement D e is the mean of the expected disagreement across all of the sentences. To compute the value of b for the output of the two coders, we subtract the ratio of these two values from 1:
As Artstein and Poesio ( 2005 ) point out, there is no significance test for agree-ment with weighted measures such as b , and the actual value is strongly affected by the distance metric that is selected. However, b values can be compared with one another to assess degrees of agreement. The overall b value between the two coders on the full set of 286 sentences processed by both was 0.561, with b values on individual facial displays ranging from a high of 0.661 on nodding to a low of 0.285 on eye narrowing (a very rare motion). To put these values into context, we also computed b on the set of 20 sentences processed by the additional coder as part of the training process (which are not included in the set of 286). The overall b value between the coders on these sentences is 0.231, with negative values for some of the individual displays, indicating that the training process had a positive effect on agreement. 3.4 Patterns in the corpus Several contextual features had a significant effect on the facial displays occurring on that node. To determine the most significant factors, we performed multinomial logit regression as described by Fox ( 2002 ); the following contextual features had the most significant effect (all p &lt; 0.001 on the Wald test). Nodding and brow raising were both more frequent on nodes with any sort of predicted pitch accent. In negative user-preference contexts, eyebrow raising, eye narrowing, and left leaning were all relatively more frequent; in positive contexts, the relative frequency of right turns and brow raises was higher. In the first half of two-clause sentences, brow lowering was also more frequent, as was upward nodding, while downward nodding and right turns showed up more often in the second clause. The impact of all of these features was similar in the corpora produced by both annotators. Foster ( 2007 ) describes the corpus patterns in detail.

The increased frequency of nodding and brow raising in on prosodically accented words agrees with other findings on emphatic facial displays such as those of Ekman ( 1979 ) and Graf et al. ( 2002 ). The findings on characteristic positive and negative displays do not have any direct analog in previous work, but when these displays were shown to human judges, they were reliably able to identify them and preferred outputs with consistent polarity on the verbal and non-verbal channels (Foster 2007 ). These findings from the corpus add to the growing body of knowledge on the communicative function of non-verbal signals: Krahmer and Swerts ( 2005 ), for example, have demonstrated that typical expressions of uncertainty are identifiable, while Rehm and Andre  X  ( 2005 ) found that an embodied agent using deceptive non-verbal behaviour was seen as less trustworthy than one that did not. 3.5 Satisfying the requirements for a generation corpus This corpus addresses all of the requirements for a generation corpus outlined in Sect. 2.2 . As in many previous corpora, we ensured that the corpus included full contextual information by basing it on output created in known pragmatic contexts. Also like many others, we designed the annotation scheme to consider only those behaviours (head and eyebrow motions) that could easily be controlled on the annotate the amplitude of mouth movements, despite the fact that this factor has been documented to be correlated with prosodic emphasis, because this is not a dimension that can easily be controlled on the target head.

In the final corpus, cross-modal links are made between facial displays and sets of nodes in the OpenCCG derivation tree, which is useful in the generation process and also allowed for respectable inter-coder agreement. Selecting a linking level took some effort and experimentation, and two other versions were considered before settling on the one in the final annotation scheme. We can use the data in the corpus to test whether these modifications to the scheme were justified.

In a previous study using the same video recordings but a different, simpler scheme (Foster and Oberlander 2006 ), facial displays could only be associated with Significance cannot be assessed for the differences in the F scores or b values, but the trend is the same. 5.2 Human preferences The face-display schedules generated by the majority strategy scored above those generated by the weighted strategy on all corpus-similarity measures. However, the majority display combination in almost all contexts (88%) is actually no motion at all, and occasionally (7.1%) a downward nod on its own. This means that the schedules generated by the majority strategy tend to have nodding on accented words as the only motion type. These schedules score highly on corpus similarity because they do not diverge greatly on average from the corpus; however, this does not necessarily mean that such facial displays will be preferred by users over those generated by the weighted strategy, which include a wider range of the non-verbal behaviours recorded in the corpus. Indeed, in other studies of corpus-driven generation systems X  X .g., Belz and Reiter ( 2006 ) X  X he versions preferred by human judges tended be those that scored lower on corpus similarity. To test whether that is the case with this system, we gathered human judgements on the generated output. 5.2.1 Materials We randomly selected 24 sentences from the corpus and generated three talking-head videos for each: using the schedules generated by the majority and weighted strategies in the cross-validation, as well as the original corpus annotations. 4 The videos were generated using the RUTH talking head (DeCarlo et al. 2004 ) and the Festival speech synthesiser (Clark et al. 2004 ), using built-in facial displays of the RUTH head synchronised with the relevant span in the speech. Figure 6 shows some sample facial displays on the RUTH head.

To map from a generation schedule to a RUTH video, we first obtained the phoneme timing for all words from Festival. We then created an animation schedule with the timing of all selected motions, where each motion in the schedule was synchronised with the start and end of the corresponding words. For example, the right turn in the weighted schedule from Fig. 5 would begin with the first phoneme of the and end at the same time as the s of tiles . Every instance of the same motion (e.g., a downward nod) was realised with the same low-level RUTH commands: built-in commands for brow motions and eye narrowing, and  X  X  X ogs X  X  for the rigid head motion. The schedule was then sent to RUTH along with the speech-synthesiser waveform to create a video. 5.2.2 Procedure This experiment was run over the world-wide web, with subjects recruited through a department student mailing list and by a posting on a website devoted to psycholinguistic experiments. A total of 56 subjects took part: 34 male subjects and 22 females, mostly between the ages of 20 and 30. Thirty-one of the subjects were expert computer users, while the rest were mainly intermediate users. Just under half of them (24) were native speakers of English, while most of the rest were speakers of other European languages.

Each subject was shown pairs of videos for all 24 sentences and asked to choose which version they preferred, following their first instinct as much as possible. Each subject performed each of the three possible pairwise comparisons between schedule types eight times, four times in each order. Both the mapping between pairwise comparisons and items and the presentation order were generated randomly for each subject. 5.2.3 Results The overall results of this study are shown in Fig. 7 . Each pair of bars shows the count of pairwise choices made between schedule types; for example, when the choice was between an original corpus schedule and one generated by the majority strategy, the original version was chosen 295 of 448 times (66%). To assess the significance of the results, we can use a binomial test, which provides an exact measure of the statistical significance of deviations from a theoretically expected classification into two categories. This test indicates that all of the trends are significant: the original vs. weighted comparison at p &lt; 0.05, and the other two at p &lt; 0.0001. None of the demographic factors had any impact on these results. 5.3 Discussion The results of the two studies differ: the cross-validation study scored the majority strategy higher on all measures, while the human subjects tended to prefer the output of the weighted strategy. This supports our prediction that the judges would prefer generated output that reproduced more of the variation in the corpus, regardless of the corpus-similarity scores; in this sense, these results are similar to those found by Belz and Reiter ( 2006 ).

The human judges preferred the regenerated corpus sentences to those generated by either strategy, although the preference over the weighted strategy was less pronounced. This suggests that making an independent choice for each node is useful, but not enough to capture the behaviour of the subject, and that more sophisticated generation strategies could be successful for this task. We discuss possible extensions in this area in the following section. 6 Conclusions and future work We have presented a multimodal corpus based on a single speaker reading scripted sentences in the domain of the COMIC multimodal dialogue system, where the corpus was annotated for the head and eyebrow motions that occur in various syntactic, prosodic, and pragmatic contexts. The speaker showed systematic differences in the displays he used; the most relevant contextual factors were the user-preference evaluation, the predicted pitch accents, and the clause of the sentence. The characteristic behaviours on prosodically stressed words agree with previous findings on non-verbal behaviour; the motions correlated with positive and negative user-preference evaluations are more specific to this domain and corpus, but still sufficiently general that users were reliably able to identify them.
We used the data from this corpus to select head and eyebrow motions for an embodied conversational agent when producing output in this same domain. We compared two selection strategies: always choosing the majority option, or making a weighted choice among all of the options. The former strategy scored higher on every measure of corpus similarity in a cross-validation study, while the output of the latter strategy was preferred by human judges. This demonstrates the danger of relying on corpus similarity for evaluating generated output, as it tends to favour strategies that discard much of the interesting variation in the corpus. There is still a place for automated corpus-based evaluation in generation, particularly during the development of a system or to verify that output is well-formed; however, it is crucial that any such evaluation be accompanied by a user study or an automated evaluation that considers other factors such as output diversity.

The human judges also preferred videos generated directly from the corpus data to the output of either strategy, with a more significant preference over the majority strategy. An interesting additional study would be to gather judgements on displays selected according to the overall corpus counts, independent of context. While the weighted strategy was partly successful, a more sophisticated implementation that better reproduces the range of corpus data would likely have greater success. COMIC uses the OpenCCG realiser, which incorporates n -gram models into its realisation process, so one possible implementation technique would be to build models combining words with multimodal behaviour and to replace the two-stage process by an integrated one. Such an implementation would also be more in line with the psycholinguistic evidence (McNeill 2000 ) that verbal and non-verbal behaviour are produced together from a common representation.

Another possible source of increased output quality is to extend the range of displays. The annotation scheme for this corpus used only five motion types, and the RUTH videos were generated using a single example for each type, varying only in duration. For future implementations, a richer set of displays X  X athered through motion capture or a different style of annotation X  X ould produce more interesting and naturalistic output. To support such an implementation, the process of controlling the embodied agent would also have to be extended to support the full set of displays, and it is possible that supporting such displays would require a different embodied-agent implementation.
 References
