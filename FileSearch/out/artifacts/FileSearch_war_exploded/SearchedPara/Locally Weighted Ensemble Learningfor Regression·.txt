 Different from single model prediction, ensemble learning trains a set of base models and combines their outputs for producing accurate prediction. And it is robust to data noise [ 11 ]. A large number of researches have been proposed for designing effective ensemble system in recent years [ 7 , 10 , 15  X  17 ]. models; (2) Combining base models to make an accurate prediction [ 15 ]. For the first task, base models are built by the same learning algorithm or different learning algorithms. After generating base models, an appropriate integration strategy is required to make an accurate prediction.This makes generalization constant weighted methods and dynamic weighted methods [ 14 ]. As to constant weighted methods, the coefficients of base models are constant in the whole data space. In 1993, Perrone and Cooper proposed the Basic Ensem-ble Method. This method calculates the mean of predictions of all base models, and achieves better results than single models [ 16 ]. In 2000, Seeger introduced Bayesian Model selection into ensemble strategy and discussed the effectiveness measure which focused on both sparsity and diversity of base models [ 21 ]. The above ensemble methods learn a unified combination function in the whole data space. However, data distribution is variable. The regression function varies from sample to sample. To deal with this problem, dynamic weighted integration is developed. Mendes-Moreira divided the dynamic weighted methods former, Wooks et al. introduced K-nearest neighbor method (KNN) which finds is obtained by other methods, like Discriminant Adaptive Nearest Neighbor [ 4 ]. Since 1991, Michal I. Jordan proposed a series of dynamic ensemble methods which are realized by changing integration function. In 1991, mixture of local set of training samples. In 1994, the team of Jordan proposed a hierarchical mixture of experts where integration function is different from other integration base models on each test sample. The final prediction is obtained by weighted of integration function methods have achieved good performance. The former is the problem of multiple objective functions, Bottou and Vapnik proposed local solving complex and dynamic distribution problem in theory.
 Local learning algorithm contains of a series of methods to obtain the local sample of each test sample, such as KNN, soft-max function and Radial Basic Function network. The objective functions are different from test samples which into ensemble regression to deal with the problem of multiple objective function, which is called Locally Weighted Ensemble algorithm (LWE).
 LWE is a data-driven method. The weights of base models in different regions are variation continuously which is assigned by soft-max function. However, the do not consider this problem. In this paper, we apply L 21 regularization and Laplacian-regularization to deal with this problem. The exper-imental assessment is carried on UCI datasets. The experiments show that our method outperforms other methods, such as single methods, constant weighted ensemble methods and dynamic ensemble methods.
 The remainder of the paper is organized as follows: Sect. 2 describes two kinds of ensemble integration methods. Section 3 presents the proposed locally weighted ensemble method in detail. Section 4 presents the experiments obtained on UCI datasets. Finally, the conclusions of this paper are drawn in Sect. 5 . Ensemble integration learns how to combine base models to make a final predic-tion. During the phase of integration, the discriminative function is where M is the number of base models, w i is the weight of the ith base model and f i ( x ) is the prediction of ith base model. 2.1 Constant Weighted Ensemble Yin investigates ensemble method which focuses on both sparsity and diversity [ 21 ]. The objective function is where  X  is the control parameter for sparse regularization and  X  is the parameter for diverse regularization. Sparsity is exposed by the l Diversity is expressed by M i =1 ( w T ( x i ) 2  X  ( w T x error-ambiguity decomposition for regression. 2.2 Dynamic Weighted Ensemble Tsymbal et al. present a dynamic ensemble method through local accuracy [ 18 ]. The weight w i is calculated by The margin is defined as The distance-based weight coefficient reflects similarity between two instance of the proposed method is significantly better than constant weighted method. we introduce a new dynamic integration method. In this paper, We think some of the base regressors are not useful for final pre-diction. The base regressors which Mean Absolute Error is the smallest among the remaining base regressors is added to ensemble learning prior, and the Mean Absolute Error is described in Sect. 4 . When the difference of MAE between two added regressors are treated as the base models of LWE. Secondly, a weighted function is trained to combine the outputs of base models. Each sample on differ-ent base models is assigned with different weights which are realized by soft-max model. Soft-max model is presented as follow number of base models. The parameters in { v i 0 ,v i } ,i =1 , 2 , learned, where i is the ith base model. In the following subsections, we analyze the objective function and the optimizational strategy of our proposed method. 3.1 Objective Function of Locally Weighted Ensemble Learning The objective function of our proposed method is Regularization is included to make the objective function well-posed [ 6 ]. Let J ( w ) denote the objective function the real value. The methods of regularization are variable, such as L L 21 -Norm, L 2 -Norm, L F -Norm and manifold regularization. In this paper, we introduce. L 21 -Norm, L F -Norm and Laplace regularization to our objective function.
 ensemble some of base models may be better than ensemble all for prediction. This leads to sparse ensemble learning. The used sparse methods are L L -Norm and L 21 -Norm. For convenient calculation, we introduce L to regularization. Each base model has its own weight value, zero or nonzero. L 2 , 1 -Norm can select the effective base models [ 6 ]. L where w ij is the weight of the i sample and the d -dimensional feature. B. L F -Norm Regularization ( LW E -L F ). In the matrix norm, F robenius -Norm is a convex function. The introduction of F robenius -Norm makes the objective function strongly convex. If we solve the problem by gradient decent method, the convergence is stable and fast [ 12 ]. F robenius -Norm is written as C. Laplace-Norm Regularization ( LW E -L ). The solution of an ill-posed problem can be approximated by variational principles, which contains the prior smooth-the smoothness, where the manifold is determined by Laplacian. Laplace-Norm keeps the information of spacial structure.
 where L = D  X  Dis , D ii = n j =1 Dis ij and n is the size of samples. Dis distance between sample i and sample j . 3.2 Optimization of Locally Weighted Ensemble Learning of optimization, these are constant. Therefore, the objective function Eq. 8 is convex. We apply Gradient Descent method to solve it. The introduction of soft-max function makes the constraint condition, w ( x | v )  X  of soft-max function are where w i is the weight of i th sample and For further understanding, we give an example to describe our method. A. L 2 , 1 -Norm Regularization ( LW E -L 2 , 1 ). The gradients of L ization are written as  X  X  ( w )  X  X  ( w ) get the optimization of w by fixing D . In next iteration, D is updated by the new w .
 B. L F -Norm Regularization ( LW E -L F ). The same as L 2 , 1 tion,the gradients of L F -Norm regularization are written as  X  X  ( w )  X  X   X  X  ( w )  X  X  C. Laplace-Norm Regularization ( LW E -L ). The gradients of Laplace-Norm reg-ularization are written as  X  X  ( w )  X  X  ( w ) D. Example. Assume that there are n samples, a attributes and M base models in training set. They consist in solving the following system p 1 M is the probability of the first sample on Mth base model. The sum of each row is 1. The final prediction is obtained as 3.3 Algorithm of Locally Weighted Ensemble Learning In this paper, a dynamic ensemble method, which depends on the characteris-tic of data, is proposed. Different integration methods are applied for different regions. Here we describe the algorithm of LWE.
 Local-Weighted Ensemble algorithm (LWE) After analysis, the time complexity of LW E  X  L 2 , 1 is lO ( n number of iteration. The time complexity of LW E  X  L F and LW E and lO ( n 3 ) respectively. sented. The UCI datasets that we selected in the experiments are presented in data) and 1 / 3 (testing data) and the regularization parameter is obtained by cross-validation method. In our experiments, seven regression methods, (Least Square (LR), Mat-primal, Support Vector Machine (SVM), Extreme Learn-ing Machine-kernel (ELM), Feed-forward neural network (FNN), Elman Neural Network (ENN), Layer-recurrent neural network (LRN)), are selected as base models. Mean Absolute Error (MAE)and Root Mean Square Error (RMSE) are selected as the criteria [ 2 ] where n t is the size of the testing data, x i is the i th value. 4.1 Convergence of Objective Function Figures 2 and 3 plot the evolution of the objective values of LW E -L L and LW E -L . We see that the objective value gets to convergence at the second iteration. This evaluates that our proposed method converges quickly. Sometimes, the convergence is different. For example, on WhiteWine dataset, LW E -L 2 , 1 gets to convergence at the fifth iteration. However, LW E -F gets to convergence at the second iteration. This evaluates our proposed method is universal. 4.2 Prediction on UCI Datasets To illustrate the behavior of LWE, we compare with single methods, (LRN, LR, ELM, SVM), constant weighted ensemble method (the weights of base models base on the MSE), dynamic ensemble method [ 1 ], AMLE [ 7 ]and Adaboost method. For the dynamic ensemble method, we should establish a set of base models for each test point. However there is no prior knowledge about the optimal number of nearest neighbors. We have not calculated the value of dynamic ensemble method on Elevator and Physic.
 ods respectively. In Tables 2 and 3 , we can find the performances of ensem-ble methods are improved. Dynamic weighted ensemble method is better than integration methods. Compared with above methods, our proposed methods, LW E -L 2 , 1 , LW E -L F and LW E -L , show strong robustness. Compared with Adaboost, the MAE of LW E -L on Housing dataset is decreased by 0.0079. Accordingly, RMSE is decreased by 0.0147. Compared with AMLE, the perfor-mance on MAE is similar. However, our proposed method is better than AMLE on RMSE. For example, RMSE is fallen by 4.07 %. This fact confirms the inte-grating effectiveness of our model. In this paper, a novel dynamic ensemble method is presented, LWEs. Regression methods, such as FNN, LR, ENN, LRN, ELM, mat-primal and SVM, are used to define base regressors. Soft-max function is introduced to assign different integration strategies to different regions. Experiments on eight UCI datasets confirm the effectiveness of LWEs. Some conclusions can be drawn as follow (1) The distribution of dataset varies from sample to sample. The distribution of dataset is variable. It is not wise to use one global model for prediction. (2) The different fusion measures must be applied for different regions. A con-stant weighted ensemble strategy cannot reflect the differences of samples.
We introduce the data-driven ensemble method, Locally Weighted Ensemble, to combine the individual models.
 (3) The convergence of our proposed method is fast. LWEs only need seconds
