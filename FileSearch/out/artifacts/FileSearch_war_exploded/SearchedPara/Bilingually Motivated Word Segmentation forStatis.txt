 YANJUN MA and ANDY WAY Dublin City University 1. INTRODUCTION State-of-the-art Statistical Machine Translation (SMT) r equires a certain amount of bilingual corpora as training data in order to achi eve competitive results. The only assumption behind most current statistic al models [Brown et al. 1993; Vogel et al. 1996; Deng and Byrne 2005] is that the aligned sen-tences in such corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word bound aries are not orthographically marked, tools which segment a sentence in to words are re-quired. Even for a language like English, where spaces can of fer an easy ap-proximation to the minimal content-bearing units, an optim al segmentation is still required when analyzing multi-word units, especia lly noncompositional compounds such as  X  X ick the bucket X  and  X  X ot dog X  [Melamed 19 97].
 without any bilingual consideration, that is, the segmenta tion of the source (target) language is performed regardless of the correspon ding target (source) language at hand, which makes the word alignment task more di fficult since different languages may realize the same concept using vary ing numbers of words (cf. Wu [1997]). This can generate a great deal of compl exity for ( bilin-gual ) word alignment models if the corresponding texts are inapp ropriately segmented. Moreover, most segmenters are usually trained o n a manually segmented domain-specific corpus. Therefore, such a segmen tation tends to be sensitive to the domain of the data and cannot produce cons istently good results when used across different domains.
 lems of word segmentation in the context of PB-SMT. Some stat istical align-ment models allow for 1-to-n word alignments for those reasons; however, they rarely question the monolingual tokenization and the basic unit of the align-ment process. 1 Morever, statistical alignment models assume a first-order de-pendency between alignment decisions in order to make the al ignment process efficient. Some long-distance dependencies cannot be captu red under such models. Some more recent research focuses on combining vari ous segmenters either in SMT training [Zhang et al. 2008] or decoding [Dyer e t al. 2008]. One important yet often neglected fact is that the optimal se gmentation of the source (target) language is dependent on the target (sou rce) language it-self, its domain, and its genre. Segmentation considered to be  X  X ood X  from a monolingual point of view may be unadapted for training alig nment models or PB-SMT decoding [Ma et al. 2007]. The resulting segmentat ion will con-sequently influence the performance of a PB-SMT system, and a bilingually motivated segmentation is highly desirable for PB-SMT task s.
 word alignment via word packing [Ma et al. 2007]. We conduct f urther exper-iments using a more established PB-SMT system and extend thi s approach to perform independent word segmentation. We focus on optim izing the seg-mentation with the goals of (1) simplifying the task of autom atic word aligners by packing several consecutive words together when we believe they cor re-spond to a single word in the opposite language. By identifyi ng enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural; from an informa tion-theoretic perspective, such a process reduces the predictive power of translation models [Melamed 1997]; and (2) capturing long-distance dependenc ies between align-ment decisions in a incremental manner, that is, we bootstra p the word pack-ing and subsequently optimize the word segmentation based o n its influence on SMT performance. We then generalize this method to produc e a bilingually motivated automatically domain-adapted word segmentatio n approach for PB-SMT without relying on any existing word segmenters. We first utilize a small bilingual corpus with the relevant language segmented into basic writing units (e.g., characters for Chinese), and then cast the segmentat ion problem into an alignment problem. We also investigate various issues rega rding scalability related to such a process.
 tistical word aligner (e.g., Giza++) to obtain a set of candi date  X  X ords X  to be packed. We evaluate the reliability of these candidates usi ng simple met-rics based on co-occurrence frequencies, similar to those u sed in associative approaches to word alignment [Melamed 2000; Tiedemann 2003 ]. We then modify the segmentation of the respective sentences in the p arallel corpus ac-cording to these candidate words; these modified sentences a re then given back to the word aligner, which produces new alignments. We evalu ate the valid-ity of our approach by measuring the influence of the segmenta tion process on Chinese-to-English Machine Translation (MT) tasks.
 the interaction between word segmentation and word alignme nt. Firstly we investigate the influence of word segmentation on PB-SMT acr oss different do-mains and demonstrate the necessity of refining word segment ation methods for PB-SMT in order to achieve consistently good results. We also discuss the particular word alignment issues related to Chinese X  X ngli sh which motivated our approach. Section 3 describes the working mechanism of o ur bilingually motivated word segmentation approach, namely word packing . In Section 4, we describe the generalization of our approach and its appli cation to indepen-dent word segmentation. The corresponding experiments are reported in Sec-tions 5 and 6. In Section 7 we discuss related work in this dire ction. Section 8 concludes and gives avenues for future work. 2. INTERACTION BETWEEN WORD SEGMENTATION AND ALIGNMENT In this section, we first detail a pilot study on the influence o f word segmen-tation on the performance of PB-SMT. Then we show that the per vasive 1-to-n alignments in Chinese X  X nglish word alignment motivate us t o take advan-tage of the interaction between word segmentation and align ment in order to simplify the alignment task.
 2.1 The Influence of Word Segmentation on PB-SMT The monolingual word segmentation step in traditional SMT s ystems has a substantial impact on the performance of such systems. A con siderable amount of recent research has focused on the influence of word segmen tation on SMT [Ma et al. 2007; Chang et al. 2008; Zhang et al. 2008]; however , most explo-rations have focused on the impact of various segmentation g uidelines and the mechanisms of the segmenters themselves. A current rese arch interest concerns consistency of performance across different doma ins. From our ex-periments, we show that monolingual segmenters cannot prod uce consistently good results when applied to a new domain.
 involves three off-the-shelf Chinese word segmenters, inc luding ICTCLAS (ICT) Olympic version, 2 LDC segmenter 3 , and Stanford segmenter version 2006-05-11. 4 Both ICTCLAS and Stanford segmenters utilize machine learn -ing techniques, with Hidden Markov Models for ICT [Zhang et a l. 2003] and conditional random fields for the Stanford segmenter [Tseng et al. 2005]. Both segmentation models were trained on news domain data with na med entity recognition functionality. The LDC segmenter is dictionar y-based with word frequency information to help disambiguation, both of whic h are collected from data in the news domain. We used Chinese character-based and manual seg-mentations as points of contrast. Table I shows the pairwise F-measure of the automatic segmenters. On the IWSLT data set in the dialogue d omain, we ob-served the strongest agreement between the LDC and ICT segme nters, which is even stronger than for Stanford and ICT segmenters. On NIS T data, as ex-pected, the Stanford and ICT segmenters agree more. On both d ata sets, the LDC and Stanford segmenters show the greatest discrepancie s.
 previously mentioned data using a state-of-the-art PB-SMT system: Moses [Koehn et al. 2007]. The performance of the PB-SMT system is m easured via Bleu score [Papineni et al. 2002].
 with respect to the three above-mentioned segmenters, name ly UN data from the NIST 2006 evaluation campaign. 5 As can be seen from Table II, us-ing monolingual segmenters achieves consistently better S MT performance than character-based segmentation (CS) on different data s izes, which means character-based segmentation is not good enough for this do main where the vo-cabulary tends to be large. We can also observe that the ICT an d Stanford seg-menters consistently outperform the LDC segmenter. Even us ing 3M sentence pairs for training, the differences between the Stanford an d LDC segmenters are still statistically significant ( p &lt; 0 . 05) using approximate randomization [Noreen 1989] for significance testing.
 alogue domain, the results seem to be more difficult to predic t. We trained the system on different amounts of data and evaluated the system on two test sets: IWSLT 2006 and 2007. From Table III, we can see that on the IWSL T 2006 test sets, LDC achieves consistently good results and the St anford segmenter is the worst. 6 Furthermore, character-based segmentation also achieves com-petitive results. On IWSLT 2007 data, all monolingual segme nters outperform character-based segmentation and the LDC segmenter is only slightly better than the other segmenters.
 sions. First, character-based segmentation cannot achiev e state-of-the-art re-sults in most experimental settings. This also motivates th e necessity to work on better segmentation strategies. Second, monolingual se gmenters cannot achieve consistently good results when used in another doma in. In the fol-lowing sections, we propose a bilingually motivated segmen tation approach which can be automatically derived from a small representat ive data set, and the experiments show that we can consistently obtain state-of-the-art results in different domains. Using this approach, we can either enh ance the existing monolingual segmenter or directly perform word segmentati on without relying on any monolingual segmenters. 2.2 The Case of 1 -to-n Word Alignment The same concept can be expressed in different languages usi ng varying num-bers of words; for example, a single Chinese word may frequen tly surface as a compound or a collocation in English given the large diff erences be-tween the two languages. To quickly (and approximately) eva luate this phe-nomenon, we trained the statistical IBM word-alignment Mod el 4 [Brown et al. 1993] 7 using the Giza++ software [Och and Ney 2003] for the follow-ing language pairs: Chinese X  X nglish (ZH X  X N), Italian X  X ng lish (IT X  X N), and German X  X nglish (DE X  X N), using the IWSLT 2007 corpus [Takez awa et al. 2002; Paul 2006] for the first two language pairs, and the Euro parl corpus [Koehn 2005] for the last one. These asymmetric models produ ce alignments between one word and several words in both directions. Word s egmentation was performed totally independently of the bilingual align ment process, that is, it is done in a monolingual context. For European languag es, we apply the maximum entropy-based tokenizer of OpenNLP; 8 the Chinese sentences were manually segmented [Paul 2006].
 for the various languages and directions. As expected, the n umber of 1: n align-ments with n 6 = 1 is high for Chinese X  X nglish (  X  40%), and significantly higher than for European languages. The case of 1-to-n alignments is, therefore, ob-viously an important issue when dealing with Chinese X  X ngli sh word align-ment. 9 We can also observe that for all three language pairs, most of the n words involved in 1-to-n alignments are consecutive (con.).
 the manually aligned Chinese X  X nglish corpus as shown in Tab le V. This manually aligned corpus is IWSLT devset 3, which contains 50 2 sentence pairs after cleaning [Ma et al. 2008]. We observed similar distrib ution for 1: n ( n &gt; 1) alignments. The main difference is that the automatic align ers tend to pro-duce more 1: 0 alignments, while human annotators tend to gen erate more m : n alignments. 2.3 Notation While in this article we focus on Chinese X  X nglish, the propo sed method is applicable to any language pair; even for closely related la nguages, we expect improvements to be seen. Notwithstanding the generality of the approach, in what follows we assume Chinese X  X nglish MT in order to explai n our notation. Chinese-to-English (resp. an English-to-Chinese) word al ignment between c J 1 and e I 1 . Since we are primarily interested in 1-to-n alignments, A C  X  E can be represented as a set of pairs a j = h c j , E j i denoting a link between one single Chinese word c j and a few English words E j (and similarly for A E  X  C ). The set E j is empty if the word c j is not aligned to any word in e I 1 . 3. BOOTSTRAPPING WORD ALIGNMENT VIA WORD PACKING Our approach (cf. Ma et al. [2007]) consists of packing conse cutive words to-gether when we believe they correspond to a single word in the other language. This bilingually motivated packing of words changes the bas ic unit of the align-ment process, and simplifies the task of automatic word align ment. We thus minimize the number of 1-to-n alignments in order to obtain more comparable segmentations in the two languages. In this section, we pres ent an automatic method that builds upon the output from an existing automati c word aligner. More specifically, we (1) use a word aligner to obtain 1-to-n alignments, (2) extract candidates for word packing, (3) estimate the relia bility of these candi-dates, (4) replace the groups of words to pack by a single toke n in the parallel corpus, and (5) reiterate the alignment process using the up dated corpus. The first three steps are performed in both directions, and produ ce two bilingual dictionaries (source X  X arget and target X  X ource) of groups of words to pac k. 3.1 Candidate Extraction In the following, we assume the availability of an automatic word aligner that can output alignments A C  X  E and A E  X  C for any sentence pair ( c J 1 , e I 1 ) in a parallel corpus. We also assume that A C  X  E and A E  X  C contain 1-to-n align-ments. Our method for repacking words is very simple: whenev er a single word is aligned with several consecutive words, they are con sidered candi-dates for repacking. Formally, given an alignment A C  X  E between c J 1 and e I 1 , if a j = h c j , E j i  X  A C  X  E , with E j = { e j j k +1  X  j k = 1, then the alignment a j between c j and the sequence of words E j is considered a candidate for word repacking. The same goes f or A E  X  C . Some examples of such 1-to-n alignments between Chinese and English (in both di-rections) that we can derive automatically are displayed in Figure 1. 3.2 Candidate Reliability Estimation Of course, the process described above is error-prone and if we want to change the input to the word aligner, we need to make sure that we are n ot making harmful modifications. 10 We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictio-nary. To perform this filtering, we use two simple statistica l measures. In the following, a j = h c j , E j i denotes a candidate.
 that is, the number of times c j and E j cooccur in the bilingual corpus. This very simple measure is frequently used in associative approache s [Melamed 1997; Tiedemann 2003]. The second measure is the alignment confide nce, defined as where C ( a j ) denotes the number of alignments proposed by the word align er that are identical to a j . In other words, AC ( a j ) measures how often the aligner aligns c j and E j when they cooccur. We also impose that | E j |  X  k , where k is a fixed integer that may depend on the language pair (betwee n 3 and 5 in practice). The rationale behind this is that it is very rare t o obtain a reliable alignment between one word and k consecutive words when k is high.
 measures are above some fixed thresholds t COOC and t AC , which allow for the control of the size of the dictionary and the quality of its co ntents. Some other measures (including the Dice coefficient) could be consider ed; however, it has to be noted that we are more interested here in the filtering than in the discovery of alignments, since our method builds upon existing aligne rs. Moreover, we will see that even these simple measures can lead to an improv ement in the alignment process in an MT context (cf. Section 6). 3.3 Bootstrapped Word Repacking Once the candidates are extracted, we repack the words in the bilingual dictio-naries constructed using the method described above; this p rovides us with an updated training corpus, in which some word sequences have b een replaced by a single token. This update is totally naive; if an entry a j = h c j , E j i is present in the dictionary and matches one sentence pair ( c J 1 , e I 1 ) (i.e., c j and E j are respectively contained in c J 1 and e I 1 ), then we replace the sequence of words E j with a single token which becomes a new lexical unit. 11 Note that this replace-ment occurs even if no alignment was found between c j and E j for the pair conservative; we trust the entry a i to be correct. This update is performed in both directions. It is then possible to run the word aligner u sing the updated (simplified) parallel corpus in order to obtain new alignmen ts. By performing a deterministic word packing, we avoid the computation of th e fertility para-meters associated with fertility-based models.
 words together, they become the new basic unit to consider, a nd we can rerun the same method to get additional groupings. However, we hav e not seen in practice much benefit from running it more than twice (few new candidates are extracted after two iterations).
 strongly depends on the language pair. For example, white wine , excuse me , call the police , and cup of (cf. Figure 1) translate respectively as vin blanc , excusez-moi , appellez la police , and tasse de in French. Those groupings would not be found for a language pair such as French X  X nglish, whic h is consistent with the fact that they are less useful for French X  X nglish th an for Chinese X  English in an MT perspective. 3.4 Word Unpacking and Phrase-Based SMT Decoding The bidirectional grouping approach can improve the qualit y of alignment and correspondingly improve the quality of phrase extraction a nd the estimation of related parameters. In the decoding stage, given that the in put is not packed and the language model is also trained on unpacked word segme ntations, we need to undertake  X  X ord unpacking X  before estimating the pa rameters. The unpacking in PB-SMT is performed following the phrase extra ction process. Specifically, in a log-linear PB-SMT system, the phrase tran slation probabili-ties and lexical distortion models are reestimated based on relative frequen-cies; the lexical weighting probabilities are calculated b ased on the lexical translation distribution with word packing. packing, that is, both source and target sentences are packe d, given that the language models are trained on texts without word packing. I f we constrain the word packing process by only packing the source language , the word un-packing step could be avoided and word-lattice decoding cou ld be utilized in-stead (cf. Section 4). 4. BILINGUALLY MOTIVATED WORD SEGMENTATION 4.1 Word Segmentation as an Alignment Problem The approach proposed in Section 3 can be applied to word segm entation by only packing the source language. The only assumption is tha t the sentence to be segmented can be split into basic writing units (e.g., cha racters for Chinese and kana for Japanese). The notation in Section 2.3 can be eas ily adapted for a Chinese-to-English character-to-word alignment betwee n c J 1 and e I 1 . Since we are primarily interested in 1-to-n alignments, A C  X  E can be represented as a set of pairs a i = h C i , e i i denoting a link between one single English word e i and a few Chinese characters C i . The set C i is empty if the word e i is not aligned to 4.2 Bootstrapping Word Segmentation We use the same approach proposed in Section 3.1 to extract ca ndidate words. Our method for Chinese word segmentation is as follows: when ever a single English word is aligned with several consecutive Chinese ch aracters, they are considered candidates for grouping. Some examples of such 1 -to-n alignments between Chinese characters and English words we can derive automatically are displayed in Figure 2.
 bility of the candidate words and use the bootstrapping appr oach in Section 3.3 to derive better word segmentation. 4.3 Word Lattice Decoding Casting word segmentation as an alignment problem implies t hat word seg-mentation of a sentence depends not only on the current sente nce to segment but also on the corresponding target language. In such a cont ext, the word lattice representation is particularly suitable in the dec oding stage which aims to search for the most possible target sentence. natives can be encoded into a compact representation of word lattices. A word lattice G = h V , E i is a directed acyclic graph that formally is a weighted finite state automaton. In the case of word segmentation, each edge is a candidate word associated with its weights. A straightforward estima tion of the weights is to distribute the probability mass for each node uniforml y to each outgoing edge. 12 The single node having no outgoing edges is designated the  X  X  nd node X . An example of word lattices for a Chinese sentence is shown in Figure 3. tices relies on multiple monolingual segmenters [Xu et al. 2005; Dyer et al. 2008]. One advantage of our approach is that the bilingually motivated seg-mentation process facilitates word lattice generation wit hout relying on other segmenters. As described in Section 4.2, the update of the tr aining corpus based on the constructed bilingual dictionary requires tha t the sentence pair meets the bilingual constraints. Such a segmentation proce ss in the training stage facilitates the utilization of word lattice decoding . tence c J 1 consisting of J characters, the traditional approach is to determine the best word segmentation and perform decoding afterward. In such a case, we first seek a single best segmentation, as in Equation (2): Then in the decoding stage, we seek the translation of the mos t likely source segmentation as in Equation (3): In such a scenario, some segmentations which are potentiall y optimal for translation may be lost. This motivates the need for word lat tice decoding. The search process can be rewritten as in Equations (4) X (6): with respect to the number of characters J , it is impractical to firstly enumer-ate all possible f K 1 and then to decode. However, it is possible to enumerate all the alternative segmentations for a substring of c J 1 which contains a very limited number of characters, making the utilization of wor d lattices tractable in PB-SMT. 5. EXPERIMENTAL SETTING 5.1 Evaluation The intrinsic quality of word segmentation is normally eval uated against a manually segmented gold-standard corpus using F-score. Wh ile this approach can give a direct evaluation of the quality of the word segmen tation, it is faced with several limitations. First of all, it is really difficul t to build a reliable and objective gold-standard given the fact that there is onl y 70% agreement between native speakers on this task [Sproat et al. 1996]. Se cond, an in-crease in F-score does not necessarily imply an improvement in translation quality. It has been shown that F-score has a very weak correl ation with SMT translation quality in terms of Bleu score [Zhang et al. 2008 ]. Consequently, we choose to extrinsically evaluate the performance of our a pproach on the Chinese X  X nglish translation task, that is, we measure the i nfluence of the segmentation process on the final translation output. The qu ality of the trans-lation output is mainly evaluated using Bleu, with NIST [Dod dington 2002] and Meteor [Banerjee and Lavie 2005] as complementary metri cs. 5.2 Data For our word packing experiments, we used the Chinese X  X ngli sh datasets pro-vided within the IWSLT 2006 and 2007 evaluation campaigns. T his multilin-gual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad [Takezawa et al. 2 002]. Specifically, we used the standard training data, to which we added devset1 and devset2. Devset4 was used to tune the parameters and the performance o f the system was tested on IWSLT 2006 and 2007 test sets. We used both test s ets because they are quite different in terms of sentence length and voca bulary size. Based on the original manual segmentation for Chinese, the variou s statistics for the corpora are shown in Table VI. To test the scalability of our a pproach, we added in the HIT corpus, 13 which was made available for IWSLT 2008 evalu-ation campaign. This data set consists of around 120K senten ce pairs in the dialogue domain.
 formance of our segmenter across different domains, we used data from parliamentary documents, that is, a portion of UN data for th e NIST 2006 eval-uation campaign. The system was developed on the LDC Multipl e-Translation Chinese (MTC) Corpus and tested on MTC part 2, which was also u sed as a test set for the NIST 2002 evaluation campaign. Based on Chin ese segmen-tation using the Stanford segmenter, the various statistic s for the UN corpora are shown in Table VII. 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: Giza++ implementation of IBM word alig nment model 4 [Och and Ney 2003], the refinement and phrase-extraction he uristics de-scribed in [Koehn et al. 2003], minimum error-rate training [Och 2003], a 5-gram language model with Kneser-Ney smoothing [Kneser an d Ney 1995] trained with SRILM [Stolcke 2002] on the English side of the t raining data, and Moses [Koehn et al. 2007; Dyer et al. 2008] to translate bo th the single best segmentation and word lattices. 6. EXPERIMENTS 6.1 Word Packing configuration described previously. From these we build two bilingual 1-to-n dictionaries (one for each direction), and the training cor pus is updated by repacking the words in the dictionaries, using the method pr esented in Sec-tion 3. As previously mentioned, this process can be repeate d several times; at each step, we can also choose to exploit only one of the two ava ilable dictionar-ies, if so desired. We then extract aligned phrases using the same procedure as for the baseline system; the only difference is the basic uni t we are consider-ing. Once the phrases are extracted, we perform the estimati on of the features of the log-linear model and unpack the grouped words to recov er the initial words. Finally, minimum error-rate training and decoding a re performed. optimized on the development set. We found out that it was eno ugh to perform two iterations of repacking: the optimal set of values was fo und to be k = 3, t
AC = 0 . 9, t COOC = 20 for packing English words and t AC = 0 . 3, t COOC = 10 for packing Chinese words in the first iteration, and t AC = 0 . 9, t COOC = 8 for packing English words and t AC = 0 . 7, t COOC = 15 for packing Chinese words in the second iteration. 14 In Table VIII, we report the results obtained on the IWSLT 2007 test set, where n denotes the iteration. For each iteration, we first considered the inclusion of only the Chinese X  X nglish dicti onary, and then only the English X  X hinese dictionary. 15 when considering one of the two dictionaries. More gain can b e obtained by packing English words, leading to an increase of 1 . 17 absolute Bleu points (3 . 46% relative). The improvement is also confirmed by NIST and M eteor eval-uation metrics. Moreover, we can gain from performing anoth er step. However, the inclusion of the Chinese X  X nglish dictionary is harmful in this case, prob-ably because 1-to-n alignments have been captured during the first step. By including the English X  X hinese dictionary only, we can achi eve an increase of 1 . 84 absolute Bleu points (5 . 44% relative) over the initial baseline, which is statistically significant ( p &lt; 0 . 01). 16 ment after simplifying the alignment task after word packin g, and subse-quently higher quality phrasal translations for PB-SMT sys tems. Figure 4 gives two examples of better translation after word packing . Phrases such as  X  X here  X  X  X ,  X  X ill it take X , and  X  X et to X  are packed words in the C-E bilin-gual dictionary so that valid phrase pairs can be included in the phrase table. Moreover, the probability of these valid phrase pairs can be boosted after word packing so that the correct hypothesis can survive in the dec oding stage. procedure, we simply manually evaluated the ratio of incorr ect entries in the dictionaries. After one step of word packing, the Chinese X  X  nglish and the English X  X hinese dictionaries respectively contain 13.6% and 8.6% incorrect entries. After two steps of packing, they only contain 7.7% a nd 7.2% incorrect entries. More interestingly, some errors committed in the fi rst step can be cor-rected in the second step, leading to a dictionary of higher q uality. Some cases generally considered to be difficult such as m -to-n noncompositional phrasal alignments can also be identified in the second step. word packing are more likely to be 1: 1 than before. Indeed, th e word sequences in one language that usually align to one single word in the ot her language have been grouped together to form one single token. Table IX shows the de-tail of the distribution of alignment types after one and two steps of automatic repacking.
 after the application of repacking: the ratio of this type of alignment has in-creased by 8 . 2% for Chinese X  X nglish and 4 . 18% for English X  X hinese. the initial word segmentation on the process of word packing , we considered an additional segmentation configuration, based on the LDC s egmenter. leads to lower results than the human-corrected segmentati on. However, the proposed method seems to be beneficial irrespective of the ch oice of segmen-tation. Indeed, we can also observe an improvement in the new setting: 0 . 84 points absolute increase in Bleu (2 . 65% relative), which is statistically signifi-cant ( p &lt; 0 . 05). The experimental results of word packing reported so fa r are based on either manual segmentation or automatic segmentat ion using mono-lingual segmenters. In the next section, we show the results of directly using the word packing approach to perform word segmentation. 6.2 Word Segmentation line configuration by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word. To optimize the weights for the features of the log-linear model using mi nimum error-rate training, we segment the Chinese sentences in the developme nt set using a simple dictionary-based maximum matching algorithm to obt ain a single best segmentation. 17 Finally, in the decoding stage, we use the same segmenta-tion algorithm to obtain the single best segmentation on the test set, and word lattices can also be generated using the bilingual dictiona ry. The various pa-rameters of the method ( k , t COOC , t AC , cf. 4.2) were optimized on the devel-opment set. One iteration of character grouping on the NIST t ask was found to be enough; the optimal set of values was found to be k = 3, t AC = 0 . 0 and t COOC = 0, meaning that all the entries in the bilingually dictiona ry are kept. On the IWSLT data, we found that two iterations of character g rouping were needed: the optimal set of values was found to be k = 3, t AC = 0 . 3, t COOC = 8 for the first iteration, and t AC = 0 . 2, t COOC = 15 for the second. statistically significantly ( p &lt; 0 . 03) better results than character-based seg-mentation when enhanced with word lattice decoding. 18 Compared to the best in-domain segmenter, namely the Stanford segmenter on this particular task, our approach is inferior according to Bleu and NIST. We first a ttribute this to the small amount of training data, from which we are unable to obtain a high-quality bilingual dictionary due to data sparseness p roblems. We also at-tribute this to the vast amount of named entity terms in the te st sets, which is extremely difficult for our approach. 19 We expect to see better results when a larger amount of data is used and the segmenter is enhanced w ith a named entity recognizer.
 character-based segmentation are both statistically sign ificant ( p &lt; 0 . 03 for IWSLT 2006 test set and p &lt; 0 . 01 for IWSLT 2007 test set respectively). Compared to the best in-domain segmenter, the LDC segmenter , our approach yielded a consistently good performance on both translatio n tasks. Moreover, the good performance is confirmed by all three evaluation mea sures. nism into the PB-SMT system trained on monolingually segmen ted data does not help or even harm the system due to the mismatch between PB -SMT train-ing and decoding. Previous research also shows that combini ng phrase ta-bles using different segmentations is necessary for word la ttice decoding [Dyer et al. 2008]. This is also the advantage of our bilingually mo tivated segmenta-tion, which facilitates word lattice decoding because it ca n generate different segmentations for the same Chinese sentence given differen t target English translations. putationally intensive. However, this can easily be parall elized. From our experiments, we observed that the translation results are v ery sensitive to the parameters and this search process is essential to achie ve good results. Figure 5 shows the search graph on the IWSLT data set in the firs t iteration step. From this graph, we can see that filtering of the bilingu al dictionary is essential in order to achieve better performance. has to overcome another challenge in order to produce compet itive results, that is, data sparseness. Given that our segmentation is based on bilingual dictio-naries, the segmentation process can significantly increas e the size of the vo-cabulary, which could potentially lead to a data sparseness problem when the size of the training data is small. Tables XIV and XV list the s tatistics of the Chinese side of the training data, including the total vocab ulary (Voc), num-ber of character vocabulary (Char. voc) in Voc, and the runni ng words (Run. words) when different word segmentations were used. From Ta ble XIV, we can see that our approach suffered from data sparseness on th e NIST task, that is, a large vocabulary was generated, of which a conside rable amount of characters still remain as separate words (15.48%). On the I WSLT task, since the dictionary generation process is more conservative, we maintained a rea-sonable vocabulary size, which contributed to the final good performance. small training corpus containing roughly 40,000 sentence p airs. We are par-ticularly interested in the performance of our segmentatio n approach when it is scaled up to larger amounts of data. Given that the optim ization of the bilingual dictionary is computationally intensive, it is i mpractical to directly extract candidate words and estimate their reliability. As an alternative, we can use the obtained bilingual dictionary optimized on the s mall corpus to per-form segmentation on the larger corpus. We expect competiti ve results when the small corpus is a representative sample of the larger cor pus and large enough to produce reliable bilingual dictionaries without suffering severely from data sparseness.
 tent results on both the IWSLT 2006 and 2007 test sets. On the N IST task (cf. Table XVII), our approach outperforms the basic charac ter-based segmen-tation; however, it is still inferior compared to the other i n-domain monolin-gual segmenters due to the low quality of the bilingual dicti onary induced (cf. Section 6.2.1). Giza++ to perform word alignment. We next show that our appro ach is not dependent on the word aligner given that we have a conservati ve reliability estimation procedure. Table XVIII shows the results obtain ed on the IWSLT data set using the MTTK alignment tool [Deng and Byrne 2005, 2 006]. 7. RELATED WORK Fertility-based models such as IBM Models 3, 4, and 5 allow fo r alignments between one word and several words in order to capture the per vasive 1-to-n correspondences. They can be seen as extensions of the simpl er IBM models 1 and 2 [Brown et al. 1993]. Similarly, Deng and Byrne [2005] pr oposed an HMM framework with special attention to dealing with 1-to-n alignment, which is an extension of the original model of Vogel et al. [1996]. Howev er, as mentioned above, these models rarely question the monolingual tokeni zation, that is, the basic unit of the alignment process is the word. One alternat ive to extending the expressivity of one model (and usually its complexity) i s to focus on the input representation ; in particular, we argue that the alignment process can benefit from a simplification of the input, which consists of t rying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentione d in Tiedemann [2003], and related issues are reported in Wu [1997].
 SMT and showed that the segmentation proposed by word alignm ents can be used in PB-SMT to achieve competitive results compared to us ing monolin-gual segmenters. However, Xu et al. [2004] used word aligner s to reconstruct a (monolingual) Chinese dictionary and reuse this dictiona ry to segment Chi-nese sentences as other monolingual segmenters do. Our appr oach features the use of a bilingual dictionary and conducts segmentation based on the bilin-gual dictionary. In addition, we add a process which optimiz es the bilingual dictionary according to translation quality. Ma et al. [200 7] proposed an ap-proach to improve word alignment by optimizing the segmenta tion of both source and target languages. However, the reported experim ents are based on a poor phrase-based SMT baseline and the issue of scalabilit y is not addressed. in PB-SMT, in order to address the problems that segmentatio n posed on the decoding. Dyer et al. [2008] extended this approach to hiera rchical SMT sys-tems and other language pairs. However, both of these method s require some monolingual segmentation in order to generate word lattice s. Our approach facilitates word lattice generation given that our segment ation is driven by the bilingual dictionary, making the training and decoding processes more coherent. More recently, Xu et al. [2008] proposed a Bayesia n semi-supervised model for word segmentation by combining knowledge from bot h monolingual segmentation and bilingual word alignment. Our approach is not specifically designed for segmentation; it is a new mechanism that can aut omatically per-form bidirectional segmentation optimization. The bootst rapping step can help the statistical aligners overcome the limitations posed by the first-order as-sumption. The bidirectional word packing can also overcome the shortcomings of the 1-to-n assumption inherent in IBM models by facilitating m -to-n align-ment structures (cf. Fraser and Marcu [2007]). On the other h and, our ap-proach can be generalized to perform word segmentation with out relying on any monolingual resources, such as dictionaries and the lik e. 8. CONCLUSIONS AND FUTURE WORK In this article, we have introduced a simple yet effective me thod for packing words together in order to give a different and simplified inp ut to automatic word aligners. We use a bootstrapping approach in which we fir st extract 1-to-n word alignments using an existing word aligner, and then est imate the confidence of those alignments to decide whether or not the n words have to be grouped; if so, this group is considered a new basic unit to co nsider. We can finally reapply the word aligner on the updated sentences. Th is approach can be used for bootstrapping word alignments based on any monol ingual word segmentation; it can also be used for direct word segmentati on without relying on any monolingual segmenters.
 ence of this process on the Chinese X  X nglish MT task based on t he IWSLT 2007 evaluation campaign with a reasonally small amount of train ing data. We re-port a 1 . 84 points absolute (5 . 44% relative) increase in Bleu score over a stan-dard phrase-based SMT system. We have verified that this proc ess actually reduces the number of 1-to-n alignments with n 6 = 1, and that it is inde-pendent of the (Chinese) segmentation strategy. We then gen eralize our ap-proach for direct Chinese word segmentation without relyin g on monolingual word segmenters and demonstrate that (1) our approach is not as sensitive to the domain as monolingual segmenters, and (2) the SMT syst em using our word segmentation can achieve state-of-the-art performan ce. Moreover, our approach can be scaled up to larger data sets and achieves com petitive results if the small data used is a representative sample of the large r one. Since our approach does not rely on monolingual segmenters, it is part icularly useful for languages which lack manually segmented resources in the co ntext of SMT. proach is built on existing fertility-based word alignment models which are computationally expensive in the process of parameter sear ch. Despite the fact that it is very effective when used on a relatively small data set, using a small data set tends to suffer from data sparseness problems especially when a large vocabulary exists. 20 Therefore, a successful scaling up of our method has to meet two constraints: (1) the data has a relatively sma ll vocabulary so that a high-quality bilingual dictionary can be obtained; a nd (2) the small data set is representative enough for the larger data set. Given s uch limitations, in future work, we firstly intend to explore the correlation b etween vocabulary size and the amount of training data needed in order to achiev e good results. We also plan to use more sophisticated association measures to estimate the reliability of the derived 1-to-n alignments. Then we will take a further step to derive 1-to-n alignments using syntactic information and heuristics [Me lamed 1997] rather than existing IBM models and integrate such inf ormation into probabilistic word aligners. By doing so, we can overcome bo th limitations. We also plan to conduct experiments using larger data sets an d more domains, including newswire and controlled technical corpus for loc alization purposes. We would like to thank the reviewers for their critical and in sightful comments.

