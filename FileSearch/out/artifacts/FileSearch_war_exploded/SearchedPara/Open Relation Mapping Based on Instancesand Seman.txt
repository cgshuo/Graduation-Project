 In open information extraction, the expressions of a specific semantic relation between two entities are various. For example, we use  X  X  was born in Y X ,  X  X  X  X  hometown Y X  or  X  X  X  X  birth in Y X  to express a semantic re lation  X  X irthPlace X , where X and Y are two named entities. These expressions, like  X  X as born in X ,  X   X  X  hometown X  and  X  X  X  birth in X  are called relation phrases [1,2,3] or relati onal patterns [1] (we call them open relations or relation mentions in this paper). We notice that there are textual mismatches between relation mentions and the corresponding semantic, which is a big obstacle for semantic discovery in many applications, like natrual language understanding, ontology-based question answering etc. For example, when we ask  X  X here is the hometown of Yao Ming? X , question answering system can give the right answer if it knows the target se-mantic relation is  X  X irthPlace X , then the syste m will match the attribute value of  X  X irth-Place X  for  X  X ao Ming X  from knowledge base . However, if the relation words in ques-tion is aforementioned  X  X ometown X , and the attribute name stored in knowledge base is  X  X irthPlace X , it X  X  difficult to obtain the corr ect answers since that they are mismatched on the surface forms (detailedly illustrated in Tab.1). Therefore, constructing the map-ping between relaion mentions and the semantic relation names stored in KB (shortly named as Open Relation Mapping) is very important and meaningful, which is the focus of this paper.

Formally, the source of open relation mapping is the relation mentions in free texts, and the target is the relations in knowledge base, which are usually human edited at-tributes (we call it attribute relation later in this paper). Intuitively, a relation mention and an attribute can be directly linked if they share the common entity pairs (which are called relation instances). Unfortunate ly, this assumption is not always correct. As illustrated in Fig.1, Yao Ming, fly back to, Shanghai is mapped to  X  X irthPlace X  ac-cording to the common instance ( Yao Ming, Shanghai in Fig.1) assumption, which is obviously incorrect.
Therefore, beside the relation instan ces, we must further consider the semantic similarity between relation mentions and relation names (attributes) in KB. For ex-ample, if we know that  X  X ometown X  has the similar semantic with  X  X irthplace X , it X  X  easily to construct a mapping between them. However, simply computing this simi-larity based on surface texts, like edit distance, is insufficient. Tab.1 lists some exam-ples of relations mappings, majority of which are mismatched by simply using surface textual. For example,  X  X eceived degree in X  has not any common words with attribute  X  X ago:graduatedFrom X , but they actually should be mapped. Thus, we must mine the deeper semantics behind the texts.

For this aim, we propose a novel method for open relation mapping which considers both of the semantics of relation mentions and relation instances. Our method mainly contains two steps. First, we use relation instances to generate mapping candidates. Second, we compute the semantic similarity between each candidates and the target relation mention. The candidate with similarity score above a threshold will be linked to the relation mention. In specific, we explore external resources of knowledge, like Wikipedia, to mine concepts for the semantic representation. Then, the semantic sim-ilarity of relation mention and attribute ca ndidate is computed on this semantic space. We further make semantic expansion for each attribute to improve the mapping perfor-mance by using WordNet synset. The behind reason is that directly mining the similarity between attribute candidates and relation m entions may miss much useful information. To demonstrate the effectiveness of the proposed method, we use YAGO [4] as knowl-edge base, PATTY [1] as open relation dataset. Experimental results on five kinds of relations show that our method can achieve 74. 4% average accuracy for open relation mapping, and the proposed semantic expansion method can effectively improve the mapping performance. In the early days, relation extraction (RE) is r elation-driven. Targe t relations are prede-fined according to task, then in stances are constructed for every target relation to gain relation patterns, and then relation patterns are used to collect more instances which share the same relation. The number of target relations in relation-driven extraction is limited and predefined. With the invention of open information extraction, RE be-came data-driven. Data-driven RE extracts r elation triples from free texts whenever two entities are detected associated by a relation by a trained classifier. The number of relations in data-driven RE is unbounded and the target relations are not known in ad-vance. The data-driven extraction is regarde d as  X  X pen Information Extraction X (Open IE) [1,5,6,7,8]. As the relation is not known , additional mapping processing is required to mine the semantics of relation expressions.

Semi-supervised mapping methods like active learning are used to match relation phrases or mentions to domain-specific relations, like NFL-scoring [9] or nutrition do-main [10]. Active learning method still requires some human labeling. Later distant supervision [11,12,13] is proposed, and the target domain of mapping is extended to more general domains covered by knowledge base such as YAGO [4], DBpedia [14] etc. Then knowledge-base-supervised method is exploited to map open relations. Taka-matsu etc. [12] and Surdeanu etc. [13] present a generative model to model the labeling process of distanct supervisio n according to statistical featur e of instances. Semantics is the key to avoid some wrong mapping, like coincidental match where mapping open re-lation  X  X lied back to X  to attribute  X  X asBornIn X  in light of the provenance of  X  X ao Ming flied back to Shanghai X .

Research about open relations has attracted increasing attentions, as open relation getting more important. Yates and Etzioni [2], Kok and Domingos [15] cluster rela-tion phrases and entities at the same time, further Min et al. [3] loose the constraint that each entity or relation phrase belongs to one cluster to handle pol ysemy relations. But the above work group synonymy relation phrases into clusters without explicit se-mantics of each cluster. PATTY [1] construc ts a WordNet-style hierarchical taxonomy for binary relation patterns (phrases). It also paraphrases canonical relations in DBpe-dia and YAGO knowledge bases with relation p atterns according to common instances assumption. They did not validate the corr ectness of the semantics of paraphrases, there are too many noisy mappings and cannot be used directly in applications. For example,  X  X ied just X  X  X uried in X  X  X [con]] graduated in X  are regarded as paraphrase for  X  X ago:wasBornIn X .
 In this section, we describe our method in detail. The main framework is illustrated in Fig. 2.
There are two parts of input data. One is relational triples stored in knowledge base taking the form of attr, ent1, ent2 .where attr is the attribute relationship holds between the first entity ent1 and the second entity ent2 .Suchas yago:wasBornIn, Zhang Yimou, Xi X  X n etc. The other is relation triples extr acted from free text taking the form of arg1, open, arg2 . open is an open relation expressing the rel ation between the first argument arg1 and the second argument arg2 .Suchas Zhang Yimou, was born in, Xi X  X n etc. The output is the explicit semantics of open relations which is called mapping pair. For example, mapping pair was born at, yago:wasBornIn means  X  X as born at X  ex-presses the semantics of  X  X a go:wasBornIn X ; mapping pair returned to, null means  X  X eturned to X  expresses no semantics defined in knowledge base.

It takes two steps to implement our mapping algorithm: candidate collection and semantics filtering.
 Candidate Collection. There are thousands of attributes recorded in knowledge base, YAGO[4] has 72 1 attributes , DBpedia[14] has 48,293 2 . It is time consuming, and also unnecessary to compare every attribute w ith open relations to eliminate unrelated candidate attributes. Hence, we use common instances assumption to find candidate at-tributes as the semantics of every open relation. This process was shown at the upper part in Fig. 2. From the fact that entity pair Ruth Gruber, New York City is shared by open relation  X  lives in X  (Row 5 in open extracted triples in Fig. 2) and two attributes:  X  X ago:wasBornIn X  and  X  X ago:livesIn X  (Row 4 and 5 in knowledge base triples in Fig. 2), we infer that  X  X ago:wasBornIn X  and  X  X ago:livesIn X  are candidate attributes for open relation  X  X ives in X . We regard attributes which share common instances with open re-lations as candidate semantics. This assumption may lead to coincidental match error, therefore next we mine semantics similarity to filter out irrelevant attributes. Semantics Filtering. Statistical features alone, that i s the co-occurrence frequency of open relations and attributes may lead to mapping error. In Fig. 2  X  X eturned to X  and  X  X ago:wasBornIn X  have a higher co-occurrence frequency (0.109) than  X  X as born at X  and  X  X ago:wasBornIn X  (0.009), but  X  X as born at X  has much more closer meaning with  X  X ago:wasBornIn X  than  X  X eturned to X . We filter out unrelated attributes to mitigate this kind of mapping problem by computing semantic similarity between relation mention and its candidates.

After explaining the function of the two steps in our mapping algorithm above, we will give the details of the implementation. 3.1 Candidate Collection Generating candidate attribute is based on common instances assumption, which re-quires matching the aforementioned entity1 and entity2 in the relation triple simultane-ously. It takes two phrases to collect attribut e candidates for every open relation. First is to put binary relations into database, and make index to facilitate subsequent search-ing. Second is online searching and matchi ng, which read each open extracted relation triples, then search entity1 and entity2 in prepared database. If finding attrib-ute relation triples that have arguments are also entity 1 and entity2, we should then save returned attributes as candidate semantics of open relation.

After the above mapping, a series of mapping pairs open relation, candidate attribute are collected. As every mapping pair shares at least one common entity pair, we can make a statistics about the number of common instances for every mapping pair and compute the confidence as Eq.1.
Where, N attr is the occurrence times of attribute attr in mapping set. N open  X  attr is the occurrence times of mapping pair open, attr in mapping set. As shown in Fig. 2, the mapping pair was born in, yago:wasBornIn in first step has a confidence of 0.755, meaning that the proba bility is 0.755 when an entity pair associated by semantic relation  X  X ago:wasBornIn X  is expressed by  X  X as born in X . 3.2 Semantics Filtering In the second step, we compute the semantic similarity between candidate attributes re-trieved in the first step and the extracted relation mentions to make filtering. We exploit empirical threshold to make decisions on whether an open relation can be mapped to an attribute. If similarity value is above a threshold, two strings have mapping relation. Otherwise, they do not. This strategy can map one open relation to multiple semantic relations and map unrelated open relations to null.

We notice open in open relation triples and attr in relational database are both to-kens. For example, relation in knowledge bases is defined like  X  X AGO:wasBornIn X , relation phrases openly extracted are like  X   X  X  hometown of X  etc. Therefore token-based strings similarity method is required to accu mulate the similarity between every two to-kens from attribute and relation mention respectively. In our observation, when phrases contains head words of relation, like phrase  X  X  birth in X  containing common head word  X  X irth X  of  X  X irthPlace X , we have very great confidence that  X  X  birth in X  indicating re-lation  X  X irthPlace X . According to this feat ure, we prefer Generalized Mongue-Elkan (GME) proposed by Jimenez et al. [16] as our external similarity, which is computed as follows:
Where, A stands for attribute, O for open relation. When m &gt; 2, GME gives greater importance to those pairs of tokens [ a i , o j ] that are more similar, which is the exact feature we need. In our experiment later, m is set to 2. Then the next problem becomes choose internal similarity sim to meet our mapping requirement which is to capture semantic similarity and immunize to expressions variations.
 Semantics Filtering Based on Text Surface. Edit-based string similarity method (like Levenshtein Distance [17] etc.) finds the similarity from textual surface forms in which words share common substrings, like  X  X ago:wasBornIn X  and  X  X as born in X  in Tab.1-1. It cannot handle different expressions to the same semantics, like  X  X  X  hometown X  expresses the meaning of  X  X ago:wasBornIn X  as shown in Tab.1-2. This method is not fit to open relation mapping.
 Semantics Filtering by Using Semantics. Alternately, we employ some lexical re-sources like experts generated WordNet [18], crowdsources generated Wikipedia etc., which expand the extensional meaning of words to mine implicature. As WordNet la-bels the semantics relations among words, WordNet-based method could find some morphological changes, such as  X  X orn X  is synonymous to  X  X irth X  (Tab.1-3). It is unable to compute the similarity between words with different part-of-the-speech (POS). The similarity score of  X  X reated X  and  X  X reator  X  (example shown in Tab1-4) computed by Jiang&amp;Conarth [19] or Lin [20] methods which exploit the taxonomy of WordNet is all zero. Wikipedia-based methods (WikiMiner [21], WikiRelate! [22], Explicit Semantics Analysis (ESA) [23] etc.) taking advantage of the vast amounts of highly organized knowledge encoded in Wikipedia could min e the similarity between words accord-ing to their linkage, hierarchy or co-occurrence information in Wikipedia. However, WikiMiner [21] compares proper nouns, like person or organization name, which share common inner-linked pages. WikiRelate! [22] compares words that occur in titles of Wikipedia articles. ESA [23] compares words appeared within the text of Wikipedia articles. For our task, ESA is much more reasonable methods, as open relations are tex-tual form of attributes. However, the text relatedness computed by ESA is not specified for open relation mapping. In the Wikiped ia Concept space,  X  X raduated from X  is more related to  X  X egree X  than  X  X eceived degree in X , as  X  X eceived X  introduced a dilution. Semantics Filtering by Using Semantic Expansion. As existing string similarity methods, like edit/character-based, WordNet -based and Wikipedia-based methods, have their limits in open relation mapping. Therefore, we proposed a semantic similarity computation method combining two lexical resources WordNet and Wikipedia under the strategy of GME [16]. In which, WordNet is used to expand the meaning of at-tributes words to adapt to various changes of open relations. Wikipedia is used to con-struct a semantic space which is defined as Wikipedia Concepts. Then the similarity of attribute candidates and relation mentions are computed under that semantic space by ESA tool. ESA stems [24] all substantivals in Wikipedia texts, and then maps them to Wikipedia Concepts. Take  X  X cted X  and  X  X layed role in films X  as an example to illus-trate the procedure of our proposed se-mantic similarity computation method (shown in Fig.3). Theprocedureisasfollows:
First, tokenize attribute, delete stop words, lemmatize remain words, expand lemma-tized words using WordNet synset, construct a string set by adding attribute to Word-Net synset. As shown in top left of Fig.3, the yago attribute  X  X ctedIn X  is lemmatized by WordNet to be  X  X ct X . Then a synset of the lemmetized word is collected from WordNet as acted in, play, behave, move, act as, rep resent. Meanwhile, tokenize the compared open relation, and regard tokenized words as a string set. As we can see from left center of Fig.3, open relation  X  X layed role in films X  is tokenized to be played, role, in, films.
Second, compute the word similarity by ESA for every pair word from the two string set, then get a matrix of similarity value. We iteratively pick one word from expanded attribute string set, compare it with every word in tokenized open relation string set using ESA. For example, ESA first stems  X  X  layed X  to  X  X lai X ,  X  X cted in X  to  X  X ct X , and retrieve the Wikipedia Con cept vector corresponding to  X  X lai X  and  X  X ct X  respectively. Then compute the cosine similarity between the retrieved vector to ob-tain the semantic relatedness of  X  X layed X  and  X  X cted in X . Re peat the above process to get a matrix of stem-similarity as shown is lower left.

Third, put the value in matrix into GME [16] (Eq.3), then get a similarity of the two relations. Finally, store the mapping from open relation to attribute when the similarity of the two is larger than the threshold. In our example, substitute the value in matrix into Eq.2, we get the similar ity value between  X  X layed rol e in films X  and  X  X ctedIn X  is 0.45. We use PATTY 3 as open relation triples input and attribute relation triples in YAGO2 4 as static relation input. The open relations of PATTY are extracted from the New York Times archive (NYT) which includes about 1,800,000 newspaper articles from the years 1987 to 2007. YAGO contains 72 attributes, 10 million entities and 120 million relation facts. The Wikipedia (WKP) data used to train ESA 5 is the English version, which con-tains about 3,800,000 articles (as of August 3, 2011). All relational triples are stored in a MySQL database. We evaluated mapping quality along five representative attributes: actedIn (who appeared in which show), cr eated(who created which novel thing), gradu-atedFrom(who graduated from which sc hool), hasAcademicAdvisor(who got academic guidance from who), wasBornIn(who was born in where).  X  X asBornIn X  is the most noisy relation type.  X  X reated X  has the most tra nsformations of lexicon.  X  X asAcademi-cAdvisor X  is a much ambiguous and noisy re lation.  X  X ctedIn X  has an acceptable quality only with instance mapping.  X  X raduatedFrom X  does not have many changes in surface form without much ambiguity.

To assess the quality of relations mapping, we ranked the open relations mapped to the above mentioned attributes and evaluated the precision of the top 100. Human judg-ment stated whether an open relation indicat ed the semantics of its mapped attribute. If so, the mapping pair was labeled as 1. If not, labeled 0. If not sure labeled 0.5.
To compare the effect of different semantics mining methods, we used the results of Candidate Collection as baseline, adding edit-based analysis, WordNet 6 lexical re-source, Wikipedia resource and our proposed method respectively. 1. Baseline: This method ranks the mapping resu lts according the confidence com-2. Edit-based Semantics Mining: Add edit-based string similarity filtering method 3. WordNet-based Semantics Mining: Add WordNet-based string similarity filter-4. Wikipedia-based Semantics Mining: Add Wikipedia-based string similarity fil-5. Our Combining method: Our method computes the similarity of two strings by
As shown in Tab. 2, we can see that for  X  X ctedIn X ,  X  X asAcademicAdvisor X  these two attributes with various lexical changes in their open relations, adding edit-based se-mantics mining method decreases the perform ance with lower precision than baseline. However, adding resources boosts the performance to reach higher precision than base-line, in which our combining method is better than adding single resource methods. For attribute  X  X raduatedFrom X  and  X  X reated X  , there are no much lexical changes in their open expressions, meanwhile, headwords of attributes , like  X  X raduated X  for  X  X raduat-edFrom X ,  X  X reated X  for attribute  X  X reated X , strongly indicate the semantics of attributes. For these kinds of attributes, adding edit-based methods can significantly improve the relation performance mapping precision. Resource-based methods are not as good as edit-based method under top100 precision assessment, but still achieve better quality than baseline for these attributes. For noisy attribute like  X  X asBornIn X , the precision of mapping with only instances is quite low. When adding different semantics mining resource, the quality will get improvement by different degrees.

From the average performance (Col. 7 in Tab. 2), we can see that relation mapping adding semantics is better than mapping with only instances. Resource-based seman-tics mining method is better than edit-based method. WordNet-based semantics mining method has a comparable quality with Wikipe dia-based method. Ou r combining method outperforms the other seman tic mining methods. The aver age accuracy of our method for open relation mapping achieves 74.4% which is 20% higher than the baseline.
To further evaluate the effectiveness of our proposed method, we compare the top 5 results between PATTY and our method for YAGO attribute  X  X asBornIn X ,  X  X reated X ,  X  X ctedIn X  shown in Tab. 3. From this table, we can see that the semantics filtering strat-egy of our method can better reduce the coincidental matching error.
 In order to reduce noisy mappings, we propos e a novel open relation mapping method, which combines the instances and semantics, to mine the explicit semantics of open relations, such as hand-editing attribute names in knowledge base. We mine semantics in two steps. In the first step, we use instance information to collect candidate attributes. And in the second step, we filter out the unrelated attributes using our novel semantic similarity method which takes advantage o f WordNet and Wikipedia resources. Exper-imental results show that our method can achieve 74.4% average accuracy for open relation mapping and the proposed semantic expansion method can effectively improve the mapping performance.

Our future works include two aspects: 1. Handling redirection of entities. There may be more than one spelling of a cer-2. Trying to find other methods to combine various features. Our present method con-Acknowledgments. This work was supported by the National Natural Science Foun-dation of China (No. 61070106, No. 61272332 and No. 61202329), the National High Technology Development 863 Program of China (No. 2012AA011102), the National Basic Research Program of China (No. 2012CB316300) and the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research (ICDD2 01201).

