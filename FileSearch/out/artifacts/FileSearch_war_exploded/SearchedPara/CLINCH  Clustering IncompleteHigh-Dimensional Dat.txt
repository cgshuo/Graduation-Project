 The clustering is a primary technique in discovering hidden patterns from mas-sive datasets. Common applications of cluster analysis involve scientific data ex-ploration, information retrieval and text mining, spatial database applications, Web analysis, marketing, computation biology, and many others. [14] led to deal with ultra large databases in data mining, which record tremendous, high-dimensional and transactional information. In tandem with this trend, con-cerns about informational privacy in data mining have emerged globally, for Data mining, with its promise to efficiently discover valuable, non-obvious informa-tion from large databases, is particularly vulnerable to misuse [5]. Specifically, a person: 1) may not divulge at all the values of certain fields; 2) may not mind giving true values of certain fields; 3) may be willing to give not true values but modified values of certain fields [5]. All these situations will lead to the creation of missing attributes, i.e. privacy concerns.
 with appropriate values [3, 8, 16]. Given a point X i with missing value in j th dimension, a general approach is to find the K -Nearest Neighbors (KNN) of X i and replace the missing attribute X ij with the average of the j th values of its KNN [16]. However, when application is extended to deal with high-dimensional data, this method becomes unreliable in that high-dimensional data are sparse-prone, which makes the KNN search on it meaningless [14, 18].
 few. Moreover, these algorithms are mostly mathematic-prone. Viewing from the perspective of data mining application, little work has started. Mining on the incomplete data would be a more and more popular issue.
 dimensional data. We employ the three-step framework, just the same as most grid-based algorithm [10, 1, 9, 7], in our clustering algorithm. The problem is mainly solved by identifying the dense units. Our approach identifies the units by processing dimension by dimension.
 after several dimensions, therefore the total cost is cut down; II) we propose an innovative prediction mechanism to solve the problem of incomplete data. From experiments and complexity analysis, we can see that our approach outperforms many existing approaches while maintaining a reasonable precision.
 the clustering problem. In Section 3, Algorithm CLIQUE is introduced simply. Algorithm CLINCH is developed in Section 4. In Section 5, we evaluate the per-formance on synthetic and real datasets via comparing CLINCH with CLIQUE and FilV-DBScan . Related work is presented in Section 6. Section 7 concludes the paper. Definition 1 (DataSpace) by-n data matrix D : D = { normalized incomplete data space D , d ij could either be in [0 , 1] or  X  X ncertain X  . The  X  X ncertain X  indicates the missing of this attribute at this point. Definition 2 (Dimension) Dim j as the j th dimension of a data space. Apparently, a dimension is the set of j th attribute of all the points. Definition 3 (Incompleteness) uncertain. For example, { 0 . 25 , uncertain, 0 . 15 , uncertain, 0 . 20 } , which means that record values in 2 nd and 4 t h dimensions. Accor dingly, the completeness  X  j of j t h the dimension follows: Generally speaking, if MAX (  X  1 , X  2 , ...,  X  m )  X  MININCOMPLETE , the dataset is complete. Here, MININCOMPLETE is a threshold specified by users and can be used to obtain an algorithmic gain.
 Definition 4 (Unit) interval on the i th dimension. A unit is a dense unit if the points in this unit exceed a given support . Namely, we say a unit is dense under support of . Definition 5 (Cluster) points are in the same cluster if the units they belong to are connected or there exist a set of dense units that are each other connected.
 Problem Statement: Given a set of data points D , desired number of intervals  X  on each dimension and support , the problem is how to predict which units the points with missing value belong to and generate clusters. In [1], CLIQUE , an automatic subspace clustering algorithm for high-dimensional data, was proposed. CLIQUE consists of mainly three steps: 1) searches for the dense units; 2) clusters are generated by grouping the connected dense units; 3) concise descriptions for each cluster are produced based on minimal cluster descriptions MDL .
 unit generation. Accordingly, the step1 takes up most of the time spent in discov-ering clusters. Remember that in CLIQUE , k -dimensional dense unit candidates are generated by self-joining all the k  X  1-dimensional dense units. However, given any several dimensions of the whole high-dimensional dataspace, there would be over-numbered points in every unit. This produces too many dense units in the first several dimensions in CLIQUE . Although most of these units will be pruned as the dimension goes high, to self-join on these large amount of units in the first several dimensions, however, would square the overall runtime. This deteriorates CLIQUE  X  X  performance on high-dimensional data.
 space D , CLIQUE seeks the k -dimensional dense units level by level. Thus while generating all the 3-dimensional dense units, CILQUE self-joins the dense units in all 2-dimensional subspaces. Now let X  X  estimate how large a number of dense units there will be if D is a high-dimensional database: in all, there are C 2 k = k  X  ( k  X  1) 2-dimensional subspaces. On every subspace, there will be ( I )  X  ( I ) units, where ( I ) is the number of intervals on each dimension. Note when D are projected to any 2-dimensional subspaces, there will be over-numbered points in every units. This probably produces as many as k  X  ( k  X  1)  X  ( I )  X  ( I )2-dimensional dense units in all. Moreover, when such large number of units are self-joined, the cost time is squared. In this section, we introduce our approach CLINCH (i.e. CL ustering INC omplete H igh-dimensional data). We employ the framework like most grid-based clustering algorithms. Figure 1 illustrates the steps of algorithm CLINCH . the units with points more than given support are marked as the dense units. clusters. Two dense units are named to be  X  X onnected X  if they share a common face [1]. Given a graph G , let every dense unit be a vertex in G . There is an edge between two dense units if and only if the two corresponding dense units are  X  X onnected X . Then the problem of step 2 is equivalent to searching for the connected components in a graph. In implementation, we use depth-first search algorithm [2] in discovering these components. Since the problem is projected into the search of the graph, the clustering result will not be affected by the order of the records, ie. CLINCH is stable.
 phrase  X  X imension by dimension X  means that 1) firstly dense units in K-1-d space are produced and then 2) dense units on K-d space are produced by combining the mining results on K-1-d and dense intervals on the Kth dimension. At first, we sort all dimensions. Then we search for the dense units in the first several complete dimensions. Suppose there are  X  complete dimensions in all, we produce  X   X  dimensional dense units and pass them to the following step. Then the rest dimensions are processed with the received  X   X  dimensional dense units. At last a set of dense units will be generated in whole dimensions. Figure 2 shows the procedure for generating dense units. 4.1 Determine the Order of Dimension for Process Intuitively, we firstly deal with the complete dimensions and then the incomplete ones. We handle the complete dimensions in the decreasing order of their en-tropies [7]. The entropy of a dimension is defined in [7, 10]. Since lager entropies mean more information, if we deal with dimensions with larger entropies, the prediction on the missing attributes will be more precise. After finishing all the complete dimensions, characteristics of clusters on this complete subspace are built through entropies, and will be employed for the prediction of missing at-tributes through the following incomplete dimensions. This process is similar to building a decision-tree for the first several dimensions, which would be used in later prediction [5]. Intuitively, we optimize the grouping of points for predic-tion if we follow the decreasing entropies. Similar idea has already been widely accepted in the study of classification algorithms [7].
 plete ones are processed. For these incomplete dimensions, we handle them ac-cording to their completeness  X  j . This is to guarantee that dimensions with more uncertain records are processed later, which maximizes the information used for prediction.
 units. 4.2 Recognition on Full Dimensions We benefit from the monotonicity introduced in [1]. If a unit is dense in its k -dimensional space, then it will be also dense in any of its k  X  1-dimensional space [1]. With this feature, we could prune many units in the first several dimensions if we recognize the dense units dimension by dimension. Our process is illustrated in Figure 3: let A be the first dimension and B be the second. Figure 4 shows the pseudo-code of our approach.
 dimensional dense unit U i and C i  X  1 . Thus the time cost of this step is bounded by the number of element in each dimension. Given every k  X  dimensional dense unit is also dense in any of its k  X  1 subspace, our k  X  1 dense unit list C k  X  1 contains the dense units in k  X  dimensional . Then by also checking the ones on the i t h dimension, we further limit the number of candidates for following prune. Line 7 then check on the subspace to maintain the truly dense one, whose implementation is straightforward.
 CLIQUE, the join of CLINCH only generate as many as k  X  ( k  X  1)  X  ( I max )  X  ( I max ) dense units. Here, I max = MAX ( dense unites of the i th dimension. Since I max  X  I , it is clear that the join of CLINCH can get less cost than the self-join of CLIQUE. Furthermore, the em-ployment of the dimension-by-dimension processing in the dense unit recognition not only ease the efficiency in the self-join step in CLIQUE but also provides the possibility to cluster incomplete data without filling the value beforehand. 4.3 Recognition on Incomplete Dimensions After all the complete dimensions are processed, we pass the dense units to the step 3. Our idea is to employ the information from the processed several dimen-sions to predict those missing values. Based on the dimension  X  by  X  dimension approach, we introduce a decision-tree like mechanism to enable predication for incomplete points. An example of such prediction is illustrated in Figure 5. Given positions are illustrated as follow: 4.4 Time Complexity The time complexity of Algorithm CLINCH mainly consists of three parts: 1. Sort the dimensions by their entropy. 2. Generate 1  X  dimensional dense unit. 3. Generate k  X  dimensional units dimension by dimension.
 the other hand, the time complexity of part 3 is bounded by O ( denotes the time in dealing with n points on i th dimension. When it is complete in dimension i , time to handle n points is O ( n ), otherwise the time depends on the completeness  X  i . Formally, for neighbor.
 O ( k  X  n ). This much outperforms the exponential time of CILQUE. In this section, we evaluate the performance of CLINCH . Criterion in estimating the performance of CLINCH includes its efficiency and quality of clustering results. In efficiency, we record the total CPU time for comparison with CLIQUE and the way introduced in [16] followed by DBSCAN . The efficiency was tested on both synthetic and real data. The synthetic data generator was introduced in [22]. The real dataset is from the completely specified database MUSK2 from UCI 2 machine learning repository. We evaluate the precision in the way introduced in [21], which would be covered in details later. We implemented all the algorithms in C++. The experiments have been run on Pentium IV with 2.2 GHz containing 512MB DDR of main memory and Windows XP as operating system. 5.1 Efficiency We use real data as well as synthetic data in the experiments. The synthetic data generator is introduced in [22]. We could set the number and size of clusters, size and dimensionality of dataset etc. To test the performance of CLINCH on different size of real data, we extract a series of subsets of MUSK2 . Experiments on the efficiency of CLINCH include its sensitivity to the dimensionality and size of dataset.
 database in different sizes. We could see from the figure 7 that CLINCH scales well with the increase of database size. Besides, CLINCH  X  X  scalability with di-mension was also investigated. We tested three algorithms on datasets with 10000 points. The effects of dimensions are shown in figure 8.
 machine learning repository MUSK2 from UCI . Scalability with dimension and database size is tested by retrieving either the subspaces of dimensions and subsets of data. Figure 9 and 10 show performances of CLINCH on different database sizes and dimensionality. 5.2 Precision In this section we would study what precision the CLINCH maintains and could see that CLINCH produces the same clustering results if both performed on complete data.
 experiments. We generate the incomplete data space by randomly removing some attributes of points.
 proach introduced in [16], which is simple and easy to implement. In assessing the quality of CLINCH , we benefit from the method introduced in [21]. Specifically, we measure the precision by solving the label-matching problem. We here com-pared our results with CLIQUE , which was taken as producing the acceptable grouping of points.
 miss values.
 clustering results as CLIQUE. In figure 11, we set the missing percentage of every dimension to be a random value no larger than 0 . 5, we presume that dimension with a large missing percentage will be useless. Figure 12 show the effect of missing percentage. We randomly miss a certain percentage of the attributes in some dimensions, precision on which is illustrated above. Several approaches are proposed to fight the problem of clustering high-dimensional data. The first category is to perform dimensionality reduction before clustering: points are projected to lower dimensional space to ease the traditional on reduced data space [4, 6]; Another category in attacking the high-dimensional clustering is based on grid partitioning [1, 9, 7, 20]. Algorithms in this category first divide the space into rectangular units and keep the high-density ones. Then the high-density units are combined to their neighbors to form clusters and summaries.
 has been done on filling the missing attributes with appropriate values. Several simple, inexpensive and easy to implement techniques were introduced in [16]. Besides, imputation, statistical or regression procedures are widely used in es-timating the missing values [13, 15]. Similar idea by reconstruction is also illus-trated in [3]. However, these techniques are prone to estimation errors when the dimensionality increases. There are also some approaches that view this problem from a different angel: they extract principal components without elimination or imputation [11, 19]. Similar PCA-like methods for missing value estimation include [11, 17]. In this paper, we develop an effective and efficient method to clustering on incomplete high-dimensional data. Our two contributions include contributing a fast high-dimensional clustering technique as well as the proposal of an prediction technique based on it. Our experiments and theoretical proof show our algorithm CLINCH has a good performance both in efficiency and precision of clustering incomplete data. In the future, we would like to transplant the algorithm to other fields such as multimedia mining and pattern recognition. Meanwhile, to meet the need of documents classification, further study in super high-dimensional data is necessary.

