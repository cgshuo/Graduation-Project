 Due to the increasing amount of information being stored and ac-cessible through the Web, Automatic Document Classificatio n (ADC) has become an important research topic. ADC usually employs a supervised learning strategy, where we first build a classifi cation model using pre-classified documents and then use it to class ify unseen documents. One major challenge in building classifie rs is dealing with the temporal evolution of the characteristics of the documents and the classes to which they belong. However, mos t of the current techniques for ADC do not consider this evolutio n while building and using the models. Previous results show that th e per-formance of classifiers may be affected by three different te mporal effects (class distribution, term distribution and class s imilarity). Further, it is shown that using just portions of the pre-clas sified documents, which we call contexts, for building the classifi ers, re-sult in better performance, as a consequence of the minimiza tion of the aforementioned effects.

In this paper we define the concept of temporal contexts as bei ng the portions of documents that minimize those effects. We th en pro-pose a general algorithm for determining such contexts, dis cuss its implementation-related issues, and propose a heuristic th at is able to determine temporal contexts efficiently. In order to demo nstrate the effectiveness of our strategy, we evaluated it using two distinct collections: ACM-DL and MedLine. We initially evaluated th e reduction in terms of both the effort to build a classifier and the en-tropy associated with each context. Further, we evaluated w hether these observed reductions translate into better classifica tion perfor-mance by employing a very simple classifier, majority voting . The results show that we achieved precision gains of up to 30% com -pared to a version that is not temporally contextualized, an d the same accuracy of a state-of-the-art classifier (SVM), while present-ing an execution time up to hundreds of times faster. This work is partially supported by CNPq, CAPES, FINEP, FAPEMIG, 5S-VQ project (CNPq/CTINFO grant number 551013/2005-2) and by the INFOWEB project (CNPq/CTINFO grant number 550874/2007-0).
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.5.4 [ Applications ]: Text processing; Algorithms, Experimentation Text Classification, Temporal Context, Digital Libraries
The widespread use of the Internet has increased the amount o f information being stored and accessed through the Web. This infor-mation is frequently organized as textual documents and is t he main target of search engines and other retrieval tools, which pe rform tasks such as searching and filtering. A common strategy to de al with this information is to associate it with semantically m eaning-ful categories, a technique known as Automatic Document Cla ssi-fication (ADC) [4]. The class assignment may support and enha nce several tasks such as automated topic tagging (i.e., assign ing labels to documents), building topic directories, identifying do cuments writing style, creating digital libraries, improving the p recision of Web searching, or even helping users to interact with search en-gines. Other useful application scenarios include spam filt ering, detection of adult content and detection of plagiarism.

ADC usually employs a supervised learning strategy, where w e first build a classification model using pre-classified docum ents, i.e., a training set, and then use the model to classify unsee n docu-ments. Building text classification models usually consist s of find-ing and weighting a set of characteristics (e.g., terms) tha t help to identify classes of documents. One major challenge in build ing classifiers is dealing with the temporal evolution of the cha racter-istics of the documents and the classes to which they belong, as a consequence of the creation of new documents, the introduct ion of new terms, the rise of new fields, and the division of large fiel ds into more specialized sub-fields. Thus, the differences, in terms of document characteristics, that once were useful for distin guishing classes may either increase or reduce, affecting the precis ion of the classifier that exploits them.

In [1], Baeza-Yates et al. advocate that time is an important di-mension of any information space and may be very useful in in-formation retrieval. Despite the potential impact of the te mporal evolution on the quality of classification models and its rec ogni-tion as a relevant factor, most of the current techniques for ADC, such as nearest-neighbor, Bayesian, support vector machin es, and association-based classification [30] do not consider this evolution while building and using the models. It is also not clear whic h tem-poral characteristics affect the quality of the model. In ou r recent work [24], we distinguish three different temporal effects that may affect the performance of automatic classifiers. The first ef fect, called class distribution, is related to the impact of the te mporal evolution on the class frequency. Classes may appear and dis ap-pear, which may happen as a consequence of splits and joins, r e-spectively, of existing classes. The second effect, called term distri-bution, is how the relationship between terms and classes ch anges over time, as a consequence of terms appearing, disappearin g, and presenting variable discriminative power across classes. The third effect, class similarity, is how the similarity among class es, as a function of the terms that occur in their documents, varies o ver time. For instance, two classes may be similar at a given mome nt, and not similar later in the future. We demonstrated the exis tence of these three factors and how each of them affects the classi fier X  X  performance.

After understanding the temporal effects in ADC, we need to build classification models that are effective regardless t hese ef-fects. In order to do this, we should be able to both classify a n unseen document in context, that is, at the moment of its crea tion, and consider just characteristics that are stable in that co ntext. One strategy is to create classification models that deal with th e afore-mentioned temporal effects. A different strategy, which is the focus of this paper, is to select a portion of the training set that m inimizes those effects, which we call temporal context selection. Th us, as the main contribution of this work, we propose a general algo rithm for temporal context selection and derive a computationall y fea-sible heuristic that instantiates the general algorithm. I n order to demonstrate the effectiveness of our strategy, we applied i t to two distinct collections, the first derived from the digital lib rary of the ACM (ACM-DL), containing documents about Computer Science , and the second from MedLine, a digital library related to Med icine. We used the resulting contexts for building a very simple maj ority voting classifier. This simple classifier was able to achieve gains of up to 30% against a version that did not consider time and the s ame accuracy of a state-of-the-art classifier (SVM), while pres enting an execution time up to hundreds of times faster.

The remainder of this paper is organized as follows. Section 2 briefly describes the characterization of the temporal effe cts pre-sented in [24], needed to understand the remainder of the pap er. Section 3 formally defines the problem that we deal in this pa-per, presents a general algorithm for temporal context sele ction and some possible strategies to implement it. In Section 4, we pr esent the technique we design for this problem. In Section 5, we eva luate this technique. Section 6 discusses the related work and, fin ally, in Section 7 we present the conclusions and discuss future work .
Before we discuss how we can handle the effects of the tempora l evolution of the collections, it is necessary to understand and quan-tify how this evolution manifests, which is the goal of this s ection. We distinguish and quantify three temporal effects that con tribute to that evolution: class distribution, terms distribution , and class similarity. We assess those effects in the ACM digital libra ry and MedLine collections (more details about these collections are given in Section 5) when building a classifier based on Support Vect or Machines (SVM) [13]. We also discuss how the size of the sampl e affects the precision of the classifiers (the sampling effec t), and the trade-off between the size of the sample and how much the clas si-fiers are affected by the temporal effects. We have published these results in [24] and summarized them in this section for a bett er un-derstanding of the remainder of this paper.

In order to demonstrate the sampling effect, a document sub-collection was built for each collection, containing docum ents from just one year, in order to isolate this effect from the tempor al ef-fects. Using these sub-collections, several classificatio n models were built, varying the size of the training set from 20% to 10 0% of the sub-collection. The result of this analysis is presente d in Fig-ure 1, where we can see that, as we increase the size of the trai ning set, the performance of the classifier gets better, as expect ed.
For the experiments that assess the temporal effects, each c ol-lection was divided into several sub-collections in a per ye ar ba-sis, each containing the same number of documents per collec tion collection, a classification model is built using it as train ing set with a cross-validation strategy and the resulting model was use d to clas-sify the documents of the other sub-collections from the sam e origi-nal collection. Figure 2 summarizes the experiments for bot h ACM and MedLine showing the relative accuracy ( y axis) for the various time distances ( x axis), which represents the difference, in years, between the test and the training sub-collections. Notice t hat, in most cases, the best scenario is found when the training and t he test documents belong to the same year (interval zero).
As we can observe in these results, there is a challenge in dea l-ing with the sampling and temporal effects simultaneously, because they indicate opposite strategies in terms of setting a trai ning set. Increasing the size of the training set may introduce docume nts that are out of the temporal context, which would reduce the preci sion of the classifier. On the other hand, reducing the training se t to just the documents close to the test document would ease the const ruc-tion of a classifier, but there is a risk of not having enough da ta to build the model or the resulting model being overfitted. In or der to determine the proper temporal context, that is, a portion of the pre-classified documents that is as large as possible and is tempo rally consistent, we need to understand and quantify the three afo remen-tioned temporal effects.
 First we analyze the variation of the class distribution ove r time. Figure 3, where the class probability distribution was plot ted for each year, illustrates the variation in terms of the represe ntative-ness of the classes across time. We can see that the oscillati on of the occurrence of the classes is frequent across the years. F or ex-ample, the class Math becomes less frequent as time progresses. Similar results can be observed in the MedLine collection. T hese results show that our temporal contexts should capture the e xist-ing characteristics when the document to be classified was cr eated, otherwise the model may not be accurate. Figure 3: Class Occurrence Variation Through Time -ACM
In order to characterize the term distribution effect, for e ach class and each year, a vocabulary is defined as the words that have th e highest values of info-gain [11] to identify the class. The v ocab-ularies for the same class in different years are compared us ing the cosine similarity [27] between them. Figure 4 shows the c o-sine similarity as we vary the time distance. Distance zero m eans we are comparing a vocabulary to itself, which obviously cor re-sponds to the maximum similarity. We can observe that the lar ger the time distance between the vocabularies, the less simila r they are, which demonstrates that vocabularies change over time . The cosine similarity between the vocabularies for the same cla ss with time distance 20 is less than 50% for ACM-DL (for all classes) and less than 80% for MedLine (for almost all classes). An intere st-ing example is the class Aids of Medline collection, which differs from the other classes w.r.t. the similarity decline rate am ong its vocabulary vectors, showing that the class Aids is very dynamic.
Finally, to verify the effect of the evolution on the class si milar-ity, we calculate the cosine similarity between the vocabul aries for each pair of distinct classes for each year. Tables 1 and 2 sho w the variation of similarity between each pair of classes over ti me for MedLine and ACM-DL, respectively. Each entry in these table s is the standard deviation from the mean of the similarities bet ween the associated pairs of classes in all years. As we can observ e, for some pairs of classes the similarity variation is very high, such as for classes Complementary Medicine (CM) and History (Hist). It means that these two classes may have been very similar in som e periods, but also loosely related in others. Consequently, the diffi-culty in separating the two classes varies significantly acr oss time.
Based on this brief discussion, it was possible to provide ev i-dence of the class and term variation over time. It was also cl ear that the similarity among these classes varies as well. More over, we could see that these effects may have great influence on the per-formance of the classifiers. In this section we formalize the problem addressed in this wo rk. Moreover, we present a general algorithm to solve this probl em and discuss some possible implementations based on it.
Before we formalize the problem of selecting temporal conte xts, we formally define text classification with temporal informa tion. Let D = { d 1 , d 2 , . . . , d K } be the documents that compose a train-ing set of a collection; C = { c 1 , c 2 , . . . , c L } egories that occur in a collection; T = { t 1 , t 2 , . . . , t set of terms associated with the documents of a collection; { m 1 , m 2 , . . . , m P } be the set of moments on a temporal space when both test and training documents are created (timestam p). We define a training document d i by the triple x, m p , and x  X  T , m p  X  M and c l  X  C . Given a set S = { s 1 , s 2 comprising the documents that we want to classify, the task o f ADC is to determine the class c i associated with the test document where each s i = { y, m p } , y  X  T , m p  X  M .

ADC usually follows a supervised learning strategy, where w e first need to select a context X  X  D that is used to build a clas-sification model. Often X is equal to D or a random sample of D . Then we use this model to classify new unseen documents ( As expected, the context X is composed by documents which were created at different time moments ( M ). As previously discussed in the last section, the sampling effect suggests that we shoul d in-crease the size of X , while the time effects induce the reverse, since using long time periods may generate conflicting evide nce that would affect the quality of the resulting classifier. Th erefore, the problem we address in this paper is to determine contexts that optimize the trade-off between the sampling effect and the t ime effects. Notice that the number of possible contexts grows e xpo-nentially with the number of documents, thus demanding stra tegies that narrow the search for better contexts efficiently.

It is important to distinguish the context selection proble m from feature selection [23], which aims to reduce the number of fe atures that will be used in the classification model, usually for effi ciency purposes, but maintaining its accuracy and robustness. Alt hough the reduction of the size of the training set is a side-effect , the main goal of temporal context selection is to determine the large st por-tion of the training set in which the time effects (class dist ribution, class similarity and terms distribution), and thus the unce rtainty, which a classifier has to deal with, is minimized.

In order to illustrate our problem, consider the training se t pre-sented in Table 3. It consists of documents that contain the t erm Pluto . Besides being the god of hell in Roman mythology, Pluto was also considered to be a planet until the middle of 2006. Th us, as we can observe in Table 3, before 2005 our training set con-sists of documents from only one class, Astrophysics. After 2005, it contains only documents from class Mythology, while in 20 05 there is one document from each class. If we employ the whole training set for building a classifier, as it is usually done, we would depend on other features to determine the correct category f or a document that contains Pluto , since this term may confuse the clas-sifier. However, considering the temporal information for d elimit-ing contexts where Pluto has a unambiguous meaning, as we pro-pose in this paper, could solve the problem.

In this section we propose an algorithm for temporal context se-lection that addresses three fundamental requirements for the con-texts. This algorithm may be used as a template for implemen-tations that select contexts from the training set X temporal effects are minimized. Based on the characterizat ion of time effects presented in [24] and briefly discussed in Secti on 2, we identify three requirements that should be fulfilled by a g ood temporal context: 1. Reference constrained: as observed in Section 2, the best 2. Characteristic stability: as observed in the previous sec-3. Uncertainty reduction: as explained in Section 2, the terms
These requirements are the basis for the Chronos 3 algorithm (Algorithm 1), which comprises the main steps towards deter min-ing contexts while reducing the time effects. As we discuss n ext, the decisions associated with the design and implementatio n of these steps will determine both the quality of the contexts g enerated and the computational complexity of the process, which is cl early a trade-off.

Chronos receives as input two document collections D ing) and S (test), and a stability threshold  X  . We address the first requirement in line 4. The function GetReference ( D, S ) the temporal characteristics (class distribution, terms d istribution and class similarity) of training set D at the moment when the test document S was created. The function Enumerate ( D ) in line 5 lists, one at a time, the possible contexts X that will be analyzed by the algorithm. In line 7, the second requirement is addres sed. Algorithm 1 Chronos Algorithm The function StabilityFunction ( option, state ) quantifies the dif-ference between the temporal characteristics of context option the characteristics of the training set D at the moment when the test document S was created. This difference needs to be less than the threshold  X  . Finally, the third requirement is addressed in line 8. The function TemporalUncertainty ( option ) calculates the uncer-tainty inherent to option . Then, among the possible contexts that are stable, the algorithm selects the one with the small est un-certainty.
In this section we discuss and analyze some implementation s trate-gies to temporal context selection, based on Algorithm 1. On e pos-sible strategy is to perform the selection of X only once for the whole test set and use it for classifying all test documents. To illus-trate this solution, we can take the example presented in Tab le 3. Considering the test set S composed by documents s 1 , s 2 from 1999, 2006, 2001, and 2005 respectively, select a porti on of the training set that considers the three requirements pres ented in the last section is not possible, since our test set consists of doc-uments from different moments. How can we contextualize tem -porally the training set with the date of test documents if ea ch test document is from a different moment? The function GetReference of Algorithm 1 will capture the temporal characteristics of different moments and the uncertainty will not be reduced, emphasizin g the need for an on-demand solution.

In [24] we presented a experiment that illustrates the probl em related to this first strategy for two real collections: ACM-DL and MedLine. The experiment uses a time-sensitive selection of the documents for training, that is, they select as their traini ng set, for each test document, just the documents that are closer in tim e to it. The proximity is defined by a time-window that may grow sym -metrically in both directions, past and future, based on the year of the test document. We varied the size of this window from 0 (th at is, the year A i of the tested document) to N , where N represents the number of years before and after A i . The value of fined as the value that maximizes the performance of the class ifier to the test documents that belong to the year A i . Figure 5 presents the optimum window size for test documents from all years in t he ACM collection (a) and the MedLine collection (b) using an SV M classifier. For instance, in the ACM collection, for the docu ments from 1990, the optimum size window is 10 years. For the docu-ments from 1989, however, the optimum size is 20 years. As we can see, there is not a single optimum size for the window for t he entire database, in both collections, but an optimum window size may be found for each specific year. Thus, a unique selection f or all test documents is not effective.

Another strategy is to perform an exhaustive search for the b est context X for each test document. In order to do this, it is necessary that the function Enumerate ( D ) lists all possible contexts set D . Then, for each test document S N , according to its temporal characteristics, the strategy chooses the best context X . Since we need to exploit characteristics of the test document, we can say that this alternative is clearly a lazy solution [32].

Lazy algorithms are based on the assumption that there is not a universal classification model and they sample the trainin g set at classification time. However, similarly to other lazy sol utions, the implementation must be computationally feasible. Cons idering that there are 2 D possible X contexts, where D is the number of training documents, a brute-force solution is not feasible , since it requires the evaluation of each of these contexts in order to find the best one for each test document. In our example, the funct ion Enumerate ( D ) would list 2 16 possible sets and evaluate whether each of them is acceptable. However, when we deal with a real collection like ACM-DL, that contains 30,000 documents, th e eval-reduce the computational costs is to employ heuristics that address the aforementioned requirements while keeping the computa tional costs low.
In this section we present GreedyChronos, a heuristic for se lect-ing temporal contexts based on the general algorithm Chrono s. The strategy of GreedyChronos is to select a context from the tra ining set where the three time effects are minimized, enabling mor e ef-fective classification models.

GreedyChronos is executed for each test document, naturall y ad-dressing the reference constrained requirement, since it c aptures the characteristics associated with just one document, mor e specif-ically its terms and creation time. The other two requiremen ts (fea-ture stability and uncertainty reduction) are addressed to gether as we describe next. We define a time-window for each term in the test document, which can grow in both directions, past and fu ture, starting from the creation moment of the test document. The s ize of each window is set by the period during which the term remai ns  X  X table X . The term stability is measured by its degree of exc lu-sivity on any class for a determined period, and is quantified by Dominance [35]. Formally, let T = { t 1 , t 2 , t 3 , . . . , t set of terms associated with a collection; C = { c 1 , c 2 be the set of categories that occur in a collection; df ( t number of training documents associated with class c j that contain t . We define the Dominance of the class c j on term t i as:
In our approach, the time-window of each test term is contigu ous for a couple of reasons. First, because the relationships be tween terms and classes tend to change smoothly as we increase the d is-tance between the test document and a document in the context , as we could see in Section 2. Second, because the size of the sear ch space for determining a non-contiguous window (or a set of ye ars) is stability and the reduction in terms of uncertainty associa ted with a window, since the stronger is the relationship between a ter m and a class, the smaller is the uncertainty.

Let X  X  come back to the example presented in Table 3 and the test set S , consisting of documents s 1 , s 2 , s 3 , s 4 2001, and 2005, in which there is an occurrence of the word Pluto in all of them. Consider that the algorithm will use a Dominance &gt; 50% to calculate the time window for the word Pluto on each test document. For example, the document s 1 is from 1999 and in this year there is only one document in the training set that belon gs to class Astrophysics. Thus, in this year, the term Pluto has a dom-inance equal to 100% and this year will compose the time win-dow of term Pluto for document s 1 . Then, the algorithm analyzes the training set in neighbor years. A similar behavior may be ob-served in years 1998, 2000, 2001, 2002, 2003, and 2004, so tha t these years will also compose the time window. However, in 20 05, there are two documents that contain the term Pluto , one of them belongs to class Astrophysics and the other one belongs to cl ass Mythology. Therefore, in this year the term Pluto does not have a Dominance &gt; 50% . Consequently, this year will not compose the time window of term Pluto for document s 1 and the algorithm terminates the search for the time window of term Pluto for docu-ment s 1 . The algorithm is executed for all terms of each test docu-ment. The time window for the word Pluto on each test document from test set S would be: s 1 (1998 to 2004); s 2 (2006 to 2007); (1998 to 2004); and s 4 (empty).

After determining the time window for each term, the last ste p is to select the documents that will be in the context and be us ed as training set for the test document. For each test term, we s elect the documents that have the term and belong to its time window . Finally, we make the union of the selected documents for all t erms from the test document. In our example, the training set for t est documents s 1 and s 3 would be the documents d 1 , d 2 , d d , and d 7 and for s 2 would be the documents d 10 , d 11 , d 14 , d 15 , and d 16 . For the test document s 4 there are no docu-ments, which will be addressed using other terms in the docum ent. It should be noticed that our heuristic can be easily adapted for scenarios where we only have past information, such as Adapt ive Document Classification [8] and Concept Drift [28].
In this section we describe the results of experiments condu cted for evaluating the GreedyChronos heuristic proposed in thi s paper. This evaluation shows how the heuristic may reduce the time e ffects in each training set and demonstrates that it enables us to fin d a set of documents that effectively reduces the time effects.
In all experiments we use two collections. The first one is a se t of 30,000 documents from the ACM-DL containing articles fro m 1980 to 2002. In this collection, classes of the ACM taxonomy are assigned to documents by their own authors. We used the first l evel of the taxonomy, comprising all 11 categories. This is a coll ection much harder to classify than ACM8, used in works such as [32], in which the 3 least frequent classes are excluded. Each docume nt is assigned to a single class. The second collection is derived from MedLine and consists of 861,454 documents, containing arti cles from 1970 to 1985, classified into 7 distinct classes. Note th at this collection is significantly larger (by almost 2 orders of mag nitude) commonly used for evaluating document classifiers [7].
We can see time as a discretization of natural changes inhere nt to any knowledge area, although detectable changes may occur a t dif-ferent time scales, depending on the area characteristics. In the case of our experimental collections, which consist of sets of sc ientific articles, we adopted yearly intervals for identifying such changes (e.g., conferences, which are usually annual), but in other scenarios the granularity may be different.

We performed a set of experiments that consist of determinin g the temporal context (and then the training set) associated with each test document, and varied the dominance parameter from 0% (n o temporal information is considered) to 90% (we will not pres ent the results for dominance greater than 90%, since the resulting contexts are very sparse, most of them are empty). In all experiments w e employed a 10-fold cross-validation [5] and the final result s of each experiment is the average of the ten runs.

We start our evaluation by analyzing the reduction in terms o f the size of the contexts selected as a function of the dominan ce. Our metric in this case is called total terms, which is the sum of the number of terms in the contexts selected for all test docu ments, quantifying the effort of a classifier to build a model for eac h test document. The results for 0% dominance are used as a baseline and all other results are calculated as relative to them. The results are shown in the graphs of Figure 6. As we can see, in both colle c-tions, temporal contexts reduce significantly the number of terms in the training sets, in particular for larger dominance val ues. Em-ploying a 90% dominance, the total number of terms used for cl as-sifying documents from ACM-DL is less than 10% of the baselin e, while in MedLine collection, it is less than 3%. Notice that, for the ACM-DL collection, by adopting a dominance lower than 20%, t he reduction in the total number of distinct terms is not so sign ificant while for MedLine the same observation is valid for dominanc es up to 40%. This may be explained by the smaller number of classes in MedLine (7) than in the ACM-DL (11).
We also evaluate the number of distinct terms, that is, the un ion of the sets of terms of the training sets for each dominance va lue, which is presented in Figure 7. There is a small reduction on t he number of distinct terms in ACM-DL as we increase the domi-nance, while in MedLine this reduction is very significant. A nalyz-ing these last graphs together with the graphs in Figures 6 an d 4, we can conclude that the training sets for the test documents overlap less in the ACM-DL than in MedLine, since the terms in ACM-DL have a stability period usually smaller than in MedLine, tha t is, the ACM-DL is more affected by the time effects as presented in [2 4].
Despite the significant reduction on the number of total term s used when we increase the dominance, we also need to evaluate whether this effort reduction is associated with a reductio n in the uncertainty for sake of classification. We then calculated t he en-tropy [26] for each training set selected by GreedyChronos w hile varying the dominance. Entropy, in our case, is understood a s the necessary information for building a classification model. This metric quantifies the terms dispersion among classes of a col lec-tion, measuring its disorder. The average of these results i s pre-sented in Figure 8, where we can see that there is a significant de-crease on the entropy as we increase the dominance, thus redu cing the uncertainty and the effort to build a classifier.
We confirm this last observation by analyzing the number of di s-tinct classes that are associated with each test term in its r espec-tive training set. The results of this analysis for each coll ection is presented in Figure 9, which shows that an increase in domina nce causes a decrease in the number of classes associated with a t erm. For example, using 0% dominance, each test term occur, on ave r-age, in 8 distinct classes in the training sets for ACM-DL and 5 for MedLine. When we increase the dominance to 90% this num-ber reduces to less than 2 for both collections. This result s hows that, besides reducing the amount of data to be processed, Gr eedy-Chronos also reduces the uncertainty of the training sets. H owever, it is still necessary to analyze whether this reduction in th e confu-sion contributes to determine the correct class of the test d ocument.
One key question is whether the reduction in uncertainty als o enables the construction of good classification models. In o rder to assess such question, we performed the following experimen t. For each dominance value, we employed an extremely simple class ifier to classify the test documents: a majority voting. This clas sifier consists of selecting the class with highest score in the tra ining set induced by the temporal context and assign it as the class of t he test document. It should be noticed that better classifiers could and will be used in the future, and we believe that they will outperfor m our majority voting classifier. But our goal here is not to produc e the best classifier that exploits the temporal information, but to show that our temporal contexts enable, in fact, uncertainty red uction and, consequently, correct classifications most of the time , even using such simple technique.

We employed three different criteria to calculate the score of each class. The first one (Class Frequency) simply selects th e class that occurs most frequently in the training set. Let df ( C number of training documents associated with class C i , the score function can be defined by Eq. (2). The second criterion (Term Occurrence) selects the class that has the largest number of occur-rences of test terms in the training set, based on the intuiti on that this class has the greatest probability of being the correct class. Let df ( t j , C i ) be the number of training documents associated with class C i that contains t j and T be the total of distinct terms in test documents, the score function is defined by Eq. (3). The t hird criterion (Weighted Term Occurrence) is derived from the se cond one. It also consists of counting the occurrences of the test terms in each class in the training set. We smooth this value by appl y-ing the log function to it and by weighting this total number o f occurrences by the ratio between the number of distinct test terms in the class ( T Ci ) and the number of distinct test terms ( weight-based smoothing minimizes the impact of a class cont ain-ing just few distinct test terms with high number of occurren ces, which might present a higher score than a class containing mo re unique terms that occur less, towards a balance between the n umber of unique terms and their occurrences. The resulting score f unction is given by Eq. (4).

In order to quantify the classification effectiveness of all major-ity voting we use standard information retrieval measures: preci-sion, recall, and F 1 [21]. Recall is defined as the fraction of the class X  documents correctly classified and precision is defin ed as the fraction of documents correctly classified from all docu ments attributed to the class. A usual approach to evaluate the cla ssifica-tion effectiveness is F 1 , a combination of precision and recall given by its harmonic mean. We then summarize our results employin g two commonly used measures of the F 1 scores: macro-average F (MacroF1) and micro-average F 1 (MicroF1). The first one mea-sures the classification effectiveness on each individual c lass and averages them, while the last one is computed globally over a ll pre-dictions.

Figures 10 and 11 show the MicroF1 and MacroF1, respectively , for all majority voting classifiers for both collections. No tice that, as we increase the dominance, the MicroF1 and MacroF1 also in -crease in both collections. Therefore, as we increase the do mi-nance, the uncertainty of the training sets reduces, while t he conver-gence for the correct class of test document increases. For i nstance, for ACM-DL, the majority voting using the Weighted Terms Oc-currence achieved a MicroF1 of 67.77% and a MacroF1 of 53,99% for 65% dominance. In the MedLine collection, using the Weig hted Terms Occurrence, the majority voting achieved a MicroF1 of 75,38% and a MacroF1 of 58,96% for 85% dominance.
In order to evaluate whether the convergences for the correc t classes achieved using the training set induced by the tempo ral con-text were good, we compared the results of the three score fun ctions used in the majority voting classifiers to a Support Vector Ma chine classifier (C-SVM using the Radial Basis Function Kernel) [1 3], since SVMs are the current state-of-the-art in ADC. For the S VM classifier, we used the combination of factors Gamma and Cost ( C ) that produced the best results for each collection in the tra ining set. The SVM achieved a MicroF1 of 68,44% and a MacroF1 of 57,77% for ACM-DL, and a MicroF1 of 73.34% and a MacroF1 of 56.63% for MedLine.

We present in Table 4 a summary, for each collection, of the be st non-temporal (dominance = 0) version of the majority voting clas-sifier, the best temporal majority voting classifier, and SVM . The table also includes the execution time of the algorithms. Fo r the SVM classifier, the execution time represents the time for tr aining and for classifying the test documents. For the temporal ver sions of majority voting, this time represents the time for calcul ating the training set for each test document (execution time of our he uris-tic) and the time to classify the test documents. Notice that , despite being somehow unfair, since we are comparing lazy and non-la zy algorithms, this comparison has two goals. First, it shows t hat the temporal context selection does not have a large impact on ex e-cution time. In fact, it is able to reduce the overall classifi cation time of the majority voting classifiers. And second, to show t hat our method is a good alternative in cases where the training t ime of SVM makes it an unfeasible choice due to the size of the coll ec-tions, for example, the entire MedLine collection. Moreove r, the classification model of SVM often needs to be rebuilt, accord ing to the frequency of changes in the collections, worsening th is prob-lem. To compare these results, we applied a two-tailed paire d t-test. Table 4 shows that the majority voting classifier with our tem poral selections achieved gains of up to 30% against a version that did not consider temporal information. Adopting a 99% confidence le vel, we can state that, for the ACM-DL collection, the MicroF1 of b oth the best majority voting classifier and SVM are statisticall y equiv-alent, while the MacroF1 values for SVM are higher than for th e best majority voting classifier. Moreover, in MedLine colle ction, adopting the same confidence level, we can say that the MicroF 1 and MacroF1 for the best majority voting outperforms SVM.

We performed experiments using all dominance values, since our goal was to characterize and evaluate our heuristic for sele cting temporal contexts. As we can see, besides reducing the amoun t of data to be processed and the uncertainty, the training set s in-duced by the temporal context also present a good convergenc e to the correct class of the test documents. This indicates the b enefits of the temporal contexts, since, even using very simple clas sifiers like majority voting, we achieved equivalent results to the state-of-the-art classifier (SVM). In an actual scenario, it would be n eces-sary to split the training set into training and validation, looking for dominance values (with folded cross-validation) that b est per-form in the validation set, in order to find good dominances, s ince specific characteristics of each collection affect the choi ce of the dominance. Moreover, we can get even better results if we use these temporal contexts with more effective techniques for ADC, such as nearest-neighbor, Bayesian, association-based cl assifica-tion or the SVM itself, as we propose as future work. In the cas e of SVM, though, this may be too expensive, since we would have to build a model for each test document using its temporal con -text. Moreover, it is important to point out that the charact eristics of the temporal contexts may change compared to the original sam-pling, therefore some premises assumed by these algorithms may also change. Thus, it will be necessary to perform a more deta iled analysis of how we can apply the techniques presented with ot hers ADC algorithms.

Although document classification is a widely studied subjec t, the analysis of temporal aspects in this class of algorithms is q uite re-cent -it has been studied just in the last decade. We distingu ish three broad areas where there has been significant efforts: a daptive document classification, adaptive information filtering, a nd concept drift. We discuss each of them in this section.
 One area where temporal aspects have been studied in detail i s Adaptive Document Classification [8], which encompasses a s et of techniques related to temporal aspects with the goal of impr oving the effectiveness of document classifiers through their inc remental and efficient adaptation. Adaptive Document Classification brings a variety of challenges to text mining [22]. The first challen ge is the notion of context and how it may be exploited towards better c lassi-fication models. Previous research in document classificati on iden-tified two essential forms of context: neighbor terms that ar e close to a certain keyword [19] and terms that indicate the scope an d se-mantics of the document [6]. The second challenge is creatin g the models incrementally [15]. The third challenge has to do wit h the computational efficiency of the document classifiers. These tech-niques often adopt a notion of context that is different from ours in the sense that they exploit semantic contexts, which aris e, for instance, from the co-occurrence of terms. The temporal con texts we propose here may also be applied to these semantic context s, exploiting their stability. Unlike those efforts to identi fy new con-texts and to deal with the aforementioned challenges, in thi s paper we are concerned about performing document classification i n its original temporal context, that is, categorize the documen t accord-ing to the period of time to which it belongs.

Concept or topic drift [31] comprises another relevant set o f ef-forts. To deal with concept drift, one common approach is to r etrain completely the classifier according to a sliding window of ples [28, 17, 16, 33, 20]. In [28] the authors keep a window con -taining the most relevant documents. The method presented i n [17] also maintains a window with the most representative docume nts and automatically adjusts the window size so that the estima ted generalization error is minimized. In [16], the methods pre sented either maintain an adaptive time window on the training data , select representative training examples, or weight the training e xamples. In [33] the authors describe a set of algorithms that flexibly react to concept drift and can take advantage of situations where con texts reappear. The main idea of these algorithms consists of keep ing only a window of currently trusted examples and hypotheses; and storing concept descriptions and re-using them when a previ ous context reappears. Unlike previous works which use a single win-dow to determine the drift in the data, in [20] the authors pre sent a method that uses three windows of different sizes to estima te the change in the data.

Other common approach to deal with concept drift focuses on t he combination of various models of classification generated f rom dif-ferent algorithms for classification [29, 18, 10]. In [29], t he authors propose a boosting-like method to train a classifier ensembl e from data streams. It naturally adapts to concept drift and allow s to quan-tify the drift in terms of its base learners. The algorithm is empiri-cally shown to outperform learning algorithms that ignore c oncept drift. Kolter et al. [18] present a technique that maintains an en-semble of base learners, predicts using a weighted-majorit y vote of these "experts", and dynamically creates and deletes exper ts in re-sponse to changes in performance. In [10], it is presented a m ethod that builds an ensemble of classifiers using Genetic Program ming (GP) to inductively generate decision trees, each trained o n differ-ent parts of the distributed training set. In both approache s, the authors consider the existence of temporal effects without exactly understanding what these effects are. They discuss that, de spite the fact that some classes present popularity explosion in some periods, their true identity does not change. We, on the other hand, ba sed on detailed empirical results that explain the temporal asp ects [24], consider that there are other two effects beyond the explosi on of popularity of certain classes (class distribution): class similarity and terms distribution. Moreover, although our proposal ca n be ap-plied to deal with concept drift and to classify new document s (data streams), we may also apply it to classify old documents.

Another field in which there is a concern about temporal issue s is Adaptive Information Filtering [12]. Information Filte ring [2] is described as a binary classification problem in which docu ments are classified as relevant or not with regard to the user X  X  int erest. According to [9], building classifiers in static domains is a suffi-ciently well controlled problem, and many applications ass ume that the training data distribution and the new data distributio n are sim-ilar. This may be true for applications that last for a short p eriod of time, but it becomes invalid for applications lasting long p eriods. In this case, it is unavoidable to adapt the classifier to new sce narios in order to maintain the quality of the classification.

There are some works that are somehow similar to ours in the sense that they investigate how the temporal aspect can be us ed to improve the quality of a certain process [14, 3, 34]. In [14] t he authors show that taking temporal aspects into considerati on when generating results for queries is very important and may imp rove the quality of the results. By analyzing the timeline of a que ry result set, it is possible to characterize how temporally de pendent the topic is, and how relevant the results are likely to be. In [3] the authors propose an algorithm, named T-Rank, which exten ds PageRank to improve ranking by exploring the temporal proxi mity between Web pages and in [34] the PageRank algorithm is adapt ed by weighting each citation according to the citation date. B esides these, there are other efforts that exploit temporal eviden ce in Web Information Retrieval; a broad survey of them is presented i n [25]. Although some of the premises of these efforts are similar to ours, none of them address the problem of building classifiers that are robust w.r.t. temporal evolution.
In this work we propose a context selection strategy that is u sed for building classification models. Our strategy consists o f select-ing contexts that are a set of pre-classified documents which mini-mizes the three temporal effects (class distribution, term s distribu-tion and class similarity), so that the resulting learned cl assifiers are less susceptible to temporal related changes. We started by identi-fying three requirements for the contexts to be selected: re ference constrained, feature stability, and uncertainty reductio n. Then we propose Chronos, an algorithm that may be used as a template b y techniques that aim to generate temporal contexts.

Based on this algorithm we also propose GreedyChronos, a heu ris-tic that effectively selects good temporal contexts. We eva luated GreedyChronos using two distinct collections: the ACM digi tal li-brary (ACM-DL) containing documents about Computer Scienc e, and MedLine, a digital library related to Medicine. We also e val-uated both the size of the contexts generated and the uncerta inty reduction they provide. Further, we used the resulting cont exts for building a simple majority voting classifiers. Some of these simple classifiers were able to outperform, in more than 30%, versio ns of them that are not temporally contextualized and produced si milar accuracy to a state-of-the-art classifier (SVM), while pres enting an execution time up to hundreds of times faster.

As future work, we plan to apply the techniques presented in this paper along with several ADC algorithms in order to eval u-ate whether our technique is able to improve them. However, i t is important to point out that the characteristics of the tem poral contexts may change compared to the original sampling, ther efore some premises assumed by these algorithms may also change. F or example, the frequency of features in classes may change, af fecting important metrics such as IDF (Inverse Document Frequency) and, consequently, in the algorithms that consider these metric s. Thus, it will be necessary to perform a more detailed analysis of ho w we can apply the techniques presented with others ADC algorith ms.
We also want to apply this technique to different Web collect ions to verify whether the same temporal evolution effects occur and can be investigated. Moreover, we intend to extend our techniqu es to consider, for example, other types of evidence, such as inli nks and outlinks within a temporal context. [1] O. Alonso, M. Gertz, and R. Baeza-Yates. On the value of [2] N. Belkin and W. Croft. Information filtering and [3] K. Berberich, M. Vazirgiannis, and G. Weikum. T-rank: [4] H. Borko and M. Bernick. Automatic document [5] L. Brieman and P. Spector. Submodel selection and [6] N. H. M. Caldwell, P. J. Clarkson, P. A. Rodgers, and A. P. [7] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. [8] W. W. Cohen and Y. Singer. Context-sensitive learning [9] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inducti ve [10] G. Folino, C. Pizzuti, and G. Spezzano. An adaptive [11] G. Forman. An extensive empirical study of feature sele ction [12] S. Haykin. Adaptive filters. In Signal Processing Magazine . [13] T. Joachims. Making large-scale support vector machin e [14] R. Jones and F. Diaz. Temporal profiles of queries. ACM [15] Y. S. Kim, S. S. Park, E. Deards, and B. H. Kang. Adaptive [16] R. Klinkenberg. Learning drifting concepts: Example [17] R. Klinkenberg and T. Joachims. Detecting concept drif t [18] J. Kolter and M. Maloof. Dynamic weighted majority: A [19] S. Lawrence and C. L. Giles. Context and page analysis fo r [20] M. M. Lazarescu, S. Venkatesh, and H. H. Bui. Using [21] D. D. Lewis. Evaluating and optimizing autonomous text [22] R. Liu and Y. Lu. Incremental context mining for adaptiv e [23] L. Molina, L.C.and Belanche and A. Nebot. Feature selec tion [24] F. Mourao, L. Rocha, R. Ara X jo, T. Couto, M. Gon X alves, [25] S. Nunes. Exploring temporal evidence in web informati on [26] J. R. Quinlan. Induction of decision trees. Machine Learning , [27] G. Salton and M. J. McGill. Introduction to Modern [28] J. C. Schlimmer and R. H. Granger. Beyond incremental [29] M. Scholz and R. Klinkenberg. Boosting classifiers for [30] F. Sebastiani. Machine learning in automated text [31] A. Tsymbal. The problem of concept drift: Definitions an d [32] A. Veloso, W. M. Jr., M. Cristo, M. Gon X alves, and M. J. [33] G. Widmer and M. Kubat. Learning in the presence of [34] P. S. Yu, X. Li, and B. Liu. On the temporal dimension of [35] O. R. Zaiane and M. L. Antonie. Classifying text documen ts
