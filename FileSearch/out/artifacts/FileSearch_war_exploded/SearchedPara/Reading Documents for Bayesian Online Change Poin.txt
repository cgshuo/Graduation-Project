 Time series data depends on the latent dependence structure which changes over time. Thus, sta-tionary parametric models are not appropriate to represent such dynamic non-stationary processes. Change point analysis (Smith, 1975; Stephens, 1994; Chib, 1998; Barry and Hartigan, 1993) fo-cuses on formal frameworks to determine whether a change has taken place without assuming the Markov property which is vulnerable to local sig-nal noise. When change points are identified, each part of the time series is approximated by specified parametric models under the stationary assump-tions. Such change point detection models have successfully been applied to a variety of data, such as stock markets (Chen and Gupta, 1997; Hsu, 1977; Koop and Potter, 2007), analyzing bees X  be-havior (Xuan and Murphy, 2007), forecasting cli-mates (Chu and Zhao, 2004; Zhao and Chu, 2010), and physics experiments (von Toussaint, 2011). However, offline-based change point analysis suf-fers from slow retrospective inference which pre-vents real-time analysis.
 Bayesian Online Change Point Detection (BO-CPD) (Adams and MacKay, 2007; Steyvers and Brown, 2005; Osborne, 2010; Gu et al., 2013) overcomes this restriction by exploiting efficient online inference algorithms. BO-CPD algorithms efficiently detect long-term changes by analyzing continuous target values with the Gaussian Pro-cess (GP), a non-parametric regression method. The GP-based CPD model is simple and flexible. However, it is not straightforward to utilize rich external data such as texts in news articles and posts in social networks.

In this paper, we propose a novel BO-CPD model that improves the detection of change points in continuous signals by incorporating the rich external information implicitly written in texts on top of the long-term change analysis of the GP. In particular, our model finds causes of sig-nal changes in news articles which are influential sources of markets of interests.
 Given a set of news articles extracted from the Google News service and a sequence of target, continuous values, our new model, Document-based Bayesian Online Change Point Detection (DBO-CPD), learns a generative model which rep-resents the probability of a news article given the run length (a length of consecutive observations without a change). By using the new prior, DBO-CPD models a dynamic hazard rate ( h ) which de-termines the rate at which change points occur.
In the literature, important information is ex-tracted from news articles (Nothman et al., 2012; Figure 1: This figures illustrates a graphical repre-sentation of BO-CPD and our DBO-CPD model. x , r t , and D t represent a continuous variable of interest, the run length (hidden) variable, and doc-uments, respectively. Our modeling contribution is to add texts D 1: t for the accurate prediction of the run length r t +1 .
 Schumaker and Chen, 2009; Gid  X  ofalvi and Elkan, 2001; Fung et al., 2003; Fung et al., 2002; Schu-maker and Chen, 2006), tweets on Twitter (Si et al., 2013; Wang et al., 2012; Bollen et al., 2011; St Louis et al., 2012), online chats (Kim et al., 2010; Gruhl et al., 2005), and blog posts (Peng et al., 2015; Mishne and Glance, 2006).

In experiments, we show that DBO-CPD can ef-fectively distinguish whether an abrupt change is a change point or not in real-world datasets (see Sec-tion 3.1). Compared to previous BO-CPD models which explain the changes by human manual map-pings, our DBO-CPD automatically explains the reasons why a change point has occurred by con-necting the numerical sequence of data and textual features of news articles. This section will review our research problem, the change point detection (CPD) (Barry and Harti-gan, 1993), and the Bayesian Online Change Point Detection (BO-CPD) (Adams and MacKay, 2007) and our model, Document Based Online Change Point Detection (DBO-CPD).

Let x t  X  R be a data observation at time t . We assume that a sequence of data ( x 1 ,x 2 ,...,x t ) is composed of several non-overlapping produc-tive partitions (Barry and Hartigan, 1992). The boundaries that separate the partitions is called the change points. Let r be the random variable that denotes the run length , which is the number of time steps since the last change point was detected. r t is the current run at time t . x most recent data corresponding to the run r t . 2.1 Online Recursive Detection To make an optimal prediction of the next data x t +1 , one may need to consider all possible run lengths r t  X  N and a probability distribution over run length r t . Given a sequence of data up to time t , x 1: t = ( x 1 ,x 2 ,...,x t ) , the run length prediction problem is formalized as computing the joint prob-ability of random variables P ( x t +1 ,x 1: t ) . This distribution can be calculated in terms of the poste-rior distribution of run length at time t , P ( r t | x 1: t as follows: The predictive distribution P ( x t +1 | r t ,x ( r t ) pends only on the most recent r t observations x t . The posterior distribution of run length P ( r t | x 1: t ) can be computed recursively: where: The joint distribution over run length r t and data x 1: t can be derived by summing P ( r t ,r t  X  1 ,x 1: t ) = = This formulation updates the posterior distribution of the run length given the prior over r t from r t  X  1 and the predictive distribution of new data.
However, the existing BO-CPD model (Adams and MacKay, 2007) specifies the conditional prior on the change point P ( r t | r t  X  1 ) in advance. This approach may lead to model biased predictions be-cause the update formula highly relies on the pre-defined, fixed hazard rate ( h ). Furthermore, BO-CPD is incapable of incorporating external infor-mation that implicitly influences the observation and explains the reasons for the current change of the long-term trend.
 Figure 2: This figure illustrates the recursive up-dates of the posterior probability in the DBO-CPD model. Even the BO-CPD model only uses current and previous run length to calculate the posterior, DBO-CPD can utilize the series of text documents to compute the conditional probability accurately. 2.2 Document-based Bayesian Online This section explains our DBO-CPD model. To represent the text documents, we add a variable D which denotes a series of text documents related to the observed data as shown in Figure 1. Let D t be a set of N t text documents D 1 t ,D 2 t ,...,D that are indexed at time of publication t , where N t is the number of documents observed at time t . Then, we can rewrite the joint probability over the run length as: recent documents. Figure 2 illustrates the recur-sive updates of posterior probability where solid lines indicate that the probability mass is passed upwards and dotted lines indicate the probability that the current run length r t is set to zero.
Given documents D ( r t ) t , the conditional proba-bility is represented as follows: = = where P gap is the distribution of intervals be-tween consecutive change-points. As the BO-CPD model (Adams and MacKay, 2007), we assume the simplest case where the probability of a change-point at every step is constant if the length of a segment is modeled by a discrete exponential (ge-ometric) distribution as: where  X  &gt; 0 , a rate parameter , is the parameter of the distribution.

The update rule for the prior distribution on r t makes the computation of the joint distribution tractable, Because r t can only be increased to  X  + 1 or set to 0 , the conditional probability is as follows: where the function T D ( t, X  |  X   X  ) is an abbrevia-T
D ( t, X  |  X  +1)= P ( r t  X  1 =  X ,D joint probability of the run length r t  X  1 and a set of documents D (  X  ) t when no change has occurred at time t and the run length becomes  X  + 1 . There-fore, we can simplify the equation by removing r t  X  1 =  X  from the condition as follows: We represent the distribution of words by the bag-of-words model. Let D i t be the set of M words that is part of the i th document at time t , i.e. t = { d sume that the probability of word d i,j t is indepen-dent and identically distributed (iid) given a run length parameter r t . In this setting, the conditional probability of the words takes the following form: The conditional probability P ( d i,j t | r t =  X  +1) is represented by two generative models,  X  wf and  X  wi which illustrates word frequency and word im-pact , respectively. The key intuition of word fre-quency is that a word tends to close to a change point if a word has been frequently seen in arti-cles, published when there was a rapid change. The key intuition of word impact is that how much does a word lose information in time which will be discussed in next section. In our paper, we use the unnormalized beta distribution of the weights of words to represent the exponential de-cays. The probability P D (  X  ) t | r t =  X  + 1 can be represented recursively as: where: potentials which contribute to represent P ( d i,j t |  X  ) .  X  wi (  X  ) is explained in Section 2.3. Here, count ( E ) is the number of times event E appears in the dataset. In Equation (9),  X  t is the time gap (dif-ference) between t and the time when a document is generated, and d i,j represents a document with-out considering the time domain.
 T D ( t, X  | 0) is represented as follows: where H (  X  ) is the hazard function (Forbes et al., 2011), Figure 3: This figure illustrates how our Equa-tion (9) is calculated and how it determines whether a change occurs or not. If the same data is given, BO-CPD gives us the same answer to a question whether an abrupt change at time t is a change point or not. However, DBO-CPD uses documents D  X  t for its prediction to incorporate the external information which cannot be inferred only from the data.
 When P gap is the discrete exponential distribution, the hazard function is constant at H (  X  ) = 1 / X  (Adams and MacKay, 2007).

As an illustrative example, suppose that we found a rapid change in Google stock three days ago. Today at t = 3 , we want to know how the articles are written and whether it will affect the change tomorrow ( t = 4 ). As shown in Figure 3, we can calculate what degree a word, for example rises or stays , is likely to appear in articles pub-lished since today, which is P ( D (  X  ) t | r t =  X  +1) , and this probability leads us to predict run lengths from the texts. Documents for each  X  t = 0 , 1 and 2 are generated from the generative models with a given predicted run length through recursive cal-culation of the Bayesian models which enables on-line prediction as shown in Equation (9). This is the main contribution of this paper that enables DBO-CPD to infer change points accurately with information included in text documents. 2.3 Generative Models Trained from cles which consist of M vocabulary over time do-main T . D i t  X  R M is the i th document of a set of documents generated at time t , and define r  X  R N as the corresponding set of the run length, which is a time gap between the time when the document is generated and the next change point occurs. Then, given a text document D i t , we seek to predict the value of run length r by learning a parameterized function f : where w  X  R d are the weights of text features for d From a collection of N documents, we use linear regression which is trained by solving the follow-ing optimization problem: min where r ( w ) is the regularization term and  X  ( w , D i t , r t ) is the loss function. Parameter C &gt; 0 is a user-specified constant for balancing r ( w ) and the sum of losses.

Let h be a function from a document into a vector-space representation  X  R d . In linear regres-sion, the function f takes the form: where is Gaussian noise.

Figure 4 illustrates how we trained a linear re-gression model on a sample article. One issue is that the run length can not be trained directly. Suppose that we train r 5 = 0 into regression, the weight w of the model will become 0 even though the set of words contained in D j is composed of salient words which can incur a possible future change point. To solve this inter-pretability problem, we trained the weight in the inverse exponential domain for the predicted vari-able, predicting e  X  r t instead of r t . In this setting, the predicted run-length takes the form: By this method, the regression model can give a high weight to a word which often appears close to change points. We can interpret that highly Figure 4: This figure illustrates a graphical rep-resentation of how we train a generative model from a regression problem. We use a regression model to predict time gap r t between the release date of article and the nearest future change point. The weights of regression model are changed into the negative exponential scale to be considered as word impact . weighted words d are more closely related to an outbreak of changes than lower weighted words.
With w , we can rewrite the probability of d, X  t given w as: The potential,  X  wi , can also be represented recur-sively as follows:  X  wi ( d, X  t +1 ) =  X  wi ( d, X  t )  X  exp(  X  1 / w d ) , (16) since given a word d ,  X  t +1 =  X  t +1 holds. Now we explain experiments of DBO-CPD in two real-world datasets, stock prices and movie rev-enues. The first case is the historical end-of-day stock prices of five information technology corpo-rations. In the second dataset, we examine daily film revenues averaged by the number of theaters. 3.1 Datasets In the stock price dataset, we gather data for five different companies: Apple (AAPL), Google (GOOG), IBM (IBM), Microsoft (MSFT), and Facebook (FB). These companies were selected because they were the top 5 ranked in market value in 2015.

We chose these technology companies because the announcement of new IT products and features and the interests of public media tend to be higher and lead to many news articles. We use the his-torical stock price data from the Google Finance Table 1: Dimensions of the datasets used in this paper, after tokenizing and filtering the news ar-ticles.  X  :N  X  means the articles are collected with additional  X  X ASDAQ: X  search query.

The second dataset is a set of movie revenues averaged by the number of theaters for five months from the release date of film. We target 5 different movies: The Dark Knight (KNGHT), Inception (INCPT), The Avengers (AVGR), Frozen (FRZ) and Interstellar (INTRS), because these movies are on highest-grossing movie list and also are screened recently. The cumulative daily revenue per theater is collected from  X  X ox Office Mojo X  (www.boxofficemojo.com).

News articles are collected from Google News and we use Google search queries to extract spe-cific articles related to each dataset in a specific time period. During the online article crawling, we store not only the titles of articles, HTML doc-uments, and publication dates, but also the num-ber of related articles. The number of articles is used to differentiate the weight of news articles during the training of regression. In the case of stock price data, we use two different queries to decrease noise. First, we search with the company name such as  X  X oogle X . Then, we use queries spe-cific to stock  X  X ASDAQ: X  to make the content of articles to be highly relevant to the stock market. In case of movie data, we search with the movie title with the additional word  X  X ovie X  to only col-lect articles related to the target movie.

With these collected articles, we used two ar-ticle extractors, newspaper (Ou-Yang, 2013) and python-goose (Grangier, 2013), to automate the text extraction of 291,057 HTML documents. Af-ter preprocessing, we could successfully extract texts from 287,389 (98.74%) HTMLs. 3.2 Textual Feature Representation After extracting texts from HTMLs, we tokenize the texts into words. We use three different tok-enization methods which are downcasing the char-acters, punctuation removal, and removing En-glish stop words. Table 1 shows the statistics on the corpora of collected news articles.

With these article corpora, we use a bag-of-words (BoW) representation to change each word into a vector representation where words from ar-ticles are indexed and then weighted. Using these vectors, we adopt three document representations, TF, TFIDF, and LOG1P, which extend BoW rep-resentation. TF and TFIDF (Sparck Jones, 1972) calculate the importance of a word to a set of doc-uments based on term frequency. LOG1P (Kogan et al., 2009) calculates the logarithm of the word frequencies. 3.3 Training BO-CPD As we noted earlier, we use BO-CPD to train the regression model to learn high weight for words which are more related to changes. When we choose the parameters for the Gaussian Process of BO-CPD, we try to find the value which makes the distance of intervals between predicted change points around 1-2 weeks. This is because we as-sume that the information included in the articles will have an immediate effect on the data right af-ter it is published to the public, so the external information in texts will indicate the short-term causes for a future change.
 For the reasonable comparison of BO-CPD and DBO-CPD, we use the same parameter for the Gaussian Process in both models. After several experiments we found that a = 1 and b = 1 for the Gaussian Process and  X  gap = 250 is appropri-ate to train BO-CPD in the stock and film datasets. We separate the training and testing examples for cross-validation at a ratio of 2 : 1 for each year. Then we train each model differently by year. 3.4 Learning the strength parameter w from The weight w of the regression model gives us an intuition of how a word is important which affect Table 2: Negative log likelihood of five stocks (Apple, Google, IBM, Microsoft, and Facebook) without and with our model per year from 2010 to 2014. DBO-CPD I represents the experiments without  X  X ASDAQ: X  as a search query and DBO-CPD II is the result of articles searched with  X  X ASDAQ: X . Facebook data is not available be-fore the year 2012. to the length of the current run. With the predicted run length calculated in Section 3.3, we change the run length domain r  X  R into 0  X  r  X  1 by pre-dicting e r t rather than r t to solve the interpretabil-ity problem. Therefore, we can think of a high weight w i as a powerful word which changes the current run length r to 0 . To maintain the scala-bility of w , we normalize the weight by rescaling the range into w  X  [  X  1 , 1] . With the word rep-resentation calculated in Section 3.2, we train the regression model by using the number of relevant articles as the importance weight of training. 3.5 Results We evaluate the performance of BO-CPD and DBO-CPD by comparing the negative log likeli-hood (NLL) (Turner et al., 2009) of two models at time t as: We calculate the marginal NLL by year and the re-sults are described in Table 2 and Table 3. (Face-book data is not available before the year 2012.) The difference between DBO-CPD I and DBO-CPD II is whether the search queries include  X  X ASDAQ X . In stock data sets of 5 years, our model outperforms BO-CPD in Apple, Google, IBM, Microsoft dataset. The improvements of DBO-CPD compared to the BO-CPD is statisti-cally significant with 90% confidence in the four stocks except for stock of Facebook. We also found that most of the DBO-CPD II shows bet-ter results than DBO-CPD I and BO-CPD in most datasets due to noise reduction of texts through the additional search query  X  X ASDAQ: X . Out of 23 datasets, APPL in 2010 and FB in 2012 are the only datasets where NLLs of BO-CPD is smaller (better) than NLLs of DBO-CPD.

One of the advantages of using a linear model is that we can investigate what the model discov-ers about different terms. As shown in Figure 5, we can find negative semantic words such as vi-cious , whip , and desperately , and words represent-ing the status of a company like propel , innova-tions , and grateful are the most strongly-weighted terms in the regression model. We analyze and vi-sualize some change points where NLL of DBO-CPD is lower than NLL of BO-CPD. The results are shown in Figure 6 and three sentences are the top 3 most weighted sentences in the regression model for two changes with the boldface words of top 5 strongly weighted terms like the terms big , money , and steadily . A particularly interest-ing case is the term earth which is found between Jan. 25 and Feb. 13 in 2013. After we investigated articles where the sentence is included, we found that Google announced a new tour guide feature in Google Earth on Jan. 31 and after this announce-Table 3: Negative log likelihood (NLL) of five movies (The Dark Knight, Inception, Avengers, Frozen, and Interstellar) without and with our model for 1 year from the release date of each movie. ment the stock price increased. We can also find that the word million is also a positive term which can predict a new change in the near feature. In this paper, we propose a novel generative model for online inference to find change points from non-stationary time-series data. Unlike previ-ous approaches, our model can incorporate exter-nal information in texts which may includes the causes of signal changes. The main contribution of this paper is to combine the generative model for online change points detection and a regres-sion model learned from the weights of words in documents. Thus, our model accurately infers the conditional prior of the change points and auto-matically explains the reasons of a change by con-necting the numerical sequence of data and textual features of news articles. Our DBO-CPD can be improved further by incor-porating more external information beyond docu-ments. In principle, our DBO-CPD can incorpo-rate other features if they are vectorized into a ma-trix form. Our implementation currently only uses the simple bag of words models (TF, TFIDF and LOG1P) to improve the baseline GP-based CPD models by bringing documents into change point detection. One possible direction of future work would explore ways to fully represent the rich in-formation in texts by extending the text features and language representations like continuous bag-of-words (CBOW) models (Mikolov et al., 2013) or Global vectors for word representation (GloVe) (Pennington et al., 2014).
 This work was supported by Basic Science Research Program through the National Re-search Foundation of Korea (NRF) grant funded by the Ministry of Science, ICT &amp; Future Planning (MSIP) (NRF-2014R1A1A1002662), the NRF grant funded by the MSIP (NRF-2014M2A8A2074096).

