 Clustering is one of the most important and fundamental techniques in data mining, information retrieval, and knowledge management. Most clustering tech-niques rely on the pairwise distances between data items. However, it is commonly believed that pairwise distances in the high-dimensional space is not informative, and the nearest neighborhood is not meaningful either [2]. As a result, many learning algorithms (including clustering methods) lose their algo-rithm effectiveness for high dimensional cases.

Recently, semi-supervised clustering has shown effectiveness in improving clus-tering accuracy by exploring  X  X eak X  supervision in the form of pairwise  X  X ust-link X  or  X  X annot-link X  constraints [1,3,5,9]. That is, if data items a and b are must-linked/cannot-linked, then a and b belong to the same/different cluster(s). Two basic semi-supervised approaches are metric learning that learns a distance measure based on constraints, and constraint enforcement that enforces con-straints by modifying the objective function of a clustering method. However, most existing semi-supervised clustering methods have difficulties in handling data with high-dimensional sparse features. For example, in order for a metric-learning method [4,10] to train a distance measure, the number of independent variables to be learned is proportional to the dimension of the feature space. For data with hundreds or thousands(e.g., text data) features, metric learn-ing is computationally expensive. Besides, due to the sparseness, only a small portion of total features are covered by constrained data items. Therefore, train-ing a distance measure for sparse featur es is not effective. For another example, graph-based methods can usually handle high-dimensional data better since they work on the low-dimensional representation (affinity matrix) of the high dimen-sional data. However, the performance of a graph-based method partially relies on the affinity matrix, which is built upon pairwise distances. Since the pair-wise distances in high dimensional space is not informative, the performance of graph-based method is impaired.

In this paper, toward these challenges, we propose two methods to tackle the high-dimensional sparse feature space problem with the help of pairwise con-straints. User-provided constraints r eflect user X  X  expectation of how the data set should be clustered. Therefore, constraints define a rough clustering struc-ture to a data set. The first method seeks a low-dimensional representation of the data through orthogonal factorizations. The clustering structure defined by prior knowledge is kept and strengthened in the subspace. Metric learning is then performed in the subspace. The second method does space-level generaliza-tion of pairwise constraints by local constraints propagation. Both methods can construct more informative pairwise distances for high-dimensional data. Our proposed schemes of exploiting constraints can be applied to any unsupervised clustering model and any high-dimensional data set. We apply the schemes to the widely used Normalized Cut method (NC), and the document data sets to demonstrate the idea and concept. Experimental results base on real data verify the effectiveness and efficiency for our proposals. 2.1 Metric Learning in Struc ture-Preserving Subspace This approach is to construct more informative distances between data items through metric learning in the reduced -dimension subspace. The motivation is obvious. In the much reduced-dimension subspace, features are not sparse. Therefore, metric learning is more effective. In addition, since the number of vari-ables to be learned is significantly reduced, metric learning is also more efficient. Most importantly, we argue that user-provided constraints define a clustering structure that best satisfies the user X  X  requirement. If we find a low-dimension representation of data where the clustering structure is more evident, we can expect more informative distances metric to be learned in the subspace.
We now introduce how to find such a structure-preserving subspace. Suppose we have n data items in the full space descr ibed by a matrix of column vectors W  X  R f  X  n , where the feature space is of f dimensions ( f n ). Given pairwise constraints, we first do transitive closure to the constraints and generate d small data groups, where the i -th group with m data items represented by matrix W each groups as column vectors. The above two steps incorporates constraints information into the data representation C . We now seek a data projection by splitting the feature space of C , which is the same as the feature space of W into two parts, attributes and noise. T hat is, we seek a projection matrix P = UV  X  R f  X  f , U  X  R f  X  r and V  X  R f  X  s , such that P T C = where r + s = f . Suppose C is the r -dim attribute part, and C  X  is the s -dim noise, a desired projectio n satisfies that C is orthogonal to C  X  and C  X  = V T s C =0, which means that the structure-irrelevant noise that existed in the full space is now removed by the projection, and onl y relevant dimensions are kept in the reduced space. Since we only care about the attributes part, all we need to find is the projection U . This projection can be found by computing the orthonormal basis U  X  R f  X  r for the column space of C ,where rank ( C )= r .Itiseasyto see that V is in fact the orthonormal basis of the left null space of C .The subspace data representation is then derived by projecting data using U ,that is, W = U T W  X  R r  X  n is the reduced r -dim representation of data. We then do metric learning in the reduced space W for informative distances.

Note that, the idea of using centroid to represent a group of data originates from work [7]. However, [7] solves the classification problem, where the number of data groups is fixed to the number of classes and each data group contains a large amount of training data such that the centroid of a group is the rank-1 approximation with less noise. On the contrary, we solve the clustering problem. The number of data groups generated by transitive closure is not fixed and is usually large. Due to the nature of pairwise constraints and the small amount of available constraints, most of the data groups only contain a very small amount of data items (i.e., 1 or 2). The centroids for such data groups may contain spurious information.
 It is easy to see that the sparse-feature problem is solved in the subspace. This is because that U is a full rank matrix and rank ( U )= rank ( C )= r , rank ( W )= r&lt;n f . The subspace thus provides a more compact data representation than the original full dimensional space. The number of variables to be learned is also greatly reduced. Now, we use the following two Lemmas 1 to show that the clustering structure defined by constraints is more evident in the subspace W . Proofs are straightforward and thus skipped.
 Lemma 1 (Group Volume Shrinkage Property). Given any data item w i Since data items in the subspace W get closer to their corresponding centroid centroid.
 Lemma 2 (Constant Center-to-Center Distance). Thepairwisedistance between any two given centroids c i and c j of the full space W is strictly preserved in the subspace W : c i  X  c j 2 =  X  c i  X   X  c j 2 . According to Lemma 1, data items in the subspace W move towards their corre-sponding centroids. According to Lemma 2, any data group W i keeps constant distance away from any other group W j in the sense of consta nt center-to-center distance. Geometrically, the volume of a data group W i shrinks and groups are still well separated in the subspace. Theref ore, the projection in fact strengthens the clustering structure defined by constraints. 2.2 Constraint-Guided Local Propagation Graph-based methods are well known for clustering high-dimensional data with better accuracy. For example, the repre sentative Normalized Cut (NC) method has been successfully applied to image se gmentation and document clustering problems. However, as we mentioned in section 1, the performance of a graph-based method can be impaired by noninformative pairwise distance. We propose a simple yet effective method to directly enforce and propagate constraints on the affinity matrix K . The idea is to do space-level generalization of pairwise constraints based on triangle geometry.

Our idea is the following. Given pairwise constraints that data item x and y must belong to the same cluster (i.e., must-link), we set the distance between the two items as 0, that is dist ( x, y ) = 0. For any other data item z ,weset dist ( z, x )= dist ( z, y )= min( dist ( z, x ) ,dist ( z, y )). Symmetri-cally, given pairwise constraints that data item x and y belong to differ-ent clusters (i.e., cannot-link), we set dist ( x, y ) = 1, where 1 is the largest value for a normalized distance mea-sure. Since constraints are propagated to at most one-hop neighbors of the constrained data items, we consider this method local . After local propagation, the matrix K contains more informa-tive pairwise distances, which enable be tter clustering performance. The follow-ing Lemma justifies our idea. Again, proof is straightforward and thus skipped. Lemma 3 (Distance propagation property). Given three data items x , y , and z , suppose dist ( z, x )  X  dist ( z, y ) .Ifdataitems x and y get closer, the 3rd item z is equally far away from x and y ,Thatis, dist ( x, y ) 0 ,then dist ( z, y ) dist ( z, x ) .
 The effectiveness of this method can be illustrated by Figure 1. Unsupervised clustering methods ignore the band structure of the data. If we know that data item a and b belong to the same cluster, and set dist ( d, a )= dist ( d, b ), dist ( c, b )= dist ( c, a ), data items in the upper band will get closer to each other and the band effect is reduced. That is, constraints on data items a and b are generalized to the whole space. The local propagation method has the time com-plexity of O ( nq ), where n isthenumberofdataitemsand q is the number of constrained data items. It is faster than the related global propagation method [6], which is based on all-pairs-shortest-path and has the O ( n 2 q ) time complexity. 3.1 Set-Up Data Sets. We have evaluated the performance of our clustering algorithms using two public available data sets: the Reuters -21578 document corpus and the 20-Newsgroups 18828 version document corpus. For the Reuters corpus, we included only documents with a single label to ensure unambiguous results.We pre-processed each document by tokenization, stop-words removal, and stem-ming. Terms that appear in only one document are removed. 12 data sets were generated from the two corpora as summarized in Table 1. Without the lose of generality, we first generated data sets of different number of clusters ranging from 2 to 6. For each given cluster number k , 5 test sets were created by first randomly picked k topics from one corpus, and then 40 docu-ments of each of the picked topics were randomly selected and mixed together (Table 1, Reu-2  X  6, News-2  X  6 report the statistics of the data sets randomly chosen from the pool of 5 data sets for each cluster number k ). We also created two challenging data sets from 20-Newsgroups corpus. News-Mediocre contains and News-Difficult contains 3 very similar topics { comp.windows.x , comp.os.ms-windows.misc ,and comp.graphics } .
 Evaluation Metrics. In addition to the running time (RT) as a metric to evaluate the speed of algorithms, to avoid biased accuracy result using a single metric, we evaluate clustering accuracy by employing three widely-used evalu-ation metrics, which are (1) Normalized Mutual Information (NMI), (2) Rand Index ,and(3) F-measure . All the three metrics take values between zero and one, with one meaning best accuracy.

We also implemented four state-of-the-art semi-supervised clustering meth-ods: (1) L-NC method does metric learning in the full feature space [10], then uses NC as the unsupervised clustering model; (2) MPCKmeans combines met-ric learning and constraint-enforcem ent into K-means through an EM process [3]; (3) C-NC is a graph-based constraint-en forcement method that has been shown effective in clustering documents [5]; and (4) Glo-NC method globally propagates constraints to adjust pairwise distances [6]. Table 2 summarizes the baseline method and seven variations of semi-supervised clustering methods that we have evaluated. Our three proposals  X  RL-NC , RLC-NC ,and Lo-NC  X  are bold faced. 3.2 Experimental Results Metric Learning. We reported the experimental r esults based on two challeng-ing data sets News-Mediocre and News-Difficult. We controlled the experiment by varying the amount of constrained data items ranging from 2.5% to 15% of the total documents. Constraints were generated by paring constrained documents based on ground truth.The final performance score was obtained by averaging the scores from 10 test runs.

We compared our sub-space metric lea rning method (RL-NC) with the full-space learning method (L-NC). Performance comparisons are reported in Table 3. For both data sets and various amount of constraints, RL-NC achieves higher and more stable learning a ccuracy. Metric learning in the subspace is also much faster than in the full feature space. When the amount of constraints increases, the learning time of both methods increases too, with the subspace learning method scales much better than the full space learning method. Last, note that although the three accuracy metrics (i.e. NMI, RI, and F) show quite different absolute val-ues, they show overall similar patterns for different algorithms. For simple presen-tation, we will only use NMI as the evaluation metric from here forward. Integrating Metric Learning with Constraint Enforcement. Metric learn-ing and constraint enforcement are two basic schemes of exploiting constraints to improve clustering performance. In this experiment, we combined the two schemes and expected to generate better cluster ing performance. We compared the sub-space metric learning method (RL-NC), with the graph-based constraint enforce-ment method (C-NC), and with the hybrid approach ( RLC-NC ). Figure 2 shows that both RL-NC and C-NC individually improved upon the regular NC method, and both show similar accuracy given the same amount of constraints. However, the hybrid algorithm, RLC-NC, always significantly outperforms the individual ones. These results empirically validate our hypothesize that the hybrid method, RLC-NC, more comprehensively utilizes available constraints.
 Constraint-Guided Local Propagation. We compared the effectiveness of the local propagation method (Lo-NC) with the global adjustment method (Glo-NC). The regular NC approach was adopted as the unsupervised clustering model for both methods. Figure 3 shows the clustering results on the two rep-resentative Newsgroups data sets. Lo-NC made sizable improvement over the unconstrained version of NC even with a small amount of constraints. Glo-NC is less effective and its performance for the News-Difficult data set is worse than the unconstrained method when the number of constraints increases. Both the local and the global methods exploit the triangle geometry. But the global method also propagates constraints based on the pairwise distances among all the data points, which may decrease the discriminative power of the constraints. Detailed results on multiple Reuters and Newsgroups data sets are shown in Table 4. Two novel semi-supervised clustering techniques are proposed for high dimen-sional and sparse data. The first method projects data onto a reduced-dimension subspace such that clustering structure defined by constraints is strengthened. Metric learning is then applied to the subspace to generate informative pair-wise distances. The second method exploi ts the triangle geometry to generalize pairwise constraints by  X  X ocal X  propagation. The validity of our proposals are empirically validated usin g extensive experiments.

