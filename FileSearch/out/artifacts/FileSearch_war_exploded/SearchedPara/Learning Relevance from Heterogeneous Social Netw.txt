 The rise of social networking services in recent years presents new research challenges for matching users with interesting content. While the content-rich nature of these social net-works offers many cues on  X  X nterests X  of a user such as text in user-generated content, the links in the network, and user demographic information, there is a lack of successful meth-ods for combining such heterogeneous data to model interest and relevance. This paper proposes a new method for mod-eling user interest from heterogeneous data sources with dis-tinct but unknown importance. The model leverages links in the social graph by integrating the conceptual represen-tation of a user X  X  linked objects. The proposed method seeks a scalable relevance model of user interest, that can be discriminatively optimized for various relevance-centric problems, such as Internet advertisement selection, recom-mendation, and web search personalization.

We apply our algorithm to the task of selecting relevant ads for users on Facebook X  X  social network. We demonstrate that our algorithm can be scaled to work with historical data for all users, and learns interesting associations between concept classes automatically. We also show that using the learnt user model to predict the relevance of an ad is the single most important signal in our ranking system for new ads (with no historical clickthrough data), and overall leads to an improvement in the accuracy of the clickthrough rate prediction, a key problem in online advertising.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining  X 
This work was done while the author was at Facebook.  X  This work was done while the author was at Facebook. Algorithms, Experimentation, Economics Online Advertising, Heterogeneous Social Networks, Cogni-tive Relevance, Clickthrough Prediction Social networking services such as Facebook, Youtube, Twitter and MySpace have a ttracted hundr eds of millions of users and become part of the Internet mainstream and an important part of our daily lives. Compared to tradi-tional interest-based online communities, social networking services offer an additional network layer on top of user inter-est, as they center around people and their connections [6]. Social network users do not usually state their interest ex-plicitly, but instead make connections, including direct con-nections with other users and groups, and implicit connec-tions formed by interactions X  X uch as commenting or click-ing behavior X  X ith user-generated content and other enti-ties in the network, as shown in Figure 1. These connections provide valuable information about a user X  X  interest, but are typically of varying importance and quality. The ability to infer a coherent model of users X  interests from the hetero-geneous parts of the network has the potential to provide personalized recommendation and more relevant advertise-ment targeting [23, 8].

Although the ideas developed in this paper can be ap-plied to recommendation and other content matching tasks, we focus on the specific task of serving relevant advertise-ments to users in a social network. The foundation of pay-per-click advertising systems is a system for predicting the clickthrough rate (CTR) of an ad for a given user or query. Typically, such predictions are based on a machine learning model that uses various hand-crafted features, and is trained on historical click data. Features employed in current social networking sites mainly inherit the keyword-targeting pro-totype that is successful in search engine advertising, and demography-targeted advertising that is prevalent in tra-ditional brand advertising. However, the characteristics of social networking sites are not fully exploited by these methods. On one hand, the large amounts of data about user activity and social context are unique in social network services and offer the opportunity for better inference of per-sonalized interests than traditional methods. For example, Joined Group Text Figure 1: An example of user-ad-source heteroge-neous network. Users are linked to different types of objects. These objects typically have associated text information. Ads are also linked to different type of objects with supporting text. We know some ads are clicked and some are shown but not clicked, for a small fraction of user-ad pairs, and we need to predict clicks for other user-ad pairs. we can use user-generated content as contextual signals for content-based advertising. On the other hand, it is nontriv-ial to utilize these data. First, not all the contents have strong contextual relevance. For example, when users check friends X  news or talk with friends, their intent is usually not related to business. Thus we cannot simply concatenate all the text from linked objects to augment a user X  X  profile. Sec-ond, if we use keyword or concept matching to measure the relevance independently of the targeting problem, simply counting the exact matching concepts or even related con-cepts in the taxonomy cannot capture the latent signal X  X or example, an ad about Photographers 1 is better targeted to users interested in Weddings, rather than users interested in Photography.

In this paper, we formulate and tackle the problem of rel-evance learning for online targeting in heterogeneous social networks. To develop a better relevance model in the con-text of a heterogeneous social network, we propose a frame-work for CTR prediction based on a variant vector space model. First, for dimensionality reduction, we use seman-tically meaningful concepts to abstract the interest of users and the topic of ads, such as  X  X artoon Movie X  and  X  X u-sic Bands and Artists X , and thus both user and ad can be represented by a concept vector where each component is the weight for corresponding topic. Second, rather than a simplified view of a friendship social graph, we use the multi-typed networks to characterize user interaction with pages, groups, and other entities. These nodes have text content from which we can extract concepts, and serve as concept sources to their linked objects. Then user interests and other target content can be summarized from the concept classes of linked source nodes. Finally, to quantitatively distinguish the level of importance of different types of nodes in the network, and to systematically capture the intuition that a
We use capitalized phrases to represent a concept class. user concept may match multiple ad concepts with different association weights, we develop a generic user model to learn the weight for heterogeneous linked sources and the associ-ation between every pair of user concept and target concept jointly. The model can be applied on top of any concept extraction method.

We demonstrate a series of offline and online experiments that validate our hypothesis: 1) our model is capable of finding hidden associations between user concepts and ad concepts, such as Photographers with Weddings; 2) finding concept class associations and source importance weighting mutually enhance each other in terms of resolving the mis-matching between user concept space and ad concept space; and 3) demography and keywords matching help with tar-geting, but not as precisely as our jointly learnt user model.
Online advertisement targeting and ranking have been widely studied in the context of search-based and content-based targeting, e.g. , Google AdWords and AdSense. Search-based targeting, or sponsored search can be thought of as a document retrieval problem, where the ads are the  X  X ocu-ments X  to be retrieved given a search query. Content-based targeting, a.k.a., contextual advertising, is a sister technol-ogy to sponsored search, where the ad retrieval is based on a webpage. Studies on such models show that the vocab-ulary mismatch problem is critical [9, 20, 7]. Motivated by that, we represent user interests and ads with vectors in a space of concept classes X  X hich is significantly lower di-mensional than the space of possible keywords X  X nd learn a weighted matching function based on click history data. There are only a few recent published studies on user-centric targeting in the context of social network sites, perhaps due to the lack of data from widely-used social network sites. Bao etal. [4] proposed a influence-based diffusion model for targeting on implicit-relationship Q&amp;A websites with little user-generated content. In comparison, we study a social network site with abundant user-generated content.
In terms of modeling user interests for search and recom-mendation systems, most works concentrate on one type of user generated data, while on a heterogeneous social net-work various sources can be used to analyze a user X  X  be-havior. Examples include user interaction history during web search [3], user generated tags [21], and browsing be-havior in quasi-social networks [19]. A recent work by Wen etal. [24] studied users in social networks where there are multiple types of data. However, they assign each source an empirical weight when combining them and study the in-terest independently of the targeting task, while our model learns the weights from history data and can optimize the relevance measure towards different tasks.

Recent researchers also start to leverage social cues to enhance user interest modeling. Most of them augment the interest of one user from other users. Piwowarski and Zaragoza [18], White etal. [25] combine the interests of other users that visit the same page. Likewise, in collaborative fil-tering tasks it is assumed that those who had similar opin-ions on a set of items tend to agree again on other items [13, 14]. Konstas etal. [16] shows that incorporation of friend-ship and social tagging can improve the performance of a music recommendation system. Nevertheless their random-walk-based approach suffer from the scalability issue in large social networks. Distinct with these works, we do not ex-plicitly reply on the friendship link to propagate interests, as previous studies already suggest that explicit relationships are not good indicators of the nature of user interactions[4] and the quality of inferring user interests from their friends varies [24]. Instead, we use links to other entity nodes to propagate interests. These nodes include the interaction with friends such as wallposts, as well as similar behaviors such as liking a page or joining a group. In this way we will not propagate the interests between one and his friends who never have interaction, while the users who share more similar linked objects will have more interest augmentation to each other through the training process.
User interests and ad contents can be extracted by unsu-pervised or supervised methods. Compared to the classification-based methods, unsupervised topic models such as Latent Dirichlet Allocation [5] are not restricted to a predefined set of labels, but are harder to use and modify when working with thousands of possible topics. With the development of knowledge representation language [11], domain specific ontologies can be constructed and used to label web con-tent according to a concept taxonomy. Supervised learning based on a reference ontology can achieve a good semantic representation of user interests [22, 10], and has the advan-tages that it is scalable and the results corresponding to a predefined ontology are easy to interpret.
We consider the task of predicting the clickthrough rate for a user on a target object recommended to the user. In our discussions, we assume the target object to be an ad. Formally, our input is a heterogeneous network G =( V,E ). The set of vertices V can be partitioned into user nodes V ad nodes V a and their various linked source nodes: V = V are the respective sources. Each user node v x  X  V u can be linked to one or more source nodes in  X  u i , 1  X  i  X  n each ad node v y  X  V a canbelinkedtooneormoresource node in  X  a j , 1  X  j  X  n a ; and each source node v z  X   X  has some text associated with it. In principle we can define source types based on both object type and link type. In this work, we regard each type of object as a type of source, and assume there are two types of links, positive and nega-tive, e.g. ,  X  X ike X  X nd X  X islike X . The linked sources accordingly have positive or negative contributions to the concepts of the linkeduseroradnode. Forsomeuserandadnodepairs, we know the existence of past impressions and whether the user clicked the ad. For any given pair of user node and ad node, our task is to predict the probability that the user will click on the ad.

Our output will serve as a feature in a machine learning model, to be trained with many different features. To enable an ad ranking system to examine the relevance between any user and any ad inside a large-scale social network in real time, we need to represent user interest and ad content in a concise yet semantically meaningful way allowing efficient measure of the relevance. We use clustered concept classes that are more precise cognitive unit than terms to summarize a user X  X  interest. These concepts also serve as a common representation for integrating information across different Figure 2: The pipeline of our relevance feature gen-eration for CTR prediction and the model to learn sources.
 Figure 2 outlines the pipeline for our relevance prediction. We first give some definitions.  X 
Concept Space. A concept space is a d -dimensional real number space R d that encodes some cognitive classifica-tion system such as ontology. Each dimension represents a concept class in the system. For example, in a human-edited Web directory [2], each category in the directory can be used as a concept class.  X 
Concept Vector. A concept vector is a vector in the con-cept space defined above that represents the cognitive property of an object. The component in each dimension indicates how strong the object is associated with the cor-responding concept. For example, (Movie:0.5, Soccer:0.3) is a short representation of a sparse concept vector where all the weights are zero except for two concepts. For generality we allow user concept vector and ad concept vector to be in different concept spaces.  X 
Concept Extraction(CE). Given a source node X  X  associ-ated text, output its concept vector c in certain concept space.  X 
Concept Aggregation(CA). Given a user or an ad v x ,and the set of linked source nodes S x  X  X or each node v i  X  S  X  Concept Matching(CM). Given a pair of user concept vec-
The pipeline of CE  X  CA  X  CM allows user concept vector and ad concept vector be extracted and aggregated offline, and stored in high speed medium. The online feature computation can be efficient with fast access to the concept vector and fast vector matching algorithm. The three mod-ules are tuned in the following manner. First, we choose a supervised concept extraction model. Then we learn the pa-rameters in the rest two models and test them on click data. In this paper we focus on how to learn a good relevance prediction model that manipulates the concept aggregation and matching module for better CTR prediction. Table 1: Examples of the text concept extraction model used for the experiments. There are tens of thousands of possible concepts, and so the output concept vector is very sparse (tens of nonzero di-mensions typically after thresholding).
 Input text Sample concepts with weights
Get low cost car insurance.

Iamcookingfor mom today.
 We first briefly describe our concept extraction method. Then we discuss several baseline methods for relevance pre-diction, followed by our solution. Learning Concepts. The task of concept extraction is to transform given text into the most salient topics that occur in the text. A variety of text modeling approaches are pos-sible here, including categorization approaches that classify text into a pre-defined ontology of topics, and topic modeling approaches that automatically find semantically meaningful clusters [5]. While our methods are applicable to any finite vector representation of users and ads, for concreteness, we present results using a categorization approach. Using la-beled webpage data from a reference ontology, we trained a large-vocabulary, bag-of-words based linear text classifier that accepts an arbitrary text input, and attempts to predict the strength of association with each label in the ontology. We use each label as a possible concept class, and use the predicted strength as the weight on that concept, thus trans-forming any input text into a concept vector. This model is not the focus of the paper, and we simply illustrate the method by examples in Table 1.
 A baseline for Relevance Prediction. Based on the out-put of the concept extraction model above, we have a simple method to aggregate the concept vectors and measure the relevance between users and ads. For each user and each ad, we can use unweighted sum of the concept vectors ex-tracted from all linked sources. Then we compute the cosine similarity of two concept vectors as the baseline feature for prediction.

There are several problems with this baseline feature. First of all, the different type of sources should have different pre-diction power of users X  interests in ads. For example, greet-ings between friends may not be as useful as shared links to a movie or clicks on a previous ad. It is natural to assume that weighted sum of the concept vector from different sources is a better strategy for relevance inference. However, empiri-cal assignment of source weights is not straightforward. It is not obvious whether the group a user belongs to should have higher weight than the posts on the wall. Not everyone publishes status messages, but most have profiles. Different sources have different level of quality, coverage and relevance to business. To find the best weighting we need to learn from the data.

Second, the cosine similarity metric needs to be improved due to the following reasons: 1) different concept may have different power for distinguishing the interest, e.g. ,toplevel node Society VS. leaf node Society/Relationships/Dating; 2) the intuition that one user concept can match multiple ad concepts ( e.g. , the video game FIFA Series can match the computer game or the sport soccer) and vice versa; and 3) user concept space and ad concept space are not necessarily identical. So a better measure should allow one user concept to match different ad concepts and vice versa, and does not require they are in the same concept space. This measure is even harder to handcraft, but can be learnt from data.
Nowwehavetwomoreimprovedmethods,onetolearn source weights and the other to learn a better measure. A natural question is can we combine the two to obtain an even better model, and whether they interact with each other. Our intuition is that better assessment of source importance should help us find a better measure; and the better match-ing function should also facilitate the noise reduction and quality of integration from different sources. Based on that intuition we develop a joint model to integrate the learning of these two parts, and it incorporates all the above means as special cases.
Based on above analysis, we add unknown parameters to the aggregation and matching model. Our relevance model consists of both aggregation and matching, and we will learn the parameters for them together from training data. For aggregation, we combine the user concept vector from each source by weighted summation, where c j is the concept vector from the node v j , S x is the set of source nodes linked with user v x ,and  X  u i is the weight for the i -th type of user source. For simplification we let s i -th type source. Then we have the matrix representation U Likewise the ad concept vector will be A y =( t y 1 ,..., t y n a ). And for concept vector matching we al-low any user concept in user concept space R d u to match any ad concept in ad concept space R d a , with a pairwise weight w i,j . Formally, we have the definition for the normalized bilinear form similarity sim as follows: where u x,i is the i -th component of the vector u x and a is the j -th component of the vector a y .Here . is the L2 normal number of the vector, and W =[ w i,j ] d u  X  d a is the transformation matrix as it transforms a user concept vector u x into ad concept space u x W  X  R d a . W virtually defines a bilinear form from R d u  X  R d a  X  X  X  R .
Substituting Eq. (1) and Eq. (2) into Eq. (3), we have a new relevance feature by using weighted sources and nor-malized bilinear form similarity,
We model the probability that a user x will click on an ad y as a generalized linear model in f ( x, y ). Specifically, using the sigmoid function g ( x )= 1 1+exp(  X  x ) , we assume: We can now collect historical data ( x i ,y i ,z i )onuser x shown ad y i ,with z i = 1 when this resulted in a click (and z = 0 otherwise). We learn the unknown parameters in the model by maximizing the discriminative likelihood of this data: where N is the number of training examples.

We have three sets of parameters to learn: the user source weights, the ad source weights, and the transformation ma-trix. Since our concept extraction model can produce tens of thousands distinct concepts, the above learning problem can have hundreds o f millions of free parameters. Using a stochastic gradient descent algorithm, we can scan the training data once and iteratively update the three sets of parameters one set after another, as shown in Algorithm 1. Given a training example ( x, y, z ) from training data, the error in our prediction is given by z  X  p (user x clicks ad y ), or simply z  X  p . We can derive the following gradient descent update rules: where r k is the learning rate for k -th set of parameters. Intuitive choices are r 0 &lt;r 1 = r 2 &lt;r 3 because the number of parameters in 3rd set is significantly larger than the first two sets.

The optimization problem is convex if we have only one set of parameters W ; in that case it is identical to logis-tic regression. However, when  X  u and  X  a are introduced the objective function is not convex. The gradient descent algorithm converges to a local maximum. The advantage of our stochastic gradient descent algorithm is that a single update is very efficient. We do not need to update all the parameters at each iteration, but just a small fraction of re-lated ones. Assuming the user concept vector and ad concept vector contain l u and l a concepts with nonzero weights, only ( l l a + n u + n a ) parameters need be updated. Eq. (8) and (9) dominate the computation time. By storing W in a big hash table, the u T x Wa y can be computed in O ( l u l a )time. The complexity for training on one sample is O (( n u + n a ) l The algorithm applies to any general concept vector space. It can learn features of user-ad matching even when the user vector and ad vector are in different spaces. For example, to Algorithm 1: Learning without regularization.
 Input :numberofsources n u , n a , user concept space Data : training set T = { ( z x,y , U x , A y ) } Output :  X  0 Result : W ,  X  u ,  X  a Initialize all w i,j  X  0 , 1  X  i  X  d u , 1  X  j  X  d a ;
Initialize all  X  u i , X  a j  X  1 , 1  X  i  X  n u , 1  X  j  X  n
Initialize  X  0  X  c 0 ; repeat 1.1 z  X  z x,y ; 1.2 Update  X  0 ,  X  u ,  X  a , W according to Eq. (7)-(10); until convergence ; learn the relevance between user demography and ad con-cepts, we can let each component of the user vector represent a demography group, and learn the transformation matrix W . Without source weight to learn, the objective function is convex and the algorithm converges to global optimum.
We further suggest regularizing the transformation ma-trix W for two reasons. First, such regularization has been shown to reduce overfitting and improve generalization to future data, especially in problems with very large numbers of free parameters [17]. Second, the online computation for sim is more expensive than the dot product. It takes O ( l time even if we assume the lookup of an element in W takes O (1) time, while the dot product only requires O ( l u + l time if we use the sorted sparse vector as data structure. To optimize it we can precompute the vector u T x W and u x for every user, and just do dot product for online computa-tion. Using the L1 regularizer provides a principled way to reduce the number of nonzeros in W .

With L1 regularization term, we now minimize the follow-ing objective function: where large values of C&gt; 0 produce fewer number of nonze-ros in W , at the potential cost of modeling power.
As before, we can use gradient descent algorithm, with an extra step to clip values at zero. The update rule becomes
Algorithm 2 shows the new online learning algorithm. We partition the training data to m chunks, and do the regu-larization after every k = N/m examples. The complexity depends on the number of nonzero terms in W ,say n w .The running time for training on N samples with m chunks is O ( N ( n u + n a ) l u l a + mn w ). In our data l u ,l a constants in the same order of magnitude as 10. n w remains almost constant, although in the order of magnitude 10 6 .So the training time grows linearly with the sample size with these constraints. Algorithm 2: Learning with an L1 regularizer Input :#ofsources n u , n a , user and ad concept space Data : training set T = { ( z x,y , U x , A y ) } Output :  X  0 Result : W ,  X  u ,  X  a Initialize all w i,j  X  0 , 1  X  i  X  d u , 1  X  j  X  d a ;
Initialize all  X  u i , X  a j  X  1 , 1  X  i  X  n u , 1  X  j  X  n
Initialize  X  0  X  c 0 ;
Separate training set into k chunks of m examples; repeat until convergence ;
We conduct experiments on Facebook X  X  large-scale online advertising system. First, we do preliminary analyses on the baseline relevance feature, and show its potential and problems in clickthrough prediction. Then we use our model to learn variant relevance features, compare them and ver-ify our assumptions with a series of performance studies and decompositional analyses. Finally, we test the performance of our feature when working together with other features and show its improvement to offline clickthrough prediction. Due to business confidentiality, we report only relevant per-formance when showing experimental results. Data Sets. We test our approaches on a sample of ads clickthrough data recorded on facebook.com .Wesample historical click data anonymously from a small fraction of the 500 million monthly active users, and use the ad impres-sions seen by these users for training. Users are connected with about 50 different kinds of objects that are treated as the concept sources; ads have 4 concept sources in our setup. We use only non-private sources from users for con-cept extraction X  e.g. , chat logs or user messages are never looked at. On average a user is connected to 80 community pages, groups and events, and create 90 pieces of content each month [1]. For offline analysis, we use anonymized click data from the user sample, gathered over one week. Since the positive examples (clicks) have low rates and can lead to biased estimates [15], we reduce the imbalance with a downsampling strategy similar to previous work [7]. Concept Extraction Model. For our experiments, we use the publicly available topic hierarchy in DMOZ Open Directory Project [2] as labeled concept classes, and use the documents linked to each class to train a concept classifier. The classifier is named ODP. The second concept extrac-tion model is a pseudo concept model that uses demography group instead of classified concepts for user concept space (while using the ODP concepts for the ads). We use it to show the generality of our learning algorithm. ODP has tens of thousands concept classes in total, and DEMO contains hundreds of demography groups, classified by country, age, gender, education, political stand and relationship status. For ODP, the concept vectors are high-dimensional though sparse, leading to millions of learning parameters to be learnt in the transformation matrix W .
 Relevance Model. We have presented algorithms for two sorts of learnt parameters: the weights  X  a ,  X  u on each heterogeneous source, and the transformation matrix W . As a demonstration, we compare with our relevance feature against various baselines:  X  f : Our relevance feature using weighted sources + learnt pseudo bilinear form similarity.  X  cos : Baseline model with unweighted sources + a plain cosine similarity feature, or in other words, a uniform diagonal transformation matrix.  X  wcos : Weighted sources + cosine similarity .  X  f unweighted : Unweighted sources + learnt similarity.  X  f sparse : Weighted sources + transformation matrix learnt
For each baseline, the parameters (source weights or trans-formation matrix) were relearnt by adding the relevant con-straint to the optimization problem.
We first explore the value provided by the baseline fea-ture cos in CTR prediction. Using labeled ads impression data, we compute the baseline feature and bucketize the impressions according to feature values. The i -th bucket (0  X  i&lt; 100) contains all the impressions with feature value cos  X  [ i 100 , i +1 100 ). For all the impressions in each bucket, we plot the average CTR divided by average esti-mated CTR(ECTR), which is given by an existent rank-ing system without cognitive relevance feature. A perfect system should always have CTR/ECTR=1. The farther CTR/ECTR is from 1, the more the estimate should be ad-justed. We see from Figure 3(a) that if we use unweighted sources and exact concept match, the feature value has a positive correlation with CTR/ECTR. It suggests that tun-ing up and down the CTR estimation for high and low rel-evance feature respectively can improve the accuracy.
We repeat this for the feature cos obtained by using user concept vectors from each single source. Figure 3(b) shows the regression trend of CTR/ECTR with cosine similarity. The larger slope indicates a stronger prediction power for that source. The prediction power varies a lot from source to source, and unweighted combination of all sources is even worse than using a certain source in this rough estimate. Figure 3(c) shows the coverage of the cosine similarity for all user-ad impressions during a week. As we can see from Figure 3(c), for each source no more than 40% of the total impressions have at least one matching concept; even we use all sources, only half of total impressions are X  X overed X  X n this sense. The low coverage of nonzero similarity values provides another motivation to improve the similarity metric beyond the analysis in Section 4.1, while the observation that every (a) Cosine similarity pivots the correc-tion to the estimated CTR; unweighted summation causes noisy pivoting effect Table 2: logloss(*)/logloss( cos ) for different rele-vance feature, where logloss is the deviation w.r.t. randomness. Higher values indicate that the model does a better job in predicting clickthroughs from input concept data.
 Iteration logloss deviation wrt randomness (ratio to cos ) 100K 2.6 4.4 5.6 5.6 200K 2.8 5.7 6.9 6.9 500K 2.7 6.7 8.1 8.0 1M 2.9 7.7 9.5 9.4 2M 2.9 8.8 10.9 10.8 test 3.8 11.2 14.3 13.7 source has unequal power and coverage suggests we should assign different weights to different sources.
This section presents the results of our learning algorithm for source weights and transformation matrix. First we compare the error reduction of by variant relevance models on the same test set. We then analyze the learnt model pa-rameters to interpret the benefits of our method, and present interesting discoveries.
We applied the algorithms described in Section 4.2 to clickthrough data collected anonymously over 4 days from 1 million sampled users, and test on the next 3 days. In our implementation, we used a fixed learning rate, and chose other algorithm parameters using validation error.
Table 2 shows the percentage logloss reduction obtained by our algorithm (left) and various baselines, where the logloss reduction is measured compared to random guess-ing with the empirical click rate c 0 , and displayed as a ra-tio of the logloss reduction achieved by cos. The baseline feature cos is only slightly better than random guessing, due to its low coverage and the lack of ability to match distinct but related concept classes. The cosine similarity with weighted sources ( wcos ) has as 4 times large logloss im-provement as unweighted sources, but has the same coverage Figure 4: Weights of concept source for two settings: cosine similarity and normalized bilinear form simi-larity. problem. The normalized bilinear form similarity can solve that problem and the logloss reduction is 10-fold larger than cos . With our jointly learnt model, the logloss reduction can be further improved, gaining a 14-fold of cos . Finally, the learning results with sparse coding demonstrate that we can reduce the number of nonzero parameters we need to store by 73-80%, thus reducing the overheads of feature computa-tion, with only a small effect (within 5%, relatively) on the observed logloss performance. That speeds up the online feature computation by 53 times.
The learned weights of concept sources for cosine simi-larity and the learnt bilinear form similarity are shown in Figure 4. Some sources are entitled nearly zero weight due to extremely low coverage, and we omit it and only show the major sources, 6 for users (the ads they clicked before, the pages they are linked to, the groups they join, their pro-file interests, status updates and wallposts) and 4 for ads (descriptions created by the advertiser, linked pages outside Facebook, Facebook pages and keywords for targeting).
It is not surprising that  X  X licked ads X  rank high as a user source, because it is the target we want to predict. We also observe that page concepts get higher source weight than (a) #nonzero weights/row Figure 5: Nonzero elements distribution in the transformation matrix, y axis in log scale group concepts, which might imply that the groups are not necessarily formed by interests, while pages better reflect the collective interests of their users. For ad concepts, the dominant source is the ad creative text, the visible part to the user. However, the other sources add extra infor-mation, beyond what is achievable just using the creative text. For ads that link to a known onsite object (e.g., ads about a music artist), the concepts for that object strongly indicate good ad concepts. Further, important information can be gleaned by looking at the landing page for the ad, as well as the targeting criteria provided by the advertisers. Taken together, these results validate our hypothesis that important semantic information can be obtained by looking at many heterogeneous sources of information, even though the sources have varying levels of noise.

An interesting observation is that if we change the con-cept matching function, the importance of sources varies qualitatively. For example, if we only allow exact concept matching with cosine similarity, some textual content such as wallposts are noisy and are assigned low source weights, indicating that exact concept match is not appropriate for them. However, the improved similarity metric can handle this problem by changing the association weight between the implicit interests and related classes of ads. When we allow matching across concept classes with our transformation ma-trix, we are able to use all of these sources successfully.
To better understand the output of the learning algorithm, we now examine the learned transformation matrix W .The i -th row of this matrix corresponds to the i -th user concept, and the j -thcolumntothe j -thadconcept. Theweight on ( i, j )isusedtomatchthe i -th user concept to the j -th ad concept. For ODP, the learned matrix is sparse and contains 3% (which is already millions) nonzero parameter values, with extremely skewed density in various rows and columns of the matrix. The number of nonzero weights in each row and each column both satisfy power-law distribu-tions (Figure 5). On average, each row and column contains several hundred nonzero values X  X hich means that a user or ad concept can potentially match several hundred concepts.
Figure 6 visualizes three 5-by-4 matrices to show the typ-ical difference between the transformation matrix learned with unweighted sources and weighted sources for ODP. The 4 columns divide the association into 4 groups. In each group we fix the ad concept class, and select 5 user concept classes whose association weight with that ad concept disagree in the two cases. Along each column, say column A ,wehave 5 cells now corresponding to 5 user-ad concept pair from ac uc 1 Music Music Arts Music/ Styles uc 2 Christianity Society Animation Movies Society uc 3 Movies Video Games Humor Humor uc 4 Music/ Styles uc 5 Parenting Weblogs Music/ Styles Movies Animation Movies Figure 6: The different transformation matrices for unweighted sources and weighted sources. 4 groups of user-ad concept association are shown by gray scale, black=1, white=-1. In each group 5 associate user concepts for a fixed ad concept are chosen. ( uc A 1 ,ac A )to( uc A 5 ,ac A ). Each cell in the grid is dyed ac-cording to the association weight. Dark color means positive association and light means negative. See the table below the figures for a map. In general, the association weight given by f looks more meaningful because it amplifies the concepts from cleaner source and shrinks the concepts from noisier source. For example, the interests in music have nothing much to do with ads of trading card games or per-sonal pages with rabbits ( A 1 ,B 1 ,A 4 ,B 5); the interests in animation movies and humors apparently signal a tendency to click ads about Shrek Series ( C 2 ,C 3). This suggests that transformation matrix can be better learned if the impor-tance of concept sources are better distinguished.
From the analyses from above two sections, we conclude is that source weighting and non-exact matching mutually enhanceeachotherandanintegratecombinationofthem lead to higher quality of prediction. Thus our algorithm for learning them jointly, instead of separately picking the source weights and the matrix W ,isexpectedtobecrucial.
The values in the learned transformation matrix capture various statistical associations between the user and ad con-cept classes. Table 3 shows some extreme cases from which we can interpret the user-ad relevance captured by the ma-trix. Eight categories of such cases are studied. We present four of them due to space limitation. For each category, we give examples from different concept extraction models. The first category we show is about a single ad concept class which is related to many user concept classes. Turn-based strategic video games are associate with more than 70% user interests, and Christianity denominations relate to almost every demography group. Ads with such concepts have di-versely targeted users. The other 3 categories we show in Table 3 are about concept pairs which have very strong or Table 3: Interpreting the transformation matrix Characteristics CE Example:#related concepts(%) Ad concepts associated with most user concepts Characteristics CE Example: association weight Strong positive associations Strong negative associations Identical but weak associations ODP Table 4: Relative variance reduction achieved when a feature is added to test feature set. High values for important features.

F 0 = user-ad relevance feature 0.12 0.34 0.008 very weak associations. Strong associations could be either positive or negative, e.g. , females favor dance while males like guitar; US people do not like Mahjongg, which is a pop-ular gambling game in Asia. Some user concepts have weak association weight with the identical ad concept. That ac-tually reflects a mismatch when we use cosine similarity X  though the concept of colleges in US can be found in many user pages and profiles, it does not imply that these users will like ads about colleges in US. On the contrary, such ads will attract people who have not been in US colleges.
There are also several hard-to-interpret results, such as the negative association between birthdays and online games. Such results may either reflect unexpected input statistics, or might be influenced by the imperfectness of concept ex-traction, or bias in the training set (as the training samples are generated by an optimized ranking system).
To compare our relevance feature with previously studied or commonly used features, such as keyword-based, clustering-based and friendship-based relevance features, we train new CTR prediction models using two different configurations of features for test purpose. In one configuration (called F ),weincludeallexistingfeaturesinanadrankingsys-tem that capture inferred user-ad relevance (and are thus meaningful even for new ads). This checks the additional relevance value brought by our new feature. In the second configuration (called F 1 ), we additionally include features that capture the bias of different users to click on ads and the past clicking behavior of users. This helps check that the observed gains from our relevance features cannot be explained solely by capturing a user X  X  past clicks with hand-crafted features. In each case, we experiment with training a model with and without our new relevance feature. We evaluate the importance of each feature by variance reduc-tion achieved [12], and the improvement of logloss reduction when each of them is added to the test feature sets. Figure 7: Improvement of logloss reduction (%) by cognitive relevance over test feature sets F 0 and F 1
Table 4 shows the approximate importance for each fea-ture given by training and testing a CTR prediction model over one week of click data (60% for training and 40% for test) for a fraction of users. Among all the inferred relevance features between a user and an ad, the feature generated by our learned model ranks highest and is twice as important as the second highest. When we include user-click-bias features in our comparison, we find our feature still outperforms all individual non-historical features and most historical fea-tures as well (not shown in the table). Weighted cosine similarity has importance 1/3 and 1/2 of f in the two cases and also ranks high, but demography relevance is of one or-der lower importance weight. As a consequence, our feature brings a significant improvement in logloss reduction, while demography feature has improvement lower than 0.1% (and thus not plotted in Figure 7). The relevance feature is espe-cially useful for the challenge of ranking newly created ads, for which historical click data is not available X  X he feature thus forms an apriori idea of the quality of an ad for each user. When an ad has been shown for an extended period of time, it is possible to collect actual clickthrough informa-tion for that ad from a large sample of users, and it is likely that the apriori relevance feature will be less significant.
As a final evaluation, we test the performance improve-ment achievable by adding the new feature to the ad ranking system that uses the existing optimized features. We ran-domly select about 5M users and partition them into two equal large groups. The first group G 0 are targeted by the ranking system trained with all previous features, but with-out our new relevance features ( F 2 ); the other group G are targeted by a retrained ranking system with the exact concept match feature ( wcos ) and our final relevance feature ( f ) added. As expected, our relevance features help improve the underlying ads metrics in these online experiments. For example, for the impressions with our relevance feature f larger than a relevance threshold, we observe that the CTR is improved by at least 30% across the board.

For post-experiment analysis, we placed the new feature values into 100 percentile buckets, such that each bucket contains an equal number of impressions. We compare the CTR improvement of G 1 over G 0 by showing observed CTR in G 1 for different values of the feature bucket as a multi-plier of the overall CTR of G 0 . For a strongly predictive feature, we expect the feature value to be very indicative of the actually observed CTR in the system. From Figure 8 we find that the two features we added have comparable power when measuring relevance for those impressions with Figure 8: The CTR boost observed (as a ratio of average CTR in the control group G 0 ) when we add our new relevance feature f and the baseline fea-ture wcos to test feature set. The plot shows that high values of our relevance feature are indicative of possible clicks, and low values are indicative of poor relevance or low clicks. The absolute values of the y-axis labels are hidden for confidentiality.  X  X oderate X  relevance (from 40% to 70%), but our feature is much stronger in finding highly relevant user-ad pairs and boosting CTR for them. Unlike the exact match similarity, which is zero for a large percentage of impressions (more than 30% as seen from Figure 3(c) and 8, when no exact concept matches are found), our feature is also good at rank-ing them and finding ad impressions that are likely to have low-relevance, and get lower clicks than expected.
In this study we aim for relevance based targeting in large scale social network sites. We infer user interests and ad concepts from heterogeneous sources and links, and develop user-ad relevance feature based on weighted matching be-tween any pair of concept classes. We learn both the link weights and matching matrix from click data, and find they mutually enhance each other. Experimental results reveal insights into user preferences in online social networks and suggest valuable potential for more relevant targeting.
We have made some simplifying assumptions in this study that can be relaxed in future work. For example, we do not model temporal aspects of user-generated content, and do not personalize weights on various sources. The method canbeappliedontopofricherconceptextractionmodels, which should help provide a clearer signal of concept associ-ations. Finally, for more complex models, parallel learning algorithms and better data infrastructure can be studied to facilitate the offline learning and online ranking.
