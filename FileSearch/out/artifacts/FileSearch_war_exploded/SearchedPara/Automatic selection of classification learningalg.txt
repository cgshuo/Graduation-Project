 Center for Bionics, Biomedical Research Institute , Korea Institute of Scienc e and Technology, Seoul, Korea Department of Computer Science, Brigham Young University, Provo, UT, USA 1. Introduction
One of the challenges faced by data mining practitioners is the selection of the most accurate algorithm for their classification tasks. Given that each algorithm performs well only on a subset of classification tasks  X  a direct consequence of the No Free Lunch theorems [36,45], and that there is a growing number of available algorithms, finding the best algorithm for a particular classification task is indeed becoming increasingly difficult.

While hiring human experts may help, it is also costly and often biased. Since no expert can be ex-pected to know all algorithms, decisions tend to be influenced by personal experience and preferences. Furthermore, comparatively little is known about each algorithm X  X  area of expertise, i.e., what character-izes tasks on which the algorithm performs well and tasks on which it performs poorly. Hence, human decisions run the risk of being suboptimal. Recent research suggests that automatic selection tools can sometimes bring up solutions that have thus far eluded human experts (e.g., see [7]). As an alternative, one could simply run all available algorithms on the given task and choose the algorithm with the best performance. However, this is not without problems either. In particular, one may not have access to the candidate algorithms, and even if they did, it might be computationally prohibitive to execute all possible algorithms. For the practitioner, who wishes to take advantage of data mining technology but lacks expertise in the area, what is needed is an automatic system capable of returning the most suitable algorithm for his/her task.

Metalearning, or the mining of data collected from the application of data mining to build metamodels that map classification tasks to algorithms, has proven a viable solution for the design and implemen-tation of such systems [11]. Metalearning for classification algorithm selection may be formulated as a special case of Rice X  X  general framework [35,38]. Let L be a set of learning algorithms for classification gives the best predictive accuracy on t . Since classification tasks may be unwieldy to handle directly, a training set and induces a metamodel that, for each new classification task, t , predicts the algorithm from L that will perform best on t . Alternatively, one may induce a metamodel that predicts a com-plete ranking of algorithms from L , thus providing the user additional information when the algorithm predicted best exhibits what appears to be a poor performance (e.g., see [8,12]). In the single algorithm prediction approach, the user has no further information as to what other algorithm to try. In the ranking approach, the user may try the second best, third best, and so on, in an attempt to improve performance.
In order to maximize value for the user, the set L should be such that it offers as broad as possible a coverage of the task space so that most tasks presented by the user will fall into the area of expertise of at least one algorithm in L . There are two possible approaches to achieving maximum (under L ) coverage: 1) know each algorithm X  X  area of expertise and select one algorithm per area, or 2) use all algorithms in L . It should be clear that if we knew the area of expertise of all learning algorithms, the problem of algorithm selection would be largely solved. Hence, the only practical alternative is to make L large and consider all of the learning algorithms in L . However, this too presents some challenges. First, because the set m of training examples at the metalevel (i.e., documented base-level size of L increases. This, in turn, makes it difficult for any metalearner to induce an accurate metamodel. Furthermore, the larger L gets, the higher the chances of similarities among algorithms in L ,whichin turn may lead the metalearner to overfit. To illustrate, consider the simple metalearning task shown in Table 1. This task consists of three training (meta-)examples, each corresponding to a specific base-level learning task characterized by two continuous-valued metafeatures. Each example is also labeled with the algorithm known to perform best on the associated learning task. Let us further assume that the labels, the metalearner would try to induce a model that discriminates between instances 1 and 2. In that attempt, it risks overfitting. Indeed, suppose that a new, previously unseen dataset is presented to the on how it handled the discrimination between instances 1 and 2 during learning, it would return either l instance may also be recorded as either l A or l B . As a result, there is a 50% chance that the metamodel correct answer.

As an effective way to overcome the foregoing challenges, we propose a novel classification algorithm selection model based on clustering, where we group the target values in the metadataset in terms of the behavior similarity of the corresponding algorithms, and induce a metamodel that predicts clusters rather than individual algorithms. This novel idea was inspired by our previous work on behavior-based algorithm clustering, where we realized that the underlying similarities among algorithms might be exploited to simplify the metalearning task (i.e., by improving the ratio of training examples to target classes), reduce the risk of overfitting, and potentially induce models more accurate than traditional algorithm-based selection [24]. We follow up on this idea here and show how it can be effectively implemented.

The paper is organized as follows. In Section 2, we briefly review previous work on metalearning for algorithm selection. In Section 3, we introduce the details of our clustering-based algorithm selec-tion system. In Section 4, we compare clustering-based selection with algorithm-based selection and demonstrates its superiority. Finally, Section 5 concludes the paper. 2. Related work
Since they indirectly draw on information about base-level learning, either in terms of characteristics of subsets of data or in terms of properties of learning algorithms, model combination techniques, such as bagging [13], boosting [37], stacking [44], delegating [16], and dynamic classifier selection [46], are often considered types of metalearning. While we recognize the value of these approaches in im-proving performance over individual learning tasks, our definition of metalearning calls for explicit and direct application of learning techniques to data extracted at the base level, in an attempt at improving performance over many tasks [11].

This form of metalearning for classification algorithm selection has a rich history. It finds its origins in VBMS, which learned to select the most accurate of three algorithms as a function of two dataset characteristics [34]. Later, the ESPRIT StatLog project extended VBMS by considering 16 dataset char-acteristics, together with a broad set of 23 candidate machine learning algorithms for selection [10,18, 27]. Continuing in the tradition, the ESPRIT METAL project covered 53 datasets, and made a number of significant contributions to metalearning, including designing novel task characterizations, such as, ad-vanced statistics [15,22,25], landmarking [4,17,30], and model-based approaches [3,6,28]. In addition, the project introduced the idea of rankings rather than  X  X est-in-class X  selection to give more information to the user [12], and implemented it in the Data Mining Advisor (DMA), a Web-enabled prototype assis-tant system [19]. An unifying framework and survey of algorithm selection, including via metalearning, in a number of domains is in [38].

More recently, some researchers have introduced the idea of active learning to metalearning as a valu-able mechanism to reduce the effort required by metadata construction [31 X 33]. Others have focused on the construction of experiment databases to produce new information, test hypotheses and verify existing hypotheses or results, through the use of what can be seen as meta-level analyses or experiment min-ing [9,41]. The current Experiment DB 1 has been populated with details of over 650,000 experiments; it is extendible and public, and is a tremendous contribution to the community and a serious boost to metalearning research. Work has also been done in designing a data mining ontology to characterize the salient features of learning algorithms that characterize their bias so that a metalearner may learn to map such bias-specific features to empirical evidence of the algorithm X  X  performance [20]. One partic-selection, which has so far been left aside by research in metalearning.
 We also note that limited work has been done in exploiting clustering techniques for metalearning. In one case, metalearning is used for the selection of clustering algorithms (the reverse of our objective here), where a ranking of 7 algorithms is produced based on a metamodel induced from 32 datasets characterized by 8 metafeatures [14]. In another case, a method for clustering time-series based on 13 extracted data characteristics is described [42]. While the authors do not use metalearning, the paper would provide a nice starting place for researchers wishing to use metalearning on time series clustering. ported in [21]. The characterization of each cluster is derived from metafeatures obtained from the tasks to which the learning algorithms were applied. In particular, the authors identify metafeatures predictive of whether learning algorithms are likely to exhibit very high or very low error correlation. No attempt is made at using the discovered clusters to inform metalearning. To the best of our knowledge, the ap-proach proposed here, where algorithms are first clusters based on behavior similarity and metalearning is then applied to predict clusters rather than individual algorithms is the first of its kind. 3. Clustering-based metalearning
In this section, we discuss the design of our clustering-based metalearning system for algorithm se-lection. 3.1. Choice of learning algorithms
To facilitate access to our results by as wide an audience as possible, we consider well-established learning algorithms from Weka (version 3.6.4), one of the most popular and richest open-source data mining software package [43]. We select the following 15 well known and often used classification algorithms:  X  Na X veBayes (NB)  X  Bayesian Network (BayesNet)  X  Logistic Regression (Logistic)  X  Radial Basis Function Network (RBFN)  X  Multilayer Perceptron (MLP)  X  Sequential Minimal Optimization (SMO)  X  One-nearest Neighbor (IB1)  X  Three-nearest Neighbor (IB3)  X  Classification and Regression Tree (SimpleCart)  X  C4.5 Decision Tree (J48)  X  NB Tree (NBTree)  X  Random Forest (RandForest)  X  Ripper (JRip)  X  Partial Tree Decision Rule Learning (PART)  X  Ripple-Down Rule Learning (Ridor)
To cater specifically to business practitioners who are unlikely to have the know-how to tune parame-ters,  X  X nd since no efficient automatic method of parameter optimization is known for most algorithms X  only the default versions of the algorithms are considered, except for Bayesian Network whose default degenerates to Na X ve Bayes. Instead for BayesNet, we allow a more complex structure to be induced by setting the maximum number of parents to 10.

We note at the onset that Weka X  X  algorithms are organized according to an internal taxonomy that consists of a number of model classes, including tree-based, rule-based, instance-based, probability-based and function-based approaches. It would, of course, be possible to let this taxonomy be the basis for a clustering of our selected algorithms. However, recall that our metamodel will select clusters rather than algorithms. Hence, our objective is to group together algorithms whose behavior is similar, so that cluster on the learning task under consideration. Weka X  X  taxonomy is human-generated, and thus may focus on intuitive criteria that miss underlying, non-obvious behavior similarity among algorithms. As a result, we prefer to rely on automatic clustering. We do include the results of using Weka X  X  taxonomy for comparison in Section 4. 3.2. Choice of behavior distance measure
Our chosen measure of behavior distance between algorithms is the classifier output difference (COD), one of the diversity measures used in ensemble learning [29]. Given two learning algorithms l 1 and l 2 , accuracy, provide only an idea of average performance over all instances, COD captures local variations among instances. For example, even though l 1 and l 2 may have the same overall accuracy on some correctly, and vice versa. COD is particularly attractive for clustering classification learning algorithms since it has been shown that 1) unlike all other local diversity measures, it satisfies the properties of a COD, they also have similar accuracy [24].

As we do not typically have access to actual probabilities, we use frequency-estimates for COD values t . Then,
To obtain COD ( l 1 ,l 2 ) , we generally compute COD on a number of learning tasks and average the values. Hence,
It is well-known that a number of rather simple factors, such as attribute types, skewness, and missing data, have a significant impact on the behavior of many learning algorithms. As a result, it is likely that different clusterings, and subsequently different metamodels, would be obtained if the datasets used in the computation of COD were to be selected along any of these dimensions. We clearly do not have space to, nor can we realistically, consider all possibilities. However, for the sake of illustration, and because it is such a simple test to run on any dataset, we focus here on the attribute types, and restrict our attention to continuous-only data. The same approach can naturally be applied along any other such a priori discrimination criterion.

For the computation of COD over our set of classification learning algorithms, we select 85 datasets with only continuous attributes. 45 datasets are from the Gene Expression Machine Learning Repos-itory [39], 28 datasets are from the UCI Machine Learning Repository [2], and 12 datasets are from the Multi-class Protein Fold Recognition Data (see http://www.cs.odu.edu/  X  sji/resources/index.html). Space does not permit us to include the detailed list, but the following provides summary information.  X  Number of examples: 28 datasets have between 65 and 200 examples; 32 datasets have between  X  Number of attributes: 21 datasets have between 1 and 20 attributes; 21 datasets have between 21  X  Number of classes: 56 datasets have 2 target classes; 11 datasets have between 3 and 9 target classes;
We note that computing COD over a set of algorithms is computationally intensive. Each algorithm must be run against each dataset, usually with cross-validation, and all predictions for all training in-stances must be recorded. This takes at least the running time of the most expensive classification learn-ing algorithm in L on the most complex learning task, assuming parallel computation is available. Pair-wise comparisons of predictions and averaging must subsequently be performed. However, this cost need only be incurred once. 3.3. Choice of clustering
Using COD distance information averaged over the 85 selected datasets, we cluster our 15 algorithms using complete-linkage, hierarchical agglomerative clustering, and obtain the dendrogram of Fig. 1.
While it captures a number of possible clusterings, the hierarchical clustering algorithm does not or partition, is typically made by selecting a level at which to cut through the dendrogram, and then defining the clusters as the groups of algorithms hanging from the subtrees whose top branches intersect with the horizontal line corresponding to the chosen level, as illustrated on a simple example in Fig. 2.
The selection of a specific clustering, or cut point, greatly impacts the actual clustering, and thus the ensuing metalearning task. If the cut point is too low (i.e., just above the leaf nodes), then each cluster contains a single algorithm, and our approach degenerates into the typical algorithm selection approach. to a single cluster, and our approach is no better than no algorithm selection at all.

Finding an appropriate level at which to cut is critical to the success of our approach. Recall that L denotes our set of 15 classification learning algorithms, T denotes our set of 85 classification tasks, and for each t  X  T , b L ( t ) denotes the algorithm in L that gives the best predictive accuracy on t , while c ( t ) denotes the characterization of t by some set of relevant metafeatures. At the metalevel, each t  X  T gives rise to a training meta-example. In the traditional algorithm selection setting, meta-examples are of the label of the cluster to which b L ( t ) belongs.

From the meta-examples, the metalearner induces a metamodel that predicts a cluster in C from which a specific classification algorithm may be chosen. The accuracy of the metalearner is computed as fol-lows.
 racy depends on the number of times the predicted cluster contains the best algorithm. When all clusters are singletons, we obtain the standard accuracy of the corresponding algorithm selection metalearner.
Assume for now that, given the predicted cluster, the user simply picks one algorithm at random from that cluster to execute on his/her target task. Note that most clusters will not be singletons so that there will be variance in the accuracy on the user X  X  target task, from the best performing to the worst performing algorithm in the selected cluster. The key idea behind our behavior-based clustering is to try to minimize this variance by ensuring that algorithms in a cluster are as similar as possible (i.e., relatively small COD) so that any one of them is likely to be as good as the best one, provided the best one is in the group. Recall that small values of COD entail similar values of accuracy.

Because we use complete linkage in our hierarchical clustering, the value of COD at each merge point in the dendrogram corresponds to the largest pairwise difference in algorithm behavior within the newly we do not know a priori which cluster will be predicted, and indeed, different clusters are predicted for different test examples, the quantity we try to minimize is actually the maximum of the COD values over all clusters in the clustering, i.e.,
On the other hand, as with any learning task, we also wish to maximize Acc Meta ( C ) . Hence, our choice of cut point is governed by the natural tension between minimizing COD ( C ) and maximizing Acc Meta . It is easy to see that these two goals cannot be reached simultaneously and that a compromise must be found. Indeed, it is easy to maximize Acc Meta ( C ) by choosing a very high cut point, such that, as mentioned above, all algorithms are in a single cluster. In this case, Acc Meta ( C ) is trivially 100%. However, COD ( C ) is then maximal and the metamodel is essentially useless because of the extreme diversity among algorithms in the predicted cluster. Similarly, it is easy to minimize COD ( C ) by choosing a very low cut point, such that, as mentioned above, all algorithms are in singleton clusters. In this case, COD ( C ) is trivially 0. However, the metamodel X  X  accuracy will suffer due to overfitting and the low ratio of examples to target classes.

Our approach to selecting an adequate cut point through the COD-generated dendrogram takes this tension into account naturally. It follows one of the standard techniques, consisting of cutting where the gap between two successive merges is largest [26]. We compute the gaps g i = COD ( C i )  X  COD ( C i  X  1 ] between all successive merges, find the largest value k = argmax g i ,andset COD ( C k  X  1 ) as our cut point (note that our clusterings are numbered from 0 to 14, from bottom to top). The corresponding clustering is then used to label the training meta-examples. Using this approach, we would obtain the clustering shown in Fig. 1 (cut point between 0.173 and 0.197), which yields 6 clusters: {BayesNet, JRip, SMO, NBTree, SimpleCart}, {Ridor}, {J48, PART}, {IB1, IB3, RandForest}, {Logistic, MLP}, and {NB, RBFN}. 4. Experimental results
In this section, we build a clustering-based metamodel for algorithm selection, and compare its per-formance with metalearning for algorithm selection. The computation of COD for each pair of clas-sification learning algorithms is based on the predictions obtained by 10-fold cross-validation. Sim-ilarly, the results on the induced metamodels are based on 10-fold cross-validation. The code for our experiments uses Weka X  X  API and in particular the associated implementation of cross-validation (see http://weka.wikispaces.com/Use+WEKA+in+your+Java+code#Classification-Cross-validation for details).

At the metalevel, each of our 85 datasets is characterized by its values over the 22 metafeatures shown in Table 2. Where the value of a metafeature is class-dependent (e.g., class entropy), it is first computed for each class and then averaged over all classes.
 examples and the choice of classification learning algorithm to use can have a significant impact on the results (i.e., the form and performance of the induced classification model). The metalevel classification learning task we are defining here is no exception. We have not attempted to optimize either of these choices systematically here. Instead, we simply tried a few typical classification learning algorithms as metalearners (e.g., J48, IB3, MLP, Random Forest) and selected the one with the best predictive ac-curacy. Similarly, we performed simple manual tuning of the metafeatures, starting with a set of 43 candidates and removing one at a time as long as the metamodel X  X  performance did not degrade. This manual tuning resulted in the 22 metafeatures listed in Table 2, and in IB3 being selected as our met-alearner. By using the same metalearner and the same metafeatures throughout, individual results may not be optimal, but comparisons across different approaches are fairer.

There are two levels at which the performance of our proposed algorithm selection method may be measured. At the metalevel, we can measure how accurate our metalearner is at predicting the target cluster. However, this provides only a limited, and somewhat misleading, view of performance, since the real measure of the metalearner X  X  value is how it impacts the accuracy on the users X  tasks of interest, i.e., at the base level. Indeed, for the practitioner, what matters most, unless the metalearner is 100% accurate, which is unlikely, is how well the algorithm suggested by the metalearner performs on his/her task of interest. There is no cost to the practitioner as long as the metalearner X  X  prediction matches the actual best algorithm. But when it does not, the cost is measured by the difference between the accuracy of the actual best algorithm and the accuracy of the predicted algorithm. It is therefore possible for the metalearner to have relatively high accuracy, so that in many cases the best algorithm (or cluster) may number of mistaken predictions, but the cost associated with these is relatively small.

Accordingly, we focus on accuracy at the base level and propose the following complementary metrics to measure the performance of cluster-based metalearning strategies. One benefit of these metrics is that they apply naturally and uniformly to all clusterings, including the case when all clusters are singletons, i.e., the traditional algorithm selection scenario.  X  AV G  X   X  Base ( C ) . For each test meta-example t , we measure the absolute value of the difference  X  AV G +  X  Base ( C ) . For each test meta-example t , we measure the absolute value of the difference
For the purposes of comparison with our COD-based clustering approach, we include the results of algorithm selection when no clustering is used. In other words, the metalearner is to build a model that maps tasks to individual algorithms. This is the traditional setting of metalearning for algorithm selection and serves as a performance baseline here. We also include the results of cluster selection when the clustering is not generated empirically by observing the behavior of classification learning algorithms, but when Weka X  X  underlying taxonomy is used as the partition of classification learning algorithms. For the 15 algorithms considered here, that fixed partition is shown in Fig. 3. This serves as a way to appreciate the potential difference between automatic and manual clustering.

Finally, we must be careful to avoid artificially inflated results due to the use of the same data for clustering and metalearning. As stated earlier, one of the difficulties of research at the metalevel is the relative scarcity of examples to learn from. In the present study, we only have 85 datasets to work with. As a result, there is a natural tension between the need to have as much training data as possible (to reliability of performance estimates). It is well known that testing the predictive accuracy of a learning algorithm on the same data it was trained with leads to overestimates. While we do not have this exact problem here, as we can use 10-fold cross-validation when computing COD to build our clustering and again when assessing the performance of the cluster selection metamodel, we still run the risk of overestimation since the clustering and the metamodel would still be built from the same underlying metalearning), we use 2-fold cross-validation as follows. 1. Split the set 85 datasets arbitrarily into two subsets S 1 and S 2 . 2. Do 3. Repeat step 2, switching the roles of S 1 and S 2 4. Average the performance of the metamodel over the two folds Table 3 reports the performance of our cluster-based metalearner ( | C | =5 ) 2 compared to the perfor-mance of a standard algorithm selection metamodel ( | C | =15 ) and a Weka taxonomy-based clustering ( | C | =6 ).

These results clearly show that cluster-based algorithm selection leads to better performance at the base level than the traditional single algorithm selection models. While the difference in performance as significant at  X  =0 . 05 for both clustering types. Finally, the difference in performance between the COD-based clustering and the Weka taxonomy-based clustering is not statistically significant, yet gives a small advantage to the COD-based approach. This could also be an indication that Weka X  X  fixed tax-onomy may not be fully consistent with the algorithms X  actual behaviors. We have raised that possibility in our earlier work on behavior-based clustering [24].

Others have suggested ranking as a means to alleviate the problem associated with predicting only a single algorithm, whereby if the user has concerns about the performance of the predicted algorithm, he/she has no reasonable alternative [8,12]. In a ranking system, the user may try algorithms in an orderly and predicted performance-based sequence. In the cluster selection approach presented here, ranking within clusters has little, if any, value. Indeed, since algorithms are clustered based on their behavior, if any of them in the cluster yields some level of performance, then most of the others will yield a similar level of performance. Hence, ranking is unlikely to help. On the other hand, it may be interesting to investigate the possibility of designing ways to predict rankings of clusters. We leave this idea to future work. 5. Conclusion
In this paper, we leverage the idea of behavior-based clustering of learning algorithms to propose a novel algorithm selection metamodel based on the prediction of clusters. The basic idea is for the metalearner to induce a model that matches a dataset to the cluster of learning algorithms whose behavior be less effective than single algorithm selection since multiple algorithms are recommended to the user. However, clustering-based metalearning has a number of advantages over traditional metalearning.  X  By clustering algorithms in terms of instance-level behavior, the metalearning process is simplified  X  The clustering-based approach lends itself naturally to the building of specialized metamodels,  X  The clustering-based approach challenges the Procustean approach to algorithm selection typically  X  When new algorithms are introduced, it will not generally be necessary to rebuild the metamodel  X  Although we have focused here on metalearning for algorithm selection, our behavior-based cluster-
Despite our promising results, there is room for improvement and future research. In particular, the absolute accuracy of our metamodels is rather low, suggesting that the metafeatures we employ are not sufficiently predictive for the metalearning task at hand. This may be further compounded by the fact that our selected algorithms, although different, do not, on average, exhibit as much variation as may be expected in terms of accuracy on the selected datasets. Indeed, the average difference in accuracy between the best and the worst of our 15 algorithms across all 85 datasets is about 10%, with 23 datasets (27%) where that difference is less than 5%. Our metafeatures may lack the needed sensitivity to catch such differences among good-performance algorithms. Discovering effective metafeatures is a key factor in improving the usefulness and operationalization of our metamodels.

When constructing our dendrograms, we have only considered the default parameter settings of all learning algorithms. Work is needed to better understand and quantify the impact that certain parameter settings have on learning algorithms, and whether/how they may impact our clusterings.

Finally, on the more practical side, our selection metamodel suffers from the fact that if the actual best algorithm is not in the predicted cluster, all others in that cluster will also perform non-optimally since they are similar to each other, and the user is left with no recourse, being stuck with only poor choices to select from. One (non trivial) possibility that may be investigated consists of ranking clusters. With such a ranking, the user could  X  X scape X  from a bad cluster and try the next best one.
 References
