 When investigating alternative estimates for term discrimi-nativeness, we discovered that relevance information and idf are much closer related than formulated in classical litera-ture. Therefore, we revisited the justification of idf as it follows from the binary independent retrieval (BIR) model. The main result is a formal framework uncovering the close relationship of a generalised idf and the BIR model. The framework makes explicit how to incorporate relevance in-formation into any retrieval function that involves an idf -component.

In addition to the idf -based formulation of the BIR model, we propose Poisson-based estimates as an alternative to the classical estimates, this being motivated by the supe-riority of Poisson-based estimates for the within-document term frequencies. The main experimental finding is that a Poisson-based idf is superior to the classical idf , where the superiority is particularly evident for long queries. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Theory, Experimentation Keywords: Information retrieval, relevance feedback, prob-ability of relevance, binary independent retrieval model, in-verse document frequency
Virtually all information retrieval systems estimate the relevance of documents to a query by comparing term statis-tics for the document at hand to those of the collection and, if available, a set of known relevant and/or non-relevant doc-uments. Combining within-document term frequency (re-ferred to as tf ) with the inverse document frequency (re-ferred to as idf ) has consistently been proven successful on a variety of retrieval benchmarks.

The intuition underlying this combination of tf and idf is that frequent occurrence of query terms in a document makes the document more likely to be relevant to the query, but only if these terms are also discriminative. A term is dis-criminative if it does not occur in many different documents, or, in other words, when its inverse document frequency is high. Many publications point out how a tf  X  idf measure results naturally from either purely probabilistic or infor-mation theoretic arguments, and although they differ in the details, in the end, all of these authors conclude that the the-oretical arguments presented fit nicely with the well-known experimental success.

Our investigation considers the role of relevance informa-tion in retrieval systems based on tf  X  idf ranking functions. Exploiting relevance information to revise the importance weights of given query terms and/or to expand the query with new terms usually improves the retrieval quality sig-nificantly, i.e., leads to a better ranking of documents than the one obtained by the initial provided by the user.
The main goal of this paper is to investigate how the esti-mate of term discriminativeness, represented by the term X  X  idf , should be updated after relevance information becomes available. From an information theoretic argument, adding relevance information to a retrieval system corresponds to a loss of entropy. Without relevance information, the corpus of all documents is the foundation for estimating the discrim-inativeness (informativeness) of each term. With relevance information, we should remove the documents for which rel-evance information is available (let it be positive or negative relevance feedback) from the corpus of all documents.
We view idf primarily as a summary statistic of a set of documents. Church and Gale have demonstrated that idf is an attractive summary statistic of term sets, because it is more stable across collections of different years and different sources than alternatives such as, e.g., the variance of term occurrence in documents [2].

The classical binary independent retrieval (BIR) model [8] explains idf as the ranking that results from the situation where we have no relevance information. We have found that considering idf as a statistic of the sets of relevant and non-relevant documents provides an even stronger connection to the BIR model. We observed that the relevance-based term weights in the weighting schemes known as F1 X  X 4 can be expressed conveniently as summations of the idf values of the different sets involved.

An open question remains how the discriminativeness of a term should be measured. The classical idf is based on the occurrence probability P ( t | c ) = n D ( t,c ) /N D n
D ( t,c ) is the number of documents in collection c in which term t occurs and N D is the number of documents in the collection. For the within-document term frequency, it is known that alternative estimates, such as those based on  X  X ifting X  [13] and those based on Poisson approximations [6, 7], lead to improved retrieval results. The question that follows directly is whether the arguments motivating these alternative estimations apply equally to the estimate used for term discriminativeness.

The paper is structured as follows. Section 2 gives a con-cise overview of related work. Section 3 presents the main results of our research, where we relate the idf summary statistic over sets of relevant and non-relevant documents to the term weighting schemes of the Binary Independent Re-trieval (BIR) model. Section 4 discusses the impact of our results for retrieval systems using tf  X  idf ranking, and Section 5 presents an experimental evaluation. We summarise the contributions of this work in the final Section 6.
Robertson recently surveyed research attempting to find plausible theoretical models explaining why tf  X  idf is a good approach to ranking [9]. The same work also presents the BM25 ranking function as another instantiation of tf  X  idf for ranked retrieval. Although we did not set out to provide a theoretical explanation for tf  X  idf approaches, the results obtained are related to these works, especially where we con-tribute into revealing a closer relationship of idf measures to the Binary Independent Retrieval (BIR) model [8].
From the start, our primary goal has been to investigate the role of the idf summary statistic in the incorporation of relevance information. Ruthven and Lalmas X  review of approaches to relevance feedback [12] pointed us into the direction of updating idf according to the classic probabilis-tic model. Like [2], we approach idf as a robust statistic of term occurrence that helps identify  X  X ood X  keywords. Other related work in information retrieval theory includes the dis-cussion of disjoint term spaces in [15], and in particular their proposal to represent discriminativeness probabilistically.
Our research can also be viewed as an extension of [10], where inverse document frequency has been related to the probability of being informative. Especially the results ob-tained on the TREC collections presented in Section 5 can be considered an experimental investigation into the applica-bility of the idea proposed there to define the probability of a term being informative based on the assumption of doc-uments as independent rather than disjoint events, which motivates the use of Poisson probabilities.
This section discusses the relationship between the IDF and the probability of relevance. We review briefly the idf definition (section 3.1), and the probability of relevance (sec-tion 3.2), and the BIR model (section 3.3). Then, section 3.4 presents the idf -based formulation of the BIR model. Sec-tion 3.5 gives an intuitive explanation, based on the idea of  X  X irtual documents X , for the parameter smoothing applied for dealing with singularities of the BIR model. Finally, we pro-pose and investigate in section 3.6 Poisson-based estimates for probabilities such as P ( t | r ) and P ( t | c ) (probability that t occurs in relevant documents and probability that t occurs in the documents of the collection).
The idf is defined as the logarithm of the probability that a term occurs in the collection. Let c be the collection, let t be a term, let N D ( c ) be the number of documents in c , and let n D ( t,c ) be the number of documents in c in which t occurs. Then, the idf is defined as follows: It reflects the discriminativeness of term t in collection c .
The probability of relevance is denoted as P ( r | d,q ), where r is the relevance event, d is the document event, and q is the query event. 1 Bayes X  theorem gives: Next, we split the conjunction r,d,q . Once we let d depend on q , and once we let q depend on d .
 Equation 1 is the foundation of the binary independent re-trieval (BIR) model and equation 2 is the foundation of the language modelling (LM) approaches.
 Using the odds O ( r | d,q ) = P ( r | d,q ) /P (  X  r | d,q ) instead of P ( r | d,q ) leads to the same ranking, but P ( d,q ), P ( d ) and P ( q ) drop out.
 This elegant formulation of the BIR model, the LM ap-proaches, and their differences, can be found in [5].
Now, to instantiate the model, documents and queries are considered as conjunctions of features, denoted with x t . If we assume the features to be independent, we obtain: Usually, the words (terms) occurring in documents and queries are considered as features.
The BIR model estimates the probability P ( d | q,r ) based on the presence and absence of terms. The probability Note that P ( R | d,q ) is also commonly used in literature. We prefer however a consistent use of lower case to denote events (as opposed to random variables). P ( X t = 1 | q,r ) is the probability that term t occurs in rele-vant documents. Analogously, P ( X t = 0 | q,r ) is the proba-bility of its absence.

As usual in formulations of the BIR model, we define ab-breviations for those probabilities. We set a t := P ( X 1 | q,r ) and b t := P ( X t = 1 | q,  X  r ). (We deviate from the more common choice of p t and q t because this frequently causes confusion with probabilities and queries).

Since X t is a binary random variable, we can rewrite the probability for an event x t as follows [14]: We obtain: The log transformation yields: Q is a constant depending only on the query. Since it has no effect on the ranking of the documents, we do not consider it further.

Term weight h t , the term discriminativeness of t , equals:
We write h ( t,c ) to refer to the discriminativeness of term t in collection c .

Estimation of probabilities a t and b t uses the proportions of relevant and non-relevant documents in the collection. First, we introduce the classical and our alternative notation (also used in [11]). As the reader will notice, the classical formulation makes use of r , which also denotes the relevance event introduced before. We apologise for this overloading, but apply the classical notation where it improves the read-ability.
Review that, in our notation (given in the second col-umn), r and c are sets of documents: r is the set of relevant documents and c is the set of all documents (the collection). This notation is better suited to clarify dualities between es-timates involving all documents, the relevant documents, or the non-relevant documents, and the document or (within-document) term frequencies.

Next, consider the four alternatives for estimation, classi-cally denoted by F1 to F4 [8].

From the formulation in the previous section, the term weights h t can be viewed as sums of idf -values.

Reconsider the definition of idf : Using the set relevant documents instead of the full collec-tion, we obtain: This dual statistic reflects the discriminativeness of term t in the set of relevant documents r .

Consider the sum of idf -values we obtain for each of the four settings of h t : F1: h t =  X  idf ( t,r ) + idf ( t,c ): Here, the BIR model can F4: h t =  X  idf ( t,r )  X  idf (  X  t,  X  r )+ idf ( t,  X  r )+ idf ( This representation of the term weights of the BIR model based on a sum of idf -values gives a clear justification of the idf through the BIR model. It generalises the results and claim in [9], page 512:  X  X t is now apparent that we can regard IDF as a simple version of the RSJ weight applicable when we have no relevance information. X 
When no relevance information is available (maximum en-tropy), the classical idf corresponds to the measure of dis-criminativeness. The more relevance information becomes available, the more the term X  X  discriminativeness (in the re-mainder of the collection) will decrease (a loss of entropy). It decreases because the corpus of documents for which no relevance information is available is smaller, and, more im-portantly, because the discriminativeness of the query term in the relevant documents is subtracted from the discrimina-tiveness obtained for all (or all non-relevant) documents.
The estimates for the discriminativeness weights h t de-rived above have to be adapted for dealing with singularities. The following cases require adaptation of the estimates: A common reformulation (smoothing) of the estimates is to add constants to the expressions in numerator and denomi-nator.

The classic smoothing proposed for F4 (see [9]) adds a constant value 0.5 to each document count:
Although the literature has given various mathematical motivations for this constant, we like to offer a particularly intuitive explanation. Consider that we add two virtual doc-uments to each collection, one of which is relevant. Assum-ing that each term occurs in half of the virtual and relevant documents gives the following probability estimates: Based on these adapted estimates, we obtain the smoothed F4: h = log ( r t +0 . 5) / ( R +1)  X  (1  X  ( n t = log ( r t + 0 . 5)  X  (( N  X  R )  X  ( n t So, extending the document sets with two virtual documents is sufficient to explain the classical parameter adaptation. Next, consider four virtual documents; two of which are rel-evant, all terms occur in half of the documents, and all terms occur in half of the relevant documents.
 This solution feels more comfortable, because all numbers are integers; they can be interpreted as document counts. We obtain: h = log ( r t + 1) / ( R + 2)  X  (1  X  ( n t = log ( r t + 1)  X  (( N  X  R )  X  ( n t As a general explanation, we derive: The number of virtual documents that is added to the col-lection reflects the uncertainty about relevance. We assume that each term t occurs in half of the virtual documents (since occurrence is a binary event), that half of the virtual documents are relevant (since relevance is a binary event), and that each term t occurs in half of the relevant doc-uments. As demonstrated, these assumptions explain the classical smoothing proposed for F4 by defining = 0 . 5.
Probability estimation for P ( t | c ) and P ( t | r ) and related probabilities have so far been estimated on what we refer to as n/N estimates and their adaptations. Consider a set of N trials, and let n t be the number of trials in which the event t is true. Then, P ( t ) = n t /N is the probability that the event is true among N trials. We refer to this estimate as the disjointness-based estimate, since it can be explained by where the events d are disjoint and exhaustive.

The appropriateness of these estimates can however be questioned, in particular in the light of the summation of idf -values as follows from the BIR model. For, the sets involved (set of relevant documents, set of non-relevant doc-uments, set of all documents) are different in cardinality: usually, only a small fraction of all documents forms the set of relevant documents.

Compare this to estimates for P ( t | d ), i.e., the probabil-ity that a term describes (occurs in) a document. While the disjointness-based probability corresponds to the prob-ability that t occurs, the Poisson-based probability of term occurrence is the probability that t occurs at least once in n trials. The usefulness of a Poisson-based probability is well-known for the within-document term frequency (referred to as tf or lf ) of a term t in a document d [6, 7].

Let n L ( t,d ) be the number of locations at which t occurs in d . Then, the Poisson-based estimate of term (location) frequency is defined as follows: Here, P ( t | d ) is the probability that term t is representative for document d . The steep rise of the within-document term frequency for small occurrences ( n L ( t,d ) relatively small) leads to a superior retrieval quality. For example, let K be the average occurrence frequency ( K := 1 /N T  X  P t n t , where N
T is the number of events/terms). We obtain P ( t ) = 0 . 5 if n t is the average occurrence, P ( t ) &lt; 0 . 5 for n the average occurrence, and P ( t ) &gt; 0 . 5 for n t greater than the average occurrence.

This effect of a steep rise is also reflected in the well-known lifting function with lifting (zooming) factor z r ([13] and related publications): For z d := 0 . 5, the lifting yields tf ( t,d )  X  0 . 5, if n 1. Compare this to the Poisson approximation, where for K d = 1, we obtain tf ( t,d ) = 0 . 5, if n L ( t,d ) = 1.
Looking at the sum of idf -values as presented in sec-tion 3.3, we deal with sets of significantly different cardi-nality. The set of relevant documents is much smaller than the set of non-relevant and the set of all documents. This different cardinality and the success of the lifting of the prob-ability P ( t | d ) are the motivations for investigating Poisson-based estimates for P ( t | c ) and P ( t | r ).

Review the Poisson-based probability estimation for P ( t | c ), and P ( t | r ): Compare these also to the formulation of P ( t | d ) given before. The notation shows the strong analogy of the probabilities involved.
 Section 5 reports the experimental results obtained for Poisson-based estimates. Before we started the experimen-tal investigation, we looked at an analytical experiment to investigate the nature of the Poisson-based estimate. Con-sider the analytical experiment in Table 1. Let a collection with N D (  X  r ) = 10 6 , one million documents, be given, and let a query with N D ( r ) = 10, i.e., ten relevant documents, be given. For ease of reading, we use logarithm base 10 for the numerical illustration (the base of the logarithm does not matter for ranking purpose, as the base is a constant, term-independent factor).

The table shows some numerical values for the discrim-as the F2 estimate. The table shows six cases of possible term distributions, and the cases (a), (d), (e) and (f) (bold face) illustrate the difference between the n ( t,x ) /N and the n ( t,x ) / ( K x + n ( t,x )) estimates. Here, x represents a set of documents: the set r of relevant documents, or the set  X  r of non-relevant documents. The main findings from this analytical investigation are: 1. The classical estimate n t /N leaves h t to be high for 2. The Poisson-based estimate leads to a dramatic im-3. The classical estimate n t /N leaves idf ( t,  X  r ) to be the 4. The Poisson-based estimate leads to small idf P ( t,  X  r ) This analytical experiment illustrates the different nature of the classical n t /N ( idf ) estimate and the Poisson-based estimate n/ ( K + n ) ( idf P ). As our TREC experiments show, the setting of K has a major impact on the retrieval quality. Before we report on the experiments, we first summarise the impact of our results on tf  X  idf -based retrieval functions.
One result of our investigation is that any retrieval system based on tf  X  idf ranking may incorporate relevance informa-tion by replacing its idf -component by h t , where h t is a a 1 1  X  log 1 10  X  log 1 10 6  X  1 + 6 = 5  X  idf ( t,  X  r )  X  log 1 b 5 1  X  log 5 10  X  log 1 10 6 &gt; 5  X  idf ( t,  X  r )  X  log c 10 1  X  log 10 10  X  log 1 10 6 6 = idf ( t,  X  r )  X  log d 1 100  X  log 1 10  X  log 100 10 6  X  1 + 4 = 3  X  idf ( t,  X  r )  X  log 1 e 5 100  X  log 5 10  X  log 100 10 6 &gt; 3  X  idf ( t,  X  r )  X  log f 10 100  X  log 10 10  X  log 100 10 6 4 = idf ( t,  X  r )  X  log sum of idf -values derived from the sets of relevant and non-relevant documents as presented in section 3.4. This result coincides with the BM25 ranking formula, and extends the relationship of idf and BIR model as presented in [9].
Consider the definition of a tf  X  idf -based retrieval function, and we illustrate the replacement of idf by the discrimina-tiveness measure h t : The probabilities based on the within-document and within-query term frequencies are estimated as follows: The factors K d and K q serve for normalisation purposes. The BM25 (as the currently most successful tf  X  idf -based retrieval function) proposes h ( t,c ) in place of idf , and adds various constants to the retrieval function. The motivation for the constants is to leverage the effect of h ( t,c ), tf ( t,d ), and tf ( t,q ), and to fit the retrieval function to the nature of the collection for which it is applied.

To illustrate, we substitute h ( t,c ) with the idf -based for-mulation of F1: The factor idf ( t,c ) is the classical idf . The factor  X  idf ( t,r ) represents the relevance information. This factor is negative, since h ( t,c ) is smaller than idf ( t,c ) for terms that occur rarely in relevant documents. The example shows that for h ( t,c ) based on F1, RSV tf  X  idf corresponds to a classic tf  X  idf if no relevance information is available, and, the formulation also shows that with idf ( t,c ) only we actually assume that t occurs in all relevant documents ( idf ( t,r ) = 0).
This framework makes the relationship between idf and relevance information fully explicit.
So far, the results of this paper have been of theoretical nature: relevance feedback leads to a revised idf in tf  X  idf -based retrieval functions. In the derivation of this result, we proposed Poisson-based probability estimates instead of the classic ( n/N -based) ones. As the weighting schemes corre-sponding to the BIR model combine idf -values of sets with highly different cardinalities, we investigate experimentally whether a Poisson-based estimation makes sense for proba-bilities P ( t | c ) and P ( t | r ).
The experimental evaluation consists of a first series of ad-hoc retrieval experiments to investigate the effect of Poisson-based estimates for idf in a setting in absence of relevance information, and a subsequent series of routing experiments to investigate its effect on processing relevance information using the four variants of the BIR model to exploit relevance information.
 We have used the TREC test collections developed at TREC -7 and TREC -8 (using the data of  X  X isks 4 and 5 X , and topic sets 351 X 400 and 401 X 450 respectively). Pre-processing of the documents included neither stop-word re-moval nor stemming, resulting in the same experimental setup as the one described in [4]. However, terms in the query that occur on Van Rijsbergen X  X  stop-word list have been removed from the query text.

Like the adhoc experiment, the routing experiment follows the setup described in [4], where the training data consists of the Los Angeles Times articles and the test data consists of the remaining data. This division of the TREC data in training and testing is a natural one for a routing task, be-cause the training articles date from 1989 and 1990, in time preceding the rest of the collection which contains publica-tions from 1991 until 1994.

The retrieval system has been developed by extending the open-source main memory database system MonetDB 2 with some minimal extra functionality to support information re-trieval. To validate the basic workings of this retrieval sys-tem implementation, we first reproduced the baseline adhoc retrieval approach presented in [4], and obtained indeed the mean average precision (MAP) reported there. 3 We then proceeded to implement the BM25 ranking, where we choose k = 1 . 2 and k 3 = 1000 following the Okapi TREC papers. We have set the length normalisation parameter b to 0 . 7627, as derived analytically in [1].
Table 2 presents an overview of the results on the TREC -7 and TREC -8 adhoc test collections. First, compare the bot-tom row displaying the baseline results of a standard BM25 ranking using idf estimation, to the other rows, showing the results for BM25 using idf P instead of idf . On the short
See http://monetdb.cwi.nl/ .
Notice that these results are lower than those reported in [3], which we attribute to differences in pre-processing. Table 2: Mean Average Precision of adhoc retrieval experiments for topics created from title (T), de-scription (D) and narrative (N). The bottom line is the normal idf estimate; the others are Poisson-estimated idf with varying K , where N = 528 , 024 , and b n t = 1 N P t n t  X  260 .
 Table 3: Mean Average Precision of adhoc retrieval experiments for topics created from title (T), de-scription (D) and narrative (N), using idf only.
 TREC -7 queries, all of the idf P results outperform the base-line. If K is sufficiently high, the mean average precision of runs using idf P is better than the baseline regardless the topic length; K = N/ 10 gives near-best results for all of the experimental conditions. From these experiments, it seems fair to conclude that it makes sense to apply the Poisson-estimation to the occurrence frequencies underlying the in-verse document frequency as a model of the discriminative power of query terms.

Tables 3 and 4 present the results obtained using idf ( t,c ) weighting only , using idf and idf P respectively. Performance of the title-only queries is still reasonably good when com-pared to the results in Table 2. Using idf on its own de-grades however for longer queries, whereas idf P results are impressive regardless the topic length.
The routing experiments use the relevance assessments on the LA Times articles in the estimation of idf ( t,r ). The baseline results are those obtained on the test data (not including the training data) without using any relevance in-formation. We then apply the four weighting schemes of the BIR model, comparing the effect of relevance information obtained with the standard idf to that using the Poisson-based idf P .
 Table 4: Mean Average Precision of adhoc retrieval experiments for topics created from title (T), de-scription (D) and narrative (N), using idf P only ( K = N/ 10 ).
 Table 5: Mean Average Precision of routing ex-periments with weighting strategies, for topics cre-ated from title (T), description (D) and narrative (N). The Poisson-based idf , denoted idf P , is param-eterised with K = N/ 10 .
 Table 6: Mean Average Precision of routing exper-iments with idf P for K = 1 , for weighting strategies, topics created from title (T), description (D) and narrative (N).

Method T TD TDN T TD TDN -idf P 0.141 0.148 0.158 0.149 0.153 0.175 F1 idf P 0.121 0.089 0.048 0.129 0.104 0.086 F2 idf P 0.121 0.089 0.048 0.129 0.104 0.086 F3 idf P 0.089 0.064 0.050 0.132 0.112 0.095 F4 idf P 0.089 0.064 0.050 0.132 0.112 0.095
Table 5 summarises the routing results. Whereas with classical idf , relevance feedback obtained from the assess-ments on the Los Angeles Times articles improves results over the baseline, applying the BIR relevance weighting us-ing the Poisson-based idf P does not lead to results that im-prove upon its baseline. Notice however that all experimen-tal results using idf P result in higher mean average precision scores than all of those using idf , with or without relevance information. While for long queries, a marginal performance improvement is observed with standard idf weighting, the results after feedback remain significantly below the idf baseline. The differences in mean average precision between F1 and F2 seem negligible, as well as those between F3 and F4. Surprisingly, the weighting schemes based on term pres-ence only (F1 and F2) outperform consistently those on term presence and absence (F3 and F4).

We presented only the experimental results for K = N/ 10, but the observations made hold for other choices of K as well. Table 6 shows an adverse effect of the Poisson weight-ing on the routing task performance (especially on the longer queries). Again, these are representative for all settings with small K .
The difference in performance between short and long queries in tables 3 and 4 can be partially explained by as-suming that users stating a short  X  X itle X  query select very carefully the most discriminative terms with respect to rel-evance, while long queries like the description and narrative also contain noisy terms that do not directly relate to the underlying information need.

The idf values do not differentiate between discriminative yet non-relevant terms and non-discriminative yet relevant terms in the query. The Poisson-based idf yields the same order of terms with respect to their discriminativeness as the classical idf does, but reduces the influence of non-discrimi-native terms on the ranking. Therefore, the effect of using one or the other approach is stronger for long queries than for short queries.

Setting the K -parameters of the Poisson-estimates to small values corresponds to weakening the effect of idf on the RSV, whereas a large K strengthens the effect of idf . Con-sistently, we can observe that a strong idf leads to better retrieval results.

A Poisson-based idf with large K is consistently supe-rior to the classical idf . This experimental result can be explained as follows from a theoretical point of view. The Poisson-based idf distinguishes stronger between discrimi-native terms, and distinguishes less between non-discrimi-native terms than the classical idf . As our results show, this nature of the Poisson-based idf of being stronger for rare terms and less strong for frequent terms leads to bet-ter retrieval quality. This result perfectly coincides with the superiority of the Poisson-based probability reflecting the within-document frequency.

Further in-depth analysis is needed to give an explana-tion as to why the ample training data in the routing exper-iment did not lead to a considerable improvement on the test data; though we like to point out that [4] also reports dis-appointing results with feedback in the routing experiment performed; so, maybe in the specific experimental setup cho-sen it is especially hard to gain improvements, and we should repeat the experiment on another ( TREC ) routing or filter-ing task.
This paper makes explicit the relationship between idf and the binary independent retrieval (BIR) model. The impact of this result is that relevance information can be incorporated into any tf  X  idf -based retrieval function by re-vising the idf according to the relevance information avail-able. Our result makes explicit that relevance information can be viewed as a loss of entropy. The discriminativeness of terms, initially high because of maximal entropy, decreases for two reasons. First, the event space for estimating term discriminativeness after feedback is smaller. Second, the dis-criminativeness of terms in relevant documents is subtracted from the original idf .

In addition to this theoretical framework for relating idf and the BIR model, and for justifying the incorporation of relevance information into a revised idf -component, we investigated Poisson-based probability estimations. Moti-vated by the success of Poisson-based estimates for within-document frequencies, we applied Poisson-estimates for the occurrence probabilities P ( t | r ) (term occurrence in relevant documents) and P ( t | c ) (term occurrence in all documents), which form the basis for the discriminativeness measures. The overall result is that the Poisson-based idf is superior to the classical idf , in particular for long queries. Unfortu-nately, we could not prove experimentally a positive effect in improving retrieval by relevance feedback.

Our next research steps include the transformation of idf -values for measuring discriminativeness (informativeness, re-spectively) into probabilities. This is a key issue for prob-abilistic retrieval models and probabilistic reasoning. For example, the framework described in [15] is based on a dis-joint space of concepts, where we could apply an idf -based probability as an estimate of the probability of term infor-mativeness. The refinement of those term space probabili-ties according to relevance feedback data has been an open problem. Though using idf -values as a basis for probability estimation poses a number of problems, the result of this paper on how to manage relevance information in an idf -based space might lead to new insights in how information theory and probability theory interrelate.
 We thank Djoerd Hiemstra for providing pre-processed TREC data.
