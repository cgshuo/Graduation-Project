 Term weighting schemes are central to the study of informa-tion retrieval systems. This article proposes a novel TF-IDF term weighting scheme that employs two different within document term frequency normalizations to capture two dif-ferent aspects of term saliency. One component of the term frequency is effective for short queries, while the other per-forms better on long queries. The final weight is then mea-sured by taking a weighted combination of these compo-nents, which is determined on the basis of the length of the corresponding query.

Experiments conducted on a large number of TREC news and web collections demonstrate that the proposed scheme almost always outperforms five state of the art retrieval models with remarkable significance and consistency. The experimental results also show that the proposed model achieves significantly better precision than the existing models. H.3.3 [ Information Systems ]: Information Search and Re-trieval : Retrieval Models Algorithm, Experimentation, Performance Document ranking, Retrieval model, Term weighting
Term weighting schemes are the central part of an infor-mation retrieval system. Effectiveness of IR systems are thus crucially dependent on the underlying term weighting mech-anism. Almost all retrieval models integrate three major variables to determine the degree of importance of a term for a document: (i) within document term frequency, (ii) document length and (iii) the specificity of the term in the collection. Term frequency and document length combina-tion is used to infer the saliency of a term in a document, and when a query contains more than one term, term specificity is used to reward the documents that contain the terms rare in the collection.

Retrieval models can be broadly classified into two ma-jor families based on their term weight estimation princi-ple. Vector space model casts queries and documents as finite dimensional vectors, where the weight of an individual component is computed using numerous variations of tf-idf scores. On the other hand, probabilistic models [16, 17] pri-marily focus on estimating the probabilities of the terms in the documents, and the estimation techniques differ from one approach to the other. But in essence all of them use the same basic principles that we have outlined before.
Most of the existing models (possibly all) employ a sin-gle term frequency normalization mechanism that does not take into account various aspects of a term X  X  saliency in a document. For example, frequency of a term in a document relative to the frequency of the other terms in the same doc-ument gives us an important clue that can not be achieved by the commonly used document length based normaliza-tion scheme. On the contrary, length based normalization can restrict the likelihood of retrieval of extremely long doc-uments which can not be taken care of by the relative fre-quency based term weighting.

Another major limitation of the present models is that they do not balance well in preferring short and long doc-uments. Such limitation makes a system to retrieve low quality documents at the top of the ranked list when they face queries of varying length. For example, in pivoted doc-ument length normalization sch eme, if the parameter is set to a smaller value, it performs better for shorter queries, and when the parameter value is larger, longer queries are bene-fited more than the shorter queries [10]. Similar observation can be made for other models such as BM25, language model or relatively recent divergen ce from randomness based mod-els [13, 10].

The main reason is that when the parameter is set to a static value, most of the models prefer either short doc-uments or long documents. If a weighting scheme prefers long documents, it pulls up extremely long documents when longer queries are encountered, since the longer documents have higher verbosity level it matches more query terms[28]. On the other direction, preference of short documents may degrade the overall retrieval performance, since it violates the likelihood of relevance versus retrieval pattern suggested by Singhal et al. [28].
This article proposes a term weighting scheme that can ad-dress these weaknesses in an effective way. In particular, we argue that the two aspects of term frequencies, when com-bined appropriately, leads to significant performance benefit. In this article we make the following contributions.
In order to assess the effectiveness of the proposed model we carry out a set of experiments on a large number of stan-dard test collections containing news and web data. The experimental results show that the proposed weighting func-tion consistently and significantly outperforms five state of the art retrieval models (from vector space as well as proba-bilities families) measured in terms of the standard metrics such as MAP and NDCG. The experimental outcomes also attest that the model achieves significantly better precision than all the other models when measured in terms of a re-cently popularized metric, namely, expected reciprocal rank (ERR) [6].
 The remainder of the article is organized as follows. In Section 2 we review the state of the art. Section 3 describes the proposed weighing scheme. Description about the test collections, evaluation metrics and the details of the base-lines are given in Section 4. Experimental results are pre-sented in Section 5, where we compare the performance of the proposed model with the state of the art vector space models, followed by the comparison with the probabilistic models. Finally, we conclude in Section 6.
Information retrieval syst ems, when encounter a query, tries to rank documents by their likelihood of relevance. Most IR systems assign a numeric score to the documents and then they are ranked based on these scores. Three widely used models in IR are the vector space model [26, 25], the probabilistic models [21] and the inference network based model [30]. In this section we review some of the state of the art models.

In vector space model, queries and documents are repre-sented as the vector of terms. To compute a score between a document and a query the model measures the similarity be-tween the query and document vector using cosine function. The central part of the vector space model is to determine the weight of the terms that are present in the query and the documents. Three main factors that come into play to compute the weight of a term are: (i) frequency of the term in the document, (ii) document frequency of the term in the collection (first proposed in [29]) and (iii) the length of the document that contains the term. Fang et al. [10] give a comprehensive analysis of four retrieval models by defining a set of constraints that needs to be satisfied for effective retrieval. Using these constraints the strengths and weak-nesses of some well known models are analyzed and some of the models are modified. There are also a number of re-cent works that focus on the constraint based analysis of the retrieval models [8, 9].

Salton and Buckley [24] summarize a number of term weighting approaches which use various types of normal-ization. It is evident that document length is an important component in effective term weighting. Singhal et al. [28] identify a number of weaknesses of cosine and maximum tf normalization and they observe that a weighting formula that retrieves documents with chances similar to their prob-ability of relevance performs better. Following this obser-vation, they propose a pivoted normalization scheme that acts as a correction factor of old normalization and is one of the most effective term weighting schemes in the vector space framework. The pivoted length normalization scheme computes the term weight as follows [27]: The parameter s controls the extent of normalization with respect to the document. Typically, the term weighting functions in vector space model are designed heuristically, which are based on the researchers experience. Several work tried to use the data to learn the patterns that satisfy the data. For example, Greiff [11] uses exploratory data analysis to uncover some important relationship between the docu-ment frequency and the relevance of a document.

The key part of the probabilistic models is to estimate the probability of relevance of the documents for a query. This is where most probabilistic model differs from one another. Binary independence model is perhaps the most widely ac-cepted technique in the classical probabilistic model. A number of weighting formula have been developed and BM25 [20] hasbeenthemosteffectiveamongtheformulae. Themajor differences between BM25 and the other commonly used TF-IDF models are the slightly variant IDF formulation and the use of the query term frequency. The length normalization factor uses the average document length and a parameter has been introduced to control the relative length effect.
Probabilistic language modeling technique [19, 14] is an-other effective ranking model that is widely used today. Typically, language modeling approaches compute the prob-ability of generating a query from a document, assuming that the query terms are chosen independently. Unlike TF-IDF models, language modeling approaches do not explic-itly use document length factor and the idf component. It seems that the length of the document is an integral part of this formula and that automatically takes care of the length normalization issue. However, smoothing is crucial and it has very similar effect as the parameter that controls the length normalization components in pivoted normalization or BM25 model. Three major smoothing techniques (Dirich-let, Jelinek-Mercer and Two-stage) are commonly used in this model [31].

Relatively recent, another probabilistic model is proposed in [3] that computes the weight of a term by measuring the informative content of a term by computing the amount of divergence of the term frequency distribution from the dis-tribution based on a random process. Like most of the well known models, they also use the same basic components. However, the integration of various component are derived theoretically. This family of formula also uses the average document length as an ideal length of the documents and the term frequencies are normalized with respect to the average document length.

In inference network, document retrieval is modeled as an inference process [30]. A document instantiates a term with a certain strength and given a query the credit from multiple terms is accumulated to compute a relevance that is very equivalent to the similarity score of vector space model. From an operational angle, the strength of instantiation of a term for a document can be considered as weight of the term in a document. The strength of instantiation of a term can be computed using any reasonable formula.
Given a query Q and a document D ,themaintaskof a ranking function is to assign a score to D with respect to the query Q . The main objective of a term weighting scheme is to quantify the saliency of the query terms in the document. This section describes a novel TF-IDF term weighting scheme that serves above purpose. En-route to the development, we are guided by a number of hypotheses that are commonplace in quantifying the importance of a term. Thus, before we give the main motivation behind our work, let us first revisit the three key hypotheses. 1. Term Frequency Hypothesis (TFH) :Theweight 2. Document Length Hypothesis (DLH) :Thishy-3. Term Discrimination Hypothesis (TDH) :Ifa
Most existing weighting schemes employ the above heuris-tics to quantify the term importance. However, they gen-erally normalize the term frequency that captures a single aspect of the saliency of the terms and hence disregards an-other important aspect that we detail next. In particular, we consider the following two aspects: 1. Relative Intra-document TF (RITF) :Inthisfac-2. Length Regularized TF (LRTF) : This factor nor-However, we believe that any document length normaliza-tion can be used to achieve this purpose. Some of the pos-sible alternatives might be the length normalization compo-nent of BM25 or that of the pivoted normalization scheme.
In order to motivate the use of two TF factors, let us consider the following two somewhat toy examples. We use these examples just to introduce the basic idea. Example 1 Let D 1 and D 2 be two documents of equal In both of the cases, LRT F considers t equally important. A little thought will convince us that this is not appropri-ate, since the focus of the document D 1 seems to be divided equally among 5 terms and therefore t should not be consid-ered salient, while t seems to be important for D 2 .Thus,in the later case RIT F seems to be a better choice to LRT F .
Let us now turn to the other direction and consider the second example.
 Example 2 Let D 1 and D 2 be two documents with the For this instance however, RIT F considers the term t equally important for both D 1 and D 2 , which is not right, since D contains more distinct terms and thus seems to cover many other topics (also possibly uses t repeatedly). Therefore, in this case, the use of LRT F seems to be a potential choice over RIT F .

Motivated by the above examples, our main goal now is to integrate the above two factors into our weighting scheme. However, we do not use the TF factors as defined in the Equations 3 and 4. We transform these TF values for our final use that in some sense makes use of the hypothesis AD-TFH . The next section details the transformation procedure and the underlying motivation.
Recall that the main motivation behind the advanced hy-pothesis on term frequency (AD-TFH) is that a good weight-ing function, while emphasizing on term frequencies and term discrimination factors, should also pay attention to the term coverage issue (i. e number of match). For example, if adocument D 1 contains a query term 10 times and another document D 2 contains two query terms (of the same query) 5 times each, then the assigned score should be higher for D (assuming that both the query terms have equal term dis-crimination values). That is probably the most important reason why raw TF does not work well in practice. Second, another common trait of many weighting schemes (for exam-ple, pivoted normalization) is that they use the TF functions that are not bounded above. We transform the TF factors using a function f ( x ) that possesses the following proper-ties: (i) vanishes at 0, (ii) satisfies the two conditions of AD-TFH hypothesis ( f ( x ) &gt; 0and f ( x ) &lt; 0), and (iii) asymptotically upper bounded to 1.

One of the simplest functions that satisfies the above prop-erties is f ( x )= x 1+ x . Indeed, similar functions have been employed before in [22] and in [3]. Using this function, we now transform the two TF factors as follows:
Now the key question that we face: how should we com-bine BRITF ( t, D )and BLRTF ( t, D )? A natural way to do this is as follows: where 0 &lt;w&lt; 1. The next important issues that arise out of Equation 7 are the following: In order to answer these questions, we now analyze the prop-erties of the two TF components. From Equation 5, it is clear that BRITF ( t, D ) has a tendency to prefer long doc-uments, since for long documents the denominator part of RIT F ( t, D ) is close to 1, and TF is usually larger. On the other hand, BLRTF ( t, D ) tends to prefer short documents, since LRT F ( t, D )  X  0as len ( D )  X  X  X  . Therefore, when a query is long, BRITF ( t, D ) heavily prefers extremely long documents, since the number of matches is more or less proportional to the length of the document [28]. On the contrary, since BLRTF ( t, D ) prefers short documents it can penalize extremely long documents when it faces longer queries, and thus it is preferable when longer queries are encountered. Another interesting property of BRITF ( t, D ) is that it emphasizes on the number of matches, since the main component of this formula RIT F ( t, D ) heavily pun-ishes the term frequency, and thus important for the short queries. Hence, the foregoing discussion suggests that, for short queries BRITF ( t, D ) should be preferred, while for longer queries, BLRTF ( t, D ) should be given more weight
Based on the discussion given in the previous section, we now turn to incorporate the query length information into our weighting formula. The value of w should decrease with the increase in query length, while it must lie between [0-1]. Specifically, we characterize the query length factor ( QLF ( Q )) by the following variables. (i) QLF ( Q )=1for |
Q | = 1, (ii) QLF ( Q ) &lt; 0 and (iii) 0 &lt;QLF ( Q ) &lt; 1. Nu-merous different functions can be constructed that satisfy the above conditions. We used the following three different functions.

The first function descends more rapidly than the second function, while the second function descends more rapidly than the third function. Our experiments suggest that func-tion 9 performs consistently better than the other two func-tions on all the collections. Hence, we set w = QLF 2 ( Q ). We leave this issue for further investigation.
The goal of the term discrimination factor in weighting is to assign higher score to the documents that contain the terms which are rare in the collection. Inverse document frequency ( IDF ) is a well known measure that serves the above purpose. A number of IDF formulation are prevalent in the IR literature, all of which essentially quantify the above intuition. We use the following standard idf measure.
The above IDF measure considers only the presence or absence of a term in a document and does not take into ac-count the document specific term occurrence. We hypoth-esize that the term discrimination is a combination of the above two factors. In particular, we hypothesize that if two terms have equal document frequencies, then the term dis-crimination should increase with the increase in average elite set term frequency. The average elite set term frequency total occurrence of the term t in the entire collection. In fact, Kwok [18] used AEF for term weighting, but the pur-pose was different. However, the combination of raw AEF with IDF may disturb the overall te rm discrimination value, since the IDF values are obtained by dampening through log function. Hence, we employ a slowly increasing function to transform the AEF values for this combination. Once again, we use the function f ( x )= x/ (1 + x ) to transform the AEF values for the final use. The final term discrimi-nation value of term t is computed as Our experiments reveal that the use of the above term dis-crimination has not very significant effect on the overall per-formance. However, it is observed that the improvements, although are small, consistent across the collections.
Integrating the above factors we now obtain the following final scoring formula.
 Again, since TFF ( q i ,D ) &lt; 1, we obtain the following rela-tionship.
 Therefore, we can easily modify Equation 13 to get the nor-malized similarity scores (0 &lt;Sim ( Q, D ) &lt; 1) as follows: Equations 13 and 15 are equivalent in the sense that they produce the same ranked lists. However, an application that requires normalized scores, Equation 15 can be used as a suitable alternative.
In this section we describe the details of our experimen-tal setup. First, in Section 1 we give the details of the test collections used in our experiments. In Section 4.2 and Sec-tion 4.3 we describe the evaluation measures and the baseline retrieval models respectively.
Table 1 summarizes the statistics on test collections used in our experiments. The experiments are conducted on a large number of standard test collections, that vary both by type, the size of the document collections and the number of queries.

TREC 6,7,8 and ROBUST are news collections contain-ing 528,155 documents and s upplemented by 150 (queries 301-450) and 100 (601-700) queries respectively. WT10G is a web collection of moderate size supplemented by 100 queries (451-550), while GOV2 is another web collection of larger size, which is crawled from .gov domain. There are 150 (queries 701-850) queries attached with GOV2 collection which were used in TREC terabyte [4] track for three years. The MQ-07 and MQ-08 set of queries are based on the Million Query Track 2007 [2] and 2008 [1] respectively. This track was designed to serve two purposes. First, it was an exploration of ad-hoc retrieval on a large collection of doc-uments. Second, it investigated questions of system evalua-tion, particularly whether it is better to evaluate using many shallow judgments or fewer thorough judgments. Both mil-lion query track use GOV2 as document collection. Topics for this task were drawn from a large collection of queries that were collected by a large Internet search engine. The queries also vary by their length, with short (2-3 words) to long (6-10 words). Specifically, MQ-07 and MQ-08 collec-tions contain 505 and 433 queries of length higher than 5 respectively. Therefore, the test collections provide us a di-verse experimental setup for assessing the effectiveness of the proposed weighting method.

Except TREC-6,7,8, all the test collections have three scale graded relevance assessment. The grades are 0, 1 and 2-meaning non-relevant, relevant and highly relevant re-spectively. TREC-6,7,8 collection uses binary relevance as-sessment.
All our experiments are carried out using TERRIER 1 re-trieval system (version 3.5). Terrier is a flexible Information retrieval system which provides the implementation of many well known models. We use title field of the topics (note that two million query data contain more than 1000 queries that contain more than 5 terms). From all the collections we re-moved stopwords during indexing. Documents and queries are stemmed using Porter stemmer. Statistical significance tests are done using two sided paired t -test at 95% confi-dence level (i,e p&lt; 0 . 05).

We use the following metrics to evaluate the systems.
The first two metrics are used to reflect the overall per-formance of the systems, while the last evaluation measure reflects better the precision of search results, thereby making more important for the precision oriented systems. ERR has been chosen as one of the official metrics for recent TREC web tracks [7].

Note that, two million query collections (MQ-07 and MQ-08) have incomplete relevance assessment. Therefore, for the sake of more reliable conclusions, we evaluate the mil-lion query sets in two different ways. First, we skip the unjudged documents from the ranked lists to compute the values of well known metrics following the recommendation made in [23]. Additionally, we also present the statistical http://terrier.org/ average precision 2 [5] which was one of the official metrics for the million query tracks [2].
We have compared the performance of the proposed weight-ing scheme with five state of the art retrieval models. Since the proposed weighting function is a TF-IDF based formula, we have taken two well known state of the art TF-IDF mod-els. We have also chosen BM25, language model with Dirich-let smoothing (LM), and relatively recent divergence from randomness based formula (PL2) as the other state of the art baselines. The choice of our baselines are primarily mo-tivated by [10], which provides a thorough and detailed de-scription of all the state of the art models along with the parameter sensitivity issues.

The performances of all the baseline models are dependent on the parameters they contain. Therefore, for the sake of more reliable compari sons with the baselines, we carry out two experiments by taking first 50 judged queries from MQ-07 and MQ-08 collections. We search parameters by opti-mizing NDCG@20. The parameters values are given in the description of the corresponding baselines. Our experiments mostly agree with the findings reported by Fang et al. [10]. In particular, we find that the performances of Pivoted TF-IDF and PL2 are very sensitive with the variation of the parameters. For example, the parameter value ( s =0 . 2) suggested for pivoted TF-IDF in the original paper [28] gives 12% poorer MAP than that we find here by training. Sim-ilarly, the default PL2 parameter ( c = 1) is 14% poorer than the one we find. Therefore, for fair comparisons, we use these optimal parameter values for the baselines. The details of the baselines are given below. 1. Pivoted length normalized TF-IDF model: This model 2. Lemur TF-IDF model: This model is another TF-IDF 3. Classical Probabilistic model (BM25): BM25 is cho-4. Dirichlet smooth language model (LM): Language model 5. Divergence from Randomness model (PL2): Finally, the code available at TREC million query page is used to compute stat AP
In this section we present the experimental results of our proposed work and compare them with the state of the art retrieval models. In Section 5.1 we compare the performance of the proposed model (MATF for Multi Aspect TF) with the two TF-IDF models, followed by the comparison with three probabilistic models-BM25, language model (LM) and PL2. We use three evaluation measures to evaluate the per-formance of all the methods.
In this section we focus on to compare the performance of the proposed model (MATF) with the Lemur TF-IDF and and Pivoted TF-IDF models. Table 2 presents the ex-perimental results for six test collections measured in terms MAP, NDCG@20 and ERR@20.

First, we describe the results in terms of MAP. Table 2 clearly shows that MATF gains significantly better MAP than both of the TF-IDF models on two news collections. MATF performs 12% and 15.7% better than Lemur TF-IDF model on TREC-678 and ROBUST respectively. MATF is also significantly surpasses the Pivoted TF-IDF model on these collections with a margin of 8.8% and 6.3% respec-tively.

The behavior of MATF is similar when we see the results for two web collections, namely, WT10G and GOV2. Once again, MATF outperforms Lemur TF-IDF model by a mar-gin of more than 20% in both of the occasions, which is clearly highly significant as confirmed by the paired t test. Like the previous two news collections, MATF maintains its superior behavior over Pivoted TF-IDF in these web collec-tions also. In particular MATF gains more than 8% and 19% average precision than Pivoted TF-IDF for both of the collections and paired t test once again attests the signifi-cance.

We now turn to describe the results on two million query data sets. These two collections are particularly interesting, since they contain real search queries collected from a com-mercial search engine and also because of their variations in length. Table 2 once again demonstrates that MATF un-equivocally outperforms the two TF-IDF models with signif-icantly large margin. The MAP achieved by MATF is nearly 11% and 7% better than that achieved by Lemur TF-IDF on MQ-07 and MQ-08 collections respectively. Similarly, MATF surpasses the Pivoted TF-IDF by more than 10% margin on both of the occasions. Significance tests show that the performance differences are always statistically sig-nificant.

AmongthetwoTF-IDFmodels,LemurTF-IDFoften seems to perform poorer than Pivoted (except MQ-08 where Lemur TF-IDF is nearly 4% better than pivoted). One po-tentially interesting outcome that we can see from Table 2 is that, when the document collection is larger MATF outper-forms Pivoted TF-IDF with larger margin. In particular, MATF gains a MAP on GOV2 collection which is almost 20% better than the pivoted TF-IDF, which is a clear sign of effectiveness of MATF over the state of the TF-IDF mod-els.

So far our discussion of experimental outcomes primar-ily confined on the basis of the binary relevance assessment. Note that five out of six test collections used in our evalua-tion have graded assessment in three scales (0,1,2). There-fore, we now turn to describe the results measured in terms of NDCG, that leverages the graded assessment.

The middle segment of Table 2 presents the results in terms of NDCG@20. It is once again clear that the perfor-mances are more or less consistent with MAP. Specifically, MATF surpasses the Lemur TF-IDF models with consis-tently and significantly large margin on all six collections and often the differences are higher or close to 10%, which once again clearly demonstrates the effectiveness of MATF. Performance of pivoted TF-IDF is once again very similar under the graded assessment and it achieves larger NDCG than Lemur TF-IDF except in one occasion. MATF once again is significantly better than the pivoted TF-IDF on all the collections and the differences are larger for larger web collections.

Our final comparison between the proposed model and the TF-IDF models focus on precision enhancing capabili-ties measured in terms of a metric ERR, that consider three things simultaneously: rank of the document, quality con-veyed by the assessor assigned grade (non-relevant, relevant and highly relevant) and the quality of the documents that have been seen before the document of our focus.
The last segment of Table 2 reports the ERR@20 values achieved by the competing models on six collections. We can easily infer that MATF once again unanimously beats the two TF-IDF models. Only on WT10G, pivoted TF-IDF performs slightly better than MATF. Consistent with the previous measures, ERR@20 results demonstrate that on larger web collections the performance differences between MATF and the two TF-IDF models are larger.
 Table 3: Comparison with TF-IDF models (statAP). Lemur means Lemur TF-IDF. Superscripts have their usual meaning.

The performances of MATF and the two TF-IDF models on two million query data, measured by statistical average precision, are shown in Table 3. MATF transcends Lemur TF-IDF by a margin of 18% and 15% on MQ-07 and MQ-08 respectively, while it is better than pivoted TF-IDF with more than 15% on both of the collections.

In summary, based on the results shown in Table 2 and Ta-ble 3 we can infer that MATF outperforms two state of the art TF-IDF models with remarkable significance and con-sistency, and the performance differences are often notice-ably large. The performance measured by three evaluation metrics unequivocally demonstrate that MATF is highly ef-fective in ranked retrieval. Moreover, the results also show that MATF is more effective for larger web collections.
In the last section we compare the performance of our model with two TF-IDF models. In this section we compare the performance of MATF with three well known state of the art probabilistic retrieval models. Our evaluation strategy is once again similar to the previous section. We compare the performances of the models under MAP, NDCG@20 and ERR@20.
 First we compare the performance of MATF with the BM25 model. Table 4 shows the summary of the retrieval results on six test collections. It is clear from the table that MATF is superior to BM25 model. This result holds for all the collections and for all three evaluation measure. When the performance differences between them are measured in terms of MAP, we notice that MATF is significantly effective for news as well as web corpora compared to BM25. In fact, MATF is nearly 10% better than BM25 on two news data, while on two web collections (WT10G and GOV2), MATF achieves 17% and 12% more MAP than BM25. The differ-ences on MQ-07 and MQ-08 are similarly significant with substantial margins. The performance differences between MATF and BM25 revealed by NDCG metric are consistent with that revealed by MAP and once again, all the differ-ences are statistically significant. ERR@20 depicts that for all the collections, MATF remains consistently superior to BM25, which clearly confirms that MATF is very effective for precision oriented systems.

We now compare the effectiveness of MATF and language model with Dirichlet prior language model. From Table 4 we clearly see that the performance differences between MATF and LM are larger in three out of six cases than that we had observed when comparing the performance of MATF and BM25. Specifically, MATF achieves close to or more than 10% MAP than LM on four out out six instances (ex-cept WT10G). The performance measured on graded rel-evance assessment also demonstrates that MATF unequiv-ocally beats the Dirichlet prior language model based ap-proach, and the differences are substantially large. On GOV2, MQ-07 and MQ-08 data, MATF surpasses LM with a margin of 14%, 8% and nearly 9% respectively. The comparison of precision enhancing abilities of MATF and LM also clearly indicates that MATF is always better than LM, which is very concordant with the experimental findings captured by MAP and NDCG.

We now compare the performance of the proposed model with another probabilistic model from the divergence from randomness family, namely, PL2. This model is relatively recent compared to the previous two probabilistic models and was also found to be better than BM25 in the exper-iments reported in [3]. Table 4 reflects two major facts. First, it appears from the table that PL2 is most effective among the probabilistic models and in particular only on MQ-08 data it performs worse than BM25 as reflected by both MAP and NDCG. The second major observation that canbemadefromTable4isthatMATFbeatsthismodel also with harmonious consistency and performance differ-ences are statistically significant on TREC-678, GOV2, MQ-07 and MQ-08 data. Similar to the previous outcomes, on web collections the performance differences between MATF and PL2 are larger than that for the news collections. Lastly, ERR metric depicts that MATF is better than PL2 across all six collection.
 Table 5: Comparison with probabilistic models (statAP).
 MQ-07 30.6 29.7 30.4 34.4 blp (12.8, 15.8, 13.2) MQ-08 29.6 27.6 27.4 32.5 blp (10.5, 17.8, 18.6)
Table 5 compares the performance of four models for mil-lion query collections measured in terms of statistical aver-age precision. It is once again clearly evident that MATF is consistently better than all three models and all the dif-ferences are very large and it is very consistent with the performance measured in terms other metrics presented in Table 4.

Overall, the comparative analysis clearly shows that MATF is the most effective retrieval model, which unequivocally outperforms all three probabilistic models, when the perfor-mances are measured in terms of MAP, NDCG and a pre-cision biased metric, namely, ERR. Also, the relative per-Table 6: Performance of two TF factors on short and long query. The values are MAP. LRTF 43.5 43.3 39.9 44.3
RITF 45.4 45.0 37.7 41.8 formance differences are often substantially large and the differences are even larger for the web collections that con-tain large number of queries. Among the three probabilistic models, PL2 and Dirichlet prior language model perform almost equally, with PL2 having a marginal edge over LM.
In this section we analyze the effect of the two TF factors on short and long queries. For this analysis we choose the two million query collections, primarily because the collec-tions have large number of queries. We divide the queries in two sets. The queries having at least 5 terms are denoted as short, while the rest of the queries (longer than 5 words) are treated as long. The main goal of this section is to validate the hypothesis made in the proposed section that relative intra-document based TF (RITF) performs better on short queries, while length regularized TF (RLTF) performs bet-ter on long queries.

Table 6 presents the experimental results on two million query data. The results seem to confirm our aforesaid as-sumption. LRTF always performs better than RITF on both of the collections, while RITF does better for short queries. However, the performance differences between the methods on longer queries are noticeably better than that for shorter queries.
In this paper, we present a novel TF-IDF term weighting scheme. The proposed term weighting scheme employs two aspects of within document term frequency normalization to determine the importance of a term. One component of the term frequency tends to prefer short documents, while the other tends to prefer long documents. We then combine these two TF components using the query length informa-tion, that maintains a balanced trade-off in retrieving short and long documents, when the ranking function faces queries of varying lengths.

Experiments carried out on a set of news and web collec-tions show that the proposed model outperforms two well known state of the art TF-IDF baselines with significantly large margin, when measured in terms of MAP and NDCG. The model also surpasses three state of the art probabilistic models with remarkable significance almost always. More-over, the proposed model is also significantly better than all of the five baselines in improving precision.
 I would like to thank Dipasree Pal, Mandar Mitra and Swa-pan Parui for their comments, suggestions and help. [1] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, and [2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, [3] G. Amati and C. J. Van Rijsbergen. Probabilistic [4] S. B  X  uttcher, C. L. A. Clarke, and I. Soboroff. The trec [5] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, [6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [7] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. [8] S. Clinchant and E. Gaussier. Retrieval constraints [9] R. Cummins and C. O X  X iordan. A constraint to [10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of [11] W. R. Greiff. A theory of term weighting based on [12] B. He and I. Ounis. A study of the dirichlet priors for [13] B. He and I. Ounis. On setting the hyper-parameters [14] D. Hiemstra, S. Robertson, and H. Zaragoza. [15] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [16] K. S. Jones, S. Walker, and S. E. Robertson. A [17] K. S. Jones, S. Walker, and S. E. Robertson. A [18] K. L. Kwok. A new method of weighting query terms [19] J. M. Ponte and W. B. Croft. A language modeling [20] S. Robertson and H. Zaragoza. The probabilistic [21] S. E. Robertson. Readings in information retrieval. [22] S. E. Robertson and S. Walker. Some simple effective [23] T. Sakai. Alternatives to bpref. In Proceedings of the [24] G. Salton and C. Buckley. Term-weighting approaches [25] G. Salton and M. J. McGill. Introduction to Modern [26] G. Salton, A. Wong, and C. S. Yang. A vector space [27] A. Singhal. Modern information retrieval: A brief [28] A. Singhal, C. Buckley, and M. Mitra. Pivoted [29] K. Sparck Jones. Document retrieval systems. chapter [30] H. Turtle and W. B. Croft. Evaluation of an inference [31] C. Zhai and J. Lafferty. A study of smoothing methods
