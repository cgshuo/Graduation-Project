 Query speller is an indispensable part of any modern search engine. In this paper we define the problem of speller perfor-mance prediction and apply it to the task of query spelling autocorrection. As candidates for query autocorrection we used the suggestions generated by a query speller. To de-termine their reliability we used a binary classifier trained on manually labeled data. In addition to the basic standard lexical and statistical features we utilized a number of new click-based features, what allowed to significantly outper-form the algorithm trained on the basic set of features. Categories and Subject Descriptors: H.3 [Query Alteration]: H.3.3 Information Search and Re-trieval Keywords: Query spelling correction, autocorrection, machine learning.
Analysis of the search engine logs shows that users make spelling errors in more than 10% of search queries [3]. As long as the result set for a distorted spelling is usually low relevant or almost irrelevant, search engines offer a wide range of tools designed to minimize the number of mis-spelling errors in search queries. Spelling-based query auto-completion, query spelling suggestion and query spelling au-tocorrection work at various stages of a search process, have different requirements to performance and accuracy, but all of them utilize a speller and face the problem of correc-tion confidence estimation. In this paper we formulate this task as query speller performance prediction and show its applicability to one of the accuracy-critical tasks -query autocorrection.

Historically, the most popular way to assist search engine users to correct their misspelled queries is to show them a  X  X id you mean X  message together with a recourse link to the search results for the corrected query (Figure 1).  X  X id you mean X  feedback mechanism is very usefull, but does not represent an ideal approach to query correction because it works in an interactive mode and awaits the con-firmation from the user, so it is not proactive. Apart from the fact that it requires additional user effort and leads to loss of time, users often do not notice a  X  X id you mean X  message or just ignore it not being sure of its meaning. At the same time, it seems obvious that most of the spelling errors are trivial and can be corrected automatically. These circumstances forced search engines to provide a more effec-tive solution -query autocorrection -which works in an au-tomatic mode and noticeably improves user experience. As the query is corrected before the search is triggered, it also reduces the number of unnecessary requests to the search engine. Many major search systems (e.g. Google, Yandex) implemented the query autocorrection function in the form of the  X  X howing results for.. . X  message (Figure 2). Figure 2: Query spelling autocorrection message at Google.

It is worth to note that an autocorrection message is al-ways accompanied with the link to the original query  X  X earch instead for.. . X  . This is done because autocorrection preci-sion is limited and users should have a chance to search with the original query. The price of wrong autocorrection is very high  X  it requires an additional click and always leads to user X  X  irritation. Therefore, while developing the query autocorrection function we have to focus on its precision as on the primary constraint on its performance.

To solve the problem of query autocorrection we used a spelling suggestion returned by a query speller and specially developed a classifier whose goal was to determine whether this suggestion is the valid correction. The classifier was trained by maximizing its precision using a set of statis-tical, lexical and click-based features. To the best of our knowledge, this is the first work addressing the task of query spelling autocorrection and the problem of query speller per-formance prediction in general.

The important feature of the proposed approach is the in-dependence of the autocorrection classifier from the speller X  X  task of ranking the spelling correction candidates. Using in-dependent algorithms allows to adjust precision/recall for spelling suggestions and autocorrections separately focus-ing on their specific problems in isolation and respectively achieve the best results for each task.
The rest of the paper is organized as follows. Section 2 reviews the related work. In Section 3 we give a problem description and an overview of our approach. Features are described in Section 4. Section 5 presents the experiments and results. Finally Section 6 summarizes this paper.
Among the large number of papers devoted to spelling correction [2, 4, 6, 8] there are only a few works related to spelling autocorrection [8]. Surprisingly, despite that many of major search engines provide query autocorrection func-tionality, we could not find any papers dedicated to search queries autocorrection.

Whitelaw et al. [8] proposed a language independent tech-nology for autocorrection of spelling errors in regular written text. For text checking and correction the three-level model was applied: 1) for each token the noisy channel spellchecker [3] generates a ranked list of corrections; 2) error detection classifier determines whether the word contains a mistyping error; 3) autocorrection confidence classifier decides whether it is possible to correct the error automatically. The authors showed that such separate classification is optimal due to the opportunity to tune the thresholds for two tasks  X  error detection and spelling autocorrection  X  independently. We applied this approach to query spelling autocorrection. Having a speller at our disposal we have to develop only the 3rd part of the system  X  autocorrection classifier. In our work we used the same method of classification and a subset of the lexical and contextual features mentioned in [8], such as the edit distance between a word and its correction, LM score of the word and its correction, etc.

Gao et al. [4] and Sun et al. [7] proposed a method of training the query speller error model using clickthrough data. Basically, after deploying the baseline speller, they ex-panded the set of examples for its training by those spelling suggestions, which were  X  X abeled X  as positive and negative by users implicitly by clicking or skipping them. Not only we focus on a different task in this paper (query autocorrection instead of spelling suggestion), but also utilize user feedback in a different way. Instead of using clicks for training (as we cannot use noisy labels for such a task with very strict re-quirements to accuracy as ours), we use clicks as features, basically  X  X earning X  over the time that it is safe to autocor-rect the query after we accumulated enough  X  X onfirmations X  from users that the suggested correction is valid.
We formulate the problem of speller performance predic-tion as a classification task and apply it to the task of query autocorrection. To solve this problem we built a binary clas-sifier which separates query suggestions into two categories: unreliable query spelling suggestions and query spelling au-tocorrections.

Before developing the approach (selecting the classifica-tion method and features) we needed to understand which types of query misspellings could be corrected automatically, and for which types of query misspellings we should provide ordinary spelling functionality.
By analyzing a random sample of queries from the one of the major search engines, we could identify 2 major types of errors: Misspelling errors  X  when user skipped, inserted, deleted or replaced letter(s) in a regular word, making non-word errors (e.g. cheif  X  chief ) or real-word errors (e.g. plan text  X  plain text ); Word Breaking errors  X  when user skipped or inserted a space between regular words (e.g. funnycats  X  funny cats, my self  X  myself) .

There are also two infrequent types of errors existing in non-Latin queries when a query and its correction belong to different alphabets: Keyboard Layout errors  X  for ex-ample, the use of the English layout for Greek typing or vice versa (e.g. chl;ow  X   X  X  X  X  X  (tall),  X  X  X  X  X  X   X  window ); Transliteration errors  X  for example, the use of the Latin alphabet for typing Russian words or vice versa, (e.g. ve-losiped  X   X  X  X  X  X  X  X  X  X  (bicycle),  X  X  X  X  X  X  X  X  X  X   X  construc-tion ). All of these types have univocal lexical attributes and could be unambiguously distinguished from each other with the help of a simple aligning algorithm. The distribution of these four types of errors, automatically calculated for 1000 query-correction pairs, randomly extracted from the 6-months query log, was the following: Misspelling (66.7%), Word Breaking (13.7%), Keyboard Layout and Translitera-tion (11.2%). The rest (8.4%) were errors of mixed types.
As the correction accuracy for various types of errors dif-fers in a wide range, for the sake of clarity of this study we considered only the most important and prevailing type of query errors  X  Misspelling.
The object of our classifier is a query-correction pair, where Q is an original user query and C is a correction of Q suggested by a speller. We divided the query autocorrection task into 4 subtasks:
Speller Task. In our work we use the noisy channel query speller [3]. The source channel framework finds the best spelling correction C for an input query Q : where P ( Q | C ) is the channel (error) model, which mod-els the transformation probability from C to Q , and P ( C ) is the source (language) model, which models the probability of C . We suppose that the best correction C is known and is provided to us by the query speller.

Alignment Task. Using Levenshtein alignment [5] we align misspelled query terms from Q to their corrections in C . For example, for the query-correction pair: howto co ok an aple pie  X  how to cook an apple pie we extract three  X  X uery term  X  correction X  pairs: howto  X  how to , co ok  X  cook , aple  X  apple Filtering Task. In order to filter Word Breaking and Keyboard Layout/Transliteration errors we apply two re-strictions for aligned  X  X uery term  X  correction X  pairs:  X  Spaces are not allowed (word-to-word alignment).  X  Both the query term and its correction should belong to the same alphabet.

As a result each query-correction pair Q  X  C is associated with a list of k word-to-word corrections q i  X  c i (where i  X  1 , .., k , k  X  is a number query terms with Misspelling errors in query Q ) ready to be used in the following classification process.

Classification Task . The classification task is to deter-mine if Q should be automatically replaced by C . In contrast to the speller X  X  task with a problem of ranking the list of spelling correction candidates [4], this task is formulated as a binary classification problem: each query-correction pair Q  X  C should be classified as valid or not valid autocorrec-tion.

In our work we used the same approach (independent from the speller task) and the same classification method (logistic regression) as [8]. But unlike [8], that spellchecked the writ-ten text and autocorrected each misspelled word indepen-dently, we dealt with search queries and had to autocorrect query terms taking the neighboring words into account. In order to autocorrect queries with multiple misspelled terms we proposed a two-level classifier with query term and query levels.

On the term level ( L QT ), for each q i  X  c i pair we find the probability p i that c i is the valid correction of q i in isola-tion (not accounting for neighborhood terms in the query). On the query level ( L Q ) we solve the binary classification problem of labeling query correction pair Q  X  C as valid or not valid. Term-level correction probabilities ( p acquired on the previous stage are used here along with the other query-level features.
Features used to train our classifier can be divided into two categories: 1) basic lexical features and 2) features extracted from clickthrough data.
As a set of basic features we used lexical and statistical features traditionally used in spelling correction tasks [4, 6, 8]. We grouped them as follows:  X  Frequency-based  X  query-based and web-based LM scores [1] for q and c ( L QT , 2 features); query-based and web-based LM scores for Q and C ( L Q , 2 features).  X  Dictionary-based  X  generated by checking if q and c are in the in-house spelling dictionary ( L QT , 2 features).  X  Length-based  X  length of q and c in characters ( L QT , 2 features); lengths of Q and C in characters; lengths of Q and C in query terms; the number of misspelled query terms in Q ( L Q , 5 features).  X  Edit distance  X  weighted Levenshtein edit distance [2] be-tween q and c , ( L QT , 1 feature).  X  Distributional similarity  X  distributional similarity [6] be-tween q and c ( L QT , 1 feature).  X  Named entity  X  generated by checking whether q or c is classified as a proper noun by the in-house name entity tagger ( L QT , 2 features).
To enhance the classifier performance we added several features which captured the information about users X  click behavior. Since users continuously provide feedback for the quality of specific spelling suggestions, we can start using this feedback as a feature in our classifier to be able to make more autocorrections instead of just spelling suggestions. From the 1-year session log for each query with a spelling suggestion we extracted original query Q , query spelling cor-rection C and the statistics about user behaviour (clicks on the recourse link). Then we aligned Q to C , extracted a list of term-correction pairs ( q i , c i ) and grouped them by ag-gregating the clicks for all unique pairs ( q , c ). This way we accumulated user behaviour statistics across the session log and extracted three click-based features:  X  the number of times ( N shows ) when users were shown spelling suggestion c for query term q ,  X  the number of times ( N clicks ) when users clicked on spelling suggestion c for query term q ,  X  clickthrough rate ( CT R ) -the ratio between N clicks
The basic clickthrough features are very effective but exist only for frequent misspellings and cover only a part of query-correction pairs (56% of the development set from Section 5.1). We tried to extend the usage of user feedback and introduced 2 new click-based features, which agregated ad-ditional statistics for the clickthrough information gathered for frequent misspellings of c and, as long as these features are only candidate-specific, we, in fact, applied them to the wide range of its unseen q  X  c pairs.

Assuming that each click on the spelling suggestion re-course link indicates an explicit user approval for a given query term correction q  X  c , we considered its CT R as a strong confidence measure and used it as a threshold to build a cluster of proven misspelled query terms q for every correction c . To train the threshold we used 200 manually labeled term-correction pairs (see Section 5.1). Having these clusters at our disposal we could calculate very important properties for every c :  X  variety of misspellings ( M SP num ), calculated as a number of unique ( q , c ) pairs for a given correction c ,  X  normalized edit distance ( N dist ) between q and c , calcu-lated as a ratio of A 0 and A , where A 0 is a weighted edit distance between q and c , A is an average edit distance between c and all its misspellings from the cluster.
These features represent diversity ( M SP num ) and com-plexity ( N dist ) of the existings misspellings in respect to the candidate correction and are applied to both frequent and infrequent (unseen) misspellings of each c existed in click-through data. These features cover 75% of the development set from Section 5.1.
We randomly selected 30,000 queries from the 6-months query log, spellchecked them using the query speller, left only queries with speller corrections, automatically retrieved query term-correction pairs of  X  X isspelling X  type (see Sec-tion 3.1) and then manually labeled them as valid and in-valid corrections. The assessment was done by a professional analyst.

Resulting 2,134 query-correction and 2,561 query-term-correction pairs have been divided into two equal-sized sets  X  the training and the test set. 200 query-correction pairs were randomly picked up from the training set to train the CTR threshold described in Section 4.2.
Our classifier X  X  quality was evaluated by measuring its pre-cision and recall.

Recall  X  the number of properly classified valid query-correction pairs divided by the number of all true query-correction pairs in the test set.

Precision  X  the number of properly classified valid query-correction pairs divided by the number of query-corrections, classified as valid.
The query-correction pair ( Q , C ) was considered properly classified if all of its term-correction pairs ( q, c ) have been properly classified.

In order to evaluate the configurability of our classifier for the different requirements to autocorrection accuracy, we carried out all experiments with two precision thresholds (0.90 and 0.95).

BASIC -classifier (Section 3.2) trained on the lexical fea-tures (Section 4.1). There is no baseline for our task, so we took the BASIC feauters set as the internal baseline.
BASIC + CLICKS  X  the basic features and the click-based features (Section 4.2).
 The results of experiments are shown in Table 1.

Precision Threshold Method Recall
Analysis of the results leads to the following conclusions:  X  Basic lexical and statistical features provide very high ini-tial level of autocorrection performance  X  58.3% of queries with misspelled terms can be automatically corrected with precision of 0.95.  X  Click-based features added a substantial improvement of 0.04 (5 %) to the classifier X  X  recall at the precision thresh-old 0.90 and 0.034 (5.8%) at the precision threshold 0.95.
Exploring the causes of false negatives we identified prob-lems with rare terms for which our features do not work. There are also a few problems with real-words when a dictio-nary word appears in a wrong context. For example, we are not able to consider correction  X  X alop media  X  gallup me-dia X  as reliable, because  X  X alop X  is a dictionary word, there is no clickthrough data for the term-correction pair  X  X alop  X  gallup X  and the number of misspellings of term  X  X allup X  is small. Our analysis of false positives has shown that most of mistakes our classifier makes are due to rare name en-tities and rare terms similar to frequent words. For exam-ple, Britanica  X  Britannica (Britanica  X  English language courses) and Dreamwear  X  Dreamweaver (Dreamwear is a lingerie shop). Another example is the wrong replacement of  X  X wittie X  with super popular term  X  X witter X , while the user was searching for  X  X wittee X  (a Twitter-client). The reasons of such wrong decision are the weighted edit dis-tance weaknesses and the lack of distributional similarity data for infrequent words.

Convinced by the high utility of clickthrough data, we con-ducted a series of experiments to evaluate the performance of individual click features, described in Section 4.2. Three basic click features ( N shows , N clicks , CT R ) were grouped into (BASECLICKS) group. Results of experiments with the precision threshold 0.90 are presented in Table 2.
The analysis of usefulness of different click-based features shows that basic features were very effective but covered only the frequent spelling errors. Diversity feature M SP and normalized edit distance feature N dist were useful for rare and, especially, for unseen errors not presented in the session log.

The experiment with click-based features shows the re-sult comparable to the one that can be obtained with a set of basic lexical features (0.742 vs. 0.794). It leads to the hypothesis, that these two feature sets might be used inde-pendently.
In this paper we presented a solution to the novel problem of speller performance prediction for query spelling auto-correction. We developed a binary classifier dividing query spelling corrections into two categories: regular spelling sug-gestions and highly reliable spelling autocorrections. In our work we also utilized the clickthrough data for the query au-tocorrection task. The experimental results demonstrated high recall of autocorrection demonstrated by our method under the rigorous constraints for precision.

The classifier works  X  X n the top X  of a query speller, using its best spelling suggestion as a candidate for autocorrec-tion, for which our method tries to predict its quality. The underlying idea of separating two spelling correction tasks  X  generating and ranking the spelling correction candidates and determining their reliability  X  allows us to tune the auto-correction precision/recall trade-off regardless of a speller X  X  settings.

To the best of our knowledge, this is the first paper propos-ing the solution for the search query autocorrection. Despite a rather high level of autocorrection recall, achieved by our method, there are still a number of opportunities for im-provement like training to detect high-quality clicks in or-der to train the classifier and autocorrecting word-breaking errors.
