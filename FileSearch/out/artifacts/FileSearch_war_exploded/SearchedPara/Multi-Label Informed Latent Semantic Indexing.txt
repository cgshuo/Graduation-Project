 Latent semantic indexing (LSI) is a well-known unsuper-vised approach for dimensionality reduction in information retrieval. However if the output information (i.e. category labels) is available, it is often beneficial to derive the in-dexing not only based on the inputs but also on the target values in the training data set. This is of particular im-portance in applications with multiple labels , in which each document can belong to several categories simultaneously. In this paper we introduce the multi-label informed latent semantic indexing (MLSI) algorithm which preserves the in-formation of inputs and meanwhile captures the correlations between the multiple outputs. The recovered  X  X atent seman-tics X  thus incorporate the human-annotated category infor-mation and can be used to greatly improve the prediction accuracy. Empirical study based on two data sets, Reuters-21578 and RCV1, demonstrates very encouraging results. H.3 [ Information Storage and Retrieval ]: Content Anal-ysis and Indexing X  Indexing methods Algorithms, Theory Latent Semantic Indexing, Dimensionality Reduction, Su-pervised Projection, Multi-label Classification
Information retrieval and pattern recognition often suffer from the problem of high dimensionality of the data, for the reason of learnability or computational efficiency. Therefore dimensionality reduction in terms of semantic indexing or feature projection is of great importance and is commonly applied to solve real world problems [2, 1, 5].
 Copyright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00.
Among various methods, latent semantic indexing (LSI) turns out to be a successful approach and is widely applied to document analysis and information retrieval [2]. To apply LSI, documents are represented in a vector space model, and singular value decomposition (SVD) is performed to find the sub-eigenspace with large eigenvalues. It is shown that LSI can find the best subspace in terms of Frobenius norm of ma-trix. Thus the technology behind LSI is also called principal component analysis (PCA) in the sense that each  X  X atent se-mantic X  can be viewed as a  X  X omponent X  to represent the data (see, e.g. [4]).

LSI is purely unsupervised and is not capable to incorpo-rate some additional knowledge. There are at least two rea-sons for further improvements on this issue. First, consider-able information about the content of documents is reflected by document X  X  labels, which is often annotated by human experts. This is particularly the case in the multi-label set-ting where each document is assigned to multiple categories. The semantic correlations of assignments for variant cate-gories and the hierarchical structure of categories expresses the semantic relationships between documents. Therefore, it is desired to have a LSI technique that can be informed by this additional knowledge and produce semantically more meaningful latent factors.

Second, the unsupervision of LSI leads to results that may be or may not be useful in discriminative analysis like au-tomatic text categorization. However in one specific clas-sification or regression problem, output information is in general very important and should be incorporated into the feature mapping or selection process. In particular we con-sider problems with multiple labels : For an input x the corresponding output is no longer a scalar but a vector y = [ y 1 , . . . , y L ] T . Thus the text categorization system solves many related tasks at the same time. In this setting the dependencies between multiple labels are worth consid-ering for multivariate data analysis, and can be used to im-prove the indexing for these specific tasks. Furthermore, training a system with multiple labels might lead to smaller parameter variance and the prediction for a particular label is improved if the labels are correlated.
 This setting is very common in real-world applications. One example is the problem of multi-label document cate-gorization, where each document is allowed to be associated with more than one category and where categories often have semantic correlations [8]. The well-known text data set Reuters-21578 contains such documents, and the new text data corpus RCV1 has additionally a topic hierarchy [7]. These two data sets will be used in the experiments.
In this paper we introduce a supervised LSI called multi-label informed latent semantic indexing (MLSI). MLSI maps the input features into a new feature space that retains the information of original inputs and meanwhile captures the dependency of output dimensions. The mapping is derived by solving an optimization problem for linear projections, and can be easily extended for nonlinear mappings with kernels. We use this method as a preprocessing step and achieve encouraging results on the multi-label text classifi-cation problems.
We consider a set of N documents. For i = 1 , . . . , N , each document i is described by an M -dimensional feature vector x i  X  X , and is associated with an L -dimensional output vector y i  X  X  . We denote the input data as a matrix X = [ x 1 , . . . , x N ] T  X  R N  X  M , and the output data as Y = [ y 1 , . . . , y N ] We aim to derive a mapping  X  : X 7 X  V that projects the input features into a K -dimensional latent space.
In the following, lower-case bold Roman letters denote column vectors, and upper-case ones denote matrices. In particular, I is reserved for identity matrix. Eigenvalues are usually denoted as  X  and it should be clear from the context which matrix they are corresponding to. k X k denotes Frobenius norm for matrices and 2-norm for vectors, and Tr [  X  ] denotes trace for square matrices.
The paper is organized as follows. In Section 2 we formu-late the data projection as an optimization problem in the linear case and then propose a regularized version to pre-vent overfitting, which is generalized to nonlinear mapping by using kernels. Then we point out its connections to re-lated work in Section 3 and report the experimental results in Section 4. In Section 5 we conclude the paper. We begin by introducing an optimization explanation for LSI, and then take into account the output information.
In LSI, we aim at finding a linear mapping from the input space X to some low-dimensional latent space V , while most of the structure in the data can be explained and recovered. We can achieve this by taking a latent variable model and solving the following optimization problem which minimizes the reconstruction error (see, e.g., [4]): where V  X  R N  X  K and A  X  R K  X  M , given K  X  M . Each column of V corresponds to one latent variable or latent semantic , and by V T V = I we constrain that they are un-correlated and each has unit variance 1 . For each document
An equivalent version of (1) has the same objective func-tion but instead constraining AA T = I . The difference be-tween the obtained projections and the computed V in (1) is just a linear scaling caused by the top K singular values of X . Here we consider the form (1) for the convenience of deriving the extensions in next section. in X (represented as one row in X ), the corresponding row in V explicitly gives its projection in V . A is sometimes called factor loadings and gives the mapping from latent space V to input space X . At the optimum, VA leads to the best K -rank approximation of the observations X .

The derived indexing explains the covariance of input data, which is however not necessarily relevant to the output quan-tities. Thus LSI may or may not be beneficial to supervised learning problems. Generally speaking, it is more desirable to consider the correlation between input X and output Y , and the intra-correlation within Y (if multiple labels). Therefore, we turn to supervised indexing in the next sub-section, incorporating both input X and output Y .
The unsupervised indexing problem (1) explicitly repre-sents the projections of input data X in matrix V . To con-sider the label information, we can enforce the projections V in problem (1) sensitive to Y as well. Thus in supervised LSI we solve the following optimization problem: where V  X  R N  X  K gives the K -dimensional projections of documents, for features of both X and Y ; A  X  R K  X  M , B  X  R K  X  L are the factor loadings for X and Y , respec-tively. 0  X   X   X  1 is a tuning parameter determining how much the indexing should be biased by the outputs. As be-fore, V T V = I restricts the K latent variables to be uncor-related and have unit variance. Clearly, the cost function is a trade-off between the reconstruction error of both X and Y . We wish to find the optimal indexing that gives the minimum reconstruction error. The second part in the ob-jective function of problem (2) enforces the latent semantics to explain the dependency structure of multiple labels. The following theorem states the interdependency between A , B and V at the optimum.

Theorem 1. Denote C = (1  X   X  ) XX T +  X  YY T , and let  X  1  X  . . .  X   X  N be eigenvalues of C with corresponding eigen-vectors v 1 , . . . , v N . If V , A and B are the optimal solutions to problem (2) , then: (a) A = V T X , B = V T Y ; (b) V = [ v 1 , . . . , v K ] R , where R is an arbitrary K  X  K (c) At the optimum, the objective function in (2) equals to To improve readability, we put all proofs into Appendix. Theorem 1 states that the leading eigenvectors of C form a solution for matrix V , and any arbitrary rotation for V does not change the optimum. Therefore to remove the ambigu-ity, we focus on the solution given by the leading eigenvec-tors of C , i.e., V = [ v 1 , . . . , v K ]. Problem (2) can thus be achieved by solving the eigenvalue problem Cv =  X  v for the first K leading eigenvectors, which is equivalent to solving Then V = [ v 1 , . . . , v K ], A = V T X , and B = V T Y gives the optimal solution for problem (2).
To complete the MLSI algorithm, we still need to consider two things. Firstly, the indexing should not rely on the la-bels, since for new documents we have no target information yet. Secondly, the stability of indexing should be taken into account, because otherwise overfitting is likely to occur.
It is not hard to see that solving problem (3) only gives the projections for training data with both features in X and Y . We wish to construct a mapping  X  : X 7 X  V that is able to handle the input features of any new documents, thus we add a linear constraint to problem (2) and restrict the latent variables as linear mappings of X , i.e., Therefore we have v i = Xw i , for i = 1 , . . . , K , if we denote W = [ w 1 , . . . , w K ]  X  R M  X  K . Plugging v = Xw into (3), we have an optimization problem with respect to w :
Similar to other linear systems, the learned mappings can be unstable when the span { x 1 , . . . , x N } has a lower rank than M , due to the small size of training set or dependence between input features 3 . As a result, a disturbance of w with an arbitrary w  X   X  span { x 1 , . . . , x N } does not change the objective function of optimization since ( w + w  X  ) T w
T x i , but may dramatically change the projections of un-seen test documents which are not in the spanned space. To improve the stability, we have to constrain w in some way.
Suppose rank( C ) = N , then maximizing (3) is equiva-lent to minimizing v T C  X  1 v . 4 We introduce the Tikhonov
Solving problem (3) itself only gives the first eigenvector v 1 of C . The full optimization problem should be recur-sively computing v j by maximizing v T Cv with the con-the problem as (3) for simplicity and also because its La-grange formulism directly leads to the eigenvalue problem.
This will be a crucial problem when we consider nonlinear mapping in the dual form (cf. Section 2.4), since the dimen-sionality of data point x in the reproducing kernel Hilbert space (RKHS) could be very high, or even infinite (e.g., in case of RBF kernel). See, e.g., [12].
This equivalence holds whenever C is positive definite and thus invertible. It is easy to show that matrix C is at least positive semi-definite, since we have u T Cu = (1  X   X  ) u T XX T u +  X  u T YY T u = (1  X   X  ) k X T u k 2 +  X  k Y 0 ,  X  u  X  R N . In case that C is not positive definite, it suf-fices to use pseudo-inverse instead, or makes it so by adding a tiny positive scalar to diagonal entries. regularization [14] into problem (4) as the following where k w k 2 = w T w is a penalty term and  X  is a tuning parameter. The following theorem shows that the regular-ization term k w k 2 removes the ambiguity of mapping func-tions by restricting w in the span of x i , i = 1 , . . . , N , and thus improves the stability of mapping functions.
Theorem 2. If w is an eigenvector of the generalized eigenvalue problem (5) , then w must be a linear combination of x i , i = 1 , . . . , N , namely where  X   X  R N .

Problem (5) is easily solvable by setting the derivative of its Lagrange formulism with respect to w to be zero. Then we obtain a generalized eigenvalue problem which gives generalized eigenvectors w 1 , . . . , w M with eigen-values  X   X  1  X  . . .  X   X   X  M . Note we sort eigenvalues in a non-decreasing order, since we take the K eigenvectors with the smallest eigenvalues to form the mapping. The first K eigen-vectors are used to form the mapping functions as the fol-lowing where the scaling caused by eigenvalues ensures that more important dimensions get larger weights. As the main re-sults we obtain  X ( x ) = [  X  1 ( x ) , . . . ,  X  K ( x )] into a K -dimensional space.

In problem (6) we are interested in the eigenvectors with the smallest eigenvalues, whose computation is however the most unstable part in solving an eigenvalue problem. Thus we let  X  = 1 /  X   X  and turn the problem into an equivalent one: where we are seeking the K eigenvectors with the largest eigenvalues. This gives the MLSI algorithm in primal form, as summarized in Table 1.

Input X  X  R N  X  M , Y  X  R N  X  L , 0  X   X   X  1,
Steps (i) Calculate C = (1  X   X  ) XX T +  X  YY T ;
Output mapping functions  X  j ( x ) =
So far we have considered linear mappings that project inputs x into a meaningful space V . However, Theorem 2 implies that we can also derive a nonlinear mapping  X .
Let a kernel function k x (  X  ,  X  ) be the inner product in X , where K x is the N  X  N kernel matrix satisfying ( K x ) i,j k ( x i , x j ). k w k 2 can also be calculated with kernel K Similarly, we can define a kernel function k y (  X  ,  X  ) for inner product in Y and obtain a kernel matrix K y = YY T . Then we can calculate the matrix C using kernels: and express the dualformalism of problem (5) with respect to coefficients  X  as which gives rise to again a generalized eigenvalue problem We obtain the generalized eigenvectors  X  1 , . . . ,  X  N , with  X   X  1  X  . . .  X   X   X  N . The first K eigenvectors are applied to form the mappings. Scaled by the eigenvalues, the j -th mapping function, j = 1 , . . . , K , is given by As before we define  X  = 1 /  X   X  and change (11) to the following equivalent form: and hence we can choose the K eigenvectors with the largest eigenvalues. The MLSI algorithm in dual form is summa-rized in Table 2.
 Input X  X  R N  X  M , Y  X  R N  X  L , 0  X   X   X  1 ,  X   X  0 , K &gt; 0
Steps (i) ( K x ) i,j = k x ( x i , x j ), ( K y ) i,j = k
Output mapping functions Several advantages of dual MLSI can be seen from Table 2. First of all, in contrast of solving a generalized eigenvalue problem for M  X  M matrices in primal MLSI, in dual MLSI we only need to solve a similar problem for N  X  N matri-ces. In a general indexing problem, the input dimension M (i.e., number of words) is much larger than the num-ber of documents N , and therefore working in dual form is more efficient. In the experiments we will use the dual form for indexing. Second, MLSI in dual form is ready to deal with nonlinear mappings. For this we consider a nonlinear mapping  X  : x  X  X 7 X   X  ( x )  X  F , which maps x into a high-dimensional or even infinite-dimensional feature space F , and change X to be [  X  ( x 1 ) , . . . ,  X  ( x N )] T function is accordingly defined as where we still have K x = XX T . Therefore, we can directly work with kernels (e.g., RBF kernel k x ( x i , x j ) = exp(  X  X  x x k 2 / 2  X  2 )), without knowing  X  (  X  ) explicitly. Similarly, we can define a nonlinear mapping for Y and directly work on the corresponding kernel matrix K y . Although this paper mainly considers the linear kernel to explore the linear cor-relation of inputs and multivariate labels, the formulism im-plies that the method can generally handle more complex inputs and outputs (e.g., images) by using some other suit-able kernels.
The proposed algorithm MLSI is seen to solve the same optimization problem as LSI when  X  = 0, as seen in (1) and (2). Therefore MLSI takes as special case the unsupervised LSI, or more specifically, kernel PCA [10, 11]. Kernel PCA is the dual form of PCA and turns out to solve the eigen-value problem K x  X  =  X   X  with kernel matrix ( K x ) i,j = k ( x i , x j ). To build this connection, we see from (9) that C = K x holds when  X  = 0 in MLSI. Therefore from Ta-ble 2 it is easy to check that MLSI solves the generalized eigenvalue problem which is identical to kernel PCA since K x is invertible. Un-der this situation, the regularization term controlled by  X  is just a rescaling of the cost function, as can be seen in (10). Hence  X  is just a nuisance parameter and we obtain rescaled eigenvalues compared to kernel PCA. From this perspec-tive, MLSI in general performs label informed kernel PCA or supervised kernel PCA, since it can be viewed as directly modifying the kernel matrix C with label information.
In the literature there are some other well-known super-vised projection methods, like linear discriminant analy-sis (LDA) (e.g., [13]), canonical correlation analysis (CCA) (e.g., [6, 3]) and partial least squares (PLS) [15, 9]. MLSI substantially differs from them. LDA is focusing on single classification problem where the output is one-dimensional, while in contrast MLSI considers predictions with multivari-ate labels and is thus more general. CCA finds the corre-lations between two representatives of the same documents (e.g., inputs X and outputs Y in our setting) by minimizing k v x  X  v y k 2 subject to both v x and v y being unitary and linear mappings of x i and y i (see a recent discussion in [3]). However, it does not require the projections v x and v y to promise low-reconstruction error of x and y and thus ig-nores the intra correlation of either (especially y ). Instead, MLSI takes into account all the inter and intra dependen-cies, since the projections minimize the reconstruction error of inputs and outputs simultaneously. PLS can be seen as a penalized CCA, but it cannot find a space of larger dimen-sionality than that of Y , thus its generalization performance on new dimensions of outputs is restricted (see discussions in [14]). Instead, MLSI can find in principle N orthogonal dimensions (if K x is positive definite). (I), and lower rows ((d),(e),(f)) show results with setting (II).
In this section we evaluate the proposed MLSI algorithm based on the task of multi-label text classification , in which we allow one document to be assigned to multiple labels. One can treat each classification problem separately, but these problems could have correlations between each other and could be solved simultaneously. We solve this problem by applying MLSI and encoding the labelling information into the mapping, and then each classification problem is solved independently using the projected features. By in-corporating the output information that may be difficult to reveal from inputs, the indexing is biased by the specific classification tasks and is thus more suitable for discriminate analysis.

We compare the classification performance using features learned by MLSI and normal LSI, where in the latter case no labelling information is used in indexing. Experiments are performed on two text data sets taken from Reuters-21578 and RCV1, respectively, followed by detailed discussions.
Our first data set is a text corpus which contains all the documents in Reuters-21578 that are associated with mul-tiple categories. Eliminating those minor categories that contain less then 50 documents, we have 47 categories to work with. Picking up all the words that occur at least in 5 documents, we finally obtain 1600 documents with 6076 words that are used in computing TFIDF feature vectors. In average, each document is assigned to 2.48 categories, and each category has 85 positive documents.

The other data set is a subset of the RCV1-v2 text data set, provided by Reuters and corrected by Lewis et al. [7]. The data set contains the information of topics, regions and industries for each document and a hierarchical structure for topics and industries. Since it is common that one document is assigned to multiple topics, this is an ideal data set for multi-label text classification. We use topics as the classifi-cation tasks and simply ignore the topic hierarchy. A small part of the data set is chosen, and similar preprocessing as for Reuters-21578 is done by picking up words with more than 5 occurrences and topics with more than 50 positive assignments. We end up with 3588 documents with 5496 words, and have 79 topics left. In average, each topic con-tains 180 positive documents, and each document belongs to 3.96 topics. In the following we denote  X  X euters X  and  X  X CV1 X  for these two data sets respectively.
We have two settings in this experiment. In the first setting (I), we randomly pick up 70% categories for clas-sification and employ 5-fold cross-validation with one fold training and 4 folds testing. This is a standard classification setting, and our goal is to evaluate whether MLSI trained on the training data are able to derive high-quality features for new test points and obtain good classification results. We will compare the following three cases in our experiment: 1. Original Features : A linear SVM with all the text 2. LSI : Standard unsupervised projection is performed 3. MLSI : Additional label information for training data (I), and lower rows ((d),(e),(f)) show results with setting (II). In both of the projection methods LSI and MLSI, we use the dual form in this experiment simply because this gives much improved efficiency. In case of linear kernels, this will give the same results as that in primal form. The second setting (II) aims to test the generalization performance of the pro-jection methods on new categorization tasks and compare it with using original features. For this we consider the classi-fication problems for the rest 30% categories. To make a fair comparison, we perform 5-fold cross-validation on previous unseen data (with the same size as training data), using the feature mappings derived from setting (I).
 The classification performance is compared using F 1 Macro, Micro and AUC (Area Under Curve) score. F 1 -measure de-fines a trade-off between precision and recall, and is known to be a good metric for classification evaluation. In case of multiple outputs, F 1 Macro is just the arithmetic average of F 1 measures of all output dimensions, and F 1 Micro can be seen as a weighted average that emphasize more on the accuracy of categories with more positive examples. Alter-natively, AUC score is the area under the ROC (receiver op-erating characteristics) curve, which plots sensitivity versus 1-specificity . It is known to measure the objective quality of ranking for specific classification problems. A higher AUC indicates a better ranking. It is also averaged over all the output dimensions. We also tried classification accuracy, but didn X  X  get informative comparison because most of the classification problems are very unbalanced (more than 90% of data are negative examples).

For these algorithms we choose all the parameters as fol-lows. We use LIBSVM with linear kernel and fix C = 100, which gives Original Features the best performance and is then fixed for the other two methods. For MLSI we set the parameter  X  to 0.5 after we scale K x and K y to ensure they have equal traces for balance.  X  is simply fixed as 0 to give the best performance. For both settings we repeat the experiments 50 times with randomization, and the per-formance versus dimensionality of projection is shown with means and standard deviations in Figure 1 and Figure 2 for Reuters and RCV1, respectively.

The first observation from these figures is that MLSI out-performs LSI in all the cases in setting (I). This indicates that the mapping functions in MLSI are generalizable to new test data, by incorporating the output information for the training data.

Another encouraging observation is that MLSI in most cases can even lead to better classification performance than Original Features , which uses almost 50 times more fea-tures. MLSI in this case can not only greatly accelerate the classification tasks, but also improve the performance. This is especially true for F 1 Macro and AUC score, where a large gap can be observed for all the figures. For F 1 Mi-cro the effect of MLSI is mixed, and an interesting decrease can be observed in Figure 1(e) and Figure 2(e). Consider the difference between F 1 Macro and F 1 Micro measures, we conclude that that MLSI mainly improves the classifica-tion accuracy for categories with fewer positive training ex-amples. For those categories with many positive examples, learning methods are easily to achieve good results and thus the space for further improvement is not big.

MLSI has two tunable parameters  X  and  X  that controls the kernel combination weights and the strength of regu-larization, respectively. For previous figures it is assumed fixed, and in this last experiments we study the classifica-tion performance when they varied. Since we can see similar results with setting (I), and (b),(d) show results with setting (II). results for both data sets on all the evaluation measures, we only show in Figure 3 the illustrations for Reuters with AUC score. Figures for  X  are shown with dimensionality K fixed as 50 since it is insensitive to the results.

A first impression from Figure 3 (a) (b) is that the curves are rather smooth (except when  X  approaching 1 in setting (II)). This indicates that the performance is not very sensi-tive to small changes of  X  value. When  X  increases from 0 to 1, it is seen that all the curves first increase and then de-crease, indicating that a good trade-off should be identified for best performance. When  X  approaches 0, MLSI tends to be LSI and thus unsupervised. Outputs are ignored in this case, and poor performance is observed for both settings. On the other hand when  X  approaches 1, the mappings tend to solely explain outputs Y , ignoring the intrinsic structure of inputs X . This also leads to poor performance, especially for setting (II) because the mappings are not good to gener-alize to new outputs. Overfitting occurs in this case, where a sharp decrease can be observed with even a much worse performance than LSI (  X  = 0). Finally,  X  = 0 . 5 is seen to be a good trade-off for both settings. From our experiences, a slightly larger  X  (e.g., 0.6) is better for setting (I), and a slightly smaller  X  (e.g., 0.4) is more stable for setting (II). For  X  we have the observation that small  X  leads to better performance for setting (I), while an appropriately chosen  X  is necessary for setting (II). This reflects its regularization effect, since for setting (II) new categories are considered and setting  X  = 0 will lead to overfitting.
In this paper we propose a novel indexing algorithm MLSI for multi-label informed latent semantic indexing. The map-pings are supervised and retain the statistical information of not only input features but also the multivariate outputs. We present both the primal and the dual formalisms for the linear mappings, and nonlinear mappings can also be de-rived by using reproducing kernels. The final solution ends up as a simple generalized eigenvalue problem that can be easily solved. The algorithm is applied for multi-label text classification with very encouraging results. Currently we are mainly exploiting linear dependency of inputs as well as outputs. In the near future we plan to apply the algo-rithm to other types of objects like images with suitable kernels (e.g., RBF kernels), and define kernels to explore richer structured outputs. [1] R. K. Ando. Latent semantic-space: iterative scaling [2] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [3] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. [4] T. Hastie, R. Tibshirani, and J. Friedman. The [5] X. He, D. Cai, H. Liu, and W.-Y. Ma. Locality [6] H. Hotelling. Relations between two sets of variables. [7] D. D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A [8] A. McCallum. Multi-label text classification with a [9] R. Rosipal and L. J. Trejo. Kernel partial least squares [10] B. Sch  X olkopf, A. Smola, and K.-R. M  X uller. Nonlinear [11] B. Sch  X olkopf, A. Smola, and K.-R. M  X uller. Kernel [12] B. Sch  X olkopf and A. J. Smola. Learning with Kernels . [13] J. Shawe-Taylor and N. Cristianini. Kernel Methods [14] A. N. Tikhonov and V. Y. Arsenin. Solutions of [15] H. Wold. Soft modeling by latent variables; the
