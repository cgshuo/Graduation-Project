 University of Texas at Austin Heidelberg University Vico Research and Consulting GmbH predicates given argument heads.
 disambiguation and semantic role information provided by a semantically tagged primary cor-mental conditions. However, frequency remains a major influence of prediction quality, and items. 1. Introduction
Selectional preferences or selectional constraints describe knowledge about possible and plausible fillers for a predicate X  X  argument positions. They model the fact that there is often a semantically coherent set of concepts that can fill a given argument posi-tion. Selectional preferences can help for many text analysis tasks which involve com-paring different attachment decisions. Examples include syntactic disambiguation (Hindle and Rooth 1993; Toutanova et al. 2005), word sense disambiguation (WSD,
McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and characterizing the conditions under which entailment holds between two predicates (Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al. 2007). Furthermore, selec-tional preferences are also helpful for determining linguistic properties of predicates and predicate X  X rgument combinations, for example in compositionality assessment (McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations (McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibil-ity judgments for predicate X  X rgument combinations (Resnik 1996) and effects in human sentence reading times (Pad X , Crocker, and Keller 2009).
 preferences for predicates and their argument positions. Given the immense effort nec-essary for manual semantic lexicon building and its associated reliability problems (see, e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences acquire selectional preferences automatically from large corpora.
 headword (or filler) from a corpus, and then to compute selectional preference as relative frequencies. However, due to the Zipfian nature of word frequencies, the first step on its own results in a very sparse list of headwords, in particular for less frequent predicates. As an example, the verb anglicize only appears with nine direct objects in the 100-million word British National Corpus (BNC, Burnard 1995). Only one of them, name , appears more than once. Many highly plausible fillers are missing from the list, such as word or spelling .
 it is crucial to add a generalization step that infers a degree of preference for new, unseen headwords for a given predicate and role. 1 The result is, in the ideal case, an assignment to every possible headword of some degree of compatibility (or plausibil-a high plausibility for words like the (previously seen) wordlist and surname as well as the (unseen) word and spelling , and a low plausibility for (likewise unseen) words like cow and machine .

Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al. 1990). The idea is to map all observed headwords onto synsets, and then generalize to a characteri-zation of the selectional preference in terms of the WordNet noun hierarchy. This can be achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson 2000; Clark and Weir 2001). The performance of these models relies on the coverage of the lexical resources, which can be a problem even for English (Gildea and Jurafsky 2002). An alternative approach to generalization uses co-occurrence information , either in the form of distributional models or through a clustering approach. These models, which avoid dependence on lexical resources, use corpus data for generalization (Dagan, Lee, and Pereira 1999; Rooth et al. 1999; Bergsma, Lin, and Goebel 2008). tion of selectional preferences. Our model is fully distributional and does not require any knowledge sources beyond a large corpus where subjects and objects can be iden-tified with reasonable accuracy. Its key point is to use vector space similarity (Lund and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen 724 headwords. The vector space representations which serve as a basis for computing similarity can in principle be computed from any arbitrary corpus, given that it is large enough. In particular, this need not be the same corpus as the one on which we observe predicate X  X eadword co-occurrences. Our model thus distinguishes between a primary corpus , from which the predicate X  X ole X  X eadword triples are extracted, and a generali-zation corpus for computing the vector space representations. This distinction makes it possible to apply our model to primary corpora with rich information that are too small for efficient generalization, such as domain-specific corpora or corpora with corpus is available. We empirically demonstrate the benefit of this distinction. We use
FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as generalization corpus, modeling selectional preferences for semantic roles with near-perfect coverage and low error rate. 2 (Yarowsky 1993), where the model decides which of two randomly chosen words is a better filler for the given argument position. This task tests model properties that are needed for concrete semantic analysis tasks, most notably word sense disambiguation, but also for semantic role labeling. The second task is the prediction of human plausibility ratings, which is a standard task-independent benchmark for the quality of selectional preferences. We test our model across a range of parameter settings to identify best-practice values and show that it robustly outperforms both WordNet-based and other distributional models on both tasks.
 have for their predicates. Although there is ample cognitive evidence for the existence of such preferences (e.g., McRae et al. 2005), to our knowledge, they have not been in-vestigated systematically in linguistics. However, statistics about inverse preferences have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al. 1999). We investigate the properties of inverse selectional preferences in comparison to regular selectional preferences, and show that it is possible to predict inverse prefer-ences with our selectional preference model as well.
 (using a pseudo-disambiguation task for evaluation) and further studied by Pad X , Pad X , and Erk (2007) (evaluating against human plausibility judgments). In the current text, we perform a more extensive evaluation and analysis, including the new evaluation on inverse preferences, and we introduce a new similarity measure, nGCM, which achieves excellent performance in many settings. 2. Computational Models of Selectional Preferences
In this section, we provide an overview of corpus-based models of selectional prefer-ences. See Table 1 for a summary of the notation that we use.  X   X   X   X  2.1 Historical Models by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual dis-ambiguation process between predicates and their modifiers. Sentences are semantically anomalous if there are no mutually consistent readings for the two words. Semantically anomalous sentences would receive no reading, whereas ambiguous sentences would receive several readings.
 was later criticized. A case in point is metaphors, which often combine predicates and arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated that  X  X ejecting utterances is just what humans do not. They try to understand them. X 
He proposes to reconceptualize selectional restrictions as preferences whose violation is dispreferred, but not fatal. His proposal for a semantic interpretation mechanism still uses semantic primitives, but always produces a single most plausible interpretation by choosing the senses of each word that maximize the compatibility between selectional preferences and semantic types. In this manner, he is able to compute semantic repre-sentations for sentences that violate selectional restrictions, including metaphors such as  X  X y car drinks gasoline. X  2.2 Semantic Hierarchy X  X ased Models
The first broad-coverage computational model of selectional preferences, and still one of the best-known ones, namely that of Resnik (1996), belongs to the class of semantic hierarchy X  X ased models. These models generalize over observed headwords using a semantic hierarchy or ontology for nouns. The two main advantages of such models are that (a) they can make predictions for all words covered by the hierarchy, even for very infrequent ones for which distributional representations tend to be unreliable; and (b) the hierarchy robustly guides generalization even for few observed headwords. which can be observed in syntactically analyzed corpora. More specifically, it concen-726 trates on selectional preferences for subjects and objects. For the generalization step,
Resnik X  X  model maps all headwords onto WordNet synsets (or classes) c . Resnik first computes the overall selectional preference strength for each verb X  X elation pair ( v , r ), the distribution of WordNet synsets for this particular verb X  X elation pair is compared to the distribution of synsets over all verbs, given the relation r . Technically, this is achieved using Kullback X  X eibler divergence: ( v , r , a ) and the membership of nouns a in WordNet classes c : The observed frequency of ( v , r , a ) is split equally among all WordNet classes for a . This avoids word sense disambiguration, but incurs a certain share of wrong attributions. The intuition of will have a posterior distribution over classes that strongly diverges from the prior.
Finally, the selectional preference between a verb, a relation, and an argument head is defined as the maximal selectional association of the verb, the relation, and any WordNet class c that the argument can instantiate. We will refer to this model as differ from Resnik X  X  model in the details of how the generalization in the WordNet hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a tree cut through the WordNet noun hierarchy that minimizes tree cut length while maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by ascending the WordNet noun hierarchy as long as the degree of selectional preference among siblings is not significantly different. Ciaramita and Johnson (2000) encode
WordNet in a Bayesian Network to take advantage of the Bayes nets X  ability to  X  X x-plain away X  ambiguity. Grishman and Sterling (1992) perform generalization on the basis of a manually constructed semantic hierarchy specifically developed on the same corpus. 2.3 Distributional Models
Distributional models do not make use of any lexicon resource for the generalization step. Instead, they use word co-occurrence X  X ypically obtained from the same corpus as the observed headwords X  X or generalization. This independence from manually constructed resources gives distributional models a good cost X  X enefit ratio and makes them especially attractive for domain-specific applications. These models, like the semantic hierarchy X  X ased models, usually use grammatical functions as the set Roles for which selectional preferences are predicted.
 latent classes of noun X  X erb pairs with soft clustering. They model the probability of aword a as the argument of a predicate v as the probability of generating v and a independently from the latent classes c : Pereira, Tishby, and Lee (1993) develop a task-specific procedure to optimize P ( c ),
P ( v | c ), and P ( a | c ). Their procedure supports hierarchical clustering and can optimize the number of clusters. Rooth et al. (1999) present a simpler Expectation Maximization X  based estimation procedure which takes the number of clusters as input parameter. We refertothismodelas ROOTH ET AL . herein.
 occurrence probabilities with similarity-based smoothing. Although not intended as a model of selectional preferences, it can also be interpreted as such. Given a similarity measure sim defined on word pairs, they compute the smoothed occurrence probability of a word w 2 given w 1 as where Simset ( w ) is the set of words most similar to w according to sim ,and Z ( w  X  Simset ( w 1 ) sim ( w 1 , w ) is a normalizing factor. This model predicts w backing off from w 1 to words w similar to w 1 . The contribution of each w in predicting
P ( w 2 | w 1 ) is weighted by sim ( w 1 , w ). The similarity sim ( w space representations.
 proach to the prediction of selectional preferences. The features they use are mainly co-occurrence statistics, enriched with morphological context features to alleviate sparse data problems for low-frequency argument heads. They train one SVM per verb X  argument position pair, using unobserved verb X  X rgument combinations as negative examples, which makes their approach independent of manually annotated training data. Schulte im Walde et al. (2008) present a model that combines features of the semantic hierarchy X  X ased and the distributional approaches by integrating WordNet into an EM-based clustering model; Schulte im Walde (2010) shows that integrating noun X  X odifier relations improves the prediction of human plausibility judgments. 2.4 Semantic Role X  X ased Models
The third class of models takes advantage of semantic resources beyond simple seman-tic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the prediction of selectional preferences for semantic roles rather than grammatical func-tions. From a linguistic perspective, semantic roles represent a more appropriate level for defining selectional preferences. For that reason, the role annotation provides cleaner and more specific training data than even a manually syntactically annotated corpus 728 would. These advantages, however, come at the cost of considerably greater sparsity issues.

Johnson, and Petruck 2003). This model estimates selectional preferences with a gen-probability of observing the thematic role r , the verb v , and the argument a ,plusthe verb X  X  FrameNet sense c and the grammatical function gf of the argument. This joint probability can be decomposed using the chain rule:
The model does not make any independence assumptions. To counteract sparse data issues for the more complex terms, the model applies WordNet-based generalization (for nouns), distributional clustering (for verbs), and Good X  X uring smoothing. We refer to this model as PADO ET AL . Another semantic role X  X ased model was proposed by Vandekerckhove, Sandra, and Daelemans (2009). It acquires selectional preferences for
PropBank roles from a PropBank-labeled corpus, generalizing to unseen headwords with memory-based learning. 3. A Distributional Exemplar-Based Model of Selectional Preferences:
We now present the EPP model of selectional preferences. It falls into the category of distributional models. More specifically, it is an exemplar model that remembers all seen headwords for a given argument position and computes the degree of plausibility for a new headword candidate through its similarity to the stored exemplars. Exemplars are modeled as vectors in a semantic space.
 ogy (Nosofsky 1986), in computational linguistics (under the name of memory-based learning [Daelemans and van der Bosch 2005]), and in linguistics, particularly phonet-ics (Hay, Nolan, and Drager 2006). The appeal of exemplar models is that they provide a cognitively plausible process of learning as storing exemplars, and categorization as similarity computation that is grounded in features of the exemplars (e.g., formants in phonetics, and contexts in lexical semantics).
 with work in psycholinguistics by McRae, Ferretti, and Amyote (1997), who studied the characterization of verb selectional preferences through features elicited from human subjects. They found high overlap between features used to characterize the selectional
For example, features generated for the agent role of frighten include mean , scary ,and ugly , features that were also highly relevant for the typical filler noun monster . modular to different notions of argumenthood, such that it is also applicable to the computation of selectional preferences for syntactic dependents of a predicate, as this is an important case for computational applications. When we compute selectional prefer-ences for syntactic dependents rather than semantic roles, we view syntactic argument positions as noisy approximations of semantic roles. 3.1 The Model
As stated previously, we assume that we have two corpora which assume different func-tions in the model: the primary corpus, which provides information about predicate X  argument co-occurrences but may be too sparse for generalization; and the large, but potentially noisy, generalization corpus, from which we obtain reliable semantic simi-larity estimates.
 be the set of argument headwords seen with an argument position r of a predicate v in the primary corpus. Given these triples, we predict the plausibility for an arbitrary noun a 0 in position ( v , r ) through the semantic similarity of a of Seenargs ( r , v ). We obtain these similarity ratings by first computing vector space representations for both and the members of seen ( r , v ) from the generalization corpus, and then using a standard vector space similarity measure. We compute the plausibility for a 0 as where sim ( a 0 , a ) is the similarity between the vector space representations of a a , wt r , v ( a ) a weight for the seen headword a ,and Z not matter. Because Selpref EPP is basically a weighted average over similarity values, the
Section 3.3). We discuss possible choices of both the similarity sim and the weight wt in Section 3.3. 3.2 Vector Space Representations
We use vector space representations for generalization. In a vector space model, each target word is represented as a vector, typically constructed from co-occurrence counts with context words in a large corpus (the so-called basis elements ). The underlying assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar meanings occur in similar contexts and will be assigned similar vectors. Thus, the distance between the vectors of two target words, as given by some distance measure (e.g., Cosine or Jaccard), reflects their semantic similarity .
 vide has found a wide range of applications. Examples in NLP include information retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette 1994), and predominant sense identification (McCarthy et al. 2004). Lexical resources based on distributional similarity (e.g., Lin [1998] X  X  thesaurus) are used in a wide range of applications that profit from knowledge about word similarity. In cognitive science, they have been used, for example, to account for the influence of context on human lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald 2000).
 is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the 730 to the members of the exemplar cloud for the agent position X  X amely, poacher , policeman , and director . Due to the high average similarity of the hunter vector to these vectors, hunter will be judged a fairly good agent of shoot . Compare this with the result for the of shoot than, for example, director . 3.3 Formalization and Parameter Choice Vector space models have been formalized by Lowe (2001) as tuples VS = ( DTrans ,
Basis , sim , STrans ), where Basis is a set of basis elements or dimensions, DTrans is a transformation of raw co-occurrence counts, sim is a similarity measure, and STrans is a transformation of the whole space, typically dimensionality reduction. An additional parameter that becomes relevant for our use of vector spaces (cf. Equation [6]) is the weighting function wt that determines the contribution of each exemplar to the overall similarity. We discuss the parameters in turn and discuss our reasons for either explor-ing them or fixing them.

Basis elements Basis . Traditionally, context words are used as basis elements, and co-occurrence is defined in terms of a surface window. Such bag-of-words spaces tend to group words by topics. They ignore the syntactic relation between context items and the target, which is a problem for selectional preference modeling. The top table in
Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though they show complementary plausibility ratings. The reason is that deer and hunter often co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities).
The bottom table in Figure 1(b) indicates a way out of this problem, namely the use of word-relation pairs as basis elements (Grefenstette 1994; Pad X  and Lapata 2007).
This space splits the co-occurrences with context words such as shoot based on the grammatical relation between target and context word, and this split looks different for different words: whereas deer occurs exclusively as the object of shoot , hunter pre-dominantly occurs as the subject. We find the reverse pattern for escape . In consequence, the resulting spaces gain the ability to distinguish between words like hunter and deer , based on differences in typical occurrences in argument positions.
 word-based spaces because they require a corpus with syntactic analysis. Thus, we explore both options. The word-based space records co-occurrences within a surface window of 10 (lemmatized) words. 3 We refer to it as WORDSPACE based space, called DEPSPACE , has basis elements consisting of a grammatical function concatenated with a word, as in the bottom example in Figure 1(b) (Pad X  and Lapata 2007). Following earlier experiments on the representation of selectional preferences in word-dependency-relation spaces (Pad X , Pad X , and Erk 2007), we use a subject X  object context specification that only considers co-occurrences between verbs and their subjects and direct objects. 4 In each case, we adopt the 2,000 most frequent context items as basis elements.

Similarity measure sim . In principle, any similarity measure for vectors can be plugged into our model. Previous studies that compared similarity measures came to various conclusions about the usefulness of different measures. Cosine similarity is very popu-lar in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice emerged in first place. Pad X  and Lapata (2007) found good results with Lin X  X  measure for predominant word sense identification.
 ize to new tasks, we will investigate a range of similarity measures shown in Table 2:
Cosine ,the Dice and Jaccard coefficients, Hindle  X  X  (1990) and Lin  X  X  (1998) mutual information-based metrics, and an adaptation of Nosofsky X  X  (1986) Generalized Context Model (GCM), a model for exemplar-based similarity from psychology. The original
GCM includes normalization by summed similarity over all classes of exemplars, which introduces competition between categories. Our version, which we call nGCM, instead normalizes by vector length to alleviate the influence of overall target frequency, but 732 preserves the central idea that similarity decreases exponentially with distance (Shepard 1987).
 basis elements, with the exception of the Lin measure, whose definition applies only to dependency-based spaces. The reason is that it decomposes the basis elements into relation X  X ord pairs ( r , v ). For semantic spaces with words as basis elements, the Lin measure can be adapted by omitting the random variable r (cf. Pad X  and Lapata 2007).
Transformations DTrans and STrans . Next, we come to transformations on counts and vec-tor spaces. Concerning the count transformations DTrans , all counts are log-likelihood transformed (Dunning 1993), a standard procedure for word-based semantic space models which alleviates the problematic effects of the Zipfian distribution of lexical items, as proposed by Lowe (2001). As for transformations on the complete space STrans , many studies do not perform dimensionality reduction at all. Others, like the LSA fam-ily of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To gauge the impact of STrans , we compare unreduced spaces (2,000 dimensions) to 500-dimensional spaces created using Principal Component Analysis (PCA), a standard method for dimensionality reduction that identifies the directions of highest variance in a high-dimensional space.

Weight functions wt. Exemplar-based models are usually applied in conjunction with a function that can assign each exemplar an individual weight, which can be interpreted cognitively as degree of activation (Nosofsky 1986). We assess a small number of weight functions to investigate their importance within the UNI , assumes a uniform distribution, wt r , v ( a ) = 1. The second one, quent exemplars should be both more activated and more reliable. Finally, we consider a weight function that is an analogue of inverse document frequency in Information
Retrieval. It weights words higher that occur with a smaller number of verb X  X ole pairs: wt r , v ( a ) = log for which a occurs as a headword. 5 We abbreviate this weight function by  X  X iscrimination X . 3.4 Discussion
Our EPP model can be seen as a straightforward implementation of the intuition to model selectional preference by generalizing from seen headwords to other, similar, words. We use vector space representations to judge the similarity of words, obtaining a completely corpus-driven model that does not require any additional resources and is very flexible. A complementary view on this model is as a generalization of traditional vector space models that represent semantic similarities between pairs of words. The
EPP model goes beyond this by computing similarity between a vector and a set of other vectors. By instantiating the set with the vectors for seen headwords of some relation r , the similarity turns into a plausibility prediction that is specific to this relation. data are available; no lexical resource is required. Additionally, it does not require the headword observation step and the generalization step (cf. Section 1) to use the same corpus. 6 This allows us to work with a relatively small and deeply linguistically ana-lyzed corpus of seen headwords, the FrameNet corpus, while using a much bigger data set to generalize over seen headwords. It also allows us to make predictions for the potentially deeper relations annotated in the primary corpus, for example, semantic roles. We will investigate the potential of this setup in our Experiments 1 and 2.
One is a coverage problem due to the limited size of the resource (see the task-based evaluation in Gildea and Jurafsky [2002]). For example, the semantic role X  X ased ET AL . model resorts to class-based smoothing methods to improve coverage, which EPP does not need. The other problem of resource-based models is that the shape of the
WordNet hierarchy determines the generalizations that the models make. These are not a high preference because tragedy in WordNet is a type of written communication, which is a preferred argument class of answer .
 , but has complementary benefits and problems. Querying the probabilistic ET AL . model takes only constant time, whereas querying the exemplar-based model takes time linear in the number of seen arguments for the argument position.
However, the ROOTH ET AL . model requires a dedicated training phase with a space complexity linear in the total number of verbs and nouns, which can lead to practical problems for large corpora (cf. Section 5.1). The separation of similarity computation and headword observation in EPP also gives the experimenter more fine-grained control over the types and sources of information in the model.
 (1999). However, they differ in the role of the similarity measure: The Dagan, Lee, and
Pereira model computes a co-occurrence probability, and it uses similarity as a weight-ing scheme. The EPP model computes similarity (of a word to the typical fillers of an argument position), and its weighting schemes are separate from the similarity measure.
The two models also differ in the kinds of items they consider as a basis for generaliza-tion (or smoothing): In computing the probability of seeing a word w in the Dagan, Lee, and Pereira model runs over all words that are similar to w the sum in the EPP model runs over all words that have been seen as headwords in the argument position in question. Given that occurrence in an argument position is a form of co-occurrence, and similarity (in both models) is computed on the basis of vectors derived from co-occurrence counts, one could say that the sum in the over words determined by first-order co-occurrence, whereas the sum in Dagan, Lee, and Pereira runs over words chosen through second-order co-occurrence (where w w are second-order co-occurring if they both tend to occur with the same words w 4. Design of the Experimental Evaluation
In this section, we give a high-level overview over the experiments and experimental settings we will use subsequently. Details will be provided in the following sections. selectional preference models with a pseudo-disambiguation task (Experiment 1).
Then, we address the task of predicting human verb X  X rgument plausibility ratings (Experiment 2). Finally, we investigate inverse selectional preferences X  X references of 734 nouns for the predicates that they co-occur with X  X gain using pseudo-disambiguation (Experiment 3).
 Section 2: RESNIK as a hierarchical model; ROOTH ET AL . as a distributional model; and
PADO ET AL . as a semantic role X  X ased model. As both Brockmann and Lapata (2003) and Pad X  (2007) have argued, no WordNet-based model systematically outperforms the others, and the RESNIK model shows the most consistent behavior across different scenarios. Among the distributional models, we choose ROOTH ET AL performs soft clustering and thus shows a marked difference to the knowledge, this is the first comparison of all three generalization paradigms: semantic hierarchy X  X ased, distributional, and semantic role X  X ased. disambiguation and the prediction of human plausibility ratings. The pseudo-disambiguation task (Yarowsky 1993) has become a standard evaluation measure for selectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al. 1999). Given a choice of two potential headwords, the task of a selectional preference model is to pick the more plausible one to fill a particular argument position of a given predicate.
Pseudo-disambiguation can be viewed as a word sense disambiguation task in which the two potential headwords together form a  X  X seudo-word, X  for example herb/struggle from the original words herb and struggle . The task is to  X  X isambiguate X  the pseudo-version of semantic role labeling and dependency parsing (depending on whether the relations are semantic roles or grammatical functions) (Zapirain, Agirre, and M X rquez 2009). In this case, the scenario is that of a sentence containing a predicate and two words that could potentially fill an argument position of that predicate, for example, the predicate recommend with the potential headwords herb and struggle for the grammatical relation of direct object. The task is to decide which of the two potential headwords is better suited to fill the argument position.
 grained distinctions than those occurring in pseudo-disambiguation tasks. Here, mod-els predict the exact human ratings for verb X  X rgument X  X ole triples. Ratings are collected to further control carefully selected experimental items for psycholinguistic studies (Trueswell, Tanenhaus, and Garnsey 1994; McRae, Spivey-Knowlton, and Tanenhaus plausibility models (Brockmann and Lapata 2003; Pad X  2007).
 ment positions. In the SEM PRIMARY setting, the predicates are FrameNet frames, each of them potentially instantiated by multiple different verbs. The argument positions in these settings are frame-semantic roles. This setting most closely matches the notion of selectional preferences as characterizations of semantic arguments of an event. In addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument positions are grammatical functions (subject and direct object). Viewing grammatical functions as shallow approximations of semantic roles, we can expect the selectional preference models for this setting to yield noisier estimates than in the setting. The two settings will differ only in the choice of primary corpus, but will use the same generalization corpus. PRIMARY setting on an example from a pseudo-disambiguation task: The setting has predicates like the FrameNet frame (predicate sense) A semantic role T HEME as argument position. In contrast, the predicates that are verb lemmas, such as cause , and argument positions that are gram-matical functions (subj). In both settings, the two potential headwords (here called headword and confounder , to be explained in more detail in the next section) to be distinguished in the pseudo-disambiguation task are noun lemmas.
 more coarse-grained and noisy characterizations of selectional preferences; however, they can be extracted from corpora with only syntactic annotation. We are therefore able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could parse almost all of the corpus, resulting in 6,005,130 parsed sentences. annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck 2003). FrameNet is a semantic lexicon for English that groups words in semantic classes called frames and lists fine-grained semantic argument roles for each frame. Ambiguity is expressed by membership of a word in multiple frames. Each frame is exemplified with annotated example sentences extracted from the BNC. The FrameNet release 1.2 comprises 131,582 annotated sentences (roughly three million words). To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser. perimentation with two different primary corpora allows us to directly study the influ-ence of the disambiguation of predicates and the semantic characterization of argument positions on the performance of selectional preference models. Note, however, that the comparison is complicated by differences between the two corpora: The primary corpus for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the determination of predicates, grammatical functions, and headwords. The primary cor-pus for the SEM PRIMARY setting is manually annotated for semantics but is parsed automatically to determine headwords. This can introduce noise in the headwords, but not in the determination of predicates and semantic roles. Also, the primary corpus for the SYN PRIMARY setting is much larger than the one used in the 5. Experiment 1: Pseudo-Disambiguation
The first experiment uses a pseudo-disambiguation task to evaluate the models X  perfor-mance on modeling the plausibility of nouns as headwords of argument positions of verbal predicates.
 736 Require: Some corpus N : a list of noun lemmas, along with a function freq 1: N mid = { n  X  N | freq N ( n )  X  30 and freq N ( n )  X  2: We define a probability distribution p N over the n  X  N 3: conf = {} # set of headword/confounder mappings, starts empty 4: A T = { a | ( v , r , a )  X  T } # set of seen headwords 5: for every a in A T do 6: choose a confounder a  X  N mid according to p N 7: conf = conf  X  X  a  X  a } 8: end for 9: Return: conf 5.1 Setup
Task and data. Inadatasetoftuples( v , r , a )ofapredicate v , argument position r ,and headword a , each tuple is paired with a confounder a . The task is to pick the original
In the SYN PRIMARY setting, this corresponds to all headwords observed in subject or direct object position of a verbal predicate in the BNC, and in the to all nouns observed as headword of some semantic role in a frame introduced by a sample that is balanced by the corpus frequency of predicates and argument position.
As test set, we choose 100 ( v , r ) pairs at random, drawing 20 pairs each from five fre-quency bands: 50 X 100 occurrences; 100 X 200 occurrences; 200 X 500; 500 X 1,000; and more equally from six frequency bands of arguments a : 1 X 50 occurrences; 50 X 100; 100 X 200; 200 X 500; 500-1,000; and more than 1,000 occurrences. These evaluation samples contain a total of 213,929 ( SYN ) and 65,902 ( SEM )tuples.
 as described in Figure 2. 8 In the literature, there have been two different approaches to choosing confounders for pseudo-disambiguation tasks: The first approach, used by
Dagan, Lee, and Pereira (1999), chooses confounders to match the headword a in frequency. The second approach, used in Rooth et al. (1999), sets the probability that a word is drawn as a confounder to its relative frequency. The advantage and dis-advantage of the first approach is that it largely eliminates the frequency bias that is a general problem of vector space-based approaches. This is an advantage in that it allows the generalization achieved by the model to be evaluated without any distortion from frequency bias. It is a disadvantage in that in any practical application making use of selectional preferences, the data will not be frequency-balanced. For example, selectional preferences could be used by a dependency parser to decide which word in the sentence to link to a given verb via a subject edge, or selectional preferences could be used by a semantic role labeler to decide which constituent is the overall best filler for the A GENT role for a given predicate. In such cases, it does not appear warranted to assume that the frequencies of different headword candidates are balanced. We choose the second option for our experiments, using relative corpus frequency to approximate the probability of encountering different headword candidates.
 Training of models. As stated earlier, we evaluate all models in the and the SEM PRIMARY setting. In all experiments herein, we perform two 2-fold cross-validations runs. In each run, we randomly split the respective ( sample into a training and a test set at the token level. Figure 3 describes the experimen-tal procedure in pseudo-code.
 evaluation sample. The EPP model additionally uses the BNC as generalization corpus in both the SYN PRIMARY setting and the SEM PRIMARY setting. This generalization corpus is used to compute either a WORDSPACE or a DEPSPACE discussed in Section 3.3. For the ROOTH ET AL . model, we had to employ a frequency Require: Aset Formalisms of formalisms to test
Require: A primary corpus T : a list of triples ( v , r , a ) of seen predicates, argument
Require: A mapping conf : Lemmas  X  Lemmas of headwords to confounders such that 1: eval _ results = {} 2: for splitno in 1:2 do 3: # prepare two independent splits 4: half 1 = {} , half 2 = {} # mappings from headwords to counts 5: for each tuple t in T do 7: Sample k  X  B( freq T ( t ), 0 . 5) 8: half 1 = half 1  X  X  t  X  k } , half 2 = half 2  X  X  t  X  freq 9: end for 10: splits = { ( half 1, half 2), ( half 2, half 1) } 11: for ( f train , f test )in splits do 12: for each formalism F in Formalisms do 13: train a model m F according to formalism F using the training set defined by 14: for each tuple ( v , r , a )in T do 15: for iin1: f test ( v , r , a ) do 16: Evaluate the performance of m F on the tuple ( v , r , a , conf ( a )) and add the 17: end for 18: end for 19: end for 20: end for 21: end for 22: Return: eval _ results 738 cutoff of five in the SYN PRIMARY setting to reduce the amount of training data due to memory limitations. The PADO ET AL . model is only used in the
FrameNet is an integral part of this model, and it cannot be used in a syntax-only setting without major changes. For details on training, see Section 2.4. Note that no verb classes had to be induced from the data, because the predicates v are already instantiated by verb classes, namely, FrameNet frames (see Table 3).
 very simple. It decides between the headword a and the confounder a by comparing the frequencies f ( a )and f ( a ). The second, more informed, baseline is triple frequency ( language model ( LM ), was constructed by training a 2-gram language model from the large English ukWAC Web corpus (Baroni et al. 2009) using the SRILM toolkit (Stolcke 2002) with default Good X  X uring smoothing. We retained only verbs, nouns, adjectives, and adverbs in order to maximize the proximity between verbs and their subjects and objects. We defined the preference score for verb X  X ubject triples as the probability of the
Evaluation. For all models, we report two evaluation figures. One is coverage :Atuple is covered if the model assigns some preference to both a and a , and the preferences are not equal. The second is error rate , which is the relative frequency, among all covered tuples, of instances where the confounder was at least equally preferred. Both coverage and error rate are averages over the 2 x 2 cross-validation runs in each setting. bootstrap resampling (Efron and Tibshirani 1994). This procedure samples correspond-ing model predictions with replacement from the set of predictions made by the models to be compared and computes the difference in error rates. On the basis of n such samples ( n = 1,000), the empirical 95% confidence interval for the difference in strength on the basis of all observed differences is computed. If the interval includes 0, the difference is not statistically significant. 5.2 SYN PRIMARY Setting: Results
Table 4 shows the results for the SYN PRIMARY setting. The overall best error rate is achieved by a variant of the EPP model, with the RESNIK model coming in second (the performance difference is significant at the 0.05 level). The near-perfect coverage, whereas the RESNIK model delivers results only for 63% of the data points. We found a very high error rate and a comparatively low coverage for
ROOTH ET AL ., which most likely stems from the data pruning necessary to reduce the training data (compare the subsequent results in the SEM PRIMARY
ET AL . model was not tested in the SEM PRIMARY setting, because it requires semantic role annotation. The HW baseline is somewhat below chance (50%), which is an effect of our by-token sampling procedure, according to which confounders often have higher corpus frequencies than the real arguments. The TRIPLE baseline has a better error rate than the LM baseline, but has very low coverage. Both the outperform the baselines in terms of error rate. That they outperform the baseline in terms of error rate indicates that we sometimes have confounders that have actually been seen more often with the verb X  X rgument pair than the headword, but that are dissimilar from other seen headwords, which allows RESNIK them as confounders in spite of their higher co-occurrence frequency.
 very high (0.95 or higher), independent of space, similarity measure, and dimensionality reduction. We generally observe that error rates are lower when word meaning is represented in DEPSPACE , and when discrimination weighting is used. In nGCM works best, yielding the overall best result with an error rate of 25.6 X 25.7%. In
WORDSPACE , the Lin measure shows the best error rates with an error rate of just below 27%. These results hold both for the unreduced and the reduced spaces and are highly significant (p  X  0.01). Hindle is clearly the worst measure at around random performance. 740 between FREQ and DISCR is less uniform. In DEPSPACE , the difference between the best measure with and without PCA (nGCM in both cases) is not significant; in the difference between the best measure with and without PCA (Lin in both cases) is significant (p  X  0.01).
 into two distinct groups: Lin, nGCM, and Cosine on the one hand and Jaccard, Dice, and
Hindle on the other, with a significant difference in performance between the groups (p  X  0.01). The use of dimensionality reduction through PCA improves performance for all similarity measures, in WORDSPACE as well as DEPSPACE especially marked for the Dice and Jaccard measures, which perform at the level of a random baseline for unreduced spaces. We assume that these set intersection-based measures benefit from the independent dimensions that PCA produces. For the simi-larity measures with best performance, the improvement through PCA is less marked. Thus, PCA-reduced spaces show more similar error rates across similarity measures.
After PCA, only nGCM and Lin still significantly (p  X  0.01) outperform the others in DEPSPACE ,andin WORDSPACE , Lin is the only measure that performs significantly differently from the rest (p  X  0.01).
 argument frequency on error rate. Figure 4 examines the performance of the with different similarity measures and weighting schemes by argument frequency bins (cf. the subsection Task and Data in Section 5). We find that the overall best weighting scheme, DISCR , also works best for all except the highest argument frequency bin. In the DEPSPACE setting (upper row), all similarity measures show a frequency bias in that error rate is lower for more frequent arguments, but this bias is much less pronounced in Cosine and nGCM than in the other measures, with error rates varying between 45% and 25% rather than 80% and 20%. (Dice and Hindle, not shown here, exhibit similar behavior to Jaccard.) In PCA-transformed DEPSPACE (middle row), this frequency bias largely disappears for all similarity measures. In WORDSPACE there is again a frequency bias in all similarity measures, Lin now joins Cosine and nGCM in being much less biased than Jaccard, Dice, and Hindle. For with PCA-transformation, not shown here, the curves resemble those of PCA-transformation.
 on error rate. Predicate X  X rgument position pairs were sampled from five frequency bins. The figure shows DISCR weighting only. In the spaces without dimensionality reduction, there is a clear division between Cosine, nGCM, and Lin on the one hand, and Jaccard, Dice, and Hindle on the other. In PCA spaces, all measures except for
Hindle are similar in their performance. In both DEPSPACE decreases towards the higher frequency predicate bins, although this is not so in 742
WORDSPACE . It seems that in the sparser DEPSPACE , models can still profit from the additional seen headwords in the highest predicate frequency bins, whereas in the less sparse but noisier WORDSPACE , the added noise is stronger than the added signal in the highest predicate frequency bins. For the lowest predicate frequency bins, the best results in WORDSPACE are better than those in DEPSPACE . 5.3 SEM PRIMARY Setting: Results
Table 5 shows the results for the SEM PRIMARY setting, where we predict head words for pairs of a frame (predicate sense) and semantic role. In comparison to the setting (Table 4), error rates are lower across the board. The difference for the is on average around 10%.
 attribute this to the extensive generalization mechanisms that the model uses, which draw on an array of lexical X  X emantic resources. However, with a coverage of 59%, the model is still unable to make predictions for many of the test items. Error rates for the RESNIK and the EPP models are comparable, at 16.5% for the best EPP variant. The two models differ sharply in coverage, however: 62.8% for
RESNIK , consistent with the findings of Gildea and Jurafsky (2002), and between 90% and 98% for EPP variants. The RESNIK model also profits from the presence of semantic disambiguation in the SEM PRIMARY setting (in the SYN PRIMARY rate was 28%), which underlines the substantial impact that properties of the training data have on semantic hierarchy X  X ased models of selectional preferences.
ET AL . now has perfect coverage, affirming our assumption that the very bad results of the ROOTH ET AL .modelinthe SYN PRIMARY setting were an artifact of the data sampling necessary for that data set. Although its error rate of 24.9% is a substantial improvement over all baselines, the EPP model achieves error rates that are up to 9 points lower at a comparable coverage. Among the baselines, in the SYN PRIMARY setting, arguments have some tendency of having lower frequency than the confounders. The TRIPLE baseline shows near-random performance, at very low coverage, a result of the very small size of the corpus. Because there is no large corpus with frame-semantic roles, nor is the annotation easily linearizable, we could not compute a LM baseline in the SEM PRIMARY setting.
 non-significant advantage for DEPSPACE among the best models. Overall error rates show the same clear divide between the three high-performing similarity measures (Cosine, nGCM, and Lin) and the three weaker ones (Dice, Jaccard, and Hindle). Di-mensionality reduction again dramatically improves the weaker models, with Jaccard yielding the best result for the PCA-reduced WORDSPACE . 9 trizations in the SYN PRIMARY setting used DISCR weighting, it is now that yields the best results.
 showing the performance of different variants of the EPP model over six argument frequency bins. The upper row shows DEPSPACE without dimensionality reduction.
Note that FREQ weighting now works especially well for the lowest argument frequency bin, much better than DISCR and PLAIN . This is the opposite of what we saw for the
SYN PRIMARY setting in Figure 4. With DISCR and PLAIN weighting, Jaccard and Lin again have noticeable problems with the lowest argument frequency bins X  X s in the
PRIMARY setting X  X ut not with FREQ weighting. With DEPSPACE reduction (middle row), we get error rates of  X  26% for all settings and all frequency bins. On the lowest frequency bin, we again see a large advantage of over the two other weighting schemes. The bottom row shows dimensionality reduction. Note that there is much less variation in error rates across frequency bins here than in unreduced DEPSPACE .
 only, as this showed the best results on this data set. The figure clearly illustrates the divide between the top and the bottom three similarity measures in as the disappearance of this divide for both PCA settings. In unreduced 744 the divide is not as clearly visible. The figure also indicates a slight tendency for error rates to rise for the lowest-frequency as well as the highest-frequency predicates, across all spaces. 5.4 Discussion
The resource-based approaches that we tested, RESNIK and PADO ET AL performance when they have coverage (which coincides with findings in other lexical semantics tasks that supervised data, when available, always increases performance), but showed low coverage, at most 63% ( RESNIK , SYN PRIMARY achieves near-perfect coverage at good error rates: In the
RESNIK model achieved an error rate of 28%, and the best EPP the SEM PRIMARY setting, error rates were 7% for the PADO ET AL the RESNIK model, and 16% for the best EPP variant. Comparing the
ET AL . models in the SEM PRIMARY setting, we find that the use of an additional gen-eralization corpus in the EPP model seems to offset any advantages introduced by the joint clustering of predicates and arguments.
 is striking. Even though the FrameNet corpus is smaller and a sparse data prob-lem might be expected, models perform at considerably lower error rates in the
PRIMARY setting than when the primary corpus is the larger BNC. This underscores the point that selectional preferences belong to a predicate sense rather than a predicate lemma, and that they describe the semantics of fillers of semantic roles rather than of syntactic dependents (recall that in this setting, we predict head words for pairs of a predicate sense and semantic role). In the SEM PRIMARY setting, the data is cleaner, so it is expected that seen headwords of an argument position will be more semantically uniform. This has a strong influence on model performance. Another factor contributing to the difference in performance between the two data sets may be that the primary corpus in the SYN PRIMARY setting is parsed automatically, whereas manual annotation is available in the FrameNet corpus. However, although this manual annotation iden-tifies predicate senses, role headwords are still determined through automatic parsing.
The division of the training data into a primary and a secondary corpus allows us to successfully use FrameNet as the basis for semantic space X  X ased similarity estimates despite the fact that this corpus alone would be too small to sustain the construction of a robust space.

Lin, and nGCM show good performance across all spaces and parameter settings; Dice and Jaccard work comparably only on spaces that use dimensionality reduction. The Hindle measure is an underperformer in all conditions. With Lin, Jaccard, Dice, and Hindle, error rates rise sharply for less frequent arguments in many spaces. Although
Cosine and nGCM also have some frequency bias, it is much less pronounced. nGCM seems to work well with sparse data sets that are not too noisy, as evidenced by the fact that it has the best performance among all EPP variants on both 746 the SYN PRIMARY setting, as well as in all SEM conditions except reduced The Lin measure seems to work well with noisier data: It is the best using WORDSPACE in the SYN PRIMARY setting. Cosine, although never showing the top performance, is among the best models in any setting. Although dimensionality reduction only improves the overall error rates of the best models by a few points, it has two important consequences: First, dimensionality reduction reduces dependence of the results on the exact similarity measure chosen, as all measures except Hindle show nearly indistinguishable error rates on reduced spaces (Figures 5 and 7). Second, low-frequency arguments profit by a huge margin when PCA is used (Figures 4 and 6).
Among weighting schemes, DISCR weighting seems to be most useful when the data is sparse but somewhat noisy (as is the case in the lower argument frequency bins in the SYN PRIMARY setting). Frequency weighting seems to work best when the data is either not sparse (as in the highest argument frequency bin in the or very clean but sparse (as in the lowest argument frequency bin in the setting). A comparison of the two vector spaces, DEPSPACE no clear winner. When the collections of seen headwords are noisier, as they are in the
SYN PRIMARY setting, DEPSPACE , with its more aggressive filtering, yields the better results. Sets of headwords collected by predicate sense, as in the are sparser but cleaner, and WORDSPACE shows lower error rates. 6. Experiment 2: Human Plausibility Judgments Experimental psycholinguistics affords a second perspective on selectional preferences:
The plausibility of verb X  X rgument pairs has been shown to have an important effect on human sentence processing (e.g., Trueswell, Tanenhaus, and Gransey 1994; Garnsey et al. 1997; McRae, Spivey-Knowlton, and Tanenhaus 1998). In these studies, plausibility was operationalized as the thematic fit or selectional preference between a verb and its argument in a specific argument position. Models of human sentence processing there-fore need selectional preference models (Pad X , Crocker, and Keller 2009). Conversely, psycholinguistic plausibility judgments can be used to evaluate computational models of selectional preferences. 6.1 Experimental Materials
We present evaluations on two plausibility judgment data sets used in recent studies.
Tanenhaus (1998). Our example in Table 6, which is taken from this data set, was elicited by asking study participants to rate the plausibility of, for example, a hunter shooting ( AGENT ) or being shot ( PATIENT ). The data point demonstrates the McRae set X  X  balanced structure: 25 verbs are paired with two argument headwords in two argument positions each, such that each argument is highly plausible in one argument position
The resulting distribution of ratings is thus highly bimodal. Models can only reliably predict the human ratings in this data set if they can capture the difference between verb argument positions as well as between individual fillers. However, because the verb X  X rgument pairs were created by hand and with strict requirements, many of the arguments are infrequent in standard corpora (e.g., wimp, bellboy ,or knight ). When
FrameNet is used to annotate senses for the verbs, no appropriate senses are available for 28 of the 100 verb X  X rgument pairs, reducing the test set to 72 data points. structed on the basis of corpus co-occurrences (Pad X  2007). Eighteen verbs are combined with their three most frequent subjects and objects found in the Penn Treebank and
FrameNet corpora, respectively, up to a total of 12 arguments. Each verb X  X rgument pair was rated both as an agent and as a patient (i.e., both in the observed and an unobserved argument position), which leads to a total of 24 rated triples per verb. The data set contains ratings for 414 triples. The resulting judgments show a more even distribution of data. With FrameNet annotation for the verbs, appropriate senses are not attested for six verb X  X rgument pairs, reducing the test set to 408 data points. 6.2 Setup We evaluate the same four models as in Experiment 1: EPP , the WordNet-based model, the distributional ROOTH ET AL . model, and the semantic role X  X ased
ET AL . model. We again compare a SYN PRIMARY setting, where the models make pre-dictions for pairs of a verb and a grammatical function, with a for which the two test data sets were annotated with verb sense and semantic roles in the FrameNet paradigm (Pad X  2007) and where models make predictions for pairs of a frame and a semantic role. As before, the PADO ET AL . model is only tested in the
PRIMARY setting. 11 For the EPP model, we focus on parsed, dimensionally unreduced spaces and DISCR weighting, following earlier results (Pad X , Pad X , and Erk 2007). We provide results for the best WORDSPACE models from Experiment 1 for comparison. The primary corpora for training selectional preference models were prepared as in Experiment 1 (cf. Section 5.1). The generalization corpus for
For the ROOTH ET AL .modelinthe SYN PRIMARY setting, we again used a frequency cutoff. We found the RESNIK model to perform better when using just a subset of the
BNC (namely, all the triples for verbs present in the test set). 6.3 Evaluation Procedure
We evaluate our models by correlating the predicted plausibility values with the human judgments, which range between 1 and 7. Because we do not assume a priori that there 748 is a linear correlation between the two variables, we do not use Pearson X  X  product-moment correlation, but instead Spearman X  X   X  , a non-parametric rank-order correlation coefficient. 12 Note that significance is harder to reach the smaller the number of data points is.
 predicts the plausibility of each item as its frequency in the BNC ( (
SEM ), respectively. With regard to an upper bound, we assume that automatic models of plausibility should not be expected to surpass the typical human agreement on the plausibility judgment. This is roughly  X   X  0 . 7 for the Pado data set. 6.4 McRae Data Set: Results and Discussion
Table 7 focuses on EPP variants with unreduced DEPSPACE for the McRae data set. We see that this data set is rather difficult to model. None of the models trained in the
PRIMARY setting achieves a significant correlation. 13 Apparently, the FrameNet corpus is too small to acquire selectional preferences that generalize well to the infrequent items that make up the McRae data set. In the SYN PRIMARY predictions reach significance.
 that we are considering. For EPP , we only show nGCM as the best-performing similarity measure from the pseudo-disambiguation task, and Cosine as a widely used vanilla measure. The results for the SEM PRIMARY setting (left-hand side) mirror the results for the SEM PRIMARY setting in Experiment 1: The deep PADO ET AL best correlation (it is the only model to predict human judgments significantly). It overcomes the sparseness in the FrameNet corpus by using semantic verb classes that are particularly geared towards grouping the existing verb occurrences in the way that is most meaningful for this task. It covers about 80% of the test data. full coverage, and although it does not make statistically significant predictions, it shows substantially higher correlation coefficients than ROOTH ET AL
The DEPSPACE and WORDSPACE variants of EPP perform similarly here, and the simple frequency baseline has very low coverage and correlation.
 better results in the SYN PRIMARY setting than in the SEM PRIMARY
ET AL . model obtains a highly significant correlation. The combination of infrequent headwords in the McRae data set and the large primary corpus brings out the benefits that the ROOTH ET AL . model can derive from generalizing from verbs and nouns to and for this reason, the difference from the best EPP model is not significant. SYN PRIMARY setting, the EPP DEPSPACE models clearly outperform the because of the DEPSPACE models X  more aggressive filtering. Interestingly, still performs poorly in the SYN PRIMARY setting: WordNet does not make the right generalizations to capture the selectional preferences at play in the McRae data, no matter how much training data is available. This is underscored by an analysis of which
WordNet classes were most frequently determined as the strongest association with 100 test cases for the McRae data ( SYN PRIMARY setting), a data set where plausibility is determined by factors much more fine-grained than animacy. (In the of 72 test cases.) The frequency baseline again performs badly. 6.5 Pado Data Set: Results and Discussion
We now turn to the Pado data set. Again, we first focus on the performance of differ-ent similarity measures in EPP using unreduced DEPSPACE (Table 9). Correlation with human judgments is much better than for the McRae data set, and highly significant for all SEM PRIMARY setting models and three of the SYN PRIMARY both settings, Cosine and Lin are the best measures (difference not significant), followed by nGCM. Hindle comes out worst once more. The difference between the strong and 750 weak measures is more pronounced for the SYN PRIMARY setting, compared with the SEM PRIMARY setting. Coverage is at or close to 100% throughout.
 we consider. In the SEM PRIMARY setting (where both the data and the primary corpus have FrameNet annotation), EPP and the deep PADO ET AL . model predict the human judgments similarly well (difference not significant). Because all verbs in this data set are covered by FrameNet, the PADO ET AL . model also shows a nearly perfect cover-at p  X  0.01). ROOTH ET AL . has the lowest coverage at 88%, but this is still higher than its coverage of the McRae data. As with the McRae data, ROOTH ET AL correlation in the SYN PRIMARY setting than the SEM PRIMARY the frequency cutoff does not harm performance as much in Experiment 2 as it did in Experiment 1. However, the coverage of ROOTH ET AL . is lower in the setting, perhaps because the SEM PRIMARY setting smoothes rare verbs by grouping them in frames with other verbs. RESNIK also achieves better correlation in the
PRIMARY setting, but recall that it was trained on a subset of the BNC only to reduce noise in the training data X  X hen trained on the whole BNC set, performance degrades to  X  = 0 . 060. The difference from the best EPP model remains numerically large. As for the McRae data set, the EPP WORDSPACE models show much worse performance than the DEPSPACE models, and do not significantly predict the human plausibility ratings. its correlations hover around zero, which underlines our intuition that verb X  X rgument combinations can be plausible without being frequent in corpora. An example is the combination (to) embarrass (an) official , which is rated as highly plausible, but occurs only once each in the BNC and FrameNet. 6.6 Discussion
The McRae data set seems in general more difficult to account for than the Pado data set, as noted by Pad X , Pad X , and Erk (2007). They explain it by a general frequency effect in the BNC data (which are a superset of the FrameNet data): The median frequency of the hand-selected McRae nouns in the BNC is 1,356, as opposed to 8,184 for the corpus-derived Pado nouns.

ROOTH ET AL . models generally do worse than EPP both in terms of coverage and quality of predictions. One notable exception is the excellent performance of the
ET AL . model on the McRae data in the SYN PRIMARY setting, which comes, however, with a low coverage of less than 50%. A closer inspection of the predictions showed that ROOTH ET AL . makes many predictions for verb X  X bject pairs but abstains from subjects, thus reducing the complexity of the task. For only 20% of verbs, predictions are made for subjects and objects. As noted in Pad X , Pad X , and Erk (2007), the relatively poor performance of the RESNIK model may be explained by the fact that its ability to generalize is limited to the structure of WordNet, where some semantic distinctions are easier to make than others. For example, a fairly easy distinction to make for WordNet-based models is animate vs. inanimate. Because the Pado set contains a portion of inanimate arguments with animate counterparts, the RESNIK
In contrast, in the McRae test set, all arguments are animates, and thus similar to one another in terms of WordNet.
 ments on both data sets, but it is limited to the SEM PRIMARY best model is not always among the EPP DEPSPACE models, they consistently show a coverage of close to 100%, and are generally statistically indistinguishable from the best model. Unlike ROOTH ET AL .and RESNIK , whose performance varies widely between the SEM PRIMARY setting and the SYN PRIMARY setting, the correlation coefficients for the EPP models are generally similar across settings. We take this as evidence that models can extract relevant information from deeper annotation on small corpora as well as from large, but noisy and shallow, training data.
 on unreduced DEPSPACE . The picture differs somewhat between the two data sets, but the Cosine measure performs well overall, with Lin and nGCM generally in second and third place. So, the group of the three best similarity measures is the same as in
Experiment 1, but Cosine shows better performance. One possible reason for this lies in the verb frequency, which is relatively high in both data sets: 68% of the McRae verbs and 83% of the Pado verbs have BNC frequencies of 1,000 and more, whereas
Experiment 1 used an equal number of predicates from five frequency bins, the highest being 1,000 and more occurrences. In that highest predicate frequency bin, Cosine consistently performed as well as Lin or better in Experiment 1 (Figures 5 and 7). 752 7. Experiment 3: Inverse Selectional Preferences
The term selectional preference is typically used to describe the semantic constraints that predicates place on their arguments. In this section, we will investigate how nominal arguments place semantic constraints or expectations on the predicates with which they occur. Such expectations can be thought of as typical events that involve the given object. For example, a noun like apple could be said to have preferences about its inverse be verbs like grow or fall ;forits inverse object position , apple probably prefers verbs like eat, cut ,or plant . We will use the term inverse selectional preference to refer to preferences of nouns for their predicates, distinguishing them from regular selectional preferences .
 role pair. Still, inverse selectional preferences warrant a closer look: To what extent do inverse selectional preferences differ from regular ones? And are the tasks of predicting regular and inverse selectional preferences equally difficult? We start in Section 7.2 with an exploratory data analysis of inverse selectional preferences, which shows that inverse selectional preferences show semantically coherent patterns like regular selectional preferences, but that, in contrast to most verbs, nouns tend to occur with multiple semantic groups of verbs. In Sections 7.3 X 7.5, we test the disambiguation task for inverse selectional preferences. 7.1 Related Work
In computational linguistics, some approaches to characterizing selectional preferences have used the symmetric nature of their models to characterize nouns in terms of the verbs that they use (Hindle 1990; Rooth et al. 1999). However, they do not explicitly compare the two types of preferences. Also, there are approaches using selectional preference information, in particular for word sense disambiguation and related tasks, that could be characterized as using regular along with inverse selectional preferences (Dligach and Palmer 2008; Erk and Pad X  2008; Nastase 2008). By comparing selectional preference model performance on the tasks of predicting inverse and regular selectional preferences in Sections 7.3 X 7.5, we hope to contribute to an understanding of what can be achieved by using inverse preferences in word sense analysis tasks.
 research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a particularly plausible argument for the existence of expectations of nouns for their predicates in human language processing is head-final word order (as in Japanese or in German subordinate clauses), where hearers may encounter all objects before the head. It is likely that these objects are immediately integrated into a preliminary event structure with an assumed predicate instead of being stored in short-term memory until the predicate is encountered (Konieczny and D X ring 2003; Nakatani and Gibson 2009).
Another strand of work is McRae et al. (2001, 2005), who have studied priming of verbs from nouns. They found that a noun engenders priming of verbs for which it is a typical agent, patient, instrument, or location.
 ical entries of nouns has been formulated in the context of Pustejovsky X  X  generative lexicon (Pustejovsky 1995), where the qualia roles TELIC and respectively. Pustejovsky uses this knowledge to account, for example, for the interpre-tation of logical metonymy ( begin a book ). Although qualia roles are instantiated with individual predicates rather than characterizations of all possible events, construction and use are arguably two very salient events for an object. Through the data exploration preferences. 7.2 Empirical Analysis of Inverse Selectional Preferences
The first question we ask concerns the selectional preference strength of regular and inverse selectional preferences, using the measure introduced by Resnik (1996) to de-termine the degree to which verbs select for nouns, and vice versa. As verb X  X ole pairs, we re-use the same 100 pairs that were used for the pseudo-disambiguation task in
Experiment 1. For the comparison, we randomly sample a total of 100 noun/inverse-role pairs from the BNC, using the same five frequency bands as for the verbs (50 X  100, 100 X 200, 200 X 500, 500 X 1,000, &gt;1,000). The sample contains approximately the same number of (inverse) subject and object roles.
 case: Unlike Resnik, we compute KL divergence not on a distribution across WordNet synsets, but on a distribution across lemmas.

For regular selectional preferences, w 1 is a verb lemma, w For inverse preferences, w 1 is a noun lemma, w 2 a verb lemma, and r an inverse role.
SelStr ( w 1 , r ) can be interpreted as a measure of the degree to which w preferences concerning the role r . We induce the probability distributions through maximum likelihood estimation on the BNC.
 tional preference strength. It is not possible that inverse selectional preference strength would be uniform throughout if regular selectional preference strength varied between verbs. After all, if we fix the relation r for the time being, P ( v lated through Bayes X  formula. Instead, the questions we will ask are more specific.
Are regular and inverse preference strengths similar in size? Are regular and inverse preference strengths similar by frequency band X  X hat is, do frequent nouns behave similarly to frequent verbs? And what effects do we see of the prior distributions P ( n and P ( v | r )? band for verbs and nouns. As expected, we see substantial strengths in both regular and inverse preferences. Both parts of speech show the same pattern of decreasing KL divergences for higher-frequency words, presumably because frequent words tend to be polysemous, and can combine with many different words. However, the strengths for inverse selectional preferences are in general lower than those for regular preferences. pair might differ, in general, from the number of verbs seen with each noun X  X ole pair. However, we find that verbs and nouns occur with roughly the same number of associates in the frequency bands up to the 200 X 500 band. In the band 500 X 1,000, verbs appear with roughly one third more nouns than nouns appear with verbs, and in the band of 1,000 occurrences or more, verbs appear with twice as many nouns (on average) 754 as nouns appear with verbs in this band (1,189 vs. 636). Incidentally, the fact that the highest-frequency verbs (which also tend to be the most ambiguous) appear in a much larger number of contexts than the highest-frequency nouns could be a contributing factor to the well-known problem that verbs are harder to disambiguate than nouns.
For the lower frequency bands, number of associates is unlikely to be the reason for the weaker inverse preferences. Instead, a more likely reason for the overall weaker inverse preferences lies in the overall distributions of nouns and verbs in the BNC.
Both show a Zipfian distribution, but there are 15,570 verbs as opposed to 455,173 nouns. Recall that KL divergence will be high whenever the individual terms to be summed are large. This, in turn, is the case when p ( w be small when the distribution p ( w 2 | r ) ranges over a larger number of words w regular selectional preferences, the w 2 are nouns, and for inverse preferences the w verbs. Because there are many more nouns than verbs, the denominator p ( w be smaller for regular preferences.
 regular selectional preferences, we next do a qualitative analysis, looking at association strength SelAssoc for individual triples verb X  X ole X  X oun and noun X  X nverse-role X  X erb. We adapt Equation (2) to the lexicon-free case and obtain
Table 12 shows the five strongest associates for one verb X  X ole pair and one noun X  X ole pair from each frequency band. The associates on both sides of the table generally are semantically coherent and make intuitive sense. However, there is an interesting difference between the verbs and nouns: We find that the nouns X  preferred verbs can often be grouped loosely into several meaning clusters, whereas the verbs X  associates tend to group into one cluster per grammatical function. For example, predicates taking wheat as objects fall into those describing production ( grow, sow ) and those describing processing ( shred, grind, mill ). Similarly, the predicates found for pill either concern and other special events. Another observation that we can make in Table 12 is that the nouns X  most preferred associates have a similarly large share in the nouns X  overall selectional preference strength as the verbs X  most preferred associates have in the verbs X  ences is similarly skewed towards the most preferred associate for verbs and nouns. preference strength than regular preferences, but that may be due more to specifics of the formula used rather than the skewness towards preferred role fillers. Two differ-ences do emerge, though. First, noun selectional preferences show more semantic filler sets than verb preferences. Second, the highest frequency verbs appear with many more different associates than the highest frequency nouns. 7.3 Modeling Inverse Selectional Preferences
In the rest of this section, we test selectional preference models on the task of pre-dicting inverse selectional preferences in a pseudo-disambiguation task, and compare the results to the performance on predicting regular preferences in Experiment 1. We do not repeat Experiment 2 even though it would have been technically possible to 756 re-use the McRae and Pado data sets and predict plausibility judgments through inverse preferences. However, the data sets combine each verb with both plausible and im-plausible nouns, but they do not combine each noun with different verbs in a balanced fashion, so a repetition of Experiment 2 with inverse preferences would not be very informative.
 butional models can, in general, be used straightforwardly to model both regular and inverse selectional preferences. This is different for models like the WordNet noun hierarchy to represent regular selectional preferences. To model inverse preferences, it would be necessary to use the WordNet verb hierarchy. However,
WordNet organizes verbs in a comparatively flat, unconnected hierarchy with a high branching factor formed by the hypernymy/troponymy ( X  X ype of X ) relation. This makes effective generalization difficult, in particular in conjunction with the marked variation in the set of preferred predicates that we observed for inverse selectional preferences in Section 7.2.
 case as follows. Let a stand for a noun, r for an inverse argument position of this selectional preference Selpref EPP of ( r , a ) for a verb v as weighted average similarity to seen verbs: 7.4 Pseudo-Disambiguation: Experimental Setup
We evaluate inverse selectional preferences on a pseudo-disambiguation task that is set up completely analogously to our experiments on regular preferences in Section 5: given a noun, an inverse argument position, one verb observed in this position, and a confounder verb, distinguish between the two verbs. We use the 100 nouns sampled across five frequency bands that we already used in Section 7.2. We experiment with both WORDSPACE and DEPSPACE models, but restrict our attention to which showed good results in Experiment 1.
 PRIMARY setting) and FrameNet ( SEM PRIMARY setting). Subsequently, we will use the SYN PRIMARY setting again, but not the SEM PRIMARY setting. In the setting, the roles are FrameNet frame elements (semantic roles). However, frame ele-ments are specific to a single frame, for example, the frame element R the frame R OPE _M ANIPULATION . 15 It would thus be pointless to predict a verb frame given a noun and a frame element name, as the frame element already gives away the frame. 7.5 Pseudo-Disambiguation: Results and Discussion
Table 13 shows the results of testing the EPP model for inverse selectional preferences on pseudo-disambiguation. Coverage is very good for all model variants, similarly to
Experiment 1. The error rates, as well, are close to those for the regular preferences in the SYN PRIMARY setting (cf. Table 4). The best model there ( with DISCR weighting) achieved an error rate of 25.6%, and the best model for inverse preferences ( WORDSPACE ,Linwith DISCR weighting) reaches an error rate of 27.2% here. Lin shows the best error rates in all conditions, closely followed by nGCM (the difference is significant in WORDSPACE and the reduced DEPSPACE in the unreduced DEPSPACE ). The Hindle similarity measure again brings up the rear.
In PCA-transformed spaces, the error rates are similar across all similarity measures except for Hindle, as in Experiment 1.

The best WORDSPACE model (Lin without PCA) reaches significantly better error rates (p  X  0 . 01) than the best DEPSPACE model (Lin with PCA). We think that the reason for this lies in the fact that for inverse selectional preferences, the true associate and the confounder that need to be distinguished in the pseudo-disambiguation task are verbs rather than nouns. A noun will probably have more other nouns in a bag-of-words context window than a verb would other verbs, which will make it easier to distinguish verbsina WORDSPACE than to distinguish nouns. A DEPSPACE , in contrast, will bring out differences in the immediate syntactic neighborhood of nouns even if they occur in the same sentence. 8. Conclusion In this article, we have presented a similarity-based model of selectional preferences, . It computes the selectional fit of a candidate role filler as a weighted sum of seman-tic similarities to headwords observed in a corpus, in a straightforward implementation 758 of the intuition that plausibility judgments should generalize to fillers with similar meaning. Our model is simple and easy to compute. In common with other distri-butional models like Rooth et al. (1999), it does not depend on lexical resources. Our model derives additional flexibility from distinguishing between a primary corpus (for observing headwords) and a generalization corpus (for inducing semantic similarities).
This allows it to use primary corpora with deeper semantic annotation that are too small as a basis for computing vector space representations.
 that can be viewed as an abstraction of both word sense disambiguation and semantic role labeling, as well as on the prediction of human plausibility judgments. The model achieves similar error rates to the semantic hierarchy X  X ased erably higher coverage, and it achieves lower error rates than the clustering model. The semantic role X  X ased PADO ET AL . model, although highly accu-rate in its predictions, has much lower coverage and needs a semantically annotated corpus as a basis. We have also demonstrated that our model is able to meaningfully model inverse selectional preferences, that is, expectations of nouns about verbs for which they appear as arguments.
 across the three tasks we have considered. nGCM, Lin, and Cosine are the best-performing similarity measures throughout. The good performance of the nGCM mea-sure, an exponential similarity measure, is particularly noteworthy. We found it to work well on data sets that are sparse and not too noisy, whereas the Lin similarity measure achieved better performance when the data was noisy (see Section 5.4 for details). Di-mensionality reduction (PCA) on the vector space raises the performance of the Jaccard and Dice similarity measures to a similar level as the best three. More importantly, PCA neutralizes a strong frequency bias that otherwise leads to a large performance drop on rare arguments. Concerning weighting schemes, we found that frequency-based weighting works well when the data is either clean or not too sparse. In the face of sparse noisy data, DISCR weighting (a variant of tf/idf) is helpful. Comparing bag-of-words X  based and dependency-based vector spaces, DEPSPACE s are sparser but cleaner than
WORDSPACE s. Accordingly, DEPSPACE s are at an advantage when many headwords are available, making efficient use of this information, whereas for predicates with few seen headwords because they are less affected by sparseness. representation of selectional preferences for polysemous verbs such as address , whose direct object can either be a person, or a problem. Polysemy leads to headwords with lower similarity among them than for non-polysemous verbs, which in turn can lead to artificially low plausibilities for all fillers. In the of polysemous verbs are separated into different frames. In future work, we hope to improve our SYN PRIMARY setting models by clustering the seen headwords, and then computing plausibility of new headwords relative to the nearest cluster. quisition of fine-grained information about nouns. As we discussed in Section 7, the preferred verbs for a noun can often be grouped into meaning clusters. In future work, we plan to investigate whether there are groups of predicates that recur across similar nouns, and how they can be characterized. We expect some groups to correspond to
Pustejovsky X  X  qualia (Pustejovsky 1995), which constitute particularly salient events for an object, namely, their creation and typical use. However, we expect corpus data to yield a more complex picture of the events connected to a noun, which manifest themselves in the form of additional, more specific meaning clusters. Acknowledgments References 760 762
