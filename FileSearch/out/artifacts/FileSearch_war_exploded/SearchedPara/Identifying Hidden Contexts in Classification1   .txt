 In classification tasks some variables dir ectly predict the class label, others can describe context. Contexts are artifacts in the data, which do not directly predict the class label, like accent in speech recognit ion. Taking contexts into the learning process can help to build more speciali zed and accurate classifiers [2], solve sample selection bias [12], concept drift [17] problems.

Context may not necessar ily be present in a form of a single variable in the feature space. To recover hidden contexts the input data can be clustered [5, 9, 15]. The problem is, that clustering can capture some class label information, which would shade away the contexts. Consider a diagnostics task, where patient tests are taken by two pieces of equipmen t, which are calibrated differently. If we cluster patient data, the resulting clusters might correspond to  X  X ealthy X  and  X  X ick X  (which are the classes) or  X  X ample taken by equipment A X  and  X  X ample taken by equipment B X  (which is a context), but likely a mix of both. Thus, to capture context specific information, we intend to force independence between contexts and class labels. In addition, capturing noise is undesired, therefore the resulting contexts need to be non-random and stable.

Context identification has predictive and descriptive goals. Grouping the data provides an opportunity to achieve more accurate classification employing con-text handling strategies, as well as better understand the phenomenon behind the data. Context identification can be c onsidered as a preprocessing step in su-pervised learning, like feature selection, instance selection, or recovering missing values. We aim for a filter approach, where contexts are generic, not tied with a particular handling strategy.

In this study we propose three techniques for identifying hidden contexts, which force independence between the contexts and class labels. The objective is to output an explicit grouping of the data. We require the grouping to ignore the class label information. Thus, we aim to hide class discriminatory information before partitioning. These techniques can be used in different context handling strategies or for forming new ones.
 We analyze the performance of the proposed techniques on thirty real datasets. We also present a case study, which illustrates one example strategy for handling the identified contexts in classification.

The paper is organized as follows. Section 2 defines context. Section 3 discusses related work. In Section 4 we propose three techniques for identifying hidden contexts. Sections 5 and 6 present experi mental evaluation. Section 7 concludes the study. Consider a classification problem in p-dimensional space. Given a set of instances with labels ( X  X  X  , y  X  X  ) the task is to produce a classifier h : X X  X  .In this study we define context as a secondary label z of an instance X ,which is independent from the class label y , but can explain the class label better when used together with the pre dictive features. That is, p ( y | z )= p ( y ), but p ( y | X  X  ,z ) = p ( y | X  X  ), where X  X   X  X . Context might be expressed as a variable in the feature space (known context) or as a latent variable (unknown context).
Consider as a toy example a task, where a patient is diagnosed  X  X ick X  or  X  X ealthy X  based on the body temperature. It is known that in the evening people tend to have higher temperature independently of being sick or healthy. If we know the context, i.e. whether the temperature was measured in the morning or in the evening, diagnostic task is easy, as illustrated in Figure 1. However, if the time is unknown, then diagnosing becomes problematic. The time itself is independent from the class label, stand alone it does not diagnose.

Some more examples of context include accent in speech recognition, light in image recognition, seasonality in sales prediction, weekday in electricity load or bus travel time prediction, industry crisis in credit scoring.

The context label may be directly observable or hidden , depending on the application. Hidden context variable z is not explicitly present as a feature x in the feature space X , but information about it is assumed to be captured in the feature space X , i.e. z = f ( X ), X  X  X  . An example of directly observable context is time. Customer segments in ma rketing tasks or bankruptcy prediction represent hidden context. Different segments might have different behavior.
Evaluation of the identified contexts is not straightforward. Different context handling strategies can lead to different gains or losses in the classification accu-racies, which are not necessarily due to good or bad context identification. We require the resulting contexts to be independent from the class labels, valid (not random grouping of the data) and stable on random subsamples of the same data. The criteria to measure these as pects are formulated in Section 5. Context-awareness is widely used in ubiquitous and pervasive computing to char-acterize the environmental variables [14]. In machine learning the term usually characterizes the features that do not determine or influence the class of an ob-ject directly [2, 15]. A wide body of liter ature on concept drift considers only time context [7, 17]. Typically contex ts assumed to be known. Mixture models (see [4]) can be considered as an approach to identify hidden contexts.
Our context identification techniques are novel as they force independency from the class labels. A need for such approaches was mentioned before in a light of context handling strategies [16] and multiple classifier systems [3]. Turney [16] formulated the problem of recovering imp licit context information and proposed two techniques: input data clustering and time sequence (which we leave out of the scope assuming that the chronological order is unknown). Turney expressed a concern that clustering might capture class label information and indicated a need for further research, our work can be seen as a follow up.

A recent work by Dara et al [3] explores th e relation between the characteris-tics of data partitions and final model accuracy in multiple classifier systems. The work experimentally confirms the benefits of partitions which are not correlated with the class labels. These results support the motivation of our work.
As a result of their analysis Dara et al [3] propose a semi-randomized partition-ing strategy to cluster and then swap som e instances across the clusters, which can be seen as a mixture of clustering an d boosting. We excluded this strat-egy from our investigations after preliminary experiments, since even though it pushes towards independence in class labels within the partitions (which is our objective as well), due to randomization the procedure of assigning an unseen instance to one of the partitions can no longer be deterministic.
 Extracting hidden context is related to the context handling strategies [16]. The strategies are not limited to building a separate classifier or a combination for each context. Contextual information can be also used to adjust the input data, model parameters or model outputs. Analysis of the performance of dif-ferent handling strategies is out of the cope of this study. The identified and validated contexts can be used as building blocks to handling strategies. In this section we present techniques for identifying hidden contexts. Given a dataset, the task is to allocate the instances into k groups, so that the data within the groups is related, but the groups are not related to the class labels.
Context identification techniques require two mechanisms: (1) how to group the training data X into k contexts and (2) how to assign an unseen instance X  X  X to one of the contexts.

Clustering (CLU) of the input data is the baseline technique to identify hid-den contexts, when building local models [5, 9 X 11]. The procedure is summarized in Figure 4. Clustering captures closen ess of the data instances in the feature space. For classification tasks the feature space is typically formed with an in-tention to predict the class labels. If class membership information is strongly present in the data, clustering is likely to capture it as well.

To overcome this issue we propose Overlay (OVE) technique. To hide the label information we move the classes on top of each other, as illustrated in Figure 2, by normalizing each class to zero mean. The technique assumes that class discriminatory information lies in the class means. We cluster the overlayed data to extract contexts. Unfortunately, for the incoming new data we cannot do overlay, because the labels are unknown. We solve this by introducing a su-pervised context learning. Given the instances X we treat the obta ined contexts z as labels and learn a classifier z = K OVE ( X ). We use the diagonal linear discriminant [4] as a classifier K OVE . The procedure is summarized in Figure 4.
Overlay technique is based on the assumption that the class distributions are symmetric across different contexts, which often might not be true. We generalize Overlay by introducing Projection (PRO) technique, which rotates the data to hide the class label information. The idea is opposite to Linear Discriminant Analysis (LDA) [4]. The goal is to find a transformation that minimizes between-class variance and maximizes with in-class variance, see Figure 3.

We seek to find a transformation w to obtain a projection  X  x = w X . Within-class c i covariance is s 2 i = y covariance is S b := 1 c c i =1 (  X  i  X   X  )(  X  i  X   X  ) ,where  X  is the mean of all the data.
In LDA Fisher criterion J ( w )= w S b w w S problem transforms into eigenvalue decomposition S  X  1 s S b w =  X  w .Wechoose the eigenvector w corresponding to the smallest eigenvalue min  X  . To determine contexts z , we transform the training data into 1D space  X  x = w X and simply split the range of values into k equal intervals (like slicing a loaf of bread). An unseen instance X is transformed into 1D  X  x = w X and assigned a context, based on the interval, to which it falls into. The procedure is presented in Figure 4.
In addition to Overlay and Projection, we explore Feature underselection (FUS) technique, which discards the features, that are the most correlated with the class label, and clusters the remaining features. It is described in Figure 4.
All the presented techniques assume that k is given. In the case study (Sec-tion 6) we will show, how k can be determined using the stability criterion. The goal of the experiments is to compare the introduced techniques in terms of the desired properties: not to capture the class labels, at the same time con-trolling, that the resulting partitions are valid and stable. 5.1 Evaluation Criteria To measure the three desired characteristics (independence from the class labels, validity and stability) we adopt metrics commonly used in clustering evaluation.
For measuring independence between the labels and the identified con-texts, we employ Normalized Mutual Information (NMI), which is widely used to assess clustering performance [18]. It evaluates the purity of clusters with re-spect to class labels. For the context identification task low NMI is desired. For two random variables y and z NMI ( y , z )= I ( y , z ) / H ( y ) H ( z ), where I ( y , z ) is the mutual information, H ( x )and H ( z ) are the respective entropies. Note, that NMI  X  [0 , 1] and NMI ( y , y ) = 1. Given the context assignments z and the respective class labels y , the NMI is estimated [18] as where n l is the number of data instances contained in the cluster C l (1  X  l  X  k ),  X  n h is the number of instances belonging to the class h (1 number of instances that are in the intersection between the cluster C l and the class h ,and n is the total number of instances.
 Measuring validity. If we optimized only NMI, assigning instances to contexts at random would be the optimal solution. To control that the identified contexts are not random, we require the identified context labels to be learnable from the data. We use the Naive Bayes (NB) classifier. VAL ( z | X ) is the error rate of NB using 10 fold cross validation, the smaller the better. We normalize it w.r.t. random assignment of contexts NVAL ( z | X )= VAL ( z | X ) / VAL (  X  ( k ) | X ), where  X  ( k )isasetofcontexts( k ) assigned at random, thus NVAL  X  [0 , 1]. Measuring stability. In addition to independence and validity we want to minimize the chance of overfitting the training data, which we measure using the stability index for clustering proposed in [13]. The dataset X is at random split into two sets of equal size { X u  X  X v } = X . Each subset is clustered us-ing the same clustering algorithm u = clust u ( X u ), v = clust v ( X v ), clust u () and clust v () denotes fixed parameterizations resulting after clustering (e.g. clus-ter centers in k-means). Then the fixed clust v () is applied to the subset X u to obtain alternative cluster assignment ` u = clust v ( X u ). If clustering is stable, given a correct permutation u  X  = map ( u ) of cluster labels ` u and u  X  should be the same. The (in)stability index is the share of different cluster assignments smaller (in)stability ( STA ) the better. We normalize STA w.r.t. to random as-signment to get NSTA  X  [0 , 1]. 5.2 Datasets and Experimental Protocol We test the techniques on thirty real classification datasets, which are diverse in size, dimensionality, number of classes and the underlying problems they repre-sent. The characteristics are summarized in Table 1. We do not expect all of the datasets to have distinct underlying contexts and we do not know the true num-ber of contexts. Thus, we use the stability measure in the evaluation to indicate whether the found contexts are persistent in the data.

In the experiments we fix the number of contexts to k = 3 for all the datasets (no specific reason). Feature underselection technique requires to specify the number of features, we choose m = 5 for all the datasets. We normalize the feature values of the input features to fall in the interval [0,1], add 1% random noise and transform the data according to its principal components. Noise does not distort class discriminatory information, neither it influences the allocation of contexts. Noise and principal compon ent rotation are needed to prevent ill posed covariance matrixes of some high dimensional datasets.

We empirically explore and compare fi vetechniques:OVE,PRO,FUS,CLU and RAN. CLU is an ordinary clustering, which we use as the baseline method. Overlay (OVE), Projection (PRO) and Feature underselection (FUS) are the three context identification techniques introduced in this paper. RAN is a bench-mark partitioning technique (sanity check), which assigns contexts uniformly at random. In this study we use k-means as the base clustering technique. 5.3 Results Table 2 presents the results aggregated into three groups based on the dimen-sionality of the datasets: small (up to 10 features), medium (10-19 features) and large (more than 20 features) and all together. The results are plotted in Figure 5, where each dot represent one dataset. The figure shows that in terms of not capturing class labels (NMI) Overlay and Projection are doing well, while Feature underselection and the baseline Clustering are doing not that well. High NMI is consistent with higher validity, where Clustering outperforming the others, as presented in Table 2.

For all the techniques the validity deteriorates with increase in dimension-ality. It can be expected, challenges of measuring distance in high dimensional space has been widely acknowledged [1]. FUS technique demonstrates the worst validity in high dimensional space. It can be explained by relatively low number of the selected features (we fixed m =5).

Clustering has the best validity and stability, but captures a part of class discriminatory information, as expected, especially in low dimensional tasks. Projection has fine independence, good validity but it is rather unstable. This is mostly due to slicing of the resulting 1D projection. Likely, the resulting cut points might be not optimal and induce instability.
Feature underselection has surprisingly low stability as well as mediocre valid-ity in high dimensional tasks. This is explainable by a fixed number of features m in our experiments (for comparability across datasets). In high dimensional spaces the selected features make rather small share of all features and are thus more likely to represent noise rather than context or predictive information. Good news is that for designing individual context handling strategies that can be resolved by manipulating m value.

Overlay has good independence and stability, while not so good but acceptable validity. This is due to supervised learning procedure to assign context to unseen instances. It introduces extra uncertainty , while the other techniques can identify context for an unseen instance directly.

To sum, all three techniques avoid capturing class label information well and show similar validity; in terms of stability, Overlay technique is preferable. The following case study illustrates how context information can be used to benefit the final classification. We present it as a proof of concept rather than an attempt to select the most accurate cont ext handling strategy. The case study focuses on determining the number of contexts ( k ) from the data and building one local classifier for each context.

We use adult dataset (alphabetically). We consider it suitable because of two reasons: the task (predicting income of a person) intuitively is context dependent and the dataset is relatively large ( &gt; 30 th. instances).

We evaluate the accuracies using six ba se classifiers: decision tree (CART), logistic regression, Naive Bayes, linear discriminant (LDA), neural network (with 4 hidden layers), 1-nearest neighbor (1 NN), and a collection. Collection means that the most accurate base classifier is selected from a pool of all but neural network (as it performs well on its own). We run 10-fold cross validation.
We run four context identification techniques (CLU, OVE, PRO, FUS) using different number of contexts k =2 ... 8. The resulting stabilities are presented in Figure 6. Several strategies show the best stability at two contexts, then at four and seven. This tendency is also visible from the two principal components inthesamefigure.Wechoosetoanalyze k = 4 for this case study, since it is more interesting because of a larger distinction from single context. For a full picture, we also report the ranking of the techniques at k =2and k =7.
How do we know that there are variable contexts at all? It can be concluded from the stability test and the plot of principal components. If there were no distinct contexts, the stability would be bad and the data in the principal com-ponent plot would be mixed.
 Set up. The simplest context handling strategy is to build one local classifier for each context. We test how it works using the context labels identified by our techniques. For comparison we incl ude a random split into contexts (RAN) and no split into contexts (ALL), which we use as baselines. We also add to the tests an ensemble (ENS) of CLU, OVE, PRO, FUS and RAN, which makes classification decision using simple majority voting.

The testing errors are provided in Table 3. For the final evaluation, we average over the errors of different classifiers. Statistical significance is tested using a paired t-test. Symbol  X   X   X  means the technique is significantly better than the baseline (ALL). symbol  X   X   X  means the technique is significantly worse than the baseline. Symbol  X   X   X  means no statistical difference.

In terms of accuracy CLU performs not bad, NMI score shows that it captures not so much class label information on this data. Interestingly, RAN sometimes outperforms ALL. It can be seen as a variant of boosting, though suffering from small training sample. OVE and FUS performs on average better than CLU, it is mainly due to bad performance of CLU on the last test (collection).
We find that an ensemble (ENS) is the best in terms of accuracy. It is sup-ported by experiments with different n umber of contexts. The rankings are: k =2ENS  X  PRO  X  FUS  X  OVE  X  ALL  X  CLU  X  RAN; k =4ENS  X  OVE  X  FUS  X  CLU  X  PRO  X  ALL  X  RAN; k =7ENS  X  PRO  X  ALL  X  OVE  X  FUS  X  RAN  X  CLU.

The scope of the study is to analyze context identification rather than explore context handling strategie s. Thus, we explore in depth only selection strategy and do not claim that it is the best. We report it as an illustration, complementary to the proposed identification techniques . It demonstrates, how the accuracy can be improved having no domain knowledge about underlying contexts, starting from identification of the number of cont exts to training the actual classifiers. Context identification techniques can b e considered as a preprocessing step in classification, aimed to improve the accuracy, as well as contribute to under-standing of the data. We require the contexts to be independent from the class labels, valid (non random) and stable.

We proposed three techniques for identifying hidden contexts from the data, directed not to capture class discriminatory information. The experiments on thirty datasets indicate that all the three techniques avoid capturing class label information pretty well and show similar validity; in terms of stability Over-lay technique is preferable. The case study illustrates the benefits of context identification when used with classifier selection strategy.

Our study opens a range of follow up research opportunities for context han-dling strategies in static and dynamic (concept drift, discrimination aware learn-ing) settings.

