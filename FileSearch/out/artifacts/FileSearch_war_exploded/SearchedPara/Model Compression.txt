 Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many clas-sifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for  X  X ompressing X  large, complex ensembles into smaller, faster models, usually with-out significant loss in performance.
 Categories and Subject Descriptors: I.5.1 [Pattern Re-cognition]: Models  X  Neural nets.
 General Terms: Algorithms, Experimentation, Measure-ment, Performance, Reliability.
 Keywords: Supervised Learning, Model Compression
An ensemble is a collection of models whose predictions are combined by weighted averaging or voting. Ensemble methods have been the focus of significant research in the past decade, and a variety of ensemble methods have been introduced. Well known ensemble methods include bagging [2], boosting [14], random forests[3], Bayesian averaging [9] and stacking [17]. Much of the interest in ensemble methods has been fueled by their excellent empirical performance.
Ensembles, however, have one disadvantage that often is overlooked: many ensembles are large and slow. This makes ensemble methods unusable for applications with lim-ited memory, storage space, or computational power such as portable devices or sensor networks, and for applications in which real-time predictions are needed. Consider, for exam-ple, boosted decision trees, bagged decision trees or random forests. These models often contain hundreds or thousands of decision trees, each of which must be stored, and executed at run-time to make predictions. Executing a single tree is fast, but executing a thousand trees is not.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
In this paper we show how to compress the function that is learned by a complex model into a much smaller, faster model that has comparable performance. Specifically, we show how to train compact artificial neural nets to mimic the function learned by ensemble selection, an ensemble learning method introduced by Caruana et al. [5]. To achieve this, we take advantage of the well known property of artificial neural nets, namely that they are universal approximators: given enough training data, and a large enough hidden layer, a neural net can approximate any function to arbitrary pre-cision. Instead of training the neural net on the original (often small) training set used to train the ensemble, we use the ensemble to label a large unlabeled data set and then train the neural net on this much larger, ensemble labeled, data set. This yields a neural net that makes predictions similar to the ensemble, and which performs much better than a neural net trained on the original training set.
The key difficulty when compressing complex ensembles into simpler models this way is the need for a large unla-beled data set. In some domains, unlabeled data is easy to obtain. In other domains, however, large data sets (labeled or unlabeled) are not available. In these domains, we gen-erate synthetic cases that as closely as possible match the distribution of the original training set. We introduce a new method for generating synthetic cases called MUNGE that outperforms other methods to which we have compared it. Using MUNGE, we are able to train neural nets that are a thousand times smaller and faster than ensemble selection ensembles, but which have nearly the same performance as the far more complex ensembles.
In some situations, it is not enough for a classifier or re-gressor to be highly accurate, it also has to meet stringent time and space requirements. In many cases, however, the best performing model is too slow and too large to meet these requirements, while fast and compact models are less accurate, because either they are not expressive enough, or they overfit to the limited training data. For such situations, we propose using model compression to obtain fast, compact yet highly accurate models.

The main idea behind model compression is to use a fast and compact model to approximate the function learned by a slower, larger, but better performing model. Unlike the true function that is unknown, the function learned by a high performing model is available and can be used to label large amounts of pseudo data. A fast, compact and expres-sive model trained on enough pseudo data will not overfit and will approximate well the function learned by the high performing model. This allows a slow, complex model such as a massive ensemble to be compressed into a fast, compact model such as a neural net with little loss in performance. An important question is how do we get the pseudo data. In some domains large amounts of unlabeled data is easy to collect (e.g. in text, web and image domains) and can be used as pseudo data. In other domains, however, unla-beled data is not readily available and synthetic cases need to be generated. This is more difficult than it might seem at first. It is important that the synthetic data match well the distribution of the real train and future test cases. Usu-ally real data lay in a small submanifold of the complete attribute space. If the synthetic data is drawn from a distri-bution that has little overlap to this manifold, the labeled synthetic points will fail to capture the target function in the region of interest. On the other hand, if the distribu-tion from which the synthetic data is sampled is too broad, only a fraction of the points will be drawn from the true manifold and many more samples will be necessary to ade-quately sample the region of interest. The best case is when the synthetic distribution is very similar to the true distribu-tion. Then a minimum number of samples will be necessary to adequately sample the target function.

We experiment with three methods of generating pseudo data: RANDOM, generate data for each attribute indepen-dently from its marginal distribution; NBE, estimate the joint density of attributes using the Naive Bayes Estimation algorithm [12] and then generate samples from this joint dis-tribution; and MUNGE, a new procedure we propose that samples from a non-parametric estimate of the joint density.
The simplest way to generate pseudo data is to indepen-dently sample the value of each attribute from the marginal distribution of that attribute. This is the procedure predom-inantly used in the literature whenever there is a need for artificial data (e.g. [13, 6]). Usually the nominal attributes are generated from a multinomial distribution whose param-eters are estimated from the training data. The continuous attributes are usually modeled using a uniform distribution, a Gaussian distribution with mean and variance estimated from the training set, or via kernel density estimation [15].
The RANDOM method for generating pseudo data uses a nonparametric bootstrap approach. For each attribute, a value is selected uniformly at random from the multiset (bag) of all values for that attribute present in the train set.
When the attribute values are generated independently, all conditional structure is lost and the pseudo examples are generated from a distribution that is usually much broader than the true distribution of the data. As a consequence many of the generated pseudo examples will cover uninter-esting parts of the space, and this may prevent the mimic model from focusing on the important regions.
Another approach to generating pseudo data is to esti-mate the joint distribution of attributes using the training set, then sample pseudo examples from this joint distribu-
For nominal attributes this is equivalent to generating the values from the multinomial distribution. For continuous attributes this procedure is slightly different than previously proposed ones, but generates similar values in practice. tion. Assuming that the true joint distribution can be esti-mated well, the conditional structure of the domain would be preserved and the new artificial examples would cover well the interesting regions of the space.

One way to estimate the joint distribution of a set of vari-ables is to use mixture model algorithms. These algorithms model the data as coming from a mixture of components, each component with a different distribution. The most well known algorithm in this category, used in domains with only continuous attributes is the mixture of Gaussians [7], where each component consists of a Gaussian distribution with a different mean and covariance matrix.

A mixture model algorithm that handles both discrete and continuous attributes, NBE (Naive Bayes Estimation), was recently introduced by Lowd and Domingos [12]. We used NBE to estimate the joint distribution of the attributes because it handles mixed attributes, it is simple to use, it performs as well as learning a Bayesian Network from the same data [12], and it is readily available.
Estimating a full joint distribution is difficult when there are many attributes and few training cases. Instead of trying to reliably estimate a joint distribution, we have developed a new algorithm that samples directly from a non-parametric estimate of the joint distribution.
 Algorithm 1 MUNGE Require: set of training examples T , size multiplier k , Returns: unlabeled training set D of size k  X  size ( T ) 1: D  X  X  X  2: loop k times 3: T  X  T 4: for all examples e in T do 5: e  X  the closest example of e from T 6: for all attributes a of example e (excluding the 7: if a is continuous then 8: with probability p : e a  X  norm ( e a ,sd ), and 9: else 10: with probability p : swap the values of attribute 11: end if 12: end for 13: end for 14: D  X  D T 15: end loop 16: Return D
Starting from the original training set, we visit each ex-ample once and determine its closest neighbor. To measure distance between cases, we use euclidean distance for con-tinuous attributes and hamming distance for nominal at-tributes. Continuous attributes are linearly scaled to [0,1].
Given example e and its closest other example e ,theval-ues for each noncontinuous attribute are swapped between e and e with probability p and are left unchanged with probability 1  X  p . For each continuous attribute a ,with Figure 1: Synthetic data generated for a simple 2D problem. probability p , e a is assigned a random value drawn from a normal distribution with mean e a and standard deviation sd = | e a  X  e a | /s ,and e a is assigned a random value drawn from the normal distribution with mean e a and the same standard deviation sd . We call this approach to generating artificial data by swapping values between neighboring cases MUNGE . 2 The method is presented in Algorithm 1.
Figure 1 shows samples generated from a simple 2D dis-tribution (TRUE DIST), and the distributions learned by RANDOM, NBE and MUNGE from a train set of 4000 points drawn from TRUE DIST. As expected, the samples generated by RANDOM cover an area much larger than the true distribution, so only relatively few of the samples over-lap with the region of interest. NBE does a better job at approximating the true distribution, but still has problems, especially in the  X  X orners X . Of the three methods, MUNGE clearly approximates the true distribution the best.
We evaluate the effectiveness of model compression on eight binary classification problems. ADULT, COVTYPE and LETTER are from the UCI Repository [1]. COVTYPE has been converted to a binary problem by treating the largest class as positive and the rest as negative. We con-verted LETTER to a binary problem in two ways. LET-TER.p1 treats the letter  X  X  X  as positive and the remaining 25 letters as negative, yielding a very unbalanced binary problem. LETTER.p2 uses letters A-M as positives and the rest as negatives, yielding a difficult, but well balanced, problem. HS is the IndianPine92 data set [10] where the dif-ficult class Soybean-mintill is the positive class. SLAC is a problem from the Stanford Linear Accelerator. MEDIS and MG are medical data sets. See Table 1 for characteristics of these problems.
We experiment with using neural networks to compress the models built using the ensemble selection algorithm pro-posed by Caruana et al. in [5]. The ensemble models gener-ated by ensemble selection are very large, complex models that have very good generalization performance, thus they are a perfect candidate for model compression.
The dictionary defines munge as  X  X o imperfectly transform information X  or  X  X o modify data in a way that cannot be described succinctly X  .
 Figure 2: Average perf. over the eight problems.

Ensemble selection first builds a library of diverse base-level modes using many different learning algorithms and parameter settings. After the library is built, the basic en-semble selection procedure builds the ensemble model by greedily selecting at each iteration the model from the li-brary that when added to the ensemble improves the per-formance of the ensemble the most. Caruana et al. also propose a number of enhancements to the basic ensemble selection algorithm that improve its performance, but as a side effect increase the size of the ensemble by increasing the number of base-level models it contains.

For all problems, a training set of 4000 points is used to train the base-level models, and a validation set of 1000 points is used as hill climb set for ensemble selection. For compression, the 4000 training points are used as a training set for the three algorithms for producing artificial data: RANDOM, NBE and MUNGE. The artificial data generated with each algorithm is then labeled by the ensemble model and used to train a neural net model that will mimic the ensemble. When necessary, the 1000 points validation set is used for early stopping.

We compare the performance of the compressed models with the performance of the target ensemble selection mod-els on the eight test problems. We also show the perfor-mance of the best single base-level model from the ensemble selection library, selected using the same 1000 points valida-tion sets, and the best neural network that could be trained on the original 4000 points training sets, using the 1000 points validation sets for early stopping and for selecting the number of hidden units. All the reported results reflect the root-mean-squared-error (RMSE) of models predictions to the binary 0/1 targets on large independent final test sets.
Figure 2 shows the average RMSE performance on the eight test problems. Lower RMSE represents better per-formance. The top horizontal line in the figure shows the performance of the best neural nets we could train on the original 4k train set. The bottom horizontal line shows the performance of ensemble selection trained on the same data. Note that the models trained with ensemble selection per-form considerably better than the neural net models. The horizontal line in the middle is the average performance of the best single base-level models from the ensemble selec-tion libraries, before an ensemble has been selected. This linerepresentsthebestperformancewecouldachievewith any of the following learning methods: SVMs, bagged trees, boosted trees, boosted stumps, simple decision trees, ran-dom forests, neural nets, logistic regression, k-nearest neigh-bor, and naive Bayes.

The other lines in Figure 2 show the performance of mimic neural nets trained on different amounts of pseudo data la-beled by the ensemble models. The lines for RANDOM, NBE, and MUNGE correspond to neural nets with 128 hid-den units trained on pseudo data generated using the three methods described in Section 2. The graph starts at 4k where there is no pseudo data added to the train sets. Be-cause of this, the performance of all three methods is similar to the performance of the best neural nets we could train on the original train set. Perf ormance of the mimic nets is slightly worse because they are restricted to using only 128 hidden units, which is not always optimal.

As the size of the train data increases beyond 4k, more pseudo data is being added to the train sets. At 400k, the train set contains 396k of pseudo data and the original 4k train data. For mimic nets trained with pseudo data gen-erated by RANDOM, performance improves slightly at 10k, and at 100k and beyond performs worse than a neural net trained on just the original 4k train set. The mimic neu-ral nets trained on pseudo data generated by NBE perform better, though the overall pattern is similar to the graph for RANDOM. The peak performance of the NBE trained nets occurs when the train set contains about 20k of pseudo data and 4k of the original data, then degrades as more artificial data is added to the train set.

The mimic neural nets trained on pseudo data generated with MUNGE dominate the nets trained with RANDOM and NBE data. Moreover, the performance with MUNGE does not degrade as more data is added to the pseudo train set. On average, once the pseudo training set contains 100k or more data, the mimic neural nets perform considerably better than the best individual models in the ensemble li-braries, and nearly as well as the target ensemble itself. This is remarkable given that the mimic neural nets are 100-100,000 times smaller than the ensembles, and 100 to 10,000 times faster to execute. It suggests that much smaller high performing models are possible if we only knew how to train them on the original training data. In summary, it appears that MUNGE is the preferred method for generating pseudo data. In the experiments in the remainder of this section we will examine results for MUNGE only.

Figure 3 presents the perfor mance of the mimic neural nets trained with MUNGE data as a function of the num-ber of hidden units in the trained network. Performance is shown for all eight problems. The graph in the bottom right shows the average performance across all eight prob-lems. All MUNGE neural nets are trained on pseudo data containing 100k samples (4k original data + 96k MUNGE data). The graphs also show the performance of the best neural nets trained on the original data, the performance of the best single models from the ensemble libraries and the performance of the target ensemble selection models. 3
Looking at the graph that averages the eight problems, the overall trend is that performance improves until around 128 hidden units when it levels off. Looking at graphs for each individual problem, we see that for some of the problems a small number of hidden units is enough for obtaining good performance. For MEDIS and MG, performance does not improve if we use networks with more than 2 hidden units, and for SLAC 16 hidden units are enough. Since the training set is large enough to prevent overfitting, performance does not degrade as more hidden units are added.

The ADULT and COVTYPE problems are the only ones where there is a significant difference between the perfor-mance of the MUNGE neural nets and the performance of the ensemble selection models. A more detailed discussion of the results on these datasets will follow later in this section, and in Section 4.

Figures 2 and 3 show that, on average, performance of the mimic networks trained on the MUNGE data improves with more hidden units and more pseudo data. As a final experiment, we compress the ensemble selection models us-ing 256 hidden unit nets and 400k MUNGE data for every problem. Table 2 shows, for each of the eight problems, the performance of mimic neural nets trained on the MUNGE data, the target ensemble selection model, the best neural net trained on the original data, and the best single model from the ensemble library. The performance of the mimic neural nets is as good as or better than the performance of the ensemble models they are trained to mimic on six of the
On HS, LETTER.p1 and MEDIS problems the best single model from the ensemble library is actually a neural net so the lines for best neural net and best single model overlap.
Table 3: Time in seconds to classify 10k cases. eight problems, and always better than the performance of neural nets trained on the original 4k data.

The values in the last column of the table indicate how effective compression is at retaining the performance of the target ensemble selection models. These values are the ra-tio between the improvement in performance the mimic nets provide over the best neural nets and the improvement in performance the target ensemble selection models provide over the best neural nets. For example, if the mimic neural net has performance half way between the original neural net and the ensemble, the ratio is 0.5. If the mimic neural net has performance equal to the target ensemble, the ratio is 1.0. The only problem on which the ratio is less than 0.8 is ADULT. (The results on this problem are discussed in the next paragraph.) For a few problems the ratio is better than 1.0, indicating that the mimic neural net out-performs the ensemble. Note, however, that in two of the cases where the ratio is much larger than 1 (SLAC at 1.69 and MEDIS at 2.29), the range in performance is very small so this large ratio does not actually indicate a very large increase in performance. The ratio in the bottom row is the ratio calculated for the average RMSE performances in the table (not the average of the ratios, which would be inflated by the two problems with artificially high ratios). On aver-age, model compression with MUNGE is able to achieve 97% of the performance increase that could at best be expected. The only problem for which compression is ineffective is ADULT. On this problem the mimic net performs only a little better than a neural net trained on the original 4k data, and the mimic net does not perform as well as the best single model in the ensemble selection library. Interestingly, ADULT is the only data set that has high-arity nominal attributes. The three attributes with the highest arity have 14, 16, 41 unique values. To train a neural net on ADULT, these attributes must first be converted to 14, 16, and 41 distinct binary attributes. The ADULT problem has only 14 attributes to begin with, yet these three attributes alone expand to 71 sparsely coded binary inputs. It is possible that neural nets are not well suited to this kind of problem, and this may prevent the mimic neural net from learning the ensemble target function. An alternate possibility is that the MUNGE procedure is not effective at generating pseudo data for this kind of problem.

Table 3 shows the time in seconds required to classify 10,000 test cases for the mimic neural nets with 256 hidden units, the target ensemble models trained by ensemble se-lection, the best neural nets trained on the original 4k train set and the single best model in the ensemble library. There is significant variability in the speed of the best single model because different kinds of models are best for different prob-lems and some of the models (e.g. boosted trees) are much more expensive than others (e.g. logistic regression). As expected, the ensemble is extremely expensive. On average, the ensemble takes about 0.5 seconds to classify a single training case (on a single workstation) and, on the SLAC problem, it takes 2.4 seconds per test case! The mimic neu-ral nets, however, are very fast and take on average only about 0.5 milliseconds per test case. 4
Table 4 shows the size in megabytes for the different mod-els in Table 2. A similar picture emerges as with execution times: on average ensembles are about 500 megabytes, about 500 times larger than the best single models, and the largest ensembles are more than a gigabyte. The mimic neural nets, however, are four times smaller than the best single models, and more than 1000 times smaller than the ensembles.
The number of hidden units needed in the compression neural net to mimic the ensemble function provides an indi-cation of how complex the function learned by the ensemble actually is. For some problems, only a few hidden units were required to learn the ensemble function with high fi-delity. For other problems (e.g. COVTYPE) the target ensemble function is more complex, and requires using 128 hidden units or more. Note that one cannot always use the size of a neural net trained on the original training data as a measure of function complexity because of the interac-tion between overfitting and network size and train set size. With compression, however, we can make the train set size arbitrarily large, so that overfitting is not an issue, and thus reliably measure the effect of ne twork size on performance.
Generating unlabeled pseudo data is not computationally expensive. Labeling large amounts of pseudo data, however, can be expensive if the target model is a large, complex ensemble. Training the neural net can also be expensive, particularly if the problem has many input features, requires a net with many hidden units, and/or requires a large pseudo training set. In the worst case, it can be more expensive
The speed of different models depends significantly on how they are implemented. The times reported here are for typ-ical implementations. For example, we use the SNNS neu-ral net package [18], the IND decision tree package [4], the SVMlight SVM package [11], WEKA [16] random forests, etc. With care, some of these numbers probably could be improved by a factor of 10 or more, though we suspect the overall picture would not change substantially.
Figure 4: Perf. vs train set size for COVTYPE. to label the pseudo data and train the mimic neural net than it was to train the original ensemble library and build the ensemble model. The expense of model compression is justified only when high performing models must be used in applications with limited storage or computational power, in applications where predictions are needed in real time, or where there will be very many test cases.

The amount of pseudo data needed to train a high-fidelity mimic model depends on the effectiveness of the method for generating artificial cases. It is clear from Figure 2 that MUNGE is more effective than RANDOM and NBE, but it would be interesting to assess the sample efficiency of MUNGE compared to unlabeled data drawn from the true distribution. Fortunately, more than 500,000 cases are avail-able for the COVTYPE problem. Figure 4 shows the per-formance of the mimic neural nets when pseudo data is gen-erated using RANDOM, NBE and MUNGE, as well as the performance of the mimic nets when real unlabeled test cases are used instead of pseudo data (TRUE DIST). Data drawn from the true distribution appears to be about 2-8 times as efficient as MUNGE data: MUNGE needs about 25k to match TRUE DIST at 10k, about 50k to match TRUE DIST at 15k, and about 200k to match TRUE DIST at 25k. This suggests that it may be possible to make further improve-ments to MUNGE, and that in domains where unlabeled data is available model compression will work even better.
Interestingly, on the HS problem, the performance of the mimic neural net trained on MUNGE pseudo data is better than the performance of the ensemble model it is trying to mimic. See HS in Figure 3. We suspect that the ensemble has overfit the data, and that the neural net may provide a beneficial form of smoothing/regularization.
Zeng and Martinez also used neural nets to approximate ensembles of classifiers [19]. In the experiments they pre-sented, Zeng and Martinez tryed to approximate only en-sembles of ten neural nets. The mimic nets were trained on synthetic examples generated using an algorithm simi-lar to RANDOM. We found that, although it yields an im-provement on a couple of problems, on average, generating synthetic data using RANDOM does not provide a signif-icant improvement over simply training the neural nets on the original data. Generating synthetic data using NBE or MUNGE works much better. Also, Zeng and Martinez showed that only a small number of synthetic examples, comparable to the size of the initial train set, is sufficient to obtain good results. This might be an indication that the functions that were approximated, ensembles of ten neural nets, were not hard enough to learn by a single neural net. In contrast, we tried to approximate much harder functions generated by ensemble selection, and as a consequence we needed a lot more synthetic data to obtain the best results.
TREPAN was used to extract tree-structured represen-tations of trained neural nets [6]. The TREPAN approach also made use of additional artificial data generated from the same train set as the one used for training the nets. New examples were generated as needed by randomly selecting new values for each attribute, while ensuring that some con-straints are satisfied. For discrete attributes the values were generated the same way as RANDOM. For continuous at-tributes, a kernel density estimator [15] was used. Although TREPAN models sometimes were more compact than the neural nets they explained, and occasionally outperformed decision trees trained directly on the original training data, the goal in TREPAN was not to train compact models or high performing models.

CMM is a meta-learner that learns one single model from a bagged ensemble of the same type of models [8]. Again, the approach makes use of additional, artificially created data and labeled according to the bagged model. The method used to generate synthetic data was specific to the base mod-els used in the ensemble. The base models were decision rule sets. For each decision rule in the rule set, some examples were generated randomly from the hyperspace classified by the decision rule. Although CMM generates smaller mod-els, they are not optimized for size or performance, but for comprehensibility instead.

DECORATE [13] uses artificial data to increase diversity so that better ensembles can be trained. Before DECO-RATE trains a new base learner, it generates and labels new data the opposite way to the predictions of the current ensemble. The new trained model is added to the current ensemble only if the addition is beneficial to the ensemble. This approach generates the synthetic data randomly from the distribution of the train set. For continuous attributes, the new values are generated from a Gaussian distribution that has the same mean and standard deviation as the set of values for that attribute. For discrete attributes, frequency counts are used, which is similar to RANDOM.
Some of the highest performing models currently available are complex ensembles containing hundreds or thousand of base-level classifiers. These models have excellent perfor-mance, but can be so large and slow that it may be infea-sible to use them when memory or computational power is limited, when predictions are needed in real time, or when test sets are extremely large.

We present a method for model compression that is able to train fast, compact models to mimic better performing, but slow and complex models with little loss in performance. Compression works by labeling a large unlabeled data set with the target model, and then training a neural net us-ing the newly labeled data. Where unlabeled data is not available, we present a new method called MUNGE for gen-erating pseudo data from a distribution similar to that of the true data. We show that MUNGE compares favorably with other methods for generating artificial cases. MUNGE, however, is not as effective as drawing unlabeled data from the true distribution, so using true unlabeled data is pre-ferredwhenitisavailable.

We present experiments where complex ensemble models are compressed using neural networks. Results on eight test problems show that, on average, the loss in performance due to compression is usually negligible, yet the mimic neural nets are 1000 times smaller and 1000 times faster. This work was supported by NSF Award 0412930.
