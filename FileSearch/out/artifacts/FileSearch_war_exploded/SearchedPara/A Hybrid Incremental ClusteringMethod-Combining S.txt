 Most of the early clustering algorithms use single clustering technique. There are many restrictions on those algorithms. For example, K-mean algorithm [1] is easily influenced by noise and outlier. DBSCAN algorithm [2] requires the user to enter the parameters. Therefore, recent researches combine different cluster-ing algorithms to improve the quality of clustering. For example, the BRIDGE algorithm [3] combines K-means algorithm and DBSCAN algorithm. K-mean algorithm is simple and fast. DBSCAN algorithm is not easily influenced by outliers. However, the clustering quality of BRIDGE algorithm is still influenced by the different input parameters.

To solve problems above, some researches combine estimation formula of simi-lar clusters into clustering algorithms so that similar clusters can be merged. For example, the Relative Interconnectivit y (RI) method and the Relative Closeness (RC) method proposed by Karypis [4], have considered the distance of the two clusters (RI) and closeness (RC) into the calculation and led out a composite index. According to the experimental re sults, this technique reaches a better result of merging. However, the time consumption is still high.

A good clustering algorithm must be able to deal with the noise, find any form of clustering, get high quality of clusters and low time complexity. For this reason, Patrick Pantel [5] has proposed clustering by committee algorithm (CBC). This algorithm can automatically find out the proper number of clusters, increase performance of clu stering and classify documents to multi clusters.
Though the performance of CBC algorit hm is excellent, however, it must re-cluster all of documents again when new documents come in. In view of this, this study tries to combine the classification and clustering technologies, and proposes a new clustering algorithm: hybrid incremental clustering method. First, the new document is classified by Support Vector Machine (SVM) according to the current classes. The reason of choosing SVM is that it has the characteristic of fast, stable and it does not require much training to get good classification result. Besides, in many researches, its performance is better than other methods [6]. Then the enhanced CBC algorithm proposed by this study is used to cluster the unclassified documents to form new classes, and the new classes will be added to the existing classes so that the number of classes will increase gradually.
In the algorithm, SVM can significantly reduce the amount of calculations and the noise of clustering. Enhanced CB C algorithm can effectively control the number of clusters, improve performance and allow the number of classes to grow gradually based on the structure current classes without clustering all of documents again. According to the experimental results, the hybrid incremental clustering performance is not just better than the enhanced CBC algorithm, it is obviously better than other algorithms. Besides, the enhance CBC algorithm outperforms the original CBC algorithm. The structure of hybrid incremental clustering is shown in Figure 1. First, the experimental data and structure of cla ssification are collected from the inter-net. The collected documents are segm ented by the Chinese Words Database and CKIP Chinese Word Segmentation System. According to the morphological features, lexicons are divided into sev eral word classes and the necessary word classes are captured. Next, Chi-square is used to find out the features and to establish the vector space. The vector sp ace is sent to SVM to try to classify the incoming documents into any existing class, if possible. After processing all of in-coming documents, we check if there is any unclassified document. If so, TFIDF is used to find out the features of the unclassified documents and establish the vector space. Enhanced CBC algorithm is then used to cluster the unclassified documents. If there are still any unclust ered documents, they will be used as a part of input for next cycle. Finally, the evaluation is performed. 2.1 First Phase: Classification with SVM Support Vector Machine (SVM) is developed from Statistical Learning Theory (SLT) [7]. The goal is to find linear functions in high dimensional space,which can discriminate information. Those functions can be used to represent the support vector of the information clusters and some extreme values will be rejected in advance.The basic definition of SVM is as follows: { 1 ,  X  1 } , p is the number of data and n is the number of vector spaces. When y equals to 1, the document belongs to the class; when y equals to -1, the document does not belong to the class. In the linea r analysis, in an optimal hyperplane, ( w  X  x )+ b = 0 can completely separate the sample into two conditions shown as below, where w is the weight vector and b is a bias.
In the linear separation, it is a typical quadratic programming problem. La-grange formula can be used to find the solution, where  X  is a Lagrange multiplier.
In the linear analysis, the original problem can be considered as a dual prob-lem. To find the optimal solution, the approach is: Constraint:
By solving the quadratic programming, the classification formula applied to classification can be obtained as shown below. Any functions that meet Mercer X  X  condition can be kernel functions. We adopt Radial kernel function below as kernel function of SVM.
 2.2 Second Phase: Clustering with Enhanced CBC Algorithm Then enhanced CBC algorithm is used to cluster the unclassified documents. Since CBC algorithm has excellent performance on clustering, therefore, this study modifies original CBC algorithm to determine beginning number of the clusters. Then the merging process of similar clusters is applied to merge clusters and determine the final number of clust ers. The similarity of each document is first calculated. The value of TFIDF of each word is calculated and sorted in descending order. Taking discrimination and time consumption into considera-tion, threshold of TFIDF value is set to two. Then the similarity matrix of each document is calculated by VSM. Finally, in order to avoid missing the docu-ments with high degree of similarity, the number of desired clusters is replaced by similarity threshold in this study.

First, the clusters of documents with high degree of similarity are temporary stored in committee candidates (L) (Step 1), and are sorted in descending order according to the similarity of average-link. It is the basis of selecting committees. The clusters in the L are examined and the similarity formula is used to calculate the density of the clusters. If similari ty between each document and the centroid is greater than threshold  X  1 , then those documents form a committee (Step 3). And make them form a committee. It becomes the new basis of classification for other documents. In this approach, the numbers of committee are fixed so that the amount of calculation can be reduced.

Then, all of documents that are not classified into any committee are exam-ined. If the similarity between a document and the central point of a committee is greater than the threshold  X  2 , the document will be put to the corresponding committee. Otherwise, the document will be classified to set R (Step 4). Next, it will check whether set C or set R is empty, or whether limited CBC algorithm has been executed twice. If it is true, the n clustering is ended and enter next step. Otherwise, it will go back to the first step and the documents in set R are regarded as a part of new input for next cycle. According to the experimental results, most of the documents in set R are outliers at this stage. If it takes too much time to force clustering of these documents in set R, the performance of clustering algorithm will be affected and time consumption will increased. Therefore, recursive timesissettonotmorethantwo.

Finally, it will check whether set C is empty. If not, the merging mechanism will be activated (Step 5). Some studies have further discussed in merging mech-anism [8]. Through the study and experiment, we choose V as the central vector must be high so that they can be merged (When  X  3 equals to 2, it means that V 1 and V 2 must be the same in order to be merged).

In the process of testing, we find that significant performance can be reached when  X  1 is set to 0.4,  X  2 is set to 0.35 and  X  3 is set to 1.92 with trial and error method. Therefore, those coefficients are adopted.
 Enhanced CBC algorithm Input: Output: First phase: Limited CBC algorithm
Repeat Until (C =  X  )or(R=  X  ) or (Recursive times &gt; 2)
Second phase: Expanded clustering merging
Return C The news webpage of YAHOO are used as the targets for verification. The col-lection duration is from 2006/1/1 to 2006/1/13. There are totally 12 classes and 4187 documents. Besides, the F-measure formula in experiment is set to 2PR/P+R, where P is pr ecision and R is recall. 3.1 Evaluation of Enhanced CBC Algorithm We compare the enhanced CBC algorithm with the original CBC algorithm in terms of precision, recall and F-measure. The experimental results are shown in Table 1. In Table 1, the average performances of precision, recall and F-measure have increased 13%, 5% and 14% respectively. The number of clusters is also under control, and the average number of clusters has dropped from 60.7 to 4.7. When the degree of similarity is 0.3, the performance is excellent. This experiment shows that mergence of t he similar clusters can increase the performance and reduce the number of clusters. Besides, we find when the threshold value of similarity degree is set to 0.7 in merging mechanism of similar clusters, most of the documents can be classified to proper classes fast. 3.2 Evaluation of Proposed Hybrid Incremental Clustering Method The performances of the proposed met hod and enhanced CBC algorithm are shown in Table 2. Table 3 shows the average performance of the proposed method and others.

In Table 2, performance of the proposed method raise obviously. The average performances of precision, recall and F -measure have increased 27%, 3% and 18% respectively. When the degree of similari ty is 0.7, the performance is excellent. The reason for this result is that all documents have been filtered in the first phase of classification. Therefore, the noise of clustering in second phase has been reduced a lot.

In Table 3, the values of precision, r ecall and F-measure in the proposed method rise 13.4%, 3.8% and 8.4% compared with the average value (36.6%, 42.2% and 38.6%) of Chameleon, Average-link, Buckshot, Bisecting K-means and Complete-link.
 This study proposes a new hybrid increm ental clustering method. The method applies SVM, enhanced CBC algorithm and merging mechanism of similar clusters. The noise of clustering is reduced. The number of clusters is under control. It simplifies the steps so that th e speed of clustering is increased. The number of clustering classes grows gradually without re-clustering existing docu-ments again when new documents come in. Those improvement can be referenced for future studies.
 This research was supported by the Chung Hua University under the grant no. CHU-95-M-20.

