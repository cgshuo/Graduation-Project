 In this paper, we present Stepping Stones and Pathways (SSP), an alternative model of building and presenting answers for the cases when queries on document collections cannot be answered just by a ranked list. Stepping Stones can handle questions like:  X  X hat is the relation of topics X and Y? X  SSP addresses when the contents of a small set of related documents is needed as an answer rather than a single document, or when  X  X uery splitting X  is required to satisfactorily explore a document space. Query results are networks of document groups representing topics, each gr oup relating to and connecting (by documents) to other groups in the network. Thus, a network answers the user X  X  information need. We devise new and more effective representations and techniques to visualize such answers, and to involve users as part of the answer-finding process. In order to verify the validity of our approach, and since the questions we aim to answer involve multiple topics, we performed a study involving a custom built broad collection of operating systems research papers, and evaluated the results with interested computer science students, using multiple measures. H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Algorithms, Design, Experimentation Query splitting, stepping stones, pathways, cluster labeling, suffix-tree clustering, collection building from syllabi The information retrieval (IR) field has been successful in developing techniques to address many types of information needs. However, there are cases in which these techniques are not able to produce adequate results. Examples include when:  X  the answer is not contained in any particular individual  X   X  X uery splitting X  [1] is required to satisfactorily explore a  X  a query represents a single concept in the user X  X  mind, but Hence, we propose a different way to interpret the information request in a user query, and proceeding from that, a different way to present query results and allow user feedback. We argue for the study of a more general type of query, able to:  X  Include multiple subjects. The typical query as interpreted  X  Be interpreted as parts in a chain of relationships. Consequently, we propose a new type of user query, interpreted as two related but separable topics. By separable we mean that the two topics are each identifiable from the query formulation. The objective of the query then is to retrieve one or more sequences of documents that support a valid set of relationships between the two topics. The input to the system is still a set of words, but now it is representing two topics. The output of the system is a connected network of chains of evidence. Each chain is made of a sequence of additional topics ( stepping stones ). Each topic in the sequence is logically connected to the next and previous one, and the chains provide a rationale (a pathway ) for the connection between the two original topics. To increase the user X  X  understanding of the chain, it is desirable that the stepping stones be justified by concrete documents, along with the connections (relationships) among those documents. To illustrate how Stepping Stones and Pathways works, a typical screenshot of a user session is depicted in Figure 1: This session was carried out over a collection of computer science operating system research papers. A session starts with the user typing two topic descriptions (subqueries) in the text fields in the top area n . In this case, the topics were  X  distributed system  X  and  X  file system  X . After the subqueries are submitted, a graph is shown in the middle part of the screen o . Nodes in this graph represent topics and are shown as labels enclosed by rectangles. In Figure 1, the leftmost node is  X  distributed systems  X  and the rightmost is  X  file system  X ; these are called endpoints. Examples of middle nodes connecting these endpoints are  X  X istributed applications X  and  X  X igh performance X . Edges in the graph represent connections between pairs of topics. Each edge has both a handle (a circle roughly halfway between rectangles) that facilitates interaction through clicking on the connection, and a nearby label (e.g.,  X  X perating, memory, shared X ) summarizing a topic-topic (e.g.,  X  X istributed system X   X   X  X igh performance X ) c onnection. The two subqueries entered by the user thus become the endpoints. Nodes near the middle between the endpoints are other topics that the system guesses may help to answer the query; each gives a partial answer and connects to each of the two s ubqueries. Each node that is not an endpoint is justified by one or more documents. A sequence of such topics/documents (a full chain of relationships) should answer the need expressed in the subqueries. The bottom area of the screen p lists all chains of relationships, and so provides an alternate visualization of the middle part of the screen o . Each chain is a sequence of documents and topics, each document connecting two or more topics. Each of these chains belongs to one path connecting the endpoints in the graph shown in the middle section, but since there are multiple documents that connect the endpoints through the same topics, there are more chains than paths in the graph. Some documents alone cover an entire path. An example is the paper entitled "Availability in the Sprite Distributed System", which would be retrieved for each of the topics "distributed system", "high performance", and "file system". Other papers only cover two topics, and so constitute a stepping stone along a path between those, such as the paper "MojaveFS: A Transactional Distributed File System", which connects the topics "high performance" and "file system", and the paper "Availability in the Sprite Distributed System", which also connects the topics "high performance" and "distributed systems". The focus of this paper is on an initial user evaluation of the effectiveness of Stepping Stones and Pathways as an IR approach and tool. 1 In order to motivate the need for this tool, Section 2 describes related work in the area of knowledge discovery. Section 3 briefly describes how Stepping Stones and Pathways are created. Section 4 discusses our experimental results and evaluation. Section 5 gives conclusions. In this section we review previous work  X  on knowledge discovery, citation analysis, and interactive retrieval  X  which inspires most of our research on relationship finding. The knowledge discovery field aims to find patterns and relationships in data aided by specific knowledge of the structure of the solution space. [15] is a g ood (if not totally up-to-date) survey of the field of computer-aided knowledge discovery. CARTwheels [10] and Arrowsmith [14] are knowledge discovery systems that address the problem of finding connections. CARTwheels builds connections among topics (similar to pathways in our approach), by  X  X tory-telling X . It works through the formalization of objects of interest described as multiple tuples of attributes, where each tuple belongs to one description set. CARTwheels then finds sets of common objects that have description attributes in different description sets. Arrowsmith works at discovering connections by automatically using the content in a collection of scientific papers. It detects indirect relationships between topics in MEDLINE by finding common keywords between two document sets. In Arrowsmith the user submits two ranked lists of documents, the A and C-Lists, which are the result of searching PubMed for the two topics in which the user is interested. From those lists, Arrowsmith retrieves the document abstracts and classification metadata, and finds common keywords and phrases. Arrowsmith arose from studies of so-called  X  X iterature-based discovery X  [13]. These studies proved that it is possible to build meaningful connections among seemingly unrelated document sets. Further investigations [11] also support the results, and examine their impact. The main difference between these studies and ours is that they require extensive use of a pre-existing classification system and accurate classification of documents. On the other hand, approaches relying more on free-form text [7, 8] need the involvement of user experts who know a great deal about the connections sought. Furthermore, chains of relationships previously studied have been Details of several years of work on this appear in a dissertation completed in 2004 [3]. limited to one intermediate step, with very specific types of relations, e.g., illness  X  (symptom, effect)  X  treatment. The use of citations as a means to find relationships among documents in a professional field originated in the library and information sciences, as a means to quantify the importance of authors and journals, and to discover patterns and trends within fields. The first comprehensive use of citations to explore the research literature was developed by Eugene Garfield in the form of the ISI Science Citation Index [4], where unskilled employees could type the references, and services aided finding relevant authors and new areas of science. Nowadays, the availability of the Internet and the World Wide Web has made it possible to have electronic access to research papers originally only available in printed versions. With the aid of citations, on-line services like CiteSeer [6] support searching and directly provide related papers. Interactive retrieval can lead to enhanced search results, especially when guided by user feedback. Rocchio X  X  algorithm introduced in SMART gave an empirical proof that user relevance feedback on the top retrieved documents improves precision. The work of Williams in RABBIT [16] adopted a wider view of query reformulation in the context of semantic networks, 1) by allowing the user to classify retrieved items as relevant or non-relevant; and 2) by letting the user request a specialization of a part of the query (part-of or contained-in relation, asking for a more restricted interpretation), or to request alternatives to a part of the query (sibling-of relation). Scatter/Gather [2] partially went beyond users only providing relevance assessments, relying upon clustering as a way to allow the user to separate a subset of the potentially relevant documents identified by the system. It showed that combining a  X  X ood enough X  approach to document clustering with user interactivity is possible, and can improve retrieval. In this section we describe the IR process from the point of view of Stepping Stones and Pathways, and how they are created. Since the focus of this paper is on evaluation, this description will necessarily be short. Further details can be found in [3]. Given a question with low-relevance results (i.e., where the number of retrieved documents judged relevant is low), there can be several possible scenarios explaining that outcome, including: 1. an incorrectly formulated query, or 2. a case where the collection does not have pertinent content. These two possible explanations for poor results are the ones assumed by designers of classic IR systems. Under our model we propose two more scenarios: 3. a low-relevance set where the results are dominated by a subset of the query formulation (e.g., a focus on a few terms), or 4. a query that retrieved a low-relevance result set, but where it is possible to get high relevance for combinations of results of (maybe modified) subsets of the query formulation. Cases 3 and 4 have not been studied in depth. Consequently, when a query result set has low-relevance, the usual approach is simple query splitting, changing the interpretation of a query to  X  X  or Y X , without also addressing the problem of coherence from the user X  X  point of view; that is: are X and Y related (possibly by other intermediary topics)? Our approach, in the first version of Stepping Stones and Pathways and Pathways, is to consider the user query as made of two subqueries. Though we continue to work on query splitting, we decided to study in parallel what occurs when we ask the user to identify two topics. While a query can be split into more than two subqueries, we work with two b ecause short user queries as found in user studies [9, 12] have few possible split points (else the meaning of the subqueries becomes too general, causing precision to drop dramatically). Then we use each s ubquery as an independent query to create initial clusters, hoping to better match the user X  X  interest. From the two subqueries, Stepping Stones and Pathways creates two initial groups of documents, the sources of the topic network, that we call endpoints. These endpoints are then connected directly or through other intermediate topics, with each connection between topics justified by one or more documents. The process to create the network of topics proceeds as follows: 1. Search using each s ubquery, trying to match all words in the 2. Create endpoints by: 3. Create Stepping Stones and Pathways by: 4. User explores documents, topics, and pathways. 5. User (optionally) joins and/or splits topics. 6. User continues exploring and modifying the network until a Document text is always indexed and available, and references are indexed whenever available. Similarity between documents d d thus is calculated from all available information using a noisy-or combination: sim(d 1 ,d 2 )=1-(1-P w (d 1 ,d 2 ))(1-P cocit (d 1 ,d 2 ))(1-P where P w is the probability of documents d 1 and d given the document texts, P cocit is the probability of d being related given their cocitations, and P ref is the probability of d and d 2 being related given the documents they cite . Since not all documents have citations available, it was important that citations have an influence on the similarity score whenever available, but also that similarity can be calculated without them. This formulation was chosen because it can ignore missing information in either text or citations and calculate a similarity score with the available parts. Document text similarity is calculated using the cosine similarity. The citation similarity ( P is calculated as the cosine similarity of the citation vectors of each document. The cocitation similarity ( P cocit ) is calculated using Garfield X  X  co-citation link strength [5]: P cocit (d 1 ,d 2 ) = ( cocitations of d 1 and d 2 citations of d 2 )-(cocitations of d 1 + cocitations of d When citations are extracted automatically, there are a number of factors that make it easier to extract text than citations from papers, and affect the interpretation of similarity scores based on citations. The number of citations per scientific paper is low, usually from 10 to 20. Not all citations can be extracted, in effect thus reducing the number of citations available. Also, citations come in many different formatting styles and conventions. Rarely if ever are different citations confused as the same, but variations and mistakes in the citations makes it difficult to find all cases where  X  X ifferent X  citations actually refer to the same document. Given these problems in finding and assessing the complete set of citations, we cannot guarantee that two papers are in fact the same paper based solely on the citations, even if the sets of citations in both papers seems to be exactly the same. There is a possibility of misclassification which depends on the extraction problems described above. Thus the effects of errors in citations can both understate and overstate the relationship between documents. The citation probability described above must be adjusted depending on the number of citations available and the estimated probability of error in extracting all citations. We lack space to include the derivation of the adjustment factor (see [3]), so we just describe it briefly. We cannot account for the references that are not recognized as equal, but we can adjust the probability P account for the probability of overstating the similarity between two groups of paper citations. This adjustment factor understates the similarity based on citations. The maximum adjustment occurs when each of the documents has only one citation that also appears in the other document, from which we would assume they are the same document when they are in fact not. The reason we favor understating the probability based on citations is that for the purpose of finding answers by connecting topics and documents, precision (more false negatives) is more important than recall (more false positives).
 Chains of topics and documents are ranked by estimated relevance. This is based on interpreting a path between topics as a bipartite graph made of documents and topics. This graph is then assigned probabilities and interpreted as a Bayesian network, where the ranking value of the chain is P(d 1 ,...,d n |T being the documents and T j being the topics involved in the chain. For additional information see [3]. Because it is so different from other IR systems, it is extremely difficult to evaluate Stepping Stones and Pathways (abbreviated as SSP). Consequently, we adopted a three-fold approach. First, we spent months in formative evaluation, refining the interface based on repeated small-scale user studies. Our main lesson was that users need a bridge to help them move their mental model of IR beyond search engines that produce ranked lists in response to a short query. The display in the bottom part of Figure 1 plays that role. Second, we made available a demo site on WWW and have noted positive comments; further studies of such use are planned. Finally, as reported below, we launched a formal summative evaluation, to compare Stepping Stones and Pathways to an implementation of a retrieval system that uses exactly the same ranking algorithm, but, instead of pathways, returns a ranked list of documents. We call the latter the Standard Query tool (SQ), to differentiate it from SSP. Both retrieval systems employed the same query/document ranking algorithm, using cosine similarity, with document titles given more weight than document bodies. One way to test the quality of the results at helping users find connections between topics, comparing Stepping Stones and Standard Query, is to ask the user to find connections using both systems, and score them. If Stepping Stones connections are valid, and they elicit the relation between topics better than what the user can find by exploring a ranked list, then the  X  X onnectedness X  of the topics should be higher in Stepping Stones. To carry out this test, we used a collection of operating system papers that we harvested from the WWW. For the sake of space, we only will briefly describe how we gathered this collection. It was created by obtaining seeds from eight operating system course syllabi that had the following characteristics: 1. The course had to be based on papers, not on a textbook, and the papers had to be listed in the syllabus. 2. The papers had to be available online in their majority, so we could harvest the papers and extract the text and citations. 3. Each syllabus had to group the papers into areas, so we could have an association between a paper and an area of the course. 4. The set of syllabi had to be about a very small set of courses, so we have multiple syllabi for the same course (set of topics). Using the papers in these syllabi as seeds, and following up to two levels of referencing, we harvested available papers plus their references from CiteSeer, for a total of 2173, almost all of them with their corresponding references. We indexed the title and up to the first 4K bytes of text for each document, plus their references. Then, from the topics in the syllabi, we chose 4 pairs of topics (i.e., 4 query pairs each made of two topics, as given in Table 2), which were weakly related by the references of the papers in both topics. The reason we harvested our own collection is that in order to create 2-topic queries we needed papers associated with topics, and topics that had some guarantee of being related. 
Query Pair # Query Pair 4 file system web server Table 2. Query numbers and the corresponding query pairs. We asked 12 subjects to use SQ and SSP to find how pairs of topics were related in the collection, and to score different aspects of the connections between the topics in each of the 4 query pairs. For each c onnection found, users were asked to answer the questions listed in Figure 3. Question 6 in particular was included to help us discard answers that the user could not explain: If the user could not explain the relationship, then the score assigned to a relation between topics measures previous knowledge, or general understanding gained with the tool, instead of knowledge of the connection itself. 1. What are the topics involved? 2. Are the topics related? (1=Not at all, Vaguely, Partially, 3. List the titles of one or more documents involved in the 4. Are the documents related? (1=Not at all, Vaguely, 5. The documents are related to the topics (1=Strongly 6. Describe the connection between the topics. 7. In a scale from 1 to 5, describe your confidence in your Each of the 12 participants were computer science students who had passed an operating systems course, and ranked their knowledge of operating systems as at least 3 (average) on a scale from 1 (very basic) to 5 (expert). They volunteered to participate in the experiment, and were paid after completing it. Each subject was assigned two query pairs to connect. Each query pair in turn was connected once by the user using Standard Query, and by the system using Stepping Stones. Thus each subject was asked to perform 4 retrieval sessions, each of which we call a run , pairs. Even though they were asked to  X  X orget X  from one session to the next, it is possible (e.g., because of learning) that the result of one run with a query pair could affect the result of the next run with the same query pair. To account for learning effects, the order of runs of users was interleaved and alternated between using SSP and SQ. Although the study is not completely randomized, b ecause users perform two runs per query pair to avoid having user knowledge of subject matter as a confounding factor, we accounted for the effects of memorization and query order in two ways: For each subject that started with a query pair using Stepping Stones, there is another subject that started with the same query pair but using Standard Query. Also, for each pair of query pairs (qp i ,qp there is one subject that started with pair qp i and then followed with query pair qp k , there is another subject that did the opposite, starting with query pair qp k and following with query pair qp numbers of connections and documents returned by SQ and SSP are listed in Table 4. 
Query Pair # N d SQ N C SSP N d SSP 
Table 4. Query pairs used in the experiment, along with the number of documents returned by Standard Query (N d SQ) or by Stepping Stones (N d SSP), and number of connections After participants felt they understood how to use both tools, they were asked to perform the 4 retrieval runs. We gave each participant the 2 pairs of topics that they were asked to connect. The instructions included 4 URLs that the user should go to, each one with instructions for each run. Additionally, we included definitions for each of the topics in each of the query pairs. For each run, each subject was asked to return up to 3 connections between the pair of topics, for a total of up to 12 answers per subject. The limit of 3 connections was set to cap the amount of time participants had to spend on each task and on the whole experiment. We also asked that each run with Standard Query should contain at least one answer involving more than one document connecting the pair of topics, whenever possible. All statistical tests were performed at significance level  X  =0.05. The standard relevance judgment in query answering in this case has two components: topics and supporting documents. We can define a pathway to be relevant if the use considers the topics in the pathway well-connected (meaning the connection is useful and understandable) and well-supported by documents. In this section we discuss how we evaluated these two components, and the effort of finding a valid pathway in Stepping Stones as compared to the effort it took for the user finding connections by himself  X  connecting the results of the search by exploring and reading, as he would do with a standard retrieval tool. By answering question 2 of the user X  X  questionnaire for each connection, each user pr oduced a set of scores for each run. Each user used 2 query pairs, and for each query pair he returned 2 sets of scores (4 runs/user). One set in each pair of scores sets belongs to Stepping Stones, the other to Standard Query. Each pair of score sets came from the same user, to account for previous user knowledge about the topics in the query pairs. Since a user returns multiple results for a query pair, and the number of results varies not only within query pairs but also for each run, we needed to reduce the 2 pairs of score sets to 2 pairs of scores (4 values). In order to be able to test scores comparing pairs from the same user, we normalized the score sets as follows: 1. We assumed the user performed the runs as her  X  X est effort X . 2. An additional consideration is that for each pair of sets of The final result is 2 pairs of scores for each participant, for a total of 24 score pairs. The scatter plot in Figure 5 shows all the pairs. This scatter plot reads as follows: each point represents a pair of score value medians. For each user and query pair, the value along the X axis is the score assigned for Standard Query; the Y value is the corresponding score for Stepping Stones. The number next to the dot in the scatter plot is the number of points with that score combination of score values, for all participants. Figure 5. Scatter plot of the count of score-pairs of medians for each query in Stepping Stones and in Standard Query. Each dot is a combination of scores, one for Standard Query and Numbers above the diagonal are counts where Stepping Stones performed better than Standard Query; numbers below the The difference in medians for each pair of scores is approximately normal with mean 0.3 and standard deviation 0.88. The critical value when testing the alternative hypothesis  X  X verage of median scores for Stepping Stones is higher than for Standard Query X  is t Paired T-Test shows a difference between SQ and SSP scores, with the average median of SSP being higher than the average median of SQ scores in each pair. The significance of the critical value is very close to  X  , so perhaps a safer assertion to make with confidence would be that connections between topics in Stepping Stones are at least as good and no worse than the ones found by the user with Standard Query. For each query and search method, the highest number of connections reported by any user is shown in Table 6. Since not all participants reported the same number of pathways, if all participants would have agreed on the pathways, then all sets of pathways reported should be a subset of the biggest set reported by any user for each query and search method. Thus, considering the highest number of pathways returned by any user in each case, if all users had agreed on the set of pathways reported, in this experiment they would have returned a total of 36 unique pathways. 
Query Pair Standard Query Stepping Stones 
Table 6. Highest number of connections reported for each query and each search method (all users). Users were required However, in the experiment the users reported a total of 61 unique pathways between topics, 51 pathways in Standard Query (12 of them duplicates of another pathway also reported with Standard Query by another user), and 50 pathways in Stepping Stones (13 of them duplicates). The intersection had a total of 47 pathways, 32 of them duplicates. This gives a total of 15 unique pathways that are common to SSP and SQ, 24 pathways exclusive to SQ, and 22 pathways unique to SSP. From these numbers, it is clear that users found different sets of connections by themselves from the set suggested by Stepping Stones. Interestingly, the summary statistics of median scores of topic connectedness for SQ were  X  =4.146  X  = 0.8403, while for SSP were  X  =4.458  X  = 0.8198. The paired difference test for difference of means of connection scores exclusive to SQ vs. means of connection scores exclusive to SSP had significance=0.096. Furthermore, as we mentioned in 4.2.1, the score of connections between topics for Stepping Stones was at least the same as for pathways found by the user with Standard Query. Therefore we can affirm that Stepping Stones was useful at providing users with connections that are valid, non-obvious, and different from the ones the user would find by herself, therefore providing novel answers to users. We used the number of unique documents inspected by the subjects before reporting each connection as a measure of the effort it took to find a valid connection. The fewer unique documents examined, the less effort it took for the user to find a valid connection that she could explain. From the log of user actions we counted the number of documents the user requested before reporting a new connection with each tool. By  X  X umber of unique documents X  we mean that each document was c ounted Median for Stepping Stones only once for every connection reported by the user, even if it was explored more than once by the participant in the same run. For each query, we calculated the average number of documents inspected per connection reported by the user. Thus we have 2 pairs of values for each of the 12 participants, and 1 pair of values for each query pair the user went through, for a total of 24 data points. The alternative hypothesis to test was  X  X he average number of unique documents inspected per connection reported by the user while using Standard Query was higher than the same average with Stepping Stones. X  The distribution of differences was approximately normal so we tested using paired samples. Tables 7 and 8 show the results of the test.
 Standard Query 5.9063 4.5401 Stepping Stones 3.2308 1.5611 Table 7. Summary statistics for average number of documents Mean 2.6754 4.5516 0.9291 2.88 23 0.008 The critical value is t critical (df=23)=1.714&lt;t affirm that the Paired T-Test shows a statistically significant difference between SQ and SSP scores, with the average number of unique documents inspected per connection reported by the user while using Standard Query being higher than the same average with Stepping Stones. The highest variation corresponds to Standard Queries (compare the standard deviations in Table 7). On average, it took about 80% more documents to find a connection with Standard Query than with Stepping Stones. This is not as obvious a conclusion as it may seem: It could in fact take more documents to report a connection in SSP, if many of the connections reported by it were not found relevant by the user  X  but this was not the case. Every pathway returned by Stepping Stones is made of topics. Every connection between every pair of topics is supported by at least 1 document. Even if a connection seems valid, according to documents chosen by the system to support that connection, indicating an error of judgment by the system. The purpose of question 5 in the questionnaire, filled in by the participants for every connection, is to measure how well the documents can be associated by the participant to the topics in the connection. We tested the scores given to question 5 in Standard Query and Stepping Stones by a Test for Equality of Means, comparing Standard Query and Stepping Stones by applying an Independent Samples Test. Since some of the data points were missing because of lack of answers from participants, equal variance was not assumed. The alternative hypothesis was  X  X verage score for Stepping Stones is different from average score for Standard Query X  (see Table 9). The null hypothesis can be rejected at  X  =0.05 with significance 0.015, with  X  SQ =0.41 vs.  X  SSP 2.498 77.749 0.015 0.39 0.157 Table 9. Results of hypothesis testing for scores of Topic/Document relationships (equal variances not assumed). This difference can be attributed to pathways made by the subjects containing mostly 2 topics, while Stepping Stone pathways always documents to a smaller set of topics, they would accordingly have a higher score. When testing the same hypotheses but only with user-reported connections containing 3 topics or more (Table 10), the significance falls dramatically, and now the explanation of equal averages for topic/document scores cannot be rejected. Table 10. Result of hypothesis testing for scores of Topic/Document relationships, considering only pathways with at least 3 topics (Welch test). A factor affecting the topic/document scores is that users may be scoring their own connections higher because they created them; it could be said that by scoring the topic/document relations, they are scoring their own work. Seven connections were found as valid by the same subject for the same query pair, both by using Standard Query (found by user) and Stepping Stones (found by the system). Perfectly fair subjects should have assigned the same score to the same connections in both cases. Also, for the case of a document reported by the same user for the same query in both Stepping Stones and Pathways, the median for topic/document scores for Standard Query (  X   X  SQ ) should be the same as for Stepping Stones (  X   X  SSP ). However, for these 7 cases,  X   X  SQ =4.1429 &gt;  X   X  SSP =3.85. While the difference is not statistically significant, due almost surely to the small sample size, it shows that users may tend to assign a higher score to themselves than to SSP. In this paper we introduced Stepping Stones and Pathways, a method to answer user queries by chains of topics and documents, where every link in the chain is an element of a partial answer; chains, as a whole, answer the user X  X  information need. Connections provided by Stepping Stones and Pathways are as good as connections found by users, but connections reported by the user took more effort. The quality of connection between topics in Stepping Stones was statistically better, or at least as good as the connections that users found by themselves, both in terms of relationships between topics and between documents supporting and explaining the connections. For every user, the average number of documents needed to find a valid connection by browsing a ranked list was almost twice the number needed to find a valid connection in the list of connections shown by Stepping Stones, indicating that finding a valid connection was easier with Stepping Stones. This was not due to one particular query in our experiments, or to the document set returned in Stepping Stones being smaller than with the ranked list. What is a valid answer to  X  X  related to Y? X  depends on what is a valid connection for the user: Stepping Stones and Pathways suggests valid connections the user was not aware of. We expected users to restrict their set of connections to a very small group, achieving a certain agreement on what connections were important. Our users in practice reported as valid a variety of connections both built by themselves and by the Stepping Stones tool. If we consider the Standard Query and Stepping Stones results as separate, disjoint sets, where all sets of connections reported by users for the same search method would be subsets of a single set of answers, the users would have reported 36 different connections (since not all users returned the same number of connections). Instead they reported a total of 61 different connections. While the popularity of connections is not uniform, the difference in the variety of answers is not just chance. It reflects the fact that what constitutes a valid connection is subjective and is affected by the knowledge background and expectations of the user. These results validate the utility of Stepping Stones and Pathways as a tool to handle queries that can be answered by a sequence of partial answers contained in multiple documents. We centered our study in scientific literature collections, because that is where we see our approach as having the biggest payoff, and because of their rich structure of topics, citations, and authors. Hence, we have opened the door to many interesting additional questions. A first question is the frequency of occurrences of queries that are candidates for better answers with Stepping Stones. A second question regards what other types of documents and collections might benefit from SSP. Third, we wonder how best to split queries automatically to test the value of Stepping Stones as a general retrieval tool and a new method to present results. We are currently working on these three questions. Another area of further research is to study the effect of noise on finding pathways over bigger collections. The collection of papers we used for the experiments is a small one. While its size did not guarantee a successful outcome of our experiments, we certainly admit that a small collection of papers on a specific area of computer science has a higher chance of allowing the system to find related papers than a wider collection, with more topics and more documents that can be misclassified by the system. Hence, we have begun studies with two other small collections, as well as with part of TREC and an XML-annotated text collection (in order to allow larger scale investigations). This research was supported in part by NSF grant IIS-0307867,  X  X xtending Retrieval with Stepping Stones and Pathways X . [1] Borodin, A., Kerr. L., Lews, F. Query Splitting in Relevance [2] Cutting, D., Karger, D., Pedersen, J. and Tukey, J. [3] Das Neves, F. Stepping Stones and Pathways: Improving [4] Garfield, E. Essays of an Information Scientist , Vol. 1, [5] Garfield, E. ABCs of Cluster Mapping. Part 1: Most Active [6] Giles, L., Bollaker, K., and Lawrence, S. Citeseer: An [7] Gordon, M., Dumais, S. Using Latent Semantic Indexing for [8] Lindsay, K., Gordon, M. Literature-Based Discovery by [9] Morickz M., Marais J., Hensinger M., Silverstein C. Analysis [10] Ramakrishan, N., Kumar, D., Mishra, B., et al. Turning [11] Spasser, M. The Enacted Fate of Undiscovered Public [12] Spink, A., Wolfram, D. Jansen, M, and Saracevik T. [13] Swanson, D. Two Medical Literatures that are logically but [14] Swanson, D., Smalheiser, N, Bookstein, A. Information [15] Valdes-Perez, R. Discovery Tools for Science Apps . CACM [16] Williams, M. What makes RABBIT run? Journal of Man-[17] Zamir, O., Etzioni, O. Web document clustering: a 
