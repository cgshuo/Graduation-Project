
LIAAD-INESC TEC, Porto, Portugal
Instituto de Telecomunica X  X es, Instituto Superior T X cnico, Lisbon, Portugal GECAD-ISEP, Institute of Engineering, Polytechnic of Porto, Porto, Portugal 1. Introduction a particular application, the same objective-functions are optimized to produce data partitions.
Sometimes, one may want to guide data, express preferences or conditions, or provide extra informa-removing or modifying data features. However, it can be very hard to accomplish in practice. A sim-solutions using a mapping of the apriori knowledge about the data in the form constraints. of objects, commonly designated as must-link and cannot-link constraints, have been widely studied. constraints.

Despite one may expect that using constraints would always improve, or at least not to worsen, the searching for relations between pairs of objects [4,7,23,28].
 Learning distance metrics may enhance the quality of the classification and clustering tasks [26]. new data representation or distance function.

We extend a previous work on acquiring constraints for data clustering [12]. In that work, relevant clustering constraints are proposed in Section 3. The evaluation of the performance of the proposed future work are disclosed in Section 5. 2. Related work
In this section the constrained data clustering problem is introduced and some methods that will be used in our experiments are concisely described. 2.1. Constrained data clustering R = the set of cannot-link constraints composed of the pairs of objects that should be grouped into
The Constrained Average-Link (CAL) [11] is an agglomerative hierarchical clustering algorithm based objects in the data set.

The Constrained Single-Link (CSL) algorithm consist of simple modifications to the single-link al-not be considered for merging [10]. 2.2. Pairwise constraint acquisition with our proposed approach.
 The first method for the acquisition of constraints has a very simple and naive design. The Random R this method is that it does not search for informative and non-redundant constraints. disjoint neighborhoods are found or the maximum number of queries is reached. Queries between x and a random object belonging to each neighborhood are posed. If x does not belong to any neighborhood, neighborhood N k . Then an object x not belonging to any neighborhood is randomly selected. Queries be applied when the data representation is feature-based. 2.3. Distance learning with pairwise constraints preferences, may improve the performance of data clustering.

The Discriminant Component Analysis (DCA) [15] is a distance metric learning algorithm capable a object in Q i . Then DCA learns a data transformation which minimizes the variance between domain as: optimizing Eq. (4).
 The learned Mahalanobis matrix M is computed as M = A A . 3. Proposed methods for acquiring pairwise constraints Also, the methods can be applied when only (dis)similarities between data are available. 3.1. Non-iterative method for acquiring constraints The first proposed (dis)similarity-based constraint acquisition method consists of four phases: 3. Rank candidates. This phase consists in ranking the candidates in descending order of interest. Algorithm 1 Intra cluster candidates Require: Dissimilarity matrix D ;Cluster C ; Number of candidates c , Neighborhood parameter  X  ;Set 2: m  X   X   X | C | 3:  X  ( x i , x j )  X  X  =  X  X  = , D ( a, b )  X  0: x a  X  X  i , x b  X  X  j 4: for l  X  1 to c do 5: ( x i , x j )  X  arg max 6: Q = Q  X  ( i, j ) 7: Compute N i as the set of the m th closest objects to x i in C , according to D original 8: Compute N j as the set of the m th closest objects to x j in C , according to D original 9: D ( N i , N j )  X  0 , D ( N j , N i )  X  0 10: end for 11: return Q Algorithm 2 Inter cluster candidates 4:  X  ( x i , x j )  X  X  =  X  X  = , D ( a, b )  X  X  X  : x a  X  X  i , x b  X  X  j 5: for l  X  1 to c do 7: Q = Q  X  ( i, j ) 8: Compute N i as the set of the m th i closest objects to x i in C 1 , according to D original 9: Compute N j as the set of the m th j closest objects to x j in C 2 , according to D original 11: end for 12: return Q Algorithm 3 Rank candidates Require: Candidates Q ; Dissimilarity matrix D ; Data partition P ;. 1: Set S  X  S 1 ,...,S | Q | , S j =0 ,  X  j  X  X  1 ,..., | Q |} 2: for l  X  1 to | Q | do 3: ( i, j )  X  Q l 4: if ( i, j ) is an intra-cluster candidate then 6: else 7: S l  X  D ( i, j ) 8: end if 9: Q ranked  X  Candidates Q sorted in ascent order according to S ; 10: end for The outline of the proposed algorithm is presented in Algorithm 4.
 Algorithm 4 Non-iterative acquisition algorithm 1: Q  X  X } 2: for all C k  X  P do 3: Q  X  Q  X  IntraClusterCandidates ( D ,C k ,c, X , R = , R = ) 4: end for 5: for all ( C i ,C j ) ,C i  X  P,C j  X  P,i &lt; j do 6: Q  X  Q  X  InterClusterCandidates ( D ,C i ,C j ,c, X , R = , R = ) 7: end for 9: l =0 10: repeat 11: l  X  l +1 13: Answer  X   X  X hould x i and x j belong to the same cluster? X  14: switch (Answer) 15: case  X  X es X  : 17: case  X  X o X  : 18: R =  X  X  =  X  ( x i , x j ) 19: end switch 20: until l = q 21: return {R = , R = } equal or higher than the number of desired queries) the following inequality must hold: q cK + the ranking phase.
 with the original data representation and the representation obtained by using random constraints. 3.2. Iterative method for acquiring constraints change the regions of the queries as the clustering algorithm learn how to correct its errors.
A second method for acquiring constraints is proposed for solving the problem above and works by query-candidates. Algorithm 5 shows the outline of the proposed algorithm. 4. Experimental results In our experiments, 5 synthetic data sets (shown in Fig. 6) and 5 real data sets taken from the UCI Algorithm 5 Iterative acquisition algorithm 1: for counter  X  1 to i do 2: Data  X  DistanceLearner ( D or X , R = , R = ) 3: P  X  ConstrainedClusteringAlgorithm ( Data , R = , R = ) 4: {R = , R = } X  NonIterativeAlgorithm ( D ,P,q, X ,c, R = , R = ) 7: Augment R = and R = by exploring the transitive closure 8: end for 9: return {R = , R = } Quarterly Almanac (125 democrats and 107 republicans). The Wine data set consists of the results of features.

In our experiments we used the CAL and CSL algorithms to produce the data partitions, defining the with replacement, setting t he size of the samples to the ori ginal size of the data sets.
The quality of the data partitions was assessed using the Consistency index [13] which measures the obtained from ground-truth information. The index is defined as where it is assumed that the correspondence problem between clusters was already solved.
Table 1 shows the average CI values and standard deviations of the partitions produced by CAL algorithm using no constraints and using constraints acquired by RAC, Explore-Consolidate and the searches was defined as accuracies rounding the 90%, which is as 7% increase when comparing with not using constraints. In representations. The improvements varied between 12% to 20%. The Proposed + L approach was again the best approach achieving 99.41% of average accuracy, which is an huge improvement when compared which was a 19% improvement. For the House Votes and Wine data sets, a drop in the quality can be observed when using learned data representations for RAC and proposed methods. For these methods it approach achieved the best result with 96.24% by performing 50 queries, which was an improvement of almost 8% when comparing with using the original data representation, and an 18% improvement representations.

As a summary of the results obtained using CAL algorithm some conclusions can be stated. Con-(not always) higher than the ones obtained without constraints. When the number of queries increase the accuracy generally also increase. Also, the average CI of the proposed and Explore-Consolidate methods were generally superior than the average accuracy achieved by the RAC method. However, by comparing our method with the Explore-Consolidate no method was a clear winner. The quality of the data representations with the proposed and Explore-Consolidate approaches than the random acquisi-is low. This may be caused by an over-fitting effect caused by performing semi-supervised distance learning using a small subset of constraints.
 Table 3 shows the average accuracies and standard deviations of the partitions produced by CSL. the best results, 99.1% which is around 15% better than the results achieved by Explore-Consolidate and RAC. The Explore-Consolidate and the proposed methods achieved 100% accuracy in Cigar data set. The RAC method best result was obtained using distance learning (92.74%) which was a clear improvement when compared to its performance using the original data representation (85.58%). In D1 data set the best results were obtained by he proposed and Explore-Consolidate methods with 100%. obtained using Explore-Consolidate method, followed by the proposed and RAC methods. The only exception was in Crabs data sets when using learned data representations in which the proposed + L approach outperformed the other methods.

Summarizing the results for the CSL algorithm, the proposed approach and the Explore-Consolidate CSL algorithm, and mainly when analyzing the results of the real data sets, it can be seen that the are comparable. When comparing the methods with and without distance learning, one may conclude that the distance learning does not increase the quality of clustering while using the CSL. Figure 7 presents a comparison of the performances of the non-iterative method presented (NI) in Subsection 3.1, and the method described in Subsection 3.2. The bars indicate the average accuracy second method was set to 5, and therefore, the number of queries per iteration was 10. By comparing should be used instead of the non-iterative version.
 [0.35; 0.45] interval. 5. Conclusions
We proposed two method for acquiring constraints for data clustering. The general idea of the first representation should be used.
 Acknowledgements
This work is supported by FCT  X  X unda X  X o para a Ci X nci aeaTecnologia X  under the project  X  X earn-ingS X   X  PTDC/EEI-SII/2312/2012. The authors also acknowledge the support of the European Commis-sion through the project MAESTRA (Grant number ICT-2013-612944).
 References
