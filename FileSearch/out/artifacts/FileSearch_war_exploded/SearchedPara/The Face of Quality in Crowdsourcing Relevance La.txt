 Information retrieval systems require human contributed relevance labels for their training and evaluation. Increasingly such labels are collected under the anonymous, uncontrolled conditions of crowd-sourcing, leading to varied output quality. While a range of quality assurance and control techniques have now been developed to re-duce noise during or after task completion, little is known about the workers themselves and possible relationships between work-ers X  characteristics and the quality of their work. In this paper, we ask how do the relatively well or poorly-performing crowds, work-ing under specific task conditions, actually look like in terms of worker characteristics, such as demographics or personality traits. Our findings show that the face of a crowd is in fact indicative of the quality of their work.

The recent industrialization of online crowdsourcing, popular-ized by commercial crowdsourcing platforms, e.g., CrowdFlower or Amazon X  X  Mechanical Turk (AMT), is turning millions of Inter-net users into crowd workers. This enables to scale up tasks that require large scale human involvement and rapid data collection. One such task is the gathering of relevance labels for the evaluation of search engines, where crowdsourcing offers a feasible alternative to employing editorial judges [1, 5]
However, the output of crowdsourcing, especially in the case of micro-tasks and when monetary incentives are involved, often suf-fers from low quality. This is typically attributed to worker error, dishonest behaviour, problems with the design of the human in-telligence task (HIT), or mismatch in the incentives or in the task requirements and the workers X  abilities [4, 12, 18, 21]. To address this issue, a range of quality assurance and control techniques have been proposed in the literature, aiming to reduce deliberate or un-intended worker error during or after task completion. Common to these methods is that they treat workers equally, without differ-entiating between workers from different backgrounds. However, there is already evidence in the literature that workers from differ-ent countries or cultures work and behave differently [6, 15]. In this poster, we explore the relationship between characteristics of the workers and the quality of their work . In particular, we ask how do the relatively well or poorly-performing crowds, working under specific task conditions, actually look like in terms of de-mographics or personality traits. We take an empirical approach and run experiments, collecting relevance labels, using Amazon X  X  crowdsourcing platform. More specifically, we devise HITs to cap-ture demographics and personality information about the workers who perform the same relevance labeling task in one of two dif-ferent conditions, differing in the richness of the employed quality control methods.

Our analysis of the characteristics of the workers reveal that di-versity matters: Both the demographics and personality profiles of the workers are strongly linked to the resulting label quality. Among the observed factors, workers X  location (region) has the greatest correlations with label quality.
The evaluation of IR systems, based on the Cranfield method, re-quires the gathering of relevance labels, which is traditionally per-formed by trained editorial judges [19]. However, this is an expen-sive process that does not scale to meet today X  X  demands due to the ever growing size and diversity of test corpora and the volume of queries for which to label documents. Crowdsourcing has proven particularly useful for labeling large data sets, such as acquiring relevance assessments for search results [1]. However, several pa-pers raised concerns about the poor quality of crowdsourced data [17, 21]. Indeed, quality assurance has surfaced as a major chal-lenge in crowdsourcing and instigated a number of research efforts to understand and remedy the problems, e.g., [7, 13, 17, 18]. Com-mon approaches include the use of gold standard data [5, 18] and building in redundancy, i.e., ensure that the task is performed by multiple individuals [7]. While research and empirical explorations of these methods have led to useful guidelines for effective crowd-sourcing, e.g., [4, 7, 11], it is not clear what sorts of crowds are better at providing the desirable outcomes. Is it possible that bet-ter workers share some common characteristics or do more diverse groups produce better labels?
Recent studies have shown that the same HIT design can have different effects across the worker population. For example, while the level of pay can affect both the quantity [13] and the quality of the work [10], it may influence different types of workers dif-ferently. Higher pay can encourage better work in some workers but it may also attract unethical workers motivated solely by the higher financial gain. Similarly, [3] found that US and non-US workers responded differently to information provided about the purpose of the task, as in the case of labeling cancer cells in im-ages. Other work focused on observing the characteristics of the engagement with the task, such as the rate of labeling documents, in order to identify behavioral patterns among crowd workers and classify them into several types [20, 21].We extend on these works by providing a side by side comparison of the characteristics of the crowds involved in a relevance labeling task, working in one of two HIT designs.
Our goal is to study the characteristics of groups of workers and the quality of their work. We experiment with two HIT designs of the same relevance labeling task that differ in the richness of the employed quality control methods:
Due to the weaker quality control in SD, we expect that the group of workers in this condition will demonstrate higher levels of unde-sired behavior, while FD is designed to deter such workers.
To characterize workers, we collect the following information as part of the HITs:
The data used in our experiments consists of the online books, search topics, participants X  official retrieval runs, and relevance judg-ments of the INEX 2010 Book Track [9]. We use the same 21 topics that were used in the official evaluation of the track and the rele-vance labels that were collected from the INEX participants (our gold set). This set includes 3,357 page level judgments, 169 judged pages per topic on average. For each topic, we selected a random set of 100 book pages from the pool of pages in the participants X  runs, ensuring that each such set contains at least 10 relevant pages. We grouped 10 pages per HIT, using the same data set in both de-signs. We collected labels from 3 workers each in both FD and SD. As part of the HIT design in both FD and SD, we administered a questionnaire to collect worker information, see Figure 1. For the personality traits, we used a standard 10 question test [14].
To measure the quality of work, we calculate the ratio of correct labels per worker based on agreement with the gold set (Accuracy) and aggregate these to groups of workers , e.g., per design. We ex-clude the 6 workers from our analysis who contributed HITs to both the FD and SD batches, thus removing data for 15 assignments. For survey style questions, we take the most frequent response or mode over the different HITs per worker. For quantitative measures, e.g., Accuracy, we take the arithmetic mean.
In this section, we present the results of our analysis of the re-lationships between the workers X  characteristics and the resulting task performance measured by the workers X  labeling accuracy. We calculate statistics over 3 groups of workers, the 117 workers who completed FD HITs, the 146 workers in SD, and the total 263 work-ers. For each of the groups, we plot the distributions of worker ac-Figure 1: Survey administered to gather information on worker characteristics (Scoring the BFI-10 scales, averages of two rows: Extraversion: 1R, 6; Agreeableness: 2, 7R; Consci-entiousness: 3R, 8; Neuroticism: 4R, 9; Openness: 5R; 10 (R = item is reversed-scored) Figure 2: Worker accuracy over location and gender demo-graphics in FD, SD and All HITs curacy scores per demographic or personality category and test the correlation between these.
Among the demographic data, we see that location has a strong relation to accuracy over FD, SD and All data (one way ANOVA p &lt; 0.001), see Figure 2a. A reason for the significant drop in Asian workers X  performance may be that they lacked the necessary lan-guage skills for the task. Such a lack of understanding of the task or of the documents that needed to be labeled led to increased ef-fort (well-performing Asian workers spent significantly longer time on the HITs than others) on the one hand and increased cheating or sloppy behavior on the other. Looking at the distribution of workers by location, see Figure 4a, we see that the majority of workers who contributed to FD HITs were from America (64%, 61% of which in the US), SD HITs were completed mostly by workers in Asia (61%, 53% of which in India). This difference could possibly be a result of the applied pre-filtering in FD, which would then suggest that most Asian workers do not have sufficiently high AMT rep-utation scores. However, regardless of the reason, based only on the differences in the accuracy (Figure 2a), similarly to [3, 16], we can conclude that Asian workers produced in our task lower quality work than American or European workers.

Gender shows no significant relation to accuracy over All data, but it is significant for SD and FD (ANOVA p &lt; 0.05 for both), see Figure 2b. Interestingly, the relation is reversed for the designs: for FD, female workers outperform male workers, whereas for SD, male workers outperform female workers. Looking at the differ-Trait Description -High Score Description -Low Score Extraversion Sociable, fun-loving, affectionate Retiring, somber, reserved ences in gender distribution between FD and SD, Figure 4b, we see that more male workers contributed to SD HITs and more females to the FD HITs. These differences correlate with the regional dis-tribution of workers: workers from India tend to be males (58%), while workers in the US are mostly female (59%, although 22% of US workers withheld this information) and are similar to those reported in [6, 15].

Age also has a significant relation with accuracy over All data (Spearman r = 0.17, p &lt; 0.05), see Figure 3a. The relation is simi-lar for FD (r = 0.16, not significant) but slightly negative for SD (r = -0.08, not significant). As we can see in Figure 4(c), SD workers are somewhat younger (average age of 26) than FD workers (aver-age age is 31), and this difference resulted in a considerable impact on accuracy. Unlike the workers X  age, and contrary to expectation, we find that education level is not significantly related to accuracy (Figure 3b), despite the correlation between age and education, and the expectation that more educated (skilled) workers would be bet-ter placed to tackle this high cognitive task [2]. Answers to the education level question, see Figure 4d, reveal that a high portion of SD workers are university students (61%, compared with 44% in FD), which corresponds with the reported age distributions. On the other hand, reading habits are significantly related to accuracy over All data (Spearman r = -0.19, p &lt; 0.01), indicating that more fre-quent readers are more accurate, see Figure 3c. This suggests that reading habits are better predictors of accuracy than education. The relation is also significant for FD (r = -0.20, p &lt; 0.05). There is a weak but insignificant relation for SD, which, given that SD work-ers claimed to be relatively more frequent readers, see Figure 4(c), may suggest a tendency of SD workers to give socially desirable responses.
Among the personality traits, both openness and conscientious-ness significantly relate to accuracy over All data (Spearman r = 0.24, p &lt; 0.001 and r = 0.21, p &lt; 0.05, resp.) and over FD (r = 0.22, p &lt; 0.05 and r = 0.24, p &lt; 0.05) but not over SD (r = 0.23 and r = 0.34), see Figure 5. Agreeableness has a weak (not significant rela-tion with accuracy over All data (r=0.13, p=0.06). Interestingly, for SD none of the BFI traits are related to accuracy. This suggests that Figure 5: Accuracy over personality: openness (left), consci-entiousness (right), top row shows accuracy for High (&gt;3) and Low (&lt;3.5) BFI values per FD, SD and All, bottom row shows calculated BFI values for All personality characteristics are not very effective at distinguishing low quality workers but are useful to distinguish between good and better workers.

Figure 6 shows the distribution of responses to the BFI question-naire in FD, SD and All. Again, we can observe clear differences between the workers in the FD and SD sets. FD workers are more open to new experiences while SD workers are more confirming to routine (low openness scores). FD workers are also more con-scientious than workers in the SD group. SD workers are more extravert, self-interested (agreeableness), and more insecure (neu-roticism) than FD workers (not significant).

Given the overall high quality of work by workers in FD, we may classify desirable workers, at least for a high cognitive relevance assessing task, as open and conscientious middle-aged females in the US, who read books on a regular basis.
Crowdsourcing is characterized by its large and anonymous work-force, yet the individual competencies of the workers are crucially determining the quality of work. This paper explored the relation-ship between certain characteristics of the workers and the quality of their work. Our main findings are as follows. First, in terms of demographics, we found that location has a very strong relation to accuracy, with Asian workers demonstrating significantly poorer performance than American or European workers. Interestingly, we also found a clear separation of the crowds by location across the two designs, with mostly Asian workers in SD and American workers in FD. Although the HITs were not restricted to geographi-cal regions, the exact HIT conditions impacted on the geographical distribution of the workers who completed the HITs, which in part then explain the differences in the quality of work. Second, in terms of personality types, we found that openness and conscientiousness relate significantly to accuracy. Again, we saw that workers with notably different personality traits worked under the different HIT conditions. Our overall conclusion is that there is a complex in-teraction between the particular HIT conditions and the types of workers who engage in a task, and that these worker characteristics are related to the quality of their work. The face of a crowd is in fact indicative of the quality of their work and should be considered when designing crowdsourcing systems.

