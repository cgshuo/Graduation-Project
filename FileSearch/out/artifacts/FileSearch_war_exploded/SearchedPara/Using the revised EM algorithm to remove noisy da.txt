 1. Introduction
Text classification, or the task of automatically assigning semantic categories to natural language texts, has become one of the key methods for organizing online information. It is a basic building block in a wide range of applications. For example, directories like Yahoo! Categorize Web pages by topic, online newspapers cus-tomize themselves to a particular user X  X  reading preferences, and routing agents at service hotlines forward incoming emails to appropriate experts by contents. To organize training examples for learning tasks, binary yet most important formulation of the learning problem. These two classes can be composed of  X  X  X elevant (positive) X  X  and  X  X  X on-relevant (negative) X  X  for information retrieval applications ( Joachims, 2002 ).
Generally, some classification tasks involve more than two classes. When we apply the binary setting to the multi-class setting with more than two classes, there is a problem that the multi-class setting consists of only positive examples of each category; each category does not have negative examples. In order to solve this problem, the one-against-the-rest method has been used in many cases ( Hsu &amp; Lin, 2002; Zadrozny &amp; Elkan, 2001; Zadrozny &amp; Elkan, 2002 ); it can reduce a multi-class problem into many binary tasks. Fig. 1 shows how binary settings using the one-against-the-rest method. For example, the documents of the  X  X olitic X  category are considered as positive examples and those of the other categories are as negative examples. That is, while all the documents of a category are generated as positive examples by hand, documents that do not belong to the category regard as negative examples indirectly. This labeling task concentrates on only selecting positive examples for each category, and it does not label the negative examples which have the opposite meaning of counterpart positive category directly. Thus the negative data set in the one-against-the-rest method prob-positive examples from various categories, it is hard to be considered as the exact negative examples of each category.

These noisy documents can be one of the major causes of decreasing the performance for binary text clas-sification. Thus classifiers need to efficiently handle these noisy documents to achieve the high performance. There are two problems to efficiently remove them from training data as follows: (1)  X  X  How can we find a boundary area containing many noisy documents?  X  X  (2)  X  X  How can we deal with noisy documents found from the boundary?  X  X 
The rest of this paper is organized as follows. Section 2 presents previous related work. In Section 3 ,we explain the proposed method in detail. Section 4 is devoted to the analysis of the empirical results. Section 5 describes conclusions and future work. 2. Related work for text classification ( Roy &amp; McCallum, 2001; Schohn &amp; Cohn, 2000; Tong &amp; Koller, 2001 ).
Liu et al. studied the problem of classification with only partial information, one class of labeled (positive) documents, and a set of mixed documents ( Liu et al., 2002 ). The main idea of this method is to first use a spy technique to identify some reliable negative documents from the unlabeled set. They theoretically showed that nique to solve the problem by utilizing the EM algorithm (S-EM) with the Naive Bayes classification method.
Yu et al. proposed an SVM based technique (called PEBL) to classify Web pages given positive and unla-reliable negative documents from the unlabeled set (called strong negative documents in PEBL), and (2) build-ing a classifier using SVM. Strong negative documents are ones that do not contain any features of the positive data. After a set of strong negative documents is identified, SVM is iteratively applied to the construction of a classifier.
 Li and Liu proposed a system which combines the Rocchio model with the SVM technique ( Li &amp; Liu, 2003 ).
This system consists of two steps: (1) extracting some reliable negative documents from the unlabeled set, (2) documents; Rocchio and Rocchio with clustering. The clustering technique that Li uses is k -means. The experi-ment using Rocchio with clustering showed better result than an experiment using Rocchio.

Tong and Koller introduced a new algorithm for performing active learning with SVM ( Tong &amp; Koller, 2001 ). By taking advantage of the duality between parameter space and feature space, they arrived at three algorithms that attempt to reduce version space as much as possible at each query. These three algorithms can provide considerable gains in both inductive and transductive settings.

Roy and McCallum presented an active learning method that directly optimizes expected future errors ( Roy &amp; McCallum, 2001 ). This becomes feasible by taking a sampling approach to estimating the expected reduction in error due to the labeling of a query.

Schohn and Cohn described a simple active learning heuristic which greatly enhances the generalization behavior of SVMs ( Schohn &amp; Cohn, 2000 ). They achieved better performance from a small subset of the data than all available data is used.

While many previous studies achieved the improved performance on specific classifiers such as SVM, we focus on improving the one-against-the-rest method, which can be applied to all kinds of classifiers. As a result, the proposed method also obtained remarkable improved performance on all the classifiers ( k -NN, Naive Bayes, Rocchio, SVM) and all the test data sets (Reuters, WebKB, Newsgroups) in our experiments. This shows that the proposed method can contribute to improvement for text classification generally. 3. The proposed binary text classification method
This section will explain the proposed approach in detail. It consists of the following four steps: (1) applying dow technique, (4) the revised EM algorithm. 3.1. The one-against-the-rest method
In the one-against-the-rest method, the documents of one category are regarded as positive examples and the documents of the other categories as negative examples ( Hsu &amp; Lin, 2002; Zadrozny &amp; Elkan, 2001;
Zadrozny &amp; Elkan, 2002 ). In order to set up training data into binary classification, multi-class setting is reformed into the binary setting using the one-against-the-rest method such as Fig. 1 . 3.2. Calculating prediction scores
The goal of this and the following sections is to find a boundary area which denotes a region including many noisy documents. First of all, using a positive data set and a negative data set for each category from the one-against-the-rest method, we can learn a Naive Bayes (NB) classifier and we can obtain a prediction score for each document by the following formula.
 where c i means a category and d j means a document of c i d to be positive in c i , and P (Negative j d j ) means a probability of the document d
According to these prediction scores, the entire documents of each category are sorted out in the descending order. Probabilities, P (Positive j d j )and P (Negative j d Bayes formula as follows ( Craven et al., 2000; Ko &amp; Seo, 2004; Lewis, 1998 ): where t i is the i -th word in the vocabulary, T is the size of the vocabulary, and N ( t t in document d j .

Note that we use the Naive Bayes formula as the base in the proposed method because it has a strong foun-dation for EM and is more efficient. 3.3. Calculating entropy using the sliding window technique
In our method, a boundary can be detected in a block with the most mixed degree of positive and negative documents. The sliding window technique is first used to detect the block ( Lee, Lin, &amp; Chen, 2001 ). In this technique, windows of a certain size are sliding from the top document to the last document in a list ordered by the prediction scores. An entropy value is calculated for estimating the mixed degree of each window as follows ( Mitchell, 1997 ): where, given a window ( W ), p + is the proportion of positive documents in W and p is the proportion of neg-ative documents in W . For example, if a window of five documents has three positive documents and two neg-the final estimated entropy value is calculated by this formula  X  Entropy  X  W  X  X  3 This value is the highest entropy value when using five documents as window size.

Two windows with the highest entropy value are picked up; one window is firstly detected from the top and the other is firstly detected from the bottom. If there are no window or only one window with the highest entropy value, windows with the next highest entropy value become targets of the selected windows. Then maximum ( max ) and minimum ( min ) threshold values can be searched from selected windows, respectively.
The max threshold value is found as the highest prediction score of a negative document in the former window and the min threshold value is as the lowest prediction score of a positive document in the latter window. The left side of Fig. 2 explains how to set up max and min threshold values.

We regard the documents between max and min threshold values as unlabeled documents. These documents are considered as potentially noisy documents.

Now three classes for training documents of each category are constructed just like the right side of Fig. 2 : definitely positive documents, unlabeled documents, definitely negative documents. By applying the revised
EM algorithm to these three data sets, we can extract actual noisy documents and remove them. 3.4. The revised EM algorithm
In this paper, the EM algorithm is used to pick out noisy documents from unlabeled data and to remove them. The general EM algorithm consists of two steps, the Expectation step and the Maximization step and labels the unlabeled documents by hard classification ( Expectation ( E or E
Bayes classifier is used in the two steps of the EM algorithm. Fig. 3 shows how the EM algorithm is revised in our method.
 E 0 -step is reformed to effectively remove the noise documents located in the boundary area. Unlike original
E -step, it does not assign an unlabeled document, d u , to the positive data set, P , because it regards d another noisy document; since positive documents are labeled by hand and have enough information for a category, additional positive documents can decrease performance. Finally, we can learn the text classifiers with binary training data generated by the revised EM algorithm. 4. Empirical evaluation
In this section, we provide empirical evidences that the proposed method is effective in improving binary text classification. We present experimental results with three different test data sets: UseNet newsgroups ( Newsgroups ), web pages ( WebKB ), and newswire articles ( Reuters ). Results show that the proposed method outperforms the original one-against-the-rest method. 4.1. Data sets and experimental settings The Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20
UseNet discussion groups ( McCallum &amp; Nigam, 1998 ). Many of the categories fall into confusable clusters; removing words that occur only once or on a stop word list, the average training data vocabulary over all five folds has 51,325 words (with no stemming).

The second data set comes from the WebKB project at CMU ( Craven et al., 2000 ). This data set contains web pages gathered from university computer science departments. The pages are divided into seven catego-ries: course, faculty, project, student, department, staff, and other. In this paper, we used the four most pop-ulous entity-representing categories: course, faculty, project, and student. The resulting data set consists of 4198 pages with an average vocabulary of 18,742 words over all five folds. It is an uneven data set; the largest category has 1641 pages and the smallest one has 503 pages.

The Reuters 21578 Distribution 1.0 data set consists of 12,902 articles and 90 topic categories from the Reu-ters newswire. In our experiments, we used only the ten most populous categories out of the 90 topic categories to identify the news topic. Since the documents in this data set can have multiple category labels, each category dard  X  X odApte X  split. The standard  X  X odApte X  train/test split divides the articles by time. We used all the words inside the title and body, a stoplist, and no stemming. The vocabulary from training data has 21,023 words.

For fair evaluation in Newsgroups and WebKB, we used the five-fold cross-validation method. That is, each data set was split into five subsets, and each subset was used once as test data in a particular run, while the remaining subsets were used as training data for that run. The split into training and test sets for each run was the same for all the classifiers. Therefore, all the results of the experiments are averages of five runs.

In the preprocessing step to extract features from each document, the content words were extracted from all the documents by the Brill POS tagger ( Brill, 1995 ). Words with noun or verb POS tags were considered as content words. In addition, we implemented conventional classifiers for experiments: k -NN, Naive Bayes (NB), Rocchio, and SVM. The text classifiers except SVM can handle multi-class problem directly. But most correspondence between category and document. That is, each document can be in multiple, exactly one, or no of binary classification tasks by using the one-against-the-rest strategy ( Joachims, 2002 ). The k in k -NN was the linear model offered by SVM light .

As performance measures, the standard definition of recall and precision is used. The micro-averaging method and the macro-averaging method are applied for evaluating performance average across categories ( Yang, Slattery, &amp; Ghani, 2002 ). Results are reported as the precision-recall BEP (BreakEven Points), which is a standard information retrieval measure for binary classification; given a ranking of documents, the pre-cision-recall breakeven point is the value at which precision and recall are equal ( Joachims, 1998; Ko &amp;
Seo, 2004; Yang, 1999 ). 4.2. Experimental results
This section provides empirical evidences for the effectiveness of the proposed method. The experimental results show that the proposed method achieved better performance than the original one-against-the-rest method in all the three training data sets and all the four classifiers. 4.2.1. The experiments for setting parameters
The purpose of experiments in this section is to make a decision about setting several parameters before main experiments are conducted. The data for these experiments is composed of 5408 training documents and 1802 validation documents, which are created from 6490 Reuter training documents. The Naive Bayes classifier is selected for these experiments, and the basis system denotes the Naive Bayes classifier using the original one-against-the-rest method. The parameter adjustment depends on the data set of application domain. However, since finding optimal parameters for each domain is a very tedious task, we attempt to rec-ommend the range of optimal parameters. Thus we first found the optimal parameters on the Reuters data set and used them for the other data sets (Newsgroups and WebKB). As a result, remarkable improvements were also achieved on the Newgroups and WebKB data sets even though the optimal parameters were not obtained from them directly. 4.2.1.1. Verifying the effectiveness of positive documents in the one-against-the-rest method. As we explained in the sufficient information for a category. Thus we think that any modification of the positive data by machine may not be helpful to the improvement of text classification. The experiments in this subsection give proofs for our assumption. In first experiment, we verify the E 0 -step of the revised EM algorithm proposed in the Section 3.4 ; the E 0 -step of the revised EM algorithm is based on our assumption. The performances of the original EM algorithm ( E -step) is compared to that of the revised EM algorithm ( E As you can see in Table 1 , we achieved higher performance in the revised EM algorithm than in the original EM algorithm.

For further evaluation, the next experiment tests whether it is more effective that the whole positive docu-ments are used as positive training data without any modification; all the positive documents are regarded as only definitely positive ones and they are never classified as the unlabeled data or the definitely negative data among three classes shown in Fig. 2 . This experimental setting is denoted as  X  Positive Fix  X  and otherwise as
Not Fix  X  case. In this experiment, the revised EM algorithm ( E changes according to the iteration number were observed.

As a result,  X  Positive Fix  X  setting obtained better performance than  X  Positive Not Fix  X  in all the iteration numbers, and the best performance was achieved in second iteration. Thus our basic assumption, that any modification of the positive data by machine may not be helpful, is verified through these experiments. The semi-supervised models such as our revised EM algorithm find a better local maximum than unsupervised models since their initialization is closer to the desired one. As shown in Fig. 4 , the performances drop with
EM iterations before converging. This decrease generally happens except when only a limited amount of hand-labeled example is available ( Merialdo, 1994 ). In addition, Liu et al. also used two iterations for their S-EM is applied to the following experiments. 4.2.1.2. Comparing the performances in different window sizes. We here observe the performance changes according to several window sizes used in the sliding window technique. Table 2 shows the performances when windows sizes are 3, 5, and 7. As a result, the window size is fixed as 5.
 4.2.2. The experimental results to verify the proposed method in each text classifier and each data set To evaluate the effectiveness of the proposed method, we implemented four different text classifiers (Naive
Bayes, k -NN, Rocchio, and SVM). And the performance of the original one-against-the-rest method is com-pared to that of the proposed method on three test data sets. Tables 3 X 5 show the experimental results from each text classifier in the Reuters data set, the WebKB data set, and the Newsgroups data set, respectively. As a result, the proposed method achieved better performances than the original method over all the classifiers and all the data sets. Note that the proposed method obtained the improved performances in even all the cat-egories of each data set. This is an obvious proof that the proposed method is more effective than the original one-against-the-rest method.

As shown in Tables 3 X 5 , SVM achieved less improvement than the other classifiers. It is caused by the fact that the performance of SVM using the original one-against-the-rest method is too high in all the data sets. Note that it is more difficult to improve a classifier with higher performance.
 4.3. Discussions 4.3.1. The analysis of the performance with regards to the number of positive data and the property of categories
In this section, we first observe the relationship between the performance improvement and the number of positive documents using the Reuters data set. As shown in Table 6 , the proposed method obtained remark-from Tables 3 X 5 can become another proof; the performance differences using the macro-averaging measure are bigger than ones using the micro-averaging measure. Moreover, we could look at anther phenomenon from the results of Table 6 . Categories with comprehensive contents such as  X  trade  X  showed much improve-that these categories can have a lot of noisy documents in the negative data set. As a result, the proposed method is more effective when there are insufficient positive data and ambiguous categories. Actually, we can frequently meet these cases in many application areas. Therefore, we believe that the proposed method is usefully applied to binary text classification applications to improve their performance. 4.3.2. The analysis of the changes of cohesion scores when applying the proposed method
A simple additional experiment was conducted to prove that the proposed method removes the noisy doc-uments properly. Salton argued that a collection of small tightly clustered documents with wide separation we employed the method used by Salton et al. (1975) to verify the proposed method.
 We define the cohesion within a negative data set and the cohesion between positive and negative data sets. within a negative data set is a measure for similarity values between documents in the negative data set of a category; note that, since a positive data set is not changed in the proposed method, only the cohesion score of the negative data set is calculated. The cohesion between positive and negative data sets is a measure for sim-latter is calculated by formula (6) : denotes k -th training data set, ~ C k denotes a centroid vector of k -th training data set, and troid vector of the total training data.

In formulae (4) , ~ C k is described by the mean vector of each document set, and mean vector of all training vectors. The cohesion within a negative data set in formula (5) is calculated by averaging cosine similarity values between ~ C 2  X  negative  X  sion between positive and negative data sets in formula (6) is calculated by averaging cosine similarity values between ~ C glob and ~ C k .

As shown in Table 7 , we can observe the high cohesion within a negative data set and the low cohesion between positive and negative data sets in each category when using the proposed method. We can find out that our proposed method reforms the vector space for a better performance: the high cohesion within a negative data set and the low cohesion between positive and negative data sets. Using the proposed method, the document vectors in a negative data set are located more closely and positive and negative data sets are separated more widely. 5. Conclusions
In this paper, we proposed a new method for binary data setting in binary text classification, which revised the original one-against-the-rest method using the sliding window technique and the revised EM algorithm.
The experimental results showed that the proposed method produced the significant improved performance any category of all the data sets were not detected from our experiments. This result proves the effectiveness of the proposed method in binary text classification. In Section 4.3 , we verify the proposed method in that it can be more useful method in real application areas and it can reform the document vector space for better per-formance in binary text categorization. As a result, the proposed method can provide much improvement when it is used in real binary text classification applications instead of the one-against-the-rest method. Acknowledgement
This work was supported by the Korea Research Foundation Grant funded by the Korean Government (MOEHRD) (KRF-2006-331-D00536).
 References
