 In this paper we study the problem of mining noisy tagging. Most of the existing discriminative classification methods to this prob-lem only consider one tag at a time as the classification target, and completely ignore the rest of the given tags at the same time. In this paper we argue that all the given multiple tags can be utilized simultaneously as an additional feature and the information con-tained in the multi-label space can be taken advantage of to improve the performance of the classification. We first propose a novel dis-tance measure to compute the distance between instances in the multi-label space. Then we propose several novel methods to in-corporate the information of the multi-label space into the discrim-inative classification methods in one view learning or in two views learning to solve a general multi-label classification problem and to mitigate the influence of the noise in the classification. We apply the proposed solutions to the problem with a more specific context  X  noisy image annotation, and evaluate the proposed methods on a standard dataset from the related literature. Experiments show that they are superior to the peer methods in the existing literature on solving the problem of mining noisy tagging.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ; H.2.8 [ Database Management ]: Database Applications X  Data mining, Image databases Algorithms, experimentation Noisy tagging; multi-label space; image annotation prediction
Recent research has witnessed a great success in developing ef-fective solutions to many important real-world problems through appropriate tagging. Thus, tagging is considered as an important means to develop solutions to many such real-world problems, es-pecially in the areas of information retrieval, knowledge manage-ment, and database. However, with the typically explosive amount of data to be tagged, one can hardly do tagging manually. Conse-quently, social tagging has become an effective alternative to many such problems. For social tagging, the tagging results can never be expected to be perfect, and it always contains lots of noise, includ-ing missing tags and/or incorrect tags. Figure 1 shows exemplar images with noisy tagging. In Figure 1(a), the given annotations of the image are buildings , cityscape , sky , and street , while there exist many other things missed to be tagged, such as person , tree , water , fountain , and street lamp . The image in Figure 1(b) is given tags of animal , plants , horses , grass , and fox , while it is obvious that the tag fox is given incorrectly. In Figure 1(c), the given annotations of the image are animal and fish , while it is obvious that the animal in the image is bird instead of fish , and there exists missing tag of plants .

In general an N -class classification problem can always be de-composed into N binary classification problems in the one-vs-all (OVA) mode. It is easy to understand that the more tags the data are given, the more information the tag space contains. However, most of the discriminative methods to the multi-label classification prob-lem only consider the multi-label space as the classification target, and fail to make use of the information contained in the multi-label space effectively. Especially in the OVA mode, these classification methods only consider one tag at a time as the classification tar-get and at the same time completely ignore the rest of the tags. In this paper, we explicitly consider all the given tags simultaneously as an additional feature which further helps improve the classifi-cation performance. We claim that when the tags contain noise, taking advantage of all the given tags shall mitigate the influence of the noise compared with only considering one tag at a time as the classification target. Assuming that the correct tags are always the majority in all the given tags of an instance, these tags can provide additional information to help improve the training accuracy even when the instance is incorrectly labeled in the OVA mode. Figure 2 shows exemplar images with multiple tags. The images in the first row, which are all tagged as fish , always have the accompanied tags of water , coral , and ocean , while the images in the second row, which are all tagged as bird , always have the accompanied tags of sky , cloud , grass , and tree . Obviously, these accompanied tags can be utilized as an additional feature to help better distinguish images tagged as fish from images tagged as bird .

In this paper we study the problem of mining noisy tagging. We argue that noisy tagging exists in many real-world applications and that the solutions to the problem of mining noisy tagging shall gen-erate great societal and technical impacts. This paper proposes a new approach which makes use of the given multiple tags simulta-neously as an additional feature to deliver a more effective classifi-cation in solving a general multi-label classification problem and in mitigating the influence of the noise in the classification. We first propose a novel distance measure to compute the distance between instances in the multi-label space, which considers the various re-lationships among the multiple tags. Then we propose three novel methods to incorporate the information of the multi-label space into the discriminative classification methods in one view learn-ing or in two views learning, which we call SVM with Multi-label Soft Membership (SVM-MSM), SVM with Multi-label Constraints (SVM-MC), and Multi-label SVM in two views (MSVM-2K), re-spectively. We apply the proposed methods to the problem of min-ing noisy tagging with a more specific context  X  noisy image an-notation, and demonstrate through extensive evaluations using real data that the proposed methods perform well in comparison with the peer methods in the literature as effective and promising solu-tions to the problem of mining noisy tagging.
The noise among the data is categorized into two types: attribute noise and class noise [10]. In this paper, we focus on eliminating the bad effect raised by the class noise. There are a number of denoising methods for classification; they can be further classified into two categories: filtered preprocessing of the data and robust design of the algorithms. In the former category, filtered prepro-cessing is developed to remove the noise from the training set as much as possible [9, 11]. For the latter category, robust algorithms are designed to reduce the impact of the noise in the classification [4, 5, 7]. Lin and Wang [4] propose the fuzzy SVM to classify the noisy data by assigning a fuzzy membership to the cost of a target function. Tang et al. [7] modify the kNN-sparse semi-supervised learning on graph to annotate the tagged images with noise.
Most of the existing discriminative classification methods to the multi-label problem only consider one tag at a time as the classifi-cation target, and completely ignore the rest of the given tags at the same time. There has been work of treating sets of labels as sin-gle labels [6, 8]. However, all these methods increase the number of the classifiers if label sets between instances differ substantially, and fail to take advantage of all the labels simultaneously in one classifier. Godbole and Sarawagi [3] extend the original datasets with | S | extra features containing the predictions of each binary classifier; then a second round of training | S | new binary classifiers takes place using the extended datasets. However, this proposed method (called SVM-HF) fails to make use of the original given multiple labels of each instance simultaneously.

Compared with the existing work, our work takes advantage of all the given multiple tags simultaneously as well as considers the various relationships among the multiple tags to mitigate the influ-ence of the noise in the classification.
The main idea of the proposed methods is to minimize the dif-ferences between the classification results of each instance and its nearest neighbors from the multi-label space in the same view and in different views, respectively. We denote a training dataset as I . Each instance I i  X  X  is tagged with various tags. The whole tag vo-cabulary for I forms the S -dimensional multi-label space T . When one tag T r ( 1  X  r  X  S ) is chosen as the classification target, the other tags can form the additional feature space of tags, denoted as L r . Obviously, the dimensionality of L r is S  X  1 . Let an S -dimensional vector d i = ( d i, 1 , d i, 2 , . . . , d i,S tation for I i , where d i,r  X  X  0 , 1 } , 1  X  r  X  S represents the occur-rence of the r th tag T r for I i . For each I i and each T y i,r as the class label of I i , where y i,r = 2  X  d i,r  X  1 . In a typical one view learning, I i = ( x i , d i ) , where x i is the feature descriptor of I i in view F . In the two views learning, I i = ( x ( a ) where x ( a ) i and x ( b ) i are the feature descriptors of I and in view F ( b ) , respectively. Ideally, the two views F F ( b ) for I are conditionally independent.
In the discriminative classification approach we intend to learn a function f : X  X  R which discriminates instances in the two classes. In the SVM-based methods, this function is defined as f = w T x +  X  b , and the binary classification problem is solved by finding the division plane to separate the instances of the two classes. The optimization problem is presented as follows. In general an N -class classification problem can always be de-composed into N binary classification problems in the OVA mode. From (1) we observe that in the OVA mode, the SVM-based meth-ods only utilize one tag of the data at a time, and ignore the other tags the data contain at the same time.

It is easy to understand that the larger the number of the tags is, the more information the tag space contains. When the tags contain noise, taking advantage of all the given tags together si-multaneously shall mitigate the influence of the noise compared with only considering one tag at a time as the classification tar-get. In the OVA mode, when one tag T r is chosen as the classifi-cation target, the other tags can form the additional feature space of the tags L r . It is a reasonable assumption that the similarity between the classification results of two instances is inversely pro-portional to the distance between the instances in L r . The closer the instances in L r are, the higher the similarity in the classifica-tion is. We denote the feature vector of I i in L r as t t the distance for instances I i and I j in L r by using || t directly is unreasonable in most cases, for it is based on the tag independence assumption, which is violated due to the potential existence of the relationships among the tags. Actually in the real-world scenarios, there may be various relationships among the tags, e.g., some tags may co-occur frequently, while other tags may never co-occur.

We discuss the relationship between T r and T k ( k  X  X  1 , . . . , r  X  1 , r +1 , . . . , S } ) by examining the effect of | d i,k I ) on the distance between I i and I j in L r . When | d the effect of | d i,k  X  d j,k | on the distance between I L r is always zero; when | d i,k  X  d j,k | = 1 , the effect of | d d j,k | on the distance between I i and I j in L r varies depending on the association degree between T r and T k . We describe the relationship between | d i,k  X  d j,k | = 1 and the value of | d as follows. We define V r = { I i | d i,r = 1 } ( I i  X  I and r  X  { 1 , 2 , . . . , S } ). Formulas (2-5) describe four special relationships between T T . In reality, when T r is distributed evenly in V k and I X  X  is an undistinguished tag for T r ; when T r is distributed unevenly in V k and I  X  X  k , T k is a distinguished tag for T r . We show the conditional probabilities of d i,r = 0 or 1 given d i,k = 0 or 1 as follows, respectively.
 From (2-6) we observe that, when the value of P 00  X  P 11 is larger, the possibility that | d i,k  X  d j,k | = 1 leads to | d d j,r | = 1 is larger, and consequently the effect of | d i,k the distance between I i and I j in L r is larger. When the value of P 00  X  P 01 or P 10  X  P 11 is larger, the possibility that | d leads to | d i,r  X  d j,r | = 0 is larger, and consequently the effect of | d i,k  X  d j,k | on the distance between I i and I j in L r is smaller. We also note P 00  X  P 11 + P 10  X  P 01 + P 00  X  P 01 + P 10  X  P
We denote the association degree vector for each tag T r as g of g r are the association degrees between tag T r and the other tags. lows: Combining the feature vectors of the instances in L r with the asso-ciation degree vector for T r , we obtain the distance between I I in L r as follows: where indicates the Hadamard product between two vectors. The neighborhood of I i in L r (not including I i itself) based on the dis-tance measure in formula (8) is denoted as N r ( I i ) . The size u of the neighborhood N r ( I i ) for each I i is defined as the count of the nearest neighbors of I i in L r . N r i , { j | I j  X  X  r ( I
In a typical one view learning, we assume that the data in the target dataset are of the following formulation: I i = { x where y i,r  X  { X  1 , +1 } is the hard class label of I i for the tag T . Based on the assumption that the closer the instances in L are, the higher the similarity in the classification is, the information contained in the multi-label space can be added into SVM by intro-ducing the multi-label soft membership to change the class label of I from { X  1 , +1 } to [  X  1 , +1] . We denote l i,r as the multi-label soft membership for I i , where the value of l i,r not only depends on the hard class label of I i , but also depends on the hard class labels of the nearest neighbors of I i in L r . With the multi-label soft mem-bership, the typical SVM constraint is changed as follows to take advantage of the information contained in the multi-label space. where D is a constant, and 0  X  D &lt; 1 . Hence, we obtain the following optimization:
Another method to take advantage of the information contained in the multi-label space is to introduce the multi-label constraints which minimize the difference between the classification results of each instance and its nearest neighbors in L r .
Combining these multi-label constraints with the typical SVM constraints and allowing different regularization constants, we ob-tain the following optimization: where C and C  X  are constants, and C  X  &lt; C .
In the two views learning, we assume that the data in the target dataset are of the following formulation: I i = { x ( a ) where y i,r  X  { X  1 , +1 } is the hard class label of I i for the tag T . Additional constraints are introduced in the two views learning to maximize the similarity between the classification results of the same instances in the two views. These two views constraints are presented as follows:
Ideally, we can take advantage of the information contained in the multi-label space by introducing the following multi-label con-straints which minimize the differences between the classification results of each instance and its nearest neighbors in the same view and in different views, respectively. However, adding all these multi-label constraints would increase the dimensionality of the parameters, as well as the computational complexity substantially. It is easy to prove that when given one of these constraints (15-18) with the multi-view constraint (14), the other three multi-label constraints can either be strictly obtained or be approximately obtained with a little larger constraint variable value.

Hence, in order to decrease the dimensionality of the parameters, as well as the computational complexity, without raising a large bias corresponding to the solution to the optimization problem with the ideal constraints (15-18), we utilize the multi-label soft mem-bership introduced in Section 3.2 instead of the constraints (15) and (16) to minimize the differences between the classification results of each instance and its nearest neighbors in the same view. Further, we only select one of the constraints (17) and (18) to minimize the differences between the classification results of each instance and its nearest neighbors in different views. We denote  X  N r as N r ( I i )  X  X  I i } and N r i  X  X  i } , respectively. Hence, we obtain the optimization of MSVM-2K as follows: C ( ab ) , 0  X  D &lt; 1 . We apply our methods, including SVM-MSM, SVM-MC, and MSVM-2K, to the problem of mining noisy tagging with a more specific context  X  noisy image annotation. Two groups of com-parative experiments on one view learning task and on two views learning task are conducted to evaluate the performances of our methods, respectively.
 The NUS-WIDE [1] image database is used in the experiments. It includes 269 , 648 web images and 81 concepts which we treat as the ground truth tags. We choose the top 75 concepts whose num-bers of positive examples are larger than 350 from the database to form the multi-label space T . Hence, the dimensionality of the ad-ditional feature space of tags ( L r ) for each T r is 74 . For each con-cept, we randomly choose 150 positive examples and 150 negative examples to form the perfectly tagged training set. In the testing set, the numbers of the positive and negative examples are both 100 . The left 100 positive examples and 100 randomly selected negative examples form the extra untagged data used only for co-training. In the experiments, s % noise is added into both of the positive and negative examples of the perfectly tagged training set for each con-cept to form the noisily tagged training set. On one view learning, the 500 -D bag of words feature based on SIFT descriptions is used Table 1: The top 8 tags ( T k ) with the highest association degree g Table 2: The F1 measure for the testing set with the noisily tagged training set. as the feature of view F . On two views learning, the 500 -D bag of words feature based on SIFT descriptions is used as the feature of view F ( a ) , and the 1000 -D bag of text words feature which de-scribes the text information correlated to the images provided by the database is used as the feature of view F ( b ) . The returned value of the classifier in one view is denoted as f i , f i = w returned value of the classifiers in two views combined is denoted
As we described before, the size u of the neighborhood N r for each I i is the count of the nearest neighbors of I i u = 0 , SVM-MSM and SVM-MC are both reduced to SVM, and MSVM-2K is reduced to SVM-2K [2]. We define R as the ratio between the regularization constants for instances and the regu-larization constants for their nearest neighbors. For SVM-MSM, R = D ; for SVM-MC, R = C  X  /C ; for MSVM-2K, R 1 = D , R experiments, we set R 1 = R 2 = R 3 = R for MSVM-2K.
We evaluate the performances of the methods using the standard performance measures of Macro-F 1 ( F 1 a ) and Micro-F 1 ( F 1 Macro-F 1 averages the F 1 measures on the predictions of different tags; Micro-F 1 computes the F 1 measure on the predictions of different labels as a whole.

We randomly select 12 exemplar tags from the 75 concepts as the target tag T r , and describe the top 8 tags ( T k ) with the high-est association degree g r,k for each T r in Table 1. From Table 1 we observe that some tags, e.g., buildings and cityscape , may co-occur frequently, while other tags, e.g., sun and moon , may never co-occur. All these relationships among the tags are utilized to de-termine the most distinguished tags for the given target tag. By using the novel distance measure we have proposed, the most dis-tinguished tags are selected for each target tag T r as the most im-portant elements of the feature to measure the distances between instances in L r . This proposed distance measure is more reason-able and effective than directly using all the tags as the indiscrim-inate elements of the feature in finding the neighborhood of each instance in L r .

In Table 2(a), we summarize the F 1 measure for the testing set when u = 3 , R = 0 . 4 , and s is selected as 40 , 30 , 20 , and 10 using SVM, fuzzy SVM [4], SVM-HF [3], SVM-MSM, and SVM-MC on one view learning, respectively. Table 2(b) shows the F 1 measure for the testing set when u = 3 , R = 0 . 4 , and s is se-lected as 40 , 30 , 20 , and 10 using SVM, fuzzy SVM, SVM-HF, co-training, SVM-2K, and MSVM-2K on two views learning, respec-tively. From Table 2 we observe that SVM-MSM and SVM-MC (a) One view learning when s = 20 . (c) T wo views learning when s = 20 . (e) One view learning when s = 20 . Figure 3: (a)(b)(c) The F1 measure for the testing set as a function of u when R = 0 . 4 using SVM-MSM, SVM-MC, and MSVM-2K, respectively; (d)(e)(f) the F1 measure for the test-ing set as a function of R when u = 3 using SVM-MSM, SVM-MC, and MSVM-2K, respectively. perform better than SVM, fuzzy SVM, and SVM-HF with the nois-ily tagged training set on one view learning, and that MSVM-2K performs better than SVM, fuzzy SVM, SVM-HF, co-training, and SVM-2K with the noisily tagged training set on two views learning when s is selected as 40 , 30 , 20 , and 10 , respectively. It shows that taking advantage of all the given tags mitigates the influence of the noise compared with only considering one tag at a time as the classification target both on one view learning and on two views learning. Further, our proposed methods perform much better than the comparing methods when the noise ratio s % increases.
We describe the F 1 measure for the testing set as a function of u when s = 20 and R = 0 . 4 using SVM-MSM, SVM-MC, and MSVM-2K with the noisily tagged training set in Figure 3(a), (b), and (c), respectively. We observe that the F 1 measures for the testing set all increase when the size of the neighborhood for each N ( I i ) increases, which shows that it is helpful to use the nearest neighbors of each I i in L r to further improve the performance of the classification. The curves for the F 1 measures of SVM-MSM, SVM-MC, and MSVM-2K all exhibit their major elevation from u = 0 to u = 4 , then level off or even decline a little when u continues to increase, indicating that there is no need to choose a much larger u since some instances far away may be regarded as the neighbors which may cause the decline of the F 1 measure. As we describe before, when u = 0 , SVM-MSM and SVM-MC are both reduced to SVM, and MSVM-2K is reduced to SVM-2K. Figure 3(a), (b), and (c) also show that the performances of SVM-MSM and SVM-MC are much better than that of SVM, and the performance of MSVM-2K is much better than that of SVM-2K.
Figure 3(d), (e), and (f) show the F 1 measure for the testing set as a function of R when s = 20 and u = 3 using SVM-MSM, SVM-MC, and MSVM-2K, respectively. As we defined be-fore, when R increases, the effect of nearest neighbors from the multi-label space in the optimization also increases. We observe that when R increases, the curves for the F 1 measures of SVM-MSM, SVM-MC, and MSVM-2K ascend, which also shows that it is helpful to use the nearest neighbors of each I i in L r the influence of the noise in the classification.
This paper studies the important problem of mining noisy tag-ging. We propose several effective solutions to this problem ei-ther in one view learning or in two views learning. The novelty of the proposed solutions is that they incorporate the information contained in the multi-label space into the discriminative classifi-cation methods to mitigate the influence of the noise in the classi-fication. A novel distance measure is also proposed in this paper to compute the distance between instances in the multi-label space, which considers the various relationships among the multiple tags to obtain the most reasonable and promising neighbors for each instance. We apply the proposed solutions to the problem with a more specific context  X  noisy image annotation, and evaluate the proposed methods on a standard dataset from the related literature. Experiments show that they are superior to the peer methods in the existing literature in solving the problem of mining noisy tagging.
This work is supported in part by National Basic Research Pro-gram of China (2012CB316400), and ZJU X  X libaba Joint Lab. Zhongfei Zhang is also supported in part by US NSF (IIS-0812114, CCF-1017828). [1] T. S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. [2] J. Farquhar, D. Hardoon, H. Meng, J. Shawe-Taylor, and [3] S. Godbole and S. Sarawagi. Discriminative methods for [4] C. F. Lin and S. de Wang. Training algorithms for fuzzy [5] Y. Liu and Y. F. Zheng. Soft SVM and its application in [6] J. Read, B. Pfahringer, and G. Holmes. Multi-label [7] J. Tang, R. Hong, S. Yan, T.-S. Chua, G.-J. Qi, and R. Jain. [8] G. Tsoumakas and I. Vlahavas. Random k-labelsets: An [9] J. Van Hulse and T. M. Khoshgoftaar. Class noise detection [10] X. Zhu and X. Wu. Class noise vs. attribute noise: a [11] X. Zhu, X. Wu, and Q. Chen. Eliminating class noise in large
