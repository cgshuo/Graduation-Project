 The impact and significance of a scientific publication is measured mostly by the number of citations it accumulates over the years. Early prediction of the citation profile of research articles is a sig-nificant as well as challenging problem. In this paper, we argue that features gathered from the citation contexts of the research papers can be very relevant for citation prediction. Analyzing a massive dataset of nearly 1.5 million computer science articles and more than 26 million citation contexts, we show that average countX (number of times a paper is cited within the same article) and aver-age citeWords (number of words within the citation context) dis-criminate between various citation ranges as well as citation cat-egories. We use these features in a stratified learning framework for future citation prediction. Experimental results show that the proposed model significantly outperforms the existing citation pre-diction models by a margin of 8-10% on an average under various experimental settings. Specifically, the features derived from the citation context help in predicting long-term citation behavior. H.2.8 [ Database Application ]: Data mining Long-Term Impact; Citation Prediction; Citation Context; Strati-fied Learning
Citation count of a publication is among the most commonly ac-cepted metric by the research community for evaluating the impact and quality of a research article. Citation count refers to the number of citations received by an article within a specified time-period [2]. Highly-cited works remain as one of the most important criteria for various organization (e.g. companies, universities and govern-ments) to identify the best talents, especially at their initial stages. c  X  An early estimate would help in identification of promising articles that could accelerate research and dissemination of new knowledge. This has motivated the interest in the field of future citation predic-tion [17, 19].

Prediction of future citation counts, however, is difficult because of the nature and dynamics of citations [8, 10]. The citation ranges for the papers published by the same authors or the same venues show a lot of variation. The same can be said about the field of the papers as well. A very recent study [7] has shown that all the scien-tific papers do not follow the same trajectory and found 6 different citation patterns.

The existing works have used various venue and author centric features, along with the citation information from the initial years for the task of citation prediction. In this paper, we argue that the features extracted from the citation contexts can be extremely help-ful for the future prediction. Citation context refers to textual de-scriptions of a given scientific paper found in other papers in the document collection which cites it [1]. A citation context is, in principle, a set of sentences where a paper is referred to. The in-tuition behind using the citation context features comes from the hypothesis that citation contexts reflect the opinion of the scientific community about the particular work. We show that even using some very simplistic features extracted from the citation context can boost the performance of a citation prediction system signifi-cantly.

Towards this objective, we use a massive dataset consisting of more than 26 million citation contexts for nearly 1.5 million re-search papers in the computer science domain, crawled from Mi-crosoft Academic Search (MAS) 1 . We extract two features from the citation contexts  X  average countX (number of times a paper is cited within the same article, averaged over all the citing pa-pers) and average citeWords (number of words within the citation context, averaged over all the citing papers). We show that these features are quite discriminative and exhibit different trends not only for different citation ranges but also for the citation categories identified in [7]. We then append these features along with various other features in an earlier framework based on stratified learning [6]. Experimental results show that addition of these two features gives an R 2 -correlation of 0.84, 0.81 and 0.78 towards predicting the citation count at 5, 7 and 9 years after publication, improving the prediction accuracy by 8-10% on an average over the nearest baseline. Specifically, these features help in predicting the long term citation behavior of the research papers. We would like to stress here that this study brings forth the tremendous potential of http://academic.research.microsoft.com/ the content of a scientific article in predicting future citation counts; the huge success of only two very simple content related features proposed here makes the authors believe that deeper analysis of the content can lead to further significant improvements in the related areas of research.

The rest of the paper has been organized as follows. We discuss the related previous works in section 2. Section 3 describes the citation context dataset used for this experimental study. The two citation context features utilized for our study have been described in section 4. The citation prediction model has been described in section 5. The experiments to evaluate our system under different settings have been reported in section 6 along with a detailed com-parison and feature analysis. Finally, conclusions and future works have been presented in section 7.
In recent years, several researchers have investigated the prob-lem of future citation count prediction [17, 19, 20, 21]. Most of the past works have proposed a set of features and used a super-vised learning model to predict the citation count at a later time point. Many works use only the information available at the time of publication to predict future citation count, while other works also use information available from the initial years after publica-tion. For instance, Fu and Aliferis [9] predict citation count with the information available at the time of publication. They incorporate features like number of authors, number of articles for the first au-thor, number of citations for the first author, number of affiliations, the journal impact factor, title, abstract, MeSH terms 2 etc. Support Vector Machines (SVM) have been used to predict citation count after 10 years of publication. Similarly, Livne et al. [14] use fea-tures like authors, author institutions, venue and references to train a Support vector regression (SVR). They observe that venue and the references are the most significant features for citation count prediction.

Callaham et al. [16] use decision trees to predict citation counts of 204 publications from emergency medicine specialty meeting. They use features like impact factor of journal, research design, number of subjects, rated subjectively for scientific quality, news-worthiness etc. Kulkarni et al. [11] use linear regression and achieve an R 2 of 0.2 for the prediction of citation count for five year ahead window using 328 medical articles. They use features like journal name, month of publication, study design, clinical category of the article etc.

Brody et al. [3] use information after the publication to fore-cast citation count. Download data within the first 6 months after publication is used as a predictive feature. Similarly, Lokker et al. [15] use features related to the article and journal, like number of authors, pages, references etc. for the prediction task. Castillo et al. [5] use the number of citations, authors X  reputation and the source of paper citations as the predictive features.

Liangyue et al. [13] propose a joint predictive model to forecast the long-term scientific impact problem, formulated as a regular-ized optimization problem. Their work addresses four key algo-rithmic challenges, including the scholarly feature design, the non-linearity, the domain-heterogeneity and dynamics. Further, they propose a fast online update algorithm to adapt joint predictive model efficiently over time. They observe that citation history is a strong indicator of long-term impact and using additional con-textual or content features brings little marginal benefits in terms of prediction performance. An analysis on 463,348 papers from Phys-ical Review (PR) corpus suggests high heterogeneity in the citation http://www.nlm.nih.gov/mesh/ histories [18]. They present three fundamental mechanisms that drive citation history, namely preferential attachment, aging and novelty, and importance of a discovery (fitness). Combining these mechanisms allows to collapse the citation histories of papers from different journals and disciplines into a single curve, indicating that all papers tend to follow a similar universal temporal pattern.
Yan et al. present two similar works on citation prediction prob-lem [19, 20]. They have introduced features covering venue pres-tige, content novelty and diversity, and authors X  influence and ac-tivity. Pobiedina and Ichise [17] introduce a new feature GERscore (Graph Evolution Rule score), based on frequent graph pattern min-ing techniques, for citation prediction. Yu et al. [21] propose a new data structure namely discriminative term buckets to capture both document similarity and potential citation relation. They also propose metapath based feature space to interpret structural infor-mation in citation prediction. Along with these novel ideas, they present an extensive analysis on differences between citation pre-diction problem and the related work, e.g., traditional link predic-tion solution.

One of the previous works [6] suggests that stratified learning approach leads to good prediction accuracy. They observe that there exist six different patterns of citation profiles of research pa-pers based on the number and position of peaks in the citation pro-file. Further, a two-stage prediction model was proposed, which maps a query paper into one of the six categories in the first stage, and then in the second stage, a regression module is run only on the subpopulation corresponding to that category to predict the future citation count of the query paper. They achieve a superior perfor-mance just by using the features at the time of publication. Mo-tivated by this study, we also use a stratified learning framework for citation prediction. However, since the prime objective of this work is to show the utility of citation context features which are available only after publication, we utilize the citation context fea-tures derived from the first two years after publication along with the publication time features to improve the prediction accuracy.
To the best of our knowledge, this is the first work that attempts to use citation context based features in the citation prediction prob-lem. We use a massive dataset of more than 26 million citation contexts from computer science research articles towards this goal. The next section describes the dataset used in this study.
In this paper, we use two computer science datasets, both crawled from Microsoft Academic Search (MAS) 3 . First dataset (biblio-graphic dataset) consists of bibliographic information of papers, the title of the paper, a unique index for the paper, its author(s), the affiliation of the author(s), the year of publication, the publi-cation venue, references, citation contexts, the related field(s) the paper, the abstract and the keywords of the papers [6]. Second dataset (citation context dataset) consists of more than 26 million citation contexts pre-processed and annotated with cited and citing paper information.
 Table 1 details various statistics for both the datasets.
Definition I: We will say that paper P cites paper C , if paper P refers to paper C in the text. P is termed as citing paper while C is termed as cited paper . P can refer to C at many places in the text. In our present work, we only consider the sentence as citation context where the reference to the paper is explicitly present. http://academic.research.microsoft.com
Note that the different sub-branches like Algorithms, AI, Operat-ing Systems etc. constitute different  X  X ields X  of computer science domain. For an example, Chakraborty et al. [6] cites Yan et al. [19] as
Recently, Yan et al. conduct two similar experiments [ 25 , 26], to study features covering venue prestige, content novelty and di-versity and authors X  influence and activity. They also account for the temporal dynamics by taking a recent version of each feature calculated on a limited time window. To the best of our knowledge, this is the latest and the most accurate future citation count pre-diction model, and therefore serves as the baseline system in this paper. We conduct an extended examination of all these factors re-lated to citation counts, with many new features added.

Although, the above context consists of four sentences, we only consider the first sentence as the citation context since it explicitly refers to Yan et al. [19].

Definition II: countX for a cited paper C with respect to a citing paper P is defined as the number of citation contexts, when a paper P cites paper C . Citation context count for a paper C denotes the sum of countX from all the citations for C .

Each paper has a specific citation context count. Figure 1 shows the distribution of papers having specific citation context count in our dataset. Long tail depicts that many papers have less number of citation context count while a small number of papers have high citation context count.
 Figure 1: Distribution of citation context count in our dataset
Definition III: citeWords for a cited paper C with respect to a citing paper P is defined as the number of words in the citation context, when a paper P cites paper C . If multiple papers are cited within the same citation context, the number of words are equally divided among all the cited papers. If a paper is cited multiple times within the same paper, citeWords is computed by summing over the words in all the citation contexts.

In the next section, we discuss in detail how the average values of countX and citeWords behave for papers with various citation ranges.
After identifying countX and citeWords as two features from the citation contexts, we study in detail as to whether these features are discriminative with respect to the number of citations. To nor-malize with respect to various ranges of citations, we only used the average values of countX and citeWords for a publication in each year starting from its publication.

To give a working example as to how these features are com-puted, Table 2 presents citation contexts for paper P titled as  X  On Relaxed Dynamic Programming in Switching Systems  X , published in 2005. The first column gives the citer_ids, which refer to MAS identifier for the papers citing paper P . Publication year of the cit-ing paper is shown in column 2. Column 3 gives the exact citation context(s) in the citing paper for paper P . Below, we describe as to how the average countX and average citeWords features are com-puted for P over the years.
For a citation edge from paper Q to paper P (i.e., Q citing P ), countX denotes the number of times paper P is cited in paper Q . A high value of countX implies that paper P is cited multiple times by paper Q and thus, P might be quite relevant for paper Q . Possibly, Q has cited P for its different aspects. Our hypothesis is that if we consider all the papers citing paper P and find the average value of countX for P , it may serve as a very strong feature to measure the importance of P .
 Let us assume that the papers Q 1 ,Q 2 ,...,Q n are citing paper P for N 1 ,N 2 ,...,N n times respectively in the t th year after pub-lication of P . We define the average countX metric for paper P for the t th year as
Using the example in Table 2, average countX value for paper P for the first year after publication (year 2006) can be calculated as: average countX ( P, 1) = 2+1 2 = 1 . 5 . This is because there are two citing papers in year 2006, one of which cites P twice within the same paper while the other cites it only once.
For a citation edge from paper Q to paper P , citeWords denotes the number of words in the citation context(s), where P has been referred to. Since more than one paper might be cited within the same citation context, number of words are divided among all the cited papers. Similar to countX, a high value of citeWords implies that paper P has been discussed in more details by paper Q and therefore, paper P might be relevant for paper Q . Dividing by the number of papers cited within the citation context takes care of the fact that the words in citation contexts have been used to describe multiple papers. Similar to countX, our hypothesis is that finding the average number of words that other papers use to describe P could be indicative of the importance of paper P .

Let us assume that paper P is cited by another paper Q i in m different citation contexts, S 1 ,...,S m . For this citation edge, cite-Words is computed as where AW ( S i ,P,Q i ) denotes the average number of words used in sentence S i to describe P . In general, if k  X  1 papers are cited within the sentence S i , the average words for each of these k papers (including P ) is given by: where Len ( S ) denotes the length of a sentence S and is simply computed by counting the number of words appearing in it. Now, assume that the papers Q 1 ,Q 2 ,...,Q n are citing paper P in the t th year after publication of P . We define the average citeWords metric for paper P for the t th year as: Average citeWords ( P,t ) =
Using Table 2, average citeWords value for paper P for the third year after publication (year 2008) can be calculated as: ( citeWords ( P, 6413388) + citeWords ( P, 5052733)) / 2
To compute citeWords ( P, 5052733) , we see that paper 5052733 cites P in one citation context when a total of two papers are cited. Thus citeWords ( P, 5052733) = 11 2 = 5 . 5 , where 11 is the length of the citation context.

Similarly, paper 6413388 cites paper P twice but in both the citation contexts, two papers are cited. Therefore,
Thus, Average citeWords ( P, 3) = 20 . 5+5 . 5 2 = 13 . We investigate whether the average countX and average cite-Words values over the years are correlated with the number of ci-tations a paper receives. We reiterate that both average countX and average citeWords are normalized with respect to the number of ci-tations received by the paper. We divide the set of papers in our dataset into 6 buckets based on the following criterion on the num-ber of citations.
 Bucket 1: Top 0.1% papers  X  citations 389-7859 Bucket 2: Top 0.1 -1% papers  X  citations 95-389 Bucket 3: Top 1 -5% papers  X  citations 29-95 Bucket 4: Top 5 -10% papers  X  citations 16-29 Bucket 5: Top 10 -25% papers  X  citations 6-16 Bucket 6: Rest of the papers  X  citations 0-6
For each of the Citation buckets, we plot the temporal profile for the average countX values, averaged for all the papers within that bucket, in Figure 2. The X  X  axis denotes the year after publication for the paper, ranging from 0 (same year as publication) to 10 ( 10 year after publication). While averaging for a citation bucket for a particular year, we consider only those papers which have non-zero citations in that year. Minimum value of countX can be 1 for any citation edge. Interestingly, as per our hypothesis, various citation ranges show differences in terms of the average countX values. Some important observations from Figure 2 are: 1. There is an increase in value of countX in initial years irre-2. Highly cited papers are cited more number of times in a sin-
We clearly see a correlation between the number of citations and the average countX profiles of the papers. Further, we investigate whether the countX values can discriminate between the 6 citation categories identified in [7]. Accordingly, we divided the set of pa-pers into 6 categories mentioned in [7]. For readability, the six categories are described below: (i) PeakInit: Papers whose citation count peaks within 5 years of publication followed by an exponential decay. (ii) PeakMul: Papers having multiple peaks in different time periods of the citation history. (iii) PeakLate: Papers having very few citations at the begin-ning and then a single peak after at least 5 years of the publication followed by an exponential decay in citation count. (iv) MonDec: Papers whose citation count peaks in the immedi-ate next year of the publication followed by a monotonic decrease in the number of citations. (v) MonIncr: Papers having a monotonic increase in the number of citations from the very beginning of the year of publication till the date of observation. (vi) Oth: Papers not belonging to any of the above mentioned categories belong to this category.

Figure 3 presents the temporal profile of average countX values for each of these 6 categories. Again, we can see that the average countX values are the highest for the MonIncr and PeakLate categories, which have been identified as having the categories cor-responding to high number of citations in [7]. Similarly, average countX values are the lowest for the MonDec and Others , which have been identified as the categories corresponding to the low number of citations (see [7] for details).
 Figure 2: Average countX: temporal profiles for six citation buckets over the publication age
We now plot the temporal profile for the average citeWords val-ues for the six citation buckets in Figure 4. Similar to average countX, while averaging for a citation bucket for a particular year, we consider only those papers which have a non-zero citation in that year. Average citeWords also shows a very similar trend as that seen with the average countX values, an initial increase and then a decreasing trend over the years. Interestingly, differences are ob-served between various citation ranges with the papers having the highest citations also earning a high number of average citeWords over the years.

We further use six citation categories to plot the temporal profiles in Figure 5. The trends are again very similar to those observed for the case of average countX values, with the MonIncr and Peak-Late categories having a higher value of average citeWords than the other categories and MonDec category having the lowest values. of citing paper. Bold face text represents cited paper reference. Figur e 3: Average countX: temporal profiles for the six citation categories [7] over the publication age
To motivate the importance of average countX and citeWords as features for future citation prediction, Table 3 shows some specific examples of papers having the same citation count in the first two years after publication but different average countX and citeWords values. What we observe is that in both the cases, among the papers having the same citation count, the paper having a high countX (and citeWords) value in the initial two years receives a much higher citation count in the future. Thus, the average countX feature from Figur e 4: Average citeWords: temporal profiles for the six cita-tion buckets over the first 10 years of publication age the initial years of publication can serve as an important feature towards predicting future citations.
 Table 3: Example paper-pairs having a similar citation count in the initial 2 years of publication but different countX values. Figure 5: Average citeWords: temporal profiles for the six cita-tion categories [7] over the first 10 years of publication age Figure 6: Correlating citation count and countX buckets. (a) Correlation at 5 years after publication; (b) Correlation at 9 years after publication Figure 7: Correlating citation count and citeWords buckets. (a) Correlation at 5 years after publication; (b) Correlation at 9 years after publication We further study whether the average countX and average cite-Words values from the initial two years after publication can serve as discriminating features to predict citations at a later point of time. We, therefore, divide all the papers in 3 ranges as per the average countX values ( { 1 } , (1 , 1 . 5] and (1 . 5 ,  X  ) ) and as per the tial two years of publication. We call these ranges as low, medium and high respectively. We now take citation counts of the papers for the time points, corresponding to 5 and 9 years after publication. For each such time point, we create 6 different citation buckets (top 0.1% etc.) and plot the distribution of the papers falling into these 6 citation buckets on various countX and citeWords ranges. For example, 5 years after publication, 75% of the papers in the lowest citation category have an average countX value=1 (see Figure 6(a)). On the other hand, more than 75% of the papers in the top two cat-egories (top 0.1% and top 0.1-1%) have a countX value  X  1 . 5 . The trend becomes much more prominent for 9 years after publication (Figure 6(b)), with the probability of a paper having countX  X  1 . 5 increasing with increasing citation counts.

Very similar trends are observed for the average citeWords as well (see Figure 7). From these figures as well as examples in Ta-ble 3, it is clear that information from average countX and average citeWords in the initial years of publication acts as a discriminating factor for the future citation counts, such that most of the highly cited papers have high values of average countX and citeWords in the initial years, which is not true for the low-cited papers.
Motivated by these examples, we now use these citation context features for the task of future citation prediction. The model is described in the next section.
We extend the two-stage stratified learning framework proposed in [6] with the addition of three features. In the first stage, a query paper is classified into one of the citation profile category using Support Vector Machine (SVM) learning model. Further, for each category, a Support Vector Regression (SVR) model is learned for predicting citation counts. Thus, given a query paper, we first clas-sify it into one of the six citation profile categories. Post classifi-cation, category based SVR is used to predict citation count. Our citation prediction model uses features at the time of publication, along with the citation information from the first 2 years after pub-lication. Features from the time of publication are the same as re-ported in [6]. These features can be divided into three categories: features based on the paper content, features based on author infor-mation and features based on venue information. For the sake of completeness, we describe these features in brief below. For more details, the reader is requested to look into [6].
We used five paper-centric features as proposed in [6]. Last three among these are entropy based features. (a) Team-size (Team): The number of authors in a paper. (b) Reference count (RefCount): The number of references men-tioned in the reference section of a paper. (c) Reference diversity (RDI): RDI measures the diversity in the fields of the referred papers. A paper citing papers of various fields has a high value of RDI. (d) Keyword diversity (KDI): Keyword diversity refers to diver-sity in the keywords mentioned in the paper. (e) Topic diversity (Topic): Each paper is assigned a set of proba-ble topics inferred from LDA. Topic diversity gives a diversity over these probable topics.
The author of a publication plays an important role in its pop-ularity. The following four author-centric features were used for citation prediction. (a) Author h-index (HIndex): H-index is a standard measure for author productivity and impact. This feature measures average h-index of the authors at the time of publication. (b) Author productivity (ProAuth): Author productivity refers to the count of his publications. A more productive author will pro-duce more. The feature is an average of the productivity of the all the co-authors of a paper. (c) Author diversity (AuthDiv): Author diversity refers to the di-versity in the research fields of author publications. A highly di-verse author will publish in different domains. The feature is an average of all the authors taken together. (d) Sociality of author (NOCA): This feature counts the number of co-authors in all the publications of each author present in the paper.
We also used certain features based on the prestige as well as the diversity of the venue, where the paper has been published. These features are described in detail below. (a) Short term venue prestige (VenPresS): Short term venue pres-tige measures the average number of citations for the papers pub-lished in a venue during the two preceding years. (b) Long term venue prestige (VenPresL): Long term venue pres-tige measures the average number of citations for the papers pub-lished in a venue so far. (c) Venue diversity (VenDiv): This feature measures the diversity in the research fields of the papers published in a venue.
In addition to these features, we also utilize the two features de-rived from the citation context, the average countX and average citeWords for the first 2 years after publication, as well citation count received after the first 2 years of publication.

In the next section, we report the experiments using our citation prediction model.
We perform experiments using the stratified learning framework for citation prediction. We selected papers having at least 10 years of history and published in between 1970 -2005. We divided this dataset into training and testing sets. For training, we consider pa-pers published in between 1970 -2000. For testing purpose, we took the range as 2001 -2005. First of all, we learn a stage-I classi-fication model using our training dataset. We also learn separate re-gression models for each citation category, for each time point, for which the citation count is to be predicted. Given a query paper, first the classification model is used to assign a citation category (stratum) to it (stage I). In stage II, a regression model trained on the assigned category is used for citation count prediction for the specified time periods. We use all the features described in section 5. We have used three different time points  X  t = 5 , 7 , and 9 for prediction.

We evaluate our model on two baselines. The first baseline [19] is similar to our model except that it does not include the classifi-cation stage. Thus, all the features are directly used in a regression model for citation prediction. We use Chakraborty et al. [6] as the second baseline. While the authors conducted experiments both with and without the initial year of publication information, we use the citation count of first two years for their method for a fair com-parison. Thus, this baseline is very similar to our model with the only difference being that we use two citation context features iden-tified in this paper, average countX and average citeWords, for the t th year after publication, with t = 0 , 1 , 2 .
We use the following three metrics for evaluating our results. 6.1.1 Coefficient of determination ( R 2 )
Coefficient of determination ( R 2 ) [4] is a number that indicates how well data fit a statistical model of future outcome prediction. It measures the variability introduced by the statistical model. It is defined as the proportionate reduction in uncertainty, measured by Kullback-Leibler divergence, due to the inclusion of regressors. Let d be the document in test document set D T , we calculate R as:
Here, C T ccp ( d ) denotes the predicted citation count for docu-ment d . C T ( D T ) denotes the mean of observed citation counts for documents in D T . ( C T ( d ) denotes actual citation count for doc-ument d . R 2 values ranges from 0 to 1 . A larger value indicates better performance.
Pearson correlation co-efficient (  X  ) [12] measures the degree of linear dependence between two variables. It is defined as the co-variance of the two variables divided by the product of their stan-dard deviations. Here, cov(X,Y) denotes covariance between X and Y,  X  X and  X  denote standard deviation values for X and Y respectively. Sim-ilarly,  X  X and  X  Y denote mean values for variables X and Y re-spectively. E represents the expected value.  X  ranges from -1 to 1, where  X  = 1 corresponds to a total positive correlation, 0 cor-responds to no correlation, and  X  1 corresponds to total negative correlation. A larger value indicates better performance.
Mean square error (  X  ) measures the expected value of the squared error loss in estimation. It is a risk function corresponding to the expected value of the squared error loss. For n number of observa-tions, we define mean squared error as: Here,  X  Y and Y denote the vectors of predicted and actual values respectively. A smaller value indicates better performance.
Next, we compare the performance of the two baselines with our model. We also present performance statistics for stage I (classifi-cation) and stage II (prediction). Along with performance analysis, we compare categories and analyze results.

Table 4 compares the performance of these baselines with our model. Columns 2-4 in Table 4 show the predictive performance for baseline I using three metrics, while columns 5-7 show the pre-dictive performance of baseline II. Columns 8-10 show the perfor-mance of our model.

We observe that for all the three systems, performance decreases with the increase in time period for prediction, with the best perfor-mance achieved for  X  t = 5 . While baseline I performs the worst among the three models, the R 2 value of 0 . 56 obtained for  X  t = 5 is in itself significantly better that some previous works. For ex-ample, Kulkarni et al. [11] achieved an R 2 value of 0.2 using 328 medical articles. Baseline II performs better than baseline I for all of the three time-periods. This performance improvement can be credited to the stratified learning approach used in baseline II, as was established in [6]. Our model performs better than both the baselines for all the three time-periods. While the improvements over the first baseline are almost over 50% in terms of R provement of the order of 8-10% are achieved over baseline II as well. Improvement in terms of  X  are of the order of 20-25% over the baseline II. Since the only difference between baseline II and our model are the average countX and average citeWords features identified in this paper, this improvement can be credited to the use of initial year information from the citation context of the paper. Table 4: Performance comparison between Baseline I, Baseline II, and our model. Three evaluation metrics  X   X  , R 2 and  X  are used. A low value of  X  and high values of R 2 and  X  represent an efficient model. Prediction is made over three time periods  X   X  t = 5 ,  X  t = 7 and  X  t = 9 .

Since we use the six categories as strata, we further analyze the prediction results for each of these categories. Table 5 presents category-wise performance metrics (except the category Oth ) val-ues for the three time-periods. Figure 8 gives the scatter plots for each category for the prediction task for the three time periods. X  X  axis denotes the actual citation count, while the Y  X  axis de-notes the predicted citation count.

From Table 5, we observe that for  X  t = 5 , the performance is the best for the PeakLate category on all the three metrics. Figure 8 also confirms this observation with most of the points densely ac-cumulated around x = y line. For  X  t = 7 , PeakLate performs the best on  X  , while MonDec and MonIncr perform well on R 2 and  X  respectively. For  X  t = 9 , MonIncr performs the best among all the categories for all the three evaluation metrics. Overall, Peak-Late and MonIncr categories perform the best. This is very crucial for the citation prediction model, as these categories correspond to the highly cited papers [6].

From Figure 8, we observe that for  X  t = 5 , all categories show roughly the same pattern. Majority of the papers lie below the line, which denotes that in the initial years after publication our model slightly under-estimates the citation counts. The only cases of over-estimation are for the PeakMul category,  X  t = 9 (majority papers above the line) and for the MonIncr category for  X  t = 7 .
The first stage SVM model classifies each paper into one of the six categories. Table 6 presents the confusion matrix of SVM clas-sification. Each entry in the first column represents a ground truth category of the paper. Similarly, each entry in the first row rep-resents predicted category. We observe that around 50% of Oth category paper are wrongly classified into PeakMul . While Mon-Dec has the highest accuracy (0.989), more than 29% PeakInit are classified into MonDec , which in turn decreases the accuracy for PeakInit category. As our dataset is highly biased towards Oth cate-gory (highest % of papers), SVM overestimates Oth category in the classification. Classification inaccuracy in the first stage decreases prediction accuracy in the second stage, with the Oth category play-ing a significant role in lowering the precision.
Table 7 presents one best representative paper from each of the five categories. For each paper, we calculate the absolute differ-ence between actual citation count and predicted citation count for our model, baseline I and baseline II for three time periods. As observed from Table 7, our model is closest to the actual values in terms of citations at any time instance.
We now study as to how various features correlate with the actual citation counts. Accordingly, we divide our features into 6 different sets and compute Spearman X  X  correlation for the three time-periods in Table 8. We can see from the table that the last three features, namely average countX, average citeWords and 2-year citations, show a much higher correlation than the other three feature sets. While the correlation for 2-year citation feature is slightly higher than average countX for  X  t = 5 , correlation is the highest for av-erage countX for  X  t = 9 . Thus, average countX serves as the most important feature for predicting the long term citation behavior of the papers.
 Table 8: Average Spearman X  X  rank correlation of each feature category (column 1) with the actual citation count without cat-egorization for  X  t=5,7 and 9 years after publication
The experimental results clearly confirm that the proposed method for citation prediction outperforms the other baselines for various time-periods. Further, we wanted to put this work in perspective of the previous related works for this problem. Table 9 lists five other works and compares them for the size of the dataset used for the study, year-ranges of the test papers, method used by the papers, as well a time period for which the R 2 values have been reported. Our dataset size is comparable to the other datasets reported in the lit-erature. Also, we achieve a better R 2 value on this massive dataset than the ones reported earlier in the literature. Our prediction time period (  X  t = 9 ) is the maximum among all these works.
In this paper, we have used a massive dataset of citation contexts to show that the features extracted from the citation contexts of the papers, in the immediate years after publication, play a vital role for the task of future citation prediction. We introduced two new features, average countX and average citeWords, and feature analy-sis showed that these citation context features are highly correlated with the actual citation counts, specifically for the long-range cita-tion prediction.

For the citation prediction task, we used a stratified learning framework, similar to [6]. Experimental results confirm that in-cluding the citation context features significantly improves the ac-curacy over [6] under various experimental settings for different evaluation metrics.

In future, we plan to extend this work along many different di-mensions. First, we only used two features from the citation context in this work. More features based on the textual analysis of citation contexts can be investigated. Also, we would like to investigate the classifier further to reduce the errors due to the classification stage. In addition, we plan to make the citation context dataset publicly available with additional insights and properties.  X   X  R 2  X   X  R 2  X   X  represents x = y line passing through origin.
 both the highest and lowest accuracy values [1] B. Aljaber, N. Stokes, J. Bailey, and J. Pei. Document [2] L. Bornmann and H.-D. Daniel. What do citation counts [3] T. Brody, S. Harnad, and L. Carr. Earlier web usage statistics [4] A. C. Cameron and F. A. Windmeijer. An r-squared measure [5] C. Castillo, D. Donato, and A. Gionis. Estimating number of [6] T. Chakraborty, S. Kumar, P. Goyal, N. Ganguly, and lists the method/model used for prediction and column 6 presents the R 2 values reported in the paper for a time period, comparable across different methods. Papers are arranged in the increasing order of R [7] T. Chakraborty, S. Kumar, P. Goyal, N. Ganguly, and [8] D. G. Feitelson and U. Yovel. Predictive ranking of computer [9] L. D. Fu and C. Aliferis. Models for predicting and [10] L. Getoor. Link mining: a new data mining challenge. ACM [11] A. V. Kulkarni, J. W. Busse, and I. Shams. Characteristics [12] J. Lee Rodgers and W. A. Nicewander. Thirteen ways to look [13] L. Li and H. Tong. The child is father of the man: Foresee [14] A. Livne, E. Adar, J. Teevan, and S. Dumais. Predicting [15] C. Lokker, K. McKibbon, R. J. McKinlay, N. L. Wilczynski, [16] C. M, W. RL, and W. E. Journal prestige, publication bias, [17] N. Pobiedina and R. Ichise. Predicting citation counts for [18] D. Wang, C. Song, and A.-L. Barab X   X asi. Quantifying [19] R. Yan, C. Huang, J. Tang, Y. Zhang, and X. Li. To better [20] R. Yan, J. Tang, X. Liu, D. Shan, and X. Li. Citation count [21] X. Yu, Q. Gu, M. Zhou, and J. Han. Citation prediction in
