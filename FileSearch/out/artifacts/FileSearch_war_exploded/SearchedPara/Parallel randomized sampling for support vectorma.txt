 Yumao Lu  X  Vwani Roychowdhury Abstract A parallel randomized support vector machine (PRSVM) and a parallel randomized support vector regression (PRSVR) algorithm based on a randomized sampling technique are proposed in this paper. The proposed PRSVM and PRSVR have four major advantages over previous methods. (1) We prove that the proposed algorithms achieve an average convergence rate that is so far the fastest bounded convergence rate, among all SVM decomposition training algorithms to the best of our knowledge. The fast average convergence bound is achieved by a unique priority based sampling mechanism. (2) Unlike previous work (Provably fast training algori-thm for support vector machines, 2001) the proposed algorithms work for general linear-nonseparable SVM and general non-linear SVR problems. This improvement is achieved by modeling new LP-type problems based on Karush X  X uhn X  X ucker opti-mality conditions. (3) The proposed algorithms are the first parallel version of ran-domized sampling algorithms for SVM and SVR. Both the analytical convergence bound and the numerical results in a real application show that the proposed algo-rithm has good scalability. (4) We present demonstrations of the algorithms based on both synthetic data and data obtained from a real word application. Performance comparisons with SVM light show that the proposed algorithms may be efficiently implemented.
 Keywords Randomized sampling  X  Support vector machine  X  Support vector regression  X  Parallel algorithm 1 Introduction The support vector machine (SVM) has recently become one of the most popular methods for classification and regression in machine learning. The underlying train-ing problem can be formulated as a quadratic programming (QP) problem that can are only able to solve some small and medium size problems since they require the loading of the whole Gram matrix.

The first decomposition algorithm, that works on a smallsubset of training vectors in each iteration was first proposed by Osuna [ 24 ]. Several improved versions of decom-position algorithms, including the Chunking algorithm [ 23 ], sequential minimization optimization (SMO) algorithm [ 25 ], successive over-relaxation (SOR) algorithm [ 22 ] training problems. The lack of a theoretical convergence bound of the decomposition algorithms for general SVM training problems, however, compromises the potential applicability of such a powerful tool set, especially for many real-time and on-line applications, where a convergence bound becomes critical [ 7 ]. This paper proposes a randomized sampling algorithm that has a provably fast convergence bound for general SVM and SVR training problems.

Sampling theory has a long successful history in optimization [ 1 , 9 ]. The applica-tion to the SVM training problem was first proposed by Balcazar et al. [ 2 ]. However, Balcazar assumed that the SVM training problem is a separable problem or a problem that can be transformed to an equivalent separable problem by assuming an arbitrarily They also stated that there were number of implementation difficulties so that no relevant experimental results could be provided [ 3 ].

We model new LP-type problems for SVM and SVR, respectively, such that the general linear nonseparable problem can be covered by our randomized support vec-tor machine (RSVM) algorithm and the general nonlinear regression problem can be covered by our randomized support vector regression (RSVR) algorithm. In the LP-type problems, Karush X  X uhn X  X ucker (KKT) optimality conditions are used as criteria to identify violators and extremes. The advantage of using KKT conditions KKT conditions will be always satisfied when a global optimum is achieved, while classification errors may not vanish in linear nonseparable problems, even when a global optimum has been achieved.

Parallel learning is necessary if a centralized system is infeasible because of geo-graphical, physical and computational reasons. It has been noted that scalable parallel algorithms are important for a machine learning algorithm to be successfully applied distributed computing facilities, we proposed a novel parallel randomized SVM/SVR, in which multiple working sets can be worked on simultaneously. To the best of our knowledge, it is the very first attempt in extending randomized sampling technique to a parallel algorithm for SVM/SVR training problems.

The basic idea of the algorithm is to randomly shuffle the training vectors among a network based on a carefully designed priority and weighting mechanism, and to solve the multiple local problems simultaneously. Unlike the previous works on parallel SVM [ 11 , 18 ] that lacks a convergence bound, our algorithms on average, converges to where  X  denotes the underlying combinatorial dimension, N denotes the total num-ber of training vector, C denotes the number of working sites, and r denotes the size of each working set. Since the RSVM/RSVR is a special case of PRSVM/PRSVR, our proof naturally works for the RSVM/RSVR. Note that, when C = 1, our result reduces to Balcazar X  X  bound [ 3 ].

This paper is organized as follows. The support vector machine and support vector regression problems are introduced and formulated in Sect. 2 , followed by our LP-type modeling in Sect. 3 . We present the parallel randomized support vector machine and support vector regression (PRSVM/PRSVR) algorithms in Sect. 4 . The theoretical global convergence bound is given in Sect. 5 , followed by a presentation of demon-strations on synthetic data and data from a geographic information system application in Sect. 6 . We discuss some interesting issues in Sect. 7 and conclude this paper in Sect. 8 . 2 Support vector machine and support vector regression We introduce fundamentals and basic notations on SVM and SVR in this section. 2.1 Support vector machine Let us first consider a simple linear separation problem. We are seeking a hyperplane to separate a set of positively and negatively labeled training data. The hyperplane is for i = 1, ... , N where x i  X  R m is a training data point and y i  X  X + 1,  X  1 } denotes the class of the vector x i . The margin is defined by the distance of the two parallel the generalization of the classifier [ 28 ]. The support vector machine (SVM) training problem is in fact a quadratic programming problem, which maximizes the margin over the parameters of the linear classifier. For general nonseparable problems, a set of slack variables  X  i , i = 1, ... , N are introduced. The SVM training problem problem is defined as follows: where the scalar  X  is usually empirically selected to reduce the testing error rate. To simplify notations, we define v i = ( x i ,  X  1 ) ,  X  = ( w , b ) ,andamatrix Z as The dual of problem ( 1 )isshownasfollows:
A nonlinear kernel function can be used for nonlinear separation of the training data. In that case, the gram matrix ZZ T is replaced by a kernel matrix K  X  R N  X  N . Our PRSVM that is described in the Sect. 4 can be kernelized and therefore is able to keep the full advantages of the SVM. 2.2 Support vector regression Support vector regression is a robust function estimation method. The basic idea is to minimize a pre-defined risk function over the parameters of the regressor.
We, again, start from the estimation of a linear function based on independent and identically distributed (IID) training samples where x i  X  R m , y i  X  R , w  X  R m , b  X  R and f : R m  X  R .

To preserve the sparse property of the solution, Vapnik used the -insensitive loss function which does not penalize errors below the tolerance [ 28 ].

To minimize a regularized -insensitive loss function, we have where  X   X  R N and  X   X   X  R N are the allowed error  X  X bove X  and  X  X elow X  the margin to balance these two terms.
 Recall that v i = ( x i ,  X  1 ) ,  X  = ( w , b ) ,andamatrix Z as The corresponding dual formulation has the form vectors x i such that the corresponding optimal dual variable For convenience, we define vector  X   X  R N such that
Similarly, a nonlinear kernel function can also be used for general nonlinear regres-sion. In that case, the gram matrix ZZ T is replaced by a kernel matrix K  X  R N  X  N .Our PRSVR (a unified algorithm for regression problems with PRSVM) that is described in the Sect. 4 works for general kernelized nonlinear support vector regression prob-lems.
 3 Randomized sampling We model two LP-type problems for support vector machines and support regression respectively in this section. The modeling procedure helps in understanding the main algorithm proposed in the following section. 3.1 The sampling lemma and LP-type problem Before presenting our LP-type models, we introduce the fundamentals and notations in randomized sampling theory.

Let X be the set of training vectors. That is, each element of X is a row vector of the matrix X . Throughout this paper, we use CALLIGRAPHIC style letters to denote An abstract problem is denoted by ( X ,  X  ). Here,  X  is a mapping from a given subset X
R of X to the solution of problem ( 1 ) with constraints corresponding to X R and X is of size N . Such a problem, where the constraints correspond to only a subset of all the training vectors will be referred to as a local problem from now on. In our approach, we will divide the training vectors into multiple subsets (with repetitions, if needed) or working sets, and SVM/SVR solutions will be found for each local problem. Define
The elements of V ( X R ) are called violators of X R and the elements of E ( X R ) are called extremes in X R . By definition, we have For a random sample X R of size r , we consider the expected values Gartner proved the following sampling lemma [ 17 ]: Lemma 3.1 (Sampling Lemma). For 0  X  r &lt; N, Proof By definitions, we have where [ . ] is the indicator variable for the event in brackets and the last row follows the fact that the set Q has r + 1 elements. The Lemma immediately follows. The problem ( X ,  X  ) is said to be a LP-type problem if  X  is monotone and local (see lems. We use the same definitions given by [ 17 ] to define the basis and combinatorial dimension as follows. For any X R  X  X ,a basis of X R is an inclusion-minimal subset B  X  X R with  X ( B ) =  X ( X R ) .The combinatorial dimension of ( X ,  X  ), denoted by  X  , is the size of a largest basis of X . For a LP-type problem ( X ,  X  ) with combinatorial dimension  X  , the sampling lemma yields This follows from the fact that | E ( X R ) | X   X  . 3.2 LP-type problem modeling for SVM Now, we are ready to relate the definitions of the extremes, violators and the basis to our general SVM training problem ( 1 )or( 2 ). For any local solution  X  p or  X  p of solutions will be the vectors that violate the Karush X  X uhn X  X ucker (KKT) necessary and sufficient optimality conditions. The KKT conditions for the problem ( 1 )and( 2 ) are listed as follows: condition needed to be tested is or The size of the largest basis,  X  is naturally the largest number of support vectors for all subproblems ( X p ,  X  ), X p  X  X . For separable problems,  X  is bounded by one plus the lifted dimension, i.e. ,  X   X  n + 1. For general nonseparable problems, we do not know the bound for  X  before we actually solve the problem. What we can do is to set a sufficiently large number to bound  X  from above. 3.3 LP-type problem modeling for SVR The similar technique can be applied to SVR training problems. For any local solution the local solutions will be the vectors that violate the KKT necessary and sufficient optimality conditions. The KKT conditions for the problem ( 4 )and( 5 ) are listed as follows: where ( . ) i denotes the i -th component of the vector in bracket.
 condition needed to be tested is or The size of the largest basis,  X  is naturally the largest number of support vectors for all subproblems ( X p ,  X  ), X p  X  X . 4 Algorithm We consider the following problem: the training data are distributed in C + 1sites, where there are C working sets and 1 nonworking set. Each working site is assigned a priority number p = 1, 2, ... , C . We also assume that each working site contains r training vectors, where r  X  6  X  2 and  X  denotes the combinatorial dimension of the underlying SVM/SVR problem.
 Define a function u ( . ) to record the number of copies of elements of a training set. For training set X ,wedefineaset W such that W contains the virtually duplicated copies of the training vectors. We have | W |= u ( X ) .Wealsodefinethevirtualset W p corresponding to training set X p at site p .

Our parallel randomized support vector machine/regression (PRSVM/PRSVR) algorithm works as follows.
 Initialization Training vectors X are randomly distributed to C + 1 sites. Assign priorities to all u ( X ) = N . We have | X p |=| W p | for all p .Set t = 0.
 Iteration Each iteration consists of the following steps.

Repeat for t = 1, 2, ... 1. Randomly distribute the training vectors over the working sites according to 2. Each site with priority p , p  X  C solves the local partial problem and record the until  X  q = p V q , p = X  for some p .
 Return the solution  X  p .

The priority setting of working sets actually defines the order of sampling. The highest priority server gets the first batch of sampled data, lower one gets the second batch and so on. This kind of sequential behavior is designed to help defining violators and extremes clearly under a multiple working site configuration.
 that we are solving, while we record this number of copies as a weight of this training vector.
 The merging procedure has two properties: Property 4.1 A training vector that is not in working set X p must not be a violator of X .Thatis,x i /  X  V ( X p ) ,ifx i  X  X p .
 Property 4.2 If multiple copies of a vector x i are sampled to a working set X p , none u ( { x i } )&gt; 1 at site p.

The above two properties follow immediately by definitions of violators and ex-tremes.

One may note that the merging procedure actually constructs an abstract problem ( of ( X p ,  X  )is V p , the number of violators of ( W p ,  X  )is u ( V p ) .
Step 4 plays the key role in this algorithm. It says that if the number of violators of the LP-type problem ( W p ,  X  ) is not too large, we double the weights of the violators of ( W p ,  X  ) in all sites. Otherwise, we keep the weights untouched since the violators already have enough weights to be sampled to a working site.

One may note when C = 1, the PRSVM/PRSVR is reduced to the RSVM. How-ever, our RSVM is different from the randomized support vector machine training algorithm in [ 2 ] in several ways. First, our RSVM is capable of solving general non-separable problems, while Balcazar X  X  method has to transfer a nonseparable problem to an equivalent separable problem by assuming an arbitrarily small  X  . Second, our RSVM merges examples after sampling them. Duplicated examples are not allowed in the optimization steps. Third, we test the KKT conditions to identify a violator instead of identifying a misclassified point. In our RSVM, a correctly classified example may also be a violator if this example violates the KKT condition.
 5 Proof of the average convergence rate We prove the average number of iterations executed in our algorithm, PRSVM/ case of our PRSVM.
 Theorem 5.1 For general SVM training problem the average number of iterations exe-cuted in the PRSVM/PRSVR algorithm is bounded by ( 6  X / C ) ln ( N + 6 r ( C  X  1 ) X ) . Proof We consider an update to be successful if the if-condition in the step 4 holds in an iteration. One iteration has C updates, successful or not.

We first show a bound on the number of successful updates. Let V p denote the set of violators from site with priority q  X  p for the solution  X  p . By this definition, we have Since the if-condition holds, we have By noting that the total number of training vectors including duplicated ones in each working sites is always r for any iterations, we have and Therefore, at each successful update, we have updates, we have
Let X 0 be the set of support vectors of the original problem ( 1 )or( 2 ). At each updates. That is, after k successful updates, u ( { x i } )  X  2 k / X  .

Therefore, we have By simple algebra, we have updates.

The rest is to prove that the probability of a successful update is higher than one half. By sampling lemma, the bound ( 6 ), we have By Markov equality, we have This implies that the expected number of updates is at most twice as large as the total number of updates. Note that, at the end of each iteration, we have Therefore, the PRSVM/PRSVR algorithm guarantees, on average, within ( 6  X / C ) ln ( N + 6 r ( C  X  1 ) X ) steps, that all the support vectors are contained by one of the C working sites. For separable problems, we have  X   X  n + 1. For general nonseparable problems, we have  X  is bounded by the number of support vectors.
The bound of average convergence rate ( 6  X / C ) ln ( N + 6 r ( C  X  1 ) X ) clearly shows very limited. 6 Simulations and applications We analyze our PRSVM and PRSVR by using synthetic data and a real-world geo-graphic information system (GIS) database.

Throughout this section, the machine we used has a Pentium IV 2.26G CPU and 512M RAM. The operating system is Windows XP. The SVM light [ 20 ] version 6.01 was used as the local SVM solver. Parallel computing is virtually simulated in a single machine. Therefore, we ignore any communication overhead.
 6.1 Demonstrations using synthetic data We demonstrate our RSVM and RSVR (reduced PRSVM and PRSVR when C = 1) in two experiments with synthesized training data.

In one experiment, a total number of 1,000 two-dimensional training vectors are generated. This data set consists of 500 positive and 500 negative labeled vectors. Each class is generated from an independent Gaussian distribution with added random Gaussian noise.
 We set the sample size r to be 100 and the regularization factor  X  to be 0.2. The RSVM converges in 13 iterations. In order to demonstrate the weighting procedure, weight the training sample has.

The RSVR (reduced PRSVR when C = 1) training procedure is demonstrated by using a synthesized one-dimensional training data set. This data set consists of 5,000 data points generated by a sigmoid function with 20 % additive Gaussian noise.
In this experiment, we choose the sample size r to be 2,000, the tube width to be 0.2 and the regularization factor  X  to be 0.2. A unit-variance Gaussian kernel is selected for fitting the sigmoid curve. The RSVR algorithm converges in 11 iterations. To demonstrate the weighting procedure, we choose three iterations (iteration 0, iter-ation 5 and iteration 11) and plot the weights of the training vectors in Fig. 2 .Inthis figure, vectors with higher weights are plotted with darker and larger markers.
Figs. 1 and 2 show how those  X  X mportant X  points stand out and get higher and higher probability to be sampled in the training process of randomized support vector machine and randomized support vector regression respectively. 6.2 Application on a geographic information system database We select covtype , a geographic information system database, from the UCI Repos-itory of machine learning databases as our PRSVM applications [ 6 ]. The covtype database consists of 5,81,012 instances. There are 12 measures but 54 columns of data: 10 quantitative variables, 4 binary wilderness areas and 40 binary soil type variables binary variable unchanged. We select 2,87,831 training vectors and use our PRSVM to classify class 4 against the rest.

We set the size of working size r to be 60,000, the regularization factor  X  to be 0.2. Since classification performance is not our major concern, we do not fine turn the parameter. A linear kernel is used so that the number of support vector can be limited. This application turns out to be a very suitable case for testing PRSVM since the database has huge number of training data and the number of SVs is relatively small.

We try three cases with C = 1, C = 2and C = 4 and compare the learning time with the SVM light in Table 1 . The results show that our implementation of RSVM and PRSVM achieves comparable result with the reported fastest algorithm SVM light , though they cannot beat SVM light in terms of computing speed for now. However, the lack of a theoretical convergence bound makes SVM light not always preferable.
We plot the number of violators and support vectors (extremes) in each iterations in Fig. 3 to compare the performance of different number of working sites. The results show the scalability of our method. The numerical results match the theoretical result very well. 7 Discussion In modem learning theory, a robust classifier or regressor usually comes from a sparse solution [ 13 ]. The beauty of support vector machine lies in its strength of optimally determining a subset of training vectors, the support vectors, that define the classi-fier/regressor. Since the complete problem, involving all the data points, are often too big to handle, iterative decomposition methods, where only a subset of the data points are handled at any one iteration, have been proposed. However, mathemat-ical programming sometimes lacks simplicity, and no convergence bound has been proved for such standard decomposition algorithms. On the other hand, boosting [ 15 ] and stochastic boosting [ 16 ] algorithms also seek to determine those critical training samples iteratively. These algorithms are simple, but only locally optimal. The parallel randomized support vector machine is an algorithm that combines sim-ple sampling/resampling techniques and mathematical programming together such that a global optimal solution is achieved with a simple convergence bound.
The proposed PRSVM/PRSVR algorithm takes advantage of the sparse solution tively limited numbers of support vectors. A well-defined problem, in which a robust support vectors are limited even if a Gaussian kernel is used, which is able to lift the dimension of feature vectors to infinity.

We cannot deny, however, that the condition r  X  6  X  2 may be too restrictive and may cause some inefficiency in the actual performance. In some of our experiments (not presented in this paper), the actual size of a working set r can be substantially smaller than the bound of 6  X  2 and yet the algorithm converges much faster in terms of cpu seconds. But we have not found theoretical evidence to guarantee a convergence if the condition r  X  6  X  2 does not hold. 8 Conclusions We have made four contributions in this paper. First, we propose a parallel randomized sampling algorithm that is able to solve general nonseparable SVM training problems. This is achieved by using KKT conditions as the criteria of identifying violators and extremes. Second, our algorithm supports multiple working sets that may work in parallel to take advantage of multiple computing facilities. Third, we prove that the PRSVM and PRSVR have a fast average convergence rate. Last, our numerical results show that multiple working sets have a scalable computing advantage. The provable convergence bound and scalable results hold the potential to make our algorithm the preferred on in many applications.
 Further research is going to be conducted to improve the performance of the PRSVM/PRSVR. If we could relax the condition r  X  6  X  2 , each working set then may contain less number of training samples so that the algorithm may become more effi-cient and suitable for additional applications that have insufficient training samples. References Authors Biography
