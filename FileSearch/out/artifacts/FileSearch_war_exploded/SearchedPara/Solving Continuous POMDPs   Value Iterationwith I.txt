 Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany Solving decision problems in the real world is ag-gravated by incomplete and noisy perception and uncertain knowledge about how the world behaves over time. Partially observable Markov decision pro-cesses (POMDPs) (Sondik, 1971; Kaelbling et al., 1998) provide a principled and powerful framework to model sequential decision problems under these constraints. Solving POMDPs exactly is computa-tionally intractable. Recent developments in approxi-mate solutions made discrete POMDPs applicable for many tasks. Point Based Value Iteration (PBVI) ap-proaches are efficient because they only explore a sam-pled subset of beliefs (Pineau et al., 2003). In combina-tion with exploration and pruning heuristics, higher-dimensional discrete POMDPs can be efficiently ap-proximated (Spaan &amp; Vlassis, 2005; Hoey &amp; Poupart, 2005; Kurniawati et al., 2008; Poupart et al., 2011). Most real world problems are of continuous nature, though. In this case, the belief space is not only high-but infinite-dimensional and known approaches for discrete state domains are not applicable. To solve POMDPs expected values need to be calculated. In continuous POMDPs these are defined by integrals, for which in general no closed form exists. For some prob-lems it is possible to find a suited symbolic represen-tation by hand, but this procedure is time-consuming and error-prone. In general, a good representation is not known a priori. To reduce the dimension of the belief space automatically, more efficient belief repre-sentations were proposed. In (Roy et al., 2005; Brooks et al., 2006; van den Berg et al., 2012) sufficient statis-tics, like the family of Gaussian distributions, were used. These are not able to represent multiple modes or sharp edges in the belief distribution. Both abilities are essential, e.g., for path planning: A robot should be able to pass a gap between the two modes of the dis-tribution of an obstacle X  X  position. Also, collision bor-ders need to be clearly represented. By reducing the Kullback-Leibler divergence between the belief state and its compression, more general representations can be found (Zhou et al., 2010). Under certain conditions beneficial properties, e.g. that the value function is piecewise linear and convex (PWLC), can be main-tained and exploited (Porta et al., 2006). In this case  X  -functions, which are defined on the state space, can efficiently represent the value function, which is de-fined on the belief space.
 These approaches share the idea to represent beliefs rather than policies as precisely and efficiently as pos-sible. However, it is not necessary to represent every detail, if it has no influence on the optimal policy. Re-search in MDPs showed that optimal state abstraction is more than just an efficient description of the state (Munos &amp; Moore, 2002). The same is true for belief representations in POMDPs. According to (Poupart &amp; Boutilier, 2002) a representation of the state space of a POMDP can be called lossless, if it preserves enough information to select the optimal policy. In discrete POMDPs, states with same value can be aggregated without influencing the policy (Feng &amp; Hansen, 2004). This leads to a problem specific compression of the belief space that is refined during the solving process. Unfortunately, these ideas cannot be applied to infi-nite state spaces directly. We show how they can help to solve continuous-state POMDPs. This paper presents a method to solve continuous-state POMDPs efficiently without constraining the reward, process or observation models. Inspired by (Feng &amp; Hansen, 2004), we utilize the fact that the value function comprises all problem relevant information about interrelations of states, rewards and observa-tions: States need to be distinguished iff that distinc-tion leads to a different decision. Using  X  -functions to represent the value function, these states have different  X  -values. For continuous spaces we combine this idea with a novel heuristic: In most problems, states which are close in the state space will usually lead to similar outcomes and thus have similar  X  -values. Hence, most results can be generalized over the state space without loss in the resulting policy. E.g., in path planning, collisions induce discontinuities in the  X  -function: For decision making a region in the state space with low values, because these states will end in a collision in the future, must be differentiated from the high value region, where the obstacle can be avoided. Actually, in path planning a robots precise position is mostly not important until it approaches, e.g., a narrow passage. Value functions even underly a hidden pattern in the state space that is more complex than just proximity. Our approach exploits these insights by automatically learning a low-dimensional, discrete representation of the continuous space during the process of solving. The resulting representation is not only efficient be-cause it is refined in accordance to the problem X  X  needs during the solving process. Moreover, we create it by applying machine learning algorithms. This enables the solver to reveal the hidden pattern of a POMDP and encode it in one space representation shared by all beliefs and  X  -functions. This way, results of the value calculation can be generalized even to regions which have not been explored, yet.
 The main contribution of this paper is the incorpo-ration of this learning process into a Monte Carlo (MC) Value Iteration Bellman backup. The results of the continuous backup (Sec. 2.2) are  X  -functions rep-resented by state-samples and their according value, from which the learning procedure (Sec. 2.3) extracts an efficient discrete  X  -representation (examples are shown in Fig. 1). This procedure allows to back-propagate representation refinements over time to-gether with the value. We show that the represen-tation implicitly defines a low-dimensional discrete POMDP and that the solving process thus converges under certain constraints.
 The second contribution of this paper is a general con-cept to maintain consistency for the value representa-tion when using sparse backups (see Sec. 2.4 ). Due to the nature of inductive learning, value results are over-generalized. The reason for this is the absence of bet-ter knowledge because an MC backup cannot explore every detail of the space. We formulate a criterion to reveal conflicting generalizations during the process of solving and an algorithm to resolve them.
 We embed the concept of value-directed representa-tion learning and conflict correction into a continuous PBVI algorithm. We present the theory to embed dis-crete representations with a finite number of arbitrary basis functions into continuous Value Iteration. In the implementation we choose a disjunct space represen-tation with piecewise constant beliefs and  X  -functions. As we use decision trees to represent the space parti-tioning, this choice is computationally efficient and can be refined by splitting nodes. The implementation uses exploration heuristics similar to  X  X xplore X  described in (Smith &amp; Simmons, 2004) for discrete POMDPs to se-lect which belief-point to backup next. Therefore ap-proximate upper and lower bounds of the value must be maintained for every belief.
 2.1. Preliminaries on Continuous POMDPs A POMDP models a system with only partially ob-servable states s  X  S . The process model p ( s 0 | s,a ) = p ( s t +1 | s t ,a ) is stochastic and depends on the taken ac-tion a  X  A . The knowledge about the system X  X  state is represented by the belief distribution b  X  B with b : S  X  R  X  0 and R s  X  S b ( s )d s = 1. It can be updated after observing o  X  O using the observation model In every time step the system receives a real-valued reward r : S  X  A  X  R . The goal of a POMDP solver is to find a policy  X  : B  X  A which maximizes the value V : B  X  R for an initial belief b 0 . The value is defined as the expected total sum of rewards for a belief b 0 over time t discounted by  X   X  [0 , 1) : Value Iteration is widely used to solve discrete POMDPs. It is based on the idea of dynamic pro-gramming (Bellman, 1957). Bellman backups prop-agate the value function back in time and their re-cursive application finally leads to convergence in the optimal value. (Porta et al., 2006) formalized it for continuous problems. Analogously to the discrete case proven in (Sondik, 1971), they showed that the optimal continuous-state value function is PWLC for discrete observations and actions. Hence, the optimal n -th step value function can be described as the supremum of a finite set  X  n of linear  X  -functions: with S  X  R N and  X  : S  X  R . In the discrete case the  X  -function reduces to an  X  -vector and the expectation operator  X  X  ,  X  X  to a dot-product. As the expectation op-erator is linear,  X  -function backup can be performed:  X  -functions dominating the next belief b 0 a,o , reached by choosing action a in belief b and observing o , are used: Repeatedly applying Eq. (3) ensures convergence of the continuous Value Iteration, if all equations are well-defined (Porta et al., 2006). In discrete POMDPs this is unproblematic as these equations are defined by sums over a finite space. The reason why continuous POMDPs are so much harder to solve is the necessity to compute continuous integrals over the state space. A closed form for Eq. (3) can only be found for some special problems: E.g., if all models in the POMDP are a linear combination of Gaussians, the  X  -functions can also be represented by linear combinations of Gaus-sians (Porta et al., 2006). Unfortunately, the number of components grows exponentially with every Value Iteration step. Thus, even if a closed form exists, for the sake of computational feasibility approximations are necessary. Notice that approximations can pre-vent Value Iteration from converging to the optimal result. 2.2. Monte Carlo POMDP Value Backup In general, there is no closed form for the integrals in the continuous-state value backup in Eq. (3) and the underlying Bayesian filtering problem in Eq. (4). Se-quential Monte Carlo (SMC) methods (also known as particle filters) can be applied to numerically approx-imate these equations. Their idea is to use a finite set of samples to evaluate the continuous integrals. SMC methods are a well-known technique approxi-mating arbitrary Bayesian filtering problems (Doucet et al., 2001). In contrast to, e.g., Kalman filtering, SMC methods are not limited to certain distributions (MacKay, 2003). Their capabilities come at the price of higher computational effort and suboptimal solu-tions, especially when dealing with very noisy den-sity functions. For highly non-linear and multi-modal problems though, the advantages of SMC outweigh. An example for the need of complex, non-linear mod-els in decision making is given in (Gindele et al., 2010; Brechtel et al., 2011) for the domain of road traffic. SMC methods have been incorporated into POMDP Value Iteration in (Bai et al., 2010; Porta et al., 2006) among others. In contrast to those approaches, we heavily reuse once sampled particles by reweighting them, in order to avoid the potentially high compu-tational effort of sampling from and evaluating the conditional density functions. Additionally, we want to minimize the expensive discretization of continuous beliefs and particles, which is necessary to look up val-ues in the current bound approximations. Ideally, due to the monotonicity of value backups, an upper and lower bound of the value can be maintained, both con-verging to the same optimal value. The lower bound defines the policy result and is PWLC. Hence, the  X  -backup in Eq. (10) can be used. The upper bound is used to estimate the solver X  X  error and guide the ex-ploration. As it is not convex, only the belief backup in Eq. (6) can be applied. Our implementation ex-plores the belief space analogously to (Smith &amp; Sim-mons, 2004). If a belief has been chosen, the MC value backup is run for all actions a . The set Q of samples q  X  Q is drawn proportional to the proposal joint den-sity p ( s,s 0 ,o | b,a ). Our MC backup uses it as basis for the backup operations in the belief b for a fixed action a . Q is recombined for different purposes.  X  s q ,s 0 q ,o q  X  X  X  p ( s,s 0 ,o | b,a ) = p ( o | s 0 ) p ( s The samples are obtained by drawing s q from b , then drawing s 0 q from p ( s 0 | s q ) and finally drawing o q p ( o | s 0 q ). Particles of a backup which are close in the state space are predicted and observed similarly. Thus, recombining them is computationally beneficial, espe-cially because a POMDP solver accounts for every pos-sible observation, not just the one encountered, like in Bayesian Filtering.
 Belief Value Backup The sample-tuples in Eq. (5) are first drawn proportional to b and thus the resulting observation samples o q can be directly used to calcu-late the next beliefs b 0 a,o q according to classical SMC methods. To minimize redundant computations, a set of distinct observations o  X  O b,a is used. The obser-vation probability p ( o | b,a ) equals the probability of getting to the next belief b 0 a,o and can be extracted by marginalizing the samples over s and s 0 . The be-lief b 0 a,o in Eq. (4) is approximated by the samples s 0 q weighted with w  X  p ( o | s 0 q ). The n -th value backup for the belief b results to: V a ( b )  X  where V n  X  1 ( b 0 a,o ) is obtained with the value function of the the previous iteration step n  X  1.  X  -Function Backup Repeatedly applying  X  -func-tion backups improves the lower bound and the policy represented by it. An  X  -function expresses the value an agent receives which is in state s , if he acts accord-ing to the best known policy for the belief b . For the backup in Eq. (10) thus the best  X  -functions for the next beliefs b 0 a,o are used: For recombining the samples Q , we apply importance sampling to compensate their bias: With these weights we calculate the  X  -function backup (Eq. (3)) at every sampled point s i of b . Note, that  X  can also be evaluated for s i : b ( s i ) = 0. This capa-bility is essential for the conflict resolution in Sec. 2.4. The evaluated  X  -samples  X  n b,a ( s i ) form a sparse repre-sentation of the continuous  X  -function. Every backup operation extends the set  X  n =  X  n  X  1  X   X  n b,a ( s i ): As we evaluate the continuous expectation opera-tor  X  X  ,  X  X  in Eq. (7) using the discrete representation, every continuous sample s 0 q has to be discretized only once, even if the used  X  -functions change (see Sec. 2.3). Moreover, the distributions b ( s k ), p ( s 0 k and p ( o j | s 0 k ) need to be evaluated only once. In both cases the results can be saved and reused, if a belief is iterated a second time. The recombination of weights is quite expensive, but a perfect candidate for massive parallelization, e.g., with GPUs because it is indepen-dent of the specific POMDP.
 The procedure is the same for continuous and dis-crete observations. However, while for discrete obser-vations it is likely to sample the same observation sev-eral times, it is very improbable in noisy, continuous domains. In such domains the number of different ob-servations | O b,a | can get very high and we apply an idea presented in (Hoey &amp; Poupart, 2005) for discrete POMDPs. The linearity of the backup allows to clus-ter all observations and with them beliefs b 0 that are dominated by the same  X  -function in Eq. (7) without affecting the result. 2.3. Value Representation and Refinement In order to converge, continuous MC Value Iteration needs the ability to generalize the results calculated in one belief by the MC backup to other beliefs. A naive implementation of Eq. (2) does not work in continu-ous state spaces: The MC backup in Eq. (10) approx-imates the continuous  X  -function with a finite set of samples and thus is undefined for most s  X  S . In (Bai et al., 2010) this problem is circumvented by using pol-icy graphs as an implicit  X  -function description. How-ever, this approach scales very poorly with long time horizons and is not applicable for continuous observa-tion spaces.
 In contrast to that, we propose to find a low-dimensional representation shared by all continuous beliefs and  X  -functions. The key idea is to find a rep-resentation suitable for the value rather than for the (possibly high-dimensional) beliefs as in previous ap-proaches. Note, that this line of action imposes a lossy compression of beliefs, but lossless compression of the policy. We employ inductive machine learning with the MC  X  -backup results as training data to obtain the representation. Learning aims to find the patterns generating the value functions. Thus, the discrete rep-resentation effectively abstracts from the given sam-ples and generalizes their result.
 During the solving process, a new space S d  X  N is build and continuously refined. We describe its re-lation to the continuous space S with a conditional density function p ( s d | s ). The discretized belief b then be obtained from b by marginalization: We assume that the continuous  X  -function can be rep-resented by the vector  X  d ( s d ) as a finite, linear com-bination of normalized, but otherwise arbitrary, basis functions p ( s d | s ): As shown in (Porta et al., 2006), the continuous opera-tor  X  X  ,  X  X  is linear in the continuous belief space. Hence, the value function V n a ( b ) in Eq. (6) is PWLC for dis-crete observations and actions. With above definitions Lemma 1 holds and the discrete expectation operator  X  X  ,  X  X  d , a simple and computationally inexpensive dot-product, equals the continuous expectation operator: Lemma 1. The continuous and the discrete expecta-tion operator are equal.
 Proof. Under these premises and assuming the MC  X  -backup in Eq. (10) to be exact, the  X  -function recursion is an isotonic contraction and converges. Further, if Lemma 1 holds, p ( s d | s ) defines a discrete problem dual to the continuous-state POMDP. Solving the discrete POMDP in S d , solves the original problem.
 Finding a small and thus efficient set of basis func-tions p ( s d | s ) that fulfill Eq. (12) for every  X  -function is a difficult task. Also, the error induced by the MC backup can only be minimized by using a a sufficient number of particles, but not prevented. The following subsections outline how the presented theory is im-plemented in a computationally efficient manner and discuss some details.
 Representation Implementation For our imple-mentation we choose S d to represent a partitioning of S using a mapping c m : S  X  S d . While this choice imposes limitations regarding the representation capa-bilities, it allows for an efficient implementation and refinement during the solving process to account for more details. Therefore m indicates the version of c . It is also able to represent discontinuities in the value function and resembles the idea of differentiat-ing states only if it is relevant to solve the problem. and consequently  X  ( s ) (12) :=  X  d ( c m ( s )). We allow a rep- X  -function to limit the number of partitions. The solver must always be able to evaluate  X  -functions created with arbitrary level of refinement. Implementation-wise, using arbitrary basis functions, it can be challenging to store p ( s d | s ) for all refinement versions m . We use a decision tree to implement the mapping c because it allows quick lookups and can be refined incrementally by splitting a leaf node s Further, a single decision tree can hold all versions of p ( s d | s ) as former leaves remain in the tree as nodes. The tree needs to be traversed only once for every par-ticle in a belief to lookup all  X  -functions in the lower bound and all belief-value-pairs in the upper bound (see Sec. 2.3 ), even if they were created with different versions m . It is sufficient to compute an overlapping belief representation b d, overlap containing all non-zero p m ( s d | s ) b ( s ) &gt; 0 regardless of their version m . Refinement Implementation A continuous backup following Eq. (10) results in a sample-based representation of the continuous  X  -function. This backup operations often reveal new details of the policy at new places in the state space. Thus, the discrete representation must be constantly refined to describe new  X  -functions.
 Our implementation uses piecewise constant basis functions p ( s d | s ) represented by a decision tree to de-fine Eq. (12). At the beginning of the solving process there is only the root node in the decision tree, cov-ering the whole continuous space S . Regions are split during the process of  X  -approximation, if the value of their contained samples differs. In that case the two subregions resulting from the split need to be distin-guished to describe the policy of the newly generated continuous  X  -function correctly. Otherwise no approx-imation error would be induced.
 To build and refine the decision tree a greedy, recursive procedure similar to C4.5 (Quinlan, 1993) is used. The classical C4.5 is only formulated for discrete spaces. We extended it with the ability to create oblique splits in the continuous space, by sampling several candi-dates and choosing the one that maximizes informa-tion gain. The precise choice of the learning algorithm is not crucial to the concept of Value Iteration pre-sented in this paper. As the sampled  X  -function values do not suffer from significant noise, overfitting can be neglected in this context.
 Algorithm 1 Value Iteration with Conflict Resolution Require: lower bound  X  LB , space representation p 1: function Value Iteration step ( b ) 3: if V ( b ) &lt;  X  C,d ( b d ) then . check (14) 4: backup and resolve ( b C ) 5: end if 6: end for 7: backup and resolve ( b ) 9: end function 10: function backup and resolve ( b C ) 11:  X  c  X  continuous backup ( b C ,  X  LB ) . (10) 12: p  X  refine representation (  X  c ,p ) . Sec. 2.3 13:  X  d  X  discretize (  X  c ,p ) 15: if V ( b A ) &lt;  X  d ( b A,d ) then . check (14) 16: b C  X  b C  X  b A | w =0 . extend b C particles 17: goto 11 18: end if 19: end for 21:  X  LB  X   X  LB  X   X  d . add new consistent  X  d 22: end function Lower and Upper Bound The lower bound at backup step n is a set  X  n LB of PWLC  X  -functions, rep-resented by sparse vectors  X  d : S d  X  R using the dis-cretization p m ( s d | s ) of a potentially different version m . If p m ( s d | s ) defines a partitioning c m (see Eq. (13)),  X  d is non-overlapping and non-empty in S . Conse-quently, the discretization of version m is implicitly stored with  X  d . To evaluate  X   X  d ,b d  X  d we can calcu-late the sparse dot-product with the overlapping rep-resentation b d, overlap of b . Zero-entries  X  d ( s d ) = 0 are undefined and do neither affect the result nor increase processing time.
 For the upper bound we use the  X  X awtooth X -interpolation method proposed in (Hauskrecht, 2000). The upper bound is represented by a set ( b p d ,V  X 
UB of pairs of beliefs and their value. It can be evaluated for a belief b by finding the belief-value-pair (BVP) with the minimal interpolation V UB ( b ) = tively carried out in S d . Ideally, both bounds converge to the optimal value function and thus share their opti-mal representation. They can benefit from each other discovering  X  X nteresting X  splits. 2.4. Correction of Value-Generalizations It is not feasible to preserve the bound-properties in general continuous POMDP problems exactly because the MC Value Iteration backup cannot be calculated for the complete state space. If not treated, this will lead to contradictory statements from the different be-liefs regarding the value and possibly corrupt the pol-icy. Quantifying these errors is very difficult if not im-possible. Instead, we give the solver the novel ability to reflect the reasons for errors and correct them. To maintain consistency for the lower and upper bound,  X  -functions and BVPs violating the bound property are detected. After identification, these inconsisten-cies are resolved by extending the sample horizon of the conflicting value approximation. This concept is universally applicable, regardless of the representation of bounds, the problem X  X  dimensionality or nature. It can also be used for discrete POMDP solvers to realize an efficient sparse backup.
 Conflict Detection Assuming an exact MC value-backup, the Value Iteration in Sec. 2.2 converges monotonically to the optimal value. Hence, it can be safely stated that the n -th step continuous belief backup V n ( b A ) defined in Eq. (6), using a lower bound  X 
LB as knowledge base, must be superior or equal to any direct  X  -function from the same knowledge base evaluated for b A : If this property is violated,  X  n  X  1 LB is no lower bound because a continuous  X  -function was overgeneralized. Consequently, the violating  X  C must be corrected. Al-gorithm 1 shows how this concept is embedded into the lower bound Value Iteration for a newly explored belief b . In the examples in Fig. 1, the  X  -vector gen-eralization in a conflicting belief b C violated the con-tinuous backup of the accusing belief b A . The beliefs in those examples are obviously different. There are also POMDP problems, where beliefs close to each other in the state space can lead to significantly dif-ferent values. Approaches which use a distance metric to differentiate beliefs or approximate beliefs directly with parametric models or kernel densities fail for such problems. The conflict mechanism in our approach en-sures that the representation gets refined to differenti-ate even such close beliefs.
 For the upper bound approximation applies similar: The BVPs in the sawtooth approximation described in Sec. 2.3 must create a higher or equal value for every b than their continuous backup of the upper bound. We assert these properties continuously by checking every new  X  -vector or BVP for conflicts with any of the so far explored beliefs. Additionally, we check, if a newly explored belief reveals a conflict with an exist-ing  X  -function or BVP. If that is the case, we resolve the conflict as described in the next section. As both bounds are formulated sparsely in the discrete space, the conflict detection is not as time-consuming as it might appear at first glance. All beliefs are already given in a discrete representation, which must at worst be updated to the most recent version. Conflict Resolution The reason for the conflicts induced by an  X  -vector is that the continuous space is only incompletely covered by sampling. Assumptions about the undefined regions had to be made, which b A falsified during conflict detection. Extending the sam-pling base for the approximation with samples from b
A invalidates the reason for the conflict. Therefore we compute the  X  -backup for every sample s i A of b A acting according to the policy of b C with Eq. (10). As can be seen in the 1D example in Fig. 1, a new ap-proximation then automatically learns the lower value of those additional  X  ( s )-samples combined with the old samples.
 The correction procedure for the upper bound approx-imations is not covered in detail. It is based on the fact that using a more refined representation for b p d speci-fies the meaning of the BVP and increases the value V d ( b d ) for any b d . It is possible and probably benefi-cial to introduce splits in this process, but the current implementation only uses splits from the lower bound. 2.5. Evaluation The presented solver is evaluated on three problems. The first one is the continuous corridor problem de-scribed by (Porta et al., 2005). While it is only 1D it comprises an infinite amount of states and can there-fore not be solved discretely. The second problem is a 2D extension of the first and shows the capability of the presented algorithm to find a low dimensional discrete representation. At last, we introduce a new higher-dimensional 8D intersection problem to analyze the solver in a more realistic scenario. 1D Corridor Problem The first problem models a robot moving in a one-dimensional corridor limited by walls. The aim of the robot is to open the right door, while perceiving its own position only uncertainly by a discrete number of Gaussians. In the upper graph of Fig. 2 the evaluation results of the policies after different solving times of the presented solver is com-pared to MCVI (Release 0.2, 17 May 2012) from (Bai et al., 2010) and C-POMDP (PERSEUS) from (Porta et al., 2006) using their open source implementations and parameters. Therefore we ran 10 , 000 trials simu-lating 100 consecutive time steps. It shows that pre-vious solvers only reach suboptimal average rewards after reasonable solving time. After 12 h MCVI did not exceed an average reward of 1 . 8. In 16 min our algorithm found a much better policy with an aver-age reward of about 2 . 55, although the smooth na-ture of the Gaussians is close to a worst case for the proposed piecewise-constant representation. Because of the reuse of samples in the presented MC backup and the particle extension by the conflict mechanism, 300 particles are sufficient to solve this problem. Af-ter about 33 min both bounds converged and the algo-rithm stopped. 2D Corridor Problem The second problem is a 2D extension of the corridor problem. The robot can additionally go up and down. The 12 observations are product densities of N (  X  3 , 2), N (3 , 2) and N (0 , 100) in x 1 with the 4 original 1D observations in x 0 . The reward only depends on x 0 . Basically x 1 only distracts the solver, but still the combinatorial com-plexity of this problem is much higher due to the increased number of observations. Additionally, the backup steps are slower because 500 particles were used. The second plot in Fig. 1 shows that the splits are mainly parallel to x 1 . The solver generalizes the values by recognizing the meaninglessness of x 1 and thus converges after reasonable time, as shown in the lower graph of Fig. 2. Interestingly, the value bounds are not monotonic. The upper bound often drops quickly at the beginning and corrects itself when a conflict detecting belief is explored. 8D Intersection Problem In the intersection sce-nario in Fig. 3 a robot agent and an obstacle move on a 2D plane following a constant velocity model with small white noise. The positions and velocities of both objects result in an 8D joint state space. The observations are continuous 8D measurements of the state with additive white noise. The objects have a circular shape with a radius of 1 m. Their po-sitions are loosely bounded by roads . Starting at ( x,y,v x ,v y ) = (0 m , 0 m , 1 m/s , 0 m/s) the agent has to cross the intersection without hitting the moving obstacle by accelerating and decelerating. The agent receives a reward of +10 for reaching the goal and  X  10 in case of a collision. The discount is  X  = 0 . 95. If the obstacle moves past y = 8 m it is respawned at y =  X  8 m so that the obstacle remains a constant threat. The difficulty of the problem is that the y -position of the obstacle is not known in advance and the agent X  X  viewing distance is limited to 4 m. The pol-icy found by the solver after 543 s achieves an average score of 5 . 0 in simulation using 400 particles to repre-sent the beliefs. The final representation of the space uses a total of 487 states. A fixed discretization of an 8D space with just two regions per dimension would already yield 256 states. The iterative refinement dis-covered an efficient representation. The final policy is encoded by 41  X  -functions describing the following behavior: The agent slowly approaches the intersec-tion to be able to stop in a safe distance where he can perceive the y -position of the obstacle with a small standard deviation of  X  y = 0 . 32 (see Fig. 4 ). Then it accelerates instantly or moves back first to pass the obstacle.
 The idea of combining planning with the gener-alization capabilities of inductive learning shows very promising results that bring the application of POMDPs in real world tasks one step closer to real-ity. The presented algorithm finds a better solution to the continuous corridor problem in about 30 s than previous approaches can obtain in hours. The novel concept worked even in this noise dominated domain and a higher-dimensional 8D obstacle avoidance prob-lem. The presented algorithm is particularly suited for POMDPs with sparse, multi-modal models.
 In future work, a pruning of  X  -functions as well as beliefs analogously to discrete solvers needs to be in-corporated. Non-linear splitting functions, refinement according to the upper bound and more sophisticated error heuristics to control the adaptation would further increase the efficiency of the representation. Finally, finding a better exploration heuristic for continuous belief spaces is still an unsolved problem.
 Acknowledgments This research is supported by the Deutsche Forschungsgemeinschaft .
 Bai, H., Hsu, D., Lee, W.S., and V.A., Ngo. Monte Carlo Value Iteration for Continuous-State
POMDPs. In Workshop on the Algorithmic Foun-dations of Robotics , pp. 175 X 191, 2010.
 Bellman, R. A Markovian decision process. Journal of Applied Mathematics and Mechanics , 6:679 X 684, 1957.
 Brechtel, S., Gindele, T., and Dillmann, R. Probabilis-tic MDP-behavior planning for cars. In Int. Conf. on
Intelligent Transportation Systems , pp. 1537 X 1542, 2011.
 Brooks, A., Makarenko, A., Williams, S., and Durrant-
Whyte, H. Parametric POMDPs for planning in continuous state spaces. Robotics and Autonomous Systems , 54(11):887 X 897, 2006. ISSN 0921-8890. Doucet, A., De Freitas, N., and Gordon, N. Sequential Monte Carlo methods in practice . Springer, 2001. Feng, Z. and Hansen, E. An Approach to State Ag-gregation for POMDPs. In Workshop on Learning and Planning in Markov Processes , pp. 7 X 12, 2004. Gindele, T., Brechtel, S., and Dillmann, R. A Proba-bilistic Model for Estimating Driver Behaviors and Vehicle Trajectories in Traffic Environments. In
Int. Conf. on Intelligent Transportation Systems , pp. 1625 X 1631, 2010.
 Hauskrecht, M. Value-function approximations for partially observable Markov decision processes.
Journal of Artificial Intelligence Research , 13(1):33 X  94, 2000.
 Hoey, J. and Poupart, P. Solving POMDPs with continuous or large discrete observation spaces. In
Int. Joint Conf. on Artificial Intelligence , pp. 1332, 2005.
 Kaelbling, L. P., Littman, M. L., and Cassandra, A. R.
Planning and acting in partially observable stochas-tic domains. Journal on Artificial Intelligence , 101 (12):99  X  134, 1998. ISSN 0004-3702.
 Kurniawati, H., Hsu, D., and Lee, W.S. SARSOP: Effi-cient point-based POMDP planning by approximat-ing optimally reachable belief spaces. In Robotics: Science and Systems , 2008.
 MacKay, D.J.C. Information Theory, Inference and
Learning Algorithms . Cambridge University Press, 2003.
 Munos, R. and Moore, A. Variable resolution dis-cretization in optimal control. Machine learning , 49 (2):291 X 323, 2002. ISSN 0885-6125.
 Pineau, J., Gordon, G., and Thrun, S. Point-based value iteration: An anytime algorithm for POMDPs.
In Int. Joint Conf. on Artificial Intelligence , vol-ume 18, pp. 1025 X 1032, 2003.
 Porta, J.M., Spaan, M.T.J., and Vlassis, N. Robot planning in partially observable continuous do-mains. In Robotics: Science and Systems , volume 1, pp. 217. The MIT Press, 2005.
 Porta, J.M., Vlassis, N., Spaan, M.T.J., and Poupart, P. Point-based value iteration for continuous
POMDPs. The Journal of Machine Learning Re-search , 7:2329 X 2367, 2006. ISSN 1532-4435.
 Poupart, P. and Boutilier, C. Value-directed compres-sion of POMDPs. Advances in Neural Information Processing Systems , 15:1547 X 1554, 2002.
 Poupart, P., Kim, K.E., and Kim, D. Closing the Gap: Improved Bounds on Optimal POMDP Solutions. In
Int. Conf. on Automated Planning and Scheduling , 2011.
 Quinlan, J.R. C4. 5: programs for machine learning . Morgan Kaufmann, 1993.
 Roy, N., Gordon, G.J., and Thrun, S. Finding approx-imate pomdp solutions through belief compression.
Journal of Artificial Intelligence Research , 23:1 X 40, 2005.
 Smith, T. and Simmons, R. Heuristic search value iteration for pomdps. In Uncertainty in Artificial Intelligence , pp. 520 X 527. AUAI Press, 2004.
 Sondik, E.J. The Optimal Control of Partially Observ-able Markov Decision Processes. PhD thesis, Stan-ford University , 1971.
 Spaan, M.T.J. and Vlassis, N. Perseus: Randomized point-based value iteration for POMDPs. Journal of Artificial Intelligence Research , 24(1):195 X 220, 2005. van den Berg, Jur, Patil, Sachin, and Alterovitz, Ron.
Efficient Approximate Value Iteration for Continu-ous Gaussian POMDPs. In Conf. on Artificial In-telligence , 2012.
 Zhou, E., Fu, M.C., and Marcus, S.I. Solving continuous-state POMDPs via density projection.
Transactions on Automatic Control , 55(5):1101 X 
