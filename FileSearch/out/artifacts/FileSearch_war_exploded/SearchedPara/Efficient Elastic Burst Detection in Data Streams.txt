 Burst detection is the activity of finding abnormal aggre-gates in data streams. Such aggregates are based on sliding windows over data streams. In some applications, we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods. We will present a general data struc-ture for detecting interesting aggregates over such elastic windows in near linear time. We present applications of the algorithm for detecting Gamma Ray Bursts in large-scale astrophysical data. Detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time Trade and Quote (TAQ) data from the New York Stock Exchange (NYSE). Our algorithm beats the direct computation approach by several orders of magnitude.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining data stream, elastic burst Consider the following application that motivates this re-search. An astronomical telescope, Milagro[ 1] was built in New Mexico by a group of prestigious astrophysicists from the Los Alamos National Laboratory and many universities. This telescope is actually an array of light-sensitive detectors covering a large pool of water about the size of a football field. It is used to constantly observe high-energy photons from the universe. When many photons observed, the sci-entists assert the existence of a Gamma Ray Burst. The scientists hope to discover primordial black holes or com-pletely new phenomena by the detection of Gamma Ray Work supported in part by U.S. NSF grants IIS-9988636, MCB-0209754 and N2010-0115586.
 Copyright 2003 ACM 1-58113-737-0/03/0008... $ 5.00. Bursts. The occurrences of Gamma Ray Bursts are highly variable, flaring on timescale of minutes to days. Once such a burst happens, it should be reported immediately. Other telescopes could then point to that portion of sky to confirm the new astrophysical event. The data rate of the observa-tion is extremely high. Hundreds of photons can be recorded within a second from a tiny spot in the sky[ 8, 9]. There are also many applications in data stream mining and monitoring when people are interested in discovering time intervals with unusually high numbers of events. For exam-ple: Intuitively, given an aggregate function F (such as sum or count), the problem of interest is to discover subsequences s of a time series stream such that F ( s ) &gt;&gt; F ( s subsequences s 0 of size | s | . In the case of burst detection, the aggregate is sum. If we know the duration of the time interval, we can maintain the sum over sliding windows of a known window size and sound an alarm when the moving sum is above a threshold. Unfortunately, in many cases, we cannot predict the length of the burst period. In fact, dis-covering that length is part of the problem to be solved. In the above example of Gamma Ray Burst detection, a burst of photons associated with a special event might last for a few milliseconds, a few hours, or even a few days. There are different thresholds associated with different durations. A burst of 10 events within 1 second could be very interesting. At the same time, a burst that lasts longer but with lesser density of events, say 50 events within 10 seconds, could be of interest too.
 Suppose that we want to detect bursts for a time series of size n and we are interested in all the n sliding window sizes. A brute-force search has to examine all the sliding window sizes and starting positions. Because there are O ( n 2 ) windows, the lower bound of the time complexity is O ( n 2 ). This is very slow for many applications. Fortunately, because we are interested only in those few windows that experience bursts, it is possible to design a nearly linear time algorithm. In this paper we present a burst detection algorithm with time complexity approximately proportional to the size of the input plus the size of the output, i.e. the number of windows with bursts. There are two categories of time series data stream moni-toring: point monitoring and aggregate monitoring . In point monitoring, the latest data item in the stream is of interest. When the latest item falls in some predefined domain, an alarm would be sounded. For example, a stock trader who places a limited sell order on Enron informs the stock price stream monitor to raise an alarm (or automatically sell the stock) once the price of stock fall under $10 to avoid further losses. Since only the latest data in the stream need to be considered, point monitoring can be implemented without much effort.
 Aggregate monitoring is much more challenging. Aggregates of time series are computed based on certain time intervals (windows). There are three well-known window models that are the subjects of many research projects [ 10, 11, 12, 22]. 1. Landmark windows: Aggregates are computed based 2. Sliding windows: In a sliding window model, aggre-3. Damped window: In a damped window model the The sliding window model is the most widely used in data stream monitoring. Motivated by the Gamma Ray example, we have generalized this to the elastic window model . In an elastic window model, the user needs to specify only the range of the sliding window sizes, the aggregate function and alarms domains, and will be notified of all those window sizes in the range with aggregates falling in the corresponding alarm domains.
 Here we give the formal definition of the problem of moni-toring data stream over elastic windows.

Problem 1. For a time series x 1 , x 2 , ..., x n , given a set of window sizes w 1 , w 2 , ..., w m , an aggregate function F and monitoring elastics window aggregates of the time series is to find all the subsequences of all the window sizes such that the aggregate applied to the subsequences cross their window sizes X  thresholds, i.e. The threshold above can be estimated from the historical data or the model of the time series. Elastic burst detec-tion is a special case of monitoring data streams on elastic windows. In elastic burst detection, the alarm domain is [ f ( w j ) ,  X  ). Note that it is also possible for the alarm do-main to be (  X  X  X  , f ( w j )]. The contributions of the paper are as follows. In this section, we first give some background on the wavelet data structure. In section 2.2 we discuss the Shifted Wavelet Tree and the algorithm for efficient elastic burst detection in an offline setting. This is extended to a streaming algo-rithm in section 2.3. Our algorithm will also be generalized to other problems in data stream monitoring over elastic windows in section 2.4 and to higher dimensions in section 2.5. In wavelet analysis, the wavelet coefficients of a time series are maintained in a hierarchical structure. Let us consider the simplest wavelet, the Haar wavelet. For simplicity of notation, suppose that the size of time series n is a power of two. This would not affect the generality of the results. The original time series makes up of level 0 in a wavelet tree. The pair wise (normalized) averages and differences of the adjacent data items at level 0 produce the wavelet coeffi-cients at level 1. The process is repeated for the averages at level i to get the averages and differences at level i + 1 un-til there is only one average and difference at the top level. Table 1 shows the process in computing the Haar wavelet decomposition. The Haar wavelet coefficients include the average in the highest level and the differences in each level. From these wavelet coefficients, the original time series can be constructed without loss of information. Usually a few wavelet coefficients can represent the trend of the time se-ries, and they are selected as a compact representation of the original time series. The wavelet coefficients above can also be viewed as the aggregates of the time series at different time intervals. Fig-ure 1-a shows the time interval hierarchy in the Haar wavelet decomposition. At level i , there are n 2  X  i consecutive win-dows with size 2 i . All the windows at the same level are disjoint. The aggregates that the Haar wavelet maintains are the (normalized) averages and differences. In our dis-cussion of burst detection, the aggregate of interest is the sum. Obviously, such a wavelet tree can be constructed in O ( n ) time.
 The first few top levels of a wavelet tree yield concise multi-resolution information of the time series. This gives the wavelet lots of applications. However, for our purpose of burst detection, such a data structure has a serious problem. Because the windows at the same level are non-overlapping, a window of arbitrary start position and arbitrary size might not be included in any window in the wavelet tree, except the window at the highest level that includes everything. For example, the window consisting of three time points in the middle, ( n/ 2  X  1 , n/ 2 , n/ 2 + 1), is not contained in any window in the wavelet tree except the largest one. This makes wavelets inconvenient for the discovery of properties of arbitrary subsequences. In a shifted wavelet tree (SWT) (figure 1-b), the adjacent windows of the same level are half overlapping. In figure 1, we can see that the size of a SWT is approximately double that of a wavelet tree, because at each level, there is an addi-tional  X  X ine X  of windows. These additional windows provide valuable overlapping information for the time series. They can be maintained either explicitly or implicitly. If we keep only the aggregates for a traditional wavelet data structure, the aggregates of the overlapping windows at level i can be computed from the aggregates at level i  X  1 of the wavelet data structure.
 Given : x[1..n],n= 2 a Return: shifted wavelet tree SWT[1..a][1..] b=x; FOR i = 1 TO a //remember a = log 2 n //merge consecutive windows and form //level i of the shifted wavelet tree FOR j = 1 TO size(b)-1 STEP 2
ENDFOR //downsample, retain a non-overlapping cover
FOR j = 1 TO size(SWT[i])/2 ENDFOR ENDFOR Figure 2: Algorithm to construct shifted wavelet tree To build a SWT, we start from the original time series and compute the pair wise aggregate (sum) for each two consec-utive data items. This will produce the aggregates at level 1. A downsampling on this level will produce the input for the higher level in the SWT. Downsampling is simply sam-pling every second item in the series of aggregates. In figure 1-b, downsampling will retain the upper consecutive non-overlapping windows at each level. This process is repeated until we reach the level where a single window includes every data point. Figure 2 gives a pseudo-code to build a SWT. Like regular wavelet trees, the SWT can also be constructed in O ( n ) time.
 For a subsequence starting and ending at arbitrary positions, there is always a window in the SWT that tightly includes the subsequence as figure 3 shows and the following lemma proves.
Lemma 1. Given a time series of length n and its shifted wavelet tree, any subsequence of length w, w  X  2 i is included in one of the windows at level i + 1 of the shifted wavelet tree.

Proof. The windows at level i + 1 of the shifted wavelet tree are: A subsequence with length 2 i starting from an arbitrary po-sition c will be included in at least one of the above windows, because [ c .. c +2 i  X  1]  X  [( j  X  1)2 i +1 .. ( j +1)2 i ] , j = b Any subsequence with length w, w  X  2 i is included in some subsequence(s) with length 2 i , and therefore is included in one of the windows at level i +1. We say that windows with size w, 2 i  X  1 &lt; w  X  2 i , are monitored by level i + 1 of the SWT.
 Because for time series of non-negative numbers the aggre-gate sum is monotonically increasing, the sum of the time series within a sliding window of any size is bounded by the sum of its including window in the shifted wavelet tree. This fact can be used as a filter to eliminate those subsequences whose sums are far below their thresholds.
 Figure 4 gives the pseudo-code for spotting potential sub-sequences of size w, w  X  2 i , with sums above its threshold f ( w ). The algorithm will search for burst in two stages. First, the potential burst is detected at the level i + 1 in the SWT, which corresponds to the subsequence x [( j  X  1)2 i + 1 .. ( j +1)2 i ]. In the second stage, those subsequences of size 2 within x [( j  X  1)2 i +1 .. ( j +1)2 i ] with sum less than f ( w ) are further eliminated. The moving sums of sliding window size 2 i can be reused for burst detection of other window size w 0 6 = w, w 0  X  2 i . The detailed search of burst on the surviving subsequences is then performed. A detailed search in a subsequence is to compute the moving sums with win-dow size w in the subsequence directly and to verify if these moving sums cross the burst threshold.
 In the spirit of the original work of [ 2] that uses lower bound technique for fast time series similarity search, we have the Given : time series x[1..n], n = 2 a , shifted wavelet tree SWT[1..a][1..], window size w , threshold f ( w ) Return: Subsequences of x with burst i = d log 2 w e ; FOR j = 1 TO size(SWT[i+1])
IF (SWT[i+1][j]&gt;f[w]) //possible burst in subsequence x[ ( j  X  1)2 i +1 .. ( j +1)2 //first we compute the moving sums with //window size 2 i within this subsequence.
 ENDIF ENDFOR following lemma that guarantees the correctness of our al-gorithm.

Lemma 2. The above algorithm can guarantee no false negatives in elastic burst detection from a time series of non-negative numbers.

Proof. From lemma 1, any subsequence of length w, w  X  2 is contained within a window in the SWT: [ c .. c + w  X  1]  X  [ c .. c + 2 i  X  1]  X  [( j  X  1)2 i +1 .. ( j +1)2 Because the sum of the time series of non-negative numbers is monotonic increasing, we have X By eliminating sequences with lengths larger than w but with sums less than f ( w ), we do not introduce false nega-tives because X In most applications, the algorithm will perform detailed search seldom and then usually only when there is a burst of interest. For example, suppose that the moving sum of a time series is a random variable from a normal distribu-tion. Let the sum within sliding window w be S o ( w ) and its expectation be S e ( w ), We assume that We set the threshold of burst f ( w ) for window size w such that the probability that the observed sums exceed the thresh-the normal cumulative distribution function, we have Because our algorithm monitors the burst based on windows with size W = Tw, 1  X  T &lt; 2, the detailed search will always report real bursts. Actually our algorithm performs a detailed search whenever there are more than f ( w ) events in a window of W . Therefore the rate of detailed search p f is higher than the rate of true alarms. Suppose that S ( W ) = TS e ( w ), we have p = Pr ( S o ( W )  X  f ( w )) = Pr  X  =  X  =  X  The rate of detailed search is very small for small p , the primary case of interest. For example, let p = 10  X  6 , T = 1 . 5, p f is 0 . 006. In this model, the upper bound of false alarm rate is guaranteed.
 The time for a detailed search in a subsequence of size W is O ( W ). The total time for all detailed searches is linear in the number of false alarms and true alarms(the output size k ). The number of false alarm depends on the data and the setting of thresholds, and it is approximately proportional to the output size k . So the total time for detailed searches is bounded by O ( k ). To build the SWT takes time O ( n ), thus the total time complexity of our algorithm is approximately O ( n + k ), which is linear in the total of the input and output sizes. The SWT data structure in the previous section can also be used to support a streaming algorithm for elastic burst detection. Suppose that the set of window sizes in the elastic window model are 2 L &lt; w 1 &lt; w 2 &lt; ... &lt; w m  X  2 simplicity of explanation, assume that new data becomes available at every time unit.
 Without the use of SWT, a naive implementation of elastic burst detection has to maintain the m sums over the sliding windows. When a new data item becomes available, for each sliding window, the new data item is added to the sum and the corresponding expiring data item of the sliding window is subtracted from the sum. The running sums are then checked against the monitoring thresholds. This takes time O ( m ) for each insertion of new data. The response time is one time unit if enough computing resources are available. By comparison, the streaming algorithms based on the SWT data structure will be much more efficient. For the set of window sizes 2 L &lt; w 1 &lt; w 2 &lt; ... &lt; w m  X  2 U to maintained the levels from L + 2 to U + 1 of the SWT that monitor those windows. There are two methods that provide tradeoffs between throughput and response time.
Lemma 3. The amortized processing time per insertion into the data stream for a batch algorithm is 2 .
Proof. At level i, L + 2  X  i  X  U + 1 , of the SWT, ev-ery 2 i  X  1 time units there is a window in which all the data are available. The aggregates at that window can be com-puted in time O (1) based on the aggregates at level i  X  1. total amortized update time for all levels (including the ex-tra level) is Figure 5: (a)Wavelet Tree 2D(left) and (b)Shifted Wavelet Tree 2D(right)
Lemma 4. The burst activity of a window with size w will be reported with a delay less than 2 d log 2 w e .
Proof. A window with size w, 2 i  X  1 &lt; w  X  2 i , is moni-tored by level i + 1 of the SWT. The aggregates of windows at level i + 1 are updated every 2 i time units. When the aggregates of windows at level i + 1 are updated, the burst activity of window with size w can be checked. So the re-sponse time is less than 2 i = 2 d log 2 w e . It should be clear that in addition to sum, the monitoring of many other aggregates based on elastic windows could benefit from our data structure, as long as the following conditions holds. 1. The aggregate F is monotonically increasing or de-2. The alarm domain is one sided, that is, [ threshold,  X  ) The most important and widely used aggregates are all mono-tonic: Max, Count are monotonically increasing and Min is monotonically decreasing. Another monotonic aggregate is Spread . Spread measures the volatility or surprising level of time series. Spread of a time series ~x is Spread is monotonically increasing. The spread within a small time interval is less than or equal to that within a larger time interval. A large spread within a small time interval is of interest in many applications in data stream because it indicates that the time series has experienced large movement. The one-dimensional shifted wavelet tree for time series can naturally be extended to higher dimensions, such as spa-tial dimensions. In this section we consider the problem Figure 6: Bursts of the number of times that coun-tries were mentioned in the presidential speech of the state of the union of discovering elastic spatial bursts using a two-dimensional shifted wavelet tree. Given a fixed image of scattering dots, we want to find the regions of the image with unexpectedly high density of dots. In an image of the sky with many dots representing the stars, such regions might indicate galax-ies or supernovas. The problem is to report the positions of spatial sliding windows (rectangle regions) having differ-ent sizes, within which the density exceeds some predefined threshold.
 The two-dimensional shifted wavelet tree is based on the two-dimensional wavelet structure. The basic wavelet struc-ture separates a two-dimensional space into a hierarchy of windows as shown in figure 5-a (similar to quadtree[ 18]). Ag-gregate information will be computed recursively based on those windows to get a compact representation of the data. Our two-dimensional shifted wavelet tree will extend the wavelet tree in a similar fashion as in the one-dimensional case. This is demonstrated in figure 5-b. At the same level of the wavelet tree, in addition to the group of disjoint windows that are the same as in the wavelet tree, there are another three groups of disjoint windows. One group of windows offsets the original group in the horizontal direction, one in the vertical direction and the third one in both directions. Any square spatial sliding window with size w  X  w is included in one window of the two-dimensional SWT. The size of such a window is at most 2 w  X  2 w . Using the techniques of section 2.2, burst detection based on the SWT-2D can report all the high density regions efficiently. Our empirical study will first demonstrate the desirability of elastic burst detection for some applications. We also study the performance of our algorithm by comparing our algorithm with the brute force searching algorithm in section 3.2. As an emotive example, we monitor bursts of interest in countries from the presidential State of the Union addresses from 1913 to 2003. The same example was used by Klein-berg[ 16] to show the bursty structure of text streams. In figure 6 we show the number of times that some countries were mentioned in the speeches. There are clearly bursts of interest in certain countries. An interesting observation here is that these bursts have different durations, varying from years to decades.
 The rationale behind elastic burst detection is that a pre-defined sliding window size for data stream aggregate mon-itoring is insufficient in many applications. The same data aggregated in different time scales will give very different pictures as we can see in figure 7. In figure 7 we show the moving sums of the number of events for about an hour X  X  worth of Gamma Ray data. The sizes of sliding windows are 0 . 1, 1 and 10 seconds respectively. For better visualiza-tion, we only show those positions with bursts. Naturally, bursts at small time scales that are extremely high will pro-duce bursts in larger time scales too. More interestingly, bursts at large time scales are not necessarily reflected at smaller time scales, because those bursts at large time scales might be composed of many consecutive  X  X umps X . Bumps are those positions where the numbers of events are high but not high enough to be  X  X ursts X . Therefore, by looking at different time scales at the same time, elastic burst detection will give more insight into the data stream.
 We also show in figure 8 an example of spatial elastic bursts. We use the 1990 census data of the population in the con-tinental United State. The population in the map are ag-gregated in a grid of 0 . 2  X   X  0 . 2  X  in Latitude/Longitude. We compute the total population within sliding spatial windows with sizes 1  X   X  1  X  , 2  X   X  2  X  and 5  X   X  5  X  . Those regions with population above the 98 percentile in different scales are highlighted. We can see that the different sizes of sliding windows give the distribution of high population regions at different scales. Our experiments were performed on a 1.5GHz Pentium 4 PC with 512 MB of main memory running Windows 2000. We tested the algorithm with two different types of data sets: In the following experiments, we set the thresholds of differ-ent window sizes as follows. We use the first few hours of Gamma Ray data and the first year of Stock data as train-ing data respectively. For a window of size w , we compute the aggregates on the training data with sliding window of size w . This gives another time series ~y . The thresholds are are the average and standard deviation respectively. The factor of threshold  X  is set to 8. The list of window sizes is 5 , 10 , ..., 5  X  N w time units, where N w is the number of win-dows. N w varies from 5 to 50. The time unit is 0 . 1 seconds for the Gamma Ray data and 1 minute for the stock data. Figure 9: The processing time of elastic burst de-tection on Gamma Ray data for different number of windows First we compare the wall clock processing time of elastic burst detection from the Gamma Ray data in figure 9. Our algorithm based on the SWT data structure is more than ten times faster than the direct algorithm. The advantage of us-ing our data structure becomes more obvious as we examine more window sizes. The processing time of our algorithm is output-dependent. This is confirmed in figure 10, where we examine the relationship between the processing time using our algorithm and the number of alarms. Naturally the number of alarms increases as we examine more window sizes. We also observed that the processing time follows the number of alarms well. Recall that the processing time of the SWT algorithm includes two parts: building the SWT and the detailed searching of those potential portions of burst. Building the SWT takes only 200 milliseconds for the data set, which is negligible when compared to the time to do the detailed search. Also for demonstration purposes, we intentionally, to our disadvantage, set the thresholds lower and therefore got many more alarms than what physicists are interested in. If the alarms are scarce, as is the case for Gamma Ray burst detection, our algorithm will be even faster. In figure 11 we fix the number of windows to be 25 and change the factor of threshold  X  . The larger  X  is, the higher the thresholds are, and therefore the fewer alarms will be sounded. Because our algorithm is dependent on the output sizes, the higher the thresholds are, the faster the al-gorithm runs. In contrast, the processing time of the direct algorithm does not change accordingly.
 For the next experiments, we test the elastic burst detection algorithm on the IBM Stock trading volume data. Figure 12 shows that our algorithm is up to 100 times faster than a brute force method. We also zoom in to show the processing time for different output sizes in figure 13.
 In addition to elastic burst detection, our SWT data struc-ture works for other elastic aggregates monitoring too. In the following experiments, we search for big spreads on the IBM Stock data. Figure 14 and 15 confirms the performance advantages of our algorithm. Note that for the aggregates of Min and Max, and thus Spread, there is no known de-terministic algorithm to update the aggregates over sliding windows incrementally in constant time. The filtering prop-erty of SWT data structure gains more by avoiding unnec-essary detailed searching. So in this case our algorithm is up to 1,000 times faster than the direct method, reflecting Figure 10: The processing time of elastic burst de-tection on Gamma Ray data for different output sizes Figure 11: The processing time of elastic burst de-tection on Gamma Ray data for different thresholds Figure 12: The processing time of elastic burst de-tection on Stock data for different number of win-dows Figure 13: The processing time of elastic burst de-tection on Stock data for different output sizes Figure 14: The processing time of elastic spread de-tection on Stock data for different number of win-dows Figure 15: The processing time of elastic spread de-tection on Stock data for different output sizes the advantage of a near linear algorithm as compared with a quadratic one. There is much recent interest in data stream mining and monitoring. An excellent survey of models and issues in data stream can be found in [ 3]. The sliding window is recog-nized as an important model for data stream. Based on the sliding window model, previous research studies the compu-tation of different aggregates of data stream, for example, correlated aggregates [ 11], count and other aggregates[ 7], frequent itemsets and clusters[ 10], and correlation[ 22]. The work [ 13] studies the problem of learning models from time-changing streams without explicitly applying the sliding win-dow model. The Aurora project[ 4] considers the systems as-pect of monitoring data streams. Also the algorithm issues in time series stream statistics monitoring are addressed in StatStream[ 22]. In this paper we extend the sliding win-dow model to the elastic sliding window model, making the choice of sliding window size more automatically. Wavelets are heavily used in the context of data manage-ment and data mining, including selectivity estimation[ 17], approximate query processing[ 20, 5], dimensionality reduc-tion[ 6] and streaming data analysis[ 12]. However, its use in elastic burst detection is innovative. We achieve efficient detection of subsequences with burst in a time series by fil-tering lots of subsequences that are unlikely to have burst. This is an extension to the well-known lower bound tech-nique in similarity search from time series[ 2].
 Data mining on bursty behavior has attracted more atten-tion recently. Wang et al. [ 21] study fast algorithms using self-similarity to model bursty time series. Such models can generate more realistic time series streams for testing data mining algorithm. Kleinberg[ 16] also discusses the problem of burst detection in data streams. The focus of his work is in modeling and extracting structure from text streams. Our work is different in that we focus on the algorithmic issue of counting over different sliding windows. We have extended the data structure for burst detection to high-spread detection in time series. Spread measures the surprising level of time series. There is also work in finding surprising patterns in time series data. However the defi-nition of surprise is application dependent and it is up to the domain experts to choose the appropriate one for their application. Jagadish et al.[ 14] use optimal histograms to mine deviants in time series. In their work deviants are defined as points with values that differ greatly from that of surrounding points. Shahabi et al.[ 19] also use wavelet-based structure(TSA-Tree) to find both trends and surprises in large time series dataset, where surprises are defined as large difference between two consecutive averages of a time series. In very recent work, Keogh et al.[ 15] propose an al-gorithm based on suffix tree structures to find surprising patterns in time series database. They try to learn a model from the previously observed time series and declare sur-prising for those patterns with small chance of occurrence. By comparison, an advantage of our definition of surprise based on spread is that it is simple, intuitive and scalable to massive and streaming data. This paper introduces the concept of monitoring data streams based on an elastic window model and demonstrates the de-sirability of the new model. The beauty of the model is that the sliding window size is left for the system to discover in data stream monitoring. We also propose a novel data structure for efficient detection of elastic bursts and other aggregates. Experiments on real data sets show that our algorithm is faster than a brute force algorithm by several orders of magnitude. We are currently collaborating with physicists to deploy our algorithm for online Gamma Ray burst detection. A robust way of setting the thresholds of burst for different window sizes is a topic for future work. Also the problem of monitoring of non-monotonic aggregates is an interesting future work. We are grateful to Prof. Allen I. Mincer of the Milagro Project, for giving us the preliminary tutorial of astrophysics. We also thank the Milagro collaboration for making the Gamma Ray data available to us. [1] http://www.lanl.gov/milagro/, 2003. [2] R. Agrawal, C. Faloutsos, and A. N. Swami. Efficient [3] B. Babcock, S. Babu, M. Datar, R. Motwani, and [4] D. Carney, U. Cetintemel, M. Cherniack, C. Convey, [5] K. Chakrabarti, M. N. Garofalakis, R. Rastogi, and [6] K.-P. Chan and A. W.-C. Fu. Efficient time series [7] M. Datar, A. Gionis, P. Indyk, and R. Motwani. [8] R. Atkins et. al. (The Milagro Collaboration). [9] A. J. Smith for the Milagro Collaboration. A search [10] V. Ganti, J. Gehrke, and R. Ramakrishnan. Demon: [11] J. Gehrke, F. Korn, and D. Srivastava. On computing [12] A. C. Gilbert, Y. Kotidis, S. Muthukrishnan, and [13] G. Hulten, L. Spencer, and P. Domingos. Mining [14] H. V. Jagadish, N. Koudas, and S. Muthukrishnan. [15] E. Keogh, S. Lonardi, and B. Y. Chiu. Finding [16] J. Kleinberg. Bursty and hierarchical structure in [17] Y. Matias, J. S. Vitter, and M. Wang. Wavelet-based [18] H. Samet. The quadtree and related hierarchical data [19] C. Shahabi, X. Tian, and W. Zhao. Tsa-tree: A [20] J. S. Vitter and M. Wang. Approximate computation [21] M. Wang, T. M. Madhyastha, N. H. Chan, [22] Y. Zhu and D. Shasha. Statstream: Statistical
