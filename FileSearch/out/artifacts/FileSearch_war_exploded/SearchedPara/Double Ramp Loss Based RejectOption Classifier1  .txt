 D classifier h ( f ( x ) , X  ) is formed as: ously. The performance of this classifier is evaluated using L is is 0.5). Thus, we always take d  X  (0 ,. 5).
 To learn a reject option classifier, the expectation of L empirical risk minimization principle is used. The risk under L Double hinge loss L DH (see Table 1 ) is another convex surrogate for L L  X  d  X  1 provided  X &lt; 1  X  d and L DH forms an upper bound to L  X   X  ( for L 0  X  d  X  1 and overcomes many of the issues of convex surrogates of L (2) L DR requires no constraint on  X  unlike L GH and L DH be easily kernelized for dealing with nonlinear problems. the paper with the discussion in Section 5 . regularized risk under L DR (double ramp loss). 2.1 Double Ramp Loss L
DR ( f ( x ) ,y, X  )= Parameter  X  defines the width of the rejection region. Fig. 2 shows L d =0 . 2 , X  = 2 for different  X  .
 Theorem 1. (i) L DR  X  L 0  X  d  X  1 ,  X   X &gt; 0 , X   X  0 . (ii) lim L  X  d  X  1 ( f ( x ) , X ,y ) . (iii) In the rejection region, yf ( x ) L ( f ( x ) ,y, X  )= d (1 +  X  ) ,aconst.(iv) L DR  X  (1 +  X  ) , When  X  =0 , L DR is same as  X  -ramp loss ([ 11 ]). (vi) L function of ( yf ( x ) , X  ) .
 L does not put any restriction on  X  for it to be an upper bound of L 2.2 Risk Formulation Using L DR Let
S = { ( x n ,y n ) ,n =1 ...N } be the training dataset, where x { X  1 , +1 } ,  X  n . As discussed, we minimize regularized risk under L a reject option classifier. In this paper, we use l [ w ramp loss is
R (  X  )= condition on  X  is required due to the following lemma. Lemma 1. At the minimum of R (  X  ) ,  X  must be non-negative. Proof. Let  X  =( w ,b , X  ) minimizes R (  X  ), where  X  &lt; 0. Thus Consider  X  =( w ,b ,  X   X  ) as another point.
 function of t [ 11 ]. Since  X  &lt; 0, thus, y n f ( x n )+  X  &lt;y implies L ramp ( y n f ( x n )+  X  )  X  L ramp ( y n f ( x n 0 R (  X  ). Thus, at the minimum of R (  X  ),  X  must be non-negative.
R 1 (  X  )=
R 2 (  X  )= convexity property of R 2 (  X  ) as follows.
 where  X  ( l ) is the parameter vector after ( l ) th iteration, R (  X  iteration, the DC program reduces the value of R (  X  ). 3.1 Learning Reject Option Classifier Using DC Programming with  X  =  X  (0) .Given  X  ( l ) , we find  X  ( l +1) as where  X  R 2 (  X  ( l ) ) is the subgradient of R 2 (  X  )at  X  where described in Eq. ( 5 ) as follows, We rewrite P ( l +1) as where  X  =[  X  1  X  2 ... X  N ] T and  X  =[  X  1  X  2 ... X  N ] T problem D ( l +1) of P ( l +1) is as follows.

D where  X  =[  X  1  X  2 ...... X  n ] T and  X  =[  X  1  X  2 ...... X  At the optimality of P ( l +1) , w can be found as w = N n To find b ( l +1) and  X  ( l +1) , we consider x n  X  SV ( l We already saw that We solve the system of linear equations corresponding to sets SV 3.3 Summary of the Algorithm  X  (0) . In any iteration ( l ), we find  X  ( l ) n , X  ( l ) n D  X  have found  X  ( l +1) .Using  X  ( l +1) , we now find  X  ( l More formal description of our algorithm is provided in Algorithm 1 . Algorithm 1. Learning Reject Option Classifier by Minimizing R (  X  ) 3.4  X  and  X  at the Convergence of Algorithm 1 any x n , only one of  X   X  n and  X   X  n can be nonzero. We observe that parameters [  X   X   X  see that x n for which y n f ( x n )  X  (  X  +  X ,  X  )  X  (  X  (  X   X   X   X  in Fig. 3 . 400 points are uniformly sampled from the square region [0 1] 80 points inside the band (width=0.225) around the separating surface. 4.1 Dataset Description from UCI ML repository [ 2 ]. 1. Synthetic Dataset : Let f 1 and f 2 be two mixture density functions in where U ( A ) denotes the uniform density function with support set A .We using the hyperplane with w =[1 0] T and b = 0. We choose 10% of these points uniformly at random and flip their labels. mation about the Ionosphere. There are 34 variables and 351 observations. 3. Parkinsons Disease Dataset [ 2 ]: This dataset is used to discriminate people with Parkinsons disease from the healthy people. There are 195 fea-ture vectors with each vector having 22 features. 4.2 Experimental Setup In the proposed L DR based approach, for solving the dual D we have used the kernlab package [ 9 ]in R . We thank the authors of L we use RBF kernel. In our approach, we set  X  =1. C and  X  (width parameter for RBF kernel) are chosen using 10-fold cross validation. 4.3 Simulation Results For every value of d , we find the cross validation risk (under L L
DH based approach. Also, the reject region found by L DR based approach is the points.
 following: 1. We see that the proposed L DR based method outperforms L lesser rejection rate compared to the L DH based approach. reject option classifier. L DR gives tighter upper bound for L
