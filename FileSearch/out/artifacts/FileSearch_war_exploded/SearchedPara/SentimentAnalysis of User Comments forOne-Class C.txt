 User-generated texts such as reviews, comments or discus-sions are valuable indicators of users' preferences. Unlike previous works which focus on labeled data from user-con-tributed reviews, we focus here on user comments which are not accompanied by explicit rating labels. We investigate their utility for a one-class collaborative ltering task such as bookmarking, where only the user actions are given as ground truth. We propose a sentiment-aware nearest neigh-bor model (SANN) for multimedia recommendations over TED talks, which makes use of user comments. The model outperforms signi cantly, by more than 25% on unseen data, several competitive baselines.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval| Information ltering Sentiment Analysis; One-Class Collaborative Filtering
Many problems in collaborative ltering (CF) such as so-cial bookmarking, news and video recommendations make use of binary user ratings in terms of `action' or lack thereof, i.e. `inaction'. The difficulty of such one-class CF prob-lems [6] comes from the lack of a negative class: it is inher-ently unsure whether user inaction means that an item was not seen or was not liked (hence not bookmarked). In this paper, we study the one-class CF problem of lecture recom-mendation over TED talks. We show how to infer additional user ratings by performing sentiment analysis (SA) of user comments and integrating its output in a nearest neighbor (NN) model. The resulting SANN model outperforms sev-eral competitive baselines, is robust to noisy SA results and improves its performance with the number of comments.
Work on inferring user ratings has mostly focused on re-views with explicit ratings [3, 11, 4]. Here, however, we show that unlabeled comments can also be leveraged to improve recommendations, in a challenging one-class setting. Other studies have used comments to enrich user pro les for news recommendation, but without attempting sentiment analy-sis and the inference of item ratings [9, 5].
The rst stage of our proposal is the sentiment classi -cation of user comments, with two possible labels: positive ( pos ) and negative ( neg ). Given the lack of ground-truth labels, we focused on dictionary-based methods and specif-ically on an extension of the rule-based sentiment classi er presented in [10] and implemented in [7] 1 . The classi er uses the MPQA polarity lexicon and can deal with negation, intensi ers, and polarity shifters. The rule-based classi er estimates the polarity of a sentence as a positive or negative numerical value. This value determines the sentiment label, pos or neg of the sentence; for zero, the neutral label ( neu ) is applied. The polarity of a comment is the sum over the polarities of the sentences that compose it, and its label ( pos or neg ) is given by the sign of the total.

To test the sentiment analysis component, we performed human labeling of a subset of the TED comments, with pos , neg or neu polarity labels (the latter included also undecided cases) 1 . Six human judges annotated 160 com-ments with 320 sentences, randomly selected from the TED data, with some overlap to assess agreement, using Fleiss' kappa ( ) metric. We obtained 260 labels for sentences and 135 for comments (excluding neu ). Among these, 61 sen-tences and 29 comments were common across annotators, and agreement was found to be, respectively, = 0 : 834 and = 0 : 650. As agreement was substantial, we used the entire set as ground truth to evaluate automatic sentiment analysis (cases of disagreements were reconciled by a majority vote).
The results of our rule-based classi er (RB) and of a ran-dom baseline (Rand) are shown in Table 1. Our system reaches F-score of 74 : 90% and 72 : 60% on sentences and re-spectively comments for the classi cation task (plus mod-erate agreement with the ground truth of = 0 : 53 and = 0 : 43), a level that is sufficient to improve the one-class recommendation task, as we will show.
The one-class collaborative ltering problem can be for-malized as follows. Let U be the set of users of size j U and I the set of items of size j I j = N . The matrix of user-item ratings is R (of size M N ), with r ui = 1 indicating an h ttps://github.com/nik0spapp/unsupervised se ntiment `action' rating (favorite item) and r ui = 0 an `inaction' one (not seen or not liked). Our goal is to predict the preference of the users in the future, therefore (as in previous studies) we hide for evaluation a certain proportion of `1' values per user and measure how well we predict them.
Nearest neighbor (NN) models are often used in collabo-rative ltering and have been proven quite effective despite their simplicity [2]. Here, we use item-based neighborhood models which are described by Eq. 1. The prediction func-tion ^ r ui estimates the rating of a user u for an unseen item i . It relies on the bias estimate b ui of the user u for vari-ous items j (given in Eq. 2) and on a score computed using the k most similar items to i that the user u has already rated, i.e. the neighborhood D k ( u ; i ). The denominator is a normalization factor.
The bias estimate b ui is computed as the sum of the aver-age ratings of items in a given dataset, the average rating b u of a user u and the average rating b i for a given item i . The coefficient d ij is the similarity between item i and item j (Eq. 2) and is computed using the similarity s ij between items i and j , weighted by the normalized importance of the number of common raters n ij (close to 1 if n ij  X  ). We determine the optimal values of and of the neighborhood size k using cross-validation.
The similarity s ij can be computed using a function such as cosine similarity or Pearson's correlation between vectors representing i and j in the N N co-rating matrix derived from R . However, given that ratings are binary and not real-valued, the biases should not be computed linearly, but using the formulas proposed below, which take into account the number of the items and the maximally rated items.
We propose a sentiment-aware nearest neighbor model (SANN) which integrates into a NN model the preferences of the users that are extracted from user-generated text by sentiment analysis. In order to achieve that, the model in Eq. 1 must be modi ed as follows: rstly, the neighborhood D k ( u ; i ) must account for the additional training data, and secondly, the rating function r uj must map the output of sentiment analysis over comments to rating values that are understandable by the model. Thus, we modify the Eq. 1 of the traditional neighborhood models as follows:
In this equation, D k c ( u ; i ) is the neighborhood of the k most similar items that the user has already rated or com-mented and r  X  uj is the result of the mapping function that accounts for both explicit ratings and those inferred from comments, de ned as follows: c uj is a mapping function which maps the polarity level of a comment C j of user u to a rating understandable by the SANN model. We will compare three such mapping func-tions, formally de ned in Table 2: two of them, noted `rand-SANN' and `SANN', generate 3-way ratings (1, 0, -1) from the random (Rand) and respectively the rule-based (RB) sentiment classi ers, while the third one, noted `polSANN', generates a polarity value between -1 and 1 by combining the polarity scores of the sentences that constitute a comment. For all the three functions, the pos class has a positive effect on ^ r ui , while neu and neg classes have a negative effect. An optimal function (per user or global) could also be learned from the data, in future work.

Moreover, the additional training data from commented items are considered for the creation of the co-rating matrix ( N N ) used for the similarity s ij in Eq. 2.
 T able 2: Three mapping functions for the SANN model. Notations: sign ( C j ) is the sentiment of com-ment C j (1 for pos , 0 for neu and -1 for neg ); pol ( s ) returns the polarity of a sentence s ; and z j is a nor-malization factor over all comments C u of a user u , de ned as z j = 1 = (1 + j C u jjf C s : t : C 2 C u ^ sign ( C ) = sign ( C j ) gj ) .
To evaluate our models on the one-class problem, we focus on multimedia recommendations on the TED dataset [8] 2 . TED (www.ted.com) is a popular online repository of public talks and user-contributed material (favorites, comments) under a Creative Commons license. We crawled the TED dataset in September 2012 and gathered data from 74,760 users and 1,203 talks, with 134,533 favorites and 209,566 comments. According to our RB classi er, 63% are positive, 27% are negative and 10% are neutral comments.

For this study, we selected users with at least 4 favorites (5,657 users) and included only comments that appear in the rst level of the commenting area, i.e. excluding all the replies to other comments, because the target of their po-larity judgment is uncertain (comments on comments rather h ttps://www.idiap.ch/dataset/ted than on the talk). The resulting data set has 129,633 ratings (favorites) and 20,792 comments (see Table 4).
For evaluation, we split the dataset into a training set (80%) and a testing set (20%). More precisely, for each user we keep 80% of their positive ratings (`1' values) for training and hold out 20% for testing. Furthermore, we divide the test set in two subsets based on comment sparsity: a sparse and a dense one (see Table 4). For the sparse set, we keep all users with at least 12 ratings and for the dense one, all users with at least 12 ratings and one comment. We optimize the parameters on the training set using 5-fold cross-validation. When training, we use all users to obtain good approximations of item similarities ( s ij in Eq. 2). S parse held-out 17 ,227 8, 728 2, 409
We evaluate the SANN model, comparing it with several baselines, and show the following: (i) the use of sentiment analysis of comments as additional training data improves performance over competitive baselines; (ii) the RB senti-ment classi er is the reason for the improvement as com-pared to a random classi er; and (iii) performance increases with the number of comments.We present rst the parame-ter selection using cross-validation (5.1), and then we eval-uate the best con gurations on the two held-out sets (5.2).
Both NN models and SANN models rely on two parame-ters: the rst one is the k parameter (the neighborhood size) and the second one is the parameter (the shrinking factor). For the selection of k , we xed = 60 for all models. For the selection of , we then xed k to the optimal values that were obtained for each model. Moreover, two other options can be selected, namely the similarity function (Pearson's correlation (PC) or cosine similarity (COS)) and the use of normalization in Eq. 1 (noted as `norm').

In Table 3, we present the results of 5-fold cross-validation on the training set for six different models, namely: TopPop-ular baseline which provides xed recommendations based on the popularity of items, NN(COS), normNN(PC), NN(PC), SANN(COS) and SANN(PC). We also experimented with other normalized NN models, but as their results were infe-rior, we do not discuss them here.

Figure 1 displays the effect of the neighborhood size k on the performance of the models. All models perform consid-erably better than the TopPopular, as expected from CF Fi gure 1: The effect of the neighborhood size k on MAP values at 50, using 5-fold cross-validation. models. The best performing model over all k values is the SANN(PC) model (with signi cance at the p &lt; 0 : 01 level). Both NN and SANN models stabilize their performance as k increases, which conforms to the theory of stability for k-nearest neighbor algorithms [1]. The decrease in perfor-mance of SANN as k increases is due to the inclusion of noisy ratings in the training data (along with ground-truth ones), while the performance of NN increases with k as no noise is included in this case. For the evaluation on the held-out sets (next section), the following values were selected based on cross-validation: k = 1 ; = 7 for the SANN(PC) model and k = 28 ; = 19 for the NN(PC) model.
We compare the performance of the SANN model with the best performing baseline NN model (NN(PC)) and with the TopPopular baseline on the two held-out sets. We con-sider the two variants of the SANN model, namely rand-SANN(PC) and polSANN(PC) (also with k = 1 and = 7) to demonstrate the value of aggregated polarities.
Figure 2 displays performance on the dense held-out test set. The SANN model performs signi cantly better than the baseline models on unseen data. The ordering of the methods based on performance is the same as in the cross-validation experiments. The randSANN(PC) model per-forms slightly better than the NN(PC) model (the difference is not signi cant) but considerably worse than SANN(PC) and polSANN(PC), demonstrating the utility of sentiment analysis over comments.
 Table 5 shows the results on both held-out test sets with MAP, MAR and MAF scores. The polSANN(PC) model outperforms signi cantly the other models, showing a rela-tive improvement of 26.8% on mean average f-metric (MAF) on the dense held-out set and 11.4% on the sparse held-out set. When averaged over MAP values from 1 to 50, the improvement is signi cant at the p &lt; 0 : 01 level (t-test). of the polSANN model over the best NN model.
 Fi gure 2: Method comparison in terms of average precision (AP) and recall (AR) (1 to 50).

The nal experiment concerns the learning ability of the algorithm in relation to the proportion of comments used for training. Figure 3 displays the performance of two SANN models on the dense held-out set, when varying the propor-tion of training comments for each user. The increase in the proportion of comments leads to a clear increase in perfor-mance. When fewer than 20% of the comments are taken into account, the models perform similarly to the NN(PC) model for neighborhood size k = 1 (see also Fig. 1).
The proposed SANN models were shown experimentally to be able to overcome some difficulties of the one-class col-laborative ltering task and to outperform signi cantly sev-eral competitive baselines. Moreover, they demonstrated robustness to noise and exhibited appropriate learning abil-ities with respect to the amount of available user-generated text. The results of this study suggest several directions for future work: (i) extending the results to other datasets; (ii) learning the optimal mapping function from sentiment analysis scores to ratings; and (iii) generalizing the proposed approach to more advanced recommendation models. The work described in this article was supported by the European Union through the inEvent project FP7-ICT n. 287872 (see http://www.inevent-project.eu ). [1] O. Bousquet and A. Elisseeff. Stability and [2] P. Cremonesi, Y. Koren, and R. Turrin. Performance [3] G. Ganu, N. Elhadad, and A. Marian. Beyond the Fi gure 3: SANN models' performance (MAP at 50) when varying the number of comments for training. [4] C. Leung, S. Chan, F.-L. Chung, and G. Ngai. A [5] A. Messenger and J. Whittle. Recommendations based [6] R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose, [7] N. Pappas, G. Katsimpras, and E. Stamatatos. [8] N. Pappas and A. Popescu-Belis. Combining content [9] J. Wang, Q. Li, Y. P. Chen, and Z. Lin.
 [10] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing [11] W. Zhang, G. Ding, L. Chen, and C. Li. Augmenting
