 Many important application areas of text classi fi ers demand high precision and it is common to compare prospective so-lutions to the performance of Naive Bayes. This baseline is usually easy to improve upon, but in this work we demon-strate that appropriate document representation can make outperforming this classi fi er much more challenging. Most importantly, we provide a link between Naive Bayes and the logarithmic opinion pooling of the mixture-of-experts framework, which dictates a particular type of document length normalization. Motivated by document-speci fi cfea-ture selection we propose monotonic constraints on docu-ment term weighting, which is shown as an e ff ective method of fi ne-tuning document representation. The discussion is supported by experiments using three large email corpora corresponding to the problem of spam detection, where high precision is of particular importance.
 Algorithms high precision text classi fi cation, Naive Bayes, low false pos-itive rates, email spam detection H.4.m [ Information Systems ]: Information Retrieval; D.2 [ Machine Learning ]: Text Categorization
Practical classi fi cation problems in the text categorization (TC) domain often involve sharp constraints with respect to the precision, false-positive or false-negative rates. While the overall accuracy is important, a spam fi ltering solution, for example, may be unacceptable if it destroys or misde-livers legitimate email as spam even at a small rate. In a cost-sensitive classi fi cation framework, one can often see signi fi cantasymmetryinthewaydi ff erent misclassi fi cation mistakes are weighted. In this context, classi fi cation sys-tems are bene fi cial only as long as the probability of certain types of errors is su ffi ciently low. Although a large amount of research has been devoted to improving the quality of text classi fi cation techniques, the focus has been primarily on the overall accuracy or global quality metrics such as the error rate, F-measure, precision-recall break-even or area under the ROC curve. Methods that improve the global metrics may also improve classi fi cation performance in the region of high speci fi city, but they are rarely investigated in this context.

In this work we focus on the aspects of document represen-tation, and in particular on the impact of document sparsity, term weighting and length normalization in problems de-manding high speci fi city. We concentrate on the important case of Naive Bayes [23], which is a highly scalable learner of great practical importance, and for which a number of recent improvements have been proposed, making it quite compet-itive with more complex techniques such as SVMs [29, 19].
In the context of the Naive Bayes (NB) classi fi er, it has been suggested that post-induction document-speci fi cfea-ture selection tends to outperform traditional document-independent approaches in applications with low tolerance for false positive errors [20]. We show that these bene fi ts can be improved by making document-speci fi c feature selection part of the induction process. We also extend the concept of local sparsity control to the area of term weighting. We provide evidence that the soft approach (i.e., downweighting of less relevant features rather than their complete elimina-tion) to the reduction of active document terms can provide substantial improvements to classi fi cation performance and in our experiments it tends to outperform hard feature selec-tion. When the soft and hard feature selection approaches are combined, feature elimination leads only to marginal im-provements.

Document length normalization provides a mechanism for controlling the in fl uence of any particular term on a docu-ment by document basis. Although it has been used widely with other text classi fi ers, its use with Naive Bayes is very recent [29] and not particularly well understood. In this work, we show that certain types of length normalization cast Naive Bayes into the mixture-of-experts framework, which provides a solid basis for this type of transformation and also explains its e ff ectiveness for this classi fi er. Based on our analysis it becomes apparent that for Naive Bayes, L 1 normalization may be more appropriate than the tra-ditional L 2 normalization, which is also supported by the experimental fi ndings.

In our experiments with several email collections we demon-strate that with appropriate document representation, Naive Bayes can compete and even outperform state-of-the-art learners such as Logistic Regression and Support Vector Ma-chines. This is particularly true for datasets with some de-gree of class noise, which is quite typical in practical appli-cations of text mining. These improvement in performance of NB do not take away its attractiveness in terms of speed of learning and ease of implementation.

The paper is organized as follows: In Section 2 we discuss why NB does not perform well in the text classi fi cation tasks that require high precision. We then investigate three di-rections of improving NB in this setting  X  document-speci fi c feature selection at induction time (Section 3), L 1 document length normalization linked to logarithmic opinion pooling (Section 4) and combined supervised and unsupervised fea-ture weighting (Section 5). The experimental setup is pre-sented in Section 6. As shown in Section 7, all these tech-niques improve the NB performance and, in addition, can be combined and yield even better results as demonstrated by our experimental results. Related work is reviewed in Section 8 and the paper is concluded in Section 9. Consider a binary classi fi cation task de fi ned over domain X , where based upon a training set theobjectiveisto fi nd a mapping f : X  X  C, C such that its expected loss is su ffi ciently low. The de fi nition of loss can be application speci fi c and often is taken to be the error rate. In many problems, the misclassi fi cation costs are asymmetric and in some cases the cost of one type of error can be high enough to demand very low, or even near zero, probability of occurrence. In some web search applications, for example, it is required that the top-N results returned of a user query have very high precision even if this signi fi cantly restricts the number of potentially relevant responses that can make it to top-N . In spam detection, users have low tolerance for false positive errors and accept email fi ltering solutionsaslongasthechanceoflosingsomeimportant email communications is negligibly low. If misclassi fi cation cost values and accurate estimates of posterior probabilities are available, optimum decisions can be made by setting the decision threshold in the probability space to minimize the expected misclassi fi cation cost [10]. Due to the practical problems in obtaining these, it is often convenient to work with the Neyman-Pearson criterion by setting the limit on the maximum acceptable false-positive rate or alternatively on the minimum acceptable precision.

Typically, a classi fi er returns a score proportional to its  X  X on fi dence X . In the case of NB, the score is computed as where the constant term captures the e ff ect of class priors (which can be ignored if classi fi cation threshold is chosen based on a validation set). For the multinomial variant of NB [24], typically used in text applications, the summation in Eq. (1) is carried out over the terms present in document d (as opposed to all possible terms) and the value of f i responds to the frequency of occurrence of term t i in d .The occurrences of terms in d are assumed to be independent given the class label and the class conditional probabilities P ( t j | C ) are estimated as where f ij  X  0 is the number of occurrences of term t j in document d i and V is the vocabulary size. In Eq. (2) the Laplace technique is applied to smooth the probability es-timates. The multinomial model was extended in [29] and [19], whereby the values f ij no longer have to correspond to in-document frequency but to a function thereof. In par-ticular, in [29] it was suggested to map f ij to a real-valued tf  X  idf weight and additionally normalize these features on a per-document basis so that the L 2 norm of each feature vector is one.

During the tuning process of a classi fi er, a threshold is cho-sen such that decisions with scores exceeding the threshold are classi fi ed as  X  X ositive X . In this context, the classi fi er X  X  inability to perform well at a low enough false-positive rate can be seen as evidence of its overcon fi dence, whereby er-roneous decisions are made with apparent high con fi dence. While this behavior can be observed in many learners, it is particularly common for Naive Bayes (NB), due to its as-sumption of conditional independence of features given the class label. Although NB can have a reasonably low error rate [7], in some cases feature inter-correlations get com-pounded resulting in overcon fi dent predictions. As shown in [1, 20], this is particularly true for long documents, which is a reason why feature selection can have a strong positive e ff ect for this type of classi fi er.

Since feature inter-correlations are at the heart of the over-con fi dence problem for NB, a number of modi fi cations to the learner have been proposed aiming to detect and counter such e ff ects [11, 36]. Unfortunately, these adjustments tend to increase the complexity of model induction to a large extent, which eliminates the key advantage of NB in prac-tical applications, i.e., its scalability and ease of implemen-tation. In some instances, however, the proposed improve-ments have been shown to signi fi cantly improve NB X  X  perfor-mancewithnegligibleincreaseinclassi fi er complexity. We focus on the ones proposed in [20] and [29], which advo-cate the use of document-speci fi c feature selection as well as tf  X  idf term weighting coupled with document length normalization, respectively.
Supported by psychological evidence [22], document clas-si fi cation can be done fairly accurately by looking at only a small portion of the text [31]. Indeed, previous work on document-speci fi c feature selection (DSFS) [20], which uses only a small set of  X  X mportant X  words in a document, has shown to improve NB X  X  performance signi fi cantly [15], espe-cially in settings with highly skewed misclassi fi cation costs. However, the method described in [20] was applied post-induction and thus was not able to take full advantage of the DSFS process. While being much simpler operationally (the selection of the optimum feature count can be applied with-out the potentially expensive retraining of the classi fi er), the technique might be suboptimal for some learners since they do not get a chance to induce a model over the reduced doc-ument representation, although as shown in [20][15] it works quite well for Naive Bayes.

We propose to naturally extend the DSFS process so that it a ff ects classi fi er induction. We hypothesize that by doing so, the method might not only be more suitable for discrim-inative learners such as SVMs, but also more e ff ective for Naive Bayes itself. The original and modi fi ed document-speci fi c feature selection process are compared below. Post-hoc Full Induction 1. Train a classifier 1. Train a classifier 2. Rank feature weights 2. Rank feature weights 3. Use top-N features 3. Retain top-N features
DSFSreliesonthechoiceofasinglecut-o ff parameter for all documents, regardless of their length and content. While this can be seen to regularize Naive Bayes, it may be suboptimal for many documents, for example those contain-ing more numerous strongly relevant terms than suggested by the cut-o ff threshold. One possible way to address this problem is to consider soft document-speci fi c term weight-ing instead of hard feature selection, which is decided by the term frequency and a prede fi ned cut-o ff threshold. This issuewillbeexaminedinmoredetailinSection5.
While  X  X ure X  versions of the Naive Bayes classi fi er may perform poorly when faced with large volumes of high di-mensional data, many improvements and modi fi cations have been suggested, which make Naive Bayes competitive with state-of-the-art discriminative learners. In one signi fi cant improvement of NB, Rennie et al. [29] proposed to use tf  X  idf feature weighting as well as L 2 document length normal-ization to improve the performance of the classi fi er in text applications. In this section, we advocate instead using an L 1 norm, which can be linked to the logarithmic opinion pooling framework of combining experts judgements.
The document representation based on using an L 2 norm over the tf  X  idf features has been widely used in Information Retrieval (hence we will denote a NB using it as NB-IR), as well as for text classi fi ers such as Rocchio or linear SVMs [9]. While it appears to help in improving Naive Bayes per-formance, the justi fi cation for its use is rather weak. Using L 2 norm makes sense in IR where document similarity is often expressed in terms of the cosine of the angle between document vectors (which for vectors having unit L 2 norm is equivalent to their dot product). For Naive Bayes, length normalization reduces the in fl uence of long documents in NB parameter estimation, but it is not clear if L 2 norm if best forthispurpose. Herewepresentanargumentputtingits use in the mixture-of-experts context.

Let us assume that a document d contains N terms. In the two-class context, the odds of the document belonging to the class C as opposed to C are computed by multinomial Naive Bayes as p ( C | d ) p C | d where f i is the number of occurrences of term t i in d ,with S i f i = N .Underthe tf  X  idf weighting and L 2 length-normalization transform of [29], the formula is changed to: p ( C | d ) p C | d where for each term t i in a document, z i is its normalized tf  X  idf weight factor, so that z i  X  0 and
Let denote the odds of term t i belonging to the target class rather than the anti-target. Through this reformulation, we can express Eq. (3) as and if the document-length normalization based on the L 1 norm instead of L 2 ,thenwehave Thus the posterior odds for a document are a weighted geo-metric mean of the term-based odds for terms contained in adocument. Thistypeofformulahasbeenwidelyusedto combine probability distributions in the mixture of experts framework and is known as logarithmic opinion pooling [16]. Under this interpretation, the terms found in a document are considered as possibly correlated  X  X xperts X , whose opinions are pooled or aggregated. The term weight z i corresponds to the relative reliability of expert i . If all experts are con-sidered equally reliable, the posterior probability of the clas-si fi er is computed as the geometric mean of the term-wise posteriors. In the log space this is equivalent to taking the arithmetic average of log-odds weights rather than just their sum as usually done for Naive Bayes. Note that unlike NB, in mixtures of experts there is no assumption of the experts X  mutual independence conditioned on the class label. Indeed, much research has been devoted to derive weight values z i that are able to take expert inter-correlation into account . Since odds are a measure of classi fi er con fi dence, taking the mean of individual opinions has the advantage that it can-not exceed the maximum of component odds. In practical terms, a document that contains only ambiguous terms can-not result in a highly con fi dent decision. This is in contrast to the regular Naive Bayes where the compounding e ff ect of many weakly positive or negative features may give the appearance of very high overall con fi dence. In the follow-ing, we will denote the mixture-of-experts variant of NB as NB-MX.

Di ff erent term weighting methods besides the in-document term frequency have been studied extensively in the Infor-mation Retrieval community, but have not been fully studied in the context of NB research. In this section, we introduce several term weighing mechanisms including the traditional unsupervised methods such as tf  X  idf and also supervised methodssuchasfeatureweightslearnedbymodelsinthe previous stage. We also investigate approaches that improve these term weightings. In particular, we study the e ff ect of combining the supervised and unsupervised approaches, and also proposed a monotonic term weighting transformation using a parameterized softmax function.
Term weighting has been widely used in Information Re-trieval and Text Categorization (TC). Typically, each term appearing in a document receives a positive weight, which is supposed to emphasize attributes of likely importance and de-emphasize common and irrelevant ones. Some of the most widely used techniques are based upon multiplicative combining of the in-document frequency with the inverse document frequency of the term in the training collection. Note that such measure of term importance does not take into account the class information, which may be relevant to the categorization task. It was suggested that for TC problems it may be advantageous to use supervised term weighting schemes [6] that derive the weight from functions used in ranking features for selection, such as Information Gain (IG) or  X  2 . Although the bene fi ts were shown for some datasets, it appeared that overall such supervised weighting schemes were quite comparable to the ones based on the tra-ditional tf  X  idf approach. In related research, [33] suggested a novel term weighting measure that was shown to improve more consistently over tf  X  idf and methods investigated in [33] for SVM and KNN classi fi ers.

Recently, it was observed that the ranking of features de-rived from absolute weight values of certain linear classi fi ers can be competitive or superior to traditional feature rank-ing function such as IG, especially when applied in the con-text of the same learning algorithm [26]. In particular, the feature ranking induced by a linear SVM has showed good performance in that regard. This suggests that the classi fi er itself can be e ff ective at deriving feature ranking. This was found to be the case for NB, where the ranking of features according to the absolute weights assigned to them by the classi fi er was shown to outperform IG-based ranking in DSFS [20]. Although the use of ALO type weights for feature ranking in NB is rather new, it has been known that the related asymmetric the odds-ratio (OR) criterion works in terms of selecting relevant features for NB in text categorization [27, 19]. In this work we will use the ALO measure (4) as the supervised component of a term weight-ing function for NB.
Giventhatsupervisedandunsupervisedtermweighting approaches are based on di ff erent types of information, it is natural to ask whether instead of using one to the exclusion of the other, a better overall weighting might be achieved by combining them together. We chose ALO (Eq. (4)) as the supervised component of a term weighting function, while retaining idf as the unsupervised component, i.e., Other term weighting functions could be used in place of ALO and idf and, indeed, more than two measures might be incorporated into an aggregate term weighting function that combines several measures of reliability. We consider a multiplicative scheme that, given N weighting functions, computes the combined term weight for term i in document d as where f j ( x i ,d ) &gt; 0 is the term weight assigned by the j -th function.

Computing term weights (Eq. (5)) requires a two-step in-duction process similar to the one presented in Section 3, i.e., 1. A NB-MX Naive Bayes classifier is built using 2. The absolute weight values are incorporated 3. A second NB-MX classifier is built Note that one can also consider performing DSFS and term weighting at the same time. Also, the process described above could continue beyond the fi rst two models, with a compounding e ff ect of importance weights produced by the consecutive classi fi ers. It seems likely, however, that the weights of the individual NB classi fi ers will be highly corre-lated, thus providing little rational for continuing with the procedure beyond the fi rst two models.
If document length normalization is not performed, for linear classi fi ers the score function is symmetrical with re-spect to classi fi er weights and term weights, i.e., Usually the term weights are fi xed and the parameter vec-tor, w , is optimized. However, one could also reverse their roles and, for a fi xed w , optimize the term weights, espe-cially when the in-document term frequency is not taken to be part of the weighting function. This indeed was suggested by several researchers in the context of Naive Bayes, where the parameter weights are inexpensive to compute. While typically term weights are assumed to be non-negative, this requirementwasdroppedin[17]and[12]. In[12],alin-ear SVM was applied to optimize the term weight vector, while in [17] the use of Logistic Regression was proposed. In essence, Naive Bayes is used here as a  X  X erm weighting X  func-tion for the more expensive algorithms, although since the term weights can take negative values this is an  X  X northo-dox X  method of applying term weights. As such, these meth-ods need to be evaluated from the standpoint of whether NB term weighting provides a better or worse performance for the target algorithm (e.g., linear SVM) when compared to thenativedocumentrepresentationoranalternativeform of term weighting. Also, because of the potential negativity of weights and lack of length normalization, the mixture of experts analogy can no longer be made.

Document length normalization introduces nonlinearity that breaks the symmetry between term weights and classi-fi er weights in Eq. (7). It also makes term weights document-speci fi c, while maintaining their relative relationship, i.e., the ratio of any two weights before and after normalization remains the same. Joint optimization of classi fi er weights and term weights is possible, but is generally di ffi cult due to the size of the parameter space. Term weighting works with many more choices than feature selection, which itself is a hard problem. We note, however, that the set of potential choices can be meaningfully constrained by the initial choice of the term weighting function. The good performance of feature selection functions in supervised term weighting sug-gests that these functions are not only useful in determining feature ranking but also in their relative importance. It is also possible that the importance values are suboptimal for a given classi fi cation task. Notice that any two feature se-lection functions that rank terms in the same order may behave di ff erently when considered as term weighting func-tions. In particular, they may have di ff erent steepness as a function of rank, with steeper functions highly emphasiz-ing the strongest terms and being analogous to aggressive document-speci fi c feature selection. In contrast, fl atter func-tions will favor document classi fi cation with signi fi cant con-tribution from a larger set of a document X  X  features, which is analogous to mild document-speci fi c feature selection. For any two equivalent ranking functions, it is di ffi cult to stipu-late apriori which one is more suitable for term weighting in a particular learning method, which suggests that their quality needs to be assessed via classi fi cation performance of the resulting classi fi er.

Let the initial ranking of features be Assuming that ranking of features is maintained, one can thus formulate the search for optimum feature weighting, as fi nding a set of values such that the performance of a given learning method built over such document representation is maximized. Even with such monotone constraints, optimizing for both classi fi er pa-rameters and term weights may be di ffi cult. We therefore consider a parameterized monotonic transformation of the original term weights for which the best parameter settings can be easily deter-mined using a validation set.

For a fi xed ranking function one can consider a parame-terized monotonic transformation of x that preserves term ranking, but also allows one to control the steepness of the mapping via parameter  X  &gt; 0 . In this work we use a para-meterized version of the softmax function.

Given a set of values { x i : i =1 ..N } , softmax transforms them as This normalization is applied on a per-document basis. For large values of  X  , Eq. (8) approximates the case of classifying with just a single  X  X ost important X  feature, while for low values of the parameter is equivalent to treating all term weights as equal. We feel that this captures quite well the intent of document-speci fi c term weighting, although other forms of transforming the feature ranking function could be considered. Also in the context of NB applied to large amounts of data, where speed and scalability are key, it seems counterintuitive to employ an optimization process for term weighting that is more expensive than the process of inducing NB itself. In this sense, evaluating the impact of Eq. (8) over a small set of  X  is acceptable and similar in complexity to the search for optimum smoothing parameter for estimating individual probabilities.
To evaluate the impact of the modeling choices described in this work we chose three email datasets, corresponding to the spam fi ltering problem, where Naive Bayes tends to be used quite often [25]. While other TC applications also often require high levels of precision, this is particularly true in spam detection. In the previous work, researchers typically emphasize comparing the performance of di ff erent spam classi fi cation solutions in the region corresponding to low false-positive rates. The data we used corresponded to the 2005 and 2006 TREC Spam Filtering Track datasets [5, 3], as well as to a non-public Hotmail dataset, which has been used in the work of Yih et al. [35]. All three collections contained a time-sorted mix of spam and non-spam mes-sages. In the experiments we used the initial portion of each dataset for training and the remaining portion for testing. As discussed among others in [4], although cross-validation is often used to estimate classi fi cation performance in the text domain, in spam fi ltering it is not appropriate as it tends to produce overoptimistic estimates. In addition to the fact that content distribution of email in general has strong time dependence, spam detection is an adversarial problem, and spam techniques are rapidly evolving to evade the current detection techniques. The details of the datasets are given in Table 1. For the case of the Hotmail data, we used the same experimental setup as described in [35]. For the two TREC datasets, we considered training with a smaller fraction of the dataset than used for testing to bet-ter re fl ect the fact that batch-trained fi lters tend to operate over prolonged periods of time without adaptation, which may cause their performance to degrade with respect to the online fi ltering approaches.

As baselines for Naive Bayes, we used the standard multino-mial version of the classi fi er (labeled as NB), as well as a modi fi cation of Rennie et al. [29], which uses idf feature weighting and L 2 length normalization of document feature vectors (labeled here as the NB-IR). To provide comparisons for Naive Bayes performance we used Logistic Regression (LR) based on binary features and linear SVM based on idf weighted and L 2 length normalized feature vectors, which re fl ects the typical document representation used in conjunc-tion with these two classi fi ers [14, 9]. Both Logistic Regres-sion and SVMs are considered as state-of-the-art learning algorithms in the text categorization domain, and have also been applied successfully to the spam fi ltering problem [21, 8, 35]. We used the default parameter settings of SVM light and for Logistic Regression we assumed Gaussian prior of variance 1 for all features.

Spam fi lters often extract a number of specialized features capturing the domain knowledge of fi lter creators. We did not include any of such features. Instead, the email messages were represented as bag of words contained in the message bodies and subject lines. The feature selection was limited to ignoring terms that occurred fewer than three times in the training collection. The in-document frequency of each fea-ture was ignored so as to provide clearer comparison of the impact of feature-based weighting and also because for NB, ignoring term frequency can actually improve the classi fi er performance [32].

In performing document-speci fi c feature selection (DSFS), we considered limiting document sparsity to top-N features, with N in {5, 10, 25, 50, 75, 100, 150, 200, 300, 500, 1000}. In the fi rst set of experiments we assessed the di ff erence of performing post-induction DSFS vs. making it part of the classi fi er learning. As illustrated in Figure 1, training Naive Bayes with the reduced representation does have a positive e ff ect on performance. It is also apparent, however, that the optimum sparsity settings obtained via the post-induction method and the double induction method match closely. A useful heuristic might therefore consist of searching for the optimum setting using the faster post-induction method and performing re-induction of NB using the fi nal sparsity set-ting to insure better accuracy. Note that feature selection has a substantial impact on NB performance and results in large improvements over the baseline multinomial Naive Bayes.

Feature selection can be combined with feature weighting and to assess the impact of such combination we consid-ered the following weighting functions listed in Table 3. In the formulas listed, w i corresponds to the ALO weight of Eq.(4),while idf i denotes the idf weight according to where f i is the number of training documents containing term i . Although the term weighting functions can be ap-plied with and without document length normalization, in our experiments we restricted ourselves to NB-MX, i.e., nor-malization according to the L 1 measure. We will label the di ff erent variants of NB-MX used by the type of the term weighting function used, e.g., NB-MX using abs_idf term weighting will be referred to as abs_idf .Thefunctions abs and softmax_abs represent purely supervised, idf and softmax_idf purely unsupervised weighting functions, while abs_idf and softmax_abs_idf correspond to combining the supervised and unsupervised notions of feature relevance. For the softmax weighting functions, we tested the steep-http://www.cs.cornell.edu/People/tj/svm_light/ Figure 1: E ff ects of using post-induction DSFS vs having it impact the classi fi er during induction. Note that the optimum settings between the two methods are very consistent. ness parameter  X  in{0.01,0.05,0.1,1,1.5}andreported the results based on the best choice. The weighting function idf closely corresponds to NB-IR with the exception that it uses L 1 rather than L 2 document length normalization. We used L 1 normalization throughout since it better fi ts the mixture-of-experts framework as described in Section 4. The geo case is the special case of combining term-wise log odds via a simple geometric mean, which re fl ects the im-pact of document length normalization under uniform term weighting.

Following [35], we decided to evaluate classi fi er perfor-mance by focusing on the ROC analysis in the area corre-sponding to the false positive rate (FP) range of [0, 0.14]. This includes the essential region of importance for the spam fi ltering application without focusing on any particular set-ting 2 .Toprovideasingle fi gure of merit we modi fi ed the commonly used metric of area under the ROC (AUC), by normalizing the area in the section of the ROC plot so that the maximum possible value is 1. This is achieved by dividing this area by the length of the FP region of interest, i.e., FP  X  0 . 1 , which we believe brackets the constraints of the spam fi ltering application. The fi gure of merit will be called AU C 0 . 1 .
Figure 1 compares the e ff ects (in terms of AUC 0 . 1 )of document-speci fi c feature selection for NB with and without having DSFS in fl uence the induction process (see Section 3
While in personal fi ltering of email the acceptable FP range wouldbemuchlower,thedatasetusedin[35]isduetoa large and diverse user population and is a ff ected by class noise (i.e., user labeling mistakes), as well as judgement dis-agreements over messages sent to multiple recipients. This considerably extends the viable FP range.
 the NB variant is italicized. for description of the two approaches). As expected, having DSFS impact the weights of the fi nal classi fi er has a ben-e fi cial e ff ect, particularly for the Hotmail and TREC-2005 datasets. It is also apparent, however, that both methods are consistent when it comes to identifying the optimum number of features. Thus the post-induction method (as being faster) should be an attractive heuristic in identifying the optimum DSFS setting, which can then be used to train the fi nal NB classi fi er. Note that the best performance is reached for low document sparsity settings (5 and 10 fea-turesperdocument),whichsupportstheargumentsofNB performing best in for low sparsity representations [26]. In the remaining experiments, whenever DSFS is applied, it uses the full induction process.

Table 2 provides comprehensive performance comparison in AUC 10 for the learning algorithms considered. For each dataset, the left column lists the performance using all fea-tures, where the right column provides the best results achieved via DSFS. Since feature selection was not performed for the benchmark discriminative learners, for SVM and LR the right column is left blank. For NB-MX methods using the softmax variant of term weighting, the results listed corre-spond to the best results achieved over the set of  X  parame-ters considered. The rows following the LR and SVM bench-marks correspond to the variants of NB-MX using L 1 feature vector normalization and di ff erent types of term weighting (detailedinTable3). ThetoptworowsrepresenttheNB baseline and NB-IR using idf based term weighting and L 2 -based normalization.

Among the di ff erent NB variants, NB-MX performs the best, particularly using feature weights incorporating both the ALO and idf relevance measures. The straightforward relevance integration of abs_idf works very well, but in some cases it can be improved with the parametric softmax. Even with just the idf weights, L 1 -based length normaliza-tion tends to improve over L 2 -based length normalization, which suggests that the mixture-of-experts way of averaging term-wise posterior distributions has an advantage over the IR-inspired approach.
 NB-MX is very competitive with discriminative learners. While LR and SVM could also be optimized with feature weighting and selection, the comparison is still important since LR and SVM in the default con fi gurations tend to per-Table 3: Term weighting functions we considered.
 Only the normalized versions were used in the ex-periments. weighting function not-normalized L1 length normalized geo NA 1 N form very well. The softmax abs _ idf variant outperforms LR on all three datasets and SVM on the Hotmail dataset. SVM still beats its competitors for TREC-2005 and TREC-2006 but we note that even then the di ff erences between SVM and softmax abs _ idf are rather small. It is interesting that both SVM and LR performed poorly compared to NB for the Hotmail dataset. We attribute this behavior to the fact that Hotmail data is known to have a signi fi cant amount of class noise (2-5%), which discriminative learners may fi nd confusing and thus over fi t [2]. The TREC datasets on the other hand were hand-labeled and cleaned so the amount of class noise is expected to be low. We intend to further in-vestigate and con fi rm the impact class noise in future work. Table 2 indicates that the di ff erences between di ff erent NB variants due to feature weighting and normalization are more prominent than the di ff erences due to feature selection vs using all features. Indeed, in some cases NB-MX actually performed best using all features. DSFS made most impact for classi fi ers using uniform term weighting ( NB and geo ), with particularly dramatic improvements in the case NB. However, the geo variant of NB provides the most straight-forward way of improving the classi fi er X  X  performance.
While AUC 10 provides a reasonable fi gure of merit, it is informative to examine the actual ROC curves in the region of interest. For comparison, we chose the con fi gurations us-ing all features without parameter optimization. NB-MX Figure 2: ROC comparison for the Hotmail dataset. NB-MX with abs _ idf term weighting has a large ad-vantage for low FP rates. was therefore represented by the abs_idf variant. In ad-dition, NB , NB-IR , geo , LR ,and SVM are also included. The ROC curves for the individual datasets are shown in Figures 2 X 4. For TREC-2005 (Figure 3) and TREC-2006 (Figure 4), NB-MX is very close to LR, with SVM showing clear dom-inance over all others. NB-IR shows lower performance in the low-FP region but catches up at higher FP rates. For the Hotmail dataset (Figure 2), NB-MX shows clear dominance in the low-FP range, both over the discriminative learners and other NB variants. LR and SVM catch on eventually, but for this noisy dataset using a generative learner such as NB-MX is actually advantageous when high-precision spam detection is concerned.
Many researchers noted the relatively high improvement of performance of standard NB to feature selection, espe-cially in high-dimensional problems such as text. Several methods were found to be e ff ective, particularly odds-ratio, information gain, and feature ranking derived from linear SVMs [27][26]. More recently, it was suggested that in appli-cations involving highly asymmetric misclassi fi cation costs, document-speci fi c feature selection may be more bene fi cial than regular feature selection, which when too aggressive may leave some documents without useful features [20].
Logarithmic option pooling has been studied extensively as a method of aggregating probability distribution in the mixture of experts context [16]. Other approaches of com-bining probability distributions are discussed in [13]. Robin-son [30] proposed a method of improving the calibration of Naive Bayes posterior probabilities in spam detection based on meta-analysis. His formula uses unweighted geometric mean of term-speci fi c probabilities, which makes it related to applying L 1 document length normalization over uniform term-weighting. The use of geometric mean with NB has also been advocated in [32]. Figure 3: ROC comparison for the TREC-2005 dataset. NB and NB-IR lag in performance in the region of low FP rates.

In [19] several di ff erent normalization techniques were in-vestigated with Naive Bayes. However, the normalization was applied to term frequency vectors and not to term im-portance weights. Also, the normalization was de fi ned some-what loosely, without requiring that a feature vector has unitnormaccordingtoawellde fi ned metric. Nevertheless, it was found that normalization of in-document frequencies has a large positive e ff ectonNB.Inthesamework,itwas also found that term importance weighting based on risk ra-tio improves NB performance in TC, where risk ratio can be related to exp ( ALO ) .An L 1 like normalization of doc-ument feature vectors was also advocated in [18] as a way of curbing the in fl uence of long document on parameter es-timates.

In [17] and [12] the weights learned by Naive Bayes were used as term weights, and with this modi fi ed document rep-resentation a more complex learning algorithm was used to train the fi nal model without document length normaliza-tion. In [28], a related scheme was proposed, but to keep the complexity of the overall model low, the feature vector foreachdocumentwassplitintoasmallnumberofnatural components (e.g., the subject line and body of an email) over which a Naive Bayes model was trained. The scores provided by these models were used to generate a low di-mensional document representation used to train a Logistic Regression model.

For applications of linear text classi fi ers demanding low false-positive rates, Yih et al. [35] proposed to chain several models, each trained on the subset of the data that could not be classi fi ed by the previous model with high enough con-fi dence. In [35], this method was shown to be particularly e ff ective for NB. Wu et al. [34] considered building a tree of NB classi fi ers, with the root trained using all data, and other nodes conditioned on the partition of the data by their par-ent node. Alternative approaches (although not particularly e ff ective for NB [35]) to improving classi fi er performance at low FP rates involve training with costs or utilities. Figure 4: ROC comparison for the TREC-2006 dataset. NB-MX tracks closely the LR classi fi er. NB-IR performs comparably for this dataset.
Inthisworkwefocusedontextclassi fi cation problems re-quiring relatively low false-positive rates. We demonstrated that with appropriate document representation, the Naive Bayes classi fi er provides a baseline for high-precision TC that is hard to beat, even for top performers such as linear SVMs or Logistic Regression. In particular, L 1 -based fea-ture vector length normalization allows Naive Bayes to be interpreted in the mixture of experts framework, with the posterior output of the classi fi er corresponding to weighted geometric mean of the term-speci fi c posteriors. This puts the use of document length normalization for Naive Bayes on a stronger footing and indeed this type of transforma-tion appears to have a practical advantage over the tradi-tional L 2 approach. Our results show that document-length normalization plays a major role in improving Naive Bayes performance, where even the simplest form of using the geo-metric mean tends to outperform multinomial Naive Bayes by a wide margin. Both supervised and unsupervised term weighting functions improve these results further, while the combination of these two approaches yields the best over-all results. We fi nd this indicative of the value of combin-ing di ff erent feature relevance measures in term weighting. While in this work we focused on idf and ALO derived term weights, many alternatives could be considered or combined for this purpose. Such combination-type term weights might bene fi t other text classi fi ers, such as SVMs as well.
Feature relevance measures are typically used for rank-ing and their values are not necessarily optimum as term weights. We proposed to optimize their values under the constraint that ranking order of term weights is preserved. We believe that this type of constraint is meaningful, as sup-ported be experimental results, and should be instrumental in avoiding the potential over fi tting involved in optimizing term weights and classi fi er weights at the same time. We intend to investigate this topic further in future work. Document-speci fi c feature selection is quite e ff ective for Naive Bayes and its variants. We showed that it can be further improved by making it part of the classi fi er induction (as in traditional feature selection), although the originally proposed post-induction approach provides a useful heuristic for cheaply identifying the optimum setting. The authors would like to thank Max Chickering, Dennis Decoste and Yang Song as well as the anonymous reviewers for their helpful comments and suggestions. [1] P. Bennett. Assessing the calibration of naive Bayes X  [2] L. Chen, J. Huang, and Z. Gong. An anti-noise text [3] G. Cormack. The TREC 2006 spam fi lter evaluation [4] G.CormackandA.Bratko.Batchandonlinespam [5] G. Cormack and T. Lynam. TREC 2005 spam track [6] F. Debole and F. Sebastiani. Supervised term [7] P. Domingos and M. Pazzani. On the optimality of the [8] H.Drucker,D.Wu,andV.Vapnik.SupportVector [9] L. Edda and J. Kindermann. Text categorization with [10] C. Elkan. The foundations of cost-sensitive learning. [11] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian [12] T. Gartner and P. Flach. WBCsvm : Weighted [13] C. Genest and J. Zidek. Combining probability [14] J. Goodman. Sequential conditional generalized [15] P. Graham. A plan for spam [16] G. Hinton. Products of experts. In Proceedings of the [17] S. Hong, J. Hosking, and T. Natarajan. Ensemble [18] A. Juan and H. Ney. Reversing and smoothing the [19] S. Kim, K. Han, H. Rim, and S. Myaeng. Some [20] A. Ko  X  cz. Local sparsity control for naive Bayes with [21] A. Ko  X  cz and J. Alspector. SVM-based fi ltering of [22] M. Lee and E. Corlett. Sequential sampling models of [23] D. D. Lewis. Naive (Bayes) at forty: the independence [24] A. K. McCallum and K. Nigam. A comparison of event [25] V. Metsis, I. Androutsopoulos, and G. Paliouras. [26] D.Mladenic,J.Brank,M.Grobelnik,and [27] D. Mladenic and M. Grobelnik. Feature selection for [28] R.Raina,Y.Shen,A.Ng,andA.McCallum.
 [29] J. Rennie, L. Shih, J. Teevan, and D. Karger. Tackling [30] G. Robinson. A statistical approach to the spam [31] M. Sauban and B. Pfahringer. Text categorisation [32] K. Schneider. Techniques for improving the [33] P. Soucy and G. Mineau. Beyond TFIDF weighting [34] H.Wu,T.Phang,B.Liu,andX.Li.Are fi nement [35] W. Yih, J. Goodman, and G. Hulten. Learning at low [36] Z. Zheng and K. T. G.I. Webb. Lazy Bayesian rules:
