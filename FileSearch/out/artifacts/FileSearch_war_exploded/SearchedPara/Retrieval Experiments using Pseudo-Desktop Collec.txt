 Desktop search is an important part of personal information management (PIM). However, research in this area has been limited by the lack of shareable test collections, making cu-mulative progress difficult. In this paper, we define desktop search as a semi-structured document retrieval problem and introduce a methodology to automatically build a reusable collection (the pseudo-desktop ) that has many of the same properties as a real desktop collection.

We then present a comprehensive evaluation of retrieval methods for semi-structured document retrieval on several pseudo-desktop collections and the TREC Enterprise collec-tion. Our results show that a probabilistic retrieval model using the mapping relation between a query term and a document field (PRM-S) has the best performance in col-lections with more structure, such as email, and that the query-likelihood language model is better for other docu-ment types. We further analyze the observed differences using generated queries and suggest ways to improve PRM-S, which makes the performance gains more significant and consistent.
 H.3.3 [ Information Storage and Retrieval ]: [Informa-tion Search and Retrieval] Algorithms Information Retrieval, Desktop Search, Test Collection Gen-eration, Semi-structured Document Retrieval
Desktop search, which plays an important role in personal information management, has become a standard feature of most major platforms. As the amount and the complexity of information we handle every day increases, improving the ef-fectiveness of desktop search will continue to be a significant research issue. Desktop search is different from other search applications (e.g., web search) in several key aspects: first, the collection is composed of documents of different types such as office documents, emails, presentations, and so on with different metadata associated with each type. For in-stance, sender and receiver will be major metadata fields for email, while author will be available for documents; sec-ond, queries are mostly targeted at known items [10]; third, major features of web collections, such as link structure and anchor text, are missing.

Past research efforts in this area have focused on the dis-covery of desktop-specific features [19] and desirable char-acteristics for user interfaces [9] [8]. This research has been limited by the lack of availability of shareable test collec-tions. For instance, desktop search prototypes such as Stuff I X  X e Seen [9], Phlat [8] and Connections [18] employ evalu-ation methods based on real users X  desktop collections and queries, distributing prototype search engines to users, and evaluating performance with logged usage data. This type of evaluation is certainly valuable as it is based on actual use cases. The lack of reusability, however, makes it dif-ficult, if not impossible, to repeat experiments and make comparisons to alternative search techniques. Another prob-lem is that this approach requires a fully functional desktop search engine, thereby setting a high entry barrier for new researchers. In addition to the problem of test collections, there has also been a lack of clear definitions of the goals and characteristics of desktop search in comparison to other search applications.

In this paper, we define desktop search as retrieval for a semi-structured document collection with multiple schemas corresponding to each file type, reflecting the observation that desktops typically contain files with metadata and each file type has different metadata associated with it. We use the term  X  X emi-structured X  instead of  X  X tructured X  because metadata in desktop environments can be loosely specified in general and is often missing, which does not fit into the tra-ditional definition of structured data. Our definition is also relevant in the broad context of personal information man-agement (PIM) considering that PIM increasingly encom-passes more diverse items (e.g. RSS feeds, calendar events, and so on) each with a rich set of metadata.

Based on this definition, we suggest a methodology for automatically building reusable pseudo-desktop collections, consisting of document gathering and query generation. The resulting collections have many of the characteristics of typ-ical desktop collections and, importantly, are free from the privacy concerns that are common with personal data. Our methodology includes a novel query generation method and a technique for comparing the generated queries with real queries.

Using our approach, we present retrieval experiments for a model that extends past work on structured document retrieval and compare it against state-of-the-art retrieval models. Based on three pseudo-desktop collections as well as the TREC Enterprise collection, our evaluation shows that a retrieval model that maps query terms to document fields (PRM-S) has the best performance for email collec-tions, whereas the query-likelihood language model is better for other document types.

Since the performance differences were not consistent, how-ever, we further analyse the performance characteristics of PRM-S and suggest how it can be improved by better es-timation of field-level scores. Again, experimental results show that this modification makes the performance gains more significant and consistent in many cases.

Our approach to desktop search has its limits, in that some elements of the desktop are missing in the pseudo-desktop. For instance, metadata such as the folder hierarchy, file cre-ation date and usage log are not available for a generated desktop collection. However, we found equivalent features for some metadata fields, and past work in related fields [5] [21] showed that missing features can be independently developed and incorporated into the retrieval model subse-quently.

Also, we cannot claim that a generated test collection is an ideal substitute for a real desktop environment with actual user queries, considering that it is impossible to simulate all aspects of document gathering and query formulation by users. Instead, we tried to make the collection genera-tion procedure as realistic as possible, and verify the validity of the resulting test collection for retrieval experiments by comparison to actual instances of desktops and user queries.
The rest of this paper is organized as follows. In the next section, we provide an overview of related work. Then we in-troduce our test collection generation method and describe how we can demonstrate equivalence to real collections. We then describe existing retrieval models for desktop search and possible improvements to the PRM-S model. In the ex-periments, we report retrieval results using a well-studied TREC collection (from the Enterprise track) and several pseudo-desktop collections generated using the suggested method.
Related work can be found in three interconnected but different areas: semi-structured document retrieval, known-item search, and desktop search. They are connected in a sense that each of them characterizes the problem in terms of the document, the query and the environment, but they differ in the emphasis of the research. The first area focuses mainly on the retrieval model, whereas research efforts in the other two areas have concentrated more on task-specific features and evaluation methods.

For semi-structured document retrieval, people have adapted traditional retrieval models to handle documents with mul-tiple fields. Early work treated each field as a smaller doc-ument and simply combined field-level scores using linear combination or a mixture of probability models [16]. This straightforward combination of field-level scores was found to have limitations, resulting in efforts such as BM25F [17]. Recently, an adaptation of score combination and smooth-ing method was suggested [23] for the language modeling approach to IR, based on the search engine Indri [15] which supports combining evidence from multiple fields.

The TREC 2005 Enterprise Track [7] provided a known-item email retrieval task, where a set of emails and corre-sponding queries were given. Among the participants, the BM25F model [6] combined a variety of document fields and other features such as the year and the thread structure to get good effectiveness. Another approach [21] combined dif-ferent independent sources to improve the performance of known-item search.
 Desktop search systems such as Stuff I X  X e Seen [9] and Phlat [8] showed that user interaction is a significant is-sue in the desktop environment, and that date is most im-portant feature since most users sorted the results by date. Other researchers focused more on improving the quality of ranking and proved that temporal locality and causality [18] are useful features. Learning feature weights with training data [5] has also been found to be effective in the desktop environment. The approach of treating desktop search as meta-search problem has been suggested [20], although the focus of this research was the characterization and selection of servers, while we focus here on the effective retrieval of individual collections.

Evaluation of desktop search or, in general, personal in-formation management (PIM), has been considered a tricky problem [11] because real desktop collections are not avail-able due to privacy concerns. Performance evaluation of major commercial desktop search engines was tried [14] in standard IR evaluation settings, using TREC Robust track data. Chernov et al [4] [3] proposed a method for creat-ing a testbed for desktop search by collecting documents and queries collaboratively, yet no experimental validation was done. Elsweiler [10] suggested an evaluation method for PIM based on user studies. The approach described in this paper is different in that it does not require any direct user involvement.
In this section, we describe our method for generating a pseudo-desktop test collection, which is composed of docu-ments, queries, and corresponding relevance judgments.
As a first step, we need a collection of documents that has the characteristics of a typical desktop. The criteria that we used for the documents in a desktop were that the docu-ments should be related to a particular person, there should be of a variety of document types, the different document types should have metadata or fields, and that the collection should be of reasonable size, although there is no hard limit on size since real-world desktops vary considerably. [8] The privacy of the target individual was another concern.
Given these conditions, our choice of a document collec-tion method was to focus on people mentioned in the email collection from the TREC Enterprise track (crawl of the W3C website) and fetch a variety of publicly-available doc-uments on the web related to those people. More specifi-cally, we filtered the existing mailing list and webpage from the collection to get emails which refer to a set of individ-uals. We then used a web search engine with the name, organization and specialization of each target individual as a query to find documents related to that person, repeating the procedure until gathered documents match the statistics of previously used desktop search collections. More details will be provided in Section 5.2.1.

In addition to satisfying the conditions above, this method provides control over the types of collected documents since most search engines have the option to limit the search result by file type. Another advantage is that we can index the rich set of metadata provided by a web search engine together with the documents. For instance, for the web search engine we used, the document title, URL, and the abstract were available.
Once we have the collection of documents, the next steps are creating queries and corresponding relevance judgments, which are usually the most time-consuming parts of building an IR test collection. However, in this case, we can generate simulated queries and relevant judgments automatically by exploiting the fact that typical requests for desktop search are known-item queries [10].
Given a situation where a user is trying to find a docu-ment that she has seen (or created) previously, she may try to come up with whatever terms she can remember from the document. Based on this observation, Azzopardi et al. [1] suggested a set of methods for generating a known-item query by algorithmically selecting a set of terms from the target document, as illustrated below. 1. Initialize an empty query q = () 2. Select document d i to be the known-item with proba-3. Select the query length s with probability P length ( s ) 4. Repeat s times: 5. Record d k and q to define a known-item/query pair
They suggested many parameterizations of P doc and P term finding that inlink-based document selection improves the validity of the queries in general and that each collection requires different term-selection strategy.
Although Azzopardi et al. [1] showed that generated queries can be used for retrieval experiments with web collections, a desktop collection has different characteristics, as discussed in the introduction. Among the differences, we assume that the users X  querying behavior would be different for the desk-top because each document is composed of multiple fields. Therefore, we modified their query generation method for desktop search by incorporating the selection of fields in the generation process, which results in the following algorithm: 1. Initialize an empty query q = () 2. Select document d i to be the known-item with proba-3. Select the query length s with probability P length ( s ) 4. Repeat s times: 5. Record d k and q to define a known-item/query pair
The modification here is step 4 . , where we choose the field from which the query term is selected. Our hypothesis is that users may (implicitly) choose fields when they choose query terms, which has an intuitive appeal given that some document fields (e.g., To and From in email) are very im-portant in characterizing the document. In the experimen-tal section, we verify this hypothesis by showing that field-based query generation method creates queries that are more similar to actual user-generated queries than the document-based generation method.

Note here that we only use terms in the target document, which may be an unrealistic model. It would be possi-ble to include terms outside the document in many ways, for instance by interpolating P term with a collection lan-guage model, but we did not study this approach in this paper. Issues with the validity of the generated queries are reduced when they are used solely for comparative evalua-tion of retrieval methods, since all methods use the same set of queries.

Although there can be many variations in choosing P doc and P field , we use a uniform distribution that assigns equal probability for every available document and field, respec-tively. For P length , we use the statistics of previously used desktop collections. For P term , we use uniform selection, TF-based selection, IDF-based selection and TF*IDF-based selection, as suggested in Azzopardi et al. [1]
For the retrieval experiments using the generated queries to be meaningful, we need to show that they are equiva-lent in some sense to hand-built queries. To do this, past work [1] introduced the notions of predictive and replica-tive validity. Predictive validity means whether the data (e.g., query terms) produced by the model is similar to real queries, while replicative validity indicates the similarity in terms of the output (e.g., retrieval scores).

Azzopardi et al. [1] dealt with only replicative validity, but in this paper we address both predictive and replicative validity as they address different aspects of the query gener-ation technique. Predictive validity is verified by comparing query terms and therefore is independent of the retrieval method. In contrast, replicative validity compares the dis-tribution of scores returned by the system and is accordingly dependent on the choice of retrieval method. This means that retrieval performance comparisons will be useful only among retrieval methods whose replicative validity has been verified.

Another point is that while the verification of predictive validity does not involve randomness once P term is given, the same does not hold true for replicative validity since the query generation procedure in general involves random selection of query terms, which in turn changes the distri-bution of scores. We therefore need to be interested in both measures since predictive validity is more stable but replica-tive validity is more strongly related to our eventual goal (retrieval results).

Lastly, we should stress that the suggested methods are not perfect measures of equivalence, since each of them tests only a particular aspect of the queries. Therefore, in the experimental section, we report the results of using hand-built queries as well as generated queries.
In verifying predictive validity, we need to evaluate how close the generated queries are to hand-built queries. To accomplish this, since query generation involves the choice of term distribution P term , we suggest using the generation probability P term ( Q ) of the manual query Q . This can be computed with the term distribution P term from the given query generation method, as follows: Getting P term for document-based query generation method is straightforward since we can just use the simple maximum-likelihood estimates for each word. For the field-based query generation method, since every field has different P term need to take the linear interpolation of P term for all fields. Since we use a uniform probability for field selection, P for each field can be combined with equal weights.
Azzopardi et al. [1] measured replicative validity by the two-sided Kolmogorov-Smirnov test (KS-test) using the score samples of real and generated queries as input. The KS-test is an independent two-sample test which tests the null hypothesis that the two samples may come from the same distribution and the result is sensitive to both the location and the shape of the samples. Since the KS-test quanti-fies the similarity between the empirical distribution func-tions of two samples, we can conclude that two distributions are equivalent if resulting p-value is greater than a certain threshold.
In this section we introduce retrieval models for desktop search. We explain existing retrieval models first. Then we suggest improvements over existing methods. The following notation will be used throughout this paper. We assume that a query Q = ( q 1 ,...,q m ) is composed of m words, and the collection C contains n field types ( F 1 ,...,F n document d in the collection may include fields ( f 1 ,...,f where each field is marked using lowercase letters to distin-guish it from the corresponding field type in the schema. We also denote field-level weights ( w 1 ,...,w n ). For retrieval models that require field-level smoothing parameters, (  X  ) was used. Lastly, | f k | means the length of field f k in words, tf ( t,f k ) denotes the term frequency of t in field f k denotes the document frequency of t . Model-specific param-eters will be explained as they appear.
BM25F [17] is the modification of the BM25 model where field-level evidence is combined at the raw frequency level rather than score level. This maintains non-linear saturation of term frequencies. The BM25F score S is calculated as: where term weight weight ( q i ,d ) is calculated as:
Here, | F j | denotes the average length of field f j across the whole collection, and field-level parameter b j controls the degree of length normalization and is tuned using training queries.
Ogilvie et al. [16] suggested a mixture of field language models by linear interpolation (MFLM) for known-item search. A document score in the MFLM is calculated by taking a weighted average of field-level scores as follows:
P QL ( q i | f j ) is the query-likelihood score of field f appropriate smoothing with the background field language model F j using Jelinek-Mercer smoothing [22] with parame-ter  X  JM . The weight parameters w j are tuned with training queries, but do not vary for different queries. This can be too restrictive if each field can provide a different strength of evidence for different query terms. The next model relaxes this restriction by inferring the relationship between query terms and document fields.
The probabilistic retrieval model for semistructured data (PRM-S) [12] is the extension of MFLM in a sense that it also combines field-level scores into a document-level score. The difference compared to MFLM is that the mapping be-tween query terms and document fields are used as weights, which vary for each query term. We can infer this map-ping between each query term and document field based on collection statistics. More formally, using Bayes X  theorem, we can estimate the posterior probability P M ( F j | w ) that a given query term w is mapped into document field F j combining the prior probability P M ( F j ) and the probability of a term occurring in a given field type P M ( w | F j ).
Here, P M ( w | F j ) is calculated by dividing the number of occurrences for term w by total term counts in the field F across the whole collection. Also, P M ( F j ) denotes the prior probability of field F j mapped into any query term before observing collection statistics.
With the mapping probabilities estimated as described above, the probabilistic retrieval model for semistructured data (PRM-S) can use these as weights for combining the scores from each field P QL ( w | f j ) into a document score, as follows:
Past work [12] showed that PRM-S has significant per-formance advantages for retrieval in some semi-structured data collections. Here we investigate some potential im-provements. As PRM-S is the combination of two elements  X  the mapping probability P M ( F j | q i ) and field-level scores P
QL ( w | f j )  X  we can divide the task into improving the esti-mation of either element. Mapping probability estimation is not addressed in this paper. For the task of field-level score estimation, the first issue is whether the field-level query likelihood score is an appropriate term-weighting function. Another issue is whether the core assumption of PRM-S that each query term is chosen from a specific document field is correct. Some users may choose query terms without con-sidering the fields, thereby making it inappropriate to use the mapping probability for weighting.

In our current work, we tried to address both issues. Specif-ically, we attempted to improve the current estimation by the following methods. Zhao et al. [23] suggested two-stage Dirichlet smoothing. Here, each document is smoothed by the collection language model C first, then this smoothed document language model is used to smooth the field language model.
 They argued that this modification would address the prob-lem of unbalanced field-level smoothing where the term oc-currences in a shorter field have more impact than those in a longer field. However, this method requires field-level smoothing parameters  X  j to be tuned as well as document smoothing parameter  X  , which adds complexity to the pre-vious model [22].
As we pointed out above, it may not make sense to assume that every query term is generated with a particular field in mind. In such cases, PRM-S may seem too extreme since it only considers field-level scores and totally disregards docu-ment scores. A simple yet effective solution for this problem is to interpolate PRM-S with the document query likelihood model (PRM-D) as in Equation 9, thereby striking a balance between these two.
 P ( Q | d ) = where  X  is the parameter that controls the interpolation ratio. This model bears similarity to two-stage Dirichlet smoothing in that it combines document language model with field-language model.
In this section we describe the experiments comparing the retrieval effectiveness of the retrieval models and validating the test collection generation method. We used the TREC 2005 Enterprise track collection for the initial experiments. We also report on experiments using three pseudo-desktop collections. We describe the collection generation procedure and then the retrieval experiments using both hand-written queries and generated queries.

For indexing both collections, each word was stemmed us-ing the Krovetz stemmer and standard stopwords were elimi-nated. Reciprocal Rank was used as the measure of retrieval performance for all experiments, since this is a known-item task where each query has only one relevant document. In-dri 1 was used as a retrieval engine, which required some modification to accommodate BM25F scoring.

The retrieval models compared were the document lan-guage model (DLM), BM25F, the mixture of field language models (MFLM), the probabilistic retrieval model for semi-structured data (PRM-S) and the linear interpolation of PRM-S and DLM (PRM-D). We also applied two-stage smooth-ing for retrieval models based on the field language model  X  MFLM (MFLM2), PRM-S (PRM-S2) and PRM-D (PRM-D2).
The TREC 2005 Enterprise Track known-item task [7] used a crawl of the W3C mailing list, containing 198,394 documents with average length of 10kb. For each docu-ment, the indexed fields were title , content , to (receiver), date , name (sender) and from (sender). Among the 150 queries provided, according to the TREC guideline, 25 were set aside for training of model parameters and the rest were used for testing.

Since each retrieval model required a different set of pa-rameters to be tuned in advance, these were determined based on the effectiveness in TREC training queries. Document-level parameters such as k 1 parameter in BM25F, inter-polation ratio  X  in PRM-D and Jelinek-Mercer smoothing parameter were found by parameter sweeps. For parame-ters that required training for each document field, such as field weights w j and two-stage Dirichlet smoothing  X  j , we adopted a Golden Section Search algorithm.
In Table 1, we compared the performance of retrieval mod-els using the TREC test queries. Here, statistically signifi-cant (using paired t-test with p-value=0.05) improvements over baseline methods were marked using the initial charac-ter (D,M,B,P) of each baseline method.

Among the baseline methods, fixed-weight combinations of field scores such as MFLM and BM25F outperformed DLM, but not significantly. PRM-S was significantly bet-ter than DLM and PRM-D showed almost the same result as PRM-S. The application of two-stage Dirichlet smooth-ing resulted in significant performance gains over document-http://www.lemurproject.org DM 0.606 D 0.619 DM 0.624 DM 0.630 Table 2: Sum of generation probabilities (in log) for different generation methods on TREC queries.
 Field-based generation method with TF*IDF-based term selection show highest probability.
 Table 3: P-values of Kolmogorov-Smirnov test for different query generation methods on TREC queries. Queries generated with field-based method have higher p-values in overall.
 level smoothing (MFLM2) or increased the performance gap (PRM-D2 and PRM-S2).

Although none of suggested methods outperformed BM25F and PRM-S significantly, two of them (PRM-D2 and PRM-S2) were better than the best submission for the TREC 2005 Enterprise track. This result is more significant considering that all of our retrieval models are purely based on term statistics of email messages, without the aid of advanced features such as anchor-text, thread structure, and date.
Next we report experiments using query generation meth-ods with the TREC data. The query generation methods we tested varied by the structure from which query terms were selected (document vs. field), and the type of probabil-ity distribution P term from which query-terms were selected (uniform / TF / IDF / TF*IDF). For P length , we used the length distribution of actual TREC queries. Since all query generation methods we used included some randomness in the process, we repeated all experiments three times and report the averaged result.

We first report the verification result of predictive valid-ity using the generation probabilities of 150 TREC queries. Table 2 shows that field-based query generation methods have higher probabilities of generating manual queries than document-based generation methods, although it does not provide an absolute criteria for judging the equivalence to TREC queries. Among term selection methods, the term selection by TF*IDF was shown to have higher probability for generating TREC queries, reflecting users X  behavior of choosing popular and discriminative terms.

We then evaluated each query generation method in terms of replicative validity using the Kolmogorov-Smirnov (KS) test, as seen in Table 3. Each cell contains the average p-value of the KS-test for the score distributions of TREC queries and generated queries using corresponding method.
We report only the results of DLM, PRM-S and PRM-D because they were best-performing models for which replica-tive validity was established for any of query-generation meth-ods. For BM25F and MFLM, none of suggested methods succeeded and two-stage Dirichlet smoothing runs were omit-ted since they were slight variants of other retrieval models. The result shows that field-based query generation meth-ods have higher p-values in general, and uniform or TF-based term selection methods have p-values greater than the threshold (0.05) for all retrieval models tested.
The verification results for predictive and replicative valid-ity do not completely agree in the sense that the generation method with highest predictive validity (field-based genera-tion with TF*IDF term selection) does not have the highest replicative validity for all retrieval models suggested. As mentioned before, a possible cause is that the replicative va-lidity is verified by comparing score distributions, which gets affected by many factors other than the collection statistics of query terms.

The results in Table 3 support this explanation since the field-based generation method with the TF*IDF term selec-tion has the highest replicative validity for DLM yet fails on PRM-S and PRM-D, which incorporate the mapping prob-ability into the field-level query likelihood score. This is also consistent with the results of Azzopardi et al. [1] where TF*IDF term selection method was shown to have high-est replicative validity for document-based retrieval models. Despite this, we can conclude that field-based generation is more valid for use in semi-structured document retrieval ex-periments, since it was shown to have both higher predictive and replicative validity.

Next, we report the retrieval performance based on gen-erated queries in Table 4, where the relative performance of each retrieval model for different query generation methods reveal interesting trends. Note that this result is equiva-lent to hand-built queries only for retrieval models (DLM, PRM-S, PRM-D) for which replicative validity was shown above.

The most conspicuous trend here is that the relative per-formance of DLM and PRM-S are different for document-based vs. field-based query generation methods. While DLM is better than any other retrieval model for document-based query generation methods, it is worse than PRM-S and its variants when field-based query generation was used. Note also that PRM-D shows the best performance regard-less of the query generation method.

Another interesting observation is that the impact of two-stage Dirichlet smoothing varies depending on which re-trieval model it is used with. When applied to MFLM (MFLM2), two-stage Dirichlet smoothing improved the per-formance considerably, resulting in better performance than BM25F. For PRM-S, it produced performance gains only for document-based query generation methods. On the other hand, it hurt the performance of PRM-D, which makes sense Figure 1: Left : Correlation of retrieval perfor-mance (Reciprocal Rank) and the accuracy of map-ping probability (MP) estimation. Right : Com-parison of MP distribution between queries where PRM-S is better (solid line) vs. DLM is better. considering that this model already incorporates the docu-ment language model. Experiments on the TREC collection confirmed that PRM-S is the best among the baseline methods and the combina-tion with DLM improves the performance further, which seems to suggest that PRM-S uses a different notion of rel-evance which complements the document language model score. An example TREC query where PRM-S was better than DLM gives some insight on the performance charac-teristics of PRM-S. For the query  X  X WSL telecon feb 2004 X , PRM-S and its variants were better than any other base-line methods. Here, all query terms were correctly mapped into the right elements ( title , title , date , date ) with very high probabilities ( &gt; 0.9). However, the use of the query-field mapping can be a problem since PRM-S was worse than other baseline methods for queries like  X  X mazon Ac-cess How Accessible X , where the query terms were predicted to be in the title field, yet were found in content .
To generalize the findings from these examples, we did an analysis of what affects the relative performance gain of PRM-S compared to DLM, which represent the best-performing field-based and document-based retrieval mod-els, respectively.

Since we first hypothesized that what makes PRM-S per-form better than DLM would be the accuracy of mapping probability (MP), we plotted the difference in retrieval scores of two models against the accuracy of MP estimation  X  in what portion of query-words MP predicted the field cor-rectly. In left side of Figure 1, there is a weak correlation (Pearson correlation coefficient = 0.27) between the accu-racy of MP estimation and the difference in Reciprocal Rank of PRM-S and DLM. In the right side of the same figure, we compared the kernel density estimation of MP accuracy for two query sets for which either PRM-S or DLM performs better, where we find that queries for which PRM-S out-performs have higher mapping probabilities in general. The means of these two densities are also significantly different, confirmed by two-sided t-test (p-value &lt; 0.05).

To verify whether the choice of field during query formula-tion has an impact on the performance advantage of PRM-S, we examined the location in the relevant documents the most query terms were found. We found that 50% of the queries where PRM-S outperformed DLM had title as the field containing the most query terms, while the most fre-quent field was content for 67% of queries for which DLM outperformed PRM-S. In Section 5.1.3, we further verify this observation using generated queries.
Next we report the experiments using three pseudo-desktop collections we generated. We first describe how pseudo-desktops were built based on W3C collection and web queries. Then we show the retrieval performance using hand-written queries, followed by the results with generated queries. For retrieval experiment in generated queries we used only DLM, PRM-S and PRM-D, for which we have shown that replica-tive validity was verified.
As described in Section 3, we built each pseudo-desktop collection so that it may contain typical file types in desktop like email , webpage ( html ) and office document ( pdf , doc and ppt ) related to specific individuals. To get the emails related to a person, we filtered the W3C mailing list collection where the name occurrence of each person was tagged [2], which enabled us to identify several individuals whose activities in W3C were prominent. For other document types, using the Yahoo! search API with the combination of name, organi-zation and speciality of each pseudo-user as query words, we collected up to 1,000 documents for each individual and document type. In identifying the specialty of each individ-ual, we used a list provided by TREC expert search track. Indexed fields were the same as the TREC collection for email . For other document types, title , URL , abstract , date , text were indexed.

Table 5 lists the statistics from the resulting pseudo-desktop Table 5: Number and average length of documents for each pseudo-desktop collection.

Type Jack Tom Kate email 6067 (555) 6930 (558) 1669 (935) html 953 (3554) 950 (3098) 957 (3995) pdf 1025 (8024) 1008 (8699) 1004 (10278) doc 938 (6394) 984 (7374) 940 (7828) ppt 905 (1808) 911 (1801) 729 (1859) collections corresponding to three pseudo-users  X   X  X ack X ,  X  X om X  and  X  X ate X . Although these are prominent figures in W3C and all the collected documents are publicly avail-able, we have anonymized their names.

To compare the statistics of documents gathered with the desktop collections used in previous research, we collected the data from publications or by contacting authors. Table 6 shows that desktop collections used in the past vary greatly in many aspects, such as the number of files and the com-position of the collection in terms of file types. These large differences further indicate the need for a more reusable test collection.
We have previously shown that field-based query gen-eration methods have replicative validity to hand-written TREC queries. To verify this again on the pseudo-desktop collections, we collected hand-written queries for the three pseudo-desktop collections by the following procedure. We first showed each participant a set of target documents. Af-ter a time period, we asked them to formulate a query based on their memory of a document assuming that the document is to be found in the desktop. Three people participated in this experiment and a total of 50 queries were manually generated for each email sub-collection of the three pseudo-desktops we described above.

Table 7 shows the retrieval results for the hand-written queries, where we also used the same set of trained param-eters for this experiment since they were subsets of TREC collection. Here, statistically significant improvements over baseline methods were marked using the initial character (D,M,B,P) of each baseline method.

Similarly to the experiments with generated queries and the TREC collection, there exists considerable variations in absolute and relative performance between DLM and PRM-S, with DLM performing better for Jack and Kate, while PRM-S is better for Tom. Still, PRM-D outperforms both methods for Jack X  X  and Tom X  X  and performs almost as well as DLM for Kate X  X , showing the same performance charac-teristics found as in the TREC collection.

The impact of two-stage smoothing is re-confirmed here, where it helped MFLM and PRM-S yet hurt the perfor-mance of PRM-D. Lastly, BM25F and MFLM are not as good as DLM for this collection. Given that these methods have two sets of field-level parameters and all parameters were trained using TREC queries with very similar charac-teristics to this one, this seems to suggest high sensitivity of these models to parameter tuning.
We generated queries using the same set of query gener-ation methods to the TREC collection. Generated queries are verified in terms of predictive and replicative validity Table 8: P-values of Kolmogorov-Smirnov test for different query generation methods in pseudo-desktop collections.
 using the same method to the TREC collection. The re-sult in Table 8 shows the same trends as the TREC collec-tion, reconfirming the replicative validity of field-based gen-eration methods, especially when query-terms were selected randomly or based on term frequency. Document-based gen-eration methods show replicative validity only for some of the retrieval models. Since the sample size for hand-written query set was smaller (50) than that of the TREC collection (150), we set a higher threshold (0.1) for the p-value.
We next show the retrieval results for pseudo-desktop col-lections, where we generated 100 queries of length 2, fol-lowing the average query length of previous works. The field-based method was used for query generation since it achieved replicative validity in experiments using this col-lection as well as the TREC collection. For term selection probability P term , we used both uniform and TF-based se-lection since they showed replicative validity in both TREC and pseudo-desktop collections. Here, we report the result only on TF-based term selection as uniform selection showed the same trend.

Table 9 shows the retrieval results for each collection type and user. Different retrieval model performed best in differ-ent sub-collections: PRM-S in the email collection, DLM in doc and ppt . PRM-D performed the best only in the html collection, yet its performance was close to the best in most cases.

The result shows that the relative performance of each retrieval method depends on the type of sub-collection, al-though there are considerable variations within the same type. PRM-S has the advantage in the email collection where each document is composed of many fields with dif-ferent term distributions, while DLM seem to have advan-tages in more traditional document collections such as web-pages and office documents where each field may contain similar words. This result is consistent with previous work [12] where the performance advantage of PRM-S was greater in a collection with a clear field semantics and therefore the estimation of query-field mapping is easier.
To support the claim above, we performed a similar anal-ysis as in Section 5.1.3 to find out the correlation between the accuracy of mapping probability (MP) estimate and the relative performance of DLM, PRM-S and PRM-D. Here, each of 45 data points (3 users  X  5 sub-collections  X  3 rep-etitions) represents an unit experiment.

In both sides of Figure 2, we can find rather strong correla-tion between the accuracy of MP estimate and the difference 0.301 MBP 0.389 MBP 0.341 MB 0.356 0.361 MP 0.463 M 0.453 MP 0.455 Table 9: Retrieval performance for generated queries in pseudo-desktop collections. Queries are generated using field-based method with random term selection.
 Figure 2: Correlation of the aggregate retrieval per-formance (mean Reciprocal Rank) and the average accuracy of mapping probability (MP) estimation. Left : difference between PRM-S and DLM. Right : difference between PRM-S and PRM-D.
 Table 10: Retrieval performance for generated queries with different field selection. For instance, title means that query-terms were chosen only from the title field of document.
 in the MAP of PRM-S and DLM (left), PRM-S and PRM-D (right). The Pearson correlation coefficient is 0.57 on the left side and 0.50 on the right side, which means that the performance advantage of PRM-S over DLM and PRM-D is correlated with the accuracy of MP estimate. While this is consistent with the observation in Figure 1 except that the correlation is stronger here, we can infer that the use of aggregate statistics would have reduced randomness and increased the correlation.

Lastly, to confirm the claim in Section 5.1.3 that the per-formance advantage of PRM-S is correlated with the choice of field from which query terms are taken, we did another re-trieval experiment with generated queries where query terms are taken from only one field ( title or content ). The result matched our expectation that PRM-S and PRM-D will have advantages when query terms are chosen from the title field as opposed to the content field, with a larger performance gap for the email collection as seen in Table 10.
In this paper, we described a method for generating a reusable test collection for desktop search experiments, showed that the pseudo-desktop collections are valid based on vari-ous criteria, and reported the results of retrieval experiments using the pseudo-desktop collections and the TREC Enter-prise collection. In addition to the verification of replicative validity, we suggested how we can verify predictive validity using the generation probabilities of manual queries. The re-trieval experiments compared a number of retrieval models designed for search with semi-structured data that is typical of a desktop collection. We suggested several modifications for better estimation of field-level scores of the PRM-S model and analyzed the performance of the different retrieval meth-ods using generated queries of varying characteristics.
Our experimental results with the TREC and pseudo-desktop collections suggest that the performance advantage of PRM-S model is dependent on the accuracy of the map-ping probability estimation and the characteristics of docu-ment fields that the query terms are taken from. Also, in all experiments with hand-built queries, the interpolation of PRM-S and the document language model gives more consistent and significant performance gains regardless of the relative performance of these baseline methods. Two-stage Dirichlet smoothing addresses the same point yet is not as robust as this simple interpolation model, showing high-sensitivity on field-level smoothing parameters. This was also observed for field-based retrieval models such as BM25 and MFLM, which require the training of field-level parameters.

Our work leaves many interesting challenges. Since this is the first attempt in building reusable desktop collections, we can refine the generation procedures using more sophis-ticated query generation models or scale the collection by adding more file types and metadata fields. For instance, the known-item query generation methods suggested in this paper can be made more realistic by including phrases as well as words.

Concerning the retrieval models, it was found that the ac-curacy of mapping probability (MP) estimation has a sub-stantial impact on the performance advantage of PRM-S. Since PRM-S only considers field-level collection statistics to estimate mapping probability, it can be improved by using more elaborate estimation techniques that takes more fea-tures into account, as recently demonstrated by Li et al.[13]
Also, as we dealt with only sub-collection level retrieval in this paper, we need to find how these results can be incorpo-rated into a single rank list effectively. Since our experiments suggest that each retrieval model has advantages in different sub-collections, we can approach the problem by retrieving each sub-collection with the most suitable method and then merging the results appropriately.
This work was supported in part by the Center for In-telligent Information Retrieval, in part by NSF grant #IIS-0707801, and NSF grant #IIS-0711348. Any opinions, find-ings and conclusions or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsor. [1] L. Azzopardi, M. de Rijke, and K. Balog. Building [2] S. Bao, H. Duan, Q. Zhou, M. Xiong, Y. Cao, and [3] S. Chernov, G. Demartini, E. Herder, M. Kopycki, [4] S. Chernov, P. Serdyukov, P.-A. Chirita, [5] S. Cohen, C. Domshlak, and N. Zwerdling. On ranking [6] N. Craswell and S. R. Hugo Zaragoza. Microsoft [7] N. Craswell and A. P. D. Vries. Overview of the [8] E. Cutrell, D. Robbins, S. Dumais, and R. Sarin. Fast, [9] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin, [10] D. Elsweiler and I. Ruthven. Towards task-based [11] W. Jones and J. Teevan. Personal Information [12] J. Kim, X. Xue, and W. B. Croft. A Probabilistic [13] X. Li, Y.-Y. Wang, and A. Acero. Extracting [14] C.-T. Lu, M. Shukla, S. H. Subramanya, and Y. Wu. [15] D. Metzler and W. B. Croft. Combining the language [16] P. Ogilvie and J. Callan. Combining document [17] S. Robertson, H. Zaragoza, and M. Taylor. Simple [18] S. Shah, C. A. N. Soules, G. R. Ganger, and B. D. [19] C. A. N. Soules and G. R. Ganger. Connections: using [20] P. Thomas. Server characterisation and selection for [21] S. Yahyaei and C. Monz. Applying maximum entropy [22] C. Zhai and J. D. Lafferty. A study of smoothing [23] L. Zhao and J. Callan. A generative retrieval model for
