 The task of data clustering is important in many fields such as artificial intelligence, data mining, data compression, computer vision, and others. Many differ ent clustering algorithms have been de-veloped. However, most of them require that the user know the number of clusters ( k ) beforehand, while an appropriate value for k is not always clear. It is best to choose k based on prior knowledge about the data, but this information is often not available. Without prior knowledge it can be es-pecially difficult to choose k when the data have high dimension, making exploratory data a nalysis difficult.
 In this paper, we present an algorithm called PG-means (PG st ands for projected Gaussian) which is able to discover an appropriate number of Gaussian cluste rs and their locations and orientations. Our method is a wrapper around the standard and widely used Ga ussian mixture model. The paper X  X  primary contribution is a novel method of determining if a wh ole mixture model fits its data well, based on projections and statistical tests. We show that the new approach works well not only in simple cases in which the clusters are well separated, but al so in the situations where the clusters are overlapped, eccentric, in high dimension, and even non-Gaussian. We show that where some other methods tend to severely overfit, our method does not, a nd that our method is comparable to but much faster than a recent variational Bayes-based appro ach for learning k . Several algorithms have been proposed to determine k automatically. Most of these algorithms wrap around either k -means or Expectation-Maximization for fixed k . As they proceed, they use splitting or merging rules to increase or decrease k until a proper value is reached.
 Pelleg and Moore [9] proposed the X-means algorithm, which i s a regularization framework for learning k with k -means. This algorithm tries many values for k and obtains a model for each k value. Then X-means uses the Bayesian Information Criterio n (BIC) to score each model [5, 12], and chooses the model with the highest BIC score. Besides the BIC, other scoring criteria could also be applied such as the Akaike Information Criterion [1], or M inimum Description Length [10]. One drawback of the X-means algorithm is that the cluster covari ances are all assumed to be spherical and the same width. This can cause X-means to overfit when it en counters data that arise from non-spherical clusters.
 Hamerly and Elkan [4] proposed the G-means algorithm, a wrap per around the k -means algorithm. G-means uses projection and a statistical test for the hypot hesis that the data in a cluster come from a Gaussian distribution. The algorithm grows k starting with a small number of centers. It applies a statistical test to each cluster and those which are not acc epted as Gaussians are split into two clusters. Interleaved with k -means, this procedure repeats until every cluster X  X  data a re accepted as Gaussian. While this method does not assume spherical cluste rs and works well if true clusters is well-separated, it has difficulties when true clusters over lap since the hard assignment of k -means can clip data into subsets that look non-Gaussian.
 Sand and Moore [11] proposed an approach based on repairing f aults in a Gaussian mixture model. Their approach modifies the learned model at the regions wher e the residual is large between the model X  X  predicted density and the empirical density. Each m odification adds or removes a cluster center. They use a hill-climbing algorithm to seek a model wh ich maximizes a model fitness scor-ing function. However, calculating the empirical density a nd comparing it to the model density is difficult, especially in high dimension.
 Tibshirani et al. [13] proposed the Gap statistic, which com pares the likelihood of a learned model with the distribution of the likelihood of models trained on data drawn from a null distribution. Our experience has shown that this method works well for finding a small number of clusters, but has difficulty as the true k increases.
 Welling and Kurihara [15] proposed Bayesian k -means, which uses Maximization-Expectation (ME) to learn a mixture model. ME maximizes over the hidden va riables (assignment of examples to clusters), and computes an expectation over model parame ters (center locations and covariances). It is a special case of variational Bayesian methods. Bayesi an k -means works well but is slower than our method.
 None of these prior approaches perform well in all situation s; they tend to overfit, underfit, or are too computationally costly. These issues form the motivati on for our new approach. Our approach is called PG-means, where PG stands for project ed Gaussian and refers to the fact that the method applies projections to the clustering model as well as the data, before performing each hypothesis test for model fitness. PG-means uses the sta ndard Gaussian mixture model with Expectation-Maximization training, but any underlying al gorithm for training a Gaussian mixture might be used. Our algorithm starts with a simple model and in creases k by one at each iteration until it finds a model that fits the data well.
 Each iteration of PG-means uses the EM algorithm to learn a mo del containing k centers. Each time EM learning converges, PG-means projects both the dataset a nd the learned model to one dimension, and then applies the Kolmogorov-Smirnov (KS) test to determ ine whether the projected model fits the projected data. PG-means repeats this projection and te st step several times for a single learned model. If any test rejects the null hypothesis that the data f ollows the model X  X  distribution, then it adds one cluster and starts again with EM learning. If every t est accepts the null hypothesis for a given model, then the algorithm terminates. Algorithm 1 des cribes the algorithm more formally. When adding a new cluster PG-means preserves the k clusters it has learned and adds a new cluster. This preservation helps EM converge more quickly on the new m odel. To find the best new model, PG-means runs EM 10 times each time it adds a cluster with a dif ferent initial location for the new cluster. The mean of each new cluster is chosen from a set of ra ndomly chosen examples, and also points with low model-assigned probability density. The in itial covariance of the new cluster is based on the average of the existing clusters X  covariances, and th e new cluster prior is assigned 1 /k and all priors are re-normalized. More than 10 EM applications c ould be used, as well as deterministic annealing [14], to ensure finding the best new model. In our te sts, deterministic annealing did not improve the results of PG-means. As stated earlier, any trai ning algorithm (not just EM) may be Algorithm 1 PG-means (dataset X , confidence  X  , number of projections p ) used to fit a particular set of k Gaussian models. For example, one might use k -means if more speed is desired. 3.1 Projection of the model and the dataset PG-means is novel because it applies projection to the learn ed model as well as to the dataset prior to testing for model fitness. There are several reasons to proje ct both the examples and the model. First, a mixture of Gaussians remains a mixture of Gaussians after b eing linearly projected. Second, there are many effective and efficient tests for model fitness in one dimension, but in higher dimensions such testing is more difficult.
 Assume some data X is sampled from a single Gaussian cluster with distribution X  X  N (  X ,  X ) in d dimensions. So  X  = E [ X ] is the d  X  1 mean vector and  X  = Cov [ X ] is the d  X  d covariance matrix. Given a d  X  1 projection vector P of unit length ( || P || = 1 ), we can project X along P as
X 0 = P T X . Then X 0  X  N (  X  0 ,  X  2 ) , where  X  0 = P T  X  and  X  2 = P T  X  P . We can project each cluster model to obtain a one-dimensional projection of an e ntire mixture along P . Then we wish to test whether the projected model fits the projected data.
 The G-means and X-means algorithms both perform statistica l tests for each cluster individually. This makes sense because each algorithm is a wrapper around k -means, and k -means uses hard assignment (each example has membership in only one cluster ). However, this approach is prob-lematic when clusters overlap, since the hard assignment re sults in  X  X lipped X  clusters, making them appear very non-Gaussian. PG-means tests all clusters and a ll data at once. Then if two true clusters overlap, the additive probability of the learned Gaussians representing those clusters will correctly model the increased density in the overlapping region. 3.2 The Kolmogorov-Smirnov test and critical values After projection, PG-means uses the univariate Kolmogorov -Smirnov [7] test for model fitness. The KS test statistic is D = max CDF F ( X ) with the sample CDF S ( X ) . The KS test is only applicable if F ( X ) is fully specified; however, PG-means estimates the model with EM, so F ( X ) cannot be specified a priori. The best we can do is use the parameter estimates, but this will cause u s to accept the model too readily. In other words, the probability of a Type I error will be too low a nd PG-means will tend to choose models with too few clusters. Lilliefors [6] gave a table of s maller critical values for the KS test which correct for estimated parameters of a single univaria te Gaussian. These values come from Monte Carlo calculations. Along this vein, we create our own test critical values for a mixture of univariate Gaussians.
 To generate the critical values for the KS test statistic, we use the projected one-dimensional model that has been learned to generate many different datasets, a nd then measure the KS test statistic for each dataset. Then we find the KS test statistic that is in t he  X  range we desire, which is the critical value we want. Fortunately, this can be done efficie ntly and does not dominate the running full dimensional data and project these to obtain the statis tic distribution, yet they are equivalent approaches. Further optimization is possible when we follo w Lilliefors X  observation that the critical value decreases as approximately  X  n , for sufficiently large n , which we have also observed in our simulations with mixtures of Gaussians. Therefore, we c an use Monte Carlo simulations with n 0 n points, and scale the chosen critical value by p n 0 /n . A more accurate scaling given by Dallal and Wilkinson [2] did not offer additional benefit in o ur tests. We use at most n 0 = 3 / X  , which is 3000 points for  X  = 0 . 001 . The Monte Carlo simulations can be easily parallelized, an d our implementation uses two computational threads. 3.3 Number of projections We wish to use a small but sufficient number of projections and tests to discover when a model does not fit data well. Each projection provides a different view o f model fitness along that projection X  X  direction. However, a projection can cause the data from two or more true clusters to be collapsed together, so that the test cannot see that there should be mul tiple densities used to model them. Therefore multiple projections are necessary to see these m odel and data discrepancies. We can choose the projections in several different ways. Ran dom projection [3] provides a useful framework, which is what we use in this paper. Other possible methods include using the leading directions from principal components analysis, which give s a stable set of vectors which can be re-used, or choosing k  X  1 vectors that span the same subspace spanned by the k cluster centers. Consider two cluster centers  X   X   X   X  1 . We assume for simplicity that the two clusters have the same spherical covariance  X  and are c -separated, that is, || m || X  c p trace ( X ) . We follow Dasgupta X  X  conclusion that c -separation is the natural measure for Gaussians [3]. Now consider the project ion of m along some randomly chosen vector P  X  N (0 , 1 /d I ) . We use this distribution because in high dimension P will be approximately unit-length. The probability that P is a  X  X ood X  projection, i.e. that it maintains c -separation between the cluster means when projected, is where Erf is the standard Gaussian error function. Here we ha ve used the relation P T  X  P = trace ( X ) /d when  X  is spherical and || P || = 1 . If  X  is not spherical, then this is true in an ex-pected sense, i.e. E [ P T  X  P ] = trace ( X ) /d when || P || = 1 . If we perform p random projections, we wish that the probability that all p projections are  X  X ad X  to be less than some  X  : Therefore we need approximately p &lt; log(  X  ) / log( Erf ( p 1 / 2))  X  X  X  2 . 6198 log(  X  ) projections to find a projection that keeps the two cluster means c -separated. For  X  = 0 . 01 , this is only 12 projec-tions, and for  X  = 0 . 001 , this is only 18 projections. 3.4 Algorithm complexity PG-means converges as fast as EM on any given k , and it repeats EM every time it adds a cluster. Let K be the final learned number of clusters on n data points. PG-means runs in O ( K 2 nd 2 l + Kn log( n )) time, where l is the number of iterations required for EM convergence. The n log( n ) term comes from the sort required for each KS test, and the d 2 comes from using full covariance matrices. PG-means uses a fixed number of projections for eac h model and each projection is linear in n , d , and k ; therefore the projections do not increase the algorithm X  X  asymptotic run time. Note also that EM starts with k learned centers and one new randomly initialized center, so EM convergence is much faster in practice than if all k + 1 clusters were randomly initialized. We must also factor in the cost of the Monte Carlo simulations for det ermining the KS test critical value, which are O ( Kd 2 n log( n ) / X  ) for each simulation. For fixed alpha, this does not increase t he run-time significantly, and in practice the simulations are a min or part of the running time. Eccentricity=1 Eccentricity=4 Figure 1: Each point represents the average number of cluste rs learned for various types of syn-thetic datasets. The true number of clusters is 20. The error bars denote the standard errors for the experiments (except for BKM, which was run once for each data set type). Eccentricity=1
VI metric score Eccentricity=4
VI metric score Figure 2: Each point represents the average VI metric compar ing the learned clustering to the correct labels for various types of synthetic datasets. Lower value s are better. For each algorithm except BKM we provide standard error bars (BKM was run once for each d ataset type). We perform several experiments on synthetic and real-world datasets to illustrate the utility of PG-means and compare it with G-means, X-means, and Bayesian k -means (BKM). For synthetic datasets, we experiment with Gaussian and non-Gaussian dat a. We use  X  = 0 . 001 for both PG-means and G-means. For each model, PG-means uses 12 projecti ons and tests, corresponding to an error rate of  X  &lt; 0 . 01 that it incorrectly accepts. All our experiments use MATLAB on Linux 2.4 on a dual-processor dual-hyperthreaded Intel Xeon 3.06 GHz co mputer with 2 gigabytes of memory. Figure 1 shows the number of clusters found by running PG-mea ns, G-means, X-means and BKM on many synthetic datasets. Each of these datasets has 4000 p oints in d = 2, 4, 8 and 16 dimensions. Figure 3: The leftmost dataset has 10 true clusters with sign ificant overlap ( c = 1 ). Though PG-means finds only 4 clusters, the model is very reasonable. On t he right are the results for PG-means, G-means, and X-means on a dataset with 5 true eccentric and ov erlapping clusters. PG-means finds the correct model, while the others overfit with 15 and 19 clus ters.
 All of the data are drawn from a mixture of 20 true Gaussians. T he centers of the clusters in each dataset are chosen randomly, and each cluster generates the same number of points. Each Gaussian mixture dataset is specified by the average c -separation between each cluster center and its nearest neighbor (either 2, 4 or 6) and each cluster X  X  eccentricity ( either 1 or 4). The eccentricity of is defined as Ecc = p  X  cluster covariance. An eccentricity of 1 indicates a spheri cal Gaussian. We generate 10 datasets of each type and run PG-means, G-means and X-means on each, an d we run BKM on only one of them due to the running time of BKM. Each algorithm starts wit h one center, and we do not place an upper-bound on the number of clusters.
 It is clear that PG-means performs better than G-means and X-means when the data are eccentric (Ecc=4), especially when the clusters overlap ( c = 2 ). In this situation G-means and X-means tend to overestimate the number of clusters. The rightmost p lots in Figure 3 further illustrate this overfitting. PG-means is much more stable in its estimate of t he number of clusters, unlike G-means and X-means which can dramatically overfit depending o n the type of data. BKM generally does very well, but is less efficient than PG-means. For examp le, on a set of 24 different datasets each having 4000 points from 10 clusters, 2-16 dimensions an d varying separations/eccentricities, PG-means was three times faster than BKM.
 Figure 1 only gives the information regarding the learned nu mber of clusters, which is not enough to measure the true quality of learned models. In order to bette r evaluate the approaches, we use Meila X  X  VI (Variation of Information) metric [8] to compare the indu ced clustering to the true labels. The VI metric is non-negative and lower values are better. It is z ero when the two compared clusterings are identical (modulo clusters being relabeled). Figure 2 s hows the average VI metric obtained by running PG-means, G-means, X-means, and BKM on the same synt hetic datasets as in Figure 1. PG-means does about as well as the other algorithms when the d ata are spherical and well-separated (see the top-right plot). However, the top-left plot shows t hat PG-means does not perform as well as G-means, X-means and BKM for spherical and overlapping data . The reason is that two spherical clusters overlap, they can look like a single eccentric clus ter. Since PG-means can capture eccentric clusters effectively, it will accept these two overlapped s pherical clusters as one cluster. But for the same case, G-means and X-means will probably recognize them as two different clusters. Therefore, although PG-means gives fewer clusters for spherical and ov erlapping data, the models it learns are reasonable. Figure 3 shows how 10 true overlapping clusters may look like far fewer clusters, and that PG-means can find an appropriate model with only 4 cluste rs.
 High dimensional data of any finite-variance distribution l ooks more Gaussian when linearly pro-jected to a randomly chosen lower-dimensional space. Proje ction is a weighted sum of the original dimensions, and the sum of many random variables with finite v ariance tends to be Gaussian, ac-cording to the central limit theorem. Thus PG-means should b e useful for high-dimensional data which are not Gaussian. To test this, we perform experiments on high-dimensional non-Gaussian synthetic datasets. These datasets are generated in a simil ar way of generating our synthetic Gaus-sian datasets, except that each true cluster has a uniform di stribution. Each cluster is not necessarily axis-aligned or square; it is scaled for eccentricity and ro tated. Each dataset has 4000 points in 8 dimensions equally distributed among 20 clusters. The ecce ntricity and c -separation values for the datasets are both 4. We run PG-means, G-means and X-means on 1 0 different datasets and BKM Table 1: Results for synthetic non-Gaussian data and the han dwritten digits dataset. Each non-Gaussian dataset contains 4000 points in 8 dimensions sampl ed from 20 true clusters each having uniform distribution. The eccentricity and c -separation are both 4. We run each algorithm except BKM on ten such datasets, and BKM on one. The digits dataset co nsists of 10 classes and 9298 examples.
 on one of them. The results are shown in the left part of Table 1 . G-means and X-means overfit the non-Gaussian datasets, while PG-means and BKM both perf orm excellently in the number of clusters learned and in learning the true labels according t o the VI metric.
 We tested all of these algorithms on the U.S. Postal Service h andwritten digits dataset (both the train and test portions, obtained from http://www-stat.stanfor d.edu/  X tibs/ElemStatLearn/data.html). Each example is a grayscale image of a handwritten digit. There ar e 9298 examples in the dataset, and each example has 256 pixels (16 pixels on a side). The dataset has 10 true classes (digits 0-9). Our goal is to cluster the dataset without knowing the true label s and analyze the result to find out how well PG-means captures the true classes.
 We use random linear projection to project the dataset to 16 d imensions and run PG-means, G-means, X-means, and BKM on it. The results are shown in the rig ht side of Table 1. PG-means gives 14 centers, which is closest to the true value. It also o btains nearly the best VI metric score. On the other hand, G-means and X-means find many more classes t han the truth, which do not help them score well on the VI metric, and BKM takes over twice as lo ng as PG-means. We presented a new algorithm called PG-means for learning th e number of Gaussian clusters k in data. Starting with one center, it grows k gradually. For each k , it learns a model using Expectation-Maximization. Then it projects both the model and the datase t to one dimension and tests for model fitness with the Kolmogorov-Smirnov test and its own critica l values. It performs multiple projec-tions and tests per model, to avoid being fooled by a poorly ch osen projection. If the model does not fit well, PG-means adds an additional cluster. This procedur e repeats until one model is accepted by all tests. We proved that only a small number of these fast t ests are required to have good per-formance at finding model differences. In the future we will i nvestigate methods of finding better projections for our task. We also hope to develop approximat ions to the critical values of the KS test on Gaussian mixtures, to avoid the cost of Monte Carlo simula tions.
 PG-means finds better models than G-means and X-means when th e true clusters are eccentric or overlap, especially in low-dimension. On high-dimensiona l data PG-means also performs very well. PG-means gives far more stable estimates of the number of clu sters than the other two methods over many different types of data. Compared with Bayesian k -means, we showed that PG-means performs comparably, though PG-means is several times fast er in our tests and uses less memory. Though PG-means looks for general Gaussian clusters, we sho wed that PG-means works well on high-dimensional non-Gaussian data, due to the central lim it theorem and our use of projection. Our techniques would also be applicable as a wrapper around the k -means algorithm, which is really just a mixture of spherical Gaussians, or any other mixture o f Gaussians with limited covariance. On the real-world handwritten digits dataset PG-means finds a very good clustering with nearly the correct number of classes, and PG-means and BKM are equally c lose to identifying the original labels among the algorithms we tested.
 We believe that the project-and-test procedure that PG-mea ns uses is a useful method for deter-mining fitness of a given mixture of Gaussians. However, the u nderlying standard EM clustering algorithm dominates the runtime and is difficult to initiali ze well, which are well-known problems. The project-and-test framework of PG-means does not depend on EM in any way, and could be wrapped around any other better method of finding a Gaussian m ixture.
 Acknowledgements : We thank Dennis Johnston, Sanjoy Dasgupta, Charles Elkan, and the anony-mous reviewers for helpful suggestions. We also thank Dan Pe lleg and Ken Kurihara for sending us their source code.

