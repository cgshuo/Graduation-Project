 The collaborative filtering approach to recommender systems pre-dicts user preferences for products or services by learning past user-item relationships. In this work, we propose novel algorithms for predicting user ratings of items by integrating complementary mod-els that focus on patterns at different scales. At a local scale, we use a neighborhood-based technique that infers ratings from observed ratings by similar users or of similar items. Unlike previous local approaches, our method is based on a formal model that accounts for interactions within the neighborhood, leading to improved esti-mation quality. At a higher, regional, scale, we use SVD-like ma-trix factorization for recovering the major structural patterns in the user-item rating matrix. Unlike previous approaches that require imputations in order to fill in the unknown matrix entries, our new iterative algorithm avoids imputation. Because the models involve estimation of millions, or even billions, of parameters, shrinkage of estimated values to account for sam pling variability proves crucial to prevent overfitting. Both the local and the regional approaches, and in particular their combination through a unifying model, com-pare favorably with other approaches and deliver substantially bet-ter results than the commercial Netflix Cinematch recommender system on a large publicly available data set.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing Algorithms collaborative filtering, recommender systems
Recommender systems [1] are programs and algorithms that mea-sure the user interest in given items or products to provide personal-ized recommendations for items that will suit the user X  X  taste. More broadly, recommender systems attempt to profile user preferences and model the interaction between users and products. Increas-ingly, their excellent ability to characterize and recommend items Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. within huge collections represent a computerized alternative to hu-man recommendations.

The growing popularity of e-commerce brings an increasing in-terest in recommender systems. While users browse a web site, well calculated recommendations expose them to interesting prod-ucts or services that they may consume. The huge economic po-tential led some of the biggest e-commerce web, for example web merchant Amazon.com and the online movie rental company Net-flix, make the recommender system a salient part of their web sites. High quality personalized recommendations deepen and add an-other dimension to the user experience. For example, Netflix users can base a significant portion of their movie selection on automatic recommendations tailored to their tastes.

Broadly speaking, recommender systems are based on two dif-ferent strategies (or combinations thereof). The content based ap-proach creates a profile for each user or product to characterize its nature. As an example, a movie profile could include attributes re-garding its genre, the participating actors, its box office popularity, etc. User profiles might include demographic information or an-swers to a suitable questionnaire. The resulting profiles allow pro-grams to associate users with matching products. However, content based strategies require gathering external information that might not be available or easy to collect.

An alternative strategy, our focus in this work, relies only on past user behavior without requiring the creation of explicit pro-files. This approach is known as Collaborative Filtering (CF), a term coined by the developers of the first recommender system -Tapestry [4]. CF analyzes relationships between users and interde-pendencies among products, in order to identify new user-item as-sociations. For example, some CF systems identify pairs of items that tend to be rated similarly or like-minded users with similar history of rating or purchasing to deduce unknown relationships between users and items. The only required information is the past behavior of users, which might be, e.g., their previous transactions or the way they rate products. A major appeal of CF is that it is domain free, yet it can address aspects of the data that are often elusive and very difficult to profile using content based techniques. This has led to many papers (e.g., [7]), research projects (e.g., [9]) and commercial systems (e.g., [11]) based on CF.

In a more abstract manner, the CF problem can be cast as missing value estimation: we are given a user-item matrix of scores with many missing values, and our goal is to estimate the missing values based on the given ones. The known user-item scores measure the amount of interest between respective users and items. They can be explicitly given by users that rate their interest in certain items or might be derived from historical purchases. We call these user-item scores ratings , and they constitute the input to our algorithm.
In October 2006, the online movie renter, Netflix, announced a CF-related contest  X  TheNetflixPrize [10]. Within this contest, Netflix published a comprehensive dataset including more than 100 million movie ratings, which we re performed by about 480,000 real customers (with hidden identities) on 17,770 movies. Competitors in the challenge are required to estimate a few million ratings. To win the  X  X rand prize, X  they need to deliver a 10% improvement in the prediction root mean squared error (RMSE) compared with the results of Cinematch, Netflix X  X  proprietary recommender system. This real life dataset, which is orders of magnitudes larger than pre-vious available datasets for in CF research, opens new possibilities and has the potential to reduce the gap between scientific research and the actual demands of commercial CF systems. The algorithms in this paper were tested and motivated by the Netflix data, but they are not limited to that dataset, or to user-movie ratings in general.
Our approach combines several components; each of which mod-els the data at a different scale in order to estimate the unknown ratings. The global component utilizes the basic characteristics of users and movies, in order to reach a first-order estimate of the unknown ratings. At this stage the interaction between items and users is minimal. The more refined, regional component ,defines  X  X or each item and user X  factors that strive to explain the way in which users interact with items and form ratings. Each such factor corresponds to a viewpoint on the given data along which we mea-sure users and items simultaneously. High ratings correspond to user-item pairs with closely matching factor values. The local com-ponent models the relationships between similar items or users, and derive unknown ratings from val ues in user/item neighborhoods.
The main contributions detailed in this paper are the following:
Both the local and regional components outperformed the pub-lished results by Netflix X  X  proprietary Cinematch system. Addition-ally, combination of the components produced even better results, as we explain later.

The paper explains the three components, scaling up from the local, neighborhood-based approach (Section 3), into the regional, factorization-based approach (Section 4). Discussion of some re-lated works is split between Sections 3 and 4, according to its rel-evancy. Experimental results, along with discussion of confidence scores is given in Section 5. We begin in Section 2 by describing the general data framework, the methodology for parameter shrink-age, and the approach for dealing with global effects.
We are given ratings about m users and n items, arranged in an m  X  n matrix R = { r ui } 1 u m, 1 i n . We anticipate three characteristics of the data that may complicate prediction. First, the numbers of users and items may be very large, as in the Net-flix data, with the former likely much larger than the latter. Sec-ond, an overwhelming portion of the user-item matrix (e.g., 99%) may be unknown. Sometimes we refer to matrix R as a sparse matrix, although its sparsity pattern is driven by containing many unknown values rather than the common case of having many ze-ros. Third, the pattern of observed data may be very nonrandom, i.e., the amount of observed data may vary by several orders of magnitude among users or among items.

We reserve special indexing letters for distinguishing users from items: for users u, v , and for items i, j, k . The known entries  X  those ( u, i ) pairs for which r ui is known  X  are stored in the set K = { ( u, i ) | r ui is known } .
The algorithms described in Sections 3 and 4 can each lead to estimating massive numbers of parameters (not including the indi-vidual ratings). For example, for the Netflix data, each algorithm estimates at least ten million parameters. Because many of the pa-rameter estimates are based on small sample sizes, it is critical to take steps to avoid overfitting the observed data.

Although cross validation is a great tool for tuning a few param-eters, it is inadequate for dealing with massive numbers of param-eters. Cross validation works very well for determining whether the use of a given parameter improves predictions relative to other factors. However, that all-or-nothing choice does not scale up to se-lecting or fitting many parameters at once. Sometimes the amount of data available to fit different parameters will vary widely, per-haps by orders of magnitude. The idea behind  X  X hrinkage X  is to impose a penalty for those parameters that have less data associ-ated with them. The penalty is to shrink the parameter estimate towards a null value, often zero. Shrinkage is a more continuous alternative to parameter selection, and as implemented in statisti-cal methods like ridge and lasso regression [16] has been shown in many cases to improve predictive accuracy.

We can approach shrinkage from a Bayesian perspective. In the typical Bayesian paradigm, the best estimator of a parameter is the posterior mean, a linear combination of the prior mean of the pa-rameter (null value) and an empirical estimator based fully on the data. The weights on the linear combination depend on the ratio of the prior variance to the variance of the empirical estimate. If the estimate is based on a small amount of data, the variance of that estimate is relatively large, and the data-based estimates are shrunk toward the null value. In this way shrinkage can simultaneously  X  X orrect X  all estimates based on the amount of data that went into calculating them.

For example, we can calculate similarities between two items based on the correlation for users who have rated them. Because we expect pairwise correlations to be centered around zero, the em-pirical correlations are shrunk towards zero. If the empirical corre-lation of items i and j is s ij , based on n ij observations, then the shrunk correlation has the form n ij s ij / ( n ij +  X  ) .Forfixed  X  , the amount of shrinkage is inversely related to the number of users that have rated both items: the higher n ij is, the more we  X  X elieve X  the estimate, and the less we shrink towards zero. The amount of shrinkage is also influenced by  X  , which we generally tune using cross validation.
Before delving into more involved methods, a meaningful por-tion of the variation in ratings can be explained by using readily available variables that we refer to as global effects. The most ob-vious global effects correspond to item and user effects  X  i.e., the tendency for ratings of some items or by some users to differ sys-tematically from the average. These effects are removed from the data, and the subsequent algorithms work with the remaining resid-uals. The process is similar to double-centering the data, but each step must shrink the estimated parameters properly.

Other global effects that can be removed are dependencies be-tween the rating data and some known attributes. While a pure CF framework does not utilize content associated with the data, when such content is given, there is no reason not to exploit it. An exam-ple of a universal external attribute is the dates of the ratings. Over time, items go out of fashion, people may change their tastes, their rating scales, or even their  X  X dentities X  (e.g., a user may change from being primarily a parent to primarily a child from the same family). Therefore, it can be beneficial to regress against time the ratings by each user, and similarly, the ratings for each item. This way, time effects can explain additional variability that we recom-mend removing before turning to more involved methods.
The most common approach to CF is the local, or neighborhood-based approach. Its original form, which was shared by virtually all earlier CF systems, is the user-oriented approach; see [7] for a good analysis. Such user-oriented methods estimate unknown ratings based on recorded ratings of like minded users. More for-mally, in order to estimate the unknown rating r ui ,weresorttoa set of users N( u ; i ) , which tend to rate similarly to u ( X  X eighbors X ), and actually rated item i (i.e., r vi is known for each v Then, the estimated value of r ui is taken as a weighted average of the neighbors X  ratings: The similarities  X  denoted by s uv  X  play a central role here as they are used both for selecting the members of N( u ; i ) and for weight-ing the above average. Common choices are the Pearson correla-tion coefficient and the closely related cosine similarity. Predic-tion quality can be improved further by correcting for user-specific means, which is related to the global effects removal discussed in Subsection 2.3. More advanced techniques account not only for mean translation, but also for scaling, leading to modest improve-ments; see, e.g., [15].

An analogous alternative to the user-oriented approach is the item-oriented approach [15, 11]. In those methods, a rating is esti-mated using known ratings made by the same user on similar items. Now, to estimate the unknown r ui , we identify a set of neighboring items N( i ; u ) , which other users tend to rate similarly to their rating of i . Analogous to above, all items in N( i ; u ) must have been rated by u . Then, in parallel to (1), the estimated value of r a weighted average of the ratings of neighboring items: As with the user-user similarities, the item-item similarities (de-noted by s ij ) are typically taken as either correlation coefficients or cosine similarities. Sarwar et al. [15] recommend using an adjusted cosine similarity on ratings that had been translated by deducting user-means. They found that item-oriented approaches deliver bet-ter quality estimates than user-oriented approaches while allowing more efficient computations. This is because there are typically significantly fewer items than users, which allows pre-computing all item-item similarities for retrieval as needed.

Neighborhood-based methods became very popular, because they are intuitive and relatively simple to implement. In our eyes, a main benefit is their ability to provide a concise and intuitive justi-fication for the computed predictions, presenting the user a list of similar items that she has previously rated, as the basis for the es-timated rating. This allows her to better assess its relevance (e.g., downgrade the estimated rating if it is based on an item that she no longer likes) and may encourage the user to alter outdated ratings.
However, neighborhood-based methods share some significant disadvantages. The most salient one is the heuristic nature of the similarity functions ( s uv or s ij ). Different rating algorithms use somewhat different similarity measures; all are trying to quantify the elusive notion of the level of user-or item-similarity. We could not find any fundamental justification for the chosen similarities.
Another problem is that previous neighborhood-based methods do not account for interactions among neighbors. Each similarity between an item i and a neighbor j  X  N( i ; u ) , and consequently its weight in (2), is computed independently of the content of and the other similarities: s ik for k  X  N( i ; u )  X  X  j } suppose that our items are movies, and the neighbors set contains three movies that are very close in their nature (e.g., sequels such as  X  X ord of the Rings 1 X 3 X ). An algorithm that ignores the similarity of the three movies when predicting the rating for another movie, may triple count the available information and disproportionally magnify the similarity aspect shared by these three very similar movies. A better algorithm would account for the fact that part of the prediction power of a movie such as  X  X ord of the Rings 3 X  may already be captured by  X  X ord of the Rings 1-2 X , and vice versa, and thus discount the similarity values accordingly.
We overcome the above problems of neighborhood-based ap-proaches by relying on a suitable model. To our best knowledge, this is the first time that a neighborhood-based approach is derived as a solution to a model, and allows simultaneous, rather than iso-lated, computation of the similarities.

In the following discussion we derive an item-oriented approach, but a parallel idea was successfully applied in a user-oriented fash-ion. Also, we use the term  X  X eights X  ( w ij ) rather than  X  X imilari-ties X , to denote the coefficients in the weighted average of neigh-borhood ratings. This reflects the fact that for a fixed item i ,we compute all related w ij  X  X  simultaneously, so that each w influenced by other neighbors. In what follows, our target is always to estimate the unknown rating by user u of item i ,thatis r
The first phase of our method is neighbor selection. Among all items rated by u , we select the g most similar to i  X  N( using a similarity function such as the correlation coefficient, prop-erly shrunk as described in Section 2.2 . The choice of g reflects a tradeoff between accuracy and efficiency; typical values lie in the range of 20 X 50; see Section 5.

After identifying the set of neighbors, we define the cost function (the  X  X odel X ), whose minimization determines the weights. We look for the set of interpolation weights { w ij | j  X  N( will enable the best prediction rule of the form We restrict all interpolation weights to be nonnegative, that is, w 0 , which allows simpler formulation and, importantly, has proved beneficial in preventing overfitting.

We denote by U( i ) the set of users who rated item i . Certainly, our target user, u , is not within this set. For each user v denote by N( i ; u, v ) , the subset of N( i ; u ) that includes the items rated by v .Inotherwords, N( i ; u, v )= { j  X  N( i ; u ) | ( K} . For each user v  X  U( i ) , we seek weights that will perfectly interpolate the rating of i from the ratings of the given neighbors: Notice that the only unknowns here are the weights ( w ij find many perfect interpolation weights for each particular user, which will reconstruct r vi from the r vj  X  X . After all, we have one equation and | N( i ; u, v ) | unknowns. The more interesting problem is to find weights that simultaneously work well for all users. This leads to a least squares problem: Expression (5) is unappealing because it treats all squared devia-tions (or users) equally. First, we want to give more weight to users that rated many items of N( i ; u ) . Second, we want to give more weight to users who rated items most similar to i . We account for these two considerations by weighting the term associated with user v by j  X  N( i ; u,v ) w ij  X  , which signifies the relative importance of user v . To simplify subsequent derivation, we chose  X  the sum to be minimized is: min
At this point, we switch to matrix notation. For notational conve-nience assume that the g items from which we interpolate the rating of i are indexed by 1 ,...,g , and arranged within w  X  R g define two g  X  g matrices A and B ,wherethe ( j, k ) -entry sums over all users in U( i ) that rated both j and k , as follows: Using these matrices, we can recast problem (6) in the equivalent form: Notice that both A and B are symmetric positive semidefinite ma-trices as they correspond to squared sums (numerator and denom-inator of (6)). It can be shown analytically that the optimal solu-tion of the problem is given by solving the generalized eigenvector equation Aw =  X Bw and taking the weights as the eigenvector as-sociated with the smallest eigenvalue. However, we requested that the interpolation weights are nonnegative, so we need to add a non-negativity constraint w 0 . In addition, since w T Aw w T under scaling of w ,wefixthescaleof w obtaining the equivalent problem:
It is no longer possible to find an analytic solution to the prob-lem in the form of an eigenvector. To solve this problem, one can resort to solving a series of quadratic programs, by lineariz-ing the quadratic constraint w T Bw =1 , using the substitution x  X  Bw and the constraint x T w =1 . This typically leads to convergence with 3 X 4 iterations. In practice, we prefer a slightly modified method that does not require such an iterative process.
A major challenge that every CF method faces is the sparseness and the non-uniformity of the rating data. Methods must account for the fact that almost all user-item ratings are unknown, and the known ratings are unevenly distributed so that some users/items are associated with many more ratings than others. The method de-scribed above avoids these issues, by relying directly on the known ratings and by weighting the importance of each user according to his support in the data. We now describe an alternative approach that initially treats the data as if it is dense, but accounts for the sparseness by averaging and shrinking.

If all ratings of users that rated i were known, the problem of interpolating the rating of i from ratings of other items  X  as in Eq. (3)  X  is related to multivariate regression, where we are looking for weights that best interpolate the vector associated with our item i , from the neighboring items N( i ; u ) . In this case, problem (5) would form an adequate model, and the user-weighting that led to the sub-sequent refined formulations of the problem would no longer be necessary. To avoid overfitting, we still restrict the weights to be positive and also fix their scale. Using our previous matrix nota-tion, this leads to the quadratic program: Recall that the matrix A was defined in (7). In the hypothetical dense case, when all ratings by person v are known, the condition j, k  X  N( i ; u, v ) is always true, and each A jk entry is based on the full users set U( i ) . However, in the real, sparse case, each A entry is based on a different set of users that rated both j and k . As a consequence, different A jk entries might have very different scales depending on the size of their support. We account for this, by replacing the sum that constitutes A jk , with a respective average whose magnitude is not sensitive to the support. In particular, since the support of A jk is exactly B jk as defined in (8), we replace the matrix A , with the matching g  X  g matrix A ,definedas: This is still not enough for overcoming the sparseness issue. Some A jk entries might rely on a very low support (low corresponding B jk ), so their values are less reliable and should be shrunk towards the average across ( j, k ) -pairs. Thus, we compute the average entry value of A , which is denoted as avg = j,k A jk / ( g 2 ) the corresponding g  X  g matrix  X  A : The non-negative parameter  X  controls the extent of the shrinkage. We typically use values between 10 and 100. A further improve-ment is achieved by separating the diagonal and non-diagonal en-tries, accounting for the fact that the diagonal entries are expected to have an inherently higher average because they sum only non-negative values.

The matrix  X  A approximates the matrix A and thus replaces it in problem (11). One could use a standard quadratic programming solver to derive the weights. However, it is beneficial to exploit
NonNegativeQuadraticOpt ( A  X  R k  X  k ,b  X  R k ) % Minimize x T Ax  X  2 b T x s.t. x 0 Figure 1: Minimizing a quadratic function with non-negativity constraints the simple structure of the constraints. First, we inject the equality constraint into the cost function, so we actually optimize:
We construct a g  X  g matrix  X  C jk =  X  A jk +  X  and a vector R g =(  X , X ,..., X  ) T , so that our problem becomes min w w 2  X  b
T w s.t. w 0 . Higher values of the parameter  X  , impose stricter compliance to the i w i =1 constraint and tend to in-crease running time. We used  X  =1 , where estimation quality was uniform across a wide range of  X  values, and strict compli-ance to the i w i =1 constraint was not beneficial. Now, all constraints have a one sided fixed boundary (namely, 0), reaching a simplified quadratic program that we solve by calling the func-tion NonNegativeQuadraticOpt(  X  C,  X  b ), which is given in Figure 1. The function is based on the principles of the Gradient Projection method; see, e.g., [12]. The running time of this function depends on g , which is a small constant independent of the magnitude of the data, so it is not the computational bottleneck in the process.
There is a performance price to pay for tailoring the interpola-tion weights to the specific neighbors set from which we inter-polate the query rating. The main computational effort involves scanning the relevant ratings while building the matrices A and B . This makes our neighborhood-based method slower than previous methods, where all weights (or si milarities) could have been pre-computed. A related approach that vastly improves running time is described in a newer work [2].
The fact that we tailor the computation of the interpolation weights to the given r ui query opens another opportunity. We can weight all users by their similarity to u . That is, we insert the user-user similarities ( s uv ) into (5), yielding the expression: And consequently, we redefine the A and B matrices as:
Recall that s uv is a similarity measure between users u and v , properly shrunk as described in Section 2.2. Usually, this would be a squared correlation coefficient, or the inverse of the Euclidean distance. Incorporating these similarities into our equations means that the inferred relationship between items depends not only on the identity of the items, but also on the type of user. For exam-ple, some users put a lot of emphasis on the actors participating in a movie, while for other users the plot of the movie is much more important than the identity of actors. Certainly, interpolation weights must vary a lot between such different kinds of users. In practice, we found this enhancement of the method significant in improving the prediction accuracy.

To summarize, we build on the intuitive appeal of neighborhood-based approaches, which can easily explain their ratings to the user. Our method differs from previous approaches by being the solution of an optimization problem. Although this leads to more extensive computational effort, it introduces two important aspects that were not addressed in previous works: First, all interpolation weights used for a single prediction are interdependent, allowing consider-ation of interactions involving many items, not only pairs. Second, item-item interpolation weights depend also on the given user. That is, the relationships between items are not static, but depend on the characteristics of the user. Another benefit of relying on a cost function is the natural emergence of confidence scores for estimat-ing prediction accuracy, as we discuss later in Subsection 5.4.
Our description assumed an item-oriented approach. However, we have used the same approach to produce user-user interpola-tion weights. This simply requires switching the roles of users and items throughout the algorithm. Despite the fact that the method is computationally intensive, we could apply it successfully to the full Netflix Data on a desktop PC. Our results, which exceeded those of Netflix X  X  Cinematch, are reported in Section 5.
Now we move up from the local, neighborhood-based approach, to a  X  X egional X  approach where we compute a limited set of features that characterize all users and items . These features allow us to link users with items and estimate the associated ratings. For example, consider user-movie ratings. In this case, regional features might be movie genres. One of the features could measure the fitting into the action genre, while another feature could measure fitting into the comedy genre. Our goal would be to place each movie and each user within these genre-oriented scales. Then, when given a certain user-movie pair, we estimate the rating by the closeness of the features representing the movie and the user.

Ranking users and items within prescribed features, such as movie genres, pertains to content-based methods, which requires addi-tional external information on items and users beyond the past rat-ings. This might be a very complicated data gathering and cleaning task. However, our goal is to undercover latent features of the given data that explain the ratings, as a surrogate for the external informa-tion. This can be achieved by employing matrix factorization tech-niques such as Singular Value Deco mposition (SVD) or Principal Component Analysis (PCA); in the context of information retrieval this is widely known as Latent Semantic Indexing [3].

Given an m  X  n matrix R , SVD computes the best rank-f ap-proximation R f , which is defined as the product of two rank-f matrices P m  X  f and Q n  X  f ,where f m, n .Thatis, R f = minimizes the Frobenius norm R  X  R f F among all rank-f ma-trices. In this sense, the matrix R f captures the f most prominent features of the data, leaving out less significant portion of the data that might be mere noise.

It is important to understand that the CF context requires a unique application of SVD, due to the fact that most entries of R are un-known. Usually SVD, or PCA, are used for reducing the dimen-sionality of the data and lessen its representation complexity by providing a succinct description thereof. Interestingly, for CF we are utilizing SVD for an almost opposite purpose  X  to extend the given data by filling in the values of the unknown ratings. Each unknown rating, r ui is estimated as R f ui , which is a dot product of the u -th row of P with the i -th row of Q . Consequently, we refer to P as the user factors and to Q as the item factors .

Beyond the conceptual difference in the way we use SVD, we also face a unique computational difficulty due to the sparsity issue. SVD computation can work only when all entries of R are known. In fact, the goal of SVD is not properly defined when some entries of R are missing.
 Previous works on adopting SVD-or PCA-based techniques for CF coped with these computational difficulties. Eigenstate [5] uses PCA factorization combined with recursive clustering in order to estimate ratings. Notably, they overcome the sparsity issue by pro-filing the taste of each user using a universal query that all users must answer. This way, they define a gauge set containing se-lected representative items for which the ratings of all users must be known. Restricting the rating matrix to the gauge set results in a dense matrix, so conventional PCA techniques can be used for its factorization. However, in practice, obtaining such a gauge set rated by each user, may not be feasible. Moreover, even when ob-taining a gauge set is possible, it neglects almost all ratings (all those outside the gauge set), making it likely to overlook much of the given information, thereby degrading prediction accuracy.
An alternative approach is to rely on imputation to fill in missing ratings and make the rating matrix dense. For example, Sarwar et al. [14] filled the missing ratings with a normalized average rating of the related item. Later, Kim and Yum [8] suggested a more in-volved iterative method, where SVD is iteratively performed while improving the accuracy of imputed values based on results of prior iterations. In a typical CF application, where the imputed ratings significantly outnumber the original ratings, those methods relying on imputation risk distorting the data due to inaccurate imputation. Furthermore, from the computational viewpoint, imputation can be very expensive requiring tone o explicitly deal with each user-item combination, therefore significantly increasing the size of the data . This might be impractical for comprehensive datasets (such as the Netflix data), where the number of users may exceed a million with more than ten thousands items.
We propose a method that avoids the need for a gauge set or for imputation, by working directly on the sparse set of known ratings. Since SVD is not well defined on such sparse matrices, we resort to SVD-generalizations that can handle unknown values. In particular we were inspired by Roweis [13], who described an EM algorithm for PCA, to which we now turn.

The common way to compute the PCA of a matrix R is by work-ing on its associated covariance matrix. However, Roweis sug-gested a completely different approach, which eventually leads to the same PCA factorization (up to orthogonalization). Recall that we try to compute rank-f matrices Q and P that will minimize R  X  PQ T F . Now, we can fix the matrix P as some matrix  X  such that minimization of R  X  PQ T F would be equivalent to the least squares solution of R =  X  PQ T . Analogously, we can fix Q as  X  Q , so our minimization problem becomes the least squares solution of R = U  X  Q T . These least squares problems can be minimized by setting Q T =  X  P T  X  P to an iterative process that alternately recomputes the matrices P and Q , as follows:
It can be shown that the only possible minimum is the global one, so that P and Q must converge to the true SVD subspace [13].
One of the advantages of this iterative SVD computation is its ability to deal with missing values. Roweis proposed to treat the missing values in R as unknowns when obtaining the least squares solution of R =  X  PQ T , which is still solvable by standard tech-niques. This approach actually uses imputation, by filling in the missing values of R as part of the iterative process. This would be infeasible for large datasets where the number of all possible user-item ratings is huge. Therefore, we modify Roweis X  idea to enable it to deal with many missing values, while avoiding imputation.
We would like to minimize the squared error between the factors-based estimates and known ratings, which is: Here, p u is the u -th row of P , which corresponds to user u .Like-wise, q i is the i -th row of Q , which corresponds to item i .Sim-ilar to Roweis X  method, we could alternate between fixing Q and P , thereby obtaining a series of efficiently solvable least squares problems without requiring imputation. Each update of Q or P decreases Err( P, Q ) , so the process must converge. However, we recommend using a different proces s for reasons that will be clari-fied shortly.

A major question is what would be the optimal value of f ,the rank of the matrices Q and P , which represents the number of latent factors we will compute. As f grows, we have more flexibility in minimizing the squared error Err( P, Q ) (16), which must decrease as f increases. However, while Err( P, Q ) measures our ability to recover the known ratings, the unknown ratings are the ones that really interest us. Achieving a low error Err( P, Q ) might involve overfitting the given ratings, while lowering the estimation quality on the unknown ratings. Importantly, our decision to avoid imputa-tion leaves us with fitting a relatively low number of known entries. Therefore, the problem does not allow many degrees of freedom, preventing the use of more than a very few factors. In fact, our ex-perience with the process showed that using more than two factors ( f&gt; 2 ) degrades estimation quality. This is an undesirable situ-ation, as we want to benefit by increasing the number of factors, thereby explaining more latent aspects of the data. However, we find that we can still treat only the known entries, but accompany the process with shrinkage to alleviate the overfitting problem.
The key to integrating shrinkage is computing the factors one by one, while shrinking the results after each additional factor is com-puted. This way, we increase the number of factors, while gradually limiting their strength. To be more specific, let us assume that we have already computed the first f  X  1 factors, which are columns 1 ,...f  X  1 of matrices P and Q . We provide below a detailed pseudo code for computing the next factor, which is the f -th col-umn of matrices P and Q : ComputeNextFactor (Known ratings: r ui , % Compute f -th column of matrices P and Q to fit given ratings % Columns 1 ,...,f  X  1 of P and Q were already computed
This way, we compute f factors by calling the function Com-puteNextFactor f times, with increasing values of f . A well tuned shrinkage should insure that adding factors cannot worsen the esti-mation. The shrinkage we performed here, res ui  X  n ui res ui duces the magnitude of the residual according to two elements. The first element is the number of already computed factors -f .Aswe compute more factors, we want them to explain lower variations of the data. The second element is the support behind the rating r which is denoted by n ui . This support is the minimum between the number of ratings by user u and the number of users that rated item i . As the support grows, we have more information regarding the involved user and item, and hence we can exploit more factors for explaining them. By using shrinkage we observed improved esti-mation as factors were added. However, estimation improvement levels off beyond 30 X 50 factors and becomes insignificant; see Sec-tion 5. The second part of the function computes the f -th factor, by alternating between fixing item-values and user-values. We can conveniently deal with each user/item separately, thus the result-ing least squares problem is trivial involving only one variable. The iterative process converges when no significant improvement of Err( P, Q ) is achieved. Typically, it happens after 3 X 5 iterations.
At the end of the process we obtain an approximation of all ratings in the form of a matrix product PQ T . This way, each rating r ui is estimated as the inner product of the f factors that we learned for u and i ,thatis p T u q i . A major advantage of such a regional, factorization-based approach is its computational effi-ciency. The computational burden lies in an offline, preprocess-ing step where all factors are computed. The actual, online rating prediction is done instantaneously by taking the inner product of two length-f vectors. Moreover, since the factors are computed by an iterative algorithm, it is easy to adapt them to changes in the data such as addition of new ratings, users, or items. We can always train the relevant variab les by running a few additional re-stricted iterations of the process updating only the relevant vari-ables. While the factorization-based approach is significantly faster than our neighborhood-ba sed approach, it is very competitive in terms of estimation quality, as we will show in Section 5. A fur-ther improvement in estimation quality is obtained by combining the local information provided by neighborhood-based approaches, with the regional information provided by the factorization-based approach. We turn now to one way to achieve this.
The factorization-based approach describes a user u as a fixed linear combination of the f movie factors. That is, the profile of user u is captured by the vector p u  X  R f , such that his/her rat-ings are given by p T u Q T . Interestingly, we can improve estimation quality by moving from a fixed linear combination ( p u )toamore adaptive linear combination that changes as a function of the item i to be rated by u . In other words, when estimating r ui compute a vector p i u  X  R k  X  depending on both u and i  X  X nd then estimate r ui as p i u T q i . The construction of p below.

The user vector p u was previously computed such as to mini-mize, up to shrinkage, the squared error associated with u : Now, if we know that the specific rating to be estimated is r can tilt the squared error to overweight those items similar to i , obtaining the error function: Recall that s ij is a shrunk similarity measure between items i and j . We use here an inverse power of the Euclidean distance, but other similarity measures can be applied as well. The minimizer of error function (18)  X  up to shrinkage  X  would be p i u , which charac-terizes user u within i  X  X  neighborhood. It is still crucial to perform a factor-by-factor computation, while shrinking during the process. Therefore, we compute the f components of p i u one by one, as in the following algorithm:
NeighborhoodAdaptiveUserFactors (Known ratings: r ui ,user u , % Compute f factors associated with user u and adapted to item i
Const  X  =25 % Initialize residuals  X  portion not explained by pre vious factors for each given rating r uj do % Factor-by-factor sweep: for l =1 ,...,f do return p i u =( p i u [1] ,p i u [2] ,...,p i u [ f ]) T
This introduction of neighborhood-awareness into the regional-oriented, factorization-based method significantly improves the qual-ity of the results, compared to neighborhood-only, or regional-only approaches (see Section 5). Moreover, typically all item-item sim-ilarities (the s ij values) are precomputed and stored for quick re-trieval. This enables a very quick execution of the function Neigh-borhoodAdaptiveUserFactors, which contains no iterative compo-nent. Overall running time is only slightly more than for the origi-nal factorization-based approach.

A complementary step would be to recompute the item factors by making them neighborhood-aware. That is, replacing q i q , which can be computed analogously to p i u by accounting for similarities of other users to user u . Consequently, the rating r is estimated by p i u T q u i . This results in an additional improve-ment in estimation accuracy. Moreover, it naturally integrates item-item similarities and user-user similarities into a single estimate, by employing item-item similarities when computing the user-fac-tors, and user-user similarities when computing the item-factors. However, making the item factors neighborhood-aware typically requires an extensive additional computational effort, when user-user similarities are not stored due to the large number of users.
We evaluated our algorithms on the Netflix data [10]. As men-tioned before, this dataset is based on more than 100 million ratings of movies performed by anonymous Netflix customers. We are not aware of any publicly available CF dataset that is close to the scope and quality of this dataset. The Netflix data is currently the sub-ject of substantial analysis, and thus is likely to become a standard benchmark for CF algorithms.

To maintain compa tibility with results pub lished by others, we adopted some standards that were set by Netflix, as follows. First, quality of the results is measured by their root mean squared error (RMSE), a measure that puts more emphasis on large errors com-pared with the alternative of mean absolute error. In addition, we report results on two test sets compiled by Netflix, one is known as the Probe set and the other is known as the Quiz set .Thesetwo test sets were constructed similarly, with both containing about 1.4 million of the most recent movie ratings (by date) performed by the users. The Probe set is a part of the training data and its true rat-ings are provided. We do not know the true ratings for the Quiz set, which are held by Netflix in order to evaluate entries. Importantly, these two test sets seem to be distributed equally across users and dates, so that the Probe data serves as a good  X  X old-out X  sample on which to calibrate models. The test sets contain many more rat-ings by users that do not rate much and are harder to predict. In a way, these datasets represent real requirements from a CF system, which needs to predict new ratings from older ones, and to equally address all users, not only the heavy raters.

Netflix provides RMSE values to competitors that submit their predictions for the Quiz set. The benchmark is Netflix X  X  proprietary CF system, Cinematch, which achieved a RMSE of 0.9514 on this Quiz set. In the experiments that follow we focus on the Probe set, since the ratings are known. At the end of the section, we discuss results for the Quiz set. When results are reported for the Quiz set, we subsume the Probe set into our training data, which improves prediction accuracy.
We begin with evaluating the neighborhood-based estimation dis-cussed in Section 3. We concentrate on item-item (or, movie-movie in this case) interpolation which we found to deliver better results than the alternative user-user interpolation, confirming the findings of Sarwar et al. [15]. Our method is compared with common item-item interpolation based on Pearson correlation coefficients. While the literature suggests many flavor s of correlation-based interpola-tion, we found that two ingredients are very beneficial here. The first is working on residuals that remain after removing global ef-fects (Subsection 2.3). The second is working with shrunk corre-lations instead of the original ones (Subsection 2.2). Therefore, to make a fair comparison, we applied these two techniques to the competing correlation-based method, significantly improving its performance. It is important to mention that our results use ex-actly the same neighborhoods as the correlation-based results. That is, for both methods we select as neighbors the available movies of highest shrunk Pearson correlation with the current movie. Our method differs only in calculating the interpolation weights for the neighboring ratings. In Figure 2 we compare the methods X  perfor-mance against varying neighborhood sizes ranging from 10 to 50. Figure 2: Comparison of three neighborhood-based ap-proaches: a common method using shrunk Pearson correla-tions, our neighborhood-based approach by itself and option-ally extended with user-user similarities. Performance is mea-sured by RMSE, where lower RMSEs indicate better prediction accuracy. RMSE is shown as a function of varying neighbor-hood sizes on Probe set. All methods use the same neighbor-hood sets, differing only in the weights given to the neighbors.
Our model-based weights consistently deliver better results than the correlation-based weights. Performance of correlation-based weights peaks at around 20 neighbors, and then starts declining as neighborhood size grows. In contrast, our methods continue to im-prove (at a moderating pace) as neighborhoods grow. We attribute this difference to the fact that our weights are able to reflect inter-dependencies among neighbors, which pairwise correlation coef-ficients cannot utilize. As nei ghborhood size grows, the number of possible interdependencies among neighbors increases quadrat-ically; hence, more information is overlooked by correlation-based weights. For example, correlation-based approaches, or any pair-wise approach for the matter, would be unforgiving to using un-needed neighbors, whose corre lation is still pos itive (high enough to get placed among closest neighbors), whereas our model can set unneeded weights to as low as zero, if their contribution is covered by other neighbors.

An important feature of our model is the ability to integrate user-user similarities within the computation of item-item weights (Sub-section 3.3). We studied the added contribution of this feature by running our model twice, once being user-aware, accounting for user-user similarities, and once w ithout this feature. As shown in Figure 2, accounting for user similarity typically lowers the RMSE by about 0.0060; e.g., using 50 neighbors, the user-aware model re-sults in RMSE=0.9148, versus RMSE=0.9203 without user aware-ness.

As indicated above, all methods were run on residuals after global effects (shrunk user mean, movie mean and various time effects) were removed. This is an important part of the modelling, but it turns out much more so for the correlation-based approach. For our method, removing these global effects was beneficial, e.g., lower-ing the RMSE for 50 neighbors from 0.9271 to 0.9148. However, it was more crucial for the correlation-based methods, which de-livered very weak results with RMSEs above 0.99 when applied without removing global effects. Why might this be? Before re-moving global effects correlations between movies are significantly higher than after removing them. Hence, we speculate that cor-relation coefficients, which isolate movie pairs, fail to account for strong dependencies between movies coexisting in the neighbor set. Capturing such interdependencies among neighbors is one of the major motivations for our method. However, after removing the global effects, correlations among movies become lower and hence the correlation-based methods lose far less information by isolat-ing movie pairs. Consequently, we strongly recommend applying correlation-based methods only after removing most global effects and decorrelating items.

In terms of running time, correlation based methods possess a significant advantage over our method. After precomputing all shrunk correlations, movie-movie interpolation could predict the full Probe set in less than 5 minutes on a Pentium 4 PC. In contrast, using our more involved weights took more than a day to process the Probe set. In a newer work [2] we explain how to eliminate this runing time gap.
Now we move to the factorization approach, whose results are presented in Figure 3. The pure-regional model, which does not account for local, neighborhood-based relationships, achieved RM-SEs that are slightly worse (i.e., higher) than the aforementioned movie-movie interpolation, and is actually on the same level that we could achieve from user-user interpolation. However, unlike our neighborhood-based models, the factorization-based model allows a very rapid computation, processing the full Probe set in about three minutes.

Accuracy is significantly improved by integrating neighborhood relationships into the factorization model, as explained in Subsec-tion 4.4. This model, which integrates local and regional views on the data, outperforms all our models, achieving RMSE of 0.9090 on the Probe set. Importantly, this model is still very fast, and can complete the whole Probe set in about five minutes when using precomputed movie-movie similarities. A modest additional im-provement of around 0.0015 (not shown in the figure), is achieved by integrating user-user similarities as well, but here computational effort rises significantly in the typical case where user-user similar-ities cannot be precomputed and stored. For these factorization-based methods the main computational effort lies in the prepro-cessing stage when factors are computed. This stage took about 3 hours on our Pentium 4 PC. Adding factors should improve ex-plaining the data and thus reduce RMSE. This was our general ex-perience, as indicated in the figure, except for a slow RMSE in-crease at the regional-only approach when using more than 40 fac-tors. This probably indicates that our shrinkage mechanism was not tuned well.
In Figure 4, we present our results for the Quiz set, the set held out by Netflix in order to evaluate entries of the Netflix Prize con-test. Our final result, which yielded a RMSE of 0.8922 (6.22% im-provement over Netflix Cinematch X  X  0.9514 result), was produced by combining the results of three different approaches: The first twoarelocalones,whicharebasedonuser-userandonmovie-movie neighborhood interpolation. The third approach, which pro-duces the lowest RMSE, was factorization-based enriched with lo-cal neighborhood information. Note that user-user interpolation is the weakest of the three, resulting in RMSE=0.918, which is still a 3.5% improvement over the commercial Cinematch system. The combination of the three results involved accounting for confidence scores, a topic which we briefly touch now.
 Figure 3: RMSEs of regional, factorization-based methods on Probe data, plotted against varying number of factors. The ex-tended model, which includes also local item-item information, achieves a significant further decrease of the RMSE.
 Figure 4: Our results (measured by RMSE) on the Quiz set. For comparison, the Netflix Cinematch reported result is RMSE=0.9514.
The fact that our models are based on minimizing cost func-tions allows us to assess the quality of each prediction, associat-ing it with a confidence score . More specifically, considering the neighborhood-based methods, each score is predicted using weights that minimize problem (9). Consequently, we use the attained value of (9), w T Aw/w T Bw , as a suitable confidence score. Low val-ues correspond to weights that could fit the training data well. On the other hand, higher values mean that the given weights were not very successful on the training data, and thus they are likely to perform poorly also for unknown ratings. Turning to factorization-based approaches, we can assess the quality of a user factor by (17), or by (18) in the neighborhood-aware case. Analogously, we also compute confidence scores for each movie factor. This way, when predicting r ui by factorization, we accompany the rating with two confidence scores  X  one related to the user u factors, and the other to item i factors  X  or by a combination thereof.

There are two important applications to confidence scores. First, when actually recommending products, we would like to choose not just the ones with high predicted ratings, but also to account for the quality of the predictions. This is because we have more faith in predictions that are associated with low (good) confidence scores. This suggests a possible shrinkage of predicted ratings, to adjust for their quality. The second application of confidence scores is for joining the results of different algorithms. Different algorithms will assign varying confidence scores to each user-item pair. Therefore, when combining results, we would like to account for the confi-dence score associated with the results, overweighting ratings as-sociated with better confidence scores.

Figure 5 shows how RMSEs of predicted ratings vary with con-fidence scores. We report results for the Probe set by using movie-movie interpolation (RMSE=0.9148). Ratings were split into deciles based on their associated confidence score. Clearly, RMSEs of deciles rise monotonically with confidence scores. Notice that top decile can be estimated quite accurately with an associated RMSE of around 0.6, while the bottom decile is much harder to estimate doubling the RMSE to above 1.2.
 Figure 5: Dependency of actual prediction accuracy (measured by RMSE) on confidence scores is revealed by partitioning Probe results into ten deciles according to confidence scores
In this work, we designed new collaborative filtering methods based on models that strive to minimize quadratic errors, and demon-strated strong performance on a large, real-world dataset. The fol-lowing components of our method were all crucial to its good per-formance: 1) removing  X  X lobal X  components up front and focusing our modelling on the residuals, 2) using shrinkage to prevent over-fitting of the large number of parameters, 3) incorporating interac-tions among the users and movies by fitting a model that jointly optimizes parameter estimates, and 4) incorporating local, neigh-borhood based analysis into a regional, factorization model.
Formulating the methods around formal models allowed us to better understand their properties and to introduce controllable mod-ifications to the models for evaluating possible extensions. Impor-tantly, we have found that extending the models to combine mul-tiple facets of the data, such as user-similarity with item-similarity or local-features with higher-scale features, is a key component in improving prediction accuracy. Another benefit of our error-based model is the natural definition of confidence scores that accompany the computed ratings and essentially  X  X redict the quality of the pre-dictions X . In a future work, we would like to study the integration of these confidence scores with the predicted ratings in order to provide better recommendation to the user. Other future research ideas include non-linear extensions to our linear models, especially to the factorization model which might be extended by such non-linear functions.

We greatly benefited from the recently introduced Netflix data, which opens new opport unities to the design and evaluation of CF algorithms. None of our models use any information about the con-tent (actors, genres, etc) and it would be interesting to devise mod-els to incorporate such data into our CF-based model. Since our factorization attempts to indirectly model this external information, we X  X  like to see how much benefit that data would provide. [1] G. Adomavicius and A. Tuzhilin,  X  X owards the Next [2] R. Bell and Y. Koren,  X  X mproved Neighborhood-based [3] S. Deerwester, S. Dumais, G. W. Furnas, T. K. Landauer and [4] D. Goldberg, D. Nichols, B. M. Oki and D. Terry,  X  X sing [5] K. Goldberg, T. Roeder, D. Gupta and C. Perkins, [6] G.H.GolubandC.F.VanLoan, Matrix Computations , [7] J. L. Herlocker, J. A. Konstan, A. Borchers and John Riedl, [8] D. Kim and B. Yum,  X  X ollaborative Filtering Based on [9] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and [10] Netflix prize -www.netflixprize.com . [11] G. Linden, B. Smith and J. York,  X  X mazon.com [12] J. Nocedal and S. Wright, Numerical Optimization , Springer [13] S. Roweis,  X  X M Algorithms for PCA and SPCA X , Advances [14] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, [15] B. Sarwar, G. Karypis, J. Konstan and J. Riedl,  X  X tem-based [16] R. Tibshirani,  X  X egression Shrinkage and Selection via the [17] J. Wang, A. P. de Vries and M. J. T. Reinders, X  X nifying
