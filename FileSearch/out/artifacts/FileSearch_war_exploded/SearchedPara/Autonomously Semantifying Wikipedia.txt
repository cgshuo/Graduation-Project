 Berners-Lee X  X  compelling vision of a Semantic Web is hindered by a chicken-and-egg problem, which can be best solved by a boot-strapping method  X  creating enough structured data to motivate the development of applications. This paper argues that autonomously  X  X emantifying Wikipedia X  is the best way to solve the problem. We choose Wikipedia as an initial data source, because it is compre-hensive, not too large, high-quality, and contains enough manually-derived structure to bootstrap an autonomous, self-supervised pro-cess. We identify several types of structures which can be auto-matically enhanced in Wikipedia (e.g., link structure, taxonomic data, infoboxes, etc.), and we describe a prototype implementation of a self-supervised, machine learning system which realizes our vision. Preliminary experiments demonstrate the high precision of our system X  X  extracted data  X  in one case equaling that of humans. H.4 [ Information Systems Applications ]: Miscellaneous Management, Design Information Extraction, Wikipedia, Semantic Web While compelling in the long term, Berners-Lee X  X  vision of the Semantic Web [5] is developing slowly. Researchers have argued that the relative difficulty of authoring structured data is a primary cause [17]. A chicken-and-egg problem results: if there was more structured data available, people would develop applications; but without compelling applications, it is not worth people X  X  time to structure their data. In order to break this deadlock, a bootstrapping method is needed  X  some method of automatically structuring a large amount of existing data.

The ideal vision is a system which autonomously extracts infor-mation from the Web. Because of the wide range of information categories, supervised machine learning will require too much hu-man effort to scale. Instead, such a system should use unsuper-vised or self-supervised techniques. Several systems of this form have been proposed, e.g. MULDER [18], AskMSR [7], and KNOW ITALL [14], showing some signs of early success. The insight un-derlying these systems stems from the huge redundancy of knowl-edge on the Web  X  many things worth extracting are stated many times, in different ways and on disparate Web pages. As a result, complex linguistic processing is unnecessary, because one of the occurrences is likely written in a form which can be correctly ex-tracted with simple methods. Furthermore, the Web X  X  statistical properties, as calculated by a search engine, are a powerful tool for extraction [8, 13, 12]. Unfortunately, many of the things published on the Web are incorrect (e.g.  X  X lvis killed John Kennedy X ), and the increasing linguistic sophistication of link spam poses a grow-ing challenge to these methods.

Our paper proposes a very different approach to massive infor-mation extraction. Instead of using the whole Web, we focus on a single site: en.wikipedia.org . 1
Focusing on Wikipedia largely solves the problem of inaccurate source data, but introduces new challenges. For example, redun-dancy is very greatly reduced  X  apparently increasing the need for deep syntactic analysis. On the other hand, Wikipedia has several attributes that make it ideal for extraction:
Actually, we view Wikipedia simply as a first bootstrapping step, which will enable subsequent extraction from the Web as a whole.
Our grand vision is a combination of autonomous and collab-orative techniques for semantically marking up Wikipedia. Such a system would create or complete infoboxes by extracting infor-mation from the page, rationalize tags, merge replicated data using microformats, disambiguate links and add additional links where needed, engage humans to verify information as needed, and per-haps add new topic pages (e.g., by looking for proper nouns with no corresponding primary page or perhaps by tracking news stories).
As a first step towards this vision we present K YLIN , a proto-type which automates part of this vision. K YLIN looks for classes of pages with similar infoboxes, determines common attributes, creates training examples, learns CRF extractors, and runs them on each page  X  creating new infoboxes and completing others. K
YLIN also automatically identifies missing links for proper nouns on each page, resolving each to a unique identifier. Experiments show that the performance of K YLIN is roughly comparative with manual labelling in terms of precision and recall. On one domain, it does even better.
Many Wikipedia articles include infoboxes , a concise, tabular summary of the subject X  X  attributes. For example, Figure 1 shows a sample infobox from the article on  X  X bbeville County, X  which was generated dynamically from the data shown in Figure 2.

Because of their relational nature, infoboxes may be easily con-verted to semantic form as shown by Auer and Lehmann X  X  DBpe-dia [3]. Furthermore, for each class of objects, infoboxes and their Figure 2: Attribute/value data generating the infobox in Fig. 1 templates implicitly define the most important and representative attributes; hence, infoboxes are valuable ontological resources. In this section we explain how K YLIN automatically constructs and completes infoboxes. The basic idea is to use existing infoboxes as a source of training data with which to learn extractors for gather-ing more data. As shown in Figure 3, K YLIN  X  X  infobox generation module has three main components: preprocessor, classifier, and extractor.

The preprocessor performs several functions. First, it selects and refines infobox schemata, choosing relevant attributes. Secondly, the preprocessor generates a dataset for training machine learners.
K YLIN trains two types of classifiers . The first type predicts whether a given Wikipedia article belongs to certain class. The second type of classifier predicts whether a given sentence contains the value of a given attribute. If there are C classes and A attributes per class, K YLIN automatically learns C + AC different classifiers.
Extractors are learned routines which actually identify and clip out the necessary attribute values. K YLIN learns one extractor per attribute per class; each extractor is a conditional random fields (CRF) model. Training data are taken from existing infoboxes as dictated by the predictions of the classifiers.

We explain the operation and performance of these modules be-low. But first we discuss the nature of the existing Wikipedia in-foboxes and why they are harder to use for training than might be expected.
While infoboxes contain much valuable information, they suffer from several challenging problems:
Incompleteness: Since infobox and article text are kept separate in Wikipedia, existing infoboxes are manually created when human authors create or edit an article  X  a tedious and time-consuming process. As a result, many articles have no infoboxes and the ma-jority of infoboxes which do exist are incomplete. For example, in the  X  X .S. County X  class less than 50% of the articles have an infobox. Still there in many classes, there is plenty of data for training.

Inconsistency: The manual creation process is noisy, causing contradictions between the article text and the infobox summary. For example, when we manually checked a random sample of 50 infoboxes in the  X  X .S. County X  class, we found that 16% contained one or more errors. We suspect that many of the errors are intro-duced when an author updates an article with a revised attribute  X  X  infobox generator.
 Figure 4: Usage percentage for attributes of the  X  X .S. County X  infobox template. value (e.g., population) and neglects to change both the text and the infobox  X  another effect of keeping infobox and text separate.
Schema Drift: Since users are free to create or modify infobox templates, and since they typically create an article by copying parts (e.g., the infobox template) from a similar article, the infobox schema for a class of articles tends to evolve during the course of authoring. This leads to several problems: schema duplication, at-tribute duplication, and sparseness. As an example of schema du-plication, note that four different templates:  X  X .S. County X  (1428),  X  X S County X  (574), X  X ounties X (50) and  X  X ounty X  (19) are used to describe the same type of object. Similarly, multiple tags denote the same semantic attribute. For example,  X  X ensus Yr X ,  X  X ensus Estimate Yr X ,  X  X ensus Est. X  and  X  X ensus Year X  all mean the same thing. Furthermore, many attributes are used very rarely. Figure 4 shows the percent usage for the attributes of the  X  X .S. County X  in-fobox template; only 29% of the attributes are used by 30% or more of the articles, and only 46% of the attributes are used by at least 15% of the articles.

Typefree System: The design of Wikipedia is deliberately low-tech, to facilitate human generation of content, and infoboxes are no exception. In particular, there is no type system for infobox at-tributes. For example, the infobox for  X  X ing County, Washington X  has a tuple binding the attribute  X  X and area X  to equal  X  2126 square miles X  and another tuple defining  X  X and area km X  to be  X  5506 square km X  despite the fact that one can be easily derived from another. Clearly this simple approach bloats the schema and increases in-consistency; the similarity between these related attributes also in-creases the complexity of extraction.

Irregular Lists: List pages, which link to large numbers of sim-ilar articles, are a potential source of valuable type information. Unfortunately, because they are designed for human consumption, automated processing is difficult. For example, some list pages sep-arate information in items, while others use tables with different schemas. Sometimes, lists are nested in an irregular, hierarchical manner, which greatly complicates extraction. For example, the  X  X ist of cities, towns, and villages in the United States X  has an item called  X  X laces in Florida X , which in turn contains  X  X ist of counties in Florida. X 
Flattened Categories: While Wikipedia X  X  category tag system seems promising as a source of ontological structure, it is so flat and quirky that utility is low. Furthermore, many tags are purely administrative, e.g.  X  X rticles to be merged since March 2007. X 
The preprocessor is responsible for creating a training suite that can be used to learn extraction code for creating infoboxes. We divide this work into two functions: schema refinement and the construction of training datasets.

Schema Refinement: The previous section explained how col-laborative authoring leads to infobox schema drift, resulting in the problems of schema duplication, attribute duplication and sparsity. Thus, a necessary prerequisite for generating good infoboxes for a given class is determining a uniform target schema.

This can be viewed as an instance of the difficult problem of schema matching [11]. Clearly, many sophisticated techniques can be brought to bear, but we adopt a simple statistical approach for our prototype. K YLIN scans the Wikipedia corpus and selects all articles containing the exact name of the given infobox template name. Next, it catalogs all attributes mentioned and selects the most common. Our current implementation restricts attention to attributes used in at least 15% of the articles, which yields plenty of training data.

Constructing Training Datasets: Next, the preprocessor con-structs training datasets for use when learning classifiers and ex-tractors. K YLIN iterates through the articles. For each article with an infobox mentioning one or more target attributes, K YLIN ments the document into sentences, using the OpenNLP library [1]. Then, for each target attribute, K YLIN tries to find a unique, corre-sponding sentence in the article. The resulting labelled sentences form positive training examples for each attribute. Other sentences form negative training examples.

Our current implementation uses several heuristics to match sen-tences to attributes. If an attribute X  X  value is composed of several sub-values (e.g.,  X  X ub cities X ), K YLIN splits them and processes each sub-value as follows: 1. For each internal hyperlink in the article and the infobox at-2. If the attribute value is mentioned by exactly one sentence 3. If the value is mentioned by several sentences, K YLIN
Unfortunately, there are several difficulties preventing us from getting a perfect training dataset. First, OpenNLP X  X  sentence detec-tor is imperfect. Second, the article may not even have a sentence which corresponds to an infobox attribute value. Third, we require exact value-matching between attribute values in the sentence and infobox. While this strict heuristic ensures precision, it substan-tially lowers recall. The values given in many are incomplete or written differently than in the infobox. Together, these factors con-spire to produce a rather incomplete dataset. 2 Fortunately, we are still able to train our learning algorithms effectively.
K YLIN learns two types of classifiers. For each class of article being processed, a heuristic document classifier is used to recog-nize members of the class. For each target attribute within a class a sentence classifier is trained in order to predict whether a given sentence is likely to contain the attribute X  X  value.

Document Classifier: To accomplish autonomous infobox gen-eration, K YLIN must first locate candidate articles for a given class  X  a familiar document classification problem. Wikipedia X  X  manually-generated list pages, which gather concepts with similar properties, and category tags are highly informative features for this task. For example, the  X  X ist of U.S. counties in alphabetical order X  points to 3099 items; furthermore, 68% of those items have additionally been tagged as  X  X ounty X  or  X  X ounties. X  Eventually, we will use lists and tags as features in a Naive Bayes, Maximum Entropy or SVM classifier, but as an initial baseline we used a simple, heuris-tic approach. First, K YLIN locates all list pages whose titles con-tain infobox class keywords. Second, K YLIN iterates through each page, retrieving the corresponding articles but ignoring tables. If the category tags of the retrieved article also contains infobox class keywords, K YLIN classifies the article as a member of the class. As shown in Section 4, our baseline document classifier achieves very high precision (98.5%) and reasonable recall (68.8%).

Sentence Classifier: It proves useful for K YLIN to be able to predict which attribute values, if any, are contained in a given sen-tence. This can be seen as a multi-class, multi-label, text classifica-tion problem. To learn these classifiers, K YLIN uses the training set produced by the preprocessor (Section 2.2). For features, we seek
Alternatively, one can view our heuristics as explicitly preferring incompleteness over noise  X  a logical consequence of our choice of high precision extraction over high-recall.
 a domain-independent set which is fast to compute; our current im-plementation uses the sentence X  X  tokens and their part of speech (POS) tags as features.

For our classifier, we employed a Maximum Entropy model [24] as implemented in Mallet [21], which predicts attribute labels in a probabilistic way  X  suitable for multi-class and multi-label classi-fications. 3 To decrease the impact of a noisy and incomplete train-ing dataset, we employed bagging [6] rather than boosting [22] as recommended by [25].
Extracting attribute values from a sentence may be viewed as a sequential data-labelling problem. We use the features shown in Table 1. Conditional random fields (CRFs) [19] are a natural choice given their leading performance on this task; we use the Mallet [21] implementation. We were confronted with two interesting choices in extractor design, and both concerned the role of the sentence classifier. We also discuss the issue of multiple extractions.
Training Methodology: Recall that when producing training data for extractor-learning, the preprocessor uses a strict pairing model. Since this may cause numerous sentences to be incorrectly labelled as negative examples, K YLIN uses the sentence classifier to relabel some of the training data as follows. All sentences which were assigned to be negative training examples by the preprocessor are sent through the sentence classifier; if the classifier disagrees with the preprocessor (i.e., it labels them positive), then they are
We also experimented with a Naive Bayes model, but its perfor-mance was slightly worse. eliminated from the training set for this attribute. Experiments in Section 4 show that this small adjustment greatly improves the per-formance of the learned CRF extractor.

K YLIN trains a different CRF extractor for each attribute, rather than training a single master extractor that clips all attributes. We chose this architecture largely for simplicity  X  by keeping each at-tribute X  X  extractor independent, we ensure that the complexity does not multiply.

Classifier X  X  Role in Extraction: We considered two different ways to combine the sentence classifier and extractor for infobox generation. The first is an intuitive pipeline mode where the sen-tence classifier selects the sentences which should be sent to the CRF extractor. We expected that this approach would decrease the number of false positives with a potential loss in recall. The sec-ond architecture treats the classifier X  X  prediction as a CRF feature, but applies the extractor to all sentences. We expected better recall at the expense of speed. The experiments of Section 4 shows that our expectations were fulfilled, but the pipeline X  X  boost to precision was higher than expected, creating a more effective architecture.
Multiple Extractions: Sometimes the extractor finds multiple values for a single attribute. This often happens as a mistake (e.g. because of an extractor error or redundant text in the article) but can also happen when the attribute is not functional (e.g. a band likely has several members). K YLIN distinguishes the cases by seeing if multiple values are found in the attribute X  X  training set. If so, the set of extractions is returned as the final result. Otherwise, K returns the single value with the highest confidence. A second goal for K YLIN is autonomous link generation for Wikipedia articles. Two Wikipedia resources are useful for this task. Disambiguation pages, which list alternative definitions of a term along with a concise description, help distinguish the correct target for new links. Redirection pages, which redirect a pointer for one term to another article, can be used to identify sets of syn-onyms. K YLIN uses the following procedure to generate internal links.

First, K YLIN extracts each noun-phrase (NP) from the article, again using OpenNLP. Next, it converts the NPs to their normal-ized forms. For example, determinants like  X  X  X  and  X  X he X  are dis-carded. Only proper nouns (i.e., whose first word is capitalized) are retained as candidates for link generation. 4 Finally, for each candidate NP, K YLIN checks the following conditions in order; if one matches, it adds a link. 1. MatchAnchor: Exactly matches some existing anchor text 2. MatchTitle: Exactly matches the title of the article. 3. MatchURI: Matches a primary URI in Wikipedia without 4. Disambiguation: If there is a corresponding disambiguation
A random sample of 50 Wikipedia articles yielded 1213 existing hyperlinks edited by users, and 70.2% had proper nouns as anchors. Clearly, K YLIN should add links defining things other than proper nouns, but selecting which are worthy is a difficult future topic. 5. InTitle: NP is contained in the title. 6. InAnchor: NP is contained in some existing anchor text.
This technique, while ad hoc, performs well. But one might question is whether our matching heuristics are applied in the cor-rect order. We answer this question experimentally in Section 4.
K YLIN assumes that a given NP string denotes the same con-cept within an article, which is reasonable for Wikipedia. However, when a NP appears in a different sentence, its context changes and hence the disambiguation rule may predict a different target. Thus K
YLIN identifies all potential targets for a NP and chooses the one with the highest confidence  X  increasing both precision and recall. To see why, consider the meaning of  X  X ortland X  in the following two sentences:
Though it is hard to determine the correct target for  X  X ortland X  in the first sentence, the context is much more informative in the second.
To avoid overloading the Wikipedia server, we downloaded the 2007.02.06 data in order to test the performance of K YLIN fobox generation and link creation algorithms.
For testing, we selected four popular classes: U.S. county, air-line, actor, and university. Each was among the top 100 classes in terms of infobox usage. We address four questions: Figure 5: Precision vs. recall curves of infobox generation. The individual points correspond to the performance of Wikipedia users X  manual edition.

Document Classification: We use sampling plus human labelling to estimate the precision and recall of the classifiers. We measure the precision of a class X  classifier by taking a random sample of 50 pages which were predicted to be of that class and manually check-ing their accuracy. Table 2 lists the estimated precision for our four classes. On average, the classifiers achieve 98.5% precision.
To estimate recall, we introduce some notation, saying that an article is tagged with a class if it has had an infobox of that type manually created by a human author. We use the set of tagged pages as a sample from the universal set and count how many of them are identified by the classifier. Table 3 shows the detailed results, but averaging uniformly over the four classes yields an average recall of 68 . 8% . This is quite good for our baseline implementation, and it seems likely that a machine-learning approach could result in substantially higher recall.

Note that there are some potential biases which might potentially affect our estimates of precision and recall. First, as mentioned in Section 2.1, some list pages are challenging to exploit, and list page formatting varies a lot between different classes. Second, articles with user-added infobox classes tend to be on more popular topics, and these may have a greater chance to be included in list pages. This could lead to minor overestimation of K YLIN recall. But we believe that these factors are small, and our estimates of precision and recall are accurate.

Infobox Attribute Extractor: In order to be useful as an au-tonomous system, K YLIN must be able to extract attribute values with very high precision. High recall is also good, but of less im-portance. Since our CRF extractor outputs a confidence score for its extraction, we can modulate the confidence threshold to control the precision/recall tradeoff as shown in Figure 5.

Interestingly, the precision/recall curves are extremely flat, which means the precision is rather stable w.r.t the variation of recall. In Table 4: Relative performance of people and K YLIN on infobox attribute extraction. practice, K YLIN is able to automatically tune the confidence thresh-old based on training data provided by the preprocessor for various precision/recall requirements. In order to reduce the need for hu-man fact checking, one can set a high threshold (e.g., 0.99), boost-ing precision. A lower threshold (e.g. 0.3) extends recall substan-tially, at only a small cost in precision.

In our next experiment, we use a fixed threshold of 0 . 6 , which achieves both reasonable precision and recall for all classes. We now ask how K YLIN compares against strong competition: human authors. For each class, we randomly selected 50 articles with exis-ting infobox templates. By manually extracting all attributes men-tioned in the articles, we could check the performance of both the human authors and of K YLIN . The results are shown in Table 4. We were proud to see that K YLIN performs better on the  X  X .S. County X  domain, mainly because its numeric attributes are relatively easy to extract. In this domain, K YLIN was able to successfully recover a number of values which had been neglected by humans. For the  X  X ctor X  and  X  X irline X  domains, K YLIN performed slightly worse than people. And in the  X  X niversity X  domain, K YLIN performed rather badly, because of implicit references and the type of flexi-ble language used in those articles. For example, K YLIN extracted  X  X wight D. Eisenhower X  as the president of  X  X olumbia University X  from the following sentence.
Unfortunately, this is incorrect, because Eisenhower was a for-mer president (indicated somewhere else in the article) and thus the incorrect value for the current president.

Implicit expressions also lead to challenging extractions. For example, the article on  X  X inghamton University X  individually de-scribes the number of undergraduate and graduate students in each college and school. In order to correctly extract the total number of students, K YLIN would need to reason about disjoint sets and perform arithmetic, which is beyond the abilities of most textual entailment systems [9, 20], let alone one that scale to a Wikipedia-sized corpus.
 Using the Sentence Classifier with the Attribute Extractor: Recall that K YLIN uses the sentence classifier to prune some of the negative training examples generated by the preprocessor before training the extractor. We also explored two ways of connecting the sentence classifier to the extractor: as a pipeline (where only sentences satisfying the classifier are sent to the extractor) or by feeding the extractor every sentence, but letting it use the classi-fier X  X  output as a feature. In this experiment, we consider four pos-sible configurations: correspond to Wikipedia users X  manual edition. Figure 6 shows the detailed results. In most cases the  X  X elabel, Pipeline X  policy achieves the best performance. We draw the fol-lowing observations:
In this section we address the following questions:
The first question is easy. Sampling over 50 randomly generated pages, we found 1213 unique human-generated hyperlinks, 852 of which were anchored by proper nouns. The set of pages contained additional 369 unique proper nouns that we judged deserving of a link. 5 Thus, we see that human authors display 69 . 8% recall,
This judgment is a bit subjective and it is important to note that we are treating links as semantic structure whose objective is to uniquely specify the noun X  X  meaning  X  not facilitate a human X  X  reading pleasure. Table 5: Performance of various link-generation heuristics on existing links.
 Table 6: Performance of various link-generation heuristics on new links. presumably with near 100% precision. When K YLIN was asked to find links for the proper nouns left unlinked by humans, it generated 291 links of which 261 were correct. This yields a link-generation precision of 89 . 7% and a human-level recall of 70 . 7% . The six heuristics listed in Section 3 form the core of K link generator. Each of the heuristics sounds plausible, but as one might suspect, the correct order matters considerably. Interestingly, we can use the set of human-authored internal links as another training set and use it to choose the right order for the heuristics  X  another form of self-supervised learning.

We first measure each heuristic X  X  performance on the suite of user-added hyperlinks, and then order them in order of decreasing precision score. Table 5 shows each heuristic X  X  precision on a ran-dom sample of 50 Wikipedia articles. The order determined by K
YLIN matches well with our intuitions. To demonstrate that the ordering works well on as-yet unlinked noun phrases, we tested it on a manually-labelled sample of 50 articles (Table 6). Note that while the individual numbers have changed substantially, the rela-tive order is stable.

The difference in quantitative precision/recall numbers is due to the distinct characteristics of the nouns in datasets corresponding to Tables 5 and 6. Table 5 is based on existing anchor texts edited by users. A quick check of Wikipedia articles reveals that users seldom add links pointing to the current article, which leads to a performance decrease of the  X  X atchTitle X  and  X  X nTitle X  heuris-tics. If the same NP appears several times, users tend to only add a link at its first occurrence, which effects the  X  X atchAnchor X  and  X  X nAnchor X  heuristics. When users add a link to help disam-biguate concepts, usually they are  X  X arder X  than randomly picked noun-phrases, which explains the lower performance of the  X  X is-ambiguation X  heuristic in Table 5.

If human linking behavior is so different from K YLIN  X  X  exhaus-tive approach, one might question the utility of automatic link gen-eration itself. We agree that K YLIN  X  X  links might not help human readers as much as those added by human authors, but our objective is to help programs, not people! By disambiguating numerous noun phrases by linking to a unique identifier, we greatly simplify sub-sequent processing. Furthermore, we note that many are to pages which have not yet been linked in the article.
 For the next experiment, we enumerated all 6! = 720 heuristic Table 7: Effect of different heuristic orders on link generation performance. orderings and measured the precision and recall of the collection as a whole. Table 7 lists the performance of K YLIN  X  X  ordering as well as the best and worst orders. We can see there is little difference between K YLIN and the optimal one, and both of them perform more than 10% better than the worst ordering.
We group related work into several categories: bootstrapping the semantic web, unsupervised information extraction, extraction from Wikipedia, and related Wikipedia-based systems.

Bootstrapping the Semantic Web: R EVERE [17] aims to cross the chasm between structured and unstructured data by providing a platform to facilitate the authoring, querying and sharing of data. It relies on human effort to gain semantic data, while our K fully autonomous. DeepMiner [30] bootstraps domain ontologies for semantic web services from source web sites. It extracts con-cepts and instances from semi-structured data over source interface and data pages, while K YLIN handles both semi-structured and un-structured data in Wikipedia. The SemTag and Seeker [10] systems perform automated semantic tagging of large corpora. They use the TAP knowledge base [27] as the standard ontology, and use it to match instances on the Web. In contrast, K YLIN doesn X  X  assume any particular ontology, and tries to extract all desired semantic data within Wikipedia.

Unsupervised Information Extraction: Since the Web is large and highly heterogeneous, unsupervised and self-supervised learn-ing is necessary for scaling. Several systems of this form have been proposed. MULDER [18] and AskMSR [7, 13] use the Web to answer questions, exploiting the fact that most important facts are stated multiple times in different ways, which licenses the use of simple syntactic processing. KNOWITALL [14] and TEXTRUN NER [4] use search engines to compute statistical properties en-abling extraction. Each of these systems relies heavily on the Web X  X  information redundancy. However, unlike the Web, Wikipedia has little redundancy  X  there is only one article for each unique con-cept in Wikipedia. Instead of utilizing redundancy, K YLIN Wikipedia X  X  unique structure and the presence of user-tagged data to train machine learners.

Information Extraction from Wikipedia: Several other sys-tems have addressed information extraction from Wikipedia. Auer and Lehmann developed the DBpedia [3] system which extracts in-formation from existing infoboxes within articles and encapsulate them in a semantic form for query. In contrast, K YLIN populates infoboxes with new attribute values. Suchanek et al. describe the Y
AGO system [28] which extends WordNet using facts extracted from Wikipedia X  X  category tags. But in contrast to K YLIN can learn to extract values for any attribute, Y AGO only extracts values for a limited number of predefined relations.

Nguyen et al. proposed to extract relations from Wikipedia by exploiting syntactic and semantic information [23]. Their work is the most similar with ours in the sense of stepping towards au-tonomously semantifying both semi-structured and unstructured data. However, there are still several obvious distinctions. First, their system only classifies whether a sentence is related to some at-tribute, while K YLIN also extracts the particular attribute value within the sentences. Second, they only care about the relationship-typed attributes between concepts (i.e. objects having their own identifying pages), while K YLIN tries to extract all important at-tributes. Third, their system targets a limited number of predefined attributes, while K YLIN can dynamically refine infobox templates for different domains.

Other Wikipedia-Related Systems: V  X  o lkel et al. proposed an extension to be integrated with Wikipedia, which allows the typing of links between articles and the specification of typed data inside the articles in an easy-to-use manner [29]. Though a great step to-wards semantifying Wikipedia, it still relies on manual labelling by human users. Gabrilovich et al. used Wikipedia to enhance text categorization [15], and later proposed a semantic-relatedness met-ric using Wikipedia-based explicit semantic analysis [16]. Ponzetto et al. derived a large scale taxonomy containing subsumption rela-tions based on the category system in Wikipedia [26]. Adafre et al. tried to discover missing links in Wikipedia by first computing a cluster of highly similar pages around a target page, then iden-tifying candidate links from those similar pages [2]. In contrast, K
YLIN searches all proper noun-phrases within the target page for link creations.
Although our objective is the automatic extraction of structured data from natural-language text on Wikipedia and eventually the whole Web, our investigation has uncovered some lessons that di-rectly benefit Wikipedia and similar collaborative knowledge repos-itories. Specifically, Wikipedia could greatly improve consistency if it were augmented with a software robot (perhaps based on K which functioned as an automatic fact-checker. When an article was created or edited, this agent could:
This paper described K YLIN , a prototype system which autonomously extracts structured data from Wikipedia and regularlizes its internal link structure. Since K YLIN uses self-supervised learning, which is bootstrapped on existing user-contributed data, it requires little or no human guidance. We make the following contributions:
For an initial prototype, K YLIN performs quite well. But there are numerous directions for improvement. Many of K YLIN  X  X  com-ponents are simple baseline implementations, because we wished an end-to-end system. We wish to apply learning to the problem of document classification, consider more sophisticated ways of com-bining heuristics (e.g., stacked metalearning), test on more cases, make the result public (e.g., as a Firefox extension), and other im-provements. In the longer term, we will investigate the following directions: We thank Oren Etzioni, Alex Yates, Matt Broadhead, and Michele Banko for providing the code of their software and useful discus-sions. We also thank anonymous reviewers for valuable sugges-tions and comments. This work was supported by NSF grant IIS-0307906, ONR grant N00014-06-1-0147, SRI CALO grant 03-000225 and the WRF / TJ Cable Professorship. [1] http://opennlp.sourceforge.net/. [2] S. F. Adafre and M. de Rijke. Discovering missing links in [3] S. Auer and J. Lehmann. What have Innsbruck and Leipzig [4] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and [5] T. Berners-Lee, J. Hendler, and O. Lassila. The Semantic [6] L. Breiman. Bagging predictors. Machine Learning , [7] E. Brill, S. Dumais, and M. Banko. An analysis of the [8] C. L. A. Clarke, G. V. Cormack, and T. R. Lynam. Exploiting [9] R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and [10] S. Dill, N. Eiron, D. Gibson, D. Gruhl, R. Guha, A. Jhingran, [11] A. Doan and A. Halevy. Semantic integration research in the [12] D. Downey, O. Etzioni, and S. Soderland. A probabilistic [13] S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. Web [14] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, [15] E. Gabrilovich and S. Markovitch. Overcoming the [16] E. Gabrilovich and S. Markovitch. Computing semantic [17] A. Y. Halevy, O. Etzioni, A. Doan, Z. G. Ives, J. Madhavan, [18] C. T. Kwok, O. Etzioni, and D. Weld. Scaling question [19] J. Lafferty, A. McCallum, and F. Pereira. Conditional [20] B. MacCartney and C. D. Manning. Natural logic for textual [21] A. K. McCallum. Mallet: A machine learning for language [22] R. Meir and G. R  X  a tsch. An introduction to boosting and [23] D. P. Nguyen, Y. Matsuo, and M. Ishizuka. Exploiting [24] K. Nigam, J. Lafferty, and A. McCallum. Using maximum [25] D. Opitz and R. Maclin. Popular ensemble methods: An [26] S. P. Ponzetto and M. Strube. Deriving a large scale [27] E. Riloff and J. Shepherd. A corpus-based approach for [28] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A core [29] M. V  X  o lkel, M. Kr  X  o tzsch, D. Vrandecic, H. Haller, and [30] W. Wu, A. Doan, C. Yu, and W. Meng. Bootstrapping
