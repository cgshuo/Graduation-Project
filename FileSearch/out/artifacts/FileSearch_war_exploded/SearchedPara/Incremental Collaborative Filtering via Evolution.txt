 Collaborative filtering is a popular approach for building recommender systems. Current collaborative filtering algo-rithms are accurate but also computationally expensive, and so are best in static off-line settings. It is desirable to in-clude the new data in a collaborative filtering model in an online manner, requiring a model that can be incrementally updated efficiently. Incremental collaborative filtering via co-clustering has been shown to be a very scalable approach for this purpose. However, locally optimized co-clustering solutions via current fast iterative algorithms give poor ac-curacy. We propose an evolutionary co-clustering method that improves predictive performance while maintaining the scalability of co-clustering in the online phase. H.2.8 [ Database Applications ]: Data Mining algorithms, theory.
 Incremental collaborative filtering, co-clustering, evolution-ary algorithm, ensembles. Recommender systems suggest items of interest to users. Collaborative filtering (CF) uses purchase or rating infor-mation to recommend items based on similarity [3]. If two users have liked (or disliked) similar items up to now, it is likely that they will have the same behavior in the future. Therefore, collaborative filtering systems recommend based on the history of ratings. One of the drawbacks in current CF approaches is that they are more appropriate for static settings; incorporating new data to the model may be a non-trivial task. In real world problems, there are always new users and items that should be incorporated into the model recommendations in an online manner. Incremental collaborative algorithms are intended to handle this need.
A few published approaches have addressed the incremen-tal CF problem. Sarwar et al. [7] and Brand [2] proposed using singular value decomposition as an online CF strategy. Das et al. [4] proposed a scalable online CF approach using MinHash clustering, Probabilistic Latent Semantic Indexing (PLSI), and co-visitation counts. However in this work, only binary ratings (such as implicit user feedback) were consid-ered and therefore the proposed approaches are not applica-ble to prediction problems. In K-nearest neighbors collab-orative filtering approaches, similarity parameters such as correlation can be updated incrementally during the online phase [6].

George and Merugu [5] used co-clustering as a scalable in-cremental CF approach for dynamic settings. They showed the performance of incremental co-clustering is comparable to incremental SVD but much more scalable. For imple-menting co-clustering they used the approach of Bregman co-clustering [1], a very fast iterative local search which can result in very poor local optima.

In this paper we propose an incremental CF method that is both scalable and accurate. We use an evolutionary co-clustering algorithm that finds better solutions than Breg-man co-clustering at the cost of offline training time, which is relatively unimportant. Also, we revise the incremental algorithm suggested in [5] and introduce an ensemble strat-egy to give better predictions. In a collaborative filtering problem, there are U users and V items. Users have provided a number of explicit ratings for items; r ui is the rating of user u for item i . There are two phases in a CF algorithm: an offline phase in which training based on known ratings is performed, and an online phase in which unknown ratings are estimated using the output of the offline phase. Most CF approaches use only the data available offline to predict ratings. In incremental CF, the data available during online phase is incorporated into future predictions, potentially improving the predictive accuracy.
The simplest way to predict a rating is the global average of all ratings. However, some users tend to rate higher and some items are more popular. Including user bias and item bias in rating, we can predict user ratings by where  X  r is the global average,  X  r u is the average of ratings by user u ,  X  r i is the average of ratings for item i , n number of ratings by user u , and n i is the number of ratings for item i . S n u , X  and S n i , X  are the support function for user u and item i which we define as If fewer ratings are available for a user or item, the support will be smaller. The parameter  X  determines the necessary support. Empirically we found three to be a robust choice for  X  . When the support of a user and an item is zero, then  X  r ui =  X  r ; when the support is one,  X  r ui =  X  r u +  X  r when the support of a user u is zero and an item i is one, then  X  r ui =  X  r i , and vice versa for the reverse.
Clustering refers to partitioning similar objects into groups [1]. Co-clustering partitions two different kinds of objects si-multaneously. If one views the clustering problem as group-ing rows of a matrix together, then co-clustering is the si-multaneous grouping of rows and columns.

In collaborative filtering via co-clustering as suggested in [5], each user u is assigned to a user cluster (represented by  X  ( u )) and each item i is assigned to an item cluster (repre-sented by  X  ( i )) and the prediction is as follows: where k =  X  ( u ) is the user cluster assigned to user u , l =  X  ( i ) is the item cluster assigned to item i ,  X  r kl is the average of ratings belonging to users in user cluster k and items in item cluster l ,  X  r k is the average of ratings belonging to users in user cluster k , and  X  r l is the average of ratings belonging to bias of user u (some users tend to rate higher) and (  X  r is the same for item i (some items are more likable).
George and Merugu [5] used a fast iterative heuristic pro-posed by Banerjee et al. [1]. This algorithm has two phases: updating user clusters and updating item clusters. When updating user clusters, we assume all co-cluster means are constant and all items are assigned to item clusters, then we assign each user so that the sum of squared errors is min-imized. Item cluster updating follows the same approach. Details can be found in [1, 5].

In the online phase, the prediction is as follows:  X  r
In [5], incremental training is achieved by using new rat-ings to update the average parameters (  X  r kl ,  X  r u ,  X  r equation (3). However, new users or items are not assigned to clusters during the online phase.
As our experimental results will show, incremental collab-orative filtering via co-clustering [5] discussed in Section 2.2 may be even worse than the baseline. Therefore, we propose a number of revisions to improve this method.

First, if the support (number of available ratings) for a user or item is low, the co-clustering approach will not pro-vide good predictions for them, and the training phase will be affected by these noisy inputs. As a strategy, users and items with low support are eliminated from the training phase so that training is both more effective and efficient.
The prediction equation (3) is not appropriate for the in-cremental scenario, because it incorporates three average is not necessarily reliable. This is due to the fact that  X  r and  X  r l tend to be very close to the global average empir-ically. Therefore, this model can be equivalent to  X  r ui  X  r kl +  X  r u +  X  r i  X  2  X  r during the online phase. On the other hand, using only the block average  X  r kl for prediction ignores user and item bias which results in poor accuracy as well. To overcome this problem, we begin by co-clustering residuals, rather than ratings. We model a rating prediction as Note that  X  r u +  X  r i  X   X  r is the same as the prediction equation (1) when the support function is 1. As a result,  X  ui is the correction parameter for (1). For a known rating, (5) can be rewritten as where  X  ui can be interpreted as the residual of the prediction via (1). For implementing co-clustering, it is enough to work with the following objective function: where w ui is one if rating r ui exists in training dataset and for user cluster  X  ( u ) and item cluster  X  ( i ).
Now we can define the prediction strategy based on (1) and (5). For old user -old item, and otherwise As mentioned, one advantage of co-clustering is scalability. Therefore creating an ensemble of different co-clusterings is desirable. Ensembles are used to improve the accuracy of a method using a group of predictors, while increasing the running time linearly with the number of ensemble elements. Let p denote a co-clustering solution and P be the number of co-clusterings we use in the model. We can predict with where z ulp is the average error of prediction for user u and item cluster l in co-clustering solution p and similarly z the average error of prediction for item i and user cluster k in co-clustering solution p . Intuitively if previous predictions of a co-clustering solution are better, its weight is higher.
In the algorithm proposed in [5], new users and items are not included in the co-clustering during the online phase. As a result, the model is unable to provide legitimate predic-tions for them. However, in the revised version, it is trivial to find an appropriate cluster for a user or item. Let u be a Figure 1: Incremental training in evolutionary co-clustering new user who has provided some ratings. If a sufficient num-ber of rated items exist in the current co-clustering solution, then the new user X  X  cluster can be found using [1]: where n uh is the number of times user u has rated the items belonging to item cluster h during the online phase, and  X   X  is the average of residuals for those ratings. A similar pro-cedure finds the cluster of a new item. Figure 1 shows the incremental training algorithm. The function numberIn () is the number of ratings a user (item) has in the co-clustering solution which is defined by (item i ). If this value is bigger than a threshold  X  , then we can trust the information to incorporate the new user or item. Otherwise, the risk of misclustering will be high. Sub-sequently, new users and items will not receive prediction from co-clustering at the very beginning. These ratings will be estimated using (1), which is more accurate experimen-tally. As a rule of thumb,  X  can be set to 3 since we wish to incorporate new users or items as soon as possible but not at the cost of bad predictions.

We now turn to the construction of co-clusterings via evo-lutionary algorithms, a population-based search approach that explores a solution space by evolving a group of in-dividuals to find good solutions. In our context, let P co-clustering solutions exist. The goal is to find better solutions by combining the current solutions. Every evolutionary al-gorithm has three main steps: selection, in which two or more individuals are chosen to create offspring; crossover, in which the selected items are combined to create new solu-tions; and replacement, in which the new solutions replace existing solutions if they satisfy some criteria.
Our evolutionary co-clustering algorithm is shown in Fig-ure 2. A group of co-clustering solutions is randomly gener-ated and locally optimized via iterative Bregman co-clustering. The numbers of user and item clusters are randomly se-lected from a specific range. In the evolutionary iteration phase, two co-clustering solutions are randomly selected for crossover. In the context of optimization, better individu-als are chosen with higher probability. However, in machine learning, generalization is more important than optimiza-tion and biased selection may result in premature conver-gence. Our initial studies indicated that randomly selecting co-clusterings improved generalization. After selecting two co-clusterings, a new solution is generated via the crossover function presented in Figure 3.

The crossover operation is between two clusters  X  1 and  X  If the required number of clusters is K , the K  X  1 largest Figure 3: Crossover algorithm.  X  qr is the intersec-tion between cluster q in clustering  X  1 and cluster r in clustering  X  2 .  X  ( k ) is the k th largest intersection. intersections between  X  1 and  X  2 are assigned to the first K  X  1 clusters and the remainder will be assigned to the last cluster. Formally, let X be a N  X  K assignment matrix in which an element ( u, k ) is one if object u is assigned to cluster k and zero otherwise. Then the intersection matrix can be defined as X  X  X .

In the next step, the offspring from crossover will be lo-cally optimized via the fast iterative Bregman co-clustering algorithm from [1]. This is an important step due to the fact that most objects will be assigned to the last cluster. However, since the iterative co-clustering first estimates av-erages and then assigns users and items, we hope that most of the blocks will have good quality averages via fewer but selected users or items.

Finally, we should either discard a current solution or the offspring to preserve the total number of solutions. The following function can be used to discard the worst solution: where e p represents the worst solution in the population. If the worst solution is the offspring we just discard it, other-wise the worst solution will be replaced with the offspring.
In this section, we present the results of experiments per-formed to evaluate the effectiveness of our method. We used the Movielens dataset 1 consisting of 100,000 ratings (1-5) by 943 users on 1682 movies. We used mean absolute error http://www.grouplens.org/data/ (MAE) to evaluate and compare different methods. Four methods were used for comparison: 1. Baseline: based on the model proposed in Section 3.1. 2. COCL: The method presented in [5]. 3. ECOCL: Evolutionary co-clustering without ensembles. 4. ECOCLE: Evolutionary co-clustering with ensembles. 5. IKNN: Incremental KNN method [6]. 6. SVD: We compared our results with those from [7] for
In our experiments, we used 5-fold cross-validation. First a part of data was held for offline training. Then the rest of data was included in an online phase which is a combination of incremental training and prediction. In the online phase, first a predicted rating was used for computing prediction er-ror, and then the new case was incorporated into the model. The sequence of data was randomized. We performed incre-mental training based on three different strategies:  X 20%-80% X , in which 20% of data was used for training and 80% for incremental training;  X 50%-50%, X  and  X 80%-20%. X 
All of the support parameters such as  X  in (1) were set to 3. For the COCL method, we implemented the algorithm for different user and item cluster numbers and the best result (10 user clusters and 2 item clusters) is reported. The number of ensembles for ECOCLE was set to 25 and the iteration limit was 250.
 The results for all methods are summarized in Table 1. First, the baseline is reasonably good compared to other methods when less data is available during the training phase. As more data is provided for the offline phase, other meth-ods are more accurate than the baseline. Also, evolutionary co-clustering algorithm (ECOCLE) is more successful when more data is available. Using ECOCLE in all phases gives the best results. Evolutionary co-clustering without ensem-bles (ECOCL) still outperforms other methods while its per-formance is slightly better than baseline for the 20%-80% case. We did not perform SVD and only report the result of Sarwar et al. [7]. For 20%-80%, SVD has the poorest per-formance. However, as more data is available for training, it gets more competent. Note that the experimental protocol of [7] was different than ours in that new users and items were incrementally added to the model in one step, based on a training/test split. However, it is non-trivial to update an SVD model based on new data. Therefore, the performance of incremental SVD in our protocol might be similar.
The time of both offline and online training is reported in the Table 2. The offline phase of ECOCLE needs more time due to the evolutionary algorithm. However, since this phase only needs to be done once, greater offline training time can Table 2: Average time (milliseconds) of different methods per rating. The online time is the sum of online prediction and incremental training be ignored. Online time is the sum of both incremental online training and online prediction. ECOCLE and IKNN have similar online speeds, while the accuracy for ECOCLE is much higher. The time problem could be mitigated by parallelizing the co-clustering operations, since updating is independent for the different solutions in the ensemble.
Online collaborative filtering methods that can incorpo-rate new data in real time are advantageous in many prac-tical situations. However, this problem has not been ade-quately addressed. In this paper we extended the idea of CF via co-clustering to fulfill this need. As our empirical results showed, our method achieved very good accuracy compared to other incremental methods. Training was com-paratively slow, but still manageable, and could be improved by a straightforward parallelization. Further, most of the al-gorithmic parameters of our method were chosen randomly from a wide range of values and no fine-tuning was done to find a better range. Our method provides a recommender system that gives accurate results and updates incrementally without spending time on tuning. [1] A. Banerjee, I. Dhillon, J. Ghosh, S. Merugu, and D.S. [2] M. Brand. Fast online SVD revisions for lightweight [3] L. Candillier, F. Meyer, and M. Boulle. Comparing [4] A.S. Das, M. Datar, A. Garg, and S. Rajaram. Google [5] T. George and S. Merugu. A scalable collaborative [6] M. Papagelis, I. Rousidis, D. Plexousakis, and [7] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.
