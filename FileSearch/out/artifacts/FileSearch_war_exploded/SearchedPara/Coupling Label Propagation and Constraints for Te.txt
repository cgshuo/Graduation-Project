 In recent years, automated fact extraction from Web contents has seen significant progress with the emer-gence of freely available knowledge bases, such as DBpedia (Auer et al., 2007), YAGO (Suchanek et al., 2007), TextRunner (Etzioni et al., 2008), or ReadTheWeb (Carlson et al., 2010a). These knowl-edge bases are constantly growing and contain cur-rently (by example of DBpedia) several million enti-ties and half a billion facts about them. This wealth of data allows to satisfy the information needs of advanced Internet users by raising queries from key-words to entities. This enables queries like  X  X ho is married to Prince Charles? X  or  X  X ho are the team-mates of Lionel Messi at FC Barcelona? X .
 However, factual knowledge is highly ephemeral: Royals get married and divorced, politicians hold positions only for a limited time and soccer players transfer from one club to another. Consequently, knowledge bases should be able to support more sophisticated temporal queries at entity-level , such as  X  X ho have been the spouses of Prince Charles before 2000? X  or  X  X ho are the teammates of Lionel Messi at FC Barcelona in the season 2011/2012? X . In order to achieve this goal, the next big step is to distill temporal knowledge from the Web.

Extracting temporal facts is a complex and time-consuming endeavor. There are  X  X onservative X  strate-gies that aim at high precision, but they tend to suffer from low recall. On the contrary, there are  X  X ggres-sive X  approaches that target at high recall, but fre-quently suffer from low precision. To this end, we introduce a method that allows us to gain maximum benefit from both  X  X orlds X  by  X  X ggressively X  gath-ering fact candidates and subsequently  X  X leaning-up X  the incorrect ones. The salient properties of our ap-proach and the novel contributions of this paper are the following:  X  A temporal fact extraction strategy that is able  X  An ILP solver incorporating constraints on tem- X  Experiments on real world news and Wikipedia Recently, there have been several approaches that aim at the extraction of temporal facts for the auto-mated construction of large knowledge bases, but time-aware fact extraction is still in its infancy. An approach toward fact extraction based on coupled semi-supervised learning for information extraction (IE) is NELL (Carlson et al., 2010b). However, it does neither incorporate constraints nor temporal-ity. TIE (Ling and Weld, 2010) binds time-points of events described in sentences, but does not dis-ambiguate entities or combine observations to facts. A pattern-based approach for temporal fact extrac-tion is PRAVDA (Wang et al., 2011), which utilizes label propagation as a semi-supervised learning strat-egy, but does not incorporate constraints. Similarly, TOB is an approach of extracting temporal business-related facts from free text, which requires deep pars-ing and does not apply constraints as well (Zhang et al., 2008). In contrast, CoTS (Talukdar et al., 2012) introduces a constraint-based approach of coupled semi-supervised learning for IE, however not focus-ing on the extraction part. Building on TimeML (Pustejovsky et al., 2003) several works (Verhagen et al., 2005; Mani et al., 2006; Chambers and Jurafsky, 2008; Verhagen et al., 2009; Yoshikawa et al., 2009) identify temporal relationships in free text, but don X  X  focus on fact extraction. Facts and Observations. We aim to extract factual knowledge transient over time from free text. More specifically, we assume time T = [0 ,T max ] to be a finite sequence of time-points with yearly granularity. Furthermore, a fact consists of a relation with two typed arguments and a time-interval defining its validity. For instance, we write worksForClub ( Beckham , RMadrid )@[2003 , 2008) to express that Beckham played for Real Madrid from 2003 to 2007. Since sentences containing a fact and its full time-interval are sparse, we consider three kinds of textual observations for each relation, namely begin , during , and end .  X  X eckham signed for Real Madrid from Manchester United in 2003. X  includes both the begin observation of Beckham be-ing with Real Madrid as well as the end observation of working for Manchester. A positive seed fact is a valid fact of a relation, while a negative seed fact is incorrect (e.g., for relation worksForClub , a positive seed fact is worksForClub ( Beckham , RMadrid ) , while worksForClub ( Beckham , BMunich ) is a negative seed fact ).
 Framework. As depicted in Figure 1, our framework is composed of four stages, where the first collects candidate sentences, the second mines patterns from the candidates sentences, the third extracts temporal facts from the sentences utilizing the patterns and the last removes noisy facts by enforcing constraints. Preprocessing. We retrieve all sentences from the corpus comprising at least two entities and a temporal expression, where we use YAGO for entity recogni-tion and disambiguation (cf. (Hoffart et al., 2011)). Pattern Analysis. A pattern is a n-gram based fea-ture vector. It is generated by replacing entities by their types, keeping only stemmed nouns, verbs converted to present tense and the last preposition. For example, considering  X  X eckham signed for Real Madrid from Manchester United in 2003. X  the cor-responding pattern for the end occurrence is  X  X ign for CLUB from X . We quantify the strength of each pattern by investigating how frequent the pattern oc-curs with seed facts of a particular relation and how infrequent it appears with negative seed facts. Fact Candidate Gathering. Entity pairs that co-occur with patterns whose strength is above a mini-mum threshold become fact candidates and are fed into the next stage of label propagation. Building on (Wang et al., 2011) we utilize Label Propagation (Talukdar and Crammer, 2009) to deter-mine the relation and observation type expressed by each pattern.
 Graph. We create a graph G = ( V F  X   X  X  P , E ) having one vertex v  X  X  F for each fact candidate observed in the text and one vertex v  X  V P for each pattern. Edges between V F and V P are introduced whenever a fact candidate appeared with a pattern. Their weight is derived from the co-occurrence frequency. Edges among V P nodes have weights derived from the n-gram overlap of the patterns.
 Labels. Moreover, we use one label for each observa-tion type ( begin , during , and end ) of each relation and a dummy label representing the unknown relation. note the graph X  X  initial label assignment, and b Y  X  R + stand for the estimated labels of all ver-tices, S l encode the seed X  X  weights on its diagonal, and R  X  l contain zeroes except for the dummy label X  X  column. Then, the objective function is: Here, the first term ( Y  X  `  X  b Y  X  ` ) T S ` ( Y  X  `  X  b ensures that the estimated labels approximate the initial labels. The labeling of neighboring vertices is smoothed by  X  1 b Y T  X  ` L b Y  X  ` , where L refers to the Laplacian matrix. The last term is a L2 regularizer. To prune noisy t-facts, we compute a consistent sub-set of t-facts with respect to temporal constraints (e.g. joining a sports club takes place before leaving a sports club) by an Integer Linear Program (ILP). Variables. We introduce a variable x r  X  { 0 , 1 } for each t-fact candidate r  X  X  , where 1 means the can-didate is valid. Two variables x f,b ,x f,e  X  [0 ,T max ] denote begin ( b ) and end ( e ) of time-interval of a fact f  X  X  . Note, that many t-fact candidates refer to the same fact f , since they share their entity pairs. Objective Function. The objective function intends to maximize the number of valid raw t-facts, where w r is a weight obtained from the previous stage: Intra-Fact Constraints. x f,b and x f,e encode a proper time-interval by adding the constraint:
Considering only a single relation, we assume the sets R b , R d , and R e to comprise its t-fact candidates with respect to the begin , during , and end observa-tions. Then, we introduce the constraints where f has the same entity pair as r and t b , t e are begin and end of r  X  X  time-interval. Whenever x r is set to 1 for begin or end t-fact candidates, Eq. (2) and Eq. (3) set the value of x f,b or x f,e to t b or t e respectively. For each during t-fact candidate with x r = 1 , Eq. (4) and Eq. (5) enforce x f,b  X  t b and t Inter-Fact Constraints. Since we can refer to a fact f  X  X  time interval by x f,b and x f,e and the connectives of Boolean Logic can be encoded in ILPs (Karp, 1972), we can use all temporal constraints expressible by Allen X  X  Interval Algebra (Allen, 1983) to specify inter-fact constraints. For example, we leverage this by prohibiting marriages of a single person from overlapping in time.
 Previous Work. In comparison to (Talukdar et al., 2012), our ILP encoding is time-scale invariant. That is, for the same data, if the granularity of T is changed from months to seconds, for example, the size of the ILP is not affected. Furthermore, because we allow all relations of Allen X  X  Interval Algebra, we support a richer class of temporal constraints. Corpus. Experiments are conducted in the soccer and the celebrity domain by considering the works-ForClub and isMarriedTo relation, respectively. For each person in the  X  X IFA 100 list X  and  X  X orbes 100 list X  we retrieve their Wikipedia article. In addition, we obtained about 80,000 documents for the soccer domain and 370,000 documents for the celebrity do-main from BBC, The Telegraph, Times Online and ESPN by querying Google X  X  News Archive Search 1 in the time window from 1990-2011. All hyperpa-rameters are tuned on a separate data-set.
 Seeds. For each relation we manually select the 10 positive and negative fact candidates with highest occurrence frequencies in the corpus as seeds. Evaluation. We evaluate precision by randomly sam-pling 50 ( isMarriedTo ) and 100 ( worksForClub ) facts for each observation type and manually evaluating them against the text documents. All experimental data is available for download from our website 2 . 6.1 Pipeline vs. Joint Model Setting. In this experiment we compare the perfor-mance of the pipeline being stages 3 and 4 in Figure 1 and a joint model in form of an ILP solving the t-fact extraction and noise cleaning at the same time. Hence, the joint model resembles (Roth and Yih, 2004) extended by Section 5 X  X  temporal constraints. Results. Table 1 shows the results on the pipeline model (lower-left), joint model (lower-right), label-propagation w/o noise cleaning (upper-left), and ILP for t-fact extraction w/o noise cleaning (upper-right). Analysis. Regarding the upper part of Table 1 the pattern-based extraction works very well for works-ForClub , however it fails on isMarriedTo . The reason is, that the types of worksForClub distinguish the patterns well from other relations. In contrast, isMar-riedTo  X  X  patterns interfere with other person-person relations making constraints a decisive asset. When comparing the joint model and the pipeline model, the former sacrifices recall in order to keep up with the latter X  X  precision level. That is because the joint model X  X  ILP decides with binary variables on which patterns to accept. In contrast, label propagation ad-dresses the inherent uncertainty by providing label assignments with confidence numbers. 6.2 Increasing Recall Setting. In a second experiment, we move the t-fact extraction stage away from high precision towards higher recall, where the successive noise cleaning stage attempts to restore the precision level. Results. The columns of Table 2 show results for different values of  X  1 of Eq. (1) . From left to right, we used  X  1 = e  X  1 , 0 . 6 , 0 . 8 for worksForClub and  X  1 = e  X  2 ,e  X  1 , 0 . 6 for isMarriedTo . The table X  X  up-per part reports on the output of stage 3, whereas the lower part covers the facts returned by noise cleaning. Analysis. For the conservative setting label propa-gation produces high precision facts with only few inconsistencies, so the noise cleaning stage has no effect, i.e. no pruning takes place. This is the set-ting usual pattern-based approaches without cleaning stage are working in. In contrast, for the standard set-ting (coinciding with Table 1 X  X  left column) stage 3 yields less precision, but higher recall. Since there are more inconsistencies in this setup, the noise cleaning stage accomplishes precision gains compensating for the losses in the previous stage. In the relaxed setting precision drops too low, so the noise cleaning stage is unable to figure out the truly correct facts. In general, the effects on worksForClub are weaker, since in this relation the constraints are less influential. In this paper we have developed a method that com-bines label propagation with constraint reasoning for temporal fact extraction. Our experiments have shown that best results can be achieved by applying  X  X ggressive X  label propagation with a subsequent ILP for  X  X lean-up X . By coupling both approaches we achieve both high(er) precision and high(er) recall. Thus, our method efficiently extracts high quality temporal facts at large scale. This work is supported by the 7 th Framework IST programme of the European Union through the fo-cused research project (STREP) on Longitudinal An-alytics of Web Archive data (LAWA) under contract no. 258105.

