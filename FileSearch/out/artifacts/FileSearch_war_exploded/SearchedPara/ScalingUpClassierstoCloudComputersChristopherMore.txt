 As the size of available datasets has gr own fr om Me gabytes to Gigabytes and now into T er abytes, mac hine learning algorithms and computing infr astructur es have continuously e volved in an ef fort to k eep pace . But at lar g e scales, mining for useful patterns still pr esents c halleng es in terms of data mana g ement as well as computation. These issues can be addr essed by dividing both data and compu-tation to b uild ensembles of classier s in a distrib uted fash-ion, b ut tr ade-of fs in cost, performance , and accur acy must be consider ed when designing or selecting an appr opriate ar c hitectur e . In this paper , we pr esent an abstr action for scalable data mining that allows us to e xplor e these tr ade-of fs. Data and computation ar e distrib uted to a computing cloud with minimal ef fort fr om the user , and multiple mod-els for data mana g ement ar e available depending on the workload and system congur ation. W e demonstr ate the performance and scalability c har acteristics of our ensem-bles using a wide variety of datasets and algorithms on a Condor -based pool with Chirp to handle the stor a g e .
The last decade witnessed a sur ge in the a v ailability of massi v e datasets. Data collected from v arious scientic do-mains and real-w orld applications is quickly o v erwhelming computing systems and data mining algorithms, presenting a challenge for theoreticians and practitioners alik e. P ar -allel and distrib uted data mining [19 ] ha v e af forded us with scalable implementations of v arious learning algorithms, al-lo wing a capability to scale to massi v e datasets while also enabling a signicant impro v ement in accurac y .

Distrib uted data mining is a particularly attracti v e solu-tion as one can partition a dataset into subsets, distrib ute them across multiple processors, and learn independent classiers before coalescing them as an ensemble. An ad-v antage of distrib uted data mining approaches is that the partition size of the learning task can be brok en do wn to t the a v ailable (commodity) computational resources. One can easily imagine a di vide-and-conquer approach in which a dataset is distrib uted to a group of processors. Each of those processors learns a classier concurrently , and reports its classier to a central processor . The central processor can then process the predictions of the independent classi-ers learned. Distrib uted data mining leads to a creation of an ensemble or committee of  X di v erse X  classiers. Each classier is gi v en a smaller sub-task of the learning task to learn, and hence the comple xity of the learning task at hand is reduced. It also introduces di v ersity among the classiers, leading to an impro v ement in accurac y . Moreo v er , learning on the entire v ery lar ge training set, without partitioning, can force the inducti v e learner to o v er -t the problem as it will try to model the entire training set; the learned classier will then tend to lose its generality .
 data partitions to create. There really is  X no kno wn method of sample selection and estimation which ensures with cer -tainty that the sample estimates will be equal to the un-kno wn population characteristics X  (p. 26) [12 ]. T o do an y intelligent subsampling, one might need to sort through the entire dataset, which could tak e a w ay some of the ef cienc y adv antages of distrib uting the w orkload in the rst place. Contrib utions While it has been sho wn that ensemble classiers generally impro v e accurac y o v er the single clas-sier and of fer computational adv antages, v arious questions remain: 1) ho w to appropriately partition the data into sub-sets for learning? 2) what are the limits of scalability? 3) ho w to best e xploit the a v ailable resources? Thus, the k e y contrib utions of the paper are as follo ws: 1) a scalable and ef cient abstraction for distrib uting data to dif ferent sites; 2) a thorough comparison of multiple w ays of partitioning and distrib uting data; 3) a scale of datasets to e v aluate the performance three dif ferent learning algorithms  X  decision trees, k-nearest neighbors, and support v ector machines  X  under the distrib uted setting.
The problem we address lies at the intersection of data mining and high-performance computing. Accordingly , we pro vide a surv e y of rele v ant w ork from both areas.
Dataset sizes that e xceed the memory capacity of a desk-top computer pose a major challenge for data mining. This limitation can be mitigated through optimized algorithm de-sign [21 ] and the use of sampling [6 ] or ensemble meth-ods [4 ]. W ith impro v ements in multi-processor machines, and more recently multicore technology , greater scalability can be achie v ed by ef fecti v ely parallelizing algorithm im-plementations [8 , 17 , 24 , 31 ]. But these approaches remain limited because (i) performance gains often cannot be real-ized be yond 8-16 cores due to communication o v erhead and (ii) dataset sizes are restricted to the total memory a v ailable in the system, generally on the order of a fe w Gigabytes.
T o o v ercome these hurdles and achie v e not incremen-tal impro v ements, b ut drastically increased scalability , the w orkload can be di vided across a much lar ger distrib uted system, or computation grid [3 , 11 , 14 , 20 , 23 , 27 ]. This ap-proach has pro v en successful for certain tasks [2, 9], b ut such systems often require an application-specic design and implementation. In contrast, general-purpose systems may require less ef fort from the programmer and/or user b ut still cannot scale be yond se v eral Gigabytes of data [10 ].
Our w ork bridges this gap by pro viding a generic ab-straction for lar ge-scale data mining, enabling the user to run his o wn algorithms with minimal programming ef fort. The abstraction is capable of managing both data and com-putation on v arious types of distrib uted systems ranging from small clusters to lar ge dynamic computing clouds.
Modern computing systems pro vide the user with a lar ge amount of parallelism. Despite man y years of research into multi-threaded, message passing, and parallel programming languages, harnessing this parallelism remains v ery dif cult for the non-e xpert user . P arallel machines commonly used today include:
Multicor e computers : machines with multiple CPUs on a single chip that share a common RAM and run a single op-erating system image. At the time of writing, most desktop machines are tw o-or four -w ay multicore CPUs, and it is e xpected that future machines will ha v e man y more cores.
Cluster computers : collections of tens to thousands of indi vidual machines, each with their o wn (perhaps multi-core) CPU, RAM, and disk, all connected by a f ast switch to some type of centralized lesystem. A cluster is typically homogeneous, reliable, and dedicated to a single user at a time. A user that requests 16 CPUs will ha v e sole access to e xactly those 16 CPUs for the length of the request.
Cloud computers : collections of hundreds to tens of thousands of machines, dif ferent from clusters in tw o k e y respects. First, fe w centralized lesystems scale to cloud size, so a cloud mak es use of indi vidual disks on each node for both temporary and permanent storage. Second, because a cloud naturally has a high f ailure rate, it does not allocate specic nodes to users, b ut assigns resources dynamically .
T o e xploit the physical parallelism in these systems, we adv ocate abstr actions that join together simple sequential programs into data parallel graphs. This allo ws rapid re-use of e xisting data mining codes without confronting the substantial challenges of writing applications using multi-threaded or message-passing libraries. This approach has been used successfully in systems such as Map-Reduce [7 ], Dryad [15 ] and All-P airs [22 ]. In this w ork, we dene the abstraction Classify as follo ws:
As sho wn in Figure 1, the Classify abstraction feeds dataset D into process P , which creates N partitions D1...DN. These are fed into N copies of F in parallel along with testing set T , generating results R1...RN. Results are combined by process C by majority v oting into a nal re-sult R returned to the user . Classier function F is simply an e xisting sequential classier with the follo wing signature:
The user may choose from a v ariety of partitioning tech-niques for the training set. Shuf e selects data items one at a time and sends each to a random partition, resulting in roughly equal-sized partitions. A shuf e partition may also be M-o verlapping , in which an item may appear in M partitions, allo wing for more accurate sampling of minority classes b ut increasing data sizes and runtimes. Chop di vides the training set into equal pieces, preserving the e xisting order . This is typically only appropriate when the data is pre-randomized, or when the user wishes to reproduce runs e xactly . W e will sho w that the choice of partition can ha v e a signicant ef fect on the implementation.
 Classify appears similar to the abstraction Map-Reduce [7 ]. Our assignment of tasks F onto D1...DN is completed by the Mapper function, and C, the collection of results of the subclassifers into a nal classication, is the job of the Reducer function. But se v eral components of classication are not strictly accounted for by the Map-Reduce abstraction. The Map-Reduce model does not con-sider logical partitioning as a rst-class component of the model, rather it dele gates partitioning as an implementation detail of physical partitioning of the underlying lesystem. Some Map-Reduce implementations [13 , 5, 26 ] adapt the Map-Reduce model to recognize logical partitioning in v ar -ious w ays, such as allo wing for custom partitioning algo-rithms or actually including partitioning as primiti v e in their adjusted models. Mapping logical partitions onto physical partitions within the lesystem, ho we v er , remains a charac-teristic highly dependent on the implementation rather than strictly dened within the Map-Reduce abstraction.

The testing set also does not t into the Map-Reduce ab-straction well. It must either be encapsulated in the Map-per and Reducer functions  X  a departure from the logical description of the Map-Reduce abstraction  X  or it must be stored on the distrib uted lesystem at a cost of multiple replicas and signicant metadata for each instance of this one-time-use le.
 Our intent is careful study of data placement and access. Instead of attempting to deri v e Classify from the general Map-Reduce, we chose to implement an abstraction that considers classication elements relating to data placement directly as rst-class components of the abstraction model.
There are man y possible w ays to implement Classify in a parallel or distrib uted system. An implementation must choose ho w man y nodes to use for computation, ho w man y to use for data, and ho w to connect the tw o. Figure 2 sho ws se v eral possibilities we ha v e e xplored, dif fering only in where data is placed in the system. Belo w we will e xplore the consequences of each of these choices on performance.
Str eaming . The simplest implementation of Classify connects each process in the system at runtime via a str eam such as a TCP connection or a named pipe. Data only e xists in memory between processes and, e xcept for some mini-mal b uf fering, a writer must block until a reader clears the b uf fer of data. Ho we v er , this requires that all processes be ready to run simultaneously and af fords no simple reco v-ery from f ailure. If one process or stream f ails, the entire abstraction must start from the be ginning. Thus, it is an ap-propriate implementation for a multicore machine when the number of partitions is less than or equal to the number of processes. Except for v ery small w orkloads, it is not practi-cal for a cluster or a cloud where the possibility of netw ork or node f ailure is v ery high. T o mak e the abstraction rob ust, we must mak e use of some storage between processes.
Pull. In this implementation, P reads data from the source node and writes partitions back to the same node. When the v arious Fs are assigned to CPUs, the y connect to the source node and pull in the proper partition. This pro-vides maximum runtime e xibility as there is no constraint on where an F may run. Because each partition is stored on disk, indi vidual Fs may f ail and restart without af fecting the rest of the computation. Ho we v er , as we will sho w , this places a signicant I/O b urden on the source node in both the partitioning and classifying stages. The technique may be appropriate for a cluster with a lar ge central le serv er , b ut is not lik ely to scale to a cloud of an y signicant size.
Push. In this implementation, P chooses in advance which nodes will be responsible for w orking on each par -tition. As it reads data items from the training set, the y are pushed out directly to the assigned nodes. The Fs are then dispatched for e x ecution. In  X Pure Push X , each F must run only on the node where data is located. This may not be possible in a cloud, where that node may ha v e been dy-namically assigned to an unrelated task. Therefore we also dene  X Relax ed Push X , where each F prefers to run on the node with its partition b ut may also run on another node and access that partition remotely . This technique can (sig-nicantly) impro v e the performance of partitioning and the o v erall I/O rate as the number of nodes increases, b ut also increases the e xposure of the system to f ailed, slo w , or oth-erwise misbeha ving disks.

In lar ge clusters or clouds, we w ould lik e to Push data to a number of remote nodes equal to the number of parti-tions to maximize parallelism. Figure 2(a) sho ws, ho we v er , that chop partitioning to a lar ge number of remote resources be gins to reduce performance due to mo ving be yond ho-mogeneous clusters and encountering a greater v ariety of hardw are. Shuf e partitioning has its o wn dra wback in a cloud en vironment, because it requires remote connections to remain open to e v ery remote node throughout the entire partitioning.

Hybrid. T o address the limitations of Push and Pull, we also dene Hybrid. In this mode, P chooses a small set of intermediate nodes kno wn to be f ast, reliable, and of suf-cient capacity to write the partitioned data. At runtime, each F then reads its partition o v er the netw ork from these nodes. This combines adv antages of Pull (e xible alloca-tion of CPUs, reliable partitioning) with adv antages of Push (increased I/O performance). Ho we v er , it requires the im-plementation to ha v e some kno wledge of the reliability of the underlying system, which may not al w ays be possible. Figure 2(b) sho ws that remote partitioning e v en to a modest set of reliable nodes is f aster than local partitioning, without the pitf alls of Pushing data to unreliable en vironments.
W e implemented Classify using Condor [29 ] to har -ness computing resources, and Chirp [28 ] to allo w remote lesystem-lik e access to the storage at each node.
The source node is responsible for se v eral tasks: parti-tioning the data, conguring local state to describe the batch jobs, submitting the batch jobs, and collection after all jobs ha v e completed. The remote cloud or cluster nodes are re-sponsible for e x ecuting the classier instances and generat-ing the prediction output. P artitioning is described abo v e, so here we describe the remaining structures:
Local State. Local state requirements include an e x ecu-tion directory , the training and test set denitions required by all classiers, and the batch job denition les. The test set and .names dataset denition are not replicated on the local disk, b ut rather shared ef ciently . The job denition les are created after the data partitioning, and the batch jobs are submitted using these denitions.

Remote Structur e. W ithin the batch jobs themselv es, we use a hierarchical architecture of processes. The batch job that is run on each remote node is the wr apper , a stan-dard piece of code that is the same for all instances of Clas-sify . The wrapper is responsible for setting up the e x ecution en vironment on the remote compute node. The wrapper' s principal job is to e x ecute the function , a user -pro vided, application-specic piece of translational middle w are. The function e x ecutes the underlying data mining e x ecutable (the application ) and maps application-specic output to the structure e xpected by the wrapper . The function al-lo ws e x ecution of an y underlying classier without ha ving to change core pieces of the abstraction frame w ork. Collection. W e consider tw o approaches for collection. The rst, by-le , is analogous to chop partitioning. The al-gorithm completes one prediction le at a time, maintaining a plurality-determining data structure for each test instance. After all les are processed, each data structure contains the combined nal prediction. The o v erall accurac y , accurac y per class, and other statistics can be computed from these data structures. As the number of instances in the test set increases, this v ersion needs more memory to maintain data structures for each instance, with memory requirements to-taling a f actor of the product of the number of test instances and the number of classes in the dataset.

The alternati v e, collecting by-instance , is akin to shuf e partitioning. All prediction les are accessed concurrently , and only one data structure is needed as each instance is tallied serially . Memory for this v ersion remains constant as the number of instances increases, since the memory re-quirement is only a f actor of the number of classes in the dataset. On the other hand, it requires more les open at once and accesses prediction les less ef ciently .
An abstraction may decide the trade-of f between le re-sources accessed concurrently and memory used for con-current tallying data structures. F or datasets fe w classes, concurrent data structures for each partition t in memory easily e v en when the test set is lar ge. Ho we v er , for v ery lar ge numbers of classes or v ery lar ge numbers of instances in the test le, it is possible for the collection to e xceed main memory capacity . Figure 2(c) sho ws the time required to collect results of a distrib uted ensemble of classiers us-ing these tw o approaches, v arying the number of partitions. The input data is the set of prediction les from a run of the KDDCup data, chosen because it the lar gest by-le memory requirement among our datasets (approximately 91MB).
Because the lar gest set of prediction les for an y con-guration we tested consisted of less than 10MB of out-put, and thus disk space w as not a concern, our imple-mentation allo ws the batch system to return all prediction les to the submitting node, instead of using a separate le serv er or distrib uted lesystem. Because the lar gest collec-tion memory requirement of an y dataset we used w as less than 100MB, all of our results use by-le collection.
T o e v aluate the performance and scalability characteris-tics of the data mining abstraction described in the pre vi-ous section, we conduct e xperiments on a di v erse body of datasets using a v ariety of popular learning algorithms. Datasets. W e use a combination of real and synthetic datasets with v arying dimensions co v ering a wide range of sizes. The Protein dataset is real data describing the fold-ing structure of dif ferent amino acids; the task is to predict the structure of ne w sequences. The second dataset stems from the 1999 KDD-Cup 1 and contains real netw ork data; the task is to distinguish the  X good X  ones from the  X bad X  (intrusion detection). The ne xt tw o datasets, Syn-SM and Syn-LG, were produced with the Q UEST generator [1] us-ing a perturbation f actor of 0.05 and function 1 for class as-signment. The last tw o datasets, Alpha and Beta, are tak en from the P ascal Lar ge Scale Learning Challenge 2 , which were deemed more appropriate for support v ector machines. W e found that the other datasets required signicant tweak-ing of SVM parameters e v en on much smaller subsamples. The focus of our paper is primarily on scalability studies and less on parameter sweep for impro v ements in accurac y , hence we only use the Alpha and Beta datasets with SVMs. Algorithms. W e include three traditional learning meth-ods for the e v aluation of our abstraction frame w ork:
The algorithms co v er a range of computational comple x-ities and rank among the most popular learning methods. F or decision trees and support v ector machines, we used the def ault parameters pro vided by the respecti v e implementa-tions. F or k -nearest neighbor classication we used k = 5 neighbors. All of the algorithms were compiled for 32-bit x86 systems with g ++ v3.4.6 using optimization -O 3 .
W e selected these algorithms because the y naturally t the distrib ute-compute-collect paradigm. Ho we v er , it is w orth noting that with only minor modications to the ab-straction we could accommodate other learning methods as well, for e xample Distrib uted K-Means Clustering [16 ] or nding frequent itemsets using Apriori-Based meth-ods [30 ], which may require multiple distrib uted stages. Computing En vir onment The platform used as testbed for our e xperiments is a Condor pool of approximately 500 machines. The pool consists primarily of w orkstations in a uni v ersity en vironment with both 32-bit and 64-bit x86 processors and memory capacities ranging from 512MB to 4GB. Although the pool as a whole is a computation cloud with limited control for the user , a 48-node subset is in our possession, gi ving us more po wer o v er the en vironment (e.g. reliability of resources, priority status for e x ecution). The machines in this dedicated cluster are dual-core 64-bit x86 architectures with either 2GB or 4GB of total memory (1GB or 2GB per core, respecti v ely). Jobs were instructed to prefer this cluster o v er other nodes when a v ailable. F or multicore e xperiments we used 64-bit dual-core AMD Opterons with 2GB of total memory .
 Practical Considerations Our main goal is to e v aluate scalability with increasing system size, so we co v er the range from 1 to 128 nodes for the v e smaller datasets. W ith Syn-LG the memory requirements for each indi vidual par -tition are much lar ger , hence we use 48 to 256 nodes in-stead. In addition, linear support v ector machines are only tractable for the Alpha and Beta datasets. F or k-nearest neighbor classication, we reduced the test set size to 1,000 instances for the synthetic datasets and to 10,000 instances for all other datasets to k eep computation feasible within the system.
W e performed a lar ge number of e xperiments across datasets, algorithms, and system sizes as described abo v e. In this section, we summarize the results and pro vide anal-yses and insights based on our ndings with respect to the trade-of fs discussed earlier .
Since our primary interest lies in the scalability analysis, we start by e xamining the trends in e x ecution time. Figure 3 sho w the e x ecution time for decision trees, k-nearest neigh-bor classication, and support v ector machines on multiple datasets for v arying number of partitions. W ithin the grid of plots, ro ws correspond to datasets and columns correspond to learning algorithms. Each indi vidual plot contains three lines for the dif ferent data distrib ution methods.
The results for Syn-LG with decision trees and k-nearest neighbors are omitted for space reasons as the trends ob-serv ed are v ery similar to Syn-SM, albeit at a lar ger scale. In addition, for massi v e datasets it is dif cult to measure Push partitioning. This task is feasible for smaller datasets and controlled en vironments, b ut becomes more dif cult as the size of the dataset or number of hosts and di v ersity of the system increases. Ne xt, we e xamine the results for each of the algorithms in more detail.
 Decision T r ees The rst column of Figure 3 sho ws strong parallelizability of decision trees across all datasets. In most of the e xperiments, the data distrib ution does not signi-cantly inuence the e x ecution time through 16 or 32 parti-tions, demonstrating e xtensi v e, though not e xclusi v e, use of the 48-node dedicated cluster . Be yond that threshold, per -formance di v er ges as jobs be gin utilizing unreliable, hetero-geneous nodes from the computing cloud. Ev en be yond the cluster/cloud threshold, ho we v er , we are able to continue to get impro v ed turnaround times for se v eral algorithms using the Hybrid approach.

As an e xample of a case where additional parallelism did not pro vide an y added benet, the KDDCup plot for deci-sion trees sho ws that no impro v ements in e x ecution time are achie v ed be yond 32 partitions. F or decision trees in partic-ular , the small w orkloads result in v ery minimal classier training times. In addition, smaller jobs yield more relati v e o v erhead and higher costs to complete the serial stages of the process. It is unsurprising, then, that almost e xactly the same amount of time is required for the e x ecution phases when e xceeding 32 partitions. F or instance, doubling the collection time (twice as man y predictions to process per instance) requires more time than is sa v ed by the mar ginal impro v ement in e x ecution time af forded by the resources.
Another f actor impacting the scalability of e x ecutions is the data set size. The Syn-SM set continues to impro v e e x ecution time using Hybrid through 128-w ay parallelism, whereas a smaller dataset, Beta, achie v es limited further im-pro v ement be yond 32 nodes. The primary dif ference here is that for small data sets, further partitioning results in no ef fecti v e gain when balancing batch job e x ecution time against additional o v erhead from greater parallelism (parti-tioning, collection, and batch system o v erhead).
F or almost all congurations the Hybrid approach yielded shortest turnaround times, and Pull yielded the longest turnaround times. Combining the adv antages (and mitigating the disadv antages) of the Push and Pull tech-niques is particularly apparent as the number of partitions gets lar ger , and for the lar ger datasets.
 K-Near est Neighbor Classication The results in the second column of Figure 3 also sho w encouraging trends in e x ecution time with respect to the number of partitions. F or all datasets, we observ e consistent impro v ements in e x-ecution time while staying within the small cluster (up to 32 nodes) and with one e xceptions also with 64 partitions.
Pr otein KDDCup Syn-SM Alpha
Beta acr oss the various congur ations. Only for 128 partitions do we see increased e x ecution times in se v eral cases, most notably for the Push method. This beha vior is due to some jobs getting placed on slo wer ma-chines in the computation cloud. In addition, the plots only sho w times for successful runs, b ut it is w orth nothing that with Push it sometimes took se v eral attempts to complete the task without e xperiencing a f ailure in the cloud.
The aforementioned trade-of fs are also apparent in these results, in particular with dataset Syn-SM. Neither Push nor Pull are able to impro v e be yond 64 partitions, and in f act both achie v e signicantly w orse performance. Ho we v er , the e xibility of the Hybrid method allo ws it to ef ciently dis-trib ute data and computation, resulting in additional gains when going to 128 partitions.

Dataset size should also be tak en into consideration when determining the appropriate conguration for a gi v en problem. F or smaller datasets, the choice of data distrib u-tion method is lar gely irrele v ant, as all three lines e xhibit v ery similar beha vior . But for lar ge problems the Push and especially Hybrid models are better suited as using the max-imum number of a v ailable partitions achie v es the best per -formance and therefore is advisable.
 Support V ector Machines As sho wn in the right column of Figure 3, support v ector machines e xhibit beha vior dif-ferent from the other algorithms. Most notably , the major -ity of e xperiments do not achie v e the best e x ecution time for the lar gest number of partitions. And with SVMs this is not only due to heterogeneity in the computation cloud, b ut also to the strong dependenc y of the algorithm runtime on the characteristics of the data.

Once again, the data distrib ution method is less of a f ac-tor than the amount of parallelism in determining the e x-ecution time, although the pull method is consistently the w orst performer . In our e xperiments, we also observ e a tendenc y to w ards a smaller number of partitions than the other algorithms. More specically , the best performance w as achie v ed with 8 to 16 partitions in all congurations.
The same constructs that apply to running on clusters or clouds also apply to a multicore en vironment on a sin-gle machine. Figure 4 sho ws the runtime of the abstraction applied o v er v arying numbers of partitions in a dual-core en vironment. When streaming using fos, the partitioner and the classiers run in parallel. F or les, the partitioner runs rst, placing the les, which are then accessed by the classiers after all partitions are created.

Streaming using les results in mar ginally f aster turnaround times. A machine with more cores w ould clearly allo w for greater scalability up to the limit at which the ab-straction is bound by data rather than by computation. As e xpected, once be yond the number of cores, ef cienc y de-creases, as each classier is ghting for limited resources. Be yond 16 concurrent classiers, progress slo ws signi-cantly and the turnaround time is much longer than the serial e x ecution. Ho we v er , processors with a lar ge number of cores are on the horizon, and future w ork should e v aluate the Classify abstraction in such en vironments.
It is generally established that ensemble learning can result in impro v ed accurac y [4]. Our fundamental goal in this paper is to w ork with that assumption and e v alu-ate the system aspects of distrib uted data mining. F or the e xperiments we consider primarily synthetic datasets, and therefore observ e only modest impro v ements.

Figure 5 sho ws the trends for each classier on all applicable datasets. W e see that, in most cases, accurac y is quite stable with an increasing number of partitions. No-table e xceptions are increased accurac y for decision trees on the Alpha and Syn-SM datasets, and decreases for decision trees on the Beta dataset as well as k-nearest neighbors on the Syn-SM dataset with 8 partitions.
W e started the paper with three fundamental questions re garding distrib uted data mining from a cluster to a cloud. T o that end, we proposed a scalable and ef cient abstrac-tion, called Classify , to knit together sequential programs into data parallel graphs, allo wing for a seamless deplo y-ment on clusters or clouds or multi-processor machines. W e e v aluated three dif ferent and popular learning algorithms with v arying de grees of comple xity on datasets with v arying sizes up to 54 GB. T able 1 summarizes the k e y results. W e reposition them with respect to our three questions belo w . 1. How to partition the data into subsets for learning? 2. What ar e the limits of scalability? 3. How to e xploit the available r esour ces? Ackno wledgments W e w ant to thank Phil Sno wber ger for his contrib utions in the early stages of this research. This w ork w as supported in part by National Science F oundation Grants CNS-06-43229 , CCF-06-21434 , and CNS-07-20813 .

