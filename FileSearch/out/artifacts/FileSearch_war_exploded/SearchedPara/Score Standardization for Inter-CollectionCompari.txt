 The goal of system evaluation in information retrieval has always been to determine which of a set of systems is superior on a given collection. The tool used to determine system ordering is an eval-uation metric such as average precision, which computes relative, collection-specific scores. We argue that a broader goal is achiev-able. In this paper we demonstrate that, by use of standardization, scores can be substantially independent of a particular collection, allowing systems to be compared even when they have been tested on different collections. Compared to current methods, our tech-niques provide richer information about system performance, im-proved clarity in outcome reporting, and greater simplicity in re-viewing results from disparate sources.
 H.3.4 [Information Storage and Retrieval]: Systems and software  X  performance evaluation .
 Retrieval experiment, evaluation, average precision, system mea-surement Measurement, performance, experimentation
A key aim of research in information retrieval (IR) is to develop search methods with improved effectiveness, but identification of improvements requires rigorous evaluation methodologies. The Cranfield evaluation methodology and its derivatives use standard test collections, consisting of documents, topics, and judgments as to which documents are relevant to which topics. The IR systems to be evaluated are used to run the topics (formulated as queries) against the document corpus to produce a ranked list of documents or run for each topic. The relevance judgments then show which Copyright 2008 ACM 978-1-60558-164-4/08/07 ... $ 5.00. documents are relevant to the topic, and an evaluation metric is applied to the list to provide a score for the run. Run scores are aggregated, typically by taking the arithmetic mean, to produce an overall score for the system. The effectiveness of different systems can be compared by their aggregate system scores, and the statisti-cal significance of any score difference assessed with a hypothesis test [Smucker et al., 2007].

Several evaluation metrics are widely used in system measure-ment, including precision-at-d (P@ d ), average precision (AP), and discounted cumulative gain (DCG). However, these measures do not allow the performance of a system tested on one collection to be readily compared with that of a s ystem tested on a different col-lection. Variability in topic difficulty, and hence in average topic scores, means that a good score for one collection might be poor for another, depending on the mix of topics in the collection. Nor-malization by the score of an ideal ranking, as employed in AP and nDCG, can be viewed as a method for correcting for such variabil-ity, but it does so with quite limited success.

In this paper, we investigate the use of score standardization [Webber et al., 2007] to enable inter-collection score comparisons. Under standardization, the difficulty of a query is directly estimated from the scores achieved by a sample of experimental systems, and parameters derived from these estimates are then used to normalize the scores both of the experimental systems and of future systems. Standardization gives each topic the same score mean and standard deviation for the experimental systems, and reduces the effect of topic variability in the evaluation of new systems.

Standardization makes scores interpretable in themselves, and it becomes possible to directly compare scores measured on different topics and test collections. With standardization, for example, re-searchers with private collections could compare their results with-out exchange of data sets, through the use of common standardizing systems. Within a single collection, reduction in variability means that all topics contribute equally to measured differences in effec-tiveness. Across multiple collections, researchers could identify how consistent a system is in different environments.

To explore the validity of score standardization, we analyze the results for runs on several key TREC collections. Our results show that standardization leads to an average two-thirds reduction in the difference in scores achieved by an IR system on different collec-tions, enabling different systems to be evaluated against different collections and still have their performance compared. Standard-ization is also more robust to differences in collection formation than existing normalization schemes. Standardization has several benefits and no obvious drawbacks, and we propose that standard-ization parameters be published with test collections to allow richer comparison and evaluation of systems than is currently possible.
Many evaluation metrics have been described in the literature [Buckley and Voorhees, 2005, J  X  arvelin and Kek  X  al  X  ainen, 2002, Mof-fat and Zobel, to appear]. Most of these are based on precision and recall. Precision is the proportion of documents up to a specified depth (that is, ordinal rank) in a run that are relevant; recall is the proportion of all relevant documents that are returned. Due to the number of documents in current collections, only a subset of the documents can be assessed for relevance, and recall is computed based on the set of known relevant documents R . The degree of incompleteness of R is, in general, unknown.

A simple evaluation metric is precision-at-d (P@ d ), which is the proportion of the top d documents in a run that are relevant. There is no adjustment for the number of relevant documents R = for each particular topic, which can vary by a factor of a hundred or more. For instance, the maximum P@10 score that any run can receive for a topic with three relevant documents is 0 . 3 , while a ceiling effect, in which most runs have P@10 at or near 1 . 0 , can occur when there many easy-to-find relevant documents. A richer metric, which also lacks an R adjustment, is rank-biased precision (RBP) [Moffat and Zobel, to appear], in which the effectiveness score is a biased, bounded sum of relevance values. The higher the rank of a relevant document, the greater its contribution to the score, with the bias controlled by a parameter p .

Some metrics adjust for the number of documents relevant to a topic. One of these is R-precision (RP), which modifies P@ d by setting d to the number of known relevant documents R for each topic, resulting in a relatively robust metric [Buckley and Voorhees, 2005]. A more complex metric is average precision (AP), which averages the precision of a run at each relevant document returned, assigning a precision of 0 to unreturned known relevant documents. The metrics RP and AP share the characteristic that a perfect rank-ing (one which places all known relevant documents at the top), and only a perfect ranking, achieves a score of 1 .

Assigning a score of 1 to a perfect run can be achieved for other metrics, including those supportin g multi-valued relevance judg-ments, by a process of explicit normalization , where a run X  X  raw metric score is divided by the score that an ideal ranking would achieve, based on the set of known relevant documents R (and, for metrics supporting multi-valued relevance, their degree of rel-evance). The discounted cumulative gain metric (DCG) [J  X  arvelin and Kek  X  al  X  ainen, 2002], which sums the relevance contributions of each rank, discounted by a logarithmically decaying weight, is nor-malized by dividing by the score of an ideal ranking to produce normalized DCG (nDCG). Such normalization can be applied to essentially any metric. In fact AP (though not RP) is a metric in this category, and can be considered as a raw metric, sum of precisions (SP), normalized by the number of relevant documents R [Aslam et al., 2006], since the SP score for a ranking with all R relevant documents at the top is itself R . Thus, AP is normalized SP (nSP). We refer to normalization by ideal ranking as R -normalization .
A metric gives a score for a system X  X  run against a topic. For an evaluation experiment, the run scores for the topics in the test collection and the systems participating in the experiment can be considered as a matrix, as illustrated in Figure 1, in which sys-tems are rows, topics are columns, and higher scores are shown by lighter-shaded cells. Note that the easy topics (white vertical lines) stand out much more clearly than the good systems, although cer-tain poor systems are distinct as horizontal black lines. Let M be a matrix of run scores such as that in Figure 1. The score for sys-tem s achieved on topic t is denoted as m st . A system X  X  score is the mean of its per-run scores, M s  X  . It is also interesting to con-sider the mean score of a topic, M  X  t . Similarly, one can consider Figure 1: Per-run average precision scores for TREC 8 AdHoc Track the standard deviation of scores for a system, sd( M s  X  ) ,andfora topic, sd( M  X  t ) .

Test theory, which originated in the assessment of the abilities of human subjects through examination, is built on the concept of a subject X  X   X  X rue X  score, which the testing process is attempting to elicit [Bodoff and Li, 2007]. In information retrieval evaluation, a similar goal is attractive, that is, to be able to say that a system X  X  true score is simply (say) 0 . 32 , and to derive this  X  X rue X  score (or a reliable estimate of it) with as little assessment effort as possi-ble. Indeed, much contemporary IR evaluation is implicitly built on the assumption of a true score hiding behind the topic scores observed on a given collection. For instance, statistical significance tests such as the t -test implicitly ask how likely it is that the true mean metric scores of two systems are in fact the same, given the observed topic scores and assuming that they are a random sample of some larger population of topic scores.

However, such absolute interpretations of metric scores, which map from an achieved aggregate score in isolation to an evalua-tion of the performance of the system, are not possible with the metrics above, due to the high degree of variability in topic scores illustrated in Figure 1. The distribution of system scores depends heavily on which topics happen to be included in the collection, and is the reason why we always need to interpret scores in the context of a particular test collection. In particular, a system might achieve quite different scores on different test collections, and, even for re-trieval contexts for which this collection is representative, might achieve a different score had a different set of topics been cho-sen. For example, Buckley [2005, p. 311] compares 8 successive versions of the SMART system on the first 8 TREC AdHoc col-lections, and, while the improvement in AP score of the latest over the earliest version on any single collection is of the order of 50% to 100%, the best collection AP score of the earliest (weakest) sys-tem is better than 4 of the collection scores of the latest (strongest) system. Even on the one collection, a system X  X  score can only be recognized as good or bad by comparing it with the scores of other systems on the same collection. In fact, knowledge of a system X  X  score is less useful than knowledge of its rank among the set of systems run against the collection.

Figure 2 illustrates the difficulty of assigning a meaning to an absolute AP score, even in the context of a single collection. The graph displays the 95% confidence intervals on mean AP scores for Figure 2: The 95% confidence intervals on mean AP scores for TREC 8 Table 1: Mean and standard deviation of AP scores for the first, tenth, the 129 systems participating in the AdHoc Track of TREC 8. The fundamental assumption here, as with statistical significance test-ing, is that the observed scores have been randomly sampled from an underlying population of scores. Thus, for instance, the system Flab8atdn achieved an observed mean AP score of 0 . 324 ;butthe best that can be said under the random sampling hypothesis is that (with 95% probability) the system X  X  true mean AP score is between 0 . 262 and 0 . 386 . Despite being the thirteenth-ranked system by observed score, its score range overlaps with those of the second-ranked and ninety-fourth-ranked systems. So, even assuming that the collection is perfectly representative of what it is intended to test, a mean AP score is, in isolation, not very informative.
The reason for the wide confidence interval on AP scores is the variability of per-topic scores. Table 1 lists AP score means and standard deviations for the first, tenth, fortieth, and seventy-fifth percentile systems and topics as ordered by mean AP. System stan-dard deviations are remarkably similar, and the four system means range by a factor of only 2 . 5 . Topics, on the other hand, are far more variable; the four means range by a factor of almost 15 ,and standard deviations by 2 . 5 ; ordered by standard deviation, the ratio from the first to the seventy-fifth percentile is almost 6 . And this variability is despite AP X  X  use of R -normalization.

Even if system comparisons are on a single collection, and paired hypothesis tests are being used to help control the difference in topic score means, the wide variability in topic score standard de-viations means that the comparisons may be less reliable than the test results suggest. In a paired hypothesis test, the computation is based on the score deltas between the two systems; but if the score standard deviation of one topic is 6 times that of another topic, then the average score delta will also be 6 times larger, meaning the higher-variance topic will have 6 times as much influence in sys-tem score deltas and paired hypothesis testing as the lower-variance topic. Nor are high-variance topics more reliable indicators of per-formance than low-variance topics. The Pearson X  X  correlation be-tween topic reliability, as measured by item-total correlation [Bod-off and Li, 2007], and topic AP standard deviation, considering the best 75% of TREC 8 AdHoc Track systems by mean AP, is only 0 . 005 , indicating no meaningful correlation. Measured differences between systems are disproportionately due to a subset of the topics rather than to the topic set as a whole.
We propose a direct form of normalization, namely standard-ization . Score standardization is a well-known technique in tests applied to human subjects [Hays, 1991, chapter 4], but it has not to our knowledge been applied to IR evaluation. In standardization, topic scores are directly adjusted by the observed mean score and standard deviation for that topic on a sample of systems. If a topic t has a score mean of  X  t = M  X  t and a score standard deviation of  X  t =sd( M  X  t ) , and if a system s receives a score for that topic of m st , then the standardized score m st for that run is: The values  X  t and  X  t are the standardization factors for topic t . Such a score is known as a z score , and expresses how many stan-dard deviations m st is from the sample mean. As such, a standard-ized score is immediately informative in a way that an unstandard-ized one is not: one can tell directly from a run X  X  score whether the system has performed well for the topic.

In a recent workshop paper [Webber et al., 2007], we explored the impact of standardization on an historical TREC collection. We found that, within the one collection and set of experimental sys-tems, standardization evens out topic score variances, making indi-vidual run scores more meaningful. In this paper we build on and extend those results, and apply the techniques to the problem of practical inter-collection system comparisons.

Standardized z scores are centered on zero and unbounded, while most IR metrics are bounded in the range [0 , 1] . To follow this practice, z -scores can to be mapped into the [0 , 1] range, with one attractive candidate being the cumulative density function of the standard normal distribution: Normal-CDF-converted standardization is used throughout this pa-per, and is referred to simply as  X  X tandardization X  from here on. Observe that, by design, a standardized score of 0 . 5 means  X  X v-erage X , and 0 . 84 and 0 . 16 represent one standard deviation above and below average. Conversion to the range [0 , 1] also has the de-sirable property of reducing the influence of outlier data points, for instance when only one system finds relevant documents for a topic.
Figure 3 shows the distribution of per-run unstandardized and standardized AP scores for TREC 8 AdHoc Track systems. The raw AP scores are heavily skewed towards lower values, with al-most half of the per-run scores being below 0 . 2 . In contrast, the standardized scores are evenly distributed across the [0 , 1] range, with the 25th, 50th, and 75th percentiles being 0 . 26 , 0 . 50 ,and 0 . 74 .
Standardization of raw scores produces precisely the same score values as standardization of R -normalized scores, since R ization involves division by a per-topic constant factor, which iden-tically scales topic mean and standard deviation. So, standardized Figure 3: Distribution of the 6 , 450 per-run AP and standardized AP SP gives precisely the same values as standardized AP (nSP), and standardized DCG as standardized nDCG. Thus, metrics can be considered as raw, R -normalized, and standardized.

The production of standardization factors requires the existence of experimental runs from which they can be calculated. However, since the current test collection creation methodology also requires such experimental systems to form the judgment pool, in practice the requirements for producing standardized scores are no higher than for producing unstandardized scores. Once standardization factors for each topic and metric have been determined from the experimental systems, they can be published along with the test collection, and used to standardize the scores of new systems run against the collection. How many experimental systems are re-quired to derive reliable standardization factors, and how long these factors remain reliable in the face of changing (and hopefully im-proving) systems, is considered in the next section. The data used in this paper is the TREC 2004 Robust Track test col-lection and runs from the participating systems. The Robust Track of TREC is designed to examine and improve the consistency of information retrieval systems by attempting to predict difficult top-ics and emphasizing them in evaluation metrics. The document set is the AdHoc Track corpus, namely TREC disks 4 and 5, minus the Congressional Record . The TREC 2004 Robust Track topic set consists not only of 49 topics newly created for the task (a 50th was dropped when no relevant documents were found), but also the 50 topics from the TREC 2003 Robust task, and 150 topics from the AdHoc tracks of TREC 6 through TREC 8 (1997, 1998, and 1999). Relevance judgments for the earlier topic sets are reused, with new judgments being made only for the new topics. A total of 110 sys-tems from 14 different groups participated, with participating sys-tems submitting runs against both the 49 new topics and the 200 old ones. The runs submitted to the original experiments in which the older experimental sub-collections were created are also available. This data set therefore is well-suited for exploring questions of inter-collection comparability and the durability of standardization factors. However, we exclude the TREC 6 AdHoc sub-collection as the lack of topic title keywords from many topic descriptions, an issue unique to this sub-collection, causes anomalous performance from description-only runs.
 Standardization factors for a collection are calculated based on the results of the standardizing systems , that is, the systems that con-tributed to the original experiment. These standardization factors Table 2: Pearson X  X  correlation of scores and Kendall X  X   X  rank correlation are then used to standardize the scores of new systems being eval-uated against the collection. Over time, as systems improve, the standardization factors may become out of date. This can have two effects. First, comparisons within a collection can become inaccu-rate, as the relative difficulty of topics may change. Second, stan-dardizations on different collections may be based on experimental systems of different intrinsic quality. The second issue is consid-ered in Section 5, the first here.
 Table 2 gives the Pearson X  X  correlation for system scores and the Kendall X  X   X  correlation for system rankings for the TREC 2004 Robust systems on each of the earlier sub-collections, comparing in each case the results obtained by standardizing using the origi-nal experimental systems and standardizing using the TREC 2004 Robust systems. Note that the Pearson and Kendall X  X   X  correla-tion coefficients work on different scales and so cannot be directly compared to each other. The Kendall X  X   X  should be compared with the 0 . 742 correlation for ranking the TREC 2004 systems based on the TREC 2003 versus the TREC 2004 topics; the Pearson X  X  co-efficients should be compared with the 0 . 943 correlation on scores between the two topic sets. Clearly, standardization using relatively outdated systems is much less distorting than comparisons between different test collections.
 The standardization factors derived from the standardizing systems can be thought of as estimates of the true factors for the topic. A key question is how many systems are required in order to get reliable standardization factors, and, in particular, what effect reducing the number of standardizing systems has upon system ranking. Here, the benchmark is the ranking obtained from standardization factors derived from the full experimental set. The following experiments use the relevance judgments from the full judgment pool, limiting the number of participating systems only when calculating stan-dardization factors.
 The test uses as standardizing systems the TREC 2003 Robust Track systems, and the test collection is the topics created for that track. The evaluated systems are the TREC 2004 Robust Track systems as run against the TREC 2003 topics. The procedure is to sample from the standardizing systems, derive standardization factors from the sample, use these to standardize the scores of the evaluated systems, and calculate the Kendall X  X   X  between the sys-tem ranking from the sampled standardization and from the full standardization. This is repeated multiple times for each sample size. The 50th (median), 95th and 99th percentile lowest Kendall X  X   X  figures are recorded. The whole process is then repeated for other sample sizes.

Figure 4 reports the Kendall X  X   X  for varying sample sets, giv-ing median values and lower-end percentiles, using standardized AP. In comparison, the Kendall X  X   X  on system rankings on the TREC 2004 Robust systems between the TREC 2003 and the TREC 2004 topics using unstandardized AP is 0 . 742 , and between the unstandardized and standardized AP scores for the TREC 2003 Figure 4: Percentiles of Kendall X  X   X  between rankings on partial and full sub-collection is 0 . 919 . Even taking the 99th lowest percentile, as few as 5 of the 78 systems need to be sampled for standardization factors that give more consistent results than inter-collection com-parisons, while 10 to 15 systems are sufficient to better the corre-lation with unstandardized scores at the 95th and 99th percentiles. A small set of systems, therefore, is sufficient to provide standard-ization factors that give reliable system rankings, far smaller than is needed to provide the relevance judgments. In investigating the question of cross-collection comparability, two kinds of collections need to be considered. The first is collections that we know to be drawn from the same population under the ran-dom sampling hypothesis. By definiti on, significance tests between two such collections are statistically valid, if it is understood that their results are being extended only to other samples of this pop-ulation. The second is collections where it cannot be assumed that they have been randomly sampled from the same underlying popu-lation, that is, where there may be factors that cause one collection to be significantly different from another.

If we use random sampling, then the sampled values will be-have as an independent and identically distributed variable, and the theoretical basis of hypothesis testing will be met. Such randomly-sampled collection pairs can be approximated by randomly sam-pling from the topics of an existing collection, or set of collections. Any set of collections can be us ed and still, via r andom sampling, be considered identically sampled, but it is preferable to choose collections that are relatively homogeneous. Here, the 100 topics from the AdHoc tracks of TREC 7 and TREC 8, Topics 351 X 450, are used, and the runs are those made by the TREC 2004 Robust Track systems. The topics (and associated runs) are randomly par-titioned into two halves to form two r andomly sampled collections. (The fact that we are sampling from such a small population, with-out replacement, means that the assumption of independence is vi-olated, but the results are adequate for our current purposes.) The random partitioning is repeated mu ltiple times to generate a set of identically-sampled collections.
 Figure 5: Mean standard-deviation normalized root mean square error
Score comparability between collections means that, if we run the same system against two collections, it should receive similar scores for each collection. Here,  X  X imilar X  can be understood in the loose sense of producing aggregate system scores that are not too different; or, more narrowly, as producing sets of topic scores that are not found to be significantly different under statistical testing.
System score comparability can be measured using root mean squared error (RMSE). Continuing the notation of Equation 1, let S be our set of evaluated systems. Consider two collections, C and D .Let M C s  X  be the score under some metric that system s  X  S achieves on collection C (that is, the mean of the scores that s achieved on the topics making up C ), and similarly for M Then the root mean squared error between C and D is: The RMSE is dependent upon the magnitude of the score values for a metric; if scores for one metric are precisely ten times the scores for another, then the RMSE will be ten times greater, even though comparability is effectively the same. To facilitate comparisons between different metrics we normalize by dividing by the average standard deviation of system scores for each collection, to derive standard-deviation normalized root mean square error or dRMSE: dRMSE = 2 Note that normalizing by the geometric rather than the arithmetic mean of the two standard deviations produces almost identical re-sults in practice.

Randomized topic set re-sampling can be used to derive distri-butions of dRMSE figures for different metrics. Figure 5 gives the results of multiple random partiti onings of the TREC 7 and TREC 8 AdHoc topics. The metrics P@10, RBP with persistence p =0 . 95 , SP (unnormalized AP), and DCG are compared, together with their R -normalized and standardized versions. The results show that ev-ery metric with standardization is more stable than all metrics in their raw form. And standardization leads to significantly greater stability than R -normalization, even on identically-sampled col-lections. (As will be seen later, normalization is far less robust to differently-sampled collections.)
A second form of collection comparability is finding statistically significant differences. If the same system is tested on two dif-ferent collections, then the results on the two collections should not be found to be significantly different; if they are, then that is a false positive, or at least the collections are not comparable, since obviously a system is not significantly different from itself. The false positive rate for a metric on two collections therefore is taken from the number of systems found to be significantly differ-ent from themselves on the two collections. The significance test employed here is a two-tailed, two-sample t -test, at significance level  X  =0 . 05 .

Randomized topic set re-sampling can also be used to test the false positive rate. Figure 6 gives the upper end of the 95% con-fidence interval on false positive rates for the TREC 2004 Robust Track systems over the TREC 7 and TREC 8 AdHoc Track top-ics. Due to random sampling, the mean false positive rates for every metric and form of normalization are close to the signifi-cance level of 0 . 05 (they range from 0 . 042 to 0 . 052 ). By look-ing at the upper end of the confidence interval, we are instead ex-amining a reasonable upper bound on how high the false positive rate can go when comparing two (identically sampled) test collec-tions. SP and DCG have higher discriminative power than RBP or P@10, so the fact that they have higher potential false positive rates is not surprising. However, standardization enormously decreases the upper-end false positive rates, from around 50% to just over 15%. This is achieved without harming discriminative power. For instance, for the TREC 8 sub-collection, the proportion of system pairs found significantly different on a two-tailed, paired t -test at level  X  =0 . 05 is 68 . 7% for DCG, 69 . 3% for nDCG, and 68 . 8% for sDCG. Normalization by R , in contrast, does little to improve false positive rates. That is to say, even where the hypothesis of ran-dom sampling from an underlying population is observed (as is the case here), use of standardized metrics rather than R -normalized metrics leads to far more reliable inter-collection comparisons. Examination of inter-collection metric comparability between two identically-sampled collections is a best-case situation, where the statistical equivalence of the collections is artificially created. In practice, different collections are not identically sampled. How-ever, the AdHoc and Robust TREC collections use the same doc-ument corpus and were built with similar methodologies, so com-parability between them would be desirable. We now explore the comparability of metrics in these circumstances, and the effect of R -normalization and standardization on this comparability.
Table 3 shows the dRMSE of system AP scores for each pair of collections used in the TREC 2004 Robust Track. The two Ad-Table 3: Standard-deviation normalized root mean square error for system Table 4: Number of the 110 TREC 2004 Robust Track systems that were Hoc collections are relatively close to each other, as are the two Robust collections. For instance, the observed dRMSE of 0 . 63 be-tween the TREC 7 and TREC 8 AdHoc collections is close to the mean randomized dRMSE over these topics of 0 . 60 reported in Fig-ure 5, indicating that from the perspective of this statistic the two collections are not significantly different. However, comparisons between any of the AdHoc and any of the Robust collections are problematic. The observed dRMSE of 1 . 857 between the TREC 7 and TREC 2003 collections, for example, compares with the mean randomized dRMSE across those two collections of 0 . 596 ,andin fact falls beyond the 99th percentile of randomized values, mean-ing that the two collections are highly significantly different for this statistic when using AP. Table 4, which gives false positive rates, also indicates severe problems. Almost all systems seem signifi-cantly better than themselves when evaluated using AP against the TREC 2003 collection than when evaluated against the TREC 7 collection, and again this false positive rate is beyond the 99th per-centile of randomized values.

Table 5 gives the inter-collection dRMSE of standardized SP/AP scores. As anticipated from Figure 5, the standardized scores have a much lower dRMSE for every collection pair than do the R normalized AP scores in Table 3. More particularly, the dRMSE figures are similar for every collection pair. The observed dRMSE figures for standardized AP are well within the 95% confidence in-terval found by randomization, and in fact sit quite close to the re-spective means, indicating that, for dRMSE with standardized AP, the collections are not significantly different. The false positive rates (not tabulated for space reasons) are also much improved, av-eraging 5% and not exceeding 11% for any collection pair, with no strong effect between AdHoc and Robust collections.

Figure 7 gives the mean dRMSE scores for various metrics, in their raw, R -normalized, and standardized forms. The value of 1 . 1 in the middle bar of the SP/AP group, for instance, is the mean of the six values reported in Table 3. Note that these means include both the two same-track pairs and the four different-track (Robust-to-AdHoc) pairs; if only the latter were included, the results would be even less flattering to R -normalization. Standardization mod-erately improves RBP X  X  observed cross-collection comparability, and, unexpectedly, marginally worsens that for P@10. However, the improvements for SP/AP an d DCG are dramatic, even from their R -normalized forms. Table 5: Standard-deviation normalized root mean square error for system Figure 7: Mean standard-deviation normalized root mean square error The R -normalized metrics are even less comparable between the Robust and AdHoc collections than for identically sampled collec-tions because of differences in the constitution of the set of known relevant documents R . Both Robust and AdHoc judgment pools were formed by pooling to depth 100 (depth 125 for TREC 2003), but the number of participant groups and therefore pooled sys-tems was quite different, with 42 and 41 systems pooled for the two AdHoc collections and only 16 and 14 for the Robust ones. Moreover, the AdHoc tracks included a large number of manual runs, identifying around 25% of the known relevant documents, whereas the Robust tracks had none. The consequences can be seen in Table 6. The average number of known relevant documents per topic is greater for the AdHoc than for the Robust collections. The Robust topics are not harder than the AdHoc ones, with the TREC 2004 Robust systems receiving very similar average system P@10 (and also RBP, not shown) scores in each of the four test en-vironments. However, the R -normalized metrics such as AP (and nDCG, not shown) are misled by the smaller values of R in the two Robust test environments into thinking their topics are harder, and the corresponding normalized scores are higher than for the AdHoc test environments. Conversely, SP (and DCG, not shown), being non-convergent metrics that evaluate deep in the runs, give higher average scores to the sub-collections with more known rel-evant documents. Standardization, shown in the last row, is not af-fected by the changes in R . Note that, as one would hope, slightly improved sAP scores are calculated for the TREC 2004 Robust sys-tems when they are standardized using the original systems X  scores.
The conclusion of these experiments is clear: although (or per-haps because) it sets out to adjust scores to reflect the weight of relevance for a topic, R -normalization is in fact very sensitive to variability in the way in which the set of known relevant documents is determined. In contrast, standardization is robust to such differ-ences, making collections with significantly different R formations comparable in the same way that identically sampled ones are. And even where R estimates are compatible, standardization offers far greater comparability, as the ra ndomized tests predicted. Table 6: Mean number of documents judged and mean number of
Average precision was developed in the context of TREC [Buck-ley and Voorhees, 2005]. Although it has been widely used for over a decade, there is no definitive paper describing the metric, and it has only recently been analyzed in th e literature. Discounted cumu-lative gain and its variants are described in J  X  arvelin and Kek  X  al  X  ainen [2002]. Rank-biased precision is described by Moffat and Zobel [to appear].

Determining the quality of a metric can easily become a circu-lar problem: a good metric is one that highly ranks good systems, but how do we know what the good systems are without first using a metric to judge them? A common approach is to examine the statistical features of metrics. Buckley and Voorhees [2000] and Sanderson and Zobel [2005] calculate the error rate of a metric by randomly partitioning a topic set and counting the number of times the resulting subsets order system pairs differently; metrics with lower error rates are regarded as m ore stable and therefore better. Similarly, Sakai [2006] suggests that the sensitivity of a metric be determined by the proportion of system pairs found to be signifi-cantly different under an hypothesis test; he proposes the bootstrap test for this purpose. Aslam et al. [2005] propose that the quality of a metric can be determined by using a maximum entropy analysis: the more constraints that a given metric score places upon the possi-ble rankings it could have been derived from, the more information that metric provides, and hence the better it is.

An alternative approach to assessing evaluation metrics is to ex-amine how well they correlate with user experience. Huffman and Hochster [2007] found that reported satisfaction of assessors corre-lates fairly strongly with relevance among the top three documents or even simply the very top-ranked document; however, their ex-periments used professional assesso rs attempting to interpret the information needs and satisfaction of the users who submitted the sampled queries. In contrast, Al-Maskari et al. [2007], working with users judging their own satisfaction, found only weak cor-relation between most metrics and user satisfaction. Rather than self-satisfaction, Turpin and Scholer [2006] gave users two specific tasks: find a single relevant document in the least time; and find as many relevant documents as possible in five minutes. Turpin and Scholer found no significant correlation between the average AP score of a system and user performance on the first (precision) task, and only a weak correlation on the second (recall) task.
It is one thing to determine that system A has scored higher than system B on a given collection and metric; it is another to confirm that this difference in scores is significant. Zobel [1998] examines the use of the t -test, ANOVA, and Wilcoxon test, and finds that the t -test and Wilcoxon diverge. Savoy [1997] examines the theoreti-cal basis of hypothesis testing in the IR environment, and proposes the use of the bootstrap hypothesis test. Smucker et al. [2007] pro-pose the randomized permutation test as requiring less assumptions about data distribution and sampling. They demonstrate that the t -test and Bootstrap tests give almost identical results, with the ran-domization test being similar, but that the Wilcoxon test diverges.
Bodoff and Li [2007] suggest that collections be viewed less as random samples from an underlying population, and more as pur-posefully created tests, similar to tests that might be applied to stu-dents. They then introduce ideas from test theory such as the relia-bility of indi vidual test components, incl uding indivi dual topics.
Zobel [1998] normalizes metric scores by dividing a run X  X  score by the highest score achieved by any run for that topic; this is done primarily to improve the comparability of scores achieved by dif-ferent topics. J  X  arvelin and Kek  X  al  X  ainen [2002] propose that scores should be normalized, not by the highest scores achieved, but by the highest score achievable, given the known distribution of rel-evance. Mizzaro and Robertson [2007] normalize per-run scores, either by topic or system, by subtracting the mean observed score for that topic or run; they do not, however, adjust for variance.
The high degree of variance in topic score distribution and by implication topic difficulty has been widely commented on. Using ANOVA techniques, Tague-Sutcliffe and Blustein [1994] observe that the topic effect is much stronger than the system effect; that is, there is more variation between topic scores than between system scores.

To our knowledge, comparing systems on disparate collections has not been systematically explored, although the practical results of Buckley [2005] indicate the difficulty of doing this with AP.
Accurate measurement is integral to improvement in all fields of science. Having measures that are reproducible, comparable, and immediately interpretable would enormously facilitate the identifi-cation and acceptance of advancements in the discipline. The eval-uation metrics currently in use, however, do not provide these char-acteristics. Instead, experimental results for one system can only be interpreted by explicit comparison with other systems, and system comparison can only meaningfully be pursued by testing all sys-tems on the one collection, something that is always inconvenient and often impossible. Worse, the existing normalization methods, reliant as they are upon an inevitably incomplete sample of the set of relevant documents for each topic, can exacerbate the problem of non-comparab ility between different collections, if the differ-ent collections have had different relevance assessment inputs. In contrast, standardization greatly increases the ability to compare system results within and between test collections, and allows for wide differences in performance to be immediately detected from aggregate scores, without the need to exhaustively test all systems on the one collection.
 Acknowledgment . This work was supported by the Australian Re-search Council.
 Standardization factors for common TREC collections and metrics can be found at: http://www.csse.unimelb.edu.au/ ~ alistair/ir_eval/
