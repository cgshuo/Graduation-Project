 A new class of associations (polynomial itemsets and polyno-mial association rules) is presented which allows for discover-ing nonlinear relationships between numeric attributes with-out discretization. For binary attributes, proposed associa-tions reduce to classic itemsets and association rules. Many standard association rule mining algorithms can be adapted to finding polynomial itemsets and association rules. We applied polynomial associations to add non-linear terms to logistic regression models. Significant performance improve-ment was achieved over stepwise methods, traditionally used in statistics, with comparable accuracy.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms, Experimentation, Performance Association rules, continuous attributes
Association rule mining is a well established data mining approach [1, 8]. Almost all association rule mining algo-rithms require binary or categorical attributes. The stan-dard approach to continuous attributes is discretization [11]. This approach can lead to significant information loss and incurs problems such as choosing correct interval widths.
We present polynomial itemsets and polynomial associ-ation rules which allow for discovering complex nonlinear relationships between attributes without the need for dis-cretization. An itemset is defined simply as a polynomial (more strictly a monomial) on a set of the attributes, and its support, as a fraction of records in which all its attributes (in their respective powers) are close to their maximum. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. An application to nonlinear logistic regression is presented. Despite many new classification algorithms available, logis-tic regression is still the major classification workhorse in natural and social sciences. While nonlinear regression mod-els are possible, methods for building them are very ineffi-cient on large datasets. This part of the paper is based on [10], where a similar approach has been presented in the context of subgroup discovery. The method is motivated by boosting [5, 7], where weights are chosen for examples in such a way that the currently built classifier loses all its pre-dictive power. New terms are then found in the reweighted dataset, making it likely that they will explain part of class variability not explained before.

As itemsets are simply monomials, they are identical to nonlinear terms used in regression models [2], and are thus naturally suited to finding terms for logistic regression. An algorithm is presented for updating logistic regression mod-els using polynomial associations, which achieves accuracy comparable to standard stepwise methods, with dramati-cally better performance on large data.

There has been little work on association rules for con-tinuous attributes, not requiring discretization. A notable exception is [12] where a general framework for defining sup-port measures has been presented, and a measure of sup-port for continuous attributes, not requiring discretization, defined within this framework. Our approach differs by al-lowing different powers of attributes significantly extending the range of nonlinear relationships which can be discov-ered. Also, the author believes that the measure of support introduced in this paper has a more intuitive interpretation.
Nonlinear relationships have been used in the context of equation discovery [15, 4], and classification (Support Vec-tor Machines, spline regression). Those methods are not directly applicable to association pattern mining.
A method for using association rules to guide adding terms to logistic regression models has been presented in [6]. There has been some amount of work on using association rules to construct or update classification models, see e.g. [14]. All those methods are presented for categorical attributes only.
Let D be a dataset. Let H = { x 1 ,...,x n } be the header of D .Let t [ x ]denotethevalueof x in a record t  X  D .We assume that all attributes x 1 ,...,x n are continuous. Non-continuous attributes, are converted as follows. Binary at-tributes are replaced with continuous attributes taking val-ues in { 0 , 1 } in the obvious way. Categorical attributes are replaced with binary attributes (one for each category).
Definition 2.1. A polynomial itemset is an expression of the form where x i j are distinct attributes (elements of H ), and  X  { 1 , 2 ,... } for all j  X  X  1 ,...,r } .
 The degree of an itemset I = x  X  1 i 1 x  X  2 i 2 ...x  X  r deg( I )=
We now define support of polynomial itemsets. Intuitively we want to define the support as the proportion of records in which all factors x  X  j i j have simultaneously high ( i.e. close to their respective maximums) absolute values. The reason is that when the itemset is used for example as a term in polynomial regression it should significantly influence the result in a large number of records. A formal definition is given below.

Definition 2.2. For each x i  X  H ,let c i  X  R ,c i &gt; 0 be a as The constants c i always exist provided that no attribute is equal to 0 in all records.

The amount of support, a record t gives to an itemset is all attributes in the itemset the record will contribute highly to the support. When some attributes are close to zero, t will contribute only marginally.
 Let us now look at the properties of polynomial itemsets. An itemset I 1 = x  X  1 i 1 ...x  X  r i r is a subset of an itemset I x 1 ...x an l  X  X  1 ,...,s } such that i k = j l and  X  k  X   X  l . Theorem 2.3. For any two polynomial itemsets I 1 , I 2 ; I 1 I 2 implies supp D ( I 2 )  X  supp D ( I 1 ) .

Theorem 2.4. For binary attributes and an itemset I = x 1 ...x sical definition of support, regardless of the values of  X  The first proof follows since the c i factors guarantee the absolute values of attributes never exceed 1. The second is obtained by replacing all binary attributes with continuous attributes taking values in { 0 , 1 } .

Support can be generalized to infinite populations as fol-lows: supp( x  X  1 i 1 ...x  X  r i r )= E ( is the expected value over all possible vectors t . Suppose all attributes x 1 ,...,x n are independent and uniformly dis-tributed on [0 , 1]. All c i  X  X  in Definition 2.2 are 1 and thus omitted. Let us now look at properties of support of poly-nomial itemsets on this population. Notice first that This shows that the support of an attribute decreases in-versely proportionally to its exponent. This is desirable since it favors polynomials with low degrees.

Take now an itemset x 1 i 1 ...x 1 i r . Since all variables are independent and uniformly distributed on [0 , 1], we have It can be concluded that for independent, uniformly dis-tributed attributes the support of polynomial itemsets with all exponents 1 behaves exactly like the support of indepen-dent binary attributes, each with support 1 2 . This provides further justification for the presented support measure.
Let x 1 and x 2 be two attributes such that c 1 = c 2 =1,it can be shown that supp( x 1 1 x 1 2 )  X | cov( x 1 ,x 2 )+ x x denotes the sample mean of attribute x . The inequality becomes most useful when x 1 = x 2 = 0, stating that a 2-itemset with low support necessarily has low covariance. Outliers. Requiring that every attribute be scaled so that the maximum of its absolute value is 1 may seem very harsh; even a single outlier with a very high absolute value may cause the support of an itemset to be close to zero.
The positive side of this property is that itemsets con-taining attributes with outliers are naturally eliminated. If this is not desirable, outlier removal can be applied prior to polynomial itemset mining.
For two polynomial itemsets I = x  X  1 i 1 ...x  X  r i r and J = x 1 ...x defined as a polynomial itemset IJ = x  X  1 i 1 ...x  X  r i Definition 2.5. A polynomial association rule is a pair I  X  J , of polynomial itemsets I , J with disjoint sets of at-tributes. Define its confidence as conf D ( I  X  J )= supp D ( Intuitively we want the absolute value of J to be high in those records where the absolute value of I high. The above definition loosely corresponds to the proportion of records with high value of I wherethevalueof J is also high. For binary attributes the definition becomes classical confidence.
Due to Theorem 2.3 many Apriori style frequent item-set mining algorithms can be applied to mining polynomial itemsets after only minimal changes.
 Figure 1 shows the adaptation of the Apriori algorithm [1]. Itemsets are generated in the order of increasing degree, sup-port counting in step 2 is done by a simple database scan.
Candidate generation is done in steps 5 to 8. Note step 6, which increases the exponent of the last attribute in the itemset. This corresponds to adding to the itemset an at-tribute which is already present in it, which is the main dif-ference from standard Apriori candidate generation. Steps 7 and 8 generate more candidates than Apriori for the sake of simplification, extra candida tes are removed in step 11.
Unfortunately support does not decrease fast enough with the increase of degrees of attributes in the itemset (compare Equations 3 and 2). Maximum degree of itemsets was thus limited to 5. In practice this limit is not a problem since polynomials of very high degree are rarely useful. Input: dataset D with header H , min. support min supp Output: all polynomial itemsets with support  X  min supp 1: k  X  1; C 1  X  X  x 1 i : x i  X  H } 2: compute support of all itemsets in C k 3: F k  X  X  I  X  C k : supp D ( I )  X  min supp } 5: for all I = x  X  1 i 1 ...x  X  r i r  X  F k do 7: for all j&gt;i r do 9: end for 10: end for 11: remove from C k +1 all itemsets with a subset ( )ofde-12: k  X  k +1; goto 2 Figure 2: Performance of PolyApriori on benchmark datasets for different values of min supp We implemented a depth first version of the algorithm in Python on a 1.7GHz Pentium4 machine. Figure 2 shows the performance of the mining algorithm for a number of datasets. A mixture of small and large datasets was used (see Table 1). The class attribute was treated like any other categorical attribute. Times for the spambase and iris datasets were dominated by startup time and are omitted for clarity. It can be seen that the algorithm performs well, even for large datasets, until the minimum support becomes too low. This is consistent with frequent set mining algo-rithms for binary data.

Short running time for spambase dataset is explained by small number of frequent itemsets it generates. This is a result of very skewed distributions of the items, when effects of outliers come into play.
We now give some examples of polynomial association rules found in an artificial dataset and the sonar data from the UCI repository. The artificial dataset sin consists of 10000 points drawn uniformly at random from the interval [  X  1 , 1]  X  [  X  1 , 1]. To each point ( x 1 ,x 2 ) we assign a class y as follows:
Figure 3a depicts the situation. It can be seen that the re-lationship between x 1 ,x 2 and y is highly nonlinear. Table 2 Table 2: Association rules with highest confidence for the artificial sin dataset. shows four association rules with the highest confidence for consequent y ,aswellastherule  X  X  X  y for comparison. Rules have been mined with min supp of 10% and maximum itemset degree of 4 allowed.

It can be seen in Figure 3a that the class y is equal to 1 mostly for values of x 2 close to  X  1 or 1. Similarly, the polynomial itemset x 3 2 has high absolute value for x 2 close to  X  1 or 1 and value close to 0 otherwise. This explains high confidence of the rule x 3 2  X  y . Similar argument holds for x are high over a larger area their confidences are lower. The the domain. This is also where the class is 1, thus the high confidence of x 1 1 x 1 2  X  y , see the contour plot in Figure 3b.
The confidence of  X  X  X  y is much lower than that of the most confident rules, which shows that they correctly iden-tify areas where y = 1. The example also shows some limita-tions of expressiveness of polynomial association rules. No single rule is able to separate the area of y =1nearthe x 1 = 0 axis from the areas of y = 0 to the left and to the right. However linear combinations of polynomial associa-tion rules can be much more expressive; a classifier based on polynomial itemsets achieves very high accuracy on the dataset, see Section 6.

Table 3 shows the most confident rules found in the sonar data to predict class= X  X ock X  . Several non-linear rules with high confidence have been discovered. The confidence of the top rules is significantly higher than that of the rule  X  X  X  class= X  X ock X  . Table 3: Association rules with highest confidence for the sonar dataset.
 boundary of the classifier given in Equation 4 (c).
Let us extend the header of D with a binary attribute y which will be our target attribute. Logistic regression models the probability of y =1intermsof x 1 ,...,x n using the following equation where logit( p )=log maps the predicted probability onto (  X  X  X  ,  X  ), such that it can be predicted by a linear model, see [2, 9] for details. Coefficients  X  0 , X  1 ,..., X  n are estimated using the maximum likelihood method. The model can be used to predict the value of Pr { y =1 } (by inverting the logit function).
The method is not limited to linear models and continuous data. Categorical variables can be incorporated by replac-ing each of them with a number of real zero-one variables. Nonlinearity is usually handled by including monomials in x ,...,x n as terms in the model. Those monomials are in fact identical to our polynomial itemsets. See [2] for details.
Most statistical packages include procedures for automatic selection of such non-linear terms. Most procedures work in a stepwise fashion [2, 9]. In forward steps the improve-ment (measured e.g. using AIC score) made to the model by adding each selected new term is computed. The best term is selected and added to the model. The backward step deletes the term whose removal gives the maximum model improvement. In the forward step not all possible terms are checked. Usually, new terms are obtained by adding every possible new variable to terms already in the model.
Since the coefficients of the model need to be recalculated for each possible term, the procedure can be very inefficient. Also, only a small fraction of the search space can effectively be checked, despite very long computation times. Another problem is that, most implementations do not take into ac-count for the fact that statistical tests are repeated many times during the procedure, which may result in overfitting.
Despite those shortcomings, logistic regression is still the major classification workhorse in natural and social sciences. This is due to well established statistical procedures for model verification, availability in major statistical packages, as well as a long tradition.
We use polynomial association rules to improve logistic regression models, making it possible to obtain small, simple models which can be constructed efficiently and nevertheless Input: min supp  X  minimum support, max deg  X  X aximum itemset degree, dataset D with header H = { x 1 ,...,x n ,y Output: T best , X  i  X  terms and coefficients of the best model 1: Split D into training set D t and validation set D v 2: T  X  X  X  set of terms of the logistic regression model 4: Fit the model logit(Pr { y =1 } )=  X  0 + 5: acc v  X  accuracy of fitted model on (unweighted) D v 6: if acc v &gt; acc best then 8: end if 9: acc t  X  accuracy of fitted model on (unweighted) D t 10: for all t  X  D t do 11: end for 12: re-scale the weights of records in D t to add up to 1 13: F  X  frequent polynomial itemsets in D t (weighted). 15: T  X  T  X  X  I  X  } 16: goto 4 see text for the stopping criterion 17: Re-fit the best model on full dataset D Figure 4: The PolyForward algorithm for building lo-gistic regression models based on polynomial item-sets. provide high classification accuracy. This is achieved with a standard logistic regression model to which all standard statistical tools can be applied, and which, as long as the number of terms is not too large, is human understandable.
Figure 4 shows the PolyForward algorithm for updating logistic regression models using polynomial itemsets. The name comes from the fact that the algorithm implements the forward stage of a stepwise algorithm. In the Figure, t [ w ] denotes the weight of record t .

Overall, the algorithm works as follows: at each iteration the training set is reweighted so that the current model loses all predictive power [5, 7] on it. Frequent polynomial item-sets are the mined from reweighted data. The frequent item-set most correlated with the class y is picked and used as the new model term. The rationale is that polynomial itemsets provide a source of nonlinear terms of correlated variables, each term explaining part of variability of y not explained by previous terms due to the reweighting in step 12.
To avoid overfitting, the initial dataset D is split into training ( D t ) and validation ( D v )parts. D t is used to fit lo-gistic regression models, and D v for selecting the best model. We use two thirds for training and one third for validation.
In step 4 current models coefficients are found on un-weighted training set. The model is tested on validation set (step 5) and its accuracy compared with current win-ner. In step 12 the training dataset is reweighted such that the current model has no predictive power; under the new weights, its accuracy is exactly 0 . 5. This is exactly the type of reweighting which is done in AdaBoost and the reader is referred to boosting literature for details [5, 7]. If we want to predict probabilities, not just the class, re-weighting in step 12 should be replaced with the method from [10].
Steps 13 and 14 are the core part of the algorithm. First (weighted) frequent polynomial itemsets are found, then the one with highest weighted sample correlation coefficient with y is chosen as the next term in the model.

The use of linear correlation is not entirely legitimate as it does not coincide with maximum likelihood in case of logistic regression [2]. Ideally we should retrain the model for ev-ery new candidate term using maximum likelihood and pick the candidate which gave the biggest improvement. Such a procedure is used by stepwise regression algorithms in most statistical packages but is much to slow to apply to all fre-quent itemsets, so we decided to use linear correlation any-way and count on the maximum likelihood procedure to as-sign totally non-predictive terms weights close to zero. The biggest risk is the performance loss due to adding useless terms. We hope that the inaccuracy during term selection will be offset by much larger size of the searchspace.
Terms which did not cause an increase in accuracy are not removed from the model in hope that combined with terms added in the future they may become useful.

It has been shown in [7], that boosting algorithms are in fact building logistic regression models. However after adding each term, the coefficients remain constant. This is the main difference from our approach (apart from using polynomial terms), where all weights are recomputed after adding each term. We use polynomial itemsets only to select terms for the model, not to find the coefficients.
There are three stopping criteria (applied after step 7): achieving perfect accuracy on the training set (non-random search based on D t is no longer possible), lack of frequent itemsets after reweighting, and an arbitrary limit of 30 on the number of iterations.
Unless otherwise stated all experiments are performed with 5% minimum support and maximum itemset degree of 4.
Let us first show an illustrative example of a model found for the artificial sin dataset. The model contains 5 terms and is given by the equation: where  X  =  X  0 . 61+23 . 87 x 2 2 +175 . 81 x 4 1  X  157 . 83 x 16 . 82 x 4 2 . Figure 3c shows contours of the predicted class. Despite the model X  X  simplicity, the nonlinear relationship between x 1 ,x 2 and y is modelled very well. Prediction ac-curacy (5-fold cross-validation) is above 96%, much higher than for simple logistic regression and comparable to leading classification algorithms such as boosted decision trees. Table 4: Performance comparison of PolyForward with other classification algorithms
In this section we evaluate the performance and accuracy of the PolyForward algorithm and compare it to boosted decision trees, a leading classification algorithm. It seems natural to also compare with Support Vector Machines, esp. with polynomial kernels. Unfortunately we found SVMs to require individual tuning of parameters for each dataset, and using default parameters gave poor results. We thus com-pared only with boosted decision trees. It should be noted that both boosted decision trees and SVMs produce huge models, involving large trees and large numbers of support vectors (often over half of the training set), while our models stay simple most of the time and can at least be inspected, if not understood, by the user.

For boosted decision trees, the AdaBoostM1 algorithm im-plemented in the Weka [13] package was used. The boosted classifiers were Quinlan X  X  J4.8 decision trees. To compare with standard statistical logistic regression procedures, we used the stepwise regression implemented in the R pack-age [9]. For stepwise regression we started from an empty model, used only forward direction with at most 30 itera-tions, just like in the case of PolyForward . For multiclass problems we use all-pairs method [3].

To assess prediction accuracy, 5-fold cross-validation was used. The same folds were used to test all algorithms. All reported quantities have been averaged over the five folds.
Results of comparison are shown in Table 4. Characteris-tics of datasets used were presented in Table 1. The columns of the table describe the dataset used, and respective perfor-mance of: the PolyForward algorithm, simple linear logistic regression model on all variables, nonlinear stepwise model built using R and boosted J4.8.

Every cell of the table shows the cross-validation accuracy, model size and computation time. For logistic models, size is the total number of terms / the sum of degrees of all terms. For multiclass problems, both this values are summed over all classifiers produced, so the size can be large, even though individual classifiers are usually simple and understandable. For boosted decision trees the total number of nodes and leaf nodes (summed over all tress) is reported. Large sizes are rounded to the nearest integer.

All parameters were tuned on sin , sonar and spambase datasets, other datasets were tested with default parameters in order to avoid overfitting due to parameter tuning.
Boosted decision trees almost always achieved higher ac-curacy than logistic regression models, but the price for it is paid in model size, much larger then for regression models, comprising several trees, hundreds of nodes each. It is hard to imagine that the user could use such a classifier to gain understanding of the data. Note, that boosted decision trees did not use the pairwise multiclass method, which gives size disadvantage to logistic regression approach.

It can be seen that the standard stepwise procedure of-ten gives better results for small datasets. We believe that this is primarily due to the fact that splitting into training and validation sets causes our method to lose a lot of pre-dictivepower. Wewerenotabletocompletethestepwise procedure for all large datasets, but for sin , segment and waveform-5000 the difference was not larger than 1%, sug-gesting that for large datasets both methods achieve com-parable accuracy.

For performance reasons the maximum term degree has often been lowered to 2 for the stepwise method. Apparently this did not adversely affect the accuracy (except for the sin dataset) which remained high. Thus, even though it is not entirely correct, we decided to compare stepwise regression X  X  accuracy with accuracy of PolyForward with maximum term degree 4 in order to better highlight performance differences.
The performance of the PolyForward algorithm was vastly superior to traditional stepwise approach. This was true even after lowering the maximum degree to 2 in the step-wise approach. For spambase and forest-cover we aban-doned the stepwise method after several hours. For the forest-cover data, PolyForward was the only algorithm which completed within a reasonable amount of time. Even simple logistic regression failed in this case. Even though PolyForward computes logistic regression models repeatedly, the models involve few terms. With a large number of terms, maximum likelihood computation becomes very slow, thus poor performance of simple logistic regression.

To assess the effects of discretization, we discretized the x 1 and x 2 attributes of the sin dataset into 3, 5 and 10 buckets and compared the accuracy of our approach and of boosted decision trees:
It can be seen that after discretization, accuracy achieved by both methods decreases significantly in comparison to the undiscretized case.
In the paper a new kind of associations: polynomial item-sets and association rules have been introduced, which allow for discovery of nonlinear relationships between numerical attributes without discretization. Polynomial itemsets can be efficiently discovered using modified association rule min-ing algorithms. An application to adding nonlinear terms to logistic regression models was presented, and shown experi-mentally to offer accuracy comparable to standard methods and a dramatic performance improvement for large datasets.
There are several alternatives and modifications to the definition of support for polynomial itemsets which we are planning to investigate in the future. [1] R. Agrawal, T. Imielinski, and A. Swami. Mining [2] A. Agresti. An Introduction to Categorical Data [3] K. Duan and S. Keerthi. Which is the best multiclass [4] S. Dzeroski and L. Todorovski. Discovering dynamics. [5] Y. Freund and R. Schapire. A decision-theoretic [6] J. Freyberger, N.T. Heffernan, and C. Ruiz. Using [7] J. Friedman, T. Hastie, and R. Tibshirani. Additive [8] B. Goethals. Survey on frequent pattern mining. [9] R Development Core Team. R: A Language and [10] M. Scholz. Sampling based sequential subgroup [11] R. Srikant and R. Agrawal. Mining quantitative [12] M. Steinbach, P.-N. Tan, H. Xiong, and V. Kumar. [13] I. H. Witten and E. Frank. Data Mining: Practical [14] X. Yin and J. Han. CPAR: Classification based on [15] R. Zembowicz and J. M. Zytkow. Discovery of
