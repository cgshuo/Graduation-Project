 Traditionally, inductive machine learning focuses on problems where the task is to pre-dict a value of a single target attribute. However, there exist many real life problems where the task is to predict not one, but several related target attributes. Of course, this problem can easily be solved by construc ting separate models for each target at-tribute. If our only goal is to achieve high predictive accuracy, the resulting collection able method for single target prediction. On the other hand, if, besides the predictive accuracy, the interpretability of induced mode ls is also important, the collection of sin-gle target models is far less understandable than a single model that jointly predicts all target attributes. Therefore, the research on extending machine learning methods that produce interpretable models (such as decisi on trees and rules) towards multiple target prediction is justified.

One of the possible approaches to multiple target prediction is predictive clustering, which was originally applied to decision trees. The goal of this paper is to adopt the predictive clustering approach in order to design a method for learning rules for multiple target prediction; we call it predictive clustering rules . 1 We focus on classification tasks only, though the method can also be extended to regression problems.
The rest of the paper is organized as follows. Section 2 summarizes predictive clus-tering. The algorithm for learning predictive clustering rules is presented in Section 3. Section 4 describes the evaluation methodology, and Section 5 presents the experimen-tal results. Last section concludes and gives some directions for further work. The predictive clustering approach [1,2 ] builds on ideas from two machine learning areas, predictive modeling and clustering [9]. Predictive modeling is concerned with the construction of models that can be used to predict some object X  X  target property from the description of this object (attribute-value representation is most commonly used for describing objects and their properties). Clustering, on the other hand, is con-cerned with grouping of objects into classes of similar objects, called clusters; there is no target property to be predicted, and usually no symbolic description of discovered clusters, (though a symbolic descriptions can be added to already constructed clusters as in conceptual clustering [13]). Both areas are usually regarded as completely di er-ent tasks. However, predictive modeling methods that partition the example space, such as decision trees and rules are also very similar to clustering [10]. They partition the set of examples into subsets in which examples have similar values of the target vari-able, while clustering produces subsets in which examples have similar values of all descriptive variables. Predictive clustering builds on this similarity. As is common in  X  X rdinary X  clustering, predictive clustering constructs clusters of examples that are sim-ilar to each other, but in general taking both the descriptive and the target variables into account. In addition, a predictiv e model is associated with each cluster which describes the cluster, and, based on the values of the descriptive variables, predicts the values of the target variables. Methods for predictive clustering enable us to construct models for predicting multiple target variables which are normally simpler and more comprehen-sible than the corresponding collection of m odels, each predicting a single variable. 2 So far, this approach has been limited to the tree learning methods. The method de-scribed in the next section extends predictive clustering towards methods for learning rules. Predictive clustering rules (PCRs) include ideas from rule learning and clustering. The learning algorithm itself is a generalization of existing rule learning approaches. The rule evaluation function which serves as a s earch heuristic, however, employs tech-niques commonly used in clustering. We start with a description of the top level of the algorithm, while specific aspects of the al gorithm, such as learning single rules and modification of the learning set between subsequent iterations of the algorithm, are dis-cussed in separate sections. 3.1 Learning Algorithm Most of existing approaches to ru le learning are based on the covering algorithm [12]. Its main problem, however, is that it was originally designed for two-class (binary) clas-sification problem domains. In addition, the rule sets produced by the original covering algorithm are by nature ordered, unless rules for only one class value are constructed. Our algorithm is based on the CN2 method [5,4], which uses a version of the covering algorithm that can learn ordered or unordered rules, and is also applicable to (single target) multi-class problems.

The algorithm for learning predictive clustering rules is presented in Table 1. Top level procedure  X  X earnRuleSet X  (Ta ble 1.a) starts with an empty rule set R and a set of learning examples E . In each iteration we learn a candidate rule r i and add it to the rule set. Next, we modify the current learning set E c and, unless some stopping criterion is met, repeat the loop. There are two stopping criteria; we stop adding rules if the  X  X indCandidateRule X  procedure could not find any non-empty rule, and when the
E c becomes zero ( E c is the number of examples with non-zero weights). Before the learning procedure is finished, we add the default rule. The default rule is a rule with an empty condition and is used for examples that are not covered by any other rule. Its prediction part is a cluster prototype of the complete learning set E .

The interpretation of PCRs is the same as that of CN2 rules: ordered rules are scanned and the first one that covers the example is used; predictions of all unordered rules that cover the example are combined into the final prediction via weighted voting, where the weights are equal to the number of covered examples on the training data. 3.2 Learning Single Rule The  X  X indCandidateRule X  procedure is given in Table 1.b, and is a general-to-specific beam search algorithm, whic h is very similar to the one implemented in the CN2. The input to the procedure is the learning set of examples E c . The width of the beam b w determines the number of partial rules maintained during the search. A set of b w best rules (or actually conditions) found so far as evaluated by the heuristic function h is denoted as C best . We start with the most general condition (  X  X rue X  ) that is satisfied by all examples in the learning set E c . Now we begin specialization of all conditions in the current set of conditions C by conjunctively adding an extra test. Here we consider all possible tests ( T p ) that are not already in the condition that we are specializing. Here we only consider conditions that cover at least a predefined minimal number of examples . Every specialization is evaluated using the heuristic function h .Ifanyspe-cialization is better than the worst condition in the set C best , we add it to this set and to set C new . We remove the worst conditions if the sizes of these sets increase over their predefined maximum sizes. When all specializations of the current set of conditions C are examined, the set C becomes set C new , and the search is continued until no better specializations can be found. At the end, the best condition from the set C best is coupled with the prototype of target attributes of examples that it covers ( p best ), and returned as a candidate rule.
 Search Heuristic. The crucial part of the algorithm is the search heuristic h .The heuristic is used for the evaluation of rules under construction and basically leads the search procedure towards rules of the desired quality. Therefore, the heuristic should reflect the qualities we expect from each individual rule in the rule set. Typically, we want the rules to be accurate and, at the same time, general, i.e., we want the rules to cover as many examples as possible. More generalization means that the rule covers more examples and in the end, it also means that the final rule set will have fewer rules and will be more comprehensible. Unfortunately, more generalization most often also means larger error in the model, and a compromise between the two must be found.
Normally, the accuracy measures are tailore d to single target prediction, while for predictive clustering rules we need a measure that also works for multiple target pre-diction. We define such a measure, we call it dispersion, as follows. Let E be the set of N examples that are covered by a specific rule, and each example e i is represented as a vector of K attribute values x ji ,where x ji stands for the value of the attribute a j of the example e i . The dispersion of a set of examples E is an average of the dispersions along each attribute Here we take into account only the target attributes, although in principle, we could include also the non-target attributes [17].

The definition of dispersion along a single nominal attribute is the average distance of a single example from a set to the prototype of this set. Let the attribute a j have L possible values with labels l 1 to l L . The prototype of a set of examples E of an attribute a is a vector of relative frequencies f k of possible values within the set where n k stands for the number of examples in the set E whose value of attribute a j equals l k . Accordingly, (the prototype of) a single example e i with the value of attribute a equal to l k is The distance between the two prototypes can be measured using any of the distance measures defined on vectors; we have decided to use the Manhattan distance. Now the distance between an example e i with the value of attribute a j equal to l k (i.e., the prototype p e where we have taken into account that f k 1and f m 1. Finally, the dispersion of the set of examples E along the nominal attribute a j is the normalized average distance The normalization factor normalizes the value of dispersion to the [0 1] interval which is necessary, if we want the dispersions between di erent attributes to be comparable.
The rule X  X  generality is typically measured by its coverage, whichisdefinedasthe proportion of covered examples, i.e., the num ber of examples covered by a rule divided by the number of all examples. This definition assumes that all examples are equally important, i.e., they all have equal weight. As we will see later, sometimes it is useful to introduce example weights that are not uniform. Each example e i then has an associated weight w ei . The relative coverage of rule r in this case is simply the sum of weights of the examples covered by r divided by the sum of weights of all examples
Now, we have to combine the two measures into a single heuristic function. Analo-gously to the WRAcc heuristic [11], we do this as follows. Let c be the condition of rule r that we are evaluating, and E be the set of all learning examples. E r is the subset of E with examples that satisfy condition c (i.e., are covered by rule r ). w e is the example weight vector. By means of example weights w e can give preference to selected exam-ples, which should more likely lead to the construction of rules covering these examples (more on this later). The heuristic function is The parameter enables us to put more (or less) emphasis on coverage w.r.t. to dis-persion; by default (like in WRAcc) it is set to 1. d def is the default dispersion, i.e., the dispersion of the entire learning set E , and the first factor of Equation 7 can be regarded as the relative dispersion loss. Rules with larger heuristic function values are better. 3.3 Modifying the Learning Set Within the main loop of the  X  X earnRuleSet X , the current learning set E c must be mod-ified, otherwise the  X  X indCandidateRule X  procedure would continuously find the same rule. Learning set modification is done by the  X  X odifyLearningSet X  procedure presented in Table 1.c.

The most common approach to modifying the learning set is the covering algo-rithm [12]. The idea is that we put more emphasis on the learning examples that have not yet been adequately covered. This should f orce the  X  X indCandidateRules X  procedure to focus on these examples and find rules to d escribe them. In the original covering al-gorithm ( M mod  X  X td-Covering X  ), examples that are already covered by a rule are removed from the current learning set. Rule learning in the next iteration will there-fore focus only on examples that have not yet been covered. This approach is used by the CN2 algorithm [5,4] for the induction of ordered rules, and ordered PCRs are also induced in this manner.

The weighted covering algorithm [8], o n the other hand, assigns a weight to each learning example. Instead of removing the covered example completely, weighted cov-ering only decreases its weight. It does this, however, only for examples that have been correctly classified by the newly added rule. The notion of  X  X orrectly classified exam-ple X  unfortunately only makes sense for single target classification problems. To over-come this limitation, we develop a more general covering scheme, which we call error weighted covering, that is applicable to single and multiple target prediction problems ( M mod  X  X rr-Weight-Covering X  ). Error weighted covering is similar to  X  X rdinary X  weighted covering, except that the amount by which example X  X  weight is reduced is proportional to the error the newly added rule makes when predicting the example X  X  target attributes X  values. The ex act weighting scheme is as follows.

Let every learning example e i have an assigned weight w ei . At the beginning, the weights of all examples are set to one. Then, whenever a new rule r is added to the rule set, the weight of each covered example e i is multiplied by the value of g ( e i r ), which is defined as where k ( e i r ) is the proportion of correctly classified target attributes of example e i by rule r and is the covering weight parameter, which enables us, together with the covering weight threshold parameter , to control the pace of removing covered examples from the current learning set. It should t ake values between 0 and 1. Setting to 0 means that examples, whose target attribut es are correctly predicted by rule r , are immediately removed from the current learning set, i.e., t heir weights are set to zero. The parameter defines the threshold under which the example weights are considered to be too small to be still included in the learning set; when the example weight falls below this value, it is set to zero. In the experiments, we investigate two issues. First, we compare the performance of predictive clustering rules (PCRs) to some existing rule learning methods for single target classification in order to show that PCRs are comparable to existing methods on this type of problems, and can be used as a baseline in the second part of the evaluation. For comparison we selected the CN2 rule l earner [5,4] and a modification of CN2, the CN2-WRAcc [15], because our approach is a generalization of these algorithms. Additionally, we compare PCRs to Ripper [6] which is a more advanced rule learner; we used the JRip implementation from the Weka data mining suite [16] which only learns ordered rules.

Second, we compare the PCRs fo r single target prediction to PCRs for multiple target prediction. The main benefit of multiple target prediction is that a collection of mod-els (rule sets) each predicting one target attribute can be replaced by just one model that predicts all target attributes at once. The task of the second part of experimental evaluation is to investigate this issue. 4.1 Data Sets In order to evaluate the performance of PCRs, we perform experiments on single target and on multiple target problems. For single target problems, we have selected a collec-tion of 15 data sets from the UCI Machine Learning Repository [14] which are widely used in various comparative studies.

Multiple target classification is a relatively new machine learning task and conse-quently there are few publicly available data sets. Nevertheless, some data sets from the UCI Repository can also be regarded as multiple target problems ( , , -,and -0387). In addition, we use the following five data sets.

The is a data set on electrical discharge machining with 154 examples, 16 descriptive attributes and two target attributes. The data set consists of 7953 questionnaires on the Slovene media space, has 7 9 descriptive attributes and 5 target at-tributes. The -is a data set on a field study of a genetically modified oilseed rape. It comprises 817 examples, 6 descriptive, and 2 target attributes. The -data set is also concerned with genetically modified oilseed rape, however, the data are produced by a simulation model. The data consists of 10368 examples with 11 de-scriptive and 2 target attributes. The -data set comprises biological and chemical data that were collected through regular monitoring of rivers in Slovenia. The data consists of 1060 examples with 16 descriptive and 14 target attributes. 4.2 Evaluation Methodology When evaluating the newly developed method, we are interested in the predictive error of the learned rule sets and their size, i.e., the number of rules within the rule sets. The CN2 and CN2-WRAcc as well as PCR algorithms can induce ordered or unordered rules, therefore we perform experiments for both. JRip can only learn ordered rules. All error rates are estimated using 10-fold cross-validation. The folds for a specific data set are the same for all experiments. As recommended by [7], significance of the observed di erences in error rates and rule set sizes of two algorithms was tested with the Wilcoxon signed-rank test.
 Unless otherwise noted, all algorithm param eters were set to their default values. CN2 can use significance testing for rule pruning, while there is no need for signifi-cance testing in CN2-WRAcc, since the number of induced rules by this algorithm is already much smaller. We use the p-value of 0.99 for significance testing in the CN2 algorithm.

The default parameter values for the PCR algorithm are as follows: beam width b w 10, minimal number of examples 2, coverage heuristic weight 1, covering weight 0, and covering weight threshold 0.1. These are set so as to emulate the CN2 and CN2-WRAcc algorithms as closely as possible and were not tuned in any way. Ordered rules were induced with the learning set modifying method ( M mod )setto  X  X td-Covering X  , while for unordered rules it was set to  X  X rr-Weight-Covering X  .
The comparison of PCRs used for multiple target prediction and PCRs used for sin-gle target prediction is performed as follows. For each data set, we have learned one multiple target PCR model and compared it to a collection of single target PCR mod-els. This collection consists of the same number of models as is the number of target attributes in a given domain. The sizes of the single target PCR rule sets for each target attribute are summed and compared to the size of the multiple target PCR rule set. The overall significance of di erences is again estimated using the Wilcoxon signed-rank test; each target attribute of each data set corresponds to one data point. 5.1 Comparison to Existing Methods First, we present the results of the comparison of predictive clustering rules (PCRs) to the CN2, CN2-WRAcc, and JRip methods. Table 2.a gives the significances of di er-ences for pairs of algorithms for ordered rules and unordered rules. Except for the JRip, we also compared ordered vs. unordered rules. Due to space limits, we have left out the table with complete results for each data set.
 For ordered rules, we can see that there are no significant di erences between the CN2, CN2-WRAcc, and PCR algorithms in terms or error, but rule sets induced by CN2-WRAcc have a significantly smaller number of rules. JRip rules are better in terms of error than ordered PCRs, but the di erence is below the significance threshold. Next, if we compare unordered rules, we see that PCRs have a significantly smaller error than the two CN2 algorithms. However, the PCR rule sets are much larger than the CN2-WRAcc rule sets. There is no significant di erence between (ordered) JRip rules and unordered PCRs in terms of error, but JRip tends to produce significantly smaller rule sets than the latter. Finally , if we compare ordered and unordered rules induced by each algorithm, th e only significant di erence is in the case of PCRs; un-ordered PCRs have a significantly smaller erro r, but this accuracy comes at a price, since their size is much larger. From these resu lts, we can conclude that the performance of PCRs on single target problems is comparable to the performance of the CN2 and CN2-WRAcc algorithms for ordered rules, and better for unordered rules. In terms of error, ordered PCRs are somewhat worse than JRip, while unordered PCRs are comparable to JRip. 5.2 Comparison of Single to Multiple Target Prediction The significances of di erences between single target and multiple target PCRs are given in Table 2.b, while error rates and rule set sizes are presented in Table 3.
From the Table 2.b we can conclude that ordered multiple target prediction models tend to be less accurate than the single target prediction models. In the case of unordered rules, however, the situation is reversed: multiple target prediction models are better than the single target prediction models. In both cases the di erence is almost significant ( p-value 0.07). The di erence in the rule set sizes, how ever, is very significant; the size of single target rule sets is roughly twofold in the case of unordered rules, and more than threefold in the case of ordered ru les. These results s uggest that the multiple target PCRs indeed outperform single target PCRs in terms of rule set size, while the accuracy of both types of models is comparable. In addition, multiple target prediction setting somewhat improves the accuracy of unordered rule sets.
 A new method for learning rules for multiple target classification, called predictive clustering rules, is proposed in this paper. The method combines ideas from supervised and unsupervised learning and extends the predictive clustering approach to methods for rule learning. In addition, it generalizes rule learning and clustering.
The newly developed method is empirically evaluated in terms of error and rule set size on several single and multiple target classification problems. First, the method is compared to some existing rule learning methods (CN2, CN2-WRAcc, and JRip) on single target problems. These results suggest that PCRs X  performance on single target classification problems is good, and they can be used as a baseline in the next part of the evaluation.

The comparison of multiple target predic tion PCRs to the corresponding collection of single target prediction PCRs on multiple target classification problems shows that in the case of ordered rules, the single target prediction models are better, while in case of unordered rules, the multiple target prediction PCRs are better. The di erences in both cases are almost (but not quite) significant. The di erence in the rule set sizes, on the other hand, is very significant. Multiple targ et prediction ordered and unordered rule sets are much smaller than the corresponding single target prediction rule sets.
The new method therefore compares favorably to existing methods on single target problems, while multiple target models (on multiple target problems) o er comparable performance and drastically lower complexity than the corresponding collections of single target models.

Let us conclude with some guidelines for further work. We have only discussed clas-sification problems in this paper. By defini ng the dispersion measure used in the search heuristic for numeric attributes, it should be possible to extend the presented algorithm towards regression problems also. Since there are not many methods for learning re-gression rules, we see this as a worthwhile direction for further research. In addition, there exist several newer methods, e.g., Ripper [6]; incorporating the ideas from these methods into predictive clustering rules could lead to improved performance.
