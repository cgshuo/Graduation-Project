 In the past few years, the government and other agencies have publicly released a prodigious amount of data that can be potentially mined to benefit the society at large. How-ever, data such as health records are typically only provided at aggregated levels (e.g. per State, per Hospital Referral Region, etc.) to protect privacy. Unfortunately aggregation can severely diminish the utility of such data when mod-eling or analysis is desired at a per-individual basis. So, not surprisingly, despite the increasing abundance of aggre-gate data, there have been very few successful attempts in exploiting them for individual-level analyses. This paper introduces LUDIA, a novel low-rank approximation algo-rithm that utilizes aggregation constraints in addition to auxiliary information in order to estimate or  X  X econstruct X  the original individual-level values from aggregate data. If the reconstructed data are statistically similar to the origi-nal individual-level data, off-the-shelf individual-level mod-els can be readily and reliably applied for subsequent predic-tive or descriptive analytics. LUDIA is more robust to non-linear estimates and random effects than other reconstruc-tion algorithms. It solves a Sylvester equation and leverages multi-level (also known as hierarchical or mixed-effect) mod-eling approaches efficiently. A novel graphical model is also introduced to provide a probabilistic viewpoint of LUDIA. Experimental results using a Texas inpatient dataset show that individual-level data can be reasonably reconstructed from county-, hospital-, and zip code-level aggregate data. Several factors affecting the reconstruction quality are dis-cussed, along with the implications of this work for current aggregation guidelines.
 H.4 [ Information Systems Applications ]: Miscellaneous Data aggregation, Low rank approximation, Multi-level model
Individual-level datasets that contain one or more records per person are rich sources for data mining applications. In the healthcare domain, the application of advanced data mining methods on individual level records across large pop-ulations can enable major breakthroughs in both personal-ized and population-level healthcare, leading to much im-proved, more cost-effective and timely diagnoses and inter-ventions [33]. However, such data often contain a substan-tial amount of privacy-sensitive attributes. In practice, pri-vacy concerns are typically addressed through multiple Sta-tistical Disclosure Limitation (SDL) techniques [10], such as data aggregation [1], data swapping [8, 13], top-coding, feature generalization such as k -anonymity [36] or l -diversity [28], and additive random noise with measurement error [17]. Each method has distinct utility and risk aspects. Often an appropriate mix of disclosure limitation techniques is care-fully chosen by domain experts and statisticians. For ex-ample, Centers for Medicare and Medicaid Services applied six different SDL techniques when publishing synthetic pub-lic use files 1 : variable reduction, suppression, substitution, imputation, data perturbation, and coarsening.

Among various SDL approaches, data aggregation is cur-rently the most widely used. Data aggregation is a process of summarizing individual-level data into a small set of rep-resentative values such as mean and median statistics com-puted over groups that are typically geographically or ad-ministratively defined (such as county, hospital group, state, etc). This process is straightforward to apply on diverse datasets: wireless sensor networks [22], regional healthcare statistics [7], and government data [9]. Moreover, such ag-gregate data can be efficiently and effortlessly generated in RDBMS [29] and statistical programming languages [37]. Data collecting agencies publish various aggregate datasets at different levels of aggregation (including individual-level for non-sensitive information). In particular, the U.S. gov-ernment X  X  open data project, data.gov has recently released a substantial amount of regional and topic-based aggregate data regarding agriculture, education, and energy. Centers for Disease Control and Prevention annually publishes vari-ous regional statistics related to aging, cancer, and diabetes. Other notable sources of aggregated health data are dart-mouthatlas.org and healthdata.gov.

The use of aggregate data is typically limited to group-level studies, often referred to as ecological studies for his-Table 1: Illustrative health data files: artificial individua l-level data (left) and aggregate-level summary (right) [24]. toric reasons. Applying the result from aggregate data to individual-level inference often results in the classic prob-lem of ecological fallacy [35]. Ecological fallacy occurs when aggregate-level statistics are misinterpreted as individual-level inferences. For example, the high correlation between  X  X er capita consumption of dietary fat X  and  X  X reast can-cer X  in different countries [6] does not imply that dietary fat causes breast cancer.

There have been many attempts to circumvent the ecolog-ical fallacy while analyzing aggregate data. This is because individual-level data acquisition is usually expensive, and it is sometimes legally and ethically implausible. Duncan [11] developed the method of bounds that uses the constraints of contingency tables, but the bounds are often uninforma-tive in real applications [14]. The constancy assumption, suggested by Goodman [21], allows an individual-level in-terpretation of ecological regression. Suppose that we want to check the relationship between Length of Stay (LoS) and Hospital Charge (HC) variables from state-level aggregate data: The constancy assumption states that daily hospital charge rates are the same across different states i.e.  X  state =  X  and c state = c . Of course, this assumption is rarely true in real datasets; for this example, it is more natural to assume that each state has a different daily charge rate, thereby indi-cating that multi-level modeling can be used [19]. Such an approach, however, is under-identified and can X  X  be solved using aggregate data, since we have more parameters than observations. King [25, 26] proposed a Bayesian prior-based multi-level approach to overcome the limitation of Good-man X  X  assumption, but Freedman [15] criticized that King X  X  method cannot be validated on the basis of aggregate data.
We provide a novel approach for addressing the ecological fallacy dilemma by leveraging available sources of individual-level data for which the values of the partitioning or aggrega-tion variable is known. For example, an aggregation variable can indicate state, county, or zip codes, that can be used to link to aggregate-level dataset that is aggregated along such geographical regions. In practice, it is not difficult to collect multiple datasets with different levels of aggregations from multiple agencies, so little added data-collection expense is involved.

Table 1 shows a simple, illustrative example of two health datasets. Non-sensitive fields are published at individual-level, while a sensitive field (hospital charge) is aggregated over the partition variable  X  X tate X . Our approach is sub-stantially different from previous ecological fallacy solutions where only aggregate data were considered.

We use a two-stage approach to avoid the ecological fal-lacy. We first reconstruct the masked individual-level vari-ables from aggregate data, then apply multi-level regression models to the reconstructed data. In other words, we first synthesize  X  X seudo individual-level X  data that are statisti-cally similar to the original (unseen) individual-level data. Not only multi-level regression models, but also numerous off-the-shelf data mining algorithms can be easily applied to such pseudo individual-level data. Our reconstruction algo-rithm is based on two key observations: We demonstrate our reconstruction algorithm on both sim-ulated and real datasets. Many factors contribute to the re-construction quality, for example, the number of data points per aggregation and correlation strength with other columns. These factors will be illustrated in Section 6 using Texas In-patient Public Use Files. The main contributions of this paper are: The first two contributions will be illustrated in Section 3, the last contribution will be explained in Section 4. Ex-perimental results are provided in Section 6, followed by discussions in Section 7.
This section starts by setting up the notation of this pa-per, and visiting two key existing approaches for tackling aggregate data. We extend these approaches to reconstruct the original individual-level data, and briefly discuss their modeling assumptions and limitations.

Aggregation is a compressive linear transformation, which we denote as A . For example, suppose that there are five individuals from two different groups: the first two from Group A and the last three from Group B. Individual-level observations, say y = 1 2 3 4 5  X  , can be aggregated into two groups by multiplying an aggregation matrix de-fined as follows: Table 2 summarizes the notation of this paper. Table 2: Notation. For simplicity but without loss of gener-a lity, we use d = 1 in this paper.

T he processes of aggregation and reconstruction can be illustrated as follows: where X represents individual-level data, and Recon is a re-construction algorithm. To give a brief overview, our recon-struction algorithm, LUDIA ( L ow-rank factorization U sing Di fferent levels of A ggregation), is a constrained low-rank factorization algorithm that can capture multi-level effects. Figure 1 illustrates the overall idea of LUDIA and other re-construction algorithms. We have two sources of errors that construct our reconstruction triangle: aggregation and mod-eling errors. LUDIA reduces the aggregation error using a low-rank model, but the LUDIA error is lower-bounded by the modeling error.

To illustrate existing approaches for aggregate data, let us consider the previous  X  X ospital charge vs. length of stay X  example. When using aggregate data, three approaches have been popular:
These previous approaches have been developed to tackle aggregate data, and need to be slightly modified to syn-thesize individual-level data. Imagine that we now obtained individual-level length of stay data 2 X and each individual X  X  location information A . To reconstruct the masked hospi-tal charge data y , two direct extensions from the previous approaches can be considered: MP and ER exhibit different failure modes. MP ignores the effects of individual-level covariates, which may substan-tially leverage the utility of aggregate data. On the other hand, ER relies on the constancy assumption, which is rarely true in real settings.

For our hospital charge example, daily charge rates are significantly different across city and rural areas (see Sec-tion 6). This geographical variation on daily charge rates can be expressed as follows: where  X  state and  X  state represent state-level biases for the in-tercept and slope; they are called random intercept and ran-dom slope, respectively. Assuming that we have two states A and B, and individuals listed by state, this multi-level approach [18] can be written in a matrix form:  X   X   X   X   X   X   X  y
N ote : This simple example has only one individual level (LoS) and one aggregated (HC) feature, and one level of ag-gregation, called  X  X tate X , so as to convey the concepts most easily. Our approach readily generalizes to multiple indi-vidual and aggregate variables as well as multiple levels of aggregations, as will be seen later We define new matrices  X  ( random effects) and G (covari-ates for random effects) to obtain a compact form: Aggregate data are obtained through the aggregation oper-ation as follows: As can be seen, the ER solution is valid only if These two conditions are rarely realistic in real applications.
MP and ER are formulated based on two orthogonal as-sumptions. MP assumes that only geographical partitions affect the dependent variable, while ER posits that geo-graphical partitions are merely random groupings. These assumptions are necessary to obtain some meaningful results from aggregate data, as the ecological fallacy is, in fact, the problem of statistical under-identification [34]. However, the direct extensions from the previous approaches do not utilize the full potential of auxiliary individual-level data.
A recent breakthrough in the use of aggregate data to aug-ment individual-level models was made by Park and Ghosh [30, 32, 31]. The suggested model, CUDIA, is a probabilis-tic clustering algorithm that utilizes both aggregated and individual-level data. CUDIA models the data points as being generated from a mixture of exponential family dis-tributions. The parameters of CUDIA are estimated us-ing a Monte-Carlo Expectation Maximization (MCEM) al-gorithm. Although CUDIA can reasonably reconstruct the data based on the estimated cluster centers, the primary ob-jective of CUDIA is still clustering rather than reconstruc-tion. Furthermore, the presented MCEM algorithm is not scalable to large-scale data. We show that CUDIA is, in fact, a special case of LUDIA with a non-negative constraint on U (see Section 5). LUDIA generalizes CUDIA with a more flexible representation of U . This generalization provides an efficient optimization algorithm that is suitable for large-scale data.
LUDIA is a low-rank factorization algorithm using aggre-gate data. We first describe the underlying data model of LUDIA, then formulate LUDIA X  X  objective function. Be-cause of the non-trivial aggregation constraint, we derive a customized minimization approach that uses the Sylvester equation.
LUDIA employs a bottom-up approach starting from in-dividual level data. We first design a data model for a com-plete matrix D = X y , then formulate an objective func-tion when y is masked and only s = Ay is provided. The data model for LUDIA is based on the low-rank approxima-tion theory as follows: where U  X  R n  X  r , V  X  R m  X  r , E  X  R n  X  m , and r  X  min( n, m ). Note that we divided V into two block matrices: V x and v , so that X  X  UV x and y  X  Uv y .

The main objective of this paper is to reconstruct the masked values, y . In theory, under certain assumptions such as an underlying low-rank structure and a uniform missing mechanism, missing values in a matrix can be reconstructed. Candes and Recht [5] showed that, for matrix entries that are missing at random, they can be exactly recovered if the number of observations exceed a certain threshold value. However, the settings for the matrix completion problem are not suitable for our problem, since we consider a situ-ation wherein one or more columns of a matrix is entirely missing, but its aggregated statistics are given.
We approximate the original matrix using two low-rank matrices. This problem is different from the matrix com-pletion problem [23]. Low-rank approximation is typically posed as a minimization problem as follows: where D and  X  D are both n  X  m matrices, and r  X  min( n, m ). The Eckart-Young-Mirsky theorem [12] says that rank r ap-proximation of the data matrix D is given as follows: where U ,  X  , V are n  X  r , r  X  r , m  X  r truncated Singular Vec-tor Decomposition matrices, respectively. The data model in Equation 2 is, however, inapplicable to our reconstruction application. The model should instead reflect the constraint that y is masked and only s = Ay is provided. A novel optimization problem for three latent matrices y , U , and V is proposed as follows: A simultaneous minimization over y , U , and V is a difficult non-convex optimization problem. However, minimization over one set of variables alone is a convex problem.
We tackle this problem by removing the equality con-straint. The equality constraint on y can be eliminated if we fix the other two variables. Given that U and V are fixed, the optimality condition [4] is given as: where f ( Y ) = k X  X  UV  X  x k 2 F + k y  X  Uv  X  y k 2 2 and  X   X   X  R is a dual variable. Y  X  is optimal if and only if there exists  X   X  satisfying the optimality conditions. It turns out that, for this system, y  X  can be solved in a closed form.
To eliminate the constraint, we solve Karush-Kuhn-Tucker (KKT) equations as follows: We multiply A on both sides of the second KKT equation, and solve for  X   X  : Thus, the optimal y  X  is: We plug the optimal y  X  i nto the original objective function to obtain: min We have thus transformed the original objective function with three variables and an equality constraint into a simpler unconstrained objective function with two variables.
Although we simplified the constrained optimization prob-lem to the non-constrained optimization problem in Equa-tion 5, solving the objective function poses another chal-lenge. Intuitively, one can approach the problem using an alternating minimization approach over U and V . Solv-ing for U , however, does not have a closed form solution, because the low rank matrix U is surrounded by A and v . Using a divide-and-conquer approach, we can solve for one row u i of U , and iterate over the entire rows. This divide-and-conquer approach is, however, susceptible to the sequence of rows, and cannot be generalized to an arbitrary aggregation matrix.

We propose a simple and efficient optimization solution by introducing an auxiliary variable  X  = AU where we treat  X  as an independent variable. We also relax the hard relationship between  X  and U as a penalty term.

Combining these two tricks, our new objective function is written as follows: min objective function, and denote as L ( U , V ,  X  ). We now ap-ply our alternating minimization technique to Equation 6.
First, we derive the partial derivative of the LUDIA ob-jective function with respect to U :  X  L ( U , V ,  X  ) Rearranging the terms, we obtain: This is a type of a Sylvester equation [2]. This form of equation widely appears in the field of control theory [3], and the continuous Lyapanov equation is a special case of the Sylvester equation. If V  X  x V x and A  X  A have no common eigenvalues, a unique solution exists and it is given as: vec( U ) = ( V  X  x V x  X  I n + I r  X  A  X  A )  X  1 vec( XV x where vec is a vectorization operator, and  X  represents the Kronecker product. For example, vec( U ) is defined as:
Next, we derive a partial derivative of the LUDIA objec-tive function with respect to  X  : Rearranging the terms, we obtain another Sylvester equa-tion: The solution is given as:
Finally, we derive closed form update equations for two block matrices V x and v y . The partial derivative with re-spect to V x is given as: Rearranging the terms, we obtain: Similarly, the partial derivative with respect to v y is: Thus, the update form is:
Algorithm 1 summarizes our alternating minimization ap-proach combining three different minimization equations for U ,  X  , and V . The algorithm takes three input matri-ces: individual-level matrix X , aggregation matrix A , and aggregate-level matrix s . The output of the algorithm is the reconstructed individual-level data  X  y . The algorithm does not require any other parameters.
 Algorithm 1: L UDIA Estimation Algorithm The initialization of U a nd V is based on the MP solution. We first pseudo-reconstruct the masked individual-level data using MP, then run SVD on the pseudo-complete matrix. The rank parameter of the SVD algorithm is given as the rank of X . This setting captures both our low-rank data model and a linear model defined as y = X  X  . If this linear model is the true underlying data model for the data, then the rank of the complete matrix is the same as the rank of X .
 The last line of the algorithm calibrates the final output. Recall that the optimal y  X  was given in Equation 4. This correction equation ensures that the aggregation of the re-c onstructed values are the same as the given aggregate data i.e. s = A  X  y . However, if the aggregate values do not nec-essarily need to match the reconstructed values (possibly from noise or sub-sampling), we can ignore the last line of the algorithm. We illustrate two extensions of the LUDIA algorithm. The first extension shows that LUDIA can directly incor-porate multi-level data models. This extended reconstruc-tion method can capture group-level effects, which were not possible in classical frameworks. The second extension ex-plores whether we can improve the reconstruction quality if we have multiple levels of aggregate data.
The ecological fallacy problem is essentially  X  X tatistical under-identification X  [34]. For aggregate data analyses, the maximum degrees of freedom are limited by the number of partitions. Individual-level analyses, such as multi-level models [19], often require more parameters than the num-ber of partitions. This under-identification problem is tradi-tionally approached by more assumptions; Goodman X  X  and King X  X  assumptions are two extreme cases. These assump-tions are usually unrealistic, and they are almost impossible to verify on the basis of aggregate data.

Smartly utilizing auxiliary individual-level data can pro-vide higher degrees of freedom than the number of parti-tions. The key observation comes from the connection be-tween the degrees of freedom and the rank of a full matrix. Suppose that a target y is a function of r degrees of freedom. Then the rank of the full matrix X y is r , since y can be expressed by a linear combination of X . Analogously, if a target is a multi-level function of r variables and p lev-els, then the degrees of freedom for this model is given by ( r  X  p ). To capture the variability of the target, the corre-sponding full matrix needs to have the rank of ( r  X  p ). In this section, we show that this rank augmentation can be seamlessly integrated with the LUDIA framework.

As illustrated in Equation 1, a multi-level model can be compactly written as: where  X   X  R l  X  1 is a random effect vector, and G  X  R n  X  l resents encoded covariates according to  X  . For this model, the degrees of freedom are given as ( r + l ) where r = rank( X ). The full matrix has ( r + l + 1) columns, and this matrix can be written as a product of two rank ( r + l ) matrices.
To fully reconstruct the masked individual-level data, the rank of our low-rank model should be at least ( r + l ). This can be achieved by augmenting the data by l :
Although one can run LUDIA with these augmented terms, we show that a simple post-processing approach can mimic the result from this augmentation. The block matrix  X  V x be treated as a nuance parameter, since it does not directly affect the reconstruction of y . The trick is to specify our low-rank matrices to be of a specific form as follows: Then we do not need to estimate  X  U and  X  V x , but only v The augmented term v a needs to minimize the second term of Equation 6: The solution for this minimization problem is given as fol-lows: Using this v a , we calibrate the reconstruction of y : This adjustment equation mimics the original augmentation.
This data augmentation technique for multi-level model-ing is not suitable for the MP and ER frameworks. MP only focuses on the aggregation matrix, and does not in-volve individual-level covariates. Adding the augmented block matrix G requires a different approach. The num-ber of covariates in ER is upper-bounded by the number of partitions. The simplest multi-level model, a random in-tercept model, requires the number of covariates to be the same as the number of partitions. LUDIA utilizes the full potential of individual-level covariates, and thus it can be easily extended to more complex models.
Thus far, we have considered only one source of aggregate data. There can be many levels of groupings based on geog-raphy, administration, or other factors. This section answers how one can further improve the reconstruction quality with additional aggregate data.

The key trick is to stack two aggregate-level datasets and create a new aggregate dataset. Algorithm 2 illustrates this approach. In the algorithm, we have two sources of aggre-gate data: ( A 1 , s 1 ) and ( A 2 , s 2 ). For example, there can be county-level and state-level aggregate data, respectively. This kind of augmentation can further improve the recon-struction accuracy. This is because we have more constraints on y , and the degrees of freedom for y decrease accordingly. Algorithm 2: L UDIA with Aggregation Stacking
T his section presents a probabilistic interpretation of the proposed LUDIA objective function. Figure 2a shows our low-rank model for the complete data. Note that the node for y is not shaded, since the variable is masked. To in-corporate the aggregation constraint, we draw another plate that represents groupings. Figure 2b illustrates the graphi-cal model for LUDIA. Each u i in a group is assumed to be drawn from a multivariate Gaussian centered at  X  p . Thus, the log-likelihood log p ( U , V ,  X  | X , s ) of LUDIA is written as: In our setting, each row of X is i.i.d., thus  X  x can be modeled as an identity matrix I n . Before characterizing  X  y , we first show that AA  X  is invertible and positive-semidefinite. This property can be shown from the fact that rank( A ) = p and AA  X   X  R p  X  p . Moreover, the ( p, p )th diagonal component of ( AA  X  )  X  1 is the same as n p , the number of data points in group p . Thus, AA  X  can replace  X  y . Finally, if we assume that  X   X  = I p , then this log-likehood is actually a negative of the LUDIA objective function.

To show the connection to CUDIA, let us assume that we restrict the shape of U to be as follows: In other words, each column of U becomes an indicator col-umn for clusters. The rank parameter r of LUDIA is now interpreted as r different clusters, and V represents cluster centers. If we plug in this constraint to the LUDIA X  X  log-likelihood function, we obtain the log-likelihood of CUDIA. Although this formulation may provide a different perspec-tive on combining multiple sources of data, the minimiza-tion of the CUDIA objective function is more complicated to solve because of the non-negative constraint. Thus, CU-DIA requires a computationally heavy MCEM algorithm, or greedy deterministic algorithm [31]. As the non-negative case is a special case of U , we also have: This is why the CUDIA imputation is not so suitable for complex modeling such as multi-level modeling and non-linear estimates, while the LUDIA reconstruction provides valid inferences in such situations (see Section 6). We provide experimental results using simulated data and Texas Inpatient Discharge data. A simulated dataset is used to illustrate the differences between ER, MP, and LUDIA. Next, we illustrate reconstruction tasks using actual health data. In this set of experiments, we mask sensitive columns, then show how well LUDIA can reconstruct the masked orig-inal values for different analytical tasks including non-linear estimates and multi-level modeling.
We generate four different simulated datasets as follows: We fix the number of partitions to be five, and vary the num-ber of total data points. Aggregation matrices are generated using random assignment of partitions.

Figure 3 shows the reconstruction errors for different simu-lated data and reconstruction methods. Each cell represents a different simulated dataset, and the horizontal axes repre-sent the number of data points per partition. The lower the curve is, the better the reconstruction quality is. MP is not affected by the number of data points per partition, but its performance is the worst from the experiments. The per-formance of ER is comparable with that of LUDIA for the FE dataset, but it does not capture the low-rank structure and random effects. For the random effect datasets, ER is largely affected by the number of data points per partition. LUDIA shows robust and stable performances over different datasets.

Figure 4 shows the reconstructed values compared to the original values from the RE1 dataset. The leftmost first two cells show the reconstructed values from MP and ER, respectively. In this figure, we show three different initial-ization methods for LUDIA: MP, ER, and random initial-ization methods. The alternating minimization approach of LUDIA does not guarantee the convergence to the global optimum, and the algorithm is susceptible to initial points. All three initialization methods provide comparable perfor-mances, and it would be worthwhile to investigate the better choice of initialization methods. The rest of the experiments use the MP initialization to maintain the consistency of our algorithm. Figure 3: Reconstruction error vs. number of data points p er partition. Except the FE case, the LUDIA reconstruc-tion shows the least absolute errors.
 Figure 4: Reconstructed vs. original. We show three differ-e nt initializations for LUDIA: MP, ER, and random initial-izations. All these three LUDIA reconstructions are closer to the original values, and the MP initialization performs the best. We use Texas Inpatient Public Use Data File [38] from the Texas Department of State Health Services (DSHS). Hos-pital billing records collected from 1999 to 2007 are pub-licly available through their website. Each yearly dataset contains about 2.8 millions events with more than 250 fea-tures including hospital name, county, patient ZIP codes, etc. Specifically, we use the inpatient records from Central Texas in the fourth quarter of 2006. Except for a few exempt hospitals, all the hospitals in Texas reported inpatient dis-charge events to DSHS. The public use data file we use is a subset of the DSHS X  X  hospital discharge database. Our pri-mary interest is the hospital charge for normal delivery. We aggregate the individual-level hospital charges at county-, hospital-, and ZIP code-levels. We assume that some of the individual-level covariates are available such race, specialty unit, length of stay variables.

Hospital charge is primarily a function of length of stay, but it is substantially different across regions and is also affected by many other factors: where HC and LoS represent Hospital Charge and Length of Stay, respectively. Note that the coefficient for LoS is indexed by hospital, since daily charge rate is a function of hospital. The distribution of HC is, in fact, similar to a log-normal distribution. It is a better practice to log-transform the data, before applying a linear model: Table 3: Reconstruction Accuracy of the Texas dataset Patient ZIP F igure 5: (a) Reconstructed vs. original for the 3 models. (b) Estimated histograms of daily hospital charges. LUDIA histogram is the closest to the original.
 This log-transformed linear model turns out to be a simple random intercept model.

Table 3 shows the reconstruction errors from three differ-ent levels of aggregation. Except for the county-level case, the LUDIA-reconstructed values are the closest to the origi-nal values with smallest variances. ER performs slightly bet-ter than LUDIA for the county-level aggregate data. This is because the multi-level effects at county-level are not distinc-tive enough i.e. the constancy assumption can be applied. Figure 5a illustrates the reconstructed values compared to the original values. If reconstruction is perfect, points should lie on the dotted diagonal lines. As can be seen, the MP re-constructions do not capture the tails. This is because, when the HC values are averaged, those tail values are typically cancelled out, and MP cannot infer beyond the provided average statistics. The ER reconstructions perform reason-ably well, but does not capture the multi-level bias. LUDIA provides better estimates for the original values in terms of Mean Absolute Error (MAE).

The advantages of LUDIA are even more highlighted when calculating non-linear estimates. As an illustrative example, suppose that we want to estimate average daily charges. To calculate this value, we first need to reconstruct individual-level hospital charges, and then divide the reconstructed charges by the individual-level length of stay variable. In other words, average daily charges are calculated as follows: F igure 5b show the histograms of the estimated average daily charges. As can be seen, the histogram from LUDIA cap-tures the asymmetrical shape of the original histogram.
As shown in Section 4, multi-level modeling can be di-rectly integrated with LUDIA. We extract rural counties of Figure 6: Multi-level modeling and the mean squared errors, s hown as  X  X SE(Random Effects) X , between the original ran-dom effects and estimated random effects. LUDIA X  X  random effects are almost the same as the original.
 Central Texas, and compare the hospital charges by apply-ing a random intercept model. Figure 6 shows the fitted lines from the multi-level models. As can be seen, the orig-inal data clearly show the random intercept terms. It was impossible to estimate the slope term from the MP recon-structed values. For the ER reconstructed values, although the global model was similar to the original data, we cannot visually check the random intercepts. This is because ER ig-nores the information from the aggregation matrix. On the other hand, LUDIA provides almost the exact same random effect coefficients.

Reconstructed values from aggregate data can be used in various data mining applications. In this paper, we show a simple predictive analysis when a target column is provided in an aggregate form. By reconstructing the individual-level target values from the aggregate data, we can train a model, and then apply the model to test data as follows: 1. Combine the aggregate and individual-level data, then 2. Train a predictive model using the pseudo complete 3. For new data points, predict the target values using We first divided the Texas inpatient dataset into a training (80%) and a hold-out (test, 20%) set. Assuming the to-tal charges (target) are provided in only an aggregate form, we reconstruct the target using three different algorithms. We trained a Lasso regression model, and then measured the predictive accuracies of the target. Figure 7 shows the results from the test set. As can be seen, the LUDIA-reconstructed training dataset provides the best Lasso model in terms of MAEs. In this example, we included the perfor-mance of a model that is trained on CUDIA-reconstructed data. The CUDIA-reconstructed dataset provides better predictive accuracies than the MP-and ER-reconstructed training datasets. However, CUDIA is still a clustering al-gorithm, and the reconstruction from CUDIA is based on es-timated cluster centers. Although CUDIA provides homoge-nous cluster centers, it does not generate fine-grained recon-struction like LUDIA. The predictive Lasso model trained on the CUDIA-reconstructed dataset exhibits higher MAE and Figure 7: Predictive performance of the Lasso ( g lmnet ) models trained on the reconstructed data. Absolute errors are measured using a hold-out dataset. variances than the model trained on the LUDIA-reconstructed dataset.
The implication of our research can be viewed from two perspectives.

Utility perspective . Our method allows aggregated data to be effectively utilized in individual-level inferential tasks. This is particularly important since standard impu-tation techniques do not make use of the summary statistics provided by aggregated data that are widely available for social good. Many machine learning algorithms that require completely observed data can now be directly applied to the LUDIA-reconstructed data.

Privacy perspective . Although the reconstructed val-ues are not guaranteed to be identical to the true values, it is clear that the estimated values are correlated with the actual values. If additional theoretical guarantees are de-veloped, data aggregation may be no longer perfectly safe from privacy attacks. With enough auxiliary information, it is possible that private information gets revealed using tech-niques similar to LUDIA. This implies that, in the future, reconstruction performance will need to be considered prior to data aggregation, to guarantee that privacy requirements are met.

The proposed LUDIA framework can be extended to more complex data models. It is also worthwhile to investigate more efficient solutions for the Sylvester equation. One can also explore theoretical reconstruction guarantees that de-pend on the characteristics of the datasets and of the aggre-gation matrices.
 This work is supported by NSF IIS-1016614 and by TATP grant 01829. [1] M. P. Armstrong, G. Rushton, and D. L. Zimmerman. [2] R. H. Bartels and G. W. Stewart. Solution of the [3] R. Bhatia and P. Rosenthal. How and why to solve the [4] S. Boyd and L. Vandenberghe. Convex Optimization . [5] E. J. Candes and B. Recht. Exact Matrix Completion [6] K. Carroll. Experimental evidence of dietary factors [7] Centers for Disease Control and Prevension (CDC). [8] T. Dalenius and S. P. Reiss. Data-swapping: A [9] Data.CMS.gov. Inpatient prospective payment system. [10] G. T. Duncan, M. Elliot, and J.-J. Salazar-Gonzalez. [11] O. D. Duncan and B. Davis. An alternative to [12] C. Eckart and G. Young. The approximation of one [13] S. E. Fienberg and J. McIntyre. Data swapping: [14] D. A. Freedman. Ecological inference and the [15] D. A. Freedman, S. P. Klein, M. Ostland, and [16] D. A. Freedman, S. P. Klein, J. Sacks, C. A. Smyth, [17] W. A. Fuller. Masking procedures for microdata [18] A. Gelman and J. Hill. Data Analysis using Regression [19] H. Goldstein. Multilevel Statistical Models . Wiley, 4th [20] L. Goodman. Ecological regression and the behavior of [21] L. Goodman. Some alternatives to ecological [22] W. He, X. Liu, H. Nguyen, K. Nahrstedt, and [23] C. R. Johnson. Matrix completion problems: a survey. [24] Kaiser Family Foundation. Hospital adjusted expenses [25] G. King. A Solution to the ecological inference [26] G. King, O. Rosen, and M. A. Tanner. Binomial-beta [27] G. King, O. Rosen, and M. A. Tanner. Ecological [28] A. Machanavajjhala, D. Kifer, J. Gehrke, and [29] C. Ordonez and Z. Chen. Horizontal aggregations in [30] Y. Park and J. Ghosh. A generative framework for [31] Y. Park and J. Ghosh. Cudia: Probabilistic cross-level [32] Y. Park and J. Ghosh. A probabilistic imputation [33] President X  X  Concil of Advisors on Science and [34] J. Richmond. Aggregation and identification. [35] W. S. Robinson. Ecological correlations and the [36] L. Sweeney. k-anonymity: a model for protecting [37] M. Templ. Statistical disclosure control for microdata [38] Texas Department of State Health Services. Texas
