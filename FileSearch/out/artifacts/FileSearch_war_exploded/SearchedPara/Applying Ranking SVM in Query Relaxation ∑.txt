 We propose an approach QRRS (Query Relaxative Ranking SVM) that divides a ranking func tion into different relaxation steps, so that only cheap features are used in Ranking SVM of early steps for query efficiency. We show search quality in the approach is improved compared to conventional Ranking SVM. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Retrieval Models Information Retrieval, Qu ery Relaxation, Ranking SVM Ranking functions in a search engine should be adjusted for different information needs. In internet web search, ranking functions need to be changed fre quently to handle search spams. In enterprise search, different de ployments of search systems or different types of search cor pora may require different ranking functions. One way to adjust ra nking functions is using machine learning methods to automatically train ranking functions. Ranking SVM [1,2] treats the ranking function learning problem as binary classification training. Ranked item pairs are input to learning process. The pair belongs to one class if the first item is ranked higher than the other, and to the other class otherwise. Output is a classification functi on minimizing classification errors with a large margin. The trained function can be used directly as a ranking function to evaluate the relevancy of a single item. On the other hand, for query response time, query relaxation technique is normally used. It divides a query execution into several subsequent relaxation step s. Sub-query from the first step is executed first. Generated hits will be ranked higher than those from other steps. If the first page of search result is filled up, the whole execution stops. Otherwise, s ub-query from the next step is then executed. The process continues until all relaxation steps are executed or the first result page is filled up. Here we propose a new approach: Query Relaxative Ranking SVM (QRRS). We apply Ranking SVM in each relaxation step. Each step has its own learned ranking function. QRRS has quick query response time benefited from its relaxation nature and can also generate more relevant hits compared to no relaxation. In a search ranking system, a ranked item is a pair consisting of a query and a hit document. Only items with the same query are ranked. Each item is represented by a feature vector, which lists features and corresponding feature weights. A feature can be any factor that is used to determine the relevance of a document related to a given query. In general, a feature can be: occurrence of query terms in metadata (title, keyword, description, reference text), occurrence of query terms in content, or static ranks (pagerank or other static ranks, etc). A ranking function accepts an item feature vector and produces a relevancy score for the item. The higher the score, the higher this item will be ranked. Ranking function evaluation normally consists of two steps. The first step is to extract feature v ector of the item. The second step is to compute relevancy score based on input feature vector. In practice, the second step can fi nish with in-memory operations. But feature vector formation has to be done by looking through an inverted index. If the inverted index is too large to put into memory, the costly disk access is necessary. Query relaxation is a method to handle this efficiency challenge. Instead of forming a whole feature vector and using a single ranking function, query relaxation forms several feature vectors, each corresponding to a ranking function for each relaxation step. In executing each step query, feature vector is formed and ranking function is evaluated. To produce the first page of search result, normally of 10 hits, query relaxation steps are executed one after anther until the relaxation steps finish or the fi rst result page is filled up. The union of hit lists from all relaxation steps should consist of all documents hit by the query for completeness. The query relaxation can improve query effici ency and generate hit list with high relevancy, when the following two conditions are satisfied:  X  Feature vector formation for early step is cheaper.  X  Early step generates higher relevant hits than in later step. The two conditions can be satisfi ed simultaneously if we choose some metadata as features for early steps. Firstly, the inverted list for metadata can be indexed in se parate rows for separate access. Normally metadata index size is much smaller compared to content, therefore smaller data need to be read in metadata step. Secondly, documents that match query terms in those metadata are more relevant than in other metadata or content. In Ranking SVM, a training item pair can be represented by two feature vectors x i , x j. To introduce non-linear kernels, we apply a generic mapping  X  ( x) on feature vectors. When  X  ( x) = x , it reduces to linear kernel. The training process is to learn a ranking function in the form of w  X  ( x) , where w is weight vector to be learned. The classification problem is formed by the Ranking SVM as [2,3]: minimize:  X  ( x j ) ) &gt; 1- X  k, when x i ranks higher, or w(  X  ( x when x j ranks higher where C is a parameter that allows trading-off margin size against training error,  X  k are non-negative slack variables to allow some training error. The only difference between the Ranking SVM and the conventional binary SVM cla ssification is that the single vector is replaced with a pair of vectors:  X  ( x i ) - X  ( x weight vector can be written in the form of training pairs as [1]: where  X  * can be computed by kernel function of training pairs [1]. From the above equation, the ranking function w *  X  ( x ) can also be written in the form of kernel k( x , x j ) =  X  ( x )  X  ( x case, w * can be computed explicitly, which makes the ranking function just a linear combination with feature weights.
 To apply the Ranking SVM to query relaxation, we train a separate ranking function for each step from the corresponding feature vector. Query efficiency with relaxation is shown in Fig 1 as a simulated experiment. A two-step relaxation is formed, with first step having the metadata features and second step having all the features. Assuming the number of doc uments hit by the first step obeys binomial distribution with total number of documents N and probability p. The diamond curve shows the probability that the first step cannot return 10 or more than 10 hits with p=0.0001. It smoothly declines as N increas es. We assume the response time for each step grows linearly with N. Therefore we can draw two straight lines showing that the response time in the first step grows in a much more gradual slope than in the last step due to index size difference. The simulated response time curve in square symbols grows first in steep slope because the last step is necessary for enough hits. As N increase, more queries only need the first step, the query respons e time decreases and gradually close to the straight line shown as the trend of first step response time only. The probability p can vary for different queries, if averaging different queries, the transition from the first step to last step will be more blur and last for much longer time. For relevancy test, we used a part of www.oracle.com as experiment collection. It cont ains about 60000 documents, most of them are html pages. Document metadata (for example: title, keywords, description, etc), c ontent, and link information (for example: reference text) are extracted and indexed. The feature weights are binary values (0 m eans query does not occur, 1 means query does occur). The features are formed based on combination of linguistic characteristics of occurring query terms and in which metadata query terms occur. For example, a feature can be: if query phrase occurs in title tag, or if all query terms occur proximately in content, etc. Th e combination results in about 50 different features. We randomly sampled query lis t and found 100 unique queries. A pool of relevant hits was generate d. It was formed by top 30 hits from each of four search engines, which have largely different ranking functions. We asked people to manually evaluate the relevancy of each pooled (query, document) pair. We only formed two relaxation step s. The first step only contains features that involve metadata and link information. The second step contains all features. We randomly chose 50 queries and corresponding feature vectors to train ranking function using SVM light [2]. Other 50 queries were us ed to test ranking functions. In Figure 2, we compare the MRR (Mean Reciprocal Rank) for the accuracy of learned ranking functions between QRRS and the Ranking SVM not using query re laxation. We see the ranking accuracy improves with different kernels for QRRS. 
MRR The ranking function learned from the last relaxation step is the same as that learned without using relaxation. So the accuracy improvement from QRRS must be due to the ranking function learned from the first relaxati on step, which picks up more relevant document onto hit list top. During the process of learning the first step ranking function, only documents whose metadata matches query terms are treated as training samples. That means, we give different weights (or ra nking parameters in [3]) on query and document pairs: 1 for pair th at matches metadata, 0 for pair that does not match metadata. The accuracy improvement by assigning different wei ghts on different document query pairs in the Ranking SVM has been shown in [3]. Here we improve ranking accuracy by choosing ranking parameters in a more heuristic way through query relaxation. [1] R. Herbrich, T.Graepel and K.Obermayer. Large margin [2] T. Joachims, Optimizing Sear ch Engines using Clickthrough [3] Y. Cao, J. Xu, T-Y Liu, H. Li, Y. Huan, H-W Hon, Adapting 
