 Clustering is an indispensable data mining technique, particularly for handling large-scale data. Applied to documents, it automatically groups ones with similar themes together while separating those with different topics. Creating a concise representation of a document is a fundamental problem for clustering and for many other applications that involve text documents, such as information retrieval, categorization and informa-tion extraction. Redundancy in feature space adds noise and often hurts subsequent tasks. This paper follows our previous work on using Wikipedia to create a bag of con-cepts (BOC) document representation [6]. By concept we mean the abstract unit of knowledge represented by a single Wikipedia article. We extend previous work by ex-ploring the semantic relatedness between concepts to calculate the similarity between documents. In the previous work, documents are connected based on the overlap of the concepts that appear in them: this does not take account of the fact that concepts are clearly related to each other. We now e xplicitly incorporate the semantic connec-tions among concepts into the document similarity measure. This allows us to iden-tify topics that are distinct and yet relate closely to each other X  USA and America ,for example X  X nd connect documents at the seman tic level regardless of terminological id-iosyncrasies. The experiments (Section 4) show that our BOC model together with the semantically enriched document similarity measure outperform related approaches. Techniques such as Latent Semantic Indexing (LSI) [2] and Independent Component Analysis (ICA) [7] have been applied to the bag-of-words (BOW) model to find latent semantic word clusters. Representing documents with these clusters also allow subse-quent clustering to relate documents that do not overlap in the original word space. In a quest for comparisons with our document similarity measure, we apply LSI and ICA to our BOC model, and use the identified latent concept structures as features for cluster-ing. Empirical results show that clustering using these latent structures is outperformed by using the plain BOC model, either with or without the enriched document similarity measure.

The paper proceeds as follows. The next s ection briefly describes our approach for identifying concepts in a document, each con cept being associated with a Wikipedia ar-ticle. Section 3 extends the semantic relatedness measure between concepts introduced in [10] to compute the semantic similarity of two documents, which forms a basis for clustering. Section 4 presents experiments and discusses results. Related work is re-viewed in Section 5; Section 6 concludes the paper. In this section we describe our approach for identifying concepts in a document. There are three steps in total: identifying candidate phrases in the document and mapping them to anchor text in Wikipedia; disambiguating anchors that relate to multiple concepts; and pruning the list of concepts to filter out those that do not relate to the document X  X  central thread. The method presented here d iffers from our previous approach in the way it measures the salience of each concept ide ntified in a document and how it selects the best ones to represent the document. 2.1 Selecting Relevan t Wikipedia Concepts The first step is to map document terms to concepts in Wikipedia. Various approaches have been proposed [3,15,5]. We take the same route as [9], and use Wikipedia X  X  vocab-ulary of anchor texts to connect words and phrases to Wikipedia articles. Given a plain text document, we first find phrases in it that match Wikipedia X  X  anchor text vocabulary. For example, Wikipedia articles refer to our planet using several anchors, including Earth , the world and the globe . If any of these phrases appear in the document, the ar-ticle about Earth will be identified as a candidate descriptor. We confine the search for phrases to individual sentences. 2.2 Resolving Ambiguous Terms Anchors may be ambiguous in that they may refer to different concepts depending on the articles in which they are found. For example, Pluto links to 26 different articles, including the celestial body, the Greek god, the Disney character, and a rock band from New Zealand. Disambiguating and selecting the intended concept is essential for creat-ing a correct thematic representation. We use machine learning to identify the correct sense. The input to the classifier is a set of possible targets for a given anchor text and the set of all unambiguous anchors from the surrounding text, which are used as con-text. The classifier predicts, for each sense, the probability of it being the intended one. The sense with the highest probability is sel ected. More details about the algorithm can be found in [9]. 2.3 Pruning the Concept List The resulting list of concepts, which togeth er cover the topics me ntioned in the input document, is rather long, because phrases ar e matched against a huge vocabulary; the Wikipedia snapshot we used (dated Nov. 2007) contains just under five million distinct anchors after lower casing. Irrelevant or marginally related concepts must be pruned: they add noise to the representation, which adversely impacts the document similarity calculation and reduces clustering performan ce. Pruning is based on salience: the aver-age strength of relationship with the other concepts in the document. Let U denote the set of concepts extracted from a doc ument, and salience of concept c i  X  U is defined by: where c j represents the o ther concepts in U and | U | is the total number of concepts iden-tified in the document. The more concepts c i relates to and the greater the strength of those relationships, the more salient c i is. The salience formula depends on SIM ( c i ,c j ) , the semantic relatedness between two concepts. For this we use Milne and Witten X  X  sim-ilarity measure [10]. All concepts in the list are ranked in descending order of SAL ,and a fixed proportion t is discarded from the bottom of the list. In our experiments t is set to 0 . 1 based on empirical observations that this yields the best representation.
It is worth noting that the computational complexity of the above approach is in general linear with the input document length. The disambiguation classifier can be built beforehand, and computing the relatedness between two concepts is a linear operation. The only non-linear calculation is the last step where the averaged relatedness with all the other concepts is computed for each concept in the document. However, this step is restricted to the set of concep ts identified from one document and normally the number of concepts per document is moderate. For example, the two datasets used in our experiments have, on average, 24 (OHSUMed) and 20 (Reuters) concepts per document (before pruning) respectively. Document similarity is typically measured using the cosine of their word vectors, so that matches indicate relatedness and misma tches indicate otherwise. Our new repre-sentation allows us to take conceptual relatedness, rather than just lexical overlap, into account. A document d i is represented by a set of concepts U i , each with a weight w ( c, d i ) (TFIDF value in our experiment). We extend the semantic relatedness between concepts mentioned earlier to the similarity between documents. Given two documents d and d j , their semantic similarity is defined as: Because SIM ( c k ,c l ) is always in [0,1], Sim sem is also bounded within [0,1]. 0 in-dicates topics in one document are completely unrelated to those in the other, and 1 indicates they are the same topics.

We then define the overall similarity between documents d i and d j as a linear com-bination of the cosine similarity Sim cos and Sim sem between two concept vectors: where  X  is a parameter that we set to 0.1 based on preliminary experiments.
In Hu et al. X  X  approach for semantically enri ched document similarity [5], cosine sim-ilarity is computed on three aspects: the two document vectors, their category vectors, and their concept vectors enriched with related terms identified from Wikipedia; and the three parts are combined linearly as the final similarity measure. In our approach, the last two parts are unified neatly by a single semantic relatedness measure.
We illustrate our measure with the example used in Hu et al. X  X  work [5]. Given two concept sets C a = { ( CS , 1) , ( ML , 1) } and C b = { ( DM , 1) , ( DB , 1) } ,Table1shows the relatedness between the four concepts obtained from Milne and Witten X  X  similarity measure. The semantic sim ilarity between document C a and document C b is therefore (0 . 45  X  1+0 . 51  X  1+0 . 80  X  1+0 . 49  X  1) / 4=0 . 5625 . This value is close to that obtained by Hu et al. [5], which is 0 . 57 . It is also worth noting that this similarity measure is not only applicable to the BOC model, but also has the potential to be extended to hybrid models where words and concepts are combined, as in [6]. To focus our investigation on the representation rather than the clustering method, we used the standard k-means algorithm. We cr eated two test sets, following [5], so as to compare our results with theirs 1 .  X  Reuters-21578 contains short news articles. The subset created consists of cate- X  OHSUMed contains 23 categories and 18302 documents. Each document is the 4.1 Methodology Before beginning the experiments we collected all anchor texts in the November 20, 2007 Wikipedia snapshot and lower-cased them. This produced just under five mil-lion distinct phrases linking to almost all of the two million articles in the snapshot. Documents were preprocessed by selecting only alphabetic sequences and numbers, lower-casing them, and remov ing concepts that appeared ju st once across the dataset.
Each document is represented by a vector its frequency in document d , df ( t ) is its document frequency, and | D | is the total number of documents in the dataset. We set the number of clusters to the number of classes in the data. Each cluster is labeled with its dominant class. Results reported are the average of 5 runs. To compare our results with previous work, we use two evaluation measures: Purity and Inverse Purity. We also use the micro-averaged F-measure [13], weighted by class size, in a separate experiment. 4.2 Evaluation of the Semantic Document Similarity Table 2 shows how our new document similarity measure performs in clustering on the two datasets. The other rows show the performance of Hu et al. X  X  algorithm and the baselines to which they compared it to: the traditional BOW, a reimplementation of Hotho et al. X  X  WordNet-based algorithm [4], and a system that applies Gabrilovich and Markovich X  X  document categorization approach [3] to clustering. Our system and Hu et al. X  X  achieve comparable results, and are the only two approaches to provide substantial improvements over the baseline. We obtained better inverse purity because classes are more concentrated into clusters rather than dispersed across multiple clusters.
To further explore the differences between these approaches, let us take a closer look at one document that was clustered. Table 3 compares some of the concepts produced when each of the systems is asked to cluster Reuters document #15264 (results for other approaches were taken from [5]). This document discusses ongoing attempts by Teck Cominco X  X  Canadian mining company X  X o begin a joint copper-mining venture in Highland Valley, British Columbia. All of the approaches are able to pick up on the different minerals and units X  copper , silver , ounce  X  X nd will (implicitly or explicitly) relate to synonyms such as Cu and oz . The first system, by Hotho et al., does so using WordNet, a lexical rather than encyclopedic resource. Thus it fails to pick up specific named entities such as Te c k C o m i n c o , but will identify terms that do not resolve to Wikipedia articles, such as complete . Each of the terms shown in the table can be further expanded with WordNet semantic relations; copper can be expanded with the associated term cupric and the hypernyms metallic element , metal and conductor .
 All of the latter three approaches use Wikipedia. The approach inspired by Gabrilovich and Markovich gathers Wikipedia concepts through term overlap with the document. This unfortunately allows tenuously related concepts such as Scottish Highlands and the Economy of Manchukuo to creep into the representation and cause problems. Additionally this system performs disambiguation only indirectly, which introduces more irrelevant concepts such as Copper (color) .

The last two systems have the tightest rep resentation of the document, because they only contain the Wikipedia concepts that are directly discussed. Both are then able to expand out from these concepts to identify related documents regardless of textual overlap. Hu et al. X  X  system considers broad er topics mined from the categories to which each article belongs, and associated topics mined from the links extending out from each article. Thus Te c k C o m i n c o is expanded with Mining companies in Canada and Con Mine in [5]. Our system, in comparison, does not need to expand concepts beforehand. Instead it can compare any two W ikipedia concepts as required, based on the relatedness measure introduced in [10]. Te c k C o m i n c o is essentially expanded on demand with a huge pool of possib ilities, such as different mining companies ( Codelco , De Beers , and about a hundred others), tools ( Drilling rig , Excavator , etc.) and locations (the Pebble Mine in Alaska, for example). All of these new concepts are weighted with a proven relatedness measure [10], and only the concepts that are necessary to connect two related documents are ever considered. 4.3 Latent Semantic Indexing with Concepts As an additional experiment, we apply Laten t Semantic Indexing (LSI) and Independent Component Analysis (ICA) on the BOC representation (concept vectors with TFIDF values). LSI and ICA find latent structures/independent components respectively by analyzing the concept-document matrix. The purpose is to use the identified latent con-cept clusters as features for clustering and co mpare its effectiveness in connecting doc-uments that do not overlap in the original concept space with using the semantically enriched document similarity measure defined in Section 3.

The only work to our knowledge so far that uses LSI with features extracted us-ing Wikipedia is [11], where LSI is used to reduce dimensionality and Wikipedia is used to enrich text models for text categorization. Instead we use Wikipedia to extract concepts from the input document and apply LSI/ICA directly to the BOC model that is generated. ICA has been applied to text documents in [8] and found to produce better group structures in feature space than LSI. We used the FastICA program 2 with its de-fault settings. For a fair comparison, th e number of independent components in ICA is set to the number of eigenvalues retained in LSI. The cosine measure ( Sim cos )isused throughout this experiment.

Table 4 shows the performance of using latent concept groups as features for clus-tering on the Reuters dataset. The OHSUM ed dataset could not be processed because it is computationally prohibitive. The results show that the latent concept groups are not as effective as the original concepts: using cosine similarity on the BOC model (ie. based on overlaps between concept sets) still outperforms. This could be explained by the fact that ICA and LSI are applied globally and do not use any knowledge about the categories in the datasets, so the latent semantic structures that are found do not retain sufficient discriminative information to differentiate the classes [14]. Local alternatives for LSI and ICA may be better choices; but are beyond the scope of this paper. Document representation is a fundamental issue for clustering, and methods such as BOW, bags of phrases and n-grams have been widely investigated. Explicitly using external knowledge bases can assist genera ting concise representations of documents. Related work in this area includes Hotho et al. [4] and Recupero [12]; both use relations defined in WordNet to enrich BOW. Techniques such as Latent Semantic Indexing and Independent Component Analysis have been used to find latent semantic structures in dataset [2,8]; each structure is a linear comb ination of the original features (typically words). Representing documents with these latent structures can reduce the dimension-ality of feature space while retaining essential semantic information, yielding significant improvement in subsequent tasks, in information retrieval [2] for example.

Despite widespread adoption for many tasks, only a limited amount of work has investigated utilizing Wikipedia as a knowledge base for document clustering [1,5,6]. Our previous work focuses on how to generate the concept-based representation for text documents and use Wikipedia to provide supervision for active learning [6]; the present paper focuses on extending the rel atedness between concepts to measuring the relatedness between documents, evaluating the impact of the semantically enriched document similarity measure on clustering, and gives a more detailed analysis of the concept-based document representation. The algorithm described in this paper also document.

Our approach differs markedly from that of Hu et al. [5]. Our process for selecting and disambiguating terms to identify relevant Wikipedia concepts draws directly on previous work [9] and has been separately evaluated against manually-defined ground truth. In contrast, theirs was developed specifically for the task and has not been inves-tigated independently. Another significant difference is the way in which the document similarity measures are calculated. They develop their own methods of measuring simi-larity through Wikipedia X  X  category links an d redirects, and append this to the traditional metric obtained from the BOW model. We instead start with an independently proven method of measuring relatedness between concepts [10] that takes all of Wikipedia X  X  hyperlinks into account, and genera lize this to compare documents. This paper has presented a new approach to document clustering that extends a se-mantic relatedness measure defined between concepts in Wikipedia to measure docu-ment similarity. Results on two datasets prove the effectiveness of our BOC model and the enriched document similarity measure. We also investigated clustering based on a transformed feature space that encodes sem antic information derived directly from the dataset, by applying LSI and ICA to the BOC model and using the latent semantic struc-tures instead of original concepts as features for clustering, as a comparison to using the semantically enriched document similarity. Results suggest that these techniques do not improve clustering using the BOC model when performed globally.

We also observed from our earlier work [6] that BOC model can often be improved by adding further words from the document that are not represented in the BOC model, especially when the topics involved are s imilar. Yet in this paper we consider only concepts, albeit with an approved similarity measure. This suggests a hierarchical ap-proach: first cluster coarsely using the BOC model, and refine clusters using hybrid models like the Replaced model in [6] X  X nother interesting avenue for future work.
