 Partial label learning deals with the problem where each training example is represented by a feature vector while associated with a set of candidate labels, among which only one label is valid. To learn from such ambiguous labeling information, the key is to try to disambiguate the candidate label sets of partial label training ex-amples. Existing disambiguation strategies work by either identi-fying the ground-truth label iteratively or treating each candidate label equally. Nonetheless, the disambiguation process is generally conducted by focusing on manipulating the label space, and thus ignores making full use of potentially useful information from the feature space. In this paper, a novel two-stage approach is proposed to learning from partial label examples based on feature-aware dis-ambiguation . In the first stage, the manifold structure of feature space is utilized to generate normalized labeling confidences over candidate label set. In the second stage, the predictive model is learned by performing regularized multi-output regression over the generated labeling confidences. Extensive experiments on artificial as well as real-world partial label data sets clearly validate the su-periority of the proposed feature-aware disambiguation approach. weak supervision; partial label learning; disambiguation; manifold
Partial label (PL) learning refers to the problem where each train-ing example is represented by a single instance (feature vector) while associated with a set of candidate labels [8, 26]. Among the candidate label set, only one label is assumed to be valid and not directly accessible to the learning algorithm. The need to learn from data with partial labeling information naturally arises in many real-world applications such as automatic image annotation [7, 25], web mining [14], ecoinformatics [16], etc. 1
In some literatures, the partial label learning framework is also named as ambiguous label learning [5, 13] or superset label learn-ing [17].

Formally speaking, let X = R d denote the d -dimensional fea-ture space and Y = { y 1 ,y 2 ,...,y q } denote the label space with q class labels. The task of partial label learning is to learn a multi-class classifier f : X7 X  X  from the partial label training set D = { ( x i ,S i ) | 1  X  i  X  m } . Here, for each PL training exam-vector and S i  X  X  is the set of candidate labels associated with x Partial label learning takes the basic assumption that the ground-truth label y i for x i resides in its candidate label set (i.e. y but unknown to the learning algorithm.

Apparently, the available labeling information in the training set is ambiguous as the ground-truth label is concealed in the candi-date label set. The key for successful partial label learning is there-fore trying to disambiguate the set of candidate labels, where ex-isting strategies include disambiguation by identification or disam-biguation by averaging. For identification-based disambiguation, the ground-truth label is regarded as latent variable and identified through iterative refining procedure such as EM [5, 15, 16, 18, 24]. For averaging-based disambiguation, all the candidate labels are treated equally and the prediction is made by averaging their mod-eling outputs [8, 13, 27].

By taking specific views on the candidate labels, both of the ex-isting strategies conduct disambiguation by only focusing on the manipulation of label space. Nonetheless, it is natural to postulate that the potentially useful information from feature space should also be exploited to facilitating the disambiguation process. Specif-ically, to help disambiguate the candidate label set, one might make use of the smoothness assumption that examples close to each oth-er in the feature space will tend to share identical label in the label space. For instance, suppose we have three PL training examples ( x , { y 2 ,y 3 } ) , ( x  X  , { y 1 ,y 2 } ) and ( x  X  X  , to be close to x  X  while far from x  X  X  in the feature space. Then, it is reasonable to assign higher labeling confidence on y 2 than y between close instances ( x and x  X  ) while y 3 is a shared candidate label between distant instances ( x and x  X  X  ).

In light of the above observation, a novel two-stage approach named P L -LEAF , i.e. Partial Label LEArning via Feature-aware disambiguation , is proposed in this paper. In the first stage, the manifold structure among training examples is analyzed in the fea-ture space and then utilized to generate normalized labeling confi-dences over candidate label set. In the second stage, a multi-class predictive model is learned by fitting a regularized multi-output re-gressor with the generated labeling confidences. Extensive experi-ments on controlled UCI data sets as well as real-world PL data sets clearly show the effectiveness of feature-aware disambiguation for partial label learning.

The rest of this paper is organized as follows. Section 2 briefly re views related work in partial label learning. Section 3 introduces the proposed P L -LEAF approach. Section 4 reports experimental results of comparative studies. Finally, Section 5 concludes the paper and discusses future research issues.
Due to the ambiguous labeling information conveyed by PL train-ing examples, partial label learning can be regarded as a weakly-supervised learning framework. It situates between two ends of the supervision spectrum, i.e. standard supervised learning with explicit supervision and unsupervised learning with blind supervi-sion. Furthermore, partial label learning is related to other well-studied weakly-supervised learning frameworks, including semi-supervised learning , multi-instance learning and multi-label learn-ing , while the weak supervision scenario for partial label learning is different to those counterpart frameworks.

Semi-supervised learning [4, 29] aims to learn a predictive mod-el f : X7 X  X  from few labeled data together with abundant un-labeled data. For unlabeled data the ground-truth label assumes the entire label space, while for PL data the ground-truth label is confined within its candidate label set. Multi-instance learning [1, 9] aims to learn a predictive model f : 2 X 7 X  X  from training examples each represented as a labeled bag of instances. For multi-instance data the label is assigned to bag of instances, while for PL data the label is assigned to single instance. Multi-label learning [11, 28] aims to learn a predictive model f : X7 X  2 Y from train-ing examples each associated with multiple labels. For multi-label data the associated labels are all valid ones, while for PL data the associated labels are only candidate ones.

Existing approaches learn from PL training examples by trying to disambiguate their candidate label sets. One disambiguation s-trategy is to assume certain parametric model F ( x ,y ; ) where the ground-truth label is regarded as latent variable and identified is refined iteratively via EM procedure which optimizes objective function defined according to the maximum likelihood criterion:  X  criterion: [18, 24]. One potential drawback of the identification-based dis-ambiguation strategy lies in that, rather than recovering the ground-itive label in the candidate label set (i.e. S i \{ y i }
Another disambiguation strategy is to assume equal importance of each candidate label and then make prediction by averaging their modeling outputs. Under discriminative learning setting, the aver-aged output from all candidate labels, i.e. 1 | S is distinguished from the outputs from non-candidate labels, i.e. F ( x i ,y ; ) ( y /  X  S i ) [8]. Under instance-based learning setting, the predicted label for unseen instance is determined by averag-ing the candidate labeling information from its neighboring exam-ples in the PL training set [13, 27]. One potential drawback of the averaging-based disambiguation strategy lies in that the essential modeling output from ground-truth label y i might be overwhelmed by the distractive outputs from false positive labels.
 In the next section, a novel partial label learning approach named P -LEAF will be introduced. Other than disambiguation by identi-fication or averaging, P L -LEAF facilitates the disambiguation pro-cess by making use of local topological information from the fea-ture space. The candidate label set is disambiguated in the form of normalized labeling confidences, which differs from the crisp-style disambiguation of identifying a single candidate label or the uniform-style disambiguation of averaging all candidate labels.
As shown in Section 1, the task of partial label learning is to learn a multi-class classifier f : X7 X  X  from the PL training set { ( x i ,S i ) | 1  X  i  X  m } . For the proposed P L -LEAF approach, its key novelty lies in how the disambiguation procedure is conducted. Specifically, for each PL training example ( x i ,S i ) , P to disambiguate its candidate label set S i via a normalized real-valued vector i = (  X  i 1 , X  i 2 ,..., X  iq )  X  . Here, each component  X  ik (1  X  k  X  q ) represents the labeling confidence of y k ground-truth label for x i , which satisfies the following constraints: Once the normalized labeling confidence vectors have been gener-ated, the predictive model will be induced by utilizing the disam-biguation results.

In the next subsections, the two basic stages of P L -LEAF feature-aware disambiguation and predictive model induction , will be scrutinized respectively.
In the first stage, P L -LEAF aims to disambiguate the candidate label set by exploiting useful information from the feature space. To fulfill this task, the popular smoothness assumption is adopted to enable information exploitation from the feature space to the la-bel space. Given the PL training set D = { ( x i ,S i ) | a weighted graph G = ( V,E, W ) is constructed over the training examples to characterize the manifold structure of feature space. Here, V = { x i | 1  X  i  X  m } corresponds to the set of vertices and directed edges from x i to x j iff x i is among the K -nearest neigh-bors of x j . Furthermore, W = [ W ij ] m  X  m corresponds to the non-negative weight matrix where W ij = 0 if ( x i , x j ) /  X  weight matrix, its j -th column W  X  j = ( W 1 j ,W 2 j ,...,W determined by solving the following linear least square problem: Conceptually, the weight value W ij (( x i , x j )  X  E ) characterizes the relative importance of neighboring example x i in reconstruct-ing x j . According to the constraint above optimization problem ( OP ) can be re-written as: Here, G j = [ G j ab ] m  X  m is the local Gram matrix for x sponds to a standard quadratic programming (QP) problem whose optimal solution can be obtained by any off-the-shelf QP solver. The weight matrix W is constructed by solving OP (2) column-wisely and the resulting matrix is generally not symmetric. Ac-cordingly, based on the local topological information embodied in G , the manifold structure in the feature space will be exploited to help disambiguate the candidate label set.

Here, in order to generate the labeling confidence vectors = [ 1 , 2 ,..., m ] , P L -LEAF exploits the smoothness assumption Inputs:
D : the partial label training set { ( x i ,S i ) | 1  X  i ,S i  X  X  , X = R d , Y = { y 1 ,y 2 ,...,y q } ) K : the number nearest neighbors used for weighted graph construction
C 1 ,C 2 : the regularization parameters for regression loss function x : the unseen instance ( x  X  X  )
Outputs: y : the predicted label for x
Process: 1: Set the weighted graph G = ( V,E, W ) with V = { x i | m } and E = { ( x i , x j ) | x i  X  KNN( x j ) , i  X  = j } ; 2: for j = 1 to m do 4: end for 6: Calculate the kernel matrix K = [  X  ( x , x j )] m  X  m 7: Calculate = [ 1 , 2 ,..., m ] with i =  X   X  1 8: Set t = 0 ; 9: Initialize (0) and b (0) with (0) k = 0 and b (0) k = 0 (1 10: repeat 12: for k = 1 to q do 13: Obtain the solution ~ k and ~ b k by solving Eq.(17); 14: end for 15: Set the descending direction P ( t ) according to Eq.(16); 17: t = t + 1 ; ) 19: Set the final predictive model with  X  = ( t ) and b  X  20: Return y = f ( x ) according to Eq.(18). that the manifold structure in the feature space should also be pre-served in the label space: According to the constraint can be re-written as:  X  ij and 1 m  X  m represent the i -th row of weight matrix W , the Kro-necker X  X  delta and the m  X  m matrix of 1 X  X  respectively.
Note that OP (4) corresponds to a standard QP problem with mq variables and m ( q +1) constraints, whose computational complex-ity would be demanding when mq is large. To improve efficiency, we can choose to solve OP (4) with alternating optimization strat-egy where a series of QP subproblems with q variables and q + 1 constraints are optimized iteratively. Without loss of generality, in each alternating optimization iteration, one labeling confidence vector i is optimized by fixing the values of other labeling confi-dence vectors j ( j  X  = i ) :
It is interesting to notice that, to some extent, the disambiguation results returned by existing strategies can be viewed as simplified versions of P L -LEAF  X  X  normalized labeling confidence vectors. For identification-based disambiguation [16, 18], a single label y S i is identified as the ground-truth label leading to unimodal la-beling confidence vector with  X  i ^ k = 1 and  X  ik = 0 ( k For averaging-based disambiguation [8, 13], all candidate labels in S are treated equally leading to uniform labeling confidence vec-tor with  X  ik = 1 | S ing confidence vector considered by P L -LEAF would accommodate more flexibility in modeling the disambiguation results compared to the unimodal or uniform setup.
Following the first stage of feature-aware disambiguation, the o-riginal PL training set D has been transformed into its disambiguat-ed counterpart: D dis = { ( x i , i ) | 1  X  i  X  m } . In the second stage, P L -LEAF aims to induce the predictive model f : based on D dis . Considering that the response variables (normal-ized labeling confidences) for each training example in D actually real-valued, it is natural to induce the predictive model by performing multi-output regression . Among various choices of multi-output regression techniques, we choose to adapt the multi-regression support vector machines (MSVR) [6, 21, 23] such that kernel trick can be readily incorporated to accommodate nonlinear modeling.

Let  X  (  X  ) : R d 7 X  R H be the (implicit) nonlinear mapping from the original feature space to the higher-dimensional Reproducing Kernel Hilbert Space (RKHS) via kernel function  X  : X X X7 X  R Furthermore, let { ( k ,b k ) | 1  X  k  X  q } denote the multi-output regression model in the RKHS with one linear predictor ( k for each class label y k  X  X  . Then, P L -LEAF induces the regression model by minimizing the following loss function:
L ( , b ) = 1 Here, = [ 1 , 2 ,..., q ] and b = [ b 1 ,b 2 ,...,b q ]  X  the regression model X  X  weight matrix and bias vector respectively.
As shown in Eq.(6), the first term of L ( , b ) controls the com-plexity of the induced model. In addition, the second term of L ( , b ) is defined based on the  X  -insensitive loss function: For each example ( x i , i ) in D dis , the corresponding input to the  X  -insensitive loss function L 1 (  X  ) is set as: u i = || with e i = i  X   X   X  ( x i )  X  b . In this way, the outputs of all linear predictors are considered simultaneously to yield a unique input to L 1 (  X  ) such that the dependencies among all the class labels can be exploited by the  X  -insensitive term. The third term of L ( , b ) considers the partial label loss for each example which is set as: Here, for candidate label set S i and its complementary set 1 i ( 1 ^ S i ) corresponds to a q -dimensional vector whose k -th ele-ment equals to 1 if y k  X  S i ( y k  X  ^ S i ) and 0 otherwise. In other words, the third term enforces the property that the average out-put from candidate labels should be larger than the average output from non-candidate ones, which has been widely-used in designing effective partial label learning algorithms [8, 13, 26].
To minimize L ( , b ) , P L -LEAF employs the gradient-based it-erative method named Iterative Re-Weighted Least Square (IRWL-S) [21, 23]. Specifically, in each iteration the descending direction is determined analytically by solving linear systems of equations. Taylor expansion: Here, u ( t ) i and e ( t ) i are calculated based on the current model b ( t ) } . Then, a quadratic approximation to L ( u i ) is further con-structed to ease analytical solution to the descending direction: where and T i is a constant which does not depend on { , b } .
Based on Eqs.(10) and (11), the objective function L ( , b ) can be re-written as: ~
L ( , b ) = (12) In contrast to standard least square objective function 1 + C 1 weighted least square problem along with partial label loss regular-ization. Minimization of ~ L ( , b ) can be decoupled for each class label, whose solution for each ( k ,b k ) (1  X  k  X  q ) is found by equating the corresponding gradient to zero:  X  C  X   X  Here,  X  ik corresponds to the k -th component of the q -dimensional vector i =  X  and (14) can be expressed as a linear system of equations: [ Here, = [  X  ( x 1 ) , X  ( x 2 ) ,..., X  ( x m )]  X  , D  X  = [ d d
Let ~ = [ ~ 1 , ~ 2 ,..., ~ q ] and ~ b = [ ~ b 1 , ~ b 2 tion obtained by solving Eq.(15) for each class label, the descend-ing direction for the next iteration would be: The subsequent model { ( t +1) , b ( t +1) } is then updated by invok-ing line search procedure from { ( t ) , b ( t ) } along the descending direction P ( t ) [19].

According to the Representer Theorem [22], under fairly gen-eral conditions, the predictive model can be expressed by a lin-ear combination of the training examples in the RKHS, i.e. Eqs.(13) and (14), the linear system of Eq.(15) can be expressed as follows: [ amples. Based on the kernel trick, the line search procedure can be readily performed in terms of k and b k as well.

Let  X  and b  X  be the resulting model after the whole iterative optimization process, P L -LEAF makes prediction on the class label of unseen instance x as follows: (with one false positive candidate label [ r = 1] ).
 Table 1 summarizes the pseudo-code of P L -LEAF . 2 Given the PL training set, a weighted graph is constructed to characterize the manifold structure of feature space which is then utilized to dis-ambiguate the candidate label set (Steps 1-5). After that, a predic-tive model based on kernelized multiregression SVR is learned via gradient-based iterative optimization (Steps 6-18). 3 Finally, pre-diction on the unseen instance is made via the learned predictive model (Steps 19-20).
To evaluate the performance of P L -LEAF , two series of compar-ative experiments are conducted on controlled UCI data sets [2] as well as real-world partial label data sets. Characteristics of the experimental data sets are summarized in Table 2.

Following the widely-used controlling protocol in partial label learning research [5, 8, 16, 24, 27], an artificial PL data set can be generated from a multi-class UCI data set with three controlling parameters p , r and  X  . Here, p controls the proportion of examples false positive labels in the candidate label set (i.e. | S and  X  controls the co-occurring probability between one coupling candidate label and the ground-truth label. As shown in Table 2, a total of 28 (4x7) parameter configurations have been considered for each UCI data set.

In addition to artificial data sets, a number of real-world PL data
Code package for P L -LEAF is publicly-available at: http://cse.seu. edu.cn/PersonalPage/zhangml/Resources.htm#kdd16  X  = 0 . 1 and the convergence condition in Step 18 is instantiated with  X  = 10  X  10 . sets have been collected from several task domains. 4 For the task of facial age estimation ( FG-NET [20]), human faces with landmarks are represented as instances while ages annotated by ten crowd-sourced labelers together with the ground-truth age are regarded as candidate labels. For the task of automatic face naming ( Lost [8], Soccer Player [25] and Yahoo! News [12]), faces cropped from an image or video frame are represented as instances while names extracted from the associated captions or subtitles are re-garded as candidate labels. For the task of bird song classification ( BirdSong [3]), singing syllables of the birds are represented as instances while bird species jointly singing during a 10-seconds period are regarded as candidate labels. For the task of object clas-sification ( MSRCv2 [16]), image segmentations are represented as instances while objects appearing within the same image are re-garded as candidate labels. The average number of candidate label-s (Avg. #CLs) for each real-world PL data set is also recorded in Table 2.
 To show the effectiveness of feature-aware disambiguation, P LEAF is compared against four state-of-the-art partial label learning approaches with diverse properties, each configured with parame-ters suggested in respective literatures: These data sets are publicly-available at: http://cse.seu.edu.cn/ PersonalPage/zhangml/Resources.htm#partial_data (with two false positive candidate labels [ r = 2] ).
Parameters for P L -LEAF (Table 1) are set as K = 10 , C and C 2 = 1 . 5 In this paper, ten runs of 50%/50% random train/test splits are performed on each artificial as well as real-world PL data set. Accordingly, the mean predictive accuracies (with standard deviation) are recorded for all comparing algorithms.
In Figure 1, the classification accuracy of each comparing algo-rithm is illustrated where the co-occurring probability  X  varies from 0.1 to 0.7 with step-size 0.1 ( p = 1 ,r = 1 ). For any ground-truth label y  X  X  , one extra label y  X   X  = y is designated as the cou-pling label which co-occurs with y in the candidate label set with probability  X  . Otherwise, any other class label would be chosen to co-occur with y . In Figures 2 to 4, the classification accuracy of each comparing algorithm is illustrated where the proportion p varies from 0.1 to 0.7 with step-size 0.1 ( r = 1 , 2 , 3 ). Together with the ground-truth label, r class labels in Y will be random-
F or P L -LEAF , Gaussian kernel  X  ( x i , x j ) = exp is employed in Step 6 with  X  being the average distance among each pair of training examples. Furthermore, on the two artificial ( usps , pendigits ) and two real-world ( Soccer Player , Yahoo! News ) data sets with large scale, alternating optimiza-tion is employed in Step 5. Sensitivity analysis on P L -rameter configuration is conducted in Subsection 4.3.
 Table 3: Win/tie/loss counts (pairwise t -test at 0.05 significance level) on the classification performance of P L -LEAF against each comparing algorithm.
 [Figure 1] 26/7/9 31/11/0 27/13/2 20/16/6 [Figure 2] 28/7/7 42/0/0 35/7/0 23/16/3 [Figure 3] 28/7/7 40/2/0 33/9/0 23/12/7 [Figure 4] 29/6/7 39/3/0 32/10/0 26/12/4 ly picked up to constitute the candidate label set for each partially labeled example.

As shown in Figures 1 to 4, in most cases, P L -LEAF achieves superior or competitive performance against the comparing algo-rithms. Based on pairwise t -test at 0.05 significance level, Table 3 summarizes the win/tie/loss counts between P L -LEAF and each comparing algorithm. Out of the 168 statistical tests (28 configura-tions  X  6 UCI data sets), it is shown that: (with three false positive candidate labels [ r = 3] ).  X  / (pairwise t -test at 0.05 significance level).
 0.047  X  0.017  X  0.058  X  0.010  X  0.056  X  0.008  X  0.240  X  0.045  X  0.343  X  0.022  X  0.344  X  0.026  X  0.343  X  0.055  X  0.473  X  0.016  X  0.478  X  0.025  X  0.670  X  0.024 0.639  X  0.056 0.591  X  0.019  X  0.375  X  0.020  X  0.417  X  0.027  X  0.431  X  0.008  X  0.624  X  0.009  X  0.671  X  0.018  X  0.692  X  0.015  X  0.347  X  0.004  X  0.430  X  0.004  X  0.506  X  0.006  X  0.457  X  0.005  X  0.615  X  0.002  X  0.594  X  0.007
Table 4 reports the detailed predictive performance of each com-paring algorithm on the real-world PL data sets, where the out-comes of pairwise t -tests at 0.05 significance level are also record-ed. Note that the average number of candidate labels (Avg. #CLs) for the FG-NET data set (i.e. 7.48 as shown in Table 2) is quite large, which makes the task of facial age estimation (based on train-ing examples with partial labels) rather challenging. Furthermore, the state-of-the-art performance on this data set (based on train-ing examples with ground-truth labels) corresponds to more than 3 years of mean average error (MAE) between the predicted age and the ground-truth age [20]. In Table 4, two extra classification accuracies are reported on the FG-NET data set where an unseen example is regarded to be correctly classified if the difference be-tween the predicted age and the ground-truth age is less than 3 years (MAE3) or 5 years (MAE5).

As shown in Table 4, it is impressive to observe that:
In addition to Table 4 reporting inductive performance on test ex-amples, it is also interesting to study the transductive performance of each comparing algorithm on classifying training examples [8, 24]. For each PL training example ( x i ,S i ) , its ground-truth label is predicted by consulting the candidate label set S i , i.e. predicting ^ y  X  S i with largest modeling output. In other words, transductive  X  / (pairwise t -test at 0.05 significance level).
  X  0.018 0.136  X  0.021 0.138  X  0.019 0.142  X  0.010
FG-NET (MAE3) 0.567  X  0.015 0.559  X  0.023 0.548  X  0.017  X  0.502  X  0.030  X  0.541  X  0.022  X  0.556  X  0.014
FG-NET (MAE5) 0.710  X  0.014 0.721  X  0.021 0.684  X  0.021  X  0.644  X  0.021  X  0.684  X  0.018  X  0.698  X  0.010  X  0.007  X  0.814  X  0.033 0.755  X  0.018  X  0.788  X  0.025  X  0.035  X  0.656  X  0.026 0.603  X  0.016  X  0.629  X  0.016  X  0.014 0.831  X  0.013 0.827  X  0.017 0.787  X  0.016  X  0.005  X  0.733  X  0.007  X  0.688  X  0.003  X  0.669  X  0.003  X  0.002  X  0.861  X  0.003  X  0.861  X  0.002  X  0.822  X  0.002 P 6 to 14 with step-size 2; (c) Classification accuracy of P performance of the partial label learning algorithm reflects its abil-ity in disambiguating the candidate label set. Accordingly, Table 5 reports the transductive accuracy of each comparing algorithm together with the outcomes of pairwise t -tests at 0.05 significance level. Out of the 32 statistical tests (8 data sets  X  4 comparing algorithm), it is shown that:
As the feature-aware disambiguation stage of P L -LEAF finishes, the generated labeling confidence vector i can also be used to pre-dict the ground-truth label of x i as ^ y i = arg max y k resulting transductive performance is reported in Table 5 (denoted as P L -LEAF  X  ) for referencee purpose. In most cases, the transduc-tive performance of P L -LEAF  X  is close to P L -LEAF indicating the usefulness of generated labeling confidence vectors.

As shown in Table 1, to study the sensitivity of P L -LEAF its parameters K , C 1 and C 2 , Figure 5 illustrates how P performs under different parameter configurations. For clarity of illustration, three data sets ( Lost , MSRCv2 and BirdSong ) are chosen here for sensitivity analysis while similar observations also hold on other data sets.
 It is obvious from Figure 5 that the performance of P L -stable across a broad range of each parameter. This property is quite desirable as one can make use of P L -LEAF to achieve robust clas-sification performance without the need of parameter fine-tuning. Therefore, the parameter configuration for P L -LEAF in Subsection 4.1 ( K =10, C 1 =10, C 2 =1) naturally follows from these observa-tions.
In this paper, a novel approach named P L -LEAF is proposed to learning from partial label examples. Different from existing s-trategies, P L -LEAF aims to disambiguate the candidate label set by manipulating useful information in the feature space. Specifically, P -LEAF generates normalized labeling confidence vectors based on manifold relationships among training examples, and then in-duces the predictive model based on multi-output regression anal-ysis. Comparative studies across comprehensive partial label data sets clearly verify the effectiveness of the proposed approach.
For P L -LEAF , an important future work is to investigate ways to perform manifold structure discovery and labeling confidence generation simultaneously. Secondly, it is worth studying whether there are better techniques to exploit the generated labeling confi-dence vectors, such as fitting probabilistic models [10]. Thirdly, it is also interesting to explore other ways to fulfill the feature-aware disambiguation strategy. The authors wish to thank the anonymous reviewers for their con-structive comments and suggestions. This work was supported by the National Science Foundation of China (61222309, 61573104, 61473087), the Natural Science Foundation of Jiangsu Province (BK20141340), the MOE Program for New Century Excellent Tal-ents in University (NCET-13-0130), and the Collaborative Innova-tion Center of Wireless Communications Technology. [1] J. Amores. Multiple instance classification: Review, [2] K. Bache and M. Lichman. UCI machine learning repository. [3] F. Briggs, X. Z. Fern, and R. Raich. Rank-loss support [4] O. Chapelle, B. Sch X lkopf, and A. Zien, editors.
 [5] Y.-C. Chen, V. M. Patel, R. Chellappa, and P. J. Phillips. [6] W. Chung, J. Kim, H. Lee, and E. Kim. General dimensional [7] T. Cour, B. Sapp, C. Jordan, and B. Taskar. Learning from [8] T. Cour, B. Sapp, and B. Taskar. Learning from partial labels. [9] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P X rez. [10] X. Geng, C. Yin, and Z.-H. Zhou. Facial age estimation by [11] E. Gibaja and S. Ventura. A tutorial on multilabel learning. [12] M. Guillaumin, J. Verbeek, and C. Schmid. Multiple instance [13] E. H X llermeier and J. Beringer. Learning from ambiguously [14] L. Jie and F. Orabona. Learning from candidate labeling sets. [15] R. Jin and Z. Ghahramani. Learning with multiple labels. In [16] L. Liu and T. Dietterich. A conditional multinomial mixture [17] L. Liu and T. Dietterich. Learnability of the superset label [18] N. Nguyen and R. Caruana. Classification with partial labels. [19] J. Nocedal and S. Wright. Numerical Optimization . Springer, [20] G. Panis and A. Lanitis. An overview of research activities in [21] M. S X nchez-Fern X ndez, M. de Prado-Cumplido, [22] B. Sch X lkopf and A. J. Smola. Learning with Kernels: [23] D. Tuia, J. Verrelst, L. Alonso, F. P X rez-Cruz, and [24] F. Yu and M.-L. Zhang. Maximum margin partial label [25] Z. Zeng, S. Xiao, K. Jia, T.-H. Chan, S. Gao, D. Xu, and [26] M.-L. Zhang. Disambiguation-free partial label learning. In [27] M.-L. Zhang and F. Yu. Solving the partial label learning [28] M.-L. Zhang and Z.-H. Zhou. A review on multi-label [29] X. Zhu and A. B. Goldberg. Introduction to semi-supervised
