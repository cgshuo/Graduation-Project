 D ialogue systems aim to engage users in rich, ada ptive natural language conversation . For these systems, understanding the role of a user X  X  utte r-ance in the broader context of the dialogue is a key challenge (Sridhar, Bangalore, &amp; Narayanan, 2009) . Central to this endeavor is d ialogue act classification , which categorizes the intention b e-hi nd the user X  X  move (e.g., asking a question, providing declarative information). Automatic di a-logue act classification has been the focus of a large body of research, and a vari ety of approac h-es, including sequential models (Stolcke et al., 2000) , vector -b ased models (Sridhar, Bangalore, &amp; Narayanan, 2009) , and most recently, feature -enhanced latent semantic analysis (Di Eugenio, Xie, &amp; Serafin, 2010) , have shown promise . These models may be further improved by leveraging regularities of the dialogue from b oth linguistic and extra -linguistic sources. Users X  expressions of emotion are one such source.
 include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knap p &amp; Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, &amp; Fernandez -Dols, 2003; Schmidt &amp; Cohn, 2001) . While the importance of emotional expressions in dialogue is widely recognized, the majority of di a-logue act classification projects have f ocused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of sp o-ken utterances to aid in online dialogue act class i-fication (Sridhar, Bangalore, &amp; Narayanan, 2009) . Other research on emotion in dialogue has i n-volv ed detecting affect and adapting to it within a dialogue system (Forbes -Riley, Rotaru, Litman, &amp; Tetreault, 2009; L X pez -C X zar, Silovsky, &amp; Griol, 2010) , but this work has not explored leveraging affect information for automatic user dialogue act cla ssification. Outside of dialogue, sentiment ana l-ysis within discourse is an active area of research (L X pez -C X zar et al., 2010) , but it is generally li m-ited to modeling textual features and not mult i-modal expressions of emotion such as facial a c-tions. Such multimodal expressions have only just begun to be explored within corpus -based dialogue re search (Calvo &amp; D'Mello, 2010; Cavicchio, 2009) . logue act classification approach that leverages knowledge of users X  facial expressions during computer -mediated textual human -human di a-logue . Intuitively, the user X  X  affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a que s-tion ). W e focus specifically on occurrences of st u-dents X  confusion -related facial actions during task -oriented tutorial dialogue . work for several reasons. First, confusion is known to be prevalent with in tutoring, and its implications for student learning are thought to run deep (Graesser, Lu, Olde, Cooper -Pye, &amp; Whitten, 2005) . Second, while identifying the  X  X round truth X  of emotion based on any ext ernal display by a user presents challenges , prior research has demonstrated a correlation between particular fac i-al action units and confusion during learning (Craig, D'Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D'Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007) . Finally, automatic fac ial action recognition technolog ies are developing ra p-idly, and confusion -related facial action events are among th ose that can be reliably recognized aut o-matically (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 200 7; Zeng, Pantic, Roisman, &amp; Huang, 2009) . This promising development bodes well for the feasibility of automatic real -time confusion detection within dialogue systems. 2.1 Dialogue Act Classification Because of the importance of di alogue act classif i-cation within dialogue systems , it has been an a c-tive area of research for some time. Early work on automatic dialogue act classification modeled di s-course structure with hidden Markov model s , e x-perimenting with lexical and prosodic feat ures, and applying the dialogue act model as a constraint to aid in automatic speech recognition (Stolcke et al., 2000) . In contrast to this sequential modeling a p-proach , which is best suited to offline processing, recent work has explored how lexical, syn tactic, and prosodic fea tures perform f or online dialogue a ct tagging ( when only partial dialogue sequences are avail able) within a maximum entropy fram e-work (Sridhar, Bangalore, &amp; Narayanan, 2009) . A recently proposed al ternative approach involves t reatin g dialogue utterances as documents within a latent semantic analysis framework, and applying fea ture enhancements that incor porate such info r-mation as speaker and utterance duration (Di Eugenio et al., 2010) . Of the approaches noted above , the modeling fra mework presented in this paper is most similar to the vector -based maximum entropy approach of Sridhar et al. (2009) . Howe v-er, it takes a step be yond the previous work by i n-cluding multimodal affective displays, specifically facial ex pressions, as feature s available to a n a f-fect -enriched dialogue act classification model. 2.2 Detecting Emotions in Dialogue Detecting emotion al states during spoken dialogue is an active area of research, much of which focu s-es on detecting frustration so that a user can be automat ically transf erred to a human dialogue agent (L X pez -C X zar et al., 2010) . R esearch on sp o-ken dialogue has leveraged lexical features along with discourse cues and acoustic information to classify user emotion, sometimes at a coarse grain along a positive/ne gative axis (Lee &amp; Narayanan, 2005) . Recent work on an affective companion agent has examined user emotion classification within conversational speech (Cavazza et al., 2010) . In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communic a-tion for which a deeper understanding of user a f-fect may substantially improve system performance.
 cues, recent work has begun to explore numerous ch annels for affect detection including facial a c-tions, electrocardiograms , skin conductance, and posture sensors (Calvo &amp; D'Mello, 2010) . A recent project in a map task domain investigates some of these sources of af fect data within task -oriented dialogue ( Cavicchio, 2009) . Like that work, the current project utilizes facial action tagging , for which promising automatic technologies exist (Bartlett et al., 2006; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009) . However, we leverage the recogniz ed expressions of emotion for the task of dialogue act classification. 2.3 Categorizing Emotions within Dialogue Sets of emotion taxonomies for discourse and di a-logue are often application -specific, for example, focusing on the frustration of us ers who are inte r-acting with a spoken dialogue system (L X pez -C X zar et al., 2010) , or on uncertainty expressed by students while interacting with a tutor (Forbes -Riley, Rotaru, Litman, &amp; Tetreault, 2007) . In co n-trast, the most widely utilized emotion framew or ks are not application -specific; for exam ple, Ekman X  X  Facial Action Coding System (FACS) has been widely used as a rigorous techni que for coding f a-cial movements based on human facial anatomy (Ekman &amp; Friesen, 1978) . Within this framework, f acial moveme nts are categorized into facial action units, which represent discrete movements of mu s-cle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman X  X  b asic emotion s (Ekman, 1999) have been used in recent work on classifying emotion e x-pressed within blog text (Das &amp; Bandyopadhyay, 2009) , while other recent work (Nguyen, 2010) utilizes Russell X  X  core affect model (Russell, 2003) for a similar task.
 quently experience Ekman X  X  basic emotions of happiness , sadness , anger , fear , surprise , and di s-gust . Instead, students appear to more frequently experience cognitive -affective states such as flow and confusion (Calvo &amp; D'Mello, 2 010) . Our work leverages Ekman X  X  facial tagging scheme to ident i-fy a par ticular facial action unit , Action U nit 4 (AU4), that has been observed to correlate with confusion (Craig, D'Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D'Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007) . 2.4 Importance of Confusion in Tutorial Di a-Among the affective states that students experience during tutorial d ialogue, confusion is prevalent , and its implications for student learning are signi f-icant . Confusio n is associated with cognitive dis e-quilibrium , a state in which students X  existing knowledge is in consistent wi th a novel learning experience (Graesser, Lu, Olde, Cooper -Pye, &amp; Whitten, 2005) . Students may express such conf u-sion within dialogue as uncertai nty , to which h u-man tutors often adapt in a context -dependent fashion (Forbes -Riley et al., 2007) . M oreover, i m-plementing adaptations to student uncertainty wit h-in a dialogue system can improve the effectiveness of the system (Forbes -Riley et al., 2009) . stand ing student utterances is paramount for a sy s-tem to positively impact student learning (Dzikovska, Moore, Steinhauser, &amp; Campbell, 2010) . The importance of frustration as a cogn i-tive -affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act class i-fication of student utterances. This paper explores the use of facial expression features in this way . The corpus was collected during a textual hum an -human tutorial dialogue study in the domain of introductory computer science (Boyer, Phillips, et al., 2010) . Students solved an introductory co m-puter programming problem and carried on textual dialogue wit h tutors, who viewed a synchronized version of the students X  problem -solving wor k-space. The original corpus consists of 48 di a-logues, one per student . Each student interacted with one of two tutors . Facial videos of students were collected using built -in w ebcams , but were not shown to the tutors . Video quality was ranked based on factors such as obscured foreheads due to hats or hair , and improper camera position resul t-ing in students X  faces not being fully captured on the video . The highest -quality set con tained 14 videos, and these videos were used in this analysis. They have a total running time of 11 hours and 55 minutes, and include dialogues with t hree female subjects and eleven male subjects. 3.1 Dialogue act annotation The dialogue act annotation scheme ( Table 1) was applied manually . The kappa statistic for inter -annotator agreement on a 10% subset of the corpus was  X  =0.80, indicating good reliability . 
Table 1 . Dialogue act tags and relative frequencies 3.2 Task act ion annotation The tutoring sessions were task -oriented, focusing on a computer programming exercise. The task had several sub tasks consisting of programming mo d-ules to be implemented by the stu dent . Each of those subtasks also had numerous fine -grained go als, and student task actions either contributed or did not contribute to the goals. Therefore, to obtain a rich representation of the task, a manual annot a-tion along two dimensions was conducted (Boyer, Phillips, et al., 2010) . First, the subtask structur e was annotated hierarchically, and then each task action was labeled for correctness according to the requirements of the assignment. Inter -annotator agreement was computed on 20% of the corpus at the leaves of the subtask tagging scheme, and r e-sulted in a simple kappa of  X  =.56 . However, the leaves of the annotation scheme feature a n implicit ordering (subtasks were completed in order, and adjacent subtasks are semantically more similar than subtasks at a greater distance ); therefore, a weighted kappa is a lso meaningful to consider for this annotation. The weighted kappa is  X  weighted =.80. An annotated excerpt of the corpus is displayed in Table 2 . 
Table 2 . Excerpt from cor pus illustrating annot a-3.3 Lexical and Synt actic Features In addition to the manually annotated dialogue and task features described above, syntactic features of each utterance were automatically extracted using the Stanford Parser ( De Marneffe et al., 2006) . From the phrase structure trees, we ext racted the top -most syntactic node and its first t wo children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Individual word tokens in the utterances were further pr ocessed with the Porter Stemmer ( Porter, 1980) in the NLTK package ( Loper &amp; Bird, 2004) . Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts du r-ing task -oriented tutorial dialogue (Boyer, Ha et al. 2010 ) . A n annotator who was certified in the Facial A c-tion Coding System (FACS) (Ekman, Friesen, &amp; Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test d esigned to an a-lyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman &amp; Rosenberg, 2005) . This annotator viewed the vid e-os continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Bro w Lowerer ) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D'Mello, Witherspoon, Sullins, &amp; Graesser, 2004; D'Mello, Craig, Sullins, &amp; Graesser, 2006; McDaniel et al., 2007) .
 cond FACS -certified annotator independently a n-notated 36% of the vi deo corpus (5 of 14 dialogues), chosen randomly after stratification by gender and tutor. This annotator followed the same method as the first annota tor, pausing the video at any point to tag facial action events. At any given time in the video, the coder was first identifying whether an action unit event existed, and then d e-scribing the facial movements that were present. The annotators also specified the beginning and ending time of each event. In this way, the action unit event tags spanned discrete durations of var y-ing length, as specified by the coders. Because the two coders were not required to tag at the same point in time, but rather were permi tted the fre e-dom to stop the video at any point where they felt a notable facial action event occurred, calculating agreement between annotators required discreti z-ing the continuous facial action time windows across the tutoring sessions . This discretizati on was performed at granularities of 1/4, 1/2, 3/4, and 1 sec ond, and i nter -rater reliability was calculated at each level of granularity ( Table 3 ). Windows in which both annotators agreed that no facial action event was present w ere tagged by default as ne u-tral . Figure 1 illustrates facial expressions that di s-play facial Action U nit 4.

Table 3 . Kappa values for inter -annotator agre e-proaches exist to identifying many facial ac tion units (Bartlett et al., 2006; Cohn, Reed, Ambadar, Xiao, &amp; Moriyama, 2004; Pantic &amp; Bartlett, 2007; Zeng, Pantic, Roisman, &amp; Huang, 2009) , manual annotation was selected for this project for two reasons. First, manual annotation is more robust than au tomatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task -oriented dialogue. Although a detailed discussion of the other em o-tions present in t he corpus is beyond the scope of this paper, Figure 2 illustrates some other spont a-neous student facial expressions that differ from those associated with confusion. 
Figure 2 . Other facial ex pressions from the corpus The goal of the modeling experiment was to d e-termine whether the addition of confusion -related facial expression features significantly boosts di a-logue act classification accuracy for student utte r-ances. 5.1 Features We take a vector -based approach, in which the fe a-tures consist of the following: 5.2 Modeling Approach A logistic regression approach was used to classify the dialogue acts based on the above feature ve c-tors. The Weka machine learning toolkit ( Hall et al., 2009) was used to learn the models and to fir st perform feature select ion in a best -first search. L o-gistic re gression is a generalized maximum likel i-hood model that discriminates between pairs of output values by calculating a feature wei ght ve c-tor over the predictors. confusion -related facial features in the context of particular dialogue act types. For this reason, a specialized classifier was learned by dialogue act. 5.3 Classification Results The classification accuracy and kappa for each specialized classifier is displayed in Table 4 . Note that kappa statistics adjust for the accuracy that would be expected by majority -baseline chance; a kappa statistic of zero indicates that the classifier performed equal to chance, and a positive kappa statistic in dicates that the classifier performed be t-ter than chance. A kappa of 1 constitutes perfect agreement. As the table illustrates, the feature s e-lection chose to utilize the AU4 feature for every dialogue act except S TATEMENT (S). When consi d-ering the accurac y of the model across the ten folds, two of the affect -enriched classifiers exhibi t-ed statistically s ignificantly better performance. (RF) , the facial expres sion features significantly improved the classification accuracy compared to a model that was learned without affective features . Dialogue act classification is an essential task for dialogue systems , and it has been addressed with a variety of modeling approaches and feature sets. We have presented a novel approach that treats facial expressions of students as constraining fe a-tures for an affect -enriched di alogue act classific a-tion model in task -oriented tutorial dialogue. The results suggest that knowledge of the student X  X  confusion -related facial expressions can signif i-cantly enhance dialogue act classification for two types of dialogue acts, G ROUNDING and R EQUEST Table 4 . Classification accuracy and kappa for sp e-cialized DA classifiers. Statistically signifi cant differences (across ten folds, one -tailed t -test) are 6.1 Features Selected for Classifi cation Out of more than 1500 features available during feature selection, each of the specialized dialogue act class ifiers selected between 30 and 5 0 features in each condition (w ith and without affect fe a-tures ) . To gain insight into the specific features that were useful for classifying these dialogue acts, it is useful to examine which of the AU4 history features were chosen during feature selection. presence of absence of AU4 in the immediately preceding utterance, either at the 1 second or 5 s e-cond granularity, were selected. Absence of this confusion -related facial action unit was associated with a higher probability of a grounding ac t, such as an acknowledgement. This finding is consistent with our understanding of how students and tutors interacted in this corpus; when a student exper i-enced confusion, she would be unlikely to then make a simple grounding dialogue move, but i n-stead wo uld tend to inspect her computer program, ask a question, or wait for the tutor to explain more.
 features were presence or absence of AU4 within ten seconds of the longest available history (three turn s in the past) , as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified). This finding suggests that there may be some lag between the student experiencing confusion and then choosing to make a re quest for feed back, and that the confusion -related facial expressions may re -emerge as the student is making a request for feedback, since the five -second window prior to the student sending the textual dialogue message would overlap with the student X  X  con struction of the message itself. tures for Q UESTION , P OSITIVE F EEDBACK , and E
XTRA -D OMAIN acts w ere not statistically reliable, examin ing the AU4 features that were selecte d for classifying these moves points to ward ways in which facial expressions may influence classific a-tion of these acts (Table 5) .
Table 5 . Number of features, and AU4 features 6.2 Implications The results pres ented here demonstrate that leve r-aging knowledge of user affect, in particular of spontaneous facial expressions, may improve the performance of dialogue act classification models. Perhaps most interestingly, displays of confusion -related facial actions pr ior to a student dialogue move enabled a n affect -enriched classifier to re c-ognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008) . Requesting feedback also see ms to be an important behavior of students , characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, &amp; Lester, 2007) . 6.3 Limitations The experiments reported here have several not a-ble limitations. First, the time -consuming nature of manual facial action tagging restr icted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been su f-ficiently clear to permit tagging, which would have increased the sample size and likely re vealed s t a-ti s tically significant trends. For example, the pe r-formance of the affect -enriched classifier was be t-ter for dialogue acts of interest such as positive feedback and questions, but this difference w as not statistically reliable. fundamental question of which affective states are indicated b y particular external displays. The field is only just beginning to understand facial expre s-sions during learning and to correlate the se facial actions with emotions . Additional research into the  X  X round truth X  of emotion expression will shed additional light on this area. Finally, the results of manual facial action annotation m a y constitute u p-per -bound findings for applying automatic facial expression analysis to dia logue act classification. Emotion play s a vital role in human interactions . In particular , the role of facial expressions in human -human dialogue is widely recognized . Facial e x-pressions offer a promising channel for unde r-stan d i ng the emotions experienced by users of dialogue systems, particularly given the ubiquity of webcam technologies and the increasing number of dialogue systems that are deployed on webcam -enabled devices. This paper has reported on a first step toward using knowledge of user facial expre s-sions to improve a dia logue act classificati on mo d-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for di s-tinguishing the pedagogically relevant dialogue act R
EQUEST FOR F EEDBA CK , and the conversational moves of G ROUNDING .
 of future work in this area . Dialogue act classific a-tion models have not fully leveraged some of the techniques emerging from work on sentiment ana l-ysis. These app roaches may prove particularly us e-ful for identifying emotions in dialogue utterances. Another important direction for future work i n-volves more fully exploring the ways in which a f-fect expression differs between textual and spoken dialogue. Finally, as au tomatic facial tagging tec h-nologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future. Acknowledgments This wor k is supported in part by the North Carol i-na S tate University Department of Computer Sc i-ence and by the National Science Foundation through Grants REC -0632450, IIS -0812291, DRL -1007962 and the STARS Alliance Grant CNS -0739216. Any opinions, findings, conclusions, or recommendations expressed in this r eport are those of the participants, and do not necessarily represent the official views, opinions, or policy of the N a-tional Science Foundation.
 References
