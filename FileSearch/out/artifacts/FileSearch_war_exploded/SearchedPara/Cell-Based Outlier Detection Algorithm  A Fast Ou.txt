 is an outstanding data mining task, methods to find outliers can be classified into four neighbor. Bay [11] found that the nested loops algorithm in conjunction with randomi-data. To find outliers efficiently, we can do compression on the initial dataset; focus on the minority of dataset. This can avoid large computations. 
Following this idea, we developed our CEBOD algorithm to detect outlier from large datasets. The outliers we found are density based, because calculations to detect neighbors very fast. 
Data space is divided into cells, and the numb er of data locating in a cell is used to High-Density Cell; otherwise Low-Density Cell. Then we filter the high density data cuses on the rest data which will be very smaller than the original one. Some definitions should be given, which can assure the accuracy of result. Definition 1. temporal-Outlier (t-Outlier) number of objects to contained in C i Definition 2. non-Outlier (n-Outlier) Outliers together with all n-Outliers are the whole dataset. Definition 3. real-Outlier (r-Outlier)  X  factors are bigger than LOF. Definition 4. relative-non-Outlier (r-n-Outlier) tionship with t-Outliers. Definition 5. absolute-non-Outlier (a-n-Outlier) Let any object p in dataset D, O be the unit of all n-outliers and p be from the set T, scan. All r-n-Outliers combined with all a-n-Outliers are the whole n-Outliers. Definition 6. remain-Dataset(r-Dataset) removed from, after the first data scan. We use Fig 1 to illustrate the definitions above. 188 points in Fig 1 are partitioned by cells. The density threshold k equal to 5, cells have more then 5 points are identified as high density ones, otherwise low density cells. Then, we can divide the dataset into are a-n-Outliers. They are surrounded by high density cells, and have no relationship are r-n-Outliers. Cells around them contain low density cells. 61 data in the low den-whole dataset, later experiments show the compression of data is always around one third. 3.1 Algorithm Description cells. The algorithm constructed two arraylists of arr_pt and arr_cell, respectively for attributes for example, a Cell X  X  length, row_cnt and col_cnt can be calculated as blow: 
The domain's D_length = max_x -min_x, D_width = max_y -miny;  X  D_Area = D_length * D_width. The cell's total number m = [N/k]+1;  X 
Cell's length = square(D_Area/m),  X 
Cell's row_cnt = (height/Cell.length)+2,  X  Cell's col_cnt = (width/Cell.length)+2. 
The algorithm receives as input a dataset DB of N points (with two dimensional at-tributes for easy explanation), the domains of each attributes in (min_x, miny, max_x, max_y), the number b_size represents the size of data loaded into memory one time, the number k of neighbors to consider. And the algorithm contains three major parts shown in Fig 3: (1) Loading each data into cells by blocks, (2) Using cells to filter the dataset and mark each t-Outlier, (3) Calculate each t-Outlier's local outlier factor. BOD, different from LOF, objects identified as a-n-Outliers are deleted before calcu-the local outlier factor of the t-Outliers nearby can' not be calculated. To make com-pensation, we initialize each r-n-Outlier's local reachability density to 0. 3.2 Complexity Analysis The time complexity of CEBOD is O(N), which is the same as LOF when it uses cell structure. But as the calculation in CEBOD only concerned to about one third of the dataset, so its efficiency is better than LOF several times. 
Because the expensive system I/O costs, we must minimize the number of pass costs. In CEBOD algorithm, if cell space is divided into m parts, the number of data-example, m=3, then the pass over number is two. 
The page exchange operations are not exist in CEBOD algorithm, because each time when doing computations, relative data are all exist in the memory. IV 2.66GHz-based machine with 512MB main memory. 4.1 Comparison of Accuracy with LOF Algorithm We use a synthetic dataset which has 1600 two dimensional tuples to test CEBOD X  X  accuracy. The distribution of this dataset is shown in Table 1 (in the second column), purpose is to find the top n% outliers in the dataset. different density areas. While, CEBOD is more apt to find outliers locate in the edge ing, and its results show a more clear deviation of four clusters. 4.2 Influence of k Value and Data Size on Algorithm Performance density areas needs more calculation, more comparison and more sorting operations, especially when k is larger, as LOF shows in Figure 2. Second reason is the cells we divide the datasets. As the increasing of k, cell X  X  number is decreasing. This decreas-when calculating k-nn queries. give the LOF X  X  time cost for large dataset, because in these experiments, the cost time of LOF algorithm is up to thousand second. results prove CEBOD has high accuracy in finding outliers, and can be used for large dataset X  X  outlier detection efficiently. 
In ongoing work, we will extend CEBOD to distributed dataset. As far as we also make comparison between distance based and density based methods. 
