 A corpus is a collection of text documents in readable or audible formats which are digitized such that they can be processed on computers (Wynne 2005). A readable corpus is digitized in written formats whilst an audible corpus is implemented in different audio/sound formats. Both types exemplify patterns of a natural language that can be used for different studies and applications. Arabic books and millions of Arabic pages on the World Wide Web. The lack of freely available Arabic corpora is the motivation behind this work. Although it is extremely difficult to place limits on a natural language, neither on its number of vocabulary nor representative as possible. Thus, we aim to support Arabic natural language processing (ANLP), language modelling, and information retrieval (IR) research by developing a method for collecting recent and ongoing documents from the Internet, and by building a text corpus for Arabic news. The major difference between ArNeCo and the previous Arabic corpora such as Arabic Gigaword (Graff 2007) and Arabic Newswire (Graff and Walker 2001) provided by Linguistics Data Consortium (LDC) is that our corpus is considered contemporary and freely accessed on the web. Current resources for Arabic corpora are few, outdated, very regional, and most of them are not available for free. Many researchers working on ANLP constructed their own datasets for their research purposes. For instance, Alzahrani and Salim (2009) constructed a dataset of 125 documents collected manually from Wikipedia for the purpose of gauging similarity and plagiarism detection in Arabic, Hmeidi et al. (1997) built a corpus of 242 abstracts collected from Saudi national conference proceedings for automatic indexing. However, such datasets are not considered a representative sample of the language due to the small amount of documents and language structures. On the other hand, Goweder and De Roeck (2001) constructed a corpus of 42591 news articles but it is very regional because it was collected from a single source. 
Few other corpora which are not available for free to the public have been reviewed popular of these include Arabic Gigaword (Graff 2007) and Arabic Newswire (Graff and Walker 2001) provided by Linguistics Data Consortium (LDC), Al-Hayat Arabic Corpus and An-Nahar Newspaper Text Corpus from European Language Resources Association (ELRA). Recently-built corpora include modern standard Arabic corpus (Abdelali et al. 2005), contemporary corpus of Arabic (Al-Sulaiti and Atwell 2006), and international corpus of Arabic (Alansary et al. 2007). 
To enrich the resources, and overcome the limitations of current Arabic corpora, we present a new well-representative corpus for Arabic. In addition, we deploy a novel method of developing corpora that can keep track of recent natural language texts posted on the Internet by using RSS feeds. 3.1 Google News: A Rich Resource To develop an Arabic news text collection from a rich resource, we used the Google News 1 in the Arabic world as a collector of news articles and stories from hundreds of sources. Google News service groups similar articles automatically and arranges them awareness of modern Arabic structure and vocabulary. A snapshot of Arabic Google news dynamic homepage is shown in Figure 1. 
As can be seen in the above figure, (1) refers to Arabic news service, (2) refers to the menu that displays the news X  categories as follows: world, Arab world, business, science &amp; technology, entertainment, sports and most popular, (3) indicates the article link forms a rich resource of similar language patterns, structures relations, and vocabulary connections stemmed from different writers in different locations. This is a same patterns in a language. 
In order to build a large and a representative reference corpus for Arabic, we defined well-selected criteria and adopted a structured method for corpus development. Section 3.2 discusses the criteria defined whereas section 3.3 explains the corpus builder method. 3.2 Criteria Good selection of the corpus texts is coupled with good selection criteria. As Wynne number, clearly separate from each other, and efficient as a group in delineating a corpus that is representative of the language under examination X . 
There are six common criteria: text mode, type, domain, language or language develop the ArNeCo corpus. 
Google provides news articles not only from electronic newspapers but also from news channels that transcribe the news on their web pages. Table 2 lists some locations for many newspapers sources as Google collected from the web, and Table 3 shows the locations of some Arabic news channels. Criteria Value Mode Electronic/digitized readable mode in UTF-8 encoding Type News articles Domain Popular news categorized in 6 domains: World, Sports, Language Modern Standard Arabic Language (without diacritics) Location Numerous Arabic news sources. See Table II and Table III. Date January-August, 2012 Location Sources Global Newspapers Saudi Arabia Syria Al-Furat, Al-Jamahir, Al-Ouruba, Al-Thawra, Tishreen, Al-Wehda, Tunisia Al-Chourouk, Al-Horria, as-Sabah UAE Akhbar Al-Arab, Al-Bayan, Al-Ittihad, Al-Khaleej, Emirates Today, Yemen Al-Ayyam, Al-Gumhuriyah, Al-Mithaq, Al-Motamar, Naba Al-Haqiqa, Morocco Al-Alam, Attajdid, Al-Ayam, Bayane Al-Yaoume, as-Sabah Oman Al-Watan, Oman Observer, Times of Oman Palestine Al-Ayyam, Filasteen Al-Muslimah, Al-Hayat Al-Jadida, Al-Karmel, Qatar Al-Rayah, Al-Sharq, Al-Watan, Gulf Times, The Peninsula Sudan Al-Rayaam , Adaraweesh Bahrain Akhbar Alkhaleej, Alayam, Al-Wasat, Bahrain Tribune, Gulf Daily News Egypt Al-Ahram, Aqidati, Al-Gomhuria, Mayo, Al-Messa, Al-Osboa, Al-Iraq Azzaman, Alahali, Al-Jihad, Al-Mendhar, Nahrain, Al-Rafidayn, Al-Jordan Al-Arab Al-Yawm, Ad-Dustour , Al-Ghad, Al-Ra'i, As-Sabeel, Irbid Kuwait Al-Qabas , Al-Rai-Al-Aam, Taleea, Al-Watan Lebanon Al-Aman, Al-Anwar, Al-Balad, Al-Intiqad, Al-Kalima, Al-Kifah Al-
Libya Al-Fajr Al-Jadid, Al-Jamahiriyah, Al-Shams, Al-Zahf Al-Akhdar 3.3 Google News RSS Feeds: An Overview According to Wikipedia, RSS is a family of web feeds formats which can be used to publish continuous updated work such as news and blog entries. Any RSS document contains some metadata such as the source URL, title, publisher, date, full text or summary, etc. that are usually specified in XML format to facilitate the use of data as elements with start-tag and end-tag. Thus, using RSS feeds is potentially valuable for parsing the XML elements into valuable news items. A sample of RSS source document taken from Arabic Google news webpage is shown in Figure 2. which is shown in Figure 3. The &lt;item&gt; element describes each news article by giving &lt;description&gt;, where they can be extracted and used to build our corpus collection and its metadata. 3.4 ArNeCo Builder: A Detailed Method After investigating the RSS files, we developed a method for collecting Arabic news documents using systematic and sequential steps shown in Figure 4. Arabic Google news web page. Two methods were used in this stage, latest-news and searched-news. In the latest-news method, the corpus builder was run to navigate to the URLs of all related articles from Google News for every article appears in the main page (see number 4 and 6 in Figure 1). Each Google News page has an RSS feeds file which could be downloaded and saved as xml file under a specific domain as specified by our criteria (see Table 1). On the other hand, in the searched-news method we aimed to search different keywords under each domain. For example, the Arabic keyword  X   X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X   X  which means  X  X 1N1 Flow X  was searched in Google News and the results page that contains the RSS feeds for retrieved documents were downloaded and categorized under the health domain. the XML elements. The most valuable element is &lt;item&gt; which is shown previously in &lt;description&gt;. Then in the third step, the value of the &lt;link&gt; tag was passed to a web crawler built in PHP 2 . The crawler visits the URL and extracts the content of the webpage but keeps the encoding as it is. The crawler was run for each domain for several hours, and the collected news articles were downloaded and stored under each domain separately. To unify the encoding of the collected webpages, another method namely encoding fixer in our ArNeCo corpus builder was used (i) to detect the document encoding and (ii) to convert the Windows-1562 and other encodings to UTF-8 encoding. The last step was utilised to cleanse the collected documents and strip HTML tags, diacritics, symbols, and English/Latin words and characters. For this purpose, a document cleanser method was employed to refine documents and obtain just-Arabic texts. The number of RSS feeds files and collected articles along with the construction time for each domain are shown in Table 4. Domain No of World 152 3621 6 hours 5 min Business 33 280 1  X  hour 30 sec Entertainment 35 865 2 hours 1 min Health 80 2379 4 hours 2 min Sci-Tech 29 543 44 min 1 min Sports 167 3377 5 hours 3 min 
Total 496 11065  X  20 hours  X  12 min The purpose of profiling ArNeCo using metadata is to add more value to the corpus be defined (Wynne 2005). The corpus metadata may include the name of corpus, producer, agency responsible for the intellectual content of the corpus, contact details, others. ArNeCo metadata sample file is shown in Figure 5. On the other hand, one metadata file was created for each category/domain including ID, name, size and last title, link and publication date in the category as presented in Figure 6. The aim of constructing ArNeCo is to prov ide a freestanding representative corpus of the modern Arabic language. The corpus cons ists of news texts collected by crawling the web from hundreds of sources and locations addressed by Google news RSS feeds. Accordingly, the developed corpus may constitute well-representative resource for Arabic language engineering. To justify this claim, firstly, ArNeCo has thousands of articles distributed over six different domains: Business, Entertainment, Health, Science &amp;Technology, Sports and World. Secondly, there are, in each domain, similar news articles about the same subject written by different authors/news agencies. Such similar documents enrich the vocabulary usage besides the language structures, varieties and dialects. Thirdly, ArNeCo has a considerable size, digitized format, and unified encoding which allows us to investigate how well is representative. Table 5 gives a statistical summary of the corpus for being analysed and assessed. studied the contribution of the document in each category to the corpus. We used the documents from two different domains:  X  X usiness X  as it has the smallest number of documents versus  X  X orld X  as it has the largest. 
As can be seen in Table 6, the number of statements (Stmt#), words (W#) and distinct words (#DW) is increased as more articles being added to the dataset. Figure 7 seen that when the number of documents increased, new distinct words and vocabulary appears. 
Article# Stmt # W# DW # Article# Stmt # W# DW # 
To assess the ArNeCo corpus, Zipf X  X  law was investigated (Goweder and De Roeck 2001; Sarkar, De Roeck et al. 2004; Abdelali et al. 2005; Alotaiby et al. 2009). Zipf's law means that  X  X iven some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table X . Therefore, it is a useful indicator for gauging the data sparseness and providing the evidence of any imbalance in the corpus. 
According to Zipf X  X  law, the relationship between the frequency of the words in the position in the list, is a constant k . The relationship appears as straight line with slope -1 in the ideal state. For this purpose, we developed a MATLAB function that reads the words from plain text documents in the corpus and displays the frequency of the words which were used at least twice. We investigated the Zipf X  X  distribution on each domain as a separate dataset, then on the whole corpus. The results were displayed in descending order so that the most frequent word has rank number one, second most frequent word has rank number two and so on. Figure 8 plots the ranks versus ideal state because this domain has the smallest number of documents but with relatively big number of redundant words appeared more than twice. 
However, as the number of documents in the corpus increased, the Zipf X  X  distribution becomes better. Figure 9 shows ranks versus frequencies for the list of Arabic words in the whole corpus shown in logarithmic scale for both axes. The curve is near to the ideal situation (straight line with slope -1). This indicator shows that the ArNeCo is representative and imbalanced. ArNeCo corpus is not an ultimate goal by itself but it is a step that supports the future research on ANLP. Therefore, the corpus is currently published to the research community and freely available online for download on the following link: http://www.c2learn.com/anlp/corpus.php . The limitations of existing Arabic corpora necessitate the development of a new corpus for modern Arabic language. ArNeCo presents a representative sample for Arabic wherein thousands of news articles have been tagged under six categories to cover more language varieties and vocabulary. The corpus consists of more than 11,000 documents with more than 6 million words which has been already published and archived online. More importantly, the use of RSS feeds in our corpus builder provides a different method for constructing corpora which keeps track of recent natural language texts posted on the Internet. That is, the present corpus may be enlarged, modified, and updated easily. The method for corpus builder can be further used to construct corpora for other languages using Google RSS feeds. However, publishing the current ArNeCo with such representative size is not an end. The use of RSS feeds from dynamic web content, such as news and blogs, facilitates increasing the number of documents and allows automatic derivation and generation of the corpus metadata. 
