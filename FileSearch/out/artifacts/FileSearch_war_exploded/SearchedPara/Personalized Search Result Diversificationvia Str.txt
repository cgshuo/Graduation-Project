 This paper is concerned with the problem of personalized diver-sification of search results, with the goal of enhancing the perfor-mance of both plain diversification and plain personalization al-gorithms. In previous work, the problem has mainly been tack-led by means of unsupervised learning. To further enhance the performance, we propose a supervised learning strategy. Specif-ically, we set up a structured learning framework for conducting supervised personalized diversification, in which we add features extracted directly from the tokens of documents and those utilized by unsupervised personalized diversification algorithms, and, im-portantly, those generated from our proposed user-interest latent Dirichlet topic model. Based on our proposed topic model whether a document can cater to a user X  X  interest can be estimated in our learning strategy. We also define two constraints in our structured learning framework to ensure that search results are both diversi-fied and consistent with a user X  X  interest. We conduct experiments on an open personalized diversification dataset and find that our su-pervised learning strategy outperforms unsupervised personalized diversification methods as well as other plain personalization and plain diversification methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Personalization; diversity; structured SVMs; ad hoc retrieval
Search result diversification has recently gained attention as a method to tackle query ambiguity. In search result diversification one typically considers the relevance of a document in light of the other retrieved documents. The goal is to identify the probable  X  X s-pects X  of the ambiguous query, retrieve documents for each of these aspects and make the search results more diverse [13]. By doing so, in the absence of any knowledge of users X  context or preferences, the chance that any user issuing an ambiguous query will find at least one of these results to be relevant to the underlying informa-tion need is maximized [10].

In both search result diversification and personalized web search, an issued query is often viewed as an incomplete expression of a user X  X  underlying need [22]. Unlike search result diversification, where the system accepts and adapts its behavior to a situation of uncertainty, personalized web search strives to change this situa-tion by enhancing the system X  X  knowledge about users X  information needs. Rather than aiming to satisfy as many users as possible, per-sonalization aims to build a sense of who the user is, and maximize the satisfaction of a specific user [26].

Although different, diversification and personalization are not in-compatible and do not have mutually exclusive goals [23]. Search results generated by diversification techniques should be more di-verse when a user X  X  preferences are unrelated to the query. Like-wise, personalization can improve the effectiveness of aspect weight-ing in diversification, by favoring query interpretations which are predicted to be more related to each specific user [26].
In this paper we study the problem of personalized diversifica-tion of search results , with the goal of enhancing both diversifica-tion and personalization performances. The problem has previously been investigated by Radlinski and Dumais [19] and Vallet and Castells [26]. They have presented a number of effective unsuper-vised learning approaches that combine both personalization and diversification components to tackle the problem. To further im-prove the performance we propose a supervised learning approach.
There are a couple of advantages to considering a supervised learning approach. Such approaches can leverage useful informa-tion underlying labeled training data, apply existing optimization techniques to the problem and are easier to adaptation. Of course, they also have disadvantages, one of which is that it is expensive to create training data for supervised learning methods. This is, however, a shortcoming for any supervised learning strategy and we leave it as future work.

Accordingly, we formulate the task of personalized search result diversification as a problem of predicting a diverse set of docu-ments given a specific user and a query. Specifically, we formu-late a discriminant based on maximizing search result diversifica-tion, and perform training using the well-known structured sup-port vector machines (SSVMs) framework [25]. The main idea is first to propose a user-interest LDA-style [5, Latent Dirichlet Al-location] topic model, from which we can infer a per-document multinomial distribution over topics and determine whether a doc-ument can cater for a specific user. Then, during training we use features extracted directly from the tokens X  statistical information in the documents and those utilized by unsupervised personalized diversification algorithms, and, more importantly, those generated from our proposed topic model. Additionally, two types of con-straint in SSVMs are explicitly defined to enforce the search results to be both diverse and relevant to a user X  X  personal interest.
We evaluate our proposed approach on a publicly available per-sonalized diversification dataset and compare it to unsupervised approaches, that focus on either personalization or diversification alone, to combined approaches like those in [19] and [26], and to two standard structured learning approaches [32, 33]. The three main contributions of our work are:(1) We tackle the problem of personalized diversification of search results differently, using a su-pervised learning method. (2) We propose a user-interest latent topic model to capture a user X  X  interest and infer per-document multinomial distributions over topics. (3) We explicitly enforce diversity and personalization through two types of constraints in structured learning for personalized diversification.
Three major types of research relate to our work: personalized diversification, structured learning, and topic modeling.
Two main components, viz., personalized web search and search result diversification, play important roles in tackling the problem of personalized search result diversification. The task of personal-ized web search aims at identifying the most relevant search results for an individual by leveraging their information. Many personal-ized web search methods have been proposed, such as the one based on social tagging profiles [27], ranking model adaption for person-alized search [29], search personalization by modeling the impact of users X  behavior [4], and personalized search using interaction behaviors in search sessions [17]. In contrast, diversification aims to make the search results diversified given an ambiguous query so that users can find at least one of these results to be relevant to their underlying information need [2]. Well-known diversification methods include the maximal marginal relevance model [8], proba-bilistic model [9], subtopic retrieval model [35], xQuAD [21], Rx-QuAD [28], IA-select [2], PM-2 [12], and more recently, matroid constraints [1], DSPApprox [13], text-based measures [3], term-level [13], and fusion-based [16]. All of the above methods focus on either personalization or diversification only.

To the best of our knowledge, only Radlinski and Dumais [19] and Vallet and Castells [26] have studied the problem of combin-ing both personalization and diversification. Radlinski and Dumais [19] analyze a large sample of individual users X  query logs from a web search engine such that individual users X  query reformulations can be obtained. Then they personalize web search by reranking some top results using query reformulations to introduce diversity into those results. Their evaluation suggests that using diversifica-tion is a promising method to improve personalized reranking of search results. Vallet and Castells [26] present a number of ap-proaches that combine personalization and diversification compo-nents. They investigate the introduction of the user as an explicit variable in state-of-the-art diversification models. Their person-alized search result diversification algorithms achieve competitive performance and improve over plain personalization and plain di-versification baselines.

All of the previous personalized diversification models are un-supervised. However, we argue that to enhance the performance, it is better to employ a supervised learning approach, and our ex-periments show that supervised learning can indeed improve the performance of unsupervised approaches. To the best of our knowl-edge, this is the first attempt to tackle the problem of personalized diversification using supervised learning methods.
Structured learning has provided principled techniques for learn-ing structured-output models, with the structured support vector machines (SSVMs) being one of the most important ones [25]. In structured learning, a set of training pairs, { ( x , y ) | x  X  X  , y  X  X } , is assumed to be available to the learning algorithm, and the goal is to learning a mapping f : X  X  Y from the input space X to the output space Y , such that a regularized task-dependent loss function  X  : Y  X  Y  X  R + can be minimized, where  X ( y ,  X y ) denotes the cost of predicting output  X y when the correct predic-tion is y . In the past few years, Structured SVMs (SSVMs) have been studied and applied in many areas, such as speech recogni-tion [36], optimizing average precision of a ranking [33], and di-versification [32]. For us, the most interesting prior application of SSVMs is the one for predicting diverse subsets [32]. How-ever, our personalized search result diversification method differs from that proposed in [32]: we work on personalized diversifica-tion where we propose a user-interest LDA-style model to capture a user X  X  interest distribution over topics, whereas they directly ap-ply existing SSVMs algorithm to tackle the problem of search re-sult diversification but not personalized diversification; our model explicitly makes results diverse and consistent to the user X  X  interest by enforcing both diversity and interest constraints, whereas their model only implicitly diversifies the results by adopting standard SSVMs. Prior work on diversification [12, 21, 28], however, has shown that explicit approaches outperform implicit ones in most cases. To the best of our knowledge, this is the first attempt to explicitly enforce diversity and personalization through additional constraints in SSVMs.
Topic modeling provides a suite of algorithms to discover hid-den thematic structure in a collection of documents. A topic model takes a collection of documents as input, and discovers a set of  X  X a-tent topics X  X  X ecurring themes that are discussed in the collection X  and the degree to which each document exhibits those topics [5]. Latent dirichlet allocation (LDA) [5] is one of the simplest topic models, and it decomposes a collection of documents into topics X  biased probability distributions over terms X  X nd represents each document with a subset of these topics. Many LDA-style models have been proposed, such as the syntactic topic model [6], mul-tilingual topic model [7], topic over time model [30], and more recently, the max-margin model [37], spatio-temporal model [31], fusion-based model [16] and multi-contextual model [24]. We pro-pose a user-interest LDA-style model to capture a multinomial dis-tribution of topics specific to a user. From our model, we infer a per-document multinomial distribution over the topics so that we can easily identify whether a document caters to a user X  X  interest. Our experimental results demonstrate that the model can help to enhance the performance of personalized search result diversifica-tion. To the best of our knowledge, this is the first time that a topic model is utilized to enhance the performance of personalized diver-sification.
Let u = { d 1 ,...,d | u | }  X  U be a set of documents of size | u | which a user u is interested in. For each query q , we as-sume that we are given u and a set of candidate documents x = { x 1 ,...,x | x | }  X  X , where X denotes the set of all possible doc-ument sets. Our task is to select a subset y  X  Y of K documents from x that maximizes the performance of personalized search re-sult diversification given q and u , where we let Y denote the space of predicted subsets y . Following the standard machine learn-ing setup, we formulate our task as learning a hypothesis function h : X  X U  X  Y to predict a y given x and u . To this end, we assume that a set of labeled training data is available: where y ( i ) is the ground-truth subset of K documents from x and u ( i ) is the set of documents that user u i is interested in, and N is the size of the training data. We aim to find a function h such that the empirical risk R  X  S ( h ) = 1 N P N i =1  X ( y ( i ) can be minimized, where we quantify the quality of a prediction by considering a loss function  X  : Y  X Y  X  R + that measures the penalty of choosing  X y = h ( x ( i ) , u ( i ) ) . Here, given the ground-truth y , viz., the ground truth ranking of relevant documents, and the predicting  X y , viz., the ranking of predicted documents, we de-fine the loss function based on a diversity metric,  X  -nDCG [10] (other diversity metrics are possible but we obtain the best perfor-mance when adopting this metric), as: We focus on hypothesis functions which are parameterized by a weight vector w , and thus wish to find w to minimize the risk, R S ( w )  X  R  X  S ( h (  X  ; w )) . We let a discriminant F : X X U X Y  X  R + compute how well the predicting  X y fits for x and u . Then the hypothesis predicts the  X y that maximizes F : We describe each ( x , u , y ) through a feature vector  X ( x , u , y ) ; the extraction will be discussed later. The discriminant function F ( x , u , y ) is assumed to be linear in the feature vector  X ( x , u , y ) such that: where w is a weight vector to be learned from training data.
In this section, we introduce the standard SSVMs learning prob-lem, propose constraints for personalized diversification and de-scribe our optimization problem and the way we make predictions.
Our personalized diversification model builds on an existing stan-dard structured learning framework. In our setting, the standard structured learning framework can be described as: given a train-ing set { ( x ( i ) , u ( i ) , y ( i ) )  X  X  X U X Y : i = 1 ,...,N } , structured SVMs are employed to learn a weight vector w for the discriminant function F ( x , u , y ) through the following quadratic programing problem: Optimization Problem 1. (Standard structured SVMs) subject to  X  i,  X  y  X  X \ y ( i ) , X  i  X  0 , w In the objective function (4), the parameter C is a tradeoff between model complexity, || w || 2 , and a hinge loss relaxation of the training loss for each training example, P  X  i . The constraints enforce the requirement that the ground-truth personalized diversity document set y ( i ) should have a greater function value than other alternative y  X  X  , and y 6 = y ( i ) .
As discussed above, we aim at training a personalized diversifi-cation model that can enforce both diversity and consistency with the user X  X  interest. This can be achieved by introducing additional constraints to the structured SVM optimization problem defined in (4). To start, diversity requires a set of retrieved documents that should not discuss the same aspects of an ambiguous query. In other words, aspects of documents returned by a diversification model should have little overlap with one another. Formally, we enforce diversity with the following constraint.
 Constraint for diversity: In (5), the sum of each document X  X  score, P w T  X ( x ( i ) should not be greater than the overall score when they are con-sidered as an ideal ranking of the document sets. As a result, commonly shared features will be associated with relatively low weights, and a document set with less redundancy will be predicted.
Additionally, personalization requires a set of returned docu-ments to match the user X  X  personal interest. Formally, we enforce personalization with the following constraint.
 Constraint for consistency with user X  X  interest: where sim( y , u ( i ) )  X  [0 , 1] is a function (see (14)) that measures subtopic distribution similarity between a set of documents y and the documents user u i is interested in, i.e., u ( i ) ,  X  is a slack variable that tends to give slightly better performance, which can be defined
In (6), (1  X  sim( y , u ( i ) )) quantifies how well a set of docu-ments matches a user X  X  interest. If the topics discussed in a set of documents y are not consistent with a user X  X  personal interest, w
T  X ( x , u , y ) will result in a relatively low score. During predic-tion, documents consistent with a user X  X  interest will be preferred.
A set of documents produced in response to an ambiguous query should be diverse and consistent to the user X  X  personal interest. To this end we integrate the proposed additional constraints with stan-dard structured SVMs. We propose to train a personalized diversi-fication model by tackling the following optimization problem: Optimization Problem 2. (Structured SVMs for personalized di-versification) subject to  X  i,  X  y  X  X \ y ( i ) , X  i  X  0 , Algorithm 1: Cutting plane algorithm Algorithm 2: Greedy subset selection for prediction
We can solve the optimization problem defined in (7) by em-ploying the cutting plane algorithm [25]. The learning algorithm is shown in Algorithm 1. The algorithm iteratively adds constraints until we have solved the original problem within a desired toler-ance . It starts with empty working sets W i , W 0 i and W i = 1 ,...,N . Then it iteratively finds the most violated constraints straints (i), (ii) and (iii) in (7), respectively. If they are violated by more than , we add them into the corresponding working sets. We iteratively update w by optimizing (7) over the updated working sets. The outer loop in Algorithm 1 can halt within a polynomial number of iterations for any desired precision ; see [25].
After w has been learned, given an ambiguous query, a set of candidate documents x , and a set of documents u the user u is interested in, we try to predict a set of documents  X y by tackling the following prediction problem: This is a special case of the Budgeted Max Coverage problem [15], and can be efficiently solved by Algorithm 2.
In this section, we first review the notation and terminology used in our user-interest topic model, and then describe the model and the features used in our structured learning framework.
 Table 1: Main notation used in user-interest topic model.
We summarize the main notation used in our user-interest topic model (UIT) in Table 1. We distinguish between queries, aspects and topics. A query is a user X  X  expression of an information need. An aspect (sometimes called subtopic at the TREC Web tracks [11]) is an interpretation of an information need. We use topic to refer to latent topics as identified by a topic modeling method (LDA).
To capture per-user and per-document multinomial distributions over topics such that we can measure whether a document can cater for the user X  X  interest, we propose a user-interest latent topic model (UIT). Topic discovery in UIT is influenced not only by token co-occurrences, but also by the relevance scores of documents evalu-ated by users. In our UIT model, we use a Beta distribution over a (normalized) document relevance span covering all the data, and thus various skewed shapes of rising and falling topic prominence can be flexibly represented.

The latent topic model used in UIT is a generative model of rel-evance and tokens in the documents. The generative process used in Gibbs sampling [18] for its parameter estimation, is as follows: i. Draw T multinomials  X  z from a Dirichlet prior  X  , one for each ii. For each user u , draw a multinomial  X  u from a Dirichlet prior Fig. 1 shows a graphical representation of our model. In the gener-ative process, the relevance scores of tokens observed in the same document are the same and evaluated by a user, although a rel-evance score is generated for each token from the Beta distribu-tion. In our experiments, there is a fixed number of latent topics, T , although a non-parametric Bayes version of UIT that automati-cally integrates over the number of topics would certainly be possi-ble. The posterior distribution of topics depends on the information from two modalities: tokens and the documents X  relevance scores.
Inference is intractable in this model. Following [6, 7, 14, 18, 30], we employ Gibbs sampling to perform approximate inference. We adopt a conjugate prior (Dirichlet) for the multinomial distri-butions, and thus we can easily integrate out  X  and  X  , analytically capturing the uncertainty associated with them. In this way we Figure 1: Graphical representation of user-interest topic model. facilitate the sampling, i.e., we need not sample  X  and  X  at all. Be-cause we use the continuous Beta distribution rather than discretiz-ing document relevance scores, sparsity is not a big concern in fit-ting the model. For simplicity and speed we estimate these Beta distributions ( b z 1 ,b z 2 ) by the method of moments, once per itera-tion of Gibbs sampling. We find that the sensitivity of the hyper-parameters  X  and  X  is not very strong. Thus, for simplicity, we use fixed symmetric Dirichlet distributions (  X  = 50 /T and  X  = 0 . 1 ) in all our experiments.

In the Gibbs sampling procedure above, we need to calculate the conditional distribution P ( z di | e w , r , z  X  di , z  X  di represents the topic assignments for all tokens except w begin with the joint probability of a dataset, and using the chain rule, we can obtain the conditional probability conveniently as: where n zv is the total number of tokens v that are assigned to topic z , n uz represents the number of topics z that are assigned to user u .

After the Gibbs sampling procedure, we can easily infer a user X  X  interest, i.e., multinomial distributions over topics for user u as: and easily infer multinomial distributions over tokens for topic z : where n zv is the number of tokens of word v that are assigned to topic z . To obtain the multinomial distribution over topics for document d , i.e.,  X  dz , we first apply the Bayes X  rule: where p ( d | z ) is the probability of d belonging to topic z , and p ( z ) is the probability of topic z . According to (10), p ( d | z ) can be ob-tained as p ( d | z ) = Q v  X  d p ( v | z ) = Q v  X  d  X  zv total number of users. Therefore, 11 can be represented as: As any d has the same chance to be considered to be returned in response to q , we can assume that p ( d ) is a constant, and likewise we also assume that p ( u ) is a constant, such that (12) becomes: stant. Then, the topic distribution similarity sim( y , u ) between a set of documents y and the documents u a user u is interested in can be measured as: where vectors  X  d = (  X  d 1 ,  X  X  X  , X  dT ) and  X  u = (  X  d 1 the multinomial distribution of topics specific to document d and user u , respectively. We use the cos function in (14); other distance functions such as one based on Euclidean distance can be employed but we found that the results were not significantly different.
The feature representation  X  must enable meaningful discrimi-nation between high quality and low quality predictions [32]. To predict a set of documents in the personalized diversification task, we propose to consider three main types of feature space.
Tokens . Following [32], we define L token sets V 1 V ( y ) . Each token set V l ( y ) contains the set of tokens that appear at least l times in some document in y . Then we use thresholds on  X  ( v, u ) (or  X  l ( v, x ) ) that describe word v at l -th importance level. Here, D l ( v ) is the set of documents that have at least l copies of v in the whole set of documents u (or x ). We let L = 20 in our experiments, as quite a few tokens can appear more than 20 times in a document. Besides, we propose to directly utilize the tokens X  statistics to capture similarity between a document x  X  y and a set of documents u that a user u is interested in as features. We consider cosine, Euclidean and Kullback-Leibler (KL) divergence similarity metrics. For each of these three metrics, we compute the minimal, maximal, and average similarity scores of the document x  X  y and the standard deviations to a set of documents u based on the content of the documents and the standard LDA model [5]. In total, we have 49 features that fall in this feature category.
Interest . In addition, based on our UIT topic model, we also compute the cosine, Euclidean and KL similarity between a doc-ument x  X  y and a set of documents u based on a multinomial distribution over topics and the user X  X  multinomial distribution over topics generated by UIT. Again, for each of these three similarity metrics, we compute the minimal, maximal, and average similarity scores and the standard deviation scores. In total, we have S = 36 features  X  s ( x, u ) that fall in this feature category.
Probability . The main probabilities used in state-of-the-art un-supervised personalized diversification methods are utilized in our learning model as features, i.e.,  X  m ( x, x , u ) . These probabilities bility of d belonging to a category c , p ( c | q,u ) , the personalized query aspect distribution, p ( c | d,u ) , the personalized aspect dis-tribution over d , and p ( d | c,u ) , the personalized aspect-dependent document distribution, where c is a category that d belongs to in the Textwise Open Directory Project category service. 1 For p ( d | q ) , we obtain 3 versions of this feature value produced by BM25 [20], Jelinek-Mercer and Dirichlet language models [34]. To get the fea-ture value of p ( c | d ) , we make use of the Textwise service which returns up to 3 possible categories for d , ranked by a score in [0 , 1] , and we use the normalized scores as features. We adopt 5 ways of computing p ( c | q,u ) as feature values [26]; for details on how to compute p ( c | q,u ) , p ( c | d,u ) and p ( d | c,u ) we refer to [26]. Then, we define  X ( x , u , y ) as follows:
In this section, we describe our experimental setup;  X 6.1 lists our research questions;  X 6.2 describes our dataset;  X 6.3 and  X 6.4 lists the baselines and metrics for evaluation, respectively;  X 6.5 details the settings of the experiments.
The research questions guiding the remainder of the paper are: (RQ1) Can supervised personalized diversification methods out-perform state-of-the-art unsupervised methods? Can our method beat state-of-the-art supervised methods? See  X 7.1. (RQ2) What is the contribution of the user-interest topic model in our proposed method? See  X 7.2. (RQ3) What is the effect of the constraints for diversity and consistence with user X  X  interest in our method? See  X 7.3. (RQ4) Does our method outperform the best supervised baseline method on each query? See  X 7.4. (RQ5) Can our method retrieve a competitive number of subtopics per query? See  X 7.5. (RQ6) What is the performance of our supervised methods when the C parameter is varied? See  X 7.6.
In order to answer our research questions we work with a pub-licly available personalized diversification dataset. contains private evaluation information from 35 users on 180 search queries. The queries are quite ambiguous, as the length of each query is no more that two keywords. In total, there are 751 subtopics for the queries, with most of the queries having more than 2 sub-topics. Over 3800 relevance judgements are available, for at least the top 5 results for each query. Each relevance judgement includes 3 main assessments: a 4-grade scale assessment on how relevant the result is to the user X  X  interests (resulting in the user relevance ground truth and the set of users X  interesting documents being cre-ated); a 4-grade scale assessment on how relevant the result is to the evaluated query (resulting in the topic relevance ground truth being created); and a 2-grade assessment whether a specific subtopic is related to the evaluated query (resulting in the subjective subtopics related to the search query being created). Details of this dataset can be found in [26]. For pre-processing, we apply Porter stem-ming, tokenization, and stopword removal (using the INQUERY list) to the documents using the Lemur toolkit. 3
Two well-known corpora, ClueWeb09 and ClueWeb12, 4 have been proposed for search result diversification tasks in the TREC 2009 X 2013 Web tracks [11]. However, they do not contain any user information or relevance judgments provided by specific users, and thus do not fit our experiments.
Let PSVM div denote our personalized diversification via struc-tured learning method. We compare PSVM div to 11 baselines: a traditional web search algorithm, BM25 [20]; 2 well-known plain (in the sense of  X  X ot personalized X ) search result diversification ap-proaches, IA-Select [2] and xQuAD [21]; a plain (in the sense of  X  X ot diversified X ) personalized search approach based on BM25 [27], Pers BM 25 ; a two-stage diversification and personalization approach, xQuAD BM 25 , as suggested by [19], that first applies the xQuAD algorithm and then Pers BM 25 ; 4 state-of-the-art unsupervised per-sonalized diversification methods [26], PIA-Select, PIA-Select PxQuAD, and PxQuAD BM 25 . As PSVM div builds on standard structured learning framework, we also consider 2 structured learn-ing algorithms: SVM div [32] that directly tries to retrieve relevant documents covering as many subtopics as possible, and a standard structured learning method, denoted as SVM rank [33] that directly ranks documents by optimizing a relevance-biased evaluation met-ric (we use  X  -nDCG and nDCG to define the loss functions for SVM div and SVM rank , respectively). 5
For the supervised methods, PSVM div , SVM div and SVM rank we use a 130/40/10 split for our training, validation and test sets, respectively. We train PSVM div , SVM div and SVM rank using val-ues of C (see (7)) that vary from 1e-4 to 1.0. The best C value is then chosen on the validation set, and evaluated on the test queries. The train/validation/test splits are permuted until all 180 queries were chosen once for the test set. We repeat the experiments 10 times and report the average evaluation results.
We use the following diversity metrics for evaluation, most of which are official evaluation metrics in the TREC Web tracks [11] and are widely used in the literature on result diversification:  X  -nDCG@ k . A version of normalized discounted cumulative gain at k in which the role of the parameter  X  is emphasized in computing the novelty of the top k documents.  X  -nDCG@ k scores a ranking by rewarding newly-found subtopics and penalizing re-dundant subtopics geometrically, discounting all rewards with a log-harmonic discount function of rank. See [10] for details on how  X  -nDCG@ k is computed.

S-Recall@ k . Subtopic recall at k [35] is computed at retrieval depth k using the following procedure. Assume there are Q am-biguous queries. Let z be an aspect of query q and N q the number of aspects (subtopics) associated with q . Then, the subtopic recall at rank k [35] is defined as the percentage of subtopics covered by one of the top k documents: where subtopics ( d i | q ) is the number of aspects covered by d response to q .

ERR-IA@ k . Intent-aware expected reciprocal rank at retrieval depth k , similarly, is computed as able at http://www.cs.cornell.edu/People/tj/ . where ERR ( k | z,q ) is the expected reciprocal rank score at k in terms of aspect z of query q .
 Prec-IA@ k . Intent-aware precision at k [2] is defined as where Prec ( k | z,q ) is the precision at k in terms of the aspects z of q , and can be computed as 1 k P k j =1 j q ( z,j ) . Here, j the document returned for q at depth j is judged relevant to aspect z of q ; otherwise, j q ( z,j ) = 0 .
 MAP-IA@ k . Intent-aware MAP at k [2] is computed as where MAP ( k | z,q ) is the MAP score for top k returned documents in terms of aspect z of q .
 For evaluating accuracy, we use nDCG [10], ERR, Prec@ k and MAP. Since users mainly evaluated the top 5 returned results [26], we compute the scores at depth 5 for all metrics. Statistical signifi-cance of observed differences between the performance of two runs is tested using a two-tailed paired t-test and is denoted using ) for significant differences for  X  = . 01 , or M (and O ) for  X  = . 05 .
We report on 6 main experiments aimed at answering the re-search questions listed in  X 6.1. Our first experiment aims at un-derstanding whether supervised personalized diversification meth-ods outperform unsupervised ones and whether PSVM div beats the supervised algorithms that apply structured learning technique di-rectly. We compare PSVM div to 2 supervised baselines, SVM and SVM rank , and the 9 unsupervised baselines with both topic relevance and user relevance ground truths, respectively.
To understand the contribution of the user-interest topic model, we conduct our second experiment where we perform comparisons between PSVM div using all features ( X  X oken X ,  X  X nterest X  and  X  X rob-ability, X  see  X 5.3) including those extracted from the topic model and PSVM div using basic features ( X  X oken X  and  X  X robability X  only, see  X 5.3). In our third experiment, aimed at understanding the ef-fect of our new constraints in PSVM div , a series of experiments is conducted by employing different sets of constraints while training.
In order to understand how PSVM div compares to the best base-line, our fourth and fifth experiment provide a query-and subtopic-level analysis, respectively. Finally, to understand the influence of the key parameter in our structured learning framework, C , we train PSVM div , SVM div and SVM rank by varying C from 1e-4 to 1.0 and report the performance.
The following subsections report, analyze and discuss our exper-imental results.
Table 2 lists the diversity scores of the unsupervised baseline methods. For all metrics in terms of either user relevance or topic relevance, none of the plain methods, viz., BM25, IA-Select, Pers BM 25 , xQuAD and xQuAD BM 25 , beats the best unsupervised personalized diversification methods, viz., PIA-Select, PIA-Se-lect BM 25 , PxQuAD or PxQuAD BM 25 . Moreover, in some cases the performance differences between the best plain method and the best unsupervised personalized diversification method are signifi-cant. This indicates that diversity and personalization are comple-mentary and can enhance each other. The same observation can be found in Table 5 where performance is evaluated by relevance-oriented metrics.
 Table 2: Performance of unsupervised methods on diversifica-tion metrics. The best performance per metric is in boldface. The best plain retrieval method (BM25, IA-Select, Pers BM 25 xQuAD and xQuAD BM 25 ) is underlined. Statistically signifi-cant differences between the best performance per metric and the best plain retrieval method are marked in the upper left hand corner of the best performance score.
 BM25 .6443 .4557 .2267 .1659 .1245 IA-Select .6099 .4282 .2241 .1624 .1177 xQuAD .6421 .4635 .2299 .1675 .1267 PIA-Select .5766 .4407 .2006 .1480 .1085 PxQuAD .6409 .4588 .2313 .1629 .1296 BM25 .7599 .4456 .2315 .1717 .1241 IA-Select .7685 .4425 .2365 .1767 .1212 xQuAD .7711 .4600 .2348 .1747 .1245 PIA-Select .7410 .4641 .2227 .1650 .1206 PxQuAD .7744 .4543 .2350 .1747 .1278
Table 3 shows the diversity-oriented evaluation results of 3 su-pervised methods using basic features ( X  X oken X , and  X  X robability X  features, see  X 5.3) in terms of both ground truths. In terms of diversity-oriented evaluation metrics all of the supervised methods significantly outperform the best unsupervised methods when mak-ing comparisons between the scores and the scores of unsupervised methods in Table 2 in most cases. We make further comparisons in Tables 5 and 6 in terms of relevance-oriented metrics, and find that supervised methods can statistically significantly outperform unsupervised ones. These two findings attest to the merits of tak-ing supervised personalized diversification methods for the task of personalized search result diversification.

Next, we compare supervised strategies to each other. Tables 3 and 4 show the diversity-oriented evaluation resutls in terms of both grounth truths. It is clear from both tables that our supervised method PSVM div statistically significantly beats plain supervised methods, SVM rank and SVM div . This is because PSVM div siders both personalization and diversity factors, whereas the other two do not take both two factors into account. SVM rank only tries to return more relevant documents, and SVM div directly utilizes standard structured learning for diversification.

As shown in Table 6, in terms of the relevance-oriented met-rics, PSVM div does not significantly outperform SVM SVM div . This is because PSVM div returns the same number of rel-evant documents that do, however, cover more subtopics than the other supervised methods. Hence, PSVM div mainly outperforms the other two in terms of diversity-oriented metrics. We provide further analyses in  X 7.4 (query-level) and  X 7.5 (subtopic-level).
Next, to understand the contribution of our UIT topic model, we compare the performance of the supervised methods using basic Table 3: Performance of supervised methods utilizing basic fea-tures on diversification metrics. The best performance per met-ric is in boldface. Statistically significant differences between supervised and the best unsupervised method (in Table 2) per metric, between PSVM div and SVM div , are marked in the up-per left hand corner of the supervised method X  score, in the right hand corner of the PSVM div score, respectively.
 Table 4: Performance of supervised methods utilizing all fea-tures on diversification metrics. The best performance per met-ric is in boldface. All the scores here are statistically signifi-cant compared to those in Table 2. Statistically significant dif-ferences between the method here and the method in Table 3, between PSVM div and SVM div , are marked in the upper left hand corner of the corresponding score, in the right hand cor-ner of the PSVM div score, respectively.
 Table 5: Performance of unsupervised methods on relevance metrics. Notational conventions are the same as in Table 2. features, i.e., all other features but not the features generated from the UIT model, with those using all the features.

We turn to Tables 3 and 4, that list the results of the supervised methods in terms of diversity-oriented metrics when using the basic features and all features, respectively. For all supervised methods, the performance of using all features is better than that of only us-ing the basic features. That is, our proposed UIT model can capture users X  interest distributions and this kind of information can be ap-plied to improve performance. Due to space limitations, we do not report the results in terms of relevance-oriented metrics; the find-ings there are qualitatively similar.
 Table 6: Performance of supervised methods utilizing basic fea-tures on relevance metrics. The best performance per metric is in boldface. Statistically significant differences between super-vised and the best unsupervised method (in Table 5) per metric, between PSVM div and SVM div , are marked in the upper left hand corner of the supervised method X  score, in the right hand corner of the PSVM div score, respectively.
 Table 7: Performance of PSVM div involving different con-straints using basic features on diversification metrics with user relevance ground truth. The best performance per metric is in boldface. Statistically significant differences against PSVM C i are marked in the upper right hand corner of the corre-sponding scores.
 Table 8: Performance of PSVM div involving different con-straints using all features on diversification metrics with user relevance ground truth. Statistically significant differeneces be-tween the score here and that in Table 7 are marked in the up-per left hand corner of the scores. Other notational conventions are the same as in Table 7.

Next, to understand the effect of the newly proposed constraints, we conduct experiments by employing different sets of constraints while training. The comparisons are again divided into those using all features and those using basic features. We write PSVM C methods trained with the standard constraint (constraint i in (7)), standard and diversity-biased constraints (constraints i and ii in (7)), standard and interest-biased (constraints i and iii in (7)), and all constraints involved (constraints i, ii and iii in (7)), respectively. Again, we only report results on diversity-oriented metrics.
According to Tables 7 and 8, when employing one more con-straint, either diversity-biased or interest-biased, the performance is statistically significantly better than that of only employing the standard constraint. In terms of all metrics, the performance of PSVM div employing all constraints statistically significantly out-performs the performance of using at most two constraints. The positive effect of the proposed constraints again demonstrates that combining diversification (the diversity-biased constraint) and per-sonalization (the interest-biased constraint) boosts the performance. Figure 4: Performance of the supervised methods using all fea-tures when varying the value of parameter C .
In order to figure out why PSVM div enhances other supervised baselines, we take a closer look at per test query improvements of PSVM div over the best supervised baseline method, viz., SVM which outperforms SVM rank in most cases. Fig. 2 shows the per query performance differences in terms of the diversify-oriented metrics of PSVM div against SVM div when they use all the fea-tures. PSVM div achieves performance improvements for many queries, especially in terms of  X  -nDCG, S-Recall, ERR-IA. In a very small number of cases, PSVM div performs poorer than SVM div . This appears to be due to the fact that PSVM motes some non-relevant documents when it tries to cover as many subtopics as possible for a given query.
Next, we zoom in on the number of different subtopics that are returned by PSVM div and SVM div , respectively, to further analyze why PSVM div beats SVM div . Here, again, we use SVM div representative. Specifically, we report changes in the number of subtopics for PSVM div against SVM div in Fig. 3 when they use all features. Red bars indicate the number of subtopics that appear in the run of PSVM div but not in the run of SVM div , white bars indicate the number of subtopics in both runs, whereas blue bars indicate the number of subtopics that are not in PSVM div SVM div ; queries are ordered first by the size of the red bar, then the size of the white bar, and finally the size of the blue bar.
Clearly, the differences between PSVM div and SVM div in the top 2 and 3 are more limited than the differences in the top 4 and 5, but in all cases PSVM div outperforms SVM div . E.g., in total there are 68 more subtopics in the top 5 of the run produced by PSVM div than those in the SVM div run (in terms of all the 180 test queries, 68 subtopics in PSVM div but not in SVM div , 7 subtopics in SVM div but not in PSVM div ).
To understand the performance of the tradeoff parameter C used in (4) and (7), which balances between weights and slacks, we show the performance of PSVM div as well as the 2 supervised baselines using all features. To save space, we only report the performance on  X  -nDCG. Fig. 4 plots the results and it illustrates that PSVM performs best when C is small. This indicates the merit of our new constraints (as well as the standard constraint used in the baselines) focusing on weight modification rather than on low training loss.
Most previous work on personalized diversification of search re-sults produce a ranking using unsupervised methods, either implic-itly or explicitly. In this paper, we have adopted a different per-spective on the problem, based on structured learning. We pro-pose to boost the diversity and match to users X  personal interests of search results by introducing two additional constraints into the standard structured learning framework. We also propose a user-interest topic model to capture users X  multinomial distribution of interest over topics and infer per-document multinomial distribu-tions over topics. Based on this a number of user interest features are extracted and the similarity between a user and a document can be effectively measured for our learning method.

Our evaluation shows that supervised personalized diversifica-tion approaches outperforms state-of-the-art unsupervised person-alization diversification, plain personalization and plain diversifi-cation algorithms. The two proposed constraints are shown to play a significant role in the supervised method. We also find that the user-interest topic model helps to improve performance. Our pro-posed learning method is able to return more subtopics.

As to future work, we aim to study other types of learning strate-gies for personalized diversification of search results. Our method employed the  X  -nDCG metric in the loss function; we plan to use other alternative metrics. Finally, our experimental results were only evaluated on a single dataset. In future work we plan to invite users to label the existing datasets, e.g., ClueWeb09, such that they can also be used for personalized diversification algorithms. [1] Z. Abbassi, V. S. Mirrokni, and M. Thakur. Diversity [2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [3] K. Bache, D. Newman, and P. Smyth. Text-based measures [4] P. N. Bennett, R. W. White, W. Chu, S. T. Dumais, P. Bailey, [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] J. Boyd-Graber and D. M. Blei. Syntactic topic models. In [7] J. Boyd-Graber and D. M. Blei. Multilingual topic models [8] J. Carbonell and J. Goldstein. The use of MMR, [9] H. Chen and D. R. Karger. Less is more: probabilistic [10] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, Figure 2: Per query performance differences of PSVM div against SVM div . The figures shown are for  X  -nDCG, S-Recall, ERR-IA, [11] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview [12] V. Dang and W. B. Croft. Diversity by proportionality: An [13] V. Dang and W. B. Croft. Term level search result [14] S. Jameel and W. Lam. An unsupervised topic segmentation [15] S. Khuller, A. Moss, and J. S. Naor. The budgeted maximum [16] S. Liang, Z. Ren, and M. de Rijke. Fusion helps [17] C. Liu, N. J. Belkin, and M. J. Cole. Personalization of [18] J. S. Liu. The collapsed Gibbs sampler in Bayesian [19] F. Radlinski and S. Dumais. Improving personalized web [20] S. E. Robertson and D. A. Hull. The TREC-9 filtering track [21] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query [22] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for [23] Y. Shi, X. Zhao, J. Wang, M. Larson, and A. Hanjalic. [24] J. Tang, M. Zhang, and Q. Mei. One theme in all views: [25] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. [26] D. Vallet and P. Castells. Personalized diversification of [27] D. Vallet, I. Cantador, and J. M. Jose. Personalizing web [28] S. Vargas, P. Castells, and D. Vallet. Explicit relevance [29] H. Wang, X. He, M.-W. Chang, Y. Song, R. W. White, and [30] X. Wang and A. McCallum. Topics over time: A [31] Q. Yuan, G. Cong, Z. Ma, A. Sun, and N. M. Thalmann. [32] Y. Yue and T. Joachims. Predicting diverse subsets using [33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support [34] C. Zhai and J. Lafferty. A study of smoothing methods for [35] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent [36] S.-X. Zhang and M. Gales. Structured SVMs for automatic [37] J. Zhu, X. Zheng, L. Zhou, and B. Zhang. Scalable inference
