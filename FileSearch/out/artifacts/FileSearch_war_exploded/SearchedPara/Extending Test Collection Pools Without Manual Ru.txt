 Information retrieval test collections traditionally use a combina-tion of automatic and manual runs to create a pool of documents to be judged. The quality of the final judgments produced for a collection is a product of the variety across each of the runs sub-mitted and the pool depth. In this work, we explore fully auto-mated approaches to generating a pool. By combining a simple voting approach with machine learning from documents retrieved by automatic runs, we are able to identify a large portion of rele-vant documents that would normally only be found through manual runs. Our initial results are promising and can be extended in fu-ture studies to help test collection curators ensure proper judgment coverage is maintained across complete document collections. H.3.3 [ Information Storage &amp; Retrieval ]: Information Search &amp; Retrieval X  clustering, retrieval models, search &amp; selection process Information retrieval, Evaluation, Test collection construction
Successful evaluation and reproducibility of experiments in in-formation retrieval (IR) depends on building reusable test collec-tions composed of documents, topics, and relevance judgments. Ideally every document in a collection would be assessed against each topic, but this approach does not scale. So judgments are nor-mally produced for a sample of the corpus, known as a pool , all other documents are assumed to be not relevant. This sample needs to be representative of the entire collection and robust enough to evaluate entirely new search algorithms. The genesis of pooling dates back to the 1970s [12].
 To produce relevance judgments, the organizers of TREC, CLEF, NTCIR, and other such conferences invite researchers to submit the top-i documents retrieved for a set of topics from a specified corpus [14, 15] (typically i = 1 , 000 ). The sets of documents are known as automatic runs . Across the runs, the top-j ranked documents for each topic are gathered for relevance assessment (typically j is set to 50 or 100 ). Such a practice seems to consistently identify most of the relevant documents, but provides no guarantee on the judgment coverage for documents retrieved by new IR approaches [4, 9, 16]. Test collections tend to have a bias towards the systems contributing to the pool, and may not reliably evaluate novel IR systems that retrieve unjudged but relevant documents.
In an attempt to  X  X uture proof X  test collections, the organizers of the evaluation conferences commonly encourage submissions of manual runs , where humans can reformulate queries and/or merge results from multiple queries [1] before a final set of top-i docu-ments is submitted. Such runs are generally highly effective and contribute many unique relevant documents to the judgment pool. However, manual runs are not always available when building a collection, so in this short paper we ask: Research question: Can we construct reliable IR test collections using only automatic retrieval runs? Our contribution: We describe a methodology that can be used to construct reusable test collections in the absence of manual re-trieval runs. We evaluate a simple voting approach combined with machine learning to show that we can achieve collection coverage similar to pooling generated with manual runs.
Efficiently building test collections for evaluation of IR systems is a well-studied problem [10]. Early research concentrated on more efficient ways for assessors to scan pools, with the objec-tive of judging more documents with a given budget or identify-ing a sufficient number of relevant documents as quickly as possi-ble. Zobel [16] showed that the number of relevant documents in a collection varies from topic to topic. He suggested that assessors should focus their effort on judging topics with more relevant doc-uments. For each topic, the number of relevant documents found so far were used to estimate the expected ratio of relevant documents in the remaining unjudged block. Each topic was assessed until relevant documents were depleted beyond an economically viable limit to assess the block.

The idea of focusing assessor effort on the most fruitful sources of relevant documents was also applied to IR systems that con-tribute to a pool. Just as some topics have more relevant documents than others, some systems retrieve more relevant documents than others. Using this insight, Cormack et al. [5] described a move-to-front pooling approach which ensured that documents from the IR systems producing the most relevant documents were moved to the
We also use Kendall X  X   X  to measure pairwise inversions between two rankings of runs, the first using full TREC relevance assess-ments and the second using relevance assessments generated from the union of the first and second pools formed by each of our meth-ods. Using a convention from Voorhees [13], if the Kendall X  X   X  correlation is  X  0 . 9 , the rankings are considered equivalent. Table 1: Effectiveness on finding relevant documents in MRJ. A  X  : significant improvement ( p &lt; 0 . 01 ) compared to Borda count.
 Table 2: Percentage of MRJ documents found in top ( k ) of the proposed rankings. z implies a similar assessment effort ( p &lt; 0 . 05 ) compared to Borda count.
The analysis is presented in Table 1. The combined method is significantly better than the other two when evaluated with MAP. The same trend is observed when measuring using precision, but none of the differences are significant. Using only the ML method produces worse results than either Borda count or combined. Note that the relatively low reported effectiveness numbers in Table 1 are largely a byproduct of evaluating using only the unique relevant documents in MRJ and not the entire second pool. We cannot make any claims about new documents retrieved by the ML method since a large portion of retrieved documents using this method are not judged, compared to other two approaches. In fact, 9 , 817 of the top-200 documents returned across all 50 topics using only ML ( 98 . 17% ) are currently unjudged. Therefore, we have to assume that these documents are not relevant until all of the docu-ments returned are judged. In future work, we hope to investigate the full impact unjudged documents have on our classifier method in more detail.

In Table 2 we measure the proportion of documents that were found to be relevant in the second pool. Again a similar trend of differences are seen, but with significant improvements across all measurements up to k = 200 for the combined method.
As indicated in Figure 1, the majority of documents uniquely judged in the manual runs (MRJ) are also retrieved by the automatic runs (ARU+ARJ). However, few appear in the first pool as they (i.e. ARJ) are not ranked highly enough to be judged. In fact, 88% of the documents judged as relevant that are uniquely pooled by manual runs could be found in the first pool, if a pool depth of 1 , 000 was used.

If there were no manual runs in a test collection (i.e. no MRJ), the effectiveness of IR systems producing results similar to such runs would be underestimated and any improvements would go un-noticed. It would appear that manual retrieval runs still play a crit-ical role in improving the re-usability of test collections. Figure 2: The number of MRJ documents, and estimated num-ber of relevant documents in the top-k of the combined ranked list on TREC GOV2 dataset and TREC topics 801  X  850 .
 Table 3: Just considering the documents in MRJ, how effective ar e ranking algorithms on retrieving relevant documents? Sig-nificant improvements ( p &lt; 0 . 01 and p &lt; 0 . 05 ) compared to
Judging the ranked lists of the combined method up to a depth k identifies a subset of the relevant documents uniquely pooled by manual retrieval runs. However, we still know little about the large number of unjudged documents in the ranked lists produced by the combined method. If we assume the proportion of relevant documents among unjudged documents in these ranked lists is the same as the proportion found among judged documents in the same ranked list up to the same depth, we can estimate the total num-ber of relevant documents that would have been found in the same depth of the ranking. Figure 2 illustrates the estimated number of relevant documents, along with the number of known relevant doc-uments found.

Missing judgments for a large portion of the ranked lists from the proposed methods is one potential reason for the low retrieval effectiveness of those methods. Therefore, we calculate retrieval effectiveness on the intersection of the second pool with MRJ, Ta-ble 3. (Note, the first pool and the ranking functions remains the same.) The ML method now re-ranks a subset of unique docu-ments top-j ranked by manual runs. The ranking produced by ML show significant improvements for all considered evaluation met-rics compared to Borda count. The combined method achieves a better effectiveness than ML for all evaluation metrics considered, except p@ 100 . This is due to ranking only the subset of documents top ranked by the Borda count. Re-ranking a carefully retrieved Figure 3: Kendall X  X   X  corr elation of IR system rankings for varying depths of assessing documents using combined method. subset of documents for topics with ML is an effective approach to locate new documents to be pooled and judged.

Whenever a new approach for pool composition is proposed, we would like to be able to quantify how well the approach ranks IR systems compared to the original method. A Kendall X  X   X  ranking correlation for varying depths of assessing documents with the pro-posed approach for various evaluation metric are shown in Figure 3. Here, we consider all 80 submitted runs rather than only the subset originally used for pooling. Manual retrieval runs are viewed as novel approaches to retrieval. The Kendall X  X   X  correlation for MAP is above 0 . 9 beyond a depth of 100 . A budget similar to origi-nal assessment permits processing up to a depth of 171 documents, which demonstrates the validity of the proposed approach in the absence of manual retrieval runs.

Another question of interest is how small the automatic runs pool can be when there are no manual runs. In Figure 4 we introduce runs incrementally in order starting with the run contributing the fewest relevant documents. When 20 or more automatic retrieval runs are pooled the Kendall  X  correlation for MAP exceeds 0 . 9 .
In this paper, we present a methodology for building reusable evaluation pools in the absence of manual retrieval runs. Our ap-proach can discover many relevant documents that were previously only found by manual retrieval runs. The approach demonstrates the potential of finding relevant documents that are not currently possible using current pooling approaches. However, the true ef-ficacy of our approach cannot be properly assessed until all of the newly retrieved documents are judged. We plan to investigate this in future work. Nonetheless, our initial results are promising as we are already able to achieve a similar IR system ranking to previ-ous approaches which depended heavily on manual runs to add the necessary diversity to the assessment pool.
 This work was supported in part by the Australian Research Coun-cil (DP130104007). Dr. Culpepper is the recipient of an ARC DE-CRA Research Fellowship (DE140100275).
 Figure 4: Kendall X  X   X  corr elation of IR system rankings with varying number of automatic systems in the pool. Automatic systems are added in the order least contributing system to most, and the ranking produced by the combined method is processed to a depth of 200 .

