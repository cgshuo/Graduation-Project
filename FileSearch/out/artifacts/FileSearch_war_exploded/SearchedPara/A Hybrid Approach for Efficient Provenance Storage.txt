 Efficient provenance storage is an essential step towards the adoption of provenance. In this paper, we analyze the prove-nance collected from multiple workloads with a view towards efficient storage. Based on our analysis, we characterize the properties of provenance with respect to long term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent du-plication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of web graph compression (adapted for provenance) and dictionary encoding, provides the best tradeoff in terms of compression ratio, compres-sion time and query performance when compared to other compression schemes.
 E.4 [ Coding and Information Theory ]: Data compaction and compression; H.3.2 [ Information Storage and Re-trieval ]: Information Storage-File organization provenance graphs, storage, compression
Provenance is the metadata that represents the history or lineage of a data object. Provenance has applications in various areas in the real world, such as experimental docu-mentation, debugging, security, and search. The provenance community has built a number of systems [9, 11, 8] to collect provenance. While these systems are a great step towards making provenance available to users, they neglect a crucial aspect that makes these systems practical: efficient prove-nance storage. Unoptimized provenance storage can take up a substantial amount of space. For instance, the base data in the PReServ [6] provenance store was 100 KB, but the prove-nance exceeded 1 MB. In MiMI [7], an online database for storing protein information, the size of provenance (6 GB) also far exceeded the size of the original data (270 MB). Sim-ilar results are also observed in other systems [8, 10].
We cannot however, directly apply existing compression techniques to provenance, because the structure and ac-cess patterns of provenance are vastly different from regular metadata:
Accordingly, we developed a hybrid method for compress-ing provenance graphs by combining web graph based com-pression and dictionary based compression algorithms. The web graph compression algorithm allows us to compress prove-nance while still satisfying the characteristics we observed. Dictionary encoding, because of its flexible processing gran-ularity, allows us to eliminate any repeated separate strings or sub-strings in the provenance graphs. Then, we compared the performance of our hybrid approach with a compression scheme designed explicitly for provenance compression: the Factorization And Inheritance (FAI) method proposed by Chapman et al. [4]. Our results indicate that the hybrid approach used by us significantly outperforms FAI along all axes.
A provenance graph is composed of two distinct parts: identity information on provenance nodes and ancestor in-formation on provenance edge. We investigated their distri-bution in a large variety of provenance traces as shown in Table 1.

It can be observed that there is no particular pattern in the distribution. In some traces, the provenance is domi-nated by the identity information and in others it is dom-inated by ancestry information. The identity information typically dominates in traces from Karma system. The rea-son is that those Karma traces are collected based on the provenance traces and where these traces can be found. Open Provenance Model (i.e., OPM [3]) and record a size-able amount of property-values (though this is optional in OPM model) to describe the elements (e.g., workflowID and serviceID) in the identity information. Ancestry information takes up a larger percentage in the traces from Tupelo [12], PASOA [9] and Taverna [11]. These traces do not record the optional property-value pairs in the identity informa-tion, but still record the ancestor information, such as role, time and account, on the edges.
Duplication is widespread among annotation information in both of the identity information (e.g., account, type and workflowID) and ancestry information (e.g., account and Time) in some OPM traces. For example, the account, which is identical for every node and edge in an individ-ual workflow, is recorded for every node and edge in OPM traces. The experimental results on those Karma traces in Section 4 show that the percentage of duplicates can be from 34.86% to 37.50% in the identity information, and 38.83% to 40.26% in the ancestor information of six workflow graphs in the Karma traces.
A web graph has a node for each URL and an edge for each hyperlink from one web page pointing to another. Existing web graph compression algorithms [1] typically exploit the following two key properties to significantly compress web graphs:
Provenance graphs also have similar structure properties as web graphs. Table 2 shows a series of provenance nodes from the NAM-WRF provenance trace presented as an adja-cency list that represents the provenance graph. We assign an ID to each node according to their order in the prove-nance trace. Provenance nodes in this adjacency list have a common ancestor node 15. This is because many pro-cesses, represented by nodes like 15, trigger a new process Table 2: A slice of the adjacency list for the NAM-WRF provenance trace.
 Table 3: Statistics of provenance nodes on locality and similarity
Trace No. of nodes NAM-
WRF NCFS 4140 2530 (61.11%) 2990 (72.22%)
SCOOP 77000 70000 (90.91%) 35000 (45.45%) (e.g., node 1) and generate a file (e.g., node 3). Further, some configuration files are also repeatedly used as input by many processes, therefore they appear as the common an-cestors of many processes. These nodes also exhibit locality. For example, the ancestors of provenance node 1 are only between 2 and 16, and the ancestors of node 4 are only be-tween 5 and 17. The reason is that we assign ID to these nodes based on the orders of their appearance in the trace, so one node has a high probability to be located not far from its ancestors.

Table 3 shows the number of provenance nodes that have these two properties in some provenance traces. We spec-ify a node has the locality property (i.e.,  X  X ocal node X ) if the difference between its biggest and smallest ancestor is no larger than 15. A node has the similarity property (i.e.,  X  X imilar node X ) if this node shares at least one common an-cestor with the node in preceding 10 nodes. One can see that all these traces have a large percentage of similar nodes and local nodes. This indicates that these provenance graphs also exhibit locality and similarity, like web graphs. rate
For provenance to be useful, it needs to be queriable, even when it is compressed. So a compression scheme that makes queries difficult or has a high query overhead is not pre-ferred.
There are two critical ideas incorporated in the current web compression algorithms [1]. First, Exploitation of Simi-larity , i.e., expressing the ancestor list of one node in terms of another node with similar ancestors, thus efficiently avoiding encoding the duplicate data. Second, Exploitation of Local-ity , i.e., rather than storing the ancestors of a node, storing the gaps between them, which typically requires fewer bits to be encoded.

We express a provenance graph as a set of provenance nodes which have a series of ancestors (See Table 2). Each provenance node is assigned an ID, in order of appearance, during provenance generation. Let Out( x ) denote the an-cestor list of node x and W indicate the window parameter. We detail the web compression algorithm as follows: 1. Reference compression: check if there exists a similar 2. Encode gaps: Let the ancestors of x yet to be en-
Note that if we want to query the ancestor list (Out( x )) of node x , we have to first decode its reference list Out( y ), and then we have to decode the reference list of Out( y ), and so on. This would form a reference list chain, with a long chain obviously resulting in bad query performance. In our implementation, we confine the length of this chain to a maximum level of 5.
Dictionary encoding scans the entire database or text files to find the frequently occurring strings, and then replace them with integer codes.

For provenance, we look for frequently occurring strings in the provenance graph data, then we use integer codes to encode them and store this mapping relationship into a dic-tionary database. The granularity of the repeated strings is very flexible. It can be a whole string, the prefix of it or an arbitrary substring in a big string. For example, the edge in an OPM graph is usually annotated with time which in-dicates when this process (or dependency relationship) hap-pens. We can use dictionary encoding to encode the com-mon prefix that consists of year, month and day in the time information.
As we have stated above, web compression algorithms can compress the ancestor information very effectively and dic-tionary encoding can eliminate the duplicates existing in the provenance graph data. Their combination provides a prac-tical and efficient method to compress a provenance graph. Additionally, since web compression and dictionary encod-ing are both light-weight compression schemes, this hybrid method (i.e., web+dictionary) retains a good query perfor-mance on provenance datasets.
The experiments were run on a machine with the Windows 7 (32-bit) operating system, Pentium(R) Dual-Core E6500 2.93 GHz*2 CPU, 2 GB memory and 500 GB hard drive.
The provenance traces we used were extracted from a 10GB noisy provenance database [5] generated by using the Karma [2] system. These traces consist of provenance gen-erated from six kinds of workflows (See Table 4). The work-flows are from different scientific domains and accordingly have different characteristics.

We compressed these traces using FAI and our hybrid method respectively. In both cases, we store the compressed provenance records in a Microsoft SQL Server 8.0 database. Table 5 shows the schema that we used for the two tech-niques. In our hybrid method, for ancestor information, we assign each node string a unique identifier (in the or-der of appearance in the trace) to make the web compres-sion easier. Since a node can have a series of ancestors, we encode them into one  X  X ncestors coding X  using web compres-sion algorithm and store each record in a concise format (i.e., (Node identifier, ancestors coding)) in AncestorDB. While FAI is composed of a series of factorization and inheritance methods (See Table 6), argument factorization achieves the best compression ratio and is used in the rest of the paper; it finds the duplicate node strings in AncestorDB and encodes them using integer codes, and stores them in DictionaryDB.
Compression techniques web+dictionary
FAI
Figure 1 shows the compression size and time for various workflow traces using FAI and the web+dictionary meth-ods respectively. Web+dictionary outperforms FAI in both cases. The reason for the improvement of compression size is that FAI can only eliminate the duplicate node strings in the ancestor information, while web+dictionary seeks the local-ity and similarity between the ancestors of different nodes, and encodes all the ancestors that belong to a node to a 0/1 sequence. In addition, it reduces the duplicate strings in the annotation information in both ancestor and identity infor-mation. This exploits the redundancy in provenance graph to the maximum extent possible.

The reason for the improvement on compression time is two-fold. First, web+dictionary eliminates the duplicate strings in the identity information, making the size of the provenance records loaded to ArtifactDB and ProcessDB much smaller. In turn, this reduces the time needed to store the provenance records. Second, FAI employs a hash table to store the duplicate node strings and the frequency with which they appear in the edge (or ancestor) information. For provenance graphs that have a large number of nodes, such as motif, the time to query the hash table increases as the number of nodes increases. This indicates that FAI is not suitable for compressing large provenance sets.
To compare the query performance of compression meth-ods, we ran a series of queries on the NAM-WRF trace as shown in Figure 2.

Q.1 looks up the ancestor of a specific object. The query process in Q.2 is similar to Q.1. However, the query has to repeat the step in Q.1 recursively until all the descendants have been located. In both cases, web compression performs better than FAI. The reason for the improvement is twofold. First, web compression significantly reduces the number of provenance record in AncestorDB. The query on the ances-tor of a node will return a series of records in FAI, but only one record under web compression. Second, web compres-sion algorithm further reduces the size of each record by exploiting the similarity and locality in the ancestors, mak-ing the size of the records to be read much smaller than in the FAI case. Though web decompression can incur time overhead during the ancestor queries, we have reduced this impact by confining the length of the provenance chain to 5 to avoid a limitless decompression.
 In Q.3, the web+dictionary encoding also outperforms FAI. The improvement is because, for the web+dictionary encoding case, the TimeDB stores all the time information of a node string (these time information are on the edges that starts from this node string) in only one database record, while FAI uses one record for storing the time information on each edge in AncestorDB. So the number of the entries that needs to be queried in TimeDB in web+dictionary encod-ing is much fewer than in AncestorDB in FAI. On the other hand, the time information in TimeDB has been encoded using dictionary encoding, so the size of the record is much smaller than in the FAI case. Though querying the Dictio-naryDB incurs overhead, the total time in web+dictionary case is still smaller than in the FAI case. Similarly, in Q.4, the elements (such as workflowID ) are encoded and stored using small integer numbers in ProcessDB in dictionary en-coding. Hence the query time in ProcessDB is much smaller than in the uncompressed case and FAI. Despite the cost of querying the DictionaryDB for the final strings, the total query performance with dictionary encoding is better than the uncompressed and FAI case.
As provenance accumulates over time, it can occupy a significant amount of storage. Today, users have two non-options: archive it in a non-queriable format or discard the provenance. In this paper, we have addressed this cru-cial issue and have provided users with a practical solution for storing provenance efficiently. Our compression method (web+dictionary) performs significantly better than the FAI algorithm in both compression and query performance.
This work was supported in part by the National Basic Re-search 973 Program of China under Grant No. 2011CB302301, NSFC No. 61025008, 61173043, 60933002, 60873028. This work was also supported by the National Science Foundation under award IIP-0934401 and Department of Energy under Award Number DE-FC02-10ER26017/DE-SC0005417. We also thank the sponsors of the SSRC and CRIS, including the National Science Foundation, Los Alamos National Lab-oratory, LSI, IBM Research, NetApp, Samsung Information Systems America, Seagate Technology, Northrop Grumman, Symantec, Hitachi, CITRIS, the Department of Energy Of-fice of Science, the NASA Ames Research Center and Xyra-tex. [1] P. Boldi and S. Vigna. The webgraph framework I : [2] B. Cao, B. Plale, G. Subramanian, E. Robertson, and [3] Provenance challenge. [4] A. P. Chapman, H. V. Jagadish, and P. Ramanan. [5] Y.-W. Cheah, B. Plale, J. Kendall-Morwick, D. Leake, [6] P. Groth, S. Miles, W. Fang, S. C. Wong, K. Zauner, [7] M. Jayapandian, A. P. Chapman, V. G. Tarcea, C. Yu, [8] K.-K. Muniswamy-Reddy, D. A. Holland, U. Braun, [9] Provenance aware service oriented architecture. [10] Y. L. Simmhan, B. Plale, and D. Gannon. A [11] Taverna workflow management system. [12] Tupelo semantic content repository.
