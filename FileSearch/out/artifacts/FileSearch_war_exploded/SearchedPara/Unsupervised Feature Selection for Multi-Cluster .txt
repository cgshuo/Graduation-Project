 In many data analysis tasks, one is often confronted with very high dimensional data. Feature selection techniques ar e designed to find the relevant feature subset of the original features which can facilitate clustering, classification a nd re-trieval. In this paper, we consider the feature selection pr ob-lem in unsupervised learning scenario, which is particularl y difficult due to the absence of class labels that would guide the search for relevant information. The feature selection problem is essentially a combinatorial optimization probl em which is computationally expensive. Traditional unsuper-vised feature selection methods address this issue by select -ing the top ranked features based on certain scores com-puted independently for each feature. These approaches ne-glect the possible correlation between different features a nd thus can not produce an optimal feature subset. Inspired from the recent developments on manifold learning and L1-regularized models for subset selection, we propose in this paper a new approach, called Multi-Cluster Feature Selection (MCFS), for unsupervised feature selection. Specifically, w e select those features such that the multi-cluster structur e of the data can be best preserved. The corresponding op-timization problem can be efficiently solved since it only involves a sparse eigen-problem and a L1-regularized least squares problem. Extensive experimental results over vari-ous real-life data sets have demonstrated the superiority o f the proposed algorithm.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Fea-ture evaluation and selection Algorithms, Theory Feature selection, Unsupervised, Clustering
In many applications in computer vision, pattern recog-nition and data mining, one is often confronted with very high dimensional data. High dimensionality significantly i n-creases the time and space requirements for processing the data. Moreover, various data mining and machine learning tasks, such as classification and clustering, that are ana-lytically or computationally manageable in low dimensional spaces may become completely intractable in spaces of sev-eral hundred or thousand dimensions [12]. To overcome this problem, feature selection techniques [3, 4, 17, 21, 29, 30] a re designed to reduce the dimensionality by finding a relevant feature subset. Once a small number of relevant features are selected, conventional data analysis techniques can then be applied.

Based on whether the label information is available, fea-ture selection methods can be classified into supervised and unsupervised methods. Supervised feature selection meth-ods usually evaluate the importance of features by the cor-relation between features and class label. The typical super -vised feature selection methods include Pearson correlatio n coefficients [23], Fisher score [12], and Information gain [1 1]. However, in practice, there is usually no shortage of unla-beled data but labels are expensive. Hence, it is of great significance to develop unsupervised feature selection algo -rithms which can make use of all the data points. In this paper, we consider the problem of selecting features in unsu -pervised learning scenarios, which is a much harder problem due to the absence of class labels that would guide the search for relevant information.

The feature selection aims at selecting the most relevant feature subset based on certain evaluation criteria. This problem is essentially a combinatorial optimization prob-lem which is computationally expensive. Traditional fea-ture selection methods address this issue by selecting the top ranked features based on some scores computed inde-pendently for each feature. The scores are usually defined to reflect the power of each feature in differentiating differ-ent classes/clusters. This approach may work well on binary classes/clusters problems. However, it is very likely to fa il in multi classes/clusters cases. Fig. (1) shows an intuitiv e example. There are three Gaussians in a three dimensional space. Without the label information, some popular unsu-pervised feature selection methods ( e.g ., Maximum variance and LaplacianScore [17]) rank the features as a &gt; b &gt; c . If one is asking to select two features, these methods will selec t features a and b , which is obviously sub-optimal. When deal-ing with multi classes/clusters data, different features ha ve sub-optimal. different powers on differentiating different classes/clust ers ( e.g ., cluster 1 vs. cluster 2 and cluster 1 vs. cluster 3). There are some studies on supervised feature selection [2] trying to solve this issue. However, without label informa-tion, it is unclear how to apply the similar ideas to unsuper-vised feature selection methods.

Inspired from the recent developments on spectral analy-sis of the data (manifold learning) [1, 22] and L1-regulariz ed models for subset selection [14, 16], we propose in this pa-per a new approach, called Multi-Cluster Feature Selection (MCFS), for unsupervised feature selection. Specifically, w e select those features such that the multi-cluster structur e of the data can be well preserved. By using spectral analysis techniques, MCFS suggests a principled way to measure the correlations between different features without label info r-mation. Thus, MCFS can well handle the data with multiple cluster structure. The corresponding optimization proble m only involves a sparse eigen-problem and a L1-regularized least squares problem, thus can be efficiently solved. It is important to note that our method essentially follows our previous work on spectral regression [5] and sparse subspace learning [6, 7].

The rest of the paper is organized as follows: in Section 2, we provide a brief review of the related work. Our multi cluster feature selection algorithm is introduced in Secti on 3. The experimental results are presented in Section 4. Finally , we provide the concluding remarks in Section 5.
Feature selection methods can be classified into X  X rapper X  methods and  X  X ilter X  methods [19, 21]. The wrapper model techniques evaluate the features using the mining algorithm that will ultimately be employed. Thus, they  X  X rap X  the selection process around the mining algorithm. Algorithms based on the filter model examine intrinsic properties of the data to evaluate the features prior to the mining tasks.
For unsupervised  X  X rapper X  methods, the clustering is a commonly used mining algorithm [10, 13, 20, 24]. These algorithms consider feature selection and clustering simu l-taneously and search for features better suited to clusteri ng aiming to improve clustering performance. However, these  X  X rapper X  methods are usually computationally expensive [19] and may not be able to be applied on large scale data mining problems. In this paper, we are particularly inter-ested in the filter methods which are much more efficient.
Most of the existing filter methods are supervised. Max-imum variance might be the most simple yet effective un-supervised evaluation criterion for selecting features. This criterion essentially projects the data points along the di -mensions of maximum variances. Note that, the Principal Component Analysis (PCA) algorithm shares the same prin-ciple of maximizing variance, but it involves feature trans-formation and obtains a set of transformed features rather than a subset of the original features.

Although the maximum variance criteria finds features that are useful for representing data, there is no reason to assume that these features must be useful for discriminat-ing between data in different classes. Recently, the Lapla-cianScore algorithm [17] and its extensions [30] have been proposed to select those features which can best reflect the underlying manifold structure. LaplacianScore uses a near-est neighbor graph to model the local geometric structure of the data and selects those features which are smoothest on the graph. It has been proven [17] that with label informa-tion LaplacianScore becomes Fisher criterion score. The la t-ter is a supervised feature selection method (filter method) which seeks features that are efficient for discrimination [12 ]. Fisher criterion score assigns the highest score to the feat ure on which the data points of different classes are far from each other while requiring data points of the same class to be close to each other.
 Wolf et al. proposed a feature selection algorithm called Q- X  [29]. The algorithm optimizes over a least-squares crite-rion function which measures the clusterability of the inpu t data points projected onto the selected coordinates. The op -timal coordinates are those for which the cluster coherence , measured by the spectral gap of the corresponding affinity matrix, is maximized [29]. A remarkable property of the algorithm is that it always yields sparse solutions.
The generic problem of unsupervised feature selection is the following. Given a set of points X = [ x 1 , x 2 , , x x i  X  R M , find a feature subset with size d which contains the most informative features. In other words, the points 1 , x  X  2 , , x  X  N } represented in the d -dimensional space R can well preserve the geometric structure as the data repre-sented in the original M -dimensional space.

Since naturally occurring data usually have multiple clus-ters structure, a good feature selection algorithm should c on-sider the following two aspects: In the remaining part of this section, we will introduce our Multi-Cluster Feature Selection (MCFS) algorithm which con-siders the above two aspects. We begin with a discussion on spectral embedding for cluster analysis with arbitrary shapes.
To detect the cluster (arbitrary shapes) structure of data, spectral clustering techniques [8, 22, 26] received signific ant interests recently. The spectral clustering usually clust ers the data points using the top eigenvectors of graph Laplacian [9], which is defined on the affinity matrix of data points. From the graph partitioning perspective, spectral cluster ing tries to find the best cut of the graph so that the prede-fined criterion function can be optimized. Many criterion functions, such as ratio cut [8], average association [26], and normalized cut [26] have been proposed along with the corre-sponding eigen-problems for finding their optimal solution s.
Spectral clustering has a close connection with the stud-ies on manifold learning [1, 25, 28], which consider the case when the data are drawn from sampling a probability dis-tribution that has support on or near to a submanifold of the ambient space. In order to detect the underlying mani-fold structure, many manifold learning algorithms have bee n proposed [1, 25, 28]. These algorithms construct a nearest neighbor graph to model the local geometric structure and perform spectral analysis on the graph weight matrix. This way, these manifold learning algorithms can  X  X nfold X  the data manifold and provide the  X  X lat X  embedding for the data points. The spectral clustering can be thought as a two-step approach [1]. The first step is  X  X nfolding X  the data manifold using the manifold learning algorithms and the second step is performing traditional clustering (typically k -means) on the  X  X lat X  embedding for the data points [22].

Consider a graph with N vertices where each vertex cor-responds to a data point. For each data point x i , we find its p nearest neighbors and put an edge between x i and its neighbors. There are many choices to define the weight ma-trix W on the graph. Three of the most commonly used are as follows: 1. 0-1 weighting . W ij = 1 if and only if nodes i and j 2. Heat kernel weighting . If nodes i and j are con-3. Dot-product weighting . If nodes i and j are con-If the heat kernel or dot-product weighting is used, some researchers [22] use a complete graph ( i.e ., put an edge be-tween any two points) instead of the p -nearest neighbors graph.

Define a diagonal matrix D whose entries are column (or row, since W is symmetric) sums of W , D ii = P j W ij , we can compute the graph Lapalcian L = D  X  W [9]. The  X  X lat X  embedding for the data points which  X  X nfold X  the data manifold can be found by solving the following generalized eigen-problem [1]: Let Y = [ y 1 , , y K ], y k  X  X  are the eigenvectors of the above generalized eigen-problem with respect to the smallest eig en-value. Each row of Y is the  X  X lat X  embedding for each data point. The K is the intrinsic dimensionality of the data and each y k reflects the data distribution along the correspond-ing dimension (topic, concept, etc .) [1]. When one tries to perform cluster analysis of the data, each y k can reflect the data distribution on the corresponding cluster. Thus, if th e cluster number of the data is known, the K is usually set to be equal to the number of clusters [22].
After we obtain the X  X lat X  X mbedding Y for the data points, we can measure the importance of each feature along each intrinsic dimension (each column of Y ), correspondingly, the contribution of each feature for differentiating each clust er.
Given y k , a column of Y , we can find a relevant subset of features by minimizing the fitting error as follows: where a k is a M -dimensional vector and | a k | = P M j =1 denotes the L1-norm of a k . a k essentially contains the com-bination coefficients for different features in approximating y . Due to the nature of the L1-norm penalty, some coeffi-cients will be shrunk to exact zero if  X  is large enough. In this case, we can select a subset containing the most rele-vant features (corresponding to the non-zero coefficients in a ) with respect to y k . Eq. (2) is essentially a regression problem. In statistics, this L1-regularized regression pr ob-lem is called LASSO [16].

The regression problem in Eq. (2) has the following equiv-alent formulation: The Least Angel Regression (LARs) algorithm [14] can be used to solve the optimization problem in Eq. (3). Instead of setting the parameter  X  , LARs provides another choice to control the sparseness of a k by specifying the cardinality (the number of non-zero entries) of a k , which is particularly convenient for feature selection.

It is very possible that some features are correlated. And ferentiate different clusters. Several supervised feature s e-lection algorithms [2] have been designed to address this issue. Thus, the advantage of using a L1-regularized regres -sion model to find the subset of features instead of evaluatin g the contribution of each feature independently is clear.
We consider selecting d features from the M feature can-didates. For a data set containing K clusters, we can use the method discussed in the previous subsections to compute K sparse coefficient vectors { a k } K k =1  X  R M . The cardinality of each a k is d and each entry in a k corresponds to a feature. If we select all the features that have at least one non-zero coefficient in K vectors { a k } K k =1 , it is very possible that we will obtain more than d features. In reality, we can use the following simple yet effective method for selecting exactly d features from the K sparse coefficient vectors.

For every feature j , we define the MCFS score for the feature as where a k,j is the j -th element of vector a k . We then sort all the features according to their MCFS scores in descending order and select the top d features.

We summarize the complete MCFS algorithm for feature selection in Table (1). Our MCFS algorithm consists of five steps as shown in Table (1). The computational cost for each step can be computed as follows: clusters if evaluated independently
Considering K  X  N and p is usually fixed as a constant 5, the total cost for our MCFS algorithm is:
In this section, several experiments were performed to show the effectiveness of our proposed MCFS for unsuper-vised feature selection. These experiments include clusteri ng and nearest neighbor classification. The following four un-supervised feature selection algorithms (filter methods) ar e compared: 2 If d is very small, this cost can be reduced to O ( dM ) performance by using all the 1024 features. After selecting the features, the clustering and classifica tion are then performed by only using the selected features. Four real world data sets were used in our experiments. The important statistics of these data sets are summarized below (see also Table 2):
Clustering is a common technique for exploratory data analysis. In this experiment, we perform k -means clustering by using the selected features and compare the results of different algorithms.
The clustering result is evaluated by comparing the ob-tained label of each data point using clustering algorithms with that provided by the data set. We use the normalized mutual information metric (NMI) [17] to measure the per-formance. Let C denote the set of clusters obtained from the 3 http://www.csie.ntu.edu.tw/  X  cjlin/libsvmtools /datasets/multiclass.html#usps 4 http://www.ics.uci.edu/  X  mlearn/MLSummary.html performance by using all the 256 features. ground truth and C  X  obtained from a clustering algorithm. Their mutual information metric MI( C, C  X  ) is defined as fol-lows: where p ( c i ) and p ( c  X  j ) are the probabilities that a data point arbitrarily selected from the data set belongs to the cluste rs c that the arbitrarily selected data point belongs to the clus -ters c i as well as c  X  j at the same time. In our experiments, we use the normalized mutual information NMI as follows: where H ( C ) and H ( C  X  ) are the entropies of C and C  X  , re-spectively. It is easy to check that NMI( C, C  X  ) ranges from 0 to 1. NMI= 1 if the two sets of clusters are identical, and NMI= 0 if the two sets are independent.
In order to randomize the experiments, we evaluate the clustering performance with different number of clusters ( K = 10, 20, 30, 40 on ORL; K = 3, 5, 7, 10 on USPS; K = 5, 10, 15, 20 on COIL20 and K = 10, 15, 20, 26 on Isolet). For each given cluster number K (except using the entire data set), 20 tests were conducted on different randomly chosen clusters, and the average performance as well as the standard deviation was computed over these 20 tests. In each test, we applied different algorithms to select d features and applied k -means for clustering. The k -means algorithm was applied 10 times with different random starting points and the best result in terms of the objective function of k -means was recorded.

Fig. (2, 3, 4 and 5) show the plots of clustering perfor-mance versus the number of selected features ( d ) on ORL, USPS, COIL20 and Isolet, respectively. As we can see, our proposed MCFS algorithm consistently outperforms all its competitors on all the four data sets. MCFS converges to the best results very fast, with typically around 50 features . For all the other methods, they usually require more than 200 features to achieve a reasonably good result, as can be seen from Fig. 2  X  5. It would be interesting to note that, on the ORL data set, our proposed MCFS algorithm performs surprisingly well by using only 20 features. For example, in 10 clusters case and only 20 features are selected, the clus-tering normalized mutual information for MCFS is 78.7%, which is even better than the clustering result by using all the 1024 features (76.4%). We can see similar results in 20 and 30 clusters cases. The MaxVariance, LaplacianScore, and Q- X  algorithms perform comparably to one another on ORL data set. On USPS data set, Variance is slightly better than LaplacianScore while LaplacianScore becomes slightl y better than Variance on COIL 20 data set. These two meth-ods also perfom comparably to each other on Isolet data set. The Q- X  performs very bad on USPS and Isolet data sets.
Since the goal of feature selection is to reduce the dimen-sionality of the data, in Table 3  X  6, we report the detailed clustering performance by using 50 features for each algo-rithm. The last column of each table records the average clustering performance over different numbers of clusters. As can be seen, MCFS significantly outperforms the other three methods on all the four data sets. LaplacianScore per-forms the second best on ORL, COIL20 and Isolet data sets. MaxVariance performs the second best on USPS data set. Comparing with second best method, MCFS achieves 10.3%, 5.2%, 10.6% and 10.6% relative improvements in average when measured by normalized mutual information on the ORL, USPS, COIL20 and Isolet data sets, respectively. The last row of each table records the clustering performances by using all the features. performance by using all the 1024 features. performance by using all the 617 features.
Our MCFS has only one parameter, which is the p in p -nearest neighbor graph construction. Figure (6) shows the clustering performance of MCFS versus the number of near-est neighbors (parameter p ). The clustering is performed using the entire data set. The feature selection algorithms select d = 10 , 20 , , 190 , 200 features (20 sets) and the av-erage performance over these 20 feature subsets is shown in the figure. As we can see, the performance of MCFS is very stable with respect to the nearest neighbors parameter p . MCFS achieves consistent good performance with the p varying from 3 to 10 on all the four data sets. When p is larger than 10, the performance slightly decreases as the p increases.

In our MCFS algorithm, we use multiple eigenvectors of eigen-problem in Eq. (1) to capture the multi-cluster struc-ture of the data. To demonstrate the effectiveness and the necessity to use multiple eigenvectors in multi-cluster pr ob-lems, we conduct the clustering experiments by modifying larger than 10, the performance slightly decreases as the p increases. as the number of used eigenvectors is equal to the number of cl usters. MCFS algorithm to use various number of eigenvectors. Fig. (7) shows the average clustering performance versus the num -ber of used eigenvectors. Same to the previous experiment, MCFS selects d = 10 , 20 , , 190 , 200 features (20 sets) and averages the performance over these 20 feature subsets. As we can see, the clustering performance increases rapidly as the number of used eigenvectors increases. MCFS achieves its best performance as the number of used eigenvectors is equal to the number of clusters. In USPS and COIL20 data sets, the performances of MCFS decrease as the number of used eigenvectors keep increasing. While in ORL and Isolet data sets, the performance is relatively stable as the num-ber of eigenvectors increases. This result clearly shows th e effectiveness and the necessity to use multiple eigenvector s in multi-cluster problems. Since the performance drop of MCFS is comparatively small as the number of eigenvectors become larger than the number of clusters, it is not a crucial problem to estimate the number of clusters if it is unknown. Table 7: Classification error rate (%) by using 50 features. The last row shows the error rate by using all the features.

In this subsection, we evaluate the discriminating power of different feature selection algorithms. We consider the nearest neighbor classifier and the good features should yiel d high classification accuracy.

We perform leave-one-out cross validation as follows: For each data point x i , we find its nearest neighbor x  X  i . Let c ( x be the class label of x i . The nearest neighbor classification error rate (ER) is thus defined as where N is the number of data points and  X  ( a, b ) = 1 if a = b and 0 otherwise.

Fig. (8) shows the plots of nearest neighbor classifica-tion error rate versus the number of selected features. As can be seen, on all the four data sets, MCFS consistently outperforms the other three methods. Similar to clustering , MCFS converges to the best result very fast, with no more than 100 features. Particularly, on the COIL20 data set, MCFS can achieve 0.1% classification error rate by using only 50 features. On this data set, the Q- X  algorithm per-forms comparably to our algorithms and much better than MaxVariance and LaplacianScore. On the ORL data set, the LaplacianScore and Q- X  algorithms perform compara-bly to each other, and MaxVariance performs the worst. On the USPS and Isolet data sets, LaplacianScore and Max-Variance algorithms perform comparably and Q- X  performs very bad. Similar to clustering, in Table (7) we show the nearest neighbor classification error rate for each algorit hm using only 50 features. As can be seen, with only 50 features, MCFS achieves comparable results with that using all the features.
The clustering and nearest neighbor classification experi-ments on four real world data sets have been systematically performed. These experiments reveal a number of interest-ing points:
This paper presents a novel unsupervised feature selection algorithm, called Multi-Cluster Feature Selection (MCFS). Inspired from the recent developments on spectral analysis of the data (manifold learning) [1, 22] and L1-regularized models for subset selection [14, 16], we propose to use mul-tiple eigenvectors of graph Laplacian, which is defined on th e affinity matrix of data points, to capture the multi-cluster structure of the data. Thus, MCFS can well handle multi-cluster data. In comparison with one simple method, that is, MaxVariance, and two state-of-the-art methods, namely, LaplacianScore and Q- X  , the experimental results validate that the new method achieves significantly higher perfor-mance for clustering and classification. Our proposed MCFS algorithm performs especially well when the number of se-lected features is less than 50.
 This work was supported in part by National Natural Sci-ence Foundation of China under Grants 60905001 and 90920303, National Key Basic Research Foundation of China under Grant 2009CB320801. Any opinions, findings, and conclu-sions expressed here are those of the authors and do not necessarily reflect the views of the funding agencies. [1] M. Belkin and P. Niyogi. Laplacian eigenmaps and [2] J. Bi, K. Bennett, M. Embrechts, C. Breneman, and [3] S. Boutemedjet, N. Bouguila, and D. Ziou. A hybrid [4] C. Boutsidis, M. W. Mahoney, and P. Drineas.
 [5] D. Cai. Spectral Regression: A Regression Framework [6] D. Cai, X. He, and J. Han. Spectral regression: A [7] D. Cai, X. He, and J. Han. Sparse projections over [8] P. K. Chan, D. F. Schlag, and J. Y. Zien. Spectral [9] F. R. K. Chung. Spectral Graph Theory , volume 92 of [10] C. Constantinopoulos, M. K. Titsias, and A. Likas. [11] T. M. Cover and J. A. Thomas. Elements of [12] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern [13] J. G. Dy and C. E. Brodley. Feature selection for [14] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. [15] M. A. Fanty and R. Cole. Spoken letter recognition. In [16] T. Hastie, R. Tibshirani, and J. Friedman. The [17] X. He, D. Cai, and P. Niyogi. Laplacian score for [18] J. J. Hull. A database for handwritten text [19] R. Kohavi and G. H. John. Wrappers for feature [20] M. H. C. Law, M. A. T. Figueiredo, and A. K. Jain. [21] H. Liu and L. Yu. Toward integrating feature selection [22] A. Y. Ng, M. Jordan, and Y. Weiss. On spectral [23] J. L. Rodgers and W. A. Nicewander. Thirteen ways [24] V. Roth and T. Lange. Feature selection in clustering [25] S. Roweis and L. Saul. Nonlinear dimensionality [26] J. Shi and J. Malik. Normalized cuts and image [27] G. W. Stewart. Matrix Algorithms Volume II: [28] J. Tenenbaum, V. de Silva, and J. Langford. A global [29] L. Wolf and A. Shashua. Feature selection for [30] Z. Zhao and H. Liu. Spectral feature selection for
