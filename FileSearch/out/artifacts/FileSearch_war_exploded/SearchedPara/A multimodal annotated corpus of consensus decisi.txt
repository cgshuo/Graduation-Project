 Fabio Pianesi  X  Massimo Zancanaro  X  Bruno Lepri  X  Alessandro Cappelletti Abstract In this paper we present an annotated audio X  X ideo corpus of multi-party meetings. The multimodal corpus provides for each subject involved in the exper-imental sessions six annotation dimensions referring to group dynamics; speech activity and body activity. The corpus is based on 11 audio and video recorded sessions which took place in a lab setting appropriately equipped with cameras and microphones. Our main concern in collecting this multimodal corpus was to explore the possibility of providing feedback services to facilitate group processes and to enhance self awareness among small groups engaged in meetings. We therefore introduce a coding scheme for annotating relevant functional roles that appear in a small group interaction. We also discuss the reliability of the coding scheme and we present the first results for automatic classification.
 Keywords Behaviour analysis Small groups Meetings Multimodality 1 Introduction Meetings are more and more important in structuring daily work in organizations. Executives on average spend 40 X 50% of their working hours in meetings (Doyle and Strauss 1993). However, the success of a meeting is often hindered by the participants X  behaviour: professionals agree that as much as 50% of meeting time is unproductive and that up to 25% of meeting time is spent discussing irrelevant issues (Doyle and Strauss 1993).

Starting from these premises, many projects are currently attempting to provide working teams with better supports for planning and managing meeting, in order to increase their productivity. For instance, the CHIL project (Waibel et al. 2004 ) aims at systems offering better ways for connecting people (the Connector service) and supporting human memory (the Memory Jog). The AMI project (McCowan et al. 2004 ) focuses on off-line multimedia retrieval and multimedia browsing of information obtained from meetings. The DARPA-funded project CALO supports a group in creating a project schedule by automatically interpreting gestures and speech, including the learning of new words (Kaiser et al. 2004 ).

Other efforts, acknowledge the importance of the quality of social interaction in meeting, both for increased productivity and for a higher satisfaction of members, this way also taking into account team building as a major goal (Pianesi et al. 2007). In these cases, inspiration is often taken from such practices as facilitation and training experiences (coaching), commonly employed to improve meeting perfor-mance by manipulating social interaction. With facilitation, experts participate in the meetings as external elements helping participants maintaining a fair and focused behaviour, as well as directing and setting the pace of the discussion. Coaching aims at increasing the relational skills of individual participants by providing an offline (with respect to meetings) guidance (coaching); the underlying idea is that the team will eventually be able to overcome, or cope with, its own disfunctionalities, thanks to the improved relational skills of its members.
Systems like those must be capable of understanding relevant abstract features of human interaction in groups, dynamically modelling them, to use to affect group relational behaviour. Examples of these group characteristics are the influence and dominance addressed by Rienks et al. ( 2006 ) and Zhang et al. ( 2006 ). Brdiczka et al. ( 2005 ) target group configurations, defined in terms of conversational hypotheses, by resorting to Hidden Markiv Models. Otzuka et al. ( 2005 ) infer conversation structure within a participation framework, by relying on gaze patterns, head direction and utterance.

In our own work, we have targeted group functional roles, using them to produce individualized reports for meeting participants, as a first step towards automatic coaching aiming at enhancing reflexive thinking and self-awareness among small groups (Pianesi et al. 2007). Falcon et al. ( 2005 ) and Pianesi et al. ( 2006 ) presented results from studies on user acceptance of a simulation of the automatic relational report, finding no differences with respect to reports produced by human experts. In this work we describe the Mission Survival Corpus, the multimodal corpus that was developed and used to experiment with the system producing relational reports. Moreover, we discuss some of the initial results on the automatic classification of relational roles.

Several other multimodal corpora have recently been already developed to analyse meetings. In particular the MM4 corpus (McCowan et al. 2004 ) and the VACE corpus (Chen et al. 2005 ) are close to the one proposed here since they annotated low-level cues, such as speech, gesture, posture, and gaze in order to interpret high level meeting events. Similar considerations hold for the AMI corpus described in (Rienks et al. 2006 ). The Survival Corpus described in this paper is different from the corpora above because it was built in a controlled but not scripted way. That is, although the groups X  composition and the task were determined, the actual behaviour of the participants was spontaneous. In this way, the Survival Corpus provides a range of natural interactions without the excessive variability that would characterize a corpus of everyday teams X  meetings. 1.1 Purpose and organization of the paper The main goal of the paper is presenting the Mission Survival Corpus, a multimodal corpus collected to support the task of automatic classifying the relational behaviour of individual involved in decision making meetings.

Section 2 introduces the Functional Role coding scheme which was used to manually annotate the relational behaviour of individuals as a ground truth for the automatic classification tasks. This section describes the theoretical basis of the scheme as well as its reliability in terms of both inter-annotator agreement and disagreements analysis. This analysis was conducted on a different corpus (made up with spontaneous meetings in a research lab) in order to tune the scheme and to train the annotators before preparing the main corpus.

Section 3 presents the setting and the procedure used to collect the corpus. It is worth noting that since the Mission Survival Corpus was meant to be used for training algorithms for automatic classification of social behaviour from acoustical and visual scene analysis, the need of high quality recordings was paramount.
Section 4 discusses a number of features which have been used to automatically annotate the corpus, and the distribution of the different roles of the coding scheme.
Finally, Sect. 5 presents some initial results on the task of automatic classifi-cation of roles. Role assignment was modelled as a multiclass-classification problem on a relatively large and very unbalanced dataset. Support Vector Machines were used as classifier. 2 The functional role coding scheme In our search of suitable categories for the coding scheme, the goal of presenting individual profiles to participants suggested that we carefully consider those approaches to social dynamics that focus on the roles that members play inside the group. Among the available notions of group members X  roles (Salazar 1996 ), those defining them in terms of the behaviour enacted in the relevant context was of particular interest to us. Indeed, it moves away from a strictly organizational perspective whereby roles are defined by the social positions within the group, or within the involved organizations, and it differs from approaches defining roles according to the social expectation associated with a given position (Katz 1978 ). What we wanted was a definition of functional roles based on information about what actually happened in the course of the interaction, and which reduces the resort to knowledge about the group X  structure, history, position in the organization, etc.
Benne and Sheats ( 1948 ) provided a list of  X  X  X unctional roles X  X  recognizable in working groups, which are based on the behaviour enacted during the meeting. They divide them into three areas: task-oriented, maintenance-oriented, and individual-oriented. The first two kinds of roles are directed towards the group X  X  need: task-oriented roles provide facilitation and coordination, while maintenance roles contribute to structure and preserve interpersonal relations, in order to reduce tensions and maintain smooth group functioning. Roles of the third type, the individual ones, are based on behaviour performed in order to satisfy individual need and accomplish individual goals, rather than the group X  X  ones. At each instant, during the interaction, each person plays one role along each dimension and the role played can change in time.

Starting from the model proposed by Benne and Sheats X  model, Bales ( 1970 ) proposes Interaction Process Analysis (IPA): a framework to study small group interaction, by classifying functions in face-to-face interaction in a two-dimensional space based only on the Task and the Socio-Emotional dimensions. In this perspective, 12 functions needed for the internal equilibrium of the group are introduced, e.g. the Show Solidarity function of the Social-Emotional Area is performed to raise other members X  status, giving them help and rewarding. Bales X  functions are discriminated in terms of the frequency of performance of the smallest verbal and non-verbal acts, hence they differ from from Benne and Sheats X  roles in that they tend to be more dynamic.
 We decided to employ Bales X  categories, given the wide acceptance of Interaction Process Analysis, while interpreting his functions as (functional) roles in terms of Benne and Sheats X  approach. This move was motivated by the expectation that the behaviour of each participant would not change too often during the meeting, hence the more static concept of a functional role should be more appropriate than the dynamic concept of function. Finally, we further adapted the resulting two-dimensional scheme, adjusting the roles according to observations performed on a number of face-to-face meetings.
 Our Functional Role Coding Scheme (FRCS) consists of five labels for the Task Area and five labels for the Socio Emotional Area. The Task Area includes functional roles related to facilitation and coordination of the tasks the group is involved in, as well as to the technical skills of the members as they are deployed in the course of the meeting. The Socio Emotional Area involves roles oriented toward the functioning of the team as a group. Below we give a synthetic description of FRCS.
 Task Area Functional Roles  X  Orienteer (o). S/he orients the group by introducing the agenda, defining goals  X  Giver (g). S/he provides factual information and answers to questions. S/he  X  Seeker (s). S/he requests suggestions and information, as well as clarifications,  X  Recorder (r). S/he uses the resources available to the group, managing them for  X  Follower (f). S/he only listens, without actively participating in the interaction. The Socio-Emotional Functional Roles
Attacker (a). S/he deflates the status of others, expresses disapproval, attacks the group or the problem.

Gate-keeper (gk). S/he is the group moderator, who mediates the communicative relations; s/he encourages and facilitates the participation and regulates the flow of communication.

Protagonist (p). S/he takes the floor, driving the conversation, assuming a personal perspective and asserting her authority.

Supporter (s). S/he shows a cooperative attitude demonstrating understanding, attention and acceptance as well as providing technical and relational support.
Neutral (n). S/he passively accepts the idea of others, serving as an audience in group discussion.

Of course, participants may X  X nd often do X  X lay different roles during the meeting, but at a given time each of them plays exactly one role in the Task Area and one role in the Socio-Emotional one. 2.1 Studies on the reliability of the coding scheme In order to test, and improve, the reliability of the annotation scheme before applying to the Mission Survival Corpus, we exploited a corpus consisting of the video and audio recordings of nine group meetings (selected from real meetings held at our place), for a total of 12.5 h. Two trained annotators labelled a subset of the corpus consisting of 130 min of meetings from three group interactions. Five participants were coded on the Socio-Emotional Area and five in the Task Area. Cross-judge consistency of class membership was assessed by means of Cohen X  X  j (Cohen 1960), computed on the confusion matrices resulting from the annotation exercise.

In the Task Area, Cohen X  X  statistics was j = 0.70 ( N = 758, SE = 0.02, p &lt; .001; confidence interval with a = 0.05: 0.67 X 0.75). According to Landis and Table 1 shows the confusion matrix (the table shows the occurrences of the different roles at sampling of 10 s).

Figure 1 shows the percentage of the different roles as they occurred in our corpus. The Orienteer is the most common role reflecting the nature of the interactions observed, which were mostly project meetings where teams had to report to their project managers about the status of the work.

Regarding the Socio-Emotional Area, the inter-annotator agreement was j = 0.60 ( N = 783, SE = 0.02, p &lt; .001; confidence interval with a = 0.05: 0.56 X 0.65). According to Landis and Koch X  X  ( 1977 ) criteria, the agreement on the Socio-Emotional roles is at the borderline between good (0.6 &lt; j &lt; 0.8) and moderate (0.4 &lt; j &lt; 0.6). Table 2 shows the confusion matrix (the table shows the occurrences of the different roles at sampling of 10 s).
 Figure 2 shows the relative percentage of the different roles in the Socio-Emotional area in our corpus. It can be noted that the Gate-Keeper role was never observed by either annotator, this being probably due to the actual absence of (either professional or de facto) facilitators in our meetings. The Attacker too is not well represented. Again, this reflects the nature of our meetings which do not favour the emergence of strong contrasts among participants. 2.2 Analysis of the disagreements The class-wise analysis of the j  X  X  for the Task Area shows that the most reliable classes are the Orienteer and the Recorder (see also the values of the z -scores in Table 3 ). The least reliable class is the Seeker , mostly because of its high standard error. The Giver and the Follower fall in between. Considering the absolute values of the j  X  X  and the lower bounds of the confidence intervals ( a = 0.05), the classes that deserve consideration in view of improvements are the Seeker and the Follower .

The use of Pearson X  X  standardised residuals from the independence model, enable us to pinpoint the disagreements that more closely follow a uniform pattern, hence those on which the judges diverge most. In these cases, the standardized residuals come close to zero. The data, reproduced in Table 4 , confirm the results based on the j statistics: almost all off-diagonal residuals are strongly negative, and often below the value of -3 that can be taken as a cut-off threshold for significance.
The most interesting disagreements between the two annotators concern two cases: in the first, judge 1 classifies roles as Giver and judge 2 classifies them as Follower; in the second, judge 1 sees a Orienteer role whereas judge 2 classifies it as a Seeker . Putting together these results with the discussion of Table 3 above, it can be concluded that in order to improve inter-annotators agreement in the task area, we must address, in the first place, the Seeker and the Follower , in particular reducing the  X  giver-follower  X  and the  X  orienteer-seeker  X  disagreement Turning to the Socio-Emotional Area, the class-wise analysis of the j  X  X , in Table 5 , confirms that the social area is slightly less reliable than the task one (see also Table 6 ). The most reliable class is the Protagonist , and the by far less reliable one is the Supporter ; the Attacker , despite its high j value, needs some consideration, given its high standard error.

The analysis of Pearson X  X  standardized residuals from the independence model shows the importance of the disagreement on Neutral and Supporter between judge 1 and judge 2, see Table 6 .

In conclusion, the weakest class in the social area is the Supporter , which is involved in a strong disagreement with the Neutral .

Finally, an important feature of coding schemes is the symmetry of their confusion matrices. In a perfectly symmetric confusion matrix, for labels a and b , any a vs. b disagreements between judge 1 and judge 2 correspond to a b vs. a disagreements between judge 2 and judge 1. Symmetry can be assessed through the Bowker test (Agresti 2002), which yields a statistics that has asymptotic v 2 distribution. In our case, the value of the Bowker statistics is 75.14 and 69.59 for the task and the social area, respectively, with 10 and 6 degree of freedom. In both cases, the null hypothesis that the matrices are symmetric can be rejected with p &lt; .0001. Tables 7 , 8 report the standardized residuals under the symmetry hypothesis.

The analysis of Pearson X  X  standardized residuals under the symmetry hypothesis confirms that the offending cases are the same as those analysed above in connection with the independence hypothesis. In detail, the  X  giver X  X ollower  X  and the  X  orienteer X  X eeker  X  disagreements are the main culprits of the lack of symmetry in the task area, whereas the  X  neutral X  X upporter  X  disagreement is the main responsible for the lack of symmetry in the social area.

To improve agreement, efforts had to be focused on the giver  X  follower and the recorder disagreements in the Task Area, with the goal of improving the j values for the follower and the seeker , respectively, and the balance/symmetry of the annotation schema. In the Socio-Emotional Area, the validity of the annotation schema could be ameliorated by reducing the neutral  X  supporter disagreements, improving the j value of supporter and the overall schema balance.

A new set of guidelines for annotators were compiled, resulting in a uniform improvement of the j statistics. 3 Methods and procedures for data collection Our multimodal annotated corpus is based on the audio and video recorded during 11 meetings, which took place in a lab setting appropriately equipped with cameras and microphones (see below). In order to provide for as much uniform context as possible, our groups were engaged in the solution of one of two versions of the Survival Task. 3.1 Interaction context X  X he Survival Task The Survival task is frequently used in experimental and social psychology to elicit decision-making processes in small groups. Originally designed by National Aeronautics and Space Administration (NASA) to train astronauts before the first Moon landing X  X he Survival Task proved to be a good indicator of group decision making processes [8]. The exercise consists in promoting group discussion by asking participants to reach a consensus on how to survive in a disaster scenario, like moon landing or a plane crashing in Canada. The group has to rank a number (usually 15) of items according to their importance for crew members to survive.
The consensus decision making scenario was chosen for the purpose of meeting dynamics analysis mainly because of the intensive engagement requested to groups in order to reach a mutual agreement, thus offering the possibility to observe a large set of social dynamics and attitudes. In consensus decision making processes, each participant is asked to express her/his opinion and the group is encouraged to discuss each individual proposal through weighing and evaluation of decision quality.

In our setting, we retained the basic structure of the Survival Task. In particular, (a) the task was competitive across groups/team, with a price being awarded to the group providing the best survival kit. (b) The task was collaborative and based on consensus within the group, meaning that a participant X  X  proposal became part of the common sorted list only if s/he managed to convince the other of the validity of his/her proposal. 3.2 Experimental protocol Before starting each recording sessions, participants were given general information about the task filled a consent form. Each participant was equipped with close-talk microphones and asked to sit around a round-shaped table without restrictions concerning their positions and movements around the table; see Fig. 3 .
A document was given to the group containing the items that were the objects of the discussion, and the instructions concerning the task. The experimenter sat in the room away from the table, without participating to the discussion, and collecting information and observations on an experimental sheet.

All the participants (40% males and 60% females) involved in the study were clerical people working at ITC-irst. In all cases they knew each other, and had often been involved in common group activities in the past. The average age was 35 years. 3.3 Setting and recording procedure Each session was recorded in the specially-equipped CHIL room at ITC-irst (see Fig. 4 ), by means of five Firewire cameras (AVT MARLIN), four placed on the four corners of the room while one was hung at the ceiling and aimed at the table. Four Web cameras (SONY SNC-RZ30P) were installed on the walls surrounding the table.

Speech activity was recorded using four closed-talk microphones, six tabletop microphones and seven T-shaped microphone arrays, each consisting of four omni directional microphones installed on the four walls in order to obtain an optimal coverage of the environment for speaker localization and tracking. 4 The corpus Eleven groups of four people each were recorded. The average duration was 25 min, the range being 0.13.08 00  X 0.30.06 00 . The total length of the audio X  X ideo corpus is 3.44.55 00 h (Table 9 ).
 4.1 Data annotation Currently, the following annotations are available for the data: functional relational roles (task roles and socio-emotional roles), which address facets of the group dynamics; speech activity; body activity (head position, head orientation and fidgeting activity). Each subject is identified according to the cardinal points (N, S, E, W).

In the following, we describe for each category the procedures and the annotation output. 4.1.1 Functional roles Functional roles were manually annotated for each participant by considering the participants X  behaviour every 5 s. In order to maintain consistency, a single annotator (one of the two involved in the exercise described in Sects. 2.1 and 2.2) coded the whole corpus. The results were tuples role -type ; participant -code ; role -code ; start: h start -time ; end: end -time ; duration: duration i ; see Fig. 5 . For instance, the tuple of orienteer ( X  X  X ) belonging to the  X  X ask X  area, as played by participant w from time 621.466 till time 645.965, for a duration of seconds 24.499023.

Functional roles annotations were then re-sampled every 330 ms to align them with the other features, see below. The resulting corpus was quite unbalanced (see Table 10 ): Expectedly, Follower and Neutral were the most frequent roles, while Attacker was quite rare, probably due to the fact the participants knew they were observed, hence tending to avoid aggressive or uncooperative behaviour. Recorder and Gate-Keeper roles were never observed. 4.1.2 Speech activity Speech activity here refers to the identification of the presence or absence of human speech, without distinguishing between verbal and non-verbal activity.

Each session was segmented by first automatically labelling the speech activity recorded by the close-talk microphones. The voice activity detector (VAD) is based on the time energy of the signal (Carli and Gretter 1992 ). For each speaker, VAD identifies the amount of speech activity, and produces an output such as participant -code ; start time ; end time ; label hi ; where label takes on the value  X  X peech X  and  X  X o-speech X .

VAD X  X  output was then manually checked and improved. In the first place, errors of the automatic annotation were removed; in particular, since subjects were close to each other, the speech activity of a subject often entered the close-talk microphone of the subject sitting nearby, giving raise to a wrong assignment.

Secondly, VAD is based on time energy, and it may not be correct in distinguishing between verbal activity and other acoustic non-verbal events. Manual annotation, performed by a human annotator, purified the VAD annotation from breaths, yawns, coughing, and noises caused by the subjects when touching the microphones. Laughs were retained and annotated by means of the additional label la . 4.1.3 3D tracking of body activity Visual cues were employed to derive head position and orientation as well as body activity. 4.1.3.1 Head position The subjects X  position in the room is tracked through head position identification. All of the 3D positions have an absolute timestamp and are referenced to an origin which is on the floor under the centre of the table. The 3D co-ordinate system for the room is oriented in the following way: X axis represents a Westerly direction, Z axis represents a Northerly direction, Y is the height from the floor. For each participant the 3D tracking produces a tuple timestamp ; x axis ; h z axis ; y axis i ; where an absolute timestamp is followed by the cardinal point which identifies head position in the room. An example of the output is presented in Table 11 . 4.1.3.2 Head orientation Stiefelhagen and colleagues ( 2002 ) estimated the potential of head orientation in detecting who is looking at whom in around-a-table setting. Starting from head position detection, colour and hedge features were used to track head orientation and to estimate focus of attention.

The output from the 3D tracking consists for each subject of tuples such as timestamp ; head orientation hi : Head orientation can take on one of the following values:  X  X  X own X  X , when subject head is oriented toward the table,  X  X  X  X  X ,  X  X  X  X  X ,  X  X  X  X  X ,  X  X  X  X  X , when the head is oriented toward South, North, West or East, each of them referring to one of the other participants; see Fig. 6 . 4.1.3.3 Fidgeting Fidgeting refers to localised repetitive motions such as when the hand remains stationary while the fingers are tapping the table, or playing with glasses, etc.

Fidgeting has been tracked by using skin region features and an MHI of the convex skin polygons and temporal motion as the trigger is used. For a more detailed description see Chippendale ( 2006 ).
 timestamp ; fidgeting energy ; hand = arm activity hi : An example of the output is the following: 1124358961419507 ; 16 ; 1 hi ; in which an absolute timestamp is followed by two normalised fidgeting values. The first ( X 16 X ) represents the fidgeting energy of the person X  X  body and the second ( X 1 X ) represents his hand/arm activity. The normalised values are referenced to that person X  X  most vigorous fidgeting during the entire recorded sequence, hence they are person specific. 5 Automatically classifying functional roles The main purpose of the Mission Survival Corpus was to be used for building systems for the automatic analysis of social behaviour. We performed experiments of automatic classification of the functional roles played by each participant in order to establish a baseline for future development. As a starting point, we considered only speech activity and body activity as predicting features. At this point the corpus consisted of 107608 rows each reporting the speech activity of one of the participants during a 330 ms interval, his/her hands and body fidgeting, the number of people speaking during that time, and the functional roles the person plays (see Fig. 7 ).
Given the lack of balance, the corpus was first reduced by considering only the cases corresponding to time intervals where the targeted participant was speaking. This lowered the impact of the Follower and Neutral roles, even if the datasets remained unbalanced (see Table 12 ).

To take into account the time dimension in the analysis we employed sliding windows (Dietterich 2002 ): for each time stamp, the classifier processes all the data comprised in the time window to assign a Task area role and a Socio-Emotional area one. We considered windows of varying size, from 0 to 14 s (i.e. 42 rows), placed both to the left the relevant time point, and windows centred on the latter. Initial attempts showed that centred windows are less effective (Zancanaro et al. 2006 ); hence in this paper we report only on the results from left windows.

For each window size, we built two datasets. For a given time and a given participant, the first included the information about his/her speech and fidgeting activity, as well as the number of simultaneous speakers, during the window time. The second one included all the above plus the information about speaking activity and the fidgeting of all the other participants.

Each dataset was then split in two equal parts, the first to be used for training and the second for testing.

We modelled role assignment as a multiclass-classification problem on a relatively large and very unbalanced dataset, and used Support Vector Machines as classifier, because of their robustness with respect to over-fitting (Cristianini 2000 ).
The bound-constrained SV classification algorithm with a RBF kernel was used (we employed the BSVM tool; Hsu and Lin 2002 ). The cost parameter C and the kernel parameter c were estimated through the grid technique by means of cross-fold validation using a factor of 10. Given the computational costs of this procedure, we estimated the parameters for the windows 0, 21 and 32 only. The parameters estimated for the window 0 were also used for the windows from 1 to 3. Similarly, the parameters estimated for the window 21 were also used for the windows from 4 to 27; and parameters estimated for the window 32 also for the windows from 30 to 42. Furthermore, the cost parameter C was weighted for each class with a factor inversely proportional to the class size.

SVM were originally designed for binary classification but several methods have been proposed to construct multi-class classifier (Hsu and Lin 2002 ). We used the  X  X  X ne-against-one X  X  method (Kressel 1999 ), whereby each training vector is compared against two different classes by minimizing the error between the separating hyperplane margins. Classification is then accomplished through a voting strategy whereby the class that most frequently wins is selected.

To provide baselines for comparisons, we used: the trivial classifier  X  X hat assigns all instances to the most populated class X  X nd the equidistributed classifier  X  X hat distributes the instances according to the prior probabilities of the classes.
Accuracy is known to be a somewhat inadequate measure of performance for unbalanced datasets, because the trivial classifier always has very high accuracy. Therefore, we used both accuracy and F -score as figures of merit, where the latter is computed as the harmonic mean of the macro-averaged one-class precisions and recalls (macro F -score). 5.1 Task area roles The accuracy values for the different windows in the two datasets are compared in Fig. 8 to the baselines.

While the classifier trained on the minimal dataset (i.e. the one containing the participant X  X  features only) improves over both baselines from windows of size 21 (7 s) up, the one trained on the features for all the participants is always above the baseline.

Focusing on the latter, performance starts from quite high values for window of size 0 (accuracy = 0.76, F -score = 0.69), then drops until window of size 12 (4 s; accuracy = 0.78, F -score = 0.69), where from the values of both figures are stably higher than for the 0-sized window. One might conjecture that contextual time information is only useful when enough temporal context is considered. The drop in performance for windows of size 30 is probably due to the way the SVM classifier parameters were estimated and applied; the 3-and 30-sized windows are, in fact, the lower bounds of the window intervals to which parameters estimated with windows 21 and 42 are applied, respectively (Fig. 9 ).

The highest accuracy is reached with window 27 (with a value of 0.90). The max value of macro F is 0.87 and is reached at the largest window size (42, 14 s). We prefer to consider window 42 since we value macro F as a better measure of accuracy on our corpus. Table 13 summarizes the precision and recall values for the Task Area roles on that window. 5.2 Socio area roles with left-only windows A very similar pattern is apparent for the roles of the Socio-Emotional Area as plotted in Figs. 10 , 11 .

The classifier trained on the minimal data set exceeds the baseline on accuracy from window size 12 while the accuracy of the classifier trained on the augmented dataset is always higher.

The pattern of the augmented classifier is virtually identical to that discussed for the task area, including the drop around the 3-and 30-sized windows, and the maximal values, which are reached with window of size 27 for accuracy (0.92), and windows size 42 for macro-F score (0.86).

Table 14 summarizes the precision and recall values for the Socio Area roles on window 42 when the macro-F score reaches its maximum.
 6 Conclusion We presented in this paper a multimodal corpus of annotated consensus decision making meetings called the Survival Corpus. The corpus provides for each subject six annotation levels: manual annotation of the participant X  X  functional role and speech activity, and automatic annotation of body activity, head position and orientation and fidgeting activity. The functional roles were annotated on the basis of a coding scheme, the FRCS, which was inspired to Benne and Sheats ( 1948 ) and Bales ( 1970 ) work. Its psychometric properties (intercoder agreement) were studied, and the results were used to improve the annotation procedure.

The corpus was used to conduct experiments on the automatic detection of functional roles, some of which are described in this paper. The performance of the classification for the Task area roles is good with a macro F of 0.89 for window size macro F reaches 0.89. In both cases, the accuracy was consistently increasing with the window width. This might mean that a state-based model (like HMM) based on accumulation of information may eventually lead to better results than a fixed window. Of course the generalizability of these results require further work but they proved that automatic monitoring of group behaviour is a challenging but feasible goal for IT.

Another direction to further improve the automatic classification is taking into consideration more features. We chose to use two simple features of the audio-visual scene: speakers X  activity and body energy. The reason is that we aimed at setting a baseline before investigating a richer set of features in view of the fact that for real applications less features not only mean a simpler and cheaper system but also help increasing acceptability. In the next step, we plan to add a few more features starting from vocal energy, 3D postures and focus of attention and to analyse which of these features have more impact for the automatic detection of group roles.

The Survival Corpus can provide a good starting point to study group behaviour in small group interaction but it lacks any measure of how good or bad were the meetings. We are now starting to collect a new corpus, the Survival Corpus II, where we will pay more attention to these kinds of measurements considering both performance measures X  X hich in the case of the Survival Task is how many items the group sorts out correctly X  X nd subjective measures of group cohesion such as group self-efficacy.
 References
