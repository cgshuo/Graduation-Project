 Spemannstr . 35-39, 72070 T  X  ubingen, German y Ten years ago, an eight-year lasting collaborati ve effort resulted in the first completely sequenced genome of a multi-cellular organism, the free-li ving nematode Caenorhabditis ele gans . Today, a decade after the accomplishment of this landmark, 23 eukaryotic genomes have been completed and more than 400 are underw ay. The genomic sequence builds the basis for a large body of research on understanding the biochemical proces ses in these organisms. Typically , the more closely related the by analyzing a wide spectru m of model organisms, one can approach an understanding of the full biological comple xity. For some organisms, certain biochemical experiments can be performed more a fraction of the original cost. This is but one e xample of a situation where transfer of knowledge across domains is fruitful.
 In machine learning, the above information transfer is called domain adaptation , where one aims to use data or a model of a well-analyzed sour ce domain to obtain or refine a model for a less analyzed tar get domain . For supervised classification, this corresponds to the case where there are ( x i , y i ) , i = m + 1 , . . . , m + n for the target domain ( n  X  m ). The examples are assumed to be in several ways: P ( X ) varies between the two domains: P S ( X ) 6 = P T ( X ) . The conditional, however, remains the domain from which the example stems. An example thereof would be if a funct ion of some biological material is conserv ed between two organisms, but its composi tion has changed (e.g. a part of a chromosome has been duplicated). while P ( X ) may or may not vary. This is the more common case in biology . Here, two organisms may have e volved from a common ancestor and a certain biolo gical function may have changed due to evolutionary pressures. The evolutionary distance may be a good indicator for how well may not be completely different, and knowledge of one of them should then provide us with some information also about the other one.
 While such knowledge transfer is crucial for biology , and performed by biologists on a daily basis, surprisingly little work has been done to exploit it using machine learning methods on biological databases. The present paper attempts to fill this gap by studying a realistic biological domain transfer problem, taking into account several of the relevant dimensions in a common experimental frame work: With the above in mind, we selected the problem of mRN A splicing (see Figure A1 in the Appendix for more details) to assay the above dimensions of domain adaptation on a task which is relevant to modern biology . The paper is organized as follows: In Section 2, we will describe the experimen-tal design including the datasets , the underlying classification model, and the model selecti on and evaluation procedure. In Section 3 we will briefly review a number of known algorithms for domain adaptation, and propose certain variations. In Section 4 we show the results of our comparison with a brief discussion. 2.1 A Family of Classificati on Problems We consider the task of identifying so-called acceptor splice sites within a large set of potential splice sites based on a sequence windo w around a site. The idea is to consid er the recognition of splice sites in different organisms: In all cases, we used the very well studied model organism C. ele gans as the source domain. As target organisms we chose two additional nematodes , namely , more distantly related P. pacificus , a lineage which has diverged from C. ele gans more than 200 million years ago [7]. As a third target organism we used D. melano gast er , which is separated from C. elegans by 990 million years [11]. Finally , we consider the plant A. thaliana , which has diverged from the other organisms more than 1,600 million years ago. It is assumed that a larger evolutionary distance will likely also have led to an accumul ation of functional differences in the molecular splicing machinery . We therefore expect that the differences of classification functions for recognizing splice sites in these organisms will increase with increasing evolutionary distance. 2.2 The Classification Model It has been demonstrated that Support Vector Machines (SVMs) [1] are well suited for the task of splice site predictions across a wide range of or ganisms [9]. In this work, the so-called Weighted De gree kernel has been used to measure the similarity between two example sequences x and x  X  of fixed length L by counting co-occurring substr ings in both sequences at the same position: substring lengths.
 In our previous study we have used sequences of length L = 140 and substr ings of length  X  = 22 cation performance, a large number of training examples is very helpful ([9] used up to 10 million examples).
 For the designed experim ental comparison we had to run all algorithms many times for different training set sizes, organisms and model parameters. We chose the source and target training set as large as possible X  X n our case at most 100,000 examples per domain. Moreo ver, not for all algorithms we had efficient implem entations available that can make use of kernels. Hence, in order to perform this study and to obtain comparable results, we had to restrict ourselv es to a case were we can explicitly work in the feature space, if necessary (i.e.  X  not much larger than two). We chose  X  = why efficient implementations that employ k ernels could not be de veloped for all methods. The development of large scale methods, however, was not the main focus of this study .
 Note that the above choices required an equivalent of about 1500 days of computing time on state-of-the-art CPU cores. We theref ore refrained from including more methods, examples or dimensions. 2.3 Splits and Model Selection In the first set of experim ents we randomly selected a source dataset of 100,000 examples from C. ele gans , while data sets of sizes 2,500, 6,500 , 16,000, 40,000 and 100,000 were selected for each target organism. Subsequently we performed a second set of experiments where we combined several sources. For our comparison we used 25,000 labeled examples from each of four remaining organisms to predict on a target organism. We ensured that the positi ves to negatives ratio is at for evaluation in the course of hyper-parameter tuning. 1 Additionally , test sets of 60,000 examples were set aside for each target organism. All experiments were repeated three times with different will be the average area under the precision-recall-curv e (auPRC) and its stand ard deviation, which is considered a sensible measure for imbalanced classification problems. The data and additional information will be made available for download on a supplementary website. 2 Regarding the distrib ution al view that was presented in Section 1, the problem of splice site pre-P
T ( Y | X ) , which is also the most realistic scenario in the case of modeling most biological pro-different predicti ve function s P S ( Y | X ) 6 = P T ( Y | X ) . 3.1 Baseline Methods ( SVM S and SVM T ) As baseline methods for the comparison we consider two methods: (a) training on the source data only (SVM S ) and (b) training on the target data only (SVM T ). For SVM S we use the source data for training however we tune the hyper-parameter on the available target data. For SVM T we use 3.2 Convex Combination ( SVM S +SVM T ) The most straightforw ard idea for domain adaptation is to reuse the two optimal functions f T and f S as generated by the base line methods SVM S and SVM T and combine them in a convex manner: Here,  X   X  [0 , 1] is the convex combination parameter that is tuned on the evaluation set ( 33% ) of the target domain. A great benefit of this approach is its efficienc y. 3.3 Weighted Combination ( SVM S + T ) Another simple idea is to train the method on the union of source and target data. The relati ve importance of each domain is integrated into the loss term of the SVM and can be adjusted by setting domain-dependent cost parameters C S and C T for the m and n training examples from the source and target domain, respectively: This method has two model parameters and requires training on the union of the training sets. Since the computation time of most classification methods increases super -linearly and full model se-lection may require to train many parameter comb inations, this approach is computationally quite demanding. 3.4 Dual-task Lear ning ( SVM S,T ) One way of extending the weighted combination approa ch is a variant of multi-task learning [2]. The idea is to solve the source and target classification problems simultaneously and couple the two solutions via a regularization term. This idea can be realized by the following optimization problem: Please note that now w S and w T are optimized. The above optim ization problem can be solved us-ing a standard QP-solv er. In a preliminary experiment we used the optimization package CPLEX to and w T . 3.5 Kernel Mean Matching ( SVM S  X  T ) Kernel methods map the data into a reproducing kernel Hilbert space (RKHS) by means of a map-on the choice of kernel, the space of H may be spanned by a large numbe r of higher order features of the data. In such cases, higher order statistics for a set of input points can be computed in H by kernels, the mapping is injecti ve [5]  X  in other words, given knowledge of (only) the mean (the right hand side), we which information we want to retain and which one we want to di sregard, see [8]). Generally speaking, the higher dimensi onal H , the more information is contained in the mean. In [6] it was propos ed that one could use this for covariate shift adaptation, moving the mean of a the source training points. We have applied this to our problem, but found that a variant of this approach perfo rmed better . In this variant, we do not re-weight the source points, but rather we translate each point towards the mean of the target inputs: This also leads to a modifi ed source input distrib ution which is statistically more similar to the Unlik e [6], we do have a certain amount of labels also for the target distrib ution. We make use of them by performing the shift separately for each class y  X  { X  1 } : examples with label y , respecti vely. The shifted examples can now be used in different ways to 3.6 Featur e Augmentation ( SVM S  X  T ) In [3] a method was proposed that augments the features of source and target examples in a domain-specific way: The intuition behind this idea is that there exist one set of parameters that models the properties common to both sets and two additional sets of parameters that model the specifics of the two domains. It can easily be seen that the kernel for the augmented feature space can be computed as: This means that the  X  X imilarity X  between two examples is two times as high, if the examples were drawn from the same domain, as if they were drawn from different domains. Instead of the factor 2 , we used a hyper-parameter B in the following. 3.7 Combination of Sev eral Sour ces Most of the above algorithms can be extended in one way or another to integrate several source do-mains. In this work we consider only three possible algorithms: (a) convex combinations of se veral domains, (b) KMM on several domains and (c) an extension of the dual-task learning approach to multi-task learning. We briefly describe these methods below: Multiple Convex Combinations ( M-SVM S +SVM T ) The most general version would be to op-timize all convex combination coefficients independently . If done in a grid-search-lik e manner , it becomes prohibitive for more than say three source domains. In principle , one can optimize these coefficients also by solving a linear program. In preliminary experiments we tried both approaches and they typically did not lead to better results than the following combination: where S is the set of all considered source domains. We therefore only considered this way of combining the predictions.
 Multiple KMM ( M-SVM S  X  T ) Here, we shift the source examples of each domain independently the shifted source examples as well as the target examples.
 Multi-task Lear ning ( M-SVM S,T ) We consider the following v ersion of multi-task learning: of regularization parameters, which we parametrized by two parameters C S and C T in the following way:  X  D We conside red two different settings for the comparison. For the first experim ent we assume that there is one source doma in with enough data that should be used to impro ve the performance in the target domain. In the second setting we analyze whether one can benefit from several source domains. 4.1 Single Sour ce Domain Due to space constraints, we restrict ourselv es to presenting a summary of our results with a fo-cus on best and worst performing methods. The detailed results are given in Figure A2 in the appendix, where we show the median auPRC of the methods SVM T , SVM S , SVM S  X  T , SVM S + T , SVM S +SVM T , SVM S  X  T and SVM S,T for the considered tasks. The summary is given in Fig-ure 1, where we illustrate which method performed best (green), similarly well (within a confidence interv al of  X / cantly better than the worst (light red) or worst (red). From these results we can make the following observ ations: From our observ ations we can concl ude that the simple convex combination approach works surpris-ingly well. It is only outperformed by the dual-task learning algorithm which performs consistently
