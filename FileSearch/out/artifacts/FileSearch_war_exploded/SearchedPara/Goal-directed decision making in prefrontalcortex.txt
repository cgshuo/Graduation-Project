 Matthew Botvinick James An Princeton Neuroscience Institute and Computer Science Department Department of Psychology, Princeton University Princeton University Princeton, NJ 08540 Princeton, NJ 08540 decision making may rely on two qualitatively different mechanisms. In habit-based goal-directed or purposive decision making, on the other hand, actions are selected based on a prospective consideration of possible outcomes and future lines of action [2]. Over the past twenty years or so, the attention of cognitive neuroscientists and computationally minded potential links between dopaminergic function and temporal-difference algorithms for now being driven by innovations in animal behavior research, which have yielded powerful directed behavior [4].
 close relationship between purposive decision making, as understood in the behavioral sciences, and model-based methods for the solution of Markov decision problems (MDPs), from states and actions to outcomes) and a reward function (a mapping from states to computations underlying goal-directed action selection (though see [6, 7]). As discussed below, a great deal of evidence indicates that purposive action selection depends critically on a particular region of the brain, the prefrontal cortex. However, it is currently a critical, and quite open, question what the relevant computations within this part of the brain might be. backward induction, linear programming, and others). However, from a cognitive and neuroscientific perspective, there is one approach to solving MDPs that it seems particularly appealing to consider. In particular, several researchers have suggested methods for solving context, derives from a recent movement toward framing human and animal information understood in those same terms.
 One challenge in investigating this possibility is that previous research furnishes no  X  X ff-the-of how goal-directed action selection can be performed based on probabilisitic inference, this account, we introduce a new algorithm for solving MDPs through Bayesian inference, along with a convergence proof. We then present results from a set of simulations are thought to involve purposive action selection. In particular, neurons in dorsolateral prefrontal cortex (DLPFC) appear to encode task-specific mappings from stimuli to responses (e.g., [16]):  X  X ask representations, X  in the language of psychology, or  X  X olicies X  in the language of dynamic programming. Although there is some understanding of how policy representations in DLPFC may guide action model-based fashion, leveraging information about action-outcome contingencies (i.e., the transition function) and about the incentive value associated with specific outcomes or states However, some evidence suggests that the enviromental effects of simple actions may be medial temporal structures may be important in forecasting action outcomes [21]. reward representations in OFC, and representations of states and actions in other brain interdependencies, and that policy selection occurs, within this network, through a process of probabilistic inference. 2.1 Architecture The implementation takes the form of a directed graphical model [22], with the layout shown of available actions, play the role of high-level cortical motor areas involved in the programming of action sequences. Policy variables ( ), each repre-senting the set of all deterministic policies associated with a specific state, capture the representational role of DLPFC. Local and global utility variables, described further below, capture the role of OFC in representing incentive value. A separate set of nodes is included for each discrete time-step up to the planning horizon. State probabilities are based on the state and action variables in the preceding time-step, and representing reward magnitude as a continuous variable, we adopt an approach introduced by with large negative rewards reduce p ( u ) to near zero. In the simulations reported below, we used a simple linear transformation to map from scalar reward values to p ( u ): In situations involving sequential actions, expected returns from different time-steps must be is a binary random variable, but associated with a posterior probab ility determined as: 1 specific plans can be evaluated and compared by conditioning on specific sets of values over which is to condition directly on the utility variable u G , as explained next. 2.2 Policy selection by probabilistic inference: an iterative algorithm Cooper [23] introduced the idea of inferring optimal decisions in influence diagrams by Although this technique has been adopted in some more r ecent work [9, 12], we are aware of tasks. We introduce here a simple algorithm that does furnish such a guarantee. The Note that temporal discounting can be incorporated into the framework through minimal modifications to Equation 2. observed ( u = 1). and convergence for this algorithm. 2.2.1 Monotonicity on every iteration. Define * as follows: (Note that we assume here, for simplicity, that there is a unique optimal policy.) The objective is to establish that: where t indexes processing iterations. The dynamics of the network entail that where represents any value (i.e., policy) of the decision node being considered. Substituting this into (4) gives subscripts. Applying Bayes X  law to (6) yields Canceling, and bringing the denominator up, this becomes Rewriting the left hand side, we obtain Subtracting and further rearranging: Note that this last inequality (12) follows from the definition of *.
 guaranteed to be part of the optimal policy. The proof above shows that * will continuously rise. globally optimal policy. The process works backward, in the fashion of backward induction. 2.2.2Convergence Continuing with the same notation, we show now that Note that, if we apply Bayes X  law recursively, Thus, p and so forth. Thus, what we wish to prove is or, rearranging, on the previous iteration, With this in mind, we can rewrite the left hand side product in (17) as follows: as of * that, for all other than *, p ( * ). Thus, 3.1 Binary choice preferred food ( r = 1). Representing these contingencies in a network structured as in Fig. 1 plan, to which the model obviously converges.
 yielding a previously valued outcome are abandoned after the incentive value of the outcome is reduced, for example by pairing with an aversive event (e.g., [4]). To simulate this within reversal in lever choice (Fig. 2B).
 Another signature of purposive actions is that they are abandoned when their causal connection with rewarding outcomes is removed ( contingency degradation, see [4]). We resulting behavior is shown in Fig. 2C.
 3.2 Stochastic outcomes A critical aspect of the present modeling paradigm is that it yields reward-maximizing choices in stochastic domains, a property that distinguishes it from some other recent illustrated in Fig. 2D, the model maximizes expected value by opting for the left coin. 3.3 Sequential decision Here, we adopt the two-step T-maze scenario used by [24] (Fig. 3A). Representing the task reward values indicated in Fig. 3A, yields the choice behavior shown in Figure 3B. behavior is shown in Fig. 3D.
 A famous empirical demonstration of purposive control involves detour behavior. Using a maze like the one shown in Fig. 4A, with a f ood reward placed at s 5 , Tolman [2] found that the longer lower route. We simulated this experiment by representing the corresponding resulting choice behavior at the critical juncture s 2 is shown in Fig. 4.
 Another classic empirical demonstration involves latent learning . Blodgett [25] allowed rats to explore the maze shown in Fig. 5. Later insertion of a food reward at s 13 was followed immediately by dramatic reductions in the running time, reflecting a reduction in entries into blind alleys. We simulated this effect in a model based on the template in Fig. 1 (right), representing the maze la yout via an appropriate transition function. In the absence of a reward at s 12 , random choices occurred at each intersection. However, setting R ( s 13 ) = 1 resulted in the set of choices indicated by the heavier arrows in Fig. 5. Initial proposals for how to solve decision problems thr ough probab ilistic inference in utility nodes. More r ecently, Attias [11] and Verma and Rao [9] have used graphical models not in a way that guaranteed convergence on optimal (reward maximizing) plans. More closely related to the present research is work by Toussaint and Storkey [10], employing the EM algorithm. The iterative approach we have introduced here has a certain resemblance to the EM procedure, which becomes evident if one views the policy variables in our models as formal equivalence between the algorithm we have proposed and the one reported by [10]. As a cognitive and neuroscientific proposal, the present work bears a close relation to r ecent work by Hasselmo [6], addressing the prefrontal computations underlying goal-directed be interesting to further consider their interrelations. set of reachable states for the relevant time-step, assuming an initial state of s 1 . Acknowledgments Thanks to Andrew Ledvina, David Blei, Yael Niv, Nathaniel Daw, and Francisco Pereira for useful comments .
 References
