 This strategy works because of the availability of vast amounts of data. We empirically show that this approach indeed improves the system performance over existing algorithms dramatically! The organization of the paper is as follows. In the next section, we review the work in web caching and prefetching. In Section 3 we introduce the formal association rule based prediction models and show how it integrates with the caching algorithms. Then, in Section 4, we present our experimental results related to this new model. In Section 5, we integrate prefetching into the caching model, and conclude in Section 6. Web caching is an important technique for improving the performance of WWW systems. Lying in the heart of caching algorithms is the so-called "page replacement policy", which specifies conditions under which a new page will replace an existing one. The basic idea behind most of these caching algorithms is to rank objects according to a key value computed by factors such as size, frequency and cost. When a replacement is to be made, lower-ranked objects will be evicted from the cache. The most successful replacement algorithm is GDSF[9]. It computes the key value of a page p as K(p)= L+F(p)*C(p)/S(p), where L is an inflation factor to avoid cache pollution, F(p) is the past occurrence frequency of p, C(p) is the cost to fetch p and S(p) is the size of p. Researchers have also considered prefetching popular documents in order to reduce perceivable network latency [7, 10, 11, 12]. [10] discussed an integrated model of prcfetching and caching in a file system. In [11] Chinen and Yamaguchi prefetch the referenced pages from hyperlinks embedded in the current object. [12] improved this idea by also considering the frequency of accesses of the hyperlinks. 
We plan to use web-log mining to improve the performance of web caching and prefetching systems. Web log mining is an important part of web mining. It extracts useful knowledge from large-scale web logs for application in other domains. The closest work done previously is [5]. Pitkow and Pirolli studied the pattern extraction techniques to predict the web surfer's path. Su et al. [13] has built an n-gram model to predict future requests. 
In data mining area, [2] has looked at sequential data mining for transaction data, but they are not applied caching and prefetching. Though they pointed out the possible application in Figure 1 shows a user session in a web log. The blocks represent HTML documents and the ellipses stand for embedded objects. 
The solid arrows indicate access paths and the value conf is the conditional probability of transition from an n-gram to a next document. The dotted arrows depict the embed/parent relationship between an HTML file and its embedded objects. 
The value Pi on the arrow is the probability that object Oibelongs to the document. For illustration purpose, we draw embedded objects only for document Sk. In our analysis of web logs, most Pi are close one. Therefore, in subsequent discussions, we assume that Pi =1. From the graph, we know that once the frequent sequences have been found, it is stralghtforvcard to generate N-gram prediction the follow format: The confidence conf, i.e. the conditional probability P(Skl SIS2...Sk-O, of this rule is expressed in terms of count of sequences: Furthermore, if Sk has embedded objects, for each object Oi belonging to Sk, the following rules can be deducted immediately from the EOT: Usually, the number of rules generated in this way is large. Hence, to reduce the memory space to store the model, we do not actually generate the rules by Equation (3.3); instead, we just put the EOT in memory and extract rules dynamically. Besides, for rules generated by Equation (3.1), we chop those with conf below a threshold he. Raising hc decreases the number of rules needed to he stored. In our experiments, hc is set between O and 0.3. By this means, we reduce the number of rules and keep only the high confidence ones. The process of building a set of association rules and an EOT is called training. Once the training is finished, we can apply these rules to give predictions of future visits. Intuitively, for any given observed sequence of URL's, we choose a rule whose LHS matches the sequence and has the longest length among all applicable rules. The detailed algorithm is given below. will be. The rationale behind our extension is that we look ahead some time in the request stream and adjust the replacement policy. We have conducted a series of experimental comparisons with two data logs that we are able to obtain. In the experiments, the EPA (United States Environmental Protection Agency) data contains a day~ worth of all HTrP requests to the EPA WWW server located at Research Triangle Park, NC. The NASA data is from NASA Kennedy Space Center WWW server in Florida containing 17 days' worth of requests. Before experiments, we removed uncacheable URLs from the access logs. A URL is considered uncacbeable when it contains dynamically generated content such as CGI scripts. We also filtered out requests with unsuccessful HTrP response code. In our experiments, we use two quantitative measures to judge the quality of our extended caching algorithm. Using test web log data, hit rate is defined as the percentage of web requests, out of all web requests in the testing data, that can be answered by the cache. Byte hit rate is the percentage of bytes that are answered directly by documents and objects in the cache, out of the total number of bytes that are requested. The results illustrating both hit rates and byte-hit rates are shown in Figures 4 to 5. The algorithms under comparison are n-gram, GDSF, GD-Size, LFUDA, and the LRU method [!]. Overall, the n-gram-based algorithm outperforms the other algorithms using performance gain is substantially larger when the n-gram algorithm is applied on the NASA dataset. This observation can be explained by considering the difference between the two datasets. The EPA dataset is the web log data collected over a period of 24 hours. We have used the first 12 hours of data for training and the remaining data for evaluation. The users' access pattern may vary dramatically between the two time periods and thus decreasing the prediction accuracy. By comparison, 6 days of the NASA log data are used for training while the remaining 7 days of data are used for evaluation. The users' access patterns are much more stable over this extended period of time, making the training data much more representative of the actual access patterns. This no doubt aids tremendously in prediction accuracy. We have shown that predictive caching improves system performance in terms of hit rate and byte hit rate. These two metrics implicitly reflect reduction of network latency. In this section, we investigate an integrated caching and prefetching model to further reduce the network latency perceived by users. The motivation lies in two aspects. Firstly, from Figure 4 to 5, we can see both the hit rate and byte hit rate are growing in a log-like fashion as a function of the cache size. Our results are consistent with those of other researchers [4, 16]. This suggests that hit rate or byte hit rate does not increase as much as the cache size does, especially when cache size is large. This fact naturally leads to our thought to separate part of the cache memory (e.g. 10% of its size) for prefetching. By this means, we can trade the minor hit rate loss in caching with the greater reduction of network latency in prefetching. Secondly, almost all prefetching methods require a prediction model. Since we have already embodied an n-gram model into predictive caching, this In our approach, the original cache memory is partitioned into two parts: cache-buffer and prefetch-buffer. A prefetching agent keeps pre-loading the prefetch-buffer with documents predicted to have the highest Wi. The prefetching stops when the prefetch-buffer is full. The original caching system behaves as before on the reduced cache-buffer except it also checks a hit in the prefetch-buffer. If a hit occurs in the prefetch-buffer, the requested object will be moved into the cache-buffer according to original replacement algorithm. Of course, one potential drawback of prefetching is that the network load may be increased. Therefore, there is a need to balance the decrease in network latency and the increase in network traffic. We next describe two experiments that show that our integrated predictive caching and prefetcbing model does not suffer much from the drawback. In our experiments, we again used the EPA and NASA web logs to study the prefetching impact on caching. For fair comparison, the cache memory in cache-alone system equals the total size of cache-buffer and prefetch-buffer in the integrated system. We assume that the pre-buffer has a size of 20% of the cache memory. Two metrics are used to gauge the network latency and increased network traffic: Fractional Latency: The ratio between the observed latency with a caching system and the observed latency without a caching system. Fractional Network Traffic: The ratio between the number of bytes that are transmitted from web servers to the proxy and the total number of bytes requested. As can be seen from Figure 6(top), prefetching does reduce network latency in all cache sizes. On EPA data, when cache size is 1% of the dataset, fractional latency has been reduced from 25.6% to 19.7%. On NASA data, when cache size is 0.001% of the dataset, fractional latency has been reduced from 56.4% to 50.9%. However, as can be seen from Figure 6(bottom), we pay a price for the network traffic, whereby the prefetching algorithm incurs an increase in network load. For example, in NASA [1] M. Arlitt, R. Friedrich L. Cherkasova, J. DiUey, and T. Jin. [2] R. Agrawal and R. Srikant. Minging Sequential Patterns. [3] C. Aggarwal, J. L. Wolf, and P. S. Yu. Caching on the [4] P. Cao and S. Irani. Cost-aware www proxy caching [5] Pitknw J. and Pirolli P. Mining longest repeating [6] T.M. Kroeger and D. D. E. Long. Predicting future file-[7] K. Chinen and S. Yamaguchi. An Interactive Prefetching [8] S. Schechter, M. Krishnan, and M.D. Smith. Using path [9] L. Cherkasova. Improving www proxies performance with [10] P. Cao, E. W. Felten, A. R. Karlin, and K. Li. A study of [11] K. Chinen and S. Yamaguchi. An interactive prefetching [12] D. Duchamp. Prefetching hyperlinks. In Proceedings of the [13]Z. Su, Q. Yang, Y. Lu, and H. Zhang. Whatnext: A [14] V. Padmanabhan and J. Mogul. Using predictive prefetching [15] E. Cohen, B. Krishnamurthy, and J. Rexford. Evaluating [16] M. Arlitt, R. Friedrich, L. Cherkasova, J. Dilley, and T. Jin. 
Evaluating content management techniques for web proxy caches. In IIP Technical report, Palo Alto, Apr. 1999. 
