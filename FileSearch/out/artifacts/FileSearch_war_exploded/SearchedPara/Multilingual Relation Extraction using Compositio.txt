 The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008). AKBC supports downstream reasoning at a high level about extracted entities and their relations, and thus has broad-reaching applications to a variety of domains.
One challenge in AKBC is aligning knowledge from a structured KB with a text corpus in order to perform supervised learning through distant supervision . Univer-sal schema (Riedel et al., 2013) along with its exten-sions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns. This propagates information between KB annotation and corresponding textual evidence.
The above applications of universal schema express each text relation as a distinct item to be embedded. This harms its ability to generalize to inputs not precisely seen at training time. Recently, Toutanova et al. (2015) ad-dressed this issue by embedding text patterns using a deep sentence encoder, which captures the compositional se-mantics of textual relations and allows for prediction on inputs never seen before.

This paper further expands the coverage abilities of universal schema relation extraction by introducing tech-niques for forming predictions for new entities unseen in training and even for new domains with no associated an-notation. In the extreme example of domain adaptation to a completely new language, we may have limited lin-guistic resources or labeled data such as treebanks, and only rarely a KB with adequate coverage. Our method performs multilingual transfer learning, providing a pre-dictive model for a language with no coverage in an exist-ing KB, by leveraging common representations for shared entities across text corpora. As depicted in Figure 1, we simply require that one language have an available KB of seed facts. We can further improve our models by ty-ing a small set of word embeddings across languages us-ing only simple knowledge about word-level translations, learning to embed semantically similar textual patterns from different languages into the same latent space. In extensive experiments on the TAC Knowledge Base Population (KBP) slot-filling benchmark we outperform the top 2013 system with an F1 score of 40.7 and per-form relation extraction in Spanish with no labeled data or direct overlap between the Spanish training corpus and the training KB, demonstrating that our approach is well-suited for broad-coverage AKBC in low-resource lan-guages and domains. Interestingly, joint training with Spanish improves English accuracy.
 not in KB Figure 1: Splitting the entities in a multilingual AKBC training set into parts. We only require that entities in the two corpora overlap. Remarkably, we can train a model for the low-resource language even if entities in the low-resource language do not occur in the KB. AKBC extracts unary attributes of the form ( subject , at-tribute ), typed binary relations of the form ( subject , rela-tion , object ), or higher-order relations. We refer to sub-jects and objects as entities . This work focuses solely on extracting binary relations, though many of our tech-niques generalize naturally to unary prediction. Gener-ally, for example in Freebase (Bollacker et al., 2008), higher-order relations are expressed in terms of collec-tions of binary relations.
 We now describe prior work on approaches to AKBC. They all aim to predict ( s, r, o ) triples, but differ in terms of: (1) input data leveraged, (2) types of annotation re-quired, (3) definition of relation label schema, and (4) whether they are capable of predicting relations for en-tities unseen in the training data. Note that all of these methods require pre-processing to detect entities, which may result in additional KB construction errors. 2.1 Relation Extraction as Link Prediction A knowledge base is naturally described as a graph, in which entities are nodes and relations are labeled edges (Suchanek et al., 2007; Bollacker et al., 2008). In the case of knowledge graph completion , the task is akin to link prediction, assuming an initial set of ( s, r, o ) triples. See Nickel et al. (2015) for a review. No accompanying text data is necessary, since links can be predicted using properties of the graph, such as transitiv-ity. In order to generalize well, prediction is often posed as low-rank matrix or tensor factorization. A variety of model variants have been suggested, where the proba-bility of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc  X   X a-Dur  X  an et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s , r , and o (Socher et al., 2013). Other approaches model the com-positionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Nee-lakantan et al., 2015). 2.2 Relation Extraction as Sentence Classification Here, the training data consist of (1) a text corpus, and (2) a KB of seed facts with provenance, i.e. supporting evidence, in the corpus. Given individual an individual sentence, and pre-specified entities, a classifier predicts whether the sentence expresses a relation from a target schema. To train such a classifier, KB facts need to be aligned with supporting evidence in the text, but this is often challenging. For example, not all sentences con-taining Barack and Michelle Obama state that they are married. A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015). An additional degree of freedom in these approaches is whether they classify individual sentences or predicting at the corpus level by aggregating information from all sentences containing a given pair of entities before prediction. The former ap-proach is often preferable in practice, due to the simplic-ity of independently classifying individual sentences and the ease of associating each prediction with a provenance. Prior work has applied deep learning to small-scale rela-tion extraction problems, where functional relationships are detected between common nouns (Li et al., 2015; dos Santos et al., 2015). Xu et al. (2015) apply an LSTM to a parse path, while Zeng et al. (2015) use a CNN on the raw text, with a special temporal pooling operation to separately embed the text around each entity. 2.3 Open-Domain Relation Extraction In the previous two approaches, prediction is carried out with respect to a fixed schema R of possible rela-tions r . This may overlook salient relations that are ex-pressed in the text but do not occur in the schema. In response, open-domain information extraction (OpenIE) lets the text speak for itself: R contains all possible pat-terns of text occurring between entities s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007). These are obtained by filtering and normalizing the raw text. The approach offers impressive coverage, avoids issues of distant supervision, and provides a useful ex-ploratory tool. On the other hand, OpenIE predictions are difficult to use in downstream tasks that expect infor-mation from a fixed schema.

Table 1 provides examples of OpenIE patterns. The ex-amples in row two and three illustrate relational contexts for which similarity is difficult to be captured by an Ope-nIE approach because of their syntactically complex con-structions. This motivates the technique in Section 3.2, which uses a deep architecture applied to raw tokens, in-stead of rigid rules for normalizing text to obtain patterns. Table 1: Examples of sentences expressing relations. Context tokens (italicized) consist of the text occurring between entities (bold) in a sentence. OpenIE patterns are obtained by normalizing the context tokens using hand-coded rules. The top example expresses the per:siblings relation and the bottom two examples both express the per:cities of residence relation. 2.4 Universal Schema When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the Ope-nIE and link-prediction perspectives. By jointly mod-eling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section 2.2. a seed KB with additional edges corresponding to Ope-nIE patterns observed in the corpus. Even if the user does not seek to predict these new edges, a joint model over all edges can exploit regularities of the OpenIE edges to im-prove modeling of the labels from the target schema.
The data still consist of ( s,r,o ) triples, which can be predicted using link-prediction techniques such as low-rank factorization. Riedel et al. (2013) explore a variety of approximations to the 3-mode ( s,r,o ) tensor. One such probabilistic model is: where () is a sigmoid function, u s,o is an embedding of the entity pair ( s,o ) , and v r is an embedding of the relation r , which may be an OpenIE pattern or a rela-tion from the target schema. All of the exposition and re-sults in this paper use this factorization, though many of the techniques we present later could be applied easily to the other factorizations described in Riedel et al. (2013). Note that learning unique embeddings for OpenIE rela-tions does not guarantee that similar patterns, such as the final two in Table 1, will be embedded similarly.
As with most of the techniques in Section 2.1, the data only consist of positive examples of edges. The absence of an annotated edge does not imply that the edge is false. In fact, we seek to predict some of these missing edges as true. Riedel et al. (2013) employ the Bayesian Person-alized Ranking (BPR) approach of Rendle et al. (2009), which does not explicitly model unobserved edges as negative, but instead seeks to rank the probability of ob-served triples above unobserved triples.

Recently, Toutanova et al. (2015) extended USchema to not learn individual pattern embeddings v r , but instead to embed text patterns using a deep architecture applied to word tokens. This shares statistical strength between OpenIE patterns with similar words. We leverage this ap-proach in Section 3.2. Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015). 2.5 Multilingual Embeddings Much work has been done on multilingual word embed-dings. Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embed-dings across languages (Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014). Others (Mikolov et al., 2013; Faruqui et al., 2014) align separate single-language embedding models using a word-level dictio-nary. Mikolov et al. (2013) use translation pairs to learn a linear transform from one embedding space to another.
However, very little work exists on multilingual re-lation extraction. Faruqui and Kumar (2015) perform multilingual OpenIE relation extraction by projecting all languages to English using Google translate. However, as explained in Section 2.3 the OpenIE paradigm is not amenable to prediction within a fixed schema. Further, their approach does not generalize to low-resource lan-guages where translation is unavailable  X  while we use translation dictionaries to improve our results, our experi-ments demonstrate that our method is effective even with-out this resource. 3.1 Universal Schema as Sentence Classifier Similar to many link prediction approaches, (Riedel et al., 2013) perform transductive learning, where a model is learned jointly over train and test data. Predictions are made by using the model to identify edges that were un-observed in the test data but likely to be true. The ap-proach is vulnerable to the cold start problem in collab-orative filtering (Schein et al., 2002): it is unclear how to form predictions for unseen entity pairs, without re-factorizing the entire matrix or applying heuristics.
In response, this paper re-purposes USchema as a means to train a sentence-level relation classifier, like those in Section 2.2. This allows us to avoid errors from aligning distant supervision to the corpus, but is more de-ployable for real world applications. It also provides op-portunities in Section 3.4 to improve multilingual AKBC.
We produce predictions using a very simple approach: (1) scan the corpus and extract a large quantity of triplets ( s,r text ,o ) , where r text is an OpenIE pattern. For each triplet, if the similarity between the embed-ding of r text and the embedding of a target relation r schema is above some threshold, we predict the triplet ( s,r schema ,o ) , and its provenance is the input sentence containing ( s,r text ,o ) . We refer to this technique as pat-tern scoring . In our experiments, we use the cosine dis-tance between the vectors (Figure 2). In Section 7.3, we discuss details for how to make this distance well-defined. 3.2 Using a Compositional Sentence Encoder to The pattern scoring approach is subject to an additional cold start problem: input data may contain patterns un-seen in training. This section describes a method for us-ing USchema to train a relation classifier that can take arbitrary context tokens (Section 2.3) as input.
Fortunately, the cold start problem for context tokens is more benign than that of entities since we can exploit sta-tistical regularities of text: similar sequences of context tokens should be embedded similarly. Therefore, follow-ing Toutanova et al. (2015), we embed raw context tokens compositionally using a deep architecture. Unlike Riedel et al. (2013), this requires no manual rules to map text to OpenIE patterns and can embed any possible input string. The modified USchema likelihood is: Here, if r is raw text, then Encoder ( r ) is parameterized by a deep architecture. If r is from the target schema, Encoder ( r ) is a produced by a lookup table (as in tradi-tional USchema). Though such an encoder increases the computational cost of test-time prediction over straight-forward pattern matching, evaluating a deep architecture can be done in large batches in parallel on a GPU.
Both convolutional networks (CNNs) and recurrent networks (RNNs) are reasonable encoder architectures, and we consider both in our experiments. CNNs have been useful in a variety of NLP applications (Col-lobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Unlike Toutanova et al. (2015), we also consider RNNs, specifically Long-Short Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997). LSTMs have proven successful in a variety of tasks requiring encoding sentences as vectors (Sutskever et al., 2014; Vinyals et al., 2014). In our experiments, LSTMs out-perform CNNs.

There are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand, Toutanova et al. (2015) adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions. Second, we apply the encoder to the raw text between entities, while Toutanova et al. (2015) first perform syntactic dependency parsing on the data and then apply an encoder to the path between the two entities in the parse tree. We avoid parsing, since we seek to perform multilingual AKBC, and many languages lack linguistic resources such as treebanks. Even parsing non-newswire English text, such as tweets, is extremely chal-lenging. 3.3 Modeling Frequent Text Patterns Despite the coverage advantages of using a deep sen-tence encoder, separately embedding each OpenIE pat-tern, as in Riedel et al. (2013), has key advantages. In practice, we have found that many high-precision pat-terns occur quite frequently. For these, there is suffi-cient data to model them with independent embeddings per pattern, which imposes minimal inductive bias on the relationship between patterns. Furthermore, some dis-criminative phrases are idiomatic, i.e.. their meaning is not constructed compositionally from their constituents. For these, a sentence encoder may be inappropriate.
Therefore, pattern embeddings and deep token-based encoders have very different strengths and weaknesses. One values specificity, and models the head of the text distribution well, while the other has high coverage and captures the tail. In experimental results, we demonstrate that an ensemble of both models performs substantially better than either in isolation. 3.4 Multilingual Relation Extraction with Zero The models described in previous two sections provide broad-coverage relation extraction that can generalize to all possible input entities and text patterns, while avoid-ing error-prone alignment of distant supervision to a cor-pus. Next, we describe techniques for an even more chal-lenging generalization task: relation classification for in-put sentences in completely different languages.
Training a sentence-level relation classifier, either us-ing the alignment-based techniques of Section 2.2, or the alignment-free method of Section 3.1, requires an avail-able KB of seed facts that have supporting evidence in the corpus. Unfortunately, available KBs have low overlap with corpora in many languages, since KBs have cultural and geographical biases. In response, we perform mul-tilingual relation extraction by jointly modeling a high-resource language, such as English, and an alternative language with no KB annotation. This approach pro-vides transfer learning of a predictive model to the al-ternative language, and generalizes naturally to modeling more languages.

Extending the training technique of Section 3.1 to cor-pora in multiple languages can be achieved by factorizing a matrix that mixes data from a KB and from the two cor-pora. In Figure 1 we split the entities of a multilingual training corpus into sets depending on whether they have annotation in a KB and what corpora they appear in. We can perform transfer learning of a relation extractor to the low-resource language if there are entity pairs occur-ring in the two corpora, even if there is no KB annotation for these pairs. Note that we do not use the entity pair embeddings at test time: They are used only to bridge the languages during training. To form predictions in the low-resource language, we can simply apply the pattern scoring approach of Section 3.1.

In Section 5, we demonstrate that jointly learning mod-els for English and Spanish, with no annotation for the Spanish data, provides fairly accurate Spanish AKBC, and even improves the performance of the English model. Note that we are not performing zero-shot learning of a Spanish model (Larochelle et al., 2008). The relations in the target schema are language-independent concepts, and we have supervision for these in English. 3.5 Tied Sentence Encoders The sentence encoder approach of Section 3.2 is com-plementary to our multilingual modeling technique: we simply use a separate encoder for each language. This approach is sub-optimal, however, because each sentence encoder will have a separate matrix of word embeddings for its vocabulary, despite the fact that there may be con-siderable shared structure between the languages. In re-sponse, we propose a straightforward method for tying the parameters of the sentence encoders across languages.
Drawing on the dictionary-based techniques described in Section 2.5, we first obtain a list of word-word transla-tion pairs between the languages using a translation dic-tionary. The first layer of our deep text encoder consists of a word embedding lookup table. For the aligned word types, we use a single cross-lingual embedding. Details of our approach are described in Appendix 7.5. We focus on the TAC KBP slot-filling task. Much re-lated work on embedding knowledge bases evaluates on the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015). Here, relation extraction is posed as link prediction on a subset of Freebase. This task does not capture the par-ticular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language.
Also, note both Toutanova et al. (2015) and Riedel et al. (2013) explore the pros and cons of learning embed-dings for entity pairs vs. separate embeddings for each entity. As this is orthogonal to our contributions, we only consider entity pair embeddings, which performed best in both works when given sufficient data. 4.1 TAC Slot-Filling Benchmark The aim of the TAC benchmark is to improve both cov-erage and quality of relation extraction evaluation com-pared to just checking the extracted facts against a knowl-edge base, which can be incomplete and where the prove-nances are not verified. In the slot-filling task, each sys-tem is given a set of paired query entities and relations or  X  X lots X  to fill, and the goal is to correctly fill as many slots as possible along with provenance from the corpus. For example, given the query entity/relation pair ( Barack Obama, per:spouse ), the system should return the entity Michelle Obama along with sentence(s) whose text ex-presses that relation. The answers returned by all par-ticipating teams, along with a human search (with time-out), are judged manually for correctness, i.e. whether the provenance specified by the system indeed expresses the relation in question.

In addition to verifying our models on the 2013 and 2014 English slot-filling task, we evaluate our Spanish models on the 2012 TAC Spanish slot-filling evaluation. Because this TAC track was never officially run, the cov-erage of facts in the available annotation is very small, resulting in many correct predictions being marked in-correctly as precision errors. In response, we manually annotated all results returned by the models considered in Table 4. Precision and recall are calculated with respect 4.2 Retrieval Pipeline Our retrieval pipeline first generates all valid slot filler candidates for each query entity and slot, based on en-tities extracted from the corpus using F ACTORIE (Mc-Callum et al., 2009) to perform tokenization, segmenta-tion, and entity extraction. We perform entity linking by heuristically linking all entity mentions from our text cor-pora to a Freebase entity using anchor text in Wikipedia. Making use of the fact that most Freebase entries contain a link to the corresponding Wikipedia page, we link all entity mentions from our text corpora to a Freebase entity by the following process: First, a set of candidate entities is obtained by following frequent link anchor text statis-tics. We then select that candidate entity for which the cosine similarity between the respective Wikipedia and the sentence context of the mention is highest, and link to that entity if a threshold is exceeded.

An entity pair qualifies as a candidate prediction if it glish and Spanish newswire corpora each contain about 1 million newswire documents from 2009 X 2012. The document retrieval and entity matching components of our relation extraction pipeline are based on RelationFac-tory (Roth et al., 2014), the top-ranked system of the 2013 English slot-filling task. We also use the English distantly supervised training data from this system, which aligns the TAC 2012 corpus to Freebase. More details on align-ment are described in Appendix 7.4.

As discussed in Section 3.3, models using a deep sen-tence encoder and using a pattern lookup table have com-plementary strengths and weaknesses. In response, we present results where we ensemble the outputs of the two models by simply taking the union of their individual out-puts. Slightly higher results might be obtained through more sophisticated ensembling schemes. 4.3 Model Details All models are implemented in Torch (code publicly 2012 TAC KBP slot-filling evaluation. We additionally tune the thresholds of our pattern scorer on a per-relation basis to maximize F1 using 2012 TAC slot-filling for En-glish and the 2012 Spanish slot-filling development set for Spanish. As in Riedel et al. (2013), we train using the BPR loss of Rendle et al. (2009). Our CNN is im-plemented as described in Toutanova et al. (2015), using width-3 convolutions, followed by tanh and max pool lay-ers. The LSTM uses a bi-directional architecture where the forward and backward representations of each hidden state are averaged, followed by max pooling over time. See Section 7.2
We also report results including an alternate names (AN) heuristic, which uses automatically-extracted rules to detect the TAC  X  X lternate name X  relation. To achieve this, we collect frequent Wikipedia link anchor texts for Model Recall Precision F1 CNN 31.6 36.8 34.1 LSTM 32.2 39.6 35.5 USchema 29.4 42.6 34.8 USchema+LSTM 34.4 41.9 37.7 USchema+LSTM+Es 38.1 40.2 39.2 USchema+LSTM+AN 36.7 43.1 39.7 USchema+LSTM+Es+AN 40.2 41.2 40.7 Roth et al. (2014) 35.8 45.7 40.2 Table 2: Precision, recall and F1 on the English TAC 2013 slot-filling task. AN refers to alternative names heuristic and Es refers to the addition of Spanish text at train time. LSTM+USchema ensemble outperforms any single model, including the highly-tuned top 2013 sys-tem of Roth et al. (2014), despite using no handwritten patterns.
 Model Recall Precision F1 CNN 28.1 29.0 28.5 LSTM 27.3 32.9 29.8 USchema 24.3 35.5 28.8 USchema+LSTM 34.1 29.3 31.5 USchema+LSTM+Es 34.4 31.0 32.6 Table 3: Precision, recall and F1 on the English TAC 2014 slot-filling task. Es refers to the addition of Span-ish text at train time. The AN heuristic is ineffective on 2014 adding only 0.2 to F1. Our system would rank 4/18 in the official TAC 2014 competition behind systems that use hand-written patterns and active learning despite our system using neither of these additional annotations (Sur-deanu and Ji., 2014). each query entity. If a high probability anchor text co-occurs with the canonical name of the query in the same document, we return the anchor text as a slot filler. In experiments on the English and Spanish TAC KBC slot-filling tasks, we find that both USchema and LSTM models outperform the CNN across languages, and that the LSTM tends to perform slightly better than USchema as the only model. Ensembling the LSTM and USchema models further increases final F1 scores in all experi-ments, suggesting that the two different types of model compliment each other well. Indeed, in Section 5.3 we present quantitative and qualitative analysis of our results which further confirms this hypothesis: the LSTM and USchema models each perform better on different pattern lengths and are characterized by different precision-recall tradeoffs.
 Model Recall Precision F1 LSTM 9.3 12.5 10.7 LSTM+Dict 14.7 15.7 15.2 USchema 15.2 17.5 16.3 USchema+LSTM 21.7 14.5 17.3 USchema+LSTM+Dict 26.9 15.9 20.0 Table 4: Zero-annotation transfer learning F1 scores on 2012 Spanish TAC KBP slot-filling task. Adding a trans-lation dictionary improves all encoder-based models. En-sembling LSTM and USchema models performs the best. 5.1 English TAC Slot-filling Results Tables 2 and 3 present the performance of our models on the 2013 and 2014 English TAC slot-filling tasks. Ensembling the LSTM and USchema models improves F1 by 2.2 points for 2013 and 1.7 points for 2014 over the strongest single model on both evaluations, LSTM. Adding the alternative names (AN) heuristic described in Section 4.3 increases F1 by an additional 2 points on 2013, resulting in an F1 score that is competitive with the state-of-the-art. We also demonstrate the effect of jointly learning English and Spanish models on English slot-filling performance. Adding Spanish data improves our F1 scores by 1.5 points on 2013 and 1.1 on 2014 over using English alone. This places are system higher than the top performer at the 2013 TAC slot-filling task even though our system uses no hand-written rules.

The state of the art systems on this task all rely on matching handwritten patterns to find additional answers while our models use only automatically generated, indi-rect supervision; even our AN heuristics (Section 4.2) are automatically generated. The top two 2014 systems were Angeli et al. (2014) and RPI Blender (Surdeanu and Ji., 2014) who achieved F1 scores of 39.5 and 36.4 respec-tively. Both of these systems used additional active learn-ing annotation. The third place team (Lin et al., 2014) relied on highly tuned patterns and rules and achieved an F1 score of 34.4.

Our model performs substantially better on 2013 than 2014 for two reasons. First, our RelationFactory (Roth et al., 2014) retrieval pipeline was a top retrieval pipeline on the 2013 task, but was outperformed on the 2014 task which introduced new challenges such as confusable en-tities. Second, improved training using active learning gave the top 2014 systems a boost in performance. No 2013 systems, including ours, use active learning. Bentor et al. (2014), the 4th place team in the 2014 evaluation, used the same retrieval pipeline (Roth et al., 2014) as our model and achieved an F1 score of 32.1. Precision Figure 3: Precision-Recall curves for USchema and LSTM on 2013 TAC slot-filling. USchema achieves higher precision values whereas LSTM has higher recall. 5.2 Spanish TAC Slot-filling Results Table 4 presents 2012 Spanish TAC slot-filling results for our multilingual relation extractors trained using zero-annotation transfer learning. Tying word embeddings be-tween the two languages results in substantial improve-ments for the LSTM. We see that ensembling the non-dictionary LSTM with USchema gives a slight boost over USchema alone, but ensembling the dictionary-tied LSTM with USchema provides a significant increase of nearly 4 F1 points over the highest-scoring single model, USchema. Clearly, grounding the Spanish data using a translation dictionary provides much better Spanish word representations. These improvements are complementary to the baseline USchema model, and yield impressive re-sults when ensembled.

In addition to embedding semantically similar phrases from English and Spanish to have high similarity, our models also learn high-quality multilingual word embed-dings. In Table 5 we compare Spanish nearest neighbors of English query words learned by the LSTM with dictio-nary ties versus the LSTM with no ties, using no unsuper-vised pre-training for the embeddings. Both approaches jointly embed Spanish and English word types, using shared entity embeddings, but the dictionary-tied model learns qualitatively better multilingual embeddings. 5.3 USchema vs LSTM We further analyze differences between USchema and LSTM in order to better understand why ensembling the models results in the best performing system. Fig-ure 3 depicts precision-recall curves for the two mod-els on the 2013 slot-filling task. As observed in earlier results, the LSTM achieves higher recall at the loss of Table 5: Example English query words (not in translation dictionary) in bold with their top nearest neighbors by co-sine similarity listed for the dictionary and no ties LSTM variants. Dictionary-tied nearest neighbors are consis-tently more relevant to the query word than untied. F1 Figure 4: F1 achieved by USchema vs. LSTM mod-els for varying pattern token lengths on 2013 TAC slot-filling. LSTM performs better on longer patterns whereas USchema performs better on shorter patterns. some precision, whereas USchema can make more pre-cise predictions at a lower threshold for recall. In Fig-ure 4 we observe evidence for these different precision-recall trade-offs: USchema scores higher in terms of F1 on shorter patterns whereas the LSTM scores higher on longer patterns. As one would expect, USchema success-fully matches more short patterns than the LSTM, mak-ing more precise predictions at the cost of being unable to predict on patterns unseen during training. The LSTM can predict using any text between entities observed at test time, gaining recall at the loss of precision. Combin-ing the two models makes the most of their strengths and weaknesses, leading to the highest overall F1.
Qualitative analysis of our English models also sug-gests that our encoder-based models (LSTM) extract re-lations based on a wide range of semantically similar patterns that the pattern-matching model (USchema) is unable to score due to a lack of exact string match in the test data. For example, Table 6 lists three exam-ples of the per:children relation that the LSTM finds which USchema does not, as well as three patterns that USchema does find. Though the LSTM patterns are all semantically and syntactically similar, they each contain different specific noun phrases, e.g. Lori , four children , toddler daughter , Lee and Albert , etc. Because these spe-cific nouns weren X  X  seen during training, USchema fails to find these patterns whereas the LSTM learns to ignore the specific nouns in favor of the overall pattern, that of a parent-child relationship in an obituary. USchema is limited to finding the relations represented by pat-terns observed during training, which limits the patterns matched at test-time to short and common patterns; all the USchema patterns matched at test time were similar to those listed in Table 6: variants of  X  X  son,  X  . Table 6: Examples of the per:children relation discovered by the LSTM and Universal Schema. Entities are bold and patterns italicized. The LSTM models a richer set of patterns By jointly embedding English and Spanish corpora along with a KB, we can train an accurate Spanish relation ex-traction model using no direct annotation for relations in the Spanish data. This approach has the added benefit of providing significant accuracy improvements for the En-glish model, outperforming the top system on the 2013 TAC KBC slot filling task, without using the hand-coded rules or additional annotations of alternative systems. By using deep sentence encoders, we can perform prediction for arbitrary input text and for entities unseen in train-ing. Sentence encoders also provides opportunities to im-prove cross-lingual transfer learning by sharing word em-beddings across languages. In future work we will apply this model to many more languages and domains besides newswire text. We would also like to avoid the entity de-tection problem by using a deep architecture to both iden-tify entity mentions and identify relations between them. Acknowledgments
Many thanks to Arvind Neelakantan for good ideas and discussions. We also appreciate a generous hard-ware grant from nVidia. This work was supported in part by the Center for Intelligent Information Re-trieval, in part by Defense Advanced Research Projects Agency (DARPA) under agreement #FA8750-13-2-0020 and contract #HR0011-15-2-0036, and in part by the Na-tional Science Foundation (NSF) grant numbers DMR-1534431, IIS-1514053 and CNS-0958392. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon, in part by DARPA via agreement #DFA8750-13-2-0020 and NSF grant #CNS-0958392. Any opinions, findings and conclusions or rec-ommendations expressed in this material are those of the authors and do not necessarily reflect those of the spon-sor.

