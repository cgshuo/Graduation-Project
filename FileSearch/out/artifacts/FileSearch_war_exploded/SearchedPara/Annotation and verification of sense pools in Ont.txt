 1. Introduction
Semantically annotated corpora are useful resources for information retrieval (IR) and natural language processing (NLP) applications. Many such corpora have been developed in recent years, and their semantic annotations can be generally di-vided into three levels, namely word-level, sentence-level, and discourse-level. In word-level annotations, SemCor ( Miller,
Leacock, Tengi, &amp; Bunker, 1993 ) and LDC-DSO ( Ng &amp; Lee, 1996 ) corpora provided word sense annotations, which can be used to disambiguate query words to improve IR performance. Zaragoza, Atserias, Ciaramita, and Attardi (2007) produced seman-tic annotations of WordNet ( Fellbaum, 1998; Miller, 1990) super senses, named entities, and dependency relations by anno-tating English Wikipedia in both word-level and sentence-level. Yang and Li (2004) built parallel corpora based on sentence (or title) alignment techniques for cross-lingual information retrieval. Ibekwe-SanJuan, Fernandez, SanJuan, and Charton (2008) further annotated the discourse structure of scientific summaries using meta-semantic tags (e.g., Objective, Result,
Conclusion, etc.), which can be incorporated into IR systems to assist researchers locating scientific discourse more effi-ciently. Another direction to produce semantic annotations is collaborative tagging via Web 2.0 services. Chklovski and
Mihalcea (2002) developed an annotation tool to build a sense tagged corpus with help of Web users. Delicious (http://deli-cious.com ) and Flickr ( http://www.flickr.com ) allow Web users to describe their resources by creating tags (e.g., keywords), and these tags can be used for, for instance, query expansion to improve retrieval results ( Abbasi &amp; Staab, 2008; Au Yeung, Gibbins, &amp; Shadbolt, 2008; Lioma &amp; Moens, 2008 ).

In this study, we describe the OntoNotes (Hovy, Marcus, Palmer, Ramshaw, &amp; Weischedel, 2006; Pradhan et al., 2007 ), a multilingual (English, Chinese and Arabic) corpus with large-scale semantic annotations, including predicate-argument structure, word senses, ontology linking, and coreference. senses that are created by grouping fine-grained sense distinctions obtained from WordNet and dictionaries into more coarse-grained senses (detailed in Section 2). There are two reasons for this grouping instead of using WordNet senses di-rectly. First, people have trouble distinguishing many of the WordNet-level distinctions in real text, and make inconsistent choices; thus the use of coarse-grained senses can improve inter-annotator agreement (ITA) ( Palmer, Babko-Malaya, &amp; Dang, 2004; Palmer, Dang, &amp; Fellbaum, 2006 ). Second, improved ITA enables machines to more accurately learn to perform sense tagging automatically. Therefore, many researchers have devised sense clustering algorithms to reduce the granularity of
WordNet senses (Agirre &amp; Lopez de Lacalle, 2003; McCarthy, 2006; Mihalcea &amp; Moldovan, 2001; Navigli, 2006; Peters, Peters, &amp; Vossen, 1998; Snow, Prakash, Jurafsky, &amp; Ng, 2007; Tomuro, 2001 ). In OntoNotes, the number of senses has been reduced from 5761 to 3714 senses (35.5% reduction) by grouping fine-grained WordNet senses of each word into more coarse-grained senses, as shown in Table 1 .

In addition to word senses, the OntoNotes also provides sense pools , i.e., sets of near-synonymous senses of words. This notion is similar to the concept nodes within hand-crafted lexical ontologies such as WordNet (called synsets), EuroWordNet (Rodr X guez et al., 1998 ) and HowNet ( Dong &amp; Dong, 2006 ), or automatically constructed ontologies ( Crouch, 1990; Lin, 1998;
Riloff, 1996; Tseng, 2002; Yeh, Wu, &amp; Chen, 2008 ). The sense pools provide rich semantic information for applications. For instance, sense pools can be used for IR systems via ontology-based query expansion approaches ( Bhogal, Macfarlane, &amp;
Smith, 2007; Liu, Liu, Yu, &amp; Meng, 2004; Mandala, Tokunaga, &amp; Tanaka, 2000; Navigli &amp; Velardi, 2003; Sparck Jones, 1999; Voorhees, 1994; Yu, Wu, &amp; Jang, 2007; Zazo, Figuerola, Alonso Berrocal, &amp; Rodr X guez, 2005 ). That is, a query word can be expanded by the other words in the same sense pool. Additionally, sense pools can also be used for (near-)duplicate detection and lexical expansion in text summarization (Vanderwende, Suzuki, Brockett, &amp; Nenkova, 2007 ), and alternative word selection in writing support systems (Inkpen, 2007; Inkpen &amp; Hirst, 2006 ).

Although a sense pool provides a set of near-synonymous senses of words, there is still no knowledge about whether two words in a pool are interchangeable in practical use. Consider the following examples: (1a) The tunnel under the bay is closed because of an accident.
 (1b) There is a parking lot near the building.

In (1a), the pool members in the given pool share the meaning of a physical structure that connects separate places by tra-versing an obstacle. Meanwhile, the word  X  X  X unnel X  in the example sentence cannot be substituted by the other pool mem-bers, since all the substitutions are semantically implausible. Similarly, in (1b),  X  X  X arcel X  cannot substitute for  X  X  X ot X  since  X  X  X arking lot X  is a fixed phrase. The above examples indicate that not all pool members can be substituted with each other even though they share the same or similar meaning. In fact, some pool members may produce inadequate substitutions due to the context mismatch problem such as the preposition mismatch in (1a), and the fixed phrase in (1b). Such inadequate substitutions may reduce the applications X  effectiveness. Therefore, the sense pools need to be verified before they can be used practically.

In measuring the substitutability of near-synonymous words, collocations and anti-collocations (e.g.,  X  X  X trong coffee X  and  X  X  X owerful coffee X  (Pearce, 2001 )) extracted from free text are useful features for capturing the collocational differences of near-synonymous words (Inkpen &amp; Hirst, 2002, 2006 ). For instance, the system can reject a given near-synonymous word in the substitution process if any anti-collocation containing the given word is detected in the extracted anti-collocations.
However, the coverage of collocations and anti-collocations may be low since many sentences may not contain such collo-cations or anti-collocations. Thus, the collocational approach often failed to deal with such cases ( Inkpen, 2007 ). Another effective method to tackle this problem is to measure the substitutability from the contexts of the sentences containing near-synonymous words, since the context can provide more information about semantic and syntactic constraints ( Charles, 2000; Inkpen, 2007; Miller &amp; Charles, 1991 ). Therefore, this study follows the contextual approach and devises an unsuper-vised algorithm to check whether a word in a pool can be substituted by other words in the same pool. The unsupervised algorithm comprises two parts: Google n -grams and a statistical test. The n -gram features are used to capture context mis-match phenomena resulted from inadequate substitutions. The statistical test can then be applied to determine a substitu-tion is adequate or inadequate based on the degree of mismatch. Such substitution information can not only help applications select both semantically and syntactically matched substitutions, but can also help OntoNotes specialists refine sense pools to improve annotation quality.

The rest of this work is organized as follows. Section 2 describes the annotation procedure of word senses and sense pools in OntoNotes. Section 3 describes the unsupervised algorithm for sense pool verification. Section 4 summarizes the exper-imental results. Conclusions are finally drawn in Section 5. 2. OntoNotes annotation procedure
This section describes the annotation procedure of word senses and sense pools in OntoNotes. Other semantic annota-tions such as predicate-argument structure and coreference are described in Hovy et al. (2006) and Pradhan et al. (2007) .
Fig. 1 shows the annotation procedure. The OntoNotes specialists generate word senses by grouping WordNet fine-grained sense distinctions into more coarse-grained senses. Once the word senses are grouped, the sense distinctions, def-initions, and a set of sentences are presented to human annotators to determine the right sense of each target word in the sentences. The ITA can then be calculated based on the annotation results. In OntoNotes, sense grouping has been calibrated to ensure that ITA averages at least 90%. That is, sense groupings lead to a revision and clarification by OntoNotes specialists if their ITA scores below 90%. Finally, an entropy-based procedure is applied to determine exemplars in which the annotators agree but are wrong for the purpose of corpus cleanup (Yu, Wu, &amp; Hovy, 2008 ).

The sense pools are created based on the OntoNotes sense tags generated by the above procedure. Table 2 shows the sense definitions and sense pools for the word  X  X  X rm X  (noun sense). OntoNotes specialists assign each word sense to a sense pool by combining each sense of a word in the OntoNotes corpus with the senses of other OntoNotes words that carry similar meaning. The detail steps are described as follows: (1) Select a noun (or verb) and identify one of its senses as the target word. Use a thesaurus to identify a set of words (2) Create a new pool for the target word if it is not already in an existing pool. (3) Take each word in the synonym set and consider their every sense, deciding whether it belongs in the new pool, (4) Repeat step (3) until all words in the synonym set have their relevant senses assigned into a pool.

Finally, each sense pool (and hence its corresponding senses) are linked to a concept node in the Upper Model of the Ome-ga ontology ( http://omega.isi.edu)( Philpot, Hovy, &amp; Pantel, 2005 ). Omega is a 120,000-node terminological ontology con-structed by merging a variety of resources, including WordNet, Mikrokosmos (Mahesh, 1996; O X  X ara et al., 1998 ), and some upper models such as SUMO (Niles &amp; Pease, 2001 ) and DOLCE ( Gangemi, Guarino, Masolo, Oltramari, &amp; Schneider, 2002). Omega contains an Upper Model of some 200 concepts that provide a very high-level categorization of the Objects,
Events, and Properties encountered in (text about) the world. Inserting a sense pool into Omega is accomplished by locating the most specific Upper Model concept that logically subsumes the pool X  X  meaning. An easy example is the pool that denotes  X  X  X ar/automobile X , which is inserted under artifact . A more difficult example is the pool denoting  X  X  X eacher X , which is inserted other operator relations are Part, Function, and Location. 3. Unsupervised sense pool verification
A proposed sense pool is verified by checking the semantic substitutability among pool members. Fig. 2 a shows the ver-ification procedure. A sense pool { s 1 , ... , s m } is first selected from the Omega ontology, where s and the others are replacing words. A set of sentences containing s the substitution process. As mentioned earlier, inadequate substitutions may result in context mismatch problems. There-fore, the verification procedure adopts n -grams to measure the degree of mismatch in a substitution, and thereby calculate a substitution score. Additionally, an unsupervised method using  X  X  query by Google  X  is developed to help obtain a reliable sub-stitution score. The advantage is that using the Web as corpus can reduce the data sparseness problem caused by n -grams. In the verification process, k words are randomly selected from the OntoNotes corpus (excluding the words in the given sense pool) to substitute for the target word, respectively. The k randomly selected words are assumed be to semantically dissim-ilar or unrelated to the target word. An average substitution score, called a randomized score , can then be calculated using the same procedure described above. The randomized score is taken as the baseline to compare with the substitution score using a statistical test. If the substitution score is significant greater than the randomized score, then this substitution is accepted as adequate. Fig. 2 b shows an example output of the verification procedure.

The main steps of the verification procedure are detailed as follows: (1) Input data : Given a sense pool, all possible substitution pairs are first generated for verification. For instance, the pool replacing-word). Additionally, a set of sentences annotated with the target word is also selected from the OntoNotes corpus.
The verification procedure then takes as input each substitution pair of the pools and the set of sentences. (2) n-Gram generation : For each pair, the n -grams (bigram to 4-gram) with the target word are generated from the anno-tated sentences. The target word in each n -gram is then substituted by the replacing word. The columns A and E of Table 3 list the n -grams with the target word  X  X  X unnel X  and replacing word  X  X  X verpass X , respectively, generated from sentence (2a) X  (2c).
 (2a) The tunnel under the bay is closed because of an accident. (2b) The railroad passes under the mountain through a tunnel . (2c) They are building a railroad tunnel right through the mountain. (3) Google queries : For each n -gram with the target word and replacing word, their frequencies can be obtained by que-rying Google (columns B and F). The frequency is then normalized as where Z ( ngram i ) denotes the normalization values for an n -gram with the target word or replacing word (columns D and H); freq ( ngram i ) denotes the frequency of an n -gram with the target word or replacing word, and freq ( ) denotes the frequency of the target word or replacing word (columns C and G). (4) Substitution score : The substitution score for an n -gram is defined as follows:
This score (column I) is used to measure the degradation of the normalization value after the substitution of the target word. A greater score indicates a lower degradation level, which means that these two senses are more substitutable in the context of the given n -gram. Additionally, the upper bound of the score is restricted to 1 for simplicity consideration.
The substitution score for a pair can then be calculated as the average of n -gram scores (bigram, trigram or 4-gram), as shown below.
 where N denotes the number of n -grams. In the remainder of this paper, we report only 4-gram scores to represent the sub-stitution scores for the pairs, since they are more discriminative than bigram and trigram scores. (5) Randomized score : Although the substitution scores indicate the degradation levels, for instance, Score tutions. One possible solution is the thresholding method, but choosing a suitable threshold for each pair is still nontrivial. To address the problem, this study proposes the use of randomized scores . A randomized score for a pair is calculated as follows.
Firstly, k words are randomly selected from the OntoNotes corpus (excluding the words in the given sense pool). The k words are assumed to be semantically dissimilar or unrelated to the target word, and taken as the replacing words to calculate k scores for the random pairs (by steps (2) X (4)). The randomized score is then calculated by averaging the k scores. Therefore, each pair has its own randomized score, representing the baseline value for the substitution. Table 4 shows an example of the randomized score ( k = 3) for the input pair (tunnel, overpass). (6) Quality pair selection : So far, each substitution pair is associated with a substitution score and a randomized score. The adequate substitutions, i.e., quality pairs , can then be determined by comparing these two scores. This study develops two unsupervised (1 and 2) and one supervised strategies (3), as described in detail below.

Strategy 1: Selection with difference : A pair is called a quality pair if and only if the substitution score is greater than the randomized score.
 greater than the randomized score. An independent t -test with confidence level 0.95 is used to determine if the difference of the two scores is statistically significant.
 classifier ( Duda, Hart, &amp; Stork, 2000 ) is positive. The LDA used herein is to decide an optimal decision boundary according to both substitution scores and randomized scores. Each substitution pair can then be classified into quality (positive) or non-quality (negative) pair (detailed in Section 4.3). 4. Experimental results 4.1. Experiment setup (1) Data : This experiment focuses on the verification for sense pools derived from noun and verb senses. A total of 500 sense pools were selected from the Omega ontology, which produced 2360 substitution pairs for verification. Table 5 pre-sents some example pools. Additionally, the annotated sentences were selected from the English corpus of OntoNotes v1.0, i.e., Wall Street Journal (WSJ). Table 6 presents the statistics for the experimental data.

The maximum number of annotated sentences that could be selected for each pair (target word) was restricted to 50 to limit computational complexity. Additionally, the pairs with five or fewer sentences were excluded from the experiment, since a smaller number of sentences may produce an unstable substitution score. (2) Human evaluation : In order to develop a gold standard for evaluation, three native English speakers (two as the ver-ifiers, and the third as the adjudicator) involved to judge each substitution pair as adequate or inadequate. For each pair, seven sentences annotated with the target word were randomly selected, and then the target word in each sentence was substituted by the replacing word. These after-substitution sentences along with the definitions of the target word and replacing word were presented to the two verifiers. The verifiers examined the substitutions in the sentences to check the semantic substitutability, and rated them by the following criteria.
 Positive -1: Exactly substitutable.
 Positive -2: Essentially substitutable.
 Negative -1: Mostly substitutable, but differ in one important aspect.
 Negative -2: Not really substitutable.
 Neutral : Don X  X  know.

Each sentence was associated with one of the five ratings, where Positive -1 and Positive -2 represents +1 point, respec-tively, Negative -1 and Negative -2 represents 1 point, respectively, and Neutral represents 0 point. For each pair, if the num-is accepted as an adequate substitution. All the disagreements between the two numbers were judged by the adjudicator for final decisions. The agreement of the two verifiers was 74.8%, indicating that the task is difficult. (3) Evaluation metrics : The evaluation metrics, recall , precision , and accuracy were used to compare the performance of the strategies for quality pair selection, as described below:
In the following sections, we conduct experiments to show the results of unsupervised and supervised verification, and the effect of high-frequency words on substitutions. 4.2. Results of unsupervised methods
The proposed two unsupervised strategies, i.e., selection with difference ( Diff. ) and statistical test ( Stat. ), were imple-mented for sense pool verification. The substitution scores and randomized scores were calculated using steps (1) X (5) de-scribed in Section 3. Additionally, the randomized scores were generated by randomly selecting three senses from the corpus. Table 7 shows a sample of the unsupervised verification results. The Diff. selected quality pairs (QPs) by comparing the difference between their substitution scores and randomized scores. The results show that Diff . correctly accepted 6 pairs (n1, n3, n4, v2, v3, v4), and correctly rejected one pair (v5). The Stat. in addition compared the two scores using t -test, and correctly accepted five pairs (n3, n4, v2, v3, v4), correctly rejected four pairs (n2, n5, v1, v5). The column  X  X  X uman(score) X  represent human-rated scores, which were calculated based on the positive X  X egative criteria described in Section 4.1. A po-sitive score indicates an adequate substitution, denoted as  X  X  X  X , otherwise, denoted as  X  X  X  X . For instance, Y (7) for the pair (arm, weapon) indicates a strong positive substitution since all the seven test sentences for this pair were rated as Positive -1 or Positive -2.

Table 8 presents the accuracy, recall rate and precision rate of both strategies. The columns Bigram, Trigram and 4-gram represent the performance of using bigrams, trigrams and 4-grams to the calculate substitution scores and randomized scores. The results indicate that the use of 4-grams achieved better performance for all part-of-speech . This reveals that 4-grams are more effective on measuring the semantic substitutability. We did not include a comparison of 5-grams or larger since most of their frequencies are zero. The recall rate of Diff. was high but the precision was low because it accepted more pairs that should be rejected. The Stat. can correctly reject more pairs by comparing the substitution scores and randomized scores with significance tests, but not just the difference of the scores. The results show that Stat. yielded an improvement of 21.4% in precision and 19.0% in accuracy (4-gram) for all part-of-speech . Additionally, both Diff. and Stat. achieved higher performance on noun senses than verb senses. One possible reason is that verbs are likely to be more constrained in sentences.

The choice of strategy may depend on the application domain. For automatic applications such as information retrieval and writing support systems, human intervention is often not possible or practical. In this circumstance, the statistical ap-proach is more useful because it can automatically suggest more correct substitution information for query expansion and alternative word selection. For semi-automatic applications such as corpus cleanup, a higher recall rate might be preferred since human intervention can be applied in the later stage for final decision. 4.3. Results of supervised methods
In order to compare the proposed unsupervised methods with other methods, this experiment implemented an LDA clas-sifier using six features, namely substitution scores (bigram, trigrams, 4-grams) and randomized scores (bigram, trigrams, 4-grams). The human labeled data, i.e., the gold standard, was used for training and testing using 5-fold cross validation. In each fold, the LDA classifier iteratively selected the best features to decide an optimal decision boundary for classification.
In this experiment, the best features are the substitution score and randomized score with 4-grams, which corresponds to the results described in the previous section. Table 9 shows the comparative results.
 The results show that LDA achieved higher precision and accuracy than did both Stat. and Diff for all part-of-speech.
Meanwhile, the difference of accuracy between LDA and Stat. was approaching: 4.4%, 6.7% and 5.7% for noun senses, verb senses and all part-of-speech, respectively. This indicates that the proposed unsupervised statistical test method can achieve comparable performance with the supervised method. 4.4. Results of substitutions at synonym level and near-synonym level
A sense pool contains a set of near-synonymous words; that is, not all words in a pool are WordNet synonyms. Therefore, the substitution pairs generated from sense pools can be classified into two groups: synonym pairs and near-synonym pairs.
A substitution pair is considered as a synonym pair if the words of the pair are synonyms in WordNet version 2.1. Otherwise, the substitution pair is considered as a near-synonym pair. Table 7 presents some examples of synonym pairs and near-syn-onym pairs. Based on this classification, the substitutions can be measured at two different levels: the synonym level and near-synonym level. In order to investigate the difference of substitutions at these two levels, this experiment included a comparison of their substitution scores. For the synonym level, the substitution score was calculated by averaging the sub-stitution scores of synonym pairs. For the near-synonym level, the substitution scores were calculated by averaging the sub-stitution scores of near-synonym pairs and all pairs (synonym pairs + near-synonym pairs), respectively. Table 10 shows the comparative results.

The results show that the average substitution scores of both near-synonym pairs and all pairs were lower than those of synonym pairs for all part-of-speech. One possible reason is that sense pools represent more coarse-grained sense distinc-tions. Accordingly, near-synonym substitutions at this level may suffer from more serious context mismatch problem, thus yielding a negative impact on substitution scores.

From the other side, near-synonyms may have a positive impact on application performance. For instance, sense pools can be used for alternative word selection in a writing support system. Consider the example illustrated in Table 11 .
In this example, the given sense pool is {A, B, C, D, E, F}, and the target word is A. The possible choices for alternative word selection are B, C, D, E and F. Suppose that B and C are the synonyms of A, and D, E, and F are the near-synonyms. The system is then asked to suggest a set of alternative words that best fit the context of the target word from the possible choices. This can be accomplished by evaluating each substitution pair using the proposed verification algorithm presented in Section 3.
Table 11 shows that the system (Machine) suggests {B, D, F} as the answer set. In human evaluation results, the gold stan-dard is divided into two levels of answers: the synonym level and near-synonym level, depending on whether or not the answers are synonyms of the target word. If the substitutability criterion is restricted to the synonym level, then only the synonyms of the target word (i.e., B and C) are considered as answers. Therefore, the recall and precision are 1/2 and 1/3, respectively. The substitutability criterion can be relaxed to the near-synonym level by considering both synonyms and near-synonyms of the target word (i.e., B, C, D and E) as answers. In this condition, the recall and precision can be improved to 1/2 and 2/3, respectively.

In order to compare the performance difference of the synonym level and near-synonym level, this experiment divided then re-calculated at both levels. Table 12 presents the comparative results.

In the synonym level, the number of answers was less than the near-synonym level. Therefore, all methods tend to accept more answers that should be rejected, yielding lower performance (mainly in precision and accuracy) in the synonym level.
After additionally considering near-synonyms, the precision and accuracy were improved for all methods. This finding indi-cates that relaxing the substitutability criterion from the synonym level to the near-synonym level contributes to system performance.
 4.5. Effect of high-frequency words
The substitution scores and randomized scores are expected to reflect the nature of semantic substitutability. However, high-frequency words may introduce interference into the estimation of the two scores. Therefore, this study conducted an experiment to investigate the effect. First, the substitution pairs were sorted in the increasing order of the frequencies of the target words. The sorted substitution pairs were then divided into 10 equal groups in the same order. For each group, the average frequency, substitution score and randomized score were computed for comparison. Fig. 3 shows the results.
The results show that either the average substitution score or randomized score did not continue to increase with the increase of the average frequency. In fact, a high-frequency may result in a low score, and vise versa. For more detailed anal-ysis, the top 10% and last 10% of the sorted pairs were taken for comparison, as presented in Table 13 .
The average frequency of the top 10% pairs is much higher than that of the last 10% pairs. However, the differences of both ing indicates that the estimation of substitution scores and randomized scores was insensitive to high-frequency words. The major reason that affects the estimation of the scores is still the degree of context mismatch in substitutions. 5. Conclusion
This paper has presented an unsupervised algorithm for sense pool verification. Verification follows a two-stage frame-work, Google n -grams and a statistical test. The n -gram frequencies obtained by querying Google are used for measuring the degree of context mismatch in substitutions. A statistical test can then be applied to determine whether a word in a pool can be substituted by other words in the same pool. Experimental results show that the proposed unsupervised method can achieve comparable performance with the supervised method.

Future work will focus on two directions. First, more significant features will be combined with n -grams to boost verifi-cation performance. For instance, skipgrams (Cheng, Greaves, &amp; Warren, 2006 ) and semantic language patterns (Yu, Wu, Yeh, &amp; Jang, 2008 ), which can capture non-contiguous word associations in sentences, will be investigated to improve the recall rate. Second, more experiments, such as evaluation of WordNet synsets, will be conducted to validate the proposed method as well as its practical applications.
 Acknowledgement
This work was supported by the National Science Council, Taiwan, ROC, under Grant No. NSC 97-2218-E-155-011. The authors would like to thank the anonymous reviewers and the guest editors for their constructive comments. References
