 ORIGINAL PAPER Da-Han Wang  X  Cheng-Lin Liu Abstract Handwritten text recognition systems commonly combine character classification confidence scores and con-text models for evaluating candidate segmentation-recognition paths, and the classification confidence is usually optimized at character level. In this paper, we investigate into different confidence-learning methods for handwritten Chi-nese text recognition and propose a string-level confidence-learning method, which estimates confidence parameters by directly optimizing the performance of character string recognition. We first compare the performances of para-metric (class-dependent and class-independent parameters) and nonparametric (isotonic regression) confidence-learning methods. Then, we propose two regularized confidence esti-mation methods and particularly, a string-level confidence-learning method under the minimum classification error cri-terion. In experiments of online handwritten Chinese text recognition, the string-level confidence-learning method is shown to effectively improve the string recognition perfor-mance. Using three character classifiers, the character correct rates are improved from 92.39, 90.24 and 88.69% to 92.76, 90.91 and 89.93%, respectively.
 Keywords Handwritten text recognition  X  Confidence learning  X  Parametric and nonparametric  X  Class-dependent and class-independent  X  String-level learning 1 Introduction Handwritten text (character string) recognition is commonly solved by the integrated segmentation-recognition approach to overcome the ambiguity of segmentation [ 1 , 2 ]. In this framework, candidate characters are formed by concatenat-ing over-segmented segments and are assigned candidate classes by a character classifier. The candidate characters and classes form a candidate segmentation-recognition lat-tice, whose paths are evaluated by combining the character classification scores, linguistic and geometric contexts. The optimal path of maximum score gives the result of character segmentation and recognition.

In the path evaluation function integrating multiple con-texts, the outputs of character classifier and geometric models are usually distances or similarities, unlike the statistical lan-guage model that gives probabilities directly. To better com-bine character classification with context models, confidence transformation (CT) [ 3 ] has been applied to convert classi-fier outputs into posterior probabilities [ 4  X  6 ]. Li et al. [ 4 ] used the adaptive confidence transform (ACT) method pro-posed in [ 7 ] and the logistic regression model for confi-dence transformation. Jiang et al. [ 5 ] applied the soft-max framework to transform classifier outputs into confidence values. Wang et al. [ 6 ] show that considering the probability of outlier/non-character class in confidence transformation benefits the character string recognition performance. These methods estimate confidence parameters only at character level, however.

According to the score mapping function, the con-fidence transformation methods can be categorized into two classes: parametric and nonparametric. Parametric CT methods, such as the logistic regression and soft-max, assume a specific functional form of score mapping. Non-parametric CT methods directly provide a lookup table for mapping classifier output values to probability val-ues.

As a parametric form, the sigmoidal function has been commonly used to approximate posterior probability for two-class (or one-versus-all) classification problems with para-meters learned by logistic regression [ 8 , 9 ]. Assuming that the two classes have one-dimensional Gaussian densities with equal variance, the class posterior probability is a sig-moidal function [ 10 ]. In multi-class problems, the multiple one-versus-all sigmoidal probabilities can be combined into multi-class probabilities using the Dempster X  X hafer (D X  X ) theory of evidence [ 3 , 11 ]. The soft-max function is often used to directly give the multi-class probabilities subject to the constraint that the probabilities sum up to one, excluding the probability of outlier class. Wang et al. [ 6 ] compared the performances of the sigmoidal function, D X  X  evidence and the soft-max function in handwritten Chinese character string recognition, and showed that the D X  X  evidence performs best, the sigmoidal function performs fairly well while the soft-max function deteriorates the performance. This result is related to the important effect of non-character resistance of classifier in integrated segmentation-recognition of character strings [ 12 ].

Instead of assuming a functional form, nonparametric methods directly provide a correspondence table from classi-fier outputs to probability values. Nonparametric confidence values are usually estimated for interval values (or bins) of outputs by discretizing the classification output space. The confidence value for a given bin is often estimated as the proportion of samples that belong to the class in a training dataset falling into the bin, which has the theoretical ground of Bayesian decision. There are two methods for learning the mapping: binning [ 13 ] and isotonic regression [ 14 , 15 ]. The binning method partitions the output space into bins of fixed equal size, and then estimates the probabilities using the Bayes rule. The bin size should be carefully set to get a good estimation. The isotonic regression automatically learns the bins which may have unequal intervals and was shown to outperform the binning [ 14 , 15 ]. The commonly used learn-ing algorithm for isotonic regression is pair-adjacent vio-lators (PAV) [ 16 ]. The confidence transform method of Lin et al. [ 7 ] is similar to binning except that the classifier outputs are transformed into generalized confidence values before binning.

In the context of handwritten text recognition, both para-metric and nonparametric confidence transformation are learned at character level and has not been learned in string-level training. Though the nonparametric method has been applied in handwritten Chinese character recognition, it has not been evaluated in the integrated segmentation-recognition framework. The functional mapping of paramet-ric methods enables parameter learning at string level using character string samples.

In this paper, we investigate into different confidence-learning methods of character classifier for handwritten Chi-nese text recognition. We consider class-dependent (CD) and class-independent (CI) confidence parameters in para-metric methods and compare their performance with the nonparametric isotonic regression. Based on the compari-son, we propose two regularized CD parametric methods and particularly, a string-level confidence-learning method under the minimum classification error (MCE) criterion [ 17 ]. In string-level learning, the confidence parameters are esti-mated to directly optimize the character string recognition performance. Experimental results on a large database of online Chinese handwriting CASIA-OLHWDB [ 18 ] demon-stratetheeffectivenessofthestring-levelconfidence-learning method.

In [ 6 ] and [ 20 ], the authors have shown that confi-dence transformation improves the handwritten text recogni-tion performance significantly (e.g., character correct rate from 85.52 to 90.80% on an offline Chinese handwrit-ten text database [ 20 ]) compared to taking classifier output scores directly. From this baseline, in this paper, we further investigate into different confidence transformation meth-ods including parametric ones and nonparametric ones, com-pare their performances, and propose two regularized learn-ing methods and particularly, the string-level confidence-learning method, which are the main contributions of this paper. Experimental results demonstrate that the string-level confidence-learning method further improves the string recognition performance.

This study is based on the integrated segmentation-recognition framework combining character classification and context models. The character classifier can be trained either at character level or string level, though we use character-level trained classifiers in our experiments. Train-ing classifier at character level is still a practical approach in Chinese text recognition owing to the availability of very large number of isolated character samples.

The rest of this paper is organized as follows. Section 2 describes the overview of handwritten Chinese text recog-nition system. Section 3 describes the character-level confi-dence transformation methods used in our system. The pro-posed string-level confidence-learning method is described in Sect. 4 . Section 5 presents the experimental results, and Sect. 6 offers concluding remarks. This paper is an extension toourpreviousconferencepaper[ 19 ]byaddingexperimental results of nonparametric confidence transformation, experi-ments with different types of classifiers, and extensive results of string-level confidence learning and discussions. 2 System overview Figure 1 shows the diagram of our handwritten Chi-nese text recognition system using integrated segmentation-recognition. The input string is first over-segmented into primitive segments, and consecutive segments are combined to generate candidate characters, which are assigned candi-date classes by the character classifier. The candidate charac-ters and classes form a candidate segmentation-recognition lattice (an example shown in Fig. 2 ). The paths in the can-didate lattice are evaluated by combining classifier outputs, linguistic and geometric context models, and the optimal path gives the string recognition result. We use the path evaluation criterion presented in [ 20 ] for a candidate character sequence X and string class C : f (
X , C ) = where k i is the number of primitive segments composing the tion confidence, P ( c | g uc ), P ( z p = 1 | g ui ), P ( and P ( z g = 1 | g bi ) are four (unary class-dependent, unary class-independent, binary class-dependent and binary class-independent) geometric context scores, and  X  i , i = 1 ,..., are five combining weights that balance the different contri-butions of different models. The idea of weighting character classification scores with multiplier k i follows the variable length HMM of [ 21 ], which is to make the sum of classifica-tionscoresinsensitivetothepathlength(numberofcandidate characters) and enables optimal path search by dynamic pro-gramming (DP). In [ 20 ], this path evaluation function gives comparable performance with the one weighting the candi-date character by its width, both are among the best perform-ers. Note that both the character classification outputs and geometric context scores are converted to posterior probabil-ities by confidence transformation.

Since character classification plays the most important role in handwritten text recognition, we focus on the effec-tiveness of string-level confidence learning for the charac-ter classifier and do not consider the string-level confidence learning for geometric models. The outputs of geometric models are converted into probabilities by the conventional sigmoidal function. 3 Confidence transformation In this section, we introduce the representative character-level parametric and nonparametric confidence transfor-mation methods, and our proposed two regularized class-dependent parametric methods. 3.1 Parametric confidence transformation The sigmoidal function gives the posterior probability of a class in the one-versus-all setting. For an M -class problem, each class  X  j ( j = 1 ,..., M ) is given a one-versus-all prob-abilityfromthecorrespondingoutputclasssimilarity(orneg-ative distance) S j : P sg ( X  j | x ) = 1 where {  X  j , X  j } are the confidence parameters. For each class, 1  X  P sg ( X  covering the other classes as well as the outlier/non-character class. The multiple sigmoidal two-class probabilities can be combined into multi-class probabilities using the Dempster X  Shafer (D X  X ) theory of evidence [ 6 , 11 ]: P ds ( X  j | x ) = exp which satisfies M i = 1 P ds ( X  i | x )  X  1, and the complement to one gives the probability of outlier class, which is desirable for non-character resistance in character string recognition.
In the above methods, if the multiple classes share the same confidence parameters  X  j =  X  0 , X  j =  X  0 , the confi-dence learning is called class-independent (CI), otherwise is class-dependent (CD).

Traditionally, the confidence parameters are estimated by minimizing the cross entropy (CE) loss function on a valida-tion dataset of character samples (preferably different from the dataset for training classifiers) [ 3 ]: min where N is the number of validating samples and P j is the sigmoidal probability of class  X  j . The parameters are learned by optimization by stochastic gradient descent [ 22 ]. 3.1.1 Regularized class-dependent CT Our experiment results (in Sect. 5 ) show that when estimating confidence parameters at character level, class-independent CT outperforms class-dependent CT in character string recognition. This can be attributed to the over-fitting effect of class-dependent CT with many parameters. We thus consider two regularized versions of class-dependent CT for alleviat-ing the over-fitting.

From Eq. ( 2 ), the underlying classification rule for sep-arating the class  X  j and the negative class is: classify x classification point (decision boundary) can be obtained by setting P sg ( X  j | x ) = 0 . 5 , that is  X  j S j ( x ) +  X  S point is different for different classes.

In this paper, we consider adding the constraint that all the classes have the same classification point: s = X   X  i / X  i 1 , 2 ,..., M . This variation can be considered as a trade-off between class-dependent CT and class-independent CT, called SCCD (Same Classification Class-Dependent) in this paper. It has M + 1 parameters (  X  1 ,..., X  M and s ). Figure 3 shows an example of three sigmoidal functions of SCCD for a character classifier. From the figure, we can see that while the same separation boundary is kept, differ-ent values of  X  provide variable probability approximation capabilities.

The second proposed method is to regularize class-dependent CT using shared parameters:  X  i = ( 1  X   X ) X  i +  X   X   X  where  X   X  = 1 M M j = 1  X  j , and  X  i is regularized in the same way. This variation regularizes the parameters such that  X  and  X  i do not differ significantly between classes and is called ReguCD in this paper. 3.2 Nonparametric confidence transformation A confidence mapping from classifier output scores into probabilities should be monotonically non-decreasing, i.e., the larger the class similarity score, the larger the mapped probability. A promising nonparametric method meeting this property is the isotonic regression, which was shown to out-perform the binning [ 14 , 15 ].

Isotonic regression can be viewed as a nonparametric form of regression, in which bins and mapped confidences are learnedautomatically,commonlyusingthepair-adjacentvio-lators (PAV) algorithm [ 15 , 16 ], which finds the stepwise con-stant isotonic function that best fits the data in the sense of mean squared error. In two-class classification setting, the PAV algorithm is illustrated in Algorithm 1. In initialization, the training samples are sorted in increasing order accord-ing to their output scores, each sample viewed as a bin and assigned initial confidence probability 0 (negative sample) or 1 (positive sample). In iterative updating, check all the pairs of neighboring bins, if the probabilities of two bins violate the non-decreasing constraint, e.g., g i &gt; g i are called pair-adjacent violators), the two bins are merged and the merged probability is the fraction of positive sam-ples in the merged bin. If there are no pair-adjacent violators (the probabilities are isotonic), the process is terminated, and the current probabilities are used as the final mapped probabilities.
When applying nonparametric CT to handwritten Chinese text recognition, we consider the multi-class classification problem as multiple one-versus-all two-class problems sim-ilarly to the sigmoidal function. After mapping the two-class probability of each character class using the PAV algorithm, the two-class probabilities are combined into multi-class probabilities using the D X  X  theory of evidence as before.
For multi-class classification, the nonparametric CT by isotonic regression can be either class-dependent or class-independent. The former case is treated as multiple one-versus-all problems as addressed above. In the latter case, the scores of positive and negative samples of all classes are pooled to estimate a confidence mapping, which is applied to the confidence transformation of every class. Class-dependent nonparametric CT may suffer from the insufficiency of positive samples of each class. The class-independent method overcomes this problem by using the samples of all classes as positive samples. 4 String-level confidence learning After character-level CT, the mapped character class prob-abilities are inserted into the path evaluation function in Eq. ( 1 ). Thus, the parameters of CT are also the parameters of character string recognition. The parametric functional form of parametric CT allows the confidence parameters to be learned at string level, i.e., estimated to optimize the string recognition performance on string samples. In string-level learning, we learn the confidence parameters and the context combining weights simultaneously under the minimum clas-sification error (MCE) criterion, which has been widely used in speech recognition and handwriting recognition [ 17 , 23  X  26 ]. The parameters of nonparametric CT cannot be learned in this framework, however. 4.1 MCE training framework Assume classifying a string sample X into L string classes {
C nese text recognition. Following [ 17 ], the misclassification measure on the string sample is approximated by d (
X ,) = X  g ( X , C t ,) + g ( X , C r ,), (5) where is the parameter set, g ( X , C t ,) is the discriminant function for the true string class C t , g ( X , C r ,) is the dis-criminant function of the closest rival class: g ( X , C r max C k = C t g ( X , C k ,) . The misclassification measure is transformed to a smoothed zero-one loss by the sigmodal function: l (
X ,) = where  X  is a parameter to control the hardness of sigmoidal nonlinearity. The parameters in MCE training are learned by stochastic gradient descent [ 22 ] on each training sample iteratively by ( t + 1 ) = ( t )  X   X ( t ) U  X  l ( X ,) | = ( t ) , (7) where  X ( t ) is the learning step, and U is related to the inverse of Hessian matrix and is usually approximated to be diagonal.
In string-level training, the parameters are estimated to optimize the string recognition accuracy. The discriminant function is the path evaluation function in Eq. ( 1 ), and the parameters include the confidence parameters and the com-bining weights. The rival segmentation-recognition path, which is the most confusable one with the correct one, is obtained by beam search [ 27 ]. 4.2 Implementation issues In Eq. ( 3 ), the outputs of M defined classes are involved in the multi-class probabilities computed by the D X  X  the-ory of evidence. For Chinese character recognition, there are thousands of classes (e.g., M = 7 , 356 in our experiments), which makes the computation of D X  X  evidence difficult. In implementation, we only use  X  M ( M ,say,  X  M = 200 in our experiments) top rank classes to compute the D X  X  evidence while viewing the probabilities of remaining classes as zero, and Eq. ( 3 ) becomes: P ( c i | x i ) = where x i is the i -th candidate pattern in a path of the can-didate segmentation-recognition lattice; c i , the hypothesized class of x i ,isthe q -th rank class (  X  r q ) output by the charac-ter classifier, q  X  X  1 ,...,  X  M } . For the convenience of illus-tration, we denote the set of  X  M top rank classes of x i D the precision of classification confidence and the computa-tion cost. We will show in experiments that when  X  M = 200, the cumulative classification accuracy of top ranks is as high as over 99 %. 4.2.1 Calculating gradient for class-dependent CT For learning confidence parameters by stochastic gradient descent under the string-level MCE criterion, the key is to calculate the partial derivatives of the loss function [Eq. ( 6 )] with respect to the parameters. For class-dependent CT, we first give the partial derivative with respect to the parameter  X  j of class  X  l  X  X  where f t and f r are the path scores [Eq. ( 1 )] for the segmentation-recognition path of the ground-truth string class and the rival segmentation-recognition path, respec-tively. In above equation,  X  f  X  X   X  f  X  X  where n is the number of character patterns in the candidate segmentation-recognition path, k i is the number of segments composing the character pattern x i , D i is the set of top rank classes of x i , c i ( c i  X  D i ) is the hypothesized class of x process is suitable for both the ground-truth segmentation-recognition path f t and the rival path f r .For f t , x i character pattern in the truth segmentation and c i is the truth segmentation of the rival path and c i is the recognition class of x i .

Consequently, the partial derivative  X  log P ( c i | x i ) lated in two situations below. 1. when  X  j = c i ( j = r q ), 2. when  X  j = c i ( j = r q ), In the formulas, P j is for short of P ( X  j | x i ) . The above for-mulas are for calculating the partial derivatives of both f and f t with respect to  X  j .

The partial derivative with respect to  X  j is calculated sim-ilarly as 1. when  X  j = c i ( j = r q ), 2. when  X  j = c i ( j = r q ), 4.2.2 Calculating gradient for SCCD CT The SCCD method has M + 1 confidence parameters: separa-tionboundary S and  X  j , j = 1 ,..., M . Thepartial derivative with respect to the separation boundary S is calculated by  X  f  X 
S where,  X  log P ( c i | x i )
The partial derivative with respect to  X  j is calculated in two situations below. 1. when  X  j = c i ( j = r q ), 2. when  X  j = c i ( j = r q ), 4.2.3 Calculating gradient for ReguCD CT For the regularized class-dependent CT, since the parameters  X  and  X  j areregularizedbythecommonaverageparameters, the D X  X  evidence in Eq. ( 8 ) involves confidence parameters of all the M classes (In class-dependent CT and SCCD CT, only the parameters of  X  M classes are involved). The partial derivative with respect to  X  j is calculated in three situations as follows. 1. when  X  j = c i ( j = r q ), 2. when  X  j  X  D i but  X  j = c i , 3. when w j /  X  D i ,
Similarly, the partial derivative with respect to  X  j is cal-culated as follows. 1. when  X  j = c i ( j = r q ), 2. when  X  j  X  D i but  X  j = c i , 3. when  X  j /  X  D i , 5 Experiments We evaluated the performance of confidence transforma-tion (CT) methods in online handwritten Chinese text line recognition on a large database CASIA-OLHWDB [ 18 ]. The compared confidence-learning methods are categorized in Fig. 4 . The nonparametric CT by isotonic regression can be either class-dependent or class-independent. Because class-dependent isotonic regression involves large number of para-meters (bins and corresponding probabilities of all classes) and suffers from the insufficiency of positive samples of each class, we only give the results of class-independent isotonic regression.

The database CASIA-OLHWDB is a large-scale, uncon-strained handwriting database of online Chinese charac-ters and texts, collected by asking 1,020 writers to produce both isolated characters and continuous texts. It contains six datasets, three for isolated characters (DB 1.0 X 1.2, called DB1 for short) and three for handwritten texts (DB 2.0 X 2.2, called DB2 for short). There are 3,912,017 isolated character samples in DB1 and 52,220 handwritten text lines (consist-ing of 1,348,904 character samples) in DB2 in total. Both the isolated data and handwritten text data have been divided into standard training and test subsets. Table 1 shows the statistics of the text dataset DB2. Its training set and test set are used for string-level training and performance evaluation, respectively. Figure 5 shows two examples of text pages in DB2.
 5.1 Experimental setup and performance metrics We use two types of classifiers that have been widely used in Chinese character recognition [ 28 ]: modified quadratic dis-criminant function (MQDF) [ 29 ] and nearest prototype clas-sifier. The MQDF is among the most popular and effective classifiersusedinhandwrittenChinesecharacterrecognition. The nearest prototype classifier (NPC) is less accurate than theMQDFbut has muchlower complexityinrecognition. We use two variations of NPC depending on the prototype learn-ing algorithm: one is trained by the LOG-likelihood of Mar-gin criterion (NPC-LOGM) [ 30 ], and one trained by One-versus-all criterion (NPC-OVA) [ 31 ]. The classifier parame-ters were learned on 4/5 of training character samples (both the isolated characters in the training set of DB1 and the segmented characters in the training set of DB2, 4,207,801 samples in total), and the remaining 1/5 training samples were used for character-level confidence parameter estima-tion. The training character samples fall in 7,356 classes, including 7,185 Chinese characters and 171 alphanumeric characters and symbols.

The character classifier uses the local stroke direction his-togram feature, implemented by the method of [ 32 ] with bi-moment normalization. To reduce the complexity of the clas-sifier, the extracted 512D feature vector is projected onto a 160D subspace learned by Fisher linear discriminant analysis (FLDA). The MQDF classifier uses 50 principal eigenvectors per class, and the NPC uses one prototype per class.
Table 2 shows some statistics of character samples seg-mented from the test pages of DB2. The  X  X ec X  row gives the correct rate of segmented character recognition by character classifiers, and  X  X ec n  X  denotes the cumulative accuracies of top n ranks. We can see that, with more than 20 ranks, the cumulative accuracy increases only slightly when increasing the number of top ranks. For all the three classifiers, the cor-rect rate of Chinese characters is the highest among four char-acter types, and the MQDF classifier is the highest for Chi-nese characters among three classifiers. The non-characters are abnormal samples and labeled as non-characters in the database, and outliers are the characters out of the defined 7,356 classes. These results have been presented in [ 27 ], and we list them here for convenience of reference.

In addition to the character classifier, the character string recognition system uses a statistical language model and four geometric models. The character bi-gram language model was trained on a text corpus containing about 50 million char-acters (about 32 million words) [ 6 ]. The geometric models were constructed using a method similar to that in [ 33 ]on the training set of DB2, and the outputs of geometric mod-els are converted to posterior probabilities by the sigmoidal function. More details of the geometric models can be found in [ 27 ].

Due to the summation nature of the path evaluation crite-rion in Eq. ( 1 ), the optimal path can be searched for by the dynamic programming (DP) algorithm. We further apply the beam search strategy to accelerate DP search by pruning the partial paths at intermediate nodes. The search algorithm is similar to the forward procedure in the Viterbi decoding algo-rithm [ 34 ] and has been presented in [ 20 , 27 ] in more details. Beam search also helps find multiple paths, including the optimal one and the competing rival one.

In string-level confidence parameter learning, the number of top rank classes for computing the D X  X  evidence [  X  M in Eq. ( 8 )] is set as 200, while the probabilities of the remain-ing classes are considered as zero, which is shown to be appropriate in our experiments. Whereas in the candidate segmentation-recognition lattice, only the top 20 candidate classes and their corresponding confidence probabilities are retained for each candidate character pattern, because this number provides a good trade-off between the string recog-nition accuracy and the search complexity [ 20 ]. The maxi-mum number of segments composing a candidate character pattern (denoted as SN) is set as 6 which is large enough to generate nearly all true characters. In path search, the beam width (BW, number of partial paths retained) is set as 10 as in [ 27 ].

To evaluate the performance of character string recogni-tion, we use two character-level metrics [ 20 , 27 , 35 ]: Correct rate (CR) and accurate rate (AR): CR = ( N t  X  D e  X  S e )/ N t , (21)
AR = ( N t  X  D e  X  S e  X  I e )/ N t , (22) where N t is the total number of characters in the ground-truth transcript. The numbers of substitution errors ( S e ), deletion the recognition result string with the transcript by dynamic programming. The metric CR denotes the percentage of char-acters that are correctly recognized, while the metric AR is also affected by the number of mis-inserted characters due to over-segmentation. 5.2 Comparing parametric and nonparametric methods We first compare the performances of parametric and non-parametric confidence transformation (CT) learned at char-acter level. We give the results of class-dependent (CD) and class class-independent (CI) parametric methods, and CI nonparametric isotonic regression. The results of CD iso-tonic regression are inferior due to the insufficiency of posi-tive samples of each class and are not given here for saving space. In CI isotonic regression learned by the PAV algo-rithm, the number of learned bins is around 450. For any of the methods, the one-versus-all two-class probabilities are combined into multi-class probabilities using the D X  X  theory of evidence. All the methods were tested in string recognition using two path evaluation schemes: character classification + language model (LM), character classification + LM + geo-metric models (GM).

Before comparing different CT methods, we conduct experiments to evaluate the effects of the number  X  M for computing the D X  X  evidence on the string recognition per-formance. Table 3 shows the string recognition results of the character-level class-independent CT with varying num-ber of  X  M . From the results, we can see that, with  X  M the variation of this number makes little difference of string recognition accuracies. This is due to the fact that, when  X  M &gt; 20, the cumulative accuracy increases only slightly when increasing the number of top ranks, and increas-ing the ranks also increases the confusion of candidate segmentation-recognition paths. We nevertheless choose  X  to be 200 for a good trade-off between the precision of confi-dence estimation and the computation cost dependent on the number of top ranks. For class-dependent and nonparamet-ric CT methods, the experimental results show similar effects of  X 
M , and so, their results are not provided here for saving space. In the following experiments, the  X  M is set as 200.
With these settings, the results of different CT methods are shown in Table 4 .

Table 4 shows that adding geometric models (GM) can significantly improve the performance of character string recognition, and regarding the character classifier, the MQDF classifier outperforms the NPC. Similar results have been reported in the literature [ 20 , 27 , 36 ]. We herein focus on the comparison of CT methods in the same setting of character classifiers and context models. We have the following obser-vations. 1. Comparing parametric CT methods, the class-2. Comparing the best results of parametric and nonpara-3. With the classifier NPC-OVA, the three CT methods per-
Figure 6 shows the curves of confidence mapping for two class-independent methods: sigmoidal function and isotonic regression. The function of confidence mapping depends on the character classifier. We can see that for the classi-fiers NPC-LOGM and NPC-OVA, the curves of sigmoidal function and nonparametric isotonic regression are close to each other or have similar tendency, which explains the comparable performances of nonparametric and para-metric CT on the two classifiers. For the MQDF classi-fier, however, the curves of nonparametric and parametric mappings show significant difference at the domain above the separation point (confidence 0.5). This is because the MQDF is a generative model, and in the one-versus-all setting, does not separate the positive and negative sam-ples very well. Due to the many negative samples mis-classified as positive in the overlapping domain, when learning probabilities by PAV, the scores of negative sam-ples and positive samples are averaged in the overlap-ping domain to give probabilities much lower than the sig-moidal function. After this overlapping domain, the prob-ability mapped by PAV steps up to one abruptly. This explains why nonparametric confidence mapping gives infe-rior string recognition performance compared to the para-metric method. 5.2.1 Performance of character-level regularized CT The ReguCD method has a hyperparameter (  X  ), which is a regularization coefficient. We set  X  as 0.3, 0.5, 0.7. and 0.9 to test its effect on the string recognition performance. The SCCD has no hyperparameter.

The string recognition results of ReguCD (with four dif-ferent values of  X  ) and SCCD, both learned at character level, are shown in Table 5 . Comparing the results of ReguCD with different values of  X  , we can see that, when  X  approaches 1, the string recognition performance is also close to that of the class-independent CT (which is the case  X  = 1). This indicates that the regularized class-dependent method can consistently improve the performance of class-dependent method, which deteriorates due to the over-fitting. Compar-ing with the results of class-dependent and class-independent parametric methods in Table 4 , we can see that except the case of  X  X haracter classification + LM X  with NPC-OVA, the regularized method ReguCD improves the performance of class-dependent parametric method significantly and com-petes with the performance of the class-independent para-metric method. The SCCD (same classification point class-dependent) method, however, does not improve the perfor-mance evidently except for the classifier NPC-LOGM. This result implies that sharing confidence parameters between classes is more important than the same classification point constraint.

In the next section, we will evaluate the string-level learn-ing of confidence parameters. For ReguCD, we set  X  = 0 . 5 as a typical choice in the following experiments. The exper-iments in Sect. 5.3 show that the ReguCD method with string-level learning provides comparable favorable results with string-level learned class-dependent method and SCCD, which implies that  X  does not affect the performance of string-level confidence learning, and on the other hand, jus-tifies the effectiveness of string-level learning. 5.3 Performance of string-level confidence parameter The performance of string-level confidence parameter learn-ing depends on the context models used in training. We con-sider three training situations below.

Before string-level training, the confidence parameters are initialized by character-level estimation. After string-level training on the training data of the text dataset DB2, the per-formance of character string recognition is evaluated on the test data of DB2 using two path evaluation schemes: charac-ter classification + LM and character classification + LM GM.

After string-level training in three situations, the string recognition performances of three class-dependent CT meth-ods (class-dependent parametric, SCCD and ReguCD) are given in Tables 6 , 7 and 8 , respectively. We also evaluated the performance of class-independent parametric CT learned at string level, and the experimental results show that the per-formances in three training situations are comparable. So, we show the results of only one situation (Situation 3), as in Table 9 .

From the results in Tables 6 , 7 , 8 and 9 , we have following observations. 1. String-level confidence learning of class-dependent 2. Comparing the performances of class-dependent CT 3. Comparing string-level learned class-dependent CT 4. Comparing three class-dependent CT methods (paramet-The comparison of character-level and string-level learned CT methods for different character classifiers show that the classifier NPC-OVA has the largest gain from string-level training, particularly for the SCCD method. Figure 7 shows the sigmoidal confidence mapping functions of character-level and string-level learned SCCD. It is shown that string-level training changes both the slope and the classification point of confidence mapping significantly to improve the string recognition performance.

We compare the best performance of character-level and string-level confidence learning in Table 10 , where the results of character-level learning are those of the class-independent parametric method in Table 4 , and the results of string-level learning are the best ones of Situation 3 in Table 8 .Wealso compare the overall correct rate (CR) and accurate rate (AR) as well as the CR for different types of characters: Chinese characters (ch), symbols (sb), digits (dg) and letters (lt). We can see that by string-level confidence learning, the near-est prototype classifiers NPC-OVA and NPC-LOGM gain larger improvements than the MQDF classifier. Regarding the different types of characters, the symbols gain relatively larger and stable improvement. With classifiers NPC-OVA and NPC-LOGM, the digits and letters also gain evident improvements. The symbols occupy a higher proportion than digits and letters in the handwritten texts and are hard to seg-ment and recognize. 5.4 String recognition examples Figure 8 shows some examples of string recognition results with character-level and string-level confidence learning. In the left example, both English and Chinese character recog-nition errors are corrected by the string-level confidence-learning method. In the middle example, a comma end-ing the string is mis-merged (under-segmented) in over-segmentation. By character-level confidence learning, a Chi-nese character is mis-segmented into two characters; while by string-level confidence learning, this character is correctly segmented and recognized. In the right example, character-level and string-level confidence-learning methods give the same segmentation result, but one character is assigned dif-ferent class labels due to the fusion of classification confi-dence and contexts. 6 Concluding remarks In this paper, we investigated into different confidence-learning methods for handwritten Chinese text recognition, including parametric (class-dependent and class-independent) and nonparametric (isotonic regression) confi-dence transformation (CT) methods. To overcome the over-fitting of class-dependent parametric CT methods, we pro-posed two regularized parametric methods. To improve the character string recognition performance, we proposed to learn the confidence parameters of the parametric CT meth-ods on string samples by optimizing the minimum classifier error (MCE) criterion. Our experimental results of online handwritten text line recognition on a large Chinese hand-writing database demonstrate that string-level confidence learning can effectively improve the string recognition per-formance.

Regarding the future work, we can view the parame-ters in the path evaluation function Eq. ( 1 ) in three levels: the first level includes the combining weights that balance the contributions of different context models, the second level includes the confidence parameters which convert raw outputs of context models (including the character classi-fier) into posterior probabilities and the third level includes the parameters of the context models. This paper reports an attempt to jointly learn the first-and second-level para-meters, with the third-level parameters trained at character level. The joint string-level training of character classifier and geometric context parameters for Chinese handwriting recognition is a future work, which encounters the large category set and the sparsity of character classes in string samples.
 References
