 Much work has attempted to model a user X  X  click-through behavior by mining the click logs. The task is not trivial due to the well-known position bias problem. Some break-throughs have been made: two newly proposed click models, DBN and CCM, addressed this problem and improved doc-ument relevance estimation. However, to further improve the estimation, we need a model that can capture more so-phisticated user behaviors. In particular, after clicking a search result, a user X  X  behavior (such as the dwell time on the clicked document, and whether there are further clicks on the clicked document) can be highly indicative of the relevance of the document. Unfortunately, such measures have not been incorporated in previous click models. In this paper, we introduce a novel click model, called the post-click click model (PCC), which provides an unbiased estimation of document relevance through leveraging both click behaviors on the search page and post-click behaviors beyond the search page. The PCC model is based on the Bayesian approach, and because of its incremental nature, it is highly scalable to large scale and constantly growing log data. Extensive experimental results illustrate that the proposed method significantly outperforms the state of the art methods merely relying on click logs.
 H.3.3 [ Information Search and Retrieval ]: Algorithms, Experimentation, Performance Post-Click Behavior, click log analysis, Bayesian model
It is one of the most important as well as challenging tasks to develop an ideal ranking function for commercial search engine. Most of existing works depend on manually labeled data, where professional editors provide the relevance rat-ings between a query and its related documents. According to manually labeled data, machine learning algorithms [5, 10, 13] are used to automatically optimize the ranking func-tion and maximize user satisfaction. However, the labeled data is very expensive to be generated and is difficult to keep up with the trend over time. For example, given a query  X  X I-GIR X , a search engine is expected to return the most up-to-date site such as the SIGIR 2010 website to users, instead of SIGIR 2009. Thus, it is very difficult to maintain the relevance labels up to date.

Compared with manually labeled data, terabytes of im-plicit user clicks are recorded by commercial search engines every day, which implies that a large scale of click-through data can be collected at a very low cost and it usually reveals the latest tendency of the Internet users. User preference on search results is encoded into user clicks, as such, the click logs provide a highly complementary information to manu-ally labeled data. Many studies have attempted to discover the underlying user preferences from the click-through logs and then learn a ranking function, or regard the click logs as a complementary data source to overcome shortcomings in manually labeled data. Following the pioneered works by Joachims et al.[14] that automatically generated the prefer-ences from the click logs to train a ranking function, many interesting works have been proposed to estimate the docu-ment relevance from user clicks, including [1, 2, 3, 6, 18].
Previous works have noticed that the main difficulty in estimating the relevance from click data comes from the so-called position bias: a document appearing in a higher position is more likely to attract user clicks even though it is irrelevant. Recently, Richardson et al.[19] suggested to reward the document relevance at a lower position by mul-tiplying a factor and this idea was later formalized as the examination hypothsis [8] and the position model [7], which indicates the user will click a document only after examining it. Craswell et al. [8] extended the examination hypothesis and proposed the cascade model by assuming that the user will scan search results from top to bottom. Furthermore, Dupret and Piwowarski[9] included the positional distance into the proposed UBM model. Guo et al.[11] proposed the CCM model and Chappell and Zhang[7] proposed the DBN model that generalizes the cascade model by introducing that the conditional probability of examining the current document is related to the relevance of the document at the previous position.

Despite their successes in solving the position-bias prob-Figure 1: The average dwell time on three levels of relevance rating. lem, previous works mainly investigate user behaviors on the search page, without considering user subsequent behaviors after a click. Nevertheless, as pointed in the DBN model, a click only represents user is attracted by the search snippet, rather than indicates the clicked document is relevant or user is satisfied with the document. Although there is a cor-relation between clicks and document relevance, they often differ with each other in many cases. For example, given two documents with similar clicks, if users often dwell longer to read the first document while close the second document im-mediately, it is likely that users feel satisfied with the first document while disappointed with the second one. Obvi-ously, the relevance difference between these two documents can be discovered from user post-click behaviors, such as the dwell time on the clicked document. As shown in Figure 1, we calculate the average dwell time on three relevance levels in a manually labeled data set 1 It is clear that there is a strong correlation between the dwell time and the relevance rating, which validates the importance of incorporating user post-click behaviors to build a better click model.
User subsequent behaviors after a click have been studied for evaluating and improving the quality of the results re-turned by search engine. Sculley et al.[20] attempted to pre-dict the bounce rates and Attenberg et al.[4] attempted to predict expected on-site actions in sponsored search. Agichtein et al.[2] optimized the ranking function through including some features extracted from post-click behaviors. Post-click behaviors can act as an effective measure of user sat-isfaction, thus, are very useful to improve the ranking func-tion. However, there are few works investigate how to in-tegrate both click behaviors and post-click behaviors into a click model.

In this paper, we propose a novel click model, called post-clicked click model (PCC), to provide an unbiased estima-tion of the relevance from both clicks and post-click behav-iors. In order to overcome the position bias in clicks, the PCC model follows the assumptions in the DBN model [7] that distinguishes the concepts of the perceived relevance and the actual relevance. It assumes that the probability that user clicks on a document after examination is deter-mined by the perceived relevance, while the probability that user examines the next document after a click is determined by the actual relevance of the previous document. Different from DBN, the post-click behaviors are used to estimate the user satisfaction in the PCC model. Some measures such as
The data set information is introduced in Section 4. the user dwell time on the clicked page, whether user has the next click, etc are extracted from the post-click behaviors, and used as features that are shared across queries in the PCC model.

The PCC model is based on the Bayesian framework that is both scalable and incremental to handle the computa-tional challenges in the large scale and constantly growing log data. The parameters for the posterior distribution can be updated in a closed form equation. We conduct exten-sive experimental studies on the data set with 54931 distinct queries and 140 million click sessions. Manually labeled data is used as the ground truth to evaluate the PCC model. The experimental results demonstrate that the PCC model sig-nificantly outperform two state of the art methods such as the DBN and CCM models that do not take post-click be-haviors into account. Because the PCC model can provide much more number of accurate preference data complemen-tary to manually labeled data, the ranking function trained on the relevance labels from both the PCC model and man-ually labeled data can produce better NDCG value than merely trained on manually labeled data.
We firstly introduce some background before delving into the algorithm details. When a user submits a query to the search engine, the search engine returns the user some ranked documents as search results. The user then browses the returned documents and clicks some of them. One query session corresponds to all the behaviors the user does under one input query, and we assume there are M displayed doc-uments in each query session.
The studies on click model attempted to solve the click bias problem in user implicit feedback. There are two im-portant hypotheses, i.e., the examination hypothesis and the cascade hypothesis, that are widely used in various click model implementations. These two hypotheses are quite natural to simulate user browsing habits, and our proposed PCC model also depends on them.

We use two binary random variables E i and C i to repre-sent the examination and click events of the document at the position i ( i = 1 ,...,M ). E i = 1 indicates the document at the position i is examined by the user, while E i = 0 in-dicates this document is not examined. C i = 1 indicates the user clicks the document at the position i , while C i indicates the user does not click this document.

The examination hypothesis assumes that when a dis-played document is clicked if and only if this document is both examined and perceived relevant, which can be sum-marized as follows: where u i is the document at the position i , and the parame-ter a u i measures the relevance 2 of the document u i indicat-ing the conditional probability of click after examination.
The cascade hypothesis assumes that the user scans linear to the search results, thus, a document is examined only if all the above documents are examined. The first document a u i is the perceived relevance in the DBN model is always examined.

Since the proposed model follows similar assumptions in the DBN model, we briefly introduce the formulation in DBN. A click does not necessarily indicates that the user is satisfied with this document. Thus, the DBN model [7] distinguish the document relevance as the perceived rele-vance and the real relevance, where whether the user clicks a document depends on its perceived relevance while whether the user is satisfied with this document and examines the next document depends on the real relevance. Thus, besides the examination and the cascade hypotheses, the DBN click model is characterized as: where S i is a binary variable indicating whether the user is satisfied with the document u i at the position i , and the parameter s u i measures the real relevance of this document. The DBN model uses the EM algorithm to find the maxi-mum likelihood estimation of the parameters.
Behavior logs in this study are the anonymized logs pro-vided by users who opted in through a widely-distributed browse toolbar. These log entities include a unique anony-mous identifier for the user, the issued query to search en-gine, the visited document, and a timestamp for each page view or search query.

We process behavior logs, and extract all the post-click behaviors after there is a document click on the search page, Thus, for each pair of query and document, several behavior sessions from different users are extracted and the length of each session is fixed no longer than 20 minutes. We then define some measures extracted from the post-click sessions: For each query and document pair, we calcuate the aver-age value of the above measures over related sessions and the averaged values are used as features into the proposed algorithm.
We now introduce a novel model, post-clicked click model (PCC), that leverages both click-through behaviors on the search page and the post-click behaviors after the click. Figure 2: The PCC model. The variables C i and f i (  X  i ) are the observed variables given a query session.
The PCC model is a generative Bayesian model and is explained in Figure 2, where the variables inside the box are defined at the session level, and the variables outside are defined at the query level. The variables E i , C i , and S defined the same as in the Section 2. Here we assume there are n features extracted from user post-click behaviors and f is the feature value of the i th feature. Thus,  X  u and  X  2 u are the parameters of the perceive rele-vance variable a u ,  X  u and  X  2 u are the parameters of the real relevance variable s u , and m i and  X  2 i are the parameters of the i th feature variable f i .

The PCC model is characterized by the following equa-tions: where  X  N (0 , X  2 ) is an error parameter and y u,i is a binary value indicating whether we can extract the value of the i th feature on the document u . It is possible that, for a document u , no user has clicked this document, thus, there is no information extracted from post-click behaviors on the i th feature. Thus, y u,i = 0 in this case. Otherwise, y
The PCC model simulates user interactions with the search engine results. When a user examines the i th document, he will read the title and the snippet of this document, and whether the document attracts him depends on the per-ceived relevance of this document a u i . If the user is not attracted by the snippet (i.e., A i = 0), he will not click the document which also indicates he is not satisfied with this document (i.e., S i = 0). Thus, there is a probability  X  that the user will examine the next document at the position i + 1, and a probability 1  X   X  that the user stops his search on this query. If the user is attracted by the snippet (i.e., A i = 1), he will click and visit the document. User post-click behaviors on the clicked document are very indicative to infer how much the user is satisfied with this document. If the user is satisfied (i.e., S i = 1), he will stop this search session; Otherwise, he will either stop this search session or examine the next document depending on the probability  X  .
The equations (10) and (17) is the cascade hypothesis and the equation (11) is the examination hypothesis. The equa-tion (12) shows that when a user examines the document, whether the user would click or not depends on the vari-able a u i and the error term. The equation (13) shows that when the user clicks and visits the document, the value of the post-click behavior features will affect whether the user is satisfied or not. The equation (14) and (15) mean that the user will not be satisfied if he does not click the docu-ment, while the user will stop the search when he is satisfied. The equation (16) shows that if user is not satisfied by the clicked document, the probability he continues browsing the next search results is  X  while the probability he abandons the session is 1  X   X  .
After observing one query session, we update the related parameters of each document in this session. For each doc-ument in one query session, it can be distinguished into five cases and the parameter update for these five cases are dif-ferent. We denote l as the last clicked position. When l = 0, it corresponds to the session with no click, and when l &gt; 0, it corresponds to the session with clicks. We define two sets of positions: A is the set of positions before the last click and B is the set of positions after the last click. Thus, the five cases are defined as follows:
For a fixed k (1  X  k  X  M ), suppose x is the parameter we want to update, we follow the equation: to get the posterior distribution. Then we approximate it to Gaussian distribution use KL-divergence. The method to derive the updating formula is based on the message passing [15] and the expectation propagation[17]. Since the space limitation, we omit the proof of these formula. For conve-nience, we will introduce some functions that will be used in the following update equations:
For the k th document, the observation is A 1 = 0 ,E 1 = 1 ,C i = 0 , 1  X  i  X  k . We update the parameters related to the i th document. This is the update of the parameter in the perceived relevance: where  X  1 ,k is a coefficient whose value is given in Appendix. The parameters of the features and the real relevance are kept the same. For the k th document, the observation is A k = 0 ,E k = 1. Thus, we update the parameters related to the k th docu-ment. The update of the parameter in the perceived rele-vance is: The parameters of the features and the real relevance are kept the same.
For the k th document, the observation is A k = 1 ,E k = 1 and S k = 0. Thus, we update the parameters related to the k th document. The update of the parameter in the perceived relevance is:
The update of the parameter in the feature is:
The update of the parameter in the real relevance is:
For the last clicked document, the observation is C 1 ,C i = 0( i = l + 1 to M ) and we update the parameters related to the l th document. The update of the parameters in the perceived relevance is:
The update of the parameters in the feature is: where  X  2 is a coefficient whose value is given in Appendix.
The update of the parameters in the real relevance is:
For the k th document, the observation is C l = 1 ,C 0( k = l +1 to M ). Thus we update the parameter related to the k th document. The update of the parameter in the perceived relevance is: where  X  3 ,k is a coefficient whose value is given in Appendix. The parameters in the features and the real relevance are kept the same.
Following the above update formula, we can easily build the PCC training algorithm as follows: tion N (  X  0 . 5 , 0 . 5).
Given a collection of training sessions, we sequentially up-date the parameters according to the five cases. Since the update formula is in a closed form, the algorithm can be trained on a large scale and constantly growing log data. After training the PCC model, we set the user satisfaction probability to zero, i.e., P ( S = 1 | C = 1) = 0, for those documents that have never been clicked.

The PCC model follows the assumption in DBN to dis-tinguish the document relevance as the perceived relevance P ( A = 1 | E = 1) and the real relevance P ( S = 1 | C = 1). We define the document relevance inferred from the PCC model as: rel u = P ( A = 1 | E = 1) P ( S u = 1 | C = 1) This document relevance rel u will be evaluated on the ground truth ratings in manually labeled data.
In the experiment, we evaluate the document relevance and the click perplexity inferred from the PCC model, and the results are compared with other click models including DBN and CCM. The experiments are organized into four parts. In the first part, we analyze the pairwise accuracies of the relevance among different click models. In the second part, we use the generated relevance to rank the documents directly and evaluate the ranking function according to the normalized discounted cumulative gain (NDCG) [12]. In the second part, we use the RankNet algorithm to learn a ranking function on the preference pairs extracted from both the click model and manually labeled data, and illustrate the ranking improvement. Finally, we illustrate the click perplexity among different click models.
The click logs used to train the click models are collected from a large commercial search engine which comprises 54,931 randomly sampled queries and about 2 million related docu-ments from the U.S. market in English language, and the to-tal number of search sessions from one month click-through log is about 143 million. For each search session, we have one input query, a list of returned documents on browsed pages and a list of positions of the clicked documents. The information on the click logs is summarized in Table 1.
For each query and document pair, we collect correspond-ing post-click sessions in 20 minutes from one month be-havior log. We calculate the average values of five features, as introduced in Section 2.3, from post-click behaviors and they are used to train and evaluate the PCC model.

The manully labeled data is used as the ground truth to evaluate the relevance from click models. In the human rele-vance system (HRS), editors provided the relevance ratings for 4,521 queries and 127,519 related documents. On av-erage, 28.2 documents per query are labeled. A five grade
Query Frequency ] Query ] Document ] Total Sessions 1,000 to 10,000 5,282 578,198 17,827,303 Table 1: The summary of the search sessions from one month click logs. rating is assigned to each query and document (4: perfect, 3: excellence, 2: good, 1: fair, 0: bad ). The documents without judgement are labeled as 0. The summary of the HRS is introduced in Table 2.
 Table 2: The summary of the data in human rele-vance system (HRS).
The document relevance is derived from the PCC model according to Equation (32), and we compute the relevance for those queries and related documents that are overlapped with the HRS data in the experiment. Since the relevance value is a real number between [0 , 1], while the rating in HRS, denoted as hrs u , is a discrete number from 0 to 4, it is unable to match them directly. We evaluate the relevance according to the pairwise accuracy based on the number of concordances and discordances in preference pairs. Given two documents u i and u j under the same query, the con-cordant pair is that if hrs u i &gt; hrs u j and rel u i &gt; rel if hrs u i &lt; hrs u j and rel u i &lt; rel u j . An discordant pair is that if hrs u i &gt; hrs u j and rel u i &lt; rel u j , or if hrs and rel u i &gt; rel u j . This pairwise accuracy is calculated as follows: Here, D represents the number of discordant pairs and N represents the total number of pairs generated by the click model.
 Similarly, we compute the document relevance from the DBN and the CCM model according to the probability P ( C = 1 | E = 1). After training click model, we generate the pref-erence pair with respect to each pair of documents under the same query. However, we notice that the number of generated preference pairs from different click models varies significantly different. Thus, even one algorithm reaches bet-ter accuracy than another one, since the number of prefer-ence pairs is different, we cannot conclude which algorithm is better. In order to provide a fair evaluation, we intro-duce a threshold  X  such that the preference pair u i &gt; u generated only when where  X   X  0. Thus, we can generate different set of prefer-ence pairs through setting different  X  value. When we set  X  as a larger value, less number of preference pairs are gen-erated. Moreover, since the relevance difference becomes large, the generated preference pairs are more reliable. Ac-cordingly, we evaluate the pairwise accuracy among differ-ent algorithms in terms of the similar number of preference pairs.

Figure 3 reports the result of pairwise accuracies among three click models. For each click model, we set a series of  X  values to generate different number of preference pairs and compute related pairwise accuracies. As  X  increases, the number of pairs decreases and the pairwise accuracy increases correspondly. When the pair number is 1 million, the PCC model reaches to the pairwise accuracy 82 . 8% while DBN and CCM reaches to 81 . 7% and 78 . 2% respectively. When the number of pairs is 0.5 million, PCC reaches to the accuracy 86 . 3% while DBN and CCM reaches to 83 . 9% and 78 . 6% respectively. On average, the PCC model achieves 2% and 5% accuracy improvement than that of the DBN and CCM models. Figure 3: The pairwise accuracy comparison among three click models in terms of the number of prefer-ence pairs.
In the part, we use the predicted relevance to rank the documents directly. For one query and their related docu-ments, every document is treated equally in computing the pairwise accuracy in the above. However, the ranking eval-uation such as NDCG often put more emphasis on the doc-uments at top positions. As such, the relative order of the documents with higher predicted relevance is more impor-tant than the documents with lower relevance.

For each query, we rank the returned documents according to the relevance value rel u i (  X  i ) and compute NDCG@1 and NDCG@3 scores for the PCC, DBN and CCM models. The results are shown in Figure 4 and 5, where we decompose the NDCG score in terms of query frequency. We can see when the query frequency is between 100 to 1000, NDCG@1 of the PCC model is 63 . 1%, which has 3% and 17% im-provement than that of DBN and CCM, respectively. For extremely low frequent queries, the NGCG@1 improvement of the PCC model over DBN and CCM becomes less sig-nificant. The main reason is because the post-click features cannot be extracted for these queries and their related doc-uments so that the post-click behaviors cannot contribute to the click model, which proves the effectiveness of incor-porating post-click behavior into click model.

The overall NDCG@1 for all queries is 63 . 2%, which has 2% and 13% improvement over DBN and CCM. We also ob-serve very similar results in NDCG@3, which demonstrates that the relevance inferred from PCC is consistently better than that from DBN and CCM. Figure 4: The NDCG@1 comparison among three click models in terms of query frequency. Figure 5: The NDCG@3 comparison among three click models in terms of query frequency
Learning to rank is to optimize a ranking function from a set of documents with relevance ratings. We follow the RankNet [5] method which is a pairwise ranking algorithm receiving the pairwise preferences to optimize the ranking function. For each query and document, we extract about three hundred of features in the experiment, where the fea-tures are similar to those defined in LETOR[16]). Since the document relevance inferred from the PCC and DBN models is better than that from the CCM model in the above two experiments, we only consider the PCC and DBN models in this part of experiment.

We partition the HRS data as described in Table 2 into the training and testing sets. We randomly choose 3,000 queries and related 85,173 documents into the training set, and other queries and documents are in the testing data. There are totally about 5.1 million preference pairs gener-ated from HRS as the training data. In addition, the click model are trained on the click log as described in Table 1, thus, there are about 7.4 million preference pairs generated from the PCC and the DBN. We construct three training sets for the RankNet: 1. only HRS; 2. PCC + HRS; 3. DBN + HRS, and evaluate the ranking function on the HRS test-ing data. The results on NDCG@1 and NDCG@3 are shown in Figure 6 and 7. Figure 6: The NDCG@1 results from the RankNet algorithm on three different training sets. Figure 7: The NDCG@3 results from the RankNet algorithm on three different training sets.

The NDCG@1 and NDCG@3 results illustrate that the ranking function trained on the  X  X CC + HRS X  data consis-tently outperform the function on the  X  X BN + HRS X  data, while the function on the  X  X BN + HRS X  data outperforms the function trained only on the  X  X RS X  data. The over-all NDCG@1 from  X  X CC+HRS X  is 1.9% higher than that from  X  X RS X , which is a significant improvment of the rank-ing function on such large scale training and evaluation data.
Click perplexity is used as an evaluation metric to eval-uate the accuracy of the click-through rate prediction. We assume that q j i is the probability of click drived from the click model, i.e. P ( C i = 1 | E i = 1) at the position i and C is a binary value indicating the click event at the position i on the j th session. Thus, the click perplexity at the position i is computed as follows: Thus, a smaller perplexity value indicates a better predic-tion.

The result on click perplexity is shown in Figure 8. We can see that the PCC model performs the best for the clicks in the first position. As for the other positions, the click perplexity from PCC are very similar to that from CCM. Al-though CCM has not inferred the document relevance very well in the above experiment, its click perplexity performs as well as PCC. The click perplexity obtained from PCC significantly outperforms the perplexity from DBN, which indicates that incorporating post-click behaviors into a click model can also produce a much better click prediction. Figure 8: The click perplexity comparisons among three click models in terms of search position.
Besides user behaviors on the search result page, post-click behaviors after leaving the search page encodes very valuable user preference information. Different from previ-ous works, this paper firstly investigates how to incorporate post-click behaviors into a click model to infer the docu-ment relevance. It proposes a novel PCC model by leverag-ing both click behaviors and post-click behaviors to estimate the degree of user satisfaction via a Bayesian approach. We conduct extensive experiments on a large scale data set and compare the PCC model with the state of the art works such as DBN and CCM. The experimental results show that PCC can consistently outperform baselines models on four differ-ent experimental setting. It is worth noting that the update of the PCC model is in a close form, which is capable of processing very large scale data sequentially.

The proposed method of incorporating post-click behav-iors in the paper is a very general solution and can be ex-tended to other click models such as CCM, UBM, etc. In the PCC model, the post-click behaviors are used as the features to estimate the user satisfication on the clicked document. However, it is not the only approach of incorporating post-click behaviors into click model. Another possible approach is to simulate user post-click behaviors through construct-ing a separate user browse model and then integrate it with the click models. We will explore these directions in future works.
