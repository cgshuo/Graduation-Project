 The information explosion on the Web has led to a proliferation of documents that are consisted of replicated pages. The repli ca proliferation causes many problems to Web based information systems, including consuming excess bandwidth and crawling time, wasting disk to store redundant data, slowing down the indexing and retrieval time, and impairing the quality of retrieval results. Large concentrations of replicated documents may also skew the content distribution statistics with potentially harmful consequences to machine learning applications [6] and Web mining applications[16]. 
Most previous studies [2,3,4,7,15,16] investigated this problem at two different granularities: duplicate hosts and duplicate documents. However, through analyzing the duplicated data recorded in the replica reduction process of our search engine [17], we obtained an important observation that resembling page collections often exists in two hosts that are obviously not mirror sites, even more serious than duplicate hosts . A typical example is the how-to documents of Linux. By submitting queries such as  X  X atlab plot X  or  X  X inux nfs howto X  to the large search engines such as Google, Alta Vista and MSN, many duplicated pages can be found even in the top ranked results. Most previous research has ignored the existence of replicated sub-collections in two distinct sites. Duplicate host detection methods [7, 16] obviously skip over these replicated collections. Although the sub-collections can be removed by pair-wise similarity computation at the document granularity, considering the scale of the Web, performing such a massive amount of computation is highly time-consuming. A better way to efficiently remove such replicated sub-coll ections is to consider them as a whole, introducing an intermediate granular ity, the directory granularity. 
In order to clarify the complex situation of replication on the Web, we start with some definitions. Directories. In most hosts, Web pages are organized into nested directories , and the path indicates the location of the directory in the tree structure of the entire site. The website managers tend to design one or more index page s to organize the pages within the directory. Index pages serve as access portals to the related Web page collection. So, the index page can be regarded as the representative of the directory. corresponding to replicated sub-collections are often identical as well. We define replicated directories as follows: given two directories D and D X  belonging to two distinct hosts H and H X , D and D X  are replicated directories if every page P nested in D can find an identical counterpoint P X  in D X . Given the example shown in Figure 1,  X  X ww.linux.org X  (H 1 ) and  X  X ww.linuxselfhelp.com X  (H 2 ) are not duplicate hosts. But  X  X ww.linuxselfhelp.com/ howto X  (D X  0 ) are identical, and thus re plicated directories. Replica Boundary. Now we can define replica boundary as follows: given a directory tree of a Web page collection, a replica bound ary of the collection is a node nearest to the root, from which all nested Web pages and sub-directories are replicated with boundaries of hosts H 1 and H 2 respectively. By this definition, duplicate hosts can be regarded as a special case of replicated dir ectories. Replica boundary serves as a brief description of the replica situation between two hosts. 
In this paper, we proposed an effective and efficient approach to discover the replicated directories and the replica boundaries. The advantages of the proposed approach include: first, it dramatically reduces pair-wise document similarity computation, making it much faster than traditional replicated document detection accurately, demonstrating to what exte nt two collections are replicated. A number of approaches for near replica detection have been proposed. They can be discovered: near replicated collection detection [4,7,16] and near replicated document detection [1, 2, 3, 8, 9, 10, 11, 12, 14]. Two typical approaches were proposed for replicated collection detection in [4, 7]. In [4], the authors proposed a clustering based approach to find replicated Webpage collections. This approach has the following drawbacks: first, the generation of trivial clusters is a bottom-up clustering process, which is very time consuming; second, the approach merges the clusters according to their link relations, the merge condition is too strict, because generally not all the Web pages within a Website can be fetched back in the crawling process, resulting in a difference in the link situations for two identical collections. In [7], the authors proposed an efficient approach to discover mirror hosts on the Web. This approach depends mostly on the syntactic analysis of the URL strings, and requires retrieval and content analysis for only a small number of pages. The approach suffers from the following drawback s: the approach can only identify whether two hosts are totally mirroring or partially mirroring, but for the partially mirrors, it can not identify which part are replicated. In [16], the authors proposed and compared several algorithms that identify mirrored hosts on the Web based only on URL strings and the hyperlink structure. These algorithms can discovery the duplicated host very efficiently, but they can not find the direct ory level replicas. And the duplicated host can not be discovered when the path is renamed. 
If not for the size of the Web, detecting near replica documents would be a traditional information retrieval problem. All the traditional text clustering approaches can be used to find near replicated web page s. Some more efficient approaches [1, 2, 3, 8, 9, 10, 11, 12, 14] are proposed to find replicated documents (web pages). These approaches can be roughly classified as [6]: similarity-based and signature-based. Despite the efforts to accelerate the computa tional time, all the proposed methods need to analyze the content of every Web page and depend on large scale pair-wise similarity computation, which makes the approaches very time-consuming. We first verify an assumption by an experiment. Based on the assumption, we propose pages in the collection, and discover candi date replicated directories by finding replicated index pages. Then, we use coordinate sampling to check the replicate status of the candidate directories. After obtaini ng the replicated directories, the replica boundaries are discovered. 3.1 Assumption Verification Nearly all the efficient duplicated host de tection approaches [7, 16] are based on one assumption: if one host is replicated, the relative path and filename of the pages within the host will not be altered. However, many replicas are not at host granularity, but at assumption still works when directories are replicated. 
In the experiment, we first used a bottom-up algorithm to discover the replicated directories in the data set. After getting the replicated top directories in the data set, we can check whether the relative paths and filenames are changed or not. Based on one hundred of replicated directory pair samples, we got the following statistical result. All characters in the URLs were first converted to lower case before comparison. The result shows that 95% of the replicat ed directories kept the identical path names. All of the rest 5% were replica boundaries and made only one variation, the renaming of the replica boundary. Though the replica boundaries were renamed, the names all sub-directories and nested files remained unaltered. 3.2 Index Page Identification Index pages are the representatives of directories. Instead of analyzing all URLs at hand, we concentrate on index pages to facilitate our algorithm. In [13], much information, including URL information, link information and text information, is considered to assist in finding the entry pa ges of websites. By constructing the decision tree on the training data, the author found that only the length of URL (the number of slashes in the URL), the slash information, and whether URL ends with some special keywords were necessarily retained. The URLs identified as entry pages by the obtained decision tree are regarded as index pages in our approach. 3.3 Finding Replicated Directories As mentioned above, index pages can be regarded as the representatives of the corresponding directories. Generally, if two directories are replicas, their index pages are likely to be replicas as well, but this is not necessarily true vice versa. So when two index pages are identified as replicas, we need to check the pages in the two corresponding directories to justify the decision . If the two directories are replicated to each other, we call the two index pages collective replicas , because they reflect not only the replica relation between themselves but also the replica relation between the two Web page collections in the two directories. Otherwise, we call two index pages individual replicas . Our approach for finding replicat ed directories consists of the following three steps: Step 1, finding the replicated index pages . The method introduced in [8] is used here. method is an equivalent relation (reflexive, transitive and symmetrical). Thus, grouping the pages with identical fingerprints into one cluster constructs a partition of the whole page collection. Step 2, coordinate sampling from candidate replicated directories . Given two certain number of pages (  X  ) at random from D 1 . By generating one fingerprint per page, D . Then the corresponding pages from D 2 with the same relative path and filenames (verified in Section 3.1). 
Searching records in a large database repeatedly is time consuming. By using a two-level hash method, we can accomplish the sampling for all candidate index page sample page, we can further improve the efficiency by representing the directory with the fingerprints of sample URL strings rather than the content of pages. This method is also evaluated in the experiments. Step 3, evaluating the replica status . As the index pages identified as replicas by the groups into even smaller clusters. 
Since not all the pages on the Web can be fetched, the sample fingerprint sets may not be the same (m&lt;=  X  ) even though two directories are replicated to each other. The replica status can be measured by computing the resemblance of the two index pages via the following formula (1): clusters where the index pages in same cluster are collective replicas to each other. 3.4 Replica Boundary Detection According to the definition of replicated directory and replica boundary, the replicated discovered. However, the replicated director ies are identified based on sampling in the previous stage. Sampling leads to a measure of similarity between two candidate directories. And due to the sampling method, the closer the candidate directories are to cannot jump to the conclusion of replicated directory, especially when large amount of sampling is impractical. 
Next, we proposed the replica boundary detection algorithm that considers the g :{d k1 ,d k2 ,..., d km }}, by parsing the paths of the index pages we construct another group in T, where all offspring nodes of d i ' are collective replicas to the corresponding nodes of d i and their parent nodes are not. To accomplish this task, we propose a top down boundary identification algorithm. In this algorithm we use containment C(T 1 , T 2 ) to estimate how much directory tree T 1 is contained in T 2 . First the directory tree is represented by the set of all nodes in the tree. where CR(node i , node j ) means that node i and node j are two replicated directories, | X | is the cardinality of a set. The value of containment naturally ranges from 0 to 1. If the contained" is a partial order relation, maintaining the transitive property. As the crawler cannot fetch the entire Web, some deep sub-directories may not be fetched by the crawler, which affects the containment of two replicated directories. To handle this contained X  is also a partial order relation, we can achieve fast redundant directory removal by deleting the  X  X ontained X  directories in a linear scan rather than requiring square time. 
The matrix C that records the containm ent of directory pairs is called the containment matrix and the entry c ij of matrix C is the value of containment (T i , T j ). Given the containment matrix, the replica boundary can be found by the following algorithm: We evaluated the accuracy of our approach on two Web page sets containing 24 million and 30 million web pages respectively. The two Web page sets were crawled by a breadth-first strategy from distinct sets of seeds. 4.1 Accuracy Evaluation of Replicated Directory Detection area to evaluate our replicated directory detection approach. 
We verify whether two directories are really replicated through checking if a set of paths sampled from one directory are all valid on the other directory and yield highly similar documents, and vice versa. In the approach, two parameters are used: the sampling sum (  X  ) and the resemblance threshold (  X  ). The experimental results shown in Table 1 illustrated how the variability of the sampling sum (  X  ) and resemblance threshold (  X  ) affect the performance. 
From the data shown in Table 1, we can see that the changes of the precision on the two data sets are similar. The precision reaches 90% and 87% when no sampling is used in the approach (  X  =0). We checked the false cases when  X  =0 and  X  =0, and found that the error results from the wrong decision on whether two index pages were replicas, due to the near replicated document detection method. We can also see that when sampling is used (  X  &gt;0), the precision can reach 100%. And the variations of  X  and  X  can only affect the recall value. That means if two index pages are replicated to each other and there exists at least one extra replicated page pairs on the two directories, the two directories can be regarded as replicated sa fely. As for the recall, we can see that the 368 Z. Zhang, W. Jia, and X. Li value of the recall varied greatly with the variation of the parameters and data sets. The reason is that the crawler has not fetched all the pages in a host, especially when the data set is not large enough. It can not be guaranteed that the two replicated pages in the sampling. 
To improve the efficiency of the algorithm, we can represent the directory using the fingerprints of sample URL strings rather than the content of pages, which can save the time on fingerprint computation. From the data shown in Table 2, we can see that such an alternative only affects the accuracy performance slightly. In conclusion, if the index directories by sampling and considering only the URL strings; as for the number of samples, 4 samples can achieve satisfactory performance .
 4.2 Accuracy Evaluation of Replica Boundary Detection The accuracy is defined as following: If two directories are identified as a boundary pair, we need check the following two conditions to verify whether the two directories are really a replica boundary pair: 1. whether the two directories ar e replicated to each other; 2. whether the parent directories of these two directories are not replicated. If the two conditions are both satisfied, the two directories are a boundary pairs. 
The containment threshold  X  affects the accuracy of replica boundary detection in two directions. On one hand, with the rising of  X   X  X  value, the accuracy is increasing as boundary, which makes the accuracy decrease. 
As shown in Figure 2, the effect of the latter one is the dominant factor. Even though the accuracies of the algorithm on the two data sets differ larger and larger when the value of  X  is rising, they are very similar when  X  is small. Especially, the accuracy of the algorithm on the two data sets all reached 97% when  X  is equal to 0. According to the boundary detection algorithm, all the top rep licated directories are regarded as replica replicated. So, the top-down algorithm is suitable. In this paper, we propose an intermediate granularity of replication on the Web at the directory level, and present an effective appr oach to discover the replicated directories and the replica boundaries. Experimental results show the effectiveness of the approach. The conclusions obtained in the experiment can also be viewed as some of work, we plan to extend the current approach so that it can handle the data in a distributed environment. We would also like to explore the online replica detection approach, with which we can avoid fetching the duplicate collections. This work is sponsored in part by City University of Hong Kong strategic grants (7001709) and CityU FSE Funding for Research (9610027), in part by 973 National Basic Research Program, Minister of Science and Technology of China under Grant No. 2003CB317003, and in part by an NSFC grant (60573166). 370 Z. Zhang, W. Jia, and X. Li [1] A.Z.Broder. On the resemblance and containment of documents. In Proceedings of [2] A. Z. Broder. Identifying and Filtering Near-Duplicate Documents. In Combinatorial [3] Z. Broder, S. C. Glassman, M. S. Manasse, and G. eig. Syntactic clustering of the Web. In [4] Junghoo Cho, Narayanan Shivakumar, Hector Garcia-Molina: Finding Replicated Web [5] N. Heintze. Scalable Document Fingerprinting. Proceedings of the Second USENIX [7] Krishna Bharat and Andrei Z. Broder. Mirror, Mirror, on the Web: A study of host pairs [8] Zhigang Zhang, Jing Chen and Xiaoming Li, "A Preprocessing Framework and Approach [9] A. Chowdhury, O. Frieder, D. A. Grossman, and M. C. McCabe. Collection statistics for [10] S. Brin, J. Davis, and Garcia-Molina. Copy detection mechanisms for digital documents. In [11] N. Shivakumar, H. Garcia-Molina. SCAM: A Copy Detection Mechanism for Digital [12] N. Shivakumar, H. Garcia-Molina. Building a Scalable and Accurate Copy Detection [13] Wensi Xi, Edward A. Fox, Roy P. Tan, Jiang Shu: Machine Learning Approach for [15] M. Henzinger, R. Motwani, Silverstein. Challenges in Web Search Engines. In Proceedings 
