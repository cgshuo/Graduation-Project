 Dealing with verbose (or long) queries poses a new chal-lenge for information retrieval. Selecting a subset of the original query (a  X  X ub-query X ) has been shown to be an ef-fective method for improving these queries. In this paper, the distribution of sub-queries ( X  X ubset distribution X ) is for-mally modeled within a well-grounded framework. Specif-ically, sub-query selection is considered as a sequential la-beling problem, where each query word in a verbose query is assigned a label of  X  X eep X  or  X  X on X  X  keep X . A novel Con-ditional Random Field model is proposed to generate the distribution of sub-queries. This model captures the local and global dependencies between query words and directly optimizes the expected retrieval performance on a training set. The experiments, based on different retrieval models and performance measures, show that the proposed model can generate high-quality sub-query distributions and can significantly outperform state-of-the-art techniques. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance Sub-query selection, verbose query, Conditional Random Field, Information Retrieval
The use of verbose (or long) queries helps users to express their information need naturally and saves efforts in choosing keywords. Unfortunately, previous work [1, 11] has shown that current search engines cannot handle verbose queries well. It seems that the additional information provided in verbose queries is more likely t o confuse current search en-gines rather than help them. Thus, dealing with verbose queries poses a new challenge for information retrieval.
Previous work on verbose queries has been in two main directions, selecting a subset of the verbose query (or sub-query) and weighting query words in the verbose query. For example, Bendersky and Croft [1] learned how to discover the key concepts of a verbose query and Kumaran and Car-valho [11] studied how to automatically reduce a verbose query to a shorter and more effective subset. Examples of the research on weighting include Lease et al [14, 13] who trained a regression model to weight all query words of a verbose query and Bendersky et al [3] who proposed a way to uniformly learn the importance of concepts underlying the verbose query. Both directions have shown promise in improving the retrieval performance of verbose queries. In this work, we focus on selecting subsets, since it simulates a common behavior of people when they deal with verbose queries.

The sub-query selection problem is defined as selecting a subset of the original verbose query. In other words, the problem is to assign a label  X  X eep X  or  X  X on X  X  keep X  for each query word. Here, we focus on the impact on this label-ing of the different types of relations that can exist between query words. For example, some query words may form a noun phrase such as  X  X panish Civil War X  and others may de-scribe named entities such as  X  X hesapeake Bay Maryland X . A query word may serve as the subject of another query word such as  X  X ississippi River X  being the subject of  X  X lood X  when they are used in the query  X  X ow frequently does the Mississippi River flood its banks? X . Generally, these rela-tions between query words imply the relations between the labels assigned to query words. For example, since  X  X panish Civil War X  is a noun phrase, these three words tend to have the same labels, either keeping all of them or dropping all of them. This is also true for other relations. Therefore, it is reasonable to model sub-query selection as a sequential labeling problem instead of a classification problem that de-termines the labels of query words independently. Moreover, instead of selecting the best sub-query, it is more general to model a distribution over the space of all possible sub-queries. This means that the probabilities of observing each sub-query in this distribution can be used as weights when we need to combine different sub-queries.

A Conditional Random Field (CRF) [12] is a well known graphical model designed for sequential labeling problems and provides a unified framework to capture features ex-tracted from the input sequence, especially those features modeling the underlying dependencies. Furthermore, a CRF provides a formally well-founded framework to model the distribution of label sequences. Therefore, based on the above analysis, a CRF model should be a good method for describing the distribution of sub-queries.

However, some unique propert ies of sub-query selection makes the direct application of CRFs difficult. A conven-tional CRF is designed to optimize labeling accuracy based on a training set of input sequences and their correspond-ing gold-standard label sequences. When selecting subset queries, however, it is unclear how to define the gold-standard subset of query words given the input verbose query. A straightforward way is to select the sub-query with the best retrieval performance. However, the sub-query with the best performance may overfit to the collection and thus may not necessarily be the best choice. For example, the sub-query  X  X ockey weight horse X  (AP 1 =77.1) has the best performance given the original query  X  X hat are the limits and regula-tions concerning jockey weight in horse racing? X . However, another sub-query  X  X ockey weight horse racing X  (AP=74.2) is clearly a better choice when being used to learn rules that can generalize to unseen queries. In this paper, a novel CRF model called CRF-perf is proposed, where instead of se-lecting the gold-standard sub-query, all sub-queries are used with their associated retrieval performance. Compared with the conventional CRF that optimizes label accuracy, CRF-perf is able to directly optimize the expected retrieval per-formance over all sub-queries. In this way, CRF-perf can discover the common rules shared by those high performing sub-queries. Those rules are thus likely to apply to unseen queries. Also, CRF-perf provides a general learning frame-work, which can be adapted to different retrieval models and performance measures.
 Several types of features are used in this paper to train CRF-perf, which captures the local and global dependencies between query words. We also propose some retrieval mod-els to incorporate the distribution of sub-queries generated by CRF-perf. Experiments on TREC collections show that CRF-perf can generate high-quality sub-query distributions and can significantly outperform state-of-the-art techniques.
The rest of this paper is organized as follows: Section 2 briefly reviews related work; Section 3 formally defines the notations and problems; Section 4 describes the proposed CRF-perf model in detail; Section 5 considers the retrieval model using sub-query distribution; Section 6 reports the experimental results and Section 7 concludes.
As mentioned above, previous work on processing verbose queries generally falls into two areas, selecting a subset of the verbose query and weighting all query words of the verbose query.

Kumaran and Allan [10] studied reducing the verbose query into a subset through human interaction. The sub-queries with high mutual information scores are displayed to the user. Their experiments showed that the user can select good sub-queries using snippet information. Bendersky and Croft [1] proposed a method to find key concepts from a verbose query using different types of features. A key con-cept can be considered as a special subset query, which helps
AP denotes the average precision. AP is usually smaller than 1 and here we report AP  X  100. to improve the retrieval performance when combined with the verbose query. Recently, Kumaran and Carvalho [11] used Ranking SVM to learn to how to automatically select sub-queries using several query quality predictors. Ranking SVM learns to rank sub-queries based on their retrieval per-formance, but, in contrast to our model, it cannot directly optimize the retrieval performance of sub-queries. Also, Ku-maran and Carvalho [11] only used the top sub-query for re-trieval. In this paper, we consider several types of retrieval models using sub-queries.

Lease et al [14] improved verbose queries by weighting query terms, which assigned more weight to important words and less weight to unimportant ones. They trained a regres-sion model to learn how to map the secondary features to the optimal weights of query words. Lease [13] further incorpo-rated their regression model into the framework of the De-pendence Model [19] and observed significant performance improvement. Bendersky et al [3] proposed a unified frame-work to measure the importance of concepts underlying the verbose queries and extended the conventional Dependence Model to its weighted version. In this paper, we focus on selecting sub-queries instead of weighting query words.
A Conditional Random Field was first proposed by Laf-ferty et al [12] for segmenting and labeling sequence data. It has been successfully applied to many applications such as shallow parsing [26], named-entity recognition [17], identify-ing protein names [25] and so on. Guo et al. [4] proposed a CRF-based model for query refinement, which solved several tasks such as spelling correction, word splitting and word stemming within the same framework. Li et al [15] used CRFs for query tagging, which assigns each query term a pre-defined category. Qin et al [22] proposed a continuous CRF to capture the dependencies between documents and assign the retrieval scores for a set of documents simulta-neously instead of one by one. In the work described here, a performance-based CRF model is proposed to solve the sub-query selection problem, which can directly optimize the expected retrieval performance of sub-queries.
In this section, we define the problems we are addressing and introduce the notations will be used in the rest of the paper.

Sub-query Selection is the problem of selecting a subset of query words from the original verbose query. The original verbose query Q can be represented as a sequence of words x = { x 1 x 2 ...x n } . x i is the query word and n is the length of the query. y = { y 1 y 2 ...y n } denotes a sequence of labels, where y i takes the value of 1 or 0, corresponding to  X  X eep X  or  X  X on X  X  keep X  x i , respectively. Thus, y represents a selection of query words. Given x and y , a sub-query Q s can be generated. Sometimes, y is also used to denote a sub-query.
Learning to Generate Sub-query Distribution is the problem of learning a model from a training set of queries and using the learned model to generate the sub-query dis-tribution for unseen queries. The training set is denoted as Train = { x , { y ,m ( y ,M ) }} , which consists of the original query x and its corresponding sub-query set { y ,m ( y ,M The sub-query set consists of all sub-queries y and their cor-responding retrieval performance m ( y ,M ). The function could be any performance measure used in information re-trieval and M is the retrieval model used. m ( y ,M )isof-ten abbreviated as m ( y ), when M is not explicitly claimed. Given the training set Train , the model parameters  X  are learned. Then, for an unseen query x , a probabilistic distri-bution over its sub-queries is predicted as P ( y | x , X  ). This distribution can then be used to select the best sub-query or used in other applications.
In this section, we first introduce the conventional CRF model, then describe the proposed performance-based Con-ditional Random Field (CRF-perf) model and finally provide information about the features used.
A Conditional Random Field (CRF) is a graphical model designed to directly model the conditional probability P ( y [12, 27]. Compared with a generative model, a CRF avoids modeling the joint probability P ( y , x ), which is actually un-necessary for a sequential labeling problem. Thus, CRF doesn X  X  need to make unreasonable independence assump-tions over the input sequence and can handle a large number of correlated features. The graphical model of a general CRF is depicted in Fig. 1 as an example, where the shaded nodes denote the input variables x i and the non-shaded nodes de-note the output variables y i . The relations between different nodes are represented by the graph structure.

In a CRF, the conditional probability P ( y | x )iscalculated as follows: where f k represent the features, which are extracted based on the input sequence x and the label sequence y .  X  k is the weight of the feature f k .  X  = {  X  k } denotes the param-eters of the model. K is the number of features. Z ( x )isa normalizer, which guarantees
The training set used by the conventional CRF can be de-noted as { x , y } , which consists of a set of input sequences x and their gold-standard label sequences y . The conditional probabilities of the training set can be calculated according to Eq. 3.
 The log-likelihood of P (  X  ) can be calculated according to Eq. 4. The last term of Eq. 4 is served as a regularizer which penalizes values of  X  that are too large.  X  2 is a parameter set by the user.

The parameters  X  = {  X  k } can be learned by maximizing the log likelihood. Its partial derivatives can be calculated by Eq. 5.  X  X  (  X  )  X  X  k =
The first term of Eq. 5 is the empirical value of f k given the gold standard-label y , the second term is the expected value of f k under the distribution P ( y | x )andthelasttermis a regularizer. Thus, without considering the regularizer, set-ting Eq. 5 to zero means making the expected value match the empirical value.
As mentioned previously, it is not easy to select the gold-standard sub-queries for our problem, which makes the di-rect application of CRFs difficult. Therefore, a performance-based based Conditional Random Field (CRF-perf) is pro-posed to solve this problem, which uses all sub-queries and their associated retrieval performance. Using the notations introduced in Section 3, the objective function of CRF-perf on the training set is defined by Eq. 6.

Here, T is the number of x in the training set Train . P y P ( y | x ) m ( y ) is the expected retrieval performance over the distribution of label sequences for a given x .Compared with Eq. 3, which optimizes the probability of observing the gold-standard label sequence y , Eq. 6 directly opti-mizes the Geometric Mean of the expected retrieval perfor-mance of each x in the training set. For example, if m ( measures average precision (AP), CRF-perf optimize the ge-ometric mean average precision (GMAP) on the training set, which is a widely used performance measure in information retrieval. According to Robertson [23], GMAP is sensitive to improvements on the difficult queries.

Since T is a constant value after the training set is pro-vided, optimizing Eq. 6 is equivalent to optimizing Eq. 7.
The corresponding log-likelihood expression of Eq. 7 is shown in Eq. 8.
From this, we can show that the partial derivatives of  X  k can be calculated by Eq. 9. P m ( y | x ) is a distribution weighted by m ( y ), which is shown as follows.

In Eq. 9, the first term is the expected value of f k weighted by the retrieval performance and the second term is the ex-pected value of f k without weighting. Setting Eq. 9 to zero means making the distribution of sub-queries match the dis-tribution of their retrieval performance.

Note that if we set m ( y ) = 1 for the case y = y and set m ( y ) = 0 for other cases, CRF-perf becomes the conven-tional CRF model. Therefore, the conventional CRF model is a special case of CRF-perf.

The inference problems of the CRF model involve how to marginalize over the label sequence during the training phase and how to pick up the best label sequence during the predicting phase. For some special graph structures such as linear-chain CRF [12], the inference problem can be solved with dynamic programming techniques. For general graph structures, more complex inference techniques are required such as Contrastive Divergence [6] and Loopy Belief Propa-gation [28].

In our situation, some properties of the sub-query selec-tion makes the inference problem easier. According to the study made by Bendersky and Croft [2] on a search log, 90.3% queries have a length of no more than 4 words, 9.6% queries have a length between 5 to 12 words and only 0.1% queries have a length more than twelve. In addition, they noticed that there are a considerable number of seemingly bot-generated queries among those 0.1% of queries. There-fore, it is reasonable to assume most verbose queries used by web users are no more than 12 words. Then, the space of the label sequence is at the scale of 2 12 = 4096, which is computationally tractable for simple iterations over the whole space.
Three types of features are used to capture different levels of dependencies underlying the input sequence. Some of thefeaturesusedherearethesameasthefeaturesusedby Bendersky and Croft [3] and by Kumaran and Carvalho [11]. Independency Features characterize a single query word. The general feature function f k ( x , y ) can be specialized as f ( x i ,y i ). This type of feature includes the standard term frequency and document frequency in the target corpus, the frequency of the query word observed in external resources such as Google Ngram, Wikipedia and some commercial query logs. Besides statistical information, they also include some syntactic features such as POS-tags.

Local Dependency Features capture the dependencies between query words. Since they characterize some query words but not all, they are called Local Dependency Fea-tures. The general feature function f k ( x , y ) can be special-bigrams, noun phrases, the dependency relations returned by a dependency parser [16] and named entities. All sta-tistical features used for single words can also be applied to bigrams. Noun phrases have been shown to be effective uTF unigram term frequency uDF unigram document frequency uNGram unigram count in Google nGram uWiki unigram count of matching Wiki titles uMSNLog unigram count in MSN query logs uPosTag unigram pos-tag= X  X N X ,  X  X B X ,  X  X J X  bTF bigram term frequency bDF bigram document frequency bNGram bigram count in Google nGram bWiki bigram count of matching Wiki titles bMSNLog bigram count in MSN query logs np noun phrases dep-obj[16] the object relation dep-subj[16] the subject relation dep-nn[16] the noun compound modifier relation PER person names LOC location names ORG organization names MI[10] mutual information SQLen[11] sub-query length QS[5] query scope QC[24] query clarity score
SOQ[11] similarity to original query psg count of passages containing sub-query h-pnode height of the parent node covering sub-query when they are combined with the original query [1]. A de-pendency parser [16] can return different types of relations between two query words. For example, given the sentence  X  X ow frequently does the Mississippi River flood its banks? X ,  X  X iver X  and  X  X lood X  satisfies the subject relation, since  X  X iver X  is the subject of  X  X lood X . Also,  X  X lood X  and  X  X anks X  have the object relation, since  X  X anks X  is the object of  X  X lood X . It helps sometimes to include words satisfying a certain relation in a sub-query. Named entities including names of people, loca-tions and organizations are usually important and may be included in sub-queries.

Global Dependency Features describe properties of the selected sub-query, which is generated from all query words ( x ) and their labels ( y ). Therefore, they are called Global Dependency Features. The feature function used here is f k ( x , y ). As indicated by Kumaran and Carvalho [11], query quality predictors are good features to characterize sub-queries. Here, several types of query quality predictors have been used, such as Mutual Information [10], Query Scope [5], Query Clarity [24] and so on. Passage-level evi-dence can also be used to describe the sub-query. If a sub-query appears frequently within a passage, it is very likely that query words of this sub-query are closely related. Thus, the number of passages containing the sub-query selected can be used as a feature. The parsing tree [9] of the input query also provides valuable information. It is interesting to consider whether query words of the selected sub-query con-centrate on a small part of the parsing tree or spread over the whole tree. This property is partly measured by the height of the direct parent node that covers the sub-query in the parsing tree.

All features used are summarized in Table 1.
As mentioned previously, CRF-perf provides a general learning framework for different retrieval models and per-formance measures, which is indicated by m ( y ,M )where m is the performance measure function and M is the retrieval model. CRF-perf( M , m )isusedtodenotetheCRF-perf model trained based on the retrieval model M and the per-formance measure m . Four types of retrieval models using sub-queries are proposed in this paper. Other types of re-trieval models can be easily incorporated within this frame-work. Q s denotes a sub-query and Q denotes the original verbose query.

SubQL denotes the query likelihood model using the sub-query. The score of a document can be calculated as follows: where T ( Q s ) represents a set of query terms of Q s . P is the probability of generating a term t from a document which is estimated using the Language Modeling approach [20] with Dirchlet Smoothing [29].

SubDM denotes the sequential dependency model [19] using the sub-query. The score of a document can be calcu-lated as follows: +  X  O where T ( Q s ) denotes a set of query terms of Q s , O ( denotes a set of ordered bigrams extracted from Q s and U ( Q s ) denotes a set of unordered bigrams extracted from Q .  X  T ,  X  O and  X  U are parameters controling the weights of different parts and are usually set as 0.85, 0.1 and 0.05 according to [19].
 The above retrieval models only use the sub-query Q s . The following retrieval models attempt to combine the sub-query Q s with the original query Q .

QL+SubQL denotes a combination of the original query and the sub-query, where both parts use the query likelihood model. The score of a document using this model can be calculated as follows: score ( D, Q, Q s )=  X score QL ( D, Q )+(1  X   X  ) score QL where  X  is a parameter weighting the original query and the sub-query.  X  is set as 0.8 in this paper according to [1].
DM+SubQL uses the sequential dependency model for the original query and uses the query likelihood model for the sub-query. The score of a document is calculated as follows: score ( D, Q, Q s )=  X score DM ( D, Q )+(1  X   X  ) score QL where  X  is a parameter and is also set as 0.8 in this paper.
The above four retrieval models can all be implemented using the Indri query language [18]. Table 2 shows an ex-ample of Indri queries used for each retrieval model.
During the training phase, the retrieval model M is used to calculate m ( y ,M ) for each sub-query. After learning CRF-perf, the probability distribution P ( y | x )ispredicted for an unseen query x . We then consider how to do retrieval Q : jobs outsourced india Q s : jobs india SubQL: #combine(jobs india)
SubDM: #weight(
QL+SubQL: #weight(
DM+SubQL: #weight( Table 3: TREC collections used in experiments for x using M and P ( y | x ). A straightforward method is to select the sub-query y with the highest probability and feed it to M . This method is denoted as Top1 . Another alter-native is to feed the top k sub-queries to M respectively and combine the retrieval scores with their corresponding prob-abilities. This method is denoted as TopK .Take SubQL for example. The retrieval scores of using top k sub-queries are calculated as follows: score QL ( D, { Q s } )= where P ( Q s i | Q ) corresponds to the probability of the sub-query Q s i .
In this section, we first describe the experimental configu-ration, then provide examples of the sub-query distributions generated by CRF-perf and finally report the experimental results.
Experiments are conducted on three TREC collections (Gov2, Robust04 and WT10g). The statistics of each col-lection are summarized in Table 3.

For each collection, Indri 2.10 [18] is used to build the index with the Porter Stemmer [21]. No stopword removal is performed during indexing. For each topic, the description part is used as the query. Following Bendersky et al [3], a short list of 35 stopwords and some frequent stop patterns (e.g.,  X  X ind information X ) are removed from the description query in order to improve the retrieval performance of the baseline methods. If the length of a query is more than 10 words, all query words are first ranked by their idf scores and then the top 10 words are kept for sub-query generating. Following Kumaran and Carvalho [11], only sub-queries with length between three to six words are considered.
The query set is split into a training set and a test set. On the training set, four types of retrieval methods mentioned in Section 5 (SubQL, SubDM, QL+SubQL, DM+SubQL) are used to help learn CRF-perf, respectively. On the test set, the performance of using each retrieval model and sub-queries selected by its corresponding CRF model is reported. Ten-fold cross validation is conducted in this paper. The parameter  X  2 of CRF is set as 100. According to Sutton and McCallum [27], the performance of the CRF model doesn X  X  appear to be sensitive to changes of  X  2 .

Several baseline methods are compared. QL denotes the query likelihood language model [20, 29]. DM denotes the sequential dependence model [19]. SRank denotes Kumaran and Carvalho X  X  method [11], which considers sub-query se-lection as a ranking problem. They used Rank SVM [8] as the ranking model. According to their suggestions, the pa-rameters of Rank SVM are set as follows: RBF kernel is used with  X  set as 0.001 and C is set as 0.01. KeyConcept denotes Bendersky and Croft X  X  method [1] that augments the original query by discovering key concepts.

The standard performance measures, mean average preci-sion (MAP) and precision at 10 (P@10), are used to measure retrieval performance 2 . The two-tailed t-test is conducted for significance.
We report MAP  X  100 and P@10  X  100 in experiments.
First, we present some examples of P ( Q s | Q )(or P ( y learned by CRF-perf(SubQL,AP) in Table 4. The retrieval performance (MAP) of the original query and sub-queries is reported on the Gov2 collection. As mentioned previ-ously, some stopwords and st op patterns are removed from the original query. Those words are italicized to improve readability. Note that they are not used for retrieval and sub-query generation.

Table 4 shows that the CRF-pref model can learn a reason-able distribution of sub-queries, which successfully assigns high probabilities to sub-queries that perform better than the original query. For example, given the original query  X  X teps manage control protect squirrels X , the top three sub-queries  X  X teps protect squirrels  X ,  X  X teps control squirrels X  and  X  X teps control protect squirrels  X  receive most of the probabil-ities and all of them perform much better than the original query.

As mentioned above, CRF-perf is a general learning frame-work that can be adapted to different retrieval models. It is interesting to compare the sub-queries learned based on different retrieval models. Table 5 compares the top one sub-query returned by SubQL and QL+SubQL and some exam-ples are also provided to compare SubDM and DM+SubQL. All models are learned based on AP.

Table 5 shows that when the sub-queries are used alone (SubQL and SubDM), CRF-perf tends to select longer sub-queries, since it is safer to cover most of the concepts in Table 6: Performance of retrieval models using sub-queries. q denotes significantly different with QL and denotes significantly different with DM.  X 1 X  de-notes the Top1 method and  X  X  X  denotes the TopK method.
 QL 25.43 52.21 25.49 43.13 19.61 32.68 DM 27.85 54.03 26.83 44.94 20.87 35.77 SRank 24.99 50.74 24.78 41.57 19.98 32.06 KeyConcept 27.52 53.83 25.97 41.65 21.01 34.02 SubQL(1) 25.90 51.88 25.43 40.84 18.97 31.55 SubQL(K) 26.66 q 53.36 25.96 41.93 19.27 31.75 QL+SubQL(1) 26.49 q 53.09 26.10 43.53 20.12 32.78 QL+SubQL(K) 26.76 q 53.15 26.20 q 43.21 19.94 33.20 SubDM(1) 28.17 q 53.49 26.56 q 42.69 20.26 33.92 SubDM(K) 28.60 q 53.76 27.07 q 43.69 20.70 34.74 DM+SubQL(1) 28.56 q 55.91 q DM+SubQL(K) 28.70 q the original query. When the sub-queries are combined with the original query (QL+SubQL, DM+SubQL), the CRF-perf model tends to favor shorter queries, since it is rea-sonable to focus on important concepts when the original query has covered all concepts. For example, given the orig-inal query  X  X istory location scottish highland games united states X , SubDM selects a sub-query  X  X ocation scottish high-land games united states X  which covers almost all concepts of the original query, while DM+SubQL simply picks up the most important concept  X  X cottish highland games X . Simi-larly, SubQL selects a subset query  X  X ocalities offer programs gifted talented students X  which only removes  X  X tates X  from the original query, while QL+SubQL selects the key concept  X  X ifted talented student X .
The first experiment is conducted to compare the pro-posed four types of retrieval models with baseline meth-ods. Those sub-query based retrieval models include SubQL, QL+SubQL, SubDM and DM+SubDM. Average Precision (AP) is optimized during training phase. Note that, unless otherwise mentioned, AP is optimized in the following ex-periments. For each model, the performance of using Top1 and TopK (K=10) sub-queries is reported, respectively. The baseline methods are QL (query likelihood language model), DM (sequential dependence model), SRank (Kumaran and Carvalho X  X  method [11]) and KeyConcept (Bendersky and Croft X  X  method [1]). The results are shown in Table 6. The best performance of each column is bolded.

Table 6 shows that SubQL(Top1) is comparable with QL, which indicates that using the top one sub-query does not outperform QL. This observation has been supported by the performance of SRank, a Ranking SVM based method, which also uses the top one sub-query for retrieval. SubQL (Top1) performs slightly better than SRank, especially on Gov2. When top K sub-queries are used, the performance improves. SubQL(TopK) performs better than QL on Gov2. QL+SubQL performs better than QL on all three collec-tions, which indicates that combining the sub-query with the original query is promising. SubDM is slightly better than DM on Gov2 and Robust04 according to MAP. The best performance is achieved by DM+SubQL, which per-forms significantly better than DM, a very strong baseline, on Gov2 and Robust04. It indicates that the most effective way is to use the sequential dependence model for the origi-nal query, use the query likelihood model for the sub-query and combine them together. DM+SubQL also performs bet-ter than KeyConcept, the state-of-the-art technique for im-proving verbose queries. Generally, the TopK method is better than the Top1 method, which is consistent over dif-ferent retrieval models.
The second experiment is conducted to help understand where the benefits of CRF-perf come from. We compare CRF-perf with two other methods. The first method con-siders the sub-query selection as a classification problem, where a classifier is trained to decide the label of each word independently. For a query in the training set, the labels of query words are decided by its best sub-query. A standard SVM classifier [7] is used here 3 , since it has been successfully applied to a variety of classification tasks. This method is denoted as SVM. The features used to describe a single word are displayed in Table 7. The comparisons between SVM and CRF-perf help indicate whether it is helpful to consider sub-query selection as a sequential labeling problem instead of a classification problem. The second method is the or-
Note that the SVM used here is a standard classification method that assigns labels to each single word. It is different with the RankSVM used by Kumaran and Carvalho [11] that ranks sub-queries.
Table 7: Features used to describe a single word Table 8: Comparisons of SVM, CRF and CRF-perf.
 SubQL(1) is the retrieval model.
 SVM 23.91 46.04 20.68 33.41 19.45 31.96 CRF 24.76 48.99 23.21 36.99 17.48 29.48
CRF-perf 25.90 51.88 25.43 40.84 18.97 31.55 dinary CRF (denoted as CRF). As discussed in Section 1, the ordinary CRF uses the sub-query with the best perfor-mance as the gold-standard label sequence, while CRF-perf considers all sub-queries with their performance. The com-parisons between CRF and CRF-perf help indicate whether it is helpful to consider all sub-queries. Table 8 shows the retrieval performance of SVM, CRF and CRF-perf. Here, SubQL(1) is used as the retrieval model, which means using the top one sub-query for retrieval.

Table 8 shows that CRF and CRF-perf perform better than SVM on Gov2 and Robust04. CRF-perf is comparable with SVM on Wt10g. These results support that modeling the sub-query selection as a sequential labeling problem is in-deed helpful. Also, CRF-perf outperforms CRF on all three collections, thus it is important to consider all sub-queries with their performance for sub-query selection.

Some examples of the sub-query returned by SVM, CRF and CRF-perf are provided in Table 9 to help understand their differences. SVM is generally able to select important nouns or noun phrases from the original query, since in most cases the selection of nouns can be decided independently. However, it has trouble in selecting verbs and adjectives, since the selection of these types of words usually depends on the relations to other words, which cannot be captured by SVM. For example, SVM selects the important nouns  X  X aryland X  and  X  X hesapeake bay X , but misses the key verb  X  X lean X . More examples can be found in Table 9. CRF, on the other hand, does consider the relations between query words, thus it can make better decisions. However, CRF is trained based on the best sub-queries. As discussed in Section 1, the best sub-queries are usually short and may cause overfitting on the collection. Therefore, CRF tends to select short sub-queries and this can lead to mistakes. Some-times, CRF selects good sub-queries such as  X  X spirin prevent cancer X  (see Table 9), but, in most cases, the sub-queries se-lected by CRF are too short to cover all important concepts of the original queries such as  X  X astric bypass surgery X  and  X  X eople puerto rico X  (see Table 9). In contrast, the sub-query selected by CRF-perf may not be the perfect one, but it reliably improves the original query. This property is ob-tained by optimizing the expected, not the best, retrieval performance on the training set.
The third experiment is conducted to explore the relative effect of different types of features (Independency Features, Local Dependency Features and Global Dependency Fea-tures). Table 10 reports the mean average precision (MAP) of different retrieval models with different types of features.
Table 10 shows that the best feature set is not consistent over different retrieval models and different collections. On Gov2, different retrieval models prefer different features. On Robust04, combining all three types of features together is clearly the best choice. On Wt10g, only using the Indepen-dency Features and the Global Dependency Features (I+G) seems to be the best choice. It is also interesting to no-tice that the best feature set almost always involves Global Dependency Features, which shows that capturing global de-pendencies of queries is important for sub-query selection. The observations using Top1 and using TopK are consistent. Generally, combining different types of features together is more effective than only using a single type of features. Us-ing all features together can achieve good performance for most cases.
Table 6 has shown that using the top ten sub-queries is better than only using the top one sub-query. In this sub-section, we explore the effect of k , i.e. the number of sub-queries used. The results are displayed in Fig. 2. The retrieval model used is SubQL.

Fig. 2 shows that on three collections the most obvious performance improvement happens when k increases from one to five and there is not much difference when k is big-ger than five. The reason is that the top five sub-queries usually receive most of the probabilities of the distribution. The above observations support using a distribution of sub-queries for improving retrieval performance. Thus, instead of returning a single sub-query, the model that can generate the sub-query distribution is preferred.
Since all above results are reported using CRF-perf trained on AP, it is interesting to compare them with the results us-ing CRF-perf trained on P@10. Table 11 reports the perfor-mance of CRF-perf trained on different performance mea-sures (i.e. AP and P@10).

Table 11 shows that the results using CRF-perf trained on different performance measure s are similar. Generally, the CRF-perf trained on AP can produce slightly better per-formance than the model trained on P@10. Although we expect that the model trained on P@10 can generate better precision scores than the model trained on AP, actually the model trained on AP still performs better on P@10 except for Wt10g. A possible explanation is that the AP metric allows the CRF model to distinguish between query subsets that would appear identical using the P@10 metric.
Selecting sub-queries provides an effective way to improve the retrieval performance of verbose queries. In this paper, the sub-query selection problem is modeled as a sequential labeling problem. A novel Conditional Random Field model is proposed to capture the local and global dependencies underlying the verbose query and to directly optimize the Table 9: Comparisons of SVM, CRF and CRF-perf. In the original query are italicized and the words actually used are bolded. Q : what is the state of maryland doing to clean up the chesapeake bay Q : what would cause a lowered white blood cell count Q : what evidence is there that aspirin may help prevent cancer Q : do people in puerto rico want for it to become a us state Features.
 retrieval performance on the training set. Four types of re-trieval models are proposed to incorporate sub-queries. Ex-periments show that the proposed CRF model successfully selects high-quality sub-queries for different retrieval mod-els. The best performance is observed when the selected sub-queries are combined with the original query. Besides generating sub-query distribution, the CRF-perf model is also promising to generate reformulated-query distribution. Studying how to combine these two types of distributions within the same CRF-based framework will be an interest-ing future issue.
 This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS-0711348, and in part by NSF grant #IIS-0534383. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. We appreciate Kumaran and Carvalho [11] and Bendersky et al [3] to provide the codes andfeaturesusedintheirwork. [1] M.BenderskyandW.B.Croft.Discoveringkey [2] M.BenderskyandW.B.Croft.Analysisoflong [3] M. Bendersky, D. Metzler, and W. B. Croft. Learning [4] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and [5] B. He and I. Ounis. Inferring query performance using [6] G. E. Hinton. Training products of experts by [7] T. Joachims. Making large-scale SVM learning [8] T. Joachims. Optimizing search engines using [9] D. Klein and C. D. Manning. Accurate unlexicalized [10] G. Kumaran and J. Allan. A case for shorter queries, [11] G. Kumaran and V. R. Carvalho. Reducing long [12] J. Lafferty, A. McCallum, and F. Pereira. Conditional [13] M. Lease. An improved Markov random field model [14] M. Lease, J. Allan, and W. B. Croft. Regression rank: [15] X. Li, Y.-Y. Wang, and A. Acero. Extracting [16] M.-C. Marneffe and C. D. Manning. Stanford typed [17] A. McCallum and W. Li. Early results for named [18] D. Metzler and W. B. Croft. Combining the language [19] D. Metzler and W. B. Croft. A Markov random field [20] J. M. Ponte and W. B. Croft. A language modeling [21] M. F. Porter. An algorithm for suffix stripping. [22] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, and [23] S. Robertson. On GMAP: and other transformations. [24] Y. Z. S. Cronen-Townsend and W. B. Croft.
 [25] B. Settles. Abner: an open source tool for [26] F. Sha and F. Pereira. Shallow parsing with [27] C. Sutton and A. McCallum. An introduction to [28] B. Taskar, P. Abbeel, and D. Koller. Discriminative [29] C. Zhai and J. Lafferty. A study of smoothing
