 With the dramatically increasing number of sites and the huge size of the web which is in the order of hundreds of terabytes [5] and also with the wide variety of user groups with their diff erent interests, probing for sites of specific interests in order to solve users X  problems is really difficult. On the other hand, almost predominant section of the information which exists in the web is not practical for many users and this portion might interfere the results which are retrieved by users X  queries. It is apparent that searching in the tiny relevant portion can provide us better information and lead us to more interesting sites and places on a specific topic.

At this time, there are a few directory services like DMOZ [3] and Yahoo [11] which provide us by useful informat ion in several topics. However, as they are constructed, managed and updated manually, most of the time they have incomplete old information. Not only webpages changes fast, but also linkage information and access records are upda ted day by day. These quick changes together with the extensive amount of information in the web necessitate auto-mated subject-specific website classification.

This paper proposes an effective new meth od for website classification. Here, we use the content of pages together with the link structure of them to obtain more accuracy and better results in cla ssification. Also among different models for representing websites, we choose tree structure for its efficiency. Tree model is useful because it displays the link structure of a given website clearly.
Before we begin to talk about ways of website classification, it is better to explain the problem more formally. Given a set of site classes C and a new website S consisting of a set of pages P, the task of website classification is to determine the element of C which best c ategorizes the site S based on a set of examples of preclassified websites. In other words, the task is to find a class C that website S is more likely to be its member.

The remaining of this paper is organized as follows. Related works are dis-cussed in Section 2. Section 3 describes the models which we use for representing websites and website classes. In Sectio n 4, we explain our method for classify-ing websites together with the extended version of Viterbi algorithm. Section 5 is about learning and sampling techniques. A performance study is reported in Section 6. Finally, Section 7 concludes the paper and discusses future works. A completer version of this paper is available through authors X  homepages. Text classification has been an active area of research for many years. A signif-icant number of these methods have been applied to classification of web pages but there was no special attention to hyp erlinks. Apparently, a collection of web pages with a specific hyperlink structure conveys more information than a col-lection of text documents. A robust statistical model and a relaxation labeling technique are presented in [2] to use the class label and text of neighboring in addition to text of the page for hypertext and web page categorization. Catego-rization by context is proposed in [1], w hich instead of depending on a document alone, extracts useful information for classifying the document from the context of the page in which the hyperlink to the document exists. Empirical evidence is provided in [9] which shows that good web-page summaries generated by human editors can indeed improve the performance of web-page classification algorithms.

On the other hand, website classification has not been researched widely; the basic approach is superpage-based system that is proposed in [8]. Pierre discussed several issues related to automated text cl assification of Web sites, and described a superpage-based system for automatically classifying Web sites into industrial categories. In this work, we just generat e a single feature vector counting the frequency of terms over all HTML-pages of the whole site, i.e. we represent a web site as a single  X  X uperpage X . Two new more natural and more expressive representation of web sites has been introduced in [6] in which, every webpage is assigned a topic out of a predefined set of topics. In the first one, Feature Vector of Topic Frequencies, each considered topic defines a dimension of the feature space. For each topic, the feature values represent the number of pages within the site having that particular topic. In the second one, Website Tree, the website is represented with a labeled tree and the k-o rder Markov tree classifier is employed for site categorization. The main difference between our work and this work is that in the latter, the topic (class) of each page is independently identified with a classifier and without considering other pages in the website tree, and then the topic of pages tree is used to compute the website class, but this independent topic identification will lower the accu racy of responses. Whereas in our work we calculate the website class without explicitly assigning a topic to each page and the topics of pages will be hidden to us. In [7] a website is represented with a set of feature vectors of terms. By choosing this representation, effort spent on deriving topic frequency vector s will be avoided. kNN-classification is employed to classify a given website acco rding to training database of websites with known classes. In [10] the website structure is represented by a two layered tree: a DOM tree for each page and a page tree for the website. In [4] a new approach is presented for classifying multipage documents by na  X   X ve bayes HMM. However, to the best of our knowledge, there is no work on extending HMM for classifying data represented as tree. Before modeling websites and website classes, defining some terminology is nec-essary.
 Definition 1. (The set of webSite class Label : SL) SL is the set of labels which can be assigned to websites as class labels, in other words, members of SL are category domains.
 Definition 2. (The set of webPage class Labels : PL) For each label sl in SL, there is a set of known labels which can be assigned to individual pages of a website in that specified category. This labels can be seen as pages X  topics. There are many choices to use as page class models like Na  X   X ve-Bayesian approach and Hidden Markov models. We prefer the former due to its simplicity. In this paper Na  X   X ve-Bayes model is adopted for modeling webpage classes. For modeling website classes, we extend Hidden Markov Model in order to satisfy the criteria of content and link structure within pages.
 Definition 3. (webSite Class Model : SCM) For each category domain sl  X  SL , the model of sl is a directed graph G ( V, E ) in which V ,G X  X  vertex set, is a set of webpage class models and for every two states v i ,v j  X  V there is two directed edges { ( i.j ) , ( j, i ) } X  E that show the probability of moving from one to another. Also there is a loop for every state which shows the probability of transition between two pages of that class.

In this model, the state-transition probability matrix A is an n  X  n matrix in which n is the cardinality of SL and a ij = P ( c t +1 = j | c t = i ) , 0  X  i, j  X  n , which shows the probability of transition from state i to state j . Also the emission generating page p by webpage class model j .

As a result, we have a hybrid of Na  X   X ve-Bayes HMM model for all of websites classes. As we mentioned before, it wa s possible to model website classes by a hierarchial HMM, if HMM was used instead of Na  X   X ve-Bayes model for describing webpage classes. It is important to note that the input of website class models are websites which are modeled by trees . We describe website models in a more formal way, as follows.
 Definition 4. (webSite Model : SM) For each website ws ,the ws  X  X  model is a page tree T ws =( V, E ) in which V is a subset of ws  X  X  pages. The root of T ws is the homepage of the website and there is a directed edge between two vertices v ,v j  X  V if and only if the corresponding page p j is a child of p i .Wecrawl the website by Breath-First-Search technique and ignore the links to previously visited pages in the time of model construction. In previous sections, we d escribed a model for each website class. We assume SCM i is the model of website class C i . Also we consider that we want to classify website ws . To find the category of ws ,wecalculate P ( SCM i | ws )for1  X  i  X  n . By Considering Bayes rule, we can determine the class of ws as P ( ws ) is constant for all classes and we neglect P ( SCM i ). It can be considered later or even it can be equal for all classes if we use a fair distribution of websites over different classes. Therefore C map = argmax i P ( ws | SCM i ).

To find the probability of P ( ws | SCM i ) and also to solve the webpage classi-fication problem, we should extend the Viterbi algorithm for our models. 4.1 Extended Viterbi Algorithm Using dynamic programming techniques and therefore Viterbi algorithm ideas seems reasonable for solving these probabilities. However as we mentioned before, the inputs of our models are trees of pages, while Viterbi algorithm is provided to calculate the maximum probability of generating sequences. Therefore, we should modify the traditional Viterbi algorithm. Before introducing the new approach, it is necessary to present theorem 1 which is discussed in [10]. Theorem 1. The probability of each node is only dependent to its parent and children. More formally, for every node n with p as its parent and q 1 , ...q n as its children, if PL represents the webpage label set then
Here, we propose a novel method in which the probability of a node and its siblings is computed simultaneously to avoid any possible inconsistency. Pos-sible inconsistencies together with an illustrating example is provided in the completer version of the paper. Thus, in the n th level of the tree, we calculate the probability of the children of an n  X  1th level X  X  node by equation 2. Algorithm 1. (Extended Viterbi Algorithm) For Classifying an indicated web-site ws which is modeled by a tree structure T ws against n different classes C ,...,C n whicharemodeledby SCM 1 ,...,SCM n ,if p is a node in level n  X  1 of T ws and it has children q 1 ,...,q n then
P [( q 1 ,...,q n )  X  ( pl s 1 ,...,pls n )] = where pl s i  X  PL .Inequation2, P [( q 1 ,...,q n )  X  ( pl s 1 ,...,pls n )] is the max-imum probability of generating the nodes of upper levels by the model as well as assigning pl s 1 to page q 1 , pl s 2 to page q 2 ,  X  X  X  ,and pl s n to page q n .Tocal-culate equation 2 for lower levels, we should have the probability of each page individually. Therefore we calculate these probabilities by equation 3. where pl s i = pl j and  X  k = i : pl s k  X  PL . The probability of generating the tree model of the website ws from the model SCM i when q represents a leaf of the tree is: Here we want to compute the complexity of our algorithm. We should compute p ( q i = pl j ) for every node in each level. Suppose we have branching factor of max-imum b in this tree, so for each set of siblings the computation of P [( q 1 , ..., q b )  X  ( pl plexity, in which n is the number of page labels. For each node in this set of Whereas we should compute this probability for every node in the tree, the total complexity will be O ( n b +1 b L  X  2 ), where L is the number of tree X  X  levels. As mentioned above, first, we determine website class and webpage class labels. Then a set of sample websites for learnin g phase are assigned by an expert or from sites like DMOZ as seeds for constructing class models. One of the fundamental steps in this phase is assigning a label to each web page. There are two types of pages: One that has predetermined lab els (i.e. form a constant part in their URL) and the other which has no specified label. We have to assign labels to the second type. We assign labels to about %2 of pages in the Training set manually. The remaining %98 of the pages will be labeled by Naive Bayes based upon this 2 percent. To construct websi te class models, we have to compute the state-transition matrix A and emission probability e . A can be easily computed as follows. in which N ( c i ,c j ) is the number of times that a page of type c i links to a page of type c j in the training set. As we use na  X   X ve-bayes, we can also easily compute e j ( p ). Therefore, for each page model c j and word w l N ( w l ,c j ) is the number of occurrence of word w l in webpages of class c j and V represents the set of selected keywords. Therefore, if p is formed from w 1 ...w n then e c j ( p )= P ( w 1 | c j ) ...P ( w n | c j ).

There are two general reasons for our motivation to use page pruning algo-rithm. First, downloading web pages in contrast with operations that take place in memory is very expensive and time consuming. In a typical website there are too many pages that cannot convey useful information for website classification, if we can prune these pages, website classification performance improves signif-icantly. The second reason that leads us to use pruning algorithm is that in a typical website, there are pages that affect classification in an undesirable direc-tion, so pruning these unrelated or unspecific pages can improve our accuracy in addition to performance.

We use the pruning measures used in [6] for its efficiency and we modify the pruning algorithm for our method. To compute measures for a partial tree which we have downloaded up to now, we should be able to compute membership of this partial tree website for each website class. Our model is suitable for this computation because we compute the probability of each partial tree incremen-tally and when we add new level to previous partial tree we just compute the P [( q 1 , ..., q b )  X  ( pl s 1 , ...pl s for all the new siblings by using previous probabilities and according to our algo-rithm. Then by using these probabilities we calculate p ( q i = pl j ) for each node in the new level. For each node q , we define P ( q | sl i )=max pl j  X  PL p ( q = pl j ) where sl i  X  SL .

For each no de q of partial website tree t : weight ( q )=  X  2 sl By adding a new node q to a partial tree t we obtain a new partial tree t 2 .Westop By means of this pruning procedure and saving probabilities, we perform classi-fication in the same time we use pruning algorithm, and then we can determine the most similar class of the given website. We examine different  X  to find ap-propriate one for our data set. Choosing proper  X  can help us to achieve even higher accuracy than comp lete website download. In this section we demonstrate the results of some experimental evaluation on our approach and compare it with other existing algorithms, mainly extracted from [6]. We use these methods to classify scientific websites into ten following classes: Agriculture, Astronomy, Biology, Chemistry, Computer Science, Earth Sciences, Environment, Math, Physics, Other. We use DMOZ[3] directory to obtain websites for the first 9 classes. We downloaded 25 website for each class and a total of 86842 web pages. At this point we downloaded a website almost completely (Limited number of pages at most 400). For the  X  X ther X  class we randomly chose 50 websites from Yahoo! Directory classes that were not in the first nine classes which had 18658 web pages. We downloaded all websites to local computer and saved them locally. To use our algorithm first we should prepare our initial data seed and then we build a model for each class and compute its parameter as stated in the learning phase. To classify the pages of a website class, we labeled about %2 of them in each web site class manually then The remaining %98 of the pages were labeled by Naive Bayes based upon this labeled pages. At the end of this process we have a na  X   X ve-bayes model for each page class of each website category. By means of these na  X   X ve-base models we classified web pages for other methods. For testing our methods we randomly downloaded 15 new website almost complete (limiting to 400 pages)for each class.
 We compare our algorithm to 4 other methods: 0-order Markov tree, C4.5, Na  X   X ve-bayes, classification of superpage. In superpage, classifying a web site is to extend the methods used for page classification to our definition of web sites. We just generate a single feature vector counting the frequency of terms over all HTML-pages of the whole site, i.e. we represent a web site as a single  X  X uper-page X . For C4.5 and na  X   X ve-bayes, first we build Feature vector of topic frequen-cies and then apply na  X   X ve-bayes and C4.5 algorithms on them. For the 0-order Markov tree we used the method descr ibed in [6]. You can find the accuracy of tested methods on testing dataset in table 1.
 As it can be seen the accuracy of our method is better than other methods. It is more accurate compared to 0-order Markov tree because page classes are hidden here and we calculate probability of the whole website that is generated from a model.

At last we examine the impact of different  X  values on the sampling algorithm in our training set. With an appropriate  X  , the accuracy increases in compari-son to complete website. To find appropriate  X  ,weincreased  X  gradually and when the overall accuracy stopped to increase, we choose  X  . In our data set the appropriate  X  was 6 , but this can change in respect to data set. In the growing world of web, taking advantage of different methods to classify websites seems to be very necessary. Websi te classification algorithms for dis-covery of interesting information leads many users to retrieve their desirable data more accurately and more quickly. This paper proposes a novel method for solving this problem. With extending Hidden Markov Model, we described models for website classes and looked for the most similar class for any website. Experimental Results show the efficiency of this new method for classification.
In the ongoing work, we are seeking for new methods to improve the efficiency and accuracy of our website cl assification method. Demonstrating websites with stronger models like website graphs can bring us more accuracy.

