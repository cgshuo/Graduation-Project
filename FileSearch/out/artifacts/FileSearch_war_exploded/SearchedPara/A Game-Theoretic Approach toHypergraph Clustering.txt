 Clustering is the problem of organizing a set of objects into groups, or clusters , in a way as to have similar objects grouped together and dissimilar ones assig ned to different groups, according to some similarity measure. Unfortunately, there is no universall y accepted formal definition of the notion satisfying two conditions: an internal coherency condition, which asks that the objects belonging to the cluster have high mutual similarities, and an external incoherency condition, which states that the overall cluster internal coherency decreases by adding to it any external object.
 Objects similarities are typically expressed as pairwise r elations, but in some applications higher-order relations are more appropriate, and approximating th em in terms of pairwise interactions can lead to substantial loss of information. Consider for insta nce the problem of clustering a given set of d -dimensional Euclidean points into lines. As every pair of d ata points trivially defines a line, there does not exist a meaningful pairwise measure of similarity f or this problem. However, it makes perfect sense to define similarity measures over triplets of points that indicate how close they are to being collinear. Clearly, this example can be generalize d to any problem of model-based point pattern clustering, where the deviation of a set of points fr om the model provides a measure of their dissimilarity. The problem of clustering objects using hig h-order similarities is usually referred to as the hypergraph clustering problem .
 In the machine learning community, there has been increasin g interest around this problem. Zien and co-authors [24] propose two approaches called  X  X lique e xpansion X  and  X  X tar expansion X , respec-tively. Both approaches transform the similarity hypergra ph into an edge-weighted graph, whose edge-weights are a function of the hypergraph X  X  original we ights. This way they are able to tackle the problem with standard pairwise clustering algorithms. Bolla [6] defines a Laplacian matrix for an unweighted hypergraph and establishes a link between the spectral properties of this matrix and the hypergraph X  X  minimum cut. Rodr` X guez [16] achieves sim ilar results by transforming the hyper-graph into a graph according to  X  X lique expansion X  and shows a relationship between the spectral pergraph. Zhou and co-authors [23] generalize their earlie r work on regularization on graphs and define a hypergraph normalized cut criterion for a k -partition of the vertices, which can be achieved by finding the second smallest eigenvector of a normalized La placian. This approach generalizes the well-known  X  X ormalized cut X  pairwise clustering algor ithm [19]. Finally, in [2] we find another work based on the idea of applying a spectral graph partition ing algorithm on an edge-weighted graph, which approximates the original (edge-weighted) hy pergraph. It is worth noting that the ap-proaches mentioned above are devised for dealing with highe r-order relations, but can all be reduced to standard pairwise clustering approaches [1]. A differen t formulation is introduced in [18], where the clustering problem with higher-order (super-symmetri c) similarities is cast into a nonnegative factorization of the closest hyper-stochastic version of t he input affinity tensor.
 All the afore-mentioned approaches to hypergraph clusteri ng are partition-based. Indeed, clusters are not modeled and sought directly, but they are obtained as a by-product of the partition of the input data into a fixed number of classes. This renders these approa ches vulnerable to applications where the number of classes is not known in advance, or where data is affected by clutter elements which do not belong to any cluster (as in figure/ground separation p roblems). Additionally, by partitioning, e.g., two intersecting lines have the point in the intersect ion belonging to both lines. In this paper, following [14, 20] we offer a radically differ ent perspective to the hypergraph cluster-ing problem. Instead of insisting on the idea of determining a partition of the input data, and hence obtaining the clusters as a by-product of the partitioning p rocess, we reverse the terms of the prob-lem and attempt instead to derive a rigorous formulation of t he very notion of a cluster. This allows one, in principle, to deal with more general problems where c lusters may overlap and/or outliers may get unassigned. We found that game theory offers a very el egant and general mathematical framework that serves well our purposes. The basic idea behi nd our approach is that the hypergraph clustering problem can be considered as a multi-player non-cooperative  X  X lustering game X . Within this context, the notion of a cluster turns out to be equivale nt to a classical equilibrium concept from (evolutionary) game theory, as the latter reflects both the i nternal and external cluster conditions alluded to before. We also show that there exists a correspon dence between these equilibria and the local solutions of a polynomial, linearly-constrained , optimization problem, and provide an al-gorithm for finding them. Experiments on two standard hyperg raph clustering problems show the superiority of the proposed approach over state-of-the-ar t hypergraph clustering techniques. Evolutionary game theory studies models of strategic inter actions (called games ) among large numbers of anonymous agents. A game can be formalized as a tri plet  X  = ( P, S,  X  ) , where strategies (in the terminology of game-theory) available to each playe r and  X  : S k  X  R is the payoff function , which assigns a payoff to each strategy profile , i.e., the (ordered) set of pure strategies played by the individuals. The payoff function  X  is assumed to be invariant to permutations of the strategy profile. It is worth noting that in general games, ea ch player may have its own set of strate-gies and own payoff function. For a comprehensive introduct ion to evolutionary game theory we refer to [22].
 By undertaking an evolutionary setting we assume to have a la rge population of non-rational agents, which are randomly matched to play a game  X  = ( P, S,  X  ) . Agents are considered non-rational, be-cause each of them initially chooses a strategy from S , which will be always played when selected for the game. An agent, who selected strategy i  X  S , is called i -strategist . Evolution in the popula-tion takes place, because we assume that there exists a selec tion mechanism, which, by analogy with a Darwinian process, spreads the fittest strategies in the po pulation to the detriment of the weakest one, which will in turn be driven to extinction. We will see la ter in this work a formalization of such a selection mechanism. The state of the population at a given time t can be represented as a n -dimensional vector x ( t ) , where x states describing a population is given by which is called standard simplex . In the sequel we will drop the time reference from the popula tion state, where not necessary. Moreover, we denote with  X  ( x ) the support of x  X   X  , i.e., the set of strategies still alive in population x  X   X  :  X  ( x ) = { i  X  S : x If y ( i )  X   X  is the probability distribution identifying which strateg y the i th player will adopt if drawn to play the game  X  , then the average payoff obtained by the agents can be comput ed as Note that (1) is invariant to any permutation of the input pro bability vectors.
 Assuming that the agents are randomly and independently dra wn from a population x  X   X  to play repeated k times. Furthermore, the average payoff that an i -strategist obtains in a population x  X   X  is given by u ( e i , x k  X  1 ) , where e i  X   X  is a vector with x An important notion in game theory is that of equilibrium [22 ]. A population x  X   X  is in equilibrium when the distribution of strategies will not change anymore , which intuitively happens when every individual in the population obtains the same average payof f and no strategy can thus prevail on the other ones. Formally, x  X   X  is a Nash equilibrium if In other words, every agent in the population performs at mos t as well as the population average payoff. Due to the multi-linearity of u , a consequence of (2) is that i.e., all the agents that survived the evolution obtain the s ame average payoff, which coincides with the population average payoff.
 A key concept pertaining to evolutionary game theory is that of an evolutionary stable strategy [7, 22]. Such a strategy is robust to evolutionary pressure i n an exact sense. Assume that in a population x  X   X  , a small share  X  of mutant agents appears, whose distribution of strategies is y  X   X  . The resulting postentry population is given by w  X  = (1  X   X  ) x +  X  y . Biological intuition suggests that evolutionary forces select against mutant in dividuals if and only if the average payoff of a mutant agent in the postentry population is lower than th at of an individual from the original population, i.e., A population x  X   X  is evolutionary stable (or an ESS) if inequality (4) holds for any distribution of mutant agents y  X   X  \{ x } , granted the population share of mutants  X  is sufficiently small (see, [22] for pairwise contests and [7] for n -wise contests).
 An alternative, but equivalent, characterization of ESSs i nvolves a leveled notion of evolutionary stable strategies [7]. We say that x  X   X  is an ESS of level j against y  X   X  , if there exists j  X  { 0 , . . . , k  X  1 } such that both conditions are satisfied. Clearly, x  X   X  is an ESS if it satisfies a condition of this form for every y  X   X  \{ x } . It is straightforward to see that any ESS is a Nash equilibriu m [22, 7]. An ESS, which satisfies conditions (6) with j never more than J , will be called an ESS of level J . Note that for the generic case most of the preceding conditions will be superfluous, i. e., only ESSs of level 0 or 1 are required [7]. Hence, in the sequel, we will consider only ESSs of level 1. It is not difficult to verify that any ESS (of level 1) x  X   X  satisfies for all y  X   X  \{ x } and small enough values of  X  . The hypergraph clustering problem can be described by an edg e-weighted hypergraph. Formally, of vertices , E  X  X  ( V ) \{ X  X  is the set of (hyper-)edges (here, P ( V ) is the power set of V ) and s : E  X  R is a weight function which associates a real value with each e dge. Note that negative weights are allowed too. Although hypergraphs may have edge s of varying cardinality, we will focus on a particular class of hypergraphs, called k -graphs, whose edges have all fixed cardinality k  X  2 . In this paper, we cast the hypergraph clustering problem int o a game, called (hypergraph) clustering game , which will be played in an evolutionary setting. Clusters a re then derived from the analy-sis of the ESSs of the clustering game. Specifically, given a k -graph H = ( V, E, s ) modeling a hypergraph clustering problem, where V = { 1 , . . . , n } is the set of objects to cluster and s is the similarity function over the set of objects in E , we can build a game involving k players, each of them having the same set of (pure) strategies, namely the set of objects to cluster V . Under this setting, a population x  X   X  of agents playing a clustering game represents in fact a clus ter, where x is the probability for object i to be part of it. Indeed, any cluster can be modeled as a probab ility distribution over the set of objects to cluster. The payoff f unction of the clustering game is defined in a way as to favour the evolution of agents supporting highl y coherent objects. Intuitively, this is accomplished by rewarding the k players in proportion to the similarity that the k played objects have. Hence, assuming ( v function can be simply defined as where the term 1 /k ! has been introduced for technical reasons.
 Given a population x  X   X  playing the clustering game, we have that the average popula tion payoff u ( x k ) measures the cluster X  X  internal coherency as the average si milarity of the objects forming the cluster, whereas the average payoff u ( e i , x k  X  1 ) of an agent supporting object i  X  V in population x , measures the average similarity of object i with respect to the cluster.
 An ESS of a clustering game incorporates the properties of in ternal coherency and external inco-herency of a cluster: internal coherency: since ESSs are Nash equilibria, from (3), it follows that eve ry object contribut-external incoherency: from (2), every object external to the cluster, i.e., every o bject in V \  X  ( x ) , Finally, it is worth noting that this theory generalizes the dominant-sets clustering framework which has recently been introduced in [14]. Indeed, ESSs of pairwi se clustering games, i.e. clustering games defined on graphs, correspond to the dominant-set clus ters [20, 17]. This is an additional evidence that ESSs are meaningful notions of cluster. In this section we will show that the ESSs of a clustering game are in one-to-one correspondence with (strict) local solution of a non-linear optimization p rogram. In order to find ESSs, we will also provide a dynamics due to Baum and Eagon, which generalizes t he replicator dynamics [22]. Let H = ( V, E, s ) be a hypergraph clustering problem and  X  = ( P, V,  X  ) be the corresponding clustering game. Consider the following non-linear optimi zation problem: It is simple to see that any first-order Karush-Kuhn-Tucker ( KKT) point x  X   X  of program (9) [13] is a Nash equilibrium of  X  . Indeed, by the KKT conditions there exist such that for all i  X  S , for all i  X  S . Moreover, it turns out that any strict local maximizer x  X   X  of (9) is an ESS of  X  . u ( x k ) , for any z  X   X  \{ x } close enough to x , which is in turn equivalent to (7) for sufficiently small values of  X  .
 The problem of extracting ESSs of our hypergraph clustering game can thus be cast into the problem of finding strict local solutions of (9). We will address this optimization task using a result due to Baum and Eagon [3], who introduced a class of nonlinear trans formations in probability domain. Theorem 1 (Baum-Eagon) . Let P ( x ) be a homogeneous polynomial in the variables x negative coefficients, and let x  X   X  . Define the mapping z = M ( x ) as follows: Then P ( M ( x )) &gt; P ( x ) , unless M ( x ) = x . In other words M is a growth transformation for the polynomial P .
 The Baum-Eagon inequality provides an effective iterative means for maximizing polynomial func-tions in probability domains, and in fact it has served as the basis for various statistical estimation techniques developed within the theory of probabilistic fu nctions of Markov chains [4]. It was also employed for the solution of relaxation labelling processe s [15].
 Since f ( x ) is a homogeneous polynomial in the variables x Theorem 1 in order to find a local solution x  X   X  of (9), which in turn provides us with an ESS of the hypergraph clustering game. By taking the support of x , we have a cluster under our framework. The describing the clustering problem and  X  is the average number of iteration needed to converge. Note that  X  never exceeded 100 in our experiments.
 In order to obtain the clustering, in principle, we have to fin d the ESSs of the clustering game. This is a non-trivial, although still feasible, task [21], w hich we leave as a future extension of this work. By now, we adopt a naive peeling-off strategy for our cluster extraction procedure. Namely, we iteratively find a cluster and remove it from the set of obje cts, and we repeat this process on the remaining objects until a desired number of clusters hav e been extracted. The set of extracted ESSs with this procedure does not technically correspond to the ESSs of the original game, but to ESSs of sub-games of it. The cost of this approximation is tha t we unfortunately loose (by now) the possibility of having overlapping clusters. In this section we present two types of experiments. The first one addresses the problem of line clustering, while the second one addresses the problem of il luminant-invariant face clustering. We tested our approach against Clique Averaging algorithm (C AVERAGE ), since it was the best per-forming approach in [2] on the same type of experiments. Spec ifically, C AVERAGE outperformed Clique Expansion [10] combined with Normalized cuts, Gibso n X  X  Algorithm under sum and product model [9], kHMeTiS [11] and Cascading RANSAC [2]. We also com pare against Super-symmetric Non-negative Tensor Factorization (S NTF ) [18], because it is the only approach, other than ours, which does not approximate the hypergraph to a graph.
 Since both C AVERAGE and S NTF , as opposed to our method, require the number of classes K to be specified, we run them with values of K  X  X  K  X   X  1 , K  X  , K  X  + 1 } among which the optimal one (
K  X  ) is present. This allows us to verify the robustness of the ap proaches under wrong values of K , which may occur in general as the optimal number of clusters i s not known in advance. Figure 1: Results on clustering 3 , 4 and 5 lines perturbed with increasing levels of Gaussian noise (  X  = 0 , 0 . 01 , 0 . 02 , 0 . 04 , 0 . 08 ).
 We executed the experiments on a AMD Sempron 3Ghz computer wi th 1Gb RAM. Moreover, we evaluated the quality of a clustering by computing the avera ge F-measure of each cluster in the ground-truth with the most compatible one in the obtained so lution (according to a one-to-one cor-respondence). 5.1 Line clustering We consider the problem of clustering lines in spaces of dime nsion greater than two, i.e., given a set of points in R d , the task is to find sets of collinear points. Pairwise measur es of similarity are useless and at least three points are needed. The dissimilar ity measure on triplets of points is given which has been optimally selected for all the approaches acc ording to a small test set. We conducted two experiments, in order to assess the robustn ess of the approaches to both local and global noise. Local noise refers to a Gaussian perturbat ion applied to the points of a line, while global noise consists of random outlier points.
 A first experiment consists in clustering 3 , 4 and 5 lines generated in the 5 -dimensional space and every point should be assigned to a line (e.g., see Figure 1(a)). Figure 1(b) shows the results obtained with three lines. We reported, for each noise level , the mean and the standard deviation of the average F-measures obtained by the algorithms on 30 ra ndomly generated instances. Note that, if the optimal K is used, C AVERAGE and S NTF perform well and the influence of local noise is minimal. This behavior intuitively makes sense under mod erate perturbations, because if the ap-proaches correctly partitioned the data without noise, it i s unlikely that the result will change by slightly perturbing them. Our approach however achieves go od performances as well, although we can notice that with the highest noise level, the performanc e slightly drops. This is due to the fact that points deviating too much from the overall cluster aver age collinearity will be excluded as they undermine the cluster X  X  internal coherency. Hence, some pe rturbed points will be considered out-liers. Nevertheless, it is worth noting that by underestima ting the optimal number of classes both C
AVERAGE and S NTF exhibit a drastic performance drop, whereas the influence of overestimations has a lower impact on the two partition-based algorithms. By increasing the number of lines involved in the experiment from three to four (Figure 1(c)) and to five ( Figure 1(d)) the scenario remains al-most the same for our approach and S NTF , while we can notice a slight decrease of C AVERAGE  X  X  performance.
 The second experiment consists in clustering 2 , 3 and 4 slightly perturbed lines (with fixed local noise  X  = 0 . 01 ) generated in the 5 -dimensional space [  X  2 , 2] 5 . Again, each line consists of 20 points. This time however we added also global noise, i.e., 0 , 10 , 20 and 40 random points as outliers (e.g., see Figure 2(a)). Figure 2(b) shows the results obtai ned with two lines. Here, the supremacy of our approach over partition-based ones is clear. Indeed, our method is not influenced by outliers and therefore it performs almost perfectly, whereas C AVERAGE and S NTF perform well only without outliers and with the optimal K . It is interesting to notice that, as outliers are introduce d, C AVERAGE and S NTF perform better with K &gt; 2 . Indeed, the only way to get rid of outliers is to group them in additional clusters. However, since outliers are not mutua lly similar and intuitively they do not form a cluster, we have that the performance of C AVERAGE and S NTF drop drastically as the number of outliers increases. Finally, by increasing the number of li nes from two to three (Figure 2(c)) and to four (Figure 2(d)), the performance of C AVERAGE and S NTF get worse, while our approach still achieves good results. 5.2 Illuminant-invariant face clustering In [5] it has been shown that images of a Lambertian object ill uminated by a point light source lie in a three dimensional subspace. According to this result, if w e assume that four images of a face form the columns of a matrix then d = s 2 where s clustering problem and we consider as dataset the Yale Face D atabase B and its extended version [8, 12]. In total we have faces of 38 individuals, each under 6 4 different illumination conditions. We compared our approach against C AVERAGE and S NTF on subsets of this face dataset. Specifically, we considered cases where we have faces from 4 and 5 random ind ividuals (10 faces per individual), and with and without outliers. The case with outliers consis ts in 10 additional faces each from a different individual. For each of those combinations, we cr eated 10 random subsets. Similarly to the case of line clustering, we run C AVERAGE and S NTF with values of K  X  X  K  X   X  1 , K  X  , K  X  + 1 } , where K  X  is the optimal one. In Table 1 we report the average F-measures (mean and standar d deviation) obtained by the three approaches. The results are consistent with those obtained in the case of line clustering with the exception of S NTF , which performs worse than the other approaches on this real -world application. C
AVERAGE and our algorithm perform comparably well when clustering 4 individuals without out-liers. However, our approach turns out to be more robust in ev ery other tested case, i.e., when the number of classes increases and when outliers are introduce d. Indeed, C AVERAGE  X  X  performance decreases, while our approach yields the same good results.
 In both the experiments of line and face clustering the execu tion times of our approach were higher than those of C AVERAGE , but considerably lower than S NTF . The main reason why C AVERAGE run faster is that our approach and S NTF work directly on the hypergraph without resorting to pair-wise relations, which is indeed what C AVERAGE does. Further, we mention that our code was not optimized to improve speed and all the approaches were run wi thout any sampling policy. In this paper, we offered a game-theoretic perspective to th e hypergraph clustering problem. Within our framework the clustering problem is viewed as a multi-pl ayer non-cooperative game, and clas-sical equilibrium notions from evolutionary game theory tu rn out to provide a natural formalization of the notion of a cluster. We showed that the problem of findin g these equilibria (clusters) is equiv-alent to solving a polynomial optimization problem with lin ear constraints, which we solve using an algorithm based on the Baum-Eagon inequality. An advantage of our approach over traditional tech-niques is the independence from the number of clusters, whic h is indeed an intrinsic characteristic of the input data, and the robustness against outliers, whic h is especially useful when solving figure-ground-like grouping problems. We also mention, as a potent ial positive feature of the proposed approach, the possibility of finding overlapping clusters ( e.g., along the lines presented in [21]), al-though in this paper we have not explicitly dealt with this pr oblem. The experimental results show the superiority of our approach with respect to the state of t he art in terms of quality of solution. We are currently studying alternatives to the plain Baum-Eago n dynamics in order to improve efficiency. Acknowledgments. We acknowledge financial support from the FET programme with in EU FP7, under the SIMBAD project (contract 213250). We also thank Sa meer Agarwal and Ron Zass for providing us with the code of their algorithms.
 [1] S. Agarwal, K. Branson, and S. Belongie. Higher order lea rning with graphs. In Int. Conf. on [2] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Perona, D. Kriegm an, and S. Belongie. Beyond [4] L. E. Baum, T. Petrie, G. Soules, and N. Weiss. A maximizat ion technique occurring in the [5] P. Belhumeur and D. Kriegman. What is the set of images of a n object under all possible [6] M. Bolla. Spectral, euclidean representations and clus terings of hypergraphs. Discr. Math. , [7] M. Broom., C. Cannings, and G. T. Vickers. Multi-player m atrix games. Bull. Math. Biology , [8] A. S. Georghiades., P. N. Belhumeur, and D. J. Kriegman. F rom few to many: illumination [9] D. Gibson, J. M. Kleinberg, and P. Raghavan. VLDB , chapter Clustering categoral data: An [10] T. Hu and K. Moerder. Multiterminal flows in hypergraphs . In T. Hu and E. S. Kuh, editors, [11] G. Karypis and V. Kumar. Multilevel k-way hypergraph pa rtitioning. VLSI Design , 11(3):285 X  [12] K. C. Lee, J. Ho, and D. Kriegman. Acquiring linear subsp aces for face recognition under [13] D. G. Luenberger. Linear and nonlinear programming . Addison Wesley, 1984. [14] M. Pavan and M. Pelillo. Dominant sets and pairwise clus tering. IEEE Trans. Pattern Anal. [15] M. Pelillo. The dynamics of nonlinear relaxation label ing processes. J. Math. Imag. and Vision , [16] J. Rodr` X guez. On the Laplacian spectrum and walk-regu lar hypergraphs. Linear and Multilin-[17] S. Rota Bul `o. A game-theoretic framework for similarity-based data clus tering . PhD thesis, [18] A. Shashua, R. Zass, and T. Hazan. Multi-way clustering using super-symmetric non-negative [19] J. Shi and J. Malik. Normalized cuts and image segmentat ion. IEEE Trans. Pattern Anal. [21] A. Torsello, S. Rota Bul `o, and M. Pelillo. Beyond parti tions: allowing overlapping groups in [22] J. W. Weibull. Evolutionary game theory . Cambridge University Press, 1995. [23] D. Zhou, J. Huang, and B. Sch  X olkopf. Learning with hype rgraphs: clustering, classification,
