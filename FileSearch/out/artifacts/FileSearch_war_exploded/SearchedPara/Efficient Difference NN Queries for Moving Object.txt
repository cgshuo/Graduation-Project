 Group Nearest neighbor (GNN) queries [1,2,3,4] are utilized in many applications, such as geographic information systems (GIS) [5], CAD/CAM, multi-media [6], knowledge discovery [7], data mining [8], and etc. As an extension of the traditional KNN queries, GNN queries maintain several query points and allow aggregate opera-tions over the query points. Such distinguishes make GNN queries more complicated than traditional KNN queries. 
GNN queries mainly focus on aggregate operations, which includes the most widely used functions like maximum , minimum , summation, and etc. However, these functions cannot satisfy all GNN applications. For example, several old friends living in different addresses want to find a place to meet together. They list a set of candi-date meeting places and hope to choose a  X  X ood X  X  place such that they can use similar time to arrive at the meeting place and do not want any of them to wait too long time. Such problem has not been efficiently solved so far. In this paper, we present a novel approach to process the problem. We define the above query problem as a difference nearest neighbors (denoted DNN for short) query. The general description DNN are defined as follows. DNN query. Given two query points q 1 and q 2 , a set of objects O and a threshold a , is the distance between query q and object o i ( o i  X  O ). The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 proposes a pruning approach based on hyperbola for DNN queries. In Section 4, we use asymptote to further improve hyperbola-based approach. Section 5 experimentally evaluates our proposed approaches. Finally, Section 6 concludes the paper. In the procedure of monitoring moving objects, NN queries have been intensively re-objects to a query q to under a specific query condition. The existing algorithms mainly adopt GRID [4,13,14,12] or its variations (e.g. CPM-CNN [14]) as index structure. 
In YPK-CNN algorithm [15], objects are assumed to fit in main memory and are indexed with a regular grid of cells with size  X   X   X  . YPK-CNN does not process up-dates as moving objects arrive, but directly applies their updates on the grid. Each continuous NN (CNN) query installed in the system is re-evaluated in every T time unit. When a query q is evaluated for the first time, a two-step NN search technique retrieves its result. The initial step keeps visiting the cells in a square R until k objects are found. Fig. 1(a) shows an example of a single NN query where the first candidate NN is p 1 and the distance between p 1 and q is d . p 1 may not be an answer to the NN query, since there may exist another object (e.g., p 2 ) outside R , such that the distance between p 2 and q is less than d . To avoid miss such objects, the second step searches contains q , with side length 2 d+  X  . In this way, the actual NN answers to q can be got. actual NN answer. When re-evaluating an existing query q , YPK-CNN makes use of approach needs to compute the new NN answers. 
SEA-CNN [13] focuses on monitoring the NN changes exclusively without including initial result is available). p 2 shown in Fig. 1(b) is the 1-NN object. Objects are stored in secondary memory, indexed using a regular grid. The answer region of a query q is NN object ( best_dist ) is the radius. When object moves, SEA-CNN considers the mov-ing status to determine a circular search region SR (shaded area in Fig. 1(b)) around q , then SEA-CNN sets r = best_dist + dist ( q , q X  ), and computes the newest NN objects of q by processing all the objects in the circle centered at q X  with radius r . 
Similar to the previous CNN query algorithms [15,13,16,17], CPM-CNN algorithm [14] uses the GRID structure to index objects. But CPM divides the search space helically centered at c q as shown in Fig. 1(c). In particular, CPM initializes an empty de-heaping entries iteratively and adjusts its answer set best_NN accordingly. The algorithm terminates when the next entry in the heap H (corresponding either to a cell or a rectangle) has a key that is greater than or equal to the current shorted distance to stripped region. When object moves (e.g. some objects in NN list moving to outside), it first searches the stripped region to get new NN results. 
Aiming at the DNN aggregate query, those CNN query approaches cannot effi-ciently answer the query. In this paper, we make use of the curvilinear geometry prop-erty to prune the query space, and reduce the vast computation. In order to simplify the discussion, we assume data objects and queries are in 2D space. Objects change their locations frequently as unpredictable manners. In this section, we present a basic hyperbola-based algorithm using grid index to answer DNN queries. Without loss of generality, we use two query points and we store objects in disk. 
Similar to existing approaches (e.g., YPK-CNN, SEACNN), we use a grid index, since a more complicated data structure (e.g., main memory R-tree [18]) is expensive to maintain dynamically. We update the top-k list in terms of the given interval  X  peri-odically, i.e., in the specific time interval, we could ensure the correctness of the re-sult of the difference query, once interval  X  is out of date, the algorithm re-computes the current top-k . Notations in this paper are shown Table 1. 3.1 Overview of the Hyperbola-Based Approach Property of Hyperbola. Fig. 2 shows a hyperbola. Let C be the hyperbola, q 1 and q 2 dist( p , q 1 ) and dist( p , q 2 ) is constant and equal to 2 a ( a is shown in Fig. 2). 
We make use of the property of hyperbola to solve DNN. We set the two query focus c of the hyperbola, set the given value in the aggregate operation as the constant a in the hyperbola. In this way one branch of the hyperbola could be contained (In terms of the specific definition of the difference aggregate operation in our algorithm, we could only consider one branch of the hyperbola). The cells comprised by this branch of hyperbola will contain vast suitable query objects with high possibility in terms of the property of the hyperbola. Therefore, we could directly delete the search region outside and non-intersect with the hyperbola. Then we only search the remain-ing space and could spare a lot of computation workload and CPU response time. 
For example, in Fig. 2, there are the dataset P = { p 1 , p 2 , p 3 }, and two query points Then, we can get a hyperbola formula using the threshold a defined by users. Accord-ing to the hyperbola property, we compute the results among the objects and query points using Equation 1. cells of dark region as candidate objects. Hence, we make used of the hyperbola to reduce the search space and improve the performance of DNN queries. 
For clearly using the hyperbola property, in Section 3.2, we introduce grid-based index structure and how to efficiently maintain the index. We give a hyperbola-based algorithm to introduce the whole query processing in detail. 3.2 Cell-Based Index Construct and Update 
The process of object index X  X  construction includes two steps: (i) scanning all of express the results of DNN queries. The algorithm of object index construction is shown in Algorithm 1. 
When the system surpasses the interval, the objects may be moved to another posi-coming into the cells. This must incur the update of object index. In the case of new cell X  X  PL ( i, j ), and insert it into the new cell X  X  PL ( i, j ). In the case that the new posi-tion still remains in the previous cell, no update is arisen. The update algorithm of the object index is shown in Algorithm 2. 3.3 Hyperbola-Based DNN Algorithm hyperbola to prune the unsuitable objects. The remaining objects are sorted according to the DNN query pruning, the number of remaining objects in this region is lower than k increases the complexity of our algorithm. However, our major concern is a large and wide environment with a great deal of mobile objects. Therefore, every cell contains a certain number of objects, and the region Rcrit contains objects farther than k . 
Furthermore, we have to maintain the objects X  index structure using Algorithm 2 and make use of the objects information and the original DNN results, find the object moving farthest, calculate the newest distance between this object and the query point q and q 2 and update this new result as the search region Rcrit . Finally, sort the new DNN in the Rcrit . Algorithm 3 is the implementation given in detail. Based on the basic algorithm presented above, we could get the search region Rcrit . Then we make use of the particularity of the aggregate algorithm and the relevant geometry knowledge to simplify our query algorithm. 
Though the definition and property of hyper bola could prune the search space, this property itself is related to quadratic operation with quite high computation cost. Therefore, we introduce the asymptote of hyperbola to prune the search space. This can both reduce the search space and reduce the computation workload. We use the obtained distance between two foci as the focus c of the hyperbola, and set the given origin. Therefore, we get the hyperbola equation: 1 is x
As shown in Fig. 4, the cells in the starboard of the hyperbola asymptote (including region as shown in Fig. 4). Then find the DNN query results from the cells remained after the pruning strategy. The cells intersected with the asymptotes shouldn X  X  be omitted being tested whether they could satisfy the DNN. Compared with the non-pruning strategy algorithm, our adapted algorithm is more excellent both in computa-tion workload and the query response time. 
Especially, without loss of generality, suppose there are lots of objects in the left of the asymptote as shown in Fig. 4. Furthermore, we could expand the special condi-tions to more common conditions, and adap t the above hyperbola into the common equation, which is shown in Equation 2. 
The asymptote equation could be deduced through getting the derivative from the original hyperbola, we will not dwell on it at this paper. The Hyperbola-based pruned algorithm is shown in Algorithm 4. In this section, we make use of Algorithms 1 and 2 to maintain the index structure of our object data. Then we run Algorithms 3 and 4 based on the same datasets from Algorithms 1 and 2. We test how the different factors affect the performance in the aspects of search response time and the number of accessed objects. 
We mainly test two factors: (i) the number of query results, (ii) the size of the cells in the search region. The experience considers the degree of the two parameters influ-(i) search response time, and (ii) the number of accessed objects. We evaluate our performance based on two kinds of data distribution conditions: uniform distribution; and zipf distribution. The datasets were all generated using our program randomly. All the experiments are performed on a notebook computer with Intel Pentium IV CPU 2.0G, 256MB RAM. The operation system is Windows XP, and all the algo-rithms are implemented in Microsoft Visual C++ 6.0 and compiled by gcc 3.2. 5.1 Performance Influenced by k First we consider the effect from the number of query results. Fig. 5 shows the distribution conditions. In Fig. 5, we find that the search response time in both of the algorithms present the trend of ascending no matter what distribution of the datasets. But our adapted algorithm has introduced the pruning strategy based on the property of hyperbola, the performance of the experiment is much super. Especially in the zipf distribution of datasets, the advantage of our adapted algorithm is excellent evidently. That is, the pruning strategy based on the hyperbola has deleted a great deal of data. So Algorithm 4 (ADNN) is much better than Algorithm 3 (DNN) in the aspect of search response time. 
We could conclude the search response time is insensitive to k . Although we intro-duce the gradual enlarging strategy of the search region, the network simulated in our all these data into memory and sort ascending in order to maintain DNN. Apparently the value k has little effect on the response time. 
Fig. 6 shows the number of accessed objects influenced by k in different data dis-tribution. As the increasing of k , we must access more objects in order to find much results. So along with the change of k , the number of accessed objects and the search response time have the same trend. The number of accessed objects is also steady. As we have introduced the pruning strategy into Algorithm 4 (ADNN), it accesses much less objects than Algorithm 3 (DNN). 5.2 Performance Influenced by Cell X  X  Size We have assumed the size of search region is [0,1) 2 , and set the number of query time interval, the total number of the objects in the network is steady. Then cells with different size contain different number of objects. The bigger cells have lots of objects, the index structure of objects need less attention for maintaining their indices X  efficiency. But in the pruning period based on the property of the hyperbolic asymp-tote, our algorithm must contain much data, which could be deleted in the environ-ment with small size cells, the unnecessary waste in the time and space become unavoidable. Correspondingly the maintaining of objects X  index structure in the envi-ronment with small cells is complex a little, but when come to the pruning session, the pruning function is much stronger, and the search region could be much accurate. 
Fig. 7 shows the condition of the search response time influenced by cell X  X  size in different data distributions. Search response time have a trend of decrease along with the algorithm will incur a I/O operation because the objects in every cells are indexed in the corresponding file in disk. The smaller cell contain less objects, every time we cells needs much more I/O operations than lager cells network. Since the cost of I/O operation is relative high than the computation in memory, the response time de-creases along with the enlargement of cells in the network. Since Algorithm 4 (ADNN) adopts the pruning strategy and deletes lot of cells impossible to maintain query results. The high cost I/O operations could be decreased a lot. Algorithm 4 (ADNN) induces less I/O disk operations than Algorithm 3 (DNN), so the superiority of Algorithm 4 is much obvious. 
Apparently, our adapted Algorithm 4 (ADNN) is much more excellent than algorithm 3 (DNN) at both the search res ponse time and the number of accessed ob-jects. It proves that the pruning strategy introduced in our algorithm is very efficient in practice. We bring forward a basic aggregate query algorithm, and adapt the basic algorithm into a algorithm with the pruning strategy based on the property of hyperbolic asymp-tote. From the evaluation of our experiments, the adapted algorithm with the pruning strategy has improved a lot in the aspects of two parameters: (i) search response time and (ii) the number of accessed objects. Esp ecially in the skewed data distribution conditions, the adapted algorithm is much excellent. 
