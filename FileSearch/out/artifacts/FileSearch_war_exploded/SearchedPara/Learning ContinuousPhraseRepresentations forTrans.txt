 The phrase translation model, also known as the phrase table , is one of the core compo nent s of phrase -based statistical machine translation (SMT) system s . The most common method of construct-ing the phrase table takes a two -phase approach (Koehn et al. 2003) . First, the bilingual phrase pairs are extracted heuristically from an aut omat-ically word -aligned training data. The second phase , which is the focus of this paper, is parame-ter estimation where e ach phrase pair is assigned with some scores that are estimated based on counting these phrases or their words using the same word -ali gned training data.
 state -of -the -art performance largely due to the fact that long phrases , rather than single words, are used as translation units so that useful context in-formation can be captured in selecting translations . However, longer phrases occur less often in tra i n-ing data, leading to a severe data sparseness prob-lem in parameter estimation . There has been a plethora of research reported in the literature on improving parame ter estimation for the phrase translation model (e.g., De N ero et al. 2006; Wuebker et al. 2010; He and Deng 2012; Gao and He 201 3 ). phrase translation pair by developing a Continu-ous -space P hrase T ranslation M odel ( C PTM) . The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag -of -words vector, called word vec-tor henceforth . We then project the word vector , in either the source language or the target lan-guage , into a respective continuous feature vector in a common low -dimensional space that is lan-guage independent. T he projection is performed by a multi -layer neural network . T he projected feature vector forms the continuous representa-tion of a phrase . Finally, the translation score of a source -target phrase pair is computed by the dis-tance between their feature vectors. alleviate the data sparseness problem associated with the traditional counting -based methods by grouping phrases with a similar meaning a cross different languages . This style of grouping is made possible because of the distributed nature of the continuous -space representations for phrases. No such sharing was possible in the original sym-bolic space for representing words or phrases. In this model, semantically or grammatically related phrases , in both the source and the target lan-guage s , would tend to have similar (close) feature vectors in t he continuous space , guided by the training objective. S ince the translation score is a smooth function of these feature vectors , a small change in the features should only lead to a small change in the translation score .
 C PTM is learn ing the continuous representation of a phrase that is effective for SMT . Motivated by recent studies on continuous -space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011 ; Schwenk et al., 2012 ), we use a neural net-work to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good trans-lation s from bad ones , for a given source phrase . However, there is no training data with explicit annotation on the quality of phrase translation s . T he phrase translation pair s are hidden in the par-allel source -target sentence pairs , which are used to train the traditional translation model s . T he quality of a phrase translation can only be judged implicitly through the translation quality of the sentence s , as measured by BLEU , which contain the phrase pair. In order to overcome this chal-lenge and let the BLEU metric guide the projec-tion learning , we propo se a new method to learn the parameters of a neural network . This new method , via the choice of an appropriate objective function in training, automatically forces the fea-ture vector of a source phrase to be closer to the feature vectors of its candidate translations . As a result, the BLEU score is improved when these translations are selected by an SMT decoder to produce final , sentence -level translations . The new learning method makes use of the L -BFGS algorithm and the expected BLEU as the objective function defined on N -best lists .
 posed in this paper is the first continuous -space phrase translation model that makes use of joint representations of a phrase in the source language and its translat ion in the target language (to be de-tailed in Section 4) and that is shown to lead to significant improvement over a standard phrase -based SMT system (to be detailed in Section 6). the translation score of each bilingual phrase pair is modeled explicitly in our model. However, in-stead of estimating the phrase translation score on aligned parallel data , our model intends to capture the grammatical and semantic similarity between a source phrase and its paired target phrase by pro-jecting them into a common, continuous space that is language independent. Section 2 reviews previous work. Sec tion 3 re-views the log -linear model for phrase -based SMT and Sections 4 presents the C PTM . Section 5 de-scribe s the way the model parameters are esti-mated , followed by the experimental re sults in Section 6 . Finally, Section 7 concludes the paper. Representations of words or documents as contin-uous vectors have a long history. Most of the ear-lier latent semantic models for learning such vec-tors are designed for information retrieval (Deerwester et al. 1990 ; Hofmann 1999 ; Blei et al. 2003) . In c ontrast, recent work on continuous space language models , which estimate the prob-ability of a word sequence in a continuous space (Bengio et al. 2003 ; Mikolov et al. 2010) , have ad-vanced the state of the art in language modeling, outperform ing the traditio nal n -gram model on speech recognition (Mikolov et al. 2012; Sunder-meyer et al. 2013 ) and machine translation (Mikolov 201 2 ; Auli et al. 2013 ).
 lingual settings, word embedding from these mod-els is not directly applicable to translation. As a result, variants of such models for cross -lingual scenarios have been proposed so that words in dif-ferent languages are projected into the shared la-tent vector space (Dumais et al. 1997 ; Platt et al. 2010 ; Vinokourov et al. 2002 ; Yih et al. 2011 ; Gao et al. 2011 ; Huang et al. 2013 ; Zou et al. 2013 ) . In principle, a phrase table can be derived using any of these cross -lingual models, although decoupling the derivation from the SMT training often re sults in suboptimal performan ce (e.g., measured in BLEU) , as we will show in Section 6 . continuous -space models for translation. T he most related to this study is the work of continu-ous space n -gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed -forward neural network language model is extended to represent translation probabilities. However, th ese earlier studies focus ed on the n -gram translation models , where the translation probability of a p hrase or a sentence is decom-posed as a product of n -gram probabilities as in a standard n -gram language model. Therefore, it is not clear how their approaches can be applied to widely used in modern SMT systems. In contrast , our model learns jointly the representations of a phrase in the source language as well as its trans-lat ion in the target language . The recurrent contin-uous translation model s proposed by Kalchbren-ner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n -gram translation models above , they make no Markov assumptions about the dependenc y of the words in the target sentence. Continuous space models have also been used for generating trans-lation s for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013).
 the phrase table in phrase -based SMT ( Marcu and Wong 2002; Lamber and Banchs 2005 ; Denero et al. 2006 ; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, ( Gao and He 2013) is most relevant to the work described in this paper . They estimate phrase t ranslation probabilities using a discrimi-native training method under the N -best reranking fra mework of SMT. In this study we use the same objective function to learn the continuous repre-sentations of phrases, integrating the strengths as-sociated with these earlier studies. Phrase -based SMT is based on a log -linear mode l which requires learning a mapping between input  X   X   X  to output  X   X   X  . We are given  X  Training samples (  X   X  A procedure GEN to generate a list of N -best  X  A vector of features  X   X   X   X  that maps each  X  A parameter vector  X   X   X   X  , which assigns a T he components GEN ( . ) ,  X  and  X  define a log -linear model that maps  X   X  to an output sentence as follows: which states that given  X  and  X  , argmax returns the highest scoring translation  X   X  , maximizing over correspondences  X  . I n phrase -based SMT,  X  consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since c omputing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding  X  , call ed the Viterbi derivation , generated by (1). The architecture of the C PTM is shown in Figures 1 and 2 , where for each pair of source and target phrases (  X   X  ,  X   X  ) in a source -target sentence pair , we first project them into feature vectors  X   X   X  work with one hidden layer (as shown in Figure 2) , and then compute the translation score , score (  X   X  ,  X   X  ) , by the distance of thei r feature vec-tors in that space .
 a phrase  X   X   X   X  , where  X  is a word vector and  X  is the size of the vocabulary consisting of words in both source and target languages , which is set to 200K in our experiments . We then learn t o pro-ject  X  to a low -dimensional continuous space  X   X  : The projection is performed using a fully con-nected neural network with one hidden layer and tanh activation functions . L et  X  1 be the projec-tion matrix from the input layer to the hidden layer and  X  2 the projection matrix from the hidden layer to the output layer, we have  X   X   X  (  X  ) = tanh (  X  2 T ( tanh (  X  1 T  X  ) ) ) (2 )
Figure 2. A neural network model for phrases giving rise to their continuous representations. 
The model with the same form is used for both source and target languages.
 The translation score of a source phrase f and a target phrase e can be measured as the similarity ( or distance) between their feature vectors. We A ccording to (2), we see that the value of the scor-ing function is determined by the projection ma-trices  X  = {  X  1 ,  X  2 } .
 into the lo g -linear model for SMT (1) by introducing a new feature  X   X  + 1 and a new feature weight  X   X  + 1 . The new feature is defined as Thus, the phrase -based SMT system , in to which the C PTM is incorporated , is parameterized by (  X  ,  X  ) , where  X  is a vector of a handful of param-eters used in the log -linear model of (1), with one weight for each feature ; and  X  is the projection matrices used in the C PTM defined by (2) and (3). In our experiments we take three steps to learn (  X  ,  X  ) : In the next section , we will describe Step 2 in de-tail as it is directly related to the C PTM training. sentation is shown in Figure 2.  X   X  This section describes the loss function we em-ploy with the C PTM and the algorithm to train the neural network weights .
 tive of the N -best list based expected BLEU, de-noted by xBleu (  X  ) . In the reranking framework of SMT outlined in Section 3, xBleu (  X  ) over one training sample (  X   X  ,  X   X  ) is defined as xBleu (  X  ) =  X   X  (  X  |  X   X  ) sBleu (  X   X  ,  X  )  X   X  GEN (  X  where sBleu (  X   X  ,  X  ) is the sentence -level BLEU score, and  X  (  X  |  X   X  ) is the translation probability from  X   X  to  X  computed using softmax as where  X  T  X  is the log -linear model of (1), which also includes the feature derived from the C PTM as defined by (4), and  X  is a tuned smoothing fac-tor.
 tiable w.r.t. the parameters of the C PTM ,  X  . We can compute the gradient of the loss and learn  X  using gradient -based numerical optimization al-gorithms , such as L -BFGS or stochastic gradient descent (SGD) . 5.1 Computing the Gradient Since the loss does not explicitly depend on  X  , we use the chain rule for differentiation:  X   X  (  X  ) which takes the form of summation over all phrase pairs occur ring either in a training sample (sto-chastic mode) or in the entire training data (batch mode).  X  (  X  ,  X  ) in ( 7 ) is known as the error term of the phrase pair (  X  ,  X  ) , and is defined as It describes how the overall loss changes with the translation score of the phrase pair (  X  ,  X  ) . We will leave the derivation of  X  (  X  ,  X  ) to Section 5 . 1. 2 , and will first describe how the gradient of si m  X  (  X   X  ,  X   X  ) w.r.t.  X  is computed . Without loss of generality, we use the following notations to describe a neural network :  X   X   X   X  is the input word vector of a phrase ;  X   X   X  is the sum vector of the l -th layer ; and  X   X   X  =  X  (  X   X  ) is the output vector of the l -th Thus, the C PTM defined by ( 2 ) and ( 3 ) can be re p-resented as T he gradient of the matrix  X  2 which project s the hidden vector to the output vector is computed as :  X  si m  X  (  X   X  ,  X   X  ) where  X  is the element -wise multiplication (Hada-mard product). Applying the back propagation principle , the gradient of the projection matrix mapping the input vector to the hidden vector  X  1 is computed as  X  si m  X  (  X   X  ,  X   X  ) =  X   X  (  X  2 (  X   X  2  X   X   X  (  X   X  2 ) )  X   X   X  (  X   X  1 ) ) The derivation can be easily extended to a neural network with multiple hidden layers. To simplify the notation, we rewrite our loss func-tion of (5) and (6) over one training sample as  X  (  X  ) =  X  xBleu (  X  ) =  X  where
G (  X  ) =  X  sBleu (  X  ,  X   X  ) exp (  X  T  X  (  X   X  ,  X  ,  X  ) ) Z (  X  ) =  X  exp (  X  T  X  (  X   X  ,  X  ,  X  ) )  X  Combining ( 8 ) and (11), we have Because  X  is only relevant to  X   X  + 1 which is de-fined in (4), we have  X   X  T  X  (  X   X  ,  X  ,  X  )  X  si m  X  (  X   X  ,  X   X  ) where  X  (  X  ,  X  ;  X  ) is the number of times the phrase pair (  X  ,  X  ) occur s in  X  . Combining (12) and (13), we end up with the following equation =  X  U (  X  ,  X  )  X  (  X  |  X   X  )  X   X  + 1  X  (  X  ,  X  ;  X  ) where (14)
U (  X  ,  X  ) = sBleu (  X   X  ,  X  )  X  xBleu (  X  ) . 5.2 The Training Algorithm In our experiments we train the parameters of the C PTM ,  X  , using the L -BFGS optimizer described in Andrew and Gao ( 2007) , together with the loss function described in (5) . The gradient is com-puted as described in Sections 5.1 . Although SGD has been advocated for neural network training due to its simplicity and its robustness to local minima (Bengio 2009), we f i nd that in our task that the L -BFGS minimizes the loss in a desirable fashion empirically when iterating over the com-pl ete training data (batch mode) . For example, the convergence of the algorithm was found to be smooth, despite the non -convexity in our loss. An-other merit of batch training is that the gradient over all training data can be computed efficiently. As shown i n Section 5.1, computing  X  si m  X  ( x  X  , x  X  ) /  X   X  requires large -scale matrix multiplications, and is expensive for multi -layer neural networks. Eq . (7) suggests that  X  si m  X  ( x  X  , x  X  ) /  X   X  and  X  (  X  ,  X  ) can be computed separately, thus making the computation cost of the former term only depend s on the number of phrase pairs in the phrase table, but not the size of training data. Therefore, the training method de-scribed here can be used on larger amounts of training data with little difficulty.
 to learn the parameters for both the log -linear model of SMT and the CPTM . W hile steps 1 and 3 can be easily parallelized on a computer cluster, the CPTM training is performed on a single ma-chine. For example, given a phrase table contain-ing 16M pairs and a 1M -sentence training set , it t a k es a couple of hours to generate the N -best lists on a cluster, and about 10 hours to train the CPTM on a Xeon E5 -2670 2.60GHz machine. is important. In our experiments we always initial-ize  X  1 using a bilingual topic model trained on parallel data ( see detail in Section 6.2), and  X  2 as an identity matrix. In principle, the loss function of ( 5 ) can be further regularized (e.g. b y adding a term of  X  2 norm) to deal with overfitting. How-ever, we did not find clear empirical advantage over the simpler early stop approach in a pilot study, which is adopted in the experiments in this paper. This section evaluates the CPT M presented on two translation tasks using WMT data sets. We first describe the data sets and baseline setup. Then we present experiments where we compare different versions of the CPTM and previous models. 6.1 Experimental Setup Baseline. We experiment with an in -house phrase -based system similar to Moses (Koehn et al. 2007), where the translation candidates are scored by a set of common features including maximum likelihood estimates of source given target phrase mappings  X   X  X  X  X  (  X  |  X  ) and vice versa  X   X  X  X  X  (  X  |  X  ) , as well as lexical weighting estimates  X   X  X  X  (  X  |  X  ) and  X   X  X  X  (  X  |  X  ) , word and phrase penal-ties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a stand-ard 5 -gram modified Kneser -Ney lang uage model trained on the target side of the parallel corpora described below. Log -linear weights are estimated with the MERT algorithm (Och 2003). Evaluation. We test our models on two different data sets. First, we train a n English to French sys-tem based on the data of WMT 2006 shared task (Koehn and Monz 2006). T he parallel corpus in-cludes 688K sentence pairs of parliamentary pro-ceedings for training. The development set con-tains 2000 sentences , and the test set contains ot her 2000 sentences, all from the official WMT 20 06 shared task.
 lish system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign. The majority of the trainin g data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sen-tences, as the development set. We evaluate on four newswire domain test sets from 2008 , 2010 and 2011 as well as the 2010 system combination test set , containing 2034 to 3003 sentence s . comparison using the WMT 2006 data set , and verify our best models and results using the larger WMT 2012 data set.
 tive BLEU score (Papineni et al. 2002) . We also perform a significance test using the Wilcoxon signed rank test . Differences are considered statis-tically significant when the p -value is less than 0.05. 6.2 Results of the CPTM Table 1 shows the results measured in BLEU eval-uated on the WMT 2006 data set , where Row 1 is the baseline system. Rows 2 to 4 are the systems enhanced by integrating different versions of the C PTM . Rows 5 to 7 present the results of previous models. Row 8 is our best system. Table 2 shows the main results on the WMT 2012 data set.
 As illustrated in Figure 2, t he number of the nodes in the input layer is the vocabulary size  X  . Both the hidden layer and the output layer have 100 nodes 6 . That is,  X  1 is a  X   X  100 matrix and  X  2 a 100  X  100 matrix. The result shows that C PTM leads to a substantial improvement over the baseline system with a statistically significant margin of 1.0 BLEU points as in Table 1 .
 to investigate two design choices we made in de-velopin g the C PTM: (1) whether to use a linear projection or a multi -layer nonlinear projection; and (2) whether to compute the phrase similarity using word -word similarities as suggested by e.g., the lexical weighting model (Koehn et al. 2003). We compare these variants on the WMT 2006 data set, as shown in Table 1.
 network to project a word vector of a phrase  X  to a feature vector  X  :  X   X   X  (  X  ) =  X  T  X  , where  X  is a  X   X  100 projection matrix. The translation score of a source phrase f and a target phrase e is measured as the similarity of their feature vectors. We choose cosine similarity because it works bet-ter than dot product for linear projection.
 C P T M W (Row 4 in Table 1) computes the phrase similarity using word -word similarity scores. This follows the common smoothing strategy of ad-dressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model (Koehn et al. 2003) and the word factored n -gram translation model (Son et al. 2012). Let  X  denote a word, and  X  and  X  the source and target phrases, respectively. We define where sim  X  (  X  ,  X  ) (or sim  X  (  X  ,  X  ) ) is the word -phrase similarity, and is defined as a smooth ap-proximation of the maximum function # Systems WMT test2006 1 Baseline 33.06 Table 1: BLEU results for the English to French task using translation models and systems built on the WMT 2006 data set. The superscripts  X  and  X  indicate statistically significant difference ( p &lt; 0.05) from Baseline and CPTM , respec-tively. where sim  X  (  X  ,  X  ) (or sim  X  (  X  ,  X  ) ) is the word -phrase similarity, and is defined as a smooth ap-proximation of the maximum function where  X  is the tuned smoothing parameter.
 ear projection to map each word (not a phrase vec-tor as in C PTM ) to a feature vector.
 C PTM in Row 2 to its variants in Table 1 . First of all, it is more effective to model the phrase trans-lation directly than decomposing it into word -word translations in the CPTMs . Second, we see that the nonlinear projection is able to generate more effective features, leading to better results than the linear projection.
 We also compare the best version of the CPTM i.e., CPTM , with three related models proposed previously. We start the discussion with the re-sults on the WMT 2006 data set in Table 1 .
 art latent semantic models that are originally trained on clicked que ry -document pairs (i.e., clickthrough data extracted from search logs) for query -document match ing (Gao et al. 2011). To adopt these models for SMT , we view source -tar-get sentence pairs as clicked query -document pairs, and trained both models using the same methods as in Gao et al. ( 2011) on the parallel bi-lingual training data described earlier. Specifi-cally, BTLM PR is an extension to PLSA, and is t he best performer among different versions of the B i -L ingual T opic Model (B LTM ) described in Gao et al. ( 2011). BLTM with Posterior Regular-ization ( BLTM PR ) is trained on parallel training data using the EM algorithm with a constraint en-forcing a source sen tence and its paralleled target sentence to not only share the same prior topic dis-tribution, but to also have similar fractions of words assigned to each topic. We incorporate d the model into the log -linear model for SMT (1) as follows. F irst of all, the topic distribution of a source sentence  X   X  , denoted by  X  (  X  |  X   X  ) , is in-duced from the learned topic -word distributions using EM. Then, each translation candidate  X  in the N -best list GEN (  X   X  ) is scored as  X  (  X   X  |  X  ) can be similarly computed. Finally, the logarithms of the two probabilities are incorpo-rated into the log -linear model of ( 1 ) as two addi-tional features. D PM is the Discriminative Projec-tion Model described in Gao et al. ( 2011 ) , which is an extension of LSA . DPM uses a matrix to pro-ject a word vector of a sentence to a feature vector. The projection matrix is learned on parallel train-ing data using the S2Net algorithm (Yih et al. 2011) . DPM can be incorporated into the log -lin-ear model for SM T (1) by introducing a new fea-ture  X   X  + 1 for each phrase pair, which is defined as the cosine similarity of the phrases in the pro-ject space. models, although leading to some slight improve-ment over B aseline , are much less effective than C PTM .
 kov Random Field model using phrase features ( MRF P in Table s 1 and 2 ) , proposed by Gao and 2012 datasets. MRF p i s a state -of -the -art large scale discriminative training model that uses the same expected BLEU training criterion , which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013 , Gao and He 2013 ) . simply treats each p hrase pair as a single feature. Therefore, al though both are trained using the # Systems dev news2011 n ews2010 news2008 newssyscomb2010 1 Baseline 23.58 25.24 24.35 20.36 24.14 0.05) from Baseline and MRF p , respectively. same expected BLEU based objective function, CPTM and MRF p model the translation relation-ship between two phrases from different angles. MRF p estimate s one translation score for each phrase pair explicitly without parameter sharing , while in CPTM , all phrases share the same neural network that projects raw phrase s to the continu-ous space, provid ing a more smoothed estimation of the translation score for each phrase pair. outperforms MRF P on most of the test sets across the two WMT data sets , but the difference be-tween them is often not significant. Our interpre-tation is that al though CPTM provides a better smoothed estimation for l ow -frequent phrase pairs , which otherwise suffer the data sparsity is-sue , MRF p provide s a more precise estimation for th ose high -frequent phrase pairs . That is, CPTM and MRF p capture complementary information for translation . W e thus combine CPTM and MRF P ( Comb in Tables 1 and 2 ) by incorporating two features, each for one model, into the log -lin-ear model of SMT (1) . We observe that for both translation tasks , accuracy improves by up to 0. 8 BLEU over MRF P alone (e. g. , on the news2008 test set in Table 2 ) . The results confirm that CPTM captures complementary translation infor-mation to MRF p . Overall, we improve accuracy by up to 1.3 BLEU over the baseline on both WMT data sets . The work presented in t his paper makes two major contributions. First, we develop a novel phrase translation model for SMT, where joint represen-tations are exploited of a phrase in the source lan-guage and of its translation in the target language , and where the translation score of the pair of source -target phrases are represented as the dis-tance between their feature vectors in a low -di-mensional, continuous space. The space is derived from the representations generated using a multi -layer neural network. Second, we present a new learning method to train the weights in the multi -layer neural network for the end -to -end BLEU metric directly. The training method is based on L -BFG S . W e describe in detail how the gradient in closed form, as required for efficient optimiza-tion, is derived. The objective function, which takes the form of the expected BLEU computed from N -best lists, is very different from the usual objective functions use d in most existing architec-tures of neural networks , e.g., cross entropy (Hin-ton et al. 2012) or mean square error (Deng et al. 2012). We hence have provided details in the der-ivation of the gradient, which can serve as an ex-ample to guide the derivation of neural network learning with other non -standard objective func-tions in the future.
 t hat incorporating the continuous -space phrase translation model into the log -linear framework significantly improves the accuracy of a state -of -the -art phrase -based SMT system, leading to a gain up to 1. 3 BLEU . Careful implementation of the L -BFGS optimiza tion based on the BLEU -centric objective function , together with the asso-ciated closed -form gradient , is a key to the suc-cess. the model and learning algorithm from shallow to deep neural networks . The deep mo dels are ex-pected to produce more powerful and flexible se-mantic representations (e.g., Tur et al., 2012) , and thus greater performance gain than what is pre-sented in this paper. We thank Michael Auli for providing a dataset and for helpful discussions. We also thank the four anonymous reviewers for their comments.
 Andrew, G. and Gao, J. 2007. Scalable training Auli, M., Galley, M., Quirk, C. a nd Zweig, G. Bengio, Y. 2009. Learning deep architectures for Bengio, Y., Duharme, R., Vincent, P., and Janvin, Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Collobert, R., Weston, J., Bottou, L., Karlen, M., Deerwester, S., Dumais, S. T., Furnas, G. W., DeNero, J., Gillick, D., Zhang, J., and Klein, D. 2006. Why generative phrase models underper-form surface heuristics. In Workshop on Statis-tical Machine Translation , pp. 31 -38.
 Deng, L., Yu, D., and Platt, J. 2012. Scalable Diamantaras, K. I., and Kung, S. Y. 1996. Princi-Dumais S., Letsche T., Littman M. and Landauer 
T. 1997. Automatic cross -language retrieval us-ing latent semantic indexing. In AAAI -97 Spring Symposium Series: Cross -Language Text and Speech Retrieval.
 Ganchev, K., Graca, J., Gillenwater, J., and Gao, J., and He, X. 2013. Training MRF -based Gao, J., Toutanova, K., Yih., W -T. 2011. Click-He, X., and Deng, L. 2012. Maximum expected bleu training of phrase and lexicon translation models. In ACL , pp. 292 -301.
 Hinton, G., and Salakhutdinov, R., 2010. Discov-Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, Hofmann, T. 1999. Probabilistic latent semantic indexing. In SIGIR , pp. 50 -57.
 Huang, P -S ., He, X., Gao, J., Deng, L., Acero, A. and Heck, L. 2013. Learning deep structured se-mantic models for web search using click-through data. In CIKM . Kalchbrenner, N. and Blunsom, P. 2013. Recur-rent continuous translation models. In EMNLP . Koehn, P., Hoang, H., Birch, A., Callison -Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., 
Constantin, A., and Herbst , E. 2007. Moses: open source toolkit for statistical machine trans-lation. In ACL 2007 , demonstra tion session. Koehn, P. and Monz, C. 2006. Manual and auto-matic evaluation of machine translation be-tween European languages. In Workshop on Statistical Machine Translation , pp. 102 -121. Koehn, P., Och, F., and Marcu, D. 2003. Statisti-cal phrase -based tran slation. In HLT -NAACL , pp. 127 -133.
 Lambert, P. and Banchs, R. E. 2005. Data inferred multi -word expressions for statistical machine translation. In MT Summit X , Phuket, Thailand. Li, P., Liu, Y., and Sun, M. 2013. Recursive auto-encoders for ITG -based tran slation. In EMNLP . Liang,P., Bouchard -Cote,A. , Klein, D. and 
Taskar, B. 2006. An end -to -end discriminative approach to machine translation. In COLING -ACL .
 Marcu, D., and Wong, W. 2002. A phrase -based, joint probability model for statistical machine transla tion. In EMNLP .
 Mikolov, T., Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural network based language model. In INTER-SPEECH , pp. 1045 -1048.
 Mikolov, T., Kombrink, S., Burget, L., Cernocky, 
J., and Khudanpur, S. 2011. Extensio ns of re-current neural network language model. In ICASSP , pp. 5528 -5531.
 Mikolov, T. 2012. Statistical Language Model based on Neural Networks. Ph.D. thesis , Brno University of Technology.
 Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. 
Exploiting similar ities among languages for machine translation. CoRR. 2013; abs/1309.4148.
 Mikolov, T., Yih, W. and Zweig, G. 2013 b . Lin-guistic Regularities in Continuous Space Word Representations. In NAACL -HLT .
 Mimno, D., Wallach, H., Naradowsky, J., Smith, 
D. and McCallum, A. 2009. Polylingual topic models. In EMNLP . Niehues J., Herrmann, T., Vogel, S., and Waibel, 
A. 2011. Wider context by using bilingual lan-guage models in machine translation.
 Och, F. 2003. Mi nimum error rate training in sta-tistical machine translation. In ACL , pp. 160 -167. Och, F., and Ney, H. 2004. The alignment tem-plate approach to statistical machine translation. Computational Linguistics , 29(1): 19 -51.
 Papine n i , K., Roukos, S., Ward, T., and Zhu W -J. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL .
 Platt, J., Toutanova, K., and Yih, W. 2010. Translingual Document Representations from Discriminative Projections. In EMNLP .
 Rosti, A -V., Hang, B., Matsoukas, S., and 
Schwartz, R. S. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Workshop on Sta-tistical Machine Translation .
 Schwenk, H., Costa -Jussa, M. R. and Fonollosa, J. 
A. R. 2007. Smooth bilingual n -gram transla-tion. In EMNLP -CoNLL , pp. 430 -438.
 Schwenk, H. 2012. Continuous space translation models for phrase -based statistical machine translation. In COLING .
 Schwenk, H., Rousseau, A., and Mohammed A. 2012. Large, pruned or continuous space lan-guage models on a GPU for statistical machine translation . In NAACL -HLT Workshop on the future of language modeling for HLT , pp. 11 -19.
 Setiawan, H. and Zhou, B., 2013. Discriminative training of 150 million translation parameters and its application to pruning. In NAACL .
 Socher, R., Huval, B., Manning, C., Ng, A., 2012. Semantic Compositionality through Recursive Matrix -Vector Spaces. In EMNLP .
 Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 2011. Parsing natural scenes and natural lan-guage with recursive neural networks. In ICML . Son, L. H., Allauzen, A., and Yvon, F. 2012. Con-tinuous space translation models with neural networks. In N AACL -HLT , pp. 29 -48.
 Sundermeyer, M., Oparin, I., Gauvain, J -L. Freiberg, B., Schluter, R. and Ney , H . 2013. 
Comparison of f eed forward and r ecurrent n eu-ral n etwork l anguage m odels. In ICASSP , pp. 8430  X  8434.
 Tur, G, Deng, L., Hakkani -Tur, D., and He, X., 2012. Towards deeper understanding: deep con-vex networks for semantic utterance classifica-tion . I n ICASSP .
 Vinokourov,A., Shawe -Taylor,J. and Cristia-nini,N. 2002. Inferring a semantic representa-tion of text via cross -language correlation anal-ysis. In NIPS .
 Weston, J., Bengio, S., and Usunier, N. 2011. 
Large scale image annotation: learning to rank with joint word -image embeddings. In IJCAI . Wuebker, J., Mauser, A., and Ney, H. 2010. Train-ing phrase translation models with leaving -one -out. In ACL , pp. 4 75 -484.
 Yih, W., Toutanova, K., Platt, J., and Meek, C. 2011. Learning discriminative projections for text similarity measures. In CoNLL .
 Zhang, Y., Deng, L., He, X., and Acero, A. 2011. 
A novel decision function and the associated de-cision -feedback learning for speech translation . In ICASSP .
 Zhila, A., Yih, W., Meek, C., Zweig, G. and 
Mikolov, T. 2013. Combining h eterogeneous m odels for m easuring r elational s imilarity. In NAACL -HLT .
 Zou, W. Y., Socher, R., Cer, D., and Manning, C. 
D. 2013. Bilingual word embeddings for phrase -based machine translation. In EMNLP .
