 In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). Driven by annotation resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), many systems developed in these studies have achieved argument F large-scale evaluations such as the one reported by Carreras and M ` arquez (2005).

More recently, the automatic identification of nominal argument structure has received increased attention due to the release of the NomBank cor-pus (Meyers, 2007a). NomBank annotates predicat-ing nouns in the same way that PropBank annotates predicating verbs. Consider the following example of the verbal predicate distribute from the PropBank corpus: (1) Freeport-McMoRan Energy Partners will be The NomBank corpus contains a similar instance of the deverbal nominalization distribution : (2) Searle will give [ This instance demonstrates the annotation of split ar-guments (Arg1) and modifying adjuncts (Location), which are also annotated in PropBank. In cases where a nominal has a verbal counterpart, the inter-pretation of argument positions Arg0-Arg5 is con-sistent between the two corpora.

In addition to deverbal (i.e., event-based) nomi-nalizations, NomBank annotates a wide variety of nouns that are not derived from verbs and do not de-note events. An example is given below of the parti-tive noun percent : (3) Hallwood owns about 11 [ In this case, the noun phrase headed by the predicate % (i.e.,  X  X bout 11% of Integra X ) denotes a fractional part of the argument in position Arg1.

Since NomBank X  X  release, a number of studies have applied verbal SRL techniques to the task of nominal SRL. For example, Liu and Ng (2007) re-ported an argument F result is encouraging, it does not take into account nominals that surface without overt arguments. Con-sider the following example: (4) The [ As in (2), distribution in (4) has a noun phrase and multiple prepositional phrases in its environment, but not one of these constituents is an argument to distribution in (4); rather, any arguments are implic-itly supplied by the surrounding discourse. As de-scribed by Meyers (2007a), instances such as (2) are called  X  X arkable X  because they contain overt argu-ments, and instances such as (4) are called  X  X nmark-able X  because they do not. In the NomBank corpus, only markable instances have been annotated. Previous evaluations (e.g., those by Jiang and Ng (2006) and Liu and Ng (2007)) have been based on markable instances, which constitute 57% of all instances of nominals from the NomBank lexicon. In order to use nominal SRL systems for down-stream processing, it is important to develop and evaluate techniques that can handle markable as well as unmarkable nominal instances. To address this issue, we investigate the role of implicit argumenta-tion for nominal SRL. This is, in part, inspired by the recent CoNLL Shared Task (Surdeanu et al., 2008), which was the first evaluation of syntactic and se-mantic dependency parsing to include unmarkable nominals. In this paper, we extend this task to con-stituent parsing with techniques and evaluations that focus specifically on implicit argumentation in nom-inals.

We first present our NomBank SRL system, which improves the best reported argument F in the markable-only evaluation from 0.7283 to 0.7630 using a single-stage classification approach. We show that this system, when applied to all nomi-nal instances, achieves an argument F 0.6895, a loss of more than 9%. We then present a model of implicit argumentation that reduces this loss by 46%, resulting in an F the more complete evaluation task. In our analyses, we find that SRL performance varies widely among specific classes of nominals, suggesting interesting directions for future work. Nominal SRL is related to nominal relation interpre-tation as evaluated in SemEval (Girju et al., 2007). Both tasks identify semantic relations between a head noun and other constituents; however, the tasks focus on different relations. Nominal SRL focuses primarily on relations that hold between nominaliza-tions and their arguments, whereas the SemEval task focuses on a range of semantic relations, many of which are not applicable to nominal argument struc-ture.

Early work in identifying the argument struc-ture of deverbal nominalizations was primarily rule-based, using rule sets to associate syntactic con-stituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). La-pata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object de-note the grammatical position of the modifier when linked to a verb.

FrameNet and NomBank have facilitated machine learning approaches to nominal argument struc-ture. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both ver-bal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothe-sis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F tively. Both studies also investigated the use of fea-tures specific to the task of NomBank SRL, but ob-served only marginal performance gains.

NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntactic dependencies, verbal and nominal predicates, and semantic dependencies (i.e., argu-ments) for the predicates. For nominals, the best se-mantic F however this score is not directly comparable to the NomBank SRL results of Liu and Ng (2007) or the results in this paper due to a focus on different as-pects of the problem (see the end of section 5.2 for details). Given a nominal predicate, an SRL system attempts to assign surrounding spans of text to one of 23 classes representing core arguments, adjunct argu-ments, and the null or non-argument. Similarly to verbal SRL, this task is traditionally formulated as a two-stage classification problem over nodes in the syntactic parse tree of the sentence containing the assigned a binary label indicating whether or not it is an argument. In the second stage, argument nodes are assigned one of the 22 non-null argument types. Spans of text subsumed by labeled parse tree nodes constitute arguments of the predication. 3.1 An improved NomBank SRL baseline To investigate the effects of implicit argumenta-tion, we first developed a system based on previ-ous markable-only approaches. Our system follows many of the traditions above, but differs in the fol-lowing ways. First, we replace the standard two-stage pipeline with a single-stage logistic regression we model incorporated arguments (i.e., predicates that are also arguments) with a simple maximum likelihood model that predicts the most likely argu-ment label for a predicate based on counts from the training data. Third, we use the following heuris-tics to resolve argument conflicts: (1) If two argu-ments overlap, the one with the higher probability is kept. (2) If two non-overlapping arguments are of the same type, the one with the higher probability is kept unless the two nodes are siblings, in which case both are kept. Heuristic (2) accounts for split argument constructions.

Our NomBank SRL system uses features that are selected with a greedy forward search strategy sim-ilar to the one used by Jiang and Ng (2006). The top half of Table 2 (next page) lists the selected ar-sections 2-21 of NomBank, used section 24 for de-velopment and section 23 for testing. All parse trees were generated by Charniak X  X  re-ranking syn-tactic parser (Charniak and Johnson, 2005). Follow-ing the evaluation methodology used by Jiang and Ng (2006) and Liu and Ng (2007), we obtained sig-3.2 The effect of implicit nominal arguments The presence of implicit nominal arguments presents challenges that are not taken into account by the evaluation described above. To assess the im-pact of implicit arguments, we evaluated our Nom-Bank SRL system over each token in the testing section. The system attempts argument identifica-tion for all singular and plural nouns that have at least one annotated instance in the training portion of the NomBank corpus (morphological variations included).

Table 3 gives a comparison of the results from the markable-only and all-token evaluations. As can be seen, assuming that all known nouns take overt argu-ments results in a significant performance loss. This loss is due primarily to a drop in precision caused by false positive argument predictions made for nomi-nals with implicit arguments. A natural solution to the problem described above is to first distinguish nominals that bear overt arguments from those that do not. We treat this as a binary classification task over token nodes. Once a nominal has been identified as bearing overt arguments, it is processed with the argument identification model developed in the previous section. To classify nominals, we use the features shown in the bottom half of Table 2, which were selected with the same algorithm used for the argument classification model. As shown by Table 2, the sets of features selected for argument and nominal classification are quite different, and many of the features used for nominal classification have not been previously used. Below, we briefly explain a few of these features.
 Ancestor subcategorization frames (ASF) As shown in Table 2, the most informative feature is ASF. For a given token t , ASF is actually a set of sub-features, one for each parse tree node above t . Each sub-feature is indexed (i.e., named) by its distance from t . The value of an ASF sub-feature is the production rule that expands the correspond-ing node in the tree. An ASF feature with two sub-features is depicted below for the token  X  X ale X : Parse tree path lexicalization A lexicalized parse tree path is one in which surface tokens from the beginning or end of the path are included in the path. This is a finer-grained version of the traditional parse tree path that captures the joint behavior of the path and the tokens it connects. For example, in the tree above, the path from  X  X ale X  to  X  X ade X  with a lexicalized source and destination would be sale : N  X  NP  X  VP  X  V : made . Lexicalization increases sparsity; however, it is often preferred by the feature selection algorithm, as shown in the bottom half of Table 2.
 PropBank markability score This feature is the probability that the context (  X  5 words) of a de-verbal nominal is generated by a unigram language model trained over the PropBank argument words for the corresponding verb. Entities are normalized to their entity type using BBN X  X  IdentiFinder, and adverbs are normalized to their related adjective us-ing the ADJADV dictionary provided by NomBank. The normalization of adverbs is motivated by the fact that adverbial modifiers of verbs typically have a corresponding adjectival modifier for deverbal nominals. Our evaluation methodology reflects a practical sce-nario in which the nominal SRL system must pro-cess each token in a sentence. The system can-not safely assume that each token bears overt argu-ments; rather, this decision must be made automat-ically. In section 5.1, we present results for the au-tomatic identification of nominals with overt argu-ments. Then, in section 5.2, we present results for the combined task in which nominal classification is followed by argument identification. 5.1 Nominal classification Following standard practice, we train the nomi-nal classifier over NomBank sections 2-21 using LibLinear and automatically generated syntactic parse trees. The prediction threshold is set to the value that maximizes the nominal F development section (24), and the resulting model is tested over section 23. For comparison, we implemented the following simple classifiers. Baseline nominal classifier Classifies a token as overtly bearing arguments if it is a singular or plural noun that is markable in the training data. As shown in Table 4, this classifier achieves nearly MLE nominal classifier Operates similarly to the baseline classifier, but also produces a score for the classification. The value of the score is equal to the probability that the nominal bears overt arguments, as observed in the training data. A prediction threshold is imposed on this score as determined by the development data ( t =0 . 23 ). As shown by Table 4, this exchanges recall for precision and leads to a significant increase in the overall F
The last row in Table 4 shows the results for the LibLinear nominal classifier, which significantly outperforms the others, achieving balanced preci-sion and recall scores near 0.9. In addition, it is able to recover from part-of-speech errors because it does not filter out non-noun instances; rather, it combines part-of-speech information with other lex-ical and syntactic features to classify nominals.
Interesting observations can be made by grouping nominals according to the probability with which they are markable in the corpus. Figure 1a gives the overall distribution of markable nominals in the training data. As shown, 50% of nominal instances are markable only 65% of the time or less, making nominal classification an important first step. Using this view of the data, Figure 1b presents the over-all F fication diminish as nominals become more overtly associated with arguments. Furthermore, nominals that are rarely markable (i.e., those in interval 0.05) remain problematic due to a lack of positive training instances and the unbalanced nature of the classifi-cation task. 5.2 Combined nominal-argument classification We now turn to the task of combined nominal-argument classification. In this task, systems must first identify nominals that bear overt arguments. We evaluated three configurations based on the nominal classifiers from the previous section. Each config-uration uses the argument classification model from section 3.

As shown in Table 3, overall argument classifi-cation F assumption that all known nouns bear overt argu-ments. This corresponds precisely to using the base-line nominal classifier in the combined nominal-argument task. The MLE nominal classifier is able to reduce this loss by 25% to an F LibLinear nominal classifier reduces this loss by 46%, resulting in an overall argument classification F of filtering out nominal instances that do not bear overt arguments.

Similarly to the nominal evaluation, we can view argument classification performance with respect to the probability that a nominal bears overt arguments. This is shown in Figure 1c for the three configura-tions. The configuration using the MLE nominal classifier obtains an argument F inals below its prediction threshold. Compared to the baseline nominal classifier, the LibLinear clas-sifier achieves argument classification gains as large as 150.94% (interval 0.05), with an average gain of 52.87% for intervals 0.05 to 0.4. As with nomi-nal classification, argument classification gains di-minish for nominals that express arguments more overtly -we observe an average gain of only 2.15% for intervals 0.45 to 1.00. One possible explana-tion for this is that the argument prediction model has substantially more training data for the nomi-nals in intervals 0.45 to 1.00. Thus, even if the nom-inal classifier makes a false positive prediction in the 0.45 to 1.00 interval range, the argument model may correctly avoid labeling any arguments.

As noted in section 2, these results are not di-rectly comparable to the results of the recent CoNLL Shared Task (Surdeanu et al., 2008). This is due to the fact that the semantic labeled F Task combines predicate and argument predictions into a single score. The same combined F our best two-stage nominal SRL system (logistic re-gression nominal and argument models) is 0.7806; however, this result is not precisely comparable be-cause we do not identify the predicate role set as re-quired by the CoNLL Shared Task. 5.3 NomLex-based analysis of results As demonstrated in section 1, NomBank annotates many classes of deverbal and non-deverbal nomi-nals, which have been categorized on syntactic and semantic bases in NomLex-PLUS (Meyers, 2007b). To help understand what types of nominals are par-ticularly affected by implicit argumentation, we fur-ther analyzed performance with respect to these classes.

Figure 2a shows the distribution of nominals across classes defined by the NomLex resource. As shown in Figure 2b, many of the most frequent classes exhibit significant gains. For example, the classification of partitive nominals (13% of all nom-inal instances) with the LibLinear classifier results in gains of 55.45% and 33.72% over the baseline and MLE classifiers, respectively. For the 5 most common classes, which constitute 82% of all nomi-nals instances, we observe average gains of 27.47% and 19.30% over the baseline and MLE classifiers, respectively.

Table 5 separates nominal and argument classifi-cation results into sets of deverbal (NomLex class nom ), deverbal-like (NomLex class nom-like ), and all other nominalizations. A deverbal-like nominal is closely related to some verb, although not mor-phologically. For example, the noun accolade shares argument interpretation with award , but the two are not morphologically related. As shown by Table 5, nominal classification tends to be easier -and ar-gument classification harder -for deverbals when compared to other types of nominals. The differ-ence in argument F nominals and the others is due primarily to relational nominals, which are relatively easy to classify (Fig-ure 2b); additionally, relational nominals exhibit a high rate of argument incorporation, which is eas-ily handled by the maximum-likelihood model de-scribed in section 3.1. The application of nominal SRL to practical NLP problems requires a system that is able to accurately process each token it encounters. Previously, it was unclear whether the models proposed by Jiang and Ng (2006) and Liu and Ng (2007) would operate ef-fectively in such an environment. The systems de-scribed by Surdeanu et al. (2008) are designed with this environment in mind, but their evaluation did not focus on the issue of implicit argumentation. These two problems motivate the work presented in this paper.

Our contribution is three-fold. First, we improve upon previous nominal SRL results using a single-stage classifier with additional new features. Sec-ond, we show that this model suffers a substantial performance degradation when evaluated over nom-inals with implicit arguments. Finally, we identify a set of features -many of them new -that can be used to reliably detect nominals with explicit arguments, thus significantly increasing the performance of the nominal SRL system.

Our results also suggest interesting directions for future work. As described in section 5.2, many nom-inals do not have enough labeled training data to produce accurate argument models. The general-ization procedures developed by Gordon and Swan-son (2007) for PropBank SRL and Pad  X  o et al. (2008) for NomBank SRL might alleviate this problem. Additionally, instead of ignoring nominals with im-plicit arguments, we would prefer to identify the im-plicit arguments using information contained in the surrounding discourse. Such inferences would help connect entities and events across sentences, provid-ing a fuller interpretation of the text.
 The authors would like to thank the anonymous re-viewers for their helpful suggestions. The first two authors were supported by NSF grants IIS-0535112 and IIS-0347548, and the third author was supported by NSF grant IIS-0534700.
