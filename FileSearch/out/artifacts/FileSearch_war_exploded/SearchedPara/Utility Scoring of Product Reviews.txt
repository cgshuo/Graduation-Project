 We identify a new task in the ongoing research in text senti-ment analysis: predicting utility of product reviews, which is orthogonal to polarity classification and opinion extrac-tion. We build regression models by incorporating a diverse set of features, and achieve highly competitive performance for utility scoring on three real-world data sets. H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Miscellaneous; I.2.7 [ ARTIFICIAL INTELLIGENCE ]: Natural Language Processing X  Language parsing and un-derstanding ; I.2.6 [ ARTIFICIAL INTELLIGENCE ]: Learn-ing X  Concept learning, Induction Algorithms, Experimentation text sentiment analysis, utility, regression
Recently, there has been a swell of interest in text subjec-tivity and sentiment analysis. In particular, product review data have been heavily used for subjectivity classification, polarity prediction, and opinion extraction, due to their lin-guistic properties and their implications for E-commerce ap-plications.

Online shoppers often wade through other people X  X  re-views of a certain product to gauge their shopping decision; manufacturers may also examine product reviews to moni-tor customer opinions and predict market trends. In both scenarios, the review readers seek (relatively) unbiased eval-uation of a given product, by leveraging information from multiple reviews, although each individual review can be subjective in nature.
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. or They certainly encode strong polarity and reflect strong opinion of their authors. However, they are not particu-larly reliable or useful in informing their readers X  shopping decisions.

When presented with a mixed bag of positive and nega-tive,usefulandnot-so-usefulreviews,howshouldonelever-age the diverse evidence? We envision the following  X  X eighted average X  framework, which should be considered as a moti-vating thought at a very general level: in which the overall evaluation E ( P ) of a product P is a weighted average of the polarity of each individual review T i ( P ).

Equation (1) implies two orthogonal characteristics of prod-uct reviews. While much previous research has been focus-ing on predicting the the polarity of text, Polarity ( T i ( P )), we try to approach the the utility of reviews, u ( T i ( P )), which is a new and important research problem. Polarity and utility of reviews are orthogonal only in the sense that they can be modeled independently; they are certainly rele-vant and to be integrated in a framework like Equation (1). The paper is organized as follows:
After reviewing related work in text subjectivity and po-larity analysis, we formally define utility prediction as a re-gression problem. We present and analyze experimental re-sults on three Amazon customer review collections before concluding the paper with remarks on future work.
Subjectivity in natural language refers to aspects of lan-guage used to express opinions, evaluations, and specula-tions. [19] provided a good overview of work in learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collo-cations, and adjectives and verbs identified using distribu-tional similarity. The features are also examined working together in concert. In addition, the authors showed that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective. Finally, the subjectivity clues are used to perform opinion piece recognition (a type of text categorization and genre de-tection). In a somewhat different perspective, [21] presented experimental results in classifying the strength of opinions in text. A wide range of features were used, including new syntactic features developed for opinion recognition.
Yu and Hatzivassiloglou [23] approached the problem of separating opinions from fact, at both the document and sentence level. They presented a Bayesian classifier for dis-criminating between documents with a preponderance of opinions such as editorials from regular (fact-based) news stories, and described three unsupervised statistical tech-niques for the task of detecting opinions at the sentence level. A model for sentence-level polarity classification was also discussed.
 positive or negative. Using the IMDB movie review data 2 , the authors showed that standard machine learning tech-niques outperform human-produced baselines. However, the three machine learning methods employed (Naive Bayes, maximum entropy classification, and support vector ma-chines) do not perform as well on sentiment classification as on traditional topic-based categorization, due to chal-lenges related to non-compositional semantics and discourse structures. [7] studied the same problem with an additional tweak. Subjective portions of text are first extracted using a graph min-cut algorithm, and then fed into text catego-rization algorithms to approach sentiment polarity classifi-cation.

Pushing further along the same line, [8] addressed the rating-inference problem, wherein rather than simply de-cide whether a review is  X  X humbs up X  or  X  X humbs down X , one must determine an author X  X  evaluation with respect to a multi-point scale (e.g., one to five stars). A metric labeling approach was compared with both multi-class and regression versions of SVMs.
 Shifting from classification to extraction, [10] introduced OPINE, an unsupervised information extraction system which mines reviews in order to build a model of important prod-uct features, their evaluation by reviewers, and their relative quality across products.

In a similar effort, [6] proposed a novel framework for ana-lyzing and comparing consumer opinions of competing prod-ucts. A prototype system called Opinion Observer is also im-plemented. Through the visualization offered by the system, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of var-ious product features. Supervised rule discovery techniques are used to extract product features and corresponding Pros and Cons.
Utility (or, reliability, usefulness, informativeness) is not the same as indifference. Totally indifferent or neutral re-views are useless; well-grounded subjective opinions can be convincing and illuminating. In other words, the utility of a product review is a property orthogonal to its polarity or embedded opinions. Our goal in this research is to build a computational model to predict the utility of reviews.
We view the problem as one of regression. Formally, given f
P ( T ) can be computed. Our task is to approximate a func-tion The output u  X  [0 , 1] should reflect the real utility of T as accurately as possible.

Givenanestimatedfunction F , we can use the following metrics to evaluate its quality, both of which are standard in regression analysis: http://reviews.imdb.com/Reviews/
In this section, we discuss two aspects of the statistical learning framework: the learning (regression) algorithms and the features.
We experiment with two types of regression algorithms:
In both cases, we apply the original algorithms as they are implemented in the machine learning packages. It is not our intention to make contribution to the learning algorithms in this study.
Generally speaking, a good product review is a  X  X eason-able X  mixture of subjective valuation and objective informa-tion. The feature space in the statistical learning framework ought to capture this linguistic phenomenon.

Given a product review text T , we compute the following features and feed them into the regression algorithms.
Clearly an informative customer review should not be a literal copy or loyal rephrase of the product specification S , because it should reflect the customer X  X  own experience with the product, not the manufacturer X  X  description or expecta-tion. On the other hand, a good review is supposed to base subjective judgement on objective observation, therefore it should echo the product specification to a reasonable extent, in a positive or negative tone.

Similar situation is conceivable between a customer review andaneditorialreview E , the latter of which approximates a relatively objective and authoritative view of the product.
With these motivations in mind, we measure the simi-larity between customer review and product specification, sim ( T,S ), and that between customer review and editorial review, sim ( T,E ), respectively.

We use the standard cosine similarity in vector space model, with TF*IDF term weighting, as defined in information re-trieval literature [14].
We compute counts of words with the following part-of-speech tags in T , in order to characterize the subjectivity-objectivity mixture of the text at a shallow syntactic level:
The only thing we have not specified so far is how to acquire the target value of the regression model, given a the gold-standard definition of u ( T i ).

Notice that almost every Amazon customer review comes with a vote regarding its usefulness ( X  x out of y people found the following review helpful X ). This actually provides a di-rect and convenient way to approximate the gold-stand util-ity value of a given review. Formally, we define the utility as
In our experiments, we only use the reviews with at least 10 votes (i.e., y&gt; 10), in order to ensure the robustness of the regression model.

Table 2 summarizes the distribution of review utility scores in the four Amazon collections, as well as their total num-ber of distinct reviews after filtering out duplicates and those with no more than 10 votes.

Table 2: Statistics of Data Sets after Treatment
Before we present the regression results, one might intu-itively expect that the utility of a review strongly correlates with its length (in a positive way). However, as we can see from Table 3, the correlation between the two variables is in fact very weak. Therefore it is necessary to build non-trivial regression models.
 Table 3: Correlation between Utility Score and Re-view Length
The regression performance on the three review collec-tions ( Canon , PG-13 ,and Engineering ) are summarized in Table 4, 5, and 6 respectively. All results presented in this section are based on 10-fold cross validation.

In all three tables, the rows represent different combina-tions of features; and the columns correspond to the perfor-mance of SVR and SLR, measured by squared correlation coefficient r 2 and mean squared error  X  2 respectively. The most competitive results are marked by bold fonts.
We have the following observations, based on the experi-mental results. [1] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [2] Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. [3] K. Dave, S. Lawrence, and D. M. Pennock. Mining the [4] V. Hatzivassiloglou and J. M. Wiebe. Effects of [5] D. Lin. Automatic retrieval and clustering of similar [6] B. Liu, M. Hu, and J. Cheng. Opinion observer: [7] B. Pang and L. Lee. A sentimental education: [16] M. Thelen and E. Riloff. A bootstrapping method for [17] P. D. Turney. Thumbs up or thumbs down?: semantic [18] J. Wiebe. Learning subjective adjectives from corpora. [19] J. Wiebe, T. Wilson, R. Bruce, M. Bell, and
