 3 School of Computer Science, With the advent of the Internet, it is now possible to use huge training sets to address challenging tasks in machine learning. As a motivating example, consider the recent work of Torralba et al. who collected a dataset of 80 million images from the Int ernet [2, 3]. They then used this weakly labeled dataset to perform scene categorization. To categori ze a novel image, they simply searched for similar images in the dataset and used the label s of these retrieved images to predict the label of the novel image. A similar approach was used in [4] for scene completion.
 Although conceptually simple, actually carrying out such methods requires highly efficient ways of (1) storing millions of images in memory and (2) quickly finding si milar images to a target image.
 Semantic hashing, introduced by Salakhutdinov and Hinton[5] , is a clever way of a ddressing both of these challenges. In semantic hashing, each item in the database is repres ented by a compact binary code. The code is constructed so that similar items will have similar binary codewords and there is a simple feedforward network that can calculate the binary code for a novel input. Retrieving similar neighbors is then done simply by retrieving all items with codes within a small Hamming distance of the code for the query. This kind of retri eval can be amazingly fast -millions of queries per second on standard computers. The key f or this method to work is to learn a good code for the dataset. We need a code that is (1) ea sily computed for a novel input (2) requires a small number of bits to code the full dataset a nd (3) maps similar items to similar binary codewords.
 To simplify the problem, we will assume that the items have already been embedded in a Euclidean space, say R d , in which Euclidean distance correlates with the desired simi-larity. The problem of finding such a Euclidean embedding has been addressed in a large number of machine learning algorithms (e.g. [6, 7]). In some cases, domain knowledge can be used to define a good embedding. For example, Torralba et al. [3] found that a 512 dimensional descriptor known as the GIST descriptor, gives an embedding where Euclidean distance induces a reasonable similarity function on the items. But simply having E uclidean embedding does not give us a fast retrieval mechanism.
 If we forget about the requirement of having a small number of bits in the codewor ds, then it is easy to design a binary code so that items that are close in Euclidean space wi ll map to similar binary codewords. This is the basis of the popular locality sensiti ve hashing method E2LSH [8]. As shown in[8], if every bit in the code is calculated by a random linear projection followed by a random threshold, then the Hamming distance between codewords will asymptotically approach the Euclidean distance between the items. But in pra ctice this method can lead to very inefficient codes. Figure 1 illustrates the problem on a toy dat aset of points uniformly sampled in a two dimensional rectangle. The figure plots the a verage precision at Hamming distance 1 using a E2LSH encoding. As the number of bits increases the precision improves (and approaches one with many bits), but the rate of conver gence can be very slow.
 Rather than using random projections to define the bits in a code, several authors have pursued machine learning approaches. In [5] the authors used an autoencoder with several hidden layers. The architecture can be thought of as a restricted Boltzmann machine (RBM) in which there are only connections between layers and not within layers. In order to learn 32 bits, the middle layer of the autoencoder has 32 hidden units, and noise was injected during training to encourage these bits to be as binary as possible. This method indeed gives codes that are much more compact than the E2LSH codes. In [9] they used multiple stacked R BMs to learn a non-linear mapping between input vector and code bits. Backpropagation using an Neighborhood Components Analysis (NCA) objective function was used to refine the weights in the network to preserve the neighborhood structure of the input space. Fig ure 1 shows that the RBM gives much better performance compared to random bits. A simpl er machine learning algorithm (Boosting SSC) was pursued in [10] who used adaBoost to classify a pair of input items as similar or nonsimilar. Each weak learner w as a decision stump, and the output of all the weak learners on a given output is a binary code. Figure 1 shows that this boosting procedure also works much better than E2LSH codes, altho ugh slightly worse than the RBMs 1 .
 The success of machine learning approaches over LSH is not limited to synthetic data. In [5], RBMs gave several orders of magnitude improvement over LSH in document retrieval tasks. In [3] both RBMs and Boosting were used to learn binary codes for a database of millions of images and were found to outperform LSH. Also, the retrieval speed using these short binary codes was found to be significantly faster than LSH (which was faster than other methods such as KD trees).
 The success of machine learning methods leads us to ask: what is the best code for perfor m-ing semantic hashing for a given dataset? We formalize the requirements for a good code and show that these are equivalent to a particular form of graph partitioning. This shows that even for a single bit, the problem of finding optimal codes is NP hard. On the other hand, the analogy to graph partitioning suggests a relaxed version of the problem that leads to very efficient eigenvector solutions. These eigenvectors are exactly the eigenvecto rs used in many spectral algorithms including spectral clustering and Laplacian eigenmaps [6, 11]. This leads to a new algorithm, which we call  X  X pectral hashing X  where the bits are calculated by thresholding a subset of eigenvectors of the Laplacian of the similarity gra ph. By utiliz-ing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltr ami eigenfunctions of manifolds, we show how to efficiently calculate the code of a novel data -point. Taken together, both learning the code and applying it to a novel point ar e extremely simple. Our experiments show that our codes outperform the state-of-the art. Figure 1: Building hash codes to find neighbors. Neighbors are defined as pairs of poi nts in 2D whose Euclidean distance is less than . The toy dataset is formed by uniformly sampling points in a two dimensional rectangle. The figure plots the average precision (num ber of neighbors in the original space divided by number of neighbors in a hamming ball using the hash codes) at Hamming distance  X  1 for three methods. The plots on the left show how each method partitions the space to compute the bits to represent each sample. Despite the simplicity of this toy data, the methods still require many bits in order t o get good performance. As mentioned earlier, we seek a code that is (1) easily computed for a novel input (2) r equires a small number of bits to code the full dataset and (3) maps similar items to s imilar binary codewords. Let us first ignore the first requirement, that codewords be easily computed fo r a novel input and search only for a code that is efficient (i.e. requires a small number o f bits) and similarity preserving (i.e. maps similar items to similar codew ords). For a code to be efficient, we require that each bit has a 50% chance of being one or zero, and that different bits are independent of each other. Among all codes that have this property, we will seek the ones where the average Hamming distance between similar points is minimal. be the affinity matrix. Since we are assuming the inputs are embedded in R d so that Thus the parameter defines the distance in R d which corresponds to similar items. Using this notation, the average Hamming distance between similar neighbors can be written: P uncorrelated we obtain the following problem: where the constraint
P Observation: For a single bit, solving problem 1 is equivalent to balanced graph partition-ing and is NP hard. Proof: Consider an undirected graph whose vertices are the datapoints and where the weight between item i and j is given by W ( i, j ). Consider a code with a single bit. The bit partitions the graph into two equal parts ( A, B ), vertices where the bit is on and vertices where the bit is off. For a single bit, by the partition: cut ( A, B ) = cut ( A, B ) with the requirement that | A | = | B | which is known to be NP hard [12]. For k bits the problem can be thought of as trying to find k independent balanced partitions, each of which should have as low cut as possible. 2.1 Spectral Relaxation By introducing a n  X  k matrix Y whose j th row is y T j and a diagonal n  X  n matrix D ( i, i ) = P j W ( i, j ) we can rewrite the problem as: This is of course still a hard problem, but by removing the constraint that Y ( i, j )  X  X  X  1 , 1 } we obtain an easy problem whose solutions are simply the k eigenvectors of D  X  W with minimal eigenvalue (after excluding the trivial eigenvector 1 which has eigenval ue 0). 2.2 Out of Sample Extension The fact that the solution to the relaxed problem are the k eigenvectors of D  X  W with minimal eigenvalue would suggest simply thresholding these eigenvectors to obtai n a binary code. But this would only tell us how to compute the code representation of items in the training set. This is the problem of out-of-sample extension of spectral methods whi ch is often solved using the Nystrom method [13, 14]. But note that the cost of calcul ating the Nystrom extension of a new datapoint is linear in the size of the dataset. In our setting, where there can be millions of items in the dataset this is impractical. In fact , calculating the Nystrom extension is as expensive as doing exhaustive nearest neighbor searc h. In order to enable efficient out-of-sample extension we assume the datapoints x i  X  R d are samples from a probability distribution p ( x ). The equations in the problem 1 are now seen to be sample averages which we replace with their expectations: a spectral problem whose solutions are eigenfunctions of the weighted Laplace-Beltrami operators defined on manifolds [15, 16, 13, 17]. More explicitly, define the weigh ted Lapla-R W ( s, x ) f ( s ) p ( s ) ds with D ( x ) = are functions that satisfy L p f =  X f with minimal eigenvalue (ignoring the trivial solution f ( x ) = 1 which has eigenvalue 0). As discussed in [16, 15, 13], with proper norma lization, the eigenvectors of the discrete Laplacian defined by n points sampled from p ( x ) converges to eigenfunctions of L p as n  X  X  X  .
 What do the eigenfunctions of L p look like ? One important special case is when p ( x ) is a separable distribution. A simple case of a separable distribution is a multidi mensional uniform distribution Pr( x ) = [ a , b i ]. Another example is a multidimensional Gaussian, which is separable once the space has been rotated so that the Gaussian is axes aligned.
 Observation: [17] If p ( x ) is separable, and similarity between datapoints is defined as e product form. That is, if  X  i ( x ) is an eigenfunction of the weighted Laplacian defined on R problem with eigenvalue  X  i  X  j  X  X  X   X  d .
 Specifically for a case of a uniform distribution on [ a, b ] the eigenfunctions of the one-dimensional Laplacian L p are extremely well studied objects in mathematics. They corre-spond to the fundamental modes of vibration of a metallic plate. The eigenfunctions  X  k ( x ) and eigenvalues  X  k are: A similar equation is also available for the one dimensional Gaussian . I n this case the eigenfunctions of the one-dimensional Laplacian L p are (in the limit of small ) solutions to the Schrodinger equations and are related to Hermite polynomials. Figure 2 shows the analytical eigenfunctions for a 2D rectangle in order of increasing eigenvalue. The eig envalue (which corresponds to the cut) determines which k bits will be used. Note that the eigenvalue depends on the aspect ratio of the rectangle and the spatial frequency  X  it is better to cut the long dimension before the short one, and low spatial frequencies are preferred. Not e that the eigenfunctions do not depend on the radius of similar neighbors . The radius does change the eigenvalue but does not affect the ordering.
 We distinguish between single-dimension eigenfunctions, which are of the form  X  k ( x 1 ) or  X  product eigenfunctions are shown marked with a red border in the figure. As we now discuss, these outer-product eigenfunctions should be avoided when building a hashing code. Observation: Suppose we build a code by thresholding the k eigenfunctions of L p with minimal eigenvalue y ( x ) = sign ( X  k ( x )). If any of the eigenfunctions is an outer-product eigenfunction, then that bit is a deterministic function of other bits in the code. This observation highlights the simplification we made in relaxing the independence co n-straint and requiring that the bits be uncorrelated. Indeed the bits corresponding to out er-product eigenfunctions are approximately uncorrelated but they are surely not independent. The exact form of the eigenfunctions for 1D continuous Laplacian for different distr ibutions is a matter of ongoing research [17]. We have found, however, that the bit codes obta ined by thresholding the eigenfunctions are robust to the exact form of the distribution. In par-ticular, simply fitting a multidimensional rectangle distribution to the data (b y using PCA to align the axes, and then assuming a uniform distribution on each axis) wor ks surprisingly well for a wide range of distributions. In particular, using the analytic eigenf unctions of a uniform distribution on data sampled from a Gaussian, works as well as using the numeri-cally calculated eigenvectors and far better than boosting or RBMs trained on t he Gaussian distribution.
 To summarize, given a training set of points { x i } and a desired number of bits k the spectral hashing algorithm works by: Figure 2: Left: Eigenfunctions for a uniform rectangular distribution in 2D. Right: Thresh-olded eigenfunctions. Outer-product eigenfunctions have a red frame. The eigenvalues de-pend on the aspect ratio of the rectangle and the spatial frequency of the cut  X  it is bet ter to cut the long dimension first and lower spatial frequencies are better than higher ones. Figure 3: Comparison of neighborhood defined by hamming balls of different radii us ing codes obtained with LSH, Boosting, RBM and spectral hashing when using 3, 7 and 15 bit s. The yellow dot denotes a test sample. The red points correspond to the locations tha t are within a hamming distance of zero. Green corresponds to a hamming ball of radius 1, a nd blue to radius 2. This simple algorithm has two obvious limitations. First, it assumes a multidimensional uniform distribution generated the data. We have experimented with using multidim ensional Gaussians instead. Second, even though it avoids the trivial 3 way dependencies that arise from outer-product eigenfunctions, other high-order dependencies between the bits may exist. We have experimented with using only frequencies that are powers of two to avoid these dependencies. Neither of these more complicated variants of spectral hashing gave a significant improvement in performance in our experiments.
 Figure 4a compares the performance of spectral hashing to LSH, RBMs and Boosti ng on a 2D rectangle and figure 3 visualizes the Hamming balls for the different methods. Despi te the simplicity of spectral hashing, it outperforms the other methods. Even when we apply RBMs and Boosting to the output of spectral hashing the performance does not improv e. A similar pattern of results is shown in high dimensional synthetic data (figur e 4b). Some insight into the superior performance can be obtained by comparing the part itions that each bit defines on the data (figures 2,1). Recall that we seek partitions that give low cut value and are approximately independent. LSH which uses random linear partitions may give very unbalanced partitions. RBMs and Boosting both find good partit ions, but the partitions can be highly dependent on each other. In addition to the synthetic results we applied the different algorithms to the i mage databases discussed in [3]. Figure 5 shows retrieval results for spectral hashing, RBMs and boosting on the  X  X abelme X  dataset. Note that even though the spectral hashing uses a terrible model of the statistics of the database  X  it simply assumes a N dimensional rectangle, it performs better than boosting which actually uses the distribution (the difference in perfor mance relative to RBMs is not significant). Not only is the performance numericall y better, but Figure 4: left: results on 2D rectangles with different methods. Even though spectral hashing is the simplest, it gives the best performance. right: Similar pattern of results for a 10 dimensional distribution. Figure 5: Performance of different binary codes on the LabelMe dataset described in [ 3]. The data is certainly not uniformly distributed, and yet spectral hashing gives better ret rieval performance than boosting and LSH. our visual inspection of the retrieved neighbors suggests that with a small n umber of bits, the retrieved images are better using spectral hashing than with boosting.
 Figure 6 shows retrieval results on a dataset of 80 million images. This da taset is obviously more challenging and even using exhaustive search some of the retrieved neighbors are se-mantically quite different. Still, the majority of retrieved neighbors seem to be semantically relevant, and with 64 bits spectral hashing enables this peformance in fractions o f a second. We have discussed the problem of learning a code for semantic hashing. We defined a hard criterion for a good code that is related to graph partitioning and used a spectr al relaxation to obtain an eigenvector solution. We used recent results on convergence of graph La placian eigenvectors to obtain analytic solutions for certain distributions and show ed the importance of avoiding redundant bits that arise from separable distributions.
 The final algorithm we arrive at, spectral hashing, is extremely simple -one s imply performs PCA on the data and then fits a multidimensional rectangle. The aspect ratio of this m ul-tidimensional rectangle determines the code using a simple formula. Despite this sim plicity, the method is comparable, if not superior, to state-of-the-art methods. Figure 6: Retrieval results on a dataset of 80 million images using the ori ginal gist descriptor, and hash codes build with spectral hashing with 32 bits and 64 bits. The input image corresponds to the image on the top-left corner, the rest are the 24 nearest neighbor s using hamming distance for the hash codes and L2 for gist.

