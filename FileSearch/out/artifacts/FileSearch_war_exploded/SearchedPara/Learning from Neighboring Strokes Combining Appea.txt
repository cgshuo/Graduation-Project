 Sketches are everywhere. From flow charts to chemical structures to electrical circuits, people use them every day to communicate information across many different domains. They are also be an important part of the early design process, helping us explore rough ideas and solutions in an in-formal environment. However, despite their ubiquity, there is still a large gap between how people naturally interact with sketches and how computers can interpret them today. Current authoring programs like ChemDraw (for chemical structures) and Visio (for general diagrams) still rely on the traditional point-click-drag style of interaction. While popular, they simply do not provide the ease of use, naturalness, or speed of drawing on paper.
 We propose a new framework for sketch recognition that combines a rich representation of low level visual appearance with a probabilistic model for capturing higher level relationships. By  X  X isual ap-pearance X  we mean an image-based representation that preserves the pictoral nature of the ink. By  X  X igher level relationships X  we mean the spatial relationships between different symbols. Our com-bined approach uses a graphical model that classifies each symbol jointly with its context, allowing neighboring interpretations to influence each other. This makes our method less sensitive to noise and drawing variations, significantly improving robustness and accuracy. The result is a recognizer that is better able to handle the range of drawing styles found in messy freehand sketches. Current work in sketch recognition can, very broadly speaking, be separated into two groups. The first group focuses on the relationships between geometric primitives like lines, arcs, and curves, specifying them either manually [1, 4, 5] or learning them from labeled data [16, 20]. Recognition is then posed as a constraint satisfaction problem, as in [4, 5], or as an inference problem on a graphical model, as in [1, 16, 17, 20]. However, in many real-world sketches, it is difficult to extract these primitives reliably. Circles may not always be round, line segments may not be straight, and previously drawn stroke), and stray ink may introduce false primitives that lead to poor recognition. In addition, recognizers that rely on extracted primitives often discard potentially useful information contained in the appearance of the original strokes.
 The second group of related work focuses on the visual appearance of shapes and symbols. These include parts-based methods [9, 18], which learn a set of discriminative parts or patches for each symbol class, and template-based methods [7, 11], which compare the input symbol to a library of learned prototypes. The main advantage of vision-based approaches is their robustness to many of the drawing variations commonly found in real-world sketches, including artifacts like over-tracing and pen drag. However, these methods do not model the spatial relationships between neighboring shapes, relying solely on local appearance to classify a symbol.
 In the following sections we describe our approach, which combines both appearance and context. It is divided into three main stages: (1) stroke preprocessing: we decompose strokes (each stroke is defined as the set of points collected from pen-down to pen-up) into smaller segments, (2) symbol detection: we search for potential symbols (candidates) among groups of segments, and (3) candi-date selection: we select a final set of detections from these candidates, taking into account their spatial relationships. The first step in our recognition framework is to preprocess the sketch into a set of simple segments, as shown in Figure 1(b). The purpose for this step is twofold. First, like superpixels in computer vision [14], segments are much easier to work with than individual points or pixels; the number of points can be large even in moderate-sized sketches, making optimization intractable. Second, in the domains we evaluated, the boundaries between segments effectively preserve the boundaries between symbols. This is not the case when working with the strokes directly, so preprocessing allows us to handle strokes that contain more than one symbol (e.g., when a wire and resistor are drawn together without lifting the pen).
 Our preprocessing algorithm divides strokes into segments by splitting them at their corner points. Previous approaches to corner detection focused primarily on local pen speed and curvature [15], but these measures are not always reliable in messy real-world sketches. Our corner detection algorithm, on the other hand, tries to find the set of vertices that best approximates the original stroke as a whole. It repeatedly discards the vertex v i that contributes the least to the quality of fit measure q , which we define as: where s is the set of points in the original stroke, v is the current set of vertices remaining in the line segment approximation, curvature ( v i ) is a measure of the local stroke curvature 1 , and ( MSE ( v \ v , s )  X  MSE ( v , s )) is the increase in mean squared error caused by removing vertex v i from the approximation.
 Thus, instead of immediately trying to decide which point is a corner, our detector starts by making the simpler decision about which point is not a corner. The process ends when q ( v i ) is greater than a predefined threshold 2 . At the end of the preprocessing stage, the system records the length of the longest segment L (after excluding the top 5% as outliers). This value is used in subsequent stages as a rough estimate for the overall scale of the sketch. Our algorithm searches for symbols among groups of segments. Starting with each segment in isolation, we generate successively larger groups by expanding the group to include the next closest segment 3 . This process ends when either the size of the group exceeds 2 L (a spatial constraint) or Figure 1: Our recognition framework. (a) An example sketch of a circuit diagram and (b) the seg-ments after preprocessing. (c) A subset of the candidate groups extracted from the sketch (only those with an appearance potential &gt; 0.25 are shown). (d) The resulting graphical model: nodes represent segment labels, dark blue edges represent group overlap potentials, and light blue edges represent context potentials. (e) The final set of symbol detections after running loopy belief propagation. when the group spans more strokes than the temporal window specified for the domain 4 . Note that we allow temporal gaps in the detection region, so symbols do not need to be drawn with consecutive strokes. An illustration of this process is shown in Figure 1(c).
 We classify each candidate group using the symbol recognizer we described in [11], which con-verts the on-line stroke sequences into a set of low resolution feature images (see Figure 2(a)). This emphasis on visual appearance makes our method less sensitive to stroke level differences like over-tracing and pen drag, improving accuracy and robustness. Since [11] was designed for classifying isolated shapes and not for detecting symbols in messy sketches, we augment its output with five geometric features and a set of local context features: stroke count : The number of strokes in the group. segment count : The number of segments in the group. diagonal length : The diagonal length of the group X  X  bounding box, normalized by L . group ink density : The total length of the strokes in the group divided by the diagonal length. stroke separation : Maximum distance between any stroke and its nearest neighbor in the group. local context : A set of four feature images that captures the local context around the group. Each The symbol detector uses a linear SVM [13] to classify each candidate group, labeling it as one of the symbols in the domain or as mis-grouped  X  X lutter X . The training data includes both valid symbols and clutter regions. Because the classifier needs to distinguish between more than two classes, we Figure 2: Symbol Detection Features. (a) The set of five 12x12 feature images used by the isolated appearance-based classifier. The first four images encode stroke orientation at 0, 45, 90, and 135 degrees; the fifth captures the locations of stroke endpoints. (b) The set of four local context images for multi-segment symbol. (c) The set of four local context images for single-segment symbols. use the one-vs-one strategy for combining binary classifiers. Also, to generate probability estimates, we fit a logistic regression model to the outputs of the SVM [12].
 Many of the features above are not very useful for groups that contain only one segment. For example, an isolated segment always looks like a straight line, so its visual appearance is not very informative. Thus, we use a different set of features to classify candidates that contain only a single segment: (e.g., wires in circuits and straight bonds in chemistry): orientation : The orientation of the segment, discretized into evenly space bins of size  X / 4 . segment length : The length of the segment, normalized by L . segment count : The total number of segments extracted from the parent stroke. segment ink density : The length of the substroke matching the start and end points of the segment stroke ink density : The length of the parent stroke divided by the diagonal length of the parent local context : Same as the local context for multi-segment symbols, except these images are cen-The final task is to select a set of symbol detections from the competing candidate groups. Our candidate selection algorithm has two main objectives. First, it must avoid selecting candidates that conflict with each other because they share one or more segments. Second, it should select candidates that are consistent with each other based on what the system knows about the likely spatial relationships between symbols.
 We use an undirected graphical model to encode the relationships between competing candidates. Under our formulation, each segment (node) in the sketch needs to be assigned to one of the candi-date groups (labels). Thus, our candidate selection problem becomes a segment labeling problem, where the set of possible labels for a given segment is the set of candidate groups that contain that segment. This allows us to incorporate local appearance, group overlap consistency, and spatial context into a single unified model. Figure 3: Spatial relationships: The three measurements used to calculate the context potential  X  ( c i ,c j ,x i ,x j ) , where v i and v j are vector representing segment x i and x j and v ij is a vector from the center of v i to the center of v j .
 The joint probability function over the entire graph is given by: where x is the set of segments in the sketch, c is the set of segment labels, and Z is a normalizing constant.
 Appearance potential. The appearance potential  X  a measures how well the candidate group X  X  appearance matches that of its predicted class. It uses the output of the isolated symbol classifier in section 4 and is defined as: where P a ( c i | x ) is the likelihood score for candidate c i returned by the isolated symbol classifier. Group overlap potential. The overlap potential  X  o ( c i ,c j ) is a pairwise compatibility that ensures the segment assignments do not conflict with each other. For example, if segments x i and x j are both members of candidate c and x i is assigned to c , then x j must also be assigned to c . To improve efficiency, instead of connecting every pair of segments that are jointly considered in c , we connect the segments into a loop based on temporal ordering. This accomplishes the same constraint with fewer edges. An example is shown in Figure 1(d).
 Joint Context Potential. The context potential  X  c ( c i ,c j ,x i ,x j ) represents the spatial compatibil-ity between segments x i and x j , conditioned on their predicted class labels (e.g., resistor-resistor, resistor-wire, etc). It is encoded as a conditional probability table that counts the number of times each spatial relationship (  X  1 ,  X  2 ,  X  3 ) occurred for a given class pair (see Figure 3). tionships (  X  1 , X  2 , X  3 ) between segments x i and x j . This potential is active only for pairs of segments whose distance at the closest point is less than L/ 2 . To build the probability table we discretize  X  1 and  X  2 into bins of size  X / 8 and  X  3 into bins of size L/ 4 .
 The entries in the conditional probability table are defined as: where N  X , class i , class j is the number of times we observed a pair of segments with spatial relationship  X  and class labels (class i , class j ) and  X  is a weak prior (  X  = 10 in our experiments). Inference. We apply the max-product belief propagation algorithm [22] to find the configuration that maximizes Equation 2. Belief propagation works by iteratively passing messages around the connected nodes in the graph; each message from node i to node j contains i  X  X  belief for each possible state of j . In our implementation we use an  X  X ccelerated X  message passing schedule [21] that propagates messages immediately without waiting for other nodes to finish. The procedure alternates between forward and backward passes through the nodes based on the temporal ordering of the segments, running for a total of 100 iterations. One goal of our research is to build a system that can handle the range of drawings styles found in natural, real world diagrams. As a result, our data collection program was designed to behave like a piece of paper, i.e., capturing the sketch but providing no recognition or feedback. Using the data we collected, we evaluated five versions of our system: Appearance uses only the isolated appearance-based recognizer from [11].
 Appearance+Geometry uses isolated appearance and geometric features.
 Appearance+Geometry+Local uses isolated appearance, geometric features, and local context. Complete is the complete framework described in this paper, using our corner detector. Complete (corner detector from [15]) is the complete framework, using the corner detector in [15]. (We include this comparison to evaluate the effectiveness of our corner detection algorithm.) Note that the first three versions still use the group overlap potential to select the best set of consistent candidates.
 Chemistry For this evaluation we recruited 10 participants who were familiar with organic chemistry and asked each of them to draw 12 real world organic compounds (e.g., Aspirin, Penicillin, Sildenafil, etc) on a Tablet PC. We performed a set of user-independent performance evaluations, testing our system on one user while using the examples from the other 9 users as training data. By leaving out sketches from the same participant, this evaluation demonstrates how well our system would perform on a new user.
 For this domain we noticed that users almost never drew multiple symbols using a single stroke, with the exception of multiple connected straight bonds (e.g., rings). Following this observation, we optimized our candidate extractor to filter out multi-segment candidates that break stroke boundaries. Note that for this dataset we report only accuracy (recall), because, unlike traditional object detec-tion, there are no overlapping detections and every stroke is assigned to a symbol. Thus, a false positive always causes a false negative, so recall and precision are redundant: e.g., misclassifying one segment in a three-segment  X  X  X  makes it impossible to recognize the original  X  X  X  correctly. The results in Table 1 show that our method was able to recognize 97% of the symbols correctly. To be considered a correct recognition, a predicted symbol needs to match both the segmentation and class of the ground truth label. By modeling joint context, the complete framework was able to reduce the error rate by 31% compared to the next best method. Figure 4 (top) shows several sketches interpreted by our system. We can see that the diagrams in this dataset can be very messy, and exhibit a wide range of drawing styles. Notice that in the center diagram, the system made two errors because the author drew hash bonds differently from all the other users, enclosing them inside a triangle.
 Circuits The second dataset is a collection of circuit diagrams collected by Oltmans and Davis [9]. The examples were from 10 users who were experienced in basic circuit design. Each user drew ten or eleven different circuits, and every circuit was required to include a pre-specified set of components. We again performed a set of user-independent performance evaluations. Because the exact locations of the ground truth labels are somewhat subjective (i.e., it is not obvious whether the resistor label should include the short wire segments on either end), we adopt the same evaluation metric used in the Pascal Challenge [2] and in [9]: a prediction is considered correct if the area of overlap between its bounding box and the ground truth label X  X  bounding box is greater than 50% of the area of their union. Also, since we do not count wire detections for this dataset (as in [9]), we report precision as well as recall.
 Table 2 shows that our method was able to recognize over 91% of the circuit symbols correctly. Compared to the next best method, the complete framework was able to reduce the error rate by 30%. On this dataset Oltmans and Davis [9] were able to achieve a best recall of 73.9% at a precision of 25.7%. Compared to their reported results, we reduced the error rate by 66% and more than triple the precision. As Figure 4 (bottom) shows, this is a very complicated and messy corpus with significant drawing variations like overtracing and pen drag.
 Runtime dataset and 0.02 seconds for the chemistry dataset (running on a 3.6 GHz machine, single-thread). With incremental interpretation, the system should be able to easily keep up in real time. Related Work Sketch recognition is a relatively new field, and we did not find any publicly available benchmarks for the domains we evaluated. In this section, we summarize the performance of existing systems that are similar to ours. Alvarado and Davis [1] proposed using dynamically constructed Bayesian networks to represent the contextual relationships between geometric primitives. They achieved an accuracy of 62% on a circuits dataset similar to ours, but needed to manually segment any strokes that contained more than one symbol. Gennari et al [3] developed a system that searches for symbols in high density regions of the sketch and uses domain knowledge to correct low level recognition errors. They reported an accuracy of 77% on a dataset with 6 types of circuit components. Sezgin and Davis [16] proposed using an HMM to model the temporal patterns of geometric primitives, and reported an accuracy of 87% on a dataset containing 4 types of circuit components.
 Shilman et. al. [17] proposed an approach that treats sketch recognition as a visual parsing problem. Our work differs from theirs in that we use a rich model of low-level visual appearance and do not require a pre-defined spatial grammar. Ouyang and Davis [10] developed a sketch recognition system that uses domain knowledge to refine its interpretation. Their work focused on chemical diagrams, and detection was limited to symbols drawn using consecutive strokes. Outside of the sketch recognition community, there is also a great deal of interest in combining appearance and context for problems in computer vision [6, 8, 19]. Figure 4: Examples of chemical diagrams (top) and circuit diagrams (bottom) recognized by our system (complete framework). Correct detections are highlighted in green (teal for hash and wedge bonds), false detections in red, and missed symbols in orange. We have proposed a new framework that combines a rich representation of low level visual appear-ance with a probabilistic model for capturing higher level relationships. To our knowledge this is the first paper to combine these two approaches, and the result is a recognizer that is better able to handle the range of drawing styles found in messy freehand sketches. To preserve the familiar experience of using pen and paper, our system supports the same symbols, notations, and drawing styles that people are already accustomed to.
 In our initial evaluation we apply our method on two real-world domains, chemical diagrams and electrical circuits (with 10 types of components), and achieve accuracy rates of 97% and 91% re-spectively. Compared to existing benchmarks in literature, our method achieved higher accuracy even though the other systems supported fewer symbols [3, 16], trained on data from the same user [3, 16], or required manual pre-segmentation [1].
 Acknowledgements This research was supported in part by a DHS Graduate Research Fellowship and a grant from Pfizer, Inc. We thank Michael Oltmans for kindly making his dataset available to us. [1] C. Alvarado and R. Davis. Sketchread: A multi-domain sketch recognition engine. In Proc. [2] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman. The pascal visual [3] L. Gennari, L. Kara, T. Stahovich, and K. Shimada. Combining geometry and domain knowl-[4] M. Gross. The electronic cocktail napkina computational environment for working with design [5] T. Hammond and R. Davis. Ladder: a language to describe drawing, display, and editing in [6] X. He, R. Zemel, and M. Carreira-Perpinan. Multiscale conditional random fields for image [7] L. Kara and T. Stahovich. An image-based, trainable symbol recognizer for hand-drawn [8] K. Murphy, A. Torralba, and W. Freeman. Using the forest to see the trees: a graphical model [9] M. Oltmans. Envisioning Sketch Recognition: A Local Feature Based Approach to Recognizing [10] T. Y. Ouyang and R. Davis. Recognition of hand drawn chemical diagrams. In Proc. AAAI [11] T. Y. Ouyang and R. Davis. A visual approach to sketched symbol recognition. In Proc. [12] J. Platt. Probabilities for sv machines. Advances in Neural Information Processing Systems , [13] J. Platt. Sequential minimal optimization: A fast algorithm for training support vector ma-[14] X. Ren and J. Malik. Learning a classification model for segmentation. In Proc. IEEE Inter-[15] T. Sezgin and R. Davis. Sketch based interfaces: Early processing for sketch understanding. In [16] T. Sezgin and R. Davis. Sketch recognition in interspersed drawings using time-based graphical [17] M. Shilman, H. Pasula, S. Russell, and R. Newton. Statistical visual language models for ink [18] M. Shilman, P. Viola, and K. Chellapilla. Recognition and grouping of handwritten text in [19] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost: Joint appearance, shape and [20] M. Szummer. Learning diagram parts with hidden random fields. In Proc. International Con-[21] M. Tappen and W. Freeman. Comparison of graph cuts with belief propagation for stereo, [22] J. Yedidia, W. Freeman, and Y. Weiss. Understanding belief propagation and its generaliza-
