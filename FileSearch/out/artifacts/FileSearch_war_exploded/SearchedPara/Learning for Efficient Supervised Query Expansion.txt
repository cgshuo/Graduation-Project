 Query expansion (QE) is a well known technique to im-prove retrieval effectiveness, which expands original queries with extra terms that are predicted to be relevant. A re-cent trend in the literature is Supervised Query Expansion (SQE), where supervised learning is introduced to better se-lect expansion terms. However, an important but neglected issue for SQE is its efficiency, as applying SQE in retrieval can be much more time-consuming than applying Unsuper-vised Query Expansion (UQE) algorithms. In this paper, we point out that the cost of SQE mainly comes from term feature extraction, and propose a Two-stage Feature Selec-tion framework (TFS) to address this problem. The first stage is adaptive expansion decision, which determines if a query is suitable for SQE or not. For unsuitable queries, SQE is skipped and no term features are extracted at all, which reduces the most time cost. For those suitable queries, the second stage is cost constrained feature selection, which chooses a subset of effective yet inexpensive features for su-pervised learning. Extensive experiments on four corpora (including three academic and one industry corpus) show that our TFS framework can substantially reduce the time cost for SQE, while maintaining its effectiveness. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Query Expansion; Supervised Learning; Efficiency
Queries provided by users can sometimes be ambiguous and inaccurate in an information retrieval system, which may generate unsatisfactory results. Query expansion (QE) is a well known technique to address this issue, which ex-pands the original queries with some extra terms that are
Part of this work was done while the first author interned at Microsoft.
 predicted to be relevant [26]. It is hoped that these expanded terms can capture user X  X  true intent that is missed in orig-inal query, thus improving the final retrieval effectiveness. In the past decades, various applications [26, 6] have proved its value.

Unsupervised QE (UQE) algorithms used to be the main-stream in the QE literature. Many famous algorithms, such as relevance model (RM) [21] and thesaurus based methods [29], have been widely applied. However, recent studies [5, 22] showed that a large portion of expansion terms selected by UQE algorithms are noisy or even harmful, which limits their performance. Supervised Query Expansion (SQE) is proposed to overcome this disadvantage by leveraging the power of supervised learning. Most of existing SQE algo-rithms [5, 22, 13, 25, 2] follow a classical machine learning pipeline: (1) utilize UQE to select initial candidate terms; (2) features of candidate terms are extracted; (3) pre-trained classifiers or rankers are utilized to select the best terms for expansion. Significant effectiveness improvement has been reported over their unsupervised counterparts, and SQE has become the new state-of-the-art.

Besides effectiveness, efficiency is another important issue in QE-involved retrieval [35]. As we will show later, UQE algorithms are usually very efficient to apply. Therefore, when UQE is adopted in retrieval, the major inefficiency comes from the second retrieval, which retrieves the entire corpus for the expanded queries. This issue is traditionally handled by indexing or documents optimization [35, 3, 30]. But recently Diaz [11] showed that simply reranking the retrieval results of original queries can already provide nearly identical performance with very low time costs, particularly for precision-oriented metrics.

However, the efficiency issue of SQE algorithms imposes new challenge beyond the UQE case. Compared with UQE algorithms, SQE requires extra time to apply supervised learning, which can incur significant time cost. Moreover, this issue is unique to SQE, and cannot be addressed by previous QE efficiency methods such as indexing optimiza-tion or reranking. Unfortunately, although important, this issue has been largely neglected in the literature.
The above observations motivate us to propose new re-search to address this SQE efficiency problem. In this pa-per, we point out that the major time cost of applying SQE algorithms comes from term feature extraction. Indeed leveraging extensive features can enhance the effectiveness of supervised learning, so that better expansion terms can be selected. However, it also inevitably decreases the effi-ciency. Aiming at this point, we propose a Two-stage Fea-ture Selection framework (TFS) to balance the two conflict-ing goals. The first stage is Adaptive Expansion Decision (AED), which predicts whether a query is suitable for SQE or not. For unsuitable queries, SQE is skipped with no fea-tures being extracted, so that the time cost is reduced most. For suitable queries, the second stage conducts Cost Con-strained Feature Selection (CCFS), which chooses a subset of effective yet inexpensive features for supervised learning. We then instantiate TFS for a RankSVM based SQE al-gorithm. Extensive experiments on four corpora (including three academic and one industry corpus) show that our TFS framework can substantially reduce the time cost of SQE al-gorithm, meanwhile maintaining its effectiveness.
The rest of the paper is organized as follows: Sec. 2 in-troduces the preliminaries of our work, including problem analysis and literature review; Sec. 3 presents the Two-stage Feature Selection framework and its instantiation; Sec. 4 gives all experiments, and in Sec. 5 we conclude this paper.
In this section, we will thoroughly analyze the SQE ef-ficiency problem. Meanwhile we will review the literature, and point out the difference between our work and previous works. The discussions below are presented in three subsec-tions, each covering one specific aspect. First we will review some basics about query epansion.
QE Formulation. Suppose we have a user query q with n terms q = { t q i | i = 1 : n } . Suppose m expansion terms are selected by a QE algorithm, denoted as { t e i | i = 1 : m } . Then the expanded query q e is their union, i.e. q e = { t q } X  X  t Each term t  X  q e is weighted by the interpolated probability where P ( t | q ) is the probability of term t occurring in q (i.e. ity given by QE algorithm, and  X  is the interpolation coeffi-cient to be tuned. As can be seen, the key question here is how to select good expansion terms.

UQE versus SQE. Unsupervised QE (UQE) algorithms used to be the mainstream in QE literature. For exam-ple, some well known UQE algorithms include relevance model (RM) [21], positional relevance model [24], and mix-ture model [36]. UQE algorithms are very popular because on one hand their formulations are in general simple, and on the other hand their empirical performance is quite rea-sonable. However, recent works [5, 22] observed that a large portion of the expansion terms from UQE can be noisy or even harmful, which limits their performance.

SQE tackles this problem by introducing supervised learn-ing to predict the quality of candidate expansion terms. Cao et al. [5] proposed perhaps the first SQE research, where they designed a set of term features and applied SVM for term classification (either good or bad). Later Lee et al. [22] claimed that ranking oriented term selection outper-forms classification oriented methods. Gao et al. [13, 12] applied SQE to web search, where search log is utilized for candidate term generation. Some other extensions include QE robustness [25], query reformulation [2], etc. A common pipeline of SQE training and testing [5, 13] is summarized in Alg. 1. Notice here we only concern test-time efficiency, rather than the training-time efficiency.
 Algorithm 1 SQE Training and Testing Pipeline 1: For testing query q , use UQE to select M candidate terms. 3: Apply H to get top m terms for expansion.
Now we will analyze the efficiency issue when QE is ap-plied in retrieval.

QE in Retrieval. The retrieval process with QE can be described as follows. Let C denote the target corpus upon which we will run and evaluate retrieval; denote S as the re-source from which expansion terms are extracted. In tradi-tional pseudo relevance feedback (PRF) scenario [6], S = C . In more general scenario, S is not necessarily the same as C . For example, in web search, C (e.g. Clueweb09) might be too low-quality to be used for QE [1]; instead some other resources of higher quality can be used as S (e.g. search log [10] or Wikipedia [1]). Assuming a retrieval algorithm (e.g. BM25 or KL divergence) is utilized, then a typical process of QE in retrieval is summarized in Table 1:
Table 1: QE in retrieval with and without reranking.
In the above table we list two possible implementations of second retrieval. The full second retrieval is more tra-ditional, in which the entire target corpus C is retrieved for expanded query q e . This, however, is painfully time-consuming, particularly on large scale corpus. Recently Diaz [11] suggested to rerank the retrieval results of original query q as the results for q e . Diaz pointed out that this reranking implementation can provide nearly identical performance as the full second retrieval, particularly for precision-oriented evaluation metrics. Our preliminary experiments also ver-ified this statement. Therefore throughout this paper, we will utilize reranking as the default implementation for sec-ond retrieval. Notice in the (A) step, we present the dif-ferent implementation details regarding both PRF scenario ( C = S ) and non-PRF scenario ( C 6 = S ).

Existing QE Efficiency Studies. Despite the useful-ness of reranking, the majority of existing works on QE ef-ficiency still focused on how to speed up the full second retrieval. As far as we know, all of these works addressed the problem by optimizing underlying data structures such as indexing or document representation. Billerbeck et al. [3] proposed to use compact document summaries to reduce retrieval time. Lavrenko et al. [20] pre-calculated pairwise document similarities to reduce the amount of calculation when searching expanded queries. Wu et al. [35] utilized a special index structure named impact-sorted indexing that improves the scoring procedures in retrieval. Theobald et al. [30] proposed the idea of merging inverted list of dif-ferent terms in an incremental on-demand manner so that document scan can be delayed as much as possible. Un-fortunately, our goal now is not the second retrieval, which is handled by reranking as [11] does. Nor can the ineffi-ciency challenge of SQE be handled by the above data-level approaches.
Now we will show why the efficiency issue of SQE is a unique and challenging problem beyond the UQE case. Step-wise Time Cost Analysis. First let X  X  see how UQE and SQE differs in the time cost spent on each re-trieval step. On Clueweb09-B corpus, we conduct UQE (RM [21]) and SQE (applying RankSVM [19] based on Cao et al X  X  work [5]) for QE with 20 expansion terms. As comparison, we apply our Two-stage Feature Selection framework (TFS) to SQE. Notice that although UQE does not involve term feature extraction, we can still apply adaptive expansion de-cision to UQE. In Figure 1, we show the time cost of each retrieval step with respect to Table 1. More experiment de-tails can be found in Sec. 4. Here we mainly discuss the observations that motivate our research.

We can observe that, indeed applying SQE model can be much more time-consuming than applying UQE model, which supports our previous statement and validates our motivation. Notice here, step  X  X irstRet X  and  X  X pplyQE X  are involved in expansion term selection, while  X  X ecRet X  in-cludes only retrieving C for original query q and reranking for expanded query q e . Also notice the reranking in second retrieval only incurs very low time cost.

Feature Extraction in SQE. It is then natural to ask, which part of SQE incurs the major time cost, and why?
We argue that term feature extraction is the major ineffi-cient part in SQE models. Recall the testing phase in Alg. 1, there are three steps to apply SQE model. The first step is essentially UQE, which is in general very efficient. The third step, which applies learned SQE model H for term classifi-cation or ranking, is also efficient in practice. For example, many SQE works [5, 22, 13, 2] adopted linear model, which is extremely fast yet effective. Therefore, the second step of term feature extraction forms the majority of inefficiency.
But why would we spend much time on term feature ex-traction? This is because predicting the quality of terms is very challenging, so we want plenty of powerful features to enhance the learning algorithm [5, 13, 22]. In Figure 2, we show the retrieval accuracy (ERR@20) when different time cost is spent on term feature extraction. The purple triangle is UQE which does not extract any term feature (i.e. time cost equals zero); the blue rectangle is the full SQE model with all available term features (defined later). In the middle is our proposed cost constrained feature selection method. Clearly, with more time spent, more term features will be obtained, and the retrieval accuracy is higher as well. How-ever, this inevitably degrades the efficiency.

We can further observe that, with more term features, although the retrieval accuracy of SQE is usually higher, the marginal gain becomes smaller. This motivates us to find a subset of features such that their total cost is low, meanwhile the effectiveness is reasonably maintained. This coincides with the idea of feature selection [15], although most of such methods only concern the number of selected features while ignoring their difference in time cost, which can be suboptimal.
Based on the above analysis, in this section we will present the proposed Two-stage Feature Selection (TFS) framework. Below we will first present TFS as a general framework, then instantiate it for an example SQE algorithm.
Assume the initial full set of term features are F t , where subscript t indicates the features are for terms. As analyzed earlier, when retrieval effectiveness is the only goal, F to become abundant and inefficient. Therefore, the goal of our TFS framework is to select a subset of term features F from F t , so that the effectiveness and efficiency can be opti-mally balanced. As mentioned earlier, the TFS framework includes the following two stages:  X  Adaptive Expansion Decision (AED), which predicts whether a query is suitable for SQE or not. For unsuit-able queries, SQE is skipped with no term features being extracted, which reduces the time cost most. To this end, AED builds a classifier V AED with pre-defined query fea-ture F q , so that V AED ( F q ) &lt; 0 for unsuitable queries and V
AED ( F q ) &gt; 0 for suitable ones.  X  Cost Constrained Feature Selection (CCFS), which se-lects a subset of effective yet inexpensive term features for SQE to apply. For those SQE-suitable queries that pass the first stage, this second stage can further reduce the time cost to some extent. To this end, CCFS builds a feature selector V
CCF S , which requires the time cost u f of each term feature f , and a pre-defined overall time cost upper bound U . In this way, there is F  X  t = V CCF S ( F t ) and P f  X  X   X  Accordingly, the complete retrieval process is shown in Table 2, which is self-evident to interpret. Below we will give more details about the two stages.
Training . The training process of AED classifier V AED is as follows. For training query q , we first retrieve corpus C and record its retrieval accuracy as r q (e.g. ERR@20 value). Then we apply the SQE model H from Alg. 1 to get the expanded query q e . Retrieve C for q e by following the pro-cedures in Table 2, and denote the retrieval accuracy as r Then if r H q &gt; r q , we assign label +1 to query q , which means SQE can help improve the retrieval effectiveness for q ; oth-erwise we assign  X  1 to q . Finally, we extract query features F q for q , and adopt some supervised learning algorithm (e.g. SVM) to get the classifier V AED .

Discussion . The idea of AED stems from query per-formance prediction. It is known that query expansion may hurt the retrieval effectiveness for some queries [25, 7]. There-fore, accurately predicting those queries and avoid applying expensive SQE model to them can substantially improve the efficiency. This idea of adaptive expansion has also been ap-plied in [18, 27, 23, 8], although their works mainly focused on retrieval effectiveness and did not report the efficiency advantage that this method might bring.
Despite the existence of AED, queries that pass AED still face the problem of expensive term feature extraction in SQE. Now we will explain how cost constrained feature se-lection is designed for those SQE-suitable queries.
Algorithm Design . As mentioned, CCFS aims to select a subset of term features F  X  t from the complete feature set F , so that the overall time cost will not exceed a pre-defined upper bound U , i.e. F  X  t = V CCF S ( F t ) with P f  X  X   X  Our CCFS algorithm is formulated as follows. Since the SQE model H is used to predict the quality of expansion terms, we assume X  X  R N  X  K as the feature matrix for N candidate terms, where each row is a K -dimensional feature vector for each term. Denote Y  X  R N  X  1 as the corresponding labels for terms in X , which is calculated as the  X  r q c Alg. 1. Assume the SQE model H is learned via a loss function L H ( X, Y |  X  ), where  X  is the model parameter. We introduce feature selector d  X  X  0 , 1 } K , where the i th element Algorithm 2 Cost Constrained Feature Selection 2: do d i = 1 means the i th feature is selected. With fixed d , those unselected features in X will become invalid, which is equivalent to X  X  Xd . Together  X  and d form a revised learning objective as follows:
The optimization process is shown in Alg. 2, where a co-ordinate descent strategy is adopted to iteratively optimize w.r.t  X  and d . During the iteration, we gradually decrease the cost upper bound (i.e.  X  U ), so that the feature selec-tion process can be smooth. In extreme case where U  X  U 0 Alg. 2 will produce the same results as vanilla H where no selection occurs (i.e. d = 1 K  X  1 ).

Discussion . Feature selection is a hot research topic in machine learning [15], and has been successfully adopted in information retrieval [13, 14, 33]. Popular methods include L1 based regularization [28, 13], cascading structure [31], feature number constraint [34], etc. Theoretically, any fea-ture selection method can reduce time cost. But in practice we find better effectiveness can be achieved if the time cost of each feature can be explicitly considered. Unfortunately, most of previous research did not model such feature cost dif-ference, which can be suboptimal. Wang et al. [33] proposed a greedy search algorithm for time cost aware feature selec-tion in learning to rank. We also compare this method in our experiments, which shows our formulation outperforms this greedy design. CCFS can also be applied to the AED if necessary, which is straightforward. But due to space limi-tation, we simplify our experiments by only applying CCFS to term features.
So far we have elaborated all the details of TFS as a gen-eral framework. Now we will instantiate it with respect to a representative SQE algorithm, to show the implementa-tion details. For SQE, we adopt a RankSVM based SQE method. Linear model has been widely applied in SQE lit-erature [2, 13, 12]. Moreover, a ranking perspective has been proven to be very effective [22, 12] for SQE. Therefore, we believe RankSVM will make a representative example. Also Lavrenko X  X  Relevance Model (RM) is utilized as the UQE model to generate candidate expansion terms, which is ar-guably one of the most successful UQE algorithms. The AED stage is simple to implement. Here we utilize SVM with Gaussian kernel for V AED training. The adopted query features F q are listed in Table 3, which are all well known query performance prediction features in the litera-ture [17, 23, 16, 37].
Applying Alg. 2 in practice requires a deeper insight into t he SQE model itself. Nonetheless, as we show below, Alg. 2 can generate elegant solutions that are easy to implement.
Objective . For training queries q , let x q i  X  R |F t | feature vector for the i th candidate term of q . Here the term features F t are adopted from [5, 13], and are listed in Table 4. Following the notations of u, U, d in Alg. 2, the objective of cost constrained RankSVM is as follows:
Here P i s the set of pairwise candidate terms ( q, i, j ) where  X  r q i &gt;  X  r  X  q,i,j = |  X  r q i  X   X  r q j | as loss weight that emphasizes large relevance difference, which works well in practice. Notice for RankSVM, there X  X  no need to add offset.

Eq. 3 is how Eq. 2 looks like when RankSVM objective [19] is introduced. The first three lines (if without d ) of Eq. 3 constitute vanilla RankSVM (with slight modification of  X  q,i,j ), while the feature selector d and the last line of con-straints formulate the cost constrained version of RankSVM. The outcome d indicates what features are selected under cost upper bound U , and the w is the resulted RankSVM model based on the selected features.

Optimization . We solve Eq. 3 by converting it into the dual form via Lagrange multiplier [4], which gives: where  X  ( q , i, j ) in Eq. 3 is re-indexed by k = 1 : K with each k representing one ( q, i, j ) triplet; y k = 1 if  X  r  X  r q j , otherwise y k =  X  1;  X x k = x [  X  1 , ...,  X  K ] is the dual parameter to be learned, and we can get w as
Based on Eq. 4, now the CCFS problem can be easily optimized by iteratively solving step 3 and step 4 in Alg. 2. (Step 3) Fix d and optimize w . When d is fixed, we can absorb d into X as X  X  X  X  d . Under this circum-stance, Eq. 3 and Eq. 4 become the standard RankSVM training problem without cost constraint, for which we can utilize existing algorithms for optimization. Considering the potential large scale of pairwise term comparison, here we adopt cutting plane method [19] for efficient optimization. (Step 4) Fix w and optimize d . With fixed w , now we aim to find the optimal d . Notice that from step 3, we have w = P K k =1  X  k  X x k (recall X is updated to absorb previous d in step 3). In this way, Eq. 4 can be reformulated as follows:
This is a standard linear integer programming problem, for which we utilize Gurobi 1 for efficient optimization.
During iteration, the upper bound U is meant to be a tunable parameter for the users. In practice, we can set U as a portion of the overall time cost, i.e. U =  X   X  P f  X  X  http://www.gurobi.com/ where  X  take values such as { 1 , 1 2 , 1 4 , ... } .  X  U controls the number of iterations. In our implementation, we set  X  U = (e.g. # Iter = 5 or 10). So far we have elaborated all the details of the proposed TFS framework. Below we will present extensive experi-ments to verify its validity.
We adopt four corpora for experiments, including three academic and one industry corpus.

Robust04. This dataset includes about 0.5 million high quality documents. 250 queries (301-450 and 601-700) pro-vided by TREC X 04 robust track [32] are utilized for the ex-periments. MAP is the primary evaluation metric. Notice here by primary evaluation metric, we mean the metric that is used to rank TREC competition teams.

Cw09BNS . Clueweb09 category B is used, which includes 50 million web pages. We utilize University of Waterloo X  X  spam scores [9] 2 to remove those with spam scores lower than 70, which leaves 29 million web pages. 150 queries (51 to 200 from TREC X 10/11/12 web track) are examined. ERR@20 is the primary evaluation metric. We denote this dataset as Cw09BNS, as NS stands for no spam.

Cw12BNSLD . Clueweb12 category B is used, which also includes 50 million web pages. Since category B contains very few relevant documents that are labeled by TREC, we add all the labeled relevant documents into this dataset. Again University of Waterloo X  X  spam scores [9] 3 are applied to remove those spam web pages (with the same threshold 70), which leaves about 15 million documents. 50 queries from TREC X 13 web track are utilized, with ERR@20 being the primary evaluation metric. We denote this dataset as Cw12BNSLD, as LD means labeled documents are added.
Industry . This is a large scale web page corpus collected from a major search engine company (i.e. Bing). We in-corporate an industry corpus to diversify our experiment settings. For example, the availability of industry search log provides a new resource for query expansion, as well as the search log related features in Table 3 and Table 4. Specif-ically, this corpus includes about 50 million web pages and 2000 queries. NDCG@20 is the primary evaluation metric as in previous research on a similar industry corpus [13]. Now we present all the detailed experiment settings.
Corpus Preprocessing. We utilize Indri 4 , one of the most popular academic search systems, to index all our cor-pora in the form of inverted index [38]. Krovetz stemmer is applied for stemming, and standard InQuery stopwords are removed. Except stopwords removal, we do not conduct any further pruning that might reduce document lengths.
Code &amp; Hardware. In accordance with Indri index, all our algorithms and experiments are implemented in C++ using Lemur/Indri API. The code is compiled by GCC4.7.3 with -O3 option. The code runs in a single thread on a single lab Linux server, which is equipped with a AMD 64bit https://plg.uwaterloo.ca/  X  gvcormac/clueweb09spam/ http://www.mansci.uwaterloo.ca/ msmucker/cw12spam/ http://www.lemurproject.org/indri.php 2.0GHz quad-core CPU, 12G memory and a 3TB disk that contains all the indexed corpora.

QE scenarios . As mentioned in Sec. 2, we can get differ-ent QE scenarios, depending on the resource S upon which expansion terms are extracted. For Robust04, we apply the traditional pseudo relevance feedback (PRF) for query ex-pansion, where resource S is identical to the target corpus C . Top 20 documents retrieved for q are considered relevant, from which expansion terms are extracted.

This PRF scenario, however, did not work well on the other corpora, which include web pages of relatively low quality [9]. We find that, on Cw09BNS and Cw12BNSLD, even after filtering spams, the traditional PRF still does not work well, which is also reported in [1]. Therefore, we try the strategy of S 6 = C .

For Cw09BNS and Cw12BNSLD, we follow the sugges-tion of Bendersky et al. in [1] to use Wikipedia as S . Top 5 ranked Wikipedia documents of original query q are con-sidered relevant, upon which QE is applied. On Industry corpus, we follow the idea of Gao et al. in [13] to use search log as S . The search log is also acquired from the same search engine company, which includes one million dis-tinct click records. Each record contains four elements: user issued query, clicked URL, the score and the rank of the clicked URL returned by the search engine. A snapshot of the search log records is shown in Table 5. For each of the 2000 queries to be experimented, we first find the top 20 sim-ilar queries from the log; then the corresponding clicked web pages are considered relevant, from which expansion terms are extracted. This is actually a one-step random walk in search log [13].

Models &amp; Parameters . Following [5], we utilize KL divergence (KLD) as the basic retrieval model for all the experiments below. The Dirichlet coefficient is set as 1500. The UQE and SQE algorithm are the same as explained in Sec. 3.2, i.e. relevance model for UQE and RankSVM for SQE. For both of them, we empirically set the number of expansion terms as m = 20. Other values of m will be examined in Sec. 4.8. For SQE, the number of candidate terms are empirically set as M = 100. Selected expansion terms are added to the original query by probability interpo-lation, as introduced in Eq. 1. The interpolation coefficient training/validation set.

Evaluation Metrics . Both retrieval effectiveness and efficiency will be evaluated. For effectiveness, MAP and Prec@20 are used for Robust04, and ERR@20 and NDCG@20 are utilized for the other three web page corpora. For effi-ciency, we report the retrieval time costs per query, which is averaged on each query set.

Training/Validation/Testing . For all the query sets, based on the order of their query IDs, we select the first 40% queries for training all the models (e.g. SQE and TFS), the middle 10% queries for parameter validation, and the re-maining 50% queries for testing evaluation. All experiments are repeated three times to report averaged time cost.
TFS Notation . As the two stages in TFS can be applied independently, we will utilize superscript  X  and  X  to indicate the case when AED and CCFS are applied alone, such as UQE  X  and SQE  X  . When the full set of TFS is applied for SQE, then we denote as SQE  X  X  .
As mentioned, the time costs reported below are all ob-tained by running experiments using a single thread on a Linux server. The absolute value might appear larger than previous works (mainly on Cw09BNS and Cw12BNSLD), for which we feel necessary to give a full explanation.
Versus Previous Studies. The reason why previous studies such as [3, 35, 20] reported very low time costs of QE retrieval is mainly due to their selection and pre-processing upon the corpus. For example, (1) Lavrenko et al. [20] and Billerbeck et al. [3] utilized corpora that only contains O (10 5  X  10 6 ) documents; in comparison our Clueweb09/12 and Industry corpora have O (10 7 ) documents, which are at least 10 times larger. (2) Billerbeck [3] and Wu [35] reduced the document length into 20  X  100 terms long, while the averaged document length for our corpora are 500  X  800, which are again about 5  X  40 times larger. The difference of corpus size and document length is the major reason why our reported time costs are larger than previous studies.
Versus Indri Official. With the above settings, our re-ported time costs are actually quite reasonable. As a proof, the Indri official website 5 reported an averaged time cost of 20 seconds per query (wall clock time) on Cw09B (50 mil-lion docs, with spam, one thread program on a 3.0GHz CPU, average query length is about 4), while ours is 9.5 seconds per query (29 million docs, no spam, 2.0GHz CPU, average query length is 2.4). After normalizing various factors 6 can conclude that our time cost per query is very close to that reported by Indri official website. Although this is not an exact comparison, it indeed partially supports our claim. http://www.lemurproject.org/clueweb09/indri-howto.php Table 8: Retrieval Performance on Cw12BNSLD
Versus Engineering Strategy. The absolute value of time costs can be reduced by engineering strategies such as better hardware or distributed/parallel computing, which are widely adopted in commercial search engines like Bing and Google. However, such devices are usually very expen-sive to equip, and are not available to us. Moreover, accu-rately counting the time costs in distributed/parallel com-puting environment becomes difficult, because usually the computing resouces (e.g. CPU or memory) are automati-cally allocated and can vary as time passes. The advantage of us utilizing single thread program on single computer is that, the overall time costs directly reflects the amount of computation (thus the efficiency), and makes it easy to com-pare different retrieval methods.
We first present the major results of retrieval performance on the four corpora, as shown in Table 6, 7, 8 and 9.
Comparison Methods. We conduct extensive compar-ison with the following retrieval configurations: (1) Retrieval for original queries without QE (OrigRet); (2  X  3) UQE and UQE+AED (UQE  X  ); (4  X  7) SQE, SQE+AED (SQE  X  ), SQE+CCFS (SQE  X  ( 1 4 ) ), and SQE+TFS (SQE  X  X  ( 1 4 ) ). Here  X  ( 1 4 ) is an example pa-rameter for upper bound U in CCFS, which means the upper bound U is a quarter of the overall time costs of all term fea-tures, i.e. U = 1 4 P f  X  X  will be examined in Sec. 4.5.

As mentioned earlier, by default reranking (top 1000 doc-uments) is utilized in the second retrieval to report the time costs for the above retrieval methods.

Table Explanation . We adopt evaluation metrics re-garding both effectiveness (e.g. ERR@20, NDCG@20, MAP) and efficiency (Time in seconds). Here (  X  ) indicates pri-mary evaluation metric. We treat UQE and SQE as the baseline, so that we can show how AED, CCFS and TFS improve the efficiency respectively. We use column  X   X   X  to represent the effectiveness difference between UQE/SQE and their speedup versions, and use % for the relative time cost reduction. For example, on Cw09BNS, SQE  X  vs SQE has ERR@20 difference 0.187-0.181=0.006; their time cost change (%, in percentage) is (20.7-27.9)/27.9=-25.8%.  X  and  X  label the positive and negative effectiveness difference that are statistically significant, and  X  means the time cost reduc-tion upon SQE is statistically significant (t-test, p &lt; 0 . 05).
Major Observations . From the tables we can draw the following two major observations. (1) SQE is more effective but also less efficient than UQE and OrigRet. Compared with OrigRet and UQE, the re-trieval effectiveness of SQE can be substantially higher. How-ever, the time costs are also substantially larger. This ver-ifies our motivation that the efficiency issue of SQE is an important research topic. (2) Both AED and CCFS can substantially improve the efficiency of SQE, meanwhile only incurring insignificant ef-fectiveness fluctuation. In the above tables, we progres-sively add each component to SQE, so that one can see how the efficiency is progressively improved. In general we can conclude that for SQE, CCFS achieves higher efficiency than AED, and their combination (i.e. our TFS framework) achieves the most efficiency improvement. For effectiveness, although both positive and negative changes are observed, most of them are statistically insignificant (t-test, p &lt; 0.05). I.e. most of the effectiveness changes are not labeled by  X  or  X  . Therefore, we can conclude that our TFS framework can well maintain the effectiveness of SQE.

We also notice that for UQE, UQE  X  has slightly increased time costs. There are two reasons for this phenomenon. First, for UQE there is no expensive term feature extraction, so that AED only skips the generation of UQE expansion terms and the reranking process in second retrieval. Since these two steps are already very fast, the reduced time cost is not substantial. Second, AED will result in some extra time costs for query feature extraction as well as the appli-cation of AED classifier. Therefore, the overall time costs of UQE  X  will be slightly higher than UQE. But notice, the absolute value of such increase is very low (at most 0.37 seconds). Furthermore, as we will show in Sec. 4.9, the effi-ciency improvement of AED can be very substantial, if the full second retrieval is applied instead of reranking, which verifies the usefulness of AED.

Below, we will present more experiments to thoroughly investigate AED and CCFS. As CCFS will also be utilized in AED experiments, we will first analyze CCFS for sake of clear presentation.
In the above we have verified the validity of feature selec-tion in speeding up SQE. Now we will investigate how our CCFS algorithm in Alg. 2 performs in this task.

Comparison Methods . The following two algorithms are compared with our CCFS algorithm.

L1-RankSVM . L1 regularization is a very popular feature selection method. When feature selection occurs, we re-place the L2 regularizer in vanilla RankSVM (Eq. 3) with L1 regularizer || w || 1 . By adjusting the coefficient G , || w || will function to different extent, thus resulting in various combinations of features. Notice this method is unaware of the difference in the time costs of extracting each feature. L1General library 7 [28] is utilized for optimization.
Wang X  X  method [33]. This is a greedy algorithm for cost aware feature selection, proposed by Wang et al. in [33] for learning to rank. In this algorithm, each feature is assigned a profit score, which is the ratio between feature weight and time cost. Features are sorted by profit scores; then top features are selected until the time cost upper bound is reached, which makes it a greedy selection. Different from L1-RankSVM, this is a cost aware feature selection method.
For both Alg. 2 and Wang X  X  method, we use the same cost upper bounds as U =  X  P f  X  X  the nodes along the curves in Figure 3. For L1-RankSVM, each node represents a different G value, which is tuned on training set so that different time costs are obtained.
Overall Results. In Figure 3 we illustrate the curves of the three methods on all corpora. These curves represent the retrieval effectiveness when various feature extraction time is spent (excluding any retrieval time). The purple tri-angles at the left end of curves represent UQE algorithm, i.e. no term feature is extracted. The blue rectangles at the right end of curves represent SQE algorithm with all avail-able term features. In the middle, various feature selection methods show different effectiveness-cost tradeoff. We can clearly observe that more features can produce higher re-trieval accuracy, but this inevitably takes more time thus decreasing the efficiency.
 CCFS performs best, particularly at low time cost. Comparing the three feature selection methods, we can see that our CCFS algorithm outperforms the others, especially when the feature cost is low. L1-RankSVM penalizes the number of selected features. That means, each feature is treated equally, ignoring the cost difference among differ-ent features. Since expensive features can be included, to reach a certain time cost, usually L1-RankSVM will over-penalizes the number of selected features, which deteriorates the retrieval effectiveness. Wang X  X  method greedily selects features based on their profit scores, i.e. the ratio between feature weight and cost. Here the feature weights are the ones derived when all features are available. However, this selection process is suboptimal, because for a single feature, its weight will become different when fewer other features are available. Therefore, the profit score may not accurately re-flect the importance of individual features, particularly when http://www.cs.ubc.ca/  X  schmidtm/Software/L1General.html few features exist (i.e. time cost is low). In comparison, our CCFS algorithm iteratively updates learning objective, and decreases the cost upper bound smoothly. Therefore, CCFS performs better, particularly when time cost is low.
Now we will examine how AED affects the effectiveness and efficiency of UQE and SQE retrieval. We show the performance in Figure 4, where UQE, SQE and SQE  X  al-gorithms are all examined before and after applying AED. For SQE  X  , the CCFS curves from Figure 3 are utilized.
For both SQE and SQE  X  , their performance is left-shifted after applying AED. Moreover, the SQE  X  curves shrink after AED. This means, the process of term feature extraction and second retrieval (reranking) are skipped for some of the queries (i.e. SQE-unsuitable), which makes the averaged time costs over all queries become smaller.

For UQE, the time costs of UQE  X  is slightly higher. This has been explained in Sec. 4.4, which are due to the fast process of reranking and UQE expansion term generation, as well as the existence of AED overhead cost.

The extent of efficiency improvement of AED depends on the number of skipped queries. In Table 10 we give the detailed number of skipped queries on each corpus for SQE.
From the perspective of effectiveness, we can observe that on all corpora, for all of UQE, SQE and SQE  X  , their effec-tiveness after applying AED is improved or at least main-tained than before applying AED. This is particularly help-ful in achieving a good balance between effectiveness and efficiency.
 Table 10: Number of skipped queries in AED for SQE. In Figure 1 we have shown the step-wise time costs on Cw09BNS for UQE, SQE and their speedup improvements. There for CCFS we adopt the same upper bound as in Ta-ble 7 (i.e. U = 1 4 P f  X  X  vance Feedback (PRF) scenario where S is Wikipedia and C is Cw09BNS. In this case, the time cost of second retrieval will be large because in second retrieval we need to firstly search query q on C then apply the reranking.

Now we further show the step-wise time costs for PRF scenario on Robust04 in Figure 5(a). In this case, S = C = Robust04, so we only need to retrieve q once in first retrieval, and the second retrieval only needs to rerank the results of first retrieval. In this case, the cost of second retrieval will be much smaller than in non PRF scenario.
Now let X  X  see how different number of expansion terms m affects the retrieval effectiveness and efficiency. In Fig-ure 5(b), we show the effectiveness-cost curves when m = { 10 , 20 , 30 } for SQE  X  X  on Industry corpus. We can see the effectiveness of m = 20 , 30 is similar, while that of m = 10 is quite degraded. The time cost gap between OrigRet and SQE  X  X  curves includes the cost of first retrieval (i.e. search-ing S ), applying AED, extracting term features, etc. Also notice the overall time cost is not obviously affected as more expansion terms are utilized. This phenomenon is mainly due to the application of reranking. Otherwise if a full sec-ond retrieval is applied, the time cost of second retrieval will be (approximately) linear with the number of m , which can be very huge in practice (see Sec. 4.9).
Finally, for the SQE retrieval process, we compare the two strategies for second retrieval: reranking vs full second re-trieval. Although we have argued the validity of reranking and have utilized it throughout the above experiments, we still feel it necessary to present a formal comparison with full second retrieval due to the following two reasons. First, as reviewed in Sec. 2.2, we find most of existing QE effi-ciency works [3, 35] only focused on indexing or document optimization, while ignored the value of reranking. It is only very recent that Diaz [11] pointed this out. Here we X  X  like to add more proof to support reranking. Second, in the above experiments for UQE and UQE  X  , we observed that pure AED may not result in substantial speedup, and point out that the application of reranking is the major reason for that. By further showing the time costs of full second retrieval, we can justify the value of AED.

In Figure 5(c) and (d), we show the performance of rerank-ing top 1000 documents for expanded queries q e with 20 ex-pansion terms. We can see that for both the case of UQE and SQE, reranking does not incur obvious effectiveness loss, yet results in substantial efficiency improvement. Particu-larly for the UQE case, the speedup becomes obvious when full second retrieval is utilized on Cw09BNS. These observa-tions verify our adoption of reranking instead of full second retrieval, as well as the usefulness of AED on efficiency.
Supervised query expansion (SQE) has recently become the state-of-the-art in the QE literature, which usually out-performs the unsupervised counterparts. To obtain good retrieval effectiveness, SQE usually extracts many term fea-tures to predict the quality of expansion terms. However, this can seriously decrease its efficiency. This issue has not been studied before, nor can it be handled by previous data-level QE efficiency methods such as indexing or documents optimization. To address this problem, in this paper we pro-pose a Two-stage Feature Selection (TFS) framework, which includes Adaptive Expansion Decision and Cost Constrained Feature Selection. Extensive experiments on four corpora show that the proposed TFS framework can significantly improve the efficiency of SQE algorithm, while maintaining its good effectiveness. [1] M. Bendersky, D. Fisher, and W. B. Croft. Umass at [2] M. Bendersky, D. Metzler, and W. B. Croft. Effective [3] B. Billerbeck and J. Zobel. Efficient query expansion [4] C. M. Bishop. Pattern Recognition and Machine [5] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting [6] C. Carpineto and G. Romano. A survey of automatic [7] K. Collins-Thompson. Reducing the risk of query [8] K. Collins-Thompson and P. N. Bennett. Predicting [9] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. [10] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. [11] F. Diaz. Condensed list relevance models. In ICTIR , [12] J. Gao, S. Xie, X. He, and A. Ali. Learning lexicon [13] J. Gao, G. Xu, and J. Xu. Query expansion using [14] X. Geng, T.-Y. Liu, T. Qin, and H. Li. Feature [15] I. Guyon and A. Elisseeff. An introduction to variable [16] C. Hauff, D. Hiemstra, and F. de Jong. A survey of [17] B. He and I. Ounis. Query performance prediction. In [18] B. He and I. Ounis. Combining fields for query [19] T. Joachims. Training linear svms in linear time. In [20] V. Lavrenko and J. Allan. Real-time query expansion [21] V. Lavrenko and W. B. Croft. Relevance-based [22] C.-J. Lee, R.-C. Chen, S.-H. Kao, and P.-J. Cheng. A [23] Y. Lv and C. Zhai. Adaptive relevance feedback in [24] Y. Lv and C. Zhai. Positional relevance model for [25] Y. Lv, C. Zhai, and W. Chen. A boosting approach to [26] C. D. Manning, P. Raghavan, and H. Sch  X  utze. An [27] S. C.-T. ownsend Y un Zhou W. Bruce Croft. A [28] M. Schmidt. Graphical model structure learning with [29] H. Sch  X  utze and J. O. Pedersen. A coocurrence-based [30] M. Theobald, R. Schenkel, and G. Weikum. Efficient [31] P. Viola and M. J. Jones. Robust real-time face [32] E. M. Voorhees. Overview of the trec 2004 robust [33] L. Wang, D. Metzler, and J. Lin. Ranking under [34] J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, [35] H. Wu and H. Fang. An incremental approach to [36] C. Zhai and J. Lafferty. Model-based feedback in the [37] Y. Zhao, F. Scholer, and Y. Tsegay. Effective [38] J. Zobel and A. Moffat. Inverted files for text search
