 1. Introduction
User-centered studies on information search are becoming increasingly common mostly due to the increas-ing popularity of Web searching. Traditionally, information retrieval (IR) studies have been system-centered, focusing on the performance of the algorithms matching queries with relevant documents. However, interest in the users X  search strategies has increased in the IR community over the years, a notable example of this interest being the interactive track in the TREC conference.

In the field of human computer interaction (HCI), the focus is by definition on the human side of comput-ing systems. Thus, in HCI studies, the involvement of users in the evaluation and comparison of computer systems has a long tradition. Although the importance of including the users in the comparisons of search sys-tems is acknowledged both in the field of HCI and IR, combining the system and user-centered approach has proven to be surprisingly difficult; researchers conducting user-centered studies on search user interfaces have had trouble in finding suitable forums to publish their work.
In Tampere Unit for Computer X  X uman Interaction (TAUCHI) at the University of Tampere, search user interfaces have been studied extensively with a user-centered approach over the past years. This paper shares our experiences on the methodological issues we have learned when conducting experimental comparisons of
Web search user interfaces. This paper provides a coherent view of the methods and measures we used in our studies. Individual studies along with methods and metrics have previously been presented in separate publi-cations, but this paper focuses on the strengths and weaknesses of these methods and metrics in order to assist researchers to choose the appropriate methodological approach when planning and conducting experimental comparisons of search user interfaces.

We will discuss two issues that play a major role in user-centered search user interface comparisons: 1. Variance brought to the test setup by the involvement of users. This variance can compromise the validity of the experiment if not handled properly as the measured effects may be smaller than the variance caused by the user X  X  personal characteristics. Based on our experience, relatively radical actions can and should be used to control the variance present in the user-centered search studies. 2. The use of appropriate measures. Appropriate measures enable and facilitate comparisons between studies and allow robust and useful conclusions to be made.

Partial solutions to these problems can be found from the previous studies reporting individual experi-ments. This paper combines the methodological suggestions and discusses their implications in detail.
Of the four phases of searching (query formulation, search execution, result evaluation, and possible query reformulation), our studies have mainly focused on enhancing the result evaluation phase through new user interface solutions. In evaluating these solutions, we have successfully employed balanced task sets, limited the users X  time in the search tasks, used pre-formulated queries, and restricted the users X  access to result doc-uments. Certainly, the restrictions raise questions about ecological validity, but they can be largely overcome by utilizing the methods when the research questions are suitable for an experimental study and by conducting complementary studies with other methods, such as log studies ( Ka  X  ki, 2005b ).

In measuring the success of search user interfaces, we have employed the existing measures of interactive precision and recall ( Veerasamy &amp; Belkin, 1996; Veerasamy &amp; Heikes, 1997 ). However, in the course of our research, a need for measures that would capture the characteristic user behavior in web searching rose.
Thus, we proposed two search speed measures and a measure called immediate accuracy to capture the success specifically in Web searching. These measures have revealed new insights on the systems and are a good addi-tion to the toolbox for user-center evaluations. 2. Related work
Although the evaluation of search systems becomes more challenging when users are involved in the pro-cess, the popularity of this approach has grown over the years ( Savage-Knepshield &amp; Belkin, 1999 ). There are numerous methods for collecting data about the use of search systems, such as transaction logs ( Jansen &amp;
Pooch, 2001; Silverstein, Marais, Henzinger, &amp; Moricz, 1999; Spink, Wolfram, Jansen, &amp; Saracevic, 2001 ), &amp; Tasso, 1996; Dumais, Cutrell, &amp; Chen, 2001; Hertzum &amp; Fr X kj X r, 1996; Heimonen &amp; Jhaveri, 2005; Paek, Dumais, &amp; Logan, 2004; White, Ruthven, &amp; Jose, 2002 ).

In user-centered comparisons of search user interfaces, the comparison is often made in relation to the three main aspects of usability, namely, effectiveness, efficiency and satisfaction, as defined in ISO-9241-1 standard (1998) . In this standard, effectiveness is defined as the  X  X  X ccuracy and completeness with which the users achieve specific goals X  X  with the system, efficiency refers to the needed resources for achieving goals with the system, and satisfaction to the user X  X  attitudes towards using the system. In user-centered evaluations of search interfaces, it is important to measure all of these aspects of usability as the different aspects do not always cor-relate ( Fr X kj X r, Hertzum, &amp; Hornb X k, 2000 ).

The results of the search interface comparisons are often reported in figures related to effectiveness (e.g., percentage of tasks completed) and/or efficiency (e.g., task times) ( White et al., 2002; Woodruff, Faulring,
Rosenholtz, Morrison, &amp; Pirolli, 2001 ). Often, these two figures are reported separately. In addition to these objective measures for search success, the role of the user X  X  perceived success and subjective satisfaction has been emphasized by several researchers ( Su, 1994; Tague &amp; Schultz, 1988; White, Ruthven, &amp; Jose, 2003 ).
The most common methods for collecting information on subjective satisfaction are interviews ( White et al., 2002; Woodruff et al., 2001 ) and questionnaires ( Brajnik et al., 1996; White et al., 2002, 2003 ).
When comparing the usability of search user interfaces, the study setup needs to be controlled to assure that the observed differences are due to the differences in the interfaces rather than some confounding variables.
However, the level of control varies between studies: users can perform tasks in their normal context while the researcher remotely logs their actions ( Anick, 2003; Hertzum &amp; Fr X kj X r, 1996 ) or the study can be con-ducted in a laboratory where the researcher can carefully control the situation ( Topi &amp; Lucas, 2005; Woodruff et al., 2001 ). In the former, the ecological validity of the study is perhaps higher, but the results may otherwise be harder to interpret due to the uncontrolled factors.
 We have concentrated in experimental studied that have mostly been carried out in a laboratory setting.
Such an approach allows and also requires a great deal of control. Next, we will discuss the controlling meth-ods we have employed. 3. Controlling complexity
Controlling the large number of variables that enter the test situation along with the user is a major chal-lenge in user-centered studies. We have successfully utilized several methods to ensure that only the pheno-menon of interest, namely, the user interface, varies while the other factors are controlled. 3.1. Experimental design
The users X  characteristics such as search and domain expertise, age, and cognitive style are known to have a 2005; Lazonder, Biemans, &amp; Worpeis, 2000 ). In addition, the type of the search task has an effect on the strat-egies that people use ( White &amp; Iivonen, 2001 ).

To control the effect of the user-related factors, we find the within-subjects design to be appropriate. In this design, all the participants use all the tested interfaces and thus, the users X  personal search strategies remain constant over the tested interfaces and variation decreases. This conclusion is in line with several TREC inter-active track guidelines from the past years (e.g., TREC interactive track guidelines, 1999, 2000, 2002 ).
Although being an attractive solution by minimizing the variability caused by individual differences, the within-subject design poses also challenges. One problem is that the presentation order of the compared sys-tems may have an effect on the users X  performance and preferences. This, however, can normally be controlled by counterbalancing the presentation order between the users.

Another and a more serious shortcoming in the within-subjects design is that the same tasks cannot be used with each compared interface because the participants learn the solutions and successful strategies on the first encounter. The obvious solution of using different tasks may, however, introduce additional variance to the setup.

To decrease the variance caused by the different tasks, we have used balanced task sets . To use balanced task sets, equally many task sets are required than there are conditions in the test. Each task set is used once in a test, in one condition. To make the task sets as similar as possible, we have found it useful to create pairs (or number of interfaces to be compared) of tasks, the difficulty and topics of which are as similar as possible.
In practice, it is often enough to just modify one term from the task description, for example:  X  X  X ind Chinese restaurants in New York X  X  and  X  X  X ind Chinese restaurants in Los Angeles X  X . Keeping the topics in the pairs of tasks constant is advisable as it reduces the effects that the users X  interest in different topics may have on their performance.
 For creating realistic tasks, we have used common topics from actual Web searches (e.g., Spink, Jansen,
Wolfram, &amp; Saracevic, 2002 ). Although we have verified the successfulness of the task set balancing with pilot tests, in reality, it is practically impossible to create two tasks sets that would be equally demanding. Thus, it is always advisable to statistically analyze whether the task sets had an effect on the dependent variables of the study despite of all the precautions.

Another problem with balanced task sets, in addition to them being laborious to construct, is that they may introduce some learning effects. In the example above, it is possible that the user discovers a smart search strat-egy for that kind of a problem on the first encounter. The second condition would then benefit from the learn-ing effect. We have simply used the test moderator X  X  judgment to decrease this risk by removing the tasks affected by learning effects based on the pilot test observations.

For controlling the variation caused by different task sets, we have used a counterbalanced design so that each task set is used with each interface equally many times and that the presentation order of the task sets and interfaces is balanced. For such balancing, the well-known Latin Square is a useful aid. While counter-balancing removes major issues, it is not a magic tool to make test setup right. Selection of the tasks and balancing of the task sets are crucial for the reliability of the test setup. 3.2. Pre-formulated queries and cached result pages
Even for exactly the same task, the queries the searchers formulate vary vastly. Without control, task times with the interfaces may be different because the queries happened to be poor in one interface and good in the other. When the users X  query formulation skills are not in the focus of the study, we have utilized pre-formu-lated queries and cached result pages . This way, the researcher can eliminate some variation from the test setup and be more confidential that the measured differences are due to the differences in the user interfaces.
The challenging aspect with the pre-formulated queries is to choose the query terms wisely  X  so that the results also apply outside of the experiment and that the queries resemble queries that the users would nor-mally make. Having conducted numerous studies where the users formulate their own queries ( Aula, 2003, 2005; Aula &amp; Ka  X  ki, 2003; Aula &amp; Nordhausen, 2006 ), we have gained a good understanding on the terms the users typically select as query terms given the task description. Additional guidance for formulating real-istic queries can be found from pages reporting common queries (e.g., http://www.google.com/press/zeit-geist.html ), as well as from studies on transaction logs ( Jansen &amp; Pooch, 2001; Silverstein et al., 1999; Spink et al., 2001 ).

The main shortcoming with pre-formulated queries is that it is unfortunately possible for the researcher to deliberately choose the query terms so that her own interface wins the comparison. In fact, this may happen even for researchers with good intentions if they do not pay attention to the selection of query terms. Unfor-tunately, we have not come across with techniques to ensure fairness of the queries. One way to lessen the problem is to have the researchers always report the tasks they used along with the pre-formulated queries.
Surprisingly, this essential information is often excluded from publications. Another shortcoming of pre-formulated queries is that they are not suitable for studies where the quality of the result set is in focus or when the interface specifically tries to aid users in query formulation.

To avoid the variance caused by the changing contents of databases and varying network delays, it is ben-eficial to locally save the result pages for the pre-formulated queries. With this approach we can be sure that all participants see exactly same set of results for a given query in a well controlled time-frame. 3.3. Time and data restrictions
From log studies, we know that the web searchers typically only evaluate one result page of ten results per query ( Jansen &amp; Pooch, 2001 ). However, in several of our early attempts to compare search interfaces, we noticed that users are often overly thorough in the test situation: they may spend several minutes evaluating the result list, which is not likely to happen in the real usage. Thus, we imposed a time limitation whereby the time the users had per task was limited to, for example, 1 min. According to our experiences, this limitation made the users X  behavior closer to the real search behavior as seen from the transaction logs and thus, made our data more realistic.

The time limitation is not without problems: it may stress the participants as they may feel that they would have performed better given more time. Careful instructions emphasizing that whatever can be found in the given time is acceptable, can lessen the anxiety. In addition, the time restriction complicates the use of task time as a measure. Especially a tight limitation causes a ceiling effect and most of the task times will be exactly the given limit. However, this can be overcame by certain measures, such as search speed (discussed in Section 4.3 ).

Without proper control, the task time can easily contain activities that are not dependent on the quality of the search interface, such as, browsing through hyperlinks and reading the result pages. Additionally, going through information on web pages inevitably changes the users X  understanding on the search task. If different users face different information during the test, their behavior may change unexpectedly and due to reasons that are not related to the search user interface. To eliminate these sources of variability, we have restricted the users X  access to web content by disabling the links from the result page. This way, all the users see the same information (namely, result summaries) and the only factor that varies between the users is the interface. In these studies, the users have selected relevant-looking results by selecting a checkbox by the summary or by clicking on its title. Thus, the user interface looks realistic, but the behavior is modified to match the needs of the experimental setting. 4. Making comparisons easy with measures
An essential part of comparing search user interfaces is to utilize effective measures that focus on meaning-ful differences. In the context of comparing search user interfaces from the user-centered perspective, mean-ingful differences are, for example, differences in the user X  X  performance (e.g., effectiveness and efficiency).
In addition to that, measures dealing with the overall user experience are important to take into account, as the willingness to use the system is largely dependent on the user X  X  perception of the system. Although we do feel that both performance and user experience factors need to be considered when evaluating search user interfaces, this paper purposefully concentrates on measures that capture the differences in the user X  X  performance leaving the measures related to user experience out of discussion.

Even the performance measuring practices in the field vary considerably, which makes it difficult to under-stand the results from different studies and practically impossible to compare them. Good measures make comparisons within one study easy and also facilitate comparisons between different studies. For this purpose, we have found suitable measures from the earlier research and proposed a few new ones. 4.1. Interactive precision and recall The traditional measures of precision and recall are the cornerstones of the research on information search. However, in user-centered studies, these measures are not effective as they do not consider the users X  actions.
In response to this problem, Veerasamy and colleagues ( Veerasamy &amp; Belkin, 1996; Veerasamy &amp; Heikes, 1997 ) have proposed modified versions of the measures: interactive recall and interactive precision . Interactive recall measures the percentage of the relevant documents in the result set that were selected by the user whereas interactive precision states the proportion of relevant documents within the selected documents. In our studies, we have found these measures to be useful and to provide interesting information on the com-pared systems. 4.2. Immediate accuracy
When using web search engines, users typically open only a couple of documents per each query for closer inspection (Spink et al., 2001). Immediate accuracy captures the relevancy of these selections ( Ka  X  ki, 2004 ).
Immediate accuracy is a cumulative measure that states the proportion of cases where the users have found at least one relevant result by the n th result selection. Immediate accuracy of 85% by second selection means that in 85% of the cases (tasks) the users have found at least one relevant result by the time they have selected two result documents. Higher immediate accuracy means better success with this search style. It is noteworthy that immediate accuracy rarely reaches 100% meaning that not everyone will find a relevant result for each task.

We have employed this measure in several experiments ( Ka  X  ki &amp; Aula, 2005; Ka  X  ki, 2005a, 2006 ) and have found meaningful differences between interfaces with it. The results of this measure are easy to understand and seem to reflect the web search behavior well. In addition, being a proportional measure, the measure makes it easy to compare different systems. 4.3. Search speed
Users X  search speed is one of the core measures in the user-centered evaluations and thus, we were surprised to see that there was no established measure for it. Previous studies frequently report raw measurements (e.g., task time and number of selected results) from the test setups, which are difficult to compare even within one study due to the lack of normalization. For example, it is not easy to see if System1 that takes on average, 10.10 min to find 16.44 answers is better or worse than System2, with which users can find 12.26 answers in 30.64 min (data from Pirolli, Schank, Hearst, &amp; Diehl, 1996 ).

To overcome this problem, we proposed a proportional search speed that is measured in answers per minute (APM) ( Ka  X  ki, 2004 ). With search speed, the relationship of the previous two systems is evident, as System1 gets 1.62 APM and System2 gets 0.40 APM. In addition, these figures make it easy for the reader to under-stand the magnitude of speed that is achieved considering the constraints posed by the test setup. Although a normalized measure is used, the comparison between different studies is not trivial. Test setup has a vast effect on the results and the measure does not take this effect into account.

To enhance the comparisons even further, we introduced the measure of qualified search speed that consid-ers the quality of the results in addition to time. It is measured in answers per minute for a relevancy category.
Our example System1 could yield 1.32 irrelevant and 0.3 relevant results per minute. If System2 yielded 0.4 relevant results per minute, the qualified search speed reveals that System2 is better although the raw search speed suggested otherwise. In our studies, this measure has revealed that the source of performance difference is typically the increase in relevant speed while the irrelevant speed has remained nearly constant. 5. Applying the methods and metrics
To better understand how the proposed methods could be used in experimental comparisons of search user interfaces, we will present a few examples. The examples are based on our own studies where the methods were developed and applied. 5.1. Investigation of the readability of search results
Aula (2004) compared three different result presentation styles in effectiveness and efficiency (task time and error rate). All of the presentation styles contained exactly the same textual information, only the text layout and the use of bolding of query term occurrences varied between the conditions. The study used a within-subjects design where all the participants completed tasks with all three presentations styles. To make it possible to compare the task times between presentation styles, we prepared three balanced task sets (1, 2, and 3), where the corresponding tasks closely resembled each other. For example, for the task  X  X  X ho is the principal of the University of Oulu X  X  only the name of the university would change between task sets but the task would otherwise be the same. All of the tasks were simple fact-finding tasks with only one correct answer.
 To control the variance caused by users X  formulating different queries, pre-formulated queries were used.
Again, the queries were formulated so that they closely resembled one another (e.g.,  X  X  X rincipal university oulu X  X ,  X  X  X rincipal university helsinki X  X ,  X  X  X rincipal university tampere X  X ). These queries were submitted to a search engine and the first ten results were saved for each query. During the experiment, the participants inter-acted with the cached result pages instead of real web content to avoid variance caused by network delays or the index of the search engine changing between participants. Furthermore, we restricted the users X  access to web content ; the next task was presented immediately after the users clicked on the title of the result they thought contained the answer to the fact-finding task.

The combinations of task set (1, 2, 3) and presentation styles (A, B, and C) were counterbalanced between participants using Latin Square . In practice, this meant that participant 1 would see first style A with tasks from task set 1, then style B with task set 2, and finally, style C with task set 3. For participant 2, the order would be A2, B3, and C1 and for participant 3 A3, B1, and C2. For the next three participants, the order of presentation styles was B, C, and A (the order of task sets remained constant); and for participants 7 X 9, the presentation styles were presented in the order C, A, and B. The order of presenting the individual tasks was randomized.

In this study, the strict control over the setup made it possible to reliably show the differences in efficiency between the presentation styles. 5.2. Evaluation of a search result user interface solution
A series of studies were conducted to evaluate a novel user interface for accessing search results. This exam-new interface (category interface) automatically categorizes search results. The aim of the categorization is to reveal major topics among the results and make it easier for the users to understand the result set and access topics that are relevant for them. The user interface lists 15 categories next to the result list. When the user selects a category, the result list is filtered to show only the results belonging to the selected category. The interface was hypothesized to let the users access moderately sized result sets (about 100 X 300 results) effectively and thus, overcome the inherent problems in result ranking when the user enters ambiguous (often short) queries.

In the evaluation of the proposed solution, we compared it with the de facto standard solution of ranked list of results (Google style result listing). Each participant used both user interfaces ( within-subjects design ), which required us to prepare two balanced task sets . The use of task sets was counterbalanced between the user inter-faces and participants using Latin Square .

Each task was associated with a pre-formulated query . In these studies, this was seen to be mandatory to reliably compare the effect of the user interface rather than, for example, difference in users X  query formulation skills. The queries were executed before the experiments and query results were cached to ensure the same stim-uli for each participant.

During the experiment, we denied the access to web content , because the users X  ability to evaluate and browse the actual web documents was not part of the phenomenon we studied. Time for each task was limited to 1 min. Although the restriction is fairly radical, it was seen both necessary and successful in this study. One minute was appropriate for this setup as the average number of collected results was similar as in an unre-stricted situation observed by Aula and Nordhausen (2006) .
 In measuring the success of the compared user interfaces, we applied all the measures presented earlier.
Reporting the search speed gave an idea of how fast the users were in the test situation. Qualified search speed revealed that the performance difference is a result of the users finding relevant results faster with the proposed category interface than with the conventional interface. Interactive recall and precision confirmed those find-ings by showing higher recall and precision measures for the category interface. In practice, this means that the users found a larger proportion of relevant results from the result set with the new interface and that the selected results contained less irrelevant results. Higher score in immediate accuracy indicated that the new user interface is also more effective in typical Web behavior where the first one or two good enough answers may be enough for the users. 6. Discussion
Researchers who have involved users in studies of search behavior or search systems can surely appreciate the complexity of the situation. When a human being is taken along into an evaluation, a number of uncon-trolled variables come in to play. For example, we have seen enormous differences in the users X  query formu-lation skills, their style of refining queries, and their habits and thoroughness of result evaluation. If such variables are not controlled in an experiment, it is impossible to draw reliable conclusions concerning the dif-ferences between the search interfaces being compared.
In this paper, we shared our experiences in dealing with this complexity. We discussed various methods of controlling the experimental studies of search interfaces. Additionally, we presented measures for enabling meaningful comparisons of search user interfaces as summarized in Table 1 .

We have successfully utilized the presented methods and metrics when comparing search user interfaces in the context of the Web. Furthermore, our experiences with the methods and metrics are based on studies where the interest has been mainly to support the users X  evaluation of search results, rather than the search process on the whole. Due to these restrictions, the applicability of the methods to other contexts and situa-tions is not trivial and needs to be considered case-by-case. However, in the following, we will aim at giving some general guidelines for applying the methods and metrics.
 First, we expect most of the proposed methods and metrics to be applicable to other search contexts than
Web, as well. We believe that within-subjects designs, balanced task sets, pre-formulated queries, cached result lists, limitation of the users X  access to result documents, search speed, and qualified search speed can also be applied also to other search environments, such as digital libraries.

Second, the applicability of other methods or metrics, such as time limitations or immediate accuracy, should not be taken for granted. Limiting the users X  time in the result evaluation phase seems to be an appro-priate method in the context of the Web as Web searchers typically evaluate the results quickly. However, in the other contexts, the evaluation of search results may not follow this pattern. Immediate accuracy is a metric that specifically captures the behavior of web searchers and again, the applicability to other contexts may not be straightforward.

One must however, that all the presented methods have their shortcomings. As any method, they could even completely jeopardize the validity of the research results if applied inappropriately. Unfortunately there are no formal procedures for ensuring the valid usage of the methods but rather their suitability must be care-fully considered for the research question at hand.

Our experiences with the methods and metrics are restricted in that we have focused on controlling the var-iability in the phase of results evaluation and those preceding it. Thus, in our studies, the users have not been allowed to formulate their own queries, spend as much time evaluating the results as they wish, or read the documents the results point to. Consequently, the studies have not given any new information about these phases of search. Future research needs to address the questions of which controlling methods are suitable when the aim is, for example, to experimentally study interface solutions aimed at facilitating initial query formulation.
 References
