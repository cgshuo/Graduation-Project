 In recent years, we have witnessed a growing interest in the unsupervised POS tag-ging (called POS induction), where no prior knowledge is available. In contrast to supervised or semi-supervised tasks [Hasan and Ng 2009], it is difficult to determine the optimal clustered tag number, for lack of gold standard extracted from dictionary [Merialdo 1994] or prototypes [Haghighi and Klein 2006] of some or all tags. The common approach to unsupervised POS induction is based on the Hidden Markov Model (HMM) [Ravi and Knight 2009]. For example, the basic first-order HMMs have been designed to model the relationships between two successive input words (i.e., bi-gram [Charniak et al. 1993; Dermatas and Kokkinakis 1995]). Thede and Harper [1999] improved the tag accuracy by incorporating a more limited amount of the contextual information, that is, replacing the bi-gram with tri-gram transi-tion probabilities. However the higher-order HMMs greatly increase the computa-tional complexity and require sophisticated smoothing techniques [Goodman 2001] (e.g., linear interpolation, Good-turing and back-off Smoothing, etc.) while handling sparse data.

Generally, a modified Expectation-Maximization (EM [Elworthy 1994; Merialdo 1994]) algorithm is used for learning HMM parameters. Recent research shows that the accuracy improvement of the EMHMM is limited because EM tends to assign a relatively equal number of tokens to each hidden state, while the empirical distribu-tion of POS tags is highly skewed toward a few high-frequency tags, such as noun, verb, adjective, and adverb [Johnson 2007; Lee et al. 2010; Mitzenmacher 2003]. To introduce priors to favor sparsity, Bayesian HMM can be used.
 Generally, there are two inference approaches for estimating parameters of Bayesian HMM: (1) Variational Bayes (VB), and (2) Markov Chain Monte Carlo (MCMC). Johnson [2007] biased the bi-gram HMM with Dirichlet priors over emis-sion and transition distributions. Further research [Graca et al. 2009] directly regularized the posterior by introducing a linear penalty on the standard marginal likelihood objective. MCMC contains a series of sampling techniques [Besag 2004]. The pointwise collapsed Gibbs sampler [Goldwater and Griffiths 2007] resamples a sin-gle state (labeling a single word) at each step. In contrast, the blocked sampler resamples the POS tag sequence for the entire sentence at a single step using dy-namic programming [Johnson et al. 2007]. The tag accuracy comparison between VB and MCMC varied in the size of the training data and number of hidden states in the HMM. Johnson [2007] experimented on the full Wall Street Journal (WSJ) part of the Penn. Treebank (1,173,766 words) and 50 hidden states and found that VB performed much better than MCMC. When the number of hidden states is re-duced to 25, VB is still better than MCMC. If the training dataset is reduced to 120,000 and 24,000 words and the number of hidden states is set to 17 [Gao and Johnson 2008], MCMC outperforms VB. In summary, VB does well on larger data sets and higher dimensionality of hidden state space, whereas MCMC performs bet-ter on small data sets and a low number of hidden states. In comparison of conver-gence speed, VB is fastest among all estimators, especially on large data sets. And the speed of convergence of Gibbs samplers depends on the value of hyper-parameters in Dirichlet priors. The increment of hyper-parameters can speed up the convergence. It is worthy to note that Gibbs algorithm is not guaranteed to converge.

The earliest Chinese POS tagger was presented by Chang and Chen [1993]. Their system was based on a first-order HMM and ran experiments on part of the 1991 United Daily corpus (approximately 10 million Chinese characters extracted from about twenty days of news articles) for training and testing. After the publication of Chinese Penn Treebank [Xia 2000], many researchers can investigate supervised POS tagging with a variety of machine learning approaches, including transformation-based learning (TBL [Forst and Fang 2009]), maximum entropy (MaxEnt [Ng and Low 2004]), RankBoost [Huang et al. 2007], bagging (combining TBL, trigram and MaxEnt baseline taggers [Xia and Cheung 2006]), and so on. Huang et al. [2009] improved a simple bigram HMM by introducing latent annotations and supplement information by self-training with a large set of unlabeled data from similar sources. Little work has been found on unsupervised POS induction for Chinese language. Following the experiments done for English in Gao and Johnson [2008], Cheng et al. [2010] compared Chinese performance of VB and Gibbs Sampling and also found that Gibbs Sampling achieved better results on three data sets with smaller size (24k, 120k, and 500k words respectively) corpus and at most 50 hidden states. In contrast, our experiments were carried out on the training corpus with 609 , 060 words. And the number of hidden states is naturally unbound because of iteratively splitting. In terms of the scale of corpus, state dimensionality and convergence speed, we apply the VB instead of MCMC for inference. There are two main challenges when using Bayesian HMM for POS induction. (1) Selection of Hidden State Number . In most HMM systems, the number of hidden (2) Processing Rare and Unknown Words . The second problem is there exist some POS induction can be formulated as a sequence labeling task. Given a sentence W = w w T = t 1 t 2 ... t n , where t i is the label for w i ,1  X  i  X  n .

HMM allows us to study the connections in observed events (input words) and hid-den states (tags). HMM aims to find the optimal tag sequence given the input word sequence, which maximizes the conditional probability p ( T | W ): argmax By applying Bayes X  rule, the term p ( W ) is eliminated. To make the problem practica-ble, the Markov assumption is used to state the transitions between hidden nodes. In the first-order Markov model, the probability of a particular state depends only on the previous one state: The second assumption simplifies the term P ( W | T ) by stating that each word is gen-erated by the current hidden state: Applying the above two assumptions, the objective function becomes: at state t i .

A variant of EM algorithm for HMM called Baum-Welch algorithm [Baum et al. 1970; Welch 2003] could be used to update the transition and emission probabili-ties by iteratively operating the E and M steps. At each iteration, the probabilities of the parameters that are on the paths traversed more by the training data are in-creased and the probabilities of other parameters are decreased. Then the posterior probability is recalculated using the revised parameters. The convergence of the pro-cess is theoretically guaranteed [Duda et al. 2000]. The algorithm is shown here.  X  EStep . Forward-Backward algorithm is to find the optimal hidden state sequence.  X  MStep . Update model parameters given the new optimal state path.
 The following section will give the inference details for Bayesian HMM. 2.2.1. Framework of Bayesian HMM. As aforementioned, the most common approach to unsupervised POS tagging is to train a HMM on an unannotated corpus using the Baum-Welch algorithm so that the likelihood of the corpus is maximized. As depicted in Figure 1, HMM simultaneously generates an output sequence W =( w 1 , ..., w n )and hidden state sequence T =( t 1 , ..., t n ). In the context of POS induction, each observed word w k was generated by a discrete hidden state t k , and the hidden state (POS tag) se-quence T was generated by a first-order Markov process. The complete-data likelihood of sequence is given by Equation (4).

For simplicity, all the parameters are assumed to be stationary, and the number of hidden states and observations are defined beforehand. The set of parameters are represented by: Suppose the set of all possible hidden states and observations are represented by T and W respectively. The parameters are as follows.
 which satisfy the following constraints: Once the parameters are learnt, the most likely hidden state sequence for the test case can be found with the Viterbi algorithm. The probability of the observation W are the sum of all possible hidden state sequences,
In Bayesian HMM, the transition and emission probabilities are specified as follows: where A i and B i represent the i-th column vector of A and B, individually. In this arti-cle, the symmetric Dirichlet priors are introduced over the multinomial distributions: where Dir ( x |  X  ) represents the Dirichlet distribution of order N with hyper-parameter vector  X  , defined as:
Then we can derive the following equations: p (  X  | W ) is the appropriate marginal of p (  X , T | W ). The variational methods seek a sim-pler distribution to approximate the complex quantity p (  X , T | W ). The simpler dis-tribution is denoted by q . According to the Jensen X  X  inequality, the lower bound of observed-data log-likelihood can be calculated by Equation (13). The lower bound F follows the fact that the log function is concave and is a func-tion of the free distribution q T and the parameters  X  . Its negative value is also called Variational Free Energy . In addition, we have: It means the maximization of the lower bound F also minimizes the Kullback-Leibler (KL) divergence between q (  X , T )and p (  X , T | W ). In the extreme case, the KL divergence is zero when q (  X , T )= p (  X , T | W ). To make inference calculation flexible, the posterior distribution can be assumed to be factorized according to the mean-field assumption:
The lower bound F can then be maximized with respect to q  X  and q T . The following part will give an EM-like algorithm to find the optimal hyper-parameters in Bayesian HMM with Dirichlet prior.
 Equation (12). We have:
According to the factorized posterior Equation (15), the lower bound becomes:
A further assumption on the dependencies between hidden variables ( A , B and  X  ) yields:
Now we can modified the Baum-Welch algorithm to optimize q T ( T ), q ( A ), q ( B )and q (  X  ), which was firstly proposed in Beal [2003]. In general, the E-step for Variational Bayesian inference is the same as the traditional EM, while the M-steps need a minor modification as explained in the following.  X  EStep . The step is to find the optimal hidden state sequence while keeping model parameters unchanged. Take the derivatives of the lower bound F (Equation (16)) with respect to the hidden state T yields: where the superscript notation ( k ) denotes the iteration number. In realization, the optimal hidden states are calculated with the Forward-Backward algorithm.  X  MStep . This step is to find the optimal model parameters given the hidden states found in the previous E step. Taking the derivatives of F with respect to  X  and equating them to zero yields
Because the log-likelihood can be decomposed into a sum of separate contributions involving A , B and  X  , we can individually optimize each parameter of the HMM and obtain the update equations: where where  X  is the Kronecker- X  function, and E q state sequence.  X  The E and M steps are iteratively repeated until convergence. This section presents our two extensions to the fully Bayesian HMM and proposes a variational inference algorithm for learning parameters. These two extensions are expected to deal with two problems discussed in Section 1.1. 3.1.1. Motivation. In most circumstances, the number of hidden states in HMM should be predefined according to some knowledge about both the problem domain and em-pirical experience. This number will be fixed during the whole training and testing process. Therefore how to set the optimal hidden state number for a certain appli-cation or a set of training data is still a problem, because much research shows that varying the size of the hidden state space greatly affects the performance of HMM. In other words, the setting of the state set should be heavily dependent on the training corpus.

Generally there are two methods to adjust the state number: top-down and bottom-up methods. In the bottom-up methods [Brand 1999], the state number is initialized with a relatively large number. During the training, the states are merged or trimmed with a result of a small set of states. On the other hand, the top-down methods [Siddiqi et al. 2007] start from a small state set and split one or more states until no further improvement can be obtained. The former approaches require huge computational cost in deciding which states to be merged, which makes it impractical for applications with large state space. In this article, we focus on the top-down approaches to find the optimal hidden state set without greatly increasing the computational complexity. In contrast to Siddiqi et al. [2007], the hyper-parameters of Dirichlet distribution are optimized. Correspondingly, a new model evaluation criterion should be adapted to our Bayesian HMM. We will discuss the detailed implementations in the following section.
Because each hidden state associates with a POS tag, the splitting of a state can be regarded as the process of subcategorization or refinement. Similar idea can be found from the coarse-to-fine syntactic parsing, where one POS tag can be sub-categorized and divided into a number of sub-symbols. For example, Petrov et al. [2006] divided  X  X P X  into 37 sub-symbols labeling from NP 1 to NP 37 . They improved parsing perfor-mance by heavily splitting some symbols (like noun and verb) while keeping some rare symbols (such as list or SYM) of Treebank POS tags. From a linguistic point of view, different sub-categorizations correspond different linguistic (either syntactic or seman-tic) roles in the sentences for a given tag. Take personal pronouns (PRP) for example. PRP could be linguistically divided into three categories, roughly: nominative case, accusative case, and sentence-initial nominative case. These subsymbols strongly in-dicate their syntactic positions. The superlative adjectives (JJS) might be split into three categories, like most , least ,and largest corresponding to different parents NP, QP, and ADVP, respectively. The splitting becomes complex for frequently used tags, for example, NP, VP, PP, ADJP, ADVP, and so on. Then a grammar  X  X P  X  DT NN X  becomes  X  NP x  X  DT y NN z  X  in parsing, where x , y and z are integers. The latent anno-tation of syntactic tags can also be introduced in fields of machine translation [Petrov et al. 2008], POS disambiguity [Huang et al. 2009], etc.
 An example of desired splitting scheme is shown in Figure 2. In the left original HMM, 3 states associate with noun, verb and adjective respectively. In the right HMM, noun is divided into two types corresponding to being subject of a sentence and object of a transitive verb. Hence the number of hidden states becomes 4.

Following the above idea, the current problems are the selection of suitable states to be divided and inference algorithm. In the next section, we will give our adaptive Bayesian HMM with state splitting. 3.1.2. Framework of Our Adaptive Bayesian HMM. Let us give some additive notations to the previous Bayesian HMM definitions. Suppose a hidden state t is selected and by  X  t and not related parameters are denoted by  X   X  t . The set of all hidden states except t is represented by T  X  t . The set of observations emitted from t are represented by W t , others are represented by W  X  t .
 The top-down scheme used in our system to adaptively adjust the HMM is shown in Algorithm 1.

The process of our scheme is as follows. An initial HMM (with a small set of hid-den states) is optimized using the standard variational inference methods mentioned in Section 2.2.2. At step k , only one state is divided into two new states, producing | T ( k ) | + 1 candidate HMMs including the original (un-splitted) model. For each can-elements related to the split state are updated while others unchanged. Particularly, only two rows and columns in the matrix A and B are considered to be modified for one HMM candidate. Then the optimal HMM is selected as the one with the highest score. The iterations are repeated until convergence.

In the following, we will investigate the details in Algorithm 1. 3.1.3. Details of Adaptive Bayesian HMM. In framework 1, three important steps should be noted. (1) Binary Splitting . We use the binary splitting schema for incrementally determin-(2) Model Update . We present a partial Baum-Welch algorithm (as shown in (3) Model Selection . Then a criterion is required to measure the goodness of the can-3.2.1. Motivation. Past experiments have shown that it is very important to design a module to process unknown words. For English, unknown words tend to be proper nouns, but in contrast common nouns and verbs are main sources for Chinese unknown words. In this section, we will discuss how to deal with these unknown Chinese words.
Most Chinese POS taggers assume that the input sentence has been correctly seg-mented into a word sequence. This prerequisite is not assumed for English, because there are natural spaces between words. An important property of Chinese is the lack of morphological variations. No cues can be extracted from the surface form of the words. Thus the order or transition information contributes more to POS tagging. Unlike English, there is no prefix or suffix in Chinese language. It indicates that we cannot make use of some derivational and inflectional affixes and should design some-what different unknown word processing from English-like language. In English, the word with the suffix -ed is likely to be an verb appeared in the past tense. Suffix -able indicates the word has high chance of being an adjective. Moreover other morphologi-cal properties (e.g., no capitalization and no hyphenation) further complicate Chinese tagging problem.

Another observation is that the first and last character in a Chinese words are most informative [Huang et al. 2007]. In compound words, the same starting or ending character tends to be a strong indicator of the same word category. The reason comes from two construction methods, that is,  X  X oot-plus-affix X  and  X  X odifier-plus-stem X  com-pounding. A prefix example is that words  X   X  (speed up),  X   X  (process),  X   X  (make heavier) and  X   X  (add) share the same prefix  X   X  and might act as verb in sentences. And words like  X   X   X  (child),  X   X  (cowboy),  X   X  (schoolchild), and  X 
 X   X  (urchin) end with the same suffix  X   X  and usually function as noun. Other most frequent affixes include  X   X (suchas X   X  (good),  X   X  (bad), etc.),  X   X  (not), etc. In this sense, it is reasonable to incorporate these affixes into our model. The difficulty is the affix characters in Chinese are quite short (length = 1), sparse and ambiguous in POS identity. On the other hand, infixes are not informative in recognizing syn-tactic categories [Packard 2000]. In addition, infixes are hard to be segmented and recognized in Chinese, therefore we do not consider infixes in our work.

The second consideration in our model is to deal with ambiguity problem. An ob-servation is that the same word could belong to different categories varied with its surrounding contexts. However in a fixed context, the word should be classified to only one role (called uniqueness). It means that the word class is highly dependent on the context in which it appears. Therefore our model clusters the given word jointly with its surrounding context.

In summary, our model is expected to combine morphological and contextual infor-mation for POS induction. 3.2.2. Unknown Word Model for HMM. Once HMM is trained, one problem in testing is how to estimate the emission probabilities for unknown words. The goal of this section is to present a simple but effective way for estimation in terms of both morphological features and context distribution.

Let us restate the notations: in step k , word w k is emitted from hidden state t k . The one immediately following words. The affixes of w i are represented by prefix ( w k )and suffix ( w k ) respectively. To introduce these morphological features, geometric average 2 [Huang et al. 2007] is used because of its simplicity and effectiveness.
 where To prevent spurious affixes, we need to identify whether the character is likely to be a correct prefix or suffix. In fact, the searching is a exhausting step. To speed up and reduce redundant prefixes (or suffixes), a threshold is introduced to filter out those trivial affixes whose frequencies are less than the threshold. In our experiments, this threshold is set to 50. Given those unknown words in testing data set, an affix-based dictionary can be constructed by collecting all words (in training samples) sharing the same possible affixes. Then Equation (31) is applied on the unknown words.
On the other hand, P c ( w k | t k ) takes context distribution into consideration. An un-derlying hypothesis in most of language model [Clark 2003] for distributional simi-larity is that the words occurring in similar contexts behave similar syntactic roles. Therefore the similarity between words can be measured by Pointwise Mutual Infor-mation (PMI) between contexts.
 the occurrence probability of c i . Similarly, a context-based corpus can be constructed by setting the cutoff threshold to 10. Then we can define In all, the emission probability for unknown word w k is computed as shown below: 3.3.1. Clustering Evaluation Metrics. In unsupervised tasks, there is no golden results available. And the numbers and scales of output clusters might be varied with algo-rithms. It is difficult to directly measure the distance between clusters. In this article, we use information theoretic based metrics to quantify the information shared by two clusters. (1) VI (Variation of Information) . The most common information-based clustering met-(2) NMI (Normalized Mutual Information) . To standardize the measures to have fixed (3) Others . Rosenberg and Hirschberg [2007] proposed V-measure to combine two de-3.3.2. Dependency Model with Valence. Because POS tags are usually treated as the intermediate inputs for higher-level processing, for example, syntactic parsing or se-mantic analysis. It is more realistic to evaluate our POS tagger in syntactic parsing. Dependency parsing is to extract the dependency graph whose nodes are the words of the given sentence. The dependency graph is a directed acyclic graph in which every edge links from a head word to its dependent. Because we work on unsupervised meth-ods in this article, we choose the most common generative head-outward model (called Dependency Model with Valence, DMV) [Headden III et al. 2009; Klein and Manning 2004] for parsing.

Because the main purpose is to evaluate our clustering results rather than improve the parsing performance, we select the standard DMV model without extensions or modifications. Starting from the root, DMV generates the head, and then each head recursively generates its left and right dependents. In each direction, the possible de-pendents are repeatedly chosen until a STOP marker is seen. DMV use inside-outside algorithm for re-estimation. We choose the  X  X armonic X  initializer proposed in Klein and Manning [2004] for initialization. The valence information is the simplest binary value indicating the adjacency.

During evaluation, we directly use the induced syntactic categories from our model as the input samples of DMV throughout training and testing. The Chinese data set used in our experiments is from the CoNLL-2009 shared task [Haji  X  c et al. 2009], which is generated by merging the Chinese Treebank 6.0 [Xue et al. 2005] and Chinese Proposition Bank 2.0 [Xue and Palmer 2009] and then con-verting the constituent structure to a dependency graph. For POS tagging, Proposition Bank is not used which means semantic annotations are neglected. The data sources of the Chinese Treebank range from Xinhua newswire (mainland China encoded with simplified characters), Hong Kong news and Sinorama Magazine (Taiwan, traditional characters). The number of tokens in training, development, and test sets are 609,060, 49,620, and 73,153 respectively. All sentences in training set are used for training, whose maximum length is 242. Finally the gold POS tag set contains 41 labels. The hyper-parameters  X  and  X  are initialized with values 0.001 and 0.1 to favor the spar-sity in the transition and emission matrix.

In preprocessing, the digits (both Arabic and Chinese numbers) are replaced like the following three examples. All types of punctuation marks are treated equally in training and omitted in evalua-tion. And boundary markers are inserted before the leftmost and after the rightmost word.

In our dataset, there are 2563 distinct unknown words which appear in the test set but not in the training set. Table I gives some examples of unknown nouns and adjectives.
 The clustering results are first evaluated on different information-based metrics men-tioned in Section 3.3.1.

We performed some runs on our Adaptive HMM with different unknown words in-formation: AHMM-C just considers context similarity, whereas only morphological af-fixes are considered in AHMM-M. The overall system is denoted as AHMM, which uses the full information by Equation (35). The initial hidden state number is set to 20, which is smaller than the size of POS tagset of Chinese Treebank. The comparison among these three algorithms can reveal the importance of different linguistic knowl-edge, as shown in Figure 3.

After iterations, the converged number of hidden states is 247, 211 and 197, respec-tively. From Figure 3, we can see the number of clusters is reduced by introduction of more linguistic knowledge. All measures achieve better performance with the growing of state number. For VI metric, AHMM achieves the best performance (lowest value). An interesting observation is that the VI score is not monotonously decreased, because VI scores penalize larger numbers of clusters discovered. The advantage of AHMM can also be found in NMI scores (0.629) compared to either context (0.505) or morpho-logical considerations (0.407), which indicates higher mutual information to the gold results. Consider the effectiveness of applying two types of linguistic knowledge, the context information achieves better performance than morphological features.
Homogeneity scores prefer to assign more than one state to tokens which might belong to the same gold POS tag. Theoretically, assigning each data point into one dis-tinct cluster guarantees perfect homogeneity. For POS induction, higher homogene-ity is expected to split the same POS tags and assign subcategories to those tokens. In other words, we expect that a cluster like NP-x (where x indicates latent anno-tation) contains data points from the coarse tagset NP instead of other sets. From Figure 3(c), AHMM achieved best performance with highest homogeneity score (0.6197). The scores of AHMM-C (0.584) and AHMM-M (0.560) are comparable.
The scores of completeness metric is complicate. Theoretically, in a perfectly com-plete clustering solution, the clustering assigns all of those data points that are mem-bers of a single class to the same cluster. In other words, assigning all data points into one single cluster guarantees perfect completeness. Obviously, we focus on the sub-categorization of POS tags rather the merging. Therefore completeness is not our concern.

To test the clustering accuracies, two unsupervised POS taggers are selected for comparisons. (1) Clark 3 . [Clark 2003] (labeled with Clark) clustered words based on the distribution of contexts. They use the Ney-Essen algorithm which interactively moves each word from one cluster to another until no increase in likelihood can be achieved. It is a hard matching problem, that is, every word is assigned to only one cluster. The number of clusters should be set in advance and be the power of 2. The cluster number is fixed to 128 throughout the experiment. (2) We run EM training for the standard HMM without Dirichlet prior, but configuration with unknown process-ing (labeled with EMHMM). The hidden state number is also set to 128.

Table II shows the outcomes of different algorithms. To remove the effect of cluster numbers, the scores for 128 clusters in AHMM are also provided. The Clark per-formance is much worse because of its hard matching property. The VI scores for EMHMM is higher than AHMM by 0.855, from the contributions of Dirichlet priors.
Figure 4 gives the evolution of variational lower bound ( F ) with respect to the num-ber of hidden states. The convergence of our algorithms can be easily shown. We can also observe a drastic decrease in the lower bound on the states from 40 to 100. This experiment is to test our clustering outputs by evaluating on the unsupervised dependency parsing performance (DMV).

Our clustering results are used as training samples for DMV model. The test sentences are also annotated with our cluster before imported into the dependency parsing. In training, the sentences are restricted to those up to length 10 (excluding punctuation) 4 . For each number of hidden states, one corresponding DMV model is trained. The tests are carried separately on those individual models. Because our sys-tem is fully unsupervised, the results are evaluated with Unlabeled Attachment Score (UAS). It computes the fraction of words that are assigned to the correct parent (or no parent if the word is a root).
 Figure 5 shows the directed accuracies in terms of outputs of AHMM and AHMM-C. The gold reference (54 . 5%) is the performance using the gold POS tags (41 tokens). It is clear that the AHMM results are much better than the gold result, benefited from the sub-categorization of coarse tags. The highest AHMM-C result is just slightly worse than the gold reference. However, with the growth of POS set, the computational burden in DMV is greatly increased. This is the reason that we restrict the length of the sentences. In this article, we have proposed an adaptive HMM framework for unsupervised POS tagging and presented a variational inference for learning. Our underlying model first initializes a Bayesian HMM with a small number of hidden states. The optimal size of state set is found by continuously splitting one state into two new states until the variational lower bound is converged. In HMM, the emission probabilities for unknown words are estimated with knowledge from Chinese language. Two types of knowledge are borrowed: both morphological affixes and context distribution similarities can help our HMM tagger. Finally, experiments were carried out in the real Chinese corpus and showed the effectiveness of our tagger in terms of several clustering metrics and unlabeled attachment scores.

The major contribution is twofold: (1) A novel adaptive HMM can automatically infer the number of hidden states and learn the hyper-parameters in Bayesian HMM with Dirichlet priors. Our model comes from the linguistic considerations, i.e. sub-categorization of coarse lexical classes. (2) In terms of the individual properties of Chinese language, we present an unknown word processing module which takes both morphology and context knowledge into considerations. In the future, we consider how to exploit rich linguistically-motivated features for improvement [Berg-Kirkpatrick et al. 2010].

