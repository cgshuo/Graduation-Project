 a variation of the  X  X ag of words X  ( BOW ) approach, which represents the features in a document by the weighted occurrence frequencies of individual word in it. Apparently, the BOW approach is limited since it can only use the set of terms explicitly mentioned in the documents and ignores relationships between important terms that do not co-occur literally, e.g. terms as  X  X uma X  and  X  X ougar X  are concepts belonging to the same genus  X  X elines X , but they are not equal literally; so BOW approach doesn X  X  work at this circumstance. To overcome this kind of problem, several research works have been done to exploit ontology for content-based categorization of large document corpus. Hotho et al [1] utilizes WordNet , structured term ontology, to improve the BOW text representation. It adopts strategies which represent text documents with taking background knowledge into account to various extents. For instance, terms like  X  X uma X  and  X  X ougar X  are considered to be similar, because they both are subconcepts of the concept  X  X elines X  in WordNet . BOW , however, treats these two words as totally difference ones. Although experiment results showed some improvement in classification or clustering accuracy, WordNet is manually maintenance is painstaking. Therefore other research works take advantages of the world knowledge bases, whose coverage are more extensive than Wordnet , such as Open Directory Project ( ODP ) and Wikipedia  X  the largest human encyclopedia to date: Gabrilovich et al [2] applied feature generation techniques to text processing based on ODP ; Gabrilovich et al [3] empowered machine learning techniques to Wikipedia to enrich document representation. Many text classification experiments have confirmed better performance of the improved categorization methods, which classify documents with the help of backgrougd-knowledge-based features generated from ODP or Wikipedia , than that of BOW approach. thesauri as Wordnet , when enriching documents with features generated by ODP or Wikipedia , they are not as suitable in themselves as Wordnet to handle the problems of synonymy and polysemy, which are two fundamental problems in text categorization. To overcome these problems, Gabrilovich et al [2][3] perform feature generation using a multi-resolution approach: Features are generated for each document at the level of individual words, sentence, paragraph, and finally the entire document. This feature generation procedure acts similar to a retrieval process: it receives a text fragment (such as words, sentence, paragraph, or whole document) as input, and then maps it to the most relevant ODP categories or Wikipedia articles. This method, however, only leverages text similarity between text fragments and Wikipedia articles, ignoring the abundant structural information within Wikipedia as links. Then, the relevant ODP categories X  name or Wikipedia articles X  title retrieved by the former step are treated as new features to enrich document representation. Gabrilovich et al claim that their feature generation method implicitly performs words sense disambiguation: polysemous words within the context of a text fragment are mapped to the concepts which correspond to the sense shared by other context words. But, the processing effort of Gabrilovich X  X  method is too huge, since it has to scan each document many times. And based the principle of Information Retrieval, the feature generation procedure inevitably brings a lot of noise, because an article which contains part of input text doesn X  X  mean that it is relevant to the input text. Thus Gabrilovich X  X  method has to rely too much on feature selection to identify correct concepts and eliminate spurious ones, and too much noise makes feature selection less discriminating. Meanwhile, the implicit word sense disambiguation processing can only work at some cases (We will explain this in Sec. 4). informative encyclopedia thesaurus based on Wikipedia , which explicitly derives relationships between concepts of Wikipedia , including synonymy, polysemy, hyponymy and associative relation. So our thesaurus takes full advantages of the profuse structural knowledge of Wikipedia . The thesaurus serves as a controlled vocabulary that bridge the variety of idiolects and terminologies present in document corpus; and it may facilitate integration world knowledge in Wikipedia into text documents, since it resolves synonyms and introduces more general concepts which help identify related topics between text documents. Meanwhile, the thesaurus surpasses any manually constructed thesaurus like Wordnet in its coverage, and rivals them in its accuracy. Although Milne et al [5] have built a professional thesaurus of agriculture from Wikipedia , it is a domain-specific one, and However, the thesaurus we built is a more general one, which supports documents of extensive topics, not limited to any field. Beyond that, we then investigate a way to utilize our thesaurus, to facilitate integrating the rich semantic relations between Wikipedia concepts to text document represent. Meanwhile, when enriching documents, we try to expand only the most relevant concepts into documents and make explicit word sense disambiguation. Hence, our method won X  X  bring as much noise as [3] does, and makes feature selection more contributing. To come up with what beneficial effects can be achieved with this method, we have performed an empirical evaluation. We compared a simple baseline with different strategies of enriching text documents with Wikipedia knowledge to various extents. describes the related works. In Section 3, our method of building thesaurus from Wikipedia is discussed. We outline the algorithm of categorization with document enrichment in Section 4 before introducing our data set and evaluating our algorithm X  X  performance in Section 5. knowledge into text classification or other related tasks is quite few and the results are not good enough even worse. Buenaga Rodriguez et al. [16] and Urena Loez et al. [17] successfully integrate the WordNet resource for a document categorization task. They evaluate their methods on Reuters corpus, and show improved classification results of Rocchio and Widrow-Hoff algorithms. In contrast to our approach, [16] and [17] utilize WordNet to a supervised scenario, and do not use WordNet relations such as hypernyms, not mention associate terms we used in Wikipedia . Meanwhile, they build the term vectors manually. Dave et al. [17] has utilized WordNet synsets as features for document representation and subsequent clustering. He did not perform word sense disambiguation and found that WordNet synsets decreased clustering performance in his experiments. Hotho et al. [1] intergrate WordNet knowledge into text clustering, and investigate word sense disambiguation strategies and feature weighting schema by considering the hypernym relations from WordNet . The experimental evaluation on Reuters shows improvement compared with the best baseline. However, considering the few word usage contexts provided by WordNet , the word sense disambiguation effect is quite limited. Meanwhile, WordNet does not provide the associate terms as Wikipedia . render text classification systems with encyclopedic knowledge  X  Wikipedia . They first build an auxiliary text classifier that can match documents with the most relevant articles of Wikipedia , and then augment the conventional BOW representation with new features which are the concepts (mainly the titles) represented by the relevant Wikipeida articles. Empirical results show that this representation improve text categorization performance across a diverse collection of datasets. However, they do not make full use of the rich information in Wikipedia . Wikipedia is not merely a simple article collection; each article describes a single concept: its title is a succinct, well-formed phrase, and hyperlinks between articles capture many of semantic relations [6] such as hyponym, synonyms and associated terms. In addition, as described in [2], the feature generation process will bring much noise although the feature selection step can eliminate some extraneous features. free content encyclopedia written collaboratively by more than 10,000 regular editing contributors; its articles can be edited by anyone with access to its web site. Wikipedia is a very dynamic and quickly growing resource  X  articles about newsworthy events are often added within days of their occurrence. Today it ranks among one of the most-visited worldwide websites. The ways Wikipedia organizes its content are quite similar as the structure of traditional thesauri like WordNet , etc. English version, as of November 30, 2006, contains 4,197,766 articles with about 100 million internal hyperlinks. And each article in Wikipedia describes a single topic; its title is a succinct, well-formed phrase that resembles a term in a conventional thesaurus [5]. Meanwhile, each article must belong to at least one category of Wikipedia . Hyperlinks between articles keep many of the same semantic relations as defined in the international standard for thesauri [1], such as equivalence relation (synonymy), hierarchical relation (hyponym) and associative relation. a concept and identify several relations between concepts from Wikipedia  X  X  structure as described below. Such concepts and relations are just the building blocks of a thesaurus. named by a  X  X referred term X , and Wikipedia ensures that there is only one article for each concept by using  X  X edirect X  hyperlink to group equivalent concepts to the preferred one. A redirect page which only contains a redirect link exists for each alternative name of the concept that can be used to refer to the preferred one in Wikipedia . So synonymy in Wikipedia comes from redirect pages. For example,  X  X ord puma racing X  is the full name of the Ford Puma car. Therefore  X  X ord puma racing X  is an alternative name for  X  X ord puma X ; consequently, in Wikipedia the article of the title  X  X ord puma racing X  links to the article  X  X ord puma X . And that  X  X edirect X  link also copes with capitalization and spelling variations, abbreviations, synonyms, colloquialisms, and scientific terms. As an instance in [6], an example entry with a considerably higher number of redirect pages is  X  X nited States X . Its redirect pages correspond to acronyms (U.S.A., U.S., USA, US), Spanish translations (Los Estados Unidos, Estados Unidos), misspellings (Untied States) or synonyms (Yankee land). concepts, each already has a corresponding article in Wikipedia . And for each mentioned concept in it, a Wikipedia article usually links at least its first mention to the corresponding article by using hyperlink. The anchor text on each hyperlink may be different with the title of the linked article. Thus anchor texts are also synonymies of the concepts of linked articles. pages, which are created for ambiguous terms, i.e. terms that denote two or more entities. Such as, the term  X  X uma X  may refer to either a kind of animal or a kind of racing car or a famous sportswear brand. So, in Wikipedia , it provides disambiguation pages that present various possible meanings from which users could select articles corresponding to their intended concepts. For example, the disambiguation page for the term  X  X uma X  lists 22 associated concepts, from persons, vehicles to sport clubs. ambiguous term may also help users find intended meaning, i.e. the term  X  X uma X  yields several options, including  X  X uma X , a genus of large cats like Cougar and Jaguarundi,  X  X uma (car) X , a brand of Brazilian-made sports cars, and  X  X UMA AG X , a German shoe and sportswear company. Meanwhile, Wikipedia articles correspond to each meaning of an ambiguous term serve as detailed scope notes, since they fully describe the intended meaning of the term. categorization structure. For example, the article about animal  X  X uma X  has corresponding category  X  X elines X , which contains several more specific subcategories and articles, such as  X  X ats X  and  X  X ougar X . more than one category, i.e. the article of  X  X uma X  belongs to two categories:  X  X at stubs X  and  X  X elines X . These categories can be further categorized by associating them with one or more parent categories. So, the category structure of Wikipedia does not form a simple tree-structured taxonomy but a directed acyclic graph, in which multiple categorization schemes co-exist simultaneously [5]. Thus, for a concept in Wikipedia , we use the name of each ancestor category as its hyponymy. express relatedness between them. For example, there are hyperlinks from the article titled  X  X uma X  to the other articles such as  X  X elidae X ,  X  X ougar X  and  X  X aguarundi X ; some of the linked articles link back to the original one, as the article  X  X ougar X  links back to the article  X  X uma X . articles that are only tenuously related. For example, comparing the following two links: one from the article  X  X ougar X  to the article  X  X outh America X , the other from the article  X  X ougar X  to the article  X  X uma X ; it is clear that the former two articles are not as closely related as the later pair. So, how to measure the relatedness of hyperlinks within articles in Wikipedia is an important issue. Milne et al [5] only considers mutual cross links between articles as intended ones, casting away all other one-way links. However, their method is far too strict. Too many one-way links are discarded, for there are only 2,366,472 mutual links of total 13,947,302 hyperlinks in Wikipedia 1 . Meanwhile we found that a lot of one-way links are not tenuously related, and they should be retained. Here X  X  an example: there is only one-way link from article  X  X ata Mining X  to the article  X  X achine Learning X , no back link; and we know that  X  X ata Mining X  and  X  X achine Learning X  are two closely related concepts; therefore the one-way links between them should be considered. Thus, it is important to evaluate the relatedness of each hyperlink between Wikipedia articles and find out the most relevant ones for each article. Here, we introduce three kinds of measurements to rank links in an article of Wikipedia . Relatedness of two linked articles is evaluated by their contained terms: given the occurrence probability of each term in a corpus, the relatedness of two articles is modeled as the extent to which they share terms. Intuitively, if two text documents address to a similar topic, it is inevitable that these two documents may share some common substantive terms; whereas it is not possible that two irrelevant documents contain a lot of same words. Thus a text fragment could be represented as a term vector using TFIDF scheme. This link count statistic is calculated from the snapshot of 
Wikipedia on Nov. 30, 2006. To compute semantic relatedness of a pair of text fragments is just to compute the cosine similarity of their corresponding term vectors. Accordingly, the cosine similarity of TFIDF may reflect the relatedness of any pair of articles in Wikipedia . However the drawback of this measurement is the same as that of BOW approach, since they only consider terms appeared in text documents. We need synthesize other measurements together with this one. between a pair of Wikipedia articles is to compare out-linked categories of the two linked articles. Out-linked categories of an article are the categories that out-linked articles of the original article belong to. In Wikipedia , each article must belong to at least one category, and we found that if most of the out-linked categories of two articles focus on several same ones, the concepts described in these two articles are most likely strongly related. As for three related concepts,  X  X ata mining X ,  X  X achine learning X  and  X  X omputer Network X , 75 out linked articles from the article  X  X achine learning X  and 52 out linked ones from the article  X  X ata mining X  share 22 same categories; 24 out linked articles from the article  X  X ata mining X  and 39 out linked ones from the article  X  X omputer Network X  share 10 same categories; 23 out linked articles from the article  X  X achine learning X  and 20 out linked ones from the article  X  X omputer Network X  share 10 same categories. Table 1 shows part of the common out-linked categories shared by  X  Data mining X ,  X  X achine learning X  and  X  X omputer Network X  each Wikipedia article. Suppose there are f out-linked articles belonging to category c, we denote it as: f) olc(c, . The out-is )} f , olc(c , ), f , {olc(c
G , which is also a kind of vector, each entry in c out-linked category similarity to measure the relatedness of two linked articles. Since c similarity between out-linked category name vectors is applicable. So, the out-link category similarity olc linked articles is defined as: vectors of two linked articles. Apparently, the higher the out-linked category similarity of two articles the more relevant these two articles. As for three concepts mentioned above, the out-linked category similarity between  X  X ata mining X  and  X  X achine learning X  is 0.656; the similarity between  X  X ata mining X  and  X  X atabase X  is 0.213; and the similarity between  X  X achine learning X  and  X  X atabase X  is 0.157. It is very clear that  X  X ata mining X  is more related with  X  X achine learning X  than with  X  X omputer Network X  and the relation between  X  X omputer Network X  and  X  X achine learning X  is not as close as that between  X  X ata mining X  and  X  X achine learning X , which accord with our human comprehension. linked category similarity between two linked articles; and then, for each concept, we can rank all its out-linked concepts according to the out-linked category similarities of corresponding articles. straightforward edge counting method, which measures semantic distance as the number of nodes in the taxonomy along the shortest path between two conceptual nodes [7]. So, with the acyclic graph formed by the Wikipedia hierarchical categorization structure, we define the category distance of two articles on this graph: suppose article A 1 belongs to category C 1 and article A 2 belongs to category C category distance of A 1 and A 2 is the shortest path of C C on the categorization graph; and it is rational to say that the shorter the category distance the closer the relation of two articles. Accordingly, semantic relatedness is defined as the inverse score of the category distance. A normalized path-length measure taking into account the depth of the taxonomy in which the concepts are found is defined as: shortest path between the two nodes (as given by the edge counting method), and D is the maximum depth of the taxonomy. Hereby, we could judge the pertinence of two articles by evaluate their category distance. three methods, we can obtain three evaluation results of relatedness and then integrate them to get an overall relatedness evaluation result. We use linear combination of TFIDF similarity, out-linked category similarity and normalized category distance, which is: experiments. Later in Sec 5 we will explain how to adjust these weight parameters. linked articles according to the overall relatedness evaluation of corresponding links. Thus, we get a relatedness ranking on out-linked concepts of each concept, and we deem the out-linked concepts with relatedness above certain threshold as associative ones for each concept. huge mine of information about concepts and hierarchical and associative relations: concepts represent the basic units of meaning; relations between them serve humans to organize and share their knowledge. Our thesaurus derived from Wikipedia maintains its coverage and accuracy. It has been successfully exploited for content-based categorization of large document collections, yielding a more perspicuous representation of text document. approach only leverages the terms explicitly mentioned in text documents, thus fail to reflect relationships between important terms that do not co-occur literally. So integrating background knowledge to text documents may overcome the shortage of BOW approach. Moreover, Wikipedia is well-known for its most strength of containing much information about specific entities in the world, and such knowledge is not available through other electronic resources. Therefore, we build a general thesaurus from Wikipeida to exploit the background knowledge for text corpus. We first describe text document representation, then various strategies of background knowledge integration into the initial representation of text documents. weighted bags of terms. Let ) , ( t d TF be the absolute frequency of a term t  X  T in a document d  X  D , where D is the set of documents and } , , { different terms occurring in D . First, stopwords are removed from T using a standard stopwords list 2 , since stopwords are considered as non-descriptive terms within BOW approach. Then words in each document are stemmed using the Porter stemmer [8]. And the stemmed terms construct a vector representation denoted as ) ) , ( , ... ), , ( ( (term frequency-inverted document frequency) weighs the frequency of each term in a document with a factor that discounts its importance when it appears in almost all documents. The TFIDF of a term t in document d is defined as: See http://www.aifb.uni-karlsruhe.de/WBS/aho/clustering counts in how many documents where term t appears. After G G to integrate background knowledge into text documents. As shown in Figure 1, firstly based on a filtered Wikipedia concept index, we search candidate concepts mentioned in each text document, and then add synonymies, hyponymies and associative concepts of these candidate concepts into documents. Thus, new concepts are added according to the content of original documents, and their purpose is to enrich the representation of original text documents. Then added concepts are new features for original documents which can be leveraged for categorization. Thus, related documents which do not share common terms literally may be enriched with the same concepts, as related documents connote the same background knowledge and concepts are add according to content of a text document. In the rest part of this section, we will introduce the steps in detail. concepts using thesauri, the approach addresses two main problems of natural language processing  X  synonymy and polysemy [2]. Enriching text documents with concepts from thesauri has two benefits. First it resolves synonyms, and second it introduces more general concepts which help identifying related topics. For instance, a document about  X  X uma X  may not be related to another document about  X  X ougar X  by classification algorithm if there are only  X  X uma X  and  X  X ougar X  in each term vector. But in our thesaurus, concept  X  X uma X  and  X  X ougar X  belong to category  X  X elines X . So if the more general concept  X  X elines X  is added to both documents, their semantic relationship is revealed. concept, except some useless titles, such as  X  X ist of ISO standards X ,  X 1960s X  and so on. So before indexing Wikipedia concepts, it is necessary to remove improper titles. categories related to chronology, such as  X  X ears X ,  X  X ecades X  and  X  X enturies X  are removed. Second, because every title in Wikipedia must begin with a capital letter, we can judge whether a title is a concept by the following sequence of heuristic steps: capitalization of all words other than prepositions, determiners, conjunctions, or negations. If all the words are capitalized we considered it as a concept. article more than 3 times, we consider it as a concept. Give a word, the index will find out all Wikipedia concepts containing this word. are concepts holding multiple meanings, into the ambiguous concept set. For instance, the concept  X  X uma X  is an ambiguous one, since it may refer to a kind of animal, car or something else. concepts mentioned in documents. The concepts mentioned in text documents are called candidate concepts. We search candidate concepts in documents as the following steps: punctuations such as semicolon, interrogation, exclamatory point and full stop. window filtering condition described below. subsumed by other candidate concepts. searches candidate concepts by Front Maximum Matching algorithm, and requires that every word of a concept much appear in the sequence within a window of certain length. For concepts of less than three words, the window length is the number of words in the concept; for concepts of more than three words, the window length is 1 plus the number of words in the concept. length of the filtering window for the concept  X  X ord Puma X  is 2. As for the term sequences listed in Table 2, although all the four sequences mention  X  X ord Puma X , only the first sequence will introduce the concept  X  X ord Puma X  as a candidate one because it satisfies the window filtering condition: the two words  X  X ord X  and  X  X uma X  appear with the window of the length 2, and their order is consistent with their arrangement in the concept  X  X ord Puma X ; whereas the concept  X  X ord Puma X  won X  X  be considered as a candidate one for the other three sequences. must appear in a sequences within certain distance, which ensures the concept is truly mentioned in this document (consider the sentence  X  Harrison Ford , a famous actor, was in a suit of Puma sportswear.  X , although this sentence contains words  X  X ord X  and  X  X uma X , it doesn X  X  talk about the car  X  X ord Puma X ). the related concepts of each candidate concept into documents. The related concepts of a candidate concept include its synonymies, hyponymies and associative concepts. set, that is to say the candidate concept is a polysemous one, it is necessary to do word sense disambiguation to find its most proper meaning mentioned in documents. the first one is to utilize text similarity for disambiguation; the second is to disambiguate with context. measured by term overlap to perform explicit word sense disambiguation. For instance, the Reuters document #15264 talks about copper mining, but the concept  X  X opper X  in Wikipedia refers to several different meanings as listed in Table 3. The correct meaning of a polysemous concept may be found out by comparing the cosine similarity between TFIDF term vector of the text document and that of Wikipedia articles describing different meanings of the polysemous concept. As discussed in Sec. 3.1.4, the higher the cosine similarity of two TFIDF term vectors the more related these two text documents. Thus, for Wikipedia articles describing different meanings of a polysemous concept, the meaning described by the article with the highest TFIDF cosine similarity is considered as the most appropriate one. From Table 3, the Wikipedia article describing  X  X opper X  has the max similarity with the Reuters document #15264, and then it is confirmed that the term  X  X opper X  in document #15264 refers to the concept  X  X opper X  in Wikipedia , not other concepts as  X  X opper (color) X  or  X  X opper (I) oxide X . conceptual distance like Agirre et al [9]. We give an example first, and then describe this method in detail. meaning of the concept  X  X uma X  referred in it, a kind of car or animal? In this sentence there also mentioned other Wikipedia concepts, such as  X  X ougar X ,  X  X ountain Lion X  and  X  X elidae X , in which  X  X ougar X  and  X  X elidae X  are also polysemous concepts. However, in Wikipedia there is a  X  X edirect X  link from the concept  X  X ountain Lion X  to the concept  X  X ougar X . Then it X  X  clear that the concept  X  X ougar X  in this sentence refers to a kind of animal. And in Wikipedia the concepts  X  X ougar X ,  X  X uma X  and  X  X elidae X  belong to the same category  X  X elines X . Therefore, it is sure that  X  X ougar X ,  X  X uma X  and  X  X elidae X  all refer to a kind of animal in this sentence. So the meaning of  X  X uma X  is disambiguated by other concepts in the same context. 
The cougar , also known as the puma or mountain lion , is a New World mammal of the Felidae family.
 between concepts within a context. And the relations between concepts are represented by the structural information of Wikipedia . Since our thesaurus has integrated the structural information of Wikipedia into itself, it is convenient to leverage the thesaurus for disambiguation. Here is the conceptual distance function between any two concepts in the thesaurus. For a concept C 1 and another concept C 2 , their conceptual distance is defined as: between C 1 and C 2 , their conceptual distance is 1, otherwise is their category distance. In Table 4, the conceptual distance between  X  X ougar X  and  X  X ountain Lion X  is 1. calculate the conceptual distance for each meaning of the concept with other non-polysemy concepts mentioned in this sentence; and then compute the average conceptual distance of each meaning, which is: distance is considered as the most appropriate one. Wikipedia concept in them. So disambiguation with context is not always applicable for every polysemy. When disambiguation with context is not applicable, disambiguation with text similarity is adopted. If disambiguation with context is available, we take the average of two disambiguation results as combined result. Reuters-21578 discusses a joint mining venture by a consortium of companies, and belongs to the category  X  X opper X . This document mentions several concepts as  X  X opper X ,  X  X ining X  and  X  X eck Cominco X  (which is a Canadian mining company). Table 5 shows the hyponymies, associative concepts and synonymies introduced into this document by these concepts. hyponymies of a candidate concept into a text document, how many new concepts should be added? As for hyponymies, the direct hyponymies (which are the category names a concept directly belongs to) are most strongly related with a concept, as ancestor categories of the article corresponding to the concept are far too general; and the further the distance between ancestor categories and a Wikipedia concept the weaker their relations. For example, the concept  X  X uma X  belongs to the category  X  X elines X ; and the category  X  X elines X  belongs to the category  X  X arnivores X , which belongs to the category  X  X ammals X . The relation between  X  X uma X  and  X  X elines X  are much stronger than those between  X  X uma X  and  X  X arnivores X  or  X  X ammals X . Later, Sec 5.3 demonstrates the experiment results of adding different number of synonymies and hyponymies dumped on November 30, 2006. After decompression, the resulting XML file was 8.6GB in size. is easily obtainable. It is available in the form of database dumps that are released periodically, from several days to several weeks apart. The version used in this study was released on Nov. 30, 2006. The full content and revision history at this point occupy 70 GB of compressed data. We consider only the link structure and basic statistics for articles, which consume 1.9 GB (compressed). and redirections) that constitute the vocabulary of thesauri. These were organized into 120,000 categories with an average of two subcategories and 26 articles each. The articles themselves are highly inter-linked; each links to an average of 25 others. we got 627,255 concepts. Table 6 breaks down the data. the ModApte split (9603 training, 3299 testing documents) and two category sets, 10 largest categories and 90 categories with at least one training example and one testing example. contains 348,566 medical documents. Each document contains a title, and about two-thirds (233,445) also contain an abstract. Each document is labeled with an average of 13 MeSH3 categories (out of total 14,000). Following Joachims [13], we used a subset of documents from 1991 that have abstracts, taking the first 10,000 documents for training and the next 10,000 for testing. To limit the number of categories for the experiments, we randomly generated 5 sets of 10 categories each. 20 categories containing 1000 documents each. classification model is used to learn model to classify documents. We measured text categorization performance using the precision-recall break-even point (BEP). For the Reuters and OHSUMED datasets, we report both micro-averaged and macro-averaged BEP, since their categories differ in size substantially (micro-averaged BEP operates at the document level and is primarily affected by categorization performance on larger categories; whereas macro-averaged BEP averages results over categories, and thus small categories have large impact on the overall performance). Following established practice, we used a fixed data split for the Reuters and OHSUMED datasets, and consequently used macro sign test (S-test) [15] to assess the performance. For 20NG dataset, we performed 4-fold cross-validation, and used paired t-test to assess the significance. methods to measure the relatedness between Wikipedia concepts, and use linear combination of TFIDF similarity, out-linked category similarity and normalized category distance to merge the results of three methods. Here we introduce how to tune extract all the out-linked concepts in the Wikipedia articles corresponding to the 10 concepts. To obtain high quality ground truth for tuning, we asked three assessors to manually label all the linked concepts in the 10 articles to three relevance levels (relevant -3, neutral -2, and not relevant -1). The labeling process was carried out independently among assessors. No one among the three assessors could access the labeling results of others, who are graduate students and have good command of the English language. After labeling, each out-linked concept in the 10 articles is labeled with 3 relevance tags, and we use the average value as the final relatedness value. For example, if one user labels two linked concepts as neutral and the other two user label them as relevant, then the final relatedness of the pair of linked concepts is 1.67 ( (1+2+2)/3 ). Based on the labeled tuning data, we calculate TFIDF similarity, out-linked category similarity and normalized category distance between the 10 concepts and the concepts in these concept documents. We tune the value of to 1.0, and thus we can find the proper values of with which result of linear combination matches user evaluation result best. From experiments, and first find candidate concepts that mentioned in a text document, and then enrich documents of new concepts introduced by candidate concepts. We have considered different strategies: adding synonymies, adding hyponymies and adding associative concepts. And how many new concepts should be added? Here demonstrated the effect of classification with documents of adding different kinds of concepts and different number of concepts. augment with hyponymies. We first add the direct hyponymies (which are category names a candidate concept directly belongs to) for each candidate concepts, and then hyponymies of both first and second level (which are parent category names of the direct category a candidate concept belongs to), until hyponymies within 5 levels. In Table 7,  X  X aseline X  means we don X  X  add any concepts into documents; and  X  X 1 X  means adding direct hyponymies into documents; and  X  X 2 X  means adding hyponymies of both first and second level, so does for  X  X 3 X  to  X  X 5 X . Then we found that adding direct hyponymies and hyponymies within first two levels achieves the best result on categorization, and adding more hyponymies of further levels even deteriorates the classification result. associative concepts. For each candidate concepts, we append documents of its top 5, 10, 15, 20 and 25 most similar associative concepts.  X  X aseline X  still means we don X  X  add any concepts into documents; and  X  X 5 X  means adding 5 most associative concepts into documents, so does for  X  X 10 X  to  X  X 25 X . Likewise, we found that adding 5 or 10 most associative concepts brings best classification result, whereas adding more associative concepts even worse than the baseline. And overall, the result of adding associate concepts is better than that of adding hyponymies. bring as prominent improvement as former two strategies, as showed in Table 9. Since we can X  X  rank synonymies of a given candidate concept, we just add all its synonymies into documents, which inevitablely brings some noise into documents. As mentioned in Sec. 3.1.1,  X  X edirect X  link also copes with capitalization and spelling variations, abbreviations, synonyms, colloquialisms, and scientific terms. For instance, the document #15264 from Reuters-21578 is talking about copper mining, and the synonymies of the word  X  X opper X  in this document are  X  X opper (element) X ,  X  X opper band X ,  X  X opper sheet X ,  X  X opper sheet metal X ,  X  X uprous X ,  X  X uprum X ,  X  X opper mine X ,  X  X upper X ,  X  X upric X  and  X  X lement 29 X . We found that  X  X upper X  maybe a misprint of  X  X opper X , and  X  X upper X  should not be added. concepts together into documents and found out that, when adding into documents direct hyponymies and 5 most associative concepts for each candidate concept, this strategy achieves more improvement on categorization, as showed in Table 10. thesaurus from Wikipedia , and utilize the thesaurus to facilitate text categorization. Wikipedia is a huge resource of encyclopedia knowledge, and we mine relation of concepts from it to build our thesaurus. Then, we enrich documents with the related concepts of candidate concepts mentioned in documents. And when enriching documents, we perform explicit disambiguation to find out the correct meaning of each polysemous concept expressed in documents. By doing so, background knowledge can be introduced into documents, which remedies the shortage of BOW approach. And experiments demonstrate that our method brings a lot improvement. effect of adding sysnonymies by filtering  X  X edirect X  links. After removing useless redirect links such as spelling variations and keeping significative ones as sysnonyms and abbreviations, we think adding sysnonymies into text documents won X  X  bring as much noise as before, and its effect will be better. improved. Since the thesaurus has build a relation graph for each concept, which includes its synonymies, hyponymies and associative concepts, when disambiguation, the graph can be utilized. and our thesaurus only explores part of its resources. Other information in Wikipedia can be minded. For example, the link relation in Wikipedia is such a meaningful resource, and our thesaurus hasn X  X  taken advantage of anchor text. Since anchor texts of links are also synonymies of the titles of linked articles, our thesaurus can be expanded. Moreover, Wikipedia includes articles of many languages, so cross language information retrieval can be fulfilled. outweighed many traditional resources, and its exploitation is booming. improves text document clustering. In Proceedings of the Semantic Web Workshop at SIGIR X 03. for Text Categorization Using World Knowledge. In IJCAI X  05. brittleness bottleneck using Wikipedia: Enhancing text categorization with encyclopedic knowledge. In AAAI X 06. Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. In IJCAI X  07. Domain-Specific Thesauri from Wikipedia: A case study. In WI X 06. knowledge for named entity disambiguation. In EACL-06. semantic relatedness using Wikipedia. In AAAI X 06. Program, 14(3). 1980. pp. 130 X 137. Disambiguation using Conceptual Distance. In the Proceedings of the First International Conference on Recent Advances in NLP. 1995. http://www.daviddlewis.com/resources/testcollections/reuters 21578/. OHSUMED: An interactive retrieval evaluation and new large test collection for research. In SIGIR X 94, pp. 192 X 201. ICML X 95, pp. 331 X 339. machines: Learning with many relevant features. ECML X 98, pp. 137 X 142. text categorization. ACM Computing Surveys 34(1). 2002. pp.1 X 47. categorization methods. In SIGIR X 99, pp. 42 X 49. D  X  az-Agudo. Using WordNet to complement training information in text categorization. In Recent Advances in Natural Language Processing II, volume 189. John Benjamins. 2000. peanut gallery: opinion extraction and semantic classification of product reviews. In WWW X 03. 
