 ORIGINAL PAPER Morteza Valizadeh  X  Ehsanollah Kabir Abstract In this paper, we propose a new algorithm for the binarization of degraded document images. We map the image into a 2D feature space in which the text and back-ground pixels are separable, and then we partition this feature space into small regions. These regions are labeled as text or background using the result of a basic binarization algorithm applied on the original image. Finally, each pixel of the image is classified as either text or background based on the label of its corresponding region in the feature space. Our algorithm splits the feature space into text and background regions with-out using any training dataset. In addition, this algorithm does not need any parameter setting by the user and is appropriate for various types of degraded document images. The pro-posed algorithm demonstrated superior performance against six well-known algorithms on three datasets.
 Keywords Degraded document  X  Binarization  X  Mode association clustering  X  Structural contrast  X  Feature space partitioning 1 Introduction Document image analysis is an important field of image processing and pattern recognition. It consists of image capturing, binarization, layout analysis, and character recog-nition. Image binarization aims to convert a gray-scale image into binary and its quality affects the overall performance of document analysis systems. Although various thresholding algorithms have been developed, binarization of document images with poor and variable contrast, shadow, smudge, and variable foreground and background intensities is still a challenging problem.

The binarization methods reported in the literature are generally global or local. Global methods find a single thresh-old value by using some criteria based on the gray levels of the image. These methods compare the gray level of each pixel with this threshold and classify it as a text or back-ground pixel. Global thresholding based on clustering [ 1 ], entropy minimization [ 2 ], and valley seeking in the intensity histogram [ 3 ] as well as feature-based [ 4 ] and model-based [ 5 ] methods has been proposed. These methods are efficient for the images in which the gray levels of the text and back-ground pixels are separable. If histogram of the text overlaps with that of the background, they result in improper binary images.

To overcome the disadvantages of global methods, vari-ous local binarization methods have been presented. These methods use local information around a pixel to classify it as either text or background. We classify local binarization methods into two categories. The methods in the first cate-gory use the gray level of each pixel and its neighborhood and examine some predetermined rules to binarize the image locally [ 6  X  13 ]. The second category contains the methods that use local information to extract local features and apply a thresholding algorithm on them to obtain the binary image [ 14  X  17 ]. Finding appropriate threshold is an important stage of these methods.

Recently, some binarization algorithms have been pre-sented that work based on the classification methods. These algorithms use some training samples to improve the binari-zation results [ 11 , 18 , 19 ]. The main limitation of these algo-rithms is that they are efficient exclusively for the specific types of images included in the training set.
Finding a proper boundary or threshold for separating text and background pixels in the feature space is a challenging problem in binarization algorithms. Some methods minimize a global criterion to find a threshold [ 14 , 15 , 17 ], while the algorithm [ 16 ] uses two global attributes of the text and back-ground pixels, labeled using another binarization algorithm, to calculate an adaptive threshold. When the features of the text and background pixels are highly variable in different regions of the image, thresholding based on global criteria or global attributes leads to some errors.

In this paper, we use the mode-association clustering algo-rithm based on hill climbing to partition a 2D feature space into small regions. In this way, the feature space is parti-tioned in such a way that almost only the instances from either text or background pixels occupy a region, hence resulting many pure regions. Then, we employ the result of Niblack X  X  algorithm to classify these regions into text or background. Classifying a region in feature space instead of classifying its points individually makes our method robust against the errors of Niblack X  X  algorithm. Each pixel is then classified as either text or background according to its corresponding region in the feature space. Our algorithm is applicable for various types of degraded document images.

The rest of this paper is organized as follows. Section 2 briefly reviews some related works on local binarization methods and their drawbacks. Section 3 describes the pro-posed binarization algorithm. Experimental results and com-parison with some well-known algorithms are discussed in Sect. 4 , and conclusions are given in Sect. 5 . 2 Survey of local binarization algorithms In general, document image binarization algorithms are cat-egorized into global and local. Since local algorithms yield better results for degraded images, we concentrate on them.
Niblack proposed a dynamic thresholding algorithm that calculates a separate threshold for each pixel by shifting a window across the image [ 6 ]. The threshold T ( x , y ) for the center of window is computed using local information. T ( x , y ) = m ( x , y ) + ks ( x , y ) (1) where m ( x , y ) and s ( x , y ) are local mean and standard devia-tion in the window centered on pixel ( x , y ). The window size and k are the predetermined parameters of this algorithm. The value of k is set into  X  0.2. This method can separate text from background in the areas around the text, but wherever there is no text inside the local window, some parts of the background are regarded as text and background noise is magnified.
Sauvola solved the problems of Niblack X  X  method assum-ing that the text and background pixels have gray values close to 0 and 255, respectively [ 13 ]. He proposed a threshold criterion as follows: T ( x , y ) = m ( x , y ) [ 1  X  k ( 1  X  s ( x , y )/ R )) ] (2) where R is a constant set to 128 for an image with 256 gray levels and k is set into 0.5. This procedure gives satisfac-tory binary image in the case of high contrast between fore-ground and background. However, the optimal values of R and k are proportional to the contrast of the text. For poor-contrast images, if the parameters are not set properly, the texts regions are missed.

Chen proposed an algorithm for locally setting the binari-zation parameters [ 11 ]. This algorithm is implemented in two stages. In the first stage, a feature representing the region con-trast is extracted, and using this feature, the original image is decomposed into sub-regions. In the second stage, three fea-tures are extracted from each sub-region. These features are used to examine the sub-regions and classify them into four classes: background, faint strokes, heavy strokes, and heavy and faint strokes. For each sub-region, appropriate parame-ters are set according to its class.
 T ( x , y ) = w m ( x , y ) + kG N ( x , y ) where G region in the direction of stroke slant. The parameters w k are set experimentally and are not applicable to different types of images. Logical level thresholding technique uses not only the image gray level values but also the stroke width of the characters to improve the binarization quality [ 9 ]. This algorithm is based on the idea of comparing the gray level of each pixel with some local averages in its neighborhood. These comparisons need a threshold to produce some logical values, which are utilized to generate binary images. Yang [ 12 ] proposed an adaptive threshold calculation method to improve the logical-level technique, but this threshold is pro-portional to a predetermined parameter, so the quality of the final binary image depends on the parameter setting by the user.

Gatos estimated the background surface for the document image and compared the differences between the original gray levels and this surface with an adaptive threshold to label each pixel as either text or background [ 16 ]. To esti-mate the background surface, he used Sauvola X  X  binariza-tion algorithm to roughly extract the text pixels and for them calculated the background surface by interpolation of neigh-boring background pixels intensities. For other pixels, back-ground surface is set to the gray level of original image. In this algorithm, the average distance between foreground and background, the average background values of background surface and three predetermined parameter are utilized to cal-culate the adaptive threshold. It yields satisfactory results for various types of degraded images. However, for document images with uneven background, the parameters should be adapted to obtain better performance.

A stroke-model-based method uses the two attributes of a text pixel to extract characters [ 14 ]: (i) its gray level is lower than that of its neighbors and (ii) it belongs to a thin con-nected component with a width less than the stroke width. Based on these two attributes, the gray-scale image is mapped into a double-edge feature image. This mapping increases the separation of text and background pixels and a global thres-holding followed by a reliable post-processing extracts the text. In the double-edge image, the separability of the text and background depends on the contrast of the text in the original image. Global thresholding of double-edge image is not suit-able for the images with variable foreground and background intensities where the low-contrast texts are missed. 3 Proposed binarization algorithm The diagram of the proposed binarization algorithm is illus-trated in Fig. 1 . This algorithm consists of feature extrac-tion, feature space partitioning, partition classification, and finally pixel classification stages. The details of each stage are described in this section. 3.1 Feature extraction Feature extraction is one of the most important steps in pattern recognition applications. Mapping the objects into appropriate feature space leads to simple and accurate clas-sification or clustering algorithms. Therefore, we try to map the document image into a feature space in which the text and background pixels are separable. We propose a new feature named structural contrast and use it together with gray level in this application.

For document binarization, the most powerful features are those that take into account the structural characteristics of the characters. The stroke width is an important structural characteristic that helps us to extract reliable features. In log-ical-level technique [ 9 ], based on the stroke width and gray level of the image, for each pixel, eight logical values are generated. These values are placed in a logical rule to clas-sify each pixel as either text or background. In this work, we modify the logical-level technique to extract the structural contrast. Since this feature takes into account the structural characteristics of the text, it increases the discrimination of the text and non-text pixels. Suppose we want to extract the structural contrast, SC ,atpixel( x 0 , y 0 ) shown in Fig. 2 .Itis defined as follows: SC ( x 0 , y 0 ) = 3 max where G ( x 0 , y 0 ) represents the gray level of pixel ( x p kx , p ky are the coordinates of p k , p i denotes the stroke width of the characters determined auto-matically.

We use the structural contrast as the first feature in our application. Figure 3 shows a degraded image and its corre-sponding structural contrast feature.

In the conventional histogram based binarization algo-rithms, the gray level of each pixel is utilized as feature. Although, in degraded document images, this feature alone cannot separate the text and background pixels, it contains valuable information and using it beside structural contrast makes the text pixel more separable from background. There-fore, we use the gray level as the second feature in this work. The pixel ( x , y ) is mapped into feature space, f =[ f 1 where f 1 = SC ( x , y ) and f 2 = G ( x , y ) 3.1.1 Finding the stroke width Stroke width, SW , is a useful characteristic of the text. It is used to extract the feature SC . We used our previous method [ 20 ] to find SW automatically. This method computes the his-togram of the distances between two successive edge pixels in the horizontal scan. Suppose that h ( d ) is a one-dimensional array, denoting the distance histogram and d  X  X  2 ,..., L where L is the maximum distance to be counted. The SW is defined as the distance, d , with the highest value in h ( method finds the SW in degraded images more accurately than needed and satisfies our requirements. Experimental results showed that the tolerance of this method in finding SW is less than 20%, whereas we experimentally observed that our binarization algorithm works efficiently if SW is found with 40% tolerance. The details of this experiment are cited in Appendix 1 . 3.1.2 Discrimination power of the 2D feature space To illustrate the discrimination power of the proposed fea-tures, we map some randomly chosen pixels of a typical degraded image shown in Fig. 4 a into the 2D feature space and show them in Fig. 4 b. 3.2 Feature space partitioning In pattern recognition applications, clustering algorithms are utilized to group similar objects. In our work, we encounter large number of objects. For example, an image of 2 , 000 3 , 000 pixels contains 6,000,000 objects in the feature space. Clustering such a large number of objects is not a trivial task and is very time consuming. Instead of clustering the objects, we partition the feature space using the mode-association clustering technique [ 21 ]. Our partitioning algorithm con-sists of the following stages.

Histogram estimation: the feature space is divided into bins of equal size and the number of objects inside each bin is counted. In the 2D feature space, we define a 1  X  1 square as a bin.

Mode association clustering: starting from any point in the feature space, we use hill climbing algorithm to find the local maxima of the histogram. Those points that climb to the same local maximum are grouped into one partition or region. This algorithm partitions the feature space into N small regions in such a way that R = N i = 1 R i where R ={ f | H ( f )&gt; 0 } , R imum} and H ( f ) denotes the number of pixels in bin f . Figure 5 shows an example of the resulting regions in the feature space. 3.2.1 Purity of the regions In the proposed binarization algorithm, we partition the fea-ture space into many small regions and classify those regions. Text and background pixels inside a wrong region are incor-rectly classified in the final binarization stage. Therefore, the purity of regions is important in our application. Like other clustering algorithms, the mode-associated clustering does not guarantee that all the objects assigned to the specific cluster belong to the same class. However, because of the fol-lowing reasons, it results in relatively pure regions. (i) Text and background pixels are dense in the regions of the feature space associated with related classes while become sparse in the regions that lie between these classes. In most cases, there are clear gaps between text and background classes in the fea-ture space. During clustering, each point gradually climbs to a local maximum of the correct class and does not jump the gaps to an incorrect region. (ii) In this clustering algorithm, the number of clusters is not predetermined; instead, it is determined according to the distribution of the pixels in the feature space. This leads to a large number of small regions with high purity.

To examine our algorithm, we used 30 document images from three datasets and their ground truth to measure the purity of the regions experimentally. We first partitioned the feature space of each image into small regions and then counted the number of text and background pixels in each region. The purities of text and background regions are mea-sured as follows: PTR = i PBR = i where N t ( i ) and N b ( i ) denote the number of text and back-ground pixels inside the i th region. PTR and PBR represent the purities of text and background regions, respectively. This experiment yielded PTR = 95.7% and PBR = 99.2%, showing acceptable purity. Since manual labeling of the boundary pix-els of the text is subjective, some differences between ground truth and the results of our algorithm are negligible. 3.2.2 Complexity of clustering In our method, the complexity of clustering algorithm depends on the size of the feature space rather than the num-ber of pixels. To give a measure of complexity, we briefly explain the implementation of the clustering algorithm.
We start the algorithm from an arbitrary point in the fea-ture space and place a 5  X  5 window at this point. The max-imum value of histogram inside this window is found, and the searching window is moved to it. This process contin-ues until a local maximum is reached. The starting point, the local maximum, and all points in the path between them are labeled and are considered as a group. Then, we repeat this process starting from another point not labeled yet. However, we terminate the maximum search process when we reach either a new local maximum or a previously labeled point. If reaching a labeled point, the starting and all points in the path to the destination point are labeled same as the destination point. Otherwise, the starting point, the new local maximum, and the points in the path between them are labeled as a new group. This process continues until all points in the feature space are labeled.

Using this method, the searching window is placed on each point only once. We need 25 comparisons to find the maximum point inside each searching window and one com-parison to determine whether the central point is previously labeled or not. Our feature space has 256  X  256 points. So, the number of comparisons is 256  X  256  X  26. For an image with 2000  X  3000 resolution, the number of comparisons needed is 0.28 per pixel. Feature space partitioning consumes only a small portion of time needed for binarization of an image. 3.3 Region classification After partitioning the feature space into small regions, we use the result of an auxiliary binarization algorithm to classify each region as either text or background. In this way, an aux-iliary binary image is produced to generate the primary labels of pixels. To classify region R i , , the text and background pix-els of the auxiliary binary image, which their feature vectors lie in R i , are counted. Suppose N ab ( R i ) and N at ( number of pixels in R i labeled as background and text in the (b) auxiliary image, respectively. R i is classified as follows: Class ( R i ) = where Class( R i ) represents the class of R i . Although this method accurately classifies the partitions, using it for classi-fying the single points in the feature space leads to some clas-sification errors because there are some errors in the primary labels. This is the reason why we first partition the feature space and then classify the resulting regions instead of clas-sifying all single points in the feature space. 3.3.1 Choosing the auxiliary binarization algorithm Our region classification method performs well when we use an auxiliary binarization algorithm that correctly labels more than 50% of the pixels inside each region. We found that Niblack X  X  method is appropriate for this application. As men-tioned in Sect. 1 , Niblack X  X  algorithm extracts the text objects effectively but it classifies some parts of the background as text. The classification result of background pixels depends on the following conditions. 1. Both background and text pixels inside the sliding win-2. Only the background pixels exist inside the sliding win-
In the first condition, almost all of the background pix-els inside the sliding window are classified correctly. In the second condition, in most cases, the number of correctly clas-sified background pixels is larger than the number of back-ground pixels classified falsely. Therefore, more than 50% of background pixels inside a small area in the image are classified correctly. The background pixels inside each small area are almost similar and are expected to map into the same region in the feature space. Therefore, it is likely that more than 50% of the pixels inside each region in the feature space are classified correctly if the sliding window is small enough to maintain the local attributes of the image. To justify this, we carried out the following experiment.

We applied our partitioning algorithm over 30 document images and classified the small regions in the feature space manually. Then, we measured the percentage of correctly classified pixels using Niblack X  X  algorithm in each region as follows: TP = TN = where TP and TN are calculated for the text and background regions, respectively. The results of this experiment showed that Niblack X  X  algorithm correctly classifies more than 50% of the pixels, which lie in the same region (Fig. 6 ). Therefore, this algorithm satisfies our requirement. In our work, using a very large window for implementing Niblack X  X  algorithm leads to not eliminating some smudges, while a very small window may miss some large characters. We used a 60  X  60 sliding window, which is appropriate for a wide range of character size.

We use the Niblack X  X  algorithm to generate the primary labels and apply our region classification algorithm to sepa-rate the regions into text and background. Figure 7 illustrates the result of our region classification algorithm applied to the regions in Fig. 5 . 3.4 Final binarization We use the classification results of regions to binarize the document image. Suppose G ( x , y ) is mapped into [ f 1 , in the feature space where [ f 1 , f 2 ] X  R i . The binary image, B ( x , y ) , is obtained as follows: B ( x , y ) = In this way, we obtain a binarization algorithm that can deal with document images suffering from uneven background, shadows, non-uniform illumination, and low contrast. Partitioning the feature space into small regions and clas-sifying them rather than directly dividing the feature space into text and background regions are the main reason for the success of our algorithm. The binarization result of our algorithm applied to the image in Fig. 4 a is shown in Fig. 8 . 4 Experimental result We carried out a comparative study to evaluate the perfor-mance of our algorithm. In this experiment, Otsu X  X  global thresholding [ 1 ], Niblack X  X  local binarization [ 6 ], Sauvola X  X  local binarization [ 13 ], Oh X  X  algorithm [ 15 ], Ye X  X  algorithm [ 14 ], and Gatos X  method [ 16 ] were implemented, and the results obtained were utilized as benchmarks. For Sauvola X  X  algorithm, as recommended in [ 22 ] we set the parameter k into 0.34. Using this value yields better results in our exper-iments. Both visual and quantitative criteria were used in our evaluation. We used three datasets of document images. Each dataset corresponds to the specific degradations and demonstrates the efficiency of different algorithms dealing with those degradations. Dataset I is ICDAR 2009 Dataset [ 23 ], which includes 10 historical document images suffering from the degradations such as smear, smudge, variable fore-ground and background intensities, and bleeding through. In this dataset, most of the degradations are due to aging. Data-set II includes 10 document images selected from the Media Team Dataset [ 24 ]. The images of this dataset have degra-dations such as textured background, shadow through, vari-able foreground and background intensities, and low-contrast text. These images are not severely degraded, and most of binarization algorithms can binarize them with small errors. Dataset III involves 10 document images we captured under badly illumination conditions. The images in this dataset suf-fer from severely variable foreground and background inten-sities and low contrast due to image acquisition conditions. Binarization of these images is not a trivial task, and most algorithms fail to binarize them properly. 4.1 Visual evaluation We applied the binarization algorithms to document images of different datasets and visually compared the results. These experiments showed that, compared with six well-known algorithms, our algorithm yields superior performance on most of document images. 4.1.1 Experiment 1 In this experiment, the evaluation is carried out on the images of Dataset I. Figure 9 shows two examples of this exper-iment. The evaluation results are summarized as follows. Otsu X  X  algorithm regards some background regions as text and misses some texts. It yields the worst results. Niblack X  X  algorithm retains all texts but labels large number of noisy blobs as text. Oh X  X  algorithms miss the low-contrast texts. Sauvola X  X  algorithm labels some parts of background as text. Gatos X  and Ye X  X  algorithms result in binary images with small degradations. Our algorithm yields the best binary images in most cases. Only in the cases of severely bleeding-through degradation, our algorithm does not eliminate some bleeding-through texts and Sauvola X  X  algorithm outperforms ours. The binarization results of our binarization algorithm for 8 remaining images of Dataset I are given in Appendix 2 . 4.1.2 Experiment 2 In this experiment, Dataset II is utilized for evaluation. The images in this dataset are not highly degraded. There-fore, most of binarization algorithms can produce clean images with small degradations. We visually compared the results. Figure 10 shows a document image with shadow-through degradation and the binarization results. Niblack X  X  and Oh X  X  algorithms are not efficient dealing with this deg-radation and label many noisy blobs as text. Although other evaluated algorithms correct the shadow-through, they miss some details. For example, they fill the small holes of char-acters  X  X  X  and  X  X  X  in the word  X  X elefax X , magnified for better visualization. Our algorithm corrects this degradation while retaining the details. 4.1.3 Experiment 3 In this experiment, we used the images of Dataset III. There are degradations such as blurring, low-contrast text, and severely variable foreground and background intensities in these images. Figure 11 shows a low-contrast and blurred image and the binarization results of different algorithms. Using Otsu X  X  algorithm, the text is properly extracted from the background. However, it yields broken and touching char-acters in some parts of the image. Sauvola X  X  algorithm gives a binary image in which many characters are missed and some are broken. Niblack X  X  algorithm extracts many background blobs as text. In addition, this algorithm leads to highest amount of touching characters. Oh X  X  and Gatos X  X  algorithms result in similar binary images that have large number of bro-ken characters. Ye X  X  and our algorithms result in clean binary images.

Figure 12 shows a document image with non-uniform illu-mination and the binarization results of different algorithms. Otsu X  X  algorithm cannot extract the text from background, because the gray-level values of the text overlaps with those of the background. Sauvola X  X  algorithm assumes that the gray levels of text and background are close to 0 and 255, respec-tively. Low-contrast texts with gray levels close to 255 do not satisfy this assumption and are missed or broken up. Niblack X  X  algorithm labels many background blobs as text. Oh X  X , Ye X  X  and Gatos X  algorithms miss the text in the shadow area. Our algorithm results in a clean binary image. Dealing with document images captured with camera, our algorithm extremely outperforms other algorithms evaluated. 4.2 Quantitative evaluation by F-measure In this experiment, we used F-measure to compare the per-formance of binarization algorithms. This criterion is utilized in the first document image binarization contest [ 25 ]. It is defined as follows: F-measure = where Recall = TP TP + FN , Precision = TP TP + FP and TP , FP and FN denote true-positive, false-positive, and false-nega-tive values, respectively. We used all three datasets for eval-uation. The ground truth of Dataset I is available from [ 23 ], and the ground truths of two other datasets were manually generated. The comparison of 7 algorithms are given in Table 1 . Our algorithm yields the best results for all data-sets.

Comparing the results of our algorithm on Dataset I with the algorithms participated in DIBCO 2009 shows that our algorithms stands in the second step after the winner based on the F-measure. 4.3 Evaluation based on quality of text extraction We used the following five criteria to compare the binari-zation algorithms quantitatively. The criteria 1, 2, and 3 are chosen from reference [ 26 ], and the others are proposed by us. 1. Broken characters: characters with undesirable gaps 2. Connected characters: characters that touch the adjacent 3. Deformed characters: characters that are binarized inef-4. Missing characters: characters that cannot be extracted 5. Blob in background region: False object extracted as text.
We applied different binarization algorithms to the images of Dataset III and measured these criteria manually. In this experiment, the test images have 6,304 characters in total. The results are given in Table 2 .

The results of our evaluation are described as follows:  X  Otsu X  X  algorithm is not suitable for the images with  X  Niblack X  X  method does not miss the text, but it yields  X  Sauvola X  X  method eliminates the background properly but  X  Gatos X  algorithm uses three predetermined parameters  X  Ye X  X  algorithm maps the original image into the double- X  Oh X  X  algorithm uses a global threshold to extract regions  X  Our algorithm maps the pixels into an appropriate feature 5 Conclusion In this paper, we proposed a new algorithm for the binari-zation of document images. We presented a novel feature, structural contrast, and mapped the document image into a 2D feature space in which the text and background pixels are separable. Then, we partitioned this space into small regions. These regions were classified to text and background using the result of Niblack X  X  algorithm. Each pixel is then classi-fied as either text or background based on its location in the feature space. Our binarization algorithm does not need any parameter setting by the user and is appropriate for various types of degraded images. The main advantage of our algo-rithm against the previous classification-based algorithms is that it does not need any training dataset and uses the result of Niblack X  X  algorithm. In different experiments, the pro-posed algorithm demonstrated superior performance against six well-known algorithms on three datasets of degraded images.
 Appendix 1 In our work, stroke width, SW , is used as a structural char-acteristic of the text, and the accuracy of its value affects the efficiency of the feature SC . It is important to determine that how sensitive the binarization results are to the errors in the estimation of SW . We did the following experiment to mea-sure the quality of binarization algorithm for different values of error in the estimation of SW . We manually determined the SW of the text for 30 document images and used [  X   X  SW ] in our binarization algorithm instead of automatic estimation of SW , where 0 . 2  X   X   X  1 . 8 and [A] denotes the rounded value of A. F-measure was calculated for different values of  X  as shown in Fig. 13 . F-measure has small variation when  X   X  0 . 6 and decreases rapidly for  X &lt; 0 . 6. Visual inspec-tion of resulting binary images verified this experiment, and we observed that for 0 . 6  X   X   X  1 . 5, the binary images have almost same qualities. However, setting  X &lt; 0 . 6 breaks or misses some low-contrast characters, and setting  X &gt; fills small holes of characters and labels some small smudges in the images as text. Therefore, the efficiency of our algo-rithm decreases if stroke width estimation is off more than approximately 40%.
 Appendix 2 To give the readers more sense from the performance of our algorithm, we illustrate the results of our algorithm for the 8 remaining images of ICDAR 2009 dataset in Fig. 14 . Although SC is sensitive to character size, in these images our algorithm yields satisfactory results even for the docu-ments that have text with different font size, as in Fig. 14 e and f. In these images, the gray level discriminates the text from background. Therefore, our algorithm can handle these cases although SC is not a good feature for them.
 References
 ORIGINAL PAPER Morteza Valizadeh  X  Ehsanollah Kabir Abstract In this paper, we propose a new algorithm for the binarization of degraded document images. We map the image into a 2D feature space in which the text and back-ground pixels are separable, and then we partition this feature space into small regions. These regions are labeled as text or background using the result of a basic binarization algorithm applied on the original image. Finally, each pixel of the image is classified as either text or background based on the label of its corresponding region in the feature space. Our algorithm splits the feature space into text and background regions with-out using any training dataset. In addition, this algorithm does not need any parameter setting by the user and is appropriate for various types of degraded document images. The pro-posed algorithm demonstrated superior performance against six well-known algorithms on three datasets.
 Keywords Degraded document  X  Binarization  X  Mode association clustering  X  Structural contrast  X  Feature space partitioning 1 Introduction Document image analysis is an important field of image processing and pattern recognition. It consists of image capturing, binarization, layout analysis, and character recog-nition. Image binarization aims to convert a gray-scale image into binary and its quality affects the overall performance of document analysis systems. Although various thresholding algorithms have been developed, binarization of document images with poor and variable contrast, shadow, smudge, and variable foreground and background intensities is still a challenging problem.

The binarization methods reported in the literature are generally global or local. Global methods find a single thresh-old value by using some criteria based on the gray levels of the image. These methods compare the gray level of each pixel with this threshold and classify it as a text or back-ground pixel. Global thresholding based on clustering [ 1 ], entropy minimization [ 2 ], and valley seeking in the intensity histogram [ 3 ] as well as feature-based [ 4 ] and model-based [ 5 ] methods has been proposed. These methods are efficient for the images in which the gray levels of the text and back-ground pixels are separable. If histogram of the text overlaps with that of the background, they result in improper binary images.

To overcome the disadvantages of global methods, vari-ous local binarization methods have been presented. These methods use local information around a pixel to classify it as either text or background. We classify local binarization methods into two categories. The methods in the first cate-gory use the gray level of each pixel and its neighborhood and examine some predetermined rules to binarize the image locally [ 6  X  13 ]. The second category contains the methods that use local information to extract local features and apply a thresholding algorithm on them to obtain the binary image [ 14  X  17 ]. Finding appropriate threshold is an important stage of these methods.

Recently, some binarization algorithms have been pre-sented that work based on the classification methods. These algorithms use some training samples to improve the binari-zation results [ 11 , 18 , 19 ]. The main limitation of these algo-rithms is that they are efficient exclusively for the specific types of images included in the training set.
Finding a proper boundary or threshold for separating text and background pixels in the feature space is a challenging problem in binarization algorithms. Some methods minimize a global criterion to find a threshold [ 14 , 15 , 17 ], while the algorithm [ 16 ] uses two global attributes of the text and back-ground pixels, labeled using another binarization algorithm, to calculate an adaptive threshold. When the features of the text and background pixels are highly variable in different regions of the image, thresholding based on global criteria or global attributes leads to some errors.

In this paper, we use the mode-association clustering algo-rithm based on hill climbing to partition a 2D feature space into small regions. In this way, the feature space is parti-tioned in such a way that almost only the instances from either text or background pixels occupy a region, hence resulting many pure regions. Then, we employ the result of Niblack X  X  algorithm to classify these regions into text or background. Classifying a region in feature space instead of classifying its points individually makes our method robust against the errors of Niblack X  X  algorithm. Each pixel is then classified as either text or background according to its corresponding region in the feature space. Our algorithm is applicable for various types of degraded document images.

The rest of this paper is organized as follows. Section 2 briefly reviews some related works on local binarization methods and their drawbacks. Section 3 describes the pro-posed binarization algorithm. Experimental results and com-parison with some well-known algorithms are discussed in Sect. 4 , and conclusions are given in Sect. 5 . 2 Survey of local binarization algorithms In general, document image binarization algorithms are cat-egorized into global and local. Since local algorithms yield better results for degraded images, we concentrate on them.
Niblack proposed a dynamic thresholding algorithm that calculates a separate threshold for each pixel by shifting a window across the image [ 6 ]. The threshold T ( x , y ) for the center of window is computed using local information. T ( x , y ) = m ( x , y ) + ks ( x , y ) (1) where m ( x , y ) and s ( x , y ) are local mean and standard devia-tion in the window centered on pixel ( x , y ). The window size and k are the predetermined parameters of this algorithm. The value of k is set into  X  0.2. This method can separate text from background in the areas around the text, but wherever there is no text inside the local window, some parts of the background are regarded as text and background noise is magnified.
Sauvola solved the problems of Niblack X  X  method assum-ing that the text and background pixels have gray values close to 0 and 255, respectively [ 13 ]. He proposed a threshold criterion as follows: T ( x , y ) = m ( x , y ) [ 1  X  k ( 1  X  s ( x , y )/ R )) ] (2) where R is a constant set to 128 for an image with 256 gray levels and k is set into 0.5. This procedure gives satisfac-tory binary image in the case of high contrast between fore-ground and background. However, the optimal values of R and k are proportional to the contrast of the text. For poor-contrast images, if the parameters are not set properly, the texts regions are missed.

Chen proposed an algorithm for locally setting the binari-zation parameters [ 11 ]. This algorithm is implemented in two stages. In the first stage, a feature representing the region con-trast is extracted, and using this feature, the original image is decomposed into sub-regions. In the second stage, three fea-tures are extracted from each sub-region. These features are used to examine the sub-regions and classify them into four classes: background, faint strokes, heavy strokes, and heavy and faint strokes. For each sub-region, appropriate parame-ters are set according to its class.
 T ( x , y ) = w m ( x , y ) + kG N ( x , y ) where G region in the direction of stroke slant. The parameters w k are set experimentally and are not applicable to different types of images. Logical level thresholding technique uses not only the image gray level values but also the stroke width of the characters to improve the binarization quality [ 9 ]. This algorithm is based on the idea of comparing the gray level of each pixel with some local averages in its neighborhood. These comparisons need a threshold to produce some logical values, which are utilized to generate binary images. Yang [ 12 ] proposed an adaptive threshold calculation method to improve the logical-level technique, but this threshold is pro-portional to a predetermined parameter, so the quality of the final binary image depends on the parameter setting by the user.

Gatos estimated the background surface for the document image and compared the differences between the original gray levels and this surface with an adaptive threshold to label each pixel as either text or background [ 16 ]. To esti-mate the background surface, he used Sauvola X  X  binariza-tion algorithm to roughly extract the text pixels and for them calculated the background surface by interpolation of neigh-boring background pixels intensities. For other pixels, back-ground surface is set to the gray level of original image. In this algorithm, the average distance between foreground and background, the average background values of background surface and three predetermined parameter are utilized to cal-culate the adaptive threshold. It yields satisfactory results for various types of degraded images. However, for document images with uneven background, the parameters should be adapted to obtain better performance.

A stroke-model-based method uses the two attributes of a text pixel to extract characters [ 14 ]: (i) its gray level is lower than that of its neighbors and (ii) it belongs to a thin con-nected component with a width less than the stroke width. Based on these two attributes, the gray-scale image is mapped into a double-edge feature image. This mapping increases the separation of text and background pixels and a global thres-holding followed by a reliable post-processing extracts the text. In the double-edge image, the separability of the text and background depends on the contrast of the text in the original image. Global thresholding of double-edge image is not suit-able for the images with variable foreground and background intensities where the low-contrast texts are missed. 3 Proposed binarization algorithm The diagram of the proposed binarization algorithm is illus-trated in Fig. 1 . This algorithm consists of feature extrac-tion, feature space partitioning, partition classification, and finally pixel classification stages. The details of each stage are described in this section. 3.1 Feature extraction Feature extraction is one of the most important steps in pattern recognition applications. Mapping the objects into appropriate feature space leads to simple and accurate clas-sification or clustering algorithms. Therefore, we try to map the document image into a feature space in which the text and background pixels are separable. We propose a new feature named structural contrast and use it together with gray level in this application.

For document binarization, the most powerful features are those that take into account the structural characteristics of the characters. The stroke width is an important structural characteristic that helps us to extract reliable features. In log-ical-level technique [ 9 ], based on the stroke width and gray level of the image, for each pixel, eight logical values are generated. These values are placed in a logical rule to clas-sify each pixel as either text or background. In this work, we modify the logical-level technique to extract the structural contrast. Since this feature takes into account the structural characteristics of the text, it increases the discrimination of the text and non-text pixels. Suppose we want to extract the structural contrast, SC ,atpixel( x 0 , y 0 ) shown in Fig. 2 .Itis defined as follows: SC ( x 0 , y 0 ) = 3 max where G ( x 0 , y 0 ) represents the gray level of pixel ( x p kx , p ky are the coordinates of p k , p i denotes the stroke width of the characters determined auto-matically.

We use the structural contrast as the first feature in our application. Figure 3 shows a degraded image and its corre-sponding structural contrast feature.

In the conventional histogram based binarization algo-rithms, the gray level of each pixel is utilized as feature. Although, in degraded document images, this feature alone cannot separate the text and background pixels, it contains valuable information and using it beside structural contrast makes the text pixel more separable from background. There-fore, we use the gray level as the second feature in this work. The pixel ( x , y ) is mapped into feature space, f =[ f 1 where f 1 = SC ( x , y ) and f 2 = G ( x , y ) 3.1.1 Finding the stroke width Stroke width, SW , is a useful characteristic of the text. It is used to extract the feature SC . We used our previous method [ 20 ] to find SW automatically. This method computes the his-togram of the distances between two successive edge pixels in the horizontal scan. Suppose that h ( d ) is a one-dimensional array, denoting the distance histogram and d  X  X  2 ,..., L where L is the maximum distance to be counted. The SW is defined as the distance, d , with the highest value in h ( method finds the SW in degraded images more accurately than needed and satisfies our requirements. Experimental results showed that the tolerance of this method in finding SW is less than 20%, whereas we experimentally observed that our binarization algorithm works efficiently if SW is found with 40% tolerance. The details of this experiment are cited in Appendix 1 . 3.1.2 Discrimination power of the 2D feature space To illustrate the discrimination power of the proposed fea-tures, we map some randomly chosen pixels of a typical degraded image shown in Fig. 4 a into the 2D feature space and show them in Fig. 4 b. 3.2 Feature space partitioning In pattern recognition applications, clustering algorithms are utilized to group similar objects. In our work, we encounter large number of objects. For example, an image of 2 , 000 3 , 000 pixels contains 6,000,000 objects in the feature space. Clustering such a large number of objects is not a trivial task and is very time consuming. Instead of clustering the objects, we partition the feature space using the mode-association clustering technique [ 21 ]. Our partitioning algorithm con-sists of the following stages.

Histogram estimation: the feature space is divided into bins of equal size and the number of objects inside each bin is counted. In the 2D feature space, we define a 1  X  1 square as a bin.

Mode association clustering: starting from any point in the feature space, we use hill climbing algorithm to find the local maxima of the histogram. Those points that climb to the same local maximum are grouped into one partition or region. This algorithm partitions the feature space into N small regions in such a way that R = N i = 1 R i where R ={ f | H ( f )&gt; 0 } , R imum} and H ( f ) denotes the number of pixels in bin f . Figure 5 shows an example of the resulting regions in the feature space. 3.2.1 Purity of the regions In the proposed binarization algorithm, we partition the fea-ture space into many small regions and classify those regions. Text and background pixels inside a wrong region are incor-rectly classified in the final binarization stage. Therefore, the purity of regions is important in our application. Like other clustering algorithms, the mode-associated clustering does not guarantee that all the objects assigned to the specific cluster belong to the same class. However, because of the fol-lowing reasons, it results in relatively pure regions. (i) Text and background pixels are dense in the regions of the feature space associated with related classes while become sparse in the regions that lie between these classes. In most cases, there are clear gaps between text and background classes in the fea-ture space. During clustering, each point gradually climbs to a local maximum of the correct class and does not jump the gaps to an incorrect region. (ii) In this clustering algorithm, the number of clusters is not predetermined; instead, it is determined according to the distribution of the pixels in the feature space. This leads to a large number of small regions with high purity.

To examine our algorithm, we used 30 document images from three datasets and their ground truth to measure the purity of the regions experimentally. We first partitioned the feature space of each image into small regions and then counted the number of text and background pixels in each region. The purities of text and background regions are mea-sured as follows: PTR = i PBR = i where N t ( i ) and N b ( i ) denote the number of text and back-ground pixels inside the i th region. PTR and PBR represent the purities of text and background regions, respectively. This experiment yielded PTR = 95.7% and PBR = 99.2%, showing acceptable purity. Since manual labeling of the boundary pix-els of the text is subjective, some differences between ground truth and the results of our algorithm are negligible. 3.2.2 Complexity of clustering In our method, the complexity of clustering algorithm depends on the size of the feature space rather than the num-ber of pixels. To give a measure of complexity, we briefly explain the implementation of the clustering algorithm.
We start the algorithm from an arbitrary point in the fea-ture space and place a 5  X  5 window at this point. The max-imum value of histogram inside this window is found, and the searching window is moved to it. This process contin-ues until a local maximum is reached. The starting point, the local maximum, and all points in the path between them are labeled and are considered as a group. Then, we repeat this process starting from another point not labeled yet. However, we terminate the maximum search process when we reach either a new local maximum or a previously labeled point. If reaching a labeled point, the starting and all points in the path to the destination point are labeled same as the destination point. Otherwise, the starting point, the new local maximum, and the points in the path between them are labeled as a new group. This process continues until all points in the feature space are labeled.

Using this method, the searching window is placed on each point only once. We need 25 comparisons to find the maximum point inside each searching window and one com-parison to determine whether the central point is previously labeled or not. Our feature space has 256  X  256 points. So, the number of comparisons is 256  X  256  X  26. For an image with 2000  X  3000 resolution, the number of comparisons needed is 0.28 per pixel. Feature space partitioning consumes only a small portion of time needed for binarization of an image. 3.3 Region classification After partitioning the feature space into small regions, we use the result of an auxiliary binarization algorithm to classify each region as either text or background. In this way, an aux-iliary binary image is produced to generate the primary labels of pixels. To classify region R i , , the text and background pix-els of the auxiliary binary image, which their feature vectors lie in R i , are counted. Suppose N ab ( R i ) and N at ( number of pixels in R i labeled as background and text in the (b) auxiliary image, respectively. R i is classified as follows: Class ( R i ) = where Class( R i ) represents the class of R i . Although this method accurately classifies the partitions, using it for classi-fying the single points in the feature space leads to some clas-sification errors because there are some errors in the primary labels. This is the reason why we first partition the feature space and then classify the resulting regions instead of clas-sifying all single points in the feature space. 3.3.1 Choosing the auxiliary binarization algorithm Our region classification method performs well when we use an auxiliary binarization algorithm that correctly labels more than 50% of the pixels inside each region. We found that Niblack X  X  method is appropriate for this application. As men-tioned in Sect. 1 , Niblack X  X  algorithm extracts the text objects effectively but it classifies some parts of the background as text. The classification result of background pixels depends on the following conditions. 1. Both background and text pixels inside the sliding win-2. Only the background pixels exist inside the sliding win-
In the first condition, almost all of the background pix-els inside the sliding window are classified correctly. In the second condition, in most cases, the number of correctly clas-sified background pixels is larger than the number of back-ground pixels classified falsely. Therefore, more than 50% of background pixels inside a small area in the image are classified correctly. The background pixels inside each small area are almost similar and are expected to map into the same region in the feature space. Therefore, it is likely that more than 50% of the pixels inside each region in the feature space are classified correctly if the sliding window is small enough to maintain the local attributes of the image. To justify this, we carried out the following experiment.

We applied our partitioning algorithm over 30 document images and classified the small regions in the feature space manually. Then, we measured the percentage of correctly classified pixels using Niblack X  X  algorithm in each region as follows: TP = TN = where TP and TN are calculated for the text and background regions, respectively. The results of this experiment showed that Niblack X  X  algorithm correctly classifies more than 50% of the pixels, which lie in the same region (Fig. 6 ). Therefore, this algorithm satisfies our requirement. In our work, using a very large window for implementing Niblack X  X  algorithm leads to not eliminating some smudges, while a very small window may miss some large characters. We used a 60  X  60 sliding window, which is appropriate for a wide range of character size.

We use the Niblack X  X  algorithm to generate the primary labels and apply our region classification algorithm to sepa-rate the regions into text and background. Figure 7 illustrates the result of our region classification algorithm applied to the regions in Fig. 5 . 3.4 Final binarization We use the classification results of regions to binarize the document image. Suppose G ( x , y ) is mapped into [ f 1 , in the feature space where [ f 1 , f 2 ] X  R i . The binary image, B ( x , y ) , is obtained as follows: B ( x , y ) = In this way, we obtain a binarization algorithm that can deal with document images suffering from uneven background, shadows, non-uniform illumination, and low contrast. Partitioning the feature space into small regions and clas-sifying them rather than directly dividing the feature space into text and background regions are the main reason for the success of our algorithm. The binarization result of our algorithm applied to the image in Fig. 4 a is shown in Fig. 8 . 4 Experimental result We carried out a comparative study to evaluate the perfor-mance of our algorithm. In this experiment, Otsu X  X  global thresholding [ 1 ], Niblack X  X  local binarization [ 6 ], Sauvola X  X  local binarization [ 13 ], Oh X  X  algorithm [ 15 ], Ye X  X  algorithm [ 14 ], and Gatos X  method [ 16 ] were implemented, and the results obtained were utilized as benchmarks. For Sauvola X  X  algorithm, as recommended in [ 22 ] we set the parameter k into 0.34. Using this value yields better results in our exper-iments. Both visual and quantitative criteria were used in our evaluation. We used three datasets of document images. Each dataset corresponds to the specific degradations and demonstrates the efficiency of different algorithms dealing with those degradations. Dataset I is ICDAR 2009 Dataset [ 23 ], which includes 10 historical document images suffering from the degradations such as smear, smudge, variable fore-ground and background intensities, and bleeding through. In this dataset, most of the degradations are due to aging. Data-set II includes 10 document images selected from the Media Team Dataset [ 24 ]. The images of this dataset have degra-dations such as textured background, shadow through, vari-able foreground and background intensities, and low-contrast text. These images are not severely degraded, and most of binarization algorithms can binarize them with small errors. Dataset III involves 10 document images we captured under badly illumination conditions. The images in this dataset suf-fer from severely variable foreground and background inten-sities and low contrast due to image acquisition conditions. Binarization of these images is not a trivial task, and most algorithms fail to binarize them properly. 4.1 Visual evaluation We applied the binarization algorithms to document images of different datasets and visually compared the results. These experiments showed that, compared with six well-known algorithms, our algorithm yields superior performance on most of document images. 4.1.1 Experiment 1 In this experiment, the evaluation is carried out on the images of Dataset I. Figure 9 shows two examples of this exper-iment. The evaluation results are summarized as follows. Otsu X  X  algorithm regards some background regions as text and misses some texts. It yields the worst results. Niblack X  X  algorithm retains all texts but labels large number of noisy blobs as text. Oh X  X  algorithms miss the low-contrast texts. Sauvola X  X  algorithm labels some parts of background as text. Gatos X  and Ye X  X  algorithms result in binary images with small degradations. Our algorithm yields the best binary images in most cases. Only in the cases of severely bleeding-through degradation, our algorithm does not eliminate some bleeding-through texts and Sauvola X  X  algorithm outperforms ours. The binarization results of our binarization algorithm for 8 remaining images of Dataset I are given in Appendix 2 . 4.1.2 Experiment 2 In this experiment, Dataset II is utilized for evaluation. The images in this dataset are not highly degraded. There-fore, most of binarization algorithms can produce clean images with small degradations. We visually compared the results. Figure 10 shows a document image with shadow-through degradation and the binarization results. Niblack X  X  and Oh X  X  algorithms are not efficient dealing with this deg-radation and label many noisy blobs as text. Although other evaluated algorithms correct the shadow-through, they miss some details. For example, they fill the small holes of char-acters  X  X  X  and  X  X  X  in the word  X  X elefax X , magnified for better visualization. Our algorithm corrects this degradation while retaining the details. 4.1.3 Experiment 3 In this experiment, we used the images of Dataset III. There are degradations such as blurring, low-contrast text, and severely variable foreground and background intensities in these images. Figure 11 shows a low-contrast and blurred image and the binarization results of different algorithms. Using Otsu X  X  algorithm, the text is properly extracted from the background. However, it yields broken and touching char-acters in some parts of the image. Sauvola X  X  algorithm gives a binary image in which many characters are missed and some are broken. Niblack X  X  algorithm extracts many background blobs as text. In addition, this algorithm leads to highest amount of touching characters. Oh X  X  and Gatos X  X  algorithms result in similar binary images that have large number of bro-ken characters. Ye X  X  and our algorithms result in clean binary images.

Figure 12 shows a document image with non-uniform illu-mination and the binarization results of different algorithms. Otsu X  X  algorithm cannot extract the text from background, because the gray-level values of the text overlaps with those of the background. Sauvola X  X  algorithm assumes that the gray levels of text and background are close to 0 and 255, respec-tively. Low-contrast texts with gray levels close to 255 do not satisfy this assumption and are missed or broken up. Niblack X  X  algorithm labels many background blobs as text. Oh X  X , Ye X  X  and Gatos X  algorithms miss the text in the shadow area. Our algorithm results in a clean binary image. Dealing with document images captured with camera, our algorithm extremely outperforms other algorithms evaluated. 4.2 Quantitative evaluation by F-measure In this experiment, we used F-measure to compare the per-formance of binarization algorithms. This criterion is utilized in the first document image binarization contest [ 25 ]. It is defined as follows: F-measure = where Recall = TP TP + FN , Precision = TP TP + FP and TP , FP and FN denote true-positive, false-positive, and false-nega-tive values, respectively. We used all three datasets for eval-uation. The ground truth of Dataset I is available from [ 23 ], and the ground truths of two other datasets were manually generated. The comparison of 7 algorithms are given in Table 1 . Our algorithm yields the best results for all data-sets.

Comparing the results of our algorithm on Dataset I with the algorithms participated in DIBCO 2009 shows that our algorithms stands in the second step after the winner based on the F-measure. 4.3 Evaluation based on quality of text extraction We used the following five criteria to compare the binari-zation algorithms quantitatively. The criteria 1, 2, and 3 are chosen from reference [ 26 ], and the others are proposed by us. 1. Broken characters: characters with undesirable gaps 2. Connected characters: characters that touch the adjacent 3. Deformed characters: characters that are binarized inef-4. Missing characters: characters that cannot be extracted 5. Blob in background region: False object extracted as text.
We applied different binarization algorithms to the images of Dataset III and measured these criteria manually. In this experiment, the test images have 6,304 characters in total. The results are given in Table 2 .

The results of our evaluation are described as follows:  X  Otsu X  X  algorithm is not suitable for the images with  X  Niblack X  X  method does not miss the text, but it yields  X  Sauvola X  X  method eliminates the background properly but  X  Gatos X  algorithm uses three predetermined parameters  X  Ye X  X  algorithm maps the original image into the double- X  Oh X  X  algorithm uses a global threshold to extract regions  X  Our algorithm maps the pixels into an appropriate feature 5 Conclusion In this paper, we proposed a new algorithm for the binari-zation of document images. We presented a novel feature, structural contrast, and mapped the document image into a 2D feature space in which the text and background pixels are separable. Then, we partitioned this space into small regions. These regions were classified to text and background using the result of Niblack X  X  algorithm. Each pixel is then classi-fied as either text or background based on its location in the feature space. Our binarization algorithm does not need any parameter setting by the user and is appropriate for various types of degraded images. The main advantage of our algo-rithm against the previous classification-based algorithms is that it does not need any training dataset and uses the result of Niblack X  X  algorithm. In different experiments, the pro-posed algorithm demonstrated superior performance against six well-known algorithms on three datasets of degraded images.
 Appendix 1 In our work, stroke width, SW , is used as a structural char-acteristic of the text, and the accuracy of its value affects the efficiency of the feature SC . It is important to determine that how sensitive the binarization results are to the errors in the estimation of SW . We did the following experiment to mea-sure the quality of binarization algorithm for different values of error in the estimation of SW . We manually determined the SW of the text for 30 document images and used [  X   X  SW ] in our binarization algorithm instead of automatic estimation of SW , where 0 . 2  X   X   X  1 . 8 and [A] denotes the rounded value of A. F-measure was calculated for different values of  X  as shown in Fig. 13 . F-measure has small variation when  X   X  0 . 6 and decreases rapidly for  X &lt; 0 . 6. Visual inspec-tion of resulting binary images verified this experiment, and we observed that for 0 . 6  X   X   X  1 . 5, the binary images have almost same qualities. However, setting  X &lt; 0 . 6 breaks or misses some low-contrast characters, and setting  X &gt; fills small holes of characters and labels some small smudges in the images as text. Therefore, the efficiency of our algo-rithm decreases if stroke width estimation is off more than approximately 40%.
 Appendix 2 To give the readers more sense from the performance of our algorithm, we illustrate the results of our algorithm for the 8 remaining images of ICDAR 2009 dataset in Fig. 14 . Although SC is sensitive to character size, in these images our algorithm yields satisfactory results even for the docu-ments that have text with different font size, as in Fig. 14 e and f. In these images, the gray level discriminates the text from background. Therefore, our algorithm can handle these cases although SC is not a good feature for them.
 References
