 Peer reviewing of student writing has been widely used in various academic fields. While existing web-based peer-review systems largely save instruc-tors effort in setting up peer-review assignments and managing document assignment, there still remains the problem that the quality of peer reviews is of-ten poor (Nelson and Schunn, 2009). Thus to en-hance the effectiveness of existing peer-review sys-tems, we propose to automatically predict the help-fulness of peer reviews.

In this paper, we examine prior techniques that have been used to successfully rank helpfulness for product reviews, and adapt them to the peer-review domain. In particular, we use an SVM regression al-gorithm to predict the helpfulness of peer reviews based on generic linguistic features automatically mined from peer reviews and students X  papers, plus specialized features based on existing knowledge about peer reviews. We not only demonstrate that prior techniques from product reviews can be suc-cessfully tailored to peer reviews, but also show the importance of peer-review specific features. Prior studies of peer review in the Natural Lan-guage Processing field have not focused on help-fulness prediction, but instead have been concerned with issues such as highlighting key sentences in pa-pers (Sandor and Vorndran, 2009), detecting impor-tant feedback features in reviews (Cho, 2008; Xiong and Litman, 2010), and adapting peer-review assign-ment (Garcia, 2010). However, given some simi-larity between peer reviews and other review types, we hypothesize that techniques used to predict re-view helpfulness in other domains can also be ap-plied to peer reviews. Kim et al. (2006) used re-gression to predict the helpfulness ranking of prod-uct reviews based on various classes of linguistic features. Ghose and Ipeirotis (2010) further exam-ined the socio-economic impact of product reviews using a similar approach and suggested the useful-ness of subjectivity analysis. Another study (Liu et al., 2008) of movie reviews showed that helpful-ness depends on reviewers X  expertise, their writing style, and the timeliness of the review. Tsur and Rappoport (2009) proposed RevRank to select the most helpful book reviews in an unsupervised fash-ion based on review lexicons. However, studies of Amazon X  X  product reviews also show that the per-ceived helpfulness of a review depends not only on its review content, but also on social effects such as product qualities, and individual bias in the presence of mixed opinion distribution (Danescu-Niculescu-Mizil et al., 2009).

Nonetheless, several properties distinguish our corpus of peer reviews from other types of reviews: 1) The helpfulness of our peer reviews is directly rated using a discrete scale from one to five instead of being defined as a function of binary votes (e.g. the percentage of  X  X elpful X  votes (Kim et al., 2006)); 2) Peer reviews frequently refer to the related stu-dents X  papers, thus review analysis needs to take into account paper topics; 3) Within the context of edu-cation, peer-review helpfulness often has a writing specific semantics, e.g. improving revision likeli-hood; 4) In general, peer-review corpora collected from classrooms are of a much smaller size com-pared to online product reviews. To tailor existing techniques to peer reviews, we will thus propose new specialized features to address these issues. In this study, we use a previously annotated peer-review corpus (Nelson and Schunn, 2009; Patchan et al., 2009), collected using a freely available web-based peer-review system (Cho and Schunn, 2007) in an introductory college history class. The corpus consists of 16 papers (about six pages each) and 267 reviews (varying from twenty words to about two hundred words). Two experts (a writing instructor and a content instructor) (Patchan et al., 2009) were asked to rate the helpfulness of each peer review on a scale from one to five (Pearson correlation r = 0 . 425 , p &lt; 0 . 01 ). For our study, we consider the average ratings given by the two experts (which roughly follow a normal distribution) as the gold standard of review helpfulness. Two example rated peer reviews (shown verbatim) follow: As shown in Table 1, we first mine generic linguistic features from reviews and papers based on the results of syntactic analysis of the texts, aiming to replicate the feature sets used by Kim et al. (2006). While structural, lexical and syntactic features are created in the same way as suggested in their paper, we adapt the semantic and meta-data features to peer reviews by converting the mentions of product properties to mentions of the history topics and by using paper ratings assigned by peers instead of product scores. 1
In addition, the following specialized features are motivated by an empirical study in cognitive sci-ence (Nelson and Schunn, 2009), which suggests that students X  revision likelihood is significantly cor-related with certain feedback features, and by our prior work (Xiong and Litman, 2010; Xiong et al., 2010) for detecting these cognitive science con-structs automatically:
Cognitive-science features (cogS) : For a given review, cognitive-science constructs that are signifi-cantly correlated with review implementation likeli-hood are manually coded for each idea unit (Nel-son and Schunn, 2009) within the review. Note, however, that peer-review helpfulness is rated for the whole review, which can include multiple idea units. 2 Therefore in our study, we calculate the dis-tribution of feedbackType values ( praise , problem , and summary ) ( kappa = . 92 ), the percentage of problems that have problem localization  X  X he pres-ence of information indicating where the problem is localized in the related paper X  ( kappa = . 69 ), and the percentage of problems that have a solution  X  the presence of a solution addressing the problem mentioned in the review X  ( kappa = . 79 ) to model peer-review helpfulness. These kappa values (Nel-son and Schunn, 2009) were calculated from a sub-set of the corpus for evaluating the reliability of hu-man annotations 3 . Consider the example of the help-ful review presented in Section 3 which was manu-ally separated into two idea units (each presented in a separate paragraph). As both ideas are coded as problem with the presence of problem localization and solution , the cognitive-science features of this review are praise% =0, problem% =1, summary% =0, localization% =1, and solution% =1.

Lexical category features (LEX2) : Ten cate-gories of keyword lexicons developed for automat-ically detecting the previously manually annotated feedback types (Xiong et al., 2010). The categories are learned in a semi-supervised way based on syn-tactic and semantic functions, such as suggestion modal verbs (e.g. should, must, might, could, need), negations (e.g. not, don X  X , doesn X  X ), positive and neg-ative words, and so on. We first manually created a list of words that were specified as signal words for annotating feedbackType and problem localiza-tion in the coding manual; then we supplemented the list with words selected by a decision tree model learned using a Bag-of-Words representation of the peer reviews. These categories will also be helpful for reducing the feature space size as discussed be-low.

Localization features (LOC) : Five features de-veloped in our prior work (Xiong and Litman, 2010) for automatically identifying the manually coded problem localization tags, such as the percentage of problems in reviews that could be matched with a localization pattern (e.g.  X  X n page 5 X ,  X  X he section about X ), the percentage of sentences in which topic words exist between the subject and object, etc. Following Kim et al. (2006), we train our helpful-ness model using SVM regression with a radial ba-sis function kernel provided by SVM light (Joachims, 1999). We first evaluate each feature type in iso-lation to investigate its predictive power of peer-review helpfulness; we then examine them together in various combinations to find the most useful fea-ture set for modeling peer-review helpfulness. Per-formance is evaluated in 10-fold cross validation of our 267 peer reviews by predicting the absolute helpfulness scores (with Pearson correlation coeffi-cient r ) as well as by predicting helpfulness rank-ing (with Spearman rank correlation coefficient r s ). Although predicted helpfulness ranking could be di-rectly used to compare the helpfulness of a given set of reviews, predicting helpfulness rating is desirable in practice to compare helpfulness between existing reviews and new written ones without reranking all previously ranked reviews. Results are presented re-garding the generic features and the specialized fea-tures respectively, with 95% confidence bounds. 4.1 Performance of Generic Features Evaluation of the generic features is presented in Table 2, showing that all classes except syntac-tic (SYN) and meta-data (MET) features are sig-nificantly correlated with both helpfulness rating ( r ) and helpfulness ranking ( r s ). Structural fea-tures (bolded) achieve the highest Pearson ( 0 . 60 ) and Spearman correlation coefficients ( 0 . 59 ) (al-though within the significant correlations, the dif-ference among coefficients are insignificant). Note that in isolation, MET (paper ratings) are not sig-nificantly correlated with peer-review helpfulness, which is different from prior findings of product re-views (Kim et al., 2006) where product scores are significantly correlated with product-review help-fulness. However, when combined with other fea-tures, MET does appear to add value (last row). When comparing the performance between predict-ing helpfulness ratings versus ranking, we observe r  X  r s consistently for our peer reviews, while Kim et al. (2006) reported r &lt; r s for product reviews. 4 Finally, we observed a similar feature redundancy effect as Kim et al. (2006) did, in that simply com-bining all features does not improve the model X  X  per-formance. Interestingly, our best feature combina-tion (last row) is the same as theirs. In sum our results verify our hypothesis that the effectiveness of generic features can be transferred to our peer-review domain for predicting review helpfulness. 4.2 Analysis of the Specialized Features Evaluation of the specialized features is shown in Table 3, where all features examined are signifi-cantly correlated with both helpfulness rating and ranking. When evaluated in isolation, although specialized features have weaker correlation coeffi-cients ([0.43, 0.51]) than the best generic features, these differences are not significant, and the special-ized features have the potential advantage of being theory-based. The use of features related to mean-ingful dimensions of writing has contributed to va-lidity and greater acceptability in the related area of automated essay scoring (Attali and Burstein, 2006).
When combined with some generic features, the specialized features improve the model X  X  perfor-mance in terms of both r and r s compared to the best performance in Section 4.1 (the baseline). Though the improvement is not significant yet, we think it still interesting to investigate the potential trend to understand how specialized features cap-ture additional information of peer-review helpful-ness. Therefore, the following analysis is also pre-sented (based on the absolute mean values), where we start from the baseline feature set, and gradually expand it by adding our new specialized features: 1) We first replace the raw lexical unigram features (UGR) with lexical category features (LEX2), which slightly improves the performance before rounding to the significant digits shown in row 5. Note that the categories not only substantially abstract lexical information from the reviews, but also carry simple syntactic and semantic information. 2) We then add one semantic class  X  topic words (row 6), which en-hances the performance further. Semantic features did not help when working with generic lexical fea-tures in Section 4.1 (second to last row in Table 2), but they can be successfully combined with the lexi-cal category features and further improve the perfor-mance as indicated here. 3) When cognitive-science and localization features are introduced, the predic-tion becomes even more accurate, which reaches a Pearson correlation of 0.67 and a Spearman correla-tion of 0.67 (Table 3, last row). Despite the difference between peer reviews and other types of reviews as discussed in Section 2, our work demonstrates that many generic linguistic features are also effective in predicting peer-review helpfulness. The model X  X  performance can be alter-natively achieved and further improved by adding auxiliary features tailored to peer reviews. These specialized features not only introduce domain ex-pertise, but also capture linguistic information at an abstracted level, which can help avoid the risk of over-fitting. Given only 267 peer reviews in our case compared to more than ten thousand product reviews (Kim et al., 2006), this is an important con-sideration.

Though our absolute quantitative results are not directly comparable to the results of Kim et al. (2006), we indirectly compared them by ana-lyzing the utility of features in isolation and com-bined. While STR+UGR+MET is found as the best combination of generic features for both types of reviews, the best individual feature type is differ-ent (review unigrams work best for product reviews; structural features work best for peer reviews). More importantly, meta-data, which are found to signif-icantly affect the perceived helpfulness of product reviews (Kim et al., 2006; Danescu-Niculescu-Mizil et al., 2009), have no predictive power for peer re-views. Perhaps because the paper grades and other helpfulness ratings are not visible to the reviewers, we have less of a social dimension for predicting the helpfulness of peer reviews. We also found that SVM regression does not favor ranking over predict-ing helpfulness as in (Kim et al., 2006). The contribution of our work is three-fold: 1) Our work successfully demonstrates that techniques used in predicting product review helpfulness ranking can be effectively adapted to the domain of peer reviews, with minor modifications to the semantic and meta-data features. 2) Our qualitative comparison shows that the utility of generic features (e.g. meta-data features) in predicting review helpfulness varies be-tween different review types. 3) We further show that prediction performance could be improved by incorporating specialized features that capture help-fulness information specific to peer reviews.
In the future, we would like to replace the man-ually coded peer-review specialized features (cogS) with their automatic predictions, since we have al-ready shown in our prior work that some impor-tant cognitive-science constructs can be successfully identified automatically. 5 Also, it is interesting to observe that the average helpfulness ratings assigned by experts (used as the gold standard in this study) differ from those given by students. Prior work on this corpus has already shown that feedback fea-tures of review comments differ not only between students and experts, but also between the writing and the content experts (Patchan et al., 2009). While Patchan et al. (2009) focused on the review com-ments, we hypothesize that there is also a difference in perceived peer-review helpfulness. Therefore, we are planning to investigate the impact of these dif-ferent helpfulness ratings on the utilities of features used in modeling peer-review helpfulness. Finally, we would like to integrate our helpfulness model into a web-based peer-review system to improve the quality of both peer reviews and paper revisions. This work was supported by the Learning Research and Development Center at the University of Pitts-burgh. We thank Melissa Patchan and Christian D. Schunn for generously providing the manually an-notated peer-review corpus. We are also grateful to Christian D. Schunn, Janyce Wiebe, Joanna Drum-mond, and Michael Lipschultz who kindly gave us valuable feedback while writing this paper.
