 Anan Banharnsakun, Booncharoen Sirinaovakul, Tiranee Achalakul n 1. Introduction
The Job Shop Scheduling Problem (JSSP) is a real-world problem in a field of production management. To survive in the modern competitive marketplace, which requires lower cost and shorter product life cycles, a corporation must respond quickly and precisely to the customer X  X  demands. Effective scheduling plays an important role in this adaptation.

JSSP is an optimization problem that can be described in terms of a set of jobs, each with one or more operations. The operations of a job have to be processed in a specified sequence on a specific set of machines. The time required for all operations to complete their processes is called the makespan . The objective of JSSP aims to minimize the makespan value.

Many approaches using both mathematical formulations and heuristic methods have been developed to solve this problem. For a JSSP situation of small size, mathematical formulations such as integer programming techniques ( Fisher, 1981 ) can be used to solve this problem in a reasonable computational time. However, Garey et al. (1976) provided a proof that this problem is
NP-complete, i.e. as the problem size increases, the computational time to find the best schedule using the mathematical formula-tion methods grows exponentially. To handle this issue, heuristic methods such as branch-and-bound ( Carlier and Pinson, 1989 ) have been considered.

Metaheuristics ( Yang, 2008 ) are one of many approximation methods widely used to solve practical optimization problems. In recent years, several algorithms employing a metaheuristic approach such as Genetic Algorithm (GA), Ant Colony Optimiza-tion (ACO), Particle Swarm Optimization (PSO), Bee Colony Optimization (BCO), and Artificial Bee Colony (ABC) have been applied to solve JSSP.

Watanabe et al. (2005) introduced a genetic algorithm (GA) with a modified crossover operator and a search area adaptation method for controlling the tradeoff balance between global and local searches. Goncalves et al. (2005) presented a hybrid method called Hybrid Genetic Algorithm (HGA). The method combined GA and local search based on a disjunctive graph model and a neighborhood approach to improve the solution. To enhance the performance of GA, Asadzadeh and Zamanifar (2010) proposed an agent-based parallel GA approach. This parallel approach is based on a coarse-grained model. The initial population is divided into sub-populations, and each sub-population is evolved separately. Communication between sub-populations is restricted to the migration of chromosomes. Heinonen and Pettersson (2007) developed a hybrid approach based on an Ant Colony Optimiza-tion algorithm (ACO) and a post-processing algorithm to enhance the ACO performance for solving the JSSP.

To improve the solution quality in JSSP, Tasgetiren et al. (2006) presented a hybrid method (PSO-VNS) based on the Particle Swarm Optimization (PSO) and the Variable Neighboring Search (VNS). To further improve efficiency of PSO, a new hybrid swarm intelligence algorithm (MPSO) consisting of particle swarm opti-mization, Simulated Annealing (SA) and a multi-type individual enhancement scheme was developed by Lin et al. (2010) . Ge et al. (2007) employed a high global search efficiency of PSO with a powerful ability to avoid being trapped in local minima of SA by introducing an algorithm called Hybrid Evolutionary Algorithm (HEA). Ge et al. (2008) exploited the capabilities of distributed and parallel computing in swarm intelligence approaches by proposing a computationally efficient algorithm for combining
PSO with an Artificial Immune System (AIS) to find the minimum makespan for Job Shop Scheduling.

Most of these aforementioned approaches aim to improve their algorithm X  X  performance by introducing a hybrid method.
They combined the advantageous features of each algorithm to improve both the local search and global search capability of their algorithms.
 Inspired by the decision making capability of bee swarms,
Chong and Low (2006) explored an evolutionary computation based on Bee Colony Optimization (BCO) to solve JSSP. The scheduling construction in this approach was done based on a state transition rule. Yao et al. (2010) presented an Improved
Artificial Bee Colony algorithm (IABC) to enhance the search performance of the original ABC for solving the JSSP. The mutation operation was also utilized in this method for exploring the search space and avoiding local optima.

In this paper, we propose a modified version of the ABC algorithm called Best-so-far ABC. The research aims to improve the solution quality, which is measured based on  X  X  X est X  X ,  X  X  X ver-age X  X ,  X  X  X tandard Deviation (S.D.) X  X , and  X  X  X elative Percent Error (RPE) X  X  of the objective value. The algorithm is presented and applied to solve the JSSP.
 This paper is organized as follows. Section 2 introduces the Best-so-far ABC Metaheuristic algorithm. Section 3 describes the JSSP. Section 4 proposes a mapping for applying the Best-so-far
ABC to the JSSP. Section 5 presents the experiments. Section 6 compares and discusses the performance results. Finally, Section 7 presents our conclusions. 2. The Best-so-far ABC Metaheuristic
The intelligent behavior of honey bees for seeking a quality food source in nature was the inspiration for the Artificial Bee
Colony (ABC) algorithm presented by Karaboga (2005) . The focus of this section is first on the introduction of ABC algorithm. Next, we explain the concepts of the ABC algorithm based on the Best-so-far method for solving the combinatorial optimization problems. 2.1. Bees X  foraging behavior
Foraging ( Biesmeijer and Seeley, 2005 ) is how honey bees find food sources. In this process, quality food sources are selected based on group decision making by the swarm. Independence and interdependence in collective decision making are important factors in this mechanism.

The bees independently evaluate the quality of different new candidate food sources on their own. However, the interdepen-dence among them makes them more attentive to candidate food sources discovered and advertised by others. Waggle dances, which are done by employed bees in the food source selection process are used to exchange information on new candidate food sources and to recruit unemployed bees to follow employed bees to their sources. Through this kind of information exchange and learning, the honeybee swarm manages to discover the highest quality food sources. 2.2. ABC algorithm
ABC algorithm takes concepts from this foraging process to discover good solutions in an optimization problem. Essential components in ABC modeled after the foraging processes are defined as follows:
Food Source represents a feasible solution in an optimization problem.

Fitness Value represents the profitability of a food source. For simplicity, it is represented as a single quantity associated with an objective function of a feasible solution.

Bee Agents is a set of computational agents. The agents in ABC are categorized into three groups: employed bees, onlooker bees, and scout bees. The colony is equally separated into employed bees and onlooker bees. Each solution in the search space consists of a set of optimization parameters, which represent a food source X  X   X  X  X ocation X  X . The number of employed bees is equal to the number of food sources, i.e. there would be one employed bee for each food source.

The employed bees will be responsible for investigating their food sources and sharing the information about these food sources to recruit the onlooker bees. The onlooker bees will make a decision to choose a food source based on this information.
A food source that has a higher quality will have a larger probability of being selected by onlooker bees. An employed bee whose food source is rejected as low quality by employed and onlooker bees will change to a scout bee to search randomly for new food sources. The details of the algorithm are as follows.
First, randomly distributed initial food source positions are generated. The objective function determines how good a solution is. It can be represented by
F  X  x
 X  , x i A R D , i A f 1 , 2 , 3 ... , SN g X  2 : 1  X  x is the position of a food source as a D-dimensional vector, F ( x is the objective function, and SN is the number of food sources.
After initialization, the population is subjected to repeated cycles of four major steps: updating feasible solutions by employed bees, selection of feasible solutions by onlooker bees, updating feasible solutions by onlooker bees, and avoidance of suboptimal solutions by scout bees. 2.2.1. Updating feasible solutions by employed bees
The position of the new feasible food source discovered by an employed bee is calculated from u  X  x
In Eq. (2.2), u ij is a new feasible solution that is modified from its previous solution value ( x ij ) based on a comparison with the randomly selected position from its neighboring solution ( x is a random number between [ 1,1] which is used to adjust the old solution to become a new solution in the next iteration. k
A {1,2,3 y , SN } 4 k a i and j A {1,2,3 y , D } are randomly chosen indexes. The difference between x ij and x kj is a difference of position in a particular dimension.

If a new food source ( u ij ) is better than an old food source ( x the old food source is replaced by the new food source. 2.2.2. Selection of feasible solutions by onlooker bees
When the employed bees return to their hive, they share information with the onlooker bees about candidate solutions they found. The onlooker bees select these solutions based on probability. Solutions of higher fitness have a larger chance of being selected by onlooker bees than ones of lower fitness. The probability that a food source will be selected can be obtained from
P  X  where fit i is the fitness value of the food source i, which is related to the objective function value ( F ( x i )) of the food source i . 2.2.3. Updating feasible solutions by onlooker bees
Based on the information obtained from the employed bees, the onlooker bees select their feasible food sources. The selected food sources are then updated using Eq. (2.2), i.e. an old food source is replaced by a new food source if the new food source is of a better quality. 2.2.4. Avoidance of suboptimal solutions by scout bees
This step is done by reassigning employed bees whose con-tributions are rejected as low quality to become scout bees who will randomly search for new solutions. The new random position chosen by the scout bee will be calculated from x  X  x min j  X  rand  X  0 , 1  X  x max j x min j  X  X  2 : 4  X  where x min j is the lower bound of the food source position in dimension j and x max j is the upper bound of the food source position in dimension j .

These four major steps mentioned above are repeated until an optimal solution is found or the number of iteration (cycle) reaches the termination criteria, MCN ( Karaboga and Basturk, 2007 ).

The processes of the ABC Metaheuristic can be shown in pseudo-code as Fig. 1 .

In a robust search process, exploration and exploitation must be carried out together. While the exploration process is related to the independent search for an optimal solution, exploitation uses existing knowledge to bias the search space. In the men-tioned ABC algorithm, the exploitation will be handled by employed and onlooker bees while the exploration will be maintained by scout bees. This mechanism enables ABC to have both the local and the global search ability. Based on this advantage, the ABC algorithm will be able to get out of a local optimum point in the search space and find the global optima better than other heuristic approaches such as Simulated Anneal-ing and Tabu Search that only have a local search ability. As a result, many researchers have applied the ABC algorithm to solve several science and mathematical applications ( Rao et al., 2008;
Singh, 2009; Sabat et al., 2010 ). The comparison results showed that the ABC algorithm performs better than other heuristic algorithms in terms of solution quality and computation efficiency.

Although the activities of exploitation and exploration are well balanced and help to mitigate both stagnation and premature convergence in the ABC algorithm, the convergence speed is still an issue in some situations. 2.3. Best-so-far ABC algorithm
The Best-so-far ABC is a modified version of the ABC algorithm proposed by Banharnsakun et al. (2011) to enhance the exploita-tion and exploration processes.

In the original algorithm, each onlooker bee selects a food source based on a probability that varies according to the fitness function. Then the new candidate food sources are generated by updating the onlooker solutions based on a single neighboring employed bee as shown in Eq. (2.2).

In the Best-so-far ABC method, all onlooker bees use the information from all employed bees to make a decision on a new candidate food source. Thus, the onlookers can compare information from all candidate sources and are able to select the best-so-far position. As a result, the Eq. (2.2) for onlooker bees is modified. The old food source will be updated to the new food source by comparing with the best-so-far food source rather than comparing with the neighboring food source. This change should make the Best-so-far ABC algorithm converge more quickly because the solution will be biased towards the best solution found so far. The new method used to calculate a new candidate food source for the onlooker bee is  X  x ij  X  F f b  X  x ij x bj  X  X  2 : 5  X  where n id is the new candidate food source for onlooker bee position i and dimension d , d  X  1,2,3, y D ; x ij is the selected food source position i in a selected dimension j ; F is the random number between 1 and 1; f b is the fitness value of the best food source so far; x bj is the position of the best-so-far source in a selected dimension j.

Although the best-so-far method can increase the local search ability compared to the original ABC algorithm, the solution is easily entrapped in a local optimum. In order to resolve this issue, an adjustable search radius for the scout bee was also introduced in the Best-so-far ABC algorithm. Based on the assumption that the food source of a scout bee will be far from the optimal food source in the first iteration and it will become closer to the optimal food source in later iterations, the scout bee in the Best-so-far ABC will randomly generate a new food source using Eq. (2.6) rather than using Eq. (2.4) u where u ij is a new feasible solution of a scout bee that is modified from the current position of an abandoned food source ( x is a random number between [ 1,1]. The value of o max and o represent the maximum and minimum percentage of the position adjustment for the scout bee, respectively. The value of o min are fixed to 1 and 0.2, respectively. These parameters were empirically chosen by the experimenter ( Banharnsakun et al., 2011 ). With these selected values, the adjustment of scout bee X  X  position based on its current position will linearly decrease from 100% to 20% in each experiment round.

Similar to the original ABC, the Best-so-far ABC algorithm is designed for solving numerical optimization problems and the algorithm utilizes Eqs. (2.5) and (2.6) for finding a solution in a continuous function domain. These equations cannot be used directly to solve problems in combinatorial optimization, while its solution is in a discrete domain. However, the algorithm can be expanded for this problem type with suitable modifications ( Karaboga and Akay, 2009 ).

There are several methods proposed to modify the ABC algorithm for solving discrete optimization problems. Singh (2009) employed a subset encoding to represent a solution in an artificial bee colony algorithm for solving the leaf-constrained minimum spanning tree problem. Pan et al. (2011) also proposed a discrete artificial bee colony (DABC) algorithm to minimize total weighted earliness and tardiness penalties for the lot-streaming flow shop scheduling problems. In our work, we introduce a set theory and a discrete job permutation to represent the solution.
We consider the search process in the Best-so-far ABC Meta-heuristic in terms of ( S , f , O ), where S is the set of candidate solutions, f is the fitness function, which assigns a fitness value f ( s ) to each candidate solution s A S and O is a set of constraints.
Solutions ~ s belonging to the set ~ S D S that satisfy O are called feasible solutions.

In our Best-so-far ABC, the initial solutions, which are the feasible solutions ( ~ s ) in the feasible search space ~ constructed and assigned to employed bees. After initialization, the artificial bees are subjected to repeated cycles of three major processes: updating feasible solutions, selecting feasible solu-tions, and avoiding suboptimal solutions.

In the process of updating feasible solutions, employed bees update their solutions with their neighbors by using the concept of Eq. (2.2). The old solutions in an employed bee X  X  memory are replaced with new solutions of higher fitness.

During the selection of feasible solutions ( ~ s ), each onlooker bee selects one of the proposed solutions depending on the informa-tion obtained from the employed bees. The probability of a solution being selected by an onlooker bee is proportional to its fitness as calculated by Eq. (2.3). After solutions are selected, the onlooker bees also update their selected solutions using the concept of Eq. (2.5) based on the best-so-far solution s b A f  X  s  X  Z f  X  ~ s  X  for all ~ s obtained from employed bees in each iteration.
Next, the old solutions in an onlooker bee X  X  memory are replaced with new solutions of higher fitness from the same step of employed bee.

In the process of avoiding suboptimal solutions, solutions that do not improve the fitness are replaced with new solutions randomly constructed by the scout bees. The concept to generate the new solution is based on the Eq. (2.6).

These three major processes are repeated until a globally optimal feasible food source s n A ~ S , where f  X  s n  X  Z is found or the number of iteration reaches the Maximum Cycle
Number (MCN). 3. Job Shop Scheduling Problem description
French (1982) described the Job shop Scheduling Problem (JSSP) as a set of n jobs denoted by J j where j  X  1,2, y have to be processed on a set of m machines denoted by M k where k  X  1,2, y , m. Operation of j th job on the k th machine will be denoted by O jk with the processing time P jk . Each job must be processed through all machines in a particular order also known as the technological constraint and processing time varies with each job. A machine can process only one job at a time. The required order of machines also varies from one job to another. Once a machine starts to process a job, no interruption is allowed.
The time required for all operations to complete their processes is called makespan. The objective of JSSP aims to minimize the makespan value. A solution can be expressed as
MinC max  X  max  X  C 1 , C 2 , C 3 , ... C n  X 
C  X  where C j is the completion time of job j ; w jk is the waiting time of job j at sequence k ; and p jm ( k ) is the processing time needed by job j on machine m at sequence k
The difficulty in finding good solutions to JSSP is due to the number of feasible solutions. For large problem size, the number of possible solutions makes it nearly impossible to explore the entire solution space. The total number of all possible solutions is ( n !) 4. Mapping Best-so-far ABC algorithm to the JSSP
We applied the Best-so-far ABC method to JSSP described above. The goal is to find a global optimization of the makespan, i.e. we try to find the job operation scheduling list that minimizes the makespan value. The adopted algorithm is illustrated in Fig. 2 . The steps of operation can be described as follows: The First Step
The initial parameters such as the number of employed bees, onlooker bees and maximum cycle number (MCN) are set.
Next, the job X  X  processing time on each machine and the job X  X  machine sequence will be given at this step. Examples can be found in Tables 1 and 2 , respectively.

In our solution representation, a solution in JSSP is an operation scheduling list, which is represented as a food source ( x )inour
Best-so-far ABC algorithm. Each dimension in a food source represents one operation of a job. Each job appears exactly m times in an operation scheduling list. For the n -job and m -machine problem, each food source contains n m dimensions correspond-ing to n m operations. This representation is illustrated in Fig. 3 .
For Fig. 3 , J i stands for the operation of job i . Since each job has three operations, it occurs three times in the operation scheduling list.

The interpretation of the example above is as follows. As we scan the operation scheduling list from left to right, the first J corresponds to the first operation of job J 1 , which will be processed on machine M 1 , the first J 2 corresponds to the first operation of job J 2 , which will be processed on M 2 , the first J corresponds to the first operation of job J 3 , which will be processed on M 3 , the second J 1 corresponds to the second operation of job J 1 , which will be processed on machine M second J 2 corresponds to the second operation of job J 2 be processed on M 3 ,andthesecond J 3 corresponds to the second operation of job J 3 , which will be processed on M 1 .Thus,the feasible schedule can be constructed as shown in Fig. 4 .
The fitness of each food source f ( x ) is determined by the inverse of its makespan value ( C max ( x )), which is calculated during the selection of feasible solutions. This mapping is shown in Fig. 5 . The Second Step
We employ the concept of the updating of a candidate food source based on a neighboring food source using the following equation u ij  X  x ij  X  | ij  X  x ij x kj  X  X  4 : 1  X  The candidate food sources are updated by employed bees.
Position Base Crossover (PBX) ( Kellegoz and Toklu, 2008 ) is the mechanism used to update the employed bees X  old food sources ( x ij ) to new food sources ( u ij ) based on their neighbor-ing food sources ( x kj ) randomly taken from other employed bees. An example of PBX is shown in Fig. 6 .

Based on the PBX method, a set of job operations from the old food source is selected randomly. Each dimension in the operation scheduling list of the old food source is selected to produce the new position with the probability of 0.5. This probability is empirically chosen by the experimenter to allow the new food source to be generated from both the old food source and the neighboring food source in the same propor-tion. The jobs already selected from the old food source are ignored and will not be selected again from the neighboring food source. The job operations on the neighboring food source that are not yet selected from the old food source will be selected and placed into empty positions from the left to the right of the operation scheduling list in the new food source.
To guarantee that each job will be included exactly m times in the new food source, if any job has already been selected for m times (from either the old or the neighboring food source), it will be skipped and the next job will be considered.

The old food source ( x ij ) in the employed bee X  X  memory will be replaced by the new candidate food source ( u ij ) if the new position has a better fitness value.
 The Third Step
The employed bees share new solutions ( u ij ) that they have found with the onlooker bees, who then select these solutions based on probability ( P i ) calculated from an equation below
P i  X  f  X  u i  X  P SN where f ( u i ) is the fitness value of the food source i and SN is the number of food sources.

After onlooker bees have selected the solutions ( x ij ) from employed bees, then the best solution ( x bj ) is identified from among those selected solutions. The best solution ( x bj ) will replace the best-so-far solution ( x bj of the previous iteration) if the new best solution is better and its fitness value ( f used for guiding the direction of the search space as shown in equation below n id  X  x ij  X  F f b  X  x ij x bj  X  X  4 : 3  X 
The PBX method is also used to update the old solution ( x the onlooker bees to the new solution ( u ij ) based on the best-so-far food source ( x bj ) instead of the neighboring food sources ( x
The old food source ( x ij ) in the onlooker bee X  X  memory will be replaced by the new candidate food source ( u ij ) if the new position has a better fitness value.
 The Fourth Step
To avoid suboptimal solutions, the scout bees ignore the old solution and randomly search for new solutions by using the concept of the equation below u
Based on this concept, the operation scheduling lists ( x whose fitness values have not improved after a certain period are abandoned and replaced by new operation scheduling lists ( v ij ) updated by the scout bees using the radius search method as shown in Fig. 7 .

In the radius search method, a set of job operations from the old food source is selected randomly. Each dimension in the operation scheduling list is selected with the probability of max in the first iteration and will linearly decrease to o the last iteration. The value of o max and o min are fixed to 1 and 0.2, respectively. These parameters were chosen by the experi-menter ( Banharnsakun et al., 2011 ). So the first iteration will replace every dimension, that is, will swap the randomly selected solution for the one being abandoned.

In this swapping, the dimensions that are not marked as selected dimension from the old food source will be placed into the same dimension in the new food source. Next, the dimensions marked as selected dimension from the old food source will be placed into the empty positions from the left to right of the operation scheduling list in the new food source. The Fifth Step
A local search based on the Variable Neighboring Search method (VNS) ( Hansen et al., 2008 ) is performed on the best-so-far solution ( x b of Eq. 4.3) to improve the solution quality. The pseudo code of VNS method is shown in Fig. 8 .
Although it seems that VNS would actually find the best solution by itself, it sometimes takes a long time to reach useful solutions whilst solving large scale Job Shop Scheduling.
To overcome this shortcoming, our Best-so-far ABC helps VNS to find solutions more quickly by substantially narrowing down the search space.

From Fig. 8 , let a and b are the random integer numbers between 1 and nm, Exchanging _ Process ( x 0 , a , b ) means exchan-ging the job operations in solution x 0 between a th and b th dimensions, a a b . Inserting _ Process ( x  X , a , b ) means removing the job operation in solution x  X  from the a th dimension and inserting it in the b th dimension.

The example of the exchanging process and the inserting process are shown in Figs. 9 and 10 , respectively.

In Fig. 9 , for the exchanging process, suppose the two random numbers a and b generated are 2 and 8, respectively. In this case, the job operations between the 2nd dimension and 8th dimension will be interchanged. In this example, the job operation J 3 at the 2nd dimension and the job operation J the 8th dimension will be swapped.

In Fig. 10 , for the inserting process, suppose the two random numbers a and b generated are 2 and 8, respectively. In this case, the job operation J 3 at the 2nd dimension will be removed, the job operations at 3rd dimension through 8th dimension will be shifted to the 2nd dimension, and then insert job operation J 3 in the 8th dimension.
 The Termination Step
Steps 2 X 5 are repeated until the number of iteration reaches the MCN. 5. Experiments
The aim of this experiment is to evaluate the solution quality based on the same parameter settings, i.e. the number of popula-tions and the number of iterations. The Best-so-far algorithm is compared with other approaches including a hybrid Particle Swarm Optimization with Variable Neighboring Search (PSO-VNS) ( Tasgetiren et al., 2006 ), a multiple-type individual enhance-ment PSO (MPSO) ( Lin et al., 2010 ), a Hybrid Intelligent Algorithm (HIA) ( Ge et al., 2008 ), a Hybrid Evolutionary Algorithm (HEA) ( Ge et al., 2007 ), a Hybrid Genetic Algorithm using parameterized active schedules (HGA-Param) ( Goncalves et al., 2005 ), and an Improved Artificial Bee Colony algorithm (IABC) ( Yao et al., 2010 ).
The performance of our Best-so-f ar ABC algorithm is evaluated by testing them on the following 62 benchmark problems taken from the Operations Research Library (OR-Library) ( Beasley, 1990 ). 3 problems from Fisher and Thompson (1963) : referred as ft06 , ft10 , and ft20 . 40 problems from Lawrence (1984) : referred as la01 X  X a40 . 5 problems from Adams et al. (1988) : referred as abz05 X  X bz09 . 10 problems from Applegate and Cook (1991) : referred as orb01 X  X rb10 . 4 problems from Yamada and Nakano (1992) : referred as yn01 X  X n04 .

The size of these problem instances range from 6 to 30 jobs and 5 to 20 machines.

The objective is to find the minimum makespan value from these benchmark problems. The numbers of employed and onlooker bees were set to 25. The MCN was set to 200. Each of the experiments was repeated 20 times with different random seeds. Our algorithm was coded in C  X  X  and ran on a PC with a 2.83-GHz Intel Core 2 Quad CPU and 4 GB of memory. We used the results reported in Goncalves et al. (2005) , Tasgetiren et al. et al. (2010) for the comparison. As a result, some problem instances cannot be evaluated in all aforementioned algorithms. 6. Results and discussion
We compare our results with the results in other aforemen-tioned studies in Tables 3 X 5 . In the Tables ,  X  X  X nstance X  X  means the problem name,  X  X  X ize X  X  means the problem size of n jobs on m machines,  X  X  X KS X  X  means the best known solution for the instance reported by Jain and Meeran (1999) ,  X  X  X est X  X  means the best solution found by each algorithm,  X  X  X verage X  X  and  X  X  X .D. X  X  means the average and standard deviation, respectively, of the results over 20 runs, and  X  X  X PE X  X  means the relative percent error with respect to the best known solution, which is calculated from the equation below RPE  X   X  Best BKS  X  BKS 100  X  6 : 1  X 
In each table of comparison results, boldface represents problems in which our Best-so-far ABC reached a higher quality solution than at least one of the previously reported algorithms. It can be noticed from the  X  X  X est X  X ,  X  X  X verage X  X ,  X  X  X .D. X  X , and  X  X  X PE X  X  values in Tables 3 X 5 .
 The first comparison is based on the results of an Improved
Artificial Bee Colony algorithm (IABC) proposed by Yao et al. (2010) . The comparison results are shown in Table 3 .
From Table 3 , it can be seen that the Best-so-far ABC generates better results in term of the best makespan (Best) and relative percent error (RPE) than the IABC. The Best-so-far ABC method is able to find all of the best known solutions for the 11 instances reported and average of RPE for Best-so-far ABC is 0% whereas the
IABC is able to find only 8 best known solutions among 11 instances reported with an average RPE of 1.47%.

The second comparison is based on the results of a multiple-type individual enhancement PSO (MPSO) proposed by Lin et al. (2010) , a Hybrid Intelligent Algorithm (HIA) proposed by Ge et al. (2008) , a Hybrid Evolutionary Algorithm (HEA) proposed by Ge et al. (2007) , and a Hybrid Genetic Algorithm us ing parameterized active sche-dules (HGA-Param) proposed by Goncalves et al. (2005) .The comparison results are shown in Table 4 .

According to Table 4 , for instances ft06 , ft10 , ft20 , and la01 X  la40 , our Best-so-far ABC can find the best known solution (Best) for 41 of 43 instances. These results are better than MPSO, HIA,
HEA, and HGA, which can only find 35, 32, 29, and 31 best know solutions among 43 instances, respectively. Moreover, in term of the average RPE, the Best-so-far ABC generates better results than these aforementioned approaches. The average RPE for the Best-so-far ABC is 0.03% whereas MPSO, HIA, HEA, and HGA got 0.15%, 0.33%, 0.39%, and 0.44%, respectively.
 The last comparison is based on the results of hybrid Particle Swarm Optimization with Variable Neighboring Search (PSO-
VNS) by Tasgetiren et al. (2006) . The comparison results are shown in Table 5 .

As shown in Table 5 , the Best-so-far ABC can find the best known solutions (Best) for 26 of 35 instances reported where the
PSO-VNS only finds the best know solutions with 23 of 35 instances reported. Moreover, the average RPE in the 35 instances reported for PSO-VNS is 0.20% whereas the average RPE for Best-so-far ABC is 0.16%. This shows that the Best-so-far ABC does better in finding the best known solution than the PSO-VNS. The mean of the average of makespan for the Best-so-far ABC is 993.85 whereas PSO-VNS is 996.94. The results indicate that the
Best-so-far ABC gives slightly better makespan values on average than the PSO-VNS. It is also clear that the Best-so-far algorithm is more robust than the PSO-VNS algorithm as shown by the lower standard deviations in the S.D. column. The average of the standard deviations for the Best-so-far ABC is only 3.60 whereas it is 6.40 for the PSO-VNS. Note that we do not report standard deviations for other comparisons because standard deviations are not reported in the papers presenting the other algorithms.
In addition, the average RPE for the Best-so-far ABC on all 62 benchmark problems is 0.09%. A comparison of all aforemen-tioned approaches is provided in Appendix A . 7. Conclusions In this paper, a Best-so-far ABC algorithm for solving the Job
Shop Scheduling Problem is proposed. We have replaced the neighboring solutions based approach with the Best-so-far tech-nique in order to increase the local search ability of the onlooker bees. This will bias the solution selected by onlooker bees towards the optimal solution more quickly. The use of set theory was introduced to describe the mapping between our Best-so-far ABC method and the combinatorial optimization problem.

The experiment was benchmarked against six state-of-the-art methods from three well known algorithms. There are three methods from Particle Swamp Optimization, including hybrid Particle Swarm Optimization with Variable Neighboring Search (PSO-VNS), multiple-type individual enhancement PSO (MPSO), and Hybrid Intelligent Algorithm (HIA). For Genetic Algorithms, Hybrid Evolutionary Algorithm (HEA) and Hybrid Genetic Algo-rithm using parameterized active schedules (HGA-Param) was included. Finally, Improved Artificial Bee Colony algorithm (IABC) from ABC algorithm was used.
 The results obtained from our proposed method show that Best-so-far ABC can find the best known solution more effectively than other aforementioned approaches. This difference is statis-tically significant for three of the six alternative algorithms used for comparison. Moreover, the Best-so-far ABC algorithm gives a better average makespan value. The results also show that our Best-so-far ABC is more robust than the PSO-VNS method as shown by the value of SD in Table 5 .

Thus, we can conclude that our Best-so-far ABC is effective from both the perspective of solution quality and algorithm robustness. The algorithm can serve as an alternative method in the Job Shop Scheduling domain.

In the future, we will investigate JSSP in a case where  X  X  X ob interrupt X  X  is permitted. Our proposed algorithm may split a job into a number of smaller sub-jobs. This will allow each job to be preempted by other jobs with higher priorities. The preempted job can later be resumed when a machine is free. With this variation, we believe that Best-so-far ABC will be even more practical for use in most job shop scheduling problems. Acknowledgments
This work is supported by the Thailand Research Fund through the Royal Golden Jubilee Ph.D. Program under Grant no. PHD/ 0038/2552.
 Appendix A See Appendix Table A1 below.
 References
