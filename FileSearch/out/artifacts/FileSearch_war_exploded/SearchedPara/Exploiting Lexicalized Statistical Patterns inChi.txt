 Most of current statistical natural language processing systems rely on large well-organized annotated corpus. For example, the state-of-the-art dependency parser uses Treebanks to extract POS-tag features [9]. Nevertheless, these corpora are highly time-consuming and labor-intensive to build and extend. Moreover, they are mostly in limited size. The main cause of error for many natural language processing task is the lack of related statistical information in the training set. the most significant issues in shallow parsing. In Chinese, a chunk that has two components: VP+NP, can possibly be a verbal phrase or a substantive phrase. For instance, w O  X   X  (farewell ceremony) and w O * l (say goodbye to a friend) are both composed by VP+NP. while the former is a substantive phrase and the latter is a verbal phrase. However, the famous Stanford parser incorrectly categorizes w O  X   X  as a verbal phrase, as shown in Figure 1. Resolving this type of error requires information that is not present in Treebanks.
 tial of web-scale corpus to NLP tasks. The key advantage of web corpus lies in its massive scale, which contributes to the completeness of statistical informa-tion. The main challenge is that web corpus usually consists of huge amounts of unstructured plain-text documents. As far as we know, there is not a flexible approach for analyzing the deep structure of natural language automatically, so that the major methodology of utilizing web search engine is simply based on string-matching strategy.
 and access word count statistics via the search engines. For example, we find that w O * l (have said goodbye to a friend) appears more frequently than w O  X   X  (have said goodbye to a ceremony) by comparing the amount of results acquired from web search engine 2 . Furthermore, it indicates that for VP NP phrases, the occurrence of  X  X P+ +NP X  structure is probably a discriminative cue to determine its phrasal category.
 for the given template are defined as lexicalized patterns , such as w O  X   X  and w O * l . Our work is based on a reasonable hypothesis that the phrase frequency information collected from large corpora can reflect the correctness of phrase usage. If a lexicalized pattern frequently occurs in web-scale corpora, we would be confident that it is linguistically valid. In contrast, if a lexicalized pattern hardly appears, we would say that it is linguistically invalid. We de-note those lexicalized patterns with frequency counts as lexicalized statistical patterns , examples of which are shown in Table 1.
 levels. In this paper, we discuss the usage of lexicalized statistical patterns on three levels of Chinese grammatical analysis. At the lexicalization level, we focus on analyzing the cohesion of compound noun phrases. At the syntactical level, we focus on determining the phrasal category. At the semantic level, we focus on discovering patient objects among the predicate-object phrases. We integrate templates and utilize valuable lexicalized statistical patterns at each level. way to enhance the performance of NLP systems. Volk used frequencies counts of query patterns to resolve PP attachment ambiguities via web search engine [5]. Lapata and Keller used web counts to resolve preposition attachments and compound noun interpretation [3][4]. Bansal used Google n-grams to generate full range of syntactic attachments [1]. Moreover, contextual statistics collected from web have also significantly improved the quality of automatic thesaurus extraction [6]. Yuan designed a series of rule-based scoring method for word categorization [7][8]. It was proven to be effective for validating parts of speech of Chinese words and categorizing phrases. Nevertheless, the principle of judging whether a word or a phrase follows a rule is completely based on decisions of linguistic experts. It requires significant amount of manual effort, which to a great extent weakens the scalability of patterns.
 cal patterns to conveniently acquire frequency information from any plain-text datasets. In this work, we experimentally collected Chinese plain-text sentences from the SogouT corpus. In section 3, we present some useful templates for Chi-nese analysis at different linguistic levels. For each level, the results of our case study are demonstrated respectively. Finally, we provide conclusion in Section 4. A naive way to obtain frequencies from web-scale corpus is to directly query from traditional web search engine. For example, if we query the phrase  X  w O * l  X  using Google, it will return about 37,200 results. However, there are three main disadvantages of traditional search engine:  X  There are duplicate pages and spam sites in the web environment. It will  X  Web search engines ignore stop words and punctuation in general. But under  X  The users have no way to perform complex type of queries, such as near tain noises like  X   X   X   X  |  X  ( w O , * l  X  2  X  ! X  (It X  X  a silent farewell. Goodbye friends!). The solution to this problem is to complement a period or a question mark behind the query, such as  X  w O * l . X  and  X  w O * l ? X . More-over, we hope to permit the punctuation and query to be separated by no more than two words, like  X  w O * l j ! X  or  X  w O * l  X  ? X . To our knowledge, we are not aware of any web search engine that can deal with such kind of queries. patterns more accurately. In fact, a similar product known as the Sketch Engine. 3 has been published by Lexical Computing Ltd. It can deal with complex types of word queries, but it is not capable of handling Chinese documents without word segmentation.
 source enterprise search platform. Solr provides document indexing APIs and support term proximity with slop factors. We implement a query builder to support formalized complex queries based on Solr. The unified regular expression of query is described as follow: where W represents a candidate word list, R represents a range of wildcard gaps, and E represents a set of punctuation at the end of sentences. For example, one possible query to describe the pattern w O * l can be written as: where  X  w O * l j ! X  and  X  w O * l  X  ? X  both match this query.
 sentences from SogouT web page dataset, removing unnecessary html tags, hy-perlinks, scripts and independent anchor texts. We take the number of matched sentences as the word count result for corresponding query pattern. To make sure the word count of our sketch search engine is reliable, we eliminate dupli-cates and short sentences (no longer than 2). We eventually index 729,008,561 unique sentences with a cost of 165.2 GB free disk space. The lexicalized search engine can handle all quires in the format described in (1), with an average of 10 seconds query response time. Three examples of word count statistics are demonstrated in Table 1. In this section, we analyze Chinese phrases at three linguistic levels: lexical, syntactic and semantic. The basic idea is to integrate handcrafted templates and then automatically acquire phrase counts via the search engine. For each level, we present the result of our case study on SogouT corpus respectively. 3.1 Lexical level We focus on analyzing the cohesion of a compound noun phrase at lexical level. Phrases with low cohesion should not be included into vocabulary. For instance,  X  f  X  n (rabbit tail) has low cohesion, while S  X   X  (round table) has high cohesion. Mutual information is commonly used to measure the cohesion of a phrase, but the boundary value between high and low cohesion is often fuzzy. For example, if we use SogouT to get frequency counts, the point-wise mutual information of  X  f (rabbit) and  X  n (tail) is  X  low. It is not enough to prove that this phrase has low cohesion.
 component words can hardly be separated by particles. Hence, we construct a template  X  X P 1 + +NP 2  X  to determine whether a compound noun phrase  X  X P 1 NP 2  X  has low cohesion. The statistic of lexicalized patterns are provided in the following: similar amount of occurrences, while S  X   X  (the conference of round table) is significantly less frequent than S  X   X  . In fact, S  X   X  hardly appears in SogouT corpus. It indicates that the occurrence ratio of lexicalized patterns of NP 1 + +NP 2 is more effective than mutual information. 3.2 Syntactical level A typical problem at syntactic level is to determine the phrasal category of  X  X P NP X  phrases. For example, w O  X   X  is a substantive phrase, while w O * l is a verbal phrase. These two phrases share the same verb w O (say goodbye to). We can give many other examples, such as ? n  X  i (repair company) and ? n g 1  X  (repair the bicycle),  X  S | (study group) and  X  S = X  (study English),  X  I  X  (purchasing demand) and  X   X  7 (purchase funds), etc.
 designing handcrafted rules [8]. Inspired by these rules, we construct a set of templates for  X  X P NP X  phrases via frequently-used auxiliaries and conjunctions in Chinese. For simplicity, we only take two-word compound phrase into account. We enumerate 14 templates in Table 2. each template t . If obtain the frequency counts via the search engine for lexical-ized patterns, the phrase category of p is determined by the relative frequency F ( p ), which is derived as follow: where Count( l ( p,t )) represents the frequency count of each lexicalized pattern and Count( p ) represents the total count of phrase p . As we can see, the higher value of F ( p ), the more likely the phrase p is to be verbal phrase. A simple validation algorithm is to set a lower threshold  X  for verbal phrases, where  X  may vary for different corpus.
 We collect 20 example phrases, half of which are verbal. The frequency result-s are demonstrated in Table 3. We find that the average phrase frequency of verbal phrases is only 60% higher than that of substantive phrases, while the average relative frequency F ( p ) of verbal phrases is 52.3 times higher than that of substantive phrases. It indicates that the lexicalized patterns are effective to distinguish the two categories of phrases. Since the minimum F ( p ) score a-mong verbal phrases is 0.049 and the maximum F ( p ) among substantive phrases is 0.006, the appropriate threshold  X  for SogouT corpus can be set within the range of (0 . 006 , 0 . 049).
 3.3 Semantic level At the semantic level, we focus on discovering the patient object, which is a nominal phrase that acts as the recipient of the action stated by a verbal phrase. It is a significant semantic relation between nominal and verbal phrases. For example,  X  (dinner) is the patient of verb  X  (eat) in the phrase  X   X  (having dinner), while  X  , (canteen) is not the patient of  X  (eat) in the phrase  X   X  , (eating at the canteen).
 tient object usually rely on the judgement of linguists. In this paper, we pro-pose to utilize a set of templates for this task, such as  X  r +NP+VP+  X ,  X   X  +VP+ +NP X  and  X  +VP+ +NP X . If the nominal phrase occur to be the patient object, the lexicalized pattern for this template tends to occur fre-quently in web-scale corpus. For example, r  X   X  (to have dinner) appears 63 times in SogouT, while r  X  ,  X  (to eat the canteen) has never appeared. Here, we present the frequencies of 8 examples in SogouT to verify the reliability of our template. Patient objects: Non-patient objects: where the translations of these phrases are given in Appendix.
 proportional to the frequency of the VP+NP phrase. For non-patient object, the frequency of lexicalized pattern is almost always zero. It indicates that our template is effective for discovering patient objects among VP+NP phrases. We exploit lexicalized statistical patterns collected from the web corpus at three linguistic levels of Chinese analysis. Results of our case study indicates that these patterns are effective on analyzing the cohesion of phrases, determining the phrasal category and discovering patient objects.
 workloads of linguistics [7][8]. The search engine for lexicalized patterns can also be used for verifying the effectiveness of batched and hand-crafted linguistic rules. In the future we aim at integrating the lexicalized statistical patterns as feature templates to enhance the precision of Chinese parser.
 Acknowledgments. This work is supported by the National Natural Science Foundation of China (Grant No. 61133012) and the National High Tech. Devel-opment Program of China (863 Program) (Grant No. 2012AA011102).
 Here we present the English glosses of some Chinese phrases in this paper to make sure one has a clearer understanding of their meanings.
