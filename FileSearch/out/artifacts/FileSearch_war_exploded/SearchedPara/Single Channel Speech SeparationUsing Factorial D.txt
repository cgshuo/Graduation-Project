 One of the hallmarks of human perception is our ability to sol ve the auditory cocktail party problem: we can direct our attention to a given speaker in the presence of interfering speech, and understand what was said remarkably well. Until now the same could not be said for automatic speech recog-nition systems. However, we have recently introduced a syst em which in many conditions performs this task better than humans [1][2]. The model addresses the Pascal Speech Separation Challenge task [3], and outperforms all other published results by mor e than 10% word error rate (WER). In this model, dynamics are modeled using a layered combinatio n of one or two Markov chains: one for long-term dependencies and another for short-term depe ndencies. The combination of the two speakers was handled via an iterative Laplace approximatio n method known as Algonquin [4]. Here we describe experiments that show better performance on the same task with a simpler version of the model.
 The task we address is provided by the PASCAL Speech Separati on Challenge [3], which provides standard training, development, and test data sets of singl e-channel speech mixtures following an arbitrary but simple grammar. In addition, the challenge or ganizers have conducted human-listening experiments to provide an interesting baseline for compari son of computational techniques. The overall system we developed is composed of the three comp onents: a speaker identification and gain estimation component, a signal separation compone nt, and a speech recognition system. In this paper we focus on the signal separation component, wh ich is composed of the acoustic and grammatical models. The details of the other components are discussed in [2].
 Single-channel speech separation has previously been atte mpted using Gaussian mixture models (GMMs) on individual frames of acoustic features. However s uch models tend to perform well only when speakers are of different gender or have rather differe nt voices [4]. When speakers have similar voices, speaker-dependent mixture models cannot unambigu ously identify the component speakers. In such cases it is helpful to model the temporal dynamics of t he speech. Several models in the models have typically been based on a discrete-state hidden Markov model (HMM) operating on a frame-based acoustic feature vector.
 Modeling the dynamics of the log spectrum of speech is challe nging in that different speech compo-nents evolve at different time-scales. For example the exci tation, which carries mainly pitch, versus the filter, which consists of the formant structure, are some what independent of each other. The for-mant structure closely follows the sequences of phonemes in each word, which are pronounced at a rate of several per second. In non-tonal languages such as En glish, the pitch fluctuates with prosody over the course of a sentence, and is not directly coupled wit h the words being spoken. Neverthe-less, it seems to be important in separating speech, because the pitch harmonics carry predictable structure that stands out against the background.
 We address the various dynamic components of speech by testi ng different levels of dynamic con-dynamics , high-level grammar dynamics , and a layered combination, dual dynamics , of the acoustic and grammar dynamics. The grammar dynamics and dual dynamic s models perform the best in our experiments.
 The acoustic models are combined to model mixtures of speech using two methods: a nonlinear model known as Algonquin , which models the combination of log-spectrum models as a su m in the power spectrum, and a simpler max model that combines two log spectra using the max function. I t turns out that whereas Algonquin works well, our formulatio n of the max model does better overall. With the combination of the max model and grammar-level dyna mics, the model produces remark-able results: it is often able to extract two utterances from a mixture even when they are from the same speaker 1 . Overall results are given in Table 1, which shows that our cl osest competitors are human listeners.
 The model consists of an acoustic model and temporal dynamics model for each source, and a mixing model , which models how the source models are combined to describe the mixture. The acoustic features were short-time log spectrum frames computed ever y 15 ms. Each frame was of length 40 ms and a 640-point mixed-radix FFT was used. The DC component was discarded, producing a 319-dimensional log-power-spectrum feature vector y The acoustic model consists of a set of diagonal-covariance Gaussians in the features. For a given speaker, a , we model the conditional probability of the log-power spec trum of each source signal x covariance matrix  X  of each speaker. For efficiency and tractability we restrict the covariance to be diagonal. A model Acoustic Dynamics : To capture the low-level dynamics of the acoustic signal, w e modeled the acoustic dynamics of a given speaker, a , via state transitions p ( s a There are 256 acoustic states, hence for each speaker a , we estimated a 256  X  256 element transition matrix A a .
 Grammar Dynamics : The grammar dynamics are modeled by grammar state transiti ons, the Speech Separation Challenge grammar [3] and are modeled using a set of pronunciations that map from words to three-state context-dependent phone mode ls. The state transition probabilities derived from these phone models are sparse in the sense that m ost transition probabilities are zero. speaker-dependent acoustic states. These are learned from training data where the grammar state sequences and acoustic state sequences are known for each ut terance. The grammar of our system has 506 states, so we estimate a 506  X  256 element conditional probability matrix B a for each speaker.
 Dual Dynamics : The dual-dynamics model combines the acoustic dynamics wi th the grammar dynamics. It is useful in this case to avoid modeling the full combination of s and v states in the joint transitions p ( s a is the normalizing constant. Here we simply use the probabil ity matrices A a and B a , defined above. The speech separation challenge involves recognizing spee ch in mixtures of signals from two speak-ers, a and b . We consider only mixing models that operate independently on each frequency for analytical and computational tractability. The short-tim e log spectrum of the mixture y frequency band, is related to that of the two sources x a in one feature dimension, given the source states is thus: In general, to infer and reconstruct speech we need to comput e the likelihood of the observed mixture and the posterior expected values of the sources given the st ates, and similarly for x b quences { s a tors E ( x a where  X  s a all frames in the signal.
 The mixing model can be defined in a number of ways. We explore t wo popular candidates, for which the above integrals can be readily computed: Algonquin , and the max model . Algonquin : The relationship between the sources and mixture in the log power spectral domain is approximated as where  X  is introduced to model the error due to the omission of phase [ 4]. An iterative Newton-Laplace method accurately approximates the conditional po sterior p ( x a Gaussian. This Gaussian allows us to analytically compute t he observation likelihood p ( y and expected value E ( x a Max model : The mixing model is simplified using the fact that log of a sum is approximately the log of the maximum: In this model the likelihood is where  X  In [5], such a model was used to compute state likelihoods and find the optimal state sequence. In [8], a simplified model was used to infer binary masking value s for refiltering.
 We take the max model a step further and derive source posteri ors, so that we can compute the MMSE estimators for the log power spectrum. Note that the sou rce posteriors in x a a mixture of a delta function and a truncated Gaussian. Thus w e analytically derive the necessary expected value: with weights  X  a many pairs of states one model is significantly louder than an other  X  band, relative to their variances. In such cases it is reason able to approximate the likelihood as Because of the large number of state combinations, the model would not be practical without tech-niques to reduce computation time. To speed up the evaluatio n of the joint state likelihood, we employed both band quantization of the acoustic Gaussians and joint-state pruning .
 Band Quantization : One source of computational savings stems from the fact tha t some of the approximating each of the D Gaussians of each model with a shared set of d Gaussians, where d  X  D on the use of a diagonal covariance matrix, so that p ( x a | s a ) = Q are the diagonal elements of covariance matrix  X  Gaussians with one of the d Gaussians in band f . Now  X  p ( x a | s a ) = Q is used as a surrogate for p ( x a | s a ) . Figure 3 illustrates the idea.
 Under this model the d Gaussians are optimized by minimizing the KL-divergence Despite the relatively small number of components d in each band, taken across bands, band quanti-zation is capable of expressing d F distinct patterns, in an F -dimensional feature space, although in practice only a subset of these will be used to approximate th e Gaussians in a given model. We used d = 8 and D = 256 , which reduced the likelihood computation time by three ord ers of magnitude. Joint State Pruning : Another source of computational savings comes from the spa rseness of the rest for a given observation. Only these states are required to adequately explain the observation. By pruning the total number of combinations down to a smaller nu mber we can speed up the likelihood calculation, estimation of the components signals, as well as the temporal inference. However, we must estimate the likelihoods in order to determ ine which states to retain. We therefore model on the pruned states using the exact parameters. In the experiments reported here, we pruned down to 256 state combinations. The effect of these speedup m ethods on accuracy will be reported in a future publication. In our experiments we performed inference in four different conditions: no dynamics , with acoustic dynamics only, with grammar dynamics only, and with dual dynamics (acoustic and grammar). With no dynamics the source models reduce to GMMs and we infer MMSE estimates of the sources based each source is estimated, we estimate the corresponding tim e-domain signal as shown in [4]. search, described below, with acoustic temporal constrain ts p ( s (1), to find the most likely joint state sequence s 2-D Viterbi search is used to infer the grammar state sequenc es, v the likelihood models, however, we have mixture models in th is case. So we can perform an MMSE estimate of the sources by averaging over the posterior prob ability of the mixture components given the grammar Viterbi sequence, and the observations.
 It is critical to use the 2-D Viterbi algorithm in both cases, rather than the forward-backward algo-rithm, because in the same-speaker condition at 0dB, the aco ustic models and dynamics are sym-metric. This symmetry means that the posterior is essential ly bimodal and averaging over these modes would yield identical estimates for both speakers. By finding the best path through the joint state space, the 2-D Viterbi algorithm breaks this symmetry and allows the model to make different estimates for each speaker.
 In the dual-dynamics condition we use the model of section 2( b). With two speakers, exact inference is computationally complex because the full joint distribu tion of the grammar and acoustic states, inference by alternating the 2-D Viterbi search between two factors: the Cartesian product s a  X  s b of the acoustic state sequences and the Cartesian product v a  X  v b of the grammar state sequences. When evaluating each state sequence we hold the other chain co nstant, which decouples its dynamics strongly with each other and similarly for v a and v b . Again, in the same-talker condition, the 2-D Viterbi search breaks the symmetry in each factor. 2-D Viterbi search : The Viterbi algorithm estimates the maximum-likelihood s tate sequence s given the observations x number of states and T is the number of frames. For producing MAP estimates of the 2 s ources, we require a 2 dimensional Viterbi search which finds the most li kely joint state sequences s a be computed in O ( T D 3 ) operations. This stems from the fact that the dynamics for ea ch chain are independent. The forward-backward algorithm for a factori al HMM with N state variables requires is true for the Viterbi algorithm. In the Viterbi algorithm, we wish to find the most probable paths leading to each state by finding the two arguments s a The two maximizations can be done in sequence, requiring O ( D 3 ) operations with O ( D 2 ) storage for each step. In general, as with the forward-backward algo rithm, the N -dimensional Viterbi search requires O ( T N D N +1 ) operations.
 We can also exploit the sparsity of the transition matrices a nd observation likelihoods, by pruning unlikely values. Using both of these methods our implementa tion of 2-D Viterbi search is faster than the acoustic likelihood computation that serves as its input, for the model sizes and grammars chosen in the speech separation task.
 Speaker and Gain Estimation : In the challenge task, the gains and identities of the two sp eakers were unknown at test time and were selected from a set of 34 speakers which were mixed at SNRs ranging from 6dB to -9dB. We used speaker-dependent acousti c models because of their advantages when separating different speakers. These models were trai ned on gain-normalized data, so the models are not well matched to the different gains of the sign als at test time. This means that we have to estimate both the speaker identities and the gain in o rder to adapt our models to the source signals for each test utterance.
 The number of speakers and range of SNRs in the test set makes i t too expensive to consider every possible combination of models and gains. Instead, we devel oped an efficient model-based method for identifying the speakers and gains, described in [2]. Th e algorithm is based upon a very simple idea: identify and utilize frames that are dominated by a sin gle source  X  based on their likelihoods under each speaker-dependent acoustic model  X  to determine what sources are present in the mixture. Using this criteria we can eliminate most of the unlikely spe akers, and explore all combinations of the remaining speakers. An approximate EM procedure is th en used to select a single pair of speakers and estimate their gains.
 Recognition : Although inference in the system may involve recognition o f the words X  for models that contain a grammar  X  X e still found that a separately trai ned recognizer performed better. After reconstruction, each of the two signals is therefore decode d with a speech recognition system that incorporates Speaker Dependent Labeling (SDL) [2].
 speaker identities provided by the speaker ID and gain modul e, we followed the approach for gender dependent labeling (GDL) described in [11]. This technique provides better results than if the true speaker ID is specified. The Speech Separation Challenge [3] involves separating th e mixed speech of two speakers drawn from of a set of 34 speakers. An example utterance is place white by R 4 now . In each recording, letter and the digit of the speaker that said white . Using the SDL recognizer, we decoded the two estimated signals under the assumption that one signal cont ains white and the other does not, and vice versa. We then used the association that yielded the hig hest combined likelihood. Human listener performance [3] is compared in Figure 4 to res ults using the SDL recognizer without speech separation, and for each the proposed models. Perfor mance is poor without separation in all conditions. With no dynamics the models do surprisingly wel l in the different talker conditions, but poorly when the signals come from the same talker. Acoustic d ynamics gives some improvement, mainly in the same-talker condition. The grammar dynamics s eems to give the most benefit, bring-ing the error rate in the same-gender condition below that of humans. The dual-dynamics model performed about the same as the grammar dynamics model, desp ite our intuitions. Replacing Algo-nquin with the max model reduced the error rate in the dual dyn amics model (from 24.3% to 23.5%) and grammar dynamics model (from 24.6% to 22.6%), which brin gs the latter closer than any other model to the human recognition rate of 22.3%.
 Figure 5 shows the relative word error rate of the best system compared to human subjects. When both speakers are around the same loudness, the system excee ds human performance, and in the same-gender condition makes less than half the errors of the humans. Human listeners do better when the two signals are at different levels, even if the targ et is below the masker (i.e., in -9dB), suggesting that they are better able to make use of differenc es in amplitude as a cue for separation. we limited the grammar to just the two test utterances, and th e error rate on the estimated sources dropped to around 10%. This may be a useful paradigm for separ ating speech from background noise when the text is known, such as in closed-captioned recordin gs. At the other extreme, in realistic speech recognition scenarios, there is little knowledge of the background speaker X  X  grammar. In such cases the benefits of models of low-level acoustic continuit y over purely grammar-based systems may be more apparent.
 It is our hope that further experiments with both human and ma chine listeners will provide us with a how the human auditory system functions, as well as how autom atic speech perception in general can be brought to human levels of performance.

