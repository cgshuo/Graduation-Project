 In recent years, Bootstrapping methods [1, 2, 4] have attracted great attention in many IE and NLP fields and achieved good results. Although supervised methods usually could get better results than weakly supervised methods, they are highly constrained by annotated corpus and transplantation from a field to another often costs approxi-unannotated corpus to build semantic lexicon. This feature makes bootstrapping a useful approach in many IE [4, 5] and NLP fields, such as exploiting role-identifying nouns [3]. Semantic lexicons have been proved to be useful for many IE and NLP fields. Knowing the semantic classes of words (e.g.,  X  X enz X  is a brand of automobile) can be extremely valuable for many tasks, including question answer [15, 16], IE [17] and so on. Although some semantic dictionaries do exist (e.g. Word-Net), they are rarely complete, especially for large open classes (e.g., classes of people and objects) and rapidly changing categories (e.g., computer technology). It is reported that for every five terms generated by their semantic lexicon learner three were not present in Word-Net. Automatic semantic lexicon acquisition could be used to enhance existing resources such as Word-Net, or to produce semantic lexicons for specialized categories of domains. In recent years, several algorithms have been proposed to automatically learn semantic lexicons using supervised methods [6, 7], and weakly unsupervised methods [1, 4]. As a weakly-supervised method requires little manu-ally-labeled training data, it has received more and more attention [8-10]. In this paper, a weakly-supervised bootstrapping algorithm called MSGA-Bootstrapping (Mutual Screening Graph Algorithm Based Bootstrapping) which automatically generates semantic lexicons is developed. Like other bootstrapping methods, MSGA-Bootstrapping begins with unlabeled corpus and a few seed words, Basilisk [1] and GMR [4], we found that they both have a number of areas for im-provement, such as the pattern format and the method of scoring patterns and words. Therefore, we develop a new format for extracted patterns and a more reasonable method for scoring patterns and words. We also add a new process to filter words without any field-related information. From the experimental results, lexicons ac-quired by MSGA-Bootstrapping (more details in Section 4) are more precise than those of Basilisk and GMR-Bootstrapping. 
The reminder of the paper is organized as follows. In section 2, we introduce Basi-lisk and GMR-Bootstrapping, and point out the problem occurred by using these algo-rithms and explain why these problems occur. In Section 3, we introduce our MSGA-Bootstrapping and its new feature. In Section 4, experiments are presented to show the improvements, and the experimental results are discussed. Conclusions are given in Section 5. Many research works have been done on Bootstrapping in recent years. Among them, Basilisk [1] and its improved algorithm GMR-Bootstrapping [4] get most attention. 2.1 Basilisk Michael Thelen and Ellen Riloff developed a weakly-supervised bootstrapping algo-rithm called Basilisk (Bootstrapping Approach to Semantic Lexicon Induction using Semantic Knowledge) [1], which automatically generates semantic lexicons. The input to Basilisk is an unannotated corpus and a few manually-defined seed words for each semantic category, as well. 
The bootstrapping process begins by selecting a subset of the extraction patterns with the AutoSlog system [11]. Then it scores each pattern using the RlogF metric [11]. The top N extraction patterns are put into a pattern pool. Then Basilisk collects all noun phrases extracted by patterns in the pattern pool and puts the result into the candidate pool. The top M words are added to the lexicon, and the process starts over again. 
In Basilisk, a pattern receives a high score if a high percentage of its extractions are category members. Or if a moderate percentage of its extractions are category mem-patterns that also have a tendency to extract known category members. 
Basilisk gives us a good method to learn Semantic Lexicon at a less cost than other previous methods. However, after analyzing the procedure of Basilisk we found that the method treats every word extracted by patterns equal, and it does not distinguish how many times the word has been extracted and how many patterns have extracted it. In other word, Basilisk evaluates a word without any quality and quantity informa-tion on the patterns that extract it. Suppose that word  X  has been extracted 10 times by Basilisk also has this problem in scoring patterns. 2.2 GMR-Bootstrapping Hany Hassan, Ahmed Hassan and Ossama Emam proposed a new method based on bootstrapping called GMR-Bootstrapping (Graph Mutual Reinforcement based Bootstrapping) [4]. Different with Basilisk, GMR did not evaluate every word in the as pattern candidates. So does the processing of extracting words. 
GMR-Bootstrapping introduces a good idea that it assigns every word (or pattern) a words candidate (patterns candidate). When we use these words (patterns) to score a the experimental result given by the author, we saw that GMR got a better result than Basilisk for adding quality information of words/patterns. 
However, there is still some information we should add to in our effort to make the result more accurate. For example, we assume that pattern  X  is a perfect pattern suit-able for extracting words from the corpus, and word  X  has been extracted 10 times by pattern  X  while word  X  has been extracted by this pattern only once. In GMR-Bootstrapping these two words will be evaluated equal, even though intuitively it seems that word  X  should get a higher score than word  X  . This paper focuses on how to improve this bootstrapping algorithm to get a more accurate result, and the key improvement of MSGA to the previous method. MSGA is a weakly supervised machine learning method. Same as other bootstrapping methods, the input of MSGA-Bootstrapping are a large set of unlabeled data and a small set of labeled data. Then MSGA-Bootstrapping iteratively trains and evaluates a classifier in order to improve its performance. For lexical acquisition, we need a lot of unannotated corpus and a few seed words. Customarily, we select seed words by sorting the frequency of words in corpus and manually select some words which we think have more information on this field. Step 1, MSGA-Bootstrapping selects a subset of the extraction patterns that tend to extract the seed words. We then add these patterns in a set called the pattern pool. Step 2 we score every pattern in the pattern pool and select the best m patterns and put them in candidate pool and choose the best n words that do not belong to the common word dictionary. Then, we use the words in the word candidate pool to select patterns, and the whole process starts over again. After several rounds of iteration, the word candi-date pool will expand and eventually it will become the semantic lexicons we need. Figure 1 shows the process of MSGA-Bootstrapping. Figure 2 shows the MSGA-Bootstrapping process. 
We make three major changes to improve Basilisk and GMR-Bootstrapping. 3.1 Pattern Formats In order to find new lexicon entries, extraction patterns are used to provide contextual evidence that shows which semantic class a word belongs to. Basilisk and GMR-Bootstrapping use the AutoSlog system [12] to represent extraction patterns. AutoSlog X  X  extraction patterns represent linguistic expressions that extract a head prepositional phrase object. 
However, AutoSlog X  X  extraction patterns only can provide the evidence of linguis-tic expressions and words extracted by it are head nouns of noun phrases. This feature make AutoSlog X  X  extraction pattern limited in processing corpuses of some field, for words that belongs to the lexicon of this fiel d can be used as an adjective or other part of speech in our corpus. In other words, AutoSlog X  X  extraction pattern can work well in a few fields, but if we want make our algorithm processing available for more cor-pus, we need find out a new format of extracted patterns. 
To solve this problem, we performed many experiments and finally choose the pos pattern as our pattern format. In every pattern, we only use pos tags and their combi-nations to locate words in the text and extract them into the word pool. For example, suppose that we get a pattern  X (*)/NN */AD */VV X , where the asterisk represents any word; the word in brackets is what we want to extract. Using this pattern, we use pos message instead of information of the speci al word to match. According to the com-bination and the sequence of pos tag, we can extract patterns and words without knowing the exact word. Another advantage of using this pattern format is, for we do not need to know the exact word, we can us e pos pattern format to process corpuses of different languages while AutoSlog X  X  extraction patterns only can deal with English corpus. 3.2 Common Word Dictionary In our experiment, we find that many words without any field-related information appear in many semantic lexicons. After analyzing these words, we find they often solve this problem and exclude these words from our lexicon, we add a dictionary called Common Word Dictionary(CWD) in our MSGA-Bootstrapping. We build CWD with these steps: Step 1. Calculate word frequency in every corpus. Step 2. Select words that have relatively high frequency in most corpuses. In our When scoring the words in the word pool, we filter out words contained in the CWD and this process can make words without any field-related information to be excluded from our lexicons. Experiments show that this method can greatly improve the preci-sion of the lexicon we create. 3.3 MSGA Scoring MSGA is different with Basilisk and GMR-Bootstrapping in its method for scoring words and patterns. We assume patterns which match words that have a higher preci-sion of belonging to the semantic lexicon tend to be important. For example, pattern  X  can extract word  X  which has the possibility of 80% to belong the semantic and pat-word which proved to belong to the semantic lexicons many times, this pattern tends word 4 times in the corpus while pattern  X  can only extract this word once in the same corpus. Then we assumed pattern  X  is more important in this field than pattern  X  , and it should achieve a higher score than pattern  X  . Similar as processing the word. By this method, we can use not only quality information but also quantity information on calculating the score of patterns and words. 
Figure 3 shows an example of MSGA scoring, where left pool contains some words (such as a, b) and right pool contains some patterns (pattern A, B). The number beside words shows the score of these words, where the number of the line between a word further explain the difference between MSGA and Basilisk and GMR, Basilisk only count on the information that whether exist a line between a word and a pattern. GMR involve the precision message of words in calculate the score of a pattern. MSGA use both quality information of words(score) and quantity information(times that words extract patterns) to calculate the score of patterns. 
When got a lot of patterns that extract by the words in word candidate pool, we cal-culate these patterns with words X  score of previous step. The score for each pattern is computed as: Where F(p) is a temporary variable used in calculating the score of a pattern; W(p) is the set of words which extract this pattern; sw(w) is the word X  X  score which is calcu-information sw(w) and the quantity information C(w) in scoring patterns. F(p) is the result of Eq. (1). | W(p)| is the number of words that extract this pattern. A and they extract the patterns many times. We calculate SP(p) , which is the normaliza-tion factor: Then the sp(p) is normalized by the following equations: Where sp(p) is the final score of the pattern. Then we sort the patterns in the pattern candidate. Note that j is the number of iterations. We use the value j to avoid our word candidate pool stagnant. For example, we assume that our MSGA performs perfectly, and we have three reliable patterns. Initially, they work well and give us a lot of cor-extract decreases. Why? Because the word set that these three patterns can extract becomes stagnant. In each round of iteration, we only get the top five words and the second top five words in the next round. For this reason, the pattern pool needs to be infused with new patterns so that more words become available for consideration. 
Then we use these patterns to match the cor pus again, and the words that have been matched are put into the word pool. We calculate the word score with the pattern that matches the words and the equation is: match this word, and F(p) is the score of the pattern calculated by Eq. (1). C(p) is the number of time that this pattern matches the word in the corpus. | P(w) | is the number denoting how many patterns in the pattern pool can match this word. A word receives patterns have a high possibility on extracting words for this semantic lexicon) and match it many times. 
At the end of this iteration, SW, which is the normalization factor, is calculated as Then sw(w) is normalized by the following equation: Where sw(w) is the final score of the word. Sorting the words in word pool, we choose best 5 words and add them in a set called word candidate pool. Then the boot-strapping processing iterated. To compare the performance of MSGA-Bootstrapping with Basilisk and GMR-Bootstrapping, we design several experiments on official corpus of COAE 2008 (Chi-nese Opinion Analysis Evaluation) [13], which contains about 3000 texts (including both tests and training parts). All the words in the corpus are divided into four seman-tic categories: automobile, notebook, digital camera and cell phone. We compare MSGA-Bootstrapping with Basilisk and GMR-Bootstrapping on all of these fields. In order to ensure the fairness of the experiment, we add CWD also in MSGA, GMR and Basilisk. The results we show next are ruled out the influence of this proc-essing, and they just show the improvement of MSGA-Bootstrapping to Basilisk and GMR-Bootstrapping on scoring patterns and words. Figure 4 presents results of MSGA-Bootstrapping, Basilisk and GMR-Bootstrapping. Table 1 shows the results of some specific point of the experiment. For seed words. It is the same as the Thelen and Riloff (2002) X  X  method, and we ran the algorithm for N iterations, so 5*N words ar e extracted. The X axis shows the number of words extracted. The Y axis shows the number of correct ones. From the results we know that MSGA has a better performance on both precision and stability, contrasting with GMR and Basilisk. More specifically, MSGA-Bootstrapping receives better results than GMR-Bootstrapping and Basilisk do in all four fields of our experiment. For precision, MSGA often gets a result more than 10 percent higher than GMR, and even more than 30 percent higher than that of Basilisk. For stability, we saw GMR and Basilisk obtain puses. MSGA, on the other hand, retains a stable and excellent result on all corpuses. 
By analyzing the experimental result, we found that the effect of segmentation di-rectly restricted the operation of our algorithm results. Some words were wrongly words in semantic lexicons correctly, b ecause we use pos tag as our patterns. In this paper, we present an approach for lexical acquisition using a novel bootstrap-ping method called MSGA-Bootstrapping. We show the impact of both the quality information and quantity information of words and patterns when scoring the words and patterns created by them. Based on this idea, we improve the algorithm of scoring patterns and words so that it contains both quality information and quantity informa-tion. Experiments show that with this improvement MSGA-Bootstrapping signifi-cantly outperforms Basilisk and GMR-Bootstrapping in COAE corpus. By changing the pattern format, we also make our algo rithm capable of processing corpuses of more languages, while Basilisk and GMR-Bootstrapping can only deal with English corpus. Furthermore, we add a new processing by using CWD in MSGA-Bootstrapping, which can prevent words without any field-related information from being added to semantic lexicons acquired by MSGA-Bootstrapping. 
In the future, we will mainly improve our algorithm in two aspects: one is to add a segment; another is to explore a new appro ach to use our algorithm as an unsuper-vised method that needs no field-related information to acquire semantic lexicons. 
