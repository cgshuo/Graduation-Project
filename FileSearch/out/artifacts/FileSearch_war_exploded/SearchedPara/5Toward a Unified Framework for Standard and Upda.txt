 HONGLING WANG and GUODONG ZHOU, Soochow University Multi-document summarization (MDS) aim s to produce a single summary from a set of documents. Usually, it can be described as a three-step process: selection of salient portions of text, aggregation or abstraction of the salient information, and presentation of the summary text [Jones 1999; Mani and Bloedorn 1999].

Over the past few years, MDS has drawn more and more attention and made much progress. However, various evaluations indicate that MDS is highly complex and de-manding, and there is still a long way for automatic summarizers to catch up with human beings [Dang and Owczarzak 2008]. Currently, there exist two major issues with respect to MDS:  X  How to detect and select salient information to be included in the summary;  X  How to control the redundancy in the final summary, given a set of documents. To address these two issues, this article presents a purely statistical framework for both standard and update MDS in an extractive way. In particular, a topic modeling approach is employed for salience determination using the topic distribution informa-tion and a dynamic modeling approach is proposed for redundancy control using the similarities between various kinds of text units, such as sentence, summary, docu-ment, and documents. For salience detection, the intuition behind the topic model-ing approach is that the summary X  X  topic probability distribution should be similar to the documents X  topic probability distribution. For various topics contained in the documents, the central topic represents the main theme and the other topics sup-port around the central one. Together, the central topic and the other ones form a topic probability distribution for the documents. Here, we ignore the summary focus (a short description of desired summary) and a topic is defined as a weighted  X  X ag-of-words X  rather than the summary focus while the topic modeling approach is employed to derive the hidden topics from a set of documents, either the given documents or a related corpus. In this way, we can represent various kinds of text units, such as word, sentence, document, documents, and summary, using a single vector space model via their corresponding probability distributions over the derived topics, and calculate the similarity between any two text units via their topic probability distributions. For re-dundancy control, the dynamic modeling approach is proposed for both standard and update summarization. In particular, for standard summarization, we consider the similarity between the summary and the given documents, and the similarity between the sentence and the summary, besides the similarity between the sentence and the given documents while for update summarization, we also consider the similarity be-tween the sentence and the history documents or summary. Finally, the summary is generated greedily by selecting the sentences according to linear interpolation of these similarity scores in a dynamic way.
 The rest of this article is as follows. Section 2 gives an overview of related work. Section 3 gives a brief introduction to topic modeling, in particular LDA, and presents the topic-driven similarity between any two text units. Section 4 presents our MDS framework in details. Section 5 and Section 6 present experimental results on both English and Chinese MDS respectively. Finally, Section 7 draws a conclusion. At present, the literature of summarization has grown to a level which is very hard to overview in detail [Jones 2007]. However, we can still identify some critical commonali-ties in the way of extracting and producing salient information into output summaries. Generally, summarization methods can be ca tegorized into either extraction-based or abstraction-based. Since this article deals only with extraction-based summarization, we only give an overview of the literature in extraction-based summarization.
In the literature, the development of summarization has been largely promoted by Document Understanding Conferences (DUC) and Text Analysis Conferences (TAC) 1 . Recently, new summarization tasks, such as update summarization [Dang and Owczarzak 2008] and comparative summarization [Wang et al. 2009] have been pro-posed. Here, update summarization is a task relative to standard summarization and the update summary is generated from a set of documents, under the assumption that the user has already read a set of history documents. In the literature, the standard summary is also called main summary in DUC 2007 and initial summary in TAC 2008 and 2009.

In the following paragraphs, we will overview the state-of-the-art in summarization from the view points of salience determination and redundancy control. Besides, we will also make a brief overview for the literature of summarization in Chinese language. To determine the salience of information, researchers have used various positional and structural properties of the judged sentences with respect to the source texts. These properties can be the sentence position in a document, the fact that a sentence is part of the title or the abstract of a document [Edmundson 1969; Radev et al. 2000], and the relation of sentences with respect to a user-specific query or a specified topic [Park et al. 2006; Varadarajan and Hristidis 2006].

Following the bag-of-words assumption, a sentence in the given documents is often represented as a vector of features, such as the sentence position, the sentence length, the cosine similarity between the sentence and the document title, and the sentence X  X  TF-IDF value, in summarization [Torralbo et al. 2005]. In other cases, further analy-sis is performed, aiming to reduce the dimensionality and produce a vector in a latent topic space [Bhandari et al. 2008; Steinberger and Jezek 2004]. The vector represen-tation can be exploited for measuring the semantic similarity between information chunks by using various measures, such as the cosine distance and Euclidean distance between vectors. Such a vector representation can be effectively exploited to measure the salience of a sentence.

In this article, different kinds of text units, such as word, sentence, document, doc-uments, and summary, are all represented as a probabilistic distribution vector over the inherent topics in the given documents or a related corpus. Here, topic modeling is deployed as a clustering tool to derive the inherent topics. In this way, each sentence can be represented accordingly.

Similar to our work, Haghighi and Vanderwende [2009], Arora and Ravindran [2008b], and Bhandari et al. [2008] use a topic model to represent a sentence. Haghighi and Vanderwende [2009] utilize a hierarchical LDA-style model to represent a topic as a hierarchy of topic vocabulary distributions. Each sentence is then represented us-ing three kinds of topic distributions, that is, the background vocabulary distribution, the content distribution, and the document-specific vocabulary distribution. Arora and Ravindran [2008a] use LDA to find different topics in the documents and include the probability of a topic as a feature in sentence presentation. Arora and Ravindran [2008b] further improve the performance by using SVD to find a better topic represen-tation for a sentence. Bhandari et al. [2008] use PLSI, another popular topic modeling tool, to divide a document into several topics and include as a feature the single prob-ability that the sentence occurs in the major topic.

Among others, Nastase [2008] defines a topic using a set of sentences or questions (i.e. query). The disadvantage of this approach is that it assumes a large amount of (related) manual summaries in advance, which is impractical for automatic summa-rization. In comparison, this article automatically derives the topic probability dis-tribution from a document collection using a topic modeling tool, which is scalable to automatic summarization on a large number of documents.

With the progress of graph-based learning in the machine learning community, graph-based methods have been drawing more and more attention in recent years [Erkan and Radev 2004; Mihalcea 2005]. Graph-based methods view each sentence as a node in a graph and the similarity between sentences as links between corresponding nodes. In particular, the sentences are ranked using some graph ranking algorithms such as HITS [Kleinberg and Authoritative 1998] and PageRank [Brin and Page 1998]. However, such graph ranking algorithms tend to give the highest rank to the sentences which are related to the central topic in th e documents. Therefore, these algorithms tend to choose those sentences related with the central topic and ignore other sentences related with other topics. This makes the e xtracted summary fail to cover other useful topics in the documents. This problem is particularly serious in multi-document sum-marization. Another drawback is that such algorithms are normally very complicated, especially given a large number of documents for summarization. As the core component in MDS, sentence selection has drawn most attention in the literature. There are two main methods to sentence selection: ranking and clustering [Dang and Owczarzak 2008]. In the first method, the sentences in the given documents are ranked and those top-ranked sentences are selected to form the summary. In the second method, the sentences are clustered and a central sentence is extracted from each cluster to form the summary.

One key problem here is how to control the redundancy. In fact, the research on redundancy control has given birth to MR (Marginal Relevance) and MMR (Maximal Marginal Relevance) sentence selection criteria [Carbonell and Goldstein 1998]. For example, MMR chooses the sentences according to the weighed combination of their general relevance with the document and their redundancy with the sentences already chosen. Alternatively, CSIS (Cross-Sentence Informational Subsumption) determines whether and in what degree a sentence has been contained in another sentence al-ready chosen in the summary [Radev et al. 2000]. Among others, Allan et al. [2003] use statistical features of the judged sentences with respect to the sentences already chosen in summary to avoid repetition.

In particular, among topic-driven approaches, Haghighi and Vanderwende [2009] greedily augment the sentences to form a summary via minimizing the KL-divergence between the summary and the documents. Arora and Ravindran [2008b] use SVD to find the most orthogonal sentences over the topic distribution to form the summary. Bhandari et al. [2008] extract the sentences from the highest-ranked topic as the ba-sic summary and gradually combine the sent ences from different topics to form the final summary. Although this approach is successful in single document summariza-tion, it may not be readily applied to multi-document summarization. The reason is that it is difficult for this approach to cope with the huge redundancy in the given documents.

Among others, Nastase [2008] generates a summary based on the probability dis-tribution of unigrams in human summaries. The disadvantage of this approach is that it assumes a large amount of (related) manual summaries in advance, which is impractical for automatic summarization.

In this article, a dynamic modeling approach is proposed to control the redundancy for both standard and update summarization given a set of documents. For standard summarization, we extend Haghighi and Vanderwende [2009] by considering both the similarity between the sentence and given documents, and the similarity between the sentence and the summary, besides the similarity between the summary and given documents. For update summarization, we also consider the similarity between the sentence and the history documents or summary. In particular, the KL-divergence is employed to measure the similarity between any two text units. Finally, the sentences are chosen greedily to form a summary according to linear interpolation of these simi-larity scores.

In summary, although some studies have used the topic distribution information in summarization, our study extends the idea to use such information in redundancy control via a dynamic modeling approach for both standard and update MDS in a unified framework. Compared with the large number of studies on English MDS, there are only a few on Chinese MDS, much due to the lack of publically-available annotated corpora and evaluation criteria. Representative works include Xu et al. [2007] and Liu et al. [2006].
Inspired by Radev et al. [2001], Xu et al. [2007] propose a multi-document frame-work, which first simplifies the traditional multi-document representation model in the cross-structure theory and then supplements the change and distribution informa-tion about event topics absent from the information fusion theory.

Liu et al. [2006] propose a MDS model to both maximize the coverage of topics and minimize the redundancy of contents. They fi rst extract a set of concepts by combin-ing semantic analysis and various statistical techniques to represent the information content of documents and then calculate the relevance between sentences based on concept cohesion. In this way, the relationship among sentences and documents are established. This section briefly introduces topic modeling and presents the topic-driven similar-ity between any two text units using the topic model derived from a topic modeling approach. Among various topic modeling approaches, Latent Dirichlet Allocation (LDA) [Blei et al. 2003] has drawn the most attention recently in the NLP community and has been applied successfully in topic detection. In principle, LDA is a generative three-level hierarchical Bayesian probabilistic model for analyzing the content of documents and the meaning of words. Similar to other topic models, such as LSA, PLSA, and PLSI, LDA assumes that documents are mixtures of topics and a topic can be repre-sented as a probability distribution over words. In this article, we use LDA to capture the topics in the documents. In particular, we employ the LDA toolkit 2 developed by Phan and Nguyen in our multi-document summarization system. Since LDA assumes a topic to be a weighted  X  X ag-of-words X  and we don X  X  involve any grammatical informa-tion, our approach is purely statistical.

Mostly, the basic LDA model will be extended to a smoothed version to gain better result. The plate notation is shown in Figure 1.  X  is the parameter of the uniform Dirichlet prior on the per-document topic distributions.  X  is the parameter of the uni-form Dirichlet prior on the per -topic word distribution.  X  i is the topic distribution for document i, z ij is the topic for the jth word in document i, and w ij is the specific word. w ij is the only observable variable, and the other variables are latent variables. K de-notes the number of topics considered in the model and  X  is a K*V (V is the dimension of the vocabulary) Markov matrix each line of which denotes the word distribution of atopic.
In this article, we investigate two ways to apply LDA. One is to derive a single topic model using all the document sets in the given corpus (called Generic Topic Model). Another way is to derive a topic model for each particular document set under consid-eration (called Specific Topic Model). Here, both LDA models use the following default parameters:  X  =50 / K , X  =0 . 1, where K stands for the number of topics. In particular, K is fine-tuned to the number of document sets for the generic topic model and the number of documents in the particular document set for the specific topic model while the number of iterations is fine-tuned to 2000, using the DUC 2007 English evaluation corpus as the development data. For details, please refer to Section 5.1. Given any two text units a and b, the topic-driven similarity between them is calcu-lated in this paper as, where P a and P b are the probability distributions of text units a and b over the derived topics respectively. Here, D KL ( P a || P b ) measures the Kullback-Leibler (KL) divergence [Kullback and Leibler 1951] between two probability distributions P a and P b as follows: Since the KL divergence is asymmetric, both D KL ( P a || P b )and D KL ( P b || P a )are included to guarantee the symmetry of the topic-driven similarity measure.
Given any text unit, its topic probability distribution can be easily determined from the topic model using the topic distribution P(Z | D), where D indicates the given docu-ment set and Z indicates the K derived topics, and the word distribution for each topic z  X  Z ( i =1 , 2 ... K ), that is, the topic-word distribution P(W | z set of words (i.e., bag-of-words) considered in the topic model. For example, assume X is any text unit and P X represents X X  X  probability distribution over the K derived topics, that is, where P ( z i | X) stands for the probability of text unit X belonging to topic z i and can be computed as Where x j is the j-th token in text unit X and P(X) is a normalization factor, which can be computed as Figure 2 illustrates the unified framework for standard and update MDS. Please note that various kinds of preprocessing , such as stemming, stop-word removal, word segmentation, and sentence segmentation are called upon beforehand. This section focuses on salience determination in how the salience of various kinds of text units is determined using the topic model derived from the topic modeling approach, and standard/update summary generation in how a dynamic modeling approach is employed to remove the redundancy. Given the topic-driven similarity between any two text units as described in Section 3.2, the salience of a sentence s can be easily measured by the topic-driven similarity between the sentence and the given documents, TSim(s, Docs) .

In the literature, some popular features have been widely used and proven impor-tant for the success of MDS [Dang and Owczarzak 2008], for example, the sentence position, the sentence length, the cosine similarity between the sentence and the doc-ument title (using the bag-of-words representation), and the sentence X  X  TF-IDF value. Due to the failure of our topic-driven similarity to cover such useful information, we adjust the salience score of a sentence via linear interpolation of those popular fea-ture scores with the original salience score as follows, for example, the topic-driven similarity between the sentence S r and the given documents Docs : where w 1 , w 2 , w 3 ,and w 4 are relative weights of the following popular feature scores. (1) Pos ( S r ). For the sentence position, early sentences are considered more likely to (2) Len ( S r ). For the sentence length, since very short and very long sentence are con-(3) CosSim ( S r , Title ). The title of a document often contains important information. So (4) Tfidf ( S r ). The TF-IDF score has also been widely used in information retrieval to
In this article, w 1 , w 2 , w 3 ,and w 4 are fine-tuned to 2, 0.3, 0.9, and 0.3, respectively, using the DUC 2007 English evaluation corpus as the development data. For details, please refer to Section 5.1. In this article, we explore two approaches in generating the standard summary: a static modeling approach and a dynamic modeling approach. Compared with the for-mer baseline approach, the latter further controls the redundancy in a dynamic way. 4.2.1. Static Modeling. The static modeling approach extracts the sentences with the highest salience scores to produce the summary in the following way. (1) Run LDA to construct the topic model. (2) Compute the salience score of each sentence, i.e. the similarity between the sen-(3) Order the sentences accord ing to their salience scores. (4) Pick up the sentences with the highest salience scores in the descending order and 4.2.2. Dynamic Modeling. The dynamic modeling approach is proposed to remove the redundancy in the given documents. It incrementally chooses a sentence and aug-ments it into the summary to maximize th e similarity between the augmented sum-mary and the given documents in a dynamic way. In other words, whenever a sentence is added to the summary, the similarity between the augmented summary and the given documents is calculated. In particular, the sentence which makes the summary most similar to the given documents is picked up to augment the summary. In this way, redundant information is avoided effectively. Compared with the above static modeling approach, the dynamic modeling approach generates the summary in the following dynamic way. (1) Run LDA to construct the topic model, co mpute the salience score of each sentence (2) Pick the top-scoring sentence as the initial summary. (3) Pick the sentence which maximizes the similarity score between the augmented (4) Repeat last step until the summary has reached the size limitation.
 Figure 3 illustrates the dynamic model for standard summary generation, which cal-culates the salience score of a sentence S via linear interpolation as Here, besides the similarity between the augmented summary and the given docu-ments ( f 2 ), we also consider both the similarity between the sentence S and the given documents ( f 1 ) (i.e., the salience score as shown in Equation (6)), and the similarity between the sentence S and the augmented summary ( f 3 ). Obviously, it becomes a static model when  X  2 =0 , X  3 = 0, and the simple dynamic model as described in Haghighi and Vanderwende [2009] when  X  1 =0 , X  3 = 0. In this article, the parameters are fine-tuned to  X  1 =1 . 5 , X  2 =0 . 4 , X  3 =  X  0 . 1 in standard summarization using the DUC 2007 English evaluation corpus as the development data. For details, please refer to Section 5.1. Since an update summary is generated under the assumption that the user has already read a history set of documents, we extend the dynamic model in standard summary generation to consider the similarity between the sentence and the given history docu-ments or simply the corresponding history summary for update summary generation. Figure 4 illustrates the idea, where DocSet A, Summary A, DocSet B, and Summary B indicate the history set of documents, the history summary (for DocSet A), the up-date set of documents and the current summary (for DocSet B), respectively. In this model, a sentence is scored by linearly interpolating the similarity between the sen-tence S and the current documents ( f 1 ), the similarity between the current summary and the current documents ( f 2 ), the similarity between the sentence S and the current summary ( f 3 ) and the similarity between the sentence S and the history documents ( f 4 ) or the history summary A ( f 5 ).

Obviously, this dynamic model becomes t he static model for standard summary gen-eration when  X  2 =0 , X  3 =0 , X  4 = 0, and the dynamic model for standard summary generation when  X  4 = 0. Besides, we could replace f 4 with f 5 by simply considering the similarity between the sentence S and the history summary (dynamic update with history summary) instead of the similarity between the sentence S and the history documents (dynamic update with history documents). In this article, all the history reference summaries generated by the human annotators are included.

In this article, the parameters are fined-tuned to  X  1 =0 . 4 , X  2 =1 . 5 , X  3 =  X  0 . 1 , X  4 =  X  0 . 1 for dynamic update summarizati on with history summary and  X  1 =0 . 4 , X  2 = 1 . 5 , X  3 =  X  0 . 1 , X  5 =  X  0 . 2 for dynamic update summarization with history documents, using the DUC 2007 English evaluation corpus as the development data. For details, please refer to Section 5.1. In this article, we have systematically evaluated our unified framework on TAC 2008 for both standard and update summary generation. Besides, we also report the perfor-mance on TAC 2009. The update summarization task at TAC 2008 consisted of two kinds of summaries: a history summary and an update summary. Here the history summary is a standard summary. The documents for TAC 2008 are retrieved from the AQUAINT-2 collection of newswire articles. There are 48 document sets, each of which contains 20 AQUAINT-2 documents. Besides, the retrieved documents in each document set describe the same theme and are ordered chronologically and divided into two sets of 10 documents each, such that Set B follows Set A in the temporal order, with Set A for standard sum-marization and Set B for update summarization. The task is to produce a 100-word summary for each set, guided by a statement describing the reader X  X  need for informa-tion (not explored in this article). Therefore, a total of 96 summaries will be produced, half of them as standard summaries and the other half as update summaries. Similar to TAC 2008, TAC 2009 is also an update summarization task and includes 44 docu-ment sets, each of which contains 20 documents. An example statement, including a title and a narrative, is shown here.
For evaluation, we use the ROUGE toolkit [Lin and Hovy 2003] provided by TAC 2008 3 . Rouge-1 (recall against unigrams), Rouge-2 (recall against bigrams) and Rouge-SU4 (recall against skip-4 bigrams) sco res at the 95% confidence level are computed by running ROUGE-1.5.5, where Rouge-2 and Rouge-SU4 are automatic ROUGE eval-uation scores in TAC 2008. In order to compare our model with others, we extract a 100-word summary as required by TAC 2008 and TAC 2009.
In all the experiments, all the documents are pre-processed using an in-house, state-of-the-art sentence segmentation and word tokenization toolkit. Besides, stop words are removed using a popular stop word list 4 and all the remaining words are stemmed using the Porter stemmer 5 . For model development, we utilize the DUC 2007 English evaluation corpus (including 45 document sets each of which contains 25 documents) as the development data to fine-tune all the free parameters, including the number of topics K, coefficients w s in Equation (6) for adjusting the salience score via linear interpolation with the popular feature scores, and coefficients  X  s in the dynamic models for both standard and update summarization.

In particular, the number of topics K is fine-tuned to the number of document sets in the given corpus for the generic topic model and the number of documents in the particular document set for the specific topic model 6 , respectively, using the DUC 2007 English evaluation corpus as the development data. This is consistent with our intu-ition that there should be at least one particular topic for each document set in the generic topic model and each document in the specific topic model since each of them should have its own specialty. For MDS, it is reasonable to set the number of topics according to the number of document sets for the generic topic model and the number of documents in the particular document set for the specific topic model 7 . Table I presents various Rouge scores using different models on TAC 2008 Set A (stan-dard summarization). It shows that the specific topic model performs better than the generic topic model. It indicates that the obtained topic probability distribution is more accurate when LDA is applied on a small-scale highly-relevant data. This may be re-lated to the structure of the corpus, which contains 48 document sets with 20 docu-ments each. Each document set has a central topic and thus the entire corpus has 48 central topics. When applying LDA on each document set, the derived topic prob-ability distribution may better reflect the natural distribution of all the topics for the given document set. As expected, Table I shows that the dynamic model outperforms the static model due to redundancy control. Since our framework performs better when applying LDA on each document set using the dynamic model, we use the specific topic model and the dynamic model by default in all the following experiments.
Tables II and III compare various static and dynamic models in redundancy control on Set A and Set B. Here,  X  X tandard X  (for Set B) means that we treat the update sum-marization task as the standard summarization task without considering the history documents or summary. Moreover, the content in parentheses indicates the exact set of topic-driven similarities considered in the corresponding model. As expected, it shows that the dynamic model also works well for update summarization. It also shows that the dynamic update model with history documents much outperforms the one with history summary. From the statistical point of view, a summary always contains only part of information in a set of documents. Moreover, it shows that considering both history summary and documents further improves the performance. In all the follow-ing experiments on update summarization, we employ the dynamic model with history summary and documents by default.

In order to better evaluate the advantage of the dynamic model in redundancy control on standard summarization, we compare it with a state-of-the-art statistical model, which uses various statistical characteristics of the sentence with respect to sentences already included in the summary to avoid repetition [Larkey et al. 2003]. Here, two features are used: one is the number of words repeated and another is the cosine similarity of TF-IDF between two sentences. Table IV compares the statistical model with our dynamic model in removing the redundancy of the summary. It shows that, while the statistical model only slightly improves the performance, the dynamic model much outperforms the statistical one. It is also interesting to notice that the statistical model fails to complement the dynamic one via linear interpolation. It may be due to that the two features employed in the statistical model have been captured by the dynamic model in a better (more systematic) way.

Figure 5 evaluates the performance when different numbers of words are consid-ered in the summary. As expected, all the Rouge scores increase monotonously as the summary length increases and almost dou ble when increasing the summary length from 100 words to 400 words. From the statistical point of view, the more the number of words included in a summary, the more similar the probability distribution of a summary to that of documents over the derived topics is expected.

In the preceding experiment, we adjust the salience score by linearly interpolating the topic-driven similarity score with the popular feature scores, as shown in Equation (6). Table V examine the complementary nature of the topic-driven similarity and the popular features. It shows that, although the popular features contributes much less than the topic-driven similarity on most experiments, they are much complementary to each other and linear interpolation of the topic-driven similarity score with the popular feature scores much improves the performance in all experiments. This is due to the failure of the topic-driven similarity to cover such popular features.
Finally, Table VI compares several widely-used similarity measures. It shows that the KL-divergence performs best while the cosine measure performs worst. Table VII presents the performance of various static and dynamic models on the TAC 2009 update summarization task, which is consistent with our experiments on the TAC 2008 update summarization task. This suggests the effectiveness of the dynamic model in both standard and update summarization. Table VIII compares our system with the best-reported systems on the TAC 2008 and 2009 update summarization tasks.

Compared to the official experimental results (Rouge-2 and Rouge-SU4) published by the TAC 2008 organizers [Dang and Owczarzak 2008], our system outperforms the best system on the TAC 2008 update summarization task [Gillick et al. 2008] (0.10412 vs. 10381 in Rouge-2, 0.13798 vs. 0.13625 in Rouge-SU4 score). Although our system performs worse than the best system on the TAC 2009 update summarization task [Gillick et al. 2009], our system is much simpler and thus much more flexible for fur-ther improvement.

However, the performance gap is still la rge compared to human beings. One ma-jor reason may be that, although we consider the sentence length, our approach still prefers long sentences. This raises the necessity of effective summary compression, which is worth exploring in the future. Actually, Gillick et al. [2008, 2009] much bene-fit from a sentence compression module in post-processing. Since our proposed MDS framework is purely statistical and language-independent, it can be easily applied to MDS in Chinese language. However, due to the lack of annotated corpora on update summarization in Chinese language, we only focus on standard summarization in Chinese language. Currently, there are few publically available corpora on Chinese MDS. For fair com-parison, we use the corpus described in Xu et al. [2007]. Actually, this is the only publically available corpus with reported performance in Chinese MDS. This corpus comes from online news reports with topics covering sports, economic, emergency, etc., and contains 19 document sets, each of which includes 5 X 10 documents.

In particular, golden summary sentences in each document set are marked as well as candidate sentences which could replace summary sentences but couldn X  X  be con-current with those golden summary sentences. Moreover, each candidate sentence is given a corresponding weight value between (0, 1) according to its degree of replace-ment. Based on such annotation, three criteria are used to measure the quality of the evaluation summary: precision (P), re dundancy (R) and total quality (T). Where K is the total number of sentences in the evaluation summary, k 1 is the number of golden summary sentences in the evaluation summary; (  X  1 , X  2 , ...,  X  k ) is the weight of the sentence (annotated manually in the corpus); and  X  ( s i , s j ) is a binary discrimi-nation function, 1 if s i and s j are the same class summary sentences, 0 otherwise.
In this article, we adopt the ICTCLAS 2009 system 8 for word segmentation. Due to the failure to having another publically available Chinese MDS corpus as the develop-ment data to fine-tune the free parameters, we simply adopt the experimental setting fine-tuned using the DUC 2007 English evaluation corpus as the development data. Table IX presents the performance of our MDS framework. It shows that the dynamic model always outperforms the static model. This indicates the effectiveness of the dy-namic model in Chinese summarization, similar to English summarization. It also shows the effectiveness of the dynamic model in removing the redundancy of Chinese summarization.

Table X gives the performance of an upper bound system and a state-of-the-art multi-document framework (MDF), both described in Xu et al. [2007], on the same Chinese data. In the upper bound system, the summary is extracted based on some information marked by hand, while all of the information in MDF is automatically gen-erated. Comparison of Table IX and Table X shows that our MDS framework (using the dynamic model) much outperforms MDF. In this study, we present a unified framework for both standard and update summa-rization, which adopts a topic modeling approach for salience determination and a dynamic modeling approach for redundancy control. In particular, the topic modeling approach is based on the probability distribution over the derived topics, which applies to various kinds of text units, such as word, sentence, document, multi-documents and summary. The dynamic modeling approach considers both the similarity between the sentence and given documents, the similarity between the sentence and the summary, besides the similarity between the summary and given documents, for standard summarization, and is further extended to consider the similarity between the sentence and the history documents or summary, for update summarization. Eval-uation on TAC 2008 and 2009 shows promising results. In addition, we also evaluate our framework to Chinese summarization. Evaluation shows the effectiveness of our framework on Chinese summarization. In particular, the dynamic modeling approach is very effective at redundan cy control on both English and Chinese summarization.
The main contribution of this article lies in the dynamic modeling approach for redundancy control in MDS, w hich is simple but effective.
In the future, we will explore better ways in integrating various similarity scores be-tween different kinds of text units. In addition, we will explore summary compression to further improve the quality of both English and Chinese MDS.

