 Query recommendation is an invaluable tool for enabling users to speed up their searches. In this paper, we present algorithms for generating query suggestions, assuming no previous knowledge of the collection. We developed an on-line OLAP algorithm to generate query suggestions for the users based on the frequency of the keywords in the selected documents and the correlation between the keywords in the collection. In addition, performance and scalability expe r-iments of these algorithms are presented as proof of their feasibility. We also present sampling as an additional ap-proach for improving performance by using approximate re-sults. We show valid recommendations as a result of com-binations generated using the correlations between the key -words. The online OLAP algorithm is also compared with the well-known Apriori algorithm and found to be faster only when simple computations were performed in smaller collections with a few keywords. On the other hand, OLAP showed a more stable behavior between collections, and al-lows us to have more complex policies during the aggregation and term combinations. Additionally, sampling showed im-provement in the time without a significant change on the suggested queries, and proved to be an accurate alternative with a few small samples.
 H.2.7 [ Database Management ]: Database Administra-tion X  Data Warehouse and Repository Algorithms, Experimentation OLAP, IR, Query Processing CCF 0937562 and IIS 0914861.

There have been numerous approaches for improving the user experience while browsing digital collections [9]. ,Q uery recommendation has been used extensively to provide clever hints or suggestions for concepts related to a given query [4]. Query recommendations are usually based on the anal-ysis of  X  X nown queries X  stored in query logs. Nevertheless, query suggestion without any prior information or X  X nbiase d query suggestion X  requires the analysis of vast amounts of data, usually performed in a precomputed way (e.g. clus-tering). As a result, an online analysis of the selected docu -ments may look like an unfeasible option. The base assump-tion is that users will often start their searches with na  X   X ve queries, which are somehow related to the target query (e.g.  X  X hicago Bulls X  can lead to  X  X ichael Jordan X ), and the tar-get query is reached when the user begins to understand the collection. Given this, the resulting documents can be ana-lyzed to return a set of queries that will allow the users to pick any of the suggestions to refine their searches.
In order to provide such recommendation features, we ex-ploit the knowledge discovery properties of On-Line Analyt -ical Processing (OLAP)[7]. OLAP can be used in query rec-ommendation to build data cubes containing the frequency of candidate sets of keywords to be suggested to the user. A similarly efficient approach is conducted by using the well-known Apriori algorithm, widely used in association rules. Although both methods are quite efficient in the computa-tion of aggregations, performing such a task in large collec -tions may be time prohibitive, especially if an immediate result is needed. Therefore, we decided to use sampling as an alternative for managing large collections in a  X  X anilla  X  universe. In addition, the validity of our recommendations is improved by the computation of the correlation of the terms in the collection and analysis of the frequency of occurrenc e of the recommended terms in the subset of documents.
This paper is divided as follows: In Section 2, we intro-duce the definitions. Section 3 presents algorithms used for obtaining valid query recommendations. Section 5 discusse s previous work in this area. Finally, Section 6 contains our final remarks and future work.
Let us focus on defining the notation. Let C be a col-lection, or corpus, of n documents { d 1 , d 2 , . . . , d document d i or query q is composed of a set of terms t ij addition, let x i be a frequency vector of all the different key-words in C for each document d i . The collection is stored in an inverted list format within the table vtf that contains the document id, term and position in the i th document. A summarization table tf is computed from vtf as table with the document id i , the term t , and the term frequency f stored. In addition, let  X  d be a subset of documents, b subset of terms from tf , and d vtf a subset of vtf .
The backbone data structure for the OLAP data cubes is the dimensional lattice, which has a size of 2  X  t  X  1 , where the number of terms. One level (or depth in the lattice) of the cube is denoted by |  X  t |
Our query recommendation analysis process is the result of the aggregation of the frequencies of a set of keywords (that have a positive correlation) appearing in a subset of documents. In practice, it is not feasible to obtain the com-binations of all the keywords in a set of documents. There are often many keywords have a tractable problem. Thus, in order to speed up the execution of the process, we select the  X  d most important documents of the collection based on an initial user query. Unfortunately, even with an abridged se t of documents, the number of keywords in such documents can still be considered too overwhelming for efficient corre-lation analysis to be conducted. As a result, we also obtain the top-k keywords from these  X  d documents and apply an importance metric. With these objectives, we adapted two different approaches: using the Apriori algorithm and using an online OLAP algorithm. Since the Apriori algorithm is well known, we will only cover how we altered the OLAP cube algorithm to efficiently compute recommendations.
The correlation will represent how related the frequency of one keyword is with another in terms of the whole collection. We adapted an efficient one-pass method to compute the correlation values of unique terms in the collection [6]. In addition to the previous definitions, let L be an additional set containing the total sum of all the different terms in the collection ( L = triangular matrix containing the squared measures between the terms ( Q = of a pair of terms a and b is obtained by applying  X  ab =  X 
Once the combinations have been created and verified for having valid correlations, a final analysis of the importanc e of the concept can be performed by summarizing the oc-currences of such combinations of terms in the subset of ranked documents. We propose three different techniques for ranking these recommendations: a single match method, a distance method, and a correlation method.

The single match technique is built on performing a sin-gle scan of the data counting all the occurrences based on the appearance of the set of terms in a document. Therefore, the occurrence of a combination will be represented by the MIN frequency of any of the terms in the combinations within a particular document. A user-defined threshold is used to filter the final results. The distance match method, also called proximity match, takes into account the position of the terms in the document and counts the number of times a set of selected terms in the combination appear within a cer-tain user-defined distance,  X  . As a result, highly-ranked rec-ommendations will imply that the concept holds a stronger meaning due to the proximity of the terms. The correla-
Input : level , b t 1 : Init data structure S 2 : Init List tf 3 : foreach &lt; i, b t ,v &gt; r in  X  tf sorted by i 4 : if i changed 5 : List tf  X  { r.t, r.v } 6 : else 7 : C  X  GetNextCombo ( List tf , level ) 8 : while isCorrelated ( C ) 9 : if checkTolerance ( v , tol ) 10: S C ++ 11: else 12: S C  X  1 13: C  X  GetNextCombo ( List tf , level ) 14: return S
Figure 1: One-level online OLAP Cube algorithm. tion method uses the correlation table to make recommen-dations. In this approach, the validity of a set of keywords depends on its correlation with the initial query. We ob-tain the correlation between each keyword of the user query with each keyword of the candidate set. From these, an average correlation is then obtained to determine if the key -word set should be recorded. The difference between these three methods is in how the initial query is handled. For the frequency and distance methods, the initial query is used to filter the document collection into a more manageable size. While the same approach is used by the correlation method, it also takes into account the correlation between the initi al user query and the keyword set candidates.
OLAP techniques can be used to efficiently obtain query recommendations. Figure 1 shows a general online algo-rithm for the computation of aggregations on keywords in the vertical format. In this algorithm, the structure S is a data structure that is used to store the combinations of key-words along with their frequency. The main input table for this algorithm is the keyword table, which always contains document id and keyword columns. The third column, v , changes depending on the final filtering method.

The validity of a set of keywords is determined by the fil-tering method: single match, distance, or correlation. For the single match method, v is the frequency of the keyword. We select the minimum frequency of the keywords to repre-sent the set. The distance method bases the validity of a set of keywords on their positioning. In this case, v represents the position of the keyword in the document. The correla-tion method bases the validity on the correlation between the initial query and the sets of keywords. The average cor-relation of the initial query and the candidate set is obtain ed by retrieving them from the correlation tables. If multiple levels of the lattice are desired, the online OLAP algorithm can take a bottom-up or top-down approach.

While our algorithm may seem similar to the Apriori algo-rithm, there are two major differences. First, our algorithm uses main memory to compute all possible combinations us-ing only one pass while the Apriori algorithm requires one pass for each group of itemsets. Second, our one-pass algo-rithm does not prune the results while the Apriori algorithm is able to prune between different iterations.
To further improve the efficiency, we decided to explore the effect of sampling on both performance and accuracy. The first step is to obtain a smaller list of documents,  X  based on  X  ( q, tf ), where  X  is the ranking function based on the query and the original set of documents. From this set, we draw  X  samples, each having a  X  d = { d 1 , d 2 , . . . , d replacement from the result of  X  ( q, tf ). The probability of any document in  X  d of being selected in the  X  samples is P (1  X  (1  X  1 /m )  X  ). Each of the  X  d samples of documents is then used as a source for obtaining its top-k keywords b t and computing the techniques. Finally, the scores for each combination ar e obtained as the average of the frequencies. Notice that the set of keywords b t is different between samples. We calculate accuracy as the percentage of total combinations of the top k keywords that are found through the course of the iterations .
The experiments were performed on an Intel Xeon E3110 server with 4 GB of RAM. The algorithms were implemented in SQL and UDFs. Four different collections were used dur-ing the experiments (see Table 1). The first collection is a subset of the ACM Digital Library. The NYTIMES and PUBMED were obtained from the UCI Repository. The Texas Water Well Data Set of Texas (TWWD) was used only for analyzing the quality of the results.

The quality of the results was validated in TWWD with ten initial keyword queries (air, america, dissolved, info r-mation, quality, residential, texas, treatment, waste, an d water). With these queries, we recorded the query rec-ommendations using the frequency, the distance, and the correlation. Table 2 shows the query recommendation re-turned by the initial query for  X  X nformation X . Of these thre e approaches, the correlation method definitely returned the most accurate results. In this case, we consider a recom-mendation accurate if a majority of the returned queries contains data similar to what a user might search for. The recommendations from the correlation method are almost always close to what the user initially queried while those from the other two methods are often more general. For the single and distance methods, the quality of their recom-mendations heavily depends on the initial query. We found an average of 25% overlap between these two recommenda-tions. The remainder of the queries are very similar to one another, with only subtle differences.
 The set of plots in Figure 2 present the performance re-Collection C Total t Avg. t per d | tf | ACM 7675 23404 57 0.5M NYTIMES 299752 92829 226 68M PUBMED 8200000 134317 58 479M
TWWD 1002 129470 5926 6M sults of the Apriori and OLAP cube algorithms. Note that the execution time increases with the size of the collection because of the time required to perform the extraction of the  X  tf subset. However, because the extraction of this sub-set is obtained differently in both algorithms, the perfor-mance change may also be different. Interestingly, such a difference between the two algorithms provides us with some intriguing trends. For example, the OLAP cube algorithm performs faster on collections in which the average number of keywords per document is large. In such situations, the Apriori algorithm is heavily penalized because of the extra matches that the algorithm must perform. Hence, we be-lieve that OLAP proves to be a more stable option due to its ability to be fairly independent of the size of the collectio n. Having said this, the Apriori algorithm is still quite fast, especially when  X  tf can be extracted quickly and managed in main memory. However, the OLAP approach is favored whenever the computations are complex.

Figures 2c and 2d were focused on the execution time with respect to an increasing the number of documents. The results show that the online OLAP algorithm appears to have a linear reaction. For the Apriori algorithm, when the number documents to be analyzed is small, the performance trend is similar to n 2 . However, as the number of documents analyzed increases, the algorithm becomes much more sta-ble, and is significantly faster than the cube approach. For the online OLAP algorithm, we fully expected the perfor-mance to be linear (depends on the number of documents).
The execution times for 10K documents become unman-ageable. Hence, we analyzed the effect of sampling on both overall performance and safety. Figure 3a and Figure 3b present the performance results of executing  X  = 10 itera-tions. Each iteration would only analyze a small portion of the overall original data set. From these experiments, the Apriori algorithm is slightly faster on data sets with a smal l average number of keywords per document. In addition, the OLAP Cube algorithm also appears to be more stable than the other options. However, the question arises regarding the validity of using sampling as an approximate solution. Figures 3c and 3d show results on the number of iterations required to arrive to the final solution. Surprisingly, only a few iterations are needed (normally less than 5) to reach high accuracy. Hence, bootstrapping proves to be a viable alternative for reducing the times.
Recently, a term  X  X opularity X  approach is explored in [4] in which each term is compared to the query terms. Simi-larly, the authors present the idea of na  X   X ve suggestions, but they do not explore how to efficiently compute such aggre-gations. Early straightforward attempts on query logs for unbiased query recommendations where presented in [1, 3]. But the authors assume previous knowledge by using these logs. OLAP has been explored less than association rules for query recommendation. However, a couple of applications have implemented OLAP to explore document collections [8, 2]. In [8] a contextualized warehouse (XML data warehouse) of the documents was built, and then queried by the usage of OLAP on relevant measures assigned to the documents. However, they focus on the metadata of the documents. In addition, OLAP sampling was explored initially in [5] as a technique for speeding up OLAP aggregations with approx-imate results. However, this analysis is not presented in th e context of collections of keywords.
We showed that OLAP can greatly enhance query recom-mendations. Through this paper, we have shown how the analysis of the intrinsic knowledge of keywords in a collect ion of documents can be efficiently performed. Our experimen-tal results showed that while the Apriori algorithm is sligh tly faster than our online OLAP algorithm, the latter appears to be more stable in its times. Furthermore, we showed that even with a few iterations, sampling was able to obtain a fairly high accuracies. As a result, the experimental resul ts proved that obtaining a few samples with replacement of large collection can achieve a high accuracy. We also cre-ated three main approaches to determining the validity of a set of keywords: single, distance, and correlation. We foun d that the single match method is the fastest while the corre-lation method produces better quality results.

Future work includes comparing our system to other query recommender systems and trying different policies for find-ing the occurrences of the set of keywords. Similarly, query suggestion enhanced with the injection of previous knowl-edge should be explored (e.g. ontologies). Finally, we want to study the effect of different sampling techniques. [1] B.M. Fonseca, P.B. Golgher, E.S. De Moura, and [2] C. Garcia-Alvarado, Z. Chen, and C. Ordonez. OLAP [3] A. Giacometti, P. Marcel, E. Negre, and A. Soulet. [4] G. Koutrika, Z.M. Zadeh, and H. Garcia-Molina. Data [5] X. Li, J. Han, Z. Yin, J.G. Lee, and Y. Sun. Sampling [6] C. Ordonez. Statistical Model Computation with [7] C. Ordonez and Z. Chen. Evaluating statistical tests on [8] J.M. Perez, R. Berlanga, M.J. Aramburu, and [9] R. R. Kraft and J. Zien. Mining anchor text for query
