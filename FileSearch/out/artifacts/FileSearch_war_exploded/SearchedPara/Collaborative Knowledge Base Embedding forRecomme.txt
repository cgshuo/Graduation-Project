
Among different recommendation techniques, collaborative fil-tering usually suffer from limited performance due to the sparsity of user-item interactions. To address the issues, auxiliary informa-tion is usually used to boost the performance. Due to the rapid collection of information on the web, the knowledge base provides heterogeneous information including both structured and unstruc-tured data with different semantics, which can be consumed by var-ious applications. In this paper, we investigate how to leverage the heterogeneous information in a knowledge base to improve the quality of recommender systems. First, by exploiting the knowl-edge base, we design three components to extract items X  semantic representations from structural content, textual content and visu-al content, respectively. To be specific, we adopt a heterogeneous network embedding method, termed as TransR, to extract items X  structural representations by considering the heterogeneity of both nodes and relationships. We apply stacked denoising auto-encoders and stacked convolutional auto-encoders, which are two types of deep learning based embedding techniques, to extract items X  tex-tual representations and visual representations, respectively. Final-ly, we propose our final integrated framework, which is termed as C ollaborative K nowledge Base E mbedding ( CKE ), to jointly learn the latent representations in collaborative filtering as well as item-s X  semantic representations from the knowledge base. To evalu-ate the performance of each embedding component as well as the whole system, we conduct extensive experiments with two real-world datasets from different scenarios. The results reveal that our approaches outperform several widely adopted state-of-the-art rec-ommendation methods.

Recommender Systems, Knowledge Base Embedding, Collabo-rative Joint Learning
Due to the explosive growth of information, recommender sys-tems have been playing an increasingly important role in online ser-vices. Among different recommendation strategies, collaborative filtering (CF) based methods, which make use of historical inter-actions or preferences, have made significant success [23]. How-ever, CF methods usually suffer from limited performance when user-item interactions are very sparse, which is very common for scenarios such as online shopping where the item set is extremely large. In addition, CF methods can not recommend new items s-ince these items have never received any feedbacks from users in the past. To tackle these problems, hybrid recommender system-s, which combine collaborative filtering and auxiliary information such as item content, can usually achieve better recommendation results and have gained increasing popularity in recent years [2].
Over the past years, more and more semantic data are published following the Linked Data principles 1 , by connecting various in-formation from different topic domains such as people, books, mu-sics, movies and geographical locations in a unified global data s-pace. These heterogeneous data, interlinked with each other, form-s a huge information resource repository called knowledge base. Several typical knowledge bases have been constructed, including academic projects such as YAGO 2 , NELL 3 , DBpedia 4 , and Deep-Dive 5 , as well as commercial projects, such as Microsoft X  X  Satori and Google X  X  Knowledge Graph 7 . Using the heterogeneous con-nected information from the knowledge base can help to develop insights on problems which are difficult to uncover with data from a single domain [6]. To date, information retrieval [9], communi-ty detection [25], sentiment analysis [4] -to name a few -are the noteworthy applications that successfully leverage the knowledge base.

Actually, since a knowledge base provides rich information in-cluding both structured and unstructured data with different seman-tics, the usage of the knowledge base within the context of hybrid recommender systems are attracting increasing attention. For ex-ample, Yu et al. [30] uses a heterogeneous information network to represent users, items, item attributes, and the interlinked relation-ships in a knowledge base. They extract meta-path based latent features from the network structure and apply Bayesian ranking optimization based collaborative filtering to solve the entity recom-mendation problem. Grad-Gyenge et al. [11] extended collabora-tive filtering by adopting a spreading activation based technique to incorporate a knowledge base X  X  network features for recommender systems X  rating prediction task. However, previous studies have not http://linkeddata.org/ https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/ http://rtw.ml.cmu.edu/rtw/ http://wiki.dbpedia.org/ http://i.stanford.edu/hazy/deepdive/ http://searchengineland.com/library/bing/bing-satori http://www.google.com/insidesearch/features/search/knowledge.html fully exploited the potential of the knowledge base since they suffer from the following limitations: 1) only utilize the single network structure information of the knowledge base while ignore other im-portant signals such as items X  textual and visual information. 2) rely on heavy and tedious feature engineering process to extract features from the knowledge base.

To address the above issues, in this paper, we propose a nov-el recommendation framework to integrate collaborative filtering with items X  different semantic representations from the knowledge base. For a knowledge base, except for the network structure infor-mation, we also consider items X  textual content and visual content (e.g., movie X  X  poster). To avoid heavy and tedious manual feature extractions, we design three embedding components to automati-cally extract items X  semantic representations from the knowledge base X  X  structural content, textual content and visual content, re-spectively. To be specific, we first apply a network embedding approach to extract items X  structural representations by consider-ing the heterogeneity of both nodes and relationships. Next, we adopt stacked denoising auto-encoders and stacked convolutional auto-encoders, which are two types of deep learning based embed-ding techniques, to extract items X  textual representations and visu-al representations, respectively. Finally, to integrate collaborative filtering with items X  semantic representations from the knowledge base smoothly, we propose our final framework, which is termed as Collaborative Knowledge Base Embedding ( CKE ), to learn differ-ent representations in a unified model jointly.

Our empirical studies consist of multiple parts. First, we conduc-t several experiments to evaluate the performance of three knowl-edge base embedding components, respectively. Next, we evaluate the effectiveness of our integrated framework by comparing with several competitive baselines.

The key contributions of this paper are summarized as the fol-lowing:
The rest of this paper is organized as follows. Section 2 intro-duces the preliminary concepts and present our recommendation problem. Section 3 gives an overview of our framework. Sec-tion 4 delves into the usage of embedding components to extract representations from the knowledge base. In Section 5, we discuss how to effectively integrate collaborative filtering with knowledge base embedding into a unified model. The empirical results are dis-cussed in Section 6, followed by a brief review of related work in Section 7 and a conclusion of this paper in Section 8.
 Figure 1: Illustration of a snippet of user implicit feedback data and knowledge base data.
In this section, we will first clarify some terminologies used in this paper, and then explicitly present our problem.
The recommendation task considered in this paper are targeted for implicit feedback. Assume there are m users and n items, we define the user implicit feedback matrix R  X  R m  X  n as where the value 1 in matrix R represents the interactions between users and items, e.g., users watched a movie or users searched a book in the search engine. Note that the value 1 in the implicit feedback data does not mean that users actually like the items. Ac-tually a user searched a book because he is interested in the book but he might probably dislike the book after browsing the related information on the Internet. Similarly, the value 0 in R mean that the users dislike the items, but can be regarded as a mix-ture of negative feedbacks (users are not interested in such items) and potential interactions (users are not aware of such items).
Actually, we are interested in leveraging the knowledge base for enhancing the quality of recommender systems, therefore items in recommender systems are mapped to entities in the knowledge base (e.g., a movie item can usually be mapped to an entity describing this movie), and these entities are termed as item entities in this article.

We consider the information stored in the knowledge base can be divided into three parts: structural knowledge, textual knowledge and visual knowledge. The detailed definition of each part is given as follows:
Definition 1 : Structural knowledge . This knowledge can be re-garded as a heterogeneous network with multiple types of entities and multiple types of links to express the structure of the knowledge base. For movie recommendation, entities usually include movie items and corresponding attributes (e.g., the genre  X  X cience fiction X  and the actor  X  X evin Space X ), and links describe the relationship between these entities (e.g.,  X  X cting X  behavior and  X  X ating X  behav- X  + + ior). The network structure implies some similarity between item entities, which is most probably useful for recommendation.
Definition 2 : Textual Knowledge . For an item entity such as book or movie in the knowledge base, we use the textual summary to represent the textual knowledge, which usually gives the main topics of this book or this movie.

Definition 3 : Visual Knowledge . For an item entity, except for previous textual description, there are usually some images in the knowledge base, we use a book X  X  front cover image or a movie X  X  poster image to represent its visual knowledge.

User implicit feedback interactions and structural knowledge serve as the structural features of an item, while textual knowledge and visual knowledge serve as the content features. A snippet of the knowledge base with three kinds of knowledge as well as user im-plicit feedback are presented in Figure 1. We define our recommendation problem in this paper as follows:
Given a knowledge base with structural knowledge, textual knowl-edge and visual knowledge, as well as user implicit feedback, we aim to recommend each user with a ranked list of items he will be interested.
In this article, by fully exploiting the structural knowledge, textu-al knowledge and visual knowledge in the knowledge base, we pro-pose a Collaborative Knowledge Base Embedding model ( CKE ) for supporting our recommendation task. Our model mainly con-sists of two steps: 1) knowledge base embedding and 2) collabora-tive joint learning.

In the knowledge base embedding step, we extract an item en-tity X  X  three embedding vectors from structural knowledge, textu-al knowledge and visual knowledge, respectively. These embed-ding vectors indicate an item entity X  X  latent representation in each domain. For structural embedding component, we apply a net-work embedding procedure (Bayesian TransR) to find the laten-t representation from the heterogeneous network in the structural knowledge. For textual embedding component, we apply an un-supervised deep learning model called Bayesian stacked denoising auto-encoder (Bayesian SDAE) [29] to find the latent representa-tion from the textual knowledge. Similarly, we apply another unsu-pervised deep learning model called Bayesian stacked convolution-al autoencoder (Bayesian SCAE) to find the latent representation from the visual knowledge.

In the collaborative joint learning step, an item X  X  latent vector is finally represented as the integration of three embedding vectors from the knowledge base as well as a latent offset vector. The final item latent vector represents an item X  X  knowledge from structural content, textual content, visual content as well as historical user-item interactions. Then we use collaborative filtering by optimiz-ing the pair-wise ranking between items to learn both user latent vectors and item latent vectors. Final recommendation is generated from these user latent vectors and item latent vectors.

The flowchart of our framework is presented in Figure 2. Knowl-edge base embedding and collaborative joint learning will be de-tailed in Section 4 and Section 5, respectively.
In this section, by leveraging network embedding and deep learn-ing embedding, we present the details of how we extract an item entity X  X  representations from structural knowledge, textual knowl-edge and visual knowledge, respectively.
The heterogeneous network encodes structured information of entities and their rich relations. To capture this structured knowl-edge, a promising approach is to embed this heterogeneous network into a continuous vector space while preserving certain information of the network. In this subsection, we first briefly review a state-of-the-art network embedding method called TransR [15], and then give a Bayesian formulation of TransR for our task.

First, to represent the structural knowledge, we use an undirected
Figure 3: Illustration of TransR for structural embedding graph G =( V , E ) , where V = { v 1 ,...,v | V | } is a set of vertices referring to different entities and E is a set of edges referring to different types of relation between these entities.

TransR [15] is a state-of-the-art embedding approach for hetero-geneous network. Being different from other methods which as-sume embedding of entities and relations within the same space , TransR represents entities and relations in distinct semantic space bridged by relation-specific matrices. In TransR, for each triple ( v h ,r,v t ) in the network ( v h and v t are two linked entities, r is the type of edge between them), entities are embedded into vec-tors v h , v t  X  R k and relation is embedded into r  X  R d relation r , we set a projection matrix M r  X  R k  X  d , which projects entities from entity space to relation space. As shown in Figure 3, the projected vectors of entities are defined as
The score function of this triple is correspondingly defined as
Similar to [22], we use a sigmoid function to calculate the pair-wise triple ranking probability instead of margin-based objective function adopted in original TransR. Then we extend TransR to a Bayesian version and propose the generative process as follows: 1. For each entity v , draw v  X  X  ( 0 , X   X  1 v I ) . 2. For each relation r , draw r  X  X  ( 0 , X   X  1 r I ) and 3. For each quadruple ( v h ,r,v t ,v t )  X  X  , draw from the prob-It is routine to corrupt correct triple ( v h ,r,v t entity with another entity of the same type, and construct incorrect triple ( v h ,r,v t ) . Note that step 3 implies the fact that when score function of a correct triple is larger than that of an incorrect triple, the quadruple is more likely to be sampled.
 For each item entity j , we use embedding vector v j from Bayesian TransR to denote its structural representation.
In this subsection, we investigate how to apply an unsupervised deep learning model called stacked denoising auto-encoders (S-DAE) to get item entities X  textual representations from the textual knowledge.

SDAE [27] is a feedback neural network for learning the repre-sentation of the corrupted input data by learning to predict the clean itself in the output. Before presenting the model detail, we give the notations used in SDAE. Assume the number of network layers is Figure 4: Illustration of a 6-layer SDAE for textual embedding L , we use matrix X l to represent the output of layer l in SDAE. Note that we use the last layer output X L t to represent the origi-nal clean textual knowledge of all item entities, where the j -th row is the bag-of-words vector X L t ,j  X  for item entity j . Similarly, we use matrix X 0 to represent the noise-corrupted matrix (randomly masking some entries of X L t by making them zero). W l and are weight parameter and bias parameter, respectively, for layer l .
Figure 4 gives the illustration of a 6-layer SDAE for our textual embedding component. As shown in this figure, the first L t of the network (from X 0 to X 3 ) usually acts as encoder part, which maps the corrupted input X 0 to a latent compact representation X 3 , and the last L t 2 layers (from X 3 to X 6 ) usually acts as the decoder part, which recovers the clean input X 6 from the latent representation X 3 .

Similar to [29], given that both the clean input X L t and the cor-rupted input X 0 are observed, we present the generative process of each layer l in Bayesian SDAE as follows: 1. For weight parameter W l , draw W l  X  X  ( 0 , X   X  1 W I ) 2. For bias parameter, draw b l  X  X  ( 0 , X   X  1 b I ) . 3. For the output of the layer, draw
The embedding vector in the middle layer, i.e., X 3 ,j  X  in Figure 4, is used as the textual representation for item entity j .
In this subsection, similar to previous textual embedding part, we apply another unsupervised deep learning model, termed as s-tacked convolutional auto-encoders (SCAE), to extract item enti-ties X  semantic representations from the visual knowledge.
For visual objects, convolutional layers based deep learning ar-chitectures often beat the common fully connected architectures due to the fact that they can preserve the image X  X  neighborhood relations and spatial locality in the latent higher-level feature rep-resentation [7]. Furthermore, convolutional layers restrict the num-ber of free parameters by sharing weights so that they scale well to high-dimensional image content. Given above, by following the work in [16], we adopt the stacked convolutional auto-encoders (S-CAE) by using convolutional hidden layers to replace fully-connected layers in previous SDAE.

Assume that there are L v layers in SCAE, similar to the nota-tion in SDAE, we use a 4-dimensional tensor Z L v to denote the collection of clean images, where the j -th row is a 3-dimensional tensor Z L v ,j  X  of raw pixel representation in RGB color space for item entity j . Similarly, we use Z 0 to denote the corrupted im-ages (randomly masking some entries of Z L v by adding Gaussian noise). Next, for each layer l , we use Z l to represent the output, Q l to represent the weight parameter, and c l to represent the bias parameter.

In SCAE, we set layer L v 2 and layer L v 2 +1 as fully connected layers, while other layers as convolutional layers. Figure 5 gives the illustration of a 6-layer SCAE, which also consists of encoder part and decoder part. As shown in the figure, encoder part consists of two convolutional layers (from Z 0 to Z 2 ) and a fully connected layers ( Z 2 to Z 3 ). Similarly, decoder part consists of a fully con-nected layer ( Z 3 to Z 4 ) and two following deconvolutional layers (from Z 4 to Z 6 ). Note that the output of the middle hidden layer Z 3 is a matrix, which denotes the collection of all item entities X  visual embedding vectors, while the output of other hidden layers are usually termed as feature maps [7], which are 4-dimensional tensors generated from convolutional layers. The mapping for a convolutional layer is given as where  X  denotes the convolutional operator, which can preserve the local connectivity of previous output. More details about convolu-tional operator can be referred to [20].

Similar to textual embedding component, given both clean im-age input Z L v and corrupted input Z 0 , we present the generative process of each layer l in Bayesian SCAE as follows: 1. For weight parameter, draw Q l  X  X  ( 0 , X   X  1 Q I ) . 2. For bias parameter, draw c l  X  X  ( 0 , X   X  1 c I ) . 3. For the output of the layer, The embedding vector in the middle layer, i.e., Z 3 ,j  X  in Figure 5, is used as the visual representation for item entity j .
In this section, in order to integrate collaborative filtering with items X  embedding representations from the knowledge base, we propose the collaborative joint learning procedure in our CKE frame-work.

Given user implicit feedback R , motivated by [22], we consider the pair-wise ranking between items for the learning approach. To be more specific, when R ij =1 and R ij =0 , we say that user i prefers item j over j , and then use p ( j&gt;j ; i |  X  pair-wise preference probability, where  X  represents the model pa-rameters. In collaborative filtering, we use a latent vector representation for user i , and a latent vector  X  j as the representation for item j . To simultaneously capture an item X  X  latent representa-tion in collaborative filtering and representations in the knowledge base, the item latent vector can be re-expressed as Then the pair-wise preference probability can be given as
Using Bayesian TransR, Bayesian SDAE, and Bayesian SCAE in knowledge base embedding step as components, the generative process of our framework CKE by using collaborative join learning is given as follows: 1. Considering the structural knowledge, 2. Considering the textual knowledge, for each layer l in SDAE, 3. Considering the visual knowledge, for each layer l in SCAE, 4. For each item j , draw a latent item offset vector  X  j 5. For each user i , draw a user latent vector as u i  X  X  ( 0 6. For each triple ( i, j, j )  X  X  , draw from the probability
Here, D is a collection of triples, where each triple ( i, j, j fies that R ij =1 and R ij =0 ( j is randomly sampled from user i  X  X  uninterested items). Note that v j , X L t as the bridges between implicit feedback preference and structural knowledge, textual knowledge as well as visual knowledge, respec-tively. Learning the parameters. Computing the full posterior of the pa-rameters is intractable. Like [28], maximizing the posterior proba-bility of u , e , r , M , W , b , Q and c is equivalent to maximizing the log-likelihood as follows:
To maximize the objective in Eq. (7), we employ a stochastic gradient descent (SGD) algorithm similar to [22]. In each iteration, for a randomly sampled triple ( i, j, j )  X  X  , we find the subset S item j . Then we perform a SGD update for each parameter using the gradient of the corresponding objective function [10]. Prediction The final item recommendation for a user i is given according to the following ranking criterion:
In this section, we evaluate our proposed framework on two real-world datasets for both movie and book recommendation scenarios. The experimental results demonstrate evidence of significant im-provement over many competitive baselines.
To demonstrate the effectiveness of the proposed collaborative knowledge base embedding for recommendation framework, we use two datasets from different domains (movie and book) for em-pirical studies. The first dataset, MovieLens-1M 8 , consists of 1 M ratings with 6,040 users and 3,706 movies. Similar to [31], in order to be consistent with the implicit feedback setting, we extract only positive ratings (rating 5) for training and testing. After remov-ing users with less than 3 positive ratings, we have 5,883 users, 3,230 movies, and 226,101 ratings in the final dataset. The second dataset, termed as IntentBooks, is collected from Microsoft X  X  Bing search engine and Microsoft X  X  Satori knowledge base [1]. In this dataset, users X  interests for books are extracted from click/query actions, e.g., if a user has conducted queries with  X  X arry Potter X  or clicked documents containing that name, this user might be inter-ested for the related book entity. To reduce the name conflict issue (e.g., instead of the book,  X  X arry Potter X  might actually mean the related movie), we extracted user X  X  book interests by combining un-supervised similarity computation with supervised classification by following [13]. Moreover, to validate the effectiveness of book in-terest extraction, we randomly selected 200 extracted book interest http://grouplens.org/datasets/movielens/1m/ instances and manually labeled each one as true or false. The result shows that the precision is 91.5%, which we believe is accurate enough for subsequent experiments. We sample the user implicit feedback data from Bing X  X  search log from Sep. 2014 to Jun. 2015. After removing users with less than 5 book interests, we finally have 92,564 users, 18,475 books, and 897,871 user-book interests.
We also use Satori knowledge base to extract structural knowl-edge, textual knowledge and visual knowledge for these two dataset-s. First, we applied a two staged method described in [24] (includ-ing both title match and attributes match) to map each movie from MovieLens-1M dataset to an entity in the knowledge base (note that a book in the IntentBooks dataset is already an entity in the knowledge base, therefore matching step is ignored). We explicitly observed 200 paired results, where 92% of the pairs are correct-ly matched (the match precision is good enough for later process). In addition, we find that only 134 movies can not be mapped to any movie entities from the knowledge base. Next, to build the structural knowledge, we extract a subgraph from the knowledge base which contains item entities, entities which are 1-step away from item entities, and the corresponding relationships. For movie entities, the 1-step entities include genre, director, writer, actors, language, country, production date, rating, nominated awards, and received awards; for book entities, the 1-step entities include genre, author, publish date, belonged series, language, and rating. Then for text knowledge, we follow the word hashing procedure as that in [12] to preprocess the text information extracted from the plots of the movies and the descriptions of the books. Finally, for visual knowledge, we use the poster image of a movie entity and the front cover image of a book entity, where the finally used visual input are images that are reshaped to the 3  X  64  X  64 tensor format in the RGB space. Some detailed statistics of the two datasets are sum-marized in Table 1. For example,  X #sk nodes X  indicates the total number of nodes in the extracted structural knowledge,  X #tk items X  indicates the number of items having textual knowledge, and  X #vk items X  indicates the number of items having visual knowledge.
As discussed in [19], precision is not a suitable performance measure for implicit feedback recommendation. Therefore, in our experiments, we use MAP @K (mean average precision) [26] and Recall @K to evaluate the performance of the top K recommenda-tion. For each dataset, similar to [14], we randomly select 70% items associated with each user to constitute the training set and use all the remaining as the test set. For each evaluation scenario, we repeat the evaluation five times with different randomly selected training sets and the average performance is reported in the follow-ing parts. For each dataset, we also use a validation set from the training set to find the optimal hyperparameters for our methods as well as baselines introduced in later parts. In the following reported results, hyperparameter settings of our methods are given in Table 2 since best performance is achieved. In Table 2, dim denotes the latent dimension, denotes the noise masking level,  X  denotes the Table 2: Hyperparameter settings of our framework for the t-wo datasets. cf, sk, tk and vk indicate the parameters in the component of collaborative filtering, structural knowledge em-bedding, textual knowledge embedding and visual knowledge embedding, respectively.
 standard deviation for image X  X  Gaussian filter noise, L t note the number of layers, N l denotes the number of hidden units when layer l is not a middle layer or an output layer in textual em-bedding X  X  SDAE step, N f and S f denote the number and the size of filter maps in each convolutional layer of visual embedding X  X  S-CAE step, respectively. Note that for the purpose of keeping the collaborative filtering part the same in different factorization meth-ods, the latent dimension of compared baselines in the following subsections are set the same as that in Table 2, other hyperparame-ters of baselines are determined by grid search.

In the following subsections, we will evaluate the proposed frame-work according to four aspects. First, we evaluate the recommenda-tion performance with regard to structural knowledge usage, textual knowledge usage, and visual knowledge usage, respectively. Then, we compare the joint model CKE with state-of-the-art baselines to demonstrate the effectiveness of our system.
In this subsection, to study the performance of our structural knowledge usage, we only incorporate structural knowledge em-bedding component into collaborative filtering in the joint learning process. We compare our method, termed as  X  X KE(S) X , against the following baselines:
The results of different methods are given in Figure 6(a), Figure 7(a), Figure 8(a), and Figure 9(a). The results precipitate sever-al observations for both two datasets, which we summarize as: 1) BPRMF performs the worst among all approaches. Since BPRM-F is the only one which totally ignores structural knowledge, the results imply that the additional usage of structural knowledge can significantly improve the recommendation performance. 2) PRP performs worse than other approaches leveraging structural knowl-edge. This is because among these, PRP is the only one which does not leverage factorization, which can capture the latent low-rank approximation of user-item interactions under the sparseness of the datasets. 3) BPRMF+TransE outperforms LIBFM(S) and PER, which demonstrates that instead of directly using structural knowledge in a feature-engineering way, network embedding can capture the semantic representation in a more reasonable way and thus improve the recommendation quality. 4) Our method CKE(S) beats BPRMF+TransE, which implies that by using TransR, there is still room for improvement when considering the heterogeneity of network embedding.
In this subsection, we investigate the performance of our textual knowledge usage. Specifically, we use the textual knowledge em-bedding component and collaborative filtering in the joint learning process. We compare our method, which is termed as  X  X KE(T) X , against BPRMF as well as the following baselines:
As shown in Figure 6(b), Figure 7(b), Figure 8(b), and Figure 9(b), the comparison of the results presents the following observa-tions: 1) CKE(S) outperforms CKE(T) and LIBFM(S) outperforms LIBFM(T), which implies that compared to structural knowledge, textual knowledge has weaker improvement on recommendation performance. 2) LIBFM(T), CTR and CKE(T) usually give better performance than CMF, which reveals that the direct factorization of item-word matrix can not make the full use of textual informa-tion. 3) CTR is a strong baseline and sometimes even achieve the best performance in the IntentBooks dataset. However, our method CKE(T) can beat CTR most of the time, which demonstrates that compared to topic modeling, deep learning embedding are really good at extracting text X  X  semantic representation by delving into deep structure.
In this subsection, we focus on studying the performance of our visual knowledge usage. As before, we use the visual knowledge embedding component and collaborative filtering in the joint learn-ing. We compare our method, which is named as  X  X KE(V) X  against BPRMF as well as the following baselines: baselines for dataset MovieLens-1M.
 baselines for dataset MovieLens-1M.
 baselines for dataset IntentBooks.
 baselines for dataset IntentBooks. Figure 10: Recall@K results comparison between our frame-work and related baselines for both datasets.
The results of different approaches are shown in Figure 6(c), Fig-ure 7(c), Figure 8(c), and Figure 9(c), which provide us the follow-ing observations: 1) Compared to structural knowledge and textual knowledge, the performance improvement with the usage of visual knowledge is limited but still significant. 2) CKE(V) and BPRM-F+SDAE(V) outperform other approaches, which demonstrates the superiority of deep network for visual knowledge embedding. 3) The performance gap between CKE(V) and BPRMF+SDAE(V) is still significant, which reveals that convolutional layers are more suitable for extracting visual representation. Finally, we evaluate the performance of our whole framework. We compare our ultimate model using three components in knowl-edge base embedding, denoted as  X  X KE(STV) X  against the follow-ing baselines:
The results are shown in Figure 10 and Figure 11, which give the following observations: 1) CKE(STV) outperforms CKE(ST), CKE(SV) and CKE(TV). This implies that the additional usage of each kind of knowledge can improve the recommendation per-formance, which demonstrate the insight of our framework com-bining different kinds of knowledge together for enhancing rec-ommendation. 2) CKE(STV) outperforms LIBFM(STV), which demonstrates that compared to direct usage of the knowledge base in a feature-engineering way, the embedding components can cap-ture the knowledge base X  X  semantic representations more effective-ly and thus improve the recommendation performance. 3) CK-E(STV) still archives better performance than BPRMF+STV, which Figure 11: MAP@K results comparison between our frame-work and related baselines for both datasets. suggests that the joint learning of collaborative filtering and knowl-edge base embedding can target the recommendation optimization task in a more direct way and thus improve the recommendation quality.
The usage of the knowledge base within the context of recom-mender systems are attracting increasing attention in recent years. For example, Cheekula et al. [5] explores the hierarchical catego-ry knowledge derived from the pruned DBpedia knowledge base and applies the spreading activation algorithm to identify person-alized entities as recommendation. Passant [18] computes the se-mantic distance in the graphical structure of a knowledge base and used this distance measure to build music recommendation system-s. Compared to previous works which investigated this problem mainly by leveraging the structure of a knowledge base, we present a framework to integrate the heterogeneous knowledge from struc-ture, text as well as visual content for recommendation.
Previous works of structural knowledge embedding mainly aim to embed entities and relations into a continuous vector space and model the semantics of structure in that space. For example, TransE [3] represents a relation by a translation vector so that the pair of em-bedded entities in a triple can be connected with low error. How-ever, TransE has flaws in dealing with many-to-many relations for the heterogeneity of structural information in a knowledge base. To solve the issue of TransE, TransR [15] steps to propose that entities and relations are completely different objects and models entities and relations in distinct spaces and performs translation in relation space by using a translation matrix. In this paper, we in-tegrate TransR with our recommendation task to fully leverage the structural knowledge in the knowledge base.
With the success of deep learning in recent years, there is a marked switch from hand-crafted features to those that are learned from raw data in recommendation research. For example, Elkahky et al. [8] proposes a multi-view deep learning model to learn user features and item features from different domains, and uses the en-riched feature representation to improve the recommendation qual-ity across all the domains. Wang [29] significantly improve the rec-ommendation performance by jointly performing deep representa-tion learning for the textual content information and collaborative filtering for the ratings matrix. Our framework distinguishes it-self from the above-mentioned works in the following aspects: 1) We specifically apply stacked convolutional auto-encoder to extract the semantic representation of visual content, which has not been exploited in previous works. 2) We design a joint model by inte-grating collaborative filtering and the heterogeneity of structural, textual and visual knowledge to boost the recommendation quality. This paper proposes a hybrid recommender system termed as CKE , which integrates collaborative filtering and knowledge base for recommendation. Following this framework, we first design three components, which leverage heterogeneous network embed-ding and deep learning embedding approaches, to automatically extract semantic representations from structural knowledge, tex-tual knowledge and visual knowledge in the knowledge base, re-spectively. Next, we combine collaborative filtering and knowl-edge base embedding components into a unified framework and learn different representations jointly. The extensive experiments we have conducted validated the effectiveness of our CKE frame-work. Besides, this research sheds new light on the usage of het-erogeneous information in the knowledge base, which can be con-sumed in more application scenarios.
