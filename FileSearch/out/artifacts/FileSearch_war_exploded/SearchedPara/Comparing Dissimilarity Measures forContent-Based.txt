 Content-based image retrieval is normally performed by computing the dissimi-larity between the data objects and querie s based on their multidimensional rep-There have been a large number of dissimilarity measures from computational geometry, statistics and information theory, which can be used in image search. However, only a limited number of them have been widely used in content-based image search. Moreover, the performance of a dissimilarity measure may largely depend on different feature spaces. Although there have been some attempts in theoretically summarizing existing dissimilarity measures [6], and some evalua-tion to find which dissimilarity measure for shape based image search [13], there various dissimilarity measures on different feature spaces for large-scale image retrieval.

In this paper, we systematically investigate 14 typical dissimilarity measures from different fields. Firstly, we classify them into three categories based on colour, texture and structure category and some of their combinations, on the standard Corel image collection. Our systematic empirical evaluation provides initial evidence and insights on which dissimilarity measure works better on which feature spaces. Based on McGill and others X  studies on dissimilarity measures [6,4,12], we choose 14 typical measures that have been used in information retrieval. 2.1 Geometric Measures Geometric measures treat objects as vectors in a multi-dimensional space and compute the distance between two objects based along pairwise comparisons on dimensions.
 Minkowski Family Distances ( d p ) test object vector respectively. The Min kowski distance is a general form of the Euclidean (p=2), City Block (p=1) and Chebyshev ( p =  X  ) distances. p&lt; 1) [3], which is not a metric because it violates the triangle inequality. Howarth and R  X  uger [3] have found that the retrieval performance would be increases in many circu mstances when p=0.5.
 Cosine Function Based Dissimilarity ( d cos ). The cosine function computes the angle between the two vectors, i rrespective of vector lengths [13]: Canberra Metric ( d can ) [4] Squared Chord ( d sc ) [4]
Obviously, this measure is not applicable for feature spaces with negative values.
 Partial-Histogram Intersection ( d p  X  hi ): This measure is able to handle partial matches when the sizes of the two object vectors are different [13]. When A and B are non-negative and have the same size, in terms of the City Block metric ( | x | = i | x i | ), it is equivalent to the City Block measure. [12, 9] 2.2 Information Theoretic Measures Information-theoretic measures are various conceptual derivatives from the Shannon X  X  entropy theory and treat objects as probabilistic distributions. There-fore, again, they are not applicable to features with negative values. Kullback-Leibler (K-L) Divergence ( d kld ). From the information theory point of view, the K-L divergence measures how one probabilistic distribution diverges from the other. However, it is non-symmetric. [7] Jeffrey Divergence ( d jd ) gence, is numerically stable and symmetric. [10] 2.3 Statistic Measures Unlike geometric measures, statistical measures compare two objects in a dis-tributed manner rather than simple pair wise distance.  X  2 Statistics ( d  X  2 ) tion) from the mean of both vector s (expected distribution). [13] Pearson X  X  Correlation Coefficient ( d pcc ). A distance measurement derived from Pearson correlation coefficient [5] is defined as where Note the larger | p | is the more correlated the vectors A and B. [1] Kolmogorov-Smirnov ( d ks ). Kolmogorov-Smirnov distance is a measure of dissimilarity between two probability distributions [2]. Like K-L divergence and Jeffrey divergence, it is defined only for one-dimensional histograms [12]: F object vectors, which are interpreted as probability vectors of one-dimensional histogram.
 Cramer/von Mises Type (CvM) ( d cvm ). A statistics of the Cramer/von (PDF) [11]: Our experiment aims to address the performance of 14 dissimilarity measures on different feature spaces. We use mean average precision as the performance indicator. 3.1 Experimental Setup Data Set. In this experiment, we use a subset of the Corel collection, devel-oped by [8]. There are 63 categories and 6192 images in the collection, which is randomly split into 25% training data, and 75% test data. We take the training set as queries to retrieve similar images from the test set. Features. Six typical image feature spaces are applied in the experiment.  X  Colour feature spaces: RGB is three-dimensional joint colour histogram,  X  Texture feature spaces: Gabor, is a texture feature generated using Gabor  X  Structure feature space: Ko nvolution (Konv), discriminates between low level Approach. Here, we use the vector space model approach for image retrieval. The difference from [8] is that we aim to test various dissimilarity measures instead of using traditional cosine based or city block measures. 3.2 Single Feature Spaces We investigate the performance of the 14 dissimilarity measures on 6 single image feature spaces as described above. 3.3 Combined Feature Spaces In a further experiment, we picked up three typical features from colour, texture and structure, respectively. This expe riment we use the same set up on the three and their combined feature spaces, HSV and Gabor, HSV and Konv, Gabor and Konv, and HSV, Gabor and Konv. 3.4 Results Table 1 and Table 2 show the experimental results, from which the following observations can be made. Firstly, most of the dissimilarity measures from the geometric category have better performance than other two categories; Secondly, the performance of most of the dissimilarity measures in the color feature spaces outperform the other feature space; Fina lly, after identifying the top five per-forming dissimilarity measures on ev ery feature space, we find Canberra met-ric, Squared Chord from the geometric measures category, Jeffrey Divergence from the information-theoretic measures category, and  X  2 from the statistical measures category have better performan ce than Euclidean and City Block dis-similarity measures, which have been most widely used in image retrieval field. Significance tests, using the paired Student X  X  t-test (parametric test), the sign test and the paired Wilcoxon signed-rank test (non-parametric test), have shown that the improvements over the city-block measure are statistically significant (p-value less than 0.05). Therefore we would recommend them for image retrieval applications.
 We have reviewed fourteen dissimilarity measures, and divided them into three been empirically compared on six typical content based image feature spaces, and their combinations on the standard Corel image collection.

Interesting conclusions are drawn fr om the experimental results, based on which we recommend Canberra metric, Squ ared Chord, Jeffrey Divergence, and  X  2 for future use in the Content based Image Retrieval.

This work will be a foundation for developing more effective content-based image information retrieval systems. In the future, we are going to test how the dissimilarity measures work on multi-image queries, and what their performances are on different data collections.
 This work was funded in part the European Union Sixth Framework Programme (FP6) through the integrated project Pharos (IST-2006-045035). In addition, we would like to thank Peter Howarth for helping construct the feature spaces and setting up the experiments.

