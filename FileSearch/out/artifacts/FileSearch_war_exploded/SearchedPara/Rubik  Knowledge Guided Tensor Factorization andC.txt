 Computational phenotyping is the process of converting het-erogeneous electronic health records (EHRs) into meaningful clinical concepts. Unsupervised phenotyping methods have the potential to leverage a vast amount of labeled EHR data for phenotype discovery. However, existing unsupervised phenotyping methods do not incorporate current medical knowledge and cannot directly handle missing, or noisy data.
We propose Rubik, a constrained non-negative tensor fac-torization and completion method for phenotyping. Rubik incorporates 1) guidance constraints to align with existing medical knowledge, and 2) pairwise constraints for obtain-ing distinct, non-overlapping phenotypes. Rubik also has built-in tensor completion that can significantly alleviate the impact of noisy and missing data. We utilize the Alternat-ing Direction Method of Multipliers (ADMM) framework to tensor factorization and completion, which can be easily scaled through parallel computing. We evaluate Rubik on two EHR datasets, one of which contains 647,118 records for 7,744 patients from an outpatient clinic, the other of which is a public dataset containing 1,018,614 CMS claims records for 472,645 patients. Our results show that Rubik can discover more meaningful and distinct phenotypes than the baselines. In particular, by using knowledge guidance constraints, Rubik can also discover sub-phenotypes for sev-eral major diseases. Rubik also runs around seven times faster than current state-of-the-art tensor methods. Finally, Rubik is scalable to large datasets containing millions of EHR records.
 H.2.8 [ Database Applications ]: Data mining c  X  Diagnoses Medications Hypertension Statins Ischemic heart disease Angiotensin receptor blockers Hyperlipidemia ACE inhibitors Obesity Loop diuretics Table 1: An example of a phenotype that a group of patients may exhibit. The diagnoses and medica-tions are shown in rank order of importance.
 Tensor Analysis; Constraint Optimization; Computational Phenotyping; Healthcare Analytics The widespread adoption of EHR systems in the United States and many other countries has resulted in a tsunami of EHR data, which is becoming an increasingly important source of detailed medical information. Successful pheno-typing efforts on EHR data can enable many important ap-plications, such as clinical predictive modeling [12, 27] and EHR-based genomic association studies [12, 17, 27]. Fur-thermore, medical professionals are accustomed to reason-ing based on concise and meaningful phenotypes. Thus, it is imperative that robust phenotyping methods be developed and refined to keep up with the growing volume and hetero-geneity of EHR data.

A typical phenotyping algorithm takes EHR data as in-put, and defines a group or several groups of patients, each of which is referred to as a phenotype. An example of a phenotype is shown in Table 1, which depicts a collection of diseases and associated medications that may co-occur in a patient 1 . In Table 1 and in all subsequent displays of pheno-types throughout the paper, the diagnoses and medications are shown by rank order of importance.

Most existing phenotyping methods are supervised ap-proaches, which are either expert-defined rule-based meth-ods [16] or classification methods [8]. However, as labeled data are difficult to obtain, efficient unsupervised phenotyp-ing approaches are needed to leverage the vast amount of
Note that individuals that belong to a phenotype will often have some, but not all, of the diagnoses and medications listed in the phenotype definition. unlabeled EHR data for discovering multiple interconnected phenotypes. The only such algorithm to our knowledge is based on sparse nonnegative tensor factorization [14, 15], which models interconnected data as tensors and discovers sparse nonnegative factors as phenotypes. However, there are still several formidable challenges in unsupervised phe-notyping methods:  X  Leverage of existing knowledge . Existing medical knowledge, such as a physician X  X  experience or a medical ontology, should be incorporated into the phenotyping al-gorithms in order to identify more meaningful phenotypes that align more closely with existing medical knowledge.  X  Deriving distinct phenotypes . Existing unsupervised phenotyping methods, such as Marble [15], can lead to overlapping phenotypes which hinder their interpretation.
Ideally, the resulting phenotypes should be distinct with-out much overlap.  X  Missing and noisy data. EHR data often contain miss-ing and noisy records. It is important to ensure that phe-notyping algorithms remain robust against missing and noisy data.  X  Scalability. Real world EHR data contains millions of records and spans multiple dimensions. It is important to develop innovative phenotyping methods that scale well with increases in data size.
 Contributions : We propose Rubik , an unsupervised phe-notyping method based on tensor factorization and comple-tion, which addresses all the aforementioned challenges:  X  We incorporate guidance constraints based upon medical knowledge in order to derive clinically meaningful pheno-types.  X  We introduce pairwise constraints in the formulation to ensure distinct phenotypes.  X  Our proposed algorithm embeds efficient tensor comple-tion, thereby alleviating both missing and noisy informa-tion in EHR tensors.  X  We design a scalable algorithm based on Alternating Di-rection Method of Multipliers (ADMM) for solving this problem, which significantly outperforms several baseline methods.
 We evaluate Rubik on two large EHR datasets. Our results demonstrate that Rubik achieves at least a 60% reduction in the number of overlapping phenotypes compared to Mar-ble as a baseline [15]. Rubik also increases the number of meaningful phenotypes by 50%. Furthermore, the pheno-types and the baseline characteristics derived from the real EHR data are consistent with existing studies on the pop-ulation. Rubik is also much more computationally scalable compared to all baseline methods, with up to a 7-fold de-crease in running time over the baselines.
 Table 3: Common symbols used throughout the pa-per.

Table 2 compares the properties between our model and other tensor methods.
 Outline : The remainder of the paper is organized as fol-lows. We review preliminaries in Sec. 2. We present our framework in Sec. 3. Datasets and experimental evalua-tion are discussed in Sec. 4. Related work is summarized in Sec. 5. Finally, we conclude by discussing future research directions.
This section describes the preliminaries of tensor factor-ization. Table 3 defines symbols commonly used in the pa-per.

Definition 1. A rank-one N th order tensor X is the outer product of N vectors, a (1)  X  a (2)  X   X  X  X   X  a ( N ) each element X ~ i = X ( i 1 ,i 2 ,  X  X  X  ,i N ) = a (1) i Definition 2. The Kronecker product of two matrices A  X  B of sizes I A  X  R and I B  X  R respectively, produces a matrix of size I A I B  X  R 2
Definition 3. Khatri Rao product of two matrices A B of sizes I a  X  R and I B  X  R respectively, produces a matrix C of size I a I b  X  R such that C = [ a 1  X  b 1  X  X  X  a R  X  b
Definition 4. The mode-n matricization of X , denoted the elements of a N -way array into a matrix.
Definition 5. The CANDECOMP-PARAFAC (CP) ap-proach approximates the original tensor X as a sum of rank-one tensors and is expressed as where A ( n ) r corresponds to the r th column of A ( n ) A (1) ,  X  X  X  , A ( N ) the factor matrices and use hand notation of the sum of rank-one tensors.
We first formulate the problem and then provide a general overview of the formulation. Finally, we present an efficient optimization algorithm for solving the problem.
We formulate our model as a constrained tensor optimiza-tion, where four constraints (one hard and three soft) are involved:  X  Completion : This is the hard constraint. The unknown full tensor X matches the observed elements in the par-tially observed tensor O .  X  Guidance: A subset of columns in a factor matrix A ( p ) are close to the columns represented by prior knowledge  X   X  Pairwise constraints: The columns in a factor matrix
A ( k ) should be close to orthogonal.  X  Non-negativity: The factor matrices A (1) ,..., A ( N ) ntain only nonnegative entries to enhance interpretability.  X  Sparsity: We adjust the sparsity of each factor matrix
A ( n ) by removing non-zero entries less than  X  n . where,  X  = kX  X  X  X  X k 2 F  X  T =  X  A 1  X  X  X  X  X   X  A N ,  X  A n = { A  X  X  0 } X  [  X  n , +  X  )  X  Next, we formally define all the necessary notations in Ta-ble 4.

In particular, the unknown full tensor X is approximated by two terms, 1) a rank one bias tensor C and 2) a rank-R interaction tensor T . The bias tensor C captures the base-line characteristics of the entire tensor, which is a rank-one tensor with all positive vectors J u (1) ,  X  X  X  , u ( N ) teraction tensor T is a CP tensor model, with nonnegative constraints on all factor matrices. Let Q  X  R R  X  R the pairwise constraints for a specific factor matrix say A with positive  X  q capturing the weights of this constraint. In A rank-R tensor is defined as the sum of R rank-one tensor our experiments, we set Q to be an identity matrix. The guidance knowledge is encoded as a vector where positive entries indicate relevant feature dimensions. For example, we can have a guidance vector corresponding to a hyperten-sion diagnosis, where the hypertension-related entries are set to positive values (e.g., one), and the remaining entries are zero. Now, let us assume that we have R 0 guidance vec-tors ( R 0  X  R ). To ease subsequent derivation, we construct the guidance matrix  X  A ( p ) by adding zero columns to make  X  A ( p ) of the same size as the corresponding factor matrix A columns, we multiply the difference between A ( p ) and  X  by a weight matrix where I  X  R R 0  X  R 0 is the identity matrix.

Next, we explain the intuition behind the model.
At a high-level, Rubik aims to simultaneously conduct the non-negative CP factorization and recover the non-negative low rank tensor X from a partially observed tensor O . This idea is captured through the Factorization error and Com-pletion term in Eq. 1. Note that each A ( k )  X  R I an estimated rank of R . Hence the interaction tensor T has rank up to R and the low rank property is enforced.
Bias tensor. Rubik includes a rank-one bias tensor C to capture the baseline characteristics common in the overall population, which is similar to Marble [15]. In phenotyping applications, it represents the common characteristics of the n th phenotype amongst the entire population (e.g., the value of the element in the diagnosis mode of the bias tensor cor-responding to hypertension represents the overall possibility of any given patient having hypertension).

Guidance information. In real world applications, we may know guidance information that can be encoded into the corresponding factor matrices. For example, we might have the knowledge that some phenotypes should be re-lated to hypertension. Utilizing this guidance information can lead to more intuitive and understandable phenotypes. This constraint is captured through the Guidance informa-tion term in Eq. 1.

Pairwise constraints. We hope to discover distinct phe-notypes in order to obtain more concise and interpretable results. We can penalize the cases where phenotypes have overlapping dimensions (e.g., two common diagnoses be-tween two phenotype candidates from the diagnosis mode). This constraint is captured through the Pairwise constraint term in Eq. 1.
Next we describe the detailed algorithm. The main idea is to decouple constraints using an Alternating Direction Method of Multipliers (ADMM) scheme [4]. For each mode, the algorithm first computes the factor matrix associated with the interaction tensor. Once the interaction factor ma-trix is computed, the bias vector is computed. The whole process is repeated until convergence occurs.
Originally, the objective function  X  is non-convex with respect to A ( k ) due to the fourth order term in pairwise constraints k Q  X  A ( k ) T A ( k ) k 2 F . By variable substitution A the equivalent form of  X  Note that the objective function  X  is now convex w.r.t. A
Using a similar variable substitution technique, Eq. 1 is reformulated into the following equivalent form: where B = { B (1) ,  X  X  X  , B ( N ) } and V = { v (1) ,  X  X  X  , v the collections of auxiliary variables. The partial augmented Lagrangian function for  X  is: L =  X  + where Y = { Y (1) ,  X  X  X  , Y ( N ) } and P = { p (1) ,  X  X  X  , p are the set of Lagrange multipliers.  X  X,Y  X  = P ij X denotes the inner product of two matrices X and Y . {  X , X  } are penalty parameters, which can be adjusted efficiently according to [21].

Here we solve Eq. 3 by successively minimizing the La-grangian with respect to each variable in block coordinate descent procedures. Each iteration involves updating one variable, with the other variables fixed to their most recent values. The updating rules are as follows.

Update the interaction tensor. Without loss of gener-ality, we assume that the prior and pairwise guidance infor-mation are on the n th mode. One can easily modify  X  a , X  to zero if there is no guidance information on a particular mode. Set R = X  X  X  , which is the residual tensor left over after subtracting the effects of the bias tensor in objective function  X  . To compute A ( n ) t +1 , the smooth optimization problem is formulated as follows: where  X  ( n ) is defined as Next, setting the derivatives of Eq. 4 with respect to A ( n ) to zero yields the Sylvester equation: The Sylvester equation can be solved by several numerical approaches. Here we use the one implemented as dlyap func-tion in MATLAB.

To solve for the auxiliary variable B ( n ) , we obtain the following optimization problem. The closed form update for B ( n ) is:
Efficient computation of (  X  ( n ) ) T  X  ( n ) . For two matri-ces M , N , we have the following property of the Khatri-Rao product [30]: As a result, we can efficiently compute (  X  ( n ) ) T  X  ( n )
Update the bias tensor. At this point, we set E to be the residual tensor left over after subtracting the effects of interaction tensor in objective function  X  . E = X  X  X  . For each u ( n ) , we solve the following problem, min where E ( n ) is the mode-n matricization of tensor E .  X  defined as: The closed form solution for u ( n ) is u The optimization problem for auxiliary variable v ( n ) is: Algorithm 1 Minimize  X  1: Input: O ,  X  A , W , Q ,  X  a ,  X  q 2: Initialize A ( n ) 0 randomly, set Y ( n ) 0 = 0 , p 3: repeat 4: for n=1:N do 8: end for 9: Update X t +1 by Eq. 12 10: Update parameter  X  t +1 by  X  t +1 = min(  X  X  t , X  max ) 11: Update parameter  X  t +1 by  X  t +1 = min(  X  X  t , X  max ) 13: return T , C The closed form solution is
Update Lagrange multipliers. We optimize the La-grange multipliers using gradient ascent. Y ( n ) t +1 , p directly updated by
Update the full tensor. We now have the following optimization problem w.r.t. X : The optimal solution is where  X  c is the complement of  X , i.e. the set of indexes of the unobserved entries.

Based on the above analysis, we develop the ADMM sch-eme for Rubik, as described in Algorithm 1.
Parallelization. Our scheme follows the Gauss-Seidel type of updating rule. The Jacobi type updating rules can easily be implemented with slight modification. As a result, our algorithm can be parallelized and scaled to handle large datasets.

Flexible extension . Rubik can be easily modified to incorporate other types of guidance, depending on the do-main application. For instance, in the analysis of brain fMRI data [10], we might need to consider pairwise relationships between different factor matrices. This cross-mode regular-ization can also be easily incorporated in our framework.
Complexity analysis . The time complexity is mainly consumed by computing  X  ( n ) and A ( n ) in Eq. 5, which is of the largest mode as D . Then Rubik has the complexity of O ( D N ). Although Rubik incorporates guidance infor-mation, the computational complexity remains the same as state-of-the-art methods such as Marble [15], CP-APR [15] and WCP [1]. These competitors have similar time complex-ity, but they are much slower in practice due to their gradient based solving scheme and time consuming line search. By contrast, Rubik yields closed-form updates at each iteration.
We evaluate Rubik with two EHR datasets, from Vander-bilt University and the Center for Medicare and Medicaid Services (CMS), each of which contains diagnosis and med-ication information and is roughly in the form of a tensor. Raw diagnosis data in both datasets are encoded following the International Classification of Diseases (ICD-9) classi-fication system. To avoid an overly sparse concept space, similar diagnoses codes were grouped together according to the Phenome-wide Association Study (PheWAS) terminol-ogy [11] and medications were grouped by their correspond-ing classes using the RxNorm ontology 3 .
 Vanderbilt . We use a de-identified EHR dataset from Vanderbilt University Medical Center with 7,744 patients over 5 years of observation. We construct a 3 rd order ten-sor with patient, diagnosis and medication modes of size 7,744 by 1,059 by 501, respectively. The tensor element X ( i,j,k ) = 1 if patient i is prescribed with medication k for treating diagnosis j . 4
CMS. We used a subset of the publicly available CMS 2008-2010 Data Entrepreneurs X  Synthetic Public Use File (DE-SynPUF) dataset from the CMS [6]. For this dataset, the tensor element is based on all co-occurrences of prescrip-tion medication events and diagnoses from outpatient claims of the same patient happening on the same date, for years 2008-2010. Specifically, we constructed a tensor representing 472,645 patients by 11,424 diagnoses by 262,312 medication events.

The goal of our evaluation is four fold: 1. Phenotype discovery: Analyze how Rubik discovers meaningful and distinct phenotypes with different com-binations of guidance. 2. Noise analysis: Evaluate Rubik X  X  performance with dif-ferent scenarios of noisy and missing data. 3. Scalability: Assess the scalability of Rubik in compar-ison to the state-of-art methods for tensor factorization and completion. 4. Constraints analysis: Analyze the contribution of dif-ferent constraints towards model performance.
 In particular, the phenotype discovery and noise analysis are evaluated using the Vanderbilt data because it is real, while scalability is evaluated on both datasets. To tune hyper-parameters  X  a and  X  q , we run experiments with different values and select the ones that give most meaningful results.
We compare Rubik with several baseline models as de-scribed below:  X  Marble: This method applies sparse tensor factorization for computational phenotyping [15].  X  TF-BPP: This is the block principle pivoting method [18] for non-negative CP tensor factorization. http://www.nlm.nih.gov/research/umls/rxnorm/
We assume a medication may be used to treat a specific diagnosis if both diagnosis and medication occurred within 1 week. Diagnoses Medications Hypertension Statins Disorders of lipoid metabolism Loop diuretics Heart failure Miscellaneous analgesics Respiratory &amp; chest symptoms Antihistamines Chronic kidney disease Vitamins Other and unspecified anemias Calcium channel blockers Diabetes mellitus type 2 Beta blockers Digestive symptoms Salicylates Other diseases of lung ACE inhibitors Table 5: Elements of the diagnosis and medication modes in the bias tensor.  X  CP-APR: This method is designed for non-negative CP
Possion factorization [9].  X  WCP: This is a gradient based tensor completion ap-proach [1].  X  FaLRTC: This approach recovers a tensor by minimizing the nuclear norm of unfolding matrices [22].
Phenotype discovery is evaluated on multiple aspects, in-cluding: 1) qualitative validation of bias tensor and interac-tion tensors and 2) distinctness of resulting phenotypes. We choose Marble as the competitor, since it is the only other method that generate sparse phenotypes, which is clinically important.
A major benefit of Rubik is that it captures the charac-teristics of the overall population. Note that the Centers for Disease Control and Prevention (CDC) estimates that 80% of older adults suffer from at least one chronic condition and 50% have two or more chronic conditions [7]. In our bias ten-sor, fice of the ten diagnoses are chronic conditions, which supports the CDC claim. In addition, the original data had a large percentage of patients with hypertension and related co-morbidities, such as chronic kidney disease, disorders of lipoid metabolism and diabetes. Most of the elements (for diagnosis and medication modes) in the bias tensor shown in Table 5 are also found among the most commonly oc-curring elements in the original data. Based on the above observations, we can see that the elements of the bias tensor factor are meaningful and that they accurately reflect the stereotypical type of patients in the Vanderbilt dataset.
Next, we evaluate whether the interaction tensor can cap-ture meaningful phenotypes. To do so, we conducted a sur-vey with three domain experts, who did not know which model they were evaluating or introduce the guidance. Each expert assessed 30 phenotypes (as in Table 1) from Rubik and 30 phenotypes from Marble. For Rubik, we introduced four phenotypes with partial diagnosis guidance. For each phenotype, the experts assigned one of three choices: 1) YES -clinically meaningful, 2) POSS -possibly meaningful, 3) NOT -not meaningful.

We report the distribution of answers in Figure 1. The inter-rater agreement is 0.82, indicating a high agreement. Rubik performs significantly better than the baseline method. On average, the domain experts determined 65% (19.5 out of 30) of the Rubik phenotypes to be clinically meaningful, Figure 1: A comparison of the meaningfulness of the phenotypes discovered by Marble and Rubik. with another 32% of them to be possibly clinically mean-ingful. On the other hand, the clinicians determined only 31% of the baseline Marble derived phenotypes to be clini-cally meaningful, and 51% of Marble derived phenotypes to be clinically meaningful. Only 3% of Rubik derived pheno-types were determined to be not meaningful, while 18% of Marble derived phenotypes were considered not meaningful. These results collectively suggest that Rubik may be capable of discovering meaningful phenotypes.
Here we demonstrate how novel combinations of guidance information and pairwise constraints can lead to the discov-ery of fine-grained subphenotypes. In this analysis, we add the diagnosis guidance to hypertension , type 1 diabetes , type 2 diabetes and heart failure separately.

To gain an intuitive feel for how guidance works, let us use the guidance for hypertension as an example. We construct the guidance matrix  X  A (2) as follows. Assume the index cor-responding to hypertension to be n . Then, we set the n th tries equal to zero. The pairwise constraint will push A and A (2) 2 to be as orthogonal as possible (i.e., small cosine similarity). In other words, the resulting factors will share less common entries, and will thus be more distinct. In summary, by introducing multiple identical guidance fac-tors (e.g., 2 hypertension vectors), the algorithm can derive different subphenotypes which fall into a broader phenotype described by the guidance factors.

Table 6 shows an example phenotype for hypertension pa-tients that was discovered using Marble. While this pheno-type may be clinically meaningful, it is possible to stratify the patients in this phenotype into more specific subgroups. Table 7 demonstrates the two subphenotypes discovered by Rubik with the hypertension guidance, which effectively in-clude non-overlapping subsets of diseases and medications from the Marble derived phenotype.

In a similar fashion to the evaluation of the interaction tensor, we asked domain experts to evaluate the meaning-fulness of subphenotypes. Rubik incorporated four guid-ance constraints (for four separate diseases), and generated two subphenotypes for each guidance constraint, resulting in eight possible subphenotypes. Domain experts were asked to evaluate whether or not each subphenotype was made sense as a subtype of the original constraint. The inter-rater agree-ment is 0.81. On average, the clinicians identified 62.5% (5 Diagnoses Medications Chronic kidney disease Central sympatholytics Hypertension Angiotensin receptor blockers Unspecified anemias ACE inhibitors Fluid electrolyte imbalance Immunosuppressants Type 2 diabetes mellitus Loop diuretics Other kidney disorders Gabapentin Table 6: An example of a Marble-derived pheno-type.
 A. Metabolic syndrome phenotype Diagnoses Medications Hypertension Calcineurin inhibitors Chronic kidney disease Insulin Ischemic heart disease Immunosuppressants Disorders of lipoid metabolism ACE inhibitors Anemia of chronic disease Calcium channel blockers B. Secondary hypertension phenotype Diagnoses Medications Secondary hypertension Class V antiarrhythmics Fluid &amp; electrolyte imbalance Salicylates Unspecified anemias Antianginal agents Hypertension ACE inhibitors Table 7: Examples of Rubik-derived subphenotypes.
 The two tables show separate subgroups of hyper-tension patients: A) metabolic syndrome, and B) secondary hypertension due to renovascular disease. out of 8) of all subphenotypes as clinically meaningful, and the remaining 37.5% of subphenotypes to be possibly clin-ically meaningful. None of the subphenotypes were identi-fied as not clinically meaningful. These results suggest that Rubik can be effective for discovering subphenotypes given knowledge guided constraints on the disease mode.
One important objective of phenotyping is to discover dis-tinct phenotypes. In this experiment, we show that adding pairwise constraints guidance reduces the overlap between phenotypes. We also fix  X  a = 0 and change the weight of pairwise constraint (  X  q ) to evaluate the sensitivity of Rubik to this constraint.

We first define cosine similarities between two vectors x measure the degree of overlapping between all phenotype pairs. This is defined as the average of the cosine similarities between all phenotype pairs in the diagnosis mode. The formulation for AvgOverlap is as follows.
 where A (2) m denotes the m th column of factor matrix A (2) which is the vector representation of the m th phenotype on the diagnosis mode. Figure 2: The average level of overlap in the pheno-types as a function of the pairwise constraint coeffi-cient  X  q .

Figure 2 shows the change of average cosine similarity as a function of  X  q . We can see that the average similarity tends to stabilize when  X  q is larger than 10. Figure 2 also compares Rubik with Marble, CP-APR and TF-BPP. Note that the three competitors do not incorporate pairwise con-straints and clearly lead to significantly overlap in their phe-notypes. As such, we can conclude that adding the pairwise constraints can effectively lead to more distinct phenotypes.
There are multiple sources of noise in real clinical appli-cations. For example, part of the EHR data could be miss-ing or simply incorrect. Alternatively, the clinical guidance could be noisy. To test Rubik against such conditions, we systematically evaluate different scenarios of noise. The re-sults are summarized over 10 independent runs.
The EHR data may have missing records. Consequen-tially, the observed binary tensor O might contain missing values. To emulate the missing data, we flip the value of each non-zero cell O ~ i to zero with probability p .
Let T = J A (1) ;  X  X  X  ; A ( N ) K be the solution without miss-ing data and T p = J A p (1) ;  X  X  X  ; A p ( N ) K be the solution with missing data level p on the input tensor O . We first pair T p with T using a greedy algorithm. Then we define the average cosine similarities ( AvgSim ) between T and T p as
Figure 3 shows the change in similarity measures with the change of missing-data level. It can be seen that Rubik performs well even if 30% of the data is missing, indicating its robustness. By contrast, the other baseline methods are not robust to missing data quantities as small as 10%.
In creating documentation in the EHR dataset, clinicians may introduce erroneous diagnosis and medication informa-tion. The implication of such errors is that the observed binary tensor O will also contain noise. To emulate this set-ting, we randomly select zero value cells, such that the total number of selected cells is equal to the number of non-zero cells. Then, for each cell in this collection, we flip its value with probability p . The difference from introducing missing data is that we introduce multiple incorrect ones as noisy Figure 3: An average similarity comparison of differ-ent methods as a function of the missing data level. Figure 4: An average similarity comparison of dif-ferent methods as a function of the noise level. data while missing data case will remove multiple correct ones.

Figure 4 shows that Rubik performs well -even when the noise level is as high as 20% -30%. One intuitive explanation for such a high tolerance is that the model is dependent on the observed tensor, as well as various constraints. In summary, Rubik is resilient to noise. As such, Rubik will provide more generalizable phenotypes than its competitors.
At times, guidance knowledge may be incorrectly docu-mented (or the state of medical belief may be incorrect). Take hypertension for example, to simulate incorrect guid-ance on this diagnosis, we randomly pick K (we use 4 in our experiments) entries from the corresponding column in the guidance matrix  X  A (2) and set it to be one with probability p . Note K is small here for two reasons. First, in phenotyp-ing applications we hope to achieve sparse solutions, which implies that the guidance should also be sparse. Second, medical experts do not typically have much guidance on a particular phenotype.

We then compare the cosine similarity between the phe-notypes obtained by correct and incorrect guidance. Fig-ure 5 demonstrates that as the level of incorrect guidance increases the cosine similarity slowly decreases. Therefore, even when a significant portion of the guidance is incorrect, the phenotype can remain fairly close to the original.
Rubik computes the sparse factor representation using a set of predefined thresholds  X  n , which provide a tunable Figure 5: The similarity between the true solution and the solution under incorrect guidance as a func-tion of the incorrect guidance level. Figure 6: The count of non-zero elements along the three modes as a function of the thresholding pa-rameters. knob to adjust the sparsity of the candidate phenotypes. In our experiment, we numerically evaluate the sensitivity of the sparse solution with respect to different threshold val-ues. To do so, we randomly downsample the tensor O by 50% and run Rubik on this smaller tensor. The above pro-cedure is averaged over 10 independent runs.

Figure 6 shows the distribution of the non-zero factor val-ues along the three modes. For all three plots, there is a no-ticeable difference in size between the first two bins, which suggests the threshold occur at the start of the second bin. Thus, for the paper, we set  X  = 0.00005, 0.0001 and 0.01 for the patient, diagnosis and medication modes, respectively.
We test the scalability of Rubik on both datasets. We randomly sample a different number of patients from the datasets and construct the tensors. The results are sum-marized over 10 independent runs and the performance is measured in runtime (seconds).

Figure 7 reports the runtime comparison of different ten-sor factorization methods on the V anderbilt dataset. We can clearly see the advantage of our framework over CP-APR. Specifically, the runtime is reduced by 70%. TF-BPP is comparable to Rubik on the V anderbilt dataset. For the CMS dataset, Rubik is around six times faster than the two baselines.

For the tensor completion task, Figure 7 shows that Ru-bik is nearly 5-7 times faster than WCP and FaLRTC. Fig-ure 8 further demonstrates Rubik X  X  superiority over these methods. Note that FaLRTC and WCP will reach their maximum default number of iterations before the algorithm Figure 7: A runtime comparison of different meth-ods on the V anderbilt dataset as a function of the number of patients.
 Figure 8: A runtime comparison of different meth-ods on the CMS dataset as a function of the number of patients. actually converges on the CMS dataset, so that more time is actually needed to complete the baselines.
We investigate the sensitivity of Rubik with regard to three constraints: completion, guidance and pairwise con-straints. We begin by running Rubik under baseline con-ditions without any constraints. Then, we iteratively add constraints independently and compute AvgSim (Eq. 13) between the solution obtained with that constraint and with-out that constraint. Then, we normalize all AvgSim scores and let them sum to one. The contribution of each con-straint is measured by the normalized AvgSim score. Note that we did not incorporate guidance information into Rubik on the CMS dataset.

Figure 9 reports the proportion of each constraint X  X  con-tribution to the overall model. The bars labeled Basic repre-sent the baseline performance of the model without any con-straints imposed. The bars labeled Pairwise, Guidance and Completion represent the contributions of the corresponding constraints, respectively. We see that all of the constraints represent a significant contribution to the model X  X  overall performance. Amongst the constraints, the pairwise con-straint provides the largest contribution to the performance.
Non-negative Tensor Factorization. Kolda [19] pro-vided a comprehensive overview of tensor factorization mod-Figure 9: Proportion of contribution of each con-straint. els [19]. It is desirable to impose a non-negativity con-straint on tensor factorizations in order to facilitate eas-ier interpretation when analyzing non-negative data. Ex-isting non-negative matrix factorization algorithms can be extended to non-negative tensor factorization. Welling and Webber proposed multiplicative update algorithm [32]. Chi and Kolda [9] proposed nonnegative CP alternation Possion regression (CP-APR) model. Kim et al. [18] proposed an alternating non-negative least square method with a block pivoting technique.

Constrained Factorization . Incorporating guidance in tensor factorization has drawn some attention over the past few years. Carroll et al. [5] used linear constraints. David-son [10] proposed a framework to incorporate pattern con-straints for network analysis of fMRI data. However, this method is domain specific and might not be applicable to other areas such as computational phenotyping. Narita [25] provided a framework to incorporate auxiliary information to improve the quality of factorization. However, this work fails to incorporate nonnegativity as constraints to the fac-tor matrix. Solving for non-negativity constraints usually requires a nontrivial calculation step.

Coupled Factorization . In this case, we have addi-tional data matrices that share the same dimension as the tensor. The goal is to jointly factorizing the tensor and ma-trices [13]. Acar et al. [2] used first order optimization tech-niques. There are also scalable algorithms on Hadoop [3, 26]. However, this framework does not directly cover all the constraints we need in our application.

Tensor Completion . Liu et al. [22] and Signoretto [29] generalized matrix completion to the tensor case to recover a low-rank tensor. They defined the nuclear norm of a tensor as a convex combination of nuclear norms of its unfolding matrices. Tomioka and Suzuki [31] proposed a latent norm regularized approach. Liu et al. [23] substituted the nuclear norm of unfolding matrices by the nuclear norm of each fac-tor matrix of its CP decomposition. A number of other alternatives have also been discussed in [20, 24, 28]. How-ever, these methods suffer from high computational cost of SVDs at each iteration, preventing it from scaling to large scale problems.

Without minimizing the nuclear norm, Acar et al. [1] pro-posed to apply tensor factorization in missing data to achieve low rank tensor completion. However, none of these meth-ods are guaranteed to output a non-negative factor matrix for each mode or non-negative tensor. As a consequence they are not applicable to our non-negative tensor setting. Incorporating non-negativity as constraints to factor matri-ces, Xu et al. [33] proposed an alternating proximal gradient method for non-negative tensor completion. However, they did not take the guidance information into account and their gradient based method is not scalable to large datasets.
In summary, existing tensor factorization and completion methods are not applicable to computational phenotyping.
This paper presents Rubik, a novel knowledge-guided ten-sor factorization and completion framework to fit EHR data. The resulting phenotypes are concise, distinct, and inter-pretable. One distinguishing aspect of Rubik is that it is able to discover subphenotypes. Furthermore, Rubik cap-tures the baseline characteristics of the overall population via an augmented bias tensor.

In the experiments, we demonstrate the effectiveness of adding the guidance on diagnosis. Rubik can also incorpo-rate guidance from other sources such as medications and patients. We also demonstrate the scalability of Rubik on simulated EHRs in a dataset with millions of records. Rubik can potentially be used to rapidly characterize and manage a large number of diseases, thereby promising a novel solution that can benefit very large segments of the population. Fu-ture work will focus on evaluating Rubik on larger datasets and conduct larger medical validations with experts. Acknowledgement: This work was supported by NSF IIS-1418511, IIS-1418504, IIS-1417697; NIH R01LM010207, K99LM011933, CDC, Google Faculty Award, Amazon AWS Research Award and Microsoft Azure Research Award.
