 Automatic analysis of sentiments expressed in large scale online reviews is very important for intelligent business applications. Sentiment classification is the most popular task of sentiment analysis, which is more challenging than traditional topic-based text classification. Basic features, such as vocabulary words, are not enough to classify sentimen ts well. Deep Belief Network (DBN) is introduced to discover more abstract features of sentiments. To capture full inform ation of the features, large-size network can be constructed, but at the same time, large-size network tends to over fit the training data and even noise, which will reduce the generalization ability of the network. In this paper, L2-norm Deep Belief Network (L 2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN. L2DBN is first initialized by an unsupervised layer-wise training algorithm, and then fine-tuned by a supervised procedure. Network parameters are optimized using both classification loss and network complexity. Experimental results show that the proposed L2DBN outperforms the st ate-of-the-art method and the basic DBN on golden, noisy a nd heterogeneous datasets. I.2.7 [ Artificial Intelligence ]: Natural language processing Algorithms, Experimentation. Sentiment Classification, Deep Belief Network, L2-norm Regularization. With the development of Web 2.0, web users can freely post their opinions and reviews about vari ous resources. Automatic mining these huge amounts of online reviews can help companies and customers to make more intelligent business decisions. The task of sentiment classification [1, 2] is naturally proposed, and its goal is to label the sentiment orientation of subjective texts as  X  X humb up X  or  X  X humb down X , which has received more and more research attention in recent years [3, 4]. Although much research attention ha s been paid on this task, it is still challenging to develop a sentiment classification system with high performance because this task is more difficult than traditional topic-based text cla ssification (TC). For topic-based TC, domain-specific terms are good indicators to distinguish topic classes, but it is difficult to dis tinguish sentiments by isolated terms because of the following reasons. 1) The same expression may indicate different sentiments in different contexts. For instance,  X  X  feel sleepy X  conveys a negative sentiment when the speaker is watching a film, but it indicates a positive opinion of someone who has just taken a sl eeping pill. 2) A single review may contain both positive and negative polarity words. For instance, the review  X  X he food is delicious, but the service is terrible X  for a restaurant includes both positive word  X  X elicious X  and negative word  X  X errible X . 3) Sentiments may often be expressed in a subtle manner. More abstract semantic features are needed to overcome the difficulty of sentiment classifi cation. Deep Belief Network (DBN) [5] presents a new way to obtain the high-level abstraction of input data, and has been successfully applied to digit recognition [5], language model [6], etc. In this paper, we introduce DBN to sentiment classification to obtain more abstract features of sentiments. Before constructing DBN for a real application, it is important to choose a proper size of its network structure. On the one hand, a small-size network structure with fewer neurons and connections cannot catch adequate enough information for sentiment classification. On the other hand, an oversize structure with more neurons and connections will cover some trivial features of training data and cause over-fitting. Because large-scale lexical features have already formed a huge dimension of space for the sentiment classification problem, in order to capture the full information of input features, a large-size network should be used. In order to avoid over-fitting and to improve the generalization ability of the network, we construct an L2-norm Deep Belief Network (L2DBN) for sentimen t classification. L2-norm regularization is a commonly used technique for generalization. To the best of our knowledge, this is the first attempt to apply L2-norm regularization to DBN. The system framework of sen timent classification based on L2DBN is shown in Fig.1, whic h include three parts. Review documents are preprocessed and represented as feature vectors at first, and then divided into two parts for training and test. The training process in the second part can be divided into two steps: pre-training and fine-tuni ng. Pre-training is unsupervised, and an initial network will be obt ained using a greedy layer-wise training algorithm. During fine-tuning, the parameters of all the layers will be updated using back propagation algorithm. An L2-norm strategy is added to the fi ne-tuning process so that better network parameters can be achieved. The performance of the system is evaluated by comparing predicted labels with actual labels. Given a set of review documents X ={ x 1 , x 2 ,..., x sentiment classification is to identify the sentiment polarity (positive or negative) expressed in each review. Each review x represented as a feature vector x i =( x i 1 , x i 2 number of feature words in the dataset. The label of each review x is also represented as an i ndicator vector. In this paper, y is used to represent positive reviews and y i =(-1,1) is used to represent negative reviews. Supposing the label set Y ={ y y }, mapping function YXf  X  : needs to be found to classify each review to its corresponding polarity class. A deep architecture is constructe d to model the sentiments of reviews, so that high-level abstrac tions of data can be discovered. DBN is one of the most popular deep architectures. As shown in Fig.2, it is a multi-layer connected network with symmetric weighted connections between two adjacent layers, but no intra-layer connection. The input layer accepts the feature vector of each review. The output layer, which represents the predicted label for each review, is compared with the actual label by the loss function which is used to fine-tune the network. For convenience, the following notations are used to denote the components of the network:  X  v=h 0 : input layer  X  h i ( i =1,2,..., K-1): the i th hidden layer  X  o=h K : output layer  X  w i ( i =1,..., K ): connection weight matrix between h  X  b i ( i =0,..., K-1): biases for neurons of layer h  X  c i ( i =1,..., K ): biases for neurons of layer h defined as: where )( x  X  is the logistic function: Although gradient descent algorithm can be used to tune the weights of the network, it works well only when the initial weights are near to the good solution. An unsuitable initial network will make the deep networ k trap into a local optimized solution. A layer-wise pre-training procedure proposed by Hinton top, each pair of two adjacent layers can be regarded as a Restricted Boltzmann Machine (RBM) by taking the lower layer as visible layer v and the upper layer as hidden layer h . The whole network is constructed by training one RBM at a time in a bottom-up fashion. One RBM has the following energy function: The goal of pre-training is to maximize the probability of generating training data. The probability of each training data assigned by the network can be calculated using the energy function above: Then the gradient of the log probability of the training data can be given by: where  X  s v and  X  t h can be obtained by alternating Gibbs sampling from the network. For simplicity, the following approximate objective is used to update the weight: where  X  is the learning rate. A similar version of the learning rule is used for the biases. In order to enable DBN to be more proper to solve the sentiment classification task, a discrimina tive fine-tuning algorithm is used to minimize the classification e rror on the training data, and the stochastic activities is replaced by deterministic, real-valued probabilities. The optimization problem can be formulated as: and Z is the labels predicted by the network. Supposing the number of training examples is N , ),,(  X  ZYL can be defined as an exponential loss function: where N denote the number of training examples and C denote sentiment classes. The popular grad ient descent algorithm is used to optimize the goal above. The discriminative fine-tuning can make the network fit with the training data very well, but it may also lead to over-fitting on the following three aspects: 1) some redundant neurons, connections or weights tend to be generated to model some trivial features of training data; 2) the network will fit with noise when the training data is not clean; 3) the network will perform poorly when the test data has inconsistent distributions with the training data. On the other hand, for the sentim ent classification task, a smaller network with fewer neurons or hi dden layers will lose the useful information essential for classification. For example, for the ELE review dataset used in our experiment, the accuracy obtained by network  X 1500-1200-900 X  is higher than that by simple network  X 500-400-300 X  and  X 200-100-50 X  by 2 and 4 percent respectively. A relatively larger network should be used to catch enough information, but such a complex ne twork tends to over fit the data and reduce the generalization ability of the model. Many methods can be used to improve the generalization power of a machine learning method. L2-norm regularization [7] is a popular technique for generalization, and can be defined as: L2-norm regularization assume s the simplest network can generalize well, and make the unn ecessary connection weights to get lower values or even zeroes. This network complexity penalty factor is then combined with the loss function to form a new optimization goal, which can be expressed as: where  X  is the relative importance of the network complexity penalty in comparison with the classification error rate. This new optimization goal enables the network to obtain lower classification error and better generalization ability. The effectiveness of the proposed method was evaluated on four datasets of product reviews, including books (BOO), DVDs (DVD), electronics (ELE), and kitche n appliances (KIT) [8]. Each dataset includes 1,000 positive a nd 1,000 negative reviews. the 2,000 reviews were randomly divide d into ten equal sizes and all the methods are tested by ten-fold cross validation. In order to reduce the effect of random factor, training and test were done for evaluation criterion can be expressed as: where # right is the number of test exam ples labeled with correct labels by using the method, and # total represents the total number of test examples. All of the reviews are tokenized and then represented as a binary weight vector for word presence. Some stopwords are removed, including punctuations, numbers, one -character words, and words that occur in a single review. All the words are converted to lowercases and no stemming is conducted. In addition, we use document frequency as the basic f eature selection method to filter top 1.5% common words. The number of hidden layers of L2DBN and DBN is set to three and the number of neurons of hidden layers is set to be 1500-performance over network complexity is set to be 0.01. Other parameters of networks are fixed as the default parameter settings of Hinton X  X  DBN package. The performance of L2DBN is compared with that of SVM, the most popularly used classification method on this problem [2]. SVMLight with linear kernel function is used. We also compare L2DBN with the basic DBN proposed by Hinton [5]. Table 1 shows the accuracy obtained by SVM, DBN and L2DBN. It also can be observed that L2DBN achieves the best performance among all these methods on these four datasets. The similar experimental result with SVM can be referred in [9]. Table 1. Accuracy Comparison on Four Golden Datasets In order to verify the noise tolerance ability of L2DBN, we generate some noise by randomly selecting r percent training examples and reversing their cla ss labels. Different noise ratios golden data. Figure 3 presents the comparative results of L2DBN with DBN and SVM on the ELE and KIT datasets with different label noise ratios. 
Figure 3. Performance for Diff erent Noise Ratios on a) ELE It can be seen that the accuracy of DBN and SVM deceases obviously as the noise ratio in creases, while the accuracy of L2DBN is more stable and is still satisfactory even on noisy datasets. L2DBN obtains the best performance on all datasets at different noise ratios. The improve ment is much larger as the noise ratio increases. This experiment indicates that L2DBN can tolerate noise much better than traditional methods. If the training data and test data have inconsistent distribution, then the classification becomes much more difficult. We attempt to create heterogeneous training and test data in the following way. We randomly substitute a certain number of training examples of one dataset by th e same number of examples of another dataset, and remain the te st dataset as same as before. Here for illustration, we substitute 40 percent of examples of ELE by the same number of examples of BOO, DVD and KIT respectively and we get three datasets. Table 2. Accuracy Comparison on Heterogeneous Dataset ELE golden data 80.8% 80.9% 82.0% ELE with BOO 78.0% 79.3% 82.7% ELE with DVD 78.8% 79.4% 82.6% ELE with KIT 80.8% 80.6% 82.9% As shown in Table 2, among all th e systems, L2DBN can still be used to achieve the best accuracy. The performance of L2DBN is more stable comparing with ot her two methods. It is very interesting that when 40 percent of ELE training data is substituted by examples of KIT, all the methods obtain similar accuracy as original ones because the feature distribution of ELE and KIT are very similar, which is also measured by Blitzer [8]. In this paper, L2DBN is pr oposed to solve the sentiment classification task. It can obtain high-level abstract features from input data and achieve better ge neralization ability. After pre-training and discriminative fine-tuning with L2-norm regularization, the final deep network is more powerful for sentiment classification task. Experimental results show that the L2DBN outperforms the basic DB N and the state-of-the-art classifier on four product review datasets. The improvement is much larger on noisy data a nd heterogeneous data. We can conclude that L2DBN is more proper to solve problems in real applications with noisy a nd heterogeneous data. The good performance of L2DBN on heterogeneous data also indicates the potential of L2DBN for domain transfer learning, and we will conduct more work on this topic in the future. We will also apply L2DBN on more applications to verify its effectiveness. This work was supported by the National Natural Science Foundation of China with grant No. 61003204, 60873017, and HGJ project with grant No. 2010ZX01042-002-002-03. [1] Turney P. D. 2002. Thumbs up or Thumbs Down? Semantic [2] Pang B., Lee L., and Vaithyanathan S. 2002. Thumbs Up? [3] Pang B. and Lee L. 2008. Op inion Mining and Sentiment [4] Liu B. eds. 2010. Sentiment Analysis and Subjectivity: [5] Hinton G. E. and Salakhutdi nov R. R. 2006. Reducing the [6] Mnih A. and Hinton G. E. 2008. A Scalable Hierarchical [7] Hastie, T., Tibshirani, R., Friedman J. 2001. The Elements of [8] Blitzer J., Dredze M., and Pereira F. 2007. Biographies, [9] Dasgupta S. and Ng V. 2009. Mi ne the Easy, Classify the 
