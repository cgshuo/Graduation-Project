 There is an exploding amount of user-generated content on the Web due to the emergence of  X  X eb 2.0 X  services, such as Blogger, MyS-pace, Flickr, and del.icio.us. The participation of a large number of users in sharing their opinion on the Web has inspired researchers to build an effective  X  X nformation filter X  by aggregating these in-dependent opinions. However, given the diverse groups of users on the Web nowadays, the global aggregation of the information may not be of much interest to different groups of users. In this paper, we explore the possibility of computing personalized aggre-gation over the opinions expressed on the Web based on a user X  X  indication of trust over the information sources. The hope is that by employing such  X  X ersonalized X  aggregation, we can make the recommendation more likely to be interesting to the users. We ad-dress the challenging scalability issues by proposing an efficient method, that utilizes two core techniques: Non-Negative Matrix Factorization and Threshold Algorithm , to compute personalized aggregations when there are potentially millions of users and mil-lions of sources within a system. We show that, through experi-ments on real-life dataset, our personalized aggregation approach indeed makes a significant difference in the items that are recom-mended and it reduces the query computational cost significantly, often more than 75%, while the result of personalized aggregation is kept accurate enough.
 H.3.3 [ Information Search and Retrieval ]: Information filter-ing; H.3.4 [ Systems and Software ]: Performance evaluation (effi-ciency and effectiveness) Algorithms, Experimentation Persoanlized Recommendation, Aggregate Queries, Matrix Factor-ization, Web-mining.

The amount of user-generated content on the Web is explod-ing as the broadband internet connection becomes ubiquitous and many  X  X eb 2.0 X  services, such as Blogger, MySpace, Flickr, and del.icio.us, make it extremely easy even for a novice user to post and share their own creation on the Web. These user-generated contents are the expression of individuals X  opinion on various is-sues and items around them. For example, a technical blogger may write his own opinion on a new and highly-anticipated gadget on his blog, and a del.icio.us user may  X  X ookmark X  a page if she finds the page worth a later visit.

The participation of a large number of users in sharing their opin-ion on the Web has inspired researchers to build an effective  X  X n-formation filter X  by aggregating these independent opinions. For example, del.icio.us and Digg  X  two popular online bookmarking sites  X  count how many times a page is  X  X ookmarked X  or  X  X igged X  by their users, and prominently show the most popular pages on their front page. The hope of these aggregation services is that by leveraging the independent judgement of millions of users, we can tap into the wisdom of the crowds and make a good collective judgement on what items and/or topics are truly interesting.
Such aggregation service is shown to be effective in finding the popular and interesting items from the Web. However, this ap-proach suffers from two shortcomings: (1) a particular user X  X  in-terest may be significantly different from the interest of the general publics. (2) it may be vulnerable to spam. For example, a group of users can collaborate to promote pages for their own benefit to the front page of Digg. Also, given the diverse groups of users on the Web nowadays, a Web page that has been recommended by a particular group of users may not be of much interest to different groups of users.

In this paper, to address the above-mentioned problems, we ex-plore the possibility of computing personalized aggregation of the opinions expressed on the Web. Under our approach, we assume that each user indicates how much the user  X  X rusts X  each informa-sonalized recommendation of items based on how many times each item is endorsed by their trusted sources. By employing this  X  X er-sonalized X  aggregation, we can make the recommendation more likely to align with the user X  X  interest and less vulnerable to spam created by untrusted sources. In fact, in our experiments, we ob-tion, such as a blog page maintained by a particular blogger, or the bookmark page maintained by a particular del.icio.us user. page mentioned on blogs, or a hot topic or gadget being discussed on blogs. serve that our personalized aggregation approach indeed makes a significant difference in the items that are recommended and cap-tures the user X  X  interest much better than a overall aggregation ser-vice.

Although such an idea can be extremely effective, one important challenge in providing personalized aggregation is the problem of scalability. Given that there are potentially millions of users who may use the system and millions of sources to get the opinions from, computing the personalized aggregation every time when a user requests incurs significant computational cost as we will see later. The main focus of this paper is to address this scalability is-sue. Roughly, our idea for addressing this challenge is to model the personalized aggregation problem as matrix multiplication and apply an effective matrix decomposition method to reduce the mul-tiplication cost. As we will show, our approach for personalized ag-gregation reduces the computational cost significantly, often more than 75%, while the result of personalized aggregation is kept ac-curate enough.

The rest of the paper is organized as follows. In Section 2, we formulate the personalized aggregation as the problem of weighted sum computation and present two basic methods to perform this computation. We then present a matrix representation of this prob-lem and propose an efficient method to support personalized aggre-gations . In Section 3, we will describe the experiments with real dataset to show the impact of personalization and the efficiency of our proposed method. Finally, we give concluding remarks and a brief discussion of some possible future investigations in Section 5 after we briefly go over related work in Section 4.
In this paper, we assume that we want to compute personal-ized recommendations for n users ( U = { u 1 , u 2 , . . . , u aggregating opinions from m different individual bloggers ( { b 1 , b 2 , . . . , b m } ). While our method is general enough, we focus on blogs to make our discussion more concrete. It is straightfor-ward to apply our framework to other application domain. We as-sume that each user u i has expressed his level of  X  X rust X  on each blogger b j as the trust score T ( u i , b j ) . We refer to the set of trust scores that the user u i places on bloggers b 1 , . . . , b as the trust vector of u i . These trust scores may be provided ex-plicitly by the users (e.g., users may subscribe to a list of blog RSS feeds, indicating their interest and/or trust in those blogs) or the trust scores may be estimated by analyzing the past behavior of the users (e.g., by monitoring how frequently a user reads arti-cles from each blog). We assume that there are l items of interest (
O = { o 1 , o 2 , . . . , o l } ) that bloggers mention and that can poten-tially be recommend to the users. The exact definition of an item is application dependent. For example, for a system that recommends Web pages, an item will be a Web page, and for a system that rec-ommends electronic gadgets, an item will be an electronic gadget that is mentioned on the blogs. Whenever a blogger b j mentions an item o k on his blog and expresses his opinion, we assume that he provides a certain degree of  X  X ndorsement X  for o k , represented as the endorsement score E ( b j , o k ) . For example, when a blogger includes a link to the Web page o k in one of his articles, we may assume that b j is giving the endorsement score 1 for o k all endorsement scores for the item o k by the bloggers b is referred to as the endorsement vector of the item o k .
Under this notation, our problem of computing personalized ag-gregate recommendation can be stated as follows: P
ROBLEM 1 For each user u i and each item o k , we want to com-pute the personalized endorsement score R ( u i , o k ) and return items with the highest personalized scores to the user. Equation 1 can be restated using the following vector notation:
In order to compute the personalized endorsement score for each user, we have to maintain the trust score T ( u i , b j ) pair and the endorsement score E ( b j , o k ) for every o b . We may record these scores in the following two tables: Each tuple in the Trust table records a user X  X  trust score on a blogger in the score attribute. Each tuple in the Endorsement table records the endorsement score by a blogger for an item in the score at-tribute.

Given the above two tables, computing the items with highest personalized endorsement scores for user u i can be expressed as the following SQL query Q 1 :
Here, we assumed that we want to return the top-20 items, using the syntax ORDER BY score DESC LIMIT 20 , for the user u i
One simple way of returning the items with the top personal-ized endorsement scores is to compute the answer for the query Q 1 on-the-fly when the user wants personalized recommendation. This approach is illustrated in Figure 1(a) and is referred to as OTF (short for on-the-fly). Under OTF, we maintain one global Endorse-ment table, where a new tuple is inserted (or an existing tuple is updated) whenever a blogger posts a new article and provides an endorsement for an item. Using this global Endorsement table, we then execute Q 1 by joining the table with a user X  X  trust scores in the Trust table when the user asks for recommendation.

Unfortunately, repeatedly executing Q 1 for every user X  X  request can be prohibitively expensive. Because there are millions of blog-gers who keep posting new articles (and thus providing new en-dorsements), and a user may trust a large number of bloggers and have many non-zero entries in his trust vector, the join in Q1 may involve millions of tuples, which will be too expensive to perform on the fly.

Alternatively, to avoid this runtime query execution cost, we may proactively precompute the personalized endorsement scores for every user. That is, for each user u i , we maintain his personalized endorsement score table as we show in Figure 1(b). Under this approach, whenever a blog-ger b j provides a new endorsement for the item o k , all Personal-izedEndorsement tables for the users with non-zero trust on updated to reflect this new endorsement. We refer to this approach as VIEW , since this approach maintains a materialized view of per-sonalized endorsement scores for every user.

Under this VIEW approach, a user X  X  personalized endorsement scores are always precomputed and are available in the user X  X  Per-sonalizedEndorsement table, so the user X  X  request for recommen-dation can be handled quickly. We simply need to look up the top-scored items from the user X  X  PersonalizedEndorsement table. However, maintaining the Personalized Endorsement tables will in-cur significant update cost, because whenever there is a new update on a blog with a large number of trusting users, all corresponding PersonalizedEndorsement tables should be updated.
In the previous section, we described the two baseline approaches for computing the personalized endorsement scores: OTF and VIEW. The main problem of OTF is that the query execution cost is too high due to a join between two large tables. The main problem of VIEW, on the other hand, is that the maintenance cost of the PersonalizedEndorsement tables is too high because one new en-dorsement may trigger updates on a large number of tables. In this section, we investigate how we can minimize both the table up-date cost and the query computation cost. In order to develop an approach with lower costs, we first reformulate the execution of query Q1 as a matrix multiplication problem.

It is easy to see that the trust scores in the Trust table can be viewed as an ( n  X  m ) matrix T , where each entry ( i, j ) the user u i  X  X  trust score on the blog b j . Similarly, the endorsement scores in the Endorsement table can be viewed as an ( m  X  l E , where each entry ( j, k ) represents the blogger b j  X  X  endorsement score for the item o k . Then from the definition of the personalized endorsement score R ( u i , o k ) in Equation 1, we see that can be computed from the matrix multiplication of T and E P
ROPOSITION 1 The personalized endorsement score of the item o for the user u i , R ( u i , o k ) , is the ( i, k ) entry of the matrix mul-tiplication of T and E . That is, where M ( i,k ) represents the ( i, k ) entry of the matrix
We use the following example to illustrate this matrix interpreta-tion of the problem.
 E
XAMPLE 1 Suppose that there are three users, four bloggers and three items. The user u i  X  X  trust score on the blogger b the trust matrix T in Figure 2. For example, the value of the (1,2) entry, 0.8, indicates that u 1 trusts b 2 with the score 0 . 8 blogger b j  X  X  endorsement for the item o k is given in the endorse-ment matrix E in the figure. For example, the value, 2, of the (4,2) entry, indicates that the blogger b 4 endorses the item o score 2.

Note that each row in the trust matrix T corresponds to the trust vector ~ T i of the user u i . Also each column in the endorsement ma-trix E corresponds to the endorsement vector ~ E k of the item Given that R ( u i , o k ) = ~ T i  X  ~ E k , we see that R ( u the ( i, k ) entry of the matrix multiplication T E . That is, comput-ing the personalized endorsement score R ( u i , o k ) is equivalent to performing the matrix multiplication of T and E . In Figure 3 we show the result of this multiplication. 2 Given this matrix formulation, OTF can be viewed as follows: We maintain separate T and E matrices as new endorsements are provided by the bloggers by updating the E matrix. Then, when the users request their personalized endorsement scores, we perform the multiplication of T and E on the fly . The cost for updating the E matrix, therefore, will be low, because a new endorsement from a blogger incurs an update to a single entry in the matrix E blogger b j provides the endorsement for o k , only the ( j, k ) of the E matrix will be updated. The computation cost of the per-sonalized endorsement scores, on the other hand, will be very high because of the high cost of the multiplication of two large matrices.
The other VIEW approach can also be seen as the following ma-trix operations: We maintain the single matrix T E as new endorse-ments are provided by the bloggers by updating the appropriate entries in T E . Since all personalized scores R ( u computed in the matrix T E and are readily available, the cost for answering a user X  X  request for R ( u i , o k ) will be low. On the other hand, the cost for maintaining the matrix T E will be high, because a single endorsement from a blogger may trigger multiple updates on a large number of entries in the T E matrix. For example, an en-dorsement from b j who have 1,000 trusting users will incur updates to 1,000 entries in the T E matrix.

The above two extreme approaches, either precomputing the en-tire matrix multiplication T E or performing it lazily at the user X  X  request suggests exploring a possible middle ground, where we per-form part of the multiplication proactively before the user X  X  request and finish the rest of the multiplication on the fly. In the next sec-tion, we will see that finding this middle ground is equivalent to finding a good low-rank decomposition of the matrix T .
To understand how matrix decomposition can be used to find the middle ground, we consider the trust matrix T in Figure 2 of Example 1. From the trust table, we observe that even though there are three users with different trust vectors, the user u 2 is simply a linear combination of ~ T 1 and ~ T 3 . That is, Now, given that R ( u 2 , o k ) = ~ T 2  X  ~ E k , we observe that can actually be computed from R ( u 1 , o k ) and R ( u 3 This observation hints a possible modification to the VIEW ap-proach: instead of maintaining one PersonalizedEndorsement table for every user, maintain the table only for the two  X  X epresentative users, X  u 1 and u 3 . The personalized endorsement score for then computed indirectly by combining the personalized scores for u 1 and u 3 . This modification to the VIEW approach has the fol-lowing two merits in terms of its update and the query computation cost: 1. Update cost : The update cost for the PersonalizedEndorse-2. Query computation cost : The computation cost for person-
Our modified approach to computing R ( u i , o k ) can be viewed as the following matrix decomposition of T into W and H : Here, each row of the second matrix H corresponds to the trust vector of the two  X  X epresentative X  users, u 1 and u 3 . Each row of the first matrix W then represents how each user u i  X  X  trust vec-tor can be obtained from the trust vectors of the two representative users. For example, the first row of W is 1 for the first entry, indi-cating that u 1  X  X  trust vector is identical to the trust vector of the first representative user. The second row has 0.25 and 1.2 as its entries, indicating that the trust vector of u 2 is the the linear combination of the trust vectors of the two representative users, weighted by 0.25 and 1.2, respectively.

Given this decomposition, we see that maintaining the Person-alizedEndorsement tables for the two representative users u 3 is equivalent to precomputing the multiplication of H and Note that each row of the HE matrix has the R ( u i , o k each of the two representative users. Then during the query time, we compute R ( u i , o k ) of u i by multiplying the precomputed with W as follows:
The main benefit of this approach comes from the fact that the number of rows in H is much smaller than the number of rows in so we need to maintain a much smaller number of PersonalizedEn-dorsement tables. More formally, we reduce both the update and the query computation cost for personalized endorsement scores by decomposing the ( n  X  m ) matrix T into ( n  X  r ) and ( r  X  m ) matrices W and H where r  X  n and by precomputing the multi-plication HE .

In general, this process of matrix decomposition have the fol-lowing intuitive interpretation: We first identify the representative groups of users who have similar trust vectors, so that we can pre-compute the personalized endorsement scores for each represen-tative user group. Then during the query time, we compute the personalized endorsement scores of each user by combining their scores on the representative user groups. In Figure 1(c) we graph i-cally illustrate this interpretation. Note that in the figure, note that we maintain one PersonalizedEndorsement table per user group not per every user. Since the number of user groups is much smaller than the number of users, this approach is likely to lead to signif-icant improvement in the update and the query computation cost compared to to VIEW and OTF.
Our previous discussion shows that we can reduce the costs of multiplication by finding a way to decompose ( n  X  m ) T into ( n  X  r ) matrix W and ( r  X  m ) matrix H where r  X  min { n, m } . Since r corresponds to the number of  X  X epresentative X  user groups for which we maintain separate PersonalizedEndorse-ment tables, we would like to find a decomposition W H such that r is minimal. Unfortunately, it is known that the minimum r for the decomposition W H is the rank of the matrix T . That is, for a matrix T whose rank is close to min { n, m } , it is not possible to obtain a decomposition W H with a low r value. Given that exact decomposition is almost impossible , we relax our goal to find the closest approximation of the trust matrix T with W H for a given value.
 D
ESIDERATA 1 Given the desired rank r of the matrix decompo-sition of T , find the W and H such that W H is closest to One well-known method for low-rank matrix approximation is Singular Value Decomposition (SVD). If we use SVD to decom-pose matrix T into two orthogonal matrices, the decomposition is guaranteed to be the best in terms of the Forbenius norm, at any given rank approximation. Unfortunately, the decomposed matri-ces W and H incur significant update and query execution cost due to the high density of the the matrices; almost all entries in the and H are non-zero, which can be interpreted that every blogger b is trusted by every representative user group and that every user u belongs to every representative user group to a certain extent. Therefore, an update from the blogger b j incurs updates to the Per-sonalizedEndorsement table of almost every user group. Also, the computation of R ( u i , o k ) for the user u i , requires us to combine the scores from almost all PersonalizedEndorsement tables.
This high update and query cost resulting from the SVD decom-position led us to introduce the second desiderata for the matrix decomposition: D
ESIDERATA 2 Given the desired rank r of the matrix decompo-sition of T , find the W and H , T  X  W H , such that matrices and H are as sparse as possible. 2 Intuitively, by making most of the entries of W and H zero, we are trying to identify the user groups such that that each blog trusted by only a small number of groups and each user u i to only a small number of groups.

We find that Non-negative Matrix Factorization (NMF) is one of the methods with both properties. In fact, NMF allows us to spec-ify the desired sparsity of the resulting matrix W and H for a given rank value r . Later in Section 3, we will provide the experimental comparison between SVD and NMF, in terms of their approxima-tion accuracy and the sparsity of the decomposed matrices. In the rest of this paper, we refer to our approach of using NMF for the matrix decomposition and the R ( u i , o k ) computation as the NMF method .
Once the PersonalizedEndorsement tables are precomputed for every user group using our NMF method, we can compute R ( u for u i by combining the scores from the tables u i belongs to. In most case, as the query Q1 suggests, users are interested in a top-K items with the highest endorsement scores, not every item endorsed by bloggers. This suggests another possibility for optimization un-der our NMF method. Instead of computing R ( u i , o k ) item o k , we compute R ( u i , o k ) only for the items that are likely to have high scores. More specifically, from each PersonalizedEn-dorsement table that u i belongs to, we obtain only those items with high R ( u i , o k ) scores in each table. We then compute the personal-ized R ( u i , o k ) values only among these items and return the top-K. This way, we can further reduce the execution cost of Q 1 we read only a few tuples from each PersonalizedEndorsement ta-ble, avoiding scanning most of the tuples in the tables.
In [1], Fagin et.al. proposed an algorithm, called Threshold Al-gorithm, suitable for this optimization. Through a careful analysis of the algorithm, the authors have shown that we can indeed com-pute the correct top-K items optimally, by looking at just the top few entries from each PersonalizedEndorsement table. More for-mally, they have shown that when the score of an item o i puted by by a monotone aggregate function t ( o i ) = t ( x where x 1 , . . . , x m are the basis scores from which the final score is computed, we only have to read the top items with the high-See [1] for more detailed description of the algorithm. In our later experiment section, we implement Threshold Algorithm for the top-K item computation and report the performance numbers with this optimization. Note that the optimization based on the Thresh-old Algorithm does not involve any approximation, because the re-turned top-K items are guaranteed to be the correct top-K item.
The NMF method brings advantages by approximating and de-composing a dense user-blog trust matrix into fewer number of user groups; hence, we do not need to update a large number of Person-alizedEndorsement tables or to aggregate a large number of tuples in the Endorsement table at query time. However, the user-blog trust matrix may not be dense over all users and blogs. In a real world dataset, some users may only trust a few blogs, while some blogs may only have a few followers. For this kind of users and blogs, the two baselines, OTF and VIEW, can already handle the personal aggregate query efficiently.

As shown in the literature [2] (and also in the experiments after-wards), the user-blog trust matrix (also referred as the subscription matrix) is usually skewed and shows a power-law like distribution. Figure 5 shows a a subscription matrix with rows and columns as the blogs and users respectively. When the rows and columns of the matrix are ordered by the number of non-zero entries, it may be divided roughly into three regions as illustrated in Figure 4. In the figure, the region marked as  X 1. OTF X  corresponds to the users that subscribe to just a small number of blogs. The other two regions,  X 2. VIEW X  and  X 3. NMF X , correspond to the users who subscribe to a large number of blogs, where  X 2. VIEW X  indicates the blogs with just a few subscribers and  X 3. NMF X  indicates the blogs with many subscribers.

Based on such division of users and blogs, we can apply dif-ferent methods to process aggregate queries. For the region  X 1. OTF X , OTF is efficient enough; since the users in the region sub-scribes to a small number of blogs, personalized endorsement is computed from just a small number of blogs. For the other two re-gions, the query computation cost can be high if OTF is used, since these users subscribe to a large number of blogs. Fortunately, for the region  X 2. VIEW X , we can use VIEW method to pre-compute the PersonalizedEndorsement table for each user whenever there is a new endorsement from each blog; the blogs in this region are subscribed by a small number of users, so an update from a blog in this region triggers updates to just a few PersonalizedEndorse-ment tables. The third region  X 3. NMF X , however, incurs too much cost if either OTF or VIEW method is used, so we apply NMF method to this region to reduce the query computation cost. Later in Section 3, we experimentally investigate how the relative size of the three regions impacts the query and update performance of the overall system.

Under such partitioning scheme, whenever new users or blogs are added, they can first be handled in region  X 1.OTF X  and  X 2.VIEW X  because they are assumed to have smaller number of subscriptions and subscribers respectively, for which the query cost and update cost are low. Either periodically or after the subscription data has changed from the previous version beyond a threshold, the trust ma-trix is repartitioned and the region  X 3.NMF X  is recomputed again to update these changes. In this paper, we assume the trust matrix is fairly stable and will leave the study of handling dynamic subscrip-tion matrix as future work. In this section, we evaluate the effectiveness of the proposed NMF method. In Section 3.1, we first describe the dataset used Figure 4: Three different regions of the user-blog trust matrix. Figure 5: Subscription matrix with rows and columns ordered by the number of subscribers and subscriptions respectively. for our experiments. Then in Section 3.2, we investigate how much difference personalization makes in the recommendation quality by comparing recommended items with or without personalization. In Sections 3.3 and 3.4, we evaluate the effectiveness of the NMF method by measuring the accuracy of the NMF method in approxi-mating the top-k popular items (Section 3.3) and by comparing the query and update cost of the NMF method to the other two base-line methods(Section 3.4). Finally in Section 3.5, we measure how different choices of parameters affect the performance of the NMF method. Trust matrix We obtained the users X  trust information on the blogs, by collecting a snapshot of the user-blog subscription data from Bloglines [3]. Bloglines is a Web-based online RSS reader, where users can specify the list of RSS feeds that they are inter-ested in, so that they can access new articles from the subscribed blogs at a single location.

This subscription dataset contained 91,366 users who subscribed subscribed to 30 distinct RSS feeds, leading to a total of 2.7 million user-blog subscription pairs. Figure 5 shows the collected subscrip-tion matrix, where the users and the feeds are sorted by their num-ber of subscriptions and subscribers, respectively. The subscriptio n pattern follows a power-law distribution as reported in similar pre-vious studies [2]. The matrix is indeed very sparse, with most of the subscription pairs located in the lower-right-hand corner (This may not be noticeable because of the printing issue). There were 24,340 users with more than 30 subscriptions and 10,152 blogs with more than 30 subscribers, which correspond to the box in the lower-right corner in Figure 5. This region consists of roughly 1 million sub-scription pairs inside. Since the Bloglines users do not indicate their level of interest over the feeds, the subscription matrix is a degenerate version of the user-blog trust matrix having values of either zero or one.
 Endorsement matrix In order to obtain the endorsement matrix for our experiments, we have collected all articles posted at the 487,694 RSS feeds between October 2006 and July 2007 and an-alyzed their contents. Again, we consider that when the item appears in an article posted by the blogger b j , the blogger  X  X n-dorses X  the item. Still, an important decision that we have to make is what constitutes an item. For our experiments, we explored two possibilities  X  the URLs appearing on blog articles and the key-words appearing on the blog articles  X  but due to space limit, we report our results obtained from keywords. This choice of item may be interpreted as identifying  X  X opular buzzwords X  within the subscribed blogs for each user. To assign the endorsement score for every keyword appearing in the blog articles, we extracted only the nouns from the articles using a basic NLP part-of-speech tag-ger and used the standard tf.idf score of those nouns, where is calculated from all the blog entries published on the same day. There are other methods like n-grams technique and KL-divergence with background corpus [4] to extract key phrases from blog arti-cles to improve the quality of recommendation, we apply the method for simplicity and focus our work on the query optimization in this paper.
To quantitatively show how much difference personalization makes in overall recommendation, we first present the top few recom-mended items when they are computed globally by aggregating the endorsements from all bloggers with equal weights and when they are computed individually for each user by weighting the endorse-ments with each user X  X  trust vector. Table 1 shows the list of top 10 recommended keywords among all RSS feeds and for three sample users in the week between 2007-01-07 and 2007-01-13. This is the week when Apple Inc. announced its iphone .

From this list we can see that personalizing the aggregation based on a user X  X  interest does make a big difference in terms of the rec-ommendation. Globally, the announcement of iphone by Apple was indeed very popular among bloggers and showed up as one of the top-10 recommended keywords. This globally popular event, how-ever, was essentially filtered out for the recommendations for the user 90550 and the user 91017, whose interests seem to be less technology oriented, but more towards media/entertainment (user 90550) and politics (user 91017). As another data point, we also show the same list computed in the week between 2007-04-01 and 2007-04-07. The global top keywords, again, pick up an important event close to the week ( Easter holiday ), while top keywords for individual users are less sensitive to the change of global trend and continue to be related to their personal interests.

In order to quantitatively measure the difference of the recom-mended keywords among the users, we compute the following met-rics. Let L G be the set of global top-20 keywords and L i of top-20 keywords of the user i . Then the average overlap between the global recommendation and the individual recommendations L | , where n is the number of users. When we measured these overlaps, they were 1.12 and 1.13, respectively, for the top-1000 users with the largest number of subscriptions, indicating that the recommended keyword lists shared only one keywords, on aver-
We show the shared-keyword count distributions in Figure 6(a) and 6(b). As we can see, even among users with large number of subscriptions (that are likely to have overlapping interest), their personalized answers differ significantly from the global answer and also among the users themselves. scriptions. Table 1: Global and individual list of top keywords during the week of 2007-01-07 to 2007-01-13 and 2007-04-01 to 2007-04-07. Figure 6: Distribution of the number of overlapping top-20 keywords among top 1000 users.
The NMF method attempts to compute the aggregation quickly at the cost of losing the accuracy of the aggregation. In this section, we investigate accuracy of the approximated result computed by the NMF method.

We first compare NMF with SVD on how close they can approx-imate the original matrix when both are approximated to the same ces as the output, when SVD approximation is done to rank-choose n  X  r and r  X  m to be the size of W and H from NMF, for a fair comparison with r rank SVD approximation. In Table 2, we report the accuracy of these two methods. The first column shows the rank of the approximated matrix. The second column shows the Frobenius norm of the difference between the original matrix and the SVD-approximated matrix (i.e., | T  X  U SV T the third column shows the Frobenius norm under the NMF approx-imation (i.e., | T  X  W H | ). From this result, we can see that SVD and NMF both results in roughly the same accuracy in terms of the Frobenius norm; NMF approximation is only 1% worse than that of SVD; however, NMF significantly outperforms SVD in terms of the sparsity (the percentage of non-zero entries) of the decom-posed matrices. From the third column of the table, we see that NMF gives an average sparsity of 23% in W and 13% in H , while nius norm for a given rank.
 Figure 7: Visual comparison of the accuracy of subscription matrix approximation. the sub-matrices decomposed from SVD contain almost 100% non-zero entries.

To make the accuracy of the approximation easier to see visually, we plot the density map of the subscription matrix (only the dense region) comparing the original subscription matrix, the SVD, and the NMF approximations in Figure 7. From the figure, we can see that both NMF and SVD leads to very close approximation of the original subscription matrix.
 Table 2: Comparison of the accuracy and sparsity between SVD and NMF at different rank approximation.

In addition to measuring the accuracy in terms of subscription matrix approximation, we investigate NMF X  X  accuracy in terms of the top-20 recommended items to the users. To quantify this, sup-pose A i and T i are the top-k recommended items computed with and without using the NMF approximation method for the user Then we measure the degree of overlap between the correct and overlap for the top of the top 1000 users (those with most num-ber of subscriptions) and took the average, we observed that 70% of recommended items were shared between the two lists. In Fig-ure 8, we show this overlap varies as we change k in the top-From the figure, we can see that the higher-ranked items, that are considered more important to the users, are more likely to be ap-proximated by the NMF method. For example, the top-ranked item was recommended by the NMF method in about 90% of the cases.
We now compare the update and query performance of NMF with two other baselines, OTF and VIEW. To compare their up-date performance, we assume that we monitor the blogs for one week from 2007-01-07 to 2007-01-13 and as new items appear in the blogs, we update the Endorsement table in OTF, the Person-alizedEndorsement table of users in VIEW, and the group tables in NMF. From this measurement, we see that the total number of updates for OTF, VIEW, and NMF are roughly 222K, 23.6M, and Table 3: Statistics of time taken by OTF and NMF (among the top 1000 users) 3.2M, respectively, for the region where NMF approximation is ap-plied. That is, while the update cost of NMF is higher than OTF, NMF still reduces the update cost by an order of magnitude com-pared to VIEW.

In Table 3, we report the query response time of OTF and NMF for top-1000 users (with a large number of subscriptions). We do not report the result from VIEW here, because answering a query under VIEW is a simple table lookup. The response time of OTF is measured by running the Q 1 in Section 2 on MySQL and that of NMF is measured by a python implementation of the NMF and Threshold Algorithm interfacing a MySQL 5.0.27 server (with 600MB main memory as index key cache) running on a AMD Dual Core Fedora machine with database files residing on a RAID disk. On average, the OTF method spends to answer a query 2.05s, while the NMF method spends 0.46s. From the table, we observe that the reduction in the query response time is even more signifi-cant when we compare the maximum response time; NMF reduces the maximum from 84.42s to 2.84s and allows that all users get results within a reasonable amount of time in an interactive setting.
In summary, we can see the NMF method is a middle ground between OTF and VIEW by trading more update cost for a faster query response time.
We have previously studied on an arbitrary choice (users with &gt; 30 subscriptions and feeds with &gt; 30 subscribers) of the dense subscription region to apply the NMF method. In this experiment, we empirically try different sizes of the NMF region and study its impact on the query cost, update cost, and approximation accuracy. Table 4 shows the size and sparsity of the NMF region under differ-ent choices of boundary. For each of these settings, we apply NMF with the same parameter r = 100 .

Figure 9 shows how the update cost (as the number of SQL up-date statements used to process one week of data) changes with the size of the NMF region. We observe that changing the NMF region size along the users and feeds dimension has opposite effect on the update cost. The blue solid line shows that when fewer users are included in the NMF region (i.e. more users are handled by the OTF method), the lower is the update cost because users handled by the OTF method do not require their PersonalizedEndorsement table to be maintained; hence, results in lowering the update cost. The red dashed line shows that when fewer feeds are included in the NMF region (i.e. more feeds are handled by the VIEW method),
Table 4: Characteristics of different sizes of NMF region. the higher is the update cost because less feeds are benefited from the NMF method to reduce number of updates. This relationship suggests us to include less users but more feeds to be approximated by the NMF if update cost minimization is the desirable objective. Figure 9: The impact of NMF region size on update cost.

Next, we examine the impact of the size of NMF region on the query efficiency and accuracy of approximation. Since our evalu-ation focus on the set of users with large number of subscriptions, changing the NMF boundary along the user dimension will not af-fect the group of users evaluated, so, we investigate only the change along the feed dimension that determines the proportion of feeds to be handled by VIEW and NMF method.

Figure 10 shows the average approximation accuracy (with  X  1 standard deviation) of the top 1000 users under different sizes of the NMF region. We observe that when fewer feeds are being approx-imated by the NMF method, the higher is the approximation ac-curacy because more feeds are now handled by the VIEW method without any approximation. Other than this, we also observe that the query response time remains similar regardless of the NMF re-gion size. This can be explained by the fact that the Threshold Algorithm is combining from similar number of ordered lists be-cause sparsity of the factorized matrices are similar when we use the same r parameter for different NMF region size.

Based on the above observation, when we want to best leverage the advantage brought by the NMF method, we should aim at in-cluding fewer users but more feeds in the NMF region, so that we can reduces the update cost, while still giving good approximation accuracy.
Our work span over several areas such as Web data mining, per-sonalization and query optimization, the related literature roughly fall into the three categories: 1) Web mining for trends, 2) Web personalization and collaborative filtering, and 3) OLAP and query optimization of aggregates and ranking operations.

Blogging activates have brought to the Web a huge amount of user-generated content. There have been research efforts [4, 5, 6, Figure 10: The impact of different size of NMF region on ap-proximation accuracy. 7] on mining Web data for trends and information retrieval specific on blog data. For example, Gruhl et.al. [5] describes several tech-niques to find representative keywords and phrases (memes) that corresponds to discussion topics in the blogosphere and presents methods to trace the spreading of information among blogs. Wang et.al [7] has proposed method to correlate similar trends from mul-tiple sources. The majority of the prior work on Web mining focus on finding a set of global trend by aggregating large number of sources; while our work further extend this idea to provide more personalization to improve user experience.

There are significant amount of work on Web personalization and collaborative ranging from fast learning of accurate user pro-files [8], personalized recommendation through collaborative filter-ing [9], and optimization of complex systems [10, 11]. Recently, Das et.al. [11] describe how Google News provides personalized news feed items. It addresses on the implementation of PLSI and EM algorithm using the map-reduce programming paradigm and user profile management within the Google cluster. We see there will be an interesting investigation to apply our method in a dis-tributed environment to further improve the efficiency.

Optimization of aggregation and ranking query have long been studied in the database community, especially in the OLAP con-text. Recently, Li et.al. [12] have introduced an extension to the SQL semantics to support custom clustering method using "group by" and to support general ranking function using "order by". Qu and Labrinids [13] describes how to optimize the scheduling pro-cess of a streaming database engine to accommodate different user preferences on freshness and accuracy of results. While some othe r previous work [14, 15, 16] on efficient aggregate query processin g may differ from application domains and assumptions, we share the same line of thoughts to improve query processing through approx-imation and reusing partially computed results among queries.
Our work benefits from two existing prior arts: 1)the core tech-nique, Non-negative Matrix Factorization, had been applied by ma-chine learning researchers in various application domains such as pattern recognition [17], computer vision [18] and clustering [19, 20]. 2) The Threshold Algorithm is an efficient method proposed by Fagin et.al. [1] to rank objects by merging from multiple sorted attribute lists.
In this paper, we formalized the problem of personalized aggre-gation. We presented two baseline approaches and discussed their limitations. We then present a matrix representation of the prob-lem and proposed a method that uses Non-negative Matrix Factor-ization and Threshold Algorithm to speed up the query process-ing and reduce the update cost. We showed, using experiments on real-life blog dataset, the significance of personalized aggregation and effectiveness of our proposed solution. In particular, the NMF method is able to cut the query response time by 75% at the expense of paying reasonable amount of update cost, while maintaining an approximation accuracy of roughly 70%. Some interesting future directions include the investigating the optimal choice of the num-ber of users and blogs to be included in the NMF approximation, the effect of matrix sparsity on query efficiency, and the application of NMF method in a distributed computing environment.
 This work is partially supported by NSF grants, IIS-0534784 and IIS-0347993. Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding institutions.
