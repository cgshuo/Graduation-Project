 Structural similarity search among small molecules is a sta n-dard tool used in molecular classification and in-silico dru g discovery. The effectiveness of this general approach de-pends on how well the following problems are addressed. The notion of similarity should be chosen for providing the highest level of discrimination of compounds with respect t o the bioactivity of interest. The data structure for perform -ing search should be very efficient as the molecular databases of interest include several millions of compounds. In this paper we summarize the recent applications of k -nearest-neighbor search method for small molecule classifi -cation. The k -nn classification of small molecules is based on selecting the most relevant set of chemical descriptors whi ch are then compared under standard Minkowski distance L p . Here we describe how to computationally design the opti-mal weighted Minkowski distance wL p for maximizing the discrimination between active and inactive compounds wrt bioactivities of interest. k -nn classification requires fast sim-ilarity search for predicting bioactivity of a new molecule . We then focus on construction of pruning based k -nn search data structures for any wL p distance that minimizes simi-larity search time.
 The accuracy achieved by k -nn classifier is better than the alternative LDA and MLR approaches and is comparable to the ANN methods. In terms of running time, k -nn classi-fier is considerably faster than the ANN approach especially when large data sets are used. Furthermore, k -nn classifier is capable of quantification of the level of bioactivity rath er than returning a binary decision and can bring more insight to the nature of the activity via eliminating unrelated de-scriptors of the compounds with respect to the activity in question. Small chemical molecules (with molecular weights  X  500) are very important to the exploration of molecular and cel-lular functions. They also play key roles in treating diseas es: almost all medicines available today are small molecules. One of the fundamental research challenges we are facing today is the identification of small molecules that play an  X 
Corresponding author active role in regulation of a given biological process or di s-ease state.
 Recent developments in the chemistry have given researcher s the ability to efficiently synthesize and screen large num-ber of related small molecules, a capability previously ava il-able only to pharmaceutical companies. As a result of these developments, public small chemical compound databases started to emerge with exponentially increasing number of compounds. The Molecular Libraries Roadmap of NIH, PubChem, is one of the largest public databases which aims to offer access to the large number of chemical compounds and their functions. Currently PubChem contains 100,000 compounds with known bioactivities and a total of 10 mil-lion unique small chemical compounds. This initiative is expected to lead new data mining and machine learning techniques to reveal the relationship between the structur al information of chemical compounds and their bioactivity. I t is anticipated that these projects will also facilitate the de-velopment of new drugs by providing early stage chemical compounds to validate new drug targets which could be then move into drug-development pipeline.
 Structural similarity search among small molecules is one of the standard tools used in conventional in silico drug discovery as structural similarity usually implies simila r-ity in physicochemical properties and/or biological activ -ities [14]. Thus, it is common to query small molecules databases with a probe compound possessing desirable bi-ological activity (bioactivity) to discover chemically similar database entries. It is also common to perform classificatio n of a compound with an unknown bioactivity level through a similarity search among compounds whose bioactivity level s are known. Clustering small molecules with similar bioac-tivities can lead to better understanding of the underlying chemical and structural properties related to the bioactiv -ity. It is known that certain structural components of small molecules can target other components via establishing sta -ble chemical bonds. Identification of common characteristi cs of a given biological function significantly reduces the num -ber of compounds need to be tested for a particular drug target.
 This important ligand-based drug discovery methodology and classification approach are associated with the follow-ing two fundamental computational problems. (1) The no-tion of similarity used in search determines the molecules that are extracted from the database. A notion of similarity which has the highest level of bioactivity discrimination i s very desirable and needs to be determined computationally. (2) It is desirable to have efficient algorithms for structura l and/or chemical similarity search as the molecular databas es of interest include several millions of compounds and lin-ear/brute force search may take significant amount of time (several days in certain large private databases). In this summary, we describe frequently used computational methods for addressing these two fundamental research prob -lems. The first part of the paper focuses on the common similarity/distance measures used in small chemical com-pound databases and their application for classification of compounds according to a given bioactivity. Typically smal l chemical compounds are represented as a vector of descrip-tors, that are extracted from the 2D/3D structure of chem-ical compounds. The most common measures of similar-ity/distance amongst sets of molecular descriptors are Tan-imoto coefficient , standard L 1 and L 2 distances. These sim-ilarity/distance measures are global and independent from the bioactivity of the compounds. In order to capture the similarity between compounds with better accuracy with re-spect to a particular bioactivity more sophisticated measu res must be used.
 Similarity/distance measure itself is not enough for class i-fication of new compounds; it must be trained in order to capture the bioactivity of interest. The most commonly used classification techniques of chemical compounds include Mu l-tiple Linear Regression (MLR) [6], Linear Discriminant Ana l-ysis (LDA) [12] and machine-learning techniques such as Support Vector Machines (SVM) [19] and Artificial Neu-ral Networks(ANN) [21]. In this summary we focus on the k -nearest neighbor ( k -nn) classification, which deduces the level of the bioactivity of a query molecule based on the num-ber (and the bioactivity levels) of active elements among it s k nearest neighbors with respect to a distance measure of choice. In order to determine the best similarity/distance measure, we introduce use of the (more general) weighted Minkowski distance of order 1, namely wL 1 . For each bioac-tivity of interest, we determine real valued weights w i of the wL 1 distance so as to maximize the discrimination between active and inactive compounds in a training set. We com-pute the optimal values for weights w i via a linear optimiza-tion procedure [10].
 The second part of our summary focuses on the second problem, efficient data structures for fast chemical similar ity search which is essential for k -nn classification. Space Cov-ering Vantage Point (SCVP) tree is one of the well known data structures for performing fast nearest neighbor searc h in dimensional metric spaces via triangle inequality. In th e SCVP tree construction, the vantage points in each level are chosen randomly until all search space is covered [15]. Clearly, it is desirable to minimize the number of vantage points that cover the search space as a better space utiliza-tion can be achieved, implying that more levels of the tree can be fitted in the available memory. In this summary we show how to approximate the minimum number of vantage points and thus obtain the optimum allocation through a simple polynomial time algorithm. The resulting data struc -ture, which we call the deterministic multiple vantage poin t tree (DMVP tree) [10], when built in full, is guaranteed to have O (log  X  ) levels, where  X  is the size of the data set. If the maximum number of children of an internal node at level i is c i , the query time guaranteed by our data structure is O ( P log  X  i =1 c i ). Because c i is typically a small constant (ap-proximately minimized for each level), the query time is onl y O (log  X  ), a significant improvement over linear/brute force search.
 Due to redundant representation of data items, the mem-ory usage of the DMVP tree can be super-polynomial. In case the DMVP tree requires more memory than available, lower levels of the DMVP trees could be cut out. In this case, the pruning in the leaf nodes can be achieved by linear search. We also show how to obtain the optimum cut so as to minimize the expected query performance.
 Our data structure is not only interesting for classificatio n purposes; similarity search among small molecules under various notions of similarity is of independent interest. T o the best of our knowledge, this is the first application of an efficient similarity search data structure to small molecule data collections. Given a notion of similarity among data points, it is usu-ally possible to obtain a corresponding distance measure; searching for structurally most similar molecules to a quer y molecule in this context corresponds to searching for molec ules with the smallest distance to the query molecule. The key premise of this approach is that the notion of a distance is mathematically well defined and algorithms for handling dis -tance based classification, clustering and search are bette r understood. Under a given distance measure, the search for the most similar molecule to a query compound becomes the Nearest Neighbor Search (NN) problem. Thus, the above two problems in structural similarity search, i.e. classifi ca-tion and querying, can be mapped to corresponding prob-lems in nearest neighbor search.
 There are various ways to define the descriptors/parameters for the chemical structures stored in electronic collectio ns conventionally used in the modern computer-aided drug dis-covery [2; 1].
 Such parameters either (1) merely reflect the structural or-ganization of molecules in qualitative manner, such as thos e used in the popular structural fingerprints (employed in NCBI X  X  PubChem database), e.g. the existence of a doubly bonded Carbon pair, a three membered ring, an aromatic atom etc. [13] or (2) reflect various local and global physical-chemic al molecular features (chemical descriptors) which are quant i-tative, such as atomic weight, aromaticity, hydrophobicit y, the number of specific atoms, charge, density, etc. These de-scriptors serve as independent variables for modern QSAR (Quantitative Structure-Activity Relationship) tools in clud-ing the structural similarity search engines in chemical co m-pound databases.
 Given an adequate set of descriptors, it is desirable to have a measure of similarity or alternatively a distance measure under which functionally related molecules have a high leve l of similarity or small distance, and non-related compounds have a low level of similarity or large distance. The most common measure of similarity amongst sets of molecular descriptors is the so called Tanimoto coefficient [17]. Given two descriptor sets (which can be organized in arrays) X and Y , the Tanimoto coefficient is defined to be the ratio of the number of descriptors that are identical in X and Y and the total number of descriptors available for X and Y . The Tanimoto coefficient is in the range [0 , 1]; a value close to 1 implies similarity and a value close to 0 implies a dissimilarity among the two descriptor sets compared. Often a collection of descriptors are represented as a bit-vector (e.g. structural fingerprints) where each one of the n possible descriptors is assigned a dimension , i.e. natural number between 1 and n (this is the representation used by PubChem and other databases). Let B ( x ) represent the bit-vector corresponding to a molecule x and let B ( x )[ i ] rep-resent its i th dimension. Given two compounds x and y , the Tanimoto coefficient T ( x, y ) is then defined as T ( x, y ) = though the Tanimoto coefficient provides a measure of sim-ilarity , it is possible to define a Tanimoto distance measure as D T ( x, y ) = 1  X  T ( x, y ).
 The Tanimoto coefficient is very popular mostly due to its simplicity. For real valued descriptor arrays (where each d i-mension has a real value) it is also quite common to use the Minkowski distance of order p , denoted L p for measur-ing their similarity. Given two real valued n dimensional descriptor arrays X and Y , their Minkowski distance of or-der p , namely L p , is defined as L p ( X, Y ) = ( P n i =1 Y [ i ] | p ) 1 /p . When comparing two structural fingerprints B ( x ) and B ( y ), the Minkowski distance of order 1 is equivalent to the well known Hamming distance (see for example [3]): H ( B ( x ) , B ( y )) = P n i =1 | B ( x )[ i ]  X  B ( y )[ i ] | . In order to capture the similarity between compounds with respect to a particular bioactivity, it is possible to assig n a relative importance to each structural descriptor in the fo rm of a weight w i  X  [0 , 1]. The resulting weighted Minkowski distance of order 1 can then be defined for two descriptor arrays X and Y as wL 1 ( X, Y ) = P n i =1 w i  X | X [ i ]  X  Y [ i ] | . 1 The descriptor arrays described above can be used for clas-sification of compounds according to a given bioactivity. One of the most popular classification techniques is the MLR (Multiple Linear Regression) [6] method which quantifies the activity level of a descriptor array X as: Activity ( X ) = c + P n i =1  X  i  X  X [ i ] where c is a constant. If Activity ( X )  X  t for a (user specified) threshold value t then it is likely that the molecule is active with respect to the bioactivity of int er-est. Notice that the MLR classifier is described by a planar separator in the multi-dimensional descriptor array space ; those points on one side of the separator are classified as active and those on the other side are classified as inactive. The most widely used optimization criteria for determin-ing the coefficients (which we used in our experiments) is the partial least squares criteria [7], which suggests to mi n-imize the sum of the squares of differences between actual and predicted activity levels of the compounds in a training set. The separator plane which satisfies this criteria is NP-hard to compute deterministically but can be approximated through genetic algorithms, local search heuristics, etc. Another popular statistical classification method is Linea r rection show how to assign binary values to weights w i i.e. how to choose the specific descriptors that are most relevant for the application of interest (e.g. [20; 9]). As will becom e clear later in the paper, we show how to compute optimal real valued weights so as to improve the predictive power of our classifier.
 Discriminant Analysis(LDA) [12]. Given a set of descriptor arrays, LDA computes a linear projection of the descriptor array space into a Euclidean space with 2 or 3 dimensions (i.e. each descriptor array is mapped to a point in the 2/3-D Euclidean space). The projection aims to maximize the ra-tio of between-class variance and within-class variance. T he projection of descriptor arrays to points in the Euclidean space is followed by the computation of a line/plane which best separates the active and inactive compounds, i.e. max-imizes the accuracy of the classifier. For a given query com-pound with unknown activity, its class is then simply de-termined by checking to which subspace its projection falls into; clearly this can be performed very fast.
 It is also possible to perform compound classification via we ll known machine-learning techniques such as SVM (Support Vectors Machines) [19] and, more commonly, ANN (Artifi-cial Neural Networks)[21].
 Although k -nn classification is a conceptually simple ap-proach and is applied to solve several chemistry and biology problems, it was not considered for small molecule classi-fication until recently [20; 9]. For each predefined number of variables, it seeks to optimize (i) the number of nearest neighbor k used to estimate the activity of each compound (ideal case is where k =1) (ii) selection of variables from the original set of all descriptors. The compounds are then com-pared under the standard (unweighted) L 1 or L 2 distance. Given a chemical compound s , its descriptor array S is de-fined to be an n dimensional vector in which each dimension i , denoted by S [ i ], is a real value corresponding to the de-scriptor associated with dimension i . For a given bioactivity, it is of significant interest to come up with a distance mea-sure D ( S, R ) between pairs of descriptor arrays S and R that correspond to the similarity in the bioactivity levels of th e corresponding compounds s and r : if the bioactivity levels are similar, the distance must be small and vice versa. Such a distance measure could be very useful in the classification of new chemical compounds in terms of the bioactivity of in-terest: the bioactivity level of the new compound is likely t o be identical to the bioactivity level of its closest neighbo rs. Metric distances are particular interesting candidates fo r fast similarity search, due to the availability of efficient d ata structures. A distance measure D forms a metric if the fol-lowing conditions are satisfied. (i) D ( S, S ) = 0 for all S and D ( S, R )  X  0 for all S and R (non-negativity). (ii) D ( S, R ) = D ( R, S ) (symmetry). (iii) D ( S, R )  X  D ( S, Q ) + D ( Q, R ) (triangle inequality). Metric distance of interest includ e the Hamming distance, Euclidean distance and the Tanimoto distance.
 The commonly used QSAR approach estimates the level of bioactivity of a compound via a linear combination of its descriptors. In distance based compound classification, it is natural to consider a distance between two descriptor array s which is a linear combination of the differences in each one of the dimensions.
 More specifically one can define D ( S, R ) = P n i =1 w i R [ i ] | where w i , the weight of the dimension i is a real value in the range [0 , 1]. It is easy to show that this distance, which is usually called the weighted Minkowski distance of order 1 forms a metric.
 In this paper we focus on classification of biomolecules ac-cording to binary bioactivities. The biomolecular data set s available usually do not specify the level of bioactivity of interest but rather provide whether a compound is active or inactive. Thus we only perform a binary classification of compounds for each bioactivity, although our methods are general to provide a real valued level of bioactivity. Our classification method for a given bioactivity first com-putes a distance measure for a training data set which sepa-rates the subset of active compounds from those that are inactive. Given a training set of descriptor arrays T = { T 1 , T 2 , ..., T  X  } (each of which belonging to a compound) we determine the distance measure D , more specifically com-pute the associated weights w i , through a combinatorial op-timization approach.
 Given the training set T , let T A = { T A 1 , T A 2 , ..., T its subset of active compounds and T I = { T I 1 , T I 2 , ..., T denote its subset of inactive compounds. Clearly T = T I  X  T We obtain a linear program for determining each w i as fol-lows. The objective function of the linear program which is to be minimized is f ( T ) = ( subject to the following conditions where C is a user defined constant.
 The objective function f ( T ) has three components: Com-ponent (1) is the average distance among active compounds and component (2) is the average distance among the inac-tive compounds; their sum provides the within-class average distance. Component (3), on the other hand, is the average distance between an active compound and an inactive one; thus it stands for the between-class average distance. As a result our linear programming formulation aims to max-imize the difference between the average between-class dis-tance and the average within-class distance. The distance measure obtained will separate the typical active compound from the typical inactive compound, while clustering all ac-tive compounds and all inactive compounds as much as pos-sible.
 There are three types of constraints on the weights w i in our linear programming formulation. Constraint (4) ensure s that the average distance among active compounds is no more than the average distance between active and inactive A distance measure defined as above can be used for the clas-sification of compounds with unknown levels of bioactivity as the bioactivity level of a compound is likely to be similar to the bioactivity levels of compounds within its close prox -imity. Our k -nn classifier estimates the (binary) bioactivity of a given compound by (1) either taking the majority of the bioactivities of its k -nearest compounds w.r.t. the dis-tance measure or by (2) checking whether sum of the binary bioactivity levels of the k -nearest neighbors normalized by their distances to the compound is above a threshold value. Under each approach, it is possible to select the value of k which maximizes the accuracy of the estimator, i.e. the ratio of the sum of true positives and true negatives to the size of the training data set.
 Once the method of classification is determined, it is desir-able to construct an efficient data structure for performing k -nn search. In the remainder of the paper we focus on constructing an efficient k -nn search data structure for the metric distance we developed and provide some experimen-tal results.
 Typical similarity search methods for large collections of data elements usually perform iterative partitioning of th e data set into smaller subsets so as to perform efficient query-ing by pruning -which is achieved at each iteration by check-ing out into which partition the query falls[16; 18]. The pruning strategy can be made particularly effective on data collections where similarity is measured with respect to a metric distance. The partitions in such a metric space are usually achieved with respect to simply defined planar cuts; given a query element, it is quite simple to check into which side of the planar cut it falls.
 Given a set of data elements X = { X 1 , . . . , X  X  } in a metric space with distance D , similarity search for a query element Y can be posed in two flavors. (1) Range query: retrieve all items whose distance to Y is at most some user defined R . tive compounds such that the distance between a given ac-tive compound T A h and any other active compound is no more than the distance between T A h and any inactive com-pound. Such a set of constraints can, in principle, can sep-arate active and inactive compounds into tighter clusters. Unfortunately, the number such constraints, m 2  X  (  X   X  m ), turns out to be impractical, even for the most advanced lin-ear program solvers. is usually no more than a few, thus it is desirable to sim-plify the distance measure by limiting the number of non-zero weights. The final constraint aims to achieve this by imposing an upper bound on the sum of the weights. Al-though this constraint does not guarantee to upper bound the number of non-zero weights, in practice, the number of non-zero weights obtained is no more than 2 C .
 (2) k -nn query: retrieve the k  X  1 items whose distances to Y are as small as possible.
 Efficient data structures for performing nearest neighbor search in high dimensional metric spaces usually exploit th e triangle property satisfied by the metric distance measure. One particularly efficient similarity search tool for perfor m-ing range queries is the Vantage Point (VP) trees [16; 18]. In a VP tree, efficient similarity search in a large data set is achieved through iterative pruning. Traditionally, a vant age point tree is defined as a binary tree that recursively parti-tions a data set into two equal size subsets according to a randomly selected vantage point X v as follows. Let M is the median distance among the distances of the data elements to X v . The inner partition consists of the elements Y such that D ( X v , Y ) &lt; M and the outer partition consists of the elements Z such that D ( X v , Z )  X  M . The two subsets are further partitioned via the iterative application of the ab ove procedure until each subset includes a single data element. For a given query element Y , the set of data elements X i for which D ( Y, X i )  X  R for the search radius R can be computed as follows. Let X v be the vantage point cho-sen for the entire data set and let M be the median dis-tance among the distances of the data elements to X v . If D ( X v , Y ) + R  X  M then recursively search the outer par-tition . If D ( X v , Y )  X  R &lt; M then recursively search the inner partition . If both conditions are satisfied then both partitions must be searched implying that no pruning has been achieved. The correctness of the search routine follow s from the triangle inequality.
 A natural extension to the traditional vantage point trees i s what we call the Space Covering VP trees (SCVP trees) first described by Sahinalp et al [15]. At each level of the SCVP trees, multiple vantage points are chosen so as to increase the chance of inclusion of the query region in one of the inner partition of the vantage points. This can be achieved by selecting vantage points in a way that the union of the inner partitions of these vantage points cover the entire da ta set. In other words, each data element is included in at least one of the inner partitions of a vantage point. Thus a SCVP tree has multiple branches at each internal node, each representing a vantage point and its inner partition. I f a query element is not close to any of the vantage points at a given level, it is deduced that there are no similar items to i t in the data set. The original SCVP trees chose the vantage points at each level randomly. Although this approach can perform quite well for certain data collections, it can also result in poor space utilization. The SCVP trees introduce some redundancy in the representation of the data elements: clearly each data element may be included in more than one inner partition and thus need to be represented in more than one subtree. Thus the memory requirements of the SCVP tree can be fairly large.
 Clearly it is desirable to cover the entire data collection by the fewest number of (inner partitions of) vantage points. However, the problem of minimizing the number of vantage points for this purpose turns out to be an NP-hard prob-lem under all distance measures of interest (i.e. weighted Minkowski distance of any order p , wL p )[10]. Neverthe-less it is possible to approximate the minimum number of vantage points in any metric space through a simple poly-nomial time algorithm as we show later. As a result we obtain a data structure that deterministically picks the va n-tage points (whose inner partitions cover the entire data se t) which results in almost optimal redundancy; we call this data structure Deterministic Multiple Vantage Point tree (DMVP tree) [10].
 The variant of the optimal vantage point selection problem (OVPS) for which we establish NP-hardness assumes a fixed radius r for each neighborhood around a vantage point. One can think of two natural variants of the OVPS problem: (1) each neighborhood includes a fixed number of points (e.g.  X / 2 points as per the original VP Tree construction), (2) each neighborhood has at least  X /k and at most  X /k  X  points for some k  X  k  X  . It is not difficult to show that these variants are NP-hard as well.
 In the remainder of the paper we focus on variant (2) of the OVPS problem and describe a polynomial time O (log  X  ) ap-proximation algorithm for solving it. Such a solution will also imply an O (log  X  ) approximation algorithm for vari-ant (1) by setting k = k  X  . The approximation algorithm is achieved by reducing the OVPS problem to the weighted set cover problem as follows.
 Consider each point X i in S . We construct the following  X  i consists of X i and its nearest neighbor. In general, X consists of X i and its j  X  1 nearest neighbors. Let the cost of X j i be j .
 Now given sets X j i , for all 1  X  i  X   X  and k  X  j  X  k  X  , each with cost j , if we can compute the minimum cost collection of sets such that each X h  X  S is in at least one such set, we would get a solution to the variant (2) of the OVPS problem. This problem is equivalent to the weighted set cover problem for which a simple greedy algorithm provides an O (log  X  ) approximation (e.g. [5]). The greedy algorithm works iteratively: each iteration simply picks a set where the cost-per-uncovered-element is minimum possible. The algorithm terminates when all elements are covered. Although the deterministic multiple vantage point tree im-proves the memory usage of the randomized space covering vantage point tree, it is still possible that the tree may not fit in the main memory. If this is indeed the case, we try to place a connected subtree (which includes the root) to the memory. The search again is performed starting with the root. When an internal node whose children are not rep-resented in the memory is reached, the search is done in a brute force manner on the set of points represented by that node.
 Clearly it is of interest to obtain the best subtree for op-timizing the query performance of the data structure. For that we use the following 0  X  1 programming formulation [10].
 Given a Multiple Vantage Point tree T and a node i , let S be the number of points in the neighborhood represented by i . During a search, when a node j is reached, its children i, i + 1 , . . . are considered for further search in linear order; i.e. we first check whether the query fits in the neighborhood of i , then we check i + 1 and so on until a suitable vantage point i + h is found. Let S  X  i + h be the number of points in the neighborhood represented by node i + h which are not in the neighborhoods represented by i, i + 1 , . . . , i + h  X  1. Our 0  X  1 programming formulation sets the probability that node i + h is reached during a search to S  X  i + h / X  . If the children of the node i + h are not placed in the memory, i.e. if node i + h is on the cut-set , the time needed for performing a search on the neighborhood represented by this node is S i + h . Thus the expected contribution of node i + h to the Let b i be a binary variable, which takes the value 1 if vertex i is in the cut-set and is 0 otherwise. Our goal is to min-imize the expected running time of the brute-force search performed for each query; i.e. our objective function is f ( T ) = P  X  i b i S i S  X  i subject to the following constraints. For any pair of consecutive sibling nodes i and i +1, we must We should not exceed the memory M dedicated to the cut-set; thus P  X  i b i S i  X  M.
 Finally, at least one node in every path from the root to a leaf in T must include one vertex in the cut-set. Thus for any such path P we have P i  X  P b i = 1 .
 A 0  X  1 assignment to b i  X  X  that minimize the objective func-tion will minimize the expected query time while fitting the data structure in the main memory. In this section we aim to provide some insight into the com-parative performance of our k -nn classifier, both in terms of accuracy and efficiency. We applied our classifier to five types of bioactivities [11]: (i) being antibiotic, (ii) bei ng a bacterial metabolite, (iii) being a human metabolite, (iv) being a drug, and (v) being drug-like.
 The first data set we used is the complete small molecule col-lection from [4], which includes 520 antibiotics, 562 bacte rial metabolites, 958 drugs, 1202 drug-like compounds, and an additional 1104 human metabolites. The total number of the compounds in the data set is 4346. Each compound in the data set is represented with a descriptor array of 62 dimensions, which is a combination of 30 inductive QSAR descriptors [4] and 32 physicochemical properties such as molecular weight, number of specific atoms (O, N, S), acid-ity, density, etc. This data set was used for testing the classification accuracy of k -nn approach. A second data set which enriches the first data set by the addition of 20000 additional drug like compounds was later used for testing performance of DMVP tree.
 For each bioactivity, a wL 1 distance is determined to estab-lish a model for compound classification w.r.t. this bioac-tivity using our k -nn method. Note that the descriptors of each compound are normalized according to the observed maximum and minimum values in the data set in order to remove the bias to parameters with larger values. The comparative results of the four classification methods, namely k -nn, LDA,MLR and ANN are provided in Table 1. For each bioactivity, we provide the sensitivity, specifici ty and accuracy obtained by each classifier. We demonstrate the performance of our k -nn classifier only for k = 1; i.e. given a query compound, our classifier returns the bioac-tivity of its nearest neighbor in the training data set. It is possible to set k &gt; 1, however it requires determining the best k value, as well as the best method for assigning the bioactivity of the query compound such as majority rule or distance weighted majority rule. In order to keep our clas-sifier simple, we set k = 1.
 We constructed the wL 1 measure for three different values of C -the upper bound on the sum of weights, i.e., P n i =1 C . Setting C =  X  removes the restriction on the sum of weights and thus computes the wL 1 distance that achieves the best classification. We also set C to 3 and 10 to restrict the number of non-zero weights, with the aim of focusing only on the C most relevant descriptors to the bioactivity of interest. As the resulting non-zero weights turned out to be equal to or very close to 1, these two classifiers are quite similar to those described in recent papers (e.g. [20; 9]) that focus on determining the most relevant descriptors for modeling a bioactivity of interest.
 We used MOE(Molecular Operating Environment) PLS mod-ule for MLR classification and SNNS (Stuttgart Neural Net-work Simulator) with default parameters (52 nodes and 420 connection network) for ANN classification. LDA classifica-tion is performed through the use of standard C libraries for matrix operations.
 For each bioactivity, a training data set comprising of 70 percent of both the active and the inactive compounds are formed via random selection. The remaining compounds are used as the test data set . Each training data set is used for building the four classifiers corresponding to the relat ed bioactivity and the test data is used for the evaluating thei r performance.
 For each bioactivity/classifier pair we report the followin g test results: The number of true positives (T P), the number of true negatives (T N), the number of false positives (F P), the number of false negatives (F N), sensitivity (T P/(T P+F N)), specificity (T N/(T N+F P)), accuracy ((T N+T P)/(T P+T N+F P+F N)), positive predictive value (T P/(T P+F P)), negative predictive value (T N/(T N+F N)).
 Our similarity search data structure for computing the near -est neighbor of the query compound is quite efficient, espe-cially when compared to brute force search. We tested our data structure under the wL 1 distance computed for each of the five bioactivities, on both of the data sets. The cru-cial parameter that determines the performance of our data structure is the pruning it achieves for any given query com-pound. Thus we determined the percentage of compounds pruned in the second training data set (the first training data set enriched with 20000 drug like compounds), aver-aged over all compounds in the test data set. On a 32GB Sun Fire V40Z server (with 2.4 Ghz AMD 64bit Opteron processor) the respective pruning ratios are as follows. We achieved (i) 84.4% pruning for being antibiotic, (ii) 84.5% pruning for being bacterial metabolite, (iii) 86.1% prunin g for being human metabolite, (iv) 81.7% pruning for being drug, and (v) 81% pruning for being drug-like. This is sig-nificant improvement over brute force search.
 As a result our k -nn classifier turns out to be very fast. On the first data set, the running time of our k -nn classifier aver-aged over all 4346 compounds (training+test data sets) and all five bioactivities is 0.3 milliseconds on the above serve r. In contrast the ANN classifier requires 39.7 milliseconds on the same data set. On the second data set (which simply has additional 20000 compounds in the data structure) the running time of our k -nn classifier increases only to 1.3 mil-liseconds (again averaged over the 4346 compounds from the first data set and five bioactivities), still 30 times better t han the ANN trained over a much smaller set.
 Model T P T N F P F N SPEC SENS ACCUR PPV NPV Antibacterial Model, C=  X  Train 269 2610 69 95 .97 .74 .95 .8 .96 Antibacterial Model, C=10 Train 224 2538 141 140 .95 .62 .91 .61 .95 Antibacterial Model, C=3 Train 201 2526 153 163 .94 .55 .90 .57 .94 Antibacterial Model, LDA Train 364 0 2679 0 0.00 1.00 0.12 0.12 -Antibacterial Model, MLR Train 194 564 2115 170 0.21 0.53 0.25 0.08 0.77 Antibacterial Model, ANN Train 294 2651 27 70 0.99 0.81 0.97 0.92 0.97 Drug Model, C=  X  Train 474 2158 214 197 .91 .71 .86 .69 .92 Drug Model, C=10 Train 349 2072 300 322 .87 .52 .80 .54 .87 Drug Model, C=3 Train 305 2026 346 366 .85 .45 .77 .47 .85 Drug Model, LDA Train 0 2372 0 671 1.00 0.00 0.78 -0.78 Drug Model, MLR Train 279 2234 138 392 0.94 0.42 0.83 0.67 0.85 Drug Model, ANN Train 489 2178 194 182 0.92 0.73 0.88 0.72 0.92 Druglike Model, C=  X  Train 674 2043 158 168 .93 .80 .89 .81 .92 Druglike Model, C=10 Train 560 1959 242 282 .89 .67 .83 .70 .87 Druglike Model, C=3 Train 467 1813 388 375 .82 .55 .75 .55 .83 Druglike Model, LDA Train 683 1917 284 159 0.87 0.81 0.85 0.71 0.92 Druglike Model, MLR Train 665 1951 250 177 0.89 0.79 0.86 0.73 0.92 Druglike Model, ANN Train 734 2086 114 107 0.95 0.87 0.93 0.87 0.95 Human Metabolite Model, C=  X  Train 773 2270 0 0 1.00 1.00 1.00 1.00 1.00 Human Metabolite Model, C=10 Train 772 2266 4 1 .99 .99 .99 .99 .99 Human Metabolite Model, C=3 Train 772 2270 0 1 1.00 0.99 .99 1.00 .99 Human Metabolite Model, LDA Train 773 2270 0 0 1.00 1.00 1.00 1.00 1.00 Human Metabolite Model, MLR Train 773 2270 0 0 1.00 1.00 1.00 1.00 1.00 Human Metabolite Model, ANN Train 773 2270 -0 0 1.00 1.00 1.00 1.00 1.00 ANN.
 We have demonstrated that our k -nn classifier with respect to wL 1 distance obtains better accuracy than the LDA and MLR, sometimes significantly so. It is comparable to the ANN classifier in terms of accuracy and is superior in the sense that it is capable of determining a real valued level of bioactivity rather than giving a simple YES or NO an-swer. k -nn approach also provides insights into the level of bioactivity or the importance of the descriptors with respe ct to bioactivity. Analysis of the relative weights of the de-scriptors for the 5 different bioactivity model demonstrate d certain characteristics of these activities. Our models ve r-ify that bacterial metabolites and antimicrobial drugs are significantly overlapping which can be attributed to their similar origin. We also observe that human metabolites dis-play distinctive properties compared to the other 4 bioacti v-ities. Another important observation is that QSAR models for drugs and human metabolites are dominated by few de-scriptors that are correspondingly favored by the drug de-velopers and natural evolution. The distribution of the val -ues for these descriptors may be an important factor for the overlaps among different bioactivities used in our ex-periments. Overall results of the k -nn classification method bring more insight into the nature and structural dominants of the studied classes of small molecules and if necessary, can help rationalizing the design and discovery of novel an-timicrobials and human therapeutics with metabolite-like chemical profiles [11].
 Our classifier is faster compared to alternative approaches , thanks to the DMVP tree data structure we develop for fast similarity search. Our DMVP tree data structure improves the existing vantage point tree data structures in multiple ways. It provides a deterministic selection of the optimal vantage points in each level as well as providing the optimal cut of the tree so as to fit it in the available memory. Our data structure can be applied to any metric distance includ-ing the wL p distance for any p and the Tanimoto distance. It performs very well in practice, achieving fast similarit y search and classification. [1] Adamson, G. W., Cowell, J., Lynch, M. F., McLure, A.
H. W., Town, W. G., Yapp, A. M. (1973) Strategic Con-siderations in the Design of a Screening System for Sub-structure Searches of Chemical Structure Files, J. Chem.
Doc , 13 , 153-157. [2] Brown, R. D. (1997) Descriptors for Diversity Analysis,
Persp. Drug Discovery Des. , 7/8 , 31-49. [3] Chen, X., Reynolds, C. H. (2002) Performance of Simi-larity Measures in 2D Fragment-Based Similarity Search-ing: Comparison of Structural Descriptors and Similarity
Coefficients, J. Chem. Inf. &amp; Comp. Sci. , 42 , 1407-1414. [4] Cherkasov, A. (2005) Inductive Descriptors. 10 Success -ful Years in QSAR, Curr. Computer-Aided Drug Des. , 1 , 21-42. [5] Chvatal, V. (1979) A Greedy Heuristic for the Set Cov-ering Problem, Math. of Operations Research , 4 , 233-235. [6] Cramer, R.D.,Bunce, J.D., and Patterson, D.E. (1988) Crossvalidation, Bootstrapping, and Partial Least
Squares Compared with Multiple Regression in Conven-tional QSAR Studies, Quant. Struct.-Act. Relat. 7 , 18-25. [7] Geladi P.,and Kowalski B. R. (1986) Partial Least-
Squares Regression: A Tutorial, Analytica Chimica Acta , 185 , 1-17. [8] Good, A. C., So, S. S., Richards W. G. (1993) Structure -Activity relationships from Molecular similarity Matrice s,
J. Medicinal Chemistry , 36 , 433-438. [9] Itskowitz, P., and Tropsha, A. (2005) Kappa Nearest neighbors QSAR modeling as a variational problem: the-ory and applications, J. Chem. Inf. Model. , 45(3) , 777-85. [10] Karakoc, E., Cherkasov A., and Sahinalp S.C. (2006)
Distance based ALgorithms for Small Biomolecule Clas-sification and Structural Similarity Search, ISMB X 06 In-telligent Sytems for Molecular Biology , May 2006. [11] Karakoc, E., Sahinalp S.C., and Cherkasov A. (2006)
Comparative QSAR-and Fragments Distribution Anal-ysis of Drugs, Druglikes, Metabolic Substances and An-timicrobial Compounds, Journal of Chemical Information and Modelling , 46(5) , 2167-2182. [12] Livingstone, D. J. (1995) Data analysis for chemists.
Applications to QSAR and chemical product design, Ox-ford Univ. Press , 239. [13] MACCS II Manual, MDL Information Systems, Inc 14600 Catalina Street, San Leandro, CA 94577 USA. [14] Maggiora, G. M., Johnson, M. A. (1990) Concepts and
Applications of Molecular Similarity, Wiley , New York. [15] Sahinalp, S. C., Tasan, M., Macker, J., Ozsoyoglu Z. M.(2003) Distance-Based Indexing for String Proximity
Search, Proc. IEEE Int. Conf. on Data Eng. , 19 , 135-138. [16] Uhlmann, J. K. (1991) Satisfying general proxim-ity/similarity queries with metric trees, Inf. Proc. Lett. , 4 , 175-179. [17] Willett, P., Banard, J. M., and Downs, G. M. (1998) Chemical Similarity Searching, J. Chem. Inf. &amp; Comp..
Sci. , 38 (6) , 983 -996. [18] Yianilos, P. N. (1993) Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces,
Proc. ACM-SIAM Symp. on Discr. Alg. , 1 , 311-321. [19] Zernov, V. V., Balakin, K. V., Ivaschenko, A. A.,
Savchuk, N. P., Pletnev, I. V. (2003) Drug Discovery Us-ing Support Vector Machines. The Case Studies of Drug-likeness, Agrochemical-likeness, and Enzyme Inhibition
Predictions, J. Chem. Inf. &amp; Comp. Sci. , 43(6) ,2048-2056. [20] Zheng, W. and Tropsha, A. (2000) Novel Variable se-lection quantitative structure-property relationship ap -proach based on the k-nearest neighbor principle, J.
Chem. Inf. &amp; Comp. Sci. , 40 , 185. [21] Zupan, J., Gasteiger, J. (1999) Neural Networks in Chemistry and Drug Design, 2nd ed., Wiley , New York.
