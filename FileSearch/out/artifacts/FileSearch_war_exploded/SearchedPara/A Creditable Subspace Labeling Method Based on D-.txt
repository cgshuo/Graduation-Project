 Clustering is a powerful exploration tool capable of uncovering previously unknown tional clustering algorithm have sufficient effect to low dimension data sets and fail to detect the meaningful result in high-dimensional space, due to the characteristic of the CLIQUE. The bottom-up approach of finding such dense units starts with 1-dimensional dense units. The recursive step from (1) k  X  -dimensional dense units to k -dimensional dense units takes (1) k  X  -dimensional dense units as candidates, and gen-dimensional in common. Generated candidates which density doesn X  X  exceed  X  , are eliminated. For efficiency reasons, a pruning criterion called  X  X overage X  is introduced to eliminate dense units lying in less  X  X nteresting X  subspaces as soon as possible. For deciding whether a subspace is  X  X nteresting X  or not, the Maximum Description Length significant results. After generating all  X  X nteresting X  dense units, clusters are found as a maximal set of connected dense units. And then give a DNF interpret. ENCLUS [9] is discrete random variable. The entropy of any subspace S is high when the points are for clustering. MAFIA [10] is another modification of CLIQUE. An adaptive grid sizes concentrating on the portions of the data space which have more points and thus more they heavily depend on the position of the grids, and only get the axis-parallel results. SUBCLUS [11] is an density-based subspace clustering algorithm. The essential idea of SUBCLUS is redefined the notion of DBSC AN to adapt to subspace clustering re-and m . The global parameter makes SUBCLUS can not deal with data sets, which have no uniform density subspace. 
Those classical subspace clustering algori thm almost use a local search method to ascertain, that is one don X  X  know which attribute subset can contain interesting result. the criterion to evaluate creditable subsp aces which contain the meaningful clustering results. All subsets of the original attribute space have a hypothesis: attribute contain cluster and discernment { } assignment function. For finding meaningful clustering result, the probability assign-ute, so we use it as the probability assignment function of { } of subset S is calculated by the probability assignment function. If the Belief exceeds subspaces, which embedded in the original attribute space. CSL generates a candidate Belief values of new candidate creditable su bspace is calculated and for deciding keep amounts simulation data sets show that CSL can find true subspace of original attrib-ute space. This method proves a new path to deal with high dimension data set clus-tering problem, using conventional clustering algorithm. 
The remainder of the paper is organized as follows. In Section 2, we introduce the background of D-S evidence theory. A Cr editable Subspace Labeling Method based Section 4. Section 5 draws conclusions and points out our future work. D-S evidence theory was presented by Dempster in 1967 [12] . Shafer, Dempster X  X  stu-etc, and has been applied in multi sensors network, diagnosis of hospital areas. (FOD), denotes finite system state { } theory is to educe all the current system states based on some observation of the sys-tem states proposed first. Definition 1: Assume function :() [0,1] mP  X  X  , and satisfy: Definition 2: Belief Function :() [0,1] Bel P  X  X  is defined as: different sources. So Dempster rule is used to combine those functions. come from different sources, then the Dempster rule 12 mm m = X  satisfies: space are proposed first. And then a framework of CSL is given. 3.1 Relevant Define in the subspace clustering algorithm: first to confirm the subspace and then to evalu-proposition:
According to the definition of D-S theory, the frame of discernment  X  is denoted as { } as this character we define NP-hard problem. But, the time complex of Dempster rule is () Od , since the frame of discernment only contains mutex element [14] . 
Local character and density distribution of data set can be captured by KNN kernel density estimate. So we use the KNN kernel density to denote the probability distribu-tion of subspace S . function, denote: as a multi-dimensional KNN kernel density estimate, where 
KNN kernel density estimate method has some characters, such as smoothly esti-data set. in the subset S . mH CountS = and of Definition 5: Given a subspace S , if () () space, or Creditable Subspace. 3.2 CSL Algorithm 3.2.1 The idea and Frame of CSL nearest neighbor table of KNN kernel density estimation of each distribution about the hypothesis of set space  X  X  X  , {} candidate candidate S = X  . When there no new subset appends to S , let til candidate = X  . 3.2.2 Algorithm Complexity neighbor of each Algorithm 1 : CSL 
Input: data set D , nearest neighbor number k , kernel function kernal and belief pa-rameter  X  . 2.1. T = X  ; dft = X  ; is 2 (( 3)) Odn n + at most. In while loop, the Dempster rule is used to combine probabil-ity distribution which comes from different sources, and calculates the belief of new proposed CSL algorithm is implemented in matlab 6.5. permits to control the size and structure of the generated data sets through parameters, simple, we use the same label method, which was represented in [14], to express data set.  X  X  X , X  X  X , X  X  X , X  X  X  denote the records of data set, the number of clusters, the dimen-sions of data set and subspace respectively. For example, B10000C10D50S15 ex-there are 10 clusters through 15 dimensions. In this paper, we evaluate LCS from the precision and efficiency aspects. (1) Precision of CSL the precision of the CSL. We perform CSL  X  CLIQUE ENCLUS and SUBCLUS on B30000C10D50S5, B30000C10D50S10, B30000C10D50S15, B30000C10D50 S20, B30000C10D50S25, B30000C10D50S30, B30000C10D50S35, and compare their precision. Experiment results as shown in fig2. The parameter of CLIQUE ENCLUS character makes CSL can capture the local distribute of data set, and easy to find the the concept of DBSCAN, and can not do well without the parameter X  X  influence. (2) Efficiency of CSL. 
We evaluate the Efficiency of CSL from three aspects: the size and the dimensions CSL against the size of dataset was performed on D5000C6D20S5, D10000C6D20S5, D15000C6D20S5, D20000C6D20S5, 15000C6D20S5, D30000C6D 20S5, D35000C6D20S5, D40000C6D20S5, D45000C6D20S5. On data sets: D20000C6 D10S5, D20000 C6D15S5, D20000C6D20S5,D20000C6D25S5, D20000C6 D30S5, we evaluate the scalability of CSL against the dimensions of data set. Analogously, we D30000C6D50S2, D30000C6D50S4, D30000C6D50S6, D30000C6D50S8, and D30000C6D50S10. we observe that CS L has better scalability of the size and dimensions of data set, the dimensions of subspace. Subspace clustering is an important method to deal with high-dimensional clustering true data distribution in high-dimensional space. Subspace clustering tries to find the contains interest cluster is uncertain. So we regard subspace clustering as an uncertain Subspace method based on D-S evidence theory is proposed. Simultaneously, we also give the definition of Belief Subspace and its evaluation criterion. LCS iterative finds conventional clustering algorithm on the subspace set to get the results. Experiments show that LCS can find the true subspace of data set and has better scalability of the size and dimensions of data set, the dimensions of subspace. The method of this paper shows a new way to use conventional clustering algorithm to deal with high-dimensional data set. 
