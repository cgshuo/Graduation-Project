 Evaluating adaptive and personalized information retrieval tech-niques is known to be a difficult endeavor. The rapid evolution of novel technologies in this scope ra ises additional challenges that further stress the need for new evaluation approaches and meth-odologies. The BARS 2013 workshop seeks to provide a specific venue for work on novel, persona lization-centric benchmarking approaches to evaluate adaptive retrieval and recommender sys-tems. H.3.3 [ Information Search and Retrieval ]: search process , information filtering . Evaluation, adaptive information retrieval, recommender systems, benchmarking, metrics, methodology. Great progress has been made in recent years in the development of recommendation, retrieval and personalization techniques. Yet the evaluation of these systems is still based on traditional met-rics, e.g. precision, recall or RM SE, often not taking the use-case and situation of the system into consideration, and failing to pro-vide a suitable proxy of user satisfaction and business goals. Moreover, the rapid evolution of novel information retrieval (IR) and recommender systems foster the need for new evaluation paradigms. New evaluation approaches of adaptive systems should evaluate both functional and non-functional requirements. Functional re-quirements go beyond traditional re levance metrics and focus on user-centered utility metrics, such as novelty, diversity and seren-dipity. Non-functional require ments focus on performance and technical aspects, e.g. scalability and reactivity. The evaluation of adaptive IR systems has been acknowledged to find difficulty in fitting in established evaluation paradigms and methodologies, which can be identified as a hurdle to research and progress in this area. Active research efforts and open discus-sion are currently taking place in parallel in the Recommender Systems and Adaptive IR fields, where devising methodologies 
