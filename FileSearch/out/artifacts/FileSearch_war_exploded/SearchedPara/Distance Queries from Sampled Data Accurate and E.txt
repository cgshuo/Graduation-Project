
Data is commonly generated or collected repeatedly, where each instance has the form of a value assignment to a set of keys : Daily summaries of the number of queries containing certain keywords, activity in a social network, transmitted bytes for IP flow keys, per-formance parameters (delay, throughput, or loss) for IP source des-tination pairs, environmental measurements for sensor locations, and requests for resources. In these examples, each set of values (instance) corresponds to a particular time or location. The uni-verse of possible key values is fixed across instances but the values of a key are different.

One of the most basic operations in data analysis are Distance queries , which are used to detect, measure, and localize change [7, 26]. Their applications include anomaly detection, monitoring, and planning. Formally, a difference query between instances is specified by a meta-data based selection predicate that is applied to the keys. The result is the distance between the vectors projected on the selected keys. The simple example in Figure 1 shows two instances on 7 keys, and some queries specified on this data using Euclidean and Manhattan distances. Figure 1: Distances between two instances, Table shows a data set with two instances i  X  X  1 , 2 } and 7 keys { a,...,f } and the values v i ( h ) of key h in instance i . The figure also provides example distance queries, specified for a selected set H of keys.
The collection and warehousing of massive data is subject to lim-itations on storage, throughput, and bandwidth. Even when the data is stored in full, exact processing of queries may be slow and re-source consuming. Random sampling of datasets is widely used as a means to obtain a flexible summary over which we can query the data while meeting these limitations [27, 36, 6, 5, 8, 20, 21, 3, 17, 3, 22, 12, 19, 10, 13].

The sampling scheme is applied to our data, and a set of random bits, and returns a small subset of the entries. Our sample includes these entries and their values and has a fraction of the size of the original dataset. The sample only includes nonzero entries, this is particularly important when the data is sparse, meaning that the vast majority of keys have associated value 0 . When the values distri-bution is skewed, we often apply weighted sampling, meaning that the probability a key is sampled depends on its value  X  Favoring heavier keys allow for more accurate estimates of sums and other statistics.

Perhaps the most basic sampling scheme is Poisson sampling, where keys are sampled independently. Weighting is often done using Probability Proportional to Size (PPS) sampling [23], where the inclusion probability is proportional to the value. Other com-mon sampling schemes are bottom-k (order) samples, which have the advantage over independent sampling of yielding a sample size of exactly k . Bottom-k sampling generalizes reservoir sampling and includes Priority (Sequential Poisson) sampling and weighted sampling without replacement [33, 32, 30, 11, 19, 12, 13]. These sampling schemes are very efficient to apply also when the data is streamed or distributed.

Once we have the sample, we can quickly process approximate queries posed over the original data. This is done by applying an estimator to the sample. We seek estimators that provide good re-sults when a small fraction of the data is sampled. In particular, we would like them to be admissible , that is, optimally use the infor-mation we have in the sample, and be efficient to compute. When estimating nonnegative quantities, such as distances, we are inter-ested in nonnegative estimators.

Consider the basic problem of estimating, for a given selection predicate, the sum of values of keys selected by the predicate. This problem is solved well by the classic Horvitz-Thompson (HT) [24] estimator. The estimate is the sum, over sampled keys i satisfying the predicate, of the ratio v i /p i , where v i is the value and p the inclusion probability of i . This inverse-probability estimate is clearly unbiased (if v i &gt; 0 =  X  p i &gt; 0 ): if the key is not sam-pled, the estimate is 0 and otherwise it is v i /p i , giving expectation v . The estimate is also nonnegative when values are. Moreover, the inverse-probability estimator is a UMVUE (uniform minimum variance unbiased estimator), meaning that among all unbiased and nonnegative estimators, it minimizes variance point wise. To apply this estimator, we need the inclusion probability p i to be available to it when v i is. This is the case with both Poisson and bottom-k sampling, when implemented correctly.

The HT estimate is a sum estimator: conceptually, we apply an estimator to each key in the universe. Keys that are not sampled have an estimate of 0 . Keys that are sampled have a positive esti-mate. We then sum the estimators of different keys. This makes it highly suitable for domain (selection) queries, which aggregate over a subset of keys that can be specified by a predicate.
We now turn to our problem of estimating the distance between instances from their samples. We seek an estimator with similar properties to the HT estimator: a sum estimator, unbiased and non-negative, and with optimal use of the information in the sample.
This problem turns out to be significantly more challenging. One can attempt to apply again the HT estimator: When the outcome re-veals the value of the estimated quantity, the estimate is equal to the value divided by the probability of such an outcome. The estimate is 0 otherwise. Inverse probability estimates, however, are inap-plicable to distance estimation over weighted samples, since they require that there is a positive probability for an outcome which re-veals the exact value of the estimated quantity: the absolute differ-ence between the values of the key in two instances. With multiple instances and weighted sampling, keys that have zero value in one instance and positive value in another have positive contribution to the distance but because zero values are never sampled, there is zero probability for determining the value from the outcome: In the Example in Figure 1, key c has value 5 in the first instance and value 0 in the second, and thus has a contribution of 5 to the L distance. The key however will never be included in a weighted sample of instance 2.

When considering multiple instances, we also need to specify how their samples relate to each other. The sampling schemes we mentioned, Poisson and bottom-k , specify the distribution for a single instance. Our first requirement, since data of different in-the sample of one instance can not depend on values assumed in another [16, 14]. The random bits, however, can be reused (using random hash functions), to make sampling probabilities dependent. The two extremes of the joint distribution of samples of different in-stances are independent sampling (independent sets of random bits for each instance) and coordinated sampling (identical sets of ran-dom bits). With coordinated sampling, which is a form of locality sensitive hashing, similar instances have similar samples whereas independent samples of identical instances can be completely dis-joint. Each of these two relations has unique advantages and there-fore in our work here we address both: Coordination [4, 34, 31, 33, 6, 5, 8, 20, 21, 3, 12, 22, 13, 16] allows for tighter estimates of many basic queries including distinct counts (set union) [8, 20, 21, 13], quantile sums [16], Jaccard similarity [6, 5], and more recently L distance [16]. The drawbacks of coordination are that it results in unbalanced  X  X urden X  where same keys tend to be sampled across instances  X  an issue when, for example, being sampled translates to overhead which would like to balance across keys. Moreover, while beneficial for some queries, the variance on other queries  X  notably sum queries that span values from multiple instances ( X  X otal number of search queries by Californians on Monday-Wednesday X , from daily summaries)  X  is larger than with independent sampling  X  an issue if our samples are primarily used for such queries. Contributions:
We derive unbiased nonnegative estimators for L p p distance queries over weighted samples. These include the two important cases of Manhattan distances L 1 and Euclidean distances L 2 (which is the square root of L 2 2 ). Our estimators apply with either Poisson or bottom-k sampling. We also address the two important cases where the samples of different instances are independent or coordinated. Our work facilitates, for the first time, the use of weighted samples as summaries that support distance queries.

Our estimators have several compelling properties. Similarly to the HT estimator, our estimators are sum estimators: We estimate L p as a sum, over the selected keys , of nonnegative unbiased es-timates of RG p = | v 1  X  v 2 | p of the values assumed by the key (see Figure1). Our estimators are unbiased, nonnegative, and ad-missible (Pareto optimal), meaning that another (nonnegative and unbiased) estimator with strictly lower variance on some data must have strictly higher variance on another data. The estimate  X  tained for a particular key has high variance, since most likely, the key is not sampled in any instance (in which case the estimate is 0 ), but unbiasedness allows for diminishing relative error when more keys are selected. The distance L p can be estimated by the p th root of our L p p estimate.

Our estimates  X  RG p can be positive also when the samples do not reveal the respective value RG p but only partial information on it. This property turns out to be critical for obtaining admissible es-timators, what is not possible, as we mentioned above, with the inverse-probability estimate. It also means, however, that the esti-mators we derive have to be carefully tailored to the power p .
For independently-sampled instances, we present an estimator for RG p of two values ( p &gt; 0 ). This derivation uses a technique we presented in [14]. Our estimator, which we call the L  X  estimator, is the unique symmetric and monotone admissible estimator, where monotonicity means that the estimate is non-decreasing with the information we can glean from the outcome.

For coordinated samples of instances, we apply our framework of monotone sampling [9]. We derive the L* and U* estimators for the RG p functions, which are admissible, unbiased, and nonnega-tive. The L* estimator is monotone and has lower variance for data with small difference (range) whereas the U* estimator performs better when the range is large. This choice is important, because it allows us to customize estimation to properties of the data set. Net-work traffic data, for example, is likely to have larger differences and thus the U* estimator may perform better whereas the L* es-timator would be preferable when differences are smaller. The L* estimator also exhibits a compelling theoretical property of being  X  X ariance competitive X  [15], meaning that for all data vectors, its variance is not too far off the minimum possible variance for the vector by a nonnegative unbiased estimator. This property makes the L* estimator a good default choice.
For p = 1 , 2 , which are the important special cases of the Man-hattan and the Euclidean distances, we compute closed form ex-pressions of estimators and their variance and also obtain tighter bounds on the  X  X ompetitiveness X  of the L* estimator. We evalu-ate and compare the performance of our L 1 and L 2 2 distance esti-mators on queries over diverse data sets. The queries vary in the support size (number of keys in the data set satisfying the selection predicate) and in the relative difference (difference normalized by norm). We show that in all cases, we achieve good results when a small fraction of the data is sampled. Over coordinated sam-ples, we examine the behavior of the L* and U* estimators and also consider the optimally competitive estimator, which minimizes the worst-case ratio, and we compute by a program. Finally, we provide guidelines to choosing between these estimators based on properties of the data.
 Roadmap: Section 2 contains necessary background and defini-tions. We present difference estimators for independent samples in Section 3 and for coordinated samples in Section 4. Section 7 contains an experimental evaluation.
We denote by v ih  X  R  X  0 the value of key h  X  K in instance i  X  [ r ] and by the vector v ( h ) , the values of key h in all instances (the column vector v ih ). The exponentiated range of a vector v is: where max( v )  X  max i v i and min( v ) = min i v i are the max-imum and minimum entry values of the vector v . We omit the subscript when p = 1 .
 We are interested in queries which specify a selected subset H  X  K of keys, through a predicate on K , and return The L p -distance of two instances ( r = 2 ) is L p ( H )  X  ( L
When data is sampled, we estimate L p p , by summing estimates  X  p for the respective single-key primitive RG p ( v ( h )) over keys h  X  H . We use nonnegative unbiased estimators for the primitives, which result, from linearity of expectation, in unbiased estimates for the sums. We measure error by the coefficient of variation (CV), which is the ratio of the square root of the variance to the mean. Our estimates for each key have high variance, but when inclusions of different keys are pairwise independent, variance is additive and the CV decreases with | H | , allowing for accurate estimates of the sum. Finally, we can estimate L p ( H ) by taking the p th root of the estimate for L p p ( H ) . This estimate is biased, but the error is small when the CV of our L p p ( H ) estimate is small.

A basic component in applying sum estimators is obtaining from basic sampling schemes of instances the respective estimation prob-lems for a single key . We cast the basic sampling schemes discussed in the introduction in the following form, which facilitates separate treatment of each key.

The sampling of the entry v ih is specified by a threshold value  X  ih  X  0 , and random seed values u ih  X  U [0 , 1] chosen uniformly at random. The estimation problem for a single key h is then as follows. We know  X  = (  X  1 h ,..., X  rh ) , the seeds u = ( u 1 h ,...,u the results of the sampling for h (the value v ih in all instances i where h was sampled). We apply an estimator  X  RG p to this infor-mation, which we refer to as the outcome S , to estimate RG The availability of the seeds u ih to the estimator turns out to be critical for estimation quality [14, 9]. We facilitate it by generating the seeds using random hash functions (pairwise independence be-tween keys suffices for variance bounds). When we treat a single key h , we omit the reference to h from the notation.
 Sampling scheme of instances. We now briefly return to sampling schemes of instances, and show how we obtain the single-key for-mulation (3) from them.

With Poisson PPS (Probability Proportional to Size) sampling of instance i , each key h is sampled with probability proportional to v . An equivalent formulation is to use a global threshold value T , such that a key h is sampled if and only if v ih  X  u ih pected sample size E [ | S | ] = P h  X  K min { 1 ,v ih /T by T i . The sampling can be easily implemented with respect to ei-ther a desired sample size or a fixed threshold value. The respective estimation problem (3) for key h has  X  ih  X  T i . As an example, we can obtain a PPS Poisson sample of expected size E [ | S | ] = 3 for the instances in Figure 1 using T 1 = 29 / 3 (instance 1) and T 2 = 33 / 3 = 11 (instance 2).

Priority (sequential Poisson) sampling [30, 19, 35] is performed by assigning each key a priority r ih = v ih /u ih . The sample of instance i includes the k keys with largest priorities, the ( k + 1) largest priority T i , and the k th largest priority T 0 i
To obtain the single-key formulation (3) for key h , we consider the sampling conditioned on fixing the seeds u ij (and thus the pri-orities r ij ) for all j 6 = h [12, 19]. The effective threshold  X  the k th largest priority in K \{ h } : Key h is then sampled if and only if v ih /u ih &gt;  X  ih . When h  X  S ,  X  ih = T i and when h 6 X  S ,  X 
When sampling several instances, we can make the samples in-dependent when we use independent u ih for all i . The samples are coordinated (shared-seed) if the same seed is used for the same key in all instances, that is,  X  h  X  K,  X  i  X  [ r ] ,u ih = u
Figure 2 shows Poisson PPS and priority samples, independent and coordinated, obtained for the two instances in Figure 1 from a random seed assignment u ih . The figure also shows the threshold values  X  ih , which are available to the estimators  X  RG p example, the outcome for key 4 , (the outcome is the input to the es-timator), is as follows. When instances are independently Poisson sampled the outcome includes: v 1 = 5 (key 4 is sampled only in instance 1 so we know v 14 exactly), u 1 = 0 . 15 , u 2 = 0 . 36 (seeds available by applying hash functions to the key), and  X  1  X  = 11 (with Poisson PPS sampling, the same threshold applies to all keys in each instance). With coordinated priority sampling of instances the outcome includes v 1 = 5 , u = 0 . 15 ,  X  (since key 4 is sampled in instance 1 ) and  X  2 = 30 . 4 (since key 4 is not sampled in instance 2 ).

From here onward, we focus on estimating RG p ( v ) for a single key from the respective outcome S . We return to sum aggregates only for the experiments in Section 7.
 Estimators: For an outcome S , we denote by
S  X  = { v | S = S ( u , v ) } the set of all data vectors consistent with S . We can equivalently define the outcome as the set S  X  since it captures all the informa-tion available to the estimator on v and hence on RG our example in Figure 2, considering independent Poisson sam-pling and key 4 , the set S  X  includes all vectors (5 ,x ) such that x &lt; u 2  X  2 = 0 . 36  X  11 = 3 . 96 . The actual data vector for key 4 is (5 , 0) , but the outcome only partially reveals it. size 3 ( k = 3 ).

We denote by S the set of all possible outcomes, that is, any out-come consistent with any data vector v in our domain. For data v , we denote by S v the probability distribution over outcomes consis-tent with v . As mentioned, we are seeking nonnegative estimators  X  p ( S )  X  0 for all S  X  S , since RG p are nonnegative. Since we sum many estimates, we would like each estimate to be unbiased E
S  X  S v [  X  RG p ( S )] = RG p ( v ) . We also seek bounded variance on all optimality): there is no nonnegative unbiased estimator with same or lower variance on all data and strictly lower on some data. An intuitive property that is sometimes desirable is monotonicity : the estimate value is non decreasing with the information on the data that we can glean from the outcome S  X   X  S 0 X  =  X   X  RG p ( S )  X   X  p ( S 0 ) .

When S  X  includes vectors v such that RG p ( v ) = 0 , any unbi-ased and nonnegative estimator must have  X  RG p ( S ) = 0 (with prob-ability 1 ). We therefore limit our attention to estimators satisfying this property.

We can also see that when the key h is not sampled in any in-stance, then S  X  is consistent with RG p ( v ) = 0 , which means that  X  p ( S ) = 0 and we therefore do not need to explicitly compute the contribution of the vast majority of keys that are not sampled in at least one instance.

Finally, we will use the following definition of order optimal-ity of estimators in our constructions. Given a partial order  X  on the data domain an estimator  X  f is  X  -optimal (respectively,  X  optimal) if it is unbiased (resp., and nonnegative) for all data v , and minimizes variance for v conditioned on the variance being minimized for all preceding vectors. Formally, if there is no other unbiased (resp., nonnegative) estimator that has strictly lower vari-ance on some data v and at most the variance of  X  f on all vectors that precede v . An order optimal estimator is admissible.
We derive estimators for RG p ( v ) , where v = ( v 1 instances r = 2 ). The estimation scheme is specified by  X  = (  X  , X  2 ) . The outcome S ( u , v ) is determined by the data vector v = ( v 1 ,v 2 ) and u = ( u 1 ,u 2 ) , where u i  X  U [0 , 1] are indepen-dent. The set S  X  of vectors consistent with S is (4).

We derive the L* estimator,  X  RG ( L ) p , which is the unique symmet-ric, monotone, and admissible estimator. The construction adapts a framework from [14], which was used to estimate max { v We specify an order  X  on the data domain. We then formulate a set of sufficient constraints for an unbiased symmetric and order-optimal estimator  X  f (  X  ) of RG p ( v ) . The constraints, however, do not incorporate nonnegativity, as this results in much more complex dependencies. But if we find a nonnegative solution  X  f (  X  ) find an estimator with all the desired properties. We therefore hope for a good  X  X uess X  of  X  .

We work with  X  that prioritizes smaller distances, that is, v  X  z if and only if RG ( v ) &lt; RG ( z ) . With each outcome S  X  S , we associate its determining vector  X  ( S ) , which we define as the obtained when using a non-strict inequality in (4). The determining vector is unique for all outcomes S that are not consistent with p ( v ) = 0 , that is, RG p ( v ) &gt; 0 for all v  X  S  X  . As we mentioned earlier, we only need to specify the estimator on these outcomes, since we only consider estimators that are 0 on outcomes consistent with RG p ( v ) = 0 . The mapping of outcomes S to the determining vector  X  ( S ) is shown in Table 1 (Bottom).

We now formulate sufficient constraints for  X  -optimality. Con-ditioned on fixing the estimator on outcomes S such that  X  ( S )  X  v , the  X  X est X  we can do, in terms of minimizing variance, is to set it to a fixed value on all outcomes such that  X  ( S ) = v . This fixed value is determined by the unbiasedness requirement. Since  X  f same for all outcomes with same determining vector, we specify it as a function of the determining vector  X  f (  X  ) ( S )  X 
We use the notation S 0 ( v ) for the set of outcomes S that are consistent with v but also consistent with a vector that precedes v : The contribution of the outcomes S 0 ( v ) to the expectation of when data is v is where I is the indicator function. We obtain the following sufficient constraints for a  X  -optimal unbiased estimator, that may not be nonnegative, but is forced to be 0 on all outcomes consistent with p ( v ) = 0 . For all v ,
We derive  X  RG ( L ) p by solving the right hand side of (6) for all v such that PR S  X  X  v [  X  ( S ) = v ] &gt; 0 . The solution  X  is provided in Table 1 through a mapping of determining vectors to estimate values. The estimator is specified for  X  1  X   X  estimator  X  RG ( L ) p is nonnegative, monotone (for all y ,  X  is non-increasing for x  X  (0 ,y ] ) and has finite variances (follow (5) holds. Vectors v with PR S  X  X  v [  X  ( S ) = v ] = 0 are exactly those with one positive and one zero entry. We can verify that (5) Table 1: Top: Estimator  X  RG ( L ) p for p &gt; 0 over independent samples, stated as a function of the determining vector  X  = (  X  1 , X  2 ) when  X  1  X   X  2 (case  X  2 &gt;  X  1 is symmetric). Bottom: mapping of outcomes to determining vectors. is satisfied, that is, E S  X  X  v  X  RG ( L ) p ( S ) = RG p Table 2 shows explicit expressions of  X  RG ( L ) and  X  RG Table 2: Explicit form of estimators  X  RG ( L ) and  X  RG ( L ) over independent samples. Estimator is stated as a function of the determining vector (  X  1 , X  2 ) when  X  1  X   X  2 (case  X  symmetric).

We now provide the derivation. We consider vectors v in increas-ing  X  order and solve (6) for f (  X  ) on outcomes with determining vector v = ( v,v  X   X ) , where v  X   X   X  0 .  X  Case: v  X   X   X   X  2 . The outcomes always reveals the second entry. The determining vector is v when u 1  X  1  X  v , which happens with probability min { 1 ,v/ X  1 } . Otherwise, the outcome is consis-tent with ( v  X   X  ,v  X   X ) and the estimate is 0 . We solve the equality  X  p = min { 1 ,v/ X  1 }  X  RG ( L ) p , obtaining  X  Case: v  X   X  &lt;  X  2 . The determining vector is ( v,v  X   X ) with y &lt;  X  . We use (6) to obtain an integral equation:
Taking a partial derivative with respect to  X  , we obtain
We use the boundary value for  X  = max { 0 ,v  X   X  2 } : and obtain the solution  X  The special case  X  1 =  X  2 =  X  : The estimators  X  RG ( L ) a function of the determining vector and their variance are provided in Tables 3 and 4.

Table 3:  X  RG ( L ) and its variance for independent samples.
We derive estimators for RG p ( v ) ( p &gt; 0 ), where v = ( v for r  X  2 . The sampling uses the same random seed u for all en-tries. The outcome S ( u, v ) is determined by the data v and a scalar seed value u  X  (0 , 1] , drawn uniformly at random: Entry i is in-cluded in S if and only if v i  X   X  i u .

We apply our work on estimators for monotone sampling [9] to derive two unbiased nonnegative admissible estimators: The L* form expressions of estimators and variances when  X  has all en-tries equal (to the scalar  X  ). The derivations easily extend to non-uniform  X  .
 The set of data vectors consistent with outcome S ( u, v ) is S  X  = { z | X  i  X  [ r ] ,i  X  S =  X  z i = v i , i 6 X  S =  X  z i Observe that the sets S  X  ( u, z ) are the same for all consistent data vectors z  X  S  X  ( u, v ) . Fixing the data v , the set S decreasing with u , which means that the information on the data that we can glean from the outcome can only increase when u de-creases. This makes the sampling scheme monotone in the random-ization, which allows us to apply the estimator derivations in [9]. The lower bound function. The derivations use the lower bound function RG p , which maps an outcome S to the infimum of
Table 4:  X  RG ( L ) 2 and its variance for independent samples. values on vectors that are consistent with the outcome: For RG , the lower bound is the difference between a lower bound on the maximum entry and an upper bound on the minimum entry. The lower bound on RG p is the p th power of the respective bound on RG , that is, RG p ( S ) = RG ( S ) p . For S ( u, v ) , we use the notation p ( S ( u, v ))  X  RG p ( u, v ) . For all-entries-equal  X  : condition | S | RG ( S ) Figur e 3: Left: The lower bound function and corresponding lower hull for example vectors and p  X  { 0 . 5 , 1 , 2 } . Right: the corresponding optimal, L*, and U* estimates on outcomes con-sistent with the vector.
 The L* estimator. The L* estimator [9] is defined for any mono-tone estimation problem for which an unbiased and nonnegative estimator exists. This estimator is specified as a function of the corresponding lower bound function. For RG p we have  X  S (  X , v ) ,  X  RG ( L ) p ( S ) = RG p
From [9], we know that  X  RG ( L ) p has the following properties:  X  It is nonnegative and unbiased.  X  It is the unique (up to equivalence) admissible unbiased non-negative monotone estimator, meaning that the estimate is non-decreasing with u .  X  It is  X  + -optimal with respect to the partial order  X   X  + -optimality with respect to this particular order means that any estimator with a strictly lower variance for a data vector must have strictly higher variance on some vector with a smaller range  X  this means that the L* estimator  X  X rioritizes X  data where the range (or difference when aggregated) is small.
 The L* estimator has finite variances when the monotone estima-tion problem admits a nonnegative estimator with finite variances. It is also 4-competitive in terms of variance [15], meaning that for any data vector, the ratio of the expectation of the square to the minimum one possible for the data via an unbiased nonnegative estimator is at most 4 where  X  RG ( v ) p ( S ) is a nonnegative unbiased estimator which min-imizes the variance for v (We will present a construction of this estimator). Competitiveness, is a strong property that means that for all data vectors, the variance under the L* estimator is not too far off the minimum possible variance for that vector by a nonneg-ative unbiased estimator.

For our sampling scheme with all entries equal  X  , we define max( v )  X  max i v i (which is available from S whenever | S | &gt; 0 ) and v min = min( v ) if | S | = r and v min = u X  otherwise, which is also always available from S , the estimator is  X  p ( S ) = Estimators and variance for RG and RG 2 are provided in Tables 6 and 7. We also compute a tight ratio on variance competitiveness for p = 1 , 2 :
L EMMA 4.1. The U* estimator. The U* estimator [9] is the solution of the integral equation  X  S (  X , v ) ,  X  RG p (  X , v ) = sup From [9], we know that  X  RG ( U ) p has the following properties:  X  It is nonnegative and unbiased.  X  It is  X  + -optimal with respect to the partial order  X  This means that the U* estimator  X  X rioritizes X  data where the range (or difference when aggregated) is large. In particular, it is the non-negative unbiased estimator with minimum variance on data with min( v ) = 0 .

The solution for all-entries equal  X  is provided as Algorithm 1 (details are omitted due to space limitations). The estimator is ad-missible and has finite variances for all data vectors.

Expressions for the variance of  X  RG ( U ) p for p = 1 , 2 are omitted due to space limitations. v -optimality. We say that an estimator is v -optimal (for data v ), if amongst all estimators that are nonnegative and unbiased for all data, it has the minimum possible variance when the data is v . In order to measure the competitiveness of our estimators, as in Lemma 4.1, we derive an expression for the v -optimal estimate values  X  RG p ( v ) . It turns out that the values assumed by a v -optimal estimator on outcomes consistent with v are unique (almost every-where on S v [15]. Note that there is no single estimator that is v -optimal for all v , that is, there is no uniform minimum variance unbiased nonnegative estimator for RG p . Therefore, the v -optimal estimates for all possible values of v can not be combined into a single estimator.

We now obtain an explicit representation of  X  RG ( v ) p . We use the hull) of RG p ( u, v ) and the point (1 , 0) . This function is monotone non-increasing in u and therefore differentiable almost everywhere. We apply the following
T HEOREM 4.1. [15] A nonnegative unbiased estimator  X  RG p imizes VAR S  X  X  v [  X  RG p ]  X  X  X  almost everywhere on S v The estimates (13) are monotone non-increasing in u .

We can now specify  X  RG ( v ) p for PPS sampling with all-entries-equal  X  . The function RG p ( u, v ) is max { 0 , max( v )  X   X  } for u  X   X  and equal to RG p ( v ) for u  X   X  , the lower hull is H
For p  X  1 , the function is concave for u  X  [ min( v )  X  , The lower hull is therefore a linear function for u  X  max( v ) max( v )  X   X  , H ( v ) RG max( v )  X   X  , H ( v ) RG  X  ) p ) . The v -optimal estimates are therefore constant for u  X  and  X  RG ( v ) p ( u ) = RG p ( v )  X  (max( v )  X   X  ) p when max( v )  X   X  .
For p &gt; 1 , RG p ( u, v ) is convex for u  X  [ min( v ) ometrically, the lower hull follows the lower bound function for u &gt;  X  , where  X  is the point where the slope of the lower bound function is equal to the slope of a line segment connecting the cur-follows this line segment and is linear. Formally, the point  X  is the solution of If there is no solution  X   X  [ min( v )  X  , min { 1 , max( v ) min { 1 , max( v )  X  } . The estimates for u  X  [  X , min { 1 ,  X  p ( u, v ) =  X   X  ,  X 
Figure 3 (top) illustrates RG p ( u, v ) and the corresponding lower
Now that we expressed  X  RG ( v ) p ( S ) on all outcomes consistent with v , we can compute (for any vector v ), the minimum possi-ble variance attainable for it by an unbiased nonnegative estimator: We use the expectation of the square to measure the  X  X ariance com-petitiveness X  of estimators (Since the second summand in (14) is the same for all estimators). The ratio for a particular v is the ex-pectation of the square for v divided by the optimal one which is E the maximum ratio over all v . Figure 4: Ratio of the expectation of the square to the minimum possible expectation of the square for the data point (over shared-seed samples), as a function of the ratio min( v ) / max( v ) . Estimator  X  RG ( L ) p over independent samples, shared-seed samples. Sampling with all-entries equal  X  . The optimally competitive (OC) estimator. We used a program to compute the estimator with minimum competitive ratio. The es-timator was computed for an outcome that revealed max( v ) and provided an upper bound x &lt; max( v ) on min( v ) . The domain was discretized and the estimates were computed iteratively for de-creasing x so that the estimates satisfy a certain ratio c . We then performed a search to find the minimum c for which the computa-tion is successful.
 Choosing between the L*, U*, and OC estimators. Figure 3 shows the v -optimal estimates and the L* and U* estimators for example vectors, illustrating the monotonicity of L* and how the estimators relate to each other. The estimators and their variances depend only on  X  and the maximum and minimum entry values max( v ) and min( v ) . We study the variance for all-entries-equal  X  and max( v )  X   X  .

Figure 4 shows the expectation of the square of the L* estimator over independent samples and of the L*, U*, and OC estimators over shared-seed samples. This is as a function of the ratio of the minimum to the maximum value in the data vector. The expecta-tion of the square plotted is the ratio to the minimum possible ex-pectation of the square over coordinated samples (the v -optimum). Recall that the v -optimum is not simultaneously attainable for all vectors and is used only as a reference for variance competitive-ness. We can see that the L* estimator is nearly optimal when the ratio is large and that the U* estimator is nearly optimal when the ratio is small. The OC estimator outperforms both in the mid range. For the L* estimator, the ratio is always at most 2 (for p = 1 ) and 2.5 (for p = 2 ) from the optimum whereas the U* estimator can have large ratios. The OC estimator has optimal worst-case ratios of 1.204 (for p = 1 ) and 1.35 (for p = 2 ), but the ratio is the same across the range for all data vectors with min( v ) &lt; max( v )  X   X  .
We study the variance as a function of min( v ) max( v ) . The variance is 0 when RG ( v ) = 0 (ratio is 1 ). Otherwise, it is lower for  X  when the ratio is sufficiently small. The threshold point is  X  satisfies For p = 1 ,  X  1  X  0 . 285 (is the solution of the equality (1  X  x ) / (2 x ) = ln(1 /x ) ). For p = 2 ,  X  2  X  0 . 258 .

This suggests selecting an estimator according to expected char-acteristics of the data. If typically RG ( v ) &gt; (1  X   X  choose  X  RG ( U ) p . If typically RG ( v ) &lt; (1  X   X  p  X  p , and otherwise, we choose the OC estimator.
The one-sided distance , which isolates the growth or decline components, is defined as L p p + ( H ) = P h  X  H RG p + ( v ( h )) , where p + ( v ) = max { 0 ,v 1  X  v 2 } p . We can use any of our mators  X  RG p ( S ) to estimate RG p + as follows: If S  X  that v 1  X  v 2 then  X  RG p + ( S ) = 0 . Otherwise,  X  RG p + We can symetrically define L p p  X  and RG p  X  and  X  RG p  X 
Our derivations can be extended to other sampling schemes. One such extension is to weighted sampling without replacement (PP-SWR), which is bottom-k sampling with priorities r ih =  X  [33, 32, 11, 12].
Prior to our work, the only distance estimator we are aware of which obtained good estimate with small fraction of data sampled is for L 1 over coordinated samples. This estimator uses the relation | v 1  X  v 2 | = max { v 1 ,v 2 }  X  min { v 1 ,v 2 } to obtain an indirect estimate as the difference of two inverse probability estimates for the maximum and minimum [16]. Our U* estimator for p = 1 is a strengthening of this L 1 estimator.

Distance estimation over unweighted coordinated [28] or inde-pendent [14] sampling is a much simpler problem. With unweighted sampling, the inclusion probability of positive entries is indepen-dent of their weight. When carefully implemented, we can use inverse-probability estimates, which as discussed in the introduc-tion, do not work with weighted sampling. An unweighted sam-ple, however, is much less informative for its size when the data is skewed, as  X  X eavy hitters X  can be easily missed out. The estima-tors we develop here are applicable with, and take advantage, of weighted sampling.

Distance estimation of vectors (each instance in our terminology is presented as a vector of key values) was extensively studied us-ing linear sketches , which are random linear projections, e.g. [25, 2, 1, 18]. Random projections have the property that the difference vector of sketches is the sketch of the difference of the two vectors. This means they are tailored for distance-like queries which aggre-gate over functions of the coordinate-wise differences. In particu-lar, with linear sketches we can accurately estimate distances that are very small relative to the input vectors norms, whereas even with weighted sampling, accuracy depends on the relation of the distance to the vectors norms. A significant disadvantage of lin-ear sketches, however, is lack of flexibility: Unlike samples, they do not support domain (selection) queries that are specified after the summary structure is computed and can only estimate distance between the full vectors. Moreover, each sketch is tailored for a specific metric, such as L 2 2 . Lastly, linear sketches are not suit-able for estimating one-sided distances. Moreover, linear sketches have size which often depends (poly) logarithmically on the num-ber of keys. To summarize, the two techniques, sampling and lin-ear sketches, have different advantages. Sampling provides much greater flexibility in terms of supported queries and often admits a smaller summary structure.
We selected several datasets that have the form of values as-signed to a set of keys, on two instances, and natural selection predicates. We consider the L 1 and L 2 2 distances on keys satis-fying these predicates. Properties of the data sets with selections are summarized below and in Table 5.  X  destIP (IP packet traces): keys: (anonymized) IP destination ad-dresses. value: the number of IP flows to this destination IP. In-stances: two consecutive time periods. Selection: all IP destination addresses in a certain subnetwork.  X  Server (WWW activity logs): Keys: (anonymized) source IP address and Web site pairs. value: the number of HTTP requests issued to the Web site from this address. Instances: two consecutive time periods. Selection: A particular Web site.  X  Surnames and OSPD8: keys: all words (terms). value: the number of occurrences of the term in English books digitized by Google and published within the time period [29]. Instances: the years 2007 and 2008. Surnames selection: the 18 . 5  X  10 common surnames in the US. OSPD8 selection: the 7 . 5  X  10 letter words that appear in the Official Scrabble Players Dictionary (OSPD).
 Each of the two instances were Poisson PPS [23] sampled (see Section 2) with different sampling threshold T , to obtain a range of sample sizes. We used both coordinated (shared-seed) and inde-pendent sampling of the two instances.
 We study the quality of the L p p estimates obtained from our estimators. We estimate L p p = P h  X  H RG p ( v ( h )) as the sum over selected keys H of RG p estimates:  X  L p p = P h  X  H  X  RG consider the estimator  X  RG p ( L ) for independent samples (Section 3) and the estimators  X  RG p ( L ) and  X  RG p ( U ) for coordinated samples (Section 4). To apply the estimators, we apply the selection pred-icate to sampled keys to identify all the ones satisfying the predi-cate. The estimators are then computed for keys that are sampled in at least one instance (the estimate is 0 for keys that are not sampled in any instance and do not need to be explicitly computed).
Since all our RG p estimators are unbiased and nonnegative, so estimators using the variance normalized by the square of the ex-pectation, which is the squared coefficient of variation CV
Figure 5 shows the CV 2 of our L p p estimators ( p = 1 , 2 ) as a function of the sampled fraction of the dataset. We can see qual-itatively, that all estimators, even over independent samples, are fraction of the full data set. The estimator  X  RG ( L ) p shared-seed samples outperforms, by orders of magnitude, the esti-mator  X  RG ( L ) p over independent samples. The gap widens for more aggressive sampling (higher T ).

On the IP flows and WWW logs data, there are significant dif-ferences on the values of keys between instances: the L 1 is a large fraction of the total sum of values P h  X  H P i  X  [2] Therefore, for the destIP and Server selections,  X  RG ( U )  X  p on shared-seed samples. On the term count data there is typ-ically a small difference between instances. We can see that for the Surnames and OSPD8 selections,  X  RG ( L ) p outperforms  X  shared seed samples. These trends are more pronounced for the Euclidean distance ( p = 2 ). In this case, on Surnames and OSPD8,  X  p over independent samples outperform  X  RG seed samples. We can see that we can significantly improve accu-racy by tailoring the selection of the estimator to properties of the data. The performance of the U* estimator, however, can signif-icantly diverge for very similar instances whereas the competitive L* estimator is guaranteed not to be too far off. Therefore, when as percentage), sum in each instance i = 1 , 2 : P h v ih there is no prior knowledge on the difference, we suggest using the L* estimator.

The datasets also differ in the symmetry of change. The change is more symmetric in the IP flows and WWW logs data L p + L p  X  whereas there is a general growth trend L p + L p  X  in the term count data. Estimator performance on one-sided distances (not shown) is similar to the corresponding distance estimators.
Distance queries are essential for monitoring, planning, and anomaly and change detection. Random sampling is an important tool for re-taining the ability to query data under resource limitations. We pro-vide the first satisfactory solution for estimating L p distance from sampled data sets. Our solution is comprehensive, covering com-mon sampling schemes. It is supported by rigorous analysis and novel techniques. Our estimators scale well with data size and we demonstrated that accurate estimates are obtained for queries with small support size.
 Acknowledgement. The author is grateful to Haim Kaplan for many comments, helpful feedback, and suggesting the use of the ngrams data. Algorithm 1  X  RG ( U ) p ( S ) if | S | = 0 then return 0 . from hereafter | S | &gt; 0 m  X  max i  X  S v i . m = max( v ) if | S | &lt; r then n  X  0 else n  X  min i  X  S v i . n = min ( S ) if n  X   X  then return ( m  X  n ) p . case: min( v )  X   X  if p  X  1 then . case: min( v )  X   X  and p  X  1 if m  X   X  then . case: max( v )  X   X  , p &gt; 1 if  X  0  X  (0 , 1) then . subcase:  X  0  X  (0 , 1) else . subcase:  X  0 6 X  (0 , 1) from the query support. Top shows p = 1 ( L 1 ) bottom shows p = 2 ( L ).
