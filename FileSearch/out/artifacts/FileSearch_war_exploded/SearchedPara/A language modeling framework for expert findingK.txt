 1. Introduction
Human resources are a valuable asset to an organization because they possess a range of knowledge and expertise which can benefit the organization. However, ensuring that this expertise and knowledge is accessed and utilized is a major chal-ually changing. Within an organization experts may be dispersed geographically, functionally, or structurally, depending on collaboration between individuals no matter where the experts are situated and prevent investment in unnecessary effort, or costly decisions. To help address the challenges involved current research is now being directed at the problem through the given a particular topic ( Yimam-Seid &amp; Kobsa, 2003 ).

The need to find experts may arise for many reasons; an employee may want to obtain some background on a project and find out why particular decisions were made without having to trawl through documentation (if there is any), or they may viduals are new to an organization and need advice and assistance, or when individuals are within a very large and/or dis-tributed organization. By identifying the relevant experts costs may be reduced and better solutions could be obtained. In short, facilitating collaborations through expert finding applications is a fundamental part of ensuring that the expertise within an organization is effectively utilized.
Computer systems that augment the process of finding the right expert for a given problem within an organization are becoming more feasible, largely due to the widespread adoption of technology in organizations coupled with the massive amounts of online data available within the organization. Indeed, an organization X  X  internal and external websites, e-mail, database records, agendas, memos, logs, blogs, and address books are all electronic sources of information which connect employees and topics within the organization. These sources provide valuable information about the employee which can didate experts (employees, for instance) needs to be identified or obtained. This could be performed through using a named entity recognition system, or extracted from current records of employees. Then, the data is mined to extract associations between documents and candidates. These associations can be used to build representations of the candidates X  expertise required, and expert profiling is the task of identifying the topics for which a candidate is an expert.
In this paper we describe the application of probabilistic generative models, specifically statistical language models, to address the expert finding task. In recent years, language modeling approaches to information retrieval have attracted a foundations in statistical theory, the great deal of complementary work on language modeling in speech recognition and nat-ural language processing, and the fact that very simple language modeling applied to retrieval performs very well empiri-cally. The basic idea underlying these approaches is to estimate a language model for each document, and then rank associations between people and documents to build a candidate model and match the topic against this model, and the sec-ond matches the topic against the documents and then uses the associations to amass evidence for a candidate X  X  expertise. These two approaches represent the main search strategies employed for this task.

The main contribution of this paper is the introduction of a general probabilistic framework for modeling the expert find-ing task in a principled manner. Using the probabilistic generative framework, we demonstrate how these models can be further extended in a transparent fashion to incorporate the strength of association between candidates and topic terms, along with other forms of evidence. The models are then empirically validated on TREC test collections for expert finding.
We demonstrate that these two approaches deliver state-of-the art performance on the expert finding task, and address the following research questions concerning them: How do the baseline models (without co-occurrence information) for expert finding compare in terms of effectiveness?
How can these models be extended to incorporate co-occurrence information between a topic term and a candidate? We extend the baseline models with term-candidate associations, which are not based on entire documents like in the base-line models, but on the proximity of the term given the candidate in the document.

How do these extended models compare in terms of effectiveness, and how do the different window sizes (used for deter-mining proximity) affect effectiveness?
How does the strength of the associations between a document and a candidate affect performance? These associations play a very important part in the expert finding models as they determine how representative the text is of a candidate X  X  expertise.

Finally, how sensitive are the models to the parameter settings in terms of effectiveness? Since there are a number of parameters that need to be estimated, we check how robust the performance is to their change.

While the framework can be extended in many ways, our aim is not to explore all the possibilities, but rather to show how maintain the generality of the approaches. While the task we address is in the context of expert search, our models do not embody any specific knowledge about what it means to be an  X  X  X xpert. X  Generally, a co-occurrence of a (reference to a) per-son with the topic terms in the same context is assumed to be evidence to suggest  X  X  X xpertise. X  Thus, our approach is very hand can be modeled in terms of associations between topics and entities.

The remainder of the paper is organized as follows. In Section 2 we discuss related work on expertise search. Section 3 is models for addressing this task. A key component of the models, document-candidate associations, is discussed in Section 5.
After detailing our experimental setup in Section 6, we present an experimental evaluation of our models in Section 7.We discuss and analyze our findings in Section 8 and conclude in Section 9. 2. Related work The need for managing the expertise of employees has been identified by the knowledge management field ( Davenport &amp;
Prusak, 1998 ), where early approaches were mainly focused on how to unify disparate and dissimilar databases of the orga-nization into a data warehouse that can easily be mined. Most of this early work was performed by the Knowledge Manage-ment and Computer Supported Cooperative Work community, usually called yellow pages, people-finding systems or exper-tise-management (ECSCW X 99 Workshop, 1999; Yimam, 1996 ). These tools relied on employees to self-assess their skill ee accrued through his or her employment. Consequently, the need for intelligent technologies that can enhance the process of initializing and updating profiles of expert finding was recognized repeatedly; see e.g., ( Becerra-Fernandez, 2000 ).
Yimam-Seid and Kobsa (2003) provide an overview of early automatic expertise finding systems. Many early systems tended to focus on specific document genres only, such as email (Campbell, Maglio, Cozzi, &amp; Dom, 2003 ) or software and software documentation ( Mockus &amp; Herbsleb, 2002 ) to build profiles and find experts. However, the limitations of such and mine published intranet documents along with other heterogeneous sources of evidence which are accessible within the main). One of the first published approaches to overcome these limitations was the P@noptic system (Craswell, Hawking, the organization associated with that candidate. When a query is submitted to the system, it is matched against these rep-
Text REtrieval Conference (TREC) in 2005 (TREC, 2005 ). TREC has provided a common platform for researchers to empirically assess methods and techniques devised for expert finding. In the 2005 and 2006 tracks the following scenario is presented:
Given a crawl of the World Wide Web Consortium X  X  web site consisting of emails, lists, personal homepages, etc, a list of Soboroff, de Vries, &amp; Craswell, 2007 ).

At TREC, it emerged that there are two principal approaches to expert finding X  X r rather, to capturing the association be-tween a candidate expert and an area of expertise X , which have been first formalized and extensively compared by Balog,
Model 1 and Model 2 , respectively X  X ee Section 4. Model 1 X  X  candidate-based approach is also referred to as profile-based method in Fang and Zhai (2007) or query-independent approach in Petkova and Croft (2006) . These approaches build a tex-tual (usually term-based) representation of candidate experts, and rank them based on query/topic, using traditional ad-hoc retrieval models. These approaches are similar to the P@noptic system (Craswell et al., 2001 ). The other type of approach, find documents which are relevant to the topic, and then locate the associated experts. Thus, Model 2 attempts to mimic the we formalize the two approaches using generative probabilistic models. We focus exclusively on these models because they provide a solid theoretical basis upon which to extend and develop theses approaches.

Building on either candidate or document models, further refinements to estimating the association of a candidate with the topic of expertise are possible. For example, instead of capturing the associations at the document level, they may be estimated at the paragraph or snippet level. In this paper, we model both approaches, with document level associations, and then extend each model to handle snippet level associations. The generative probabilistic framework naturally lends it-self to such extensions, and to also include other forms of evidence, such as document and candidate evidence through the de Rijke, 2007 ). For example, Petkova and Croft (2006) propose another extension to the framework, where they explicitly created using pseudo-relevance feedback, and is matched against document and candidate models. Serdyukov and Hiemstra (2008) propose a person-centric method that combines the features of both document-and profile-centric expert finding approaches. Fang and Zhai (2007) demonstrate how query/topic expansion techniques can be used within the framework; the authors also show how the two families of models (i.e., Model 1 and 2) can be derived from a more general probabilistic framework. Petkova and Croft (2007) introduce effective formal methods for explicitly modeling the dependency between the named entities and terms which appear in the document. They propose candidate-centered document representations Balog and de Rijke (2008) introduce and compare a number of methods for building document-candidate associations.
Empirically, the results produced by such models have been shown to deliver state of the art performance (see Balog
Finally, we highlight two alternative approaches that do not fall into the categories above (i.e., candidate or document models). Macdonald and Ounis (2007b) propose to rank experts with respect to a topic based on data fusion techniques, without using collection-specific heuristics; they find that applying field-based weighting models improves the ranking of candidates. Macdonald, Hannah, and Ounis (2008) integrate additional evidence by identifying home pages of candidate ex-perts and clustering relevant documents. Rode, Serdyukov, Hiemstra, and Zaragoza (2007) represent documents, candidates, and associations between them as an entity containment graph, and propose relevance propagation models on this graph for ranking experts. For other models and techniques, we refer the reader to numerous variations proposed during the TREC track (see Craswell et al., 2006; Soboroff et al., 2007 ).

While in this paper we concentrate on the task of expert finding, it is worth noting that other expertise retrieval tasks have also been developed based on these models. For example, Balog and de Rijke (2007a) address the task of expert pro-filing, and Balog and de Rijke (2007b) address the task of finding similar experts. 3. The expert finding task models. Central to the proposed models is the estimation of the probability of the query topic being generated by the can-proaches to expert search lead to different language models, i.e., candidate or document language models, being used to estimate this probability.

To undertake the modeling of the task of searching for experts within an organization, we assume that there is a suffi-prise of a mixture of document types which are indexable and potentially useful in describing the expertise of individuals within an organization. Example document types include home pages, reports, articles, minutes, emails, and so forth. Fur-ther, we also assume that there is a list of candidate experts to whom the repository contains references.
Expert finding addresses the task of finding the right person(s) with the appropriate skills and knowledge:  X  X  X ho are the experts on topic X?  X  Within an organization, there may be many possible candidates who could be experts for a given topic.
For a given query, then, the task is to identify which of the candidates are likely to be an expert, or, put differently: what is the probability of a candidate ca being an expert given the query topic q ? we should be able to obtain a more accurate estimate by invoking Bayes X  Theorem: pert  X  p  X  ca  X  X  : 4), because this probability captures the extent to which the candidate knows about the query topic. The candidate priors, that the priors p  X  ca  X  are uniform, and so make no assumption about the prior knowledge we have about the candidates. 4. Modeling the expert finding task vidual X  X  knowledge according to the documents with which he or she is associated. Previously, this model has been referred uments as possible experts. Because language models for documents are being inferred, this model has previously been re-ferred to as a document model; we will refer to it as Model 2 . 4.1. Using candidate models: models 1 and 1B itions from standard language modeling techniques applied to document retrieval (Ponte &amp; Croft, 1998; Hiemstra, 2001 ).
The model is then used to predict how likely a candidate would produce a query q . Each query term is assumed to be sam-query, such that: where n  X  t ; q  X  denotes the number of times term t is present in query q . Intuitively, the candidate model p  X  t j h likelihood of what kind of things a candidate expert would write about. The presumption is that the more likely a candidate didate model is like asking whether this candidate is likely to write about this query topic.
 modeling, it is standard to smooth with the background collection probabilities: imate p  X  t j ca  X  , we use the documents as a bridge to connect the term t and candidate ca in the following way:
That is, the probability of selecting a term given a candidate is based on the strength of the co-occurrence between a term term t is generated by candidate ca by first generating document d from the set of supporting documents D made up of documents associated with ca : D ca  X f d : p  X  d j ca  X  &gt; 0 g . Alternatively, D various way in which p  X  d j ca  X  can be estimated. Next, however, we discuss the estimation of p  X  t j d ; ca  X  . 4.1.1. Model 1
Our first approach to estimating candidate models assumes that the document and the candidate are conditionally inde-standard maximum-likelihood estimate of the term, i.e., the relative frequency of the term in the document. Now, if we put didate model: where k ca is a general smoothing parameter. Here we set k rences in the documents associated with the candidate. Essentially, the amount of smoothing is proportional to the amount of information available about the candidate (and is like Bayes smoothing with a Dirichlet prior). So if there are very few documents about a candidate then the model of the candidate is more uncertain, leading to a greater reliance on the back-ground probabilities. This, then, is our Model 1, which amasses all the term information from all the documents associated didate X  X  model. 4.1.2. Model 1B
Model 1 assumes conditional independence between the document and the candidate. However, this assumption is quite didate in a particular document. In this case, both the document and the candidate determine the probability of the term.
One natural way in which to estimate the probability of co-occurrence between a term and a candidate, is by considering the proximity of the term given the candidate in the document, the idea being that the closer a candidate is to a term the that the candidate X  X  name, email, etc. have been replaced within the document representation with a candidate identifier, which can be treated much like a term, referred to as ca . The terms surrounding either side of ca form the context of the candidate X  X  expertise and can be defined by a window of size w within the document. For any particular distance (window size) w between a term t and candidate ca , we can define the probability of a term given the document, candidate, and distance: probability of a term given the candidate and document is estimated by taking the sum over all possible window sizes W : such that P w 2 W p  X  w  X  X  1.

The final estimate of a query given the candidate model using this window-based approach is shown in Eq. (9):
This is Model 1B, which amasses all the term information within a given window around the candidate in all the doc-uments that are associated with the candidate and uses this to represent that candidate. Then, as in Model 1, the prob-are possible which would lead to variations of candidate-based models. For instance, if the type of reference to the can-didate was known i.e., author, citation, etc., then the appropriate extraction could be performed. However, we leave this for further work. 4.2. Using document models: models 2 and 2B
Instead of creating a term-based representation of a candidate as in Models 1 and 1B, the process of finding an expert can queried, then the candidates associated with the documents are considered as possible experts. The document acts like a  X  X  X idden X  variable in the process which separates the querying process from the candidate finding. Under this model, we can think of the process of finding an expert as follows. Given a collection of documents ranked according to the query, we examine each document and if relevant to our problem, we then see who is associated with that document and consider this as evidence of their knowledge about the topic.
 Thus, the probability of a query given a candidate can be viewed as the following generative process: Let a candidate ca be given.
 Select a document d associated with ca (i.e., generate a supporting document d from ca ).
 From this document and candidate, generate the query q , with probability p  X  q j d ; ca  X  .

By taking the sum over all documents d 2 D ca , we obtain p  X  q j ca  X  . Formally, this can be expressed as
Assuming that query terms are sampled identically and independently, the probability of a query given the candidate and the document is:
By substituting Eq. (11) into Eq. (10) we obtain the following estimate of the document-based model:
Similarly to Models 1 and 1B, there are two ways of estimating p  X  t j d ; ca  X  , which are discussed next. 4.2.1. Model 2 We can compute the probability p  X  q j ca  X  by assuming conditional independence between the query and the candidate.
Here, p  X  t j d ; ca  X  p  X  t j h d  X  , hence, for each document d a document model h the document model h d is:
By substituting p  X  t j h d  X  for p  X  t j ca ; d  X  into Eq. (12), the final estimation of Model 2 is: where k d is set proportional to the length of the document n  X  d  X  , such that k representation of the candidate X  X  knowledge, Model 2 mimics the process of searching for experts via a document collection. ciated candidate is an expert. After amassing all such evidence, possible candidates are identified. 4.2.2. Model 2B resorting to the conditional independence assumption. To estimate the probability of co-occurrence between a term and a dow sizes, as defined in Eq. (8). This creates a localized representation of the document given the candidate (or candidate biased document model) which is used in the querying process. The final estimate of a query given the candidate using this approach is shown in Eq. (15):
Before turning to an experimental evaluation of the models introduced in this section, we need to explain how we estimate document-candidate associations, p  X  d j ca  X  . 5. Establishing document-candidate associations described using this document d . For Models 2 and 2B (Section 4.2), it provides a ranking of candidates associated with a given document d , based on their contribution made to d .

If we consider the probability p  X  d j ca  X  from a different point of view by invoking Bayes X  Theorem, we obtain:
This decomposition explicitly shows how prior knowledge about the importance of the documents can be encoded within types of documents can be favored over others. Also, prior knowledge with respect to a candidate being an expert can be mation of p  X  ca j d  X  .

We assume that all candidates X  occurrences (name, email address, etc.) have been recognized in documents, and n  X  ca ; d  X  denotes the number of times candidate ca is present (mentioned) in document d . Below, we distinguish between two ways of converting these raw frequencies into probabilities. 5.1. The boolean model of associations
Under the boolean model to establishing document-candidate associations, associations are binary decisions; they exist if the candidate occurs in the document, irrespective of the number of times the person or other candidates are mentioned in that document. Thus, we simply set using this boolean model. 5.2. A frequency-based approach that it indicates the strength, and not only the presence, of the association between candidate ca and document d .We approach it by adopting the popular TF.IDF weighting scheme commonly used within the vector space retrieval model. The also incorporating the candidate X  X   X  X  X eneral importance X  (i.e., candidates who occur only in a handful of documents will be compensated with higher IDF values). To avoid document length normalization problems, we use a  X  X  X ean X  document repre-terms are filtered out. Formally, this can be expressed as: de Rijke (2008) for an extensive study on document-candidate associations. 6. Experimental setup
Now that we have detailed our models, we present an experimental evaluation of our models. We specify our research of smoothing parameters before, finally, addressing our research questions in Section 7. 6.1. Research questions We address the following research questions: How do our expert finding models perform compared to each other? That is, how do Model 1 and Model 2 compare?
What are optimal settings for the window size(s) to be used in Models 1B and 2B? Do different window sizes lead to dif-ferent results, in terms of retrieval effectiveness?
What is the effect of lifting the conditional independence assumption between the query and the candidate (Model 1 vs. Model 1B, Model 2 vs. Model 2B)?
Which of the two ways of capturing document-candidate associations is most effective: the boolean approach or the fre-quency-based approach? 6.2. Test collection
The document collection used in both years is the W3C corpus (W3C, 2005 ), a heterogenous document repository containing a mixture of document types crawled from the W3C web site. The six different types of web pages were lists (email forum; 198,394 documents), dev (code; 62,509 documents), www (web; 45,975 documents), esw (wiki; 19,605 documents), other (miscellaneous; 3,538 documents), and people (personal homepages; 1,016 documents). The W3C corpus contains 331,037 documents, adding up to 5.7GB.

We used the entire corpus, and simply handled all documents as HTML documents. That is, we did not resort to any spe-cial treatment of document types, nor did we exploit the internal document structure that may be present; instead, we rep-resented all documents as plain text. We removed a standard list of stopwords, but did not apply stemming.
Enterprise 2005 topics (50) are names of working groups of the W3C organization. Members of the corresponding working group were regarded as experts of the topic. The 2006 topics (49) were contributed by TREC participants and were assessed manually. We used only the titles of the topic descriptions. 6.3. Personal name identification within documents. In the TREC setting, a list of possible candidates is given, where each person is described with a unique different choices are also possible (e.g., involving social security number, or employee number instead of, or in addition to, the representations just listed), nothing in our modeling depends on this particular choice.

The recognition of candidate occurrences in documents (through one of these representations) is a restricted (and spe-match types (MT) of person occurrences are identified, based on full name, email address, and various name abbreviations.
Balog et al. (2006) take a similar approach and introduce four types of matching; three attempt to identify candidates by occurrences provided by Zhu (2006) to participants in the TREC Enterprise track. In this preprocessed version of the W3C data set candidates are recognized by various representations using the Aho-Corasick matching algorithm. 6.4. Evaluation metrics
The evaluation measures on which we report for the task of finding experts are mean average precision (MAP) and mean reciprocal rank (MRR) ( TREC, 2005 ). Evaluation scores were computed using the ing we use a two-tailed, matched pairs Student X  X  t-test, and look for improvements at significance levels 0.999. 6.5. Smoothing parameters
It is well-known that smoothing can have a significant impact on the overall performance of language modeling-based prove the estimated document language model. Specifically, as detailed in Sections 4.1.1 and 4.2.1 , we need to estimate a a given candidate (Model 1), or the document length (Model 2). We set the value of b based on the average (candidate/doc-ument) representation length, thus dynamically adjusting the amount of smoothing:
For Model 1 we estimate b  X  b ca as follows: didate, approximated with the number of documents associated with the candidate, times the average document length: while j d j is the average document length.

Our estimation of b  X  b ca ; w for Model 1B is given by For Model 2 we take b  X j d j , i.e., the average document length in the collection.

And, finally, for Model 2B b  X  b d ; w is defined by:
The actual numbers obtained for b by using the choices specified above are reported in Table 1 . 7. Experimental results
We now present the outcomes of our experiments. One by one, we address the research questions listed in Section 6.1. 7.1. Model 1 vs. Model 2
Which of Model 1 and Model 2 is most effective for finding experts? We compare the two models on the 2005 and 2006 association method. The results are presented in Table 2 .
 differences in assessment procedure used. Third, on the 2006 collection Model 2 clearly outperforms Model 1, on all measures.

Moreover, all differences on the 2006 collection are statistically significant. On the 2005 collection, the picture is more subtle: Model 2 outperforms Model 1 in terms of MAP and MRR; however, the difference in MAP scores is not significant.
In conclusion, then, Model 2 outperforms Model 1, significantly so on both the 2005 and 2006 test collection in terms of a precision oriented measure such as MRR. The difference as measured in terms of MAP is significant only on the 2006 test set with its more lenient (human generated) ground truth. 7.2. Window-based models: models 1B and 2B Next we look for performance differences between models based on different window sizes, i.e., for Models 1B and 2B.
Recall that for Models 1B and 2B the candidate-term co-occurrence is calculated for a given window size w , after which a weighted sum over various window sizes is taken (see Eq. (8)). Here, we consider only the simplest case: a single window with size w , thus W  X f w g and p  X  w  X  X  1.

To be able to compare the models, first the optimal window sizes (for MAP and MRR) are empirically selected for each model and topic set. The range considered is w  X  15 ; 25 ; 50 ; 75 ; 100 ; 125 ; 150 ; 200 ; 250 ; 300. document-candidate association method. The MAP and MRR scores corresponding to each window size w are displayed in Fig. 1 . According to the plots on the left-hand side of Fig. 1 , in terms of MAP the ideal window size is between 100 and 250, and
MAP scores show small variance within this range. Model 1B on the TREC 2005 topic set seems to break this pattern of behavior, and delivers best performance in terms of MAP at window size 25. In terms of MRR, however, smaller window sizes tend to perform better on the 2005 collection; this is not suprising, as smaller windows are more likely to generate high-precision co-occurrences.

It is worth pointing out that for both measures (MAP and MRR), for both years (2005 and 2006), and both models (1B and 2B), the difference between the best-performing and worst-performing window size is statistically significant. 7.3. Baseline vs window-based models
What is the effect of lifting the conditional independence assumption between the query and the candidate? That is, what if any, are the performance differences between the baseline models (Model 1 and Model 2) and the window-based models (Model 1B and Model 2B, respectively)? For the window-based models, we use the best performing configuration, i.e., win-mized for MAP, and one based on window sizes optimized for MRR. In all cases we use the boolean document-candidate association method.

Looking at the results in Table 3 we find that, for the MAP-optimized setting, Model 1B improves upon Model 1 in nearly all cases. On the TREC 2005 topic set, the improvement is most noticeable in early precision: MRR +26% vs. MAP +7%; the difference in MRR is significant. On the TREC 2006 topics the advance of Model 1B over Model 1 is even more substantial, achieving as much as 32% improvement in MAP; the differences in MAP and MRR are highly significant. In contrast, the ben-improvements on both measures. On TREC 2006, however, the window-based model (Model 2B) is outperformed by the baseline (Model 2), although the differences are not significant. Finally, Model 2B performs better than Model 1B, but the gap between them is smaller than between Model 2 and 1. None of the differences between Model 1B and 2B are significant (i.e., neither for MAP, MRR, 2005, nor 2006).
 Next we turn to a comparison between the baseline and window-based models based on MRR-optimized settings; see Table 4 . Model 1B improves over Model 1, and in all cases except 2005 (MAP) the improvement is significant. Comparing
Models 2 and 2B we observe a slight improvement in MRR but losses in MAP; none of the differences are significant. And finally, as to the differences between Model 1B and Model 2B, only the difference in MAP on the TREC 2006 topic set is sig-nificant (at the 0.99 level).
 7.4. Association methods
Finally, we turn to a comparison of the two document-candidate association methods that we consider in this paper: the boolean approach and the frequency-based approach. The results are listed in Table 5 .

The frequency-based association method is beneficial for Model 1: on the 2005 test set the improvements are significant, both in terms of MAP and in terms of MRR; while still beneficial on the 2006 test set, only the MAP scores improve signif-icantly. As to Model 2, the usage of frequency-based document-candidate associations leads to increases in MAP scores and some loss in early precision, although none of the differences are significant.

The usage of frequency-based document-candidate associations has a mixed impact on the performance of the windows-based models (Model 1B and Model 2B). The impact on Model 2B is mostly positive, but not significant; in contrast, the
MAP
MAP at the document level (based on the number of other candidates associated with that document); however, in the window-occur in s . As the results for Model 1B show, this may favor snippets whose accuracy for describing candidates is limited. 8. Discussion eter, comment on the generalizability of our approach, compare our performance to other approaches, and conclude by dis-cussing the preferred model. 8.1. Topic-level analysis
We turn to a topic-level analysis of the comparisons detailed in Sections 6 and 7 . Rather than detailing every comparison of approaches from Section 7, we illustrate that section X  X  main findings at the topic level.

To start, we consider the comparison between Model 1 and 2. In Fig. 2 we plot the differences in performance (per topic) between Model 1 and Model 2; topics have been sorted by performance gain. The plots reflect the findings reported in Table 2: In most cases the differences between Model 1 and 2 favor Model 2 (shown as positive). The plots show that Model 1 is topics.
 Now we turn our attention to a topic-level comparison between Model 1 and 1B and between Model 2 and 2B; see Fig. 3 . both in terms of MAP and, even more clearly, in terms of MRR.

Finally, we turn to boolean vs frequency-based document-candidate associations, comparing their impact on top of our on the 2006 topic set). The 2005 topic that takes the biggest hit (in terms of reciprocal rank) by changing from boolean to frequency-based associations on top of Model 2 is no. 18: compound document formats (RR 0.6667); when using Model 1 this very topic X  X  reciprocal rank goes up by .4286 if we replace boolean associations by frequency-based ones, suggesting that different topics may perform best with different model/association settings. 8.2. Parameter sensitivity analysis
Our models involve a smoothing parameter, denoted k ca in case of Model 1 and 1B and k value of k is set to be proportional to the length of the (candidate/document) representation, thus essentially is Bayes aim with the following analysis is to determine 1. to which extent we are able to approximate the optimal value of b ; 2. how smoothing behaves in the two TREC topic sets; and 3. whether MAP and MRR scores display the same behavior (especially, if they achieve their maximum value with the same b ).
 Throughout this subsection we use boolean document-candidate associations.
 vertical line indicates our choice of b , according to Table 1 .
 is underestimated in case of Model 1. Second, with one exception (Model 1, MAP) the curves for the TREC 2005 and 2006 topic sets follow the same general trends, and maximize both MAP and MRR around the same point ( b  X  10 b  X  400 for Model 2). Third, results show small variance, especially in terms of MAP scores, in the range b  X  10 Model 1, and b  X  1 400 for Model 2.

Next, we perform a similar analysis for Model 1B and Model 2B. These models have an extra parameter, the window size,
Model 1B, the difference between the two topic sets is apparent. On the TREC 2005 topic set performance deteriorates for b &gt; 10 2 , while on the TREC 2006 set it is relatively stable throughout a wide range  X  b P 10 close to the best performance for all models/topic sets, with the exception of Model 1B on the TREC 2005 topics. This may be caused by the fact that the TREC 2005 and 2006 topics were created and assessed in a different manner (see Section 6.2). In particular, the TREC 2005 topics are names of working groups, and the assessments ( X  X  X embership of the working group X ) are independent of the document collection.

To conclude this subsection, we include a comparison of the estimated and optimal values of b in terms of MAP and MRR el 1, where our method tends to underestimate b and runs created with optimal settings for b significantly outperform runs are mixed, but on the whole Model 2 and Model 2B are much less sensitive to smoothing than Model 1 and Model 1B. 8.3. Generalizability of the models
While most methods and approaches to expert search introduced since the launch of the TREC Enterprise track in 2005 note that the W3C collection represents only one type of intranet. With only one collection it is not possible to verify whether results and conclusions generalize to other enterprise settings.

To the best of our knowledge at the time of writing there are two more collections publicly available for expertise retrie-val. The CSIRO Enterprise Research Collection (CERC) has been introduced and first used at the 2007 edition of the TREC (2007) . Experimental results reported by Balog et al. (2007) and Balog (2008) confirm that our models generalize well, and
CSIRO Enterprise Research Collection in the overview table (Table 7 ) below. 8.4. Comparison with other approaches
In this subsection we compare our methods to other published approaches. Table 7 highlights the highest scoring results for each. We start our discussion by looking at the top three performing teams from the 2005 X 2007 editions of the TREC Enterprise track.

The top two approaches from 2005 are conceptually similar to our Models 1B and 2B. Fu et al. (2006) use a candidate-centric method that collects and combines information to organize a document which describes an expert candidate (there-fore they call this method  X  X  X ocument reorganization X ). Cao et al. (2006) propose a two-stage language model approach that Bayes X  rule as we do in Eq. (1)). This leads to a different factorization of this probability, p  X  ca j q  X  X  puted based on metadata extraction (for example, recognizing whether the candidate is the author of the document and the query matches the document X  X  title) and window-based co-occurrence. Yao, Peng, He, and Yang (2006) use a document-based method, where the query is constructed from the concatenation of the topic phrase and a person name phrase.
The top three approaches at TREC 2006 all employ X  X  variation of X  X he two-stage LM approach. Zhu et al. (2007) take the documents X  internal structure into account in the co-occurrence model, moreover, they consider a weighted combination of multiple window sizes. Bao et al. (2007) improve personal name identification (based on email aliases) and block-based co-occurance extraction. You, Lu, Li, and Yin (2007) experiment with various weighting methods including query phrase weighting and document field weighting.

It is important to note that the top performing systems at TREC tend to use various kinds of document-or collection-spe-exploited the fact that the 2005 queries were names of working groups by giving special treatment to group and personal pages and directly aiming at finding entry pages of working groups and linking people to working groups. Zhu et al., 2007 employed query expansion that  X  X  X elped the performance of the baseline increase greatly, X  however there are no details disclosed how this expansion was done. You et al. (2007) tuned parameters manually, using 8 topics from the test set.
At TREC 2007 the emphasis was mainly on extracting candidate names (as the list of possible experts was not given in advance). Two out of the top three teams used the same models as they used in earlier years; Fu et al. (2008) used the can-didate-based model proposed in Fu et al. (2006) and Zhu et al. (2008) used the multiple window based co-occurrence model as described in Zhu et al. (2007) . Duan et al. (2008) computed an ExpertRank analogous to PageRank, based on the co-occur-rence of two experts. Further, they computed a VisualPageRank to degrade pages that are unhelpful or too noisy to establish good evidence of expertise.

The second group of entries in Table 7 (below the header  X  X  Other approaches  X ) were discussed in the related work section (Section 2).

The last two rows of the table correspond to our best performing candidate-based model (Model 1B) and document-based model (Model 2). Note that we report the numbers for optimal smoothing settings, but use boolean document-candidate associations. The numbers reported on the TREC 2007 data set (corresponding to the same configuration that is used for baseline results, using automatic smoothing parameter estimation, would be in the top 3 (based on MAP) for 2005 (.2321, using Model 1 and frequency-based document-candidate associations) and in the top 10 for 2006 (.4803, using Model 2 and frequency-based document-candidate associations). 8.5. Preferred model
In the case of Model 2 there is little overhead over document search, which makes it easily deployable in an online appli-associations, can be applied immediately on top an existing document indexed. In practical terms this means that Model 2 can be implemented using a standard search engine with limited effort and does not require additional indexing, but only a lookup/list of document-candidate associations.

Another reason to prefer the document-based Models 2 and 2B over Models 1 and 1B is that they are less sensitive to the smoothing settings and that they perform close-to-optimal with unsupervised smoothing estimations.

As to Model 2 vs Model 2B, the extension of incorporating co-occurrence information marginally increases the perfor-and more efficient ranking at almost negligible cost to the effectiveness.
 Our experiments showed that Model 2 outperforms Model 1 in many conditions, but, given the right smoothing setting,
Model 1B outperforms Model 2 (on the 2005 and 2007 test collections). Moreover, additional features (such as candidate search engine, Model 2 is the preferred choice. However, if a highly effective approach is wanted, one in which additional ranking features may be successfully integrated, perhaps at the expense of efficiency, Model 1B is the model of choice. 9. Conclusions
In this paper we introduced a general framework for people related search tasks. We defined two baseline models, both based on language modeling techniques, that implement various expertise search strategies. According to one model (Model mine the prominent topics in these documents. According to the second model (Model 2) we first identify important doc-uments for a given a topic and determine who is most closely associated with these documents. We found that Model 2 was to be preferred over Model 1, both because of effectiveness reasons X  X n terms of average precision and reciprocal rank X  X nd because Model 2 is easier to implement, only requiring a regular document index. We found that window-based extensions of our baseline models could lead to improved effectiveness, especially on top of Model 1, leading to a model (Model 1B) that outperforms Model 2 in a number of cases. Frequency-based document-candidate associations were espe-cially helpful for Model 1, but also helped improve the effectiveness of Model 2.

The models we have developed in this paper have been shown to be simple, flexible and effective for the expert finding task. These models provide the basic framework which can be extended to incorporate other variables and sources of evidence for better estimates and better performance. However, the models we empirically tested here, did not have any plied to search for people in other settings or to locate other named entities such as places, events, organizations. For by the context in which they are described.

For other future work, we see a number possibilities. First, in our modeling we made a few simplifying assumptions, e.g., by assuming uniform priors on candidates, documents, and document types, and including document fields and structure (see Balog, 2008 ). Reliably estimating such priors and integrating them in the modeling is an obvious next step. Second, an analysis of our estimation of the smoothing parameter shows that our estimation performs very well on one topic set (the TREC 2006 expert finding topics), for all models, but on the 2005 topic set the results were mixed, suggesting that the choice of optimal model and optimal document-candidate association method depends on the query; how can we reli-ably perform topic-dependent model and association method selection? Acknowledgement
Balog and de Rijke were supported by the Netherlands Organisation for Scientific Ripm1203 (NWO) under project number 220-80-001. De Rijke was also supported by NWO under Nos. 017.001.190, 640.001.501, 640.002.501, STE-07-012, 612.061.814, 612.061.815, and by the E.U. IST programme of the 6th FP for RTD under project MultiMATCH Contract IST-033104.
 References
