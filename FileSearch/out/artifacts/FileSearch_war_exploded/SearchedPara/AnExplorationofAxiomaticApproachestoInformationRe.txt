 Existing retriev al mo dels generally do not o er any guar-antee for optimal retriev al performance. Indeed, it is even dicult, if not imp ossible, to predict a mo del's empirical performance analytic ally . This limitation is at least partly caused by the way existing retriev al mo dels are dev elop ed where relev ance is only coarsely mo deled at the level of doc-umen ts and queries as opp osed to a ner gran ularit y level of terms. In this pap er, we presen t a new axiomatic approac h to dev eloping retriev al mo dels based on direct mo deling of relev ance with formalized retriev al constrain ts de ned at the level of terms. The basic idea of this axiomatic approac h is to searc h in a space of candidate retriev al functions for one that can satisfy a set of reasonable retriev al constrain ts. To constrain the searc h space, we prop ose to de ne a retriev al function inductiv ely and decomp ose a retriev al function into three comp onen t functions. Inspired by the analysis of the existing retriev al functions with the inductiv e de nition, we deriv e sev eral new retriev al functions using the axiomatic re-triev al framew ork. Exp erimen t results sho w that the deriv ed new retriev al functions are more robust and less sensitiv e to parameter settings than the existing retriev al functions with comparable optimal performance.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Searc h and Retriev al]: Retriev al mo dels General Terms: Exp erimen tation Keyw ords: Axiomatic mo del, retriev al heuristics, constrain ts, formal mo dels, TF-IDF weigh ting
It has alw ays been a signi can t challenge to dev elop prin-cipled retriev al metho ds that are e ectiv e, robust, and ef-cien t. Although man y information retriev al mo dels have been studied [16, 15, 13, 10, 21, 20, 3, 9, 8], they generally do not o er any guaran tee for optimal retriev al performance. Non-optimal parameter setting easily causes a mo del to per-form poorly . As a result, hea vy parameter tuning is almost alw ays needed to achiev e optimal performance on a partic-ular data set.

In a way, this limitation is caused by the way existing retriev al mo dels are dev elop ed. Most existing mo dels have been dev elop ed based on a \coarse" or \blac k box" appro x-imation of the notion of relev ance at the level of documen ts and queries. Suc h appro ximation con venien tly allo ws us to avoid addressing relev ance directly at a ner gran ularit y level of terms. For example, in the vector space mo del, the notion of relev ance is assumed to be captured through a similarit y measure on a query vector and a documen t vector, whic h al-lows us to con venien tly con vert the retriev al problem to one mainly involving vector space operations [15]. Similarly , in probabilistic retriev al mo dels, including the language mo d-eling approac hes, the notion of relev ance is assumed to be captured through a binary random relev ance variable and a probabilistic mo del is de ned to asso ciate this variable with some (probabilistic) represen tation of documen ts and queries, whic h again allo ws us to avoid directly addressing the notion of relev ance and con venien tly con vert the retriev al problem to one involving de ning and estimating probabilis-tic mo dels [8]. The lack of a detailed mo deling of relev ance mak es it dicult for suc h a mo del to achiev e optimal re-triev al performance. Thus, heuristic mo di cation of a re-triev al form ula and heuristic introduction of additional pa-rameters are often made to impro ve retriev al performance. To avoid suc h heuristic mo di cations, we will need to cap-ture relev ance more directly and at a ner gran ularit y level of terms.

Our previous work [2] sheds some ligh t on how to mo del relev ance more directly . This work sho ws that intuitiv e re-triev al heuristics can be formally de ned as constrain ts on retriev al functions and the empirical performance of a re-triev al function is tigh tly related to how well it satis es these constrain ts. It is also sho wn that none of the analyzed re-triev al form ula can satisfy all the prop osed constrain ts un-conditionally . A very interesting question is thus whether we can systematically searc h for a retriev al function that can satisfy all the desirable constrain ts and dev elop new re-triev al mo dels in this way.

In this pap er, we presen t a new axiomatic approac h to dev eloping retriev al mo dels based on direct mo deling of rel-evance with formalized retriev al constrain ts. The basic idea of this axiomatic approac h is to searc h in a space of can-didate retriev al functions for one that can satisfy a set of reasonable retriev al constrain ts. There are some previous studies along this direction, mostly based on logic [1, 6, 22], but, as far as we kno w, none of these studies has resulted in any e ectiv e retriev al function. Although the general idea is similar, our approac h is completely di eren t from these previous studies both in the space of retriev al functions con-sidered and in the way we specify the constrain ts (axioms).
One challenge in dev eloping operational retriev al mo dels using suc h an axiomatic approac h is how to appropriately de ne the searc h space for retriev al form ulas. To constrain the searc h space, we assume a \bag-of-terms" represen tation of queries and documen ts and prop ose to de ne a retriev al function inductively . Based on suc h a de nition, a retriev al function can be decomp osed into three comp onen ts, referred to as Primitive weighting function , Query growth function and Document growth function , resp ectiv ely. Thus searc h-ing for a good retriev al function boils down to searc hing for a good form ula for eac h of these three functions in our constrained searc h space.

The inductiv e de nition scheme pro vides a common ba-sis to analytically compare di eren t retriev al functions. We compare and analyze three represen tativ e existing retriev al functions in this way and nd that they share some com-monalities in their primitiv e weigh ting functions and query gro wth functions, but they generally di er in the documen t gro wth function. The analysis pro vides an interpretation of the three comp onen t functions of the inductiv e de ni-tion scheme. We further generalize these speci c comp o-nen t functions to deriv e new retriev al form ulas within the axiomatic framew ork. We use the intuitiv e retriev al con-strain ts prop osed in [2] and the technique of exploratory data analysis [4, 5] to constrain the choices for the three comp onen t functions and deriv e sev eral new retriev al func-tions. We implemen t and test these new functions with a num ber of represen tativ e test sets. The exp erimen t results sho w that the deriv ed new functions are more robust and less sensitiv e to parameter settings than the existing retriev al functions with comparable optimal performance.

The rest of the pap er is organized as follo ws. We rst presen t the axiomatic framew ork in Section 2. In Section 3, we deriv e new retriev al functions based on our axiomatic framew ork. We rep ort exp erimen t results for these new functions in Section 4 and conclude in Section 5.
To de ne an axiomatic framew ork for information retriev al, we need to de ne (1) a search space of possible retriev al func-tions; and (2) a set of retrieval constr aints that any reason-able retriev al function should satisfy . The assumption is that if a retriev al function satis es all our constrain ts, the func-tion would likely be e ectiv e empirically . The searc h space must be large enough to include e ectiv e retriev al functions, yet small enough for searc h. So there is clearly a trade-o . For the constrain ts, ideally , we want to have as man y constrain ts as possible so that we can e ectiv ely prune the searc h space and nd an e ectiv e function more easily . In realit y, however, as we add more and more constrain ts, we may introduce bias and some constrain ts may be too strong or even con tradictory . So there is also a tradeo . We now discuss how we mak e these tradeo s.
Since a retriev al function is de ned on a documen t and a query , we rst need to de ne our documen ts and queries. Follo wing the curren t retriev al mo dels, we assume that both documen ts and queries are \bags of terms". To mak e our framew ork as general as possible, we include all the scoring functions de ned on a bag-of-terms represen tation of docu-men ts and queries in our function space.

Formally , let T be the set of all terms. Let query Q = f q 1 ; :::; q n g and documen t D = f d 1 ; :::; d m g be two bags of terms, where q i ; d i 2 T , and it is possible that q i = q d = d j even if i 6 = j . Our goal is to de ne a scoring func-tion S ( Q; D ) 2 &lt; . To help us searc h through this function space ecien tly and de ne meaningful constrain ts on the retriev al functions, we prop ose to de ne a retriev al function inductiv ely.

We start with the base case, when both the documen t and query con tain only one term.
 Base Case: Assume Q = f q g and D = f d g .

S ( Q; D ) = f ( q; d ) = weig ht ( q ) = weig ht ( d ) q = d Function f gives the score of a one-term documen t and a one-term query and will be referred to as the Primitive weighting function . It rew ards the documen t with a score of weig ht ( q ) when d matc hes q and gives it a penalt y score of penal ty ( q; d ) otherwise. We will reasonably assume that 8 t 2 T , weig ht ( t ) &gt; 0 and 8 q; 8 d 6 = q; penal ty ( q; d ) &lt; weig ht ( q ).

In the inductiv e step, we consider the case when a docu-men t or a query con tains more than one term.
 Inductiv e Step: 8 Q; D suc h that j Q j 1 and j D j 1, (1) Assume Q 0 = Q [ f q g .
 (2) Assume D 0 = D [ f d g .

Function g describ es the score change when we add a term to a query , and is called the Query growth function . When a new term q is added to a query Q , the score of any documen t for the new query (i.e. S ( Q [ f q g ; D )) would be mainly deter-mined by the score of the documen t for the old query (i.e. S ( Q; D )), the score of the documen t for the added query term (i.e. S ( f q g ; D )), and any possible score adjustmen t determined by D , Q and q . Similarly , function h describ es the score change when we add a term to a documen t, and is called the Document growth function .

Unfortunately , without appropriate constrain ts on the com-ponen t functions f , g , and h , the inductiv e de nition above does not necessarily de ne a function since S ( Q; D ) may be computed in multiple ways dep ending on how we construct Q and D . Speci cally , the value S ( Q; D ) may be sensitiv e to the order of adding terms to the query and/or the docu-men t. The follo wing theorem gives a set of necessary and sucien t conditions under whic h S ( Q; D ) can be guaran teed to be a function.
 Theorem 1 S ( Q; D ) is a function if and only if all the fol-lowing conditions holds. due to the addition of term d to documen t D , and q ( q; D; Q ) = S ( Q [ f q g ; D ) S ( Q; D ) is the score change due to the addition of a term q to query Q .
 Intuitiv ely, these three conditions simply require that S ( Q; D ) remains the same no matter in whic h order the terms are added to the query and the documen t when we compute it. The pro of of this theorem involves a straigh t-forw ard mathematical induction and is omitted due to the space limit.
Another imp ortan t comp onen t in the axiomatic frame-work is the retriev al constrain ts. In our previous work [2], we prop osed six retriev al constrain ts that any reasonable re-triev al form ula should satisfy . However, two of them (i.e., LNC2 and TDC) do not app ear to be general enough for a general framew ork. So we only use the other 4 retriev al con-strain ts in our axiomatic framew ork, whic h are re-formalized as follo ws: Constrain t 1: 8 Q; D and 8 d 2 T , if d 2 Q , S ( Q; D [ f d g ) &gt; This constrain t says that adding one query term to a doc-umen t must increase the score. It corresp onds to the con-strain ts TF-LNC and TF C1 in [2].
 Constrain t 2: 8 Q; D and 8 d 2 T , if d = 2 Q , S ( Q; D [ f d g ) &lt; This constrain t ensures that adding a non-query term to a documen t must decrease the score. It is essen tially the LNC1 constrain t in [2].
 Constrain t 3: 8 Q; D and 8 d 2 T , if d 2 Q , d ( d; D; Q ) &gt; This constrain t says that the amoun t of increase in the score due to adding a query term d to a documen t must decrease as we add more and more d 's. It is similar to the TF C2 constrain t de ned in [2].
In order to obtain some sense about the relationship be-tween the existing retriev al functions and this new way of de ning a retriev al function, we rewrite 3 represen tativ e existing retriev al functions using the inductiv e de nition schema. The follo wing notations will be used in this sec-tion. C D t ( C Q t ) is the coun t of term t in documen t D (query Q ). N is the total num ber of documen ts in the collection. df ( t ) is the num ber of documen ts con taining term t . j D j is the length of documen t D . avdl is the average documen t length in the collection. p ( t j C ) is the probabilit y of a term t given by the collection language mo del [23].
PN is a represen tativ e of e ectiv e vector space retriev al functions with the follo wing scoring form ula [18, 17]:
After rewriting, we have The decomp osition results sho w that weig ht ( q ) is related to an IDF-lik e discriminativ e value of q , while h () app ears to implemen t documen t length normalization and TF normal-ization.
Okapi is an e ectiv e retriev al form ula represen ting the classical probabilistic retriev al mo del [11, 12]: k 1 (between 1.0-2.0), b (usually 0.75), and k 3 (between 0-1000) are constan ts.

After rewriting, we have It sho ws again that weig ht ( q ) is an IDF-related value of q . And h () again implemen ts length normalization and TF nor-malization, though the form of the form ula is more complex than in the case of PN. DP is an e ectiv e langauge mo deling approac h [23]:
After rewriting, we have
The results sho w that weig ht ( q ) is yet again an IDF-related value of q . However, penal ty () is not equal to 0 as in the previous two metho ds; instead, it is a negativ e value, whic h also con tributes documen t length normaliza-tion. Function h tak es yet another complex form, involving not only TF and length normalization but also the IDF-lik e variable p ( d j C ). Function is playing a role for additional score adjustmen t due to the addition of the terms.
The rewriting exercise pro vides some interesting insigh ts on how we may deriv e new functions. (1) All the instan tia-tions of weig ht ( q ) are related to an IDF-lik e discrimination value of q . However, weig ht ( q ) in Okapi can be smaller than penal ty ( q; d )(=0), whic h causes poor performance on verb ose queries. (2) There are two ways to implemen t doc-umen t length normalization in our framew ork. The rst metho d is to set penal ty ( w; q ) &lt; 0, whic h would penalize any non-query terms in the documen t, as in the DP metho d. The second is to use documen t length related parameters to adjust the documen t relev ance score as in PN (i.e. 1 () and ()) and Okapi (i.e. T F LN ()). (3) It sho ws three possible ways to instan tiate the documen t gro wth function, whic h we summarize below in a more general form.

In this section, we study how to instan tiate eac h comp o-nen t function in the framew ork to deriv e a new reasonable retriev al function.
The primitiv e weigh ting function has two comp onen t func-tions: weig ht ( q ) and penal ty ( q; d ). As discussed in the pre-vious section, the decision on penal ty ( q; d ) a ects the instan-tiation of the documen t gro wth function, so we will discuss it later together with the documen t gro wth function.
We consider two ways to de ne weig ht ( q ), both connected with how the matc hing of q con tributes to relev ance. The rst is to de ne it as the point-wise mutual information between the presence/absence of q in a documen t ( p ( occ )) and whether the documen t is relev ant to the given query ( p ( rel )).
The second is to de ne it as the conditional probabilit y that a documen t is relev ant if q occurs in the documen t: p ( occ ) can be estimated as p ( occ ) = df ( q ) N . If the rele-vance information of documen ts is available (e.g. through feedbac k from the users), it would also be easy to estimate p ( occ j rel ) and p ( rel j occ ), so weig ht ( q ) can be computed ac-cordingly . However, when we have no or insucien t rele-vance information about documen ts, it would be hard to compute weig ht ( q ) directly . One possible solution is to em-ploy techniques of exploratory data analysis [4, 5]. The ba-sic idea is to nd some empirical function that can explain the relationship between suc h unkno wn variables and some kno wn variables well on some training data. For example, we may relate weig ht ( q ) to the kno wn variables p ( occ ) and try to nd a function of p ( occ ) that can appro ximate weig ht ( q ) well. Speci cally , for a given data set, we compute weig ht ( q ) (according to Equation (1) or (2)) and p ( occ ) for eac h query term. Since the variance of these variables is large, we follo w [4] and group the data points together in bins. We average both kno wn and unkno wn variables for a bin to obtain a \pseudo data point". Finally , we plot the graph for these two variables (i.e. weig ht ( q ) vs. p ( occ )) for every pseudo data point. Figure 1: Plot of weig ht ( q ) (computed using Equation 1)
In the left plot of Figure 1, we plot the weig ht ( q ) com-puted using Equation(1) against log ( P ( occ )) = log df ( q ) some AP data set. (The plots on other data sets are similar.) There app ears to be a negativ e linear correlation between them. Thus we assume weig ht ( q ) = a log df ( q ) N + b , where a and b are constan ts. Visually examining sev eral suc h plots on di eren t data sets indicates a = 1 and b = 0 may be a good appro ximation. That is weig ht ( q ) = log N df ( q ) will be referred to as LOG weighting function . Note that the LOG weigh ting function is just the typical IDF [19, 14].
The righ t plot in Figure 1 sho ws how log( weig ht ( q )), where weig ht ( q ) is computed using Equation(2), is related to log We also see a negativ e linear correlation between them. Again, as a crude appro ximation, we may assume where k is a parameter. We call this form ula EXP weighting function .
The analysis of existing retriev al functions rev eals that their query gro wth functions are quite similar and of a rel-ativ ely simple form. The sligh tly more complicated form of Okapi has not sho wn any clear bene t in our prelimi-nary exp erimen ts. Thus we x our choice of query gro wth function to the follo wing simple form: S ( Q [ f q g ; D ) =
We generalize the documen t gro wth functions of the three existing retriev al functions and explore how to generate some interesting alternativ e choices.
The generalized form of the PN documen t gro wth function is whic h is a weigh ted linear com bination of S ( Q; D )and S ( Q; f d g ) with the weigh ts dep ending on three unkno wn functions (i.e. ; 2 ; ).

We can easily reco ver PN with the follo wing instan tia-tions:
We now discuss how we may exploit our inductiv e de ni-tion scheme and retriev al constrain ts to nd some interesting alternativ e instan tiations of 1 , 2 and .
 First, we need to mak e sure that S ( Q; D ) is a function. Applying theorem 1, we nd that condition (1) and con-dition (3) can be satis ed unconditionally , but in order to satisfy condition (2), the follo wing two equations must hold.
Next, the analysis of constrain t 1 suggests that
Since S ( Q; D ) is roughly a sum of weigh ts over all matc hed Thus we may consider the follo wing someho w stronger, but simpler condition; if Equation (6) holds, we may exp ect Equation (5) to hold for most documen ts.
Furthermore, Constrain t 2 implies that
One way to satisfy this condition is to let 1 ( k ) = f ( k ) where f ( k ) decreases when k increases. A natural simple choice for f ( k ) is f ( k ) = a k + b , where a &gt; 0. In this case, 1 ( k ) = a k + b a ( k +1)+ b . According to Equation (3), (6) is equiv alen t to 1 + b a &gt; avdl . Thus we can assume = avdl=s and 0 &lt; s &lt; 1. So, we have
Finally , it follo ws from the analysis of Constrain t 3 that ( C D d ) decreases when C D d increases. It is easy to sho w that (0) = 1. So 8 x; ( x ) 1. Lea ving the study of a better form of ( C D d ) for our future work, we can simply tak e the corresp onding comp onen t from the piv oted normalization form ula. That is, ( k ) = ln (1+ ln ( k +1)) ln (1+ ln ( k )) ; k 1 and (0) = 1.

Using this documen t gro wth function together with penal ty ( q; d ) = 0, we obtain the follo wing retriev al function turns into PN with parameter s 0 . The constrain t 0 s 1 is equiv alen t to 0 s 0 0 : 5, whic h is a narro wer range than the full range (0 ; 1) allo wed by the standard PN metho d. Empirical study [2] sho ws that the optimal value of s 0 is alw ays smaller than 0.4, thus the new form ula we deriv ed using the axiomatic framew ork has a more reasonable pa-rameter range than the original PN, whic h is due to the introduction of the extra constrain t Equations (5) and (6).
The generalized form of the Okapi documen t gro wth func-tion is
It di ers from the documen t gro wth function of PN in that the weigh ts of linear com bination are related to not only the documen t length but also the term coun t and we also have just one unkno wn function (i.e. ) to instan-tiate. The follo wing instan tiation clearly reco vers Okapi.
We now explore how to nd any interesting alternativ e instan tiations of ( x; y ), where x is related to the documen t length and y is related to the term coun t. Again, we chec k all the constrain ts to see whether they can pro vide us more clues about .

All the three conditions in Theorem 1 are satis ed un-conditionally . Constrain t 1 indicates that ( x + 1 ; y + 1) &gt; ( x; y ). The analysis of constrain t 2 sho ws that ( x +1 ; y ) &lt; ( x; y ), whic h means ( x; y ) decreases when x increases. From these, it follo ws that ( x + 1 ; y + 1) &gt; ( x + 1 ; y ), i.e., ( x; y ) increases as y increases. Constrain t 3 indicates that ( x; y ) should be a sublinear function w.r.t. y . From the Okapi instan tiation, it seems that ( x; y ) con trols how to pe-nalize a long documen t as well as how to normalize the term frequency for every term. We consider a sligh tly more gen-eral form than the Okapi instan tiation, ( x; y ) = y ( ax + b )+ y The analysis of constrain t 1 implies that b a &gt; avdl r; (0 &lt; r &lt; 1) and 0 &lt; b 1. One way to satisfy this condition is to set a = s=av dl and b = s , where 0 &lt; s 1. Using this documen t gro wth function together with penal ty ( q; d ) = 0, we obtain
Di eren t from PN and Okapi, the DP metho d partially implemen ts length normalization through setting a negativ e value to penal ty ( q; d ). The generalized form of the DP doc-umen t gro wth function is
Setting penal ty () = ln (1 + 1 ) and setting and as follo ws would reco ver the DP function.
To seek for any interesting alternativ e instan tiations, we follo w DP and set penal ty ( q; d ) = c where c is a negativ e constan t. We consider a simple case where () = 0 and () is only related to C D d and C Q d as follo ws. d is a query term (i.e. C Q d &gt; 0) , the change of term fre-quency is captured by ( C D d ). As before, the function can be constrained by Constrain t 3. We use the same im-plemen tation of () in PN. On the con trary , when d is a non-query term (i.e. C Q d = 0), we simply assume that the score change due to the addition of d is alw ays the same. To balance the score between the rew ard and the penalt y, we assume c = s=av dl , where 0 s 1. We obtain the follo wing hybrid variation of PN and DP:
Com bining all the choices, we obtain the follo wing six new retriev al functions.

Previous works [14, 24] have also attempted to vary com-ponen ts to form various retriev al form ulas in a someho w arbitrary way. Our framew ork pro vides more guidances on how to choose the comp onen ts and can guaran tee the per-formance of the deriv ed functions in some sense.
In this section, we exp erimen tally compare the deriv ed new retriev al functions with the three existing ones. We also examine their parameter sensitivit y. Our exp erimen t results sho w that the new functions can generally achiev e compara-ble optimal performance with the three existing functions, but are more robust and less sensitiv e to the parameter set-tings.
To cover di eren t types of queries and documen t sets, we follo w [2, 23] and conduct our exp erimen ts over six data sets: news articles (AP), technical rep orts (DOE), govern-men t documen ts (FR), the Web data used in TREC8 (W eb), the ad hoc data used in TREC7 (Trec7) and the ad hoc data used in TREC8 (Trec8). For eac h query , we try di er-ent types of queries: short-k eyw ord(SK), short-v erb ose(SV), long-k eyw ord(LK), and long-v erb ose (LV). The prepro cess-ing of documen ts and queries only involves stemming with the Porter's stemmer. We inten tionally did not remo ve stop words for two reasons: (1) A truly robust mo del should be able to discoun t the stop words automatically; (2) Remo v-ing stop words would introduce at least one extra parameter (e.g. the num ber of stop words) into our exp erimen ts. We set k in the EXP weigh ting function to 0.35 based on some preliminary exp erimen ts.
To compare the optimal performances of the six deriv ed functions, we vary the parameter value from 0 to 1.0 and select a best run with the highest average precision for eac h function on eac h data set. We compare the average pre-cisions of these best runs in Table 1. We mak e the fol-lowing observ ations. First, the optimal performances of all the six functions are comparable, and F1-LOG (i.e., PN) is relativ ely worse than others. Second, the functions with EXP weigh ting usually perform better than those with LOG weigh ting for verb ose queries, but worse for keyw ord queries. Finally , the functions with F2 usually performs better than those with F1 and F3. The parameter sensitivit y study also sho ws that F2-EXP app ears to be more stable than others. So, it app ears that F2-EXP is overall a better choice than others. Belo w we compare its performance with existing retriev al functions. We compare the performance of one deriv ed function (i.e.F2-EXP) with PN, Okapi, and DP . Due to the poor perfor-mance of the original Okapi on verb ose queries, we also com-pare with the mo di ed Okapi (i.e., Okapi with traditional IDF) [2]. Since other retriev al functions all have just one pa-rameter, we set k 1 = 1 : 2, k 3 = 1000 and only vary the value of b in Okapi and mo di ed Okapi. For every metho d, we randomly sample 12 values within the range of the parame-ter. Skewed samples with 25% or more of the values falling into an interv al of 0.1 are discarded. For eac h metho d on eac h collection, we select the top/b ottom 25-p ercen tile runs (i.e., 3 runs with the best/w orst average precision) from the 12 runs for comparison.
 The results are sho wn in Table 2 (top 25-p ercen tile) and Table 3 (bottom 25-p ercen tile). F2-EXP0.5 is F2-EXP with a xed value of 0.5 for s . From Table 2, we see that the optimal performance of F2-EXP is quite comparable with that of all the existing retriev al form ulas. Even the perfor-mance of the deriv ed form ula with a xed parameter value (i.e. F2-EXP-0.5) is also comparable, demonstrating the robustness of this axiomatic retriev al function. The robust-ness is further con rmed in Table 3, where we see that F2-EXP mostly outp erforms others and in Table 4, where we see that the average variance of all 12 runs for F2-EXP is mostly smaller than for all others. It is interesting to note that mo di ed Okapi alw ays performs better than F2-EXP on AP; indeed, AP and DOE seem to be the only data sets where F2-EXP has not sho wn adv antages. Further analysis and exp erimen ts are clearly necessary to better understand this. We compare the parameter sensitivit y between F2-EXP , PN Okapi, and mo di ed Okapi. We did not include DP because its parameter is in a di eren t scale, but it is kno wn that its performance is sensitiv e to the smo othing parameter [23]. We vary the parameter from 0 to 1. The results on TREC7 are sho wn in Figure 2. The plot demonstrates the stabilit y of F2-EXP , whic h we have also observ ed in the plots for other data sets and query types.
Figure 2: Performance Sensitivit y on Trec7-L V
In this pap er, we presen t a novel axiomatic framew ork for dev eloping information retriev al mo dels, in whic h the no-tion of relev ance is directly captured by retriev al constrain ts. The framew ork consists of an inductiv e scheme for function de nitions and a set of formalized retriev al constrain ts. Our work can be regarded as an extension of some previous study [2] to seek for a reasonable retriev al function that can satisfy all the desired retriev al constrain ts.

The inductiv e de nition scheme pro vides a common ba-sis to analytically compare di eren t retriev al functions. We compare and analyze three represen tativ e existing retriev al functions in our framew ork and nd that while the three functions implemen t similar heuristics, they implemen t them in di eren t ways.
We further deriv e new retriev al functions using the ax-iomatic framew ork. We use both intuitiv e retriev al con-strain ts and exploratory data analysis to guide us in instan-tiating the three comp onen ts of the inductiv e de nition and obtain sev eral new retriev al functions. We evaluate these new retriev al functions on a num ber of represen tativ e test sets. The exp erimen t results sho w that the deriv ed new functions are more stable than the existing retriev al func-tions with comparable optimal performance.

The axiomatic framew ork opens up man y new possibili-ties for exploring and dev eloping principled retriev al mo dels based on direct mo deling of relev ance through constrain ts. This pap er has only moved a very small step in this direc-tion. There are man y interesting future researc h directions. First, we have used only three basic constrain ts when deriv-ing new retriev al functions. Presumably , with more reason-able constrain ts, the deriv ed functions will be more special-ized and performing better. It would be interesting to add additional constrain ts, including the two constrain ts that we have not used from [2]. Second, it would be very interesting to study what constrain ts are appropriate for mo deling rele-vance/pseudo feedbac k, whic h often leads to signi can t per-formance impro vemen t over a simple non-feedac k retriev al function. Finally , it would be interesting to study theoreti-cal prop erties of retriev al functions along a similar line to a related work on clustering algorithms [7].
This material is based in part upon work supp orted by the National Science Foundation under award num ber IIS-0347933. We thank Tao Tao and three anon ymous SIGIR review ers for their useful commen ts.
