 The Entity Linking (EL) problem consists in automatically linking short fragments of text within a document to en-tities in a given Knowledge Base like Wikipedia. Due to its impact in several text-understanding related tasks, EL is an hot research topic. The correlated problem of devis-ing the most relevant entities mentioned in the document, a.k.a. salient entities (SE) , is also attracting increasing in-terest. Unfortunately, publicly available evaluation datasets that contain accurate and supervised knowledge about men-tioned entities and their relevance ranking are currently very poor both in number and quality. This lack makes very dif-ficult to compare different EL and SE solutions on a fair basis, as well as to devise innovative techniques that relies on these datasets to train machine learning models, in turn used to automatically link and rank entities.

In this demo paper we propose a Web-deployed tool that allows to crowdsource the creation of these datasets, by sup-porting the collaborative human annotation of semi-structured documents. The tool, called Elianto , is actually an open source framework, which provides a user friendly and re-active Web interface to support both EL and SE labelling tasks, through a guided two-step process.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.5.2 [ Information Interfaces and Presentation ]: User Interfaces
In the latest years many research efforts have been spent in the Entity Linking (EL) task applied to text documents. The task, also known as Wikification , has been introduced by Mihalcea and Csomai [8], and consists in finding small fragments of text (hereinafter named interchangeably spots or mentions ) referring to an entity (identified by a URI) that is listed in a Knowledge Base like Wikipedia. Each Wikipedia article is considered as an entity , and the the title of an article (or the anchor text of the links that point to the article) are possible mentions of the same entity. As an example, consider a short text document containing the sentence:  X  X aradona played his first World Cup tournament in 1982 when Argentina played Belgium in the opening game of the 1982 Cup in Barcelona X  .

The goal of an EL system is to: i) spot the fragments of text referring to entities, e.g., Maradona , Argentina , or Belgium , and ii) link each spot to the referred entity, e.g., link the spot Maradona to the corresponding Wikipedia page http://en.wikipedia.org/wiki/Diego_Maradona . It is worth remarking that the second step is not trivial due to mentions X  ambiguity: in our example the mention Belgium does not refer to its most common sense, i.e., the country, but rather to its national football team, i.e, http://en.wikipedia.org/ wiki/Belgium_national_football_team . Figure 1 illustrates the result of a correct EL process applied to our running exam-ple.

The Salient Entity (SE) [2, 4, 10] discovery task can be thought as a subsequent step to EL. Given the entities men-tioned in the document, only the most relevant ones should be returned. In our example, the most relevant entities are surely the ones linked to mentions Maradona and 1982 Cup .

The SE discovery task may have a strong impact in the evaluation of EL algorithms. Let us consider two EL al-gorithms applied to the text of Figure 1, where the former produces the annotations for Maradona and 1982 Cup , while the latter produces the annotations for Belgium and Barcelona . In terms of precision, the two algorithms ap-pear to be equivalent, since both can correctly detect two correct entities. However, the former should be considered better than the latter, since it detected two entities that are more salient for the document. Indeed, this is a well-known issue in EL evaluation: although some algorithms are not able to discover SEs, they still score high just because they can match correctly trivial mentions, like the city referred to in the dateline of a news.

Unfortunately, publicly available benchmark datasets that contain accurate supervised knowledge about mentioned en-tities and their saliency ranking are currently very poor, both in number and quality. The importance of such data is two-fold. On the one hand, as discussed above they are necessary for a sound comparison of different EL techniques. On the other hand, these datasets can also be used to train played his first World Cup tournament machine learning models, in turn used to automatically link and rank entities.

This demo paper just focuses on the generation of hu-man annotated datasets for the EL/SE tasks. Specifically, we present Elianto 1 , an open-source Web-deployed frame-work that crowdsources the production of publicly available rank-enriched datasets for EL and SE tasks with the goal of involving the research community in producing publicly available rank-enriched datasets for EL and SE tasks. It sup-ports human labelling of semi-structured documents through a guided two-step process. In the first step, entities men-tioned in a given document are annotated by users. In a second step, such entities are ranked on the basis of the per-ceived relevance/saliency. Note that, such two-step process implicitly forces the user to evaluate twice the entities she annotated, and correct them if needed.

It is worth mentioning that Elianto is back-ended by the spotting module of Dexter [3], a framework that provides all the tools needed to develop any EL technique. Elianto thus exploits Dexter to identify a set of spots in the text documents to annotate, along with a list of candidate enti-ties for each spot. This makes easier for the users the heavy job of identifying spots and associated entities in text doc-ument. In addition, Elianto allows users to provide new spots (not suggested by the tool) as well as new entities to associate with them.
Elianto (E ntity Li nking An notation To ol) allows to an-notate collections of documents and provides tools for driv-ing multiple users annotations (e.g., minimum number of annotator per document), for inspecting user annotations and for analyzing collection status.

The back-end of Elianto allows the ingestion of docu-ments collection trough a command line program. It al-lows to process semistructured documents and to specify an HTML template describing how documents must be dis-played.

In order to simplify the linking task, candidate spots and entities for each document are pre-computed. In the demo we used our open source system Dexter 2 , but it could be replaced with another system if needed. We are investigating the merging of annotations from different tools, so as to avoid any bias.

Elianto offers a Web interface for annotator users. When annotator logs in to the system, an introduction to the En-tity Linking task and a guideline explaining how to annotate documents are presented to him. Thanks to the login mech-anism, we can monitor users activity. The annotation of
For the source code, details on the framework, or a demo please visit the project webpage at http://dexter.isti.cnr. it/elianto dexter.isti.cnr.it a document is organized in two steps as described in the following.
 In the first step, the document is presented to the annota-tor on the left side of the page: if the system has candidate mentions for the document, these are displayed in red. If the annotator clicks on a mention the list of the candidates entities is displayed on the right side of the page. The an-notator can decide to: i) select one entity from the list ii) add an other candidate entity inserting its Wikipedia url in a form iii) delete the mention if it is wrong or not relevant.
The annotator can also decide to create a new mention just highlighting a piece of text and selecting the option Create Spot from the contextual menu. If the annotator generates a new mention the system can automatically pro-vide a list of candidates entities: this is performed calling a REST service that given a mention returns a list of can-didate entities. In the demo we used Dexter which provides such service.

The mentions that the annotator links to entities are high-lighted in green, while the currently selected mention is high-lighted in yellow. In the contextual menu we provide an op-tion that allows the annotator to extend an annotation to all the occurrences of the same mention in the document. The interface requires to the annotator to annotate (or delete) all the mentions before moving to the step 2.

It is worth to note that we also added the possibility to skip a document if the user thinks that is not a good to annotate e.g. , a web document containing only noisy text, or a tweet with no linkable entities. This was indeed useful in the annotation of the CoNLL dataset in which several documents contain table of sport competition results, i.e., a long list of person names.
 In the second step, the system still presents the document on the left, and on the right the list of the distinct entities associated to the mentions in the previous step. The anno-tator is asked to rate the entities that she selected in the previous step, according to how much they are central to the document story. We defined 4 different ratings:
Top Relevant (3 Stars) if the entity tells you what a
Highly Relevant (2 Stars) we named them satellite enti-
Partially Relevant (1 Star) entities that provide back-
Not Relevant mentions that the annotator linked to an The system architecture is composed of three layers:
The Data Access Object : allows to store and retrieve The Core : implements the logic of the application;
The Interfaces : a REST api that allows external applica-We implemented a web interface using the Angular 3 and Bootstrap 4 frameworks, all the actions are performed call-ing the REST api provided by the server. We also provide a dashboard interface for the user that allows to see the pre-viously annotated documents and to edit them if needed. It is possible to define admin users that can visualize analytics over all the dataset and the user annotations.
The demonstration will present the main functionalities provided by our system. We will illustrate the two tasks on some documents, showing the annotation facilities pro-vided by the interface. During the demo we will present two case studies: the annotation over the AIDA-CoNLL dataset, where we ask the users to perform both the EL and the ES tasks, and the annotation of a dataset extracted from Wikinews 5 , a Wikipedia X  X  project that collects news annotated by the Wikipedia X  X  editors. We will show that is possible to perform just the ES task on this kind of dataset.
Finally we will discuss about how to merge different scores by different users on the same document, in order to create a golden truth: Elianto provides scripts for collecting the user annotations and computing different agreement mea-sures, but there could be different ways to create a golden truth from these data. The project will be released open source. Datasets and impact of entity saliency. One dataset frequently used for the evaluation of EL methods is the IITB dataset 6 by Kulkarni et al. [7]. The dataset contains 103 documents, annotated by 10 distinct human annotators. The majority of the documents are annotated by only one user, sometimes by two. The dataset contains 19 , 751 dis-tinct annotations, 54 annotations per document on average. Annotations such as Year , Month , Week , Day , Monday , Tuesday . . . occur very often even if they are not very informative. Milne and Witten [9] annotated a subset of the AQUAINT corpus, consisting of English news. The corpus is annotated in the Wikipedia style : only the first mention of each entity, and only the most important are linked. https://angularjs.org http://getbootstrap.com http://en.wikinews.org/wiki/Main_Page
Annotations are available here http://www.cse.iitb.ac.in/ soumen/doc/CSAW/Annot/CSAW_Annotations.xml
Several works adopted the AIDA-CoNLL dataset, in-troduced by Hoffart et al. [6]. It consists of 1393 documents belonging to the CoNLL-2003 dataset, a subset of news from the Reuters Corpus V1, annotated with entities URI. The authors annotated the mentions referring to named entities, but not the common names. Entities are annotated at each occurrence of a mention.

Gamon et al. [4] proposed a dataset for SE: the dataset consists of 99 webpages annotated with named entities (on average 24 entities per page). For each named entity they asked 3 judges to give a salient score among Most Salient, Less Salient, or Not Salient, and they kept the score with the majority of votes. The difference with our approach is that authors used named entities (i.e., just text, no links to a knowledge base), so the EL task cannot be evaluated. Manual Annotation Frameworks. We are not aware of any other open source framework for generating human assessments for entity linking, and for ranking the entities with respect to their saliency. We examined the Semantic Annotation Platforms reviewed by Reeve and Han [11] but we did not find a tool suitable for generating entity linking annotations. Bayerl et al. [1] propose a methodology for creating annotations, and different measures for evaluating agreement among assessors. Our interface was inspired by the loomp On Click Annotator proposed by Hinze et al. [5].
Acknowledgements This work was partially supported by the EU project E-CLOUD (no. 325091), the Regional (Tus-cany) project SECURE! (POR CReO FESR 2007-2011), and the Regional (Tuscany) project MAPaC (POR CReO FESR 2007/2013). [1] P. S. Bayerl, H. L  X  ungen, U. Gut, and K. I. Paul. [2] P. Bruza and T. W. Huibers. A study of aboutness in [3] D. Ceccarelli, C. Lucchese, S. Orlando, R. Perego, and [4] M. Gamon, T. Yano, X. Song, J. Apacible, and [5] A. Hinze, R. Heese, M. Luczak-R  X  osch, and A. Paschke. [6] J. Hoffart, M. Yosef, I. Bordino, H. F  X  urstenau, [7] S. Kulkarni, A. Singh, G. Ramakrishnan, and [8] R. Mihalcea and A. Csomai. Wikify!: linking [9] D. Milne and I. H. Witten. Learning to link with [10] D. Paranjpe. Learning document aboutness from [11] L. Reeve and H. Han. Survey of semantic annotation
