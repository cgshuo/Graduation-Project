 Feature selection is used to control the dimensionality of the feature space so as to eliminate noise from the feature set and reduce the computational complexity. In the text domain, the number of features is on the order of ten thousand. Feature selection is a must in most cases. We use the popular vector space model for text representation. Every text document is regarded as a vector, and every vector consists of a set of words. Every word is regarded as a feature. As a result, we have a multi-dimensional space.
Broadly speaking, feature selection can be divided into two steps: (1) rank all fea-tures in the corpus by using some scoring function, and (2) select some of the top-ranked features, such that any feature in the corpus which does not belong to those selected fea-tures would be disregarded. The select ed features are usually either the top K or top K percent of the ranked features. While we agre e that there are already numerous effective scoring functions for feature selection ([1], [2], [3], [4], [6], [7], [8], [9], [13], [14], [15], [17], [18], [19]), we argue that we do not have any systematic approach for determining K . Identifying an optimal K is always treated as an empirical question by training and evaluating the classifier for a range of different K using a separate validation dataset or simply by the so-called  X  X ule of thumb X . Usually we have no understanding about the properties of the selected features and do not know why they perform the best in that particular K .

In addition, we have found that simply selecting the top K features may not always lead to the best classification performan ce. In fact, it may turn many documents into zero length, and they cannot contribute to the classifier training. This observation has not been recorded elsewhere to the best of our knowledge. In this paper, we formulate the feature selection process as a dual object ive optimization problem: maximize the information presented in the documents and minimize the feature dimensionality. It is worthwhile to note that in our formulation, a feature which is being regarded as useful and retained in one document may not necessarily be retained in another document as well. Details will be discussed in later sections.

In this paper we will provide an in-depth analysis for the existing feature selection strategy, point out its fallacies and limitations, and suggest a novel model for selecting the features automatically. All discussion of feature selection in this paper will be with regards to the text domain. This work will also provide a documented framework for conducting feature selection in text classification such that the classifier performances would be optimized. By doing so, we can have a fairer comparison among different classification algorithms. For instance, Naiv e Bayes is long been regarded as inferior to Support Vectors Machine. Yet, how inferior is it? By fine tuning the text preprocessing, we will see that their differences are lower than expected.

The rest of this paper is organized as follows . Section 2 presents the issues related to feature selection; Section 4 discusses existing works; Section 3 reports the experimental results; Section 5 concludes this paper. For research that focuses on feature selection, the researchers merely aim at studying how effective a scoring function (e.g. Information Gain,  X  2 , mutual information, etc.) is. In this paper, we move the research in th is line a step forward by not only relating the classifier performances on the number of f eatures that have to be selected, but also attempt to answer the question of why such a number would be optimal. 2.1 An Overview A typical feature selection framework usually contains the following three steps: 1. Feature Scoring. A text corpus usually contains many text categories. Let C be a 2. Feature Ranking. In the previous step, we have assigned | C | number of different 3. Feature Selection. According to the overall scores of the features in the previous 2.2 Analysis Figure 1 (a) shows the accuracy of a multi-nominal Naive Bayes classifier [11] versus the number of features selected. 1 The x -axis denotes the numbe r of features selected and the y -axis denotes the accuracy of the classi fier. The accuracy is measured using the well-known F 1-measure [16]. Each line in the figure denotes one of the following commonly used scoring functions: (1) Information Gain ( IG ); (2)  X  2 ; (3) Odds ratio ( OR ); (4) GSS Coefficient ( GSS ); and (5) Inverse document frequency ( IDF ). Naive Bayes is chosen because it does not require f eature weighting, and therefore its perfor-mance depends solely on the number of features selected.

In Figure 1 (a), we can see that different scoring functions have different optimal values. If we want to understand  X  why  X , reviewing their mathematical formulas may not help since they all adopt the same idea that the best features for c k are those distributed most differently between c k and  X  c k . Hence, we need further analysis in order to gain a better understanding.

Logically, when K is small (few features have been selected), the number of features remaining in a document will be even fewer. Eventually, a document may contain no features (i.e. its size is zero) if none of its features are found in the top K features. The number of training examples will shrink if the number of zero-size documents in-creases. Intuitively, the number of documents with zero size will affect the classification accuracy negatively because these documents cannot contribute to the classifier training process (since there are no features in these documents). Figure 1 (b) plots the number of features selected against the percentages of documents with non-zero size. In the figure, the x -axis denotes the number o f features selected and the y -axis denotes the percentage of non-zero-size documents (i.e. contains at least one feature).

In Figure 1 (b), we can see that only IDF will assign higher scores to features that appear in only a few documents across the corpus. We understand that IDF is not a good feature selection function for text classification [16] [18] [19], but for the purposes of case, it will yield a very sparse document rep resentation. According to Figure 1 (a) the effectiveness of the scoring functions is roughly: GSS &gt; OR &gt; IG &gt;  X  2 &gt; IDF ,where the number of non-zero-size documents (accord ing to Figure 1 (b)) is also of this order. Thus, we believe that there may be a relationship between the number of non-zero-size documents remaining after feature selec tion and the performance of the classifier.
In order to have a better understanding of the relationship between the number of zero-size documents and classifier perform ance, we plot a graph with the percentage of non-zero-size documents against F 1-measure in Figure 1 (c). In this figure, we found that the effectiveness among different sco ring functions seems to be  X  X mproved X  (and much similar) with respect to Figure 1 (a).

Until now, it should be clear that different scoring functions will affect the distribu-tion of zero-size documents greatly. Yet, it does not mean including these documents in training could improve the classificatio n accuracy. So, we conduct another analysis as follows. For each document, we try to maintain it to have at least M features. If the number of features remaining in the document is less than M after feature selection, then we will retain those features that are rank highest in that document until the size of the document is M . Figure 2 shows two plots of this analysis by using GSS (overall the best scoring function) and IDF (overall the worst scoring function). In the figures, x -axes are the number of features selected (in the order of 1000), y -axes are the value of M ,and z -axes is the F 1-measure. For the purpose of visualization, we contrain M &lt; 10. Setting a different M does not affect the conclusions of this paper.
 Regardless of the scoring functions, both of them in Figure 2 show a trend that when M increases to some value, the accuracy of the classifier will be maximum given that the number of features selected is the same. This is especially obvious for the scoring function IDF which increases performance from 0.08 to 0.389 when M = 10 and the number of features selected is 1000. This number is chosen as an example, setting any other value will arrive at the same conclusion. This is one of the major reasons why we include IDF in this analysis, becau se we can see that even a feature selection algorithm that is sub-optimal for our purposes could improve dramatically if we have an appropriate scheme for conducting feature selection. The best case for GSS here is 0.838 ( M = 8 and 3000 features are selected), but it only obtains 0.810 if M = 0. These two figures suggest that those documents that would become zero-size after feature selection could in fact positively contribute to the classifier training if we have some way to make them become  X  X ot zero-size X  .

To take a step forward, one may think that the number of features remaining in the document is highly related to the classifier effectiveness. In order to justify this statement, we conduct a final analysis. Identify the F 1-measures of the classifier such that the maximum number of features in a document could not exceed M . Figure 3 shows two plots for this analysis. Its definitions are similar to Figure 2 except that the z -axis becomes M (the maximum number of features allowed in a document). Similar to Figure 2, their performances would be a maximum for some values of M with a given number of features selected.

According to the analysis from Figure 1 to Figure 3, we claim that it is not the number of features selected in the corpus that would affect the performances of the classifiers, but it is the number of features remaining in the documents . 2.3 Modeling We should not aim at identifying a single K that optimizes the p erformance over all categories. It will only lead to local optimization. We suggest that we should study at the document level  X  minimize the number of zero-size documents by preserving maximum information in a document. Yet, this objective alone is not enough, as one of the objectives of feature selection is to reduce the feature dimensionality. Thus, we have our second objective  X  minimize the number of features in the corpus. Eventually, a dual objectives optimization problem is formulated: where x ij = 1 if the feature f j should be retained in document d i ,and x ij = 0otherwise or f j does not appear in d i ; s jk is the score of f j in category c k computed by some func-tion. The first objective (Eq. (1)) tries to maximize the information contained in every single document by retaining as many features as possible, while the second objective (Eq (2)) tries to minimize the number of feat ures selected. Specifically, if feature f j is selected in a document, y j will be 1. Alternatively, if f j is not selected in a document, y will be 0. a j is a parameter trying to average the number of documents selected in the corresponding feature f j . Our aim is to find the best combination of 0 and 1 for x ,i.e. this is a 0-1 integer programming problem. Since multi-objective problems are difficult to solve, we combine Eq. (1) and Eq. (2): where  X  is the parameter to balance the two objectives. It has to be set carefully so as to balance the two components, as the first and second component will be heavily affected by the number of features and the number of documents, respectively. In this paper, we let  X  be the ratio between these two factors by using a validation dataset. We plan to have more detail on finding the optimal  X  in future works. We will show how effective this is in formulation of our experimental study. Extensive experiments are conducted using two benchmarks: Reuters-21578 2 and Newsgroup-20 3 . For Reuters-21578 we use ModApte spit and remove the categories that only appear in the training set or testing set. For Newsgroup-20 there are 20 differ-ent categories. For each category, we random ly select 80% of the postings as training and the remaining 20% as testing. We repeat the experiments 30 times and report the averaged result.

For each document, we remove any punctuation, numbers, Web page addresses, and email addresses. All features are stemmed using the Lovins stemmer [10]. Following the existing works of [16], features that appear in less than 1% of the documents are regarded as noise. Features that appear in mo re than 95% of the documents are regarded as stopwords. For feature weighting, we implemented the well-known tf  X  idf schema whenever necessary. 3.1 Algorithms for Comparison For evaluating the proposed feature selection framework, we implemented all of the seven most widely used feature scoring functions as shown in Table 1, and compare our framework with the existing feature selection approach. The existing approach is to select only the top K by using a training and valid ation dataset such that this K would optimize the performance of the classifier on the validation dataset.
 For the classification algorithm, we impl emented the Naive Bayes [11] algorithm. Naive Bayes is chosen because it does not require feature weighting, so we can see the impact of different feature selection frameworks easily. We use the multinomial version of Naive Bayes and use Laplacian smoothing with m = 0 . 01 [11]. We have tried different values of m ,and m = 0 . 01 results in the best performance for almost all situations. For performance evaluation we report only the micro-F1 value due to the space limitation. Yet, the trend of all other m easurements (precision, recall, break-even point, etc.) have the same general patte rn and can lead to the same conclusion. 3.2 Result and Discussion Figure 4 shows the classification results of Na ive Bayes with the seven different scoring functions that are descr ibed in Section 4. Since NB does not require feature weighting when generating its model, we do not need to compare its performance with different weighting schemes. In the figure, the black ch arts represent the results that are achieved by our proposed feature selection framework, while the shaded charts represent the results that are obtained by identifying one single K that maximizes the performance of the classifier on the validation dataset. We call this the  X  X op K approach X .
In the figures, regardless of which scoring function, all black charts (the proposed feature selection framework) dominate the shaded charts. This result indicates our pro-posed framework is highly effective. The improvement is especially visible for MI and IDF , which are the worst two among all seven scoring functions. If the total number of features selected is small, there will be many zero-size documents. On the other hand, if the total number of features select is too high, then the noise in the corpus will be high, which will eventually deteriorate the classifier performance as well. Our proposed feature selection framework could properly remedy the above two situations by mini-mizing the number of zero-size documents and minimizing the feature dimensionality.
Figure 5 shows the performance of Naive Bayes versus Support Vector Machines (SVM). SVM has long been cited for its high effectiveness and reliability when in con-ducting text classification. Note that SVM does not require feature selection for doc-ument preprocessing as it is designed for handling very high dimensionality in a very noisy environment. Yet SVM does require feature weighting. In this experiment, we use the traditional tf  X  idf scheme for feature weighting [18]. We use a linear kernel with C = 1 . 0 which is a common setting for SVM. In Figure 5, we can see that although the performance of SVM is still better than that of Naive Bayes, the difference is marginal. From [16], among the existing work on Naive Bayes in the Reuters dataset, the best performance is 0.795 whereas it is around 0.870 for SVM. However, from our study we have found that Naive Bayes could achieve around 0.84 for the Reuters dataset if we select the features properly. Similarly, for the Newsgroup dataset, we found that the performance could obtain a mu ch higher accuracy than the reported studies using our framework. Feature selection is a topic which has permeated fields from statistics, to image pro-cessing. Largely, the goal of feature selection is to reduce dimensionality in data by selecting only the most relevant criterion for classifying a body. While studying fea-ture selection, different researchers take dif ferent approaches to which attributes of the feature selection process are most impor tant to the success of the algorihtm.
Some works focus on the scoring function as the way to identify the most important features. Yang and Pedersen [19] performed an in-depth study of feature selection in the text domain where they compared commo n scoring functions in feature selection. The results of their study show that Information Gain [1,7,8,19,18] and  X  2 [1,4,15,17] are the most effective scoring functions for text classification.

In a survey conducted by Yu and Liu [9], the feature selection algorithms are classi-fied into several clear and defined categories based upon their search strategies, evalua-tion criteria, and data mining tasks (the evaluation step for determining the validity of a subset). The result of this survey does not necessarily favor one of their defined criteria, however they propose a framework where the best feature selection algorithm is chosen behind the scenes, completely transparent to the user.

A technical report on feature selection ca rried out by Arizona State University [19] breaks feature selection algorithms down into four categories: if they are supervised, whether they are of filter or embedded model, whether they have univariate or multi-variate variable selection, and whether they do weight selection or set selection. The results on the site largely suggest that no one category can determine the success of a feature selection algorithm.

This technical report breaks the feature selection process into four parts: feature sub-set generation, subset evaluation, stopping criteria, and results validation. The four steps are ordered nicely in Figure 7. While all of the mentioned papers present different crite-ria for what is most important to feature selection, this paper focuses on the  X  X valuation X  step (as shown in Figure 7). As we have shown, having a robust evaluation framework can be crucial to the success of a feature selection algorithm.
 In this paper, we study feature selection in the text preprocessing domain. We provide an in-depth analysis for the existing selection framework, point out its fallacies and limita-tions, and suggest a novel model for selecting the features automatically. We formulate the feature selection process as a dual objectiv e optimization problem, and identify the best number of features for each document, ra ther than determinin g a fixed threshold which optimizes the overall cl assification accuracy for different categories. We provide a documented framework for conducting text preprocessing in text classification in or-der to optimize the classifier performances , regardless of which classification model one intends to use. Extensive experiments are conducted to validate our claims. The favorable experimental results indicate that our proposed work is highly feasible.
