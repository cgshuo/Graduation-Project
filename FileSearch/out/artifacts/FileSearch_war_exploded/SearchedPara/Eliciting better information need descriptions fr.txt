 1. Introduction
It is well-known that users often have a difficult time articulating their information needs ( Belkin, 2000 )and that users typically pose very short queries, usually between two and three words in length ( Jansen, Spink, &amp;
Saracevic, 2000 ). While it might be the case that the difficulty that users have articulating their information needs causes their queries to be short, it has recently been argued that one reason users pose short queries is because traditional search interfaces encourage them to do so ( Belkin et al., 2003 ). It is also well-known that in best-match systems longer queries usually result in better retrieval performance at least as explored in batch-mode information retrieval (IR) experiments (c.f., Buckley, Salton, &amp; Allan, 1994 ). Thus, there is an apparent mismatch between what interfaces encourage users to do, what users are doing and what has been demon-strated to result in good retrieval.

One effective technique for increasing the number of terms contained in users X  queries and improving retrieval performance is query expansion ( Efthimiadis, 1996 ). The underlying assumption behind query expansion is that more terms are better; that is, that query length is associated positively with retrieval performance. Query expan-sion techniques can assist users with increasing the length of their queries through automatic and interactive techniques. Automatic query expansion (AQE) occurs when the system selects appropriate terms for use in query expansion and automatically adds these terms to users X  queries. Interactive query expansion (IQE) gives users control over which of a set of system suggested terms are added to the query, and in some cases, which sources are used to automatically generate this set of terms ( Harman, 1988 ). Empirical studies have led to the general finding that users desire IQE features (c.f., Beaulieu, 1997; Belkin et al., 2001 ). However, much of the evidence from these studies indicates that relevance feedback features are not used. This has been attributed to problems related to the design of relevance feedback interfaces ( Ruthven, 2003 ), task complexity and the user X  X  lack of additional cognitive resources ( Belkin et al., 2001 ), and the amount of extra time required to use such features.
Another approach to eliciting longer information need descriptions from users is to obtain more informa-tion from them at the time of initial querying. Ingwersen X  X  theory of polyrepresentation (1996) suggests that obtaining multiple representations of a single information need is a better approach to representing user needs than relying on solitary, isolated queries. In particular, polyrepresentative extraction of information needs suggests eliciting information about what the user currently knows about a topic and why the user wants to know about the topic. It is believed that these types of questions can elicit information about the context of the user X  X  information need. For instance, asking users to describe what they already know about topics might allow one to understand more about users X  familiarity with topics; asking users why they want to know this information might lead one to discover that users are writing academic papers, shopping for gifts, or plan-ning travels. There is some empirical evidence that suggests that polyrepresentation of information needs is a useful approach to representing users X  needs. For instance, Belkin, Cool, Croft, and Callan (1993) examined the impact of different query representations on retrieval and found that a variety of representations led to better performance. This work suggests that eliciting a variety of information need representations from users, and using this information in different combinations, is likely to improve retrieval.

Interfaces to current online search services do not typically elicit information from users beyond short que-ries. Based on findings from the research in interactive IR, it seems that this may be a missed opportunity to improve retrieval for users. In this paper, we argue that users should be probed in more effective ways at the search interface. While we accept that in some situations users are unable to clearly articulate their informa-tion needs because they lack the knowledge or vocabulary to describe their needs, we propose that users have not been probed in the most effective way; that is, that traditional interfaces for querying and for relevance feedback are not optimal for eliciting the most robust and useful descriptions from users about their informa-tion needs. Similar to Ingwersen (1996) we believe that users know additional information about their infor-mation needs beyond what they typically communicate to information systems. In particular, Ingwersen (1996) identified information related to domains, tasks and interests and problems and goals that can be potentially useful for retrieval.

Based on this previous research, we developed and evaluated a generic, document-independent form that could be used in multiple information-seeking situations to elicit some of this additional information from users. We accomplished this by creating an online form and presenting this form to users after initial querying in hopes of eliciting more complete descriptions of their information needs. We used information we obtained from the form as a source of terms for query expansion, created a series of experimental runs based on this information and evaluated the performance of each run.

This work was also motivated, in part, by the supposition that traditional query expansion techniques, which typically present top-ranked documents or keywords to users for feedback, are unlikely to work well in all retrieval situations, especially when ambiguous queries are posed because there is a large chance that documents retrieved in response to such queries will be irrelevant. Furthermore, users often have a difficult time selecting the best terms for query expansion, if they are willing to select them at all. In many cases, users do not understand why certain terms have been suggested and in many other cases, the terms which the system suggests are not necessarily the best. Rather than forcing users to interact with system suggested terms or doc-uments, we were interested in investigating users as sources of terms for query expansion, independent of any information from the system. We were further interested in examining the relationship between query length and performance in a situation where queries were strictly user generated. The work reported in this paper is an extended version of the work reported in Kelly, Dollu, and Fu (2005) .

In the next section, we present literature related to the communication of information needs, user X  X nter-mediary interactions, methods of specifying information needs to search systems and interactive query expansion. This is followed by a description of the method for the study, results and discussion, and conclu-sions. 2. Related literature 2.1. Communication of information needs
A user X  X  information need is perhaps one of the most critical aspects of information-seeking and retrieval, since this need generally motivates such activities. However, it is accepted in the IR and library science com-munities that users have a difficult time articulating their information needs to other people and, even more so, to information systems. Belkin (1980) described the interactions between users and systems as occurring within a communication framework, and argued that  X  X  X he emphasis on the role of the user in the communication model and in IR compels one to recognize explicitly that representing users X  needs is at least as important as representing texts X  X  (p. 136). Belkin explicitly raised the problem of the non-specifiability of the information need, or what is more commonly referred to as the anomalous state of knowledge (ASK) hypothesis, and noted that people have a difficult time articulating their information needs due to cognitive or linguistic lim-itations. IR systems that simply ask for keywords are believed to be suboptimal; instead, Belkin suggests that  X  X  X n IR system ought to be based on means for discovering and representing the user X  X  knowledge of the prob-lem under consideration X  X  (p. 139).

The ASK hypothesis ( Belkin, 1980 ) was, in part, based on Taylor X  X  (1968) four levels of information needs: visceral, conscious, formalized, and compromised. The visceral need is the actual, but unexpressed need for information. The conscious need is the mental description of the need, or the state of the need before it is artic-ulated to another entity, whether that entity is a system or person. The formalized need is the formal statement of the need; an example of a formalized need is a written topic description. Finally, the compromised need is  X  X  X he representation of the inquirer X  X  need within the constraints of the system and its files X  X  (p. 183). Thus, the compromised need is the version of the information need that is put to a system; the most common example of a need at this level is a query. At the compromised level, it is necessary for the user to state the need in such a way that the system can understand and use the information. Because of this, it is possible that a user X  X  mental model of how IR systems work might bias and constrain what the user enters into the system. For instance, users may be overly critical in their determination of which terms to include in their query based on how they think a retrieval system works. Further, users may experience vocabulary problems with respect to how infor-mation needs are communicated to systems, how authors of documents use terms and how indexers assign terms to documents ( Furnas, Landauer, Gomez, &amp; Dumais, 1987 ).

In some other early work, Bennett (1972) describes the problem that users often have with communicating their information needs to systems, in that users are forced to communicate using the system X  X  vocabulary and not their own. Bennett recommends that interface languages for IR should enable users to address the com-puter without being exasperated by the computer X  X  inability to understand them, and that communication should occur in a language that is most familiar and natural to users. Some IR systems attempted to address this issue. For example, Croft and Thompson (1987) motivated the design of I ditional IR was ineffective because users were required to translate their information needs into the query lan-guage of the system. One goal of I 3 R was to elicit more accurate representation of the user X  X  internal knowledge structures regarding a particular topic and the user X  X  language for retrieval. Prior to searching, users specified terms and concepts that were important to them, and identified relationships between these terms and other concepts in the domain area of interest.

Ingwersen X  X  (1996) cognitive theory of IR suggests that systems should account for the variety of knowledge structures that comprise them, both with respect to the collection and users, allowing multiple representations of information objects and information needs, and multiple methods for retrieval. Allowing for this variety is termed polyrepresentative IR. With respect to information needs, Ingwersen (1996) observes that while research has demonstrated that the information need and cognitive states of the user are variable during the information-seeking process, that  X  X  X dditional cognitive structures concerned with domains, tasks/inter-ests, and problems/goals or uncertainty are present X  X  (p. 18). Information about these cognitive structures pro-vides contextual and background information related to the user X  X  need. Ingwersen (1996) proposes a process of polyrepresentative extraction for eliciting this additional information. Polyrepresentative extraction sug-gests eliciting a what (i.e., what is currently known about the topic), a why (i.e., why a user wants to know about topic) and a work task or domain description from the user. Given that current search interfaces typ-ically present users with a small box in which to describe their information needs, the research reviewed in this section suggests that the process of communicating one X  X  information needs to a system can be improved. 2.2. User X  X ntermediary interactions
Ingwersen X  X  (1996) identification of the kinds of information that could be extracted for polyrepresentation is based, in part, on research investigating user X  X ntermediary interactions in libraries (c.f., Ingwersen, 1982;
Saracevic, Mokros, &amp; Su, 1990 ). In terms of Taylor X  X  (1968) four levels of need, the goal of the intermediary is to  X  X  X ork with the inquirer back to the formalized need, possibly even to the conscious need, and then to translate these needs into a useful search strategy X  X  (p. 183). Work in this area continues to suggest what one might elicit from users and how. White (1998) , for example, analyzed the questioning behavior in refer-ence interviews and concluded that the intermediary had to elicit more information, especially about the sub-ject and service requested than what the user initially offered. Accordingly, White (1998) concluded that the minimalist approach to interfaces for Web search engines may not be the most effective for retrieval. The types of interactions and elicitations that occur online via digital reference services have also been studied. For instance, Marsteller and Mizzy (2003) studied the negotiation process in synchronous digital reference by ana-lyzing patron query types, librarian question types and patron response types. Results demonstrated that open-ended questioning was more prevalent for reference-type queries than for other types of querying, such as known item querying or fact querying.

A number of systems were developed which sought to simulate the activities performed by human search intermediaries (c.f., Brajnik, Guida, &amp; Tasso, 1987; Cole, 1998; Vickery &amp; Brooks, 1987 ). For instance, Cole (1998) proposed a system to help the user discover his or her information needs through user X  X ystem interac-tions based on the premises that  X  X  X sers do not know what information they need before accessing the system so they have to be helped in forming the query to the information system, and users become aware of their information need only through this process of interacting with the system X  X  (p. 710). Cole argued that an inter-mediary needs three kinds of knowledge in order to develop a clear understanding of the user X  X  information need: knowledge of the purpose of the search, knowledge of the user X  X  background and subject knowledge about the topic of the search. These items are similar to those described by Ingwersen (1996) . PLEXUS ( Vickery &amp; Brooks, 1987 ) modeled a user X  X  information need through a simulated reference interview. Initial query formulation began with user X  X ystem dialogue, where the system probed the user with a number of ques-tions. Users responded in natural language, and information was extracted from their responses and placed in  X  X rames X  which helped the system better understand users X  information needs and experiences with gardening.
Although robust IR systems that can simulate the user X  X ntermediary interaction have yet to come to fruition, the idea that retrieval systems should at least try to model some characteristics of search intermediaries con-tinues to influence research.

Finally, there is evidence from user X  X ntermediary studies that demonstrates that users are able to articulate a large number of effective query terms. Spink (1994) investigated the selection and effectiveness of search term sources for query expansion by conducting a micro-level analysis of transcripts of user X  X ntermediary dialogues and accompanying search logs during online searching. Spink (1994) examined the following five sources of terms: user question statements, user-interaction (terms suggested by the user during interaction with the inter-mediary), intermediary, thesaurus and term relevance feedback. In this study, term relevance feedback described terms which were extracted by the user or intermediary from retrieved documents. Spink (1994) found that the majority of terms (38%) came from user question statements, and that these terms on average retrieved about 82% of the relevant documents. In this study the most effective sources of search terms for query expansion were terms from users X  written questions statements. Based on this finding, Spink (1994) rec-ommends that IR interfaces should allow users to enter natural language information problem statements and encourage users to use their own knowledge as a source of search terms for query expansion.
 2.3. Methods of specifying information needs
Perhaps the most common method of specifying information needs to Web search services is via keyword queries. Most interfaces to such services provide short text boxes and research has shown that average query lengths are around 2.5 terms ( Jansen et al., 2000 ). It has recently been argued that one reason users pose short queries is because traditional search interfaces encourage them to do so ( Belkin et al., 2003 ). One approach to assisting users with building better queries is to encourage them to provide longer descriptions of their infor-mation needs at the time of initial querying or eliciting  X  X nhanced X  queries ( Belkin et al., 2003; Croft &amp; Das, 1990 ). Belkin et al. (2003) compared an experimental query interface, which was designed to elicit longer que-ries from users, with the standard query interface found on most retrieval systems. Although Belkin et al. (2003) found that users entered longer queries using the experimental interface than they did when using the standard interface and that query length was positively associated with user satisfaction, there was no sig-nificant correlation between query length and performance. In another study, Belkin et al. (2002) found a con-sistent relationship between query length and performance. However, their study was not designed explicitly to evaluate the effect of query length on performance and their finding was descriptive, rather than inferential.
Croft and Das (1990) presented users with a query formulation questionnaire that elicited personal data, the query and importance of each term, a ranking of query terms, words or phrases related to the query and their importance, and relation types between these words and phrases. They found that the enhanced queries sig-nificantly improved retrieval performance. Thus, it seems that changes to the query interface can lead to longer queries, better performance and increased user satisfaction.

Another approach to getting users to describe their needs more fully is to use natural language interfaces, which  X  X  X ccept as input a description of an information need in some natural language (such as English) X  X  ( Turtle, 1994, p. 220 ). Studies on several experimental natural language search systems, such as WESTLAW ( Turtle, 1994 ) and INTELLECT ( Dekleva, 1994 ), led to positive results. For example, Turtle found that  X  X  X n average a current generation natural language system provides better retrieval performance than expert searchers using a Boolean retrieval system when searching full-text legal materials X  X  (p. 220). In current
Web-based search services, natural language interfaces, such as AskJeeves, encourage users to issue queries in question format. An extension of supporting natural language queries is to use speech recognition technol-ogy to support spoken queries ( Gilbert &amp; Zhong, 2003 ). Crestani (2002) argues that speech is the most natural and interactive medium of communication; recent progress in speech recognition is making it possible to build systems that interact with users via speech interfaces. Because few studies have been conduced on spoken queries, the usefulness of this approach is unclear. However, from a theoretical perspective, it seems like a promising approach to querying.

Finally, Query-by-Example (QBE) allows users to issue queries by providing selections of text from a known relevant document. In this scenario, users place text from documents that they have already judged as relevant into a standard search box with the goal of retrieving similar documents ( Blair &amp; Kimbrough, 2002 ). Other systems have taken advantage of visualization and direct manipulation to allow users to issue dynamic queries while navigating and exploring document collections ( Ahlberg, Williamson, &amp; Shneiderman, 1992 ). 2.4. Interactive query expansion
One popular way that systems have attempted to assist users with query formulation is through interactive query expansion (IQE) ( Harman, 1988 ). At its inception, interactive query expansion via relevance feedback was thought of as technique for assisting users overcome the human X  X ystem communication problem. The predominant viewpoint was that by providing users with terms used to index documents, they would be equipped with a more appropriate vocabulary with which to formulate queries; all users needed to do was select the most appropriate terms from the display. In typical IQE scenarios, users are presented with terms that they can add to their queries. Terms are either automatically generated by the system or users can mark documents that they find relevant and the system uses these documents to suggest terms.

Empirical studies have led to the general finding that users of interactive IR systems desire IQE features (c.f., Beaulieu, 1997; Belkin et al., 2001 ). However, much of the evidence from these studies indicates that relevance feedback features are not used. For example, users in a series of studies by Belkin et al. (2001) rarely used relevance feedback features and often commented on the quality of terms suggested by the system. Belkin et al. (2001) speculated that users may not have used relevance feedback features in these experiments because they were involved in complex information-seeking tasks in a novel environment, and may not have had addi-tional cognitive resources available for learning and experimenting with features, and integrating them into their searching repertoires.

Further evidence has shown that users are not always able to select good terms. For instance, in a study of simulated interactive query expansion, Ruthven (2003) demonstrated that users are less likely than systems to select effective terms for query expansion. Ruthven (2003) demonstrated some potential benefit of term rele-vance feedback if the best terms were used in query expansion, but went on to note that users are unlikely to select these terms because of problems with current relevance feedback interfaces. In a Web-based study,
Anick (2003) found that users made use of a term suggestion feature to expand and refine their queries. How-ever, this did not result in improvements in retrieval performance. 3. Method
This experiment was carried out as part of the NISTs (National Institute of Standards and Technology) 2004 TREC (Text Retrieval Conference) High Accuracy Retrieval from Documents Track (HARD) ( Allan, 2005 ). The Track had three objectives: (1) to determine if metadata about the query, user or search context could be used to improve retrieval; (2) to determine if a single, highly focused interaction with the user could be used to improve retrieval; and (3) to determine if passage retrieval could be used to further improve retrie-val. Sixteen total sites participated in the Track.

TREC participants were free to select any of these objectives to explore in their experiments. The experi-ment presented in this paper focused only on objective (2), since we were interested in investigating techniques for eliciting information from users about their information needs. Track participants who explored (2) cre-ated clarification forms , which constituted the  X  X  X ighly focused interaction X  X . While most Track participants used clarification forms to elicit traditional relevance feedback about search results, we used it as a query fol-low-up technique.

The HARD track was conducted in collaboration with the Linguistic Data Consortium (LDC) at the Uni-versity of Pennsylvania. The LDC recruited and managed users, and mediated all interactions that occurred between users and Track participants. The experimental protocol of the HARD track differed a bit from traditional interactive IR user studies. While users created topics, completed clarification forms and evaluated documents, they did not conduct any interactive searching, or directly interact with any Track partici-pants. 3.1. Users, topics and metadata
The LDC recruited 13 users to participate in the project; these users were people who worked for the LDC and most were students completing internships. In total, these 13 users created 50 topics, with each user cre-ating approximately four topics. Only 45 topics were used in this study because for five of the 50 topics, no relevant documents were retrieved by any system participating in the Track. Most participants were in their early to mid-twenties; several were undergraduate students, others had already earned an undergraduate degree and several others held both undergraduate and graduate degrees.

Users were presented with an online Topic and Metadata Creation Form. The topic creation portion of this form is displayed in Fig. 1 . With this form, users were instructed to create TREC-style topics ( Voorhees, 2005 ), which contained a title, description and topic-narrative. Users were instructed to create topics about anything they desired. Topics included those about bass guitar amplifiers, the Heaven X  X  Gate Cult, marathon training, the diamond industry, natural disasters and global warming, and life on Mars. As part of the
HARD track, users also provided metadata. Metadata are not described in this paper since they were not used. Once topics had been created, they were distributed to all Track participants without related metadata.
Track participants used these topic descriptions to generate baseline retrieval runs which they then submitted to NIST.
 3.2. Corpus The corpus used in the 2004 HARD Track was approximately 1.5 GB in size and contained about 650,000 English-only newswire documents from 2003. Sources included AFE (Agence France Press), APE (Associated Press Newswire), CNE (Central News Agency Taiwan), LAT (Los Angeles Times), NYT (New York Times),
SLN (Salon.com), UME (Ummah Press), and XIE (Xinhua News Agency). 3.3. Clarification form
Track participants submitted up to three clarification forms per topic to the LDC for evaluation by users. A total of 10 Track participants contributed 21 clarification forms per topic. For each topic, the clarification form was a Web page that elicited information about the topic or the user (e.g., disambiguating words in the topic or finding out more about the user X  X  interests). Several mandatory guidelines were provided for con-structing clarification forms. Clarification forms had to be self-contained HTML Web pages and had to dis-play correctly on a 16-inch monitor with 1152 by 900 resolution using Netscape v4.78. Guidelines also dictated that interactive scripting could not be used. Permissible form fields included text boxes, radio buttons, check boxes, and drop-down menu selectors.

Forms contributed by Track participants were presented in random order to users. For each topic, users completed 21 clarification forms. Users completed clarification forms during a two-week period. Users were allowed to spend up to three minutes completing each form. Given the large number of forms users were com-pleting, it is important to point out that fatigue resulting from this activity might impact the amount and qual-ity of users X  responses to forms. Forms were randomized in order to distribute this impact. Once the clarification forms were completed, this data, along with the metadata, were distributed to Track participants for use in experimental retrieval runs.

Based on our research interests and previous research, we designed a generic, document-independent clar-ification form that consisted of four questions, and could be used for all topics without modification. This clar-ification form is displayed in Fig. 2 . The first question that we presented to users was a familiarity question, which asked users to indicate how many times they had searched for information about their topics in the past. We do not discuss familiarity in this paper.

Questions 2, 3 and 4 were designed to elicit information from users about their topics for use in query expansion. In designing clarification form features to elicit this information, we were careful to use large text boxes that allowed users to view the entirety of their responses and hopefully, as found in previous studies ( Belkin et al., 2003; Kalgren &amp; Franzen, 1997 ), encourage them to type in longer responses than they would if presented with a short line. Questions 2 and 3 were open-ended questions (although 2 is presented as a state-ment), and encouraged users to respond in natural language. Question 2 (Q2) asked users to describe what they already know about the topic, and Question 3 (Q3) asked users to indicate why they want to know about the topic. Our goal in using these questions was to encourage users to talk more about their topics, and hope-fully in doing so, have them provide additional information that might prove useful in retrieval. Our selection of these two questions was based on an examination of previous research on face-to-face reference interviews (c.f., Ingwersen, 1982 ), reference textbooks describing best practice (c.f., Katz, 2002 ), and the notion of  X  X  X oly-representative extraction X  X  of information needs ( Ingwersen, 1996 ). Dewdney and Michell (1997) , provide fur-ther theoretical justifications for using why questions in face-to-face reference interviews.

Question 4 (Q4) asked users to list additional keywords describing their topics. A number of Track partic-ipants from the 2003 Track used a question like this, some with quite successful results (c.f., Grunfeld, Kwok,
Dinstl, &amp; Deng, 2004 ). Thus, we included this on our form with hopes that it would again provide some useful data. It was also the case that the majority of clarification forms from the 2003 Track asked users to make a selection of good terms from a list of terms from top-ranking documents. Thus, we further hoped that this question would allow us to take advantage of the priming that users might receive by being exposed to such lists of terms before they completed our form in the experimental rotation. The assumption, of course, is that if users see a good term on another clarification form, then there is a possibility that they will remember and enter it when they reach our form. We certainly recognize this as potentially confounding the results of our own experiment, but the protocol of the HARD Track made this unavoidable. Thus, it is important to note that terms entered by users in response to Q4 cannot be considered as strictly user-generated. 3.4. Relevance judgments
Users assessed the relevance of documents retrieved in response to their topics at the LDC at a time that followed completion of clarification forms. Users were instructed to assess relevance on a three-point scale, using the labels Y (yes), N (no), M (metadata). Specifically, points were defined by the LDC as follows:
YES: this story discusses the topic in a substantial way. Stories that you label as YES should give some information about the topic and should answer the topic query. Documents do not have to contain new information about the topic  X  a story that summarizes a topic X  X  history or gives a snippet of information that you have read about before still counts as a YES. Even if the document contains a relatively small amount of information about a topic, it should be considered a YES.

NO: this story does not discuss the topic at all, mentions the topic in passing without giving any informa-tion about it, or fails to address the specific query or angle of the topic. If a document names a topic or makes reference to it but does not provide any information about it, then that document should be consid-ered a NO.

M: Because HARD topics consist not only of topic descriptions but also of metadata, we must determine whether or not the document being assessed satisfies that Metadata. Hence we have established the letter M as the in-between YES for a document that is relevant to the basic topic description, but does not meet the demands of the metadata.

In this study, documents that received a  X  X  X  X  X  and  X  X  X  X  X  were combined since we were only interested in eval-uating topic-level relevance. Thus, the relevance judgments used in this study were binary.

The determination of which documents to show users was made using the standard TREC pooling method ( Voorhees, 2005 ). This pooling method involves combining a fixed number of top retrieved docu-ments from runs contributed by Track participants for each topic, and then removing duplicates. Because
Track participants use different retrieval techniques, it is believed that this method has a high probability of retrieving most relevant documents in the corpus. The top 85 documents from one baseline run and one experimental run for each topic from each site were pooled. A baseline run refers to retrieval where only topic descriptions are used and an experimental run refers to retrieval where, for instance, topic descrip-tions might be combined with clarification form results or metadata. Each Track participant used their own retrieval system and their own retrieval techniques. Our retrieval techniques are described below in
Section 3.5 . In total, users assessed 36,938 documents for 45 topics, 3770 of which comprised the  X  X  X  X  X  and  X  X  X  X  X  set. 3.5. System and retrieval runs
We used the Lemur IR toolkit ( http://www.lemurproject.org ) to conduct our retrieval experiments, with its basic defaults for indexing, and Okapi BM25 for retrieval. Although we made use of a basic stop word and acronym list, we did not use a stemmer. Our baseline run consisted of the title and description for each topic.
We used this information for our baseline run because we felt that it most closely approximated the length of queries posed by users in online searching environments ( Jansen et al., 2000 ). Although using text from both of these fields created queries that were longer than what is reported by Jansen et al. (2000) , we did not want our baseline run to produce particularly poor results either.

Our experimental runs were constructed from the information that we obtained from users with our clar-ification form (Q2, Q3 and Q4). In all cases, we used the additional terms elicited from users by each of these techniques as a source of terms for query expansion. For runs where the same term appeared in multiple places (e.g., in the title, Q2 and Q3), the initial weight of the term was multiplied by the number of times the term appeared. Each of these runs consisted of the baseline query (title + description) plus any additional text pro-vided by the clarification form question.

We included two other types of runs as baselines, one using pseudo-relevance feedback and another using what we consider as the upper bounds for relevance feedback. We created a set of pseudo-relevance feedback runs, each of which extracted a varying number of terms from the top 10 retrieved documents for each topic from our baseline. The technique used for selecting terms for query expansion in these runs was based on Rob-ertson Selection Value (RSV) and is described more fully in Robertson, Walker, Jones, Hancock-Beaulieu, and Gatford (1995) ; this technique is included as part of the Lemur toolkit. The set of pseudo-relevance feed-back runs used the top 5, 10, 20 and 50 terms for query expansion. We created a set of upper bound relevance feedback runs, each of which extracted query expansion terms from 10 randomly selected relevant documents.
The four runs based on relevant documents used the top 5, 10, 20 and 50 terms for expansion. These addi-tional runs were included as baselines so that we would have some way to evaluate the use of terms provided by users with clarification forms for query expansion, against the use of terms generated using automatic tech-niques. The number of terms selected for use in each of the baseline relevance feedback runs (5, 10, 20 and 50) was based on the range of terms that we elicited with our clarification form questions. Runs are displayed in
Table 1 . 3.6. Performance measure
To measure performance, we used mean average precision (MAP), a standard TREC evaluation measure ( Voorhees, 2005 ). Mean average precision is the average of all topic-level average precision scores (essentially an average of averages). With respect to each topic, average precision is the mean precision after each relevant document is retrieved (zero is used for relevant documents not retrieved). Thus, a MAP score makes some use of recall in its computation; because of this we do not directly report recall values. 4. Results and discussion The mean number of terms elicited from users with each clarification form question is displayed in Fig. 3 .
Each of these numbers reflects the average number of terms that the system used for query expansion, rather than the average raw number of terms contained in users X  responses. This figure includes the mean number of terms contained in baseline queries (mean = 9.33; standard deviation = 4.38). On average (with standard devi-ation), users provided 16.18 (11.66) terms in their responses to Q2, 10.67 (7.22) in their responses to Q3, and 2.33 (4.30) in their responses to Q4. From the means and standard deviations, it is clear that the length of users X  responses varied considerably according to question, and that the length of users X  responses varied within each question. For the 45 topics, users provided some response to Q2 for 40 topics, Q3 for 42 topics, and Q4 for only about half (20) of the topics. There were a large number of spelling errors in users X  responses to Q2 and Q3 (18 and 10), which we corrected.

We were a bit surprised by the results of Q4, by both the actual number of users who responded and by the average number of terms that these users provided. Given the success of some groups from the 2003 HARD
Track using a similar question, we expected to elicit more terms with this question than with Q2 or Q3. This was especially true since we expected users to be primed to respond to this question based on their interactions with clarification forms provided by other Track participants. The low response rate to Q4 might be a result of users X  preferences for communicating in natural language rather than keywords. These results might further provide some support that Q2 and Q3 from our clarification form are better techniques for eliciting informa-tion from users about their information problems than Q4. However, these results might also be explained by an order effect. Unfortunately, we did not set up our clarification form to explicitly compare the differences in these questions. Instead, questions were always presented in the exact same order, rather than rotated. We purposely choose to list the questions in this order because we felt that the questions made the most logical sense in this order. Thus, it might be the case that users were just more fatigued by the time they reached
Q4, or out of time, or did not feel that they had anything new to add. 4.1. Overall performance
The performance of each run is displayed in Fig. 4 . The MAP score for our baseline run was 0.2843. As can been seen in the figure, our experimental runs out-performed the baseline run and all pseudo-relevance feed-back runs.

The pseudo-relevance feedback runs only out-performed the baseline in one case, when 50 terms were used for query expansion. However, in no case did these runs outperform runs constructed using terms from our clarification form. We did three follow-up pseudo-relevance feedback runs using 100, 150 and 200 terms, and found that performance was nearly identical to runs using 50 terms (0.2857, 0.2878, and 0.2843). Thus, in this case, performance did not continue to increase with additional terms. These results seem to indicate that the pseudo-relevance feedback techniques are not particularly effective in this retrieval situation. This could be because the terms that are used from these documents were not particularly good or discriminating, or because the documents themselves were not particularly good. It might be the case that the additional terms that were added for the run using 20 terms for feedback were responsible for the very slight increase in performance over the baseline and the pseudo-relevance feedback runs using 5 and 10 terms. Increasing the number of docu-ments used for pseudo-relevance feedback might be a more effective way to improve performance in this situation, but we leave this for future work. Overall, these results suggest that in some situations the user is perhaps a better source of terms for query expansion, rather than retrieved documents.

The relevance feedback runs using known relevant documents (our upper bounds runs) performed very well and are not included in Fig. 4 . As one might expect, these runs performed well, with the 50-term run perform-ing the best. The MAP for each run using 5, 10, 20 and 50 terms for query expansion was 0.4367, 0.5284, 0.5743 and 0.6129, respectively. When there are known relevant documents, it appears that the system is quite effective at selecting terms for query expansion. Clearly, if our system had retrieved these ten documents first, then the pseudo-relevance feedback runs would have performed better. However, if users X  initial queries are ambiguous, then this is a very unlikely event.

The terms that we elicited from users for query expansion improved retrieval performance in all cases. The worst performing experimental run was the run that used the terms elicited by Q4 (MAP = 0.2985) of the clarification form ( X  X  X lease input any additional keywords that describe your topic X  X ). Recall that this question elicited the fewest number of terms (2.11) from the fewest number of topics ( n = 20). Q3 (MAP = 0.3018) increased performance only marginally over Q4, even though users responded to this question for a larger number of topics ( n = 42) than Q4, and the average length of these responses was longer (10.73). Recall that
Q3 asked users to indicate why they wanted to know about the topic. It might be the case that this information alone is insufficient in improving retrieval. For instance, one user responded,  X  X  X  am female and have been studying martial arts since I was 10 years old. I am now practicing Muay Thai boxing, Brazilian Jiu Jitsu and mixed martial arts. I want to know more about other women X  X  experiences in these sports, and also what the best competitions are X  X . This response is somewhat informative in its own right. However, consider two other responses,  X  X  X o see the combination of tradition and modernity and locate where that mixing occurs X  X  and  X  X  X hat they are able to mobilize a wide scale movement based on the socioeconomic situations (namely, consensus about inequality) X  X . These responses were generated within the context of some other known infor-mation (i.e., that provided in the TREC-topic or in response to the preceding clarification form question, Q2), and are ambiguous by themselves, but accretive in combination with responses to other questions. As Ingwer-sen X  X  (1996) work suggests, these types of questions elicited contextual information from users about their information needs.

Q2 performed better than any other single question from the clarification form (MAP = 0.3279). This ques-tion asked users to describe what they already know about the topic, and resulted in the lengthiest responses from users with respect to the clarification form. We are not able to say definitively that this question caused the lengthiest and most effective responses from users, since it might be the case that users X  response behaviors are a result of the location of this question on the clarification form rather than Q2 X  X  inherent  X  X oodness X .
However, the information that users provided in response to this question typically contained background and contextual information, which is important information that is usually not provided by users in real-world information-seeking situations. This, we feel, is strong evidence for the value of this particular question.
Our combination runs were the most effective in this experiment, with the combination of Q2, Q3 and Q4 performing the best. The order in which these questions performed in combination are consistent with how they performed alone: Q4 (0.2985) &lt; Q3 (0.3018) &lt; Q2 (0.3279) and Q3Q4 (0.3325) &lt; Q2Q4 (0.3474) &lt;
Q2Q3 (0.3495) &lt; Q234 (0.3685). Although none of these differences were statistically significant, these results do not tell the whole story since they include analysis of all topics, regardless of whether users provided any responses to the particular clarification form question. 4.2. Paired sample performance
To further understand our results, we conducted paired samples t -tests between baseline and experimental runs. In these paired tests, we only included topics for which users provided some response to the clarification form question. For instance, the comparison between the baseline run and the Q2 run consists of 40 topics, while the comparison between the baseline and Q4 runs consists of 20 topics. For combination runs, we only included topics for which there were responses to all questions. Results of these paired samples t -tests are dis-played in Table 2 . For each pair, the table shows the number of topics included in analysis, MAP scores (means and standard deviations), and p -values of t -tests (ns = not significant). The degree of freedom was N 1 in each case.

For all but two pairs, there were statistically significant improvements in retrieval performance of the experi-mental runs over the baseline runs. It is difficult to compare these results since the number of topics differs in each case. Although the largest difference in performance between baseline and experimental runs occurred for
BL-Q2Q4, this difference did not result in the strongest p -value. However, it is clear that all MAP scores are higher in this analysis than the previous analysis and that Q234 was the highest performing run overall. Con-trary to the overall results, Q3 rather than Q4 is the lowest performing run which resulted in Q2Q4 performing a little better than Q2Q3. These results demonstrate that when questions elicit information from users, significant improvements in performance are possible.

These results further corroborate the general findings in overall performance reported in the previous sec-tion and provide strong evidence for the effectiveness of runs comprised of a combination of representations of the user X  X  information need, or a polyrepresentation. Taken together, these results suggest that each question is eliciting somewhat different information, which, when used in combination, present the most useful evidence for query expansion. As noted earlier, responses to Q3 often followed from, or were within the context of, responses to Q2. This suggests that probing users with a number of different, but related questions might elicit the most robust and useful problem descriptions. Furthermore, eliciting keywords from users can be helpful, but only in combination with other information. Recall that the addition of terms from Q4 boosted the per-formance of the Q2Q3 run by 0.019. Although this boost is not great, it suggests that responding to questions using natural language might prime users for providing better keywords. In cases where keywords duplicate terms from previous questions, this information can perhaps be used for indirect, but user-specified, term weighting. 4.3. Topic-level improvements We examined the number of topics for which the additional information elicited from our form improved.
Table 3 summarizes the number and percent of topics that each question or combination of questions improved, as measured by MAP scores. The second column of Table 3 shows the number of topics that each experimental run improved and the third column indicates this number as a percentage of the total number of topics. The fourth column shows the number of topics where users provided some response to each question or combination of questions. For question combinations, a topic is counted as having a response if the user pro-vided some response to at least one question. The fifth column of the table shows the percentage of topics with responses where improved MAP scores were observed.

Associated percentages (columns 3 and 5, respectively) are calculated differently. In the third column, per-centages are calculated as the number of topics each run improved [the number in column 2 divided by the total number of topics (45)]. The ranking of these percentages follows essentially the same order as the ranking of their MAP scores discussed in the previous two sections. In the fifth column, percentages are calculated using the number in column four as denominators. These percentages are less varied than those reported in column three and provide evidence that the three questions lead to fairly consistent improvements in MAP scores if they are successful in eliciting responses from users.

There was only one topic for which the user responded to all three questions and performance of all exper-imental runs was worse than baseline performance. An examination of the topic demonstrates that the topic was very general, and that the user was most likely interested in high recall, rather than high precision (Topic
Title: AIDS in Africa; Description: What is the state of AIDS in Africa?). The baseline MAP score was 0.517, while scores for experimental runs using Q2, Q3, and Q4 alone were: 0.339, 0.487, 0.297. It appears that the information provided by the user in response to our questions functioned as noise, introducing documents that were off-topic or perhaps too specific. This suggests that the techniques explored in this experiment may be more appropriate for retrieval situations where high precision is desired. 4.4. Query length and performance
The overall performance results seem to suggest a strong relationship between query length and perfor-mance, at least for our experimental techniques. Previous research has demonstrated a positive relationship between query length and performance ( Buckley et al., 1994 ) and we were interested in seeing if this finding held true in our study. A scatter-plot of performance according to query length for all runs is displayed in Fig. 5 .

From the figure, it is quite clear that there is a strong linear relationship between query length and perfor-mance for our experimental runs. A regression analysis provided further evidence for this, MAP = 0.263 + 0.000265 (query_length), p = 0.000, r 2 = 0.9068 (adjusted, 0.891), suggesting that query length is a very good predictor of MAP. This result is quite important since it demonstrates a strong positive relationship between query length and performance even in situations where the query is composed solely of user-generated terms.
 5. Conclusions
We found large differences in the lengths of users X  responses to each of the elicitation questions that we used on our feedback form, and we were excited to see that users were willing to provide such lengthy responses to some of our questions. One of our original motivations for this experiment was to identify a technique that could elicit better information need descriptions from users. Our techniques were successful at eliciting infor-mation from users, information that in turn, improved retrieval performance. This demonstrates that users can provide additional information about their information needs beyond three term queries and that this information can improve retrieval. This research further provides empirical support for Ingwersen X  X  notion of polyrepresentation, and more specifically polyrepresentative extraction of information needs. Of course, these users were in an experimental situation so their behaviors may not be reflective of what they would do in real-world situations. However, it seems that users do have more to tell when querying search systems and this additional information can improve retrieval.

Q2 was the most successful single question, both in the amount of information it elicited and increase in performance. While it was the case that Q2 was the first question that users encountered, the background and contextual information that users provided in their responses to this question provide support for its goodness as a question, regardless of order effects. Indeed, the experimental run of users X  responses to all three of our feedback form questions out-performed all other runs, which suggests that probing users with a number of different, but related questions might elicit the most robust and useful need descriptions.

As previous research has demonstrated ( Ruthven, 2003 ), users often have a difficult time making good expansion decisions, if they are even willing to do so. It can be difficult for users to choose terms since terms are often presented to users out of context. Users may also be reluctant to select terms if they do not under-stand why they were suggested or from where they came. Further, users may be censoring themselves based on how they think a retrieval system works and may be overly critical in their determination of which system sug-gested terms to select. In this study, pseudo-relevance feedback runs did not perform particularly well. These results provide some evidence that in some situations users are perhaps better sources of terms for query expansion, rather than retrieved documents and that if properly probed, users will provide large quantities of quality feedback.

In this experiment, we also demonstrated a significant relationship between query length and performance, using only user-generated terms. This result corroborates what others have found in batch-mode experiments and suggests that we should be working harder to elicit lengthier queries from users to improve search preci-sion. However, it is unclear how these results generalize to Web searching. In this study, searching was con-ducted on a closed system, with a rather homogeneous corpus of documents. Generally, Web queries are very short and some search algorithms are adept at dealing with short queries for some types of information needs. It is unclear if the relationship between query length and performance is positively related in Web settings.
While Belkin et al. (2002) found descriptive evidence that suggested query length and performance were positively related in Web search settings, additional studies need to investigate this relationship more explicitly and systematically. We believe that the relationship between query length and performance is affected by task type and, in particular, we conjecture that task type moderates the relationship between query length and per-formance. We are currently evaluating the effectiveness of the form used in this study in a Web setting with a new set of users who have real information needs. Results from this study should provide additional evidence of the effectiveness of this form with respect to eliciting better information need descriptions. Within this con-text, we are also evaluating the effectiveness of this form as technique for use in digital reference settings, where users pose their information needs to virtual librarians via the Web. There currently exists no standard for Web-based reference forms and we hope that results of this study will help define such standards.
For our future work, we would also like to compute and compare the query clarity score ( Cronen-Town-send, Zhou, &amp; Croft, 2002 ) of each initial query with the query clarity after the information from each clar-ification form question has been added. Examining the query clarity score of each query might allow us to predict which topics are most likely to benefit from a technique such as the one described in this paper.
We cannot conclude this paper without acknowledging some limitations to this study, which potentially have some impact on the generalizability of the results. This study was carried out as part of the TREC
HARD track and because of this, followed an unusual experimental protocol. Two of the more important aspects of this protocol which might have impacted our results are the delay between the time that users defined their topics and the time that they actually completed our clarification form, and the fact that users completed a number of clarification forms in a row. Further, only a small number of users ( n = 13) partici-pated in this experiment. Although the potential impact of the experimental protocol on the validity and reli-ability of our results cannot be ignored, we feel, nevertheless, that our results are important and make an essential contribution to research on query expansion and polyrepresentative extraction of information needs.
After reading this paper, one might be left wondering just how  X  X oquacious X  can we expect users to be in a real information-seeking situation? After all, it is generally believed that users will exert the least amount of effort possible. Users should not be dismissed as unwilling and unable to articulate aspects of their information needs. Instead, we should work to design clever and creative techniques for encouraging users to be loquacious rather than reticent, both during their initial querying and during follow-up interactions. While it is still the case that we are often  X  X  X elping people find what they don X  X  already know X  X  ( Belkin, 2000 ), we should also strive to help people articulate what they do not know is important.
 References
