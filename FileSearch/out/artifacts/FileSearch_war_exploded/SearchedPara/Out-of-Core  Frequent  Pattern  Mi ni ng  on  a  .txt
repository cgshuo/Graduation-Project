 In this work we focus on the problem of frequent itemset mining on large, out-of-core data sets. After presenting a characterization of existing out-of-core frequent itemset mining algorithms and their drawbacks, we introduce our efficient, highly scalable solution. Presented in the con-text of the FPGrowth algorithm, our technique involves sev-eral novel I/O-conscious optimizations, such as approximate hash-based sorting and blocking, and leverages recent ar-chitectural advancements in commodity computers, such as 64-bit processing. We evaluate the proposed optimizations on truly large data sets, up to 75GB, and show they yield greater than a 400-fold execution time improvement. Fi-nally, we discuss the impact of this research in the context of other pattern mining challenges, such as sequence mining and graph mining.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining; General Terms: Algorithms,Performance.
 Keywords: out of core, itemsets, data mining, pattern min-ing, secondary memory.
The field of knowledge discovery and data mining is de-voted to the challenge of extracting useful information from raw data. Over the past decade, technological advances have allowed us to gather an increasing amount of data in the areas of science, engineering, and business. Unfortunately, our ability to collect this data far outstrips our ability to analyze it efficiently . There are two main reasons for this problem. First, many data mining algorithms are computa-tionally complex and scale non-linearly with the size of the data set. Second, when the data sets are extremely large, many data mining algorithms demand a memory footprint This work is supported in part by NSF grants #CAREER-IIS-0347662, #RI-CNS-0403342, and #NGS-CNS-0406386; Contact email  X  srini@cse.ohio-state.edu Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. that exceeds the size of main memory. This can lead to extreme slow down due to thrashing effects. While both these problems need to be addressed for ensuring the scala-bility of data mining algorithms, much of the work to date has focused on the first issue. The second issue, while not completely ignored, has not been satisfactorily addressed.
In this work, we focus on the relatively mature problem of frequent itemset mining [1]. Since its inception, the commu-nity has witnessed a proliferation of efficient frequent itemset mining algorithms in the literature [5, 9, 10, 13, 19]. Specif-ically, we concern ourselves with finding frequent itemsets in very large data sets, where the memory footprint exceeds main memory for the majority of the execution.

There are several feasible approaches to mine out-of-core data sets, which fall into two methodologies. The first method-ology is to use an existing in-core algorithm, and leverage disk space to allow the meta-structures to exceed main mem-ory. The disk-resident memory can either be managed by the virtual memory system, or explicitly by the program-mer with the file system. Our performance study reveals that when these algorithms rely on the virtual memory sys-tem, they are extremely inefficient on out-of-core data sets. These algorithms incur a significant number of page faults, resulting in poor CPU utilization. Attempts to avoid the virtual memory system with explicit file handling are of-ten difficult to program, and become quite daunting when the meta structures are pointer-based. The second method-ology is to partition the data set such that each partition fits in main memory, use an in-core algorithm to mine each of these partitions, and aggregate and post-process the fre-quent itemsets discovered in each partition to obtain the global results [19]. While this approach is useful in some situations, typically it results in significant post-processing overhead and redundant computation. In essence, the for-mer approach leverages the benefits of search space pruning by projection at the cost of decreased memory system per-formance, while the latter approach maintains good memory performance at the cost of a poor search space traversal.
To address the aforementioned shortcomings, we propose to take one of the most efficient in-core frequent itemset mining algorithms, FPGrowth [13], and identify strategies by which it can be made I/O-conscious. The proposed I/O-conscious optimizations ensure that the resulting algorithm exhibits excellent temporal and spatial locality. Leveraging recent architectural innovations such as 64-bit microproces-sors, the algorithm affords excellent scaling on data sets close to available disk capacity, while maintaining good CPU uti-lization. This methodology results in a 400-fold improve-ment over any of the strategies we evaluated. While our optimizations focus on the problem of frequent itemset min-ing, we believe that the key findings of this work will hold for many data mining tasks, especially those in the frequent pattern domain, such as graph and sequence mining.
Specifically, we make the following contributions:
Frequent itemset mining plays an important role in a range of data mining tasks. Examples include mining as-sociations [1], correlations [3], causality [20], episodes [15], and emerging patterns [7].

The frequent pattern mining problem was first formulated by Agrawal et al. [1] for association rule mining. Briefly, the problem description is as follows: Let I = { i 1 ,i 2 ,  X  X  X  ,i asetof n items, and let D = { T 1 ,T 2 ,  X  X  X  ,T m } be a set of transactions, where each transaction T i is a subset of I itemset i  X  I of size k is known as a k -itemset. The support of i is of transactions in D that have i as a subset. The frequent pattern mining problem is to find all i  X  D with support greater than a minimum value, minsupp .

Agrawal and Srikant [2] presented Apriori , the first effi-cient algorithm to solve this problem. Apriori traverses the itemset search space in breadth-first order. Its efficiency stems from its use of the anti-monotone property: If a size k -itemset is not frequent, then any size ( k +1) -itemset con-taining it will not be frequent . The algorithm first finds all frequent 1-items in the data set, and then iteratively finds all frequent l -itemsets using the frequent ( l  X  1)-itemsets discovered previously.

This general level-wise algorithm has been extended in several different forms leading to improvements such as DHP [16] and DIC [3]. DHP uses hashing to reduce the number of candidate itemsets that must be considered during each data set scan. Furthermore, it progressively prunes the transac-tiondatasetasitdiscoversitemsthatwillnotbeuseful during the next data set scan. DIC [3] processes the data set in chunks and considers new candidate itemsets as they are discovered in the chunk. Unlike Apriori , DIC does not require that the entire data set be scanned for each new candidate. Such an approach affords fewer passes.
We proposed Eclat [21] and several other algorithms that use equivalence classes to partition the problem into inde-pendent subtasks. The use of the vertical data format in Eclat allows for fast support counting by set intersection. The independent nature of subtasks, coupled with the use of the vertical data format, results in improved I/O efficiency because each subtask is able to reuse data in main memory. We also developed schemes for parallelizing and improving the performance of apriori-style algorithms on SMP systems [17, 18]. This work is the first to illustrate the benefits of
Table 1: A transaction data set with minsupp =3 improving memory performance in data mining algorithms in both the sequential as well as the parallel setting.
Han et al. presented FPGrowth [13], an algorithm that significantly reduces the number of data set scans required. FPGrowth summarizes the data set into a succinct prefix tree or FP-tree . An additional benefit is that it does not have an explicit candidate generation phase. Rather, it generates frequent itemsets using tree projections in main memory. The payoff is an improved search space traversal. However, the pointer-based nature of the FP-tree requires costly dereferences. In previous work, we have shown this algorithm can be made cache-conscious [18, 8].
 As our optimizations are presented in the context of the FPGrowth algorithm, we will next describe the FP-tree data structure and the FPGrowth algorithm in more detail. A prefix tree (or an FP-tree [13]) is a data structure that pro-vides a potentially compact representation of transaction data set. Each node of the tree stores an item label and a count, where the count represents the number of transac-tions which contain all the items in the path from the root node to the current node. By ordering items in a transac-tion based on their frequency in the data set, a high degree of overlap is established.

A prefix tree is constructed as follows. First, we scan the data set to produce a list of frequent 1-items. Second, we sort the items in frequency descending order. Third, we sort the transactions based on the order from the second step. Fourth, we prune away infrequent 1-items. Finally, for each transaction, we insert each of its items into a tree, in sequential order, generating new nodes when a node with the appropriate label is not found, and incrementing the count of existing nodes otherwise.

Table 1 shows a sample data set, and Figure 1 shows the corresponding prefix tree. Each node in the tree consists of an item, count, nodelink ptr, (which points to the next item in the tree with the same item-id) and child ptrs (a list of pointers to all its children). Pointers to the first occurrence of each item in the tree are stored in a header table.
To compute the frequency count for an itemset, say ca ,we proceed as follows. First, we find each occurrence of item c in the tree using the node link pointers. Next, for each occurrence of c , we traverse the tree in a bottom up fashion in search of an occurrence of a . The count for itemset ca then the sum of counts for each node c in the tree that has a as an ancestor.

The FPGrowth algorithm is presented in Figure 2. First, it builds a prefix tree from the transaction database, re-moving all infrequent items, using the procedure outlined earlier. Next, using this prefix tree, the algorithm iterates through each item  X  in the tree, and performs two opera-tions. In operation one, it uses the prefix tree to find all frequent items in the conditional pattern base for the item  X  . This involves scanning the tree bottom up beginning at all node locations for item  X  . The header table provides a starting point for this search. The remaining locations for item  X  are derived using the node link pointers. In oper-ation two, assuming there has been at least one frequent item discovered in the conditional pattern base of item  X  a projected database (represented as a prefix tree) is con-structed for item  X  . The projected data set for an itemset is the subset of the transactions in the data set that con-tains the itemset. This operation also involves scanning the conditional pattern base of item  X  ,insearchforitemsto be included in the projected database. For each projected database or conditional FP-tree that is built, the algorithm proceeds recursively.
There are several approaches to find frequent itemsets in out-of-core data sets. In this section, we will briefly de-scribe these approaches and identify their strengths and weaknesses. All evaluations presented in this Section were performed on a PentiumD desktop PC with 256MB of RAM.
For the past several decades, operating systems have pro-vided programmers with a virtual address space that is sig-Table 2: CPU utilization for FPGrowth and Cache-conscious (CC) FPGrowth on Webdocs at varying support. nificantly larger than the physical address space. Paging mechanisms have been responsible for moving instructions and data in and out of memory, as and when needed. Tradi-tionally, commodity PCs have been 32-bit processors, which have a limit of 4GB on the amount of available virtual mem-ory. Today, desktop PCs are afforded 64-bit memory ad-dressing via technological innovations in CPU design. Ex-amples of these new CPUs include the Intel PentiumD, the IBM PowerPC, and the AMD Athlon 64. This translates to a nearly unlimited amount of virtual memory. An in-core solution can then use this large amount of virtual memory to process out-of-core data sets, while relying on the OS X  X  paging mechanism to automatically handle the movement of data between main memory and the disk sub-system. The key benefit of such an approach is its implementation sim-plicity.

To evaluate the efficacy of such an approach, we con-sider FPGrowth [13], which is one of the most efficient fre-quent itemset mining algorithms to date [9], and our cache-conscious FPGrowth algorithm, as presented in a previous effort [8]. We measure CPU utilization of both these al-gorithms on large, out-of-core data sets. The unmodified FPGrowth implementation is by Grahne and Zhu [11], as it has been shown to be amongst the most efficient [9].
Table 2 presents the CPU utilization of FPGrowth for min-ing the Webdocs data set (see Table 5 for details). It is clear that the CPU is largely idle, rendering a VM-based solution ineffective. Even our cache-conscious version, although an improvement, fairs quite poorly. In FPGrowth , about 28% of the execution time is spent in Build First Tree() ,the routine that builds the very first prefix tree. 10% of the execution time is spent in the Count FPGrowth() routine. This routine finds the set of all viable items in the FP-tree (projected data set) that will be used to extend the frequent itemset at that point in the search space. 62% of the execu-tion time is spent in the Project FPGrowth() routine, which scans the FP-tree to build a new projected FP-tree for the next recursive call.

Based on the provided performance characterization, to-gether with an understanding of the algorithm, we attribute the poor CPU utilization to the following reasons. First, when the prefix tree is created, the memory address of a node in the tree is relatively independent of the memory address of its child and parent nodes. This is because trans-actions in the input data set can appear in any order. Con-sequently, prefix tree accesses exhibit poor spatial locality, both during the tree construction phase and the subsequent traversal phase. Furthermore, the algorithm does not bene-fit from page prefetching, because its memory access pattern lacks structure. These issues were addressed in the cache-conscious version, which provides an improvement to the CPU utilization over the original FPGrowth . Second, when the prefix tree is larger than the size of main memory, we Table 3: Overheads from using a partitioning-based approach on Webdocs at varying supports. have a negligible amount of temporal locality during exe-cution. This prominent issue greatly affects every in-core frequent itemset mining algorithm we evaluated (only two shown due to space constraints). The above mentioned rea-sons cause the processor to wait on the completion of a page fault for the overwhelming majority of the execution time. Even our cache-conscious algorithm falters considerably due to page faults while building the first tree . In our case, the penalty is doubled because the cache-conscious algorithm builds a second, spatially improved tree.
Savasere et al. presented Partition [19], an approach for out-of-core itemset mining. To mine large out-of-core data sets, they propose to first subdivide the original data set into smaller data sets that can be processed in main mem-ory. Next, each of these partitions in mined in main memory using an in-core algorithm. This is followed by a union op-eration on all locally frequent itemsets discovered in each partition. Finally, to find the exact set of frequent itemsets, exact counts of all itemsets in the union are determined us-ingadatasetscan.

While this approach works well in some situations, it suf-fers from the following drawback. The technique is efficient only when the union set of discovered locally frequent item-sets (frequent in any one partition) is close to the actual set of actual frequent itemsets (in the data set). If this is not the case, one must calculate the exact support count of a much larger set of itemsets, often resulting in significant compu-tational overheads. In a case study, we measure the num-ber of false positives (number of itemsets in the union) and the number of true positives (number of globally frequent itemsets) as we varied the minimum support. We find that the number of false positives increases exponentially with decreasing support, as illustrated in Table 3. This finding renders a partitioning-based approach infeasible on large, out-of-core data sets.
Grahne and Zhu presented Diskmine [12], an approach for out-of-core itemset mining that uses projections to par-tition the data. The approach recursively projects the data set until it fits in main memory and then uses an in-core algorithm to mine the projected data set. This technique affords improved memory system usage at the expense of a larger search space and increased computation. The premise is that during execution, the first prefix tree may not fit in main memory, at which time the tree is deleted and all fre-quent 2-itemset projections are created. If any of the result-ing data structures for each frequent 2-itemset projection do not fit in memory, the algorithm recurses to frequent 3-itemset projections. This proceeds until the data structure does fit in main memory, at which point the mining process can proceed efficiently. The frequent itemsets can be deter-mined by simply taking a union of the itemsets mined from each projection.
 Table 4: Number of additional data set scans needed when using recursive projection on the Webdocs data set at various supports.

We find there are two significant drawbacks to this ap-proach. First, when projecting the data set, if the data structure does not fit in memory, the algorithm must project by including an additional item, and deleting the current data structure from memory. The new projection requires an additional full scan of the data set. Second, explicit can-didate generation is then required, which can be very ex-pensive [13]. In other words, when the frequent 1-item tree does not fit in memory, then all combinations of frequent 2-itemsets must be projected, even if a large subset of these itemsets are not actually frequent. In effect, one of the main advantages of the FPGrowth algorithm (elimination of can-didate generation) is negated.
 We implemented this technique to evaluate its potential. In Table 4, we show the number of additional scans as a function of support for the Webdocs data set, and its asso-ciated execution time cost 1 . If the frequent 2-itemset trees do not fit in memory, this constraint is heightened. Equa-tion 1 evaluates the additional number of data set scans, allowing n to be the number of frequent 1-items and r to be the depth of the recursion up to the point the projected trees fit in main memory. Current techniques to mitigate this liability, such as com-bining multiple frequent 2-itemset projections with a single scan, break down at low supports [12]. Intuitively, this tech-nique assumes frequent 2-itemset projections to be many times smaller than main memory (so as to afford combin-ing), which is often not the case.
One last method we briefly describe involves adjusting the problem definition to avoid exceeding main memory. Exam-ples include mining for closed i temsets, maximal itemsets, or non-derivable itemsets. These related problem definitions are quite useful in their own right, and attack two concerns simultaneously. First, they can reduce the amount of state and computation required. Strategies such as partitioning the data set can be more attractive when less information is required to process the data set. Second, they often result in a reduced result set, which eases human analysis. However, a drawback to this strategy is that the problem of mining large data sets is merely pushed to a slightly larger data set; an algorithm that could previously handle a 4GB data set can now perhaps process a 6GB data set. Such approaches do not permit scaling on truly large data sets and the under-lying concern continues to persist. In this work we maintain the problem definition to be that of mining frequent item-sets because we feel that this information has uses beyond the values of the previous alternatives. Furthermore, from an algorithmic view point, frequent itemset mining shares We require about 39 seconds to touch each item in the Web docs data set.
 common structure with other itemset mining tasks, such a closed itemset mining and ma ximal itemset mining, and we believe our optimizations can be applied to these tasks with comparable benefits.
Of the methods described in the previous section, scaling using virtual memory results in the least amount of com-putational overhead. However, when executions spill onto disk, such algorithms exhibit severe performance degrada-tion. This is understandable; typical main memory access times are about 15 nanoseconds, while typical disk access times are 5 milliseconds, constituting a 333,000-fold gap in performance. This gap is likely to widen in the future, be-cause memory access times are improving faster than disk access times. Our strategy to achieve an out-of-core solution is to minimize the performance degradation due to this gap through data and computation restructuring, to improve lo-cality. In this Section, we present several novel techniques for improving the I/O performance of frequent itemset min-ing algorithms. We choose to present these techniques via FPGrowth because it has been shown to be the most efficient frequent pattern mining algorithm to date [9].
As discussed earlier, the initial step in FPGrowth is to construct a global prefix tree. This first tree can be quite large; at low supports it can approach or even exceed the size of the data set. For in-core data sets, this construction time is typically a small percentage ( &lt; 5 %) of the total mining time. For out-of-core data sets, however, construction of the Figure 5: Geometric Partitioning Decision Diagram, for 8 Files. first tree results in severe per formance degradation. During our empirical study (presented in Section 3), we showed that our cache-conscious algorithm was a significant performance improvement over FPGrowth for out-of-core data sets. How-ever, it spent over 90% of the execution time building the first tree, due to an excessive number of page faults. The reason is that transactions within the data set appear ran-domly, which results in random writes to the tree nodes in virtual memory during tree construction. Even if the initial data set had its transactions ordered, the problem would persist since the transactions a re relabeled prior to tree con-struction (for improved overlap).

Our solution to this problem is to redistribute and ap-proximately sort the transactions after the first scan of the database. Naturally, sorting on disk is quite slow. Tradi-tional methods for external sorting (such as B-tree insertion and disk-based merge sort) do not provide an overall perfor-mance improvement. Exact sorting requires too much time. Instead, we leverage domain knowledge and the frequency information collected in the first scan to approximately sort the frequent transactions into a partition of blocks. Each block is implemented as a separate file on disk. The algo-rithm guarantees that each transaction in block i sorts be-fore all transactions in block i +1 , and the maximum size of a block is no larger than a preset threshold. By blocking the frequent data set, we can build the tree on disk in fixed memory chunks. A block as well as the portion of the tree being updated by the block will fit in main memory during tree construction, reducing page faults considerably.
We use frequency distributions to choose one of the two partitioning algorithms listed in Figures 3 and 4 by building a simple model of the distribution. We build this model us-ing the top 10% most frequent items. Essentially, if the item frequencies follow a geometric series (in descending order), we partition based on the algorithm in Figure 3, otherwise we partition based on the algorithm in Figure 4.

We first describe the algorithm in Figure 3. Let X= | partition | , or the total number of files. Let S represent the maximum file size. We define a function such that trans-actions with the most frequent item receive the top X/2 of Figure 6: Arithmetic Partitioning Decision Dia-gram, for 8 Files.
 the blocks, transactions with the second most frequent item receive the next X/4 of the blocks, and so on. Of these top X/2 partitions, the top X/4 are dedicated to the sub-set which also contain the second most frequent item. The bottom X/4 blocks of this subset are split into two equal sections. The top X/8 is dedicated to transactions contain-ing the third most frequent item exists, and the lower X/8 for those which do not. This pattern recurses until the ex-act block number is known. Therefore, in one scan ,each transaction is inserted into one of X blocks (typically 256), based on its contents. In the case that a partition has a size above our threshold, we evaluate its local distribution and process it recursively.
 Let us illustrate this algorithm with a simple example. Suppose transaction T = { 33, 11208, 11, 678, 14, 91, 278 After scanning the data set, calculating frequencies (remov-ing infrequent items) and relabeling, the transaction be-comes T X  = { 0, 1, 4, 6, 10 } .Letthenumberoffiles | X | =8, as in Figure 5. Our task is to determine X t , the file assigned to T X . We examine the first element in the transaction, and if it is the smallest (most frequent) element possible, we as-sign it to the upper half of the possible files. In our example, 0 = 0 (first item in transactionis the most frequent item), so we reduce the potential file assignment to 0-3. The second element is also its minimum (1=1,second item is the second most frequent item), and therefore the file list is reduced to 0-1. The third element is not its minimum (4 =2,orthe third item in the transaction is not the third most frequent item), so the block is assigned to file X 1 .

If the item frequency model more closely resembles a lin-ear distribution, we partition using the algorithm in Figure 4. Effectively, we assign the first N blocks to the most fre-quent items, and assign the remaining items to the remain-ing files equally. Lines 2  X  6 assign the transaction to either a dedicated file if the item in the index is highly frequent, or a shared file if the item is not highly frequent. High fre-quency is relative; we allow for a parameter N to distinguish the threshold for the top items which receive dedicated files. In practice we set N to 5% of the total number of frequent items. A decision diagram for this algorithm is presented in Figure 6.

As stated earlier, it may be the case that a resulting file in the partition exceeds our threshold S .Ifso,wesimplyre-curse on the file with the same procedure. However, we must calculate the new start index (in the transaction) to continue the partitioning. The index can be determined solely based on the file number, using the algorithm provided in Figure 7. Note that this results in n  X  X  partition | X  ( n  X  1) files in total, where n is the number of file splitting calls. For sub-splitting files which surpass the maximum file size, we may neglect to build a model of the distribution for that subfile. Note that odd file numbers contain transactions whose last element was not the minimum (most frequent) value possi-ble, and even valued file numbers contain transactions whose last element was a minimum. In practice we have found file parity provides sufficient information to evaluate which al-gorithm to use when sub-splitting; even numbered files are partitioned geometrically and odd numbered files are par-titioned arithmetically. We provide additional details in a companion technical report [4].

With the knowledge that consecutive files are in relative order, tree building can proceed by processing the files in order with a minimum number of page faults. As will be illustrated in Section 5, this partitioning technique dramati-cally reduces the cost of building the main tree on disk, and provides a significant improvement to the total execution time.
Through the characterization presented in Section 3, we conclude that approximately 10% of the execution time is spent on finding frequent items in the conditional pattern base of an item, and an additional 62% of the execution time is spent on using the results of this step to create a new projected prefix tree. Both these procedures have very poor main memory utilization and suffer from a high page fault rate due to poor data locality.

We present the I/O-conscious prefix tree , a data structure designed to significantly improve main memory utilization through spatial locality. It is a modified prefix tree which accommodates fast bottom-up traversals, and shares many of the features we presented in the context of cache perfor-mance optimizations for in-core pattern mining [8]. As has been shown earlier, the cache-conscious algorithm is not ef-fective on out-of-core data sets. An I/O-conscious solution must be mindful of the various OS mechanisms as well as the unit of transfer between disk and main memory. However, the principles of alleviating the performance gap between the cache and main memory also apply to the case involving main memory and disk.

Given a prefix tree, our solution to improve spatial local-ity is to reallocate the tree in virtual memory, such that the new tree allocation is in depth-first order. We malloc () fixed sized blocks of memory, whose sum is equal to the total size of the prefix tree. Empirically, we found that a 4 MB block size is most effective. Next, we traverse the tree in depth-first order, and (in one pass) copy each node to the next location (in sequential order) in the newly allocated blocks of virtual memory. This simple reallocation strategy pro-vides significant improvements, because FPGrowth accesses the prefix tree several times in a bottom up fashion, which is largely aligned with a depth-first order of the tree. Sec-ond, our node size is much smaller than the original node size, because we do not include child pointers, node pointers, and counts. The node fields node link pointer and count are required at the start of each bottom up traversal. There-fore, these fields are stored in a separate structure, without any performance degradation, as node link pointer and count accesses are not along the critical path.
Temporal locality states that recently accessed memory locations are likely to be accessed again in the near future. Designers of OS paging mechanisms work under the assump-tion that programs will exhibit good temporal locality, and store recently accessed data in main memory accordingly. Therefore, it is imperative that we find any existing tempo-ral locality and restructure computation to exploit it.
The goal of restructuring the algorithm is to maximize reuse of the prefix tree once it is fetched into main mem-ory. We accomplish this by reorganizing computation, and thus accesses to the prefix tree. Our approach relies on page blocking , and is analogous to our tiling techniques [8] for improving temporal locality in in-core pattern mining algo-rithms. The core loops for FPGrowth and Blocked FPGrowth are presented in Figures 8 and 9, respectively. It operates as follows:
First, we break down the tree into relatively fixed sized blocks of memory (page blocks) along paths of the tree from leaf nodes to the root. This is possible because our tree is allocated in depth first order. The blocks are identified using a starting and ending memory address. We would also like to point out that these blocks can partially overlap. Next, we iteratively fetch each block into main memory [8]. Then for each frequent item i , we traverse the part of its conditional pattern base that has leaf nodes located within the block X  X  starting and ending address. Thus, once a block is brought into the main memory, conditional pattern base accesses for all items that hit the block are managed in main memory. We would also like to point out that by controlling the size of the block, one can tune the implementation to better utilize the available m emory budget. A similar re-structuring technique is used for generating projected prefix trees. For further details, please refer to [8].
To evaluate our I/O optimizations, we employ a typical current-day desktop machine; an Intel PentiumD with a P-ATA 320GB hard dri ve, running 64-bit SUSE Linux 10. Of this 320GB of disk space, we allocate 170GB for swap pur-poses (VM). The PentiumD has two cores, but in this work, we only use 1 core throughout all experiments. We use two RAM configurations; 256 MB of DDR2 RAM and 1 GB of DDR2 RAM, and will specify the configuration in use in each experiment to follow. We believe this machine repre-sents a commodity machine. In fact, we built it for about $500.
We use several data sets to perform our evaluation. Web-docs is a 1.48GB data set from the Frequent Itemset Mining Implementations Repository (FIMI) [9] containing about 1.6 million transactions. This is the largest data set in the FIMI Repository, and at sufficiently low support the memory foot-print for the tested algorithms is several times the size of main memory (using 256MB RAM). In addition, we gener-ate several synthetic data sets using the IBM Quest Data Set Generator. D1 is a small 1GB data set with 4.7 mil-lion transactions. Transactions average 40 items in length, with 10,000 distinct items. D60 is a relatively sparse 60GB data set. It also averages 40 items per transaction but con-tains 100,000 distinct items. D75 is a much denser data set than D60, due to its increased transaction length (100) and decreased number of items (20,000). D60 and D75 are de-signed to test the end scalability of our solution. Details on these data sets can be found in Table 5.
We first evaluate the impact of spatial and temporal lo-cality improvements due to our prefix tree modifications and our page blocking scheme. For this experiment, we use 256MB of RAM. We compare our algorithm (without ap-proximate sorting) against FPGrowth by Grahne and Zhu [11]. As can be seen in Figure 10, the algorithms behave similarly while in core, but we are able to improve execu-tion time by 10-to 15-fold when the execution exceeds main memory. This is a significant improvement, and is afforded by reducing the time required during the tree mining phase. In fact, our timing results show that the tree mining time is reduced by two orders of magnitude. However, the over-all execution time is only reduced by 10-to 15-fold because we incur twice the tree building time, since we must first construct a prefix tree, and then construct an I/O-conscious tree. This tree construction now becomes the dominant bot-tleneck, representing approximately 90% of the total execu-tion time.
We now evaluate how approximate sorting can help alle-viate this bottleneck. For this experiment, we use 1GB of RAM. We set the filesize threshold to be 30% of main mem-ory; we automate this via the /proc/meminfo system file. As can be seen in Figure 11, by using approximate sorting (OoC), we are able to reduce execution time on D60 and D75 by up to 25-fold. This improvement is on top of that afforded by the spatial and temporal locality improvements. For D60 at 0.2% support, 2041 files were generated, and 53.5 GB of virtual memory was consumed. The main prefix tree was 29.4GB. The number of files generated did not have an impact on performance, so long as the file split size was large enough to accommodate a low number of recursive splits.
In summary, effective approximate sorting of the frequent transactions after the first data set scan reduces mining times from a few days to a few hours (on very large data sets), as the tree building phase is no longer the bottleneck .
The goal of this effort is to design an algorithm which can mine truly large data sets on a relatively inexpensive machine. To evaluate our progress towards achieving this goal, we investigate the performance of our algorithm on the D60 and D75 data sets (presented in Table 5). Re-turning to Figure 11, we examine how execution time varies with decreasing support. Our algorithm maintains a rea-sonable execution time throughout. We would like to point out that the overall time includes the initial two disk scans. For small data sets, these times are inconsequential. How-ever, for large data sets, these scans are costly. On D60 and D75, we require approximately 1619 and 2008 seconds to touch the entire data set, respectively. This suggests a lower bound on execution time. When using our algorithm, we also touch the frequent portion of the data set at least one more time, due to our approximate hash sorting phase. Still, this cost is small in comparison to the excessive num-ber of page faults incurred when using a naive VM-based solution, or the large number of additional scans required with the recursive projection-based techniques.

We claim that by improving data locality (and reducing page fault rate), we can sustain high CPU efficiency on a 64-bit processor. This will allow us to scale to data sets of any size, independent of the available main memory (given sufficient disk space). To evaluate this claim, we performed the following experiment using 1GB of main memory. We increased the portion of D75 that we mine, while maintain-ing a constant relative support of 1.5%. We recorded the execution time, the virtual memory consumption, and the total CPU utilization with increasing data set size. We cal-culated the CPU utilization by dividing the total user time of the execution by the total wall time. As seen in Figure 12, CPU utilization is relatively stable with increasing data set size. More importantly, we can see that overall system per-formance does not degrade as the working data structures in the algorithm move farther out of main memory. As il-lustrated, the total virtual memory required increased from 1.5GB to 34GB, but system utilization remained at about 20 % and execution time scaled linearly. We believe that with increased disk space, we can mine very large data sets ( &gt; 500GB) at relatively low support values.
To compare our algorithm with existing approaches, we return to the Webdocs data set. Larger data sets are not suitable for existing algorithms. Here we use 256MB of RAM. The goal of this experiment is to evaluate how well existing algorithms cope as they exceed available main mem-ory, and contrast them with our performance. We include FPGrowth here because it is the algorithm that we improve upon. We include AFOPT because its design intrinsically accommodates large data sets [14] through reduced mem-ory consumption. However, as shown in Figure 13, when the virtual memory footprint of AFOPT exceeds the size of main memory, its performance degrades significantly. At a support of 20%, all three algorithms must use disk res-ident memory. The slowdowns in AFOPT and FPGrowth are attributed to the fact that they were not designed to ex-hibit high spatial and temporal locality, and thus do not uti-lize the memory hierarchy efficiently. FPGrowth fairs worse between the two, as it has a larger memory footprint. In fact, we have tested all the implementations from the FIMI repository, including algorithms such as Apriori and Eclat , and all exhibited the behavior seen from AFOPT and FP-Growth when their virtual memory footprint exceeds the size of main memory. However, our optimized out-of-core algorithm maintains its efficiency even as it spills into disk resident memory, resulting in over a 400-fold performance improvement.

It can be seen that our algorithm uses more virtual mem-ory than the other two algorithms (Figure 13). This can be attributed to the space overheads of our page blocking tech-nique. However, even with increased memory consumption, we do not exhibit a performance degradation as our memory accesses are localized, unlike the other algorithms.
Traditionally, in-core data mining solutions have not been capable of processing large, out-of-core data sets. There are two main reasons for this problem. First, 32-bit computing platforms afford a virtual memory of size at most 4GB. Sec-ond, even in the presence of a 64-bit address space, in-core the Webdocs data set.
 solutions are not able to utilize the memory hierarchy ef-fectively, resulting in poor CPU utilization. Finally, dealing with explicit disk operations for custom paging can be quite difficult, particularly with pointer-based meta structures.
In this paper, we propose a very different methodology for designing out-of-core data mining algorithms. Specifically, we propose to leverage the 64-bit address space available on today X  X  commodity processor. Furthermore, for situations in which we are unable to achieve good CPU utilization due to poor locality, we propose to use relatively simple data and computation restructuring techniques to improve spatial and temporal locality, and thus improve CPU utilization. The methodology has two key benefits. First, the solutions are significantly easier to implement; existing solutions simply need to be reconsidered from the point-of-view of data lo-cality. Second, an efficient solution can be devised using an off-the-shelf PC, making it extremely affordable.
It is our contention that a large percentage of data mining algorithms will be able to glean the benefits of such a design methodology. Algorithms in the areas of tree mining, se-quence mining, and graph mining are particularly amenable to this design methodology. Solutions to these problems spend a considerable amount of time estimating the sup-port for patterns, often re-reading the same blocks of data in a streaming fashion. In all likelihood, the working set for these algorithms will not fit main memory when mining out-of-core data sets. These algorithms do not necessarily use prefix trees, however, as such the community could ben-efit from an investigation into how their data structures can be made I/O-conscious. Through localized data placement, I/O-conscious data structures also have the potential of re-ducing the average disk seek time. This in turn can reduce the energy consumption of out-of-core executions, which is especially important for large scale data centers [6].
Trends indicate that emerging architectures will likely in-corporate Chip Multiprocessing (CMP). CMPs, or multi-cores incorporate more than one processing element on the same die, providing true multiprocessing capabilities. With the availability of additional processing elements, orthogo-nal to improvements afforded th rough improved data local-ity, one can improve the I/O performance of an algorithm by employing a helper thread to prefetch pages from disk to main memory [22]. We believe that such an approach can significantly improve performance in many situations, and are currently investigating approaches by which an al-gorithm can efficiently relay information to a helper thread for prefetching data.
In this paper, we show that existing out-of-core frequent itemset mining solutions do not scale to truly large data sets. To address this limitation, we present a highly scal-able solution. Presented in the context of the FPGrowth algorithm, we leverage 64-bit computing capabilities avail-able on today X  X  commodity processor. We demonstrate that I/O-conscious optimizations such as approximate hash sort-ing and blocking afford improved spatial and temporal local-ity, permitting the scaling of FPGrowth to out-of-core data sets. Empirically, we illustrate that our approach scales to truly large data sets, with sizes beyond those that have been previously considered in the literature. We believe that the proposed methodology is directly applicable to other data mining tasks as well.
