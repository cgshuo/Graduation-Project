 Modeling the evolution of topics with time is of great value in automatic summarization and analysis of large document collections. In this work, we propose a new probabilistic graphical model to address this issue. The new model, which we call the Multiscale Topic Tomography Model (MTTM), employs non-homogeneous Poisson processes to model gen-eration of word-counts. The evolution of topics is modeled through a multi-scale analysis using Haar wavelets. One of the new features of the model is its modeling the evolution of topics at various time-scales of resolution, allowing the u ser to zoom in and out of the time-scales. Our experiments on Science data using the new model uncovers some interest-ing patterns in topics. The new model is also comparable to LDA in predicting unseen data as demonstrated by our perplexity experiments.
 I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database Applications X  data mining Algorithms, Experimentation Topic modeling, Temporal evolution, time-scale, Poisson, Probabilistic graphical models, wavelets
Explosive growth of electronic document collections in the recent past has rendered their analysis by human experts extremely tedious and expensive. As a result, an increas-ing need is felt for automatic algorithms that analyze and summarize the topics contained in such large document col-lections.

Several probabilistic graphical models have been proposed recently, to address this problem. One of the first proba-bilistic and truly generative models among them is Latent Dirichlet Allocation (LDA) [2]. LDA models a topic as a multinomial distribution over the vocabulary. Given a doc-ument collection, the LDA learns its underlying topics in an unsupervised fashion. In the recent past, several exten-sions to this model have been proposed such as the Hier-archical Dirichlet Processes [12] model that automaticall y discovers the number of topics, Hidden Markov Model-LDA [5] that integrates topic modeling with syntax, Correlated topic models [1] that model pairwise correlations between topics, etc .

All the aforementioned models ignore an important factor that reveals a huge amount of information contained in large document collections -time . Some of the large corpora such as a collection of scientific journals or patent databases sp an several decades. Hence modeling the evolution and popu-larity of topics with time can reveal tremendous amount of hidden information in those collections.

Several models have been proposed in the recent past to address this issue. One of the models, called Topics Over Time (ToT) [13] associates a beta distribution over time to each topic that represents the occurrence probability of th at topic at any given time. The model learns the parameters of this distribution for each topic based on the time-stamps of documents associated with that topic in the collection. Thi s permits us to analyze the popularity of various topics as a function of time.

Another proposed model called the Dynamic Topic Mod-els (DTM) [3] takes a slightly different approach. The DTM explicitly models the evolution of topics with time by esti-mating the topic distribution at various epochs. Thus the DTM allows us to predict what words are  X  X n vogue X  in a particular topic at different points in time. To model the evolution of a topic with time, the authors assume that the natural parameters corresponding to the topic multinomial at each epoch are conditionally distributed by a normal dis-tribution with mean equal to the natural parameters at the previous epoch. However, since the normal distribution is not a conjugate to the multinomial distribution, the model does not yield a simple solution to the problems of inference and estimation.

In this work, we present an alternative to the DTM, that is more natural to sequential modeling of counts data. The new model uses conjugate priors on the topic parameters to model evolution of topics, thereby resulting in simpler sol u-tions. In addition, our new model, which we refer to hence-forth as the Multiscale Topic Tomography Model (MTTM), allows us to analyze the evolution of topics at various resol u-tions of time scale. Its expressiveness provides the user wi th additional flexibility to zoom-in and zoom-out on the time scale and study the evolution of topics at a chosen time scale . Thus, we believe that the MTTM brings us a step closer to the ultimate goal of effective and fully automatic analysis o f document collections.

The rest of the paper is organized as follows. In section 2, we discuss past work related to the new model. In section 3, we describe the MTTM in detail including its generative process, the multi-scale analysis and the variational meth ods used for learning and inference. Section 4 presents some of the experiments we performed using the model. Section 5 concludes the paper with some analysis and directions for future work.
The Poisson distribution, being a natural model for counts-data, has been considered as a potential candidate to model text in the past. One of the earliest models is the 2-Poisson model for information retrieval [6], which generates words from a mixture of two classes called elite and non-elite classes. This model did not achieve empirical success, mainly owing to the lack of good estimation techniques, but inspired a heuristic model called BM25 [11]. The latter is considered a strong IR baseline till date.

In the area of text modeling, the GaP model [4] proposed by Canny uses a combination of Gamma and Poisson dis-tributions to discover latent topics or themes in document collections. The Gamma distribution is used to generate the topic weights vector x in each document, which the author calls theme lengths . The Poisson distribution is used to gen-erate the vector of observed word counts f from expected counts y . The expected counts y are related to the topic weights x through a matrix  X , given by y =  X  x , where each column of  X  represents the probability distribution of word s in a topic. Canny developed an EM algorithm to estimate the topic weights x for each document and the global matrix  X . Furthermore, it is also shown that the model achieves a lower perplexity on test data compared to LDA while also outperforming baseline models on the task of text retrieval . However, the modeling scheme for GaP proposed by Canny optimizes likelihood of the complete data (i.e., the data wi th the maximum likelihood values used for the unobserved vari-ables), whereas a pure generative model should optimize the likelihood of the observed data only. The model presented in this paper is very similar to the Gap model, except that the theme-weights in our case are distributed by a Dirichlet distribution over documents instead of a Gamma. This par-ticular definition of Dirichlet means that the topic-weight s are normalized over all the documents for each topic and not over all the topics per document as in LDA. Also, in our parameter-estimation, we are able to optimize a vari-ational lower-bound on the observed data log-likelihood by marginalizing the theme weights in the complete-data log-likelihood. In addition, we extend this model to sequential data by performing multi-scale analysis.

In our work, we use the Poisson distribution to model word counts not only because it is a natural choice for counts-data, but also because it is amenable to sequence modeling through Bayesian multiscale analysis. Bayesian multiscal e models for Poisson processes were first introduced by Ko-laczyk [8] and were applied to model physical phenomena such as gamma ray bursts. Nowak extended multiscale anal-ysis to build multiscale hidden Markov models and applied it to the problem of image segmentation [9]. Nowak and Kolaczyk also presented multiscale analysis for the Poisso n inverse problem [10], which is the problem of estimating la-tent Poisson means based on observed Poisson data, whose means are related to the latent Poisson means by a known linear function. In this paper, we cast the problem of topic discovery in document collections as a Poisson inverse prob -lem. Unlike in the work of Nowak and Kolaczyk [10], we do not assume that the linear relationship between the la-tent Poisson parameters and observed Poissons is known, which makes the problem slightly more complex. Hence, we use variational approximations to estimate the parameters of the model. We also extend the analysis to multi-scale rep-resentation of the Poisson parameters, thereby allowing us to model temporal evolution of topics at various time-scale s.
Following standard notation, we use bold faced letters to represent vectors and matrices and regular font to indicate scalars.

We assume that our document collection is sorted in the ascending order of the publication dates of the documents. We also assume that the sorted collection is divided into 2 equal-sized chunks (where S is an integer) of size M each, with the chunks { C 0 ,  X  X  X  ,C 2 S  X  1 } indexed in the ascending order of time. Thus, each chunk C t represents an epoch of time t ranging from the publication date of its earliest pub-lished document d = 1 to the publication date of its latest document d = M . Henceforth, we will use the term epoch to also denote the chunk of documents C t that it corresponds to. We represent each document d in an epoch t by a vec-tor of term counts n td = { n td 1 ,  X  X  X  ,n tdV } where n count of word w in document d from epoch t , and V is the vocabulary size.

We use non-homogeneous Poisson processes to model evo-lution of topics with time. Accordingly, each epoch t is asso-ciated with its unique word generating Poisson parameters given by  X  t = {  X  t 1 ,  X  X  X  ,  X  tK } corresponding to K topics. Again each  X  tk is a a vector of Poisson means over the vocabulary given by {  X  tk 1 ,  X  X  X  , X  tkV } . Thus, the parame-ter  X  tkw represents the expected number of counts of word w from topic k during the epoch t . Unlike in LDA where topics are represented as multinomial distributions over t he vocabulary, we represent topics as vectors of Poisson means over the vocabulary  X  tk . The variation in the values of the Poisson means of a particular topic as a function of t will provide us information on the evolution of the topic content with time.

We use the terms Poisson rate, Poisson mean and Poisson parameter interchangeably in the rest of the paper.
Given the Poisson parameters for each epoch, we gener-ate the data as follows. For each epoch t and topic k , we first generate the topic-weights vector  X  tk from a Dirichlet distribution, where  X  tk = {  X  tk 1 ,  X  X  X  , X  tkM } is a multinomial distribution over the documents in the corresponding chunk C . Each component of the multinomial,  X  tkd , represents the degree to which the document d  X  X aptures X  the topic k . Then, for each document d in the chunk and for each word w , we generate the counts n tdw using a Poisson distribution whose mean is given by P k  X  tkd  X  tkw , a weighted combina-tion of the Poisson means of all the topics corresponding to that word. The generative process is presented more pre-cisely below. 1. For each epoch t = 0 ,  X  X  X  , 2 S  X  1 2. For each topic k = 1 ,  X  X  X  ,K 3. Generate  X  tk  X  Dir(  X |  X  ) 4. For each document d = 1 ,  X  X  X  ,M 5. For each word w = 1 ,  X  X  X  ,V 6. Generate n tdw  X  Poiss(  X | P k  X  tkd  X  tkw )
Note that the topic-weights in the linear combination do not sum to 1 since  X  tkd represents P ( d | k ), the probability that the topic k appears in document d and not P ( k | d ), the probability that the document discusses the topic k , as de-fined in LDA. An intuitive way to understand the new model would be to think of each topic as an emission from a source, and the documents as sinks that share the topic emissions amongst themselves. Thus the new model captures how the topic is sectioned among the documents in a given epoch, hence the name Topic Tomography . This idea is illustrated in figure 1.

The generative process of the observed data is graphically represented in figure 2. Accordingly, the data likelihood given the Poisson parameters is given by:
The process described above is already a complete genera-tive process for a document collection. However, we have not yet defined how the Poisson parameters of different epochs are related to each other. In this section, we define a mul-tiscale generative process for the Poisson parameters that allows us to model temporal evolution of topics. First, we define multiscale wavelet parameters given by a binary tree representation as shown below: where the index s is called the scale and corresponds to the depth of the tree. The highest scale of resolution given by S corresponds to the leaves of the binary tree, where each leaf node represents an epoch. The multiscale Poisson parameters  X  ( S ) t at each leaf node t  X  X  0 ,  X  X  X  , 2 S set equal to the Poisson parameters corresponding to the respective epoch t . At any lower scale of resolution (0  X  s  X  S  X  1), the Poisson parameter at node t  X  X  0 ,  X  X  X  , 2 1 } , given by  X  ( s ) t , is set equal to sum of the corresponding parameters at its two children. The parameters  X  ( s ) t defined this way are known as the unnormalized Haar wavelet scaling coefficients of  X  t [9]. The multi-scale Poisson parameters are pictorially represented in figure 3.

While each leaf node in the tree corresponds to an epoch, any non-leaf node at scale 0  X  s  X  S  X  1 corresponds to a larger epoch of time whose span ranges the epochs of the leaf nodes to which it is an ancestor. At scale s = 0, we have only the root node whose epoch spans the time-period of the entire collection and the Poisson parameters at this scale correspond to the average topic representation for th e whole corpus. As we descend down the tree to a higher scale of resolution s , we have 2 s nodes at that scale with shorter epochs for each node and a breadth-wise traversal from left to right gives us the evolution of topics at that scale.
Now, we also define the canonical multiscale parameters  X  t as follows. In other words, at each scale s (except s = S ) and for each node t at that scale,  X  ( s ) t represents the ratio of the Poisson parameter at the left child and that at the node under con-sideration. The canonical parameters are also called split -ting factors since they govern how the multiscale parameter  X  t is  X  X plit X  between its children. We can also invert the relation in Eq. (4) to obtain where we obtained Eq. (6) from Eq. (3). The canonical parameters are represented at the edges of the binary tree in figure 3 to indicate that they are a function of the Poisson parameters at the two nodes that share the respective edges. We will later show that one can factor the joint likelihood of the observed data under the independent Poissons as a multi-scale likelihood using the canonical parameters.
Note that setting  X  ( s ) t = 0 . 5 is equivalent to the relation  X  the same parent are equal. This relation should immediately delight a Bayesian statistician, since we can conveniently encode our prior information that the Poisson parameters of a given topic are expected to be more or less the same over various epochs (in other words, topics do not change too drastically with time), by imposing a symmetric, conjugate
Given this background, the generative process for the Pois-son parameters is as shown below. 1. For each topic k = 0 ,  X  X  X  ,K 2. For each word w = 1 ,  X  X  X  ,V 3. Generate  X  (0) 0 kw  X  Gamma(  X |  X   X  , X   X  ) 4. For each scale s = 0 ,  X  X  X  ,S  X  1 5. For each epoch t = 0 ,  X  X  X  , 2 s  X  1 6. Generate  X  ( s ) tkw  X  Beta(  X |  X   X  , X   X  ) We used the Gamma distribution to generate the Poisson parameters since it is a conjugate prior to the Poisson. We will later show that the observed data log-likelihood can be factored into a multiscale log-likelihood in which the cano n-ical parameters act as binomial parameters. Hence we used the Beta distribution, their natural conjugate prior, to ge n-erate them. In particular, the symmetric Beta aligns topics in one epoch to topics in the adjacent epoch and also ensures that their evolution remains smooth.
The prior probability of the model parameters given the hyperparameters  X  = {  X   X  , X   X  , X   X  } is then given as follows: The generative process of the data as well as the model parameters together is represented graphically in figure 4. Combining Eq. (1) and Eq. (7), one can compute the marginal likelihood of the observed data given the topic pa-rameters and the hyper-parameters of the priors as follows.
Since estimating the parameters of the model is intractable , we use variational EM to estimate the parameters of the model [7]. We only summarize the results below but the interested reader may refer to appendix A for more details.
We introduce variational parameters given by  X  tkd and  X  tdwk , to approximate the observed data log-likelihood given in Eq. (8). One can think of the  X  tkd as proportional to the posterior probability that document d of epoch t captures topic k .  X  tdwk can be interpreted as the posterior probability that word w in document d of epoch t came from topic k . We estimate them by maximizing a variational lower-bound of Eq. (8) with respect to the variational parameters. We summarize the results in Eq. (9) and Eq. (10) below, with details in appendix A.
In the M-step, we estimate the model parameters, namely  X  . Instead of directly estimating the parameters  X  ( S ) maximizing the variational lower-bound of Eq. (8) with re-spect to these parameters, we express the likelihood in a slightly different form so as to be able to estimate the multi-scale parameters  X  ( s ) tkw for 0  X  s  X  S . Since, we are only interested in estimating  X  in the M-step, we collect all the terms in the variational lower-bound of Eq. (8), given by Eq. (19) in the appendix, that contain  X  ( S ) tkw and call the expression L [  X  ] as shown below.
 where z twk is the latent count of the word w in topic k in the entire chunk of documents corresponding to epoch t (corre-sponding to scale S ) and is given by z twk = P M d =1 n tdw Although we showed that z twk  X  Poiss(  X |  X  ( S ) tkw ) by simple al-gebraic manipulation, it is also possible to prove it theore t-ically. This proof is presented in appendix B.

In Eq. (12), the notation side is equal to its right-hand-side as far as the terms con-taining  X  are concerned. Note that Eq. (11) and Eq. (12) differ by the factor P t P w P k log( z twk !) but since it doesn X  X  contain  X  , it does not affect our estimation. Also note that one may round-off z twk to the nearest integer to account for the fact that the Poisson generates only integers, but this plays no major role in terms of estimating  X  .

We now define a new multi-scale variable z ( s ) twk on the same lines as the multiscale parameters as follows. We will show shortly that L [  X  ] can be expressed in terms of this variable. The simplified version of L [  X  ] in (12) can be equivalently expressed in terms of the multiscale parameters as shown below.

The proof for the above transformation is as follows: we first note the result that the joint probability of two inde-pendent Poisson variables x 1 and x 2 can be equivalently ex-pressed as a product of a binomial and a Poisson as follows. Applying this result to the Poisson likelihood terms in left -hand-side of Eq. (15) recursively results in its right-hand -side.
 We do a MAP estimate of the multiscale parameters using Eq. (15) and the priors defined in Eq. (7) to obtain the following relations.
We analyzed a subset of 30,000 articles from Science , 250 from each of the 120 years between 1883 and 2002. This is essentially the same data used by Blei and Lafferty in their experiments with the DTM [3]. We divided the data into 16 chunks, each consisting of 1875 documents. Each of these chunks represents a 15 year epoch. We then trained a 5-scale topic tomography model with number of topics K = 50 on this data set with the following values for the hyper-parameters : (  X   X  = 1 . 0001;  X   X  = 1;  X   X  = 50;  X  = 0 . 8). The large value of  X   X  ensures that the Poisson parameters of adjacent epochs are nearly equal, resulting in a smooth evolution of topics.

Figure 5 shows the multi-scale representation of a topic which we manually labeled  X  X article physics X  1 . We only dis-played the top 10 terms that had the highest Poisson means in that topic. The root node of the binary tree corresponds to s = 0, and it represents the summary of the topic over the entire 120 year span of the collection. At the highest scale ( s = 3) displayed, each node presents a snap-shot summary of the topic in a 15 year period (Note that owing to space constraints, we did not display the highest scale of resolut ion s = 4). Thus, the user can choose one of the four scales of resolution depending on the desired granularity. Inspecti ng the topic snap-shots at the scale s = 3, one can easily gain an understanding of the evolution of the topic. The gradual transition in the topic from macro-matter to micro-matter i s
Note that this and the other topics we displayed emerged in our run with K = 50. If we use a different value of K , it is not guaranteed that the exact same topics will emerge again. more apparent from figure 7, which plots the Poisson rates of a few representative words as a function of time. Table 1 lists the titles of documents that have the highest value of the posterior Dirichlet parameter  X  tkd among all documents in each epoch. In other words, these are the documents in which the topic particle-physics appears with the high-est probability in each epoch. The shift in the topic is also evident from these titles.

In figure 6, we displayed the multiscale representation of another topic, which we labeled  X  X enetics X . An examination of the topic snapshots at scale s = 3 clearly shows a gradual transition from evolutionary biology in the late 19th cen-tury to modern genetics in the early 21st century. Figure 8 plots the popularity of a few representative terms with time . Table 2, that displays the titles of documents in which the topic appears with the highest probability in each epoch, demonstrates a very similar pattern of topic evolution.
In figure 9, we plotted another interesting statistic, namel y the sum of the posterior Dirichlet parameters of a topic k over all documents in each epoch t given by  X  tk = P d  X  tkd as a function of t . This statistic is proportional to the occur-rence frequency of a topic in a given epoch. We normalized this statistic, so that one can interpret the plot as the prob -ability of occurrence of a topic as a function of time, simila r to the plots in [13].
Finally, in figure 10, we plot the Poisson rates of the word  X  X eaction X  in three different topics and compared with the to -tal counts in each epoch. The plot clearly demonstrates the utility of the topic model in disambiguating an ambiguous word based on its context.
Perplexity is a standard objective metric that measures the ability of a model to predict unseen data. Lower per-plexity means better predictiveness and a better model. In case of documents, the average perplexity of a word in a test set D test comprising M documents is defined as where n test is the entire vector of observed word counts in the test set and M is the model.

In this section, we compare the perplexity of the topic tomography model with that of LDA. Note that these two models generate completely different events: while the for-mer models counts-data ( e.g. : 2 a X  X  and 3 b X  X ), the latter models one particular instance of the counts vector ( e.g. :  X  X abbb X ). In order to be able to make a fair comparison, we added the multinomial normalizing coefficient ( P w n dw )! / ( Q w ( n dw !)) for each document in the expression for LDA likelihood. This term converts the probability of a string to the probability of the corresponding counts vecto r allowing us to directly compare the perplexities of both the models. Hence the perplexity numbers we show in the plots for LDA may not directly correspond to the values obtained by previous authors [2, 3].

For our experiments, we split the data time wise into 8 chunks each spanning 15 years and comprising 3750 docu-ments as done in section 4.1. We further randomly split each chunk into equal halves to generate training and test sets. The train and test sets each have 8 chunks, each of which spans 15 years but consists of only 1875 documents.
We consider three variants of the topic tomography model in our experiments.

The first variant, which we call basic TT , is the closest counterpart to LDA. In this model, we completely ignore the multiscale analysis and assume that the entire training (or test) set represents a single epoch. We estimate one set of Poissons for the entire collection, using no prior distri -butions on the Poisson parameters. The learned model is used to estimate perplexity on the test set, which is done by running the E-step of the variational EM algorithm. The second variant, which we name multiple TT model, relaxes the assumption of the basic TT model and estimates topic Poissons for each of the 8 epochs in the training set sep-arately. However, it still does not perform any multiscale analysis and uses no priors on the Poisson means. For each chunk in the test set, we predict the model X  X  perplexity by running the E-step of the variational EM with respect to the model parameters corresponding to the same epoch in the training set. The last variant is the complete multiscal e topic tomography model with multiscale analysis using beta priors on the multiscale binomials with hyper-parameters s et at the same values used in section 4.1.

For LDA baseline, we used the standard version that es-timates a single set of topic multinomials for the whole col-lection. For all the aforementioned models, we fixed the Dirichlet parameter at 0.8 to encourage sparsity of topics.
Figure 11 compares the perplexity of LDA with the three variants of the topic tomography model as a function of the number of topics used in the model. The figure shows that both LDA and basic TT are almost identical in performance. Also notice that multiscale TT has a consistently lower per-plexity than the multiple TT model. This result justifies the intuition behind our definition of the priors in the multisca le-analysis. The priors allow information to propagate from one epoch to another and hence improve the ability of the model to predict unseen data in any given epoch. Finally, we notice that although the multiple TT and the multiscale TT models have many more parameters than the basic TT model, they produce a slightly higher perplexity on the test set compared to the latter. On inspection, we noticed that the performance comparison on the training data is the ex-act reverse. This is a clear case of over-fitting, where the extra parameters in the multiple TT and the multiscale TT models result in better fitting of the training data, but hurt its generalization ability compared to the basic TT model. Notwithstanding this fact, the multiscale model is still ve ry useful since it allows us to visualize data better, through t he multiscale analysis as we have shown earlier.

Finally, we note that the DTM [3], on the contrary, re-ported slightly lower perplexity than LDA. Unlike the DTM which uses full Bayesian estimation, we use MAP estima-tion to keep the the algorithm simple. The latter setting results in a few free parameters in the form of the priors (  X   X  , X   X  , X   X  ). In our experiments, we fixed these priors to the values reported in section 4.1. Since these values are not necessarily optimal, the perplexity values of MTTM we re-ported in the experiments are actually upper bounds. It is possible to further lower the perplexity of MTTM by tuning the priors through cross-validation. But we did not venture into this direction because our perplexity experiments are only meant to be illustrative and not necessarily conclusiv e.
In this work, we presented a new approach to modeling temporal evolution of topics in a large document collection . The new approach, based on non-homogeneous Poisson pro-cesses, combined with multi-scale Haar wavelet analysis is a more natural way to do sequence modeling of counts-data than previous approaches. The new model offers us the best features of both the ToT [13] and DTM [3] models. While ToT models the probability of occurrence of a topic with time, DTM models the evolution of topic content. The topic tomography model permits us to accomplish both at the same time. In addition, the multiscale analysis used in the model provides us with an additional  X  X oom X  feature that permits the user to examine the topic evolution at multiple scales of resolution.

One of the limitations of the MTTM lies in its generative process: since the Dirichlet distribution that generates t opic proportions is defined over the set of documents in a given epoch, the model permits only generating an entire chunk of documents whose size is equal to the training set chunk size. This is the main reason why we used equal sized training and test sets in our perplexity experiments in section 4.2. We are no longer able to make inference on a single docu-ment at a time. One way to overcome this limitation is use a Gamma distribution to generate topic weights for each doc-ument independently as done in the GaP model. However, multiscale analysis using the Gamma distributed weights be -comes tricky due to the coupling between the Poisson param-eters and the Gamma weights. In our case, we were able to uncouple the Poisson parameters from the topic proportions using the relation P d  X  tkd = 1 (see Eq. (19) in appendix A). Nevertheless, we intend to construct a variational algorit hm for multi-scale analysis using a GaP like model, as part of our future work. [1] D. Blei and J. Lafferty. Correlated topic models. In [2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [3] D. M. Blei and J. D. Lafferty. Dynamic topic models. [4] J. Canny. Gap: a factor model for discrete data. In [5] T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B. [6] S. P. Harter. A probabilistic approach to automatic [7] M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. K. [8] E. D. Kolaczyk. Bayesian multiscale models for [9] R. Nowak. Multiscale hidden markov models for [10] R. Nowak and E. Kolaczyk. A statistical multiscale [11] S. E. Robertson and S. Walker. Some simple effective [12] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical [13] X. Wang and A. McCallum. Topics over time: a
We define the following variational bounds on the log-likelihood of the observed data using Jensen X  X  inequality f or the log function as shown below. where in Eq. (19), we used the relation P M d =1  X  tkd = 1 while q (  X  tk |  X  tk ) and  X  tdw are variational posterior Dirich-let and variational multinomial distributions respective ly, E [ X ] represents the expectation of the random variable X with respect to the distribution q (). H () represents the en-tropy of the distribution in its argument.
 The terms in Eq. (19) can be expanded as follows.
E q [log P (  X  tk |  X  )] =  X ( M X  )  X  M  X (  X  ) + X
H ( q (  X  tk |  X  tk )) = X Plugging back these expansions in Eq. (19) and calling the expression obtained by collecting terms that contain  X  tdwk L
L Taking the partial derivative of L [  X   X  Setting the partial derivative to zero and solving yields th e maximizing value of the variational parameter  X  tdwk as shown in Eq. (9).

Similarly, collecting the terms in Eq. (19) that contain  X  Taking the partial derivative of L [  X  gives: Equating the partial derivative to zero results in the maxi-mizing expression for  X  tkd shown in Eq. (10).

We first start with noting that the counts of a word w in a document d from epoch t is distributed as Now, let us define the variable z tdwk denoting the latent counts of the word w from topic k in the same document. Now clearly, P k z tdwk = n tdw . Since the summation of two independent Poisson random variables is also a Poisson vari -able with mean equal to the sum of the means of the original random variables, we can infer that Now z twk is the latent counts of the word w from topic k in the whole chunk that corresponds to epoch t . Therefore, by definition it follows that
