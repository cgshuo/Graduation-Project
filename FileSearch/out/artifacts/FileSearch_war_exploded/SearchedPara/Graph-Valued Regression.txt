 Let Y be a p -dimensional random vector with distribution P . A common way to study the structure of P is to construct the undirected graph G =( V, E ) , where the vertex set V corresponds to the p components of the vector Y . The edge set E is a subset of the pairs of vertices, where an edge between Y j and Y k is absent if and only if Y j is conditionally independent of Y k given all the other variables. Suppose now that Y and X are both random vectors, and let P (  X | X ) denote the conditional distribution of Y given X . In a typical regression problem, we are interested in the conditional mean  X  ( x )= E ( Y | X = x ) . But if Y is multivariate, we may be also interested in how the structure of P (  X | X ) varies as a function of X . In particular, let G ( x ) be the undirected graph corresponding to P (  X | X = x ) . We refer to the problem of estimating G ( x ) as graph-valued regression .
 Let G = { G ( x ): x  X  X } be a set of graphs indexed by x  X  X  , where X is the domain of X . Then G induces a partition of X , denoted as X 1 ,..., X m , where x 1 and x 2 lie in the same partition element if and only if G ( x 1 )= G ( x 2 ) . Graph-valued regression is thus the problem of estimating the partition and estimating the graph within each partition element.
 We present three different partition-based graph estimators; two that use global optimization, and one based on a greedy splitting procedure. One of the optimization based schemes uses penalized empirical risk minimization, the other uses held-out risk minimization. As we show, both methods enjoy strong theoretical properties under relatively weak assumptions; in particular, we establish oracle inequalities on the excess risk of the estimators, and tree partition consistency (under stronger assumptions) in Section 4. While the optimization based estimates are attractive, they do not scale well computationally when the input dimension is large. An alternative is to adapt the greedy algo-rithms of classical CART, as we describe in Section 3. In Section 5 we present experimental results on both synthetic data and a meteorological dataset, demonstrating how graph-valued regression can be an effective tool for analyzing high dimensional data with covariates. Let y 1 ,...,y n be a random sample of vectors from P , where each y i  X  R p . We are interested in the case where p is large and, in fact, may diverge with n asymptotically. One way to estimate G from the sample is the graphical lasso or glasso [13, 5, 1], where one assumes that P is Gaussian with mean  X  and covariance matrix  X  . Missing edges in the graph correspond to zero elements in the precision matrix  X = X   X  1 [12, 4, 7]. A sparse estimate of  X  is obtained by solving where  X  is positive de fi nite, S is the sample covariance matrix, and  X  1 = j,k |  X  jk | is the elementwise 1 -norm of  X  . A fast algorithm for fi nding  X  was given by Friedman et al. [5], which involves estimating a single row (and column) of  X  in each iteration by solving a lasso regression. The theoretical properties of  X  have been studied by Rothman et al. [10] and Ravikumar et al. [9]. In practice, it seems that the glasso yields reasonable graph estimators even if Y is not Gaussian; however, proving conditions under which this happens is an open problem.
 We brie fl y mention three different strategies for estimating G ( x ) , the graph of Y conditioned on X = x , each of which builds upon the glasso.
 Parametric Estimators. Assume that Z =( X, Y ) is jointly multivariate Gaussian with covariance matrix  X =  X  X  X  XY quantities  X  X ,  X  Y , and  X  XY , and the marginal precision matrix of X , denoted as  X  X , can be estimated using the glasso. The conditional distribution of Y given X = x is obtained by standard Gaussian formulas. In particular, the conditional covariance matrix of Y | X is  X  Y | X =  X  Y  X   X  YX  X  X  X  XY and a sparse estimate of  X  Y | X can be obtained by directly plugging  X  Y | X into glasso. However, the estimated graph does not vary with different values of X .
 Kernel Smoothing Estimators. We assume that Y given X is Gaussian, but without making any assumption about the marginal distribution of X . Thus Y | X = x  X  N (  X  ( x ) ,  X ( x )) . Under the assumption that both  X  ( x ) and  X ( x ) are smooth functions of x , we estimate  X ( x ) via kernel smoothing: where K is a kernel (e.g. the probability density function of the standard Gaussian distribution),  X  is the Euclidean norm, h&gt; 0 is a bandwidth and Now we apply glasso in (1) with S =  X ( x ) to obtain an estimate of G ( x ) . This method is appealing because it is simple and very similar to nonparametric regression smoothing; the method was ana-lyzed for one-dimensional X in [14]. However, while it is easy to estimate G ( x ) at any given x ,it requires global smoothness of the mean and covariance functions.
 Partition Estimators. In this approach, we partition X into fi nitely many connected regions X ,..., X m . Within each X j , we apply the glasso to get an estimated graph G j . We then take and regression trees) [3]. We take the partition elements to be recursively de fi ned hyperrectangles. As is well-known, we can then represent the partition by a tree, where each leaf node corresponds to a single partition element. In CART, the leaves are associated with the means within each partition element; while in our case, there will be an estimated undirected graph for each leaf node. We refer to this method as Graph-optimized CART, or Go-CART. The remainder of this paper is devoted to the details of this method. from the joint distribution of ( X, Y ) . The domains of X and Y are denoted by X and Y respectively; and for simplicity we take X =[0 , 1] d . We assume that covariance function. We also assume that for each x ,  X ( x )= X ( x )  X  1 is a sparse matrix, i.e., many for some R  X  X  1 ,...,d } with cardinality | R | d . The task of graph-valued regression is to fi nd a sparse inverse covariance  X ( x ) to estimate  X ( x ) for any x  X  X  ; in some situations the graph of  X ( x ) is of greater interest than the entries of  X ( x ) themselves.
 Go-CART is a partition based conditional graph estimator. We partition X into fi nitely many con-nected regions X 1 ,..., X m , and within each X j we apply the glasso to estimate a graph G j .We then take G ( x )= G j for all x  X  X  j .To fi nd the partition, we restrict ourselves to dyadic splits, as studied by [11, 2]. The primary reason for such a choice is the computational and theoretical tractability of dyadic partition based estimators.
 Let T denote the set of dyadic partitioning trees (DPTs) de fi ned over X =[0 , 1] d , where each DPT T  X  X  is constructed by recursively dividing X by means of axis-orthogonal dyadic splits. Each node of a DPT corresponds to a hyperrectangle in [0 , 1] d . If a node is associated to the hyper-by the leaf nodes of T . For a dyadic integer N =2 K ,wede fi ne T N to be the collection of all DPTs Before formally de fi ning our graph-valued regression estimators, we require some further de fi ni-tions. Given a DPT T with an induced partition  X ( T )= {X j } m T j =1 and corresponding mean and and its sample version R ( T, X  T ,  X  T ) are de fi ned as follows: R ( T, X  T ,  X  T )= Let [[ T ]] &gt; 0 denote a pre fi x code over all DPTs T  X  X  N satisfying T  X  X  1) log d/ log 2 . A simple upper bound for [[ T ]] is Our analysis will assume that the conditional means and precision matrices are bounded in the  X   X  and  X  1 norms; speci fi cally we suppose there is a positive constant B and a sequence L 1 ,n ,...,L m T ,n , where each L j,n  X  R + is a function of the sample size n , and we de fi ne the domains of each  X  X j and  X  X j as With this notation in place, we can now de fi ne two estimators.
 De fi nition 1. The penalized empirical risk minimization Go-CART estimator is de fi ned as Empirically, we may always set the dyadic integer N to be a reasonably large value; the regulariza-tion parameter  X  n is responsible for selecting a suitable DPT T  X  X  N .
 We also formulate an estimator that minimizes held-out risk. Practically, we could split the data into for validation with n 1 + n 2 = n . The held-out negative log-likelihood risk is then given by De fi nition 2. For each DPT T de fi ne minimization Go-CART estimator is where R out is de fi ned in (6) but only evaluated on D 2 .
 The above procedures require us to fi nd an optimal dyadic partitioning tree within T N . Although dynamic programming can be applied, as in [2], the computation does not scale to large input dimen-sions d . We now propose a simple yet effective greedy algorithm to fi nd an approximate solution empirical performance. But note that our greedy approach is generic and can easily be adapted to the penalized empirical risk minimization form.
 First, consider the simple case that we are given a dyadic tree structure T which induces a partition The glasso is then used to estimate a sparse precision matrix  X  X j . More precisely, let  X  X j be the sample covariance matrix for the partition element X j , given by where  X  j is in one-to-one correspondence with L j,n in (5). In practice, we run the full regularization path of the glasso, from large  X  j , which yields very sparse graph, to small  X  j , and select the graph that minimizes the held-out negative log-likelihood risk. To further improve the model selection per-formance, we re fi t the parameters of the precision matrix after the graph has been selected. That is, and then we re fi t the Gaussian model without 1 -regularization, but enforcing the sparsity pattern obtained in the fi rst step.
 The natural, standard greedy procedure starts from the coarsest partition X =[0 , 1] d and then computes the decrease in the held-out risk by dyadically splitting each hyperrectangle A along dimension k  X  X  1 ,...d } . The dimension k  X  that results in the largest decrease in held-out risk is selected, where the change in risk is given by If splitting any dimension k of A leads to an increase in the held-out risk, the element A should no longer be split and hence becomes a partition element of  X ( T ) . The details and pseudo code are provided in the supplementary materials.
 This greedy partitioning method parallels the classical algorithms for classi fi cation and regression that have been used in statistical learning for decades. However, the strength of the procedures given in De fi nitions 1 and 2 is that they lend themselves to a theoretical analysis under relatively weak assumptions, as we show in the following section. The theoretical properties of greedy Go-CART are left to future work. We de fi ne the oracle risk R  X  over T N as risk. To obtain oracle inequalities, we make the following two technical assumptions. Assumption 1. Let T  X  X  N be an arbitrary DPT which induces a partition  X ( T )= {X 1 ,..., X assume that Assumption 2. Let Y =( Y 1 ,...,Y p ) T  X  R p . For any A X  X  ,wede fi ne We assume there exist constants M 1 ,M 2 ,v 1 , and v 2 , such that for all m  X  2 .
 Theorem 1. Let T  X  X  N be a DPT that induces a partition  X ( T )= {X 1 ,..., X m T } on X . For any  X   X  (0 , 1 / 4) , let T,  X  b tion Go-CART in De fi nition 1, with a penalty term pen( T ) of the form where C 1 =8 holds with probability at least 1  X   X  .
 A similar oracle inequality holds when using the held-out risk minimization Go-CART. Theorem 2. Let T  X  X  N be a DPT which induces a partition  X ( T )= {X 1 ,..., X m T } on X .We de fi ne  X  n ( T ) to be a function of n and T such that where C 2 =8 D De fi nition 2. Then, for suf fi ciently large n , the excess risk inequality R ( T,  X  b T ,  X  b T )  X  R  X   X  inf with probability at least 1  X   X  .
 Note that in contrast to the statement in Theorem 1, Theorem 2 results in a stochastic upper bound due to the extra  X  n ( T ) term, which depends on the complexity of the fi nal estimate T . Due to space limitations, the proofs of both theorems are detailed in the supplementary materials. We now temporarily make the strong assumption that the model is correct, so that Y given X is conditionally Gaussian, with a partition structure that is given by a dyadic tree. We show that with high probability, the true dyadic partition structure can be correctly recovered. Assumption 3. The true model is where T  X   X  X  N is a DPT with induced partition  X ( T  X  )= {X  X  j } m T  X  j =1 and Under this assumption, clearly where M T is given by M Let T 1 and T 2 be two DPTs, if  X ( T 1 ) can be obtained by further split the hyperrectangles within  X ( T 2 ) , we say  X ( T 2 )  X   X ( T 1 ) . We then have the following de fi nitions: De fi nition 3. A tree estimation procedure T is tree partition consistent in case Note that the estimated partition may be fi ner than the true partition. Establishing a tree parti-tion consistency result requires further technical assumptions. The following assumption speci fi es that for arbitrary adjacent subregions of the true dyadic partition, either the means or the variances boundaries of the true partition.
 Assumption 4. Let X  X  i and X  X  j be adjacent partition elements of T  X  , so that they have a common parent node within T  X  . Let  X   X  X  X  such that either where  X  min (  X  ) denotes the smallest eigenvalue. Furthermore, for any T  X  X  N and any A X   X ( T ) , we have P ( X  X  X  )  X  c 2 .
 Theorem 3. Under the above assumptions, we have where c 1 ,c 2 ,c 3 ,c 4 are de fi ned in Assumption 4. Moreover, the Go-CART estimator in both the penalized risk minimization and held-out risk minimization form is tree partition consistent. This result shows that, with high probability, we obtain a fi ner partition than T  X  ; the assumptions do not, however, control the size of the resulting partition. The proof of this result appears in the supplementary material. We now present the performance of the greedy partitioning algorithm of Section 3 on both synthetic data and a real meteorological dataset. In the experiment, we always set the dyadic integer N =2 10 to ensure that we can obtain fi ne-tuned partitions of the input space X . 5.1 Synthetic Data We generate n data points x 1 ,...,x n  X  R d with n =10 , 000 and d =10 uniformly distributed on hypercube into 22 subregions as shown in Figure 1 (b). For the t -th subregion where 1  X  t  X  22 , we generate an Erd  X  os-R  X  enyi random graph G t =( V t ,E t ) with the number of vertices p =20 , the number of edges | E | =10 and the maximum node degree is four. Based on G t , we generate 0 . 245 guarantees the positive de fi niteness of  X  t when the maximum node degree is 4. For each data point x i in the t -th subregion, we sample a 20-dimensional response vector y i from a multivariate Gaussian distribution N 20 0 ,  X  t  X  1 . We also create an equally-sized held-out dataset in the same manner based on {  X  t } 22 t =1 .
 The learned dyadic tree structure and its induced partition are presented in Figure 1. We also provide the estimated graphs for some nodes. We conduct 100 monte-carlo simulations and fi nd that 82 times out of 100 runs our algorithm perfectly recover the ground true partitions on the X 1 -X 2 plane and never wrongly split any irrelevant dimensions ranging from X 3 to X 10 . Moreover, the estimated graphs have interesting patterns. Even though the graphs within each subregion are sparse, the estimated graph obtained by pooling all the data together is highly dense. As the greedy algorithm proceeds, the estimated graphs become sparser and sparser. However, for the immediate parent of the leaf nodes, the graphs become denser again. Out of the 82 simulations where we correctly identify the tree structure, we list the graph estimation performance for subregions 28, 29, 13, 14, 5, 6 in terms of precision, recall, and F1-score in Table 1.
 We see that for a larger subregion (e.g. 13, 14, 5, 6), it is easier to obtain better recovery perfor-mance; while good recovery for a very small region (e.g. 28, 29) becomes more challenging. We also plot the held-out risk in the subplot (c). As can be seen, the fi rst few splits lead to the most signi fi cant decreases of the held-out risk. The whole risk curve illustrates a diminishing return be-splitting the middle rectangles does not reduce the risk as much. We also conducted simulations where the true conditional covariance matrix is a continuous function of x ; these are presented in the supplementary materials. 5.2 Climate Data Analysis In this section, we apply Go-CART on a meteorology dataset collected in a similar approach as in [8]. The data contains monthly observations of 15 different meteorological factors from 1990 to 2002. We use the data from 1990 to 1995 as the training data and data from 1996 to 2002 as the held-out validation data. The observations span 100 locations in the US between latitudes 30.475 to 47.975 and longitudes -119.75 to -82.25. The 15 meteorological factors measured for each month include levels of CO 2 , CH 4 , H 2 , CO , average temperature ( TMP ) and diurnal temperature range ( DTR ), minimum temperate ( TMN ), maximum temperature ( TMX ), precipitation ( PRE ), vapor ( VAP ), radiation ( DIR ).
 As a baseline, we estimate a sparse graph on the data pooled from all 100 locations, using the glasso algorithm; the estimated graph is shown in Figure 2 (b). It is seen that the greenhouse gas factor CO 2 is isolated from all the other factors. This apparently contradicts the basic domain knowledge that CO 2 should be correlated with the solar radiation factors (including GLO , DIR ), according to the IPCC report [6] which is one of the most authoritative reports in the fi eld of meteorology. The reason for the missing edges in the pooled data may be that positive correlations at one location are canceled by negative correlations at other locations.
 Treating the longitude and latitude of each site as two-dimensional covariate X , and the meteorology data of the p =15 factors as the response Y , we estimate a dyadic tree structure using the greedy algorithm. The result is a partition with 66 subregions, shown in Figure 2. The graphs for subregions 3 and 10 (corresponding to the coast of California and Arizona states) are shown in subplot (a) of Figure 2. The graphs for these two adjacent subregions are quite similar, suggesting spatial smoothness of the learned graphs. Moreover, for both graphs, CO 2 is connected to the solar radiation factor GLO through CH 4 . In contrast, for subregion 33, which corresponds to the north part of Arizona, the estimated graph is quite different. In general, it is found that the graphs corresponding to the locations along the coasts are sparser than those corresponding to the locations in the mainland. Such observations, which require validation and interpretation by domain experts, are examples of the capability of graph-valued regression to provide a useful tool for high dimensional data analysis. [1] O. Banerjee, L. E. Ghaoui, and A. d X  X spremont. Model selection through sparse maximum [2] G. Blanchard, C. Sch  X  afer, Y. Rozenholc, and K.-R. M  X  uller. Optimal dyadic decision trees. [3] L. Breiman, J. Friedman, C. J. Stone, and R. Olshen. Classi fi cation and regression trees . [4] D. Edwards. Introduction to graphical modelling . Springer-Verlag Inc, 1995. [5] J. H. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the [6] IPCC. Climate Change 2007 X  X he Physical Science Basis IPCC Fourth Assessment Report . [7] S. L. Lauritzen. Graphical Models . Oxford University Press, 1996. [8] A. C. Lozano, H. Li, A. Niculescu-Mizil, Y. Liu, C. Perlich, J. Hosking, and N. Abe. Spatial-[9] P. Ravikumar, M. Wainwright, G. Raskutti, and B. Yu. Model selection in Gaussian graph-[10] A. J. Rothman, P. J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance [11] C. Scott and R. Nowak. Minimax-optimal classi fi cation with dyadic decision trees. Information [12] J. Whittaker. Graphical Models in Applied Multivariate Statistics . Wiley, 1990. [13] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. [14] S. Zhou, J. Lafferty, and L. Wasserman. Time varying undirected graphs. Machine Learning ,
