 In this paper we propose a new word-order based graph rep-resentation for text. In our graph representation vertices represent words or phrases and edges represent relations be-tween contiguous words or phrases. The graph representa-tion also includes dependency information. Our text repre-sentation is suitable for applications involving the identifi-cation of relevance or paraphrases across texts, where word-order information would be useful. We show that this word-order based graph representation performs better than a de-pendency tree representation while identifying the relevance of one piece of text to another.
 I.2 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis Algorithms text representation, word-order graph, text relevance
Conventional text representation techniques include the vector-space model 1 , where every value in the vector rep-resents a token in the text. Vectors capture information such as presence or absence of a token, or the number of oc-currences of a token in a document. This is a bag-of-words based representation and captures only exact matches across terms of documents during comparison.
 Output from parsers such as the Charniak parser [4], Stan-ford NLP parser [14] contain syntactic parse trees, which Figure 1: Charniak parser X  X  output for the text  X  X he author writes like a professional. X  contain labels such as S, NP or PP 2 . Figure 1 depicts the output from the Charniak parser for a piece of text. Out-put from a parser does not contain additional grammatical information [9] and may not be directly suitable for text matching.
 Parsers may be used to generate dependency trees. Vertices in a dependency tree represent words and edges capture the asymmetric dependency relationships between a head word and its modifier (modifier  X  head). Figure 2(a) contains a dependency tree representation [2] for the text  X  X he study guide does not discuss much of the basics of ethics. X  We see that every token in the text is a vertex in the tree and edges depict governance relations. For example,  X  X oes X  is the root of this sentence and the edge between  X  X uide X  and  X  X oes X  signifies a subject relationship represented by SBJ. Dependency trees succeed in capturing only governance in-formation. They do not capture ordering information. In this paper we propose the use of a graph representation for texts that extends the dependency-tree based representation to capture word-ordering information.

Problem of relevance identification: We use our word-order based text representation technique to identify the degree of relevance between a review and its submission. Reviews are text-based feedback provided by reviewers to authors. At times reviewers tend to provide vague or generic comments, which are not relevant to the author X  X  submission. Nelson and Schunn [12] found that well-justified reviews help au-thors understand and use feedback effectively. Therefore we identify a review X  X  relevance to the submission in order to ensure that reviews pertain to the right submission and that it contains justifications in the form of references to the sub-mission X  X  content [13].
 A relevant review contains paraphrases of concepts described in the submission, as well as provides details of the problems identified in the author X  X  work. Since paraphrases tend to contain lexical, word order or voice changes [3], a simple, prepositional phrase, NN -noun, DT -determiner, VB(Z) -verb, RB -adverb, JJ -adjective, IN -preposition, NNS -noun plural Figure 2: Dependency tree and word-order graph representations for the text  X  X he study guide does not discuss much of the basics of ethics. X  n -gram overlap of the submission and review texts may not be suited for identifying relevance. We therefore propose the use of a graph-based representation and matching technique to identify review relevance.
Haghighi et al. [8] use a dependency tree representation to determine text entailment. They use node and path sub-stitutions to compare text graphs. Lin and Pantel [10] use dependency trees to identify inference rules in a piece of text.
 Some other forms of graph-based text representations in-clude sentence graphs [11], lexical chains [1] or encyclopedic graphs [5]. Mihalcea et al. use text graphs for two appli-cations -(1) keyword extraction and (2) text summariza-tion [11]. Barzilay et al. [1] use lexical chains to perform text summarization. Coursey et al. [5] use an encyclopedic graph constructed from Wikipedia categories to determine the topic of an input document. Most of these representa-tions do not capture word-order information. Thus ours is a novel form of text representation.
Vertices represent noun, verb, adjective or adverbial words or phrases in a text, and edges represent relationships be-tween vertices. Graph generation includes the following steps: Dividing text into segments: A piece of text may con-tain multiple text segments. A text segment is a complete grammatical unit. Each segment of a sentence is separated by period (.), semicolon (;) or exclamation (!). We use the listed set of punctuations to break the text into multiple segments.
 Part-of-speech (POS) tagging: The text is then tagged with part-of-speech information (NN, DT, VB, RB 2 etc.) We use the Stanford NLP part-of-speech tagger to generate the tagged text [14]. This information is useful in determin-ing how to group words into phrases while still maintaining the order.
 Vertex and Edge creation: The vertex and edge creation steps are explained in Algorithm 1. From each sentence seg-ment consecutive subject components (which include nouns, prepositions, conjunctions and Wh-pronouns) are combined to form a subject vertex. Consecutive verbs (or modals) are combined to form a verb vertex; similarly with adverbs and adjectives. In Figure 2(b) tokens  X  X tudy X  and  X  X uide X  are combined to form the vertex  X  X tudy guide X .
 When a subject vertex is created the algorithm looks for the last created verb vertex to form an edge between the two. If no verb vertex is found, the subject-verb edge creation is postponed until a verb vertex is found. When a verb vertex is found, it looks for the latest subject vertex to create a subject-verb edge. Ordering is maintained when an edge is created i.e., if a verb vertex was formed before a subject ver-tex a verb-subject edge is created, else a subject-verb edge is created.
 An adjective or adverb may be used in the attributive (pre-cedes the noun or verb) or predicative (linked to the subject usually through a verb) position 3 . Initially the property is attached to a former subject or verb vertex (predicative), but if a subject or verb vertex is found to immediately fol-low the adjective or adverb, the property is removed from the former vertex and is attached to the new subject or verb vertex (attributive).
 Labeling graph edges: Graph edges that are created in the previous step are labeled with dependency (word-modifier) information. After edge creation, we iterate through all the graph edges to determine if a dependency relation exists between the edge X  X  vertices. If a dependency relation exists, it is added as the edge label as shown in Figure 2(b). Algorithm 1 Vertex and Edge Creation Algorithm
The degree of match between two graphs depends on the degree of match that exists between their vertices and edges. Vertex match between two texts is the average of the best match for each of their constituent vertices. Similarity be-tween two vertices is the average of matches between their constituent words or phrases.
 V ertexM atch ( G 1 , G 2 ) = (1) In Equation 1 G 1 , G 2 are two text graphs and V ( G 1 ), V ( G are the number of vertices in each of the graphs. BestVer-texMatch(v) indicates the best match for vertex v from one graph with a vertex from another graph.
 Edge matching involves comparison of an edge X  X  vertices and labels across graphs. If two edges have the same la-bels then an average of their vertex match gives the edge match. Edge labels capture grammatical relationship infor-mation between the edge X  X  vertices and are therefore signifi-cant in deciding the degree of match. Graph edge matching involves comparing single and double edges (two contigu-ous single edges containing additional context information) across graphs. Edges of same and different syntaxes are also compared across graphs. Matching edges with the same syn-tax helps preserve the order in which they occur in the text. Matching edges with different syntaxes helps capture word or phrase shuffling, which is found to be a common practice during paraphrasing [3]. The formula for single and double-edge match calculation is similar to Equation 1 where single and double edges replace vertices.
 The semantic match between tokens is determined using WordNet [6]. Match between two tokens could be one of exact , synonym , hypernym , hyponym , meronym , holonym , overlaps across definitions or examples of the tokens or a distinct or non-match .
 Running time: While comparing vertices from across two graphs, the values are stored in a matrix and are reused during edge comparison. Thus running time of the text-matching algorithm is measured in terms of the time taken to compare its vertices only. Time complexity is calculated in terms of number of vertices in the two graphs -v 1 and v Each vertex may contain words or phrases. Let the number of tokens in vertices v 1 and v 2 be n 1 and n 2 respectively. Running time is of the order of  X ( v 1  X  n 1  X  v 2  X  n 2 ).
We compare two types of text representations (1) our word-order graph representation explained in Section 3 and (2) a dependency tree based representation [2] to determine relevance while using the same text matching approach (ex-plained in Section 4). Our aim with this study is to show the usefulness of a word-order based text representation for the task of relevance identification. Additionally we show that the use of our graph representation results in faster text matching due to the presence of fewer vertices and edges. Data Our experiments are performed on review-submission pairs from two assignments completed using Expertiza [7]. In a review-submission comparison reviews are compared with their respective submission texts. In addition to that reviews are compared with other submission texts to include Table 1: Comparing precision, recall and f -measure values between the word-order and dependency tree representations and the n -grams baseline.
 some explicit non-relevant review-submission pairs. 292 review-submission pairs with an equal distribution of relevant and non-relevant pairs are selected for evaluation. Around 50% of the review-submission data was randomly selected and labeled by two different annotators. The two annotators X  ratings had a 78.3% agreement (greater than the baseline accuracy of 50%) and a Cohen X  X  Kappa value of 0.5661. Due to the high degree of agreement labels from the first annotator, for all text pairs, were used for evaluation. Results and Discussion Table 1 contains results of the use of our word-order graph as well as a dependency tree repre-sentation to determine review relevance. A relevance value of yes or no indicating presence or absence of relevance is predicted using the average match values as thresholds. We evaluate both the graph and tree representations by com-paring the precision, recall and f -measure values for pre-dicting relevance. These metrics are calculated for each type of structural match (vertex, edge or double edge). An un-weighted average of the three structural matches (A-match in Table 1) is also used to determine relevance.
 We use an n -grams (an average of 1-, 2-, 3-and 4-gram matches) based matching as our baseline. From Table 1 we see that the unweighted average match performs well for word-order graphs and produces higher precision, recall and f -measure values than the baseline. Our graph representa-tion produces higher f -measure values than the dependency tree for vertex, edge and double edges. We see that the val-ues increase when edges are used for relevance identification than when vertices are used. Since reviews tend to make references, in the form of paraphrases, to the author X  X  sub-mission, the addition of ordering or contextual information from edges helps improve the performance of relevance iden-tification.
 However, double edges do not improve on the results achieved by the edges. This is likely because double edges from sub-missions may contain additional information that the re-views may not have focused on. For instance when an edge  X  X thics  X  involve X  is compared with another edge  X  X iscuss  X  ethics X , the similarity is higher than when a double edge  X  X thics  X  involve  X  considerations X  is compared with a dou-ble edge  X  X tudy guide  X  discuss  X  ethics X . The double edge is likely to produce a lower similarity since it contains more mismatches. Lower similarity values of the double edge matches may cause more review-submission pairs to be de-clared as non-relevant and hence the poor performance. The dependency tree representation performs best when ver-Figure 3: Comparing number of vertices between the word-order graph and dependency tree repre-sentations.
 Figure 4: Comparing number of edges between the word-order graph and dependency tree representa-tions. tices are used for relevance identification. Its values decrease when single and double edges are used. Poorer performance of the dependency tree X  X  edges and double edges indicates that word-modifier information is not sufficient for relevance identification. It is likely that since the tree X  X  single and dou-ble edges do not capture word-order they do not perform as well as our graph representation for relevance prediction. Charts in Figures 3 and 4 illustrate the difference in the number of vertices and edges between a word-order graph and a dependency tree representation for a set of 41 ran-domly selected reviews. Dependency trees contain a larger number of vertices and edges when compared to our word-order graph representation. A similar trend was observed for submission texts as well. This implies fewer vertex and edge comparisons for the word-order graph than the depen-dency tree representation. Chart in Figure 5 depicts the time taken (in milliseconds) by the two representations for text matching. We see that for most of the cases the time taken for matching by the dependency tree is greater than the time taken by our graph. For the set of 41 randomly selected reviews dependency trees suffered from an average percentage increase of 29.4% in match time.
In this paper we have shown that a graph-based text representation, which captures word-ordering information is suited for the task of relevance identification. We have shown that such a representation performs better than a dependency tree representation, which contains only gover-nance information and no word-ordering information. We see from the results that order and context information play an important role in determining relevance. We plan on ap-plying this word-order based graph representation to deter-Figure 5: Comparing text matching time (in mil-liseconds) between the word-order graph and depen-dency tree representations. mine a review X  X  coverage of the important concepts discussed in a submission.
