 images, voices and videos. In order to excavate valuable information, we depend deeply on the technique of Nature Language Processing(NLP). Chinese word similarity computation is a fundamental research in many NLP tasks. It pro-vides a generic evaluation framework and is widely used in many fields, such as information retrieval, information extraction, text categorization, semantic divergence, case-based machine translation, and so on. Currently there are two methods to compute the similarity: knowledge-based and corpus-based method-s [1].
 numerously manual semantic resources, such as semantic content or relative po-sition within hierarchies. Obviously, it is a labor-intensive task that requires maintenance when new words and new word senses are coming out, and these methods have the disadvantage that the computing will work well only when the pair members both in the lexicons. And the corpus-based methods overcome these limitations by predicting unknown words. However, when facing large-scale corpora, it often leads to curse of dimensionality. Deep learning method are then proposed to solve this problem, such as skip-gram model [3] [4] and CBOW model [5], which apply large-scale corpora to train term vector, and then do the similarity calculation. Therefore, the drawbacks have attracted increasing attention to integrate lexicons into word embeddings to capture multiple seman-tics [6] [7].
 hardly compute similarity from extreme similar words. For instance, the words in the pair (gouru/buy, goumai/buy, score = 7.6) got the score much lower than expection and in the pair(xioaji/negative, jiji/positive, score = 8.6) got the higher one. In this paper, we propose a meaningful Extreme-Similar algorithm to solve this problem and combine the Extreme-Similar algorithm, word embedding and semantic lexicons for Chinese word similarity computation.
 computation and Distributed similarity computation are detailed in Section 2. Additionally in Section 2, we propose the Extreme-Similar algorithm to com-bine these two methods. In Section 3, we provide the experimental results and anlayses. Finally, we discuss related topics and conclude in Section 4. 2.1 Semantic Similarity Computation the concept [8] [9]. In other words, Hownet is a bilingual general knowledge-base describing relation between concepts and relations between the attributes of concepts [10]. The size of the Hownet depends largely on the size of the bilingual knowledge dictionary data file and it changes dynamically. Liu and Li. et al. [11] use set and structural characteristics to rewrite the word definition in Hownet by structural method. They reviewed the similarity computation method in sememe(Broadly speaking, a sememe refers to the smallest basic semantic unit that cannot be reduced further) , set and structural characteristics and proposed the word similarity computation algorithm based on Hownet. We leverage a method based on their work as shown in Eq. 1. w (word1) and w 2 (word2) computed by the Hownet algorithm. In order to facil-itate the observation and integration of the experimental results, we transform original domain [0,1] into [1,10] via a simple function g(x) in Eq. 2: 2.2 Distributed Similarity Computation tasks, generally, we should mathematilize the language. Word vector is such kind of form. There are two representation of word vector, one is one-hot Represen-tation, the other is distributed representation [12].
 al. . develops Word2vec which is a deep learning toolkit for learning the distribut-ed representation of words. Distributed representation aims to map each word into a k-dimensional vector [13].
 word in the corpus encoding with its semantic information [14]. There are two methods can be used to acquire the word vectors. The first is the singular value decomposition-based method, another is the iteration-based method, the latter one is utilized in our experiment. This method is mostly based on the training of language models to obtain the word vectors, as for a reasonable sentence, we hope the language model will give a higher probability. There also have two models to solve this problem, they are continuous bag of words model (CBOW) and skip-gram Model, respectively. After training, the obtained vectors, compose the vector spaces, with it we can define the semantic similarity of two words by considering the cosine distance between them. [15] [16].
 Continuous Bag of Words Model The continuous bag of words model uses the given context to predict the probability of the target word. In the continuous bag of words model, the context is represented by multiple words for a given target words. This calls for a modification to the neural network architecture. The modification, shown below, consists of replicating the input to hidden layer connections C times, the number of context words, and adding a divide by C operation in the hidden layer neurons.
 coded using 1-out-of-V representation means that the hidden layer output is the average of word vectors corresponding to context words at input. The output layer remains the same and the training is done in the manner discussed above. Skip-gram Model The skip-gram Model reverses the use of target and context words. In this case, the target word is fed at the input, the hidden layer remains the same, and the output layer of the neural network is replicated multiple times to accommodate the chosen number of context words [17].
 the similarity between two words by vectors cosine distance in Eq. 3: 2.3 Our c approaches pay little attention to the words that are extreme similar. We propose a simple but meaningful algorithm called Extreme-Similar algorithm. Obviously our approach focuses more on the pairs which are very similar.
 the result of Hownet model, which is defined as simh. Also, simw stands for the computing result of Word2vec model.
 tremely similar. And when the difference between simh and simw is large, the average of two score are very likely to lower than the reality, so we must to re-fine the final score. In these situations, we set  X  and  X  as the algorithm thresh-old. Then we utilize sequence-similarity(Vsim1), which stands for the similarity between 2 Chinese Words in the inspect of Chinese character sequence, and pattern-similarity(Vsim2), which measures the similar Chinese characters in a pair based on Tongyici Cilin, to refine the average(sim) of simh and simw. The Cilin dictionary is organized by a 5-layer hierarchical structure. Corresponding-ly, it supplies 5-layer patterns to encode for a group of words, which are joined by relationships like  X  X ynonym X  or  X  X elevance X . [18] Since the Geometric Mean is more closer to the bigger one, we calculate the square root(vsim) of vsim1 and vsim2 as a reference to correct the sim. In this way, Ssp is defined to characterize the adjustable space in a pair similarity measurement. The number 10 denotes two words in the pair are exactly the same, and moreover, 10 is the upper bound of Extreme-Similar.
 data and evaluation method, followed by the results of our system. 3.1 Dataset to some reasons, we gain more interests in the 3th task. And then we down-loaded the Test Data from NLPCC-ICCPOL2016 Shared Task [20]. There are together 10,000 pairs of words in the file, in which only 500 pairs with labels that selected as the final test data. For the close of the online submission, we asked the organizers of NLPCC-ICCPOL for the golden human labelled data, which helps us to evaluate our experiments results. These words mainly from news articles and weibo text, which insure capturing word usages both in formal written documents and causal short texts. The frequency of these words is not the same, they have about 30% in high frequency, half in middle and the rest in low, respectively. In addition, the selected words consist of nouns, verbs and Algorithm 1 Extreme-Similar Strategy adjectives, as well as some functional words like adverbs and conjunctions, and they have varied length, some of which have even more than one sense. Word pairs are constructed by an expert in computational linguistics, sometimes re-ferring to Tongyici Cilin. And the final score is got by calculating the average score of twenty post-graduate students scores, who major in linguistics. History, Society, Technology, Entertainment and some other categories and se-lect the most recent versions of all articles from 2004 to May 15, 20152. During preprocessing, phrase xml labels are transformed into the plain texts. We then remove all html labels and punctuations. On the other hand, we choose Sogou News as another corpus. These news data comes from several web sites in June -July 2012, which varies from the domestic, international, sports, social, en-tertainment and other 18 channels. Then we only retain the text. Then we use Python and the JieBa segmentation tool to support Chinese word segmentation of sentences. There are three modes, we choose the accurate mode, for it cuts the sentence more accurate and suitable. Finally, we get 8,676,665 documents, 205,427,273 words from Chinese wikipedia data and 401,227,433 words from Sogou News in total [21]. 3.2 Evaluation method two coefficients are widely used to evaluate the statistical dependence between our automatic computation results and the golden human labelled data. tions of the rank variables. The bigger r R is, the more related computing results and labeled data are, and which means the results are better.
 and n is the number of sample data. 3.3 Analysis of words on it. Our approach consists of three parts, the Hownet-based method, the Word2vec-based method, the Extreme-Similar algorithm method and we denoted them by H, W, ES respectively. There has two models in the Word2vec-based method, we use W-cbow and W-skip to respect them. Besides, we set  X  =9,  X  =7, the values we choose depend on the follow experiment. Table 1 shows the results of 6 merging strategies trained with 2 groups of different corpora, where  X  represents the Spearman  X  between the result and the golden score, and r is the Pearson r between the result and the golden score [22]. change. We can find that H or W are unable to get a good performance separate-ly. This is because Word2vec-based(W) method has two drawbacks in nature. Firstly, solely embedding technique cannot capture antonyms [23]. Secondly, there are some synonymy in Chinese vocabulary and the embedding method unable to calculate the similarity of pairs . And the Hownet-based method (H) depends on the corpora, which used for train, and it is unable to calculate the similarity of unknown pairs of words. H+W improves the performance a lot. In this combination, they made up for each other X  X  weaknesses. In addition, com-paring the results of No. 1-3 and 8-10 we can see that H is more convincing. In terms of the quantity and quality of corpora, the larger scale does not absolutely mean the higher performance. Then comparing the results of No. 4,5, No. 6,7, No. 11,12 and No. 13,14, it illustrates the effectiveness of our Extreme-Similar algorithm. Finally No. 12 is selected as the best model.
 we choose the H+W-cbow+ES and Wiki to train the result. Firstly, we set  X  =8, observing the results after changing the value of  X  from 0 to 10. Next we set TRhw=4, then change the value of  X  from 0 to 10. We can easily see that, when the  X  is changeless, the result will have better performance as the  X  increasing, which indicates that it is more likely to believe that the word pair is extremely similar when the result of Hownet to reach a vary large level. It fits the fact that the higher artificial score is, the more similar the word pair is. In terms of  X  , it is obviously to say that when  X  is around 7, our experiment will perform better. Since we are more likely to believe the H, if the simh is a lot larger than simw, the average of two score are very likely to lower than the reality, so we must to refine the final score. And 7 is the best dividing line. Chinese information processing, so we present a study for computing similarity of Chinese words. What is more, we devote our mind to the word pair which is extremely similarity, which others pay little attention.
 method and Extreme-Similar method. In our experiment, we train the Word2vector-based method by using different models and corpora firstly. Then using our o-riginality Extreme-Similar method, in this method, we utilize Tongyici Cilin to measure the similarity between 2 Chinese words in terms of the similar Chinese characters. At the same time, we record the Sequence-similarity of two words. Combining them with Geometric Mean makes the score of this word pair clos-er to the larger one. After a series of operating, we finally get our best model, and the results are 0.427/0.421 of Spearman/Pearson, which outperforms the state-of-the-art performance to the best of our knowledge.
 It helps us to clarify the relationship between words more accurately, which in turn assists us acquiring preferences from what people search or comment.
