 This paper promotes a new task for supervised machine learning research: quantification X  the pursuit of learning methods for accurately estimating the class distribution of a test set, with no concern for predictions on individual cases. A variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers. These tasks cover a large and important family of a pplications that measure trends over time. The paper establishes a resear ch methodology, and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications. In empirical tests, Median Sweep methods show outstanding ability to estimate the class distribution, despite wide disp arity in testing and training conditions. The paper addresses sh ifting class priors and costs, but not concept drift in general. H.4 [ Information Systems Applications ]: decision support. I.5 [ Pattern Recognition ]: Design Methodology, classifier design and evaluation Algorithms, Measurement, Design. classification, quantification, co st quantification, text mining. Tracking trends over time constitutes a very large family of business and scientific appli cations, e.g. monitoring the prevalence of hepatitis B. If th e cases are labeled accurately, either by humans or by supervised machine learning, then a simple histogram of the class la bels gives accurate counts. But commonly such classifiers have some degree of error that leads, perhaps surprisingly, to a large and systematic bias in the counts. There is a tremendous literature in machine learning that focuses on optimizing the correctness of individual predictions (accuracy, F-measure, etc.). While useful, it is not sufficient for this large family of applications whose objective is different: accurate estimation of the histogram count s. Interestingly, it does not matter that the count may involve both false positives and false negatives, as long as they balan ce one another well to cancel each other out. This brings about tw o challenging and valuable tasks for ongoing research: 1. The quantification task for machine learning : given a At first, this task may seem al most trivial, but experience proves otherwise. The ubiquitous pr actices of random sampling and cross-validation in machine learning research completely hide the problem, since the training class di stribution is chosen to match that of testing. This has perh aps suppressed recognition of this important research setting, as well as its more complicated testing methodology, which we address in Section 3 to evaluate the quantification methods proposed in Section 2. 2. The cost quantification task for machine learning : given For example, consider the task of estimating the total time spent (labor cost) by technical support ag ents dealing with calls related to problem X each month. One solution is to train a binary Table 1. Parameters varied in the experimental comparison P = 10...100 Positives in training set N = 100...1000 Negatives in training set p = 1...95% Percent positives in test set Benchmark = 25 binary text cl assification tasks, x 10 splits Learning Algorithms: SVM linear Support Vector Machine N B multinomial Naive Bayes Performance Metrics: Abs.Err |estimated p  X  actual p| Bias estimated p  X  actual p CE normalized Cross-Entropy Methods: CC Classify &amp; Count AC Adjusted CC, plus variants with selected thresholds MS Median Sweep of AC at all thresholds MM Mixture Model classifier to recognize problem X, and for each monthly batch of call logs, return the sum of the co st values of all cases predicted positive. Partly due to reasons stated above, this is a poor approach. We address this and propose methods in Section 6.3. A flip-side benefit of quantification technology is that to obtain an equivalent accuracy of the count, a much less accurate classifier can be used. This enables some applications of machine learning where otherwise its classification accuracy would be unacceptable. This can also result in savings in the labor cost to develop labeled datasets to train cl assifiers. At first blush, this savings may not seem substantial. However, at Hewlett-Packard we train thousands of classifiers to analyze technical support logs to track the many different types of support issues that arise for our many different product lines [3]. As a concrete example: monitoring for an increase in the in cidence rate of cracked screens on HP iPAQ handheld products. Further, the training needs are ongoing because of concept drift, as well as the introduction of product lines and new support issues. So labor savings from quantification technology continues to accumulate over time. We finish this section with a di scussion of related work, before we introduce and then test methods for quantification in the subsequent sections. In this paper, without much loss of generality, we primarily focus on two-class tasks in order to simplify the exposition and resolve a key sub-problem for the multi-class setting (Section 6.2). In this binary setting we can speak of estimating the number of positives in a test set. Most supervised machine learning research attempts to optimize the correctness of individual classifi cations in one way or another. Hence, each classification can be judged independently, and the success of a method can be judged by its accuracy, error rate or F-measure averaged over a large benchmark of tasks. In contrast, quantification produces a single output for a whole batch of items. These outputs must be aggregated over many batches in a large benchmark to evaluate methods. Hence, the research methodology is unusual. It bears a superficial resemblance to the batching used in research for probability estimating classifiers. For example, if a meteorologist predicts the chance of rain at 20% on certain days of the year, we would like it to rain on 20% of those days. The correctness of a prediction on a single day cannot be judged. Some methods even require examination of the entire test set before producing any output. Howe ver, probability estimation, like traditional classification, continues to make individual predictions on each item, and judge them in aggregate. By contrast, quantification makes a single prediction based on an entire batch X  X  single scalar for tw o-class tasks. This batching requirement calls for a different research methodology (section 3). Intuitively, not having to make individual predictions should make the estimation task easier. An insurance company can estimate how many cars will have accidents next year, but cannot predict which ones. The nature of the uncertainty is shifted from the individual cases to the aggregate count. Regarding probability estimation: one obvious idea for a quantification method is to induce a classifier that outputs calibrated probability estimates , and then to sum these probabilities over the test set to estimate the count for each class. This has an intuitive advantage over simply counting discrete predictions made by a traditional classifier, which loses information about the uncertainty of individual predictions. Nonetheless, this obvious met hod is ill-posed: the calibration depends critically on the class distribution of the training set, which does not generally match that of the test set in quantification (cf. it always matches under cross-validation). Estimating the class distribution of a target dataset is not new. But existing work in machine learning estimates the test class distribution in order to adjust the classification threshold [e.g. 1,9,10]. Again, the objective metric in such research has been the correctness of the individual cla ssifications. To our knowledge, ours is the first work to empirically compare and determine machine learning methods that excel in estimating the class distribution. This paper extends our recent publication [4] with superior methods, as well as a more focused experiment protocol. Of course, once accurate and robus t methods are established for estimating the distribution, they can be used as a subroutine for the traditional purposes of calibrating probability estimating classifiers, or optimizing the cl assification decision threshold to minimize cost, e.g. in ROC analysis [1]. As a side note, there is unsupervised work in tracking shifting topic distributions [e.g. 7,8]. It naturally has uncalibrated cluster boundaries, having no bearing on supervised quantification. As a strawman method, consider si mply learning a state-of-the-art binary classifier from the traini ng set, and counting the number of items of the test set for which it predicts positive. We call this simple method Classify &amp; Count (CC). The observed count of positives from the classifier will include true positives TP and false positives FP. Ideally, we w ould like to adjust the observed count somehow for the false positiv es and false negatives. By the following characterization, we derive such a quantifier, the Adjusted Count (AC) method [4]: Actual Class: Pos Neg P(predict + | actual pos), and fpr is its false positive rate , P(predict + | actual neg). The obs erved count of positives is then: = ( tpr  X  fpr ) * Positives + fpr * total Solving for the actual number of Positives, we get: Finally, dividing both sides by the total, we express the equation in terms of percentages of positives: It remains only to estimate the fpr and tpr characteristics of the classifier, which is accomplished via standard cross-validation on the training set. These characteristics are independent of the training class distribution because they treat positives and negatives separately. Since these estimates may deviate somewhat from the actual tpr,fpr rates in testing, we must clip the output of formula (1) to the range 0% to 100% in order to avoid occasional infeasible estimates. In summary , the AC method trains a binary classifier, estimates its tpr,fpr characteristics via cross-validation on the training set, and then when applied to a given test set, adjusts its quantification estimate by formula (1). The AC method estimates the true class distribution well in many situations, but its performance degr ades severely if the training class distribution is highly imbalanced (e.g. Figure 8 in [4]). For example, with P=50 positive training cases and N=1000 negative training cases, the induced classifi er is very conservative about voting positive. If the positive class is rare enough, it will simply vote negative always, i.e. tpr=0%. While this may result in optimal classification accuracy (for the distribution in the training Backing off from this extreme, consider a classifier that is very conservative but not entirely negative. Its true positive rate tpr would be very low, and its false positive rate fpr would be non-small range, making the quotient highly sensitive to any error in the estimate of tpr and fpr , giving bad estimates under imbalance. Unfortunately, high class imbalan ce is pandemic, esp. to our business applications, and so the need to operate well under these conditions is important. A well known workaround is simply to disregard many cases in the majority class of the training set to bring it back into balance. But th is throws away information. We can do better. Having investigated the reason for the degradation, we have devised new methods that are resilient under class imbalance and take advantage of any surfeit of negative training cases, which are often freely available. The solution to the class imbalance problem lies in recognizing that accuracy on individual classifications is not important to our objective. We will select a different decision threshold for the classifier that will provide better estimates via formula (1), although the threshold may be completely inappropriate for maximizing classification accuracy. Specifically, we will admit many more false positives to avoid a threshold in the tails of the curve, where estimates of tpr,fpr are poorer. The remaining question is what policy to use for selecting a threshold. To consider the possi bilities, we use the illustration in Figure 1. The x-axis represents the spectrum of thresholds, i.e. the scores generated by the raw classifier. They are uncalibrated and may take any range, e.g. the probability output by Na X ve Bayes, which is notorious for its poor probability calibration, or the signed distance from the sepa rating hyperplane of an SVM (calibrating the SVM output via a fitted logistic regression model would have no effect on the methods, since they do not use the magnitude of the x-axis except as a decision threshold). The descending curve show s the false positive rate fpr and the ascending curve shows the invers e of the true positive rate ( false negative rate = 1  X  tpr ). The inversion of tpr is visually useful to see the tradeoff with fpr . For a perfect classifier, there would be perfect separation between these two curves (this never occurred in our benchmark tasks). These example curves represent an SVM classifier whose natural threshold of zero delivers 92% classification accuracy for an imbalanced training set having 50 positives and 1000 negatives. Because negatives abound, SVM naturally optimized for a very lo w false positive rate, even at the cost of a  X  X ew X  misclassified positiv es (28 of 50). This explains its poor F-measure of 50%. The basic AC method uses the cl assifier X  X  default threshold, which will be far in the fpr tail if positives are rare in training. An intuitively better threshold policy is where the two curves cross, where fpr = 1-tpr (labeled X in Figure 1). This  X  X  X  method nicely avoids the tails of both curves. Considering the earlier discussion of small denominators, another likely policy is where the denominator is maximized: method Max = argmax(tpr-fpr). More traditionally, a Neyman-Pearson criterion would select the threshold at a particular true positive rate (method T90 = 90%, T50 = 50%), or false positive rate (F5 = 5%). We tested all these threshold policies and others. All the threshold selection methods above run the risk that the tpr , fpr estimates from cross-validation at their chosen threshold do not happen to match the actual rates encountered on the test set. For this reason, we consider an additional approach: obtain an estimate at every threshold, and return a mean or median of these estimates. Median is prefe rred, as it is less sensitive to outliers. Specifically, the Median Sweep (MS) method computes the distribution estimate via formula (1) for all thresholds, and returns the median. Considering the earlier discussion about high sensitivity when the denominator becomes small, we also evaluate a variant (MS2) that considers only thresholds where the denominator ( tpr-fpr ) is greater than  X . For comparison, we also include a robust quantification method from our earlier work, which is based on completely different principles. Due to space limitations, we cannot describe the method fully here, but we refer readers to [4]. In short, it models the distribution of raw classifier sc ores generated on the test set as a mixture of two distributions: the classifier scores on the training negatives, and those on the training positives X  X s determined by cross-validation. An optimization step determines the mixture that results in the best fit, and th en it returns this as the estimated % positive. This method is surpri singly robust with even as few as 10 training positives (!). While it does not apparently suffer from the class imbalance problem, we shall see that the Median Sweep methods often surpass it. 0% 20% 40% 60% 80% 100% As mentioned in the introduction, quantification research necessitates a substantially differe nt experiment methodology. In particular, the class distribution mu st be varied independently and dramatically between the training a nd testing sets. Further, since each batch produces only a single estimate, we must test on many batches and aggregate measurements to identify trends. The ideal quantification method will generate accurate estimates, despite wide variation in training and testing conditions. To vary the training conditions, we randomly select P=10...100 positive training cases, and N=100 or 1000 negative training cases from the benchmark dataset at hand. These sizes are selected to cover common operating ranges of interest to our business applications, and are reasonable fo r many other situations. The larger number of negatives represents a common multi-class case where we consider one class at a time against many others that each has 10...100 example cases. To vary the testing conditions, we select as many positives and negatives from the remaining benc hmark dataset such that the percent positives matches our target p. For a specific dataset having 864 positives and 10298 negatives, we first take 100 positives and 1000 negatives out fo r training (subsetting these for varied training situations), leaving 764 positives and 9298 negatives. When targeting p=20% positive, we test with all 764 positives and a random subset of 3056 negatives; for p=1%, we use a random subset of 94 positiv es against all 9298 negatives. Our business datasets often have &gt;100,000 cases to quantify, but are not publishable and rarely labeled with ground truth. Our prior study measured performance on test sets ranging from p=5%...95% positive, stepping by 5%. While reasonable scientifically, this does not focus on the area of interest for the business problems we face: 1...20% positives is a more challenging and more important range in which to estimate well. Furthermore, we speculate that this range is preferable to study in general. Four loose arguments: (a) For &gt;50% positive, one might simply reverse the meaning of positive and negative. (b) A common type of quantification task has many mutually exclusive classes, therefore most classes are moderately rare in order to sum to 100%. (c) For ~20...80% positive, class imbalance is not a problem, and classifiers tend to ope rate better in this region. (d) Finally, to get tight confidence intervals when estimating the percent positive, e.g. by manual testing, many more cases must be examined if positives are rare X  X o, the labor savings of automatic quantification is much greater in the tails. A natural error metric is the es timated percent positives minus the actual percent positives. By aver aging across conditions, we can determine whether a method has a positive or negative bias. But even a method that guesses 5 per centage points too high or too low equally often will have zero bias. For this reason, absolute error is a more useful measure. Bu t it is unsatisfactory in this way: estimating 41% when the ground trut h is 45% is not nearly as  X  X ad X  as estimating 1% when th e ground truth is 5%. For this reason, cross-entropy is often used as an error measure. To be able to average across different test class distributions, however, it needs to be normalized so that a perfect estimate always yields zero error. Hence, we use normalized cross-entropy, defined as: where q is the estimate of the actua l percent positives p in testing. Since cross-entropy goes to infinity as q goes to 0% or 100%, we back off any estimate in these situations by half a count out of the entire test set. Matching our intuition, this back-off will increasingly penalize a method fo r estimating zero positives for larger test sets: it is worse to mistakenly estimate zero positives among thousands of test cases than among ten. The benchmark text classifi cation tasks are drawn from OHSUMED abstracts (ohscal), the Los Angeles Times (la), and the Foreign Broadcast Information Service (fbis) [6]; the feature vectors are publicly available for download from the Journal of Machine Learning Research [5]. See Table 2. For this suite of experiments, we consider the bi nary classification task of one class vs. all others. Only 25 of the 229 potential binary tasks suited our needs, because we required a large number of positives and negatives for this study. (Our prior study went up to 2000 training negatives, but this deplet es the supply of negatives for testing, leaving only 21 suitable binary tasks.) F-measure for these tasks averages in the mid-70 X  X . We use the linear Support Vector Machine (SVM) implementation provided by the WEKA library v3.4 [11]. We also repeated the experiment with the multinomial Na X ve Bayes classifier, which has respectable performance in the text domain. The adjusted count methods and the mixture model all require cross-validation on the training set to generate the distribution of scores for positives and negativ es, in order to characterize tpr and fpr . We chose 50-fold stratified cross-validation. (Note that if there are fewer than 50 positives in the training set, it feels undesirable that some test folds will contain no positives. But we found only degradation trying 10-or min(50,P,N)-folds instead.) A great deal of computer time can be saved by sharing the computation that is common to a ll methods: (a) the 50-fold cross validation on the training set, (b) training the final classifier on generate scores for the positives and for the negatives. Different subsets of these scores can then be used to evaluate various quantification methods under differe nt test class distributions. Sharing steps 1 X 3 reduced the overall computational load of this experiment by a factor of ~ 200. As it was, the experiment consumed several days on hundreds of fast 64-bit CPUs in the HP Labs Utility Datacenter. The complete experiment protocol is listed in pseudo-code in the tech report version of this paper. Given the large number of parame ters in this experiment, we break down the results into sections where we hold some conditions constant as we vary others. For each figure, we will have a pair of graphs: N=100 training negatives on the left, and N=1000 training negatives on the right; we take care that the y-axis range is identical for easy comparison. Every data point represents an average performance over the 25 benchmark text classification tasks times 10 random splits. Except where stated otherwise, we focus on the SVM base classifier. We begin by examining how resilient the various quantification methods are to wide variations in the training set, while we hold the test conditions fixed at p=5% positive. Figure 2 shows the accuracy of the quantifications, as measured by absolute error from the 5% target. Overall we see the Median Sweep methods dominate (MS and MS2, bold lines). Note the absolute scale: the MS methods estimated on average within two percent given only P=30 positives and N=100 negatives, or within one percent given P=30 &amp; N=1000 (e.g. estimating 6% positive when the ground truth is 5%). Note the perform ance with P=50 is nearly as good as at P=100. When labor costs are involved to manually label training cases, this can am ount to significant savings. In the graph for N=100 training negatives (left), the simple Classify &amp; Count (CC) method achieved the lowest absolute error, but only for very specific training conditions. We seek methods with consistently good predictions, despite training variations. Next consider N=1000 negatives (right): CC appears competitive when given a large enough set of training positives, but this is illusory. Deeper examination reveals that for smaller P it underestimates and for greater P it progressively overestimates. The training class prior is simply moving the decision threshold, resulting in more positive predictions, not better quantification. By contrast, the basic Adju sted Count (AC) method does 0 1 2 3 4 5 absolute error absolute error cross-entropy cross-entropy converge to better quantification once sufficient training positives are available. With P&lt;40 &amp; N=1000, however, the class imbalance is so great that the AC method gets severe error. Our prior work highlighted the remarkable stability of the Mixture Model (MM) method even with great imbalance. MM is dominated, however, by many of the methods in this paper designed to address the class im balance problem. Interestingly, even without class imbalance (P  X  N=100 at left), the MS and T50 ( tpr =50%) methods surpass prior met hods. We return to this point in the discussion. (A few mediocre methods were omitted from the graphs to improve readability. The tech report version of this paper available at HP has a color appendix containing all the results.) The analysis above was for a single target of p=5% positives. Does the strong result for Median Sweep generalize for p=1...20% positives? To check this , we average the performance over this whole range. As discu ssed in the methodology section, to average different targets together, we must not simply use absolute error, but rather normalized cross-entropy. See Figure 3 for these results. Though the y-axis scale is now different than Figure 2, the rankings are qualitatively similar. Median Sweep continues to dominate on average. To determine whether Median Sweep dominates for all target distributions, we expanded the study up to 95% test positives. Instead of averaging over this range, we spread out p=1..95% along the x-axis in Figure 4; but to keep the focus on the lower region, we used a log-scale x-axis. Since we are not averaging results across different values of p, the y-axis shows absolute error, which is more intuitive than cross-entropy. To view the results in two dimensions, we must hold some other variable constant: we fix P=100 training positives, where performance is relatively insensitive to changes in P. In the low p range, Median Sweep methods excel. In the higher range, two methods excel cons istently: Max (maximize tpr -fpr ) or X (where fpr and 1-tpr cross). But in the following analysis we shall see that the Max method suffers from systematic bias. Finally, the absolute error grows s ubstantially for all methods as p approaches 95%. But this is not es pecially concerning: (a) it is a rare operating range, (b) if positives are so prevalent, it is more likely that the classifier would be trained with a positive majority rather than a negative majority as we have done here, and (c) in order to experiment at 95% positives, we end up with very few test negatives, due to the shorta ge of positive training cases. For example, if only 380 positives are available for testing, we end up with merely 20 test negatives in order to achieve p=95%. So, the experimental results have naturally higher variance in this region. If one needed to research this region more effectively, larger benchmark datasets are called for, or else the meaning of positives and negatives might be reversed. absolute error bias (average signed error) bias (average signed error) Next we analyze the other component of accuracy X  X ias X  X y presenting the average signed error for each method. Figure 5 shows the bias under varied training (left vs. right) and testing conditions (p% positives on x-axis). We abandoned the log-scale here in order to show the strongly linear bias of two methods: Max and Classify &amp; Count. For the classifier trained with 50% positives (P=N=100, at left), the CC method progressively overestimates when p&lt;50%, and underestimates when p&gt;50%, as expected. When trained with 9% positives (P=100, N=1000, at right), this balance point is shifted accordingly, but not proportionately X  X t is unbiased only at p=3% instead of 9%. We have seen this behavior cons istently: SVM exaggerates the training class imbalance in testing. Although the ubiquitous suggestion is to bias the SVM cost penalty C, it proves ineffective and has been better addressed recently by Wu and Chang [12]. It is surprising that the Max me thod, being an Adjusted Count variant, also exhibits a linear bias, albeit to a lesser degree than CC. This means that the Max met hod consistently finds thresholds such that the tpr and fpr characterization does not hold well for the test set. All the other met hods have a relatively stable bias over a wide range. We see greatly increasing bias at the tails, which is expected: If a method X  X  estimates vary by a few percent and p is close to zero, any infeasible negative estimates are adjusted up to 0%, resulting in a positive bias. As we appr oach p=95% positives, the even greater negative bias is similarly caused by clipping estimates which have greater varian ce, as shown previously. Although an induced classifier shoul d learn to separate cases well enough that its true positive rate tpr is greater than its false positive rate fpr , they nonetheless fail sometimes. This usually happens under great class imbalance in training. For example, for one of the ten splits on one of the tasks trained with P=10 and N=100, the induced classifie r X  X  natural threshold gave tpr=fpr=0. It learned to classify everything as negative, which results in a troublesome zero denominator in th e adjusted count method. The commonness of this problem was part of the impetus for this research: tpr was less than or equal to fpr in 623 of 10,000 cases for AC. In progressively decreasing occurrence of failure, we have: AC, T90, F5, F10, T50 a nd X. The Max method never experienced a failure, exactly because it seeks to maximize the denominator. Naturally, this is a non-problem for either CC or the Mixture Model. We do not present graphs for the Na X ve Bayes classifier because every quantification method under every training/testing condition performed substantially better on average with SVM as the base classifier. It is well esta blished for text classification that SVM usually obtains better accuracy than Na X ve Bayes. But our finding further suggests its tpr and fpr characteristics may be more stable as well. Given the consistent performance of the Median Sweep methods, we would like to ensure their c ontinued performance in situations with even greater training class imbalance (~1%), such as we face in practice. This study so far has been limited to N=1000 training positives, in order to have 25 benchmark tasks for study. Although we could increase class imbalance by simply reducing P, this results in degenerate cla ssifiers. Instead, we would like to consider a greater number of negativ es. In addition, we want to validate these results against other classification problems. For these two purposes, we held back a dataset: new3 from [6]. It has 9558 cases partitioned into 44 cl asses. We repeated our study on its 17 classes that have at least 200 positives, setting aside 5000 negatives for training. Figure 6 shows these results for P=50 and N=5000 negatives (~1% positives in training , right) and for a subset of N=1000 negatives (~5%, left). The log-scale x-axis shows p, and the y-axis shows av erage absolute error. Although perhaps uninteresting for its similar results to Figure 4 with N=1000, it is encouraging that the conclusions generalize to held-out tasks and to greater training imbalance. The Median Sweep methods continue to estimate well for low p; they have &lt;1% absolute error for p&lt;=10% in both graphs of Figure 6 and for N=1000 in Figure 4. The competitive methods Max and X have somewhat improved performance fo r this hold-out benchmark, and now slightly beat MS for p as low as ~5%. Observe in Figure 4 that the curves cluster into two shapes: concave upward (MS,MS2,T50) and S-curve (Max,X,MM). Interestingly, the AC method under high imbalance (N=1000 and absolute error all of Figure 6) belongs to the concave upward group, but under balanced training (N=100 in Figure 4) belongs to the S-curve group. As discussed previously, the AC method under high imbalance uses thresholds with many false negatives, i.e. closer to T50 in the concave upward group. (Recall that T50 selects the threshold at tpr= 50%.) But under more balanced conditions, AC uses thresholds closer to the X crossover point in Figure 1, which results in the S-curve grouping. Looking now at MS, its consistent grouping with T50 s uggests it may be using many estimates derived from tpr rates nearer to 50% than near the cross-over point X. We set out to address a probl em that occurred under class imbalance, and we ended up di scovering methods that estimate substantially better even under ba lanced training, e.g. Median Sweep and T50. (See P~N=100 in Figures 2-4.) Since the adjusted count formula is uncha nged from the basic AC method, this implies T50 selects a threshold for which tpr,fpr characterization on the training set is more reliable than the default threshold. This may pr ovide a clue to the separate problem of estimating tpr,fpr well for other purposes. We believe the reason that Median Sweep works so well is that instead of relying on the accuracy of a single tpr,fpr estimate, it takes in information from all the estimates, of which many are likely close. In some sense, it has the advantage of bootstrapping, without the heavy computational cost of repeating the 50-fold cross-validation for many diffe rent random samplings of the available training set. Until now we have referred to the Median Sweep methods together. Overall they perform very similarly, which we should expect since the median is very insensitive to outlier clipping, unlike the mean. Even so, the MS2 variant X  X hich only considers estimates that come from larger, more stable denominators X  X hows a slight benef it, particularly in lower bias and over a broader range. This validates the idea that small denominators result in poorer estimates. Putting MS2 into production runs the risk that on some tasks there may no estimates with a sufficiently large denominator, although this never happened in our benchmark tasks. At the very least, it could fall back to MS in such cases. Further research may develop a more robust outlier clipping method that could improve Median Sweep methods. One motivation mentioned in the introduction for quantification research is reduced training effort to obtain a given level of accuracy. To illustrate this, note in the right-hand graph of Figure 3 that Median Sweep methods with P=20 positives achieve similar accuracy to AC with P=50. But for the basic Classify &amp; Count method, additional training does not lead to an accurate, unbiased quantifier. Furthermore, in the left-hand graph of Figure 3 we see that additional training examples mislead AC. The point is this: quantification research is essential because accurate estimates cannot be achieved by simple methods like CC or AC just by providing more training data (unlike active learning research where all methods produce the same classification accuracy given enough training cases). Although we are pleased to have reduced the absolute error of the estimate to less than 1% in many situations, we need to quantify increasingly rare events, where the bias and the relative error both grow. To conduct experiments in the tail of the distribution requires much larger labeled datasets made available for research. The implications of this work extend to trending over time, multi-class quantification, and quantif ication of costs, which we describe in sequence. Measuring trends over time was lis ted as a primary motivation, but so far we have only di scussed quantifying the class distribution of a single test set. In order to apply this technology for trending, the cases are partitione d into discrete bins, e.g. daily or monthly groupings, and the quantification is performed independently on each batch to obtain accurate estimates. These may then be plotted together in one graph, optionally with a fitted trend line to project into the future where no cases are yet available. As is typical with such applications, if there are too many bins for the volume of data, the counts in each bin become small and noisy. The quantifica tion methods we describe are intended to work on large batches. They will produce noisy estimates given only a handful of items. For more accurate quantification in these situations , we have used a sliding window technique to aggregate cases from adjacent bins into each batch. At the same time, this provide s smoothing like a moving average, e.g. to smooth over weekend-effects. Note that this work addresses ch anges in the class distribution but not general concept drift, where the definition of what counts as positive may gradually or suddenly change arbitrarily [2]. When trending over time, concept drift is often implicated, and can be difficult to cope with. Ideally the quantifier used on each bin is given a training set appropriate to the class concept in effect for that bin. Naturally this can be hard to determine, and requires ongoing training data. Regardless of concept drift, if additional training cases become available later X  X .g. some cases are labeled in a new monthly batch of data X  X t is best to re do the trend quantification over all bins. The additional training data may improve the quantifier X  X  estimates on the old bins as we ll. If instead one applies the improved quantifier only to the ne w batch of data, this estimate should not be appended to pre-existing trend lines. To do so would compare estimates that are not calibrated to one another. In our use of quantification, we usually want to track the trends for many classes, e.g. different types of technical support issues. Since most customers call with a single problem, these classes are competing, and therefore may be treated as a 1-of-n multi-class problem. On the other hand, occasionally multiple issues do apply to a single case. If there were a rampant trend of coinciding issues, we would not want to have designed the system to be blind to it. Hence, we quantify each issue independently, i.e. as an m-of-n multi-class topic recognition task. Nonetheless, there are situati ons where the 1-of-n multi-class setting is called for. To treat this, one should not simply apply a multi-class classifier to the data set. If some classes are much rarer than others either in the training set or in the test set, the test set counts predicted for those cl asses may be very rare. The adjusted count method applied then to each class will not lead to good multi-class estimates. Instead we recommend performing independent quantifications for each class vs. all others, and then normalizing the estimates so they sum to 100%. In this way, each independent quantification compensates for imperfect classification and for class imbalance. Simply estimating the number of cases belonging to a category may not correlate with importance. A relatively uncommon issue having a high cost can be more important to delve into than a more frequent issue having low cost. If the average cost per positive case C + is known in advance, it can simply be multiplied into the quantifier X  X  estimate to obtain the total cost of the positiv e cases. More commonly C known, and we must analyze the cost attribute attached to each case, e.g. the parts &amp; labor cost to repair each problem. Consider a rare subclass of repairs whose co sts climbed substantially in the new month of data. Classify &amp; Total: The obvious solution, akin to CC, is to train a classifier and total the cost attribute associated with all cases predicted positive. But unless that classifier is perfectly accurate, it will result in poor and systema tically biased cost estimates. Grossed-Up Total: The next obvious solution is to perform the total as above, but then to adjust it up or down according to a factor f determined by quantifica tion. If the binary classifier predicted 502 positives and the quantifier estimates 598.3 positives, then the cost total would be multiplied by f=598.3/502. But this method suffers from simila r problems as AC: it runs the risk that the binary classifier may select zero or very few cases to include in the total, if positives happen to be rare in its training or test sets. Else if positives were overly common in the training set, then the induced liberal classifier will include in its total the costs of many negatives, polluting the result. This pollution occurs even at the perfect ratio if there are some false positives. Conservative Average * Quantifier (CAQ): We can reduce the false-positive pollution by selecting the classifier X  X  decision threshold to be more conservative X  X  classic precision-recall tradeoff. Using a smaller set of highly precise predictions, we can average their costs to estimate C + , and then multiply it by the estimated size of the class from a quantifier. Ideally we would like a threshold with 100% precision, but often there is no such threshold. Furthermore, a highl y conservative threshold based on precision may predict only a few cases as positive, esp. if positives are rare. Given too few items to average over, the variance of the C + estimate will be large, giving a poor overall cost estimate. To avoid this pr oblem, one might instead always take the top, say, 100 most strongly predicted positives for the average. But this cannot ensure high precision X  X ome test sets might have only 60 positives. Precision-Corrected Average * Quantifier (PCAQ): Despite decreased precision, there is pressure to use a less conservative threshold for the reasons above, and also because at high precision/low recall the classifier X  X  precision characterization from cross-validating the traini ng set has high variance. In classification threshold with wo rse precision, but having more stable characterization as well as providing a sufficient number of predicted positives to average over. We then adjust the average according to a simple equation that accounts for the false-positive pollution: precision-corrected average C + = (1-q) C t  X  (1-P t where q is a quantifier X  X  estimate of the percentage of positives in the test set, P t is an estimate of the preci sion at a given threshold t, C is the average cost of all cases predicted positive up to the threshold t, and C all is the average cost of all cases. The derivation is in the appendix of the tech report version of this paper available online. The rema ining design decision is which threshold to use X  X or example, th e T50 or X thresholds shown in Figure 1. We suggest avoidi ng Max, given our earlier bias discussion about its choosing thresholds with poor tpr,fpr characterization. Median Sweep PCAQ: Rather than use a single threshold and hope that its precision characterization is accurate, we may sweep over many thresholds and select the median of the many PCAQ estimates of C + . This has some of the benefit of bootstrapping without the computational cost. Just as the MS2 method excludes estimates that are likely to have high variance, a profitable variant on this method might exclude estim ates from thresholds where (a) the number of predicted positives falls below some minimum, e.g. 30, (b) the confidence interval of the estimated C + is overly wide, and/or (c) the precision estimate P t was calculated from fewer than, say, 30 training cases predic ted positive in cross-validation. Mixture Model Average * Quantifier (MMAQ): Finally, rather than try to determine an estimate at each threshold, we can model the shape of the C t curve over all thresholds as the mixture where C -is the average cost of a negative case (which is also unknown). This method estimates C + (and C -regression of the points generated at many different thresholds. The same thresholds omitted by Median Sweep can be omitted here as well, in order to eliminate some outliers that may have a strong effect on the linear regression. Alternately, one may use regression techniques that are less sensitive to outliers, e.g. that optimize for L1-norm instead of mean squared error. We found MMAQ outperformed CAQ in a small test. The next logical research step is to eval uate all these methods against one another. Unfortunately, any such empirical experiment depends strongly on the cost distribution for positives vs. the cost distribution for negatives (includi ng their relative variances), in addition to variation in the trai ning set makeup and the test class distribution. Besides its being a high dimensional experiment, we must first have a large publishable benchmark with costs of reasonable interest to a family of applications. This is an open invitation to the research community. In some settings, especially those in worldwide enterprises, cost values may be missing or detect ably invalid for some cases. Given that most of the above methods begin by estimating the average cost for positives C + , such cases with missing cost may simply be omitted from the analys is. That is, the estimate of C determined by the subset of cas es having valid cost values, and the count is estimated by a quantif ier run over all the cases. This can be effective if the data are missing at random (MAR). However, if the MAR-assump tion does not hold, the missing values should first be imputed by a regression predictor. The methods above implicitly assume that the cost of positive cases is not correlated with the prediction strength of the base classifier. As an assurance, one may check the correlation between cost and the classifier scores over the positive cases of the training set. If the classifier predicts the most expensive positives strongest, then the me thods above, esp. CAQ, will overestimate badly. Negative correlation results in underestimates. (This problem also ar ises if the classifier X  X  scores have substantial correlation with cost for negative cases.) To avoid these problems, we recommend the cost attribute not be given as a predictive feature to th e classifier. If the average cost for the positive class C + is similar to the overall average, then this attribute will generally be non-predictive. But in the interesting case where it is substantially di fferent from the background, this feature may be strongly predictive, e.g. a rare but relatively expensive subclass. In this case, it is tempting to provide cost as a predictive feature to improve the classifier. But it is better not to: the methods are explicitly designed to function despite imperfect classifiers. It is fortunate that quantification can be made to compensate for the inaccuracy of a classifier, yielding substantially more precise and less biased estimates. This requires only small amounts of training data, which can reduce labor costs compared with having to train highly accurate classifiers. These factors can lead to greater acceptance of machine learning technology for business use. We have been pushing machine learning within our company for years, but have never before experienced the business pull we find for quantifi cation [3]. To data mining researchers who wish to apply and develop advanced prediction models, this comes as some surprise, since the task seems so simple X  X t least on the surface. Though the Median Sweep, Max and X methods all show great improvement over prior technology, they are surely not the last word. Future work will involve research further down the tail toward greater class imbalance. Chemists easily talk about parts per million , but machine learning is cu rrently nowhere near up to the task. To research the tail will require very large benchmark datasets, ideally publishable ones for repeatability and experimentation by others. Studying high class imbalance conclusions are more sensitive to any noise in the answer key. Ideally, such a dataset would include individual costs to support research in cost quantification . The most effective methods may depend strongly on the characteristics of the data, so hopefully such a dataset would suit a popular family of applications. Other research directions are in multi-class methods, possibly including class hierarchies, or quantificati on under various constraints, such as having less tolerance for under estimating the size or cost of a subclass, as motivated by some business applications. Finally, trending over time naturally introduces concept drift, which is a challenging but important area for research. I wish to thank my colleagues Jaap Suermondt, Evan Kirshenbaum, Jim Stinger, Tom Tripp, and Farzana Wyde for their contributions in conceiving a nd developing this application. Thanks also to Bin Zhang for pre-reviewing this paper. [1] Fawcett, T. ROC graphs: notes and practical considerations [2] Fawcett, T. and Flach, P. A response to Webb and Ting X  X  [3] Forman, G., Kirshenbaum, E., a nd Suermondt, J. Pragmatic [4] Forman, G. Counting positives accurately despite [5] Forman, G. An extensive empirical study of feature [6] Han, E. and Karypis, G. Centroid-based document [7] Havre, S., Hetzler, E., Whitney, P., and Nowell, L. [8] Mei, Q. and Zhai, C. Discovering evolutionary theme [9] Saerens, M., Latinne, P., and Decaestecker, C. Adjusting the [10] Vucetic, S. and Obradovic, Z. Classification on data with [11] Witten, I. and Frank, E., Data mining: Practical machine [12] Wu, G. and Chang, E. KBA: kernel boundary alignment 
