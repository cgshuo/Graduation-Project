 Although it is common practice to produce only a single clustering of a dataset, in many cases text documents can be clustered along different dimensions. Unfortunately, no t only do traditional text clustering algorithms fail to prod uce multiple clusterings of a dataset, the only clustering they produce may not be the one that the user desires. In this paper, we propose a simple active clustering algorithm that is capable of producing multiple clusterings of the same dat a according to user interest. In comparison to previous work on feedback-oriented clustering, the amount of user feedba ck required by our algorithm is minimal. In fact, the feedback turns out to be as simple as a cursory look at a list of words. Experimental results are very promising: our system is able to generate clusterings along the user-specified dimension s with reasonable accuracies on several challenging text cla s-sification tasks, thus providing suggestive evidence that o ur approach is viable.
 I.5.3 [ Clustering ]: Algorithms Algorithms, Experimentation interactive clustering, active clustering, spectral clus tering, multiple clusterings, disparate clusterings
Text clustering has been widely researched. The inherent ambiguity of natural language and the fact that many text clustering problems involve a large, complex feature space (usually represented as a bag of words) and a large number of data points make it an exciting clustering task. However, there is an important aspect of text clustering that is often overlooked by text mining researchers: many text datasets can be naturally clustered along multiple dimensions . For instance, news articles can be clustered by topic, the source of the news (e.g., AP, CNN), or the date the article was writ-ten; political blog postings can be clustered not only by topic, but also by the author X  X  stance on an issue (e.g., support, oppose) or her political affiliation; and movie reviews can be clustered by genre (e.g., action, documentary), sentiment (e.g, positive, negative), or even the main actors/actress es. In some text mining applications, it is desirable and some-times important to recover as many clusterings of a dataset along its important clustering dimensions as possible.
Unfortunately, not only do traditional text clustering al-gorithms fail to produce multiple clusterings of a dataset, the only clustering they produce may not be the one the user desires. The traditional X  X ptimal objective X  X pproac h to clustering is overly constrained in the sense that it forces an algorithm to produce a clustering along a single dimension, specifically the dimension along which the objective functi on is optimized. Although many different objective functions have been used, the basic qualitative criteria employed to evaluate the structure of a clustering (e.g., intra-cluster sim-ilarity, inter-cluster dissimilarity, and the size of the c lusters) have remained more or less the same over the years. One important notion that is commonly left out of a qualitative measure is the human factor. As mentioned before, different subsets of features might lead to different kinds of cluster-ings of a dataset. Although a clustering algorithm identifie s a particular clustering as the most structured one (i.e., th e one that optimizes the objective), it might not be deemed fit by an end user, as she may be interested in a clustering that is different than the optimal clustering.

The question, then, is: can a clustering algorithm produce a clustering along the user-specified dimension (which may be suboptimal with respect to the objective function)? This also leads us to our second question: is it possible for a clustering algorithm to produce multiple clusterings of th e same data simultaneously according to user interest?
One may argue that it is possible to design the feature space in a way that helps induce the user-desired cluster-ing. This typically involves having the user identify featu res that are useful for inducing the desired clusters [14]. How-ever, manually identifying the  X  X ight X  set of features is bo th time-consuming and knowledge-intensive, and may require a lot of domain expertise. To overcome this weakness, re-searchers have attempted constrained clustering [2, 18] an d learning a similarity metric from side information [19] such as constraints on which pairs of documents must or must not appear in the same cluster. However, enough side infor-mation might not be readily available for many scenarios.
By contrast, recent work has focused on active clustering , where a clustering algorithm can incorporate user feedback during the clustering process to help ensure that the docu-ments are grouped along the user-desired dimension. One way to do this is to have the user incrementally construct a set of relevant features in an interactive fashion [3, 16]. A n-other way is to have the user correct the mistakes made by the clustering algorithm in each clustering iteration by id en-tifying the set of clusters that need to be merged or split [1]. A major drawback associated with these active clustering algorithms is that they involve a considerable amount of hu-man feedback, which needs to be provided in each iteration of the clustering process.

In this paper, we attack the problem of subjectifying text clustering, or clustering documents according to user inte r-est, from a different angle. We aim to develop a knowledge-lean approach to this problem  X  an approach that can produce multiple user-desired clusterings without relyin g on human knowledge for fine-tuning the similarity function or selecting the relevant features, unlike existing approach es. To this end, we propose a novel active clustering algorithm, which assumes as input a simple feature representation (com -posed of unigrams only) and a simple similarity function (i.e., the dot product), and operates by (1) inducing the im-portant clustering dimensions of a given set of documents, where each clustering dimension is represented by a (small) number of automatically generated words that are represen-tative of the dimension; and (2) have the user select the di-mension(s) along which she wants to cluster the documents by examining these automatically generated words. In com-parison to previous work on feedback-oriented clustering, the amount of user feedback required by our algorithm is minimal. In fact, the feedback turns out to be as simple as a cursory look at these automatically generated words and is required only once. Experimental results are very promising: our system is able to generate the user-specified clustering with reasonable accuracies on several challeng ing text classification tasks, thus providing suggestive evide nce that our approach is viable.

The rest of the paper is organized as follows. In Section 2, we enumerate the properties that are desirable of an algo-rithm for producing multiple clusterings and discuss relat ed work on multiple clusterings. In Section 3, we present our active clustering algorithm. We describe our evaluation re -sults in Section 4 and present our conclusions in Section 5.
In this section, we enumerate the desiderata for an al-gorithm for producing multiple meaningful clusterings of a dataset. By meaningful, we mean that each of these clus-terings should be qualitatively strong. As mentioned in the introduction, there are different ways to evaluate the qual-ity of a clustering (e.g., intra-cluster similarity, inter -cluster dissimilarity), which is typically captured by the objecti ve function employed by the clustering algorithm. A clusterin g is meaningful if it is not overly suboptimal with respect to the objective function.

Before we formalize the concept, let us introduce some notation. Let X = { x 1 , . . . , x n } be a set of n documents to be clustered, where each document x i , i = 1: n , is repre-sented by a bag of d unigrams w 1 , w 2 , . . . , w d . We want to learn m different partitioning functions f i , i = 1 : m , that correspond to m different clusterings C i , i = 1 : m . Specif-ically, each partitioning function f i takes data X as input, and outputs a 2-way partition C i = { C i 1 , C i 2 } , i = 1: m , such that C i 1  X  C i 2 = X and C i 1  X  C i 2 =  X  . 1 Finally, a clustering algorithm employs an objective function, o : C i  X  &lt; , which assigns a qualitative score to each clustering.

To produce multiple meaningful clusterings (henceforth multiple clusterings for brevity), we require a clustering al-gorithm to satisfy three multi-clustering criteria :
Multiplicity: Given data X , a clustering algorithm should be able to produce m (where m &gt; 1) different clus-terings of the data, C i , i = 1 : m , without having to change the feature space and the similarity function.

Distinctivity: Each clustering C i , i = 1 : m , should be distinctively different . By distinctively different, we mean that two clusterings are highly dissimilar. That is, the sim -ilarity of two clusterings should be close to zero.
Quality: Each clustering C i , i = 1 : m , should be qual-itatively strong (i.e., close to optimal) with respect to th e objective function o . This condition ensures that none of the clusterings that the algorithm produces is overly suboptim al and thus completely structure-less.

Now, the question is: do existing clustering algorithms produce multiple clusterings of a dataset and satisfy multi -plicity, distinctivity and quality? It turns out that a few o f them do [5, 7, 9, 11]. Below we first describe each of these algorithms, and then explain how our algorithm differs from them. These existing clustering algorithms can be broadly divided into two categories: Semi-supervised methods. In these methods, one of the clusterings is provided (by the human) as input, and the goal is to produce the other clustering, assuming that there are only two distinct ways to cluster the data. For instance, Gondek and Hofmann X  X  approach [9] learns a non-redundant clustering that maximizes the conditional mutual informa-tion I ( C ; Y | Z ), where C , Y and Z denote the clustering to be learned, the relevant features and the known clustering, respectively. Their approach turns out to be difficult to im-plement, since it requires modeling the joint distribution of the cluster labels and the relevant features. On the other hand, Davidson and Qi X  X  approach [7] first learns a distance metric D C from the original clustering C , and then reverses the transformation of D C using the Moore-Penrose pseudo-inverse to get the new distance metric D 0 C , which is used to produce a distinctively different clustering.
 Unsupervised methods. Here, each possible clustering of a dataset is produced in an unsupervised manner (i.e., with-out using any labeled data). Caruana et al. X  X  approach [5], also known as meta clustering, produces multiple clusterin gs of the same data by running k -means multiple times, each time with a random selection of seeds and a random weight-ing of features. The goal is to present each local minimum of k -means as a possible clustering. It suffers from two weak-nesses. First, it does not ensure that the aforementioned distinctivity and quality criteria are satisfied. Second, k -means tends to produce similar clusterings regardless of th e number of times it is run (see our meta clustering results in Section 4). Jain et al. X  X  approach [11] is more sophisticate d, as it tries to learn two clusterings in a X  X ecorrelated X  k -means it can be extended to produce n -way ( n &gt; 2) clusterings. framework. Its joint optimization model achieves typical k -means objectives and at the same time ensures that each of the two induced clusterings are distinctively different. No te that Jain et al. use this framework to produce only two clus-terings of the data, as the optimization objective becomes too convoluted to generate more clusterings.

Before providing the details of our active clustering al-gorithm, we describe the primary differences between our algorithm and the aforementioned approaches. First, our al -gorithm operates in an unsupervised setting, i.e., it neith er uses any labeled data nor assumes the existence of a prior clustering, unlike the semi-supervised methods. Second, i t satisfies all three multi-clustering criteria (i.e., multi plicity, distinctivity, and quality). Finally, it is not restricted to producing only two clusterings.
In this section, we describe our active clustering algo-rithm, which can produce multiple clusterings of the same data according to user interest. At the core of our algorithm resides spectral clustering. Even though spectral cluster ing is widely researched, it has traditionally been used to pro-duce a single clustering of a dataset. To our knowledge, we are the first to exploit spectral clustering to produce mul-tiple clusterings of the same data. Interestingly, a spec-tral clustering algorithm naturally satisfies all three multi-clustering criteria that we desire: multiplicity, distinc tivity, and quality. In the rest of this section, we first show that a small extension of a spectral clustering algorithm (namely , Shi and Malik X  X  spectral clustering algorithm [17]) satisfi es the three multi-clustering criteria and hence can be used to produce multiple clusterings. We then show how to incorpo-rate human feedback into the spectral clustering algorithm to produce the clusterings that the user desires.
Many variants of spectral clustering have been proposed (e.g., [15, 17]). Here, we use Shi and Malik X  X  spectral clust er-ing algorithm [17], as it is widely used. We first show how to extend their algorithm to produce multiple clusterings of a dataset. More specifically, we propose an extension of their algorithm that takes a set of n documents, X  X  &lt; n  X  d input, and outputs m different document clusterings, where d is the number of unigrams, as defined in Section 2. 2 In addition, as spectral algorithms work in a matrix space, we introduce another notation. Let s : X  X  X  X  &lt; be a sym-metric similarity function over X (i.e., s ( x i , x j ) = s ( x and S be the similarity matrix that captures pairwise simi-larities (i.e., S i,j = s ( x i , x j )).

Before describing our algorithm, we define two concepts: optimal and suboptimal clusterings. Given a clustering al-gorithm with a predefined objective function o , the optimal clustering is the clustering that optimizes o . All other clus-terings are suboptimal clusterings . Below we show how to learn the optimal clustering and several suboptimal cluste r-ings using Shi and Malik X  X  spectral algorithm.
 Learning the optimal clustering. Spectral clustering employs a graph-theoretic notion of grouping. A set of n data points in an arbitrary feature space is represented as an undirected graph, where each node corresponds to a data terings, we will center our discussion on 2-way clusterings . point, and the edge weight between two nodes is their sim-ilarity as defined by S . The goal is to induce a clustering, or equivalently, a partitioning function f , which is typically represented as a vector of length n such that f ( i )  X  { 1,  X  1 } indicates which of the two clusters data point i should be assigned to. Note that the cluster labels are interchangeab le and can even be renamed without any loss of generality.
Normalized cut [17] is a widely used objective function in spectral clustering. Shi and Malik show that if we embed the optimization problem in the real domain (i.e, we allow f to be a real-valued vector rather than a binary-valued vec-tor), then the normalized cut partition of X can be derived from the solution to the following constrained optimizatio n problem: subject to || f || 2 = X where D is a diagonal matrix with D i,i = P j S i,j and d = D i,i . It can be proved that the closed form solution to this optimization problem is the eigenvector correspondin g to the second smallest eigenvalue of the Laplacian matrix eigenvector 4 , e 2 , is trivial: we can just apply 2-means to the n data points represented by the second eigenvector [15]. Learning suboptimal clusterings. As mentioned before, suboptimal clusterings are useful if they can capture the user-desired clustering. Our algorithm for producing mult i-ple suboptimal clusterings exploits a useful but rarely uti -lized fact: if we consider only those vectors that are or-thogonal to e 2 as candidate solutions to the constrained op-timization problem above, then e 3 is the solution. More generally, if our candidate solutions are restricted to tho se vectors that are orthogonal to the first n eigenvectors of L , then e n +1 is the solution. In other words, all e n where n &gt; 2 are suboptimal solutions to the minimization problem, with e n being more suboptimal as n increases. Hence, we can apply 2-means to each of the eigenvectors, e n , separately to produce a suboptimal clustering. To our knowledge, em-ploying suboptimal partitioning functions to produce mult iple clusterings is an unexplored idea : existing work has focused on using only e 2 (or a combination of e 2 and other eigen-vectors) to derive a single partition of the data; in contrast, we use each of the e i s (with i  X  2) separately to produce multiple clusterings of the data.

So, our algorithm for producing multiple clusterings is simple: given data X and a symmetric similarity function s , we form the Laplacian L , compute its second through ( m + 1)-th eigenvectors, and apply 2-means to each of these m eigenvectors separately to produce m different clusterings.
Next, we show that our system satisfies each of the three multi-clustering criteria:
Multiplicity: Our algorithm produces m different clus-terings, where m &gt; 1, without changing the feature space and the similarity function. Hence, it satisfies multiplici ty. imation to the (discrete) normalized cut solution. Our def-inition of optimal and suboptimal clusterings refers to the continuous normalized cut objective. 4 We refer to the n th smallest eigenvector of the Laplacian simply as the n th eigenvector, and denote it by e n . Distinctivity: With some algebra, one can show that (1) L is symmetric when the similarity matrix S is symmetric, and (2) the eigenvectors of L are orthogonal to each other when L is symmetric. Since we always use a symmetric similarity function, S and L are symmetric. As a result, the eigenvectors of L are orthogonal to each other, and their similarity (obtained via the dot product) is zero. Since we employ the principal eigenvectors of L (starting from e 2 real-valued partitioning functions, the clusterings prod uced
Quality: As noted before, if we disallow the first n eigen-vectors of L to be the solution to our optimization problem, then e n +1 is the solution. This implies that clustering using e 3 achieves the next optimal objective value after e 2 , and more generally, clustering using e m +1 achieves the next op-timal objective value after e 2 , . . . , e m . Hence, e 3 constitute the ( m  X  1)-best suboptimal solutions that can be achieved by a spectral system. This gives us direct control over suboptimality: if we do not desire overly suboptimal solutions, we can simply put restrictions on m . In our ex-periments, we set m to 4, thus producing one optimal and
So far, we have shown how to produce multiple clusterings ( C i , i = 1: m ) of a dataset. To determine which of these m clusterings is the user-desired clustering, one can possib ly have the user inspect the clusterings and decide which one corresponds most closely to the desired clustering. The mai n drawback associated with this kind of user feedback is that the user may have to inspect a large number of documents in order to make a decision. To reduce human effort, we employ an alternative procedure: we (1) identify the most informative unigrams characterizing each cluster, and (2) have the user inspect just these  X  X eatures X  rather than the documents.

To select these informative features, we rank them by their weighted log-likelihood ratio (WLLR): where w i and C j denote the i th feature and the j th clus-ter respectively, and each probability is add-one smoothed . Informally, feature w i will have a high rank with respect to cluster C j if it appears frequently in C j and infrequently in  X  C j . This correlates reasonably well with what we think an informative feature should be. Now, for each of the m par-titions, we (1) derive the top 100 features for each cluster according to the WLLR, and then (2) present the ranked lists to the user. The user then selects the feature lists tha t are most relevant to her interest by inspecting as many fea-tures in the ranked lists as needed.

There is a caveat, however. The presence of a large num-ber of ambiguous documents can adversely affect the identi-fication of informative features, owing to the fact that many text documents are ambiguous with respect to the dimen-more precisely, two clusterings) in the continuous space. Their similarity might be different in the discrete space. captured using four eigenvectors. Note that using only up to e is by no means a self-imposed limitation of our algorithm, since we can employ as many eigenvectors as we desire. sion along which they are clustered. For instance, many product reviews are sentimentally ambiguous (i.e., they co n-tain both positive and negative sentiment-bearing words), as a reviewer often likes certain aspects of the product and dislikes the others. Hence, we remove the ambiguous docu-ments before deriving informative features from a partitio n.
We employ a simple method for identifying unambiguous documents. In the computation of eigenvalues, each data point factors out the orthogonal projections of each of the other data points with which they have an affinity. Ambigu-ous points receive the orthogonal projections from both the positive and negative points, and hence they have near zero values in the pivot eigenvectors. In other words, the points with near zero values in the eigenvectors are more ambigu-ous than those with large absolute values. We therefore sort the data points according to their corresponding values in the eigenvector, and keep only the top n/ 8 and the bottom n/ 8 data points. We induce the informative features only from the resulting 25% of the data points, and present them
In the event that the user identifies more than one eigen-vector as relevant to the desired clustering dimension, we apply 2-means to re-cluster the n documents in the space defined by all of the human-selected eigenvectors. Below is the final algorithm.
 Algorithm: Active-Spectral-Clustering Input: Data X , Similarity Matrix S
Output: Clustering C = { C 1 , C 2 } 1. Construct the Laplacian matrix L = D  X  1 / 2 ( D  X  S ) D 2. Compute E = { e 1 , e 2 , . . . , e m +1 } , the eigenvectors of 3. For each e i  X  E \ { e 1 } , induce the top feature list. 4. Ask the user to identify E 0 , the subset of E that is rele-5. Create the clustering C = { C 1 , C 2 } by using 2-means
Note that this active clustering algorithm produces a sin-gle clustering of a dataset along the dimension that the user selects. If we want to use our algorithm to produce multi-ple clusterings of the same data along different dimensions, we just need to repeat steps 4 and 5 with a different set of eigenvectors chosen by the user for each intended dimension . Datasets. We employ five evaluation datasets.

Blitzer et al. X  X  book (BOO) and DVD datasets [4] each con-tains 1000 positive and 1000 negative customer reviews of books or movies, and can therefore be used to evaluate our algorithm X  X  ability to cluster by sentiment . Since we desire that each evaluation dataset possesses at least two cluster ing dimensions, we also manually annotate each review with a subjectivity label that indicates whether it is  X  X ostly subjec-tive X (where the reviewer mainly expresses her sentiment) o r ments revealed that the list of top-ranked features is not particularly sensitive to slight changes to the number of un -ambiguous documents used in the feature induction process.  X  X ostly objective X  (where the reviewer focuses on describ-ing the content of the book or the movie). Details of the annotation process are described later in this subsection.
The MIX dataset is a 4000-document dataset consisting of the 2000 BOO reviews and the 2000 DVD reviews, as described above. We can therefore cluster these reviews by topic (i.e., book or DVD), sentiment or subjectivity . Schler et al. X  X  MAN dataset [13] contains 19,320 blog posts. We randomly selected 1000 blog postings, half of which were written by males and half by females. We can therefore clus-ter these blog posts by the author X  X  gender . Since the au-thor X  X  age information is also available in each blog post, we can also cluster them by age. To do so, we automatically generate a 2-way partitioning of the documents by impos-ing an age threshold of 25. Specifically, the 932 documents written by bloggers aged below 25 are marked as young , and the remaining 1068 are marked as old .

Our own POA dataset consists of 2000 political articles written by columnists, 1000 of whom identified themselves as Republicans and the remaining 1000 identified themselves as Democrats . 8 Hence, we can cluster these articles by the author X  X  political affiliation . We also create a second cluster-ing dimension by annotating each article as either foreign or domestic , depending on the policy that the article discusses. For example, the policy on the Iraq war is foreign, whereas the policy on regulating the job market is domestic.
Table 1 summarizes the dimensions along which the doc-uments are annotated. Note that each of the seven dis-tinct dimensions yields a 2-way partitioning of the docu-ments: (1) Sentiment (positive/negative); (2) Subjectivi ty (subjective/objective); (3) Topic (book/DVD); (4) Gender (man/woman); (5) Age (young/old); (6) Political affiliation (Democrat/Republican); and (7) Policy (domestic/foreign ). Human annotation. As mentioned above, we need to an-notate the BOO, DVD, and MIX datasets with respect to Subjectivity and POA with respect to Policy . 9 We had two computer science graduate students independently annotat e the documents. For POA, we asked them to use common-sense knowledge to annotate each document with respect to the policy that the article discusses. If both foreign and do -mestic policies are discussed in the same article, we asked them to assign the label based on the one that is discussed more frequently. On the other hand, given a BOO or DVD review, we asked them to first label each of its sentences as subjective or objective; if a sentence contains both sub-jective and objective materials, its label should reflect th e type of material that appears more frequently. The review is then labeled as subjective (objective) if more than half o f its sentences are labeled as subjective (objective).
The inter-annotator agreement rate in terms of Cohen X  X   X  is 0.774 (BOO), 0.796 (DVD), and 0.820 (POA), indicat-ing substantial agreement. The annotators examined each case on which they disagreed and decided on the final label together. They reported that essentially all disagreement s arose from labeling the ambiguous data points (e.g., arti-cles that discuss both foreign and domestic policies, and sentences that contain both subjective and objective ma-terials). In the end, we obtained 1205 subjective and 795 in 2006 from http://www.commondreams.org/archives. from BOO and DVD.
 Table 1: Clustering dimensions for the five datasets. objective documents for BOO, 1124 subjective and 876 ob-jective documents for DVD, and 875 foreign and 1125 do-mestic documents for POA. To stimulate research, we make Document preprocessing. To preprocess a document, we first tokenize and downcase it, and then represent it as a vec-tor of unstemmed unigrams, each of which assumes a value of 1 or 0 that indicates its presence or absence in the docu-ment. In addition, we remove from the vector punctuations, numbers, words of length one, and words that occur in only a single document. Following the common practice in the information retrieval (IR) community, we also exclude word s with a high document frequency, many of which are stop-words or domain-specific general-purpose words. Details of this preprocessing step can be found in Dasgupta and Ng [6]. We compute the similarity between two documents by taking the dot product of their feature vectors.
 Evaluation metrics. We employ two evaluation metrics. First, we report results for each dataset in terms of accu-racy, which is the fraction of documents for which the label assigned by our system is the same as the gold-standard la-clusters produced by our approach against the gold-standar d clusters using the Adjusted Rand Index (ARI) [10]. ARI is the adjusted-for-chance form of the Rand Index, which computes the pairwise accuracy given two partitions. ARI ranges from  X  1 to 1; better clusterings have higher values. Clustering using the second eigenvector only. As our first baseline, we adopt the commonly-used approach intro-duced by Shi and Malik [17] and cluster the reviews using only the second eigenvector, e 2 , which induces the parti-tioning that is optimal with respect to spectral clustering  X  X  objective function, as described previously. Results of th is baseline, reported in terms of accuracy and ARI, are shown method can propose only one clustering per dataset but each dataset contains at least two gold-standard clusterings (o ne for each dimension), the results are obtained by comparing this proposal clustering against each of the gold-standard clusterings. As we can see, accuracy ranges from 52.9 to 77.1, and ARI ranges from 0.003 to 0.291. Note that these and other results involving 2-means are averaged over ten in -dependent runs owing to the randomness in seed selection. Non-negative Matrix Factorization. As our second base-10 http://www.utdallas.edu/  X  sajib/multi-clusterings.html induced features, we are able to compute accuracy. 12 For each dataset shown in Tables 2 and 3, Dim n refers to the n th dimension listed for the dataset in Table 1. For instance, Dim1 and Dim2 of BOO correspond to Sentiment and Subjectivity, respectively. line, we use Non-negative Matrix Factorization (NMF), whic h has recently been demonstrated to be more effective than Latent Semantic Analysis (LSA) [8] for document clustering [20]. As in the first baseline, we compare the clustering pro-posed by NMF against each of the gold-standard clusterings for each dataset. Since the algorithm involves choosing see ds at random, the NMF results shown in row 2 of Tables 2 and 3 are averaged over ten independent runs. Despite its al-gorithmic sophistication, NMF performs consistently wors e than the first baseline in terms of both accuracy and ARI. Meta clustering. Since our algorithm produces multiple clusterings, it is desirable to have a baseline that also pro -duces multiple clusterings. However, many algorithms that produce multiple clusterings operate in a semi-supervised setting [7, 9]. The notable exceptions are Caruana et al. X  X  meta clustering algorithm [5] and Jain et al. X  X  approach [11 ]. Since some of our datasets can be clustered in three different ways but Jain et al. X  X  approach produces only two cluster-ings for a given dataset, we evaluate meta clustering only. A s mentioned before, meta clustering produces multiple clust er-ings of the same data by running k -means multiple times, each time with a random selection of seeds and a random weighting of features. We produce multiple clusterings for each dataset by running this algorithm 100 times and re-port in row 3 of Tables 2 and 3 the best result obtained for sults are reported, meta clustering underperforms the first two baselines for all but two dimensions (DVD/Dim2 and POA/Dim1). The poor performance can be attributed to the fact that k -means is generally a weaker clustering algo-rithm than its more recently developed counterparts. Iterative feature removal. We designed another simple baseline for producing multiple clusterings. Given a datas et, we (1) apply spectral clustering to produce a 2-way cluster-ing using the second eigenvector, and then (2) remove from each cluster the informative features that are identified us ing WLLR. To produce another clustering, we repeat these two steps, but without the features removed in step 2. Hence, we can generate as many clusterings as we want by repeating implementation of meta clustering, which is available at http://www.cs.cornell.edu/  X  nhnguyen/metaclustering.htm. these two steps, each time with a smaller number of features. The motivation behind this algorithm is that by repeatedly removing the informative features from a partition and re-clustering, we can potentially yield different clusterings . To obtain the results of this algorithm for each dataset in row 4 of Tables 2 and 3, we (1) run it for m iterations to pro-duce m clusterings, where m is the number of dimensions the dataset possesses; and (2) find the bipartite matching between the proposal clusterings and the gold-standard clu s-terings that has the highest average accuracy/ARI. Since we need to specify the number of features to remove from each cluster in each iteration, we tested values from 100 to 5000 in steps of 100, reporting the best result.

As we can see, except for BOO/Dim2 and POA/Dim1, this algorithm never surpasses the performance of the first baseline. One reason for its poorer performance can be at-tributed to the fact that the informative features for the di f-ferent dimensions of a dataset are not disjoint. For example, terrific is likely to be an informative feature when clustering by sentiment and by subjectivity, but since this algorithm removes informative features in each iteration, terrific will only be accessible to one but not both clustering dimensions . Human experiments. An important step in our algorithm involves having a user identify the dimensions along which she wants to cluster the documents by inspecting the fea-tures. To evaluate the feasibility of this step, we performe d the experiment independently with ten humans (all of whom are computer science graduate students not involved in data annotation) and computed the agreement rate.

Specifically, for each dataset, we generated four cluster-ing dimensions using the second through fifth eigenvectors of the Laplacian. After that, we showed the human judges the top 100 features for each cluster of each dimension ac-cording to WLLR (see Tables 4 and 5 for a snippet, where the dimension and cluster labels are manually assigned by the majority of the judges). To mimic the realistic situatio n where a user of our algorithm knows a priori the dimen-sion(s) along which she wants to cluster the documents, we informed each judge of the dimensions she was expected to identify: for example, for BOO, she was told to identify the Sentiment and Subjectivity dimensions. To ensure that the judges have a consistent understanding of the cluster-ing dimensions, we explained to them the seven clustering dimensions shown in Table 1 with definitions and examples prior to the experiment. (We omitted them here due to space limitations.) Also, we told them that a clustering di-mension could be captured by multiple eigenvectors. If they determined that more than one eigenvector was relevant to a dimension, they were instructed to rank the eigenvectors in terms of their degree of relevance, where the most rele-vant one would appear first in the list. Finally, they were given the option of not choosing any eigenvector for a given dimension if it was not possible for them to do so. Note that while only the top ten features are shown in Tables 4 and 5, they based their judgment on the top 100 features.
After the judges completed the experiment, we (1) se-lected for each dimension the largest set of eigenvectors th at Table 6: Human-selected eigenvectors and the agreement rate for the five datasets. was ranked first by the majority of the judges, and (2) com-puted the agreement rate as the percentage of judges who assigned the highest rank to the eigenvector set obtained in step 1. Results are shown in Table 6, where the agreement rate is shown in parentheses. As we can see, reasonably high agreement is achieved for all dimensions in all dataset s except Political Affiliation (Dimension 1 of POA) and Age (Dimension 2 of MAN). An inspection of the feature lists induced for POA and MAN (see Table 5) reveals that they are fairly noisy, which makes it difficult to identify these tw o dimensions. In fact, many judges could not find any relevant eigenvector for these dimensions.

Overall, these results, together with the fact that a judge took 8 X 9 minutes on average to identify each dimension, in-dicate that asking a human to identify the intended dimen-sion(s) based on the induced features is a viable task. Clustering results. Next, we cluster the documents in each dataset along each dimension using the eigenvectors selected by the majority of the judges. If more than one eigenvector is selected for a dimension, the documents will be clustered using 2-means in the space defined by all of the selected eigenvectors. The clustering results are shown in row 5 of Tables 2 and 3. As we can see, our clustering algo-rithm frequently outperforms the four baselines by a large margin. These results substantiate our claim that our al-gorithm can cluster documents along multiple dimensions according to user interest. In addition, clustering accura cies are generally higher for the topic-related dimensions (e.g ., Book vs. DVD and Domestic vs. Foreign) than the other di-mensions (e.g., Gender, Age, Sentiment). This should not be surprising: non-topic-based classification tasks can be di ffi-cult even for supervised systems that are trained on a large amount of labeled data (e.g., [13]).
Overall, we believe that our work on subjectifying text clustering makes three contributions: Generation of multiple clusterings. With a few excep-tions (e.g., [5, 11]), existing clustering algorithms can o nly produce a single clustering of a dataset along its most promi-nent dimension. In contrast, our algorithm can produce mul-tiple clusterings of the same data according to user interest without using any labeled data or considerable feedback. Interactivity in IR. The active clustering algorithm that we proposed allows for more user interactivity in an easy, low effort manner. Perhaps the implications of our work for interactivity in IR are even more important: we believe this and other interactive algorithms that allow the user to make small, guiding tweaks, and thereby get results better than would otherwise be possible is the future of IR.
 Improved understanding of spectral clustering. While spectral clustering has been employed for many years to pro-duce a single clustering of a dataset, to our knowledge we are the first to empirically demonstrate that the top eigenvecto rs of the Laplacian can be used in isolation or in combination to produce semantic clusterings.
We thank the three anonymous reviewers for their invalu-able comments on an earlier draft of the paper. This work was supported in part by NSF Grant IIS-0812261.
