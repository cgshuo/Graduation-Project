 Sparse approximation is a key technique developed in engineering and the sciences which approxi-mates an input signal, X , in terms of a  X  X parse X  combination of fixed bases B . Sparse approximation relies on an optimization algorithm to infer the Maximum A-Posteriori (MAP) weights  X  W that best reconstruct the signal, given the model X  X  f ( BW ) . In this notation, each input signal forms a column of an input matrix X , and is generated by multiplying a set of basis vectors B , and a column from a coefficient matrix W , while f ( z ) is an optional transfer function. This relationship is only approximate, as the input data is assumed to be corrupted by random noise. Priors which produce sparse solutions for W , especially L 1 regularization, have gained attention because of their usefulness in ill-posed engineering problems [1], their ability to elucidate certain neuro-biological phenomena, [2, 3], and their ability to identify useful features for classification from related unla-beled data [4].
 Sparse coding [2] is closely connected to Independent Component Analysis as well as to certain approaches to matrix factorization. It extends sparse approximation by learning a basis matrix B which represents well a collection of related input signals X  X he input matrix X  X  X n addition to per-forming optimization to compute the best set of weights  X  W . Unfortunately, existing sparse coding algorithms that leverage an efficient, convex sparse approximation step to perform inference on the convincingly demonstrated that back-propagation is a crucial tool for tuning an existing generative model X  X  output in order to improve supervised performance on a discriminative task. For example, greedy layer-wise strategies for building deep generative models rely upon a back-propagation step to achieve excellent model performance [5]. Unfortunately, existing sparse coding architectures pro-duce a latent representation  X  W that is an unstable, discontinuous function of the inputs and bases; an arbitrarily small change in input can lead to the selection of a completely different set of latent weights.
 We present an advantageous new approach to coding that uses smoother priors which preserve the sparsity benefits of L 1 -regularization while allowing efficient convex inference and producing stable latent representations  X  W . In particular we examine a prior based on minimizing KL-divergence to the uniform distribution which has long been used for approximation problems [6, 7]. We show this increased stability leads to better semi-supervised classification performance across a wide variety of applications for classifiers using the latent representation  X  W as input. Additionally, because of the smoothness of the KL-divergence prior, B can be optimized discriminatively for a particular application by gradient descent, leading to outstanding empirical performance. Uppercase letters, X , denote matrices and lowercase letters, x , denote vectors. For matrices, super-scripts and subscripts denote rows and columns respectively. X j is the jth column of X , X i is the ith row of X , and X i j is the element in the ith row and jth column. Elements of vectors are indicated by subscripts, x j , and superscripts on vectors are used for time indexing x t . X T is the transpose of matrix X . Sparse coding fits a generative model (1) to unlabeled data, and the MAP estimates of the latent variables of this model have been shown to be useful as input for prediction problems [4]. (1) divides the latent variables into two independent groups, the coefficients W and the basis B , which combine to form the matrix of input examples X . Different examples (columns of X ) are assumed to be independent of each other. The Maximum A Posteriori (MAP) approximation replaces the integration over W and B in (1) with the maximum value of P ( X | W,B ) P ( W ) P ( B ) , and the values of the latent variables at the maximum,  X  W and  X  B , are the MAP estimates.
 Finding  X  W given B is an approximation problem, solving for  X  W and  X  B simultaneously over a set of independent examples is a coding problem.
 P ( X ) = Given B , the negative log of the generative model can be optimized independently for each example, and it is denoted for a generic example x by L in (2). L decomposes into the sum of two terms, a loss function D L ( x k f ( Bw )) between an input example and the reconstruction produced by the transfer function f , and a regularization function D P ( w k p ) that measures a distance between the coefficients for the example w and a parameter vector p . A regularization constant  X  controls the relative weight of these two terms. For fixed B , minimizing (2) with respect to w separately for each example is equivalent to maximizing (1).
 In many applications, the anticipated distribution of x after being corrupted by noise can be modeled by an exponential family distribution. Every exponential family distribution defines a Bregman di-vergence which serves as a matching loss function for estimating the parameters of the distribution 1 . One common choice for the loss/transfer functions is the squared loss function with its matching linear transfer function, D L ( x k f ( Bw )) = P i ( x i  X  B i w ) 2 , which is the matching Bregman Diver-gence for x drawn from a multidimensional gaussian distribution.
 The regularization function D P ( w k p ) is also often a Bregman divergence, but may be chosen for other features such as the sparsity of the resulting MAP estimate  X  w . A vector is commonly called sparse if many elements are exactly zero. The entropy [9, 10], and L p p -norm 2 , p  X  1 regularization functions [2, 3, 4] promote this form of sparsity, and all of them have shown the ability to learn bases containing interesting structure from unlabeled data. However, of these only L 1 leads to an efficient, convex procedure for inference, and even this prior does not produce differentiable MAP estimates. We argue that if the latent weight vector  X  w is to be used as input to a classifier, a better definition of  X  X parsity X  is that most elements in  X  w can be replaced by elements in a constant vector p without sig-nificantly increasing the loss. One regularization function that produces this form of pseudo-sparsity is the KL-divergence KL ( w k p ) . This regularization function has long been used for approximation problems in Geophysics, Crystallography, Astronomy, and Physics, where it is commonly referred to as Maximum Entropy on the Mean (MEM) [7], and has been shown in the online setting to compete with low L 1 -norm solutions in terms of regret [11, 12].
 L 1 regularization provides sparse solutions because its Fenchel dual [13] is the max function, mean-ing only the most useful basis vectors participate in the reconstruction. A differentiable approxima-tion to max i x i is a sum of exponentials, P i e x i , whose dual is the KL-divergence (4). Regularization with KL has proven useful in online learning, where it is the implicit prior of the exponentiated gra-dient descent (EGD) algorithm. EGD has been shown to be  X  X parse X  in the sense that it can select a few relevant features to use for a prediction task from many irrelevant ones.
 The form of KL we use (4) is the full Bregman divergence of the negative entropy function 3 . Often KL is used to compute distances between probability distributions, and for this case the KL we use reduces to the standard form. For sparse coding however, it is inconvenient to assume that k  X  w k 1 = k p k 1 = 1 , so we use the full unnormalized KL instead.
 For the prior vector p we use a uniform vector whose L 1 magnitude equals the expected L 1 mag-nitude of w . p has an analogous effect to the q parameter in L q -norm regularization. p  X  0 approximates L 1 and p  X   X  approximates L 2 . Changing p affects the magnitude of the KL term, so  X  in (2) must be adjusted to balance the loss term in the sparse coding objective function (small values of p require small values of  X  ).
 Below we provide a) an efficient procedure for inferring  X  w in this model; b) an algorithm for itera-tively updating the bases B , and c) show that this model leads to differentiable estimates of  X  w . We also provide the general form of the derivative for arbitrary Bregman losses. To compute  X  w with KL-regularization, we minimize (3) using exponentiated gradient descent (EGD) with backtracking until convergence (5). EGD automatically enforces positivity constraints on the coefficient vector w , and is particularly efficient for optimization because it is the natural mirror descent rule for KL-regularization [12]. The gradient of the objective function (2) with respect to the coefficient for the jth basis vector w j is given in (6) for matching loss/transfer function pairs. This iterative update is run until the maximum gradient element is less than a threshold, which is estimated by periodically running a random set of examples to the limits of machine precision, and selecting the largest gradient threshold that produces  X  w within of the exact solution. The  X  parameter is continuously updated to balance the number of sucessful steps and the number of backtracking steps 4 . Because L 1 -regularization produces both positive and negative weights, to compare L 1 and KL regularization on the same basis we expand the basis used for KL by adding the negation of each basis vector, which is equivalent to allowing negative weights (see Appendix B). During sparse coding the basis matrix B is updated by Stochastic Gradient Descent (SGD), giving for w and is given in (7) for matching loss/transfer function pairs. SGD implements an implicit L 2 regularizer and is suitable for online learning, however because the magnitude of w is explicitly penalized, the columns of B were constrained to have unit L 2 norm to prevent the trivial solution of infinitely large B and infinitely small w . The step size was adjusted for the magnitude of  X  w in each application, and was then decayed over time as  X   X  1 / to optimize B through backpropagation, as explained in the next section. Sparse Coding builds a generative model from unlabeled data that captures structure in that data by learning a basis B . Our hope is that the MAP estimate of basis coefficients  X  w produced for each input vector x will be useful for predicting a response y associated with x . However, the sparse coding objective function only cares about reconstructing the input well, and does not attempt to make  X  w useful as input for any particular task. Fortunately, since priors such as KL-divergence regularization produce solutions that are smooth with respect to small changes in B and x , B can be modified through back-propagation to make  X  w more useful for prediction.
 The key to computing the derivatives required for backpropagation is noting that the gradient with respect to w of the optimization (3) at its minimum  X  w can be written as a set of fixed point equations where the gradients of the loss term equal the gradient of the regularization: Then if the regularization function is twice differentiable with respect to w , we can use implicit dif-ferentiation on (8) to compute the gradient of  X  w with respect to B , and x [14]. For KL-regularization and the simple case of a linear transfer function with squared loss,  X   X  w  X  X  is given in (9), where ~ e i is a unit vector whose ith element is 1. A general derivation for matched loss/transfer function pairs as defined before is provided in appendix C. Note that the ability to compute  X   X  w  X  X  means that multiple layers of sparse coding could be used. We verify the performance of KL-sparse coding on several benchmark tasks including the MNIST handwritten digit recognition data-set, handwritten lowercase English characters classification, movie review sentiment regression, and music genre classification (Appendix E). In each applica-tion, the  X  w produced using KL-regularization were more useful for prediction than those produced with L 1 regularization due to the stability and differentiability provided by KL. 6.1 Sparsity KL-regularization retained the desirable pseudo-sparsity characteristics of L 1 , namely that each example, x , produces only a few large elements in  X  w . Figure 1 compares the mean sorted and normalized coefficient distribution over the 10,000 digit MNIST test set for KL-divergence and several L p p regularization functions, and shows that although the KL regularization function is not sparse in the traditional sense of setting many elements of  X  w to zero, it is sparse in the sense that  X  w contains only a few large elements in each example, lending support to the idea that this sense of sparsity is more important for classification. 6.2 Stability Because the gradient of the KL-divergence regularization function goes to  X  with increasing w , it produces MAP estimates  X  w that change smoothly with x and B (see Appendix A for more details). Figure 1: Left: Mean coefficient distribution over the 10,000 digit MNIST test set for various regularization Table 1 quantifies how KL regularization significantly reduces the effect on  X  w of adding noise to the input x .
 This stability improves the usefulness of  X  w for prediction. Figure 2 shows the most-discriminative 2-D subspace (as calculated by Multiple Discriminant Analysis [15]) for the input space, the L 1 and KL coefficient space, and the KL coefficient space after it has been specialized by back-propagation. The L 1 coefficients tame the disorder of the input space so that clusters for each class are apparent, although noisy and overlapping. The switch to KL regularization makes these clusters more distinct, and applying back-propagation further separates the clusters.
 Figure 2: Shown is the distribution of the eight most confusable digit classes in the input space and in the 6.3 Improved Prediction Performance On all applications, the stability provided by KL-regularization improved performance over L 1 , and back-propagation further improved performance when the training set had residual error after an output classifier was trained. 6.3.1 Handwritten Digit Classification We tested our algorithm on the benchmark MNIST handwritten digits dataset [16]. 10,000 of the 60,000 training examples were reserved for validation, and classification performance was evaluated on the separate 10,000 example test set. Each example was first reduced to 180D from 768D by PCA, and then sparse coding was performed using a linear transfer function and squared loss 5 . The validation set was used to pick the regularization constant,  X  , and the prior mean for KL, p . Maxent classifiers 6 [17] were then learned on randomly sampled subsets of the training set of vari-ous sizes. Switching from L 1 -regularized to KL-regularized sparse approximation improved perfor-mance in all cases (Table 2). When trained on all 50,000 training examples, the test set classification error of KL coefficients, 2.21%, was 37% lower than the 3.53% error rate obtained on the L 1 -regularized coefficients. As shown in Table 3, this increase in performance was consistent across a diverse set of classification algorithms. After running back-propagation with the KL-prior, the test set error was reduced to 1.30%, which improves on the best results reported 7 for other shallow-architecture permutation-invariant classifiers operating on the same data set without prior knowledge about the problem 8 , (see Table 4).
 Table 2: The ability to optimize the generative model with back-propagation leads to significant performance 6.3.2 Transfer to Handwritten Character Classification In [4], a basis learned by L 1 -regularized sparse coding on handwritten digits was shown to improve classification performance when used for the related problem of handwritten character recognition with small training data sets ( &lt; 5000 examples). The handwritten English characters dataset 9 they used consists of 16x8 pixel images of lowercase letters. In keeping with their work, we padded and scaled the images to match the 28x28 pixel size of the MNIST data, projected onto the same PCA basis that was used for the MNIST digits, and learned a basis from the MNIST digits by L -regularized sparse coding. This basis was then used for sparse approximation of the English characters, along with a linear transfer function and squared loss.
 In this application as well, Table 5 shows that simply switching to a KL prior from L 1 for sparse approximation significantly improves the performance of a maxent classifier. Furthermore, the KL prior allows online improvement of the sparse coding basis as more labeled data for the character-recognition task becomes available. This improvement increases with the size of the training set, as more information becomes available about the target character recognition task.
 6.3.3 Comparison to sLDA: Movie Review Sentiment Regression KL-regularized sparse coding bears some similarities to the supervised LDA (sLDA) model intro-duced in [19], and we provide results for the movie review sentiment classification task [20] used in that work. To match [19] we use vectors of normalized counts for the 5000 words with the high-est tf-idf score among the 5006 movie reviews in the data set, use 5-fold cross validation, compute predictions with linear regression on  X  w , and report our performance in terms of predictive R 2 (the fraction of variability in the out-of-fold response values which is captured by the out-of-fold predic-a normalized exponential transfer function, f ( B,w ) = e Bw k e Bw k input. For sparse coding we use KL-divergence for both the loss and the regularization functions, as minimizing the KL-divergence between the empirical probability distribution of the document given by each input vector x and f ( B,w ) is equivalent to maximizing the  X  X onstrained Poisson distribution X  used to model documents in [21] (details given in appendix D). Table 6 shows that the sparse coding generative model we use is competitive with and perhaps slightly better than LDA. After back-propagation, its performance is superior to the supervised version of LDA, sLDA 10 . Table 6: Movie review sentiment prediction task. KL-regularized sparse coding compares favorably with LDA This paper demonstrates on a diverse set of applications the advantages of using a differentiable, smooth prior for sparse coding. In particular, a KL-divergence regularization function has significant advantages over other sparse priors such as L 1 because it retains the important aspects of sparsity, while adding stability and differentiability to the MAP estimate  X  w . Differentiability in particular is shown to lead to state-of-the-art performance by allowing the generative model learned from unlabeled data by sparse-coding to be adapted to a supervised loss function.
 Acknowledgments David M. Bradley is supported by an NDSEG fellowship provided by the Army Research Office. The authors would also like to thank David Blei, Rajat Raina, and Honglak Lee for their help.
