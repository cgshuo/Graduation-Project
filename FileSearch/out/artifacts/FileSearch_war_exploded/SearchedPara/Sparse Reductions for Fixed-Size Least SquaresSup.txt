 LSSVM [3] and SVM [4] are state of the art learning algorithms in classification and regression. The SVM model has inherent sparsity whereas the LSSVM model lacks sparsity. However, previous works like [1],[5],[6] address the problem of sparsity for LSSVM. One such approach was introduced in [7] and uses a fixed-size least squares support vector machines . The major benefit which we obtain from FS-LSSVM is its applicability to large scale datasets. It provides a solution to the LSSVM problem in the primal space resulting in a parametric model and sparse representation. The method uses an explicit expression for the feature map using the Nystr  X  om method [8],[9] as proposed in [16]. In [7], the authors obtain the initial set of prototype vectors (PVs) i.e. M vectors while maximizing the quadratic R` enyi entropy criterion leading to a sparse representation in the primal space. The error of FS-LSSVM model approximates to that of LSSVM for M N . But this is not the sparsest solution and selecting an initial value of M is an existent problem. In [11], they try to overcome this problem by iteratively building up a set of conjugate vectors of increasing cardinality to approximately solve the over-determined FS-LSSVM linear system. But if few iterations don X  X  suffice to result in a good approximation then the cardinality will be M .
The L 0 -norm counts the number of non-zer o elements of a vector. It results in very sparse models by returning models of low complexity and acts as a regularizer. However, obtaining this L 0 -norm is an NP-hard problem. Several approximations to it are discussed in [12]. In this paper, we modify the iterative sparsifying procedure introduced in [13] and used for LSSVM the technique as shown in [14] and reformulate it for the FS-LSSVM. We apply this formulation on FS-LSSVM because for large scale datasets like Magic Gamma, Adult and Slice Localization we are overwhelmed with memory O ( N 2 ) and computational time O ( N 3 ) constraints when applying the L 0 -norm scheme directly on LSSVM [14] or SVM [13]. The second proposed method performs an iteration of FS-LSSVM and then based on a user-defined window selects a subset of PVs as SV. For classification, the s elected vectors satisfy the p roperty of being correctly classified and are either closer or farther from the decision boundary since they well determine the extent of the classes. But for regression, the SV set comprises those PVs which have the least mse and are best-fitted by the regressor. Once the SV set is determined we re-perform FS-LSSVM resulting in highly sparse models without significant trade-off in accuracy and scalable to large scale datasets. The contribution of this work involves providing smaller solutions which use M &lt;M PVs for FS-LSSVM, obtaining highly sparse models with guarantees of low complexity ( L 0 -norm of  X  w ) and overcoming the problem of memory and computational constraints faced by L 0 -norm based approaches for LSSVM and SVM on large scale datasets. Sparseness enables exploiting memory and com-putationally efficiency, e.g. matrix multiplications and inversions. The solutions that we propose utilize the best of both FS-LSSVM and sparsity inducing mea-sures on LSSVM and SVM resulting in highly sparse and scalable models. Table 1 provides a conceptual overview of LSSVM, FS-LSSVM and proposes Reduced FS-LSSVM along with the notations used in the rest of the paper. Figures 1 and 2 illustrate our proposed approaches on the Ripley and Motorcycle dataset. Algorithm 1 gives a brief summary of the FS-LSSVM method. We first propose an approach using the L 0 -norm to reduce  X  w and acting as a regularizer in the objective function. It tries to estimate the optimal subset of PVs leading to sparse solutions. For our formulation, the objective function is to minimize the error estimations of these prototype vectors regulated by L 0 -norm of  X  w .Wemodifythe procedure described in [13], [14] and consider the following generalized primal problem: where  X  w  X  R M and can be written as  X  w = is now not on  X  w 2 but on  X   X  2 . The regularization weights are given by the prefix  X  j coefficients. This formulation is similar to [14] with the difference that it is made applicable here to large scale datasets. These  X   X  j are coefficients of linear combination of the features which result in  X  w vector. The set of PVs is represented as S PV .The  X  e i are error estimates and are determined only for the vectors belonging to the set S PV . Thus, the training set comprises of the vectors belonging to the set S PV .

Introducing the coefficient  X  for Lagrangian L one obtains:  X  L / X   X   X  j =0  X   X   X   X  and  X  L / X  X  i = 0 and after little algebraic manipulation yields with H =  X  Kdiag (  X  )  X  1  X  K + I M / X  and  X  K is a kernel matrix. The kernel matrix  X  H Algorithm 1. Fixed-Size LSSVM method
This, together with  X  L / X   X  b = 0, results in the linear system The procedure to obtain sparseness involves iteratively solving the system (2) for different values of  X  and is described in Algorithm 2. Considering the t th iteration, we can build the matrix H t =  X  Kdiag (  X  t )  X  1  X  K + I M / X  and solve the system of linear equations to obtain the value of  X  t and  X  b t . From this solution we get  X   X  t +1 and most of its element tend to zero, the diag (  X  t +1 )  X  1 will end up having many zeros along the diagonal due to the values allocated to  X  t +1 .Itwas shown in [13] that as t  X  X  X  ,  X   X  t converges to a stationary point  X   X   X  and this model is guaranteed to be sparse and result in set SV. This iterative sparsifying procedure converges to a local minimum as the L 0 -norm problem is NP-hard. Since this  X   X   X  depends on the initial choice of weights, we set them to the FS-LSSVM solution  X  w , so as to avoid ending up in different local minimal solutions. Algorithm 2. L 0 reduced FS-LSSVM method
The convergence of Algorithm 2 is assumed when the  X   X  t  X   X   X  t +1 /M is lower than 10  X  4 or when the number of iterations t exceeds 50. The result of the approach is the indices of those PVs for which |  X   X  i | &gt; 10  X  6 .These indices provide the set of most appropriate prototype vectors (SV). The FS-LSSVM method (Algorithm 1) is re-perfomed using only this set SV. We are training only on the set C PV and not on the entire training data because the H matrix becomes N  X  N matrix which cannot in memory for large scale datasets.

The time complexity of the proposed methods is bounded by solving the linear system of equations (2). An interesting observation is that the H matrix becomes sparser after each iteration. This is due to the fact that diag (  X  )  X  1 = diag (  X   X  2 1 ,...,  X   X  2 M )andmostofthese  X   X  i  X  0. Thus the H matrix becomes sparser in each iteration such that after some iterations inverting H matrix is equivalent to inverting each element of the H matrix. The computation time is dominated by matrix multiplication to construct the H matrix. The H matrix construction can be formulated as multiplications of two matrices i.e. P =  X  Kdiag (  X  )  X  1 and H = P  X  K .The P matrix will become sparser as it multiplies the  X  K matrix with diag (  X  )  X  1 .Let  X  M be the number of columns in P matrix with elements =0. This number i.e.  X  M canbemuchlessthan M .Thus,forthe L 0 reduced FS-LSSVM the time required for the sparsifying procedure is given by O ( M 2  X  M ) and the average memory requirement is O ( M 2 ). In [5], it was proposed to remove the support vectors with smaller |  X  i | for the LSSVM method. But this approach doesn X  X  greatly reduce the number of support vectors. In [6], the authors proposed to remove support vectors with the larger y f ( x i ) as they are farther from decision boundary and easiest to classify. But these support vectors are important as the y determine the true extent of a class.
We propose window based SV selection method for both classification and regression. For classification, we select the vectors which are correctly classified and closer and farther from the decision boundary. An initial FS-LSSVM method determines the  X  y for the PVs. To find the reduced set SV, we first remove the prototype vectors which were misclassified (  X  y = y ) as shown in [2]. Then, we sort the estimated f ( x j ) ,  X  j  X  CorrectPV where CorrectPV is the set of correctly classified PVs to obtain a sorted vector S . This sorted vector is divided into two halves one containing the sorted positive estimations (  X  y ) corresponding to positive class and the other containing sorted negative values (  X  y ) correspond-ing to negative class. The points closer to the decision boundary have smaller positive and smaller negative estimations ( |  X  y | ) and the points farther from the decision boundary have the larger positive and larger negative estimations ( |  X  y | ) as depicted in Figure 1. So these vectors corresponding to these estimations are selected. Selecting correctly classified vectors closer to decision boundary pre-vents over-fitting and selecting vectors farther from the decision boundary helps to identify the extent of the classes.

For regression, we select the prototype vectors which have least mse after one iteration of FS-LSSVM. We estimate the squared errors for the PVs and out of these prototype vectors select those vectors which have the least mse to form the set SV. They are the most appropriately estimated vectors as they have the least error and so are most helpf ul in estimating a generalization of the regression function. By selection o f prototype vectors which have least mse we prevent selection of outliers as depi cted in Figure 2. Finally, a FS-LSSVM regression is re-performed on this set SV.

The percentage of vectors s elected from the initial set of prototype vectors is determined by the window. We experimented with various window size i.e. (30 , 40 , 50) percent of the initial prototype vectors (PVs). For classification, we selected half of the window from the positive class and the other half from the negative class. In case the classes are not balanced and number of PVs in one class is less than half the window size then all the correctly classified vectors from those PVs are selected. In this case, we observe that the number of selected prototype vector s (SV) can be less than window size. The methodology to perform this Window reduced FS-LSSVM is presented in Algorithm 3.

This method result in better generalization error with smaller variance achiev-ing sparsity. The trade-off with estimated error is not significant and in several cases it leads to better results as will be shown in the experimental results. As we increase the window size the variatio n in estimated error decreases and es-timated error also decreases until the m edian of the estimated error becomes nearly constant as is depicted in Figure 3.
 Algorithm 3. Window reduced FS-LSSVM 4.1 Computational Complexity The computation time of FS-LSSVM method involves:  X  Solving a linear system of size M +1where M is the number of prototype  X  Calculating the Nystr  X  om approximation and eigenvalue decomposition of the The computation time is O ( NM 2 )where N is dataset size as shown in [10]. We already presented the computation time for the iterative sparsifying procedure for L 0 reduced FS-LSSVM. For this approach, the computation time O ( M 3 ). So, it doesn X  X  have an impact on the overall computational complexity as we will observe from the experimental re sults. In our experiments, we selected M = k  X  written as O ( k 2 N 2 ). We experimented with various values of k andobservedthat after certain values of k , the change in estimated error becomes nearly irrelevant. In our experiments, we choose the value of k corresponding to the first instance after which the change in error estimations becomes negligible.

For the window based method, we have to run the FS-LSSVM once and based on window size obtain the set SV which is always less than PVs i.e. M  X  M . The time-complexity for re-performing the FS-LSSVM on the set SV is
O ( M 2 N )where N is the size of the dataset. The overall time complexity of the approach is O ( M 2 N ) required for Nystr  X  om approximation and the average memory requirement is O ( NM ). 4.2 Dataset Description All the datasets on which the experiments were conducted are from UCI bench-mark repository [15]. For classification, we experimented with Ripley (RIP), Breast-Cancer (BC), Diabetes (DIB), Spambase (SPAM), Magic Gamma (MGT) and Adult (ADU). The corresponding dataset size are 250,682,768,4061,19020, 48842 respectively. The corresponding k values for determining the initial num-ber of prototype vector are 2,6,4,3,3 and 3 respectively. The dat asets Motorcycle, Boston Housing, Concrete and Slice Localization are used for regression whose size is 111 , 506 , 1030 , 53500 and their k values are 6,5,6 and 3 respectively. 4.3 Experiments All the experiments are performed on a PC machine with Intel Core i7 CPU and 8 GB RAM under Matlab 2008a. We use the RBF-kernel for kernel matrix construction in all cases. As a pre-pro cessing step, all records containing un-known values are removed from consideration. Input values have been normal-ized. We compare the performance of our proposed approaches with the normal FS-LSSVM classifier/regressor, L 0 LSSVM [14], SVM and  X  -SVM. The last two methods are implemented in the LIBSVM software with default parameters. All methods use a cache size of 8 GB. Shrinking is applied in the SVM case. All comparisons are made on 10 randomizations of the methods.

The comparison is performed on an out-of-sample test set consisting of 1 / 3of the data. The first 2 / 3 of the data is reserved for training and cross-validation. The tuning parameters  X  and  X  for the proposed FS-LSSVM methods and SVM methods are obtained by first determining good initial starting values using the method of coupled simulated annealing (CS A) in [18]. After that a derivative-free simplex search is performed. This extra step is a fine tuning procedure resulting in more optimal tuning parameters and better performance.

Table 2 provides a comparison of the mean estimated error  X  its deviation, mean number of selected prototype vectors SV and a comparison of the mean computation time  X  its deviation for 10 randomizations of the proposed ap-proaches with FS-LSSVM and SVM methods for various classification and re-gression data sets. Figure 4 represents the estimated error, run time and varia-tions in number of selected prototype vectors for Adult (ADU) and Slice Local-ization (SL) datasets respectively. 4.4 Performance Analysis The proposed approaches i.e L 0 reduced FS-LSSVM and Window reduced FS-LSSVM method introduce more sparsity in comparison to FS-LSSVM and SVM methods without significant trade-off for classification. For smaller datasets L 0 LSSVM produces extremely few suppor t vectors but for datasets like SPAM, Boston Housing and Concrete it produces more support vectors. For some datasets like breast-cancer and diabe tes, it can be seen from Table 2 that pro-posed approaches results in better error estimations than other methods with much smaller set SV. For datasets like SPAM and MGT, the trade-off in error is not significant considering the reduction in number of PVs (only 78 prototype vectors required by L 0 reduced FS-LSSVM for classifying nearly 20,000 points). From Figure 4, we observe the performance for Adult dataset. The window based methods result in lower error estimate using fewer but more appropriate SV. Thus the idea of selecting correctly cl assified points closer and farther from the decision boundary results in better determining the extent of the classes. These sparse solutions typically lead to better generalization of the classes. The mean time complexity for different r andomizations is nearly the same.

For regression, as we are trying to estimate a continuous function, if we greatly reduce the PVs to estimate that function, then the estimated error would be higher. For datasets like boston housing and concrete, the estimated error by the proposed methods is more than FS-LSSVM method but the amount of spar-sity introduced is quite significant. These methods result in reduced but more generalized regressor functions and the variation in the estimated error of win-dow based approach is lesser as in comparison to L 0 -norm based method. This is because in each randomization, the L 0 -norm reduces to different number of pro-totype vectors whereas the reduced number of prototype vectors (SV) for window based method is fixed and is uninfluenced by variations caused by outliers as the SV have least mse . This can be observed for the Slice Localization dataset in Figure 4. For this dataset, L 0 reduced FS-LSSVM estimates lower error than window approach. This is because for this dense dataset, the L 0 -norm based FS-LSSVM requires more SV (495) than window based method (208 , 278 , 347) which signifies more vectors are required for better error estimation. The pro-posed models are of magnitude (2  X  10)x sparser than the FS-LSSVM method. In this paper, we proposed two sparse reductions to FS-LSSVM namely L 0 re-duced and Window reduced FS-LSSVM. These methods are highly suitable for mining large scale datasets ov ercoming the problems faced by L 0 LSSVM [14] and FS-LSSVM. We developed the L 0 reduced FS-LSSVM based on iteratively sparsifying L 0 -norm training on the initial set of PVs. We also introduced a Win-dow reduced FS-LSSVM trying to better determine the underlying structure of model by selection of more appropriate prototype vectors (SV). The resulting approaches are compared with normal FS-LSSVM, L 0 LSSVM and two kinds of SVM (C-SVM and  X  -SVM from LIBSVM software) with promising performances and timing results using smaller and sparser models.
 Acknowledgements. This work was supported by Research Council KUL, ERC AdG A-DATADRIVE-B, GOA/10/09MaNet, CoE EF/05/006, FWO G.0588.09, G.0377.12, SBO POM, IUAP P6/04 DYSCO, COST intelliCIS.
 The spatial co-location pattern mining [1] discoversthe subsets of features (co-locations) of which the events are frequently located together in a geographic space. It has been applied to many areas like mobile commerce, earth science, biology, public health, and transportation[2]. Figure 1(a) shows a sample data set containing instances of six spatial features represented by distinct shapes. The instances of features describe the presence of their instances at different locations in a 2D or 3D space. A careful review reveals four co-location patterns as illustrated in Figure 1(c). To discover these patterns, the current research (see e.g., [2,3]) adopts an approach with two phases, namely (1) con-verting the spatial data set into a neighborhoodrelationship graph (NRG for short) using a distance threshold as illustrated in Figure 1(b) in which the distance threshold is de-f ned as the maximal distance allowed for two events to be neighbors; and (2) f nding prevalent co-locations based on their clique instances in the derived graph.
The f rst phase implicitly assumes the normal distances of neighbors being smaller than the predef ned distance threshold. This requires an approximately uniform distri-bution of spatial events across the space, as we ll as the joint distributions of features. In real life however, the data density often varies across different areas, leading to more complex joint distributions. For such data sets with various densities, an improper dis-tance threshold may severely affect the mining results due to two reasons. (1) First, a small distance threshold may ignore many clique instances of prevalent co-locations in sparse areas, resulting in these co-locati ons being under-estimated. (2) Second, a large distance threshold may introduce irrelevant clique instances of candidate co-locations in density areas, resulting in these co-locations being over-estimated.

Given the above concerns, we propose to f nd the regional co-location patterns as an extension to the conventional two-phase approaches. Our motivation comes from (1) the inconsistency of neighborhood distances in a data space; that is, the distance be-tween any two neighboring events varies across the space since the data densities vary from region to region in the space. This incons istency motivates us to investigate a new measure  X  X istance variation coeff cient X  (s ee Section 3) rather than distance thresh-old to drive the mining process; and (2) the existence of spatial heterogeneity which demonstrates that most geographic processe s vary by locations [4], and indicates that the inconsistent co-location sets may be found from different regions [5]. For instance, back to our example of mobile commerce, the requesting patterns in business regions are usually different from those in tourist regions.

As a mining strategy, we propose to hierarchically merge spatial events into local regions in the form of NRGs followed by passing them to the second phase of the con-ventional approaches. To enable the regional co-location pattern discovery meaningful, we partially inherit from the conventional approaches the assumption that a reasonable region is supposed to have relatively consistent neighborhood distances. This means the distances among the data points and their neighbors inside the region vary within a small range. Besides, we assume that differen t regions have inconsistent co-locating in-formation in terms of spatial heterogeneity. Given these, this paper addresses two prob-lems: (1) identifying regions with consistent neighborhood distances and co-locating information; and (2) specifying an i ndividual NRG for each identifie region.
We adopt k -nearest neighbor graphs ( k NNG) that capture more natural neighbor-hoods [6] to describe the consistency of neighborhooddistances within a region.Similar to distance thresholds, predef ning k value for each region may lead to under-estimation or over-estimation of co-locations. Instead, we introduce a new measure  X  X istance vari-ation co-eff cient X  to control the range of distance varying and automatically determine k value for each region. With k NNG, the regions X  NRGs are naturally prepared.
We then are able to defin the similarity of co -location informatio n between adjacent regions by passing these graphs to the second phase of the conventional approaches. The definitio of similarity is a measure about whether the corresponding regions share consistent co-location information and are qualif ed to be merged.Analogous to k NNG, the mutual k -nearest neighbors (M k NN) naturally capture the inter-connectivity of ad-jacent regions [7,8]. We adopt M k NN to exclude the real region boundaries when hier-archically merging the regions.
In summary, our contributions are as follows. (1) We propose a hierarchical mining framework to discover regional co-locations by considering both varieties of neighbor-hood distances and spatial heterogeneity. (2) We propose a novel  X  X istance variation coefficient to drive the mining process a nd determine an individual NRG for each re-gion. (3) We evaluate our mining algorithm with experiments on a real world data set by comparing against the conventional approaches using distance thresholds.
The remaining sections are organized as follows. We f rst review the current research results on spatial co-location pattern mining in Section 2, followed by giving a formal description for the research problem and some basic def nitions in Section 3. We then present our hierarchical mining framework to discover regional co-location patterns in Section 4, together with the experimental ev aluation in Section 5 before concluding our work in Section 6. Various algorithms have been proposed for spatial co-location pattern mining. They can be classif ed into four types as reviewed in the following.

One main type of these techniques are the aforementioned two-phase approaches, by which the NRG is determined by a predef ned distance threshold. This general frame-work was f rst proposed by Shekhar et al. [9]. Within the framework, different algo-rithms such as joinless algorithm [2], synchronic sweep algorithm [10], and density-based algorithm [3], were proposed to improve the performance of mining process, especially the eff ciency of collectin g clique instances. For example, Xiao et al. [3] proposed to search the dense areas with high priorities so as to speed up the decision making of the algorithm. The approach was also used in spatio-temporal data sets by introducing a time factor as the time interval threshold [11].

As a distortion of the f rst type of approaches, the second type diversif es the objec-tive of spatial co-location pattern mining. For example, it was extended to mine com-plex spatial co-location patterns (e.g., one-t o-many, self-colocating, self-exclusive, and multi-feature exclusive) [12] and maximal co-location patterns [13]. Huang et al. [14] also adjusted the interest measure to treat the case with rare events. Yoo et al. [15] proposed to f nd the N -most prevalent co-location patterns.

The third type replaces the usage of distance threshold in the fir t phase. Huang et al. [16] proposed to use density ratio of different features to describe the neighborhood relationship together with a clustering algorithm. A buffer-based model [17] was also proposed to describe the neighborhood relationship for dealing with extended spatial objects such as lines and polygons. Sheng et al. [18] used the inf uence functions to model the spatial features. Among these work, a similar neighborhoodrelated threshold or function has to be predef ned by users.

These three types of techniques focus on the global co-location patterns. That is, the f rst and second types adopt a predef ned distance threshold to determine the NRG, while the third type replaces it with a simila r neighborhoodrelated threshold.The fourth type assumes that the neighborhood distances are consistent. The data sets with various densities are not sophisticatedly treated in these work.

The fourth type of techniques discovers the regional co-location patterns. Celik et al. [19] straightforwardly applied the conventional approaches to a set of zones, where a zonal space has to be specif ed by users. Eick et al. [5] adopted the prototype-based clustering [20] to f nd regional co-location patterns. The interestingness of co-locations was scored in its f tness function. As an input of this approach, every event has a vector value of all the features, in which each item needs to be a continuous type. However, this work did not explore the monotonic property of the interesting measure proposed by Shekhar et al. [9] which introduces the pruning techniques to the mining process. Moreover, this approach may not be applicable to the discrete type of inputs. In this section, we will present the problem statement after some basic definition re-lated to regional co-location mining. 3.1 Basic Definitions A spatial data set is an input of the spatial co-location pattern mining algorithm. Definition 1 (Spatial data set). A spatial data set has a set of non-spatial features where E i ( 1 i n ) is a set of events of feature f i . Every event e j  X  E i ( 1 j | E i | ) has a vector information of feature type f i , event ID j , spatial location (x, y) . Given the neighborhood constraint, a spatial data set or part of it (one of its regions) can be converted into an NRG as the foundation for co-location discovery.The conventional approaches adopt the distance threshold to describe this neighborhood constraint which may lead to limitations as discussed in Section I. To get rid of those limitations, we adopt the k NNG to def ne the NRG in the following.
 Definition 2 (Neighborhood relationship graph (NRG)). The neighborhood relation-ship graph G is implemented by k NNG, in which each vertex represents a spatial event, and there is an edge connecting two vertices if they have different features and either of them is among the k NNs of the other one. The graph X  X  vertices and edges are denoted as V ( G ) and E ( G )= k NNG( V ( G ) ), and each edge is assigned with a weight that is the Euclidean distance between two s patial events connected by an edge.
 In our approach, each NRG corresponds to an individual region. In practice, k NNG can be calculated by f rstly fi ding the k NN of spatial events and then f ltering out the edges whose end points share the same feature type. In the context of NRG represented by k NNG, the neighborhood distance is the weight of an edge in the graph.
Given the above neighborhood constraint, the interestingness of prevalent co-locations within a region is defi ed by an interesting measure known as participation index which is in turn defi ed from participation ratio. We borrow the defi itions [1,2] as follows.
 Definition 3 (Participation ratio). Given the co-location C ( C  X  F ), its participa-the relational projection operation with duplication elimination and table instance is the collection of clique instances of co-locations or features, in each instance of co-locations the spatial events are neighbors to each other. Definition 4 (Participation index). The participation index of co-location C is Pi ( C ) =min f i  X  C { Pr ( C,f i ) } , which measures the prevalence of C .
 With a predefine prevalence threshold  X  , the second phase of the conventional algo-rithms such as join [1] or joinless algor ithm [2] can wisely discover for each region a set of co-locations of which the value of participation index is not less than threshold  X  (i.e., Pi ( C )  X  ). Moreover, due to the monotonic property of participation index, i.e., Pi ( C ) Pi ( C )  X  C  X  C , pruning techniques such as apriori [21] may be introduced into the mining process.

As foregoing, we assume that a reasonable region has relatively consistent neigh-borhood distances, meaning that the edge weights in an NRG have small variation. In a usual application domain of co-location dis covery, geographers and biologists care about the spatial patterns under specif c spa tial frameworks following clumped, ran-dom or uniform distribution [22]. In clustering applications, classic algorithms (e.g., k -means algorithm [20]) often assume that the clumped clusters are compact, imply-ing that they have consistent neighborhood distances inside. As for the data sets with random or uniform distribution, the local regions are also reasonable to hold the consis-tency. Based on the above applications, we defi e the distance variation coeff cient to investigate the neighborhood distances of regions as follows.
 Definition 5 (Distance variation coefficient). The distance variation coefficient of the calculating the mean value and standard deviation of the weights of all edges in G . Our mining framework allows us to hierarchically merge regions followed by f nding their prevalent co-location sets. During the merging process, the range of distance vary-ing is controlled by a corresponding distance variation threshold . The algorithm does not stop until the distance variation of every newly merged region is greater than , i.e.,  X  ( G ) &gt; .

As another assumption w.r.t. spatial heterogeneity,we def ne the similarity of regions in the following. By gradually combining the most similar regions under the distance variation constraint, our algorithm guarantees the maximum consistency of co-locating information inside the regions.
 Definition 6 (Similarity of NRGs). Given the prevalence threshold  X  and two NRGs G 1 and co-locations of either G 1 or G 2 .
 The size of a co-location C is the number of distinct features it contains, namely |{ f i | f i  X  C }| [1]. We indicate Pi ( C 1 i )  X  if Pi ( C )  X   X  C  X  C 1 i . The Jac-card index is a statistic commonly used for comparing the similarity of data sets. We adopt it to investigate the intersection rate of prevalent co-locations between regions. The similarity function helps us decide the priority of candidates to be merged. Finally, we give the following defi ition to identify the regions with rare events as anomalies. Definition 7 (Significant NRG). Given a ratio threshold  X  ,anNRG G and its cor-responding region are significant if | V ( G ) |  X  | E | ,where | E | is the total number of spatial events. 3.2 Problem Statement With the above defi itions, we give a formal description of regional co-location pattern discovery in the following.
 Given: (1) A spatial data set including a set of features F and a set of their spatial events E ; (2) a prevalence threshold  X  ; (3) a distance variance threshold ;and(4)a ratio threshold  X  .
 Find: Regional co-location patterns, i.e., (1) a set of regions  X  = {G 1 , G 2 , ..., G m } in which each element is represented as a k NNG, where V ( G i )  X  E , E ( G i )= k
NNG( V ( G i ) )( 1 i m ); and (2) a set of prevalent co-locations C for each region G i where Pi ( C )  X  .
 Constraints: Consistent neighborhooddistances and consistent co-locating information within regions, i.e., (1)  X  ( G i ) and | V ( G i ) |  X  | E | ( 1 i m ); and (2) minimiz-ing the similarity between adjacent regions R ( G i , G j ) ( 1 i,j m ). In the next, we present our algorithm for mining regional co-location patterns. The formed by assigning each of them an M k NN edge with k =1 or a single event that does not participated in those mutual edges. (2) In Step 2, the algorithm iteratively merges the similar regions under the constraint of distance variation. Step 2 is achieved by two sub-steps as follows. (2.1) In Step 2.1, in each iteration, k is increased by one and every newly generated M k NN edge links two of the currentregions which compose a merging candidate; and (2.2) in Step 2.2, we decreasingly sort the merging candidates by their similarity values and sequentially merge them if the constraint of distance variation is satisf ed. (3) In Step 3, the signif cant regions represented as k NNGs are returned together with their prevalent co-locations which are discovered by the second phase of join algorithm [1] in each region.

We illustrate the process of each iteration in Figure 2, in which Figure 2(a) shows four significan regions as indi cated by circles each of which has a prevalent co-location iteration; and Figure 2(b) shows a new region 5 which is the merge result of regions 1 and 3. Algorithm 1 presents the pseudo code of the mining process.

Step 1. Algorithm 1 fi st initializes the value of k as one, and sets the graph set  X  and the candidate set S empty (lines 1 X 2). It then f nds a set of M k NN edges with k =1 , which are disjoint to each other since any e vent has at most one mutual neighbor. For these mutual edges, we also require that the end points of them have distinct feature types (line 3). Naturally, a part of initial r egions are formed by assigning a found edge to each of them (line 4); whereas the rest part is formed by assigning each region with a single event that does not participate in any of those edges (line 5).

Step 2.1. It then starts to merge the current regions iteratively. The process does not terminate until the signif cant region can be merged, which means that the number of significan regions does not change (line 6 ). In each iteration, the algorithm increases k by one and updates the set of M k NN edges (lines 7 X 8). For each newly generated edge, a merging candidate is formed if it connects two of the current regions and the similarity is calculated (lines 9 X 12).

Step 2.2. Given a set of merging candidates, the algorithm decreasingly sorts them by their similarity values (line 13), and seque ntially investigates each candidate whether its two regions have not yet been merged (lines 14 X 15). Each candidate is investigated by a test merge and an evaluation of the merged region with the constraint of distance variation. If the constraint is satisfie or both of two regions of the candidates are still insignif cant, the algorithm r eplaces them with the newly merged region (lines 16 X 19).
Step 3. Based on a set of f nal regions, the algorithm calculates a set of co-location patterns for each of them if it is significan with small variation, and uses the second phase of join algorithm to f nd prevalent co-locations (line 20). Otherwise, the region is discarded since it has rare events which can be regarded as an anomaly. Finally, a set of signif cant regions and their prevalent co-lo cation sets are returned to users (line 21). Based on a real world data set, we evaluate t he mining results of our approach against the conventional ones, and study the trends of several statistics in our mining process. 5.1 Description of Real Data Set Figure 3(a) shows a real world data set (details shown in Table 1) we use for exper-iments. It is available at the Digital Chart of the World (DCW) Data Server [23] for research on spatial co-location discovery (see e.g.,[10,18]). Its spatial events have lo-cation information of latitudes and longitudes. Moreover, they are classifie by distinct types of landmarks, such as drainage, land cover, and populated place. It is obvious that the data set includes various densities. In the data set, the geographic coordinates are transferred to projection coordinates using Universal Transverse Mercartor projection. 5.2 Mining Results In this set of experiments, we run both our regional mining algorithm and the join algorithm on the DCW data set with parameters =0 . 6 ,  X  =0 . 6 and  X  =0 . 005 . Figure 3(b) illustrates our mining results (details shown in Table 2) for the data set shown in Figure 3(a), in which we identify nine regions and some regional co-locations.
In Table 3, we select four co-locations to de monstrate the difference between these two algorithms. The join algorithm discovers only the co-location { Ds, Hs, Pp } with a small distance threshold (30km). When the value of distance threshold increases, the other two co-locations ( { Ap, Ds, Pp } and { Hs, Cl, Pp } ) are detected. However, these regional co-locations are over-estimated to be globally prevalent. By contrast, our al-gorithm can detect them in their corresponding regions. We also f nd the co-location { Ap, Cl, Pp } in a single region which is under-estimated by the conventional ap-proaches. By comparing with the previous research results [18], our mining results include their discovered co-locations, and can assign them to the correspondingregions. 5.3 Evaluation of Regional Mining Process In what follows, we study the trends of four statistics in our mining process on the DCW data set with the information of the number of regions, the number of signif cant regions, the average similarity of adjacent regions and the average distance variation. Number of Regions. As can be seen from Figure 4(a), the number of regions decreases with the increase of k value. This is because the small regions are merged into larger ones. The decreasing is fast when k is small, while most of the regions are insignifica t and the merging condition is easy to satisfy. The fast rate of decreasing also indicates that a relatively small k value can suff ciently describe the neighborhood relationship of spatial events, as well as the regional structure of space.
 Number of Significant Regions. At the initial stage of mining process, there are hardly any significa t regions as illustrated in Figure 4(b). Then the number of significa t re-gions increases in a sudden when k =6 . This is because many insignifican regions are closing to the ratio line and become signif cant. The remaining set of insignif cant regions naturally become anomalies which are hard to merge due to different neighbor-hood distance from their adjacent regions. After that, the number of signif cant regions decreases when the similar regions are sequentially merged.
 Average Similarity of Adjacent Regions. To determine whether two regions are adja-cent, we calculate each region a minimum bounding rectangle (MBR) and extend its width and height by 10% . Two regions are regarded as adjacent if they have an overlap between their MBRs. According to Defi ition 6, we calculate the average similarity of adjacent regions as shown in Figure 5(a). At the beginning of the process the similar-ity fluctuate , and then decreases rapidly. To e xplain the f uctuation, we tentatively test the relationship of k and the distance variation in Figure 6. We continually generate a set of k NNGs of the DCW data set by increasing k value from one to larger values, then calculate the value of distance vari ation for each generated graph. As can be con-cluded, the distance variation of the graph is large with small k . It decreases as k value increases. This is because small value of k indicates a large gap of the neighborhood distance between the events in dense and sparse areas. When k becomes larger, the edges with larger weights are involved and reduce the gap. Thus, at the beginning of the merging process, the regions which have different neighborhood distance are not merged even if they have consistent co-locating information. With larger k values, the distance variation of regions becomes smaller, while the consistency of co-locating in-formation generally become s a primary reference for mer ging. This explains the early fluctuatio because the impact of distance vari ation and co-locating information match each other, and the later on drop of the simila rity because the co-lo cating information competes as the primary reference.
 Average Distance Variation. Figure 5(b) shows the implication between the average distance variation of regions and k values. With slight f uctuation, the average distance variation generally becomes large. The trend of curve verif es our explanation that the impact of distance variation and co-locating information match each other at the initial stage, and then the latter becomes the primary reference for merging. We have discussed the limitations of conventional approaches to mining spatial co-locations using distance thresholds, especially for data sets with various magnitudes of neighborhood distances. To get rid of those limitations, we have proposed a hierarchi-cal mining framework to discover regional co-locations accounting for both varieties of neighborhood distances and spatial heterogeneity. By adopting k NNG instead of dis-tance thresholds, we have proposed a novel  X  X istance variation coeff cient X  to drive the mining process and determine an indivi dual NRG for each region. With rigorous ex-periments on a real world data set, we hav e demonstrated that our framework has been effective for the discovery of regional co-location patterns.
 Acknowledgement. This work is partly supported by National Key Technologies R&amp;D Program of China under Grant No. 2011BAD21B02 and MOE-Intel IT Research Fund of China under Grant No. MOE-INTEL-11-06, in which Chiew X  X  work is partly sup-ported by National Natural Science Foundation of China under Grant No. 61272303. Due to the fl xibility of the graph model, it has a wide range of recent applications, such as biological databases, social networks and XML. To optimize query processing on graph data, many indexing techniques have recently been proposed. Unfortunately, graph data are often heterogeneousand the structures of their indexes are often complex and ad-hoc . As our experiments reveal, the performances of such graph indexes may vary greatly. This leads to a natural question for database practitioners: Which index is the most efficient for a given graph?
When compared to their relational counterparts,the structures of many graph indexes are far more complex. This causes a few uni que problems. Firstly, it is sometimes time-consuming to construct the indexes. For example, our experiments on a commodity computer show that given a random graph of a modest size (  X  3,000 vertices and a den-sity of 0.02), the construction time for a graph index, namely 2-hop labeling [12], is already 8.3 seconds. (For background details of the indexes discussed, please refer to our technical report [13].) While some other graph indexes can be constructed in less than a second, the most time-eff cient index can only be identif ed after all indexes have been constructed and benchmarked. Furt hermore, performance depends not only on the algorithms but also the details and quality of the implementation, as there is not yet a well-received ( i.e. , commercial) implementation in place. Secondly, graph indexes are sometimes several times larger than the graph itself. Using the example given above, the index generated by 2-hop labeling is 19 times larger than the graph whereas that by Interval labeling [1] is almost as large as the graph itself. Hence, it is not space-efficie t to use and maintain multiple i ndexes simultaneously on the entire graph. Overall, it is clearly desirable to be able to predict the optimal index and construct it, and only it, for eff cient query processing.

As a proof of concept, we focus on reachability queries  X   X  X iven two vertices, is one reachable from the other? X   X  which is a fundamental query of graphs. However, our proposed technique does not depend on any specif c type of graph query.
In this paper, we apply data mining techni ques to predict the relative performances of different graph indexes. One of the core problems is to extract important features (or characteristics) from data graphs. While a lar ge variety of features have been studied in graph theories [9], it is not yet clear which are the most relevant to index performances. Therefore, we propose to apply a general technique derived from spectral graph theo-ries [5], namely spectral decomposition, to solve this problem. Spectral methods have been reported successful in many applications such as VLSI design and computer vi-sion. To the best of our knowledge, this is the f rst work to empirically demonstrate the relationships between graph spectrums and i ndex performances. In general, graph spec-trums are known to be characteristics of graphs and to be related to many important graph properties. Another advantage is that they are supported by industrial-strength softwares, not to mention the availability of their advanced optimizations.
The second core problem is to represent the performance of a graph index. Our pre-liminary experiments show that the runtimes of 1,000 random queries on an index, even on the same graph, can often exhibit large variances. For example, we ran 1,000 ran-dom queries on each of 8,000 random graphs and indexed each using 2-hop and Prime labeling [11]. The mean and standard deviation of 2-hop are 14.1 seconds and 4.2 and those of Prime labeling are 11.6 seconds and 59.7, respectively. Moreover, the runtimes are often skewed and have a long tail at large values. (In a later section, we illustrate some of the runtime distributions in Figure 2.) We observe a similar phe-nomenon in our collection of scale-free graphs. A possible explanation is that a graph may contain many different sub-structures and the indexes are also complex structures. This leads to a wide range of runtimes. While average runtimes are often used to quan-tify index performances, it is desirable to propose a more f exible metric.
In this paper, we f t the runtimes of queries into a distribution. From our experi-ments on estimating its parameters, the goodness of f t of the Gamma distribution is always the best. By comparing the distributions of runtimes, we can obtain a more ro-bust and f exible way to compare performances. While we may apply the research on the Gamma distribution for further analysis, in this paper, we apply the inverse cumu-lative distribution function to estimate the time when y % of queries f nish. Depending on users X  applications, they may specify a value for y % to express their  X  X olerance X  of long query runtimes. For instance, some Internet connection providers ( e.g. hotels and cafes) charge their users according to conn ection time and so long query runtimes can be undesirable. To cater for their needs, da ta practitioners may choose the index that is optimal at 98%, instead of the one that has the optimal average runtime.
 Contributions. To our knowledge, this is the f rst investigation of graph spectrums in relation to index performances.We summarize the contributions of this work below and present an overview of these in Figure 1.  X  Given a graph G i in a database D , we propose a spectral decomposer to determine  X  We propose a spectral similarity function between two graphs.  X  We propose to f t the runtimes of an index on a given graph into the Gamma dis- X  We adopt the k -Means algorithm and k -nearest neighbor with a voting method for  X  We conduct experiments with a large numbe r of random and scale-free graphs. The The rest of the paper is organized as follows. Section 2 presents the backgrounds to graph spectral decomposition.We present our problem statement in Section 3. Section 4 presents the defi ition of the optimal index. A uniform spectral representation of graphs and a similarity function between graphs are proposed in Section 5. Our prediction method is detailed in Section 6 and our experimental evaluation is reported in Section 7. Section 8 discusses related work and Section 9 concludes this paper. In this section, we provide a background to graph spectrums. Graph spectrums have been widely used to study many interesting properties of graphs, such as spectral parti-tioning and expansion [8], the cut problem [7] and graph drawing [14].

Graph spectrum is often define using L aplacian matrix. Laplacian matrix L of a directed or undirected graph G =( V,E ) is define as L = D  X  A ,where D is the degree matrix of G and A is the adjacency matrix of G . More specificall , A i,j =1 if ( v ,v j )  X  E ;and A i,j =0 otherwise, where i and i are the ID  X  X  of the vertices v i and v . The degree matrix is a diagonal matrix and D i,i is the outdegree of v i
The Laplacian matrix L of G can be eigendecomposed as L = U X U  X  1 ,where  X  is a diagonal matrix of eigenvalues of L ,and U is a matrix of the corresponding eigen-vectors. Specificall , let  X  1  X   X  2  X  ...  X   X  n ,where n = | V | , denote the eigenvalues; X 1 ,X 2 ,...,X n denote the corresponding eigenvectors, where  X  i,i =  X  i ;andthe i -th column of U is X i . We call U the eigenvector matrix and use  X  to refer to eigenvalues. We use a subscript in U G and  X  G to denote the U and  X  of graph G if needed.
Eigenvalues  X  are called spectrums in spectral graph theories. Since eigenvectors U are also known to be closely related to the c haracteristics of vertices, we include them in our algorithm. We use the  X  and U of the underlying undirected graphs of the data graphs, as they capture their structures and their properties are well-studied [5]. In this section, we formulatethe optimal graph index prediction problem based on graph eigenvalues and eigenvectors.

We assume a graph database D containing a large number of directed graphs {
G 1 , G 2 , ..., G m } . The reachability query on the graphs is formally defi ed as follows.
 Definition 3.1. Given a directed graph G =( V,E ) , u , v  X  G , v is reachable from u , denoted as u v = true, if and only if there is a path from u to v in G .
 As discussed earlier, various types of inde xes are available to support the reachabil-ity query on graphs in D . However, it is desirable to predict the optimal one without building and benchmarking all the available indexes. This problem can be described as follows.
 Problem Statement. Given a set of indexes I = { I 1 , I 2 , ..., I n } and a graph database D , we want to build a predictive model M using the eigenvalues and eigenvectors of graphs in D , in order to efficiently determine the optimal index I opt G for a graph G  X  X  . There are two main sub-problems w ithin the problem statement. (P1) How can we represent the performance of an index on a graph? As motivated in Section 1, the query runtimes of a particular index on a particular graph may deviate from the average. Therefore, in Section 4, we investigate the more fl xible notion of the optimal index in expressing desirable runtimes. (P2) How do we compare spectrums of graphs? As mentioned in Section 1, we propose to represent a graph G with eigenvalues  X  G and eigenvectors U G . A similarity function between the spectral graph representations is needed. In addition, the number of eigenvalues and the dimension of the eigenvectors of a graph G is the number of vertices of G , which are not uniform in a graph database. They are therefore transformed into a uniform representation for comparison purposes. Finally, while the eigenvalues  X  G are invariants of G , the row vectors of the eigenvector matrix U G are dependent on the permutations of the vertex IDs of G . There is hence a row permutation probl em when comparing U  X  X  of graphs.
 We def ne the performance of an index using the query times of 1,000 random queries, as the query workloads are often not known when an index is chosen. To study per-formance, we plot the query time distribution, where the x -axis is the query time and the y -axis the number of queries f nished at time x . For example, Figures 2(a) and (b) show the query runtime distributions of two indexes on a random data graph. We demonstrate that average runtime alone may lack the f exibility to describe the desired notion of performance. For example, Figure 2(a) shows that Grail(1) [18] has the smallest average time but has a long tail. In comparison, the rutntimes of Interval exhibit a relatively small variance, although Interval has a relatively large average time. Therefore, Interval could be a better choice in applications where long query times are unacceptable or commercially unfavorable.
Once the query runtimes are presented as a distribution, we fi this to some well-studied distributions, such as Normal, Poisson and Gamma distributions. To measure goodness of f t, we adopt the L 2 -norm between the estimated and the real distributions. From our experiments on a large number of random and scale-free graphs, we observe that the Gamma distribution almost always yields the best f t. A possible reason for this is that it is often used to model the waiting time and the query runtime may be considered as the waiting time until queries fi ish. (The detailed experiment on the fitti g is presented in Section 7.) Moreover, the parameters of the Gamma distribution can be efficientl estimated. Therefore, we use it to represent the query runtime.
While the optimal index is intuitively the most eff cient on 1,000 random queries, we def ne it as the one with the smallest estimated runtime w. r. t. the user-def ned parameter y %( e.g. , 98%). Once the parameters of the Gamma distribution have been estimated, the notion of the optimal index can be tuned and determined mathematically by adjusting y .
 Definition 4.2. Given a set of indexes I = { I 1 , I 2 ,..., I n } , a graph G , and a set of random queries Q ,the optimal index in I of G is the index that finishes y % of queries of Q in the shortest time.
 An application of Defi ition 4.2 is that database practitioners may check the robustness of an index. For instance, given a graph database, we may use the estimated Gamma parameters to construct prediction models for a few y values, e.g. , 75%, 85% and 95%, without rerunning the benchmarking queries . An index can be considered robust if it is optimal for all those y values, instead of a specifi one. This section presents a similarity measure for the eigenvaluesand eigenvectorsof graphs. Specif cally, we f rst transform these into a uniform representation and then permute the rows of the eigenvector matrix to allow a similarity comparison. Finally, we propose a spectral similarity function for graphs. 5.1 Unifying the Dimensionalities of Graphs The f rst issue in using  X  and U to represent and compare graphs is that the dimensions of  X  and U of different graphs are different; that is, they cannot be directly compared. Therefore, we unify the dimensions of  X  and U as follows. (i) We use the tail-k non-zero eigenvalues of  X  G and the corresponding eigenvectors of U
G to represent a graph G . According to [15], the eigenvectors of the tail-k non-zero eigenvalues provide the best approximation of U G . This unif es not only the number of eigenvalues  X  G but also the column dimension of U G . (ii) Each row of U G corresponds to a vertex in G . The row dimension of U G of the graphs can be unif ed by adding rows of zeros, until the dimension of U G matches the largest graph in the database. Using simpl e matrix theories, we have that adding zero rows to a matrix affect neither its eigenvalue s nor the directions of its eigenvectors. In our context, a zero row vector corresponds to an isolated (virtual) vertex of a graph and does not affect the relative performance of indexes.

We further remark that the computation of the similarity between the eigenvector ma-trices involves determining the cosine similarity between each pair of the eigenvectors (to be detailed in Formula 1). The computation complexity is quadratic to the number of eigenvectors in the matrices. Due to this performance issue, we opt not to introduce eigenvectors to unify the column dimension of U G . In contrast, the computation time of the similarity between U G is linear to the size of the row dimension. Thus, our di-mension unif cation does not lead to a signif cant increase in computation time while retaining the characteristics of the vertices. 5.2 Permutation of Vertex ID While the eigenvalues of G are graph invariants [3], the eigenvectors (columns) in U G are not. Since a row in U G represents the characteristics of a vertex in G and the IDs of rows are directly related to the IDs of vertices, graphs with similar structures may have very different eigenvector matrices. This can be illustrated using the simple example shown in Figure 3. Here, graphs G 1 and G 2 are isomorphic and so are expected to have the same  X   X  X  and U  X  X . However, U G 1 and U G 2 are different, and a direct similarity computation (to be defi ed in Section 5.3) yields a low similarity score of 0.57. We reorder the rows in the eigenvector matrices of U G 1 and U G 2 to obtain U respectively, as shown in Figure 3. Using U we obtain a similarity score of 1.0.

Unfortunately, the number of possible permutations is n ! ,where n is the number of rows. Therefore, we propose three practical heuristic functions to reorder the rows. The aim is to be able to compare vertices with similar spectral characteristics. (i) VP rad : for each row in U G , we compute its L 2 -norm. We order the rows by their L -norms in descending order. Intuitively, the L 2 -norm of a row vector denotes its distance (radius) from the original point in a vector space. Therefore, the row vectors that are far (or, respectively, close) to the original point are compared. (ii) VP coor : According to [14], each row of U G can be considered as the coordinates of a vertex of G in a vector space. The rows can then be ordered in descending lex-icographic order. It is worth-remarking here that such an ordering of rows is biased towards the f rst few dimensions, which correspond to smaller eigenvalues and hence are considered more important than the latter dimensions. (iii) VP W rad : Since the eigenvalues indicate the importance of a dimension, we may integrate them with the heuristic f unction. Specificall , for each row vector, we compute its weighted L 2 -norm, where the weight of the i -th entry of a row is 1 / X  i ,and then order the rows by their weighted L 2 -norms in descending order.
 In summary,the processing presented abovecan be applied to both the rows and columns of all graphs to obtain a unif ed representation for similarity computation.In subsequent discussions, we simply use  X  and U to refer to the matrices whose dimensions have been unif ed and ordered in this manner. 5.3 Spectral Similarity between Graphs The central part of the prediction framework is the spectral similarity between graphs. This has two major components.
 Firstly, we determine the most comparable eigenvectors between two graphs G 1 and G 2 . Specificall , for each eigenvector X i in U G 1 ,wepair X i with an eigenvector Y j of U
G 2 whose direction is the most similar to X i , def ned as follows: where X i in U G 1 , Y j in U G 2 and cos sim denotes the cosine similarity between vectors.
Secondly, the spectral similarity between two graphs G 1 and G 2 is define as the weighted sum of the cosine similarity between paired eigenvectors, where the weights are proportional to the difference between the corresponding eigenvalues: where  X  is a parameter that controls the importa nce between eigenvalues and eigenvec-tors. In particular, the larger the value of  X  , the greater the inf uence of the eigenvalues on the sim function. To compare two spectrums, (i) we compute the weight determined by a Gaussian function on the differ ence between the eigenvalues, e  X  (  X  i  X   X  p [ i ] ) assume such a difference of eigenvalues follows a normal distribution; (ii) the cosine similarity between the corresponding eigenv ectors is multiplied by the weight; and (iii) the denominator normalizes the similarity function. With the uniform representation of graphs in D and the spectral similarity function, we are ready to present our prediction algorithm. In this paper, the label of a graph G is its optimal index among a given set of indexes I = { I 1 , I 2 ,..., I n } . We use the eigen-values and eigenvectors of graphs in D and their labels to train a prediction model. To summarize the graphs in D , we simply adopt classical k -Means clustering, while other clustering techniques may be applied. As we shall see from experiment, prediction with eigenvalues  X  alone is not as accurate as that achieved with both  X  and U .However, some other classical methods, such as neural networks or decision trees, require to cast U into some numerical values for training, while preserving their semantics. This does not appear trivial and so these are not adopted.
 A trained model is then used to predict the label of a graph G ,where G  X  X  . Specificall , given a graph G , we determine its k -nearest cluster centers and apply a voting method to obtain the weighted majority of their labels. Such a label is returned as the optimal index of G . 6.1 Clustering Algorithm In this subsection, we highlightthe adoptionof k -Means clustering for training a predic-tion model. The overall algorithm (Procedure kMeans ) for the construction is summa-rized in Figure 4. The label ( i.e. , the optimal index) of each graph G  X  X  is determined by the defi ition presented in Section 4. Then, similar graphs in D are clustered by Procedure kMeans . The label of a cluster center represents the label of that cluster.
While Procedure kMeans follows the general framework of classical k -Means al-gorithm, we highlight this particular adoption, i.e. , Lines 06-09. Most importantly, the major modificatio is to the recalculation o f the new cluster centers in Lines 08-09. Classical k -Means algorithms often use averagin g of a distance function between data objects in a cluster to obtain a new center. However, the averages of the eigenvectors or eigenvalues do not correspond to any clear semantics. Therefore, in Line 08, we de-termine the sum of the spectral similarity between each graph and all other graphs in a cluster. The new cluster center is the graph with the highest similarity sum (Line 09). Optimization. The clustering algorith m recalculates cluster centers in each iteration. However, in later iterations of Procedure kMeans , the changes in clusters are often small. In other words, the algorithm may then recompute the spectral similarity of the same graphs many times. To optimize this, we store the spectral similarities between graphs when they are f rst computed and then retrieved in later iterations. 6.2 Prediction Algorithm To predict the optimal index of a graph G  X  X  , one may be tempted to use the label of the cluster center that is the most similar to G . However, as discussed above, the cluster centers are data graphs themselves and some times may not be optimal. Accordingly,the prediction using one cluster center may be overly sensitive to the quality of the clusters. To enhance the robustness of th e prediction, we adopt a simple k -NN algorithm, where k is a user-define parameter. Specificall , given a graph G , we decompose it into  X  and U defi ed as the spectral similarity between C [ i ] X  X  center and G . The optimal index of G is the label with the largest sum of votes. In this section, we report on an experiment conducted to verify the accuracy and eff -ciency of the spectral decomposition approach to predict the optimal graph index. 7.1 Experimental Setup Implementation: We ran our implementation on a commodity PC with a Quad-core 2.4GHz CPU with 4G memory running Windows 7. We implemented our proposed technique using MATLAB R2011a. We used the functions provided by MATLAB as far as possible, such as those determining the eig envalues and eigenvectors, and statistical distribution f ttings. Graph Collections: We used both randomand scale-free graphs for our experiments,as they are popular classes used in graph analysis. Moreover, we controlled for their sizes and densities. The generators used were providedby Zhu et al. [19]. We generated 1,024 graphs of each type. For random graphs, the average number of vertices and fanout were 3.4k and 6.8, respectively. For scale-free graphs, we set  X  =0 . 27 and  X  =10 and obtained 1,024 graphs with an average number of vertices of 3k and average fanout 7.2. We use R and S to denote experiments with random and scale-free graphs, respectively. Reachability Query Time Collections: We ran the implementationsof Interval [19], Grail [18], 2-hop [2] and Prime labeling [11] on our graph collections. We ran 1,000 random reach ability queries on each graph. The r untimes of these 1,000 queries on each index were then stored and fitte into Gamma distributions. The runtimes were obtained from warm runs. The estimated  X  and  X  of Gamma distributions were stored for determining the optimal index. The label of a graph is determined by the y value.
We observe that there were cases where t he prediction problem was trivial, e.g. , one index was almost always more efficie t than the others. These cases were omitted as our prediction model was very accurate. In other words, neither of the indexes chosen in our experiments dominated another.
 Default Parameter Settings: We conducted a set of experiments to show the effect of each parameter in our technique. Unless speci fie otherwise, we used the default set-tings shown in Table 1. We used the VP rad utility function for the vertex permutation in the sim computation by default. For ease of e xposition, we predict the optimal index from two graph indexes. That is, the prediction label of the experiments was binary. Performance Metric: Unless otherwise specified we ran each experiment 100 times and report here the average accuracies. 7.2 Experiments on Distribution Fittings To verify that the distributions of the reachability query times on each index can be fitte to some well-known distributions, we tested their fitti g functions. We generated the runtime distributions of all of our random and scale-free graphs and all of our index implementations. We used y %=98% in our experiments. In Table 2, we show the L 2 -norm between the actual and estimated distribution of Poisson, normal and Gamma distributions. We note that the Gamma distributions almost always clearly offered more accurate fitti gs and the fitti g errors were often small. Therefore, in this work, we have adopted the Gamma distribution. 7.3 Prediction Accuracies In this experiment,we tuned the parametersof ourprediction model and studied their ef-fects on prediction accuracies. Due to sp ace limitations, we present only the results ob-tained from some pairs of indexes for each dataset: 2-hop vs Grail(1) and Interval vs
Prime for random graphs, and 2-hop vs Grail(3) and 2-hop vs Grail(5) for scale-free graphs.
 Effects of the Size of Training Dataset. We studied the effect of the dataset size on prediction accuracy for random graphs, as shown in Table 3. kMeans was set to no in this experiment. From Table 3, we observe that our prediction accuracies were almost always above 80% on training sets of different sizes. We also note that the prediction accuracy fir t increased and the n slightly declined as the tra ining dataset grew larger. This was possibly because our model was overfitte by large training sets. Moreover compared our method with a previous work [6]. While our method is either comparable or slightly less accurate, it is less ad-hoc than [6]. As discussed, spectrums have known to be useful in many real applications. However, in [6], the features were chosen simply because they are verifie useful by experiments. Effects of k kmeans . We studied the effects of k kmeans on prediction accuracy as shown in Table 4. It can be seen that accuracy increased with the growth in k kmeans for random graphs. This is because the clusters become more ref ned with larger k kmeans . Thus, we had a higher probability of selecting similar cluster centers for prediction. However, accuracy may reduce slightly if each cluster is too fine as shown in the results for scale-free graphs.
 Effects of k knn . Table 5 presents the effects of k knn on prediction accuracy. It can be seen that accuracy increases with the growth of k knn . This is because that a large k results in more votings, which reduces the effects of outliers. Our accuracy was over 80% on both random and scale-free graphs when k  X  7 . The prediction accuracy was stable when k  X  9 .
 Effects of the Number of Tail-k Eigens. We used different number of eignvalues and eigenvectors in prediction and studied th e effect on accuracy, as shown in Table 6. It can be seen that accuracy increases with more eignvalues and eignvectors, while the prediction time increased roughly linearly. We exclude the time taken to determine the spectrum of a graph as this mainly depends on the algorithm used. From our data, the MATLAB function ran from 2.6s to 173s with an average of 39s. However, this is often still more efficien than choosing the optimal index by constructing all candidate indexes and running a large number of benchmark queries.
 Effects of Vertex Permutation and  X  . In this experiment, we studied the effects of vertex permutation and  X  on prediction accuracy. Table. 7 presents the results. It may be observed that while VP coor , VP W rad and VP none could all sometimes be accurate, they were sensitive to the choice of  X  , in the similarity function. In compar-ison, VP rad is both robust and accurate. Moreover, when  X  increased, the relative importance of eigenvectors re duced, and accuracy decreased. To the best of our knowledge, there have been only few preliminary studies that use graph features to predict the relative query performances of graph indexes. Deng et al. [6]. extract features from data graphs and use neural networks for prediction. How-ever, there have been many features in graph theories and it is unclear which of these are the principal ones. In contrast, we use the tail-k eigenvalues and eigenvectors for prediction. Moreover, the optimal index of [6] is def ned by the best average runtimes. In comparison, we also allow users to fi e-tune their notion of the optimal index. An-other work by Zhu et al. [19] applies multiple graph indexes to partitioned subgraphs of a data graph. An analytical cost model is proposed and illustrated with 2-hop and Interval . Our approach has been applied to various indexes. Spectral methods have been applied to produce k partitions of graphs e.g. , [10]. Our aim is not to produce exactly k partitions but to predict to the optimal index.

Finally, there is a large body of work on determining graph features, e.g. , [16], for query processing, e.g. , [17,4]. Due to space limitations, we cannot include a detailed survey on this area. However, in these studies, features are graphs (structures). It re-mains unclear how to exploit them to build a predictive model. In this paper, we propose a spectral decomposition method for predicting the optimal graph index of a given graph. Specif cally, we propose a uniform representation of a graph, a spectral similarity function and a prediction algorithm. We obtain the imple-mentation of four structurally different graph indexes. One observation is that the run-time distributions of the indexes f t accurately into a Gamma distribution. This allows us to ref ne the notion of the optimal index, using the inverse cumulative distribution function. We report on detailed experiments on the parameters in our techniques on both random graphs and scale-free graphs. W e note that our technique is robust and can achieve approximately 70% accuracy or higher. In future work, we will investigate methods for other subgraph queries.
 Acknowledgment. This work is partly supported by FRG2/10-11/106, GRF HKBU210510 and GRF 211512.

