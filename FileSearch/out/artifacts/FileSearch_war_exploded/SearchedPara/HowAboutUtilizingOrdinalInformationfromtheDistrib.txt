 Problems of ordinal regression arise in many fields such as infor-mation retrieval, data mining and knowledge management. In this paper, we consider ordinal regression in a semi-supervised sce-nario, i.e., we try to utilize the ordinal information from the dis-tribution of unlabeled data. Semi-supervised ordinal regression is more applicable than traditional supervised ordinal regression, be-cause nowadays labeled data is expensive and time-consuming as it needs human labor, whereas a large amount of unlabeled data are far accessible with the development of internet technology. As a re-sult, we first construct a general semi-supervised ordinal regression framework to formulate this problem. Based on the framework, we then propose a semi-supervised ordinal regression method called Semi-supervised Ordinal SVM (SOSVM). Additionally, in order to make our proposed method more applicable to problems with large scaled labeled data, we put forward a kernel based dual co-ordinate descent algorithm to e ffi ciently solve SOSVM. Both rig-orous theoretical analysis and promising experimental evaluations on real world datasets show the great performance and remarkable e ffi ciency of SOSVM.
 H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Infor-mation Search and Retrieval; I.2 [ ARTIFICIAL INTELLIGENCE ]: Learning Theory, Algorithms, Experimentation Ordinal Regression, Ordinal Semi-supervised SVM, Semi-supervised Learning, Dual Coordinate Descent Algorithm
Ordinal regression is the learning problem of predicting vari-ables of ordinal scale which may be viewed as a problem bridging between metric regression and classification in which the training samples are labeled by ranks or ordinal scales. Ordinal regression problems arise in many fields such as information retrieval, visual recognition, collaborative filtering, econometric models and clas-sical statistics. For example, for relative feedback in information retrieval, given a query, the system returns a bundle of retrieval re-sults, the user than rank a small fraction. with the feedback ranks by the user, the system can learn the user X  X  specific preferences and reassigns ranks to the remaining retrieval results. Another appli-cation in information retrieval is to predict a person X  X  rating on a movie given the person X  X  past ratings on similar movies.
There are several classical ordinal regression methods in litera-ture [6, 7, 8, 20, 5, 12]. Crammer and Singer [6] set thresholds for an online perceptron mapping an input vector to a potential value to obtain a rank. Herbrich et al [7, 8] present a distribution indepen-dent model for ordinal regression by enforcing large margin rank boundaries similar to support vector machines (SVM) [21], which is based on a loss function that acts on pairs of ranks. Shashua and Levin [20] generalized the support vector formulation for ordi-nal regression based on the large margin principle. However, their approach cannot guarantee the ordinal inequalities on the thresh-olds. Wei Chu and S. Keerthi [5] then propose two new support vector approaches which guarantee that the thresholds are properly ordered at the optimal solution. Ling Li and Hsuan-Ten Lin [12] present a reduction framework form ordinal regression to binary classification by extracting extended examples from the original examples, learning a binary classifier based on the extended exam-ples and constructing a ranking rule. Their framework provides a unified view for many existing ordinal regression algorithms. Tra-ditional ordinal regression methods are exposed to several draw-backs. First, they do not define the metric distances between labels or ranks and predict only a coarse interval of labels for a new sam-ple as their decision function outputs are of no physical sense. Sec-ond, traditional ordinal regression methods can be applied only in supervised settings. In many real applications, we often encounter the settings where labeled data is time-consuming and expensive to obtain while large amount of unlabeled data is more accessi-ble. Traditional ordinal regression methods cannot get su ffi ciently trained by few labeled information, which limits their applications in semi-supervised scenario.

Another relevant work is kernel regression with order prefer-ences [23], which adds a regularization term encoding order pref-erences to the standard risk minimization framework for kernel re-gression. With appropriate heuristic or expert knowledge about the order preferences of unlabeled data, the algorithm performs better than the traditional kernel regression. However, the expert knowl-edge about the order preferences of unlabeled data in many real problems is hard to obtain and the heuristic knowledge cannot guar-antee a true representation for the order preferences.

In this paper, we research on semi-supervised ordinal regression which considers ordinal regression problems in a semi-supervised setting. In many cases, labeled information are very expensive to collect as it requires manual labeling. Meanwhile, abundant col-lections of unlabeled data are easy to get. Semi-supervised learn-ing [3] aims to train better learners in this scenario and has been successfully applied to many real applications. In literature, some graph based methods [14][17] and transductive methods [21][10] have been proposed, which show the e ff ectiveness of using unla-beled data. We assume that ordinal scaled data are orderly dis-tributed in feature space, thus unlabeled data can help us discover the ordinal information. We believe that the geometrical distribu-tion of unlabeled ordinal data provides useful ordinal information, and with the help of large amount of unlabeled ordinal data, we can learn a better ordinal SVM. Note that the unlabeled information is much weaker than the pre-known expert knowledge of order prefer-ence [23]. Another motivation of our work is that we want to learn fine-grained prediction of ordinal scales, other than the coarse mea-surement in traditional ordinal regression methods. In daily life, we frequently need the computer to give more precise predictions. For example, for face direction recognition, not only need we know the interval of the predicted face direction but also are we interested on the precise direction. In sentiment analysis, given the movie reviews of the same ratings, we care more about the more refined ratings.

We introduce a Euclidean metric to the ordinal scale space and build a simple linear model for the continuous ordinal scales. The linear model can be took as a ruler to measure the ordinal scales of samples. Additionally, we assume that each unlabeled sample has its latent ordinal label which we view as a weighted sum of all the scales. We then propose a general semi-supervised ordinal regres-sion regularization framework which minimizes the combination of structural risk, the empirical ordinal loss for the labeled data, the empirical evaluation loss on both labeled and unlabeled ordinal data and the smooth loss on the data graph for our linear label-predicting model. Based on the proposed framework, we derive the first graph-based semi-supervised ordinal regression method which we call Semi-supervised Ordinal SVM or SOSVM. Furthermore, we put forward a kernel dual coordinate descent algorithm to e ffi -ciently solve SOSVM. This makes SOSVM more applicable to real problems with large labeled samples.

The main contributions of this paper include:
The paper is organized as follows. Section 2 describes the nota-tions in this paper. Section 3 gives brief reviews of related works. In section 4, we propose a general semi-supervised ordinal regression regularization framework and put forward SOSVM based on this framework. In section 5, we bring forward a kernel dual coordinate descent algorithm to e ffi ciently solve SOSVM. Section 7 provides some experimental results. Conclusions are made in section 8.
To define the notations in this paper, we assume that there are l labeled samples denoted by X L = { ( x i , y i ) } l i = 1 multi-class classification, y i  X  X  1 , 2 ,..., K } , where K is the number of classes. We use K ( x i , x j ) to denote the kernel function defined X to the high dimensional feature space. We use  X  ( X ) to denote the mapped data matrix as  X  ( X ) = [  X  ( x 1 ) , X  ( x 2 ) ,..., X  ( x the semi-supervised situation, graph Laplacian is widely used to describe the data structure. We define G = ( V , E ) as the graph as-sociated with the samples. V is the vertex set of graph, which is defined on the training set. E is the edge set containing the pairs of neighboring vertices ( x i , x j ). There are three typical methods to calculate the weight matrix S : (1) Gaussian kernel of width  X  : (2) K-nearest neighbors: (3) Linear approximation of neighborhoods [22]: where N ( x ) denotes the set comprising K nearest neighbors of x and G i jk = x i  X  x j T ( x i  X  x k ) . Let L be the graph Laplacian given by L = D  X  S where S i j is the edge weight in the data adjacency matrix defined in (1) or (2) and D is the diagonal matrix given by D ii =  X  f is a prediction vector given by f = [ f ( x 1 ) ,..., f ( x label indicator matrix which is defined by J i j = 1 only if x j th labeled data in X L , J i j = 0 otherwise.
Laplacian SVM (LapSVM) [14] is a classical method which is based on a form of regularization that exploits the geometry struc-ture of data. The manifold regularization enables LapSVM to uti-lize unlabeled data e ff ectively.

LapSVM X  X  formulation is as follows: From LapSVM X  X  formulation, we can make several observations: First, it is only a binary classification approach. Although binary LapSVM can deal with multi-class categorization using one ver-sus rest or one versus one technique, it is rather expensive espe-cially with a large number of classes. Second, when dealing with multi-scale problems, LapSVM doesn X  X  consider the ordinal rela-tions among scales. Third, LapSVM tries to find a tradeo ff be-tween the maximal margin of labeled data and the smoothness of distribution of labeled and unlabeled data. This makes LapSVM susceptible to labeling.
SVORIM [5] considers all the errors associated with all K  X  1 thresholds whose primal problem is as follows: where j runs over 1 ,..., K  X  1. In the j th category, n j is the number of training samples and x j i indicates the i th training sample. The predictive ordinal decision function is given by
SVORIM guarantees that there exist choices for the b j s that obey the natural ordering when the optimal b j s are non-unique (If the b are regularized as well in the primal problem, b j s satisfy the natural ordering). However, it is exposed to several drawbacks. First of all, it is a supervised method and it cannot utilize the information of unlabeled data. Thus it is hard to train the ordinal decision function su ffi ciently given large abundant data with few labels. Secondly, it does not define the metric between labels, so it can only give a coarse prediction and its outputs are of no physical sense. Finally, it only considers the local ordinal relations. This might lead to the ignorance of the global ordinal information especially when there exists some metric distance in the scale space.
We propose the general semi-supervised ordinal regression reg-ularization framework as follows: Below we explain the motivation behind each item in Eq(4). k f k 2 K is norm of ordinal scale function in the reproducing kernel Hilbert space (RKHS) [2], it represents the complexity of a solu-tion. By restricting the norm, we can avoid too much structural risk of possible solutions.

V Eval ( f , Y R ) is some loss function based on the Euclidean metric in ordinal scale space, where Y R is the latent ordinal scale vector for all data. This term measures the di ff erence between the evaluation of the ordinal scale function and the latent label and can be viewed as the empirical evaluation risk. Minimizing V Eval ( f , Y sure that the learned ordinal function can evaluate the latent ordinal scales of data as true as possible. One simple way to obtain the unlabeled ordinal data x i  X  X  latent ordinal scale y the weighted sum of all the ordinal scales, i.e. y j = where P i j is the probability that x i belongs to the j th scale r labeled data x , its latent ordinal scale is its given label. Sometimes, especially when the probability is hard to reach or it is not accurate enough, we can consider the error between the evaluation of the ordinal scale function and the latent label on labeled data. Note that the function gives continuous evaluations of ordinal scales, whereas traditional ordinal regression methods only predict hard scales. For instance, for movie reviews, star 2.0 is higher than star 1.0 and star 5.2 is closer to star 5.0 than it is to star 6.0.

V Order ( f ) is some loss function to measure the degree that eval-uation of ordinal scale function f violates the given rank relations on the labeled samples. In an another way, minimizing V Order aims to preserve the ordinal relations of labeled samples. k f k 2 I is a smooth regularization term that reflects the geometric structure of data, while k f k 2 S is a scale smooth regularization term to make the ordinal scale function f vary smoothly according to the scale similarities of samples.  X  is a weight factor to tune the proportion of k f k 2 S in the overall smooth regularization. The under-lying principle of introducing k f k 2 I and k f k 2 S is that the closer two samples are, the smaller the di ff erence between their scales should be according to the semi-supervised smoothness assumption [3]. When the ordinal data satisfies the semi-supervised smoothness as-sumption, the framework can observe the ordinal information from the geometrical distribution of unlabeled data by minimizing k f k 2 and k f k 2 S , which makes the ordinal scale function f vary smoothly according to the spatial distribution of data. When labeled informa-tion is insu ffi cient while large amount of unlabeled data is available, traditional supervised ordinal regression methods cannot utilize the unlabeled information from the abundant unlabeled data, whereas semi-supervised ordinal regression methods can su ffi ciently utilize the ordinal information from both labeled and unlabeled samples.
Now we give and prove a new version of the Representor Theo-rem which shows that the minimizer of the framework Eq (4) has an expansion in terms of both labeled and unlabeled examples: As in [14], we denote the linear space of the linear span of kernel func-tions centered at points of the manifold M by S which is the closure with respect to the RKHS norm of H K :
L  X  X  X  X  X  4.1 ([14]). The following properties of S hold: 1. S with the inner product induced by H K is a Hilbert space. 2. S M = ( H K ) M . 3. The orthogonal complement S  X  to S in H K consists of all We give a lemma to show that if the optimal decision functions of Eq (4): f  X   X  X  K exist, then f  X   X  X  .

L  X  X  X  X  X  4.2. If k f k 2 I and k f k 2 S are approximated on the basis of labeled and unlabeled data, and we assume that the optimizer of
P  X  X  X  X  X  . Any f  X  H K can be written as f = f S + f  X  S f is the projection of f to S and f  X  S is its orthogonal comple-f  X   X  X  .
T  X  X  X  X  X  X  X  X  4.3. The optimal solution to the minimization prob-lem in Eq (4) exists in the Reproducing Kernel Hilbert Space (RKHS) and if the term k f k 2 I and k f k 2 S are approximated on the basis of la-beled and unlabeled data, the optimizer admits a representation in terms of an expansion of kernel functions over both labeled and unlabeled data points: P  X  X  X  X  X  . Let Now we prove that the solution to Eq (4) exists and belongs to S by Lemma 4.2. According to Corollary 10 and standard results about compact embedding of Sobolev spaces [19] that a ball B H K , B r = { f  X  S , s . t . k f k K  X  r } is compact in L  X  minimizer in that ball f  X  must exist and belong to B r for any such a ball.
 Meanwhile, by substituting the zero function If the sum of the evaluation loss and the order loss is zero, then zero function is a solution, otherwise Thus, f  X   X  X  r where Therefore, beyond a certain point, we cannot decrease H ( f  X  ). That means that f  X  = f r as above. If V Eval and V Order are convex, the solution is unique.

We then prove the representer theorem for the general frame-work in Eq (4). As in [14], any function f  X  H K can be uniquely decomposed into a component f k in the linear subspace spanned by the kernel functions {K ( x i ,  X  ) } l + u i = 1 and a component f it. Thus, As the following shows, the evaluation of f on any data point x , 1  X  j  X  l + u is independent of f  X  : Therefore, the empirical terms including the labeled loss, the un-labeled loss and the intrinsic norm in Eq (4) depend only on the evaluation of f k on the given data points. Additionally, since the minimizer of problem Eq (4) must have f  X  = 0 and admit a representation in terms of an expansion of kernel function over both the labeled and the unlabeled points: Theorem 4.3 shows that a simple linear model f ( x ) =  X  w , x  X  + b can be learned by optimizing the general regularization framework in Eq (4). With kernel trick [2], we can obtain a nonlinear model f ( x ) =  X  w , X  ( x )  X  + b , which is a linear model of  X  ( x ).
The decision function usually has a bias term b , which is typi-cally considered by appending each sample with an additional con-stant dimension: In this paper, we use the general nonlinear mapping  X  .

We introduce the concept of scale similarity based upon the as-sumption of Euclidean metric in the ordinal scale space to denote the similarity of two samples according to their scales. Definition Scale Similarity. We use S S i j to denote the scale similar-ity of x i and x j which is defined by where B S is between-scale matrix and B S kk 0 represents the similarity P ik is the probability of x i belonging to the k -th class. K is the total number of scales. S S i j is large only when x i , x j most likely belong to k th scale and k 0 th scale respectively and r k = r 0
The computation of the class probability matrix P , which is also an important problem itself, is out of the domain of this paper. We just use the method proposed in [18] to calculate P based on label propagation in the sense of probability.

One question is that if there is a need to use probabilities. The answer depends on the way we utilize the unlabeled data. Since we don X  X  know the true scaled label for unlabeled data, one sim-ple way is to assume that the unlabeled sample x i belongs to the k th scale in probability P ik . In this way, when calculating the scale similarity S S i j , we consider the global information about the ordinal relation between two samples x i and x j according to all their pos-sible scales. Besides, the method to calculate P in [18] is based on probabilistic label propagation from the labeled data to the unla-beled data in high-density region, which e ff ectively utilizes the dis-tribution information of unlabeled data. Of course, if there is other way to obtain the latent ordinal scales for unlabeled data (e.g. using some transductive learning methods), we don X  X  need the probability matrix P .

Corresponding to scale similarity, when constructing graphs, we call the traditional similarity of two samples item similarity S which depends only upon their feature representations. Item sim-ilarity can be obtained by any method in Eq (1, 2 or 3) depend-ing upon the property of specific task. In our work, the meaning of neighbor is a little di ff erent from that in multi-class problems. Two samples are still neighbors if they belong to the same or near scales, even if their data representations di ff er greatly (the distance between two samples from the same scale is quite large sometime). In framework 4, we adopt square loss for V Eval : or where r k is the predetermined value of k th scale, and R is the vector taking r k as its k th element.

V Eval could be a loss term either on both labeled and unlabeled data (as in Eq (10)) or on only labeled data (as in Eq (11)) depend-ing upon the property of data. V Eval can be viewed as the risk that the evaluations of ordinal scale function deviate from true scales. Minimizing V Eval means that f i should be close to r k for larger P which also means that the evaluation of ordinal scale for x be near to r k as x i belongs to the k th scale in a larger probability P V
Eval also measures the distance or error between the latent ordinal scale vector PR and the predicting scale vector f .
 For V Order , we use hinge loss: where O ( p ) is the mapping from order to label index, e.g., the scale of x O ( p ) takes the p th place among all the l labeled data. Let  X  p = max 0 , 1  X  f x O ( p ) + f x O ( p  X  1 ) , which is equivalent preserve the ordinal relations of labeled data.

For k f k 2 I , we use the graph Laplacian to reflect the geometric structure of samples: k f k 2 I = Accordingly, for the scale smooth regularizer, k f k 2 S = trices in terms of item and scale similarities respectively. Consider-ing the weight factor  X  , the combination of can be represented by a single term: where L = ( 1  X   X  ) L I +  X  L S is the overall Laplacian.
According to Theorem (6) and we have and Finally, we obtain the formulation of semi-supervised ordinal SVM
The problem in Eq (13) is a linear inequality constrained quadratic convex optimization problem. Using the standard Lagrange multi-plier technique, we introduce a set of dual variables where each variable corresponds to a linear constraint, i.e. we introduce  X  for  X  p  X  0 and  X  p  X  0 for Thus, we write the Lagrange function of Eq (13) as follows: The optimal solution requires: where That yields Defining expansion coe ffi cient vector  X  : we can express w as w =  X  ( X )  X  and we obtain the dual problem of Eq (13): 1 When V Eval takes the form of Eq (11), the derivation is similar. where the semi-definite positive matrix  X  Q and constant vector b are Once the dual variable  X  is solved, the expansion coe ffi cient vector  X  can be calculated and the scale function f ( x ) can be represented by the kernel functions { g i ( x ) = K ( x i , x ) } l + u The soft scale of a new sample x can be obtained by Eq (18). Note that based on the metric definition in scale space and the restric-tion on scale loss term Eq (10), the output of the scale function f ( x ) has the intuitive physical sense: the refined scale in Euclidean metric space, whereas traditional ordinal regression methods only predict hard ordinal scales. Because of this mechanism, SOSVM can give more fine-grained scale predictions than hard scales. The fine-grained scales are more flexible than hard scales for many real problems where one sample might be embedded somewhere be-tween two adjacent hard scales. The continuity of scale predictions of SOSVM in the assumed Euclidean metric space intuitively ex-presses some scale ordinal relations. Even if SOSVM makes an erroneous prediction, the predicted scale may be not far away from the true scale.

Note that we can also obtain the hard scales, which is calculated by Eq (19): where m k is the mean of soft scales of the labeled samples belong-ing to scale k :
As for the number of unknown variables, SVORIM have l ( K  X  1) unknown variables and LapSVM has lK variables, while SOSVM has only l unknown variables. That means solving the dual problem of SOSVM is faster than the two baselines. In section 5, we propose an algorithm to solve it more e ffi ciently.
The coordinate descent algorithm is a broadly-used optimization approach in machine learning [13][1][4][9][11][17]. From the dual problem Eq (16), we can see that the objective g (  X  ) is twice di ff er-entiable, and each variable has a simple bound constraint 0  X   X   X  . Inspired by the coordinate descent technique, we can update one variable at a time by minimizing a single-variable sub-problem. Fortunately, since the objective function is twice di ff erentiable with simple bound constraint, each sub-problem has an analytical opti-mum. Thus, we can solve the quadratic convex problem e ffi ciently by quickly updating each component of  X  in each sub-problem.
For each variable  X  p , we solve the following sub-problem: ( I +  X  1 K +  X  3 LK )  X  1 with I +  X  1 J J T K +  X  3 LK  X  1 Y , where Y l is a l -element vector which takes its i th element as the scale of the i th labeled data. where e p has unit-value only in its p th component with other ones set to zero. The objective in this sub-problem is a quadratic func-tion of variable d given by: where the gradient  X  g (  X  ) and the Hessian matrix H (  X  ) can be sim-ply expressed by: where  X  Q pp is the p th diagonal element of  X  Q which is defined by Eq (17) and T p is the p th column of matrix T .

Define the projected gradient  X   X  p g (  X  ) by: Noting the bound constraint in Eq (16), one can see that  X  be updated when  X   X  p g (  X  ) = 0. Otherwise, one can calculate the optimal analytical solution in Eq (21):
We need to store the diagonal elements of  X  Q and compute  X  For evaluation of the gradient, we have to maintain and update  X  . According to Eq (15), let B = ( I +  X  1 K +  X  3 LK )  X  =  X  0 + B  X  , thus we can update  X  by Eq (25): where B p is the p th column of the matrix B .

T  X  X  X  X  X  X  X  X  5.1.  X  generated by Algorithm 1 globally converges to an optimal solution  X   X  . The convergence rate is at least linear: there are 0 &lt;  X  &lt; 1 and an iteration k 0 such that
P  X  X  X  X  X  . We consider the analysis in [13] which studies the coor-dinate descent method for problem in the following form: where E is a constant matrix and L i  X  [  X  X  X  ,  X  ) , U are lower and upper bounds, respectively. They prove the linear convergence of the coordinate descent method if Eq (27) satisfies the following conditions: 1. E has no zero column. 2. The set of optimal solutions for Eq (27) is nonempty. 3. The function g is strictly convex and twice di ff erentiable. We will next explain that the multi-scale ordinal semi-supervised SVM satisfies all the above conditions.
 First, let E be a Cd  X   X  ( l  X  1) matrix defined by Algorithm 1 A kernel dual coordinate descent method for semi-supervised ordinal SVM Require:  X  Q = T T J T K ( I +  X  1 K +  X  3 LK )  X  1 JT , B = ( I +  X  1 K +  X  3 LK )  X  1 JT
Start with  X  = 0 ,  X  = 0 while 1 do end while where d  X  is the dimensionality of the mapped feature space which is dependent upon the nonlinear mapping  X  and E p is the p th column of E which can be represented by: Then the objective in Eq (16) can be denoted by g ( E  X  ) + b T  X  , here g ( x ) = 1 2 x T x . E p is a zero vector when and only when  X  x elements not in this set can be eliminated from consideration, so we can still have a matrix E without a zero column. Since if i &lt; A or j &lt; A , we can minimize the following problem: However, when p &lt; A ,  X  p =  X  2 is optimal and At the first iteration of Algorithm 1,  X  p is set to  X  2 and remains the same forever. Therefore, we can minimize the function of  X  which the matrix E has no zero column:
Second, 0  X   X  p  X   X  2 is a compact set. According to the Weier-strass X  X  Theorem, any continuous function on a compact set attains its minimum. Therefore, the optimal solution set is nonempty.
Third, the function g is a convex quadratic function, thus the third condition is satisfied.

Finally, according to Theorem 2.1 of [13], we finish the proof. Figure 1: 4-Gauss data. Best decision surfaces of LapSVM, SVORIM and SOSVM. The labeled data is shown with red dia-monds, blue circles, magenta squares and green triangles, each color and shape corresponding to a scale. The unlabeled data is shown with black dots, linear kernel is used.

Theorem 5.1 means that we can obtain an -accurate solution  X  in O ( log (1 / )) iterations as long as g (  X  )  X  g (  X   X  ) + .
The training of SOSVM incorporates three steps. The first step is to calculate the probability matrix P , the second step is to com-pute the constant matrix B and  X  Q and the last step is running our proposed kernel dual coordinate descent algorithm to solve the dual problem of SOSVM. The first step costs O ( n ( K + 1)) and requires memory of O ( n 2 ) [18]. As for the second step, rather than comput-ing the inverse matrix ( I +  X  1 K +  X  3 LK )  X  1 directly, we solve the system of l linear equations to get B and  X  Q . More specifically, in ( I +  X  1 K +  X  3 LK )  X  1 JT is actually the solution to the system of lin-ear equations ( I +  X  1 K +  X  3 LK ) X = JT . Thus, the computation of input matrix B and  X  Q takes O ( n 2 l ) and requires a memory of O ( n 2 ). For the third step, calculating  X  p g (  X  ) takes O ( n ) operations, up-dating  X  needs O ( n ) operations. Thus the algorithm costs O (  X  ln ) under nonlinear kernel or O (  X  l  X  m ) using the linear kernel where  X  m is the number of nonzero elements of the training sample and  X  is the number of outer iterations (updating  X  one time). Therefore, SOSVM needs a overall computation complexity of O ( n 2 l +  X  ln ) and requires total memory of O ( n 2 ).
In this section, we do experiments on one toy dataset and three real world datasets to evaluate the performance of our proposed SOSVM against two baselines: LapSVM (one vs. rest scheme for multi-class categorization) and SVORIM. To the best of our knowledge, there are no graph-based semi-supervised ordinal re-gression methods published. We compare SOSVM with LapSVM to show that classical semi-supervised classification methods can-not perform very well for ordinal data even if they are able to utilize unlabeled data for training. We compare SOSVM and SVORIM to demonstrate that the distribution of unlabeled data contains some useful ordinal information which can help us improve the perfor-mance of ordinal regression methods. What X  X  should be noticed is that we don X  X  compare our method to ranking methods since ordinal regression di ff ers from ranking. Ordinal regression can be given a new sample to classify its scale (it can do the inductive reference), whereas ranking aims to order a fixed set of items relative to one another. In the end of experiment section, in order to validate the e ffi ciency of our dual coordinate descent algorithm, we compare the speed of our algorithm (implemented in Matlab for fair com-parison) with that of the Matlab quadprog function with varying amounts of labeled data on the Coil20 dataset. Note that we don X  X  compare our kernel dual coordinate algorithm to some known QP solvers such as Mosek or CPLEX for consideration of fair expen-diture and fair comparison. CPLEX is pretty expensive and Mosek is implemented in C ++ with a Matlab interface.

As to the evaluation, when testing semi-supervised methods, as in [14], all the parameters are the best settings in a grid search with respect to the mean error rates on the unlabeled training set with 10 random labeling conditions. The grid search ranges for all the related parameters are listed in Table 1. We construct the graph with 6-nearest neighbors using Eq (1) with parameter  X  = 1. For V Eval , we use Eq (11) for scale data v1.0, and Eq (10) for toy data, Umist and Coil 20 datasets and we set r k = k for the k th scale.  X  and  X  u is the parameters to calculate probability matrix P (we set  X  = 0 to fix all the labeled samples to their corresponding scales. By setting 0  X   X  u &lt; 1, we enable all the unlabeled samples to walk to a novel scale. The original P has K + 1 columns, and we simply eliminate the last column (corresponding to the probability of novel class) and re-normalize the remaining K columns). As to the speed evaluation, we use a desktop computer with a 3.00GHz Intel(R) Pentium (R) 4 CPU and 2.00GB memory.
We present a toy problem to show the e ff ectiveness of SOSVM in Figure 1. The toy data comprises four groups of data subject to Gaussian distribution. The four groups are distributed in a taxo-nomic order on the manifold according to their scales. Only one sample of each group is labeled in di ff erent colors and shapes, with other unlabeled data shown in black dots. From the best decision surfaces in Figure 1, we can see that 1. LapSVM does not perform very well on semi-supervised ordinal regression, as it neither con-strains the ordinal relations among labeled data nor utilizes the la-tent ordinal information from the unlabeled data, which testifies to the point that classical semi-supervised classification methods are not suitable for tackling semi-supervised ordinal regression prob-lems. 2. Even if constraining the ordinal relations among labeled data, SVORIM performs badly on unlabeled data and its best de-cision surface does not globally match the distribution of ordinal data. Compared with SOSVM, SVORIM does not use the ordinal information hinted by the unlabeled ordinal data. 3. SOSVM can extract the ordinal information from the geometrical distribution of unlabeled data and can learn a better ordinal function by using the ordinal information provided by unlabeled data. For SOSVM, not only the ordinal relations among labeled data but also those among unlabeled data are preserved.
Coil20 dataset [15] contains 1440 size-normalized grayscale im-ages of 20 objects. Each object takes 72 images, one at every 5 degrees of rotation.

In our experimental configuration, we resize each image to 32  X  32, normalize the gray scale to the range of 0 to 1 and vectorize each image as its feature vector. Without loss of generality, we evaluate SOSVM on the first object. The samples are displayed in Figure 2: Coil20 Examples (1-st object). We divide the 72 im-ages into 6 scales according to their degrees of rotation. The first row corresponds to scale 1, the second row scale 2, and so on.
 Table 2: The Average Error Rate ( % ) (std % ) on Coil20 1 st Ob-ject.
 Figure 2. Each row corresponds to a scale. Each scale contains 12 images with consecutive degrees of rotations. For each scale, we randomly select 80% of the images to build the training set with 2 of them randomly labeled, and the remaining 20% of the images to build the test set. 10 independent experiments are then performed. Here we use RBF kernel with kernel parameter  X  Kernel = 10. For LapSVM, we set  X  1 = 100 , X  2 = 1. For SVORIM, we set C = 1. For SOSVM, we set  X  l = 0 , X  u = 0 . 03 , X  1 = 10000 , X  2 1000 , X  = 0 . 6.

From Table 2, we can make the following conclusions: 1. SVORIM presents a even worse performance than LapSVM as it cannot get su ffi ciently trained with only a few labeled data without utilizing the latent ordinal information of unlabeled data. 2. Although tak-ing into account of distribution information of unlabeled data in some sense, LapSVM does not explicitly extract the underlying or-dinal information from the geometrical distribution of unlabeled data. Neither does LapSVM consider the ordinal relations among labeled data. 3. For SOSVM, not only the labeled ordinal rela-tions are constrained, but also the underlying ordinal information from the unlabeled data are utilized. All in all, the experiment re-sults strongly validates the e ff ectiveness of SOSVM for orderly dis-tributed data in semi-supervised scenario. analysis which was introduced by Bo Pang and Lillian Lee [16]. The collections of movie-review documents labeled with respect to their overall sentiment subjective rating (e.g., "two and a half 3 The data is available at http: // www.cs.cornell.edu / People / pabo / movie-review-data / as scale dataset v1.0.
 Table 3: Distribution of scales for the four reviewers in scale dataset v1.0.
 Table 4: The parameter settings on scale dataset v1.0. RBF ker-nel with  X  Kernel = 10 is used. The other parameters of SOSVM for four authors are the same:  X  3 = 0 . 01 , X  l = 0 , X  u 1 .
 Table 5: The Average Error Rate ( % ) (std % ) on scale data v1.0 four subsets.
 stars") and the sentences are labeled with respect to their subjec-tivity (subjective or objective). The dataset contains movie reviews written by four authors. We consider the sentiment categorization with four scales (a special case of a multi-scale problem with four scales). Each review is labeled as some scale according to its rat-ing. For each author, we choose the first 80% of comments to build the training set and the remaining 20% of comments as the test set. The distributions of training and test sets are listed in table 3. Ten independent experiments are performed in each dataset of which 80 reviews of each scale are randomly labeled. We use the normal-ized TFIDF [10] as the feature for each comment. We construct the words list (containing 46515 words) using all the unigrams ex-tracted from the whole dataset except commas, periods, and semi-colons. The parameter settings for SOSVM and the two baselines are listed in Table 4. The mean error rates on the unlabeled train-ing set and the test set are listed in Table 5. From the experimental results, we can make the following conclusions: 1. Sentiment clas-sification is a di ffi cult problem. Sentiment classifications cannot be simply tackled as document categorizations. Choosing suitable features with more discriminative sentiment semantic information is of importance for sentiment categorization. 2. Based on TFIDF, the proposed SOSVM exceeds all baselines in terms of accuracy. each covering a range of poses from side profile to frontal views. The number of instances of each person ranges from 19 to 48. We resize each image to 28  X  23, normalize the gray scale to the in-terval between 0 and 1 and vectorize it as feature vector. We deal with the multi-scale problem on the 19th person. The 19th per-son has 48 images (displayed in Figure 3). Each row corresponds to a scale, each of which contains 8 images with consecutive face 4 http: // www.cs.toronto.edu /  X roweis / data.html Table 6: The Average Error Rate ( % ) (std % ) on Umist 19 th Person.
 Figure 3: Umist examples ( 19 th person). We divide the 48 im-ages into 6 scales according to their face poses. The first row corresponds to scale 1, the second row scale 2, so on and so forth. poses. For each scale, we randomly select 6 images to build the training set with only one of them randomly labeled, and we use the remaining 2 images to build the test set. 10 independent ex-periments are performed using linear kernel. For LapSVM, we set  X  1 = 10 , X  2 = 1. For SVORIM, we set C = 0 . 1. For SOSVM, we set  X  l = 0 , X  u = 0 . 5 , X  1 = 10 , X  2 = 100 , X  3 = 1 , X  = 0 . 9. Ta-ble 6 shows that SOSVM clearly outperforms all baselines. The experiment once again justifies our motivation and validates the ef-fectiveness of SOSVM. Thus, SOSVM is more competent for ordi-nal regression problems in semi-supervised scenario than the clas-sical ordinal regression method SVORIM and the classical semi-supervised method LapSVM.
Figure 4 shows how the weight factor of the scale smooth regu-larizer  X  in multi-scale framework Eq (4) a ff ects the performance of SOSVM in terms of error rates on the Coil20 subset. From Fig-ure 4, we can see that an appropriate proportion of the scale smooth regularizer to the item smooth regularizer contributes to an optimal performance.
In order to show the e ffi ciency of our proposed kernel dual coor-dinate descent algorithm for SOSVM, we compare the time spent on the quadratic programming for SOSVM using Matlab X  X  quad-prog function with the proposed dual coordinate descent algorithm on the Coil20 subset with increasing amounts of labeled data. We use the same parameter configurations in subsection 7.2. Ten in-dependent experiments are performed (the labeled samples are ran-domly selected in each experiment). The mean time spent on the Matlab X  X  quadprog function (quadprog) and the proposed dual co-ordinate descent algorithm (dcd) are compared in the left plot of Figure 5. According to the computation complexity analysis in section 6, we see that our dcd method costs O (  X  Kl c n ), and the tra-ditional inner point method takes O ( tK 2 l 2 ), where l number of labeled samples per scale, and K is the total number of scales. Thus, the elapsed time of the dcd algorithm should increase linearly with increasing l c , whereas the time of traditional algorithm should increase quadratically with increasing l c . The results shown in the left plot of Figure 5 validate our analysis. We also see that Figure 4: Error rates on train unlabeled set and test set as a function of weight factor for SOSVM on Coil20 subset. Figure 5: Left: Elapsed time of quadratic programming of SOSVM as a function of number of labeled samples. Right: Error rates on unlabeled training set and test set as number of labeled samples increases. dcd is more stable than quadprog (dcd has a much smaller standard deviation than quadprog). We also plot the error rates as a function of the number of labeled data in the right part of Figure 5. We can see that as a whole the error rates on both unlabeled training data and test data decrease as the amounts of labeled data increases. This is consistent with the intuition that the more information provided, the better the performance of a semi-supervised method is. We can infer that supervised SOSVM always outperform semi-supervised SOSVM. However, the supervised version requires a large amount of labeled data, so the user can make a trade-o ff between high per-formance and heavy labeling costs.
In this paper, we try to utilize the distribution of unlabeled ordi-nal data to help deal with ordinal regression problems. We provide a general regularization framework to formulate semi-supervised ordinal regression and derive a method called semi-supervised or-dinal SVM (SOSVM) based on this framework. Ultimately, in or-der to e ffi ciently solve SOSVM, we propose a general kernel dual coordinate descent algorithm. Both rigorous theoretical analysis and solid experimental results on real-world datasets validate the outstanding performance and remarkable e ffi ciency of SOSVM. [1] A. Bordes, L. Bottou, P. Gallinari, and J. Weston. Solving [2] B.Scholkopf and A. Smola. Learning with Kernels . MIT [3] O. Chapelle, B.Scholkopf, and A. Zien. Semi-Supervised [4] G. Chen, J. Zhang, F. Wang, C. Zhang, and Y. Gao. E ffi cient [5] W. Chu and S. Keerthi. New approaches to support vector [6] K. Crammer and Y. Singer. Pranking with ranking. Advances [7] R. Herbrich, T. Graepel, and K. Obermayer. Large margin [8] R. Herbrich, T. Graepel, and K. Obermayer. Support vector [9] C. Hsieh, K. Chang, C. Lin, S. Keerthi, and S. Sundararajan. [10] T. Joachims. Transductive inference for text classification [11] S. Keerthi, S. Sundararajan, K. Chang, C. Hsieh, and C. Lin. [12] L. Li and H. Lin. Ordinal regression by extended binary [13] Z. Luo and P. Tseng. On the convergence of the coordinate [14] P. N. M. Belkin and V. Sindhwani. Manifold regularization: [15] S. Nene, S. Nayar, and H. Murase. Columbia object image [16] B. Pang and L. Lee. Seeing stars: Exploiting class [17] M. Qian, F. Nie, and C. Zhang. E ffi cient Multi-class [18] M. Qian, F. Nie, and C. Zhang. Probabilistic labeled [19] R.A.Adams. Sobolev spaces . Academic Press, New York, [20] A. Shashua and A. Levin. Ranking with large margin [21] V. Vapnik. Statistical Learning Theory . Wiley-Interscience, [22] F. Wang and C. Zhang. Label propagation through linear [23] X. Zhu and A. Goldberg. Kernel regression with order
