 With the continuing advances in data storage and commu-nication technology, there has been an explosive growth of music information from different application domains. As an effective technique for organizing, browsing, and search-ing large data collections, music information retrieval is at-tracting more and more attention. How to measure and model the similarity between different music items is one of the most fundamental yet challenging research problems. In this paper, we introduce a novel framework based on a multimodal and adaptive similarity measure for various ap-plications. Distinguished from previous approaches, our sys-tem can effectively combine music properties from different aspects into a compact signature via supervised learning. In addition, an incremental Locality Sensitive Hashing algo-rithm has been developed to support efficient retrieval pro-cesses with different kinds of queries. Experimental results based on two large music collections reveal various advan-tages of the proposed framework including effectiveness, ef-ficiency, adaptiveness, and scalability.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation, Search process; H.5.5 [ Sound and Music Com-puting ]: Systems Algorithms, Design, Experimentation, Human Factors Music, Similarity Measure, Personalization, Browsing, Search, Recommendation
Over the past decade, empowered by advances in network-ing, data compression and digital storage, modern infor-mation systems dealt with ever-increasing amounts of mu-sic data from various domain applications. Consequently, the development of advanced Music Information Retrieval (MIR) techniques have gained great momentum as a means to facilitate effective music organization, browsing, and sear-ching. One of the typical examples is that an end user might issue a text-based query to search for music records per-formed by a particular artist.

As one of the most fundamental components for MIR ap-plications, how to measure and model similarity between music items is an important yet challenging research ques-tion [5]. This is because music information can contain rich semantics and the related representations of low-level fea-tures are high-dimensional in nature. There has been in-tense research in this field and the solutions proposed so far can be generally classified into three independent families:
Metadata-based similarity measure (MBSM) -Text retrieval techniques are used to compare the similarity be-tween the input keywords and the metadata around mu-sic items [1, 2]. The keywords could include the title, au-thor, genre, performer X  X  name, etc. The main disadvantage is that high-level domain knowledge is essential for creat-ing the metadata and music facet (timbre, rhythm, melody, etc.) identification. It would be very expensive and difficult to represent this information using human languages.
Content-based similarity measure (CBSM) -Ex-tracting temporal and spectral features from music items for use as content descriptors has a relatively long history. It can be used as musical content representation to facilitate applications [8, 11, 20] for searching similar music record-ings in a database by content-related queries (audio clips, humming, tapping, etc.). However, the previous research on music content similarity measures focused mainly on a single aspect similarity measure or a holistic similarity mea-sure. In single aspect similarity, only limited retrieval op-tions are available. With this paradigm, end users have less flexibility to describe their information need. On the other hand, for the holistic similarity measure [8], high dimen-sional feature space results in slow nearest neighbor finding or complex probability model comparison (Gaussian Mix-ture Models, etc.). This is impractical for a commercial size database containing millions of songs. In addition, either the single aspect or holistic similarity is not flexible enough to adapt with the users X  evolving music information needs or retrieval context. Even worse, no personalization of the similarity measure is allowed.
 Semantic description-based similarity measure (SD-SM) -It is a proposed paradigm originally developed for image and video retrieval [17]. The basic idea is to an-notate each music item in a collection using a vocabulary of predefined words. Music can be represented as a se-mantic multinomial distribution over the vocabulary. The Kullback-Leibler (KL) divergence [17] is used to measure the distance between the multinomial distributions of the query and a music record. The same problem of limited description capability of human languages also exists in SDSM, since a limit number of keywords are used to describe music con-tent. The large vocabulary (easily hundreds of keywords) results in low efficient indexing and ranking, thus unafford-able response time.

Table 1 summarizes the existing work for music similar-ity measures. We can see that the similarity between two musical items can be measured from multiple dimensions in terms of title, author, genre, melody, rhythm, tempo, in-strumentation, etc. These dimensions are not independent. Different emphasis on each dimension will result in different similarity between the same two music items [5, 7].
Motivated by the above observations, we propose a novel framework for multifaceted music similarity measure. The key innovation of this study is to design and develop a comprehensive representation of music items called Com-positeMap. Using CompositeMap, music content-related dimensions (genre, mood, tempo, melody, etc.) are mod-eled as Fuzzy Music Semantic Vectors (FMSVs) and social information-related dimensions are described as Document Vectors (DVs). Adaptive similarity between music items can be measured using each individual musical dimension, or by any combination of those dimensions based on user X  X  pre-ferred music information need in each search process. To the best of our knowledge, this is the first method to seamlessly integrate the metadata, content, and semantic description-based similarity measure into a single framework. Moreover, personalization of music similarity can be easily enabled in related applications, where end users with certain informa-tion needs in a particular context are able to specify their desirable dimensions to retrieve similar music items. By better modeling users X  search targets based on personalized music dimensions, we can create more comprehensive sim-ilarity measures and improve the music retrieval accuracy. Compared with SDSM, high-level semantic concepts of a common music facet are grouped into a single music dimen-sion. For example, tens of genre classes are grouped into a genre dimension. Therefore, each music dimension con-tains a many fewer components than the whole vocabulary in SDSM. This advantage can provide more efficient mu-sic query and ranking in large databases. In addition, we also developed an indexing structure based on the LSH al-gorithm [3] to further improve the efficiency of the retrieval process. We implemented a showcase system of keyword and content-based music searching based on YouTube music data. Evaluation results based on two large-scale data sets collected from YouTube demonstrate the various advantages of the proposed scheme for music similarity measure. The remainder of this paper is organized as follows. In Sec. 2, we give a detailed introduction of the proposed frame-work. Sec. 3 describes the experimental setup. Evaluation results are discussed in Sec. 4, which is followed by our con-clusion in Sec. 5.
To address the problem raised in Sec. 1, a novel frame-work is developed to facilitate effective and flexible music information retrieval. As illustrated in Fig. 1, this multi-layer structure consists of two major functionality modules: music signature generation and indexing. In this approach, we propose a compact music signature, called Fuzzy Mu-sic Semantic Vector (FMSV). FMSV can explicitly describe each music content-related dimension in a structured and human-understandable way. A conceptual diagram is pre-sented in Fig. 2. By further representing the social infor-mation dimensions as Document Vectors (DVs) [13], a novel scheme called CompositeMap is proposed to map multiple and cross-modal music dimensions into a unified representa-Figure 2: Illustration of music space with exemplar music dimensions: genre, mood, and comments. tion. These music dimensions further span a music space, in which adaptive music similarity can be measured between any two music items. Each dimension can be indexed sepa-rately using incremental Locality Sensitive Hashing (iLSH) or inverted list in the indexing module. This framework fa-cilitates flexible retrieval by involving user X  X  personalization of preferred musical facets.
To represent each music content-related dimension, we de-sign a new representation -Fuzzy Music Semantic Vector (FMSV). We define the i -th music dimension as a FMSV, f = [ f i, 1 ... f i,N i ] T , 0  X  f i,j  X  1, 1  X  j  X  N i . For music dimension related to classification (genre, mood, etc.), N is the number of classes in the i -th music dimension and f i,j indicates the probability that the music item belongs to the j -th class of the i -th music dimension. For other content-related music dimensions (tempo, melody, etc.), N is the number of normalized values, f i,j , of that music di-mension 1 . We further employ Document Vectors (DVs) [13], music dimension, where d i,j  X  { 0 , 1 } and d i,j = 1 indicates the j -th word in a dictionary exists in the i -th music dimen-sion. All music dimensions are represented as real vectors with different number of components (we notate both FMSV and DV by f from here). Based on FMSV and DV, a music item can be represented as the set of all music dimensions, M = { f i | 1  X  i  X  N } . Examples of FMSVs and DVs for different music dimensions are illustrated in Fig. 2, in which the positions on genre, mood or comments axis illustrate the different vector values of FMSVs or DVs.

As discussed in [5, 6, 12], music semantic concepts are usually represented by rigid human labels, e.g., classical for a genre type. However, music concepts are fuzzy in nature. Humans do not always agree on a single label for the same music item. Besides, human labels may be too broad to com-pare the similarity between two music items. These observa-tions imply that human labels are not good representations of musical semantics when measuring music similarity.
We propose FMSV to represent each high-level music di-mension. It represents the probabilities that a music item belongs to each class of that dimension or the most proba-ble values that dimension has. It reveals the fuzzy nature (uncertainty) of human X  X  perception, which is a more ac-curate representation of human X  X  musical opinions. FMSVs are well structured and human understandable, which al-lows direct interaction between users and the music signa-ture. FMSVs are efficient to compute, as the FMSV of each music dimension has many fewer components (e.g.,  X  10 in genre [8]) than existing audio features (e.g.,  X  100 in Sec. 2.3.1). The human-understandable nature al-lows FMSVs to be customized to represent different sets of classes in various applications. These properties not only make FMSVs effective to represent music but also flexible to use and efficient to index in music retrieval applications. With the above description, we can see that FMSVs and DVs satisfy properties of Euclidean metric, i.e., symmetry, and triangle inequality. The distance between two music items M j and M k in the i -th music dimension f i can be measured by the normalized Euclidean metric as: where N i is the number of components in the i -th music dimension, and dis( f j i , f k i )  X  [0 , 1].

With all the N music dimensions, we can span a music space in which musical items can be characterized by clear and musically meaningful concepts. The music space can be personalized by users into a subspace, P = { ( p i , w p i p  X  N, 1  X  i  X  N P , N P  X  N } , by choosing the most inter-esting dimensions p i and specifying their preferred weights, w i  X  [0 , 1]. In P , a personalized music similarity measure between two music items M j and M k is defined as: where  X  and  X  are normalizing factors. If  X  = 2 e +1 e  X  1  X  = 2 e  X  1 , Sim( M j , M k ; P )  X  [0 , 1].
In order to map low-level acoustic features into FMSVs for content related music dimensions (Fig. 3) and to map text information into DVs for social information related mu-sic dimensions [13], a supervised learning based scheme, called CompositeMap, is developed to generate a new fea-ture space. During the mapping of FMSVs, the most effec-tive heuristic feature sets are selected to ensure reasonable prediction accuracy. Then a feature selection algorithm is applied to reduce dimensionality. Efficient multi-class prob-ability estimation is then conducted to generate FMSVs. For the mapping of non-classification related FMSVs, we di-rectly calculate their most probable values. For example, for tempo and melody we compute the beat histogram and pitch histogram as their FMSVs, respectively. In this framework, we consider various audio features. Based on their musical meanings, we categorized the em-ployed features as follows:
Timbral features represent the timbral texture of mu-sical sounds. Timbral features are calculated based on the magnitude spectrum of short time Fourier transform (STFT) and include: Spectral Centroid , Rolloff , Flux , Low-Energy feature [19]; Spectral Contrast [12]; Mel-Frequency Cepstral Coefficients (MFCCs) [10]. The total dimensionality is 20.
Temporal features represent musical properties based on time domain signals. They include: Zero Crossing Rate ; Autocorrelation Coefficients ; Waveform Moments ; Ampli-tude Modulation [12]. The total dimensionality is 15.
Spectral features complement timbral features in rep-resenting musical characteristics by spectra. They include Auto-regressive (AR) features ; Spectral Asymmetry, Kurto-sis, Flatness, Crest Factors, Slope, Decrease, Variation ; Fre-quency Derivative of Constant-Q Coefficients ; Octave Band Signal Intensities [12]. The total dimensionality is 20.
Rhythmic features represent musical timing character-istics of a music item. They include: Beat Histogram [19]; Rhythm Strength , Regularity and Average Tempo [12]. The total dimensionality is 12.

Melody features summarize the melody content of a music item. We employ Pitch Histogram proposed in [19] as melody features. The total dimensionality is 48.
As noticed, low-level audio features contain many more components (115) than FMSVs. High dimensionality of existing audio features has restricted the applicability of content-based music retrieval in large collections. A fea-ture selection algorithm (Alg. 1) based on localized predic-tion error [14] is applied to reduce the dimensionality of the combined features while maintaining relatively good predic-tion accuracy. In Alg. 1, t e is the stopping threshold of the decrease in prediction accuracy. Feature selection can sig-nificantly reduce the complexity of on-line prediction at an affordable cost of higher off-line computation.
 Algorithm 1 : Feature selection algorithm.
 1: Train SVM using ePEGASOS on DB tr with features F ; 2: Compute the localized prediction error e o on DB te ; 4: repeat 7: Compute the localized prediction error, e i , by keeping 8: end for
In this study, Support Vector Machines (SVMs) are used for the purpose of multi-class probability estimation. Based on an efficient SVM training algorithm, PEGASOS [16], for binary classification problems with only binary label output, we propose an extended version, ePEGASOS, to support multi-class SVMs with probability estimates. The running time of PEGASOS has inverse dependency on the training dataset. Based on our experimental results, we show that ePEGASOS reveals the same desirable property: training a better generalized SVM with less run time on a large database.

PEGASOS is an iterative algorithm for optimizing SVM w on a given training set S = { ( x i , y i ) } m i =1 , where x and y i  X  X  +1 ,  X  1 } . Each iteration involves a stochastic gra-dient descent step and a projection step. By giving T , the number of iterations, and k , the number of samples used for calculating sub-gradients at each iteration, PEGASOS op-timizes the following unconstrained training error function with a penalty term for the norm of SVM being learned: f ( w ; A t ) =  X  where A t  X  S is formed by k samples selected i.i.d. from S at each iteration t . w is initialized as zero vector and is updated at each iteration t as follows: where  X  t = 1 / (  X t ) is the learning rate, A + t is the set of samples on which w has non-zero training error. To train kernel SVMs, w t can be calculated as w t = where I t  X  { 1 , ..., m } . Then  X  w t , x t  X  = and k w t k 2 = employ the method proposed in [15] to estimate the proba-bility that a unknown sample belongs to class y = 1 as: where A and B are estimated scalars by minimizing the error function using the training data and their decision values.
Based on the above binary class SVM with a probability estimate, we further employ the generalized Bradley-Terry model [9] to extend binary-probability PEGASOS to sup-port multi-class probability estimate. In K class classifica-tion problems, one-against-the-rest scheme is employed to decouple the multi-class problem into K binary classifica-tion problems. The Bradley-Terry model is formulated as: min p  X  subject to to derive probability p j , j = 1 , ..., K , that a unknown sample belongs to the j -th class. Then the FMSV is formed as f = [ p 1 ... p K ] T .
Inspired by the inverted index used in text retrieval, we develop a hybrid indexing framework to index each music dimension separately by its most suitable algorithm in order to build an overall efficient index for the whole music space.
Music dimensions represented by FMSVs are indexed by a proposed incremental Locality Sensitive Hashing (iLSH). The original LSH was proposed in [3]. It supports fast near-est neighbor search in high dimensional space with sub-linear time, which is critical for large music database of millions of tracks. To better suit our indexing solution to real applica-tion scenarios, such as on YouTube or Last.fm, where new music samples are periodically added into existing indexes, we propose an iLSH algorithm (Alg. 2) to efficiently update the existing index structure without the need to recompute the whole index from scratch. iLSH is desirable especially in a large database. In Alg. 2, the difference function for two sets of parameters is defined as: where  X  = { k, L } ; k is the number of hashing functions chosen to construct a hash table; L is the number of hash tables [3].  X  and t  X  are two update thresholds. Inverted list [13] is used for music dimensions represented by DVs.
Based on the above indexing approach, the time complex-ity of online query is sub-linear, O ( N P  X  N d  X  n 1 /c N
P is the number of personalized music dimensions, N d is the highest number of components in all those music dimen-sions, n is the total number of music items in the database, and c is the factor for approximate nearest neighbor finding in iLSH. In a commercial system, N P and N d will be small (  X  10), while n is over a million. c &gt; 1, can be tuned to trade off between query accuracy and efficiency.
Based on users X  personalization input P discussed in 2.2, nearest music items M r i to the query M q are retrieved by iLSH in each of the personalized dimensions, ( p, w )  X  P . The adaptive music similarity measure, Sim( M q , M r i ; P ), is then used to rank all the returned items. As Sim( M q , M is of accumulative nature, music items that are near to the query in more music dimensions are more likely ranked top. Algorithm 2 : Incremental Locality Sensitive Hashing. 1: Compute the parameter set  X  of the hashing structure H 2: Hash S into H [3]; 3: Set the last update position of H , s := 1; 7: else 11: else 13: Set s := i ; 14: end if 15: end if 16: end for 17: return H .
A music search system on top of YouTube APIs and Mars-yas [18] was implemented as an exemplar application of the proposed framework. In this section, we give an intro-duction on experimental configuration for empirical study. Sec. 3.1 describes query design and two music test collec-tions. Sec. 3.2 details the methodology for the experimental study, hardware configuration and evaluation metric.
By crawling the audio stream of music videos on YouTube, we built a tunable test collection (TS1 with 3020 music items) with YouTube social text information and manually labeled content related tags (Table 2 shows its hierarchy). TS1 is labeled and cross checked by multiple amateur musi-cians to ensure the validity of the ground truth. TS1 is in-tended to evaluate the effectiveness of FMSV generation and compare the retrieval precision of FMSV with other audio signatures. A large scale test collection (TS2 with 100,000 music items) with YouTube social text information and the built FMSV description 2 was built to evaluate the effective-ness of FMSV on large scale collections and the scalability of the proposed framework.

To simulate the realistic music search behavior, we design music queries with different levels of complexity in musi-cal information need . Audio queries were designed to allow personalization of any single music dimension or any combi-nation of music dimensions. Some examples of the designed queries are listed in Table 3. Each query is associated with different music dimensions, which simulates the search situ-ation that different users may want to search similar music to the query based on its different music aspects, i.e., genre, mood, etc. Users can form low complex queries (personalize one music dimension) or high complex queries (personalize more dimensions) to search for their wanted music. 24 subjects volunteered for the evaluation. 10 of them are amateur musicians, familiar with various music styles and taxonomy. The other 14 are music hobbyists. It is noted that for each audio query, the class labels of each music dimension only serve as a reference. The subjects do not need to know the actual meaning of all the class labels in order to judge the similarity of the returned results. They just need to distinguish different music dimensions.
For each test collection, the same methodology was ap-plied to conduct experiments. A briefing was conducted before the experiment to make sure subjects understood the experimental procedure and were familiar with the music dimensions to be used. Firstly, subjects were asked to do searches with low complex queries by randomly selecting an audio query of one personalized music dimension. For each search task, subjects needed to judge whether each of the first 30 returned results was similar to the query in the personalized music dimension. For a complete trial, each subject repeated this with each of the music dimensions per-sonalized and an audio query randomly selected. With this procedure, we guaranteed that over each music dimension, the same number of searches were performed and the se-lected queries for each dimension were uniformly distributed among all the designed queries. Secondly, high complex queries were used for searches by subjects. We followed the methodology described above to ask each subject to con-duct at least one complete trail over all combinations of the music dimensions. When more music dimensions were per-sonalized, the returned result is considered relevant as long as it was similar to the query in any of the music dimensions.
Precision @ n is used as the metric to evaluate the re-trieval effectiveness. It is defined as the percentage of the relevant results in the top n returned ones. The average precision@ { 5-30 } was measured for search tasks of both low and high complex queries. The average runing/response time were employed to evaluate the system efficiency. All experiments were conducted on a DELL PowerEdge 2970 workstation with 2 CPUs (each is a Quad-Core Intel Xeon E5420, 2x6MB cache CPU) and 32GB memory (DDR-2 667MHz).
In this section, we study the proposed framework from two main aspects -effectiveness and efficiency.
Effective FMSV generation plays a very important role on the final performance of the whole system. For music di-mensions, such as genre, mood, instrument, and vocalness, multi-class SVMs were trained using randomly selected 50% of music items in each class and evaluated using the rest on TS1. 10 evaluation trials were conducted. The average classification accuracy and standard deviation are listed in Table 4. These accuracies of our approach are comparable to the state of the art performances [8]. The high quality FMSV generation is the foundation of accurate music re-trieval.
 Based on TS1, we compare the retrieval effectiveness of FMSV with other audio signatures: existing audio features (AF), described in Sec. 2.3.1, and the transformed audio fea-tures by principal component analysis (AFPCA). For AF, all the 115 features components were combined as a mu-sic signature for genre/mood dimensions, and 55 feature components (without rhythmic and melody features) were combined for instrument/vocalness. For AFPCA, 95% data variance was retained during PCA, which corresponds to 18 and 12 feature components for genre/mood and instru-ment/vocalness, respectively. 50% of data were used to train FMSV and AFPCA, the rest were used for testing.
 Fig. 4 shows the precision@ { 5-30 } of searches using FMSV, AF and AFPCA for low complex queries. In each of the four music dimensions, FMSV clearly outperforms AF and AFPCA with statistically significant improvement. Fig. 5 illustrates their retrieval precision for high complex queries. It is noted that when personalizing more music dimensions, search precision consistently gets better than personalizing one music dimension. With high complex queries, FMSV still performs the best. In some queries with genre+instrument or genre+vocalness personalized, FMSV reveals more im-provement than with low complex queries. Those results imply that music content representation based on FMSV carries more useful information and enjoy superior discrim-ination capability. It leads to better search accuracy.
Fig. 6 illustrates the average precision of FMSV for low/high complex queries on TS2. One thing worth noting is that while the size of test collection becomes larger, FMSV still can sustain superior retrieval accuracy. This result demon-strates the robustness of FMSV from another perspective. Figure 4: Average precision@ { 5-30 } comparison for low complex queries on TS1. Figure 5: Average precision@ { 5-30 } comparison for high complex queries on TS1. Figure 6: Average precision@ { 5-30 } of FMSV for both low and high complex queries on TS2. Training SVMs could be a very time consuming process. In the first set of experiment, we evaluate ePEGASOS over a large data set, the Reuters CCAT 3 . The main purpose of this study is to show that using the proposed algorithm, SVM training time has inverse dependency on the size of training data, provided that the same generalization error is maintained. The left sub-figure of Fig. 7 shows the average running time of ePEGASOS training a multi-class SVM on CCAT. It is noted that the running time decreases when more and more training data are provided. On large data Figure 7: The average running time of SMO and ePEGA-SOS in training multi-class SVMs with probability estimate on different sized datasets. Figure 8: The indexing and query time comparison in in-cremental indexing scenario. sets, this is desirable to train SVMs with less running time and better generalization performance.
 We further compare the average running time of ePEGA-SOS and SMO 4 on a smaller scale genre feature set to show its efficiency. As shown in the right sub-figure of Fig. 7, the running time of ePEGASOS almost stays the same as more training data are added, while the running time of SMO in-creases dramatically. Due to the much smaller scale of the genre feature set compared with CCAT, the running time of ePEGASOS is already very low and does not decrease as dramatically as on CCAT.

In the feature selection algorithm (Alg. 1), the stopping threshold t e was set as 0.03. With this setting, 30 out of 115 features were selected for genre/mood dimensions and 20 out of 55 features were selected for instrument/vocalness. The average FMSV generation time for a 3-minute music item is reduced from 1.561 to 1.303 seconds and from 1.334 to 1.127 seconds, respectively. The 0.2 seconds improvement is significant as it constitutes more than 10% of the total response time (  X  1 . 7 seconds), described in Sec. 4.2.3.
For large MIR systems, economic maintenance cost is an-other important concern. In this study, we compare the average index construction time of iLSH and LSH in the following scenario: firstly index a static data set, which con-tains 1,000,000 data samples of 15 components with value ranging from 0 to 1 to simulate FMSVs; then update the index structure when 20,000 new samples are added into the data set at regular time instances. The size of the initial static data set is at the comparable order of commercial mu-sic databases, such as YouTube and Last.fm. The number of samples added at each time instance simulates the mu-sic items uploaded by users on YouTube or created by new artists on Last.fm in a period of time. This scenario consid-ers the need of incremental indexing in real life applications.
Fig. 8 shows the average index construction time of 10 runs with  X  = 0 . 1 and t  X  = 0 . 08. iLSH performs signif-Figure 9: The average response time of search in single music dimension on various data set scales. icantly better than LSH at most of the time instances, as iLSH only updates the index structure instead of re-indexing from scratch like LSH. At the time instances when iLSH per-forms a complete update (re-indexing), its running time is the same as LSH.

The average top-100 query time of iLSH/LSH was com-pared with KD-tree [4]. The average query time of 100 queries is illustrated in Fig. 8. It is noted that iLSH and LSH has the same query time (  X  100 ms in a data set of 1.6 million samples), as they follow the same procedure to search nearest neighbors. Their query time is significantly lower than KD-tree over all sized data sets.
In Fig. 9, we compared the average top-100 response time of a search process including query upload, music signature generation, query, and ranking for a single music dimen-sion. Different music signatures (FMSV, AF, and AFPCA) on various sized data sets were evaluated. Since FMSV has many fewer components (  X  10) than AF (115), the response time using FMSV is significantly less than using AF, especially on large data set. After applying PCA on AF (AFPCA), the response time is reduced compared with AF. However, due to the concern of retrieval effectiveness, enough features must be retained in PCA (could be &gt; 10). This adds unpredictable factors to the response time, as dif-ferent feature sets need to keep different number of compo-nents in PCA. In our system, as AFPCA has more features than FMSV, its response time is longer. As the data set gets larger, the response time of FMSV remains acceptable (  X  0 . 5 seconds on the data set with 3000 samples and  X  1 . 7 seconds on the data set with 1 million samples). The flexi-ble indexing approach of the framework allows easy parallel implementation of music search over multiple music dimen-sions. Therefore, the above response time is illustrative even when searching is with multiple music dimensions.
With fast response time in each music dimension and effi-cient parallel computation for multiple dimensions, the pro-posed framework scales well on large databases.
We have presented CompositeMap, a novel framework of multimodal music similarity measure to facilitate vari-ous music retrieval tasks such as organizing, browsing, and searching in a large data set. We have detailed the FMSV which can map any existing audio features into high-level concepts such as genre, mood, etc. CompositeMap has uni-fied content-based, metadata-based, and semantic description-based music retrieval approaches. It combines different mu-sic facets into a compact signature which can enable per-sonalized services for users with different information needs, background knowledge, and expectations.

For a case study, we have employed CompositeMap in a music search engine to evaluate its effectiveness, efficiency, adaptiveness and scalability using two separate large scale music collections extracted from YouTube. Our objective evaluation and user study show the clear advantages of the proposed framework. Furthermore, our project has led to several innovations including an efficient SVM training al-gorithm with multi-class probability estimates and an incre-mental Locality Sensitive Hashing algorithm.
