 One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are nei-ther necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been pro-posed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose an unsupervised statistical qual-ity estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure; multi-ple workers are rst requested to work on the same tasks in the creation stage, and then another set of workers re-view and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Exper-iments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, which implies that our method can deliver high quality re-sults with lower costs.
 H.2.8 [ Information Systems ]: Database Applica-tions| Data mining ; H.1.2 [ Models and Principles ]: User/Machine Systems| Human information processing crowdsourcing; quality control; human computation
Crowdsourcing is a type of online activity of outsourc-ing speci c tasks to a large group of people. With the re-cent expansion of crowdsourcing platforms such as Amazon Mechanical Turk, one can easily outsource various complex tasks including audio transcription, article writing, language translation, program coding, and graphic designing, as well as simple tasks such as image tagging and Web content categorization. The popularity of crowdsourcing is increas-ing exponentially in computer science as well, and it has been successfully applied to elds such as natural language processing, computer vision, and human computer interac-tion (e.g., [1, 2, 18, 19]).

One of the most challenging issues in crowdsourcing re-search is quality control to ensure the quality of crowdsourc-ing results, because there is no guarantee that all workers have sufficient abilities needed to complete the offered tasks at a satisfactory level of quality. Moreover, it is known that some faithless workers try to get paid as easily as possible, which results in worthless responses. Most crowdsourcing platforms allow requesters to check submitted results, and to reject low-quality results; however, it is not realistic to check all of them manually if their volume is large.
Several approaches to efficient quality control have been proposed. They are roughly categorized into supervised and unsupervised approaches. Supervised approaches use tasks with known correct answers called gold standard datasets to estimate the ability of each worker. For example, each worker is required to qualify before they start the job by passing several tasks selected from the gold standard dataset. They can also be randomly injected into actual tasks in such a way that workers do not recognize them, which allow for ability evaluation of crowd workers to ex-clude low-quality workers. However, the use of such su-pervised approaches is limited because of the high cost of preparing the gold standard datasets, or difficulties in de-termining one unique answer.

Unsupervised quality control methods use redundancy in-stead of the gold standard datasets to ensure work quality; they assign a single task to multiple crowd workers, and ag-gregate their responses by applying majority voting or more sophisticated statistical aggregation techniques [17]. The statistical quality control methods consider the character-istics of each worker or task, such as the ability of each worker and the difficulty of each task [5, 22, 21]. However, one serious disadvantage of these methods is that most of the existing approaches assume that the response spaces are structured . Binary questions (e.g., yes-or-no questions) and Fi gure 1: Example of a two-stage work ow compris-ing a creation stage and a review stage. multiple-choice questions (e.g., ve-point ratings) are typ-ical examples where voting-like strategies work, or we can apply (weighted) averaging to real-valued questions. Un-fortunately, these approaches are not applicable for tasks with unstructured response formats 1 , such as article writing and logo design tasks, where we cannot expect an agree-ment of two outputs. Most of the crowdsourcing tasks fall into this category. Ipeirotis [8] reported that ve of the top twelve Mechanical Turk requesters (based on the total re-wards during Jan. 2009{Apr. 2010) posted unstructured response tasks such as content generation, content rewrit-ing, and website feedback. Further, graphic design tasks such as logo design and business card design are quite popu-lar on some specialized crowdsourcing marketplaces such as 99designs and DesignCrowds.

One natural approach to quality estimation of artifacts for general unstructured response tasks is to employ a two-stage work ow as shown in Figure 1, consisting of a creation stage followed by a review stage 2 . In the creation stage, several crowd workers (which we call authors ) are assigned to several unstructured response tasks. Then, their artifacts proceed to the review stage, where each of them is reviewed by mul-tiple crowd workers (called reviewers ). The review tasks are usually casted as multiple-choice questions (such as `Excel-lent,' `Good,' `Average,' `Fair,' and `Poor'). Although it is quite difficult to estimate the quality of the artifacts directly from themselves, introducing the review stage enables us to indirectly estimate the quality from the review scores, and to distinguish high-quality results from the others. For ex-ample, Zaidan and Callison-Burch [24] applied the two-stage work ow; however, their approaches are supervised so that they require extensive domain knowledge including feature representation of artifacts and gold standard scores.
In this paper, we propose an unsupervised statistical method to estimate the quality of artifacts of general unstructured response tasks using the framework of the two-stage work ow. We introduce a two-stage generative
Lin et al. [11] considered tasks with somewhat unstruc-tured formats; however, they still assume that two output instances agree.
They are called by different names in literature (e.g., [12]). model (Figure 2). The creation stage models a generative process of the true artifact quality, where both the ability and the task-dependent performance of an author affect the quality of an artifact. The review stage models the gener-ative process of the grade labels given by reviewers, where each reviewer rst determines a latent quality score for a given artifact based on their bias and contextual preference, and then the observed grade label is generated through the graded response model [16] used in the item response the-ory [20]. The true artifact quality and the model parameters are estimated using the maximum a posteriori (MAP) infer-ence. The proposed algorithm consists of simple iterations of a closed-form update and a convex optimization.
We conduct experiments using logo designing tasks, image description tasks, and language translation tasks on a com-mercial crowdsourcing platform. Our method outperforms the other methods, including the majority voting and an or-dinal label aggregation method [15] (which is an extension of the well-known method proposed by Dawid and Skene [5]) with a small number of reviewers and a moderate number of authors. The result implies that our method can deliver high-quality results with lower costs, because the number of involved workers directly affects the total monetary and time costs.

In summary, this paper makes three main contributions: 1. We address an unsupervised statistical quality esti-2. We introduce a two-stage generative model for the gen-3. We devise an efficient iterative algorithm which per-
We start with a formulation of the work ow of general crowdsourcing tasks with unstructured response formats consisting of two-stages: the creation stage and the review stage (Figure 1). Further, we describe the quality estimation problem of general crowdsourcing tasks.

Let us assume that there is a set of general crowdsourcing tasks T (such as logo designing and content generation), and let A t denote a set of crowd authors assigned to a task t 2T . In the creation stage, each author a 2A t creates an artifact for a task t . We denote the (unknown) quality of the artifact by q t;a 2 R . In the review stage, a set of crowd reviewers R t;a is assigned to evaluate the quality of the artifact created by author a for task t . The evaluation by a reviewer r 2 R t;a is given as a grade label g ( r ) set of grade labels D = f 1 ; 2 ; ;n g .

Our goal is to estimate the set of the true qualities of the artifacts f q t;a g t 2T ;a 2A t , given the set of the observed grade
In practice, the set of authors and reviewers may overlap, and some reviewers possibly give good grades to their own Fi gure 2: Graphical model of our proposed two-stage model. a 2 R denotes the ability of the author a 2A , and 1 = a 2 R + denotes the variance of the artifact-speci c noise v t;a 2 R for the pair of the task t 2T , and the author a . The true quality q t;a of the output is given as the sum of a and v t;a . r 2 R de-notes the evaluation bias of the reviewer r 2R , and 1 = r 2 R + denotes a variance of the contextual pref-erence w ( r ) t;a 2 R for the artifact created by the author a for the task t . The quality score s ( r ) t;a is the sum of , w ( r ) t;a , and the true quality q t;a , which results in the observed grade g ( r ) t;a 2f 1 ; 2 ;:::;n g through the graded response model with threshold parameters f b d g d . k and are hyper-parameters. output. However, we assume that we can exclude such work-ers with some identi ers; in other words, the sets of authors and reviewers are distinct.
To estimate the true quality q t;a of the artifact created by author a for task t , we introduce a two-stage generative model, where the rst stage models the generation of the artifact of quality q t;a , and the second stage models the gen-eration of the grade label g ( r ) t;a given by reviewer r to the artifact. Figure 2 shows the graphical model of our grade label generation process.
We assume that an author with a higher ability creates higher-quality artifacts on average; hence, each author a has ability a 2 R . We also assume that the performance of an author on each task varies according to the type and instance of the task. Considering language translation tasks as an example, even an author with a low general translation skill might sometimes produce high quality translations for sentences related to information technologies, if he is knowl-edgeable about information technologies. We model such variety depending on the combination of task t and author a as the noise v t;a 2 R . We assume that the noise v t;a lows a Gaussian distribution with zero mean and a variance of 1 = a (i.e., a precision of a ); that is, Note that each author a has their own a .

At the end of the creation stage, the quality of the artifact q t;a 2 R is given as the sum of the general ability and the artifact-speci c variation, namely,
In the review stage, we assume that each reviewer r has a base bias r 2 R , assuming that a reviewer with a lower bias tends to give lower grades to the given artifacts, and one with a higher bias gives higher grades. We also incorporate the contextual preferences of reviewers, for example, some reviewers might prefer short sentences to long sentences. We model such preferences as the noise depending on a pair of output and a reviewer denoted by w ( r ) t;a 2 R . We assume that w t;a follows a Gaussian distribution with zero mean and a variance of 1 = r (i.e., a precision of r ); that is, Note that each reviewer r has their own r . When reviewer r 2R t;a evaluates the output of author a for task t , the reviewer rst estimates the (latent) quality score s ( r ) of the output, which is given as the sum of the true quality of an artifact, q t;a , the reviewer's bias r , and contextual preference w ( r ) t;a , namely,
Finally, since the nal grade label g ( r ) t;a is a discrete value depending on the quality score, we apply Pr[ g ( r ) t;a = d which is the conditional probability of selecting d 2D given the quality score s ( r ) t;a . For modeling Pr[ g ( r ) t;a adopt the graded response model (GRM) [16] (Figure 3), which is a standard model of the graded responses of sub-jects in the item response theory (IRT) [20]. In the GRM, the conditional probability of a graded response is decom-posed by using n 1 binary response models, namely, There are several possible choices for the binary response models, and we adopt the Rasch model [14], which is one of the simplest models, given as where is the sigmoid function, and f b d g d are threshold parameters. Finally, our grade label generation model is
For simplicity, we set the thresholds ( b 1 ;b 2 ; ;b n 1 (1 ; 2 ; ;n 1) in our implementation, because it had no signi cant effect on the performance. Fi gure 3: Example of a probability density function of the graded response model (GRM), which models the probability of a graded response g given a latent score s , where a set of grade labels D = f 1 ; 2 ; 3 ; 4 ; 5 and the threshold parameters ( b 1 ;b 2 ;b 3 ;b 4 ) = (1 ; 2 ; 3 ; 4)
Based on the two-stage crowdsourcing model introduced in the previous section, we introduce our approach that uses the maximum a posteriori (MAP) inference to estimate the artifact quality as well as the other parameters such as the abilities of workers. Our algorithm consists of simple itera-tions of two optimization steps: one is a convex optimization problem, and the solution of the other step is given in closed forms.
We introduce prior distributions on the model parameters to apply the MAP inference. In the creation stage, we as-sume that the prior for worker ability is given as a Gaussian distribution with zero mean and a variance of 1; that is, S ince the precision parameter a in the artifact-speci c noise (1) must be positive, we assume a Gamma prior, wh ere k and are hyperparameters.

Similarly, in the review stage, we give the prior for the bias of each worker as a Gaussian distribution, and assume a Gamma prior on the precision parameter r for the contextual preference (2); that is, where k and are hyperparameters.
The total likelihood L is a function of f a g a , f a g a f L = where s ( r ) t;a is de ned in Eq. (3). Its logarithm is given as log L = where T a denotes the set of tasks done by author a , and is the set of task-author pairs evaluated by reviewer r .
Our strategy to optimize the objective function is to split the parameters into the set of f a g a and f r g r and the set of the other parameters, f a g a , f r g r , f v t;a g f w f a g a and f r g r are given in closed forms. The optimiza-tion problem with respect to the other parameters is a con-vex programming problem; hence, we can apply standard nonlinear optimization methods such as the gradient ascent and the Newton-Rhapson method. These facts naturally give the following iterative optimization procedure. 1. Set initial parameters (to zeros) 2. Maximize the objective function (4) w.r.t. f a g a and 3. Maximize the objective function (4) w.r.t. f a g a , 4. If the solution has not yet converged, go to Step 2 The closed form solution of Step 2 is given as always satis ed if jT a j &gt; 1 and jU r j &gt; 1, or k&gt; 1 = 2.
In our implementation, we employed a simple gradient ascent to solve the convex optimization problem in Step 3.
To evaluate our proposed two-stage model, we prepared three tasks, logo designing (Section 5.1.1), image description (Section 5.1.2), and English-to-Japanese translation (Sec-tion 5.1.3), and posted them on a commercial crowdsourcing service. Using the artifacts obtained in the creation stage, we posted review tasks for each artifact. We compared the accuracies of the qualities estimated by our two-stage model with those by three other methods (Section 5.2).
We built our datasetsusing the Lancers crowdsourcing marketplace 3 . Tables 1 and 2 give their general statistics. Fi gure 4: Example of a review task for an image description
Graphic design is a typical example of unstructured re-sponse format tasks. Design tasks usually take the form ht tp://www.lancers.jp of contest. A requester chooses the most preferable design from ones submitted by crowd designers. Only the winner gets the prize, and the others are not paid (or paid only a small amount of money).

We collected the data from 34 (already closed) logo design contests from Lancers, and used the submitted logos as the artifacts in the creation stage. We posted evaluation tasks asking workers to give ve-point ratings to the artifacts; each evaluation task includes 10 logo designs.
Textual descriptions for images are useful resources for en-hancing image search accuracy, or to help visually impaired people understand the content of a picture. Generating im-age description is a typical example of a problem that is relatively easy for a human but very difficult for a computer.
We requested tasks of writing a description of a picture within 140 Japanese characters. We randomly selected 20 pictures from the SBU Captioned Photo Dataset [13]. Each author was asked to complete one or more batch of tasks comprising 10 randomly selected pictures. After the com-pletion of the description task, we posted tasks of reviewing the submitted descriptions. Figure 4 shows an example of the review tasks. Reviewers were instructed to review the description in terms of both adequacy and uency, and as-sign a ve-point grade to each description.
Language translation is one of the most common tasks in crowdsourcing marketplaces, and several research efforts (e.g. [24]) attempt to collect high-quality translations from non-professional translators using crowdsourcing. We posted sentence translation tasks from English to Japanese, and then posted grading tasks for each submit-ted translation. We selected 20 English sentences from the Japanese-English Bilingual Corpus of Wikipedia's Kyoto Ar-ticle 4 . We made batches, each of which consisted of ten randomly selected sentences, and one of these batches was assigned to each author. h ttp://alaginrc.nict.go.jp/WikiCorpus/index_E. html
In the review stage, we requested crowdsourcing workers to review each of the translated Japanese sentences and to assign a ve-point grade to it. We asked the reviewers to take into account the uency and adequacy of each sentence when assigning a grade.
The review stage makes it possible to apply label aggre-gation methods to quality estimation for unstructured re-sponse format tasks. The existing label aggregation methods for multiple-choice questions can be applied to the collected grade labels f g ( r ) t;a g t;a;r . We compare our proposed two-stage model with two aggregation methods: majority voting and the modi ed Dawid-Skene model [15]. Majority voting is a simple method to aggregate the labels; however, it shows a good performance on several crowdsourcing tasks [18, 9]. The label aggregation proposed by Dawid and Skene [5] has been successfully applied to various crowdsourcing applica-tions. They give a generative model of labels created by each worker with their own ability, where the ability of a worker is represented as a confusion matrix which represents the con-ditional probability of an observed label given a true label. The true labels and the confusion matrices are estimated by using the EM algorithm. Since we focus on ordinal scores in this paper, we use the modi ed version proposed by Raykar et al. [15] (which we call ` ordinal Dawid-Skene ').
The major differences between our proposed two-stage model and the other two competing methods are summa-rized as follows. To estimate the quality of the artifact by author a for task t , majority voting only uses f g ( r ) graded labels limited to the given artifact. On the other hand, the Dawid-Skene model and our two-stage model ex-thermore, our two-stage model incorporates both the abili-ties of the workers and the biases of the reviewers in contrast with the Dawid-Skene model which only considers the abil-ities of the reviewers.

To evaluate the advantage of introducing the creation stage, we also tested our two-stage model without the cre-ation stage (which we call ` review stage model '). Concretely, we xed the parameters in the creation stage at the prior means, i.e., a = 0 and a = k for each author a .
We calculated the correlation coefficients between the es-timated artifact quality scores and the ground truth grades. We also evaluated nDCG@1, which is de ned as the ratio of the true quality of the estimated best artifact to that of the true best artifact. because we are often interested in nding the best artifact for each task. Since we could not know the \ground truths," we simulated the ground truth scores using majority voting with sufficiently many labels. Concretely, we used a small part of the collected grade labels for esti-mation, and used the others for simulating the ground truth scores. This is supported by the results of Snow et al. [18], where they suggested majority voting with ten or more non-expert worker is on par with that with experts for various NLP tasks. For example, we collected 25 grade labels for each artifact in the image description dataset as in Table 2.
We also investigated the impact on the estimation accu-racy by the number of authors assigned to each task and the number of reviewers assigned to each artifact. We var-ied the number of reviewers for each artifact from one to ten. Similarly, we varied the number of authors from one to its maximum value for each task.

For statistical testing, we sampled 100 subsets of the data for each ( n;m ) pair, where m denotes the number of authors and n denotes the number of reviewers, and performed the Wilcoxon signed rank test. We set k = 16 and = 0 : 5 throughout the experiments. Figure 5: Correlation and nDCG@1 between esti-mated quality scores and ground truth scores along with the number of reviewers per artifact. In most cases, the proposed two-stage model outperforms the other three baselines especially with small num-bers of reviewers.
Table 3 and Figure 5 show the correlations and nDCG@1 between estimated artifact scores and ground truth scores for each number of reviewers (ranging from one to ten). In most cases, our proposed two-stage model achieved statisti-ca lly signi cant higher performance over the other methods. In particular, when the number of reviewers is small, our method showed large improvements. It is notable that our model performed better even in such cases where we had only one reviewer and therefore the voting-like strategies do not work. This is because our model incorporates the cre-ation stage with the ability parameters of authors for making the most of available information. The difference of the per-formance between the two-stage model and the review stage model (i.e., a two-stage model with xed author parame-ters) shows the bene t of modeling the variance of author abilities.

Only in the language translation task, the simple majority voting performed the best in terms of the correlation mea-sure. This is partly explained by Figure 6 showing the distri-bution of the correlations between the scores given by each reviewer and the ground truths. While the reviewer abilities widely distribute in the design task and the description task, those in the translation task skew to large positive values, which implies the majority of the reviewers are reliable.
The overall improvements in the nDCG@1 measure show the proposed model is good at nding the best artifact, which is a desirable feature in crowdsourcing-contest-style scenarios, where tasks are highly heterogeneous with un-structured response formats, and domain knowledge such as features and ground truths are not available.

Finally, we investigate the impact of changing the number of authors assigned to each task. Figure 7 shows the average correlations and nDCG@1 with varying number of authors for each task (with the number of reviewers xed at three). Again, in most of the cases, our proposed two-stage model with moderate numbers of authors outperformed the others. Note that nDCG@1 degrades with increased number of au-thors due to its de nition, since it becomes more difficult to nd the best one as the number of artifacts increases.
In summary, in terms of both the quality of overall ranking and the accuracy of nding the best artifact, we veri ed the effectiveness of our two-stage model, especially with a small number of reviewers and a moderate number of authors. Fi gure 6: Distributions of the correlations between each reviewer's scores and ground truth scores. The distribution in the language translation task skews to large positive values. Figure 7: Correlation and nDCG@1 between esti-mated quality scores and ground truth scores along with the number of authors per task. The number of reviewers per artifact is xed at three. In most cases, the proposed two-stage model outperforms the other three baselines with moderate numbers of authors per task.
A number of unsupervised methods were proposed for quality control of structured response format tasks. Most of them guarantee the quality by assigning each task to mul-tiple workers and by aggregating redundant answers. The simplest way to aggregate answers is taking majority votes, and it is used in various NLP tasks [18] and information re-trieval tasks [9]. Inspired by the seminal work of Dawid and Skene [5] who modeled the generative process of the answers of workers by introducing their ability parameters, many more sophisticated aggregation methods were proposed. To name a few, Whitehill et al. also included the difficulty of the task in the model [22], and Welinder et al. proposed a model considering workers compatibility for each task [21].
Lin et al. proposed an aggregation method for the tasks that deals with unstructured format responses [11]; how-ever, they targeted only the tasks where each answer possi-bly agrees to one of the others (e.g., arithmetic problems), and cannot be applied to such tasks we considered in this paper.

Other domain speci c quality estimation method for un-structured response format tasks were studied, for example, for language translation tasks [24]. Although they consider the similar problem as ours, their approach is specialized for translation task, and requires extensive domain knowledge including feature representation of artifacts and gold stan-dard scores for employing a supervised learning approach.
While our work addresses the parallel procedure, Dai et al. [4] proposed a quality control method for an iterative improvement procedure [12]. They offered review tasks of comparing artifacts before and after the improvement and proceeded to the next improvement task for a better en-hancement.

Reviewing processes of scienti c papers and commercial products have similar form to crowdsourcing with unstruc-tured output formats. There are several attempts to obtain appropriate review scores by correcting reviewer-dependent biases [7, 10]; however, the existing models do not include the creation process which we consider in this paper.
In this paper, we proposed an unsupervised statistical method to estimate the quality of the artifacts for a general crowdsourcing tasks with unstructured response formats. We proposed a two-stage model consisting of the creation stage, where multiple authors create their outputs for same tasks, and the review stage, where another set of workers review and grade the outputs. Our model introduced both the ability of each author and the bias of each reviewer, and modeled the process of grade label selection by reviewers by using the graded response model in the item response theory. We also proposed a simple iterative algorithm for the MAP inference of the true quality and model parameters. Experi-mental results showed the advantage of our two-stage model compared with some existing label aggregation methods, es-pecially when limited numbers of reviewers and authors are available, which implies that the proposed method can de-liver high-quality crowdsourcing results with lower costs.
Finally, we mention some possible future work. We em-ployed the absolute scoring in the review stage, that is, we asked each reviewer to assign grade labels. Instead, we can also use relative scoring such as pairwise ranking [3] by ask-ing which one of two given artifacts is better. Design of the review tasks is also an important open question. Although we requested the reviewers to evaluate randomly chosen ar-tifacts at once, showing artifacts from the same task may be an alternative method. Active selection of tasks and workers is also an important direction to pursue [17, 6, 23]. This research was supported by the Funding Program for World-Leading Innovative R&amp;D on Science and Technology (FIRST Program). [1] M. Bernstein, G. Little, R. Miller, B. Hartmann, [2] J. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, [3] X. Chen, P. N. Bennett, K. Collins-Thompson, and [4] P. Dai, Mausam, and D. S. Weld. Arti cial intelligence [5] A. P. Dawid and A. M. Skene. Maximum likelihood [6] P. Donmez, J. G. Carbonell, and J. Schneider. [7] P. A. Flach, S. Spiegler, B. Golenia, S. Price, [8] P. G. Ipeirotis. Analyzing the Amazon Mechanical [9] G. Kazai, J. Kamps, M. Koolen, and [10] H. W. Lauw, E.-P. Lim, and K. Wang. Summarizing [11] C. Lin, M. Mausam, and D. Weld. Crowdsourcing [12] G. Little, L. Chilton, M. Goldman, and R. Miller. [13] V. Ordonez, G. Kulkarni, and T. Berg. Im2text: [14] G. Rasch. Probabilistic models for some intelligence [15] V. C. Raykar and S. Yu. Ranking annotators for [16] F. Samejima. Estimation of latent ability using a [17] V. Sheng, F. Provost, and P. Ipeirotis. Get another [18] R. Snow, B. O'Connor, D. Jurafsky, and A. Y. Ng. [19] A. Sorokin and D. Forsyth. Utility data annotation [20] W. van der Linden and R. Hambleton. Handbook of [21] P. Welinder, S. Branson, S. Belongie, and P. Perona. [22] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and [23] Y. Yan, R. Rosales, G. Fung, and J. G. Dy. Active [24] O. F. Zaidan and C. Callison-Burch. Crowdsourcing
