 Name disambiguation in digital libraries refers to the task of attributing the publications to the proper authors. It is very common that several authors share the same name in a digital library. For instance, there are at l east 50 different authors who call  X  X ei Wang X  in DBLP and there are more than 400 entries under this name. At present, although DBLP provides some mechanisms to solve thi s problem already but wrong associations can still be found occasionally.

One may argue that we can use some existing techniques such as record linkages [1], duplication detection [2,3] and citation matching [4] for solving the name disambigua-tion problem. Unfortunately, this may not always be practical for all digital libraries. For example, in DBLP, we cannot access the full contents of most papers, unless we have joined those organizations (e.g. ACM and IEEE) who publish them. In this paper, we are interested in solving such kind of problem  X  disambiguate the authors in a digital library with only a limited amount of informa tion such as authors X  names and papers X  titles. Hence, Our problem is how to cluster the entries that belong to the same author based on the entries X  information only.

Broadly speaking, there are four different directions to solve this problem, namely, supervised learning [6], unsupervised learning [7], semi-supervised learning [5] and topic-based modeling [17]. However, supervised learning and topic-based modeling both require expert knowledge to label the data, which is very time consuming. On the other hand, the effectiveness of unsupervised learning method and semi-supervised learning both depend heavily on the data sel ection and the data preprocessing, which is difficult to guarantee reliable results are obtained.

In order to avoid obtaining unreliable results, such as in the case of unsupervised learning, we believe that having domain knowledge, such as in the case of supervised learning, is very important. In this paper, we try to obtain the domain knowledge auto-matically by constructing some term-based taxonomies. In each term-based taxonomy, each term is regarded as a node and all terms a re linked together according to their rela-tionships with each others. Thus, a graph mode l is resulted after a term-based taxonomy is built. Based on these graphs, we compute the similarity among entries by not just on how many common terms entries shared, but also on how the terms are related based on the graphs (i.e. domain knowledge). For computing the similarity, two models are proposed in this paper, namely, graph-base d similarity model and graph-based random walk model. The former model aims at computing the similarity among terms, whereas the later model aims at investigating how likely would a set of terms be transformed to another set of terms. Eventually, the similar entries will be gradually grouped together according to either of the models, such that each set of entries, ideally, belongs to a unique author. In other words, the authors X  names can be disambiguated.

To summarize, the main contributions of this paper are as follows: (1) Up to our knowledge, we are the first ones to apply t axonomy to perform name disambiguation in DBLP, even though taxonomy have long been used in text classification problem [17]; (2) We propose a generic clustering framework which is not only applicable on DBLP, but also can be applied to any other similar domains. This is very different from the existing works which are highly domain specific, therefore, attributes like co-authorship are not considered in this approach; (3) We propose how graph models can be integrated to the taxonomy for calculating whether two entries belong to the same author. This technique is not being reported elsewhere; (4) Our approaches do not need to obtain the domain knowledge manually as supervised approaches. In this paper, we propose two models, graph-based similarity model and graph-based random walk model, to be used together with the hierarchical clustering model to solve the name disambiguation problem. Both of the graph models rely on some graphs generated by some term-based taxonomies. 2.1 Name Disambiguation [7] proposes an unsupervised learning approach using K-way spectral clustering to solve the name disambiguation problem. Although this method is fast but it depends heavily on the initial partition and the order of processing each data point. Further-more, this clustering method may not work well when there are many ambiguous au-thors in the dataset. [6] proposes a supervised learning framework (by using SVM and Naive Bayes) to solve the name disambigua tion problem. Although the accuracy of this method is high but it relies heavily on the quality of the training data, which is diffi-cult to obtain. In addition, this method will assemble multiple attributes into a training network without any logical order, which may deteriorate the overall performance of the framework. [5] proposes a semi-supervised learning method which uses SVM to train different linkage weights in order to distinguish objects with identical names. In general, this approach may obtain sound resu lts. Yet, it is quite difficult to generate appropriate linkage weights if the linkages are shared by many different authors. [17] uses a topic-based model to solve the name disambiguation problem. Two hierarchi-cal Bayesian text models, Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA), are used. In gener al, it achieves better experimental results over the other approaches. Unfortunately, this approach is difficult to implement be-cause it requires too much labor intensive preprocessing (e.g. manually extract the first page of each paper) and relies on some special assumptions (e.g. the entries of each paper is clean and complete). In addition, an approach has been proposed in [24] that analyzes not only features but also inter relationships among each object in a graph format to improve the disam biguation quality which provides very good reference to our work, however, the attributes has been used in their approach like affiliation are not available in DBLP. 2.2 Taxonomy Building Taxonomy is a structure that is used to reflect the domain knowledge. A taxonomy is normally built base on terms. Building a term-based taxonomy generally involves two steps: terms extraction and terms linking. For terms extraction, some of the most popular techniques are: domain terminology extraction based on lexical cohesion mea-sure [8], traditional C/NC-value method combined with statistic measures of term frequency [9] and Yahoo! Term Extraction 1 [10]. Recently, an approach called PAT-Tree-based local maxima algorithm is proposed [11], which provides another efficient way to extract terms in a large text collection. In this paper, we adopt this latest algo-rithm for extracting terms from DBLP. The implementation details will be given in the next section.

For terms linking, the classical way to solve it is either to create a regular expres-sion list on the lexical level [12] or to use machine learning methods based on the co-occurrence of terms in text [13,14]. Recently, some studies [15,16] propose link-ing terms together by uncovering their semantic relationships. In this paper, we use the classical methods to solve the term linking problem. The reason is that semantic rela-tionships among terms cannot be easily obtained in DBLP. Certainly, our framework is independent of choosing whichever term linking method, and choosing classical method to solve it is for the shake of convenience. In this section, we will discuss our models in details, which include how the term-based taxonomy is built and how the two proposed graph models, graph-based similarity model and graph-based random walk model, are constructed. 3.1 Overview Recall that our goal is to cluster the papers w hich belong to the same author based only on the entries X  information (e.g. titles and authors). Given a paper titled  X  X n effective way to performing indexing in multimedia database X  and another paper titled  X  X -SQL: A quicker SQL query X , then even though they do not share any common terminology, we understand that these two entries both b elong to the database p aradigm because the terms such as  X  X ndexing X ,  X  X QL X ,  X  X uery X  and  X  X atabase X  are highly related to this area. Hence, we claim that simply using some string matching strategies to measure the similarity between two entries is not enough. We should include domain knowledge when we have to compute their similarity. Yet, obtaining domain knowledge is usually very expensive as it requires experts X  judgments.

In this paper, we try to obtain the domain knowledge automatically by construct-ing three different term-based taxonomies. In each term-based taxonomy, each term is regarded as a node and all terms are linked t ogether according to their relationships with each other. Hence, a graph model will be resulted naturally on each constructed term-based taxonomy.

Based on these three graphs, we compute the similarity between two entries by not just how many common terms they shared, but also how the terms are related to each others based on the taxonomy graph. For computing the similarity between two entries, two models are proposed: a graph-based s imilarity model and a graph-based random walk model. In short, graph-based similarity model calculates how similar two entries are by computing the similarity of the terms e xhibit in two entries according to the three taxonomy graphs, whereas the graph-based random walk model calculate the similarity by measuring how many steps that the term s in one entry need to  X  X alk X  to the terms in another entry.

Finally, the entries are gradually grouped t ogether based on a traditional hierarchical clustering algorithm. The stopping criteria of the clustering will be discussed in the later sections. 3.2 Term-Based Taxonomy Construction In our term-based taxonomy, we define term as follows: Definition 1 (Term). Each term, t k consists of at least one word, w i ,e.g. X  X ata X  X s a term and  X  X ata mining X  also is a term. t k = { w 1 , w 2 ... w n } , n  X  k; k is an input parameter that represents the number of words to make up a term.
 The ordering of words in a term is important because changing their orders may result in different meanings. Hence, the traditiona l bag-of-words representation, which means the words in a text is unordered, is not appropriated here [23]. In the followings, we will describe how we extract the terms and how they are linked together. Terms Extraction. All terms are extracted from the paper titles and the session titles (if the paper is from a conference and the conference contains session titles) in DBLP. We implement the extraction method called P AT-Tree-based local maxima algorithm [11] 2 .

Yet, in order to use this PAT-Tree-based local maxima algorithm, we need to define an association measurement first. In this pa per, we define an association measurement called LSCPCD , which is a modification of the traditional SCPCD [11] 3 : total number of terms in the set, and f ( t 1 ... t n ) is the frequency of unique left adjacent terms in the pool.
 Terms Linking. Once we have collected all terms, the next step is to uncover their relationships. In general, terms can be linked together based on their lexical similarity, their co-occurrences in texts, and some semantic rules. In this paper, we take the first two measurements only (i.e. without seman tic rules). This is because semantic infor-mation can hardly be obtained in our domain (DBLP text). Fig. 1 shows an overview of the taxonomy construction. Initially, we obt ain a seed taxonomy from Microsoft Libra Academic Search 4 which can roughly partition DBLP int o several distinct computer science disciplines. We iterate each term in the set of terms and identify their relation-ships against the seed taxonomy by using lexical similarity and term co-occurrence. For lexical similarity, two rules are applied: Definition 2 (String Contain Rule). Suppose W 1 is a set of words that belongs to term t and W 2 is another set of the words that belongs to term t 2 ;ifW 1  X  W 2 ,thent 1 and t 2 are very likely to refer to the same thing and they will be linked together. Definition 3 (String Similarity Rule). Suppose s 1 is a string representation of the term t , and s 2 is a string representation of the term t 2 ;ifS 1 and S 2 are very similar, then t 1 and t 2 are very likely to refer to the same thing and they will be linked together. The string contain rule is based on a hyper relationship in terms level, whereas the string similarity rule is based on a lexical level comparison. Now, the only problem remained here is that how we define  X  X imilar X  in the string similarity rule. There are many algorithms that aim at computing the similarity between two strings, such as soundex, edit distance, and longest common substring [20]. In this paper, we use Jaccard coefficient to do so because it considers not only the single longest common substring, but also the other common substrings in there, as it uses every character pairs for the string matching. Eventually, if the similarity between two terms is greater than a pre-defined threshold, then a link is established between these two terms.

Apart from the above two rules, from [13], two different terms may both be related to the same area frequently if they appear in the same paper title simultaneously. Hence, in this paper, we propose term co-occurrence rule: Definition 4 (Term Co-occurrence Rule). Suppose there are tot ally N paper titles, the same paper, t 1  X  P t and t 2  X  P t ,thent 1 ,t 2 co-occur together, and they will be linked together.
 According to our observations and our pre liminary studies, the accuracy to link terms together by using term co-occurrence (Definition 4) is generally lower than that of using lexical similarity (Definition 2 and Definition 3). The reason is that terms that co-occur in the paper title may usually mean different issues. For example, consider the paper ti-tled:  X  X iscovering a Term Taxonomy from Term Similarities Using Principal Component Analysis X . In this title,  X  X erm Taxonomy X  and  X  Principal Component Analysis X  refer to different issues but they co-occur together. In this paper, we are trying to demonstrate the feasibility of applying these rules to improve the retrieval results. Certainly, some rules may have side-effects. We will study this issue more in-depth in our future work. Finally, three taxonomies will be generated based on the above three rules (refer to Fig. 1 also). We named them as: (1) String Similarity Rule Taxonomy; (2) String Con-tain Rule Taxonomy; and (3) Co-occurrence Rule Taxonomy. Note that the relationships among terms are undirectional. Eventually, three undirected graphs will be generated where each vertex denotes a term and each link d enotes there is a relationship between two terms (vertex). 3.3 Taxonomy-Based Clustering Approach Once we have the taxonomies, we can use it as the source to perform hierarchical agglomerative clustering. The clusterin g process proceeds according to the following steps: (1) Initially, each paper represents a single cluster; (2) Compare clusters pair-wisely and decide if the two clusters with the highest similarity should be merged to form a new cluster based on a pre-defined t hreshold; (3) Repeat Step 2 until there are no more clusters that can be merged. Among these three steps, Step 2 is obviously the key step. In this paper, we try two different measurements to see if two clusters should be merged. The first one is called graph-based similarity model, which computes statis-tical similarity based on only the linking structure of the taxonomy, and the other one is called graph-based random walk model, which calculates the distance of two clusters with random walk algorithm [21].
 Graph-Based Similarity Model. Let G =( V , E ) be a graph, where V is a set of ver-texes and E is a set of edges. As we discussed in the previous sections, our term-based taxonomy is in fact an undirected graph with t k  X  V and the links among terms are edges. As we mentioned earlier, initially, each cluster contains one entry where an en-try is represented by a list of terms. In order to decide whether two clusters should be merged, we compute the overlapping of the two clusters based on the similarity of the terms in two clusters. Certainly, the similarity measurement is based on G . Specifically, we follow the existing approaches which use the well-known Dice Coefficient for mod-eling the similarity between two clusters, which calculate it by the weights of the edges: weights of the corresponding sets of edges. In our model, each edge has a weight of 1; W ( e 1 ... e n ) is the total weight of the set of edges linked by other terms to those terms that are shared by c and c . The set of terms that are shared by c and c can simply be and c , respectively. Finally, if Similarity ( c , c ) is greater than a pre-defined threshold parameter  X  ,then c and c will be merged together.
 Graph-Based Random Walk Model. The graph-based random walk model is a math-ematical formalization of a trajectory that consists of taking successive random steps [22]. In our term-based taxonomy, each cluste r is a DBLP entry, which is represented by a list of terms that can be found in the taxonomy. The basic idea of our graph-based random random walk model is to compute the pr obability of the terms from one cluster can  X  X alk X  to the terms in another clusters within a certain number of steps. We use this probability to denote the similarity between two clusters.

The probability is represented as a score manner such that the higher the score is, the more dissimilar is two clusters because more number of steps is involved for one cluster to walk to another cluster. Specifically, given a term-based taxonomy graph G =( V , E ) , let S ( v 1 ) be a set of vertexes that connect to the vertex v 1 , then the score from v 1 to another vertex v 2 is: where d ( v 1 ) is the number of degrees of v 1 (i.e. number of edges in v 1 ). Eq. (3) is a classical random walk model. Based on this equation, we extend it and apply its
Algorithm 1. GraphBasedRandomWalkModel( G ( V , E ) ,  X  ,  X  , c , c ) extension to measure the similarity between two clusters, c and c as follows: Eventually, we apply Eq. (4) to our model to see whether two clusters can be merged. Algorithm 1 outlines the major steps for this graph-based random walk model.
In Algorithm 1, we need to set up a parameter  X  in order to limit the maximum num-ber of walking steps for each iteration. In addition, the value of  X  has to be changed in different graphs (taxonomies) because the confi dences of linking two terms are different in different graphs, as we mentioned in the previous section. Quite obviously, the lower the confidence is, the higher value  X  should be as there are more links existing from each vertex. It is worth noting that a larger taxonomy usually requires more walking steps for a term to walk to another term. Considering the size of taxonomy, the size of String Contain Rule Taxonomy is smaller than that of String Similarity Rule Taxonomy and the size of String Similarity Rule Taxonomy is smaller than that of Co-occurrence Rule Taxonomy, Thus, in this paper, we have the following settings according to our preliminary testing and observations: (1) For the String Contain Rule Taxonomy,  X  =  X  ; (2) For the String Similarity Rule Taxonomy,  X  = 1 . 25  X  ; and (3) For the Co-occurrence Rule Taxonomy,  X  = 1 . 5  X  . In our model, by default, we set the value of  X  is 1. These parameters are tuned based on some empirical studies. The sensitivity of the parameters will be left as our future work. In our experiments, we selected ten common names from the DBLP. These names are shown in Table 1. We then generate ten datasets, each of which contains entries that appeared in DBLP from one of these authors. We manually checked each of the dataset through Google, read the information in the papers, and sent e-mails to the authors to validate whether the datasets are correct. We removed those entries that are ambiguous and some authors who only have one entry are also removed compared to the datasets in [5]. Hence, the datasets that we generat ed are clean and can be used for evaluating our proposed work.

For the term-based taxonomy, we extracted terms from over 400K paper titles, and we only considered the terms that contai ned up to three words according to our ob-servations and the general length of a paper title. Punctuation, numbers and stopwords are removed. There were three taxonomies that were generated from seed taxonomy against four linking decision rules. Following the standard evaluation process, we use precision to evaluate our model: where PC is the number of pairs of entries being clustered correctly and PIC is the number of pairs being clustered incorrectly. In the following sections, we use this mea-surement to evaluate the performances of our graph-based similarity model and graph-based random walk model. Note that we will us e the traditional string match hierarchal agglomerative clustering method as a baseline method for comparison. 4.1 Evaluation of Graph-Based Similarity Model In this section, we are going to evaluate the graph-based similarity model. As described in the earlier sections, when the similarity between two clusters that is calculated by the intersection of clusters in the graph which means the set of terms in the taxonomy have linking with both clusters. For the case where exact terms match, cluster A and cluster B both have term X in their terms lists, for example. We make a rule that states that two clusters can be merged when at least N% terms in one cluster X  X  term list are matched in the other cluster X  X  term list, and if the percentage of term matches is lower than N%. Thereafter, the merge will be based on the similarity between clusters. For evaluation purpose, we set up three levels, namely, 10%, 20%, and 30%. The criterion also applies to the random walk model and the baseline method.

In this model, there is a threshold to control if the similarity of two clusters is enough to perform clustering. As shown in Fig. 2(a) and Fig. 2(b), there are two thresholds in the evaluation, 0.5 and 0.7. Thus, a higher threshold means that the higher similarity between clusters is required. From the data chart, we know that mean precision values over 70% have been reached in both thresholds when the terms match level is 20%. We also note that in the case of authors  X  X un Zhang X  and  X  X ei Li X , the precision values are very low if the terms match level is 10%, which means t hat authors often use same terms in their paper titles although they are not the same people. However, the precision values in the 10% terms match level are generally better than the other two levels for most other authors because a lower level can make the merge process easier. The same scenario exists for two thresholds; the model performs slightly better when the threshold is 0.5. 4.2 Evaluation of Graph-Based Random Walk Model For this model, we set the process that will run up to ten steps when we try to link the term from one cluster to the other. In addition, there is a need for a total score to check if two clusters can be merged. The total s core for each linking process between two clusters will be the number of taxonomy mu ltiple by the size of t erm list multiple by the max numbers of steps. If the score of one cluster link to the other cluster is less than a certain threshold of the total score in this model, then we merge these two clusters together. In the evaluation, we evaluate two thresholds, 3% and 5%.

From Fig. 3(a) and Fig.3(b), we can see the same problem occurs in the random walk model as well for authors  X  X un Zhang X  and  X  X ei Li X  when the terms match level is 10%. The mean values are higher at about 80% compared to the graph-based similarity model because this model does not only look at the neighboring linking terms of the cluster X  X  term list in the graph, but also has the probability to run through the graph to get a term start from term list up to ten edges. Hence, this model will cover a greater range of the taxonomies. The performance of the 5% threshold is worse than that of the 3% threshold because more walking steps ar e allowed for each linking process between the two clusters which increases the probability of errors. 4.3 Comparisons with Baseline Method This section compares the two models with the baseline method that only checks terms match among clusters without applying taxonomy. Fig. 4(a) shows the precision of the baseline method, and the mean values in all levels of terms match are lower than 70%. Figure 4(b) shows the precision of all three methods, and we pick the mid level of 20% terms match to compare. As we can see from Fig. 9, the graph-based random walk model performs better than the others and the mean value is about 15% over the baseline method. The mean value of the graph-based similarity model is also acceptable and is about 10% over that of the base line method. This paper describes a clustering appro ach for name disambiguation in DBLP based on term-based taxonomies. By utilizing the gr aph-based similarity model and graph-based random walk model and combining the m with general hierarchal agglomerative clustering, the approach efficiently classifies authors and generates sound results than baseline method. The approach is only based on internal references in DBLP, which avoids the problems in other previous clustering approaches, manually label and extract data for example. Last but not least, the taxonomies we built are generic and can be applied to other digital libraries or can be used for other purposes, examples of which include paper ranking and social network discovery.

