 Liang Wang, Christopher Leckie, Kotagiri Ramamohanarao, and James Bezdek As an exploratory data analysis tool, clustering aims to group objects of a similar kind into their respective categories (se e [1] for a comprehensive survey). Given adataset O comprising n objects { o 1 ,o 2 ,  X  X  X  ,o n } , (crisp) clustering partitions the data into c groups G 1 ,G 2 ,  X  X  X  ,G c ,sothat G i  X  G j = X if i = j and G 1  X  G clustering [2], present an appealing alternative to traditional central grouping techniques (such as K -means), because 1) they are applicable to situations in which the objects are not naturally representable in terms of feature vectors; and 2) they avoid the assumption that all examples in a cluster must be close to a prototype. This means they are amenable to irregular-shaped clusters.
Spectral clustering algorithms usually rely on the eigendecomposition of a n  X  n similarity matrix (where n is the number of examples), which generally takes O( n 3 )timeandO( n 2 ) space complexity. In addition, to obtain such a sim-ilarity matrix it is necessary to compare all possible pairs of examples, which is computationally expensive for a large data set. These limitations make spec-tral clustering methods impractical (or computationally infeasible) when han-dling large data sets. Additional strategies are thus required to adapt to growing data sizes while maintaining both cluster quality and speed. A spectral group-ing approach based on the Nystr  X  om approximation was proposed in [3], which first solves a small-scale eigendecomposition problem on randomly chosen sam-ple data, then computes approximated eigenvectors via extrapolation. Another incremental spectral clu stering algorithm was proposed to handle  X  X ynamic X  evolving data in [4] by introducing the incidence vector/matrix. In [5], the spec-tral clustering algorithm is parallelized across distributed machines. However, these two methods either sacrifice accura cy or require distributed computing infrastructure. In contrast, this paper considers approximate spectral clustering (ASC) for  X  X tatic X  data without the use of distributed computation.

The motivations of approximate spectral clustering can be described as fol-lows. When the data set is large and unloadable on the available computing platform, ASC provides an approximate solution to clustering (i.e., making clus-tering feasible), whereas it is impossible to use a literal clustering approach in batch mode on such data. If the data set is small, medium, or merely large but still loadable, then ASC may offer an approximation comparable to the literal so-lution but at a significantly reduced computational cost. In summary, the benefits of an approximate clustering scheme are  X  X easibility X  for very large data sets and  X  X cceleration X  for manageably-sized data sets. To this end, this paper proposes two new methods for approximate spectral clustering, as well as the examination of an existing method described in [3]. One of them is based on matrix approx-imation, and the other uses a  X  X ampling plus extension X  approach. Our major contributions are as follows: 1) we prese nt two different schemes for approximate spectral clustering; 2) we provide a comprehensive quantitative comparison of several approximate spectral clustering algorithms, together with a comparison of several sampling schemes; and 3) we prov ide extensive experimental results on synthetic and real data sets, and several meaningful conclusions are highlighted.
The rest of this paper is organized as follows. Section 2 gives a brief review of spectral clustering. Section 3 details se veral approximate algorithms. Section 4 introduces four kinds of sampling schemes . The results are presented in Section 5, prior to discussion and conclusion in Section 6. Spectral methods for clustering, e.g., n ormalized cut [6] and max-min cut [7], are based on the eigenvectors and eigenvalues of a symmetric positive semidefinite (SPSD) matrix of size n  X  n derived from a given data set. Let the symmetric W  X  X  n  X  n denote the weighted adjacency matrix for a graph G =( V , E )with nodes V representing the n objects in O to be analyzed and edges E whose weights capture pairwise affi nities between objects. Let D be a diagonal matrix with entries D ii = d i ,where d i = j W ij denotes the degree of the i th node, then the graph Laplacian matrix is defined as L = D  X  W [8].

Let C 1 and C 2 be a bipartition of V , i.e., C 1  X  C 2 = X and C 1  X  C 2 = V ,andthe volume of a set as the sum of the degrees within the set, i.e., vol( C j )= i  X  C j d i .
The normalized cut between sets C 1 and C 2 is defined as [6] where denotes the harmonic mean and cut( C 1 ,C 2 )= i  X  C 1 ,j  X  C 2 W ij .To minimize (1), Shi and Malik [6] showed that an approximate solution may be obtained by thresholding the eigenvect or corresponding to the second smallest eigenvalue of the normalized Laplacian matrix L , i.e., The matrix L is positive semidefinite, even when W is indefinite.

Extensions to multiple groups are possible, e.g., using multiple eigenvectors [9]. In this work we adopt this approach by computing the leading eigenvectors V from ( D  X  1 / 2 WD  X  1 / 2 ) V = V X   X  . These eigenvectors induce an embedding of the objects in a low-dimensional subspace, in which the K -means clustering algorithm is then used to discover final partitions by grouping columns of V . To address the complexity of sp ectral decomposition for large n , we now outline several approximate approaches to the problem. Spectral clustering generally deals with an n  X  n SPSD matrix, say M , which can be decomposed as M = U X U T with  X  the eigenvalues of M and U the associated eigenvectors. Suppose m n columns of M are sampled without replacement. Let A be the n  X  m matrix of these sampled columns, and S be the m  X  m matrix consisting of the intersection of these m columns with the corresponding m rows. Without loss of generality, we can rearrange the columns and rows of M such that where B  X  X  m  X  ( n  X  m ) contains the elements from the samples to the rest of the objects, and C  X  X  ( n  X  m )  X  ( n  X  m ) contains the elements between all of the remaining objects. In the case of m n , C is usually large. 3.1 nSPEC The Nystr  X  om approximation has recently been studied in the machine learning community, e.g., for fast approximate Gaussian process classification [10] and low-rank approximation to the kernel matrix [11]. A spectral grouping method based on the Nystr  X  om approximation was proposed for image segmentation in [3]. We refer to this as nSPEC (N ystr  X  om-based Spe ctral C lustering). The Nystr  X  om approximation uses S and A to approximate M as where  X + X  is the pseudoinverse. The Nystr  X  om approximation models C by BS + B T , and the resulting approximate e igenvalues and eigenvectors of M are where S = U S  X  S U + S [10].

The eigenvectors generated from the Nystr  X  om approximation are not exactly orthogonal because they are extrapolated from the eigenvectors of S ,losing orthogonality in this process. If S is positive definite, Fowlkes et al. [3] used a one-shot method to solve for the orthogonalized approximate eigenvectors. can be diagonalized as  X  M =  X  U X  Q  X  U T with 3.2 cSPEC An alternative column-sampling technique has been analyzed in the theoretical computer science community [12], which can also be used to approximate spectral decomposition of a large matrix using a subset of columns. The column-sampling method was initially introduced to approximate SVD (Singular Value Decompo-sition) for any rectangular matrix [12,13]. However, it has not yet explicitly been used in spectral clustering. Here we us e it to approximate the spectral decompo-sition of M , and call the resulting method cSPEC (C olumn-sampling Spe ctral C lustering).

The column-sampling technique approximates the eigendecomposition of M by using the SVD of A directly. Suppose A = U A  X  A V T A , then the approximate eigenvectors and eigenvalues of M are given by the left singular vectors of A and the corresponding scaled singular values, i.e., Accordingly the approximation of M can be written as [14] which has a very similar form to (4). When n is very large, the SVD on A directly is still quite demanding. Fortunately, we have It is thus easy to obtain U A and  X  A by computing the SVD on A T A  X  X  m  X  m . Relatively, A T A can be easily computed even for large n . 3.3 eSPEC Another way to attack the scalability problem is  X  X xtensibility X  (e.g., [15,16]). An extended scheme applies a clustering algorithm to a representative sample set, and then extends the sample result to obtain (approximate) clusters for the re-maining data. Also, the extended cluster ing schemes can be effectively applied in cases in which data are coll ected sequentially. Here we propose a  X  X ampling, clus-tering plus extension X  solution, called eSPEC (E xtensible Spe ctral C lustering), for approximate spectral clustering.

After the sample data S is obtained, we first use a literal spectral clustering al-gorithm to group them. Next we address the problem of out-of-sample extension, i.e., to assign each of the remaining n  X  m objects to one of the c previously de-termined groups. We regard the whole m  X  n matrix (i.e., A T ) as a semantically-meaningful vectorial representation ( i.e. , each object has m attributes, each of which corresponds to a similarity relation between the object and one of the m sample objects, leading to a virtual data set { x i } n i =1 ). For learning the cluster-preserving embedding space from S , we adopt computationally-efficient locality preserving projection (LPP) [17], which is very similar to the mapping procedure used in spectral clustering algorithms.

Let G S denote a graph with m nodes corresponding to the m labeled samples { x to k -nearest neighbors ( k  X  X  ). Let W S be a symmetric m  X  m matrix, whose element W ij S is the weight of the edge joining nodes i and j , and is 0 if there is no such edge ( W S is thus sparse). To obtain the embedding space, we solve the generalized eigenvector problem where L S and D S are the corresponding Laplacian matrix and diagonal degree matrix of G S . Let the column vectors f 1 ,  X  X  X  ,f l be the solutions of the eigenvec-the embedding of x i in the l -dimensional sp ectral space is represented as Out-of-sample extension can then be treated as a prediction problem in this embedding space. For each x e j ( j = m +1 ,m +2 ,  X  X  X  ,n )in B to be extended, we use F to project x e j to y e j in the learned embedding space. Together with the embedding { y S j } m j =1 of the m labeled samples o S j ,weusethe k -nearest neighbor classifier to assign the object o e j to the class label with the maximum votes from its k nearest neighbors measured in the spectral domain. How to effectively sample a small set of re presentative samples is critical to encoding the structure of the whole data set. In this work, we focus on the following four sampling schemes in our empirical comparison:  X  Random sampling (RS) uses a uniform distribution { p i =1 /n } n  X  Selective sampling (SS) [18] was shown to be superior to progressive sampling  X  K -means sampling (KS) suggested in [11] simply chooses the sample points  X  Probabilistic sampling (PS) [13] uses the probability distribution { p i } n In order to test these approximate algorithms, we carried out a number of ex-periments on several artificially generated and real-world data sets. Unless oth-erwise mentioned, in the following experiments the (Euclidean) distance matrix was computed in the original attribute space, which was then transformed to the affinity matrix by the Gaussian function, i.e., W i,j =exp(  X  o i  X  o j 2 / 2  X  2 ). The number of clusters c was chosen manually, since choosing c is a difficult model-selection problem which lies outside of the scope of this work. All experiments were implemented in a Matlab 7.2 environment on a PC with an Intel 2.4GHz CPU and 2GB memory running Windows XP.

An accuracy metric AC has been widely used for clustering performance evaluation [4,19]. Suppose that l c i is the clustering label of object o i and l g i is  X  ( l 1 ,l 2 ) is the delta function that equals 1 if and only if l 1 = l 2 and 0 oth-erwise, and map is the mapping function that permutes clustering labels to match equivalent ground-truth labels. The Kuhn-Munkres algorithm is usually used to obtain the best mapping [20]. I n addition to accuracy, we also measure computational efficiency. For each experiment, we performed these approximate algorithms multiple times, and reported results in terms of the average accuracy (AAC) and the average computation time (ACT). Note that our programs have not been optimized for run-time efficiency. 5.1 Results on Synthetic Data Sets We begin with four synthetic data sets of different types and sizes (i.e., 3Gaus-sian, 4Line, 2HalfMoon, and 3Circle), whose scatter plots are shown in Figure 1, in which each color represents a cluster. The  X 3Gaussian X  is a simple case in which even central grouping techniques can perform well. The later three cases are generally hard for central grouping, but easy for spectral clustering. These synthetic 2D data sets are relatively small so that we can perform both literal clustering and approximate clustering to measure the approximation error. We include these progressively harder data sets to test these approximate algorithms, though these synthetic cases are no t necessarily realistic in practice.
First we performed literal spectral clu stering on these data sets, and ob-tained fully correct clustering results at the computational cost of about 11s for  X 3Gaussian X , 15s for  X 4Line X , 27s for  X 2HalfMoon X  and 27s for  X 3Circle X . Then, we applied eSPEC, nSPEC and cSPEC. We tried multiple sampling rates (i.e., m/n ) 1 . For each sampling rate, 50 trials were made, then we computed the av-erage clustering accuracy and the aver age computation time consumed by the clustering procedure. The results for t he AACs and ACTs are respectively shown in Figures 2 and 3, from which it can be seen that:  X  The approximate algorithms can achieve a good approximation, sometimes  X  For complex-shaped clusters (e.g., 2HalfMoon and 3Circle), more samples  X  For simple data sets such as 3Gaussian and 4Line, the three algorithms  X  In terms of accuracy, nSPEC performs the best overall, then eSPEC, and  X  In terms of overall accuracy, SS per forms best, then KS, and finally RS and These numerical experiments on synthetic data sets suggest that the strategy of approximate spectral clustering can obtain comparable results to the literal approach but in much less time. In addition, overall nSPEC performs best but is most expensive in time. SS achieves the best tradeoff between accuracy and computation efficiency. Considering that synthetic data sets with controllable structures are designed only for simulation and are not realistic, real-world data sets with unknown data distributions would speak louder. Therefore, we will further evaluate these algorithms on several real data sets. 5.2 Results on Real Data Sets We first considered two medium-sized da ta sets. 1) The multiple features (MF) data set from the UCI consists of binary image features of handwritten numerals ( X 0 X   X   X 9 X ) extracted from a collection of Dutch utility maps. Each class has 200 patterns, thus there are n = 2000 patterns in total. These digits are represented as a 649-dimensional vector in terms of 6 feature sets. We set the number of clusters c = 10 corresponding to 10 different numerals. 2) The Yale-B face data set 2 contains single light source images of 10 individuals, each seen under 585 viewing conditions (9 poses  X  65 illumination conditions) [21]. Hence, the total number of images is n = 5850. Each original image was down-sampled to 30  X  40 pixels, leading to a 1200-dimensional vector representation. We set the number of clusters c = 10 corresponding to 10 different subjects.

For each data set, we applied the algorithms 25 times for each of several sampling rates with the selective sam pling scheme. The AACs and ACTs are summarized in Table 1, where ane means the average number of examples for each cluster in the sample set. Table 1 shows that 1) the computation time of nSPEC is most expensive, then cSPEC, and finally eSPEC, which is consistent with the results on the synthetic data sets. 2) On the MF data set, nSPEC obtained the best accuracy, then eS PEC, and finally cSPEC, which is basically consistent with the results on the synthetic data sets (with similar data sizes). However, it is interesting to see that on the Yale-B data, cSPEC performed better than the other two algorithms. Note that cSPEC performs SVD on a larger submatrix of M than does the Nystr  X  om method ( A n  X  m versus S m  X  m ). This could be a reason why cSPEC performs better than nSPEC in such a relatively large real-world problem.

We also applied these clustering algorithms to the problem of high-resolution image segmentation (where it is generally infeasible to use literal spectral clustering). Differ ent features (such as intensity, color, texture, and proximity) can be used to compute the similarities between image pixels, e.g., locally-windowed color and texture histograms were used in [3]. We just used the intensity feature since our main concern is to demonstrate the feasibility of these approximate algorithms in the context of image segmentation, but not purely for image segmentation. Figure 4 shows segmentation results on three 481  X  321 images 3 , in which pixels with the same color represent one group. We set c = 3 (or 4) for these images according to the number of visually meaningful components.

Running a literal spectral clustering algorithm on the whole image (which con-tains n = 481  X  321 = 154 , 401 pixels) would be simply impossible in the Matlab environment. For these images, the number of sampled pixels was empirically chosen to be 150 (less than 0 . 1% of the number of total pixels), considering that there are far fewer coherent groups ( i.e., c n ) in a scene than pixels. We can-not measure the clustering error in this c ase because literal sp ectral clustering cannot be performed and we lack any form of ground truth. So, the best we can do for evaluation here is to resort to vis ual inspection of the segmentation re-sults. In these three cases, all algorithms partitioned the images into meaningful components when c = 4 regardless of slight differen ces. More interesting, when c = 3, nSPEC gave results that were inconsistent with human perception of the intensity values in the images (i.e., a tendency to over-segmentation), whereas cSPEC and eSPEC performed similarly well. This seems to demonstrate again that cSPEC could be superior to nSPEC on these larger image data sets. This paper has examined several approximate spectral clustering approaches. Extensive experiments on synthetic and real-world data sets show that these algorithms are not only feasible for very large data sets, but also provide ac-celeration on large data sets with compar able accuracy comp ared to the literal solution. In particular, in terms of memory, the matrix (i.e., A ,or S and B ) needed in these three approximate al gorithms can be simply computed on de-mand. This greatly reduces the memory requirements for very large-scale problems.

Accuracy and efficiency are two important factors in data clustering. Com-parative results on the synthetic data sets have shown that nSPEC performs best in terms of accuracy, but this is not always the case in real-world data sets. We cannot thus say that a specific algorithm is always superior to the others. The computation time of nSPEC is consistently highest among all of the three algorithms, which may be due to its additional strategy for computing approxi-mated orthogonalized eigenvectors. Relatively, eSPEC is cheapest, which makes out-of-sample extension more appealing when a large number of samples have been accumulated. To summarize, just as discussed in [1], there is no clustering algorithm that can be universally used to solve all problems. It is more likely that the performance of each clustering method depends strongly on the real characteristics of the data sets used. In this sense, it is not rational to claim a  X  X est X  in the context of clustering algorithms, though comparison on a wider va-riety of data is possible. However, amon g the four compared sampling schemes, selective sampling provides the best ch oice in terms of accuracy, efficiency and applicability for various types of data.

