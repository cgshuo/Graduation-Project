 We consider a model in which background knowledge on a given domain of interest is available in terms of a Bayesian network, in addition to a large database. The mining prob-lem is to discover unexpected patterns: our goal is to find the strongest discrepancies between network and database. This problem is intrinsically difficult because it requires in-ference in a Bayesian network and processing the entire, po-tentially very large, database. A sampling-based method that we introduce is efficient and yet provably finds the ap-proximately most interesting unexpected patterns. We give a rigorous proof of the method X  X  correctness. Experiments shed light on its efficiency and practicality for large-scale Bayesian networks and databases.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms, Experimentation, Performance Bayesian Networks, Association Rules, Sampling
The general task of knowledge discovery in databases (KDD) is the  X  X utomatic extraction of novel , useful, and valid knowledge from large sets of data X  [5]. However, most data mining methods  X  such as the Apriori algorithm [1]  X  are bound to discover any knowledge that satisfies the chosen usefulness criterion, including (typically very many) rules that are already known to the user. Tuzhilin et al. [20, 15, 16] have studied the problem of finding unexpected rules relative to a set of rules that encode background knowledge. Bayesian networks provide not only a graphical, easily in-terpretable alternative language for expressing background knowledge, but they also provide an inference mechanism; that is, the probability of arbitrary events can be calcu-lated from the model. Intuitively, given a Bayesian net-work, the task of mining interesting unexpected patterns can be rephrased as discovering itemsets in the data which are much more  X  or much less  X  frequent than the background knowledge suggests.

An algorithm which performs this discovery task exactly has been discussed by Jaroszewicz and Simovici [10]. This approach, however, incurs two intrinsic problems. The first problem is the necessary exact inference in the Bayesian network  X  which is known to be a very hard problem. The second is the necessity to process the entire database in order to assess all patterns and identify the most interesting ones.
Inference can be approximated by sampling from the joint distribution of the network; evaluating the interestingness of patterns can be approximated by evaluating patterns on a random sample instead of the entire database. However, a discovery algorithm which follows this approach cannot come with the guarantee of finding the patterns which ac-tually maximize the interestingness criterion. We devise an efficient sequential sampling algorithm which approximates the inference in the network and draws a sample from the database in such a way that the resulting interesting at-tribute sets are, with high probability 1  X   X  , the most inter-esting ones up to some small difference in interestingness  X  , where  X  and  X  are user-adjustable parameters.

Bayesian networks are widely used in practice; for in-stance, Volkswagen uses a Bayesian model in their produc-tion planning and scheduling system [13]; it contains 250 nodes with between 2 and 50 possible values. Finding dis-crepancies between a huge model and the (very large) trans-action database is a difficult and relevant problem.
The rest of this paper is organized as follows. We discuss related work in Section 2 and introduce our framework and notation in Section 3. In Section 4, we present the sampling algorithm for finding frequent itemsets that are most inter-esting relative to background knowledge and our main result on its correctness. We study the behavior of this algorithm empirically in Section 5. Section 6 concludes.
The Apriori algorithm [1] finds frequent itemsets and, suc-ceedingly, all sufficiently confident rules over these itemsets. Many measures of interestingness of rules and itemsets have been discussed ( e.g., [12, 2, 9, 18]). A fundamental short-coming of all of these interestingness measures is that neither of them takes into account whether discovered knowledge is entailed by previously available background knowledge.
Some algorithms take background knowledge expressed as rules into account during the discovery process [20, 15, 16]. These methods assess the unexpectedness of a new pat-tern given the background rules on a purely syntactic basis, without inference. That is, they discover rules that are un-expected given the background rules, rather than rules that are unexpected given what can be inferred from the back-ground rules. For instance, if  X  A  X  B  X  and  X  B  X  C  X  were known, then  X  A  X  C  X  would still be considered an unex-pected pattern. A more detailed discussion of data mining with background knowledge can be found in [10].

Bayesian networks are a powerful representational scheme for background knowledge; they are graphical models, easy to understand and modify. Bayesian networks encode the joint distribution over all attributes. Inference mechanisms are well understood; general inference in Bayesian networks is a hard problem that can be approximated by sampling ( e.g., [11]). The inference problem given values of some random variables is approximated by MCMC methods [6]. Jaroszewicz and Simovici [10] define interestingness of a pat-tern as difference between observed frequency and inferred probability. Their algorithm finds all itemsets whose in-terestingness is above  X  but, unfortunately, relies on exact inference  X  which is tractable for small networks.
It is straightforward to replace the exact inference by sampling-based approximate inference, but we would like to do this in such a way that the algorithm X  X  output maintains a well-defined optimality property. Sampling algorithms [21] estimate the interestingness of patterns by drawing examples from the database. The most elementary sampling schemes [21] calculate worst-case sample bounds, often based on Ho-effding X  X  inequality. The sample size necessary for a desired  X  /  X  level of optimality can be reduced substantially by em-ploying data-dependent , sequential sampling [3, 14]. Here, the data are processed incrementally; the necessary sample size is determined online, dependent on characteristics of the data that have already been processed.

Practical sequential sampling algorithms have been stud-ied for interestingness functions which are averages over the data (such as accuracy) [7, 4, 8] as well as for more gen-eral interestingness functions [19]. These algorithms mini-mize the number of database accesses needed to find, with high probability, all approximately sufficiently interesting, or the n most interesting patterns. Unfortunately, all of these methods assume that the bottleneck in the assessment of candidate patterns lies only in the database access. By contrast, in our problem setting we need to manage uncer-tainty originating from limited database access as well as uncertainty originating from approximate inference in the Bayesian network.
In this section, we introduce the necessary notation and define the problem that we will solve in the following.
We are given a database D with attributes Z = { A 1 , . . . , A m , } ; attributes are categorical with finite do-mains Dom( A ). P D I ( i ) denotes the probability that an at-tribute set I  X  Z assumes a vector of values i in the database D .

A Bayesian network BN is a set of random variables (corresponding to our attributes) Z = { A 1 , . . . , A m constitute the vertices of a directed, acyclic graph, a set of edges E  X  Z  X  Z , and, for each vertex A i with direct ancestors par ( A i ), a conditional distribution P A i | par ( A A Bayesian network defines a joint distribution P BN Z = Q
Given an attribute set I and values i , we write P BN I ( i ) to denote the probability of itemset I = i as determined by a Bayesian network BN . According to [10], we define the unexpectedness, or interestingness, of an event I = i as I ( I, i ) = | P D I ( i )  X  P BN I ( i ) |  X  the absolute difference between an event X  X  probability inferred from the network, and observed in the database. We will now leverage the definition of unexpectedness of events to interestingness of attribute sets : an attribute set I is interesting, if there is an event I = i for which inferred and observed probability diverge.
 Definition 1. Given a Bayesian network BN and data D , the interestingness of attribute set I is defined in Equa-tion 1.
 The definition of interestingness refers to P BN I ( i ), the exact probability of I = i inferred from the network, and P D I the probability of I = i in the (potentially very large) data-base. P BN I ( i ) can be estimated by sampling from the net-work; P D I ( i ) by sampling from the database. Since the net-work has no cycles we can always draw from the conditional distributions P ( A i | par ( A i )) of each vertex A i after the val-ues of all parents have been drawn. Thus we obtain a sample S
BN of independent assignments of values to the attributes according to P BN . After additionally drawing records S D from D independently under uniform distribution, we obtain an estimate  X  I ( I, i ) as in Equation 3, and of  X  I ( I ) in Equation 4.  X  P D I ( i ) is the relative frequency of I = i in sample S and  X  P BN I ( i ) the relative frequency of I = i in S BN A special case occurs when the Bayesian network is too large for exact inference but the database is compact and P D I be determined exactly. In this case, only P BN I has to be approximated by  X  P BN I , but S D can be the entire database D and therefore  X  P D I = P D I .

A possible problem setting would be to find all attribute sets whose interestingness exceeds some  X  . However, from the user X  X  point of view it is often more natural to constrain the number of returned patterns, rather than the somewhat less intuitive interestingness value. We therefore define the n most interesting attribute sets problem as follows. Definition 2. Let D be a database over attributes Z and BN a Bayesian network. The n most interesting attribute sets problem is to find n attribute sets H = { I 1 , . . . , I I  X  Z , such that there is no other attribute set I 0 which is more interesting than any of H (Equation 5). Any solution to the n most interesting attribute sets problem has to calculate the I ( I ) which requires exact inference in the Bayesian network and at least one pass over the entire database. We would like to find an alternative optimality property that can be guaranteed by an efficient algorithm. We therefore define the n approximately most interesting attribute sets problem as follows.
 Definition 3. Let D be a database over itemsets Z and BN a Bayesian network. The n approximately most in-teresting attribute sets problem is to find n attribute sets H = { I 1 , . . . , I n } ; I j  X  Z , such that, with high probabil-ity 1  X   X  , there is no other attribute set I 0 which is  X  more interesting than any of H (Equation 6).
We are now ready to present our solution to the n approx-imately most interesting attribute sets problem . The Apri-oriBNS algorithm is presented in Table 1; it refers to con-fidence bounds provided in Table 2. We will now briefly sketch the algorithm, then state our main Theorem, and finally discuss some additional details and design choices. AprioriBNS generates candidate attribute sets like the Apriori algorithm does: starting from all one-element sets in step 1, candidates with i +1 attributes are generated in step 2g by merging all sets which differ in only the last element, and pruning those with infrequent subsets.

In each iteration of the main loop, we draw a batch of database records and observations from the Bayesian net-work. Only one such batch is stored at a time and the sam-ple size and frequency counts of all patterns under consid-erations are updated; the batch is deleted after an iteration of the loop and a new batch is drawn. The interestingness of each attribute set I is estimated based on N BN ( I ) obser-vations from the network and N D ( I ) database records.
There are two mechanisms for eliminating patterns which are not among the n best ones. These rejection mechanisms are data dependent : if some attribute sets are very uninter-esting, only few observations are needed to eliminate them from the search space and the algorithm requires few sam-pling operations. Step 2c is analogous to the pruning of low support itemsets in Apriori. P D I ( i ) can be interpreted as the support of the itemset I = i with respect to the database, and P BN I ( i ) as the support of I = i with respect to the net-work. The interestingness  X  which is the absolute difference  X  can be bounded from above by the maximum of these. No superset of I can be more frequent than I and therefore all supersets can be removed from the search space, if this up-per bound is below the currently found n -th most interesting attribute set. Since only estimates  X  P BN I ( i ) and  X  P known, we add a confidence bounds E I and E s to account for possible misestimation.

The pruning step is powerful because it removes an entire branch, but it can only be executed when an attribute set is very infrequent. Therefore, in step 2d, we delete an attribute set I 0 if its interestingness (plus confidence bound) is below that of the currently n -th most interesting pattern (minus confidence bound). We can then delete I 0 but since interest-Table 1: AprioriBNS: Fast Discovery of the Approx-imately Most Interesting Attribute Sets Input: Bayesian network BN , database D over attributes Z , approximation and confidence parameters  X  and  X  , the desired number of interesting itemsets n . 1. Let i  X  1 (iteration); generate initial candidates 2. Repeat until break: 3. Return the n best itemsets (according to  X  I ) from H i ingness does not decrease monotonically with the number of attributes, we cannot prune the entire branch.

There are two alternative stopping criteria. If every at-tribute set in the current set of  X  X hampions X  H  X  i (minus an appropriate confidence bound) outperforms every attribute outside (plus confidence bound), then the current estimates are sufficiently accurate to end the search (step 2e). This stopping criterion is data dependent : If there are hypothe-ses which clearly set themselves apart from the rest of the hypothesis space, then the algorithm terminates early.
In addition, the algorithm may terminate if all estimates are tight up to  X  2 . This worst-case criterion uses bounds which are independent of specific hypotheses ( data indepen-dent ) and a fixed amount of allowable error is set aside for it. Its purpose is to guarantee that the algorithm will always terminate. As long as the candidate set C i is not empty, there are still hypotheses which have not yet been assessed at all. In this case, the search cannot yet terminate. Af-ter exiting the main loop, the n apparently most interesting attribute sets are returned.
 AprioriBNS refers to error bounds which are detailed in Table 2. We provide both, exact but loose confidence bounds based on Hoeffding X  X  inequality, and their practically more relevant normal approximation. Statistical folklore says nor-mal approximations can be used for sample sizes from 30 onwards; in our experiments, we encounter sample sizes of 1000 or more. z denotes the inverse standard normal cumu-lative distribution function and n BN , n D the minimum sam-ple size (from Bayesian network and database, respectively) for any I  X  H . We furthermore distinguish the general case in which samples are drawn from both, the Bayesian network and database, from the special case in which the database is feasibly small and therefore  X  P D I = P D I , samples are drawn only from the network. We are now ready to state our main result on the optimality of the result returned by our discovery algorithm.
 Theorem 1. Given a database D , a Bayesian network BN over nodes Z , and parameters n ,  X  , and  X  , the Apri-oriBNS algorithm will output a set of the n approximately most interesting attribute sets H  X  . That is, with probabil-ity 1  X   X  , there is no I 0  X  Z with I 6 X  H  X  and I ( I 0 min I  X  H  X  I ( I ) +  X  . Furthermore, the algorithm will always terminate (even if the database is an infinite stream); the number of sampling operations from the database and from the Bayesian network is bounded by O ( | Z | 1  X  2 log 1  X 
The proof of Theorem 1 is given in the Appendix. We will conclude this section by providing additional design de-cisions of the algorithm. A copy of the source code is avail-able from the authors for research purposes.

In step 2a, we are free to choose any size of the batch to draw from the network and database. As long as C i 6 =  X  , the greatest benefit is obtained by pruning attribute sets in step 2c (all supersets are removed from the search space). When C i =  X  , then terminating early in step 2e becomes possible, and rejecting attribute sets in step 2d is as beneficial as pruning in step 2c, but easier to achieve. We select the batch size such that we can expect to be able to prune a substantial part of the search space ( C i 6 =  X  ), terminate early, or reject substantially many hypotheses ( C i =  X  ).
We estimate the batch size required to prune 25% of the hypotheses by comparing the least interesting hypothesis in H i to a hypothesis at the 75-th percentile of interestingness. We find the sample size that satisfies the precondition of step 2c for these two hypotheses (this is achieved easily by inverting E I and E s ). If C i =  X  , then we analogously find the batch size that would allow us to terminate early in step 2e and the batch size that would allow to reject 25% of the hypotheses in step 2d and take the minimum. In order to efficiently update the interestingness of many attribute sets simultaneously, we use a marginalization algorithm similar to the one described in [10].
Theorem 1 already guarantees that the attribute sets re-turned by the algorithm are, with high probability, nearly optimal with respect to the interestingness measure. But we still have to study the practical usefulness of the method for large-scale problems. In our experiments, we will first focus on problems that can be solved with the exact dis-covery method AprioriBN [10] and investigate whether the sampling approach speeds up the discovery process (while [10] call only the core part of their algorithm AprioriBN, we use this term to refer to the entire exact discovery method). More importantly, we will then turn towards discovery prob-lems with large-scale Bayesian networks that cannot be han-dled by known exact methods . We will investigate whether any of these problems can be solved using our sampling-based discovery method.

In order to study the performance of AprioriBN and Apri-oriBNS over a range of network sizes, we need a controlled environment with Bayesian networks of various sizes and corresponding datasets. We have to be able to control the divergence of background knowledge and data, and, in order to assure that our experiments are reproducible, we would like to restrict our experiments to publicly available data. We create an experimental setting which satisfies these re-quirements. For the first set of experiments, we use data sets from the UCI repository and learn networks from the data using the B-Course [17] website. These generated networks play the role of expert knowledge in our experimentation. In order to conduct experiments on a larger scale, we start from large Bayesian networks, generate databases by draw-ing from the network, and then learn a slightly distorted network from the data which again serves as expert knowl-edge (see below for a detailed description). For the small UCI datasets, the algorithm processes the entire database whereas, for the large-scale problems, AprioriBNS samples from both, the database and the network.

We first compare the performance of AprioriBN and Apri-oriBNS using the UCI data sets. For all experiments, we use  X  = 0 . 01,  X  = 0 . 05, and n = 5. We constrain the cardinality of the attribute sets to max k . Here, the databases are small and therefore only the network is sampled and  X  P D I = P for all I . Table 3 shows the performance results. The | Z | column contains numbers of attributes in each dataset, t [ s ] computation time, N BN the number of samples drawn from the Bayesian network, max  X  I and max I are the estimated and actual interestingness of the most interesting attribute set found by AprioriBNS and AprioriBN, respectively.
We refrain from drawing conclusions on the absolute run-ning time of the algorithms because of a slight difference in the problems that AprioriBN and AprioriBNS solve (finding all sufficiently versus finding the most interesting rules). We do, however, conclude from Table 3 that the relative benefit of AprioriBNS over AprioriBN increases with growing net-work size. For 61 nodes, AprioriBNS is many times faster than AprioriBN. More importantly, AprioriBNS finds a so-lution for the audiology problem; AprioriBN exceeds time and memory resources for this problem.

The most interesting attribute set has always been picked correctly by the sampling algorithm and its estimated in-terestingness is close to the exact value. The remaining 4 most interesting sets were not always picked correctly, but remained within the bounds guaranteed by the algorithm.
We will now study how the execution time of AprioriBNS depends on the maximum attribute set size max k . Figure 1 shows the computation time for various values of max k for the lymphography data set. Note that the search space size grows exponentially in max k and this growth would be max-imal for max k = 10 if no pruning was performed. By con-trast, the runtime levels off after max k = 7, indicating that the pruning rule (step 2c of AprioriBNS) is effective and reduces the computation time substantially.

Let us now investigate whether AprioriBNS can solve dis-covery problems that involve much larger networks than Figure 1: Computation time versus maximum at-tribute set size max k for lymphography data.
 AprioriBN can handle. We draw 1 million observations governed by the Munin1 network from the Bayesian Net-work Repository. We then use a small part of the resulting dataset to learn a Bayesian network. Thus, the original net-work plays the role of a real world system (from which the dataset is obtained) and the network learned from a subset of the data plays the role of our imperfect knowledge about the system. By varying the sample size M used to build the network we can affect the quality of our  X  X ackground X  knowledge. The Munin1 network has 189 attributes. Exact inference from networks of this size is very hard in practice.
Table 4 shows the results for various values of M and max k = 2 . . . 3. We sample at equal rates from the Bayesian network and from data; both numbers of examples are there-fore equal and denoted by N in the table. We use the same setting for the next experiment with the Munin2 network containing 1003 attributes. The problem is huge both in terms of the size of Bayesian network and the size of data: The file containing 1 million rows sampled from the original network is over 4GB large, and 239227 rows sampled by the algorithm amount to almost 1GB. The experiment took 4 hours and 50 minutes for max k = 2.

Figure 2 summarizes Tables 3 and 4, it details the rela-tionship between the number of nodes in the network and the computation time of AprioriBN and AprioriBNS. We observe a roughly linear relationship between logarithmic network size and the logarithmic execution time, Figure 2 shows a model fitted to the data. From these experiments, we conclude that the AprioriBNS algorithm scales to very large Bayesian networks and databases, yet it is guaranteed to find a near-optimal solution to the most interesting at-tribute set problem with high confidence. We can apply the exact AprioriBN algorithm to networks of up to about 60 nodes. Using the same computer hardware, we can solve discovery problems over networks of more than 1000 nodes using the sampling-based AprioriBNS method.
We studied the problem of discovering unexpected pat-terns in a database. We formulated the approximately most interesting attribute sets problem and developed an algo-rithm which solves this problem. AprioriBNS uses sampling-based approximate inference in the Bayesian network and, when the database is large, also samples the data.
We proved that AprioriBNS always finds, with high con-fidence, the approximately most interesting attribute sets. Figure 2: Network size and computation time.
 We studied AprioriBNS empirically using moderately sized as well as large-scale Bayesian networks and databases. From our experiments, we can draw the following main conclusions. (1) The relative performance benefit of Apri-oriBNS over the corresponding exact method AprioriBN in-creases with the network size. For moderately sized net-works, AprioriBNS can be many times faster than Apri-oriBN. (2) More importantly, while AprioriBN scales to net-works with about 60 nodes, we can apply AprioriBNS to Bayesian networks of 1000 nodes and databases of several gigabytes using the same hardware.
 T.S. is supported by the German Science Foundation under Grant SCHE 540/10-1.
The proof of Theorem 1 has two parts: we will first prove the guaranteed sample bound of O ( | Z | 1  X  2 log 1  X  ). We will then show that AprioriBNS in fact solves the approximately most interesting attribute sets problem.

Theorem 2. The number of sampling operations of Apri-oriBNS from the database and from the Bayesian network is bounded by O ( | Z | 1  X  2 log 1  X  ) .

Proof. We can disregard the possibility of early stopping and show that the stopping criterion in step 2f applies after polynomially many sampling operations.
 Let r = max A  X  Z | Dom( A ) | . First note that P sentation, let n BN = n D = N . The stopping condition becomes Equation 7.
From the Hoeffding bound, it follows that Equation 7 is satisfied for N given in Equation 8.
 This proves Theorem 2 Throughout the proof, P i and max i are abbreviations for P a helpful concept which we call the support of an attribute set. The support of an attribute set I is the maximum sup-port of any itemset I = i with respect to the Bayesian net-work or the database, whichever is greater. Using this defini-tion, it is easy to see that support upper-bounds interesting-ness which is helpful to understand the pruning mechanism of step 2c.

Definition 4. The support of an attribute set I is de-fined in Equation 10. We write the estimated support as [ supp( I ) = max n max i  X  P BN I ( i ) , max i  X  P D I ( i ) o .
G  X  X ood X  hypotheses output by AprioriBNS in
R i Attribute sets rejected before iteration i . Note
H i Collection of attribute sets still under consid-H  X  i n most interesting attribute sets in H i .

C i Collection of candidate attribute sets in itera-
U i Collection of unseen attribute sets: U i = 2 Z \ i max Value of i after the main loop terminates.  X 
I ( I ) Estimate of interestingness of attribute set I [ supp( I ) Estimate of support of attribute set I during n
BN , n D Minimum sample size (from Bayesian network
Lemma 1. The support of an attribute set I upper-bounds its interestingness: supp( I )  X I ( I ) .

Proof. The proof of Lemma 1 follows directly from Def-inition 1: the absolute difference max i | P BN I ( i )  X  P greatest if either P BN I ( i ) or P D I ( i ) is zero.
Table 5 defines additional notation that we use during the proof. U i is the set of unseen attribute sets in iteration i . It is important to note that no hypotheses remain unseen when the candidate set C i is empty.
 Lemma 2. C i =  X  implies U i =  X  for all 1  X  i  X  i max .
Proof. Lemma 2 follows primarily from the complete-ness of Apriori X  X  candidate generation procedure invoked in step 2g: if no attribute set was ever pruned, then S i C i 2 Z \{ X  X  . In step 2h, the candidates C i are accumulated in H i +1 . In step 2d, one or more hypotheses I 0 can be removed from H i . By the definition of R i , each removed I 0 is then an element of R i . In step 2c, hypotheses I 0 and all their su-persets are removed from C i and H i . In this case, supersets of C i will not be generated in step 2g but, by the definition of R i all of them become members of R i . This implies that U = 2 Z \{ X  X \ H i \ R i =  X  .

The proof heavily relies on confidence intervals for esti-mates of the interestingness, support, and the difference of interestingness values. We have to show that the confidence bounds given in Table 2 are in fact valid.

Lemma 3. E I as defined in Table 2 is a valid confidence bound: P r  X  |I ( I )  X   X  I ( I ) | &gt; E I ( I,  X  )  X   X  different versions of E I based on the (exact but loose) Ho-effding bound, and an approximate (but practically useful) bound based on the normal approximation. For the spe-cial case  X  P D I = P D I and samples are drawn only from the Bayesian network (but not from the database), additional Hoeffding and normal bounds are given.

Proof. Let us begin by giving a bound on the difference of estimated probabilities. Let X 1 , . . . , X n be independent random variables and let X i  X  [ a i , b i ]. Let S n = P Hoeffding X  X  inequality states that where E ( S n ) denotes the expected value of S n . Since  X  P
I ( i )  X  values in { 0 , 1 N BN ( I ) } , and N D ( I ) random variables taking values in { 0 ,  X  1 N D ( I ) } , Equation 11 follows. In Equation 12 we expand the definition of I . We remove the absolute value in Equation 13 by summing over the two possible ways in which the absolute value can exceed the bound E I . Since max i { a i  X  b i }  X  max i { a i } X  max Equation 14 follows. We apply the union bound in Equation 15, replace the two symmetric differences by the absolute value in Equation 16. Since | a  X  b | X || a | X  X  b || , Equation 17 follows; we expand E I , apply (11) and arrive in Equation 18
To prove the bounds based on normal approxima-tion notice that  X  P BN I ( i ) follows the binomial distribu-tion, which can be approximated by the normal dis-tribution with mean P BN I ( i ) and standard deviation p ( N BN ( I ))  X  1 P BN I ( i )  X  (1  X  P BN I ( i )). When sampling from data,  X  P D I ( i ) follows the hypergeometric distribution which can be approximated by the normal distribution with mean P
I ( i ), and standard deviation Combining the two we get
Since we use the estimates of probabilities to compute the standard deviation, Student X  X  t distribution governs the ex-act distribution, but for large sample sizes used in the algo-rithm the t distribution is very close to normal.
The proof is identical to the Hoeffding case until Equa-tion 16, where the Hoeffding bound needs to be replaced by the above expression. The special case of sampling only from the Bayesian network (  X  P D I = P D I ) follows immediately from the more general case discussed in detail.

Lemma 4. E s as defined in Table 2 is a valid confidence bound for the support: P r  X  | supp( I )  X  [ supp( I ) | &gt; E  X  . Table 2 details a Hoeffding bound and an approximate normal bound. For the special case that  X  P D I = P D I and sam-ples are drawn only from the Bayesian network, Hoeffding and normal bounds are given, too.

Proof. In Equation 21, we expand the support defined in Equation 10. To replace the absolute value, we sum over both ways in which the absolute difference can exceed E s Equation 22. In Equation 23, we exploit max i { a i  X  b i max i { a i } X  max i { b i } ; we then use the union bound and in-troduce the absolute value again in Equation 24. Equation 25 expands the definition of E s ; the Chernoff bound (Equa-tion 26) proves that the the confidence is in fact  X  . For the normal approximation based bounds we start with Equation 25 above which becomes
The special case of  X  P D I = P D I follows immediately from the general case.

Lemma 5. E d as defined in Table 2 is a valid, data in-dependent confidence bound for the interestingness value of an attribute set values: P r  X  |I ( I, i )  X   X  I ( I, i ) | &gt; E ( n BN , n D ,  X  )  X   X   X  . A Hoeffding bound for the special case that  X  P D I = P D I and samples are drawn only from the Bayesian network is given.

Proof. The proof for the Hoeffding inequality based bound follows directly from (11) in the proof of Lemma 3. For the normal case, it follows from (20) by substituting  X  P
I ( i ) = possible standard deviation.

Theorem 3. Let G be the collection of attribute sets out-put by the algorithm. After the algorithm terminates the following condition holds with the probability of 1  X   X  :
Proof. We will first assume that, throughout the course of the algorithm, the estimates of all quantities lie within their confidence intervals (assumptions A1a, A1b, and A2). We will show that under this assumption the assertion in Equation 27 is always satisfied when the algorithm termi-nates. We will then quantify the risk that over the entire execution of the algorithm at least one estimate lies outside of its confidence interval; we will bound this risk to at most  X  . These two parts prove Theorem 3. (A1a)  X  i  X  { 1 , . . . , i max } X  I  X  H i : |  X  I ( I )  X  I ( I ) |  X  Equation (Inv1) shows the main loop invariant which, as we will now show, is satisfied after every iteration of the main loop as well as when the loop is exited. (Inv1)  X  K  X  R i there exist distinct I 1 , . . . , I n  X  H We will prove the loop invariant (Inv1) by induction. For the base case ( R i =  X  ), (Inv1) is trivially true. For the inductive step, let us assume that (Inv1) is satisfied for R and H i before the loop is entered and show that it will hold for R i +1 and H i +1 after the iteration. (Inv1) refers to R and H , so we have to study steps 2c, 2d, and 2h, which alter these sets. Note that, by the definition of R , R i +1 always a superset of R i ; it contains all elements of R i addition to those that are added in steps 2c and 2d. Step 2c Let K be an attribute set pruned in this step. The pruning condition together with our definition of support (Equation 10) implies Equation 28; we omit the confidence parameter of E s for brevity. Equation 28 is equivalent to Equation 29. Assumption (A1a) says that  X  I ( I 00 )  X  E I ( I 00 )  X I ( I assumption (A1b) we can conclude that [ supp( K )+ E s ( K )  X  supp( K ) which leads to Equation 30. From the definition of support, it follows that all supersets J of K must have a smaller or equal support (Equation 31); Lemma 1 now implies that if the support of K is lower than that of J , so must be the interestingness (Equation 32). K cannot be an element of H  X  i because, in order to satisfy Equation 28, the error bound E s would have to be zero or negative which can never be the case. Since K 6 X  H  X  i , and | H i | = n , we can choose I 1 , . . . , I n to lie in H  X  i now prunes K and all supersets J  X  K , but Equation 32 im-plies that for any J  X  K : I ( J )  X  I ( I 1 ) , . . . , I ( I fore, (Inv1) is satisfied for R i +1 = R i  X  (supersets of K ) and the  X  X ew X  H i ( H i \ rejected hypotheses).
 Step 2d Let K be one of the attribute sets rejected in this step. The condition of rejection implies Equation 33; we omit the con-fidence parameter of E I for brevity. Let I 00 be any attribute set in H  X  i . Equation 33 implies Equation 34. Together with assumption (A1a), this leads to Equation 35. Note also that a rejected hypothesis K cannot be an ele-ment of H  X  i because otherwise the error bounds E I and E would have to be zero or negative which can never be the case. Since K 6 X  H  X  i , and | H  X  i | = n , we can choose I to lie in H  X  i and Equation 35 implies 36. Since furthermore R i +1 = R i  X  X  K } , Equation 36 implies (Inv1) for R i +1 the  X  X ew X  H i ( H i \ rejected hypotheses); below  X   X   X   X  abbre-viates  X  X here exist distinct X .  X   X  I 1 , . . . , I n  X  H i \{ K } :  X  j  X  X  1 , . . . , n }I ( I This implies that (Inv1) holds for R i +1 and the current state of H i after step 2d.
 Step 2h R i +1 is not altered, H i +1 is assigned a superset of H i requires the existence of n elements in H . If it is satisfied for R i +1 and H i (which we have shown in the previous para-graph), it also has to be satisfied for any superset H i +1 This proves that the loop invariant (Inv1) is satisfied after each loop iteration.
 Final Step (immediately before Step 3) The main loop terminates only when C i =  X  , from Lemma 2 we know that U i max =  X  . Since U i max =  X  , and G = H  X  suffices to show that all attribute sets in G are better than all sets in R i max and in H i max \ H  X  i max . We distinguish between the two possible termination criteria of the main loop. Case (a): Early stopping in Step 2e The stopping criterion, we are assured the Equation 37 is satisfied. By assumption (A1a), this implies Equation 38. From the invariant (Inv1) we know that  X  K  X  R that is, for every rejected hypothesis there are n hypothe-ses in H i which are at least as good. Take any such S = { I 0 1 , . . . , I 0 n } . For every I 0  X  S either I I 6 X  H  X  i max . In the former case it follows immediately that I  X  G ; that is, I 0 is better than the rejected K and I 0 is in the returned set G . If I 0 6 X  H  X  i max , then Equation 38 guar-antees that every hypothesis I  X  H  X  i max is  X  X lmost as good of Theorem 3.
 Case (b): Stopping in Step 2f Assumption (A2) assures Equation 39. Analogously to case (a), we can argue that (Inv1) guar-antees that  X  K  X  R i max  X   X  I 1 , . . . , I n  X  H i max Theorem 3.

We have shown that if the main loop terminates, the out-put will be correct. It is easy to see that the loop will in fact terminate after finitely many iterations: Since Z is finite, the candidate generation has to stop at some point i with C i =  X  . When the sample size becomes large enough, the loop will be exited in step 2f. This is guaranteed because a guaranteed fraction  X  3 is reserved for the error bound of step 2f and the error bound (Table 2) vanishes for large sample sizes.
 Risk of violation of (A1a), (A1b), and (A2) We have proven Theorem 3 under assumptions (A1a), (A1b), and (A2). We will now bound the risk of a violation of any of these assumptions during the execution of Apri-oriBNS. We first focus on the risk of a violation of (A1a). A violation of |I ( I )  X   X  I ( I ) | X  E I can occur in any iteration of the main loop and for any I  X  H i (Equation 40). We use the union bound to take all of these possibilities into account (Equation 41). Lemma 3 implies Equation 42.
The risk of violating assumption (A1b) can be bounded similarly in Equations 43 and 44.

We now address the risk of a violation of (A2). In step 2b, H  X  i is assigned the hypotheses with highest values of  X  I ( I ); i.e., for all I  X  H  X  i and I 0 6 X  H  X  i :  X  I ( I )  X  (A2) to be violated, there has to be an I  X  H  X  i max and an I  X  H i max \ H  X  i max such that I ( I ) &lt; I ( I 0 )  X   X  but Equation 45 is satisfied in spite. This is only possible if there is at least one hypothesis I  X  H i max with |I ( I )  X   X  I ( I ) | &gt;  X  Equation 45 assures that all elements of H i max have been estimated to within a two-sided confidence interval of  X  2 since all I  X  H  X  i max appear at least as good as I 0 6 X  H I can be at most  X  better than I .

In Equation 46 we substitute Equation 45 into this condi-tion. We expand the definition of interestingness in Equa-tion 47, use the union bound in Equation 48 and refer to Lemma 5 in Equation 49.

We can now calculate the combined risk of any violation of (A1a), (A1b), or (A2) using the union bound in Equation 51; this risk can be bounded to at most  X  in Equation 52 This completes the proof of Theorem 3.

Together, Theorems 3 and 2 prove Theorem 1.
