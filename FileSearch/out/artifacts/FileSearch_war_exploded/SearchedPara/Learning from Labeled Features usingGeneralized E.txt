 It is difficult to apply machine learning to new domains be-cause often we lack labeled problem instances. In this pa-per, we provide a solution to this problem that leverages domain knowledge in the form of affinities between input features and classes. For example, in a basebal l vs. hockey text classification problem, even without any labeled data, we know that the presence of the word puck isastrongindi-cator of hockey . We refer to this type of domain knowledge as a labeled feature . In this paper, we propose a method for training discriminative probabilistic models with labeled fea-tures and unlabeled instances. Unlike previous approaches that use labeled features to create labeled pseudo-instances, we use labeled features directly to constrain the model X  X  predictions on unlabeled instances. We express these soft constraints using generalized expectation (GE) criteria  X  terms in a parameter estimation objective function that ex-press preferences on values of a model expectation. In this paper we train multinomial logistic regression models us-ing GE criteria, but the method we develop is applicable to other discriminative probabilistic models. The complete objective function also includes a Gaussian prior on param-eters, which encourages generalization by spreading param-eter weight to unlabeled features. Experimental results on text classification data sets show that this method outper-forms heuristic approaches to training classifiers with labeled features. Experiments with human annotators show that it is more beneficial to spend limited annotation time labeling features rather than labeling instances. For example, after only one minute of labeling features, we can achieve 80% ac-curacy on the ibm vs. mac text classification problem using GE-FL, whereas ten minutes labeling documents results in an accuracy of only 77% I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Measurement, Performance Learning with Domain Knowledge, Labeled Features, Semi-Supervised Learning, Text Classification
Supervised machine learning methods require costly la-beled problem instances, and this limits the applicability of learning to new domains. Semi-supervised learning meth-ods [21], which aim to leverage available unlabeled instances, are an appealing solution for reducing labeling effort. How-ever, despite recent interest in this problem, real applica-tions of semi-supervised learning remain rare. Reasons for this may include the time and space complexity and reliance on sensitive hyperparameters of semi-supervised methods. Additionally, many methods make strong assumptions that may hold in small, synthetic data sets, but tend to be vio-lated in real-world data.

Instead, we want a simple, robust method that facilitates training models for new domains and requires minimal anno-tation effort. One potential solution involves incorporating existing domain knowledge into learning. There has been much recent interest in this idea [20, 19, 9, 15, 17, 7]. In this paper, we propose a discriminative semi-supervised learn-ing method that incorporates into training one particular type of domain knowledge: affinities between features and classes. For example, in a basebal l vs. hockey text classi-fication problem, even without any labeled data, we know that the presence of the word puck is a strong indicator of hockey . We refer to this type of domain knowledge as a labeled feature . Unlike previous heuristic approaches that use labeled features for feature selection or to create labeled pseudo-instances [6, 14, 17, 19, 20], we use labeled features directly to constrain the model X  X  predictions on unlabeled instances. We specify these soft-constraints using general-ized expectation (GE) criteria.

A GE criterion [16] is a term in a parameter estimation objective function that express preferences on values of a model expectation. GE is similar to the method of mo-ments for parameter estimation, but allows us to express ar-bitrary scalar preferences on expectations of arbitrary func-tions, rather than requiring equality between sample and model moments. We also note three important differences from traditional training objective functions for undirected graphical models. (1) A one-to-one relationship between GE terms and model factors is not required. For example, a GE term may score expectations on sets of variables that form a subset of parameterized model factors, or on sets of variables larger than model factors. (2) Model expectations in different GE terms can be conditioned on different data sets. (3) The score function can be arbitrary. Examples of possible score functions include the distance to some target expectation or a smooth hinge-loss.

In this paper, we use leverage property (3) to specify an objective function that penalizes parameter settings if the resulting model predictions do not conform to prior expecta-tions. We use property (1) to express constraints only on the subsets of variables for which prior information is available. Specifically, for each labeled feature, there is a correspond-ing GE term that scores the model X  X  predicted class distribu-tion conditioned on the presence of that feature. The score function penalizes these distributions according to their KL-divergence from reference distributions estimated using the labeled features. We derive a specific objective function for a multinomial logistic regression classifier, but the idea is applicable to other discriminative probabilistic models. We refer to this method as Generalized Expectation with Fea-ture Labels, or GE-FL.
 We evaluate GE-FL on six text classification data sets. First, we show that GE-FL outperforms several baseline methods that use labeled features. Next, we compare with three previous methods that incorporate labeled features into learning [19, 20, 17], and show that GE-FL attains comparable or better performance despite using no labeled documents. Finally, we conduct human annotation experi-ments in which we compare the performance over time of (a) a system that trains a classifier with labeled features using GE-FL and (b) a system that uses semi-supervised train-ing with labeled documents. The results show that given limited annotation time, it is more beneficial to spend that time labeling features rather than labeling instances. For example, after only one minute of labeling features, we can achieve 80% accuracy on the ibm vs. mac text classification problem using GE-FL, whereas ten minutes labeling doc-uments results in an accuracy of only 77%. Given equal labeling time, the accuracy difference is often much more pronounced, with absolute accuracy improvements as high as 40%. In our experiments, labeling features is on aver-age 3.7 times faster than labeling documents, a result that supports similar findings in previous work [18]. The methods described in this paper are semi-supervised [21]. However, the supervision comes in the form of labeled fea-tures, or more generally arbitrary expectations from domain knowledge, rather than labeled instances. We suggest that this approach is beneficial because it avoids some of the common assumptions of semi-su pervised learning methods. For example, unlike discriminative semi-supervised learning methods such as Transductive Support Vector Machines [12] and Entropy Regularization [8], we do not assume low-density regions between classes.

There has been much recent interest in incorporating do-main knowledge into learning, including several methods that use labels or relevance judgments on features. Nearly all of these methods convert labeled features into labeled instances, and apply a standard learning algorithm. Liu, et al. [14] use human annotators to label features that are highly predictive of unsupervised instance clustering assign-ments. The unlabeled instances are soft-labeled according to their cosine similarity with pseudo instances that only con-tain labeled features, and this soft-labeled data is used as an initialization point for the expectation maximization (EM) algorithm. Schapire, Rochery, and Gupta [19] use hand-crafted rules based on relevant features to label instances, and modify AdaBoost to choose weak learners that both fit the labeled training data and the soft-labeled data. Wu and Srihari [20] use labeled features to assign labels and confi-dence scores to unlabeled instances, which are then used in conjunction with labeled data during training. We compare with the methods of Schapire, Rochery, and Gupta [19] and Wu and Srihari [20] in Sections 5.2 and 5.3, respectively. Dayanik, et al. [4] propose several methods that use labeled features to specify prior distributions on the parameters of a logistic regression model.

There is also recent work in the natural language pro-cessing community with similar goals. Chang, Ratinov, and Roth [2] propose an EM-like algorithm that incorporates prior constraints into semi-supervised training of structured output models. In the E-step, the inference procedure pro-duces an N-best list of outputs ranked according to the sum of the output X  X  score under the model and a penalty term for violated constraints. In the M-step, the N-best list is used to re-estimate the model parameters. Haghighi and Klein [9] use prototypes, which are analogous to what we refer to as labeled features, to learn log-linear models for structured output spaces. The prototypes are used to hypothesize ad-ditional soft prototypes for features that are syntactically similar. All prototypes are then used as features during maximum likelihood training on limited labeled data.
Other types of domain knowledge have also been incorpo-rated into learning. Jin and Liu [11] and Mann and McCal-lum [15] provide methods for incorporating prior informa-tion about the class distribution into discriminative training. Huang and Mitchell [10] propose a new generative clustering model and provide methods for the user to exert influence over the learned clusters. For example, the user can specify that a feature indicates a cluster, an instance belongs to a cluster, or that a cluster should be deleted.

Many of the above methods convert domain knowledge into labeled instances. In this paper, we take an alternative approach in which domain knowledge is used to constrain model predictions. Gra  X  ca, Ganchev, and Taskar [7] pro-vide a related method that incorporates prior constraints into the EM algorithm. Specifically, the E-step is modified so that the expectation over output variables is the clos-est distribution (in terms of KL-divergence) to the model prediction that respects a specified set of constraints. In the M-step, the model parameters are re-estimated using this modified expectation. We note several differences be-tween this method and GE-FL. First, the constraints in con-strained EM are per-instance, whereas in this paper we use global constraints over entire data sets. Next, Gra  X  ca et al. use a generative model, whereas here we use direct maxi-mization in a discriminative model. Finally, Gra  X  ca et al. put constraints only on the output variables, whereas here the constraints additionally consider input variables.
Work in active learning is also relevant. In active learning, the learner can choose the particular instances to be labeled. In pool-based active learning [3], the learner has access to a set of unlabeled instances, and can choose the instance that has the highest expected utility according to some metric. A standard pool-based active learning method is uncertainty sampling [13], in which the instance chosen is the one for which the model predictions are most uncertain. Although in theory this method is problematic because it ignores the distribution over instances [5], in practice it often works well, and is easy to implement. We use uncertainty sampling as a baseline in our user experiments.

Some recent work has addressed active learning by label-ing features. Raghavan, Madani, and Jones [18] interleave feedback on instances and features in an algorithm called tandem learning. They show that incorporating feedback on features can significantly accelerate active learning. Exper-iments also demonstrate that humans can provide accurate information about features, and that it takes five times as long to label instances as to label features. Raghavan and Allan [17] provide additional methods for training SVMs with labeled features, including scaling the parameters of la-beled features, creating specially-weighted pseudo-instances containing only labeled features, and soft-labeling unlabeled instances. We compare with tandem learning in Section 5.4. Godbole et al. [6] describe software for interactive classifi-cation that uses both feature and instance active learning. Similarly to Raghavan and Allan [17], Godbole et al. incor-porate information about features into training by creating pseudo-instances containing only labeled features.
In this section, we describe Generalized Expectation crite-ria and derive the specific objective function we use to train classifiers with labeled features. Section 4 describes the pro-cess of obtaining labeled features and converting them into specific constraints.

A generalized expectation (GE) criterion is a term in a parameter estimation objective function that assigns scores to values of a model expectation [16]. In this paper we use GE in conjunction with discriminative probabilistic models. Given a score function S , an empirical distribution  X  p , a func-tion f , and a conditional model distribution p parameterized by  X  , the value of a GE criterion is One specific type of score function S is some measure of distance between the model expectation and a reference ex-pectation. Given some distance function  X (  X  ,  X  ), a reference expectation  X  f , an empirical distribution  X  p , a function f ,and a conditional model distribution p , this criterion is
In this paper, x is a vector of input feature counts, y is a discrete class label, and p  X  ( y | x ) is a conditionally trained Markov random field with a single output variable and ob-servation variables that are conditionally independent given this output. The probability of output y conditioned on input x is given by where Z ( x ) is a normalizer that assures the literature, this model is often referred to as multinomial logistic regression or a maximum entropy classifier.
We use GE terms in which  X  p is the distribution of unla-beled data U , and we compute the expectation of f k ( x ,y )= I ( y ) I ( x k &gt; 0), an indicator of the presence of feature k in x times an indicator vector with 1 at the index corresponding to label y and zeros elsewhere. Therefore, E U [ E p  X  ( y | x ) is a vector in which the i th value is the expected number of instances that contain feature k and have label y i .Ifwe additionally add a normalizing constant into f k , f k ( x ,y )= k I ( y ) I ( x k &gt; 0), where C k = tation is the predicted label distribution on the set of in-stances that contain feature k ,  X  p  X  ( y | x k &gt; 0). WeusetheKL divergence for  X (  X  ,  X  ). A single term is then where  X  p ( y | x k &gt; 0) are reference distributions obtained us-ing domain knowledge. The estimation of reference distri-butions is discussed in Section 4. The combined objective function is composed of a GE term for each labeled feature k  X  K , and a zero-mean  X  2 -variance Gaussian prior on pa-rameters.
 We use L-BFGS, a quasi-Newton optimization method, to estimate model parameters. The gradient of Equation 1 with respect to the model parameter for feature j and label y ,  X  Above, we observe that the degree to which the gradient of a parameter for an unlabeled feature j and label y is affected by a GE-FL term for labeled feature k depends on how often j and k co-occur in an instance.

Because we only expect to have prior knowledge for a subset of features, there will be more parameters in the model than constraints in the objective functions. Conse-quently, we expect the optimization problem to be under-constrained, meaning that there will be many optimal pa-rameter settings. Therefore, in practice we use GE in con-junction with other objective functions that help to choose among these possible models.

The Gaussian prior on parameters addresses this prob-lem by preferring parameter settings with many small values over settings with a few large values. This encourages the model to have non-zero values on parameters for unlabeled features that co-occur often with a labeled feature. That is, if the word goal occurs often in documents with puck , increasing the weight of goal canhelptosatisfythecon-straint that the model should predict hockey conditioned on the presence of puck . The Gaussian prior prefers this set-ting, in which puck and goal both have moderate weights, to the setting in which puck has high weight and goal has zero weight, since it penalizes the square of the parameter values. We use this term in all experiments in this paper with  X  = 1. Other terms that could help choose amongst possible models include standard conditional log-likelihood on labeled instances and agreement objective functions that encourage model predictions to be consistent when using different subsets of features.
In this section, we describe methods for selecting candi-date features for labeling, obtaining labels for these features, and estimating the reference expectations needed for the KL divergence from target objective function.
Oracle-features: Ideally, a selected feature should be both highly predictive of some class, and occur often enough to have a large impact. In practice we will not be able to determine whether a feature is predictive if we have no labeled instances. However, in order to obtain an upper bound on feature selection methods, we assume there exists an oracle that can reveal the label of each unlabeled instance. We then select features according to their predictive power as measured by the mutual information of the feature with the class label.

LDA-features: Another potential feature selection method would select features randomly only according to their fre-quency. The problem with this method is that it tends to select common, non-predictive features, such as stopwords in text classification. Instead we run unsupervised feature clustering and select the most prominent features in each cluster. In this paper we cluster unlabeled data with latent Dirichlet allocation (LDA) [1], a widely used topic model. For each LDA topic t i ,wesortfeatures x k by p ( x k | t choose the top f features. There is no guarantee that the candidate features selected by this heuristic are relevant to the learning task of interest. However, in practice this per-forms much better than selecting candidate features by fre-quency.

For experiments in this paper, we choose the top 25 L fea-tures according to these metrics, where L is the number of classes.
We first discuss the labeling process. When shown a can-didate feature, the labeler can choose to accept the labeling request or discard the feature. The labeler only labels fea-tures that are accepted. Note that this process is different from traditional instance labeling because labeling requests may be refused. For example, if presented with the word  X  X he X , the labeler will likely discard it because it does not have strong affinity with any one particular label.
Oracle-labeler: For some experiments we use feature la-bels provided by an oracle rather than a human. To decide whether to accept a feature, the oracle is able to reveal the labels of the unlabeled instances in order to simulate hu-man background knowledge of the relevance of the feature. Using the instance labels, the oracle computes the mutual information of the feature with the class label, and accepts if this mutual information is above a threshold  X  .Inthis paper,  X  is the mean of the mutual information scores of the top M most predictive features, where M = 100 L , or 100 times the total number of labels. Note that a feature can be labeled with more than one class. If accepted, the oracle labels a feature with the class with which the feature occurs most often, and any other class that occurs with the feature at least half as often. We note that because M is typically small relative to the total number of input features, the ora-cle is somewhat conservative in the features it accepts. This simulates a scenario in which the user only knows about the most prominent and important features.

The second method for obtaining feature labels is to ask real annotators. We explore this approach in Section 6. For the experiments in Sections 5.2 and 5.3, we use labeled fea-tures provided in prior work.
Target or reference expectations are required by the KL divergence calculation. We present two methods for esti-mating reference expectations. We note that we could al-ternatively allow the users to specify the reference distribu-tions directly during the labeling process. We choose not to do this because it is not clear that users can provide ac-curate estimates of these distributions. However, we could instead have the labeler specify a degree of association be-tween a label and feature in terms of discrete categories such as strongly indicative . We plan to explore such approaches in future work, but note that the results in this paper seem to indicate that precise estimates of the reference distributions are not required to achieve good performance.

Schapire-distributions: As proposed by Schapire, et al. [19], we use a simple heuristic in which a majority of the probability mass for a feature is distributed uniformly among its associated classes(s), and the remaining proba-bility mass is distributed uniformly among the other non-associated classes. Define q maj as the probability for the associated classes. Then, if there are n associated classes out of L total classes, each associated class has probabil-ity  X  p ( y | x k &gt; 0) = q maj /n and each non-associated class has iments in this paper, we use q maj =0 . 9.

Feature-voted-distributions: Alternatively, we use the labeled features to vote on labels for the unlabeled instances. For each feature x k in an instance x , it contributes a vote for each of its labels. We then normalize the vote totals to get a distribution over labels for each instance. With this soft-labeled data, we can estima te the reference distributions directly.
We evaluate the effectiveness of GE-FL on six text clas-sification data sets. For all data sets, instances correspond to documents and features are word counts. For the tasks in which a single instance can be assigned multiple labels, we split the task into L one vs. all binary learning tasks, where L is the number of labels. For other data sets, we use multi-class classification. We describe the data sets below.
For data sets without a standard test/train split, we ran-domly split the data such that 75% is used as training data, and the remaining 25% is reserved for testing. For the exper-iments in sections 5.1, 5.2, and 5.4 we use 10 such random splits and report the mean of the results. For experiments that do not use labeled instances we simulate unlabeled data by hiding labels of all instances. Experiments with GE-FL never include labeled instances.
We first compare GE-FL with several baseline methods, described below. For these experiments, we use the oracle-labeler . We run experiments comparing the above baselines with GE-FL and provide the results in Tables 1 and 2. Datasets med-space , ibm-mac ,and baseball-hockey are subsets of the 20 newsgroups data set; healthcare-financial is a subset of the industry sector data set. The parenthe-sized number with each data set indicates the mean num-ber of features labeled by the oracle labeler. The results presented in Table 1 are obtained using oracle-features and Schapire-distributions . This simulates a scenario in which there is a domain expert who can suggest and label relevant features. We also run experiments using LDA-features and http://kdd.ics.uci.edu/ http://www.cs.umass.edu/~mccallum/code-data.html http://www.cs.cornell.edu/People/pabo/movie-review-data/ http://www.cs.cmu.edu/~webkb Schapire-distributions , which simulates a scenario in which some candidate features are presented to the labeler. The results are presented in Table 2. GE-FL attains the highest macro-F1in7ofthe9datasetsusing oracle-features ,and 7of9using lda-features . Results marked with a * indicate that GE-FL performs significantly better under a two-tailed paired t-test with p =0 . 05.

We motivated GE-FL in terms of bootstrapping mod-els for new domains, so we also perform experiments to determine the effectiveness of GE-FL in relation to semi-supervised training with labeled documents. To do this, we use entropy regularization [8], a discriminative semi-supervised learning method that aims to minimize the uncertainty of predictions on unlabeled data. This method introduces a tuning parameter  X  that controls the weight of the regu-larizer relative to the data likelihood. We set  X  =0 . 2, a value that provided the best mean results across all data sets, and perform training with a deterministic annealing procedure. We report the number of instances at which the performance of GE-FL and the instance learning method are statistically indistinguishable. Raghavan, et al. [18] perform a thorough user study in which they conclude that it is five times faster to label a feature than to label a document. We use this result to present estimated speed-ups using GE-FL over entropy regularization. We note that in the computa-tion of this estimated speed-up, we consider the number of features presented to the labeler, including those that are discarded. Since we expect discarding a feature to be faster than labeling a feature, the estimates in Table 2 are likely conservative.

Each of the baselines demonstrates an important point about GE-FL. Feature voting uses the domain knowl-edge only, whereas GE-FL uses this information to constrain model predictions on unlabeled data, and in the process learns about co-occurring features without labels. Labeled only demonstrates the importance of incorporating these co-occurring features without labels. Finally, feature la-beling is equivalent to using the labeled features to infer constraints on all features, whereas GE-FL only specifies constraints on features that are known to be relevant.
In this experiment, we compare GE-FL with boosting with prior knowledge [19]. Boosting with prior knowledge aims to maximize the conditional log likelihood of both la-beled instances and instances classified using a hand-crafted model. The hand-crafted model classifies instances using the product of label probabilities for features, which are esti-mated from labeled features using the Schapire-distributions heuristic. Schapire et al. provide 138 labeled features for the 20 newsgroups data set. For comparison, we use the same feature labels and use the Schapire-distributions heuristic to estimate reference distributions. We note that the experi-ments in [19] use n-gram features, whereas we use only uni-gram features. Comparing using the domain knowledge only, GE-FL gives approximately a 15% absolute error reduction from 64% error ([19] Figure 3) to 49% error. Furthermore, the boosting method requires the domain knowledge and be-tween 400 and 800 labeled documents for boosting with prior knowledge to match the accuracy of GE-FL, which uses no labeled documents. movie (43.7 of 50) 0.763* 0.766* 0.772* 0.797 150 15.0 sraa (97.5 of 100) 0.630* 0.596* 0.585* 0.651 160 8.0 webkb (88.8 of 100) 0.496* 0.477* 0.745* 0.774 70 3.5 med-space (50.0 of 50) 0.907* 0.932* 0.930* 0.952 90 9.0 ibm-mac (43.7 of 50) 0.853 0.864 0.861 0.855 110 11.0 baseball-hockey (50 of 50) 0.925* 0.927* 0.939* 0.954 200 20.0 20 newsgroups (494.4 of 500) 0.554* 0.560* 0.643* 0.704 650 6.5 financial-healthcare (50 of 50) 0.653 0.443* 0.539* 0.583 50 5.0 sector.top (163.9 of 175) 0.664* 0.657* 0.719* 0.730 140 4.0 adocument.
 movie (4.6 of 50) 0.616 0.608 0.607* 0.623 20 2.0 sraa (29.5 of 100) 0.577 0.526* 0.520* 0.559 80 4.0 webkb (17.5 of 100) 0.514* 0.513* 0.593* 0.615 20 1.0 med-space (14.3 of 50) 0.857* 0.862* 0.867* 0.927 40 4.0 ibm-mac (10.4 of 50) 0.740* 0.817 0.762* 0.817 50 5.0 baseball-hockey (10.8 of 50) 0.779* 0.840* 0.853* 0.915 40 4.0 20 newsgroups (269.6 of 500) 0.493* 0.514* 0.585* 0.667 300 3.0 financial-healthcare (9.4 of 50) 0.552* 0.456* 0.595 0.588 50 5.0 sector.top (50.7 of 175) 0.538* 0.534* 0.544* 0.596 60 1.7
Next, we compare GE-FL with a method for leverag-ing labeled features using Weighted Margin Support Vector Machines (WMSVMs) [20]. Wu and Srihari provide a few features associated with each of the top 10 most frequent classes in the ModApte split of the Reuters21578 data set. With WMSVMs, a macro-average break-even-point of around 0.53 is obtained using only this domain knowledge, and a macro-average break-even-point of around 0.60 is ob-tained using domain knowledge and 16 labeled examples ([20] Figure 3). Using the same domain knowledge, feature-voted-distributions , and no labeled documents, GE-FL at-tains a break-even-point of 0.630.
We also provide an informal comparison with tandem learn-ing [17], an active learning algorithm that incorporates feed-back on instances and features into learning with Support Vector Machines. We call the comparison informal because tandem learning is quite different from GE-FL. Importantly, GE-FL uses neither active learning nor labeled documents. In the referenced experiments, tandem learning uses a total of 12 labeled documents, and shows at most 100 features to the annotator. Both features and instances are actively selected to reduce uncertainty. Conversely, we use a static list of features, chosen before learning begins using unsu-pervised clustering. We compare performance on the 20 newsgroups data set. We use a one vs. all setup for better comparison. Raghavan et al. report macro-F1 of 0.354 ([17] Table 3). With 100 candidate features selected using lda-features , reference distributions estimated using association-voted-distributions ,andthe oracle-labeler , we attain macro-F1 of 0.477, averaged over 10 random splits of the data. This result is encouraging because it suggests that combin-ing GE-FL with active feature learning could produce even better results.
Finally, we conduct annotation experiments in which we time three users as they label 100 documents and 100 fea-tures for binary classification tasks. The candidate features are selected using lda-features . The features are presented one at a time, and the user can choose a single label for the feature or choose to discard the feature. After the users finish labeling features, they label documents, again with the option to choose a label for the document or to ignore the document if it appears ambiguous. We prefer this or-dering (labeling features followed by documents) in order to give maximum benefit to the traditional document labeling method. We choose documents to present to the user with uncertainty sampling: after each instance is labeled, the in-stance with the most uncertain classification under the cur-rent model is selected next for labeling. We do this to ensure that the instances chosen for labeling are beneficial. The list of candidate features is static.

First, we are interested in the accuracy of the human an-notators. Table 3 shows the labeling precision and recall for different annotators. For feature labeling, performance is measured using the oracle labeler as ground truth; for document labeling, performance is measured using the true Table 3: User labeling performance with respect to the oracle. med : blood, cancer, care, disease, doctor, doctors, drugs, health, medical, medicine, pain, patients, vita-min, yeast space : earth, launch, mars, mission, moon, nasa, or-bit, planet, satellite, shuttle, sky, space, universe ibm : hp, dos, ibm mac : apple, mac baseball : ball, baseball, braves, cubs, hit, hitter, jays, pitching, runs hockey : flyers, goal, hockey, leafs, nhl, period, shots labels. The labelers provided precise labels for documents, but also discarded many documents. Conversely, the label-ers were able to correctly label most features that the oracle considers relevant, but often also labeled other features. In-spection of these other features indicates that they are in fact moderately relevant. We defined the oracle to be con-servative when labeling features, only choosing features that are almost certainly relevant. These results indicate that we may be able to allow the oracle to be less discerning in fu-ture work and perhaps further i ncrease accuracy. User 2 had the most trouble selecting and labeling features. We suspect that this indicates insufficient familiarity with the learning tasks. This suggests that future experiments should involve an opportunity to look through the data before annotation. However, it does not seem unreasonable to assume that the annotators are familiar with the task they are trying to solve.
Figure 1 shows the accuracy of two trained systems over time. The first uses the labeled features and unlabeled in-stances with GE-FL. Reference distributions are estimated using Schapire-distributions with q maj =0 . 9. The second uses entropy regularization (ER) [8] (in this experiment we use direct maximization and weighting parameter  X  =0 . 01) with the labeled and unlabeled instances. Annotating fea-tures yields large accuracy improvements for the same amount of time. On average across all experiments, labeling features is 3.7 times faster than labeling documents, and the models trained with GE-FL have 1.0% higher final accuracy. Note that the point at which the GE-FL curve changes from a dotted line into dots indicates the point at which the user had processed all 100 features.

When the annotator is accurate, the results with feature labeling can be quite striking. For example, consider the re-sults of User 1 for the ibm vs. mac classification task. The accuracy of the GE-FL system after 30 seconds of feature la-beling is better than the accuracy of the ER system after 12 minutes of document labeling, a 24x speed-up. As another example, User 3 achieves accuracy of 90% on the baseball vs. hockey task after 90 seconds with the GE-FL system, at which point the ER system accuracy is around 50%.
Notice that the ER system gives erratic performance, with large accuracy jumps in consecutive 30 second intervals. This reinforces our earlier assertions about the brittleness of current semi-supervised methods.
In this paper, we have contributed GE-FL, a method for learning discriminative probabilistic models from labeled fea-tures and unlabeled documents. In experiments on text clas-sification data sets this method outperforms heuristic meth-ods that leverage labeled features. A preliminary user study supports the claim made in previous work [18] that it is much faster to label a feature than an instance. Consequently, GE-FL can provide dramatic decreases in the amount of time needed to train a classifier for a new domain.

In ongoing research, we are applying GE to models for structured output spaces and to the problems of active learn-ing and domain adaptation. We are also interested in in-corporating domain knowledge from ontologies and existing resources, and encoding task-specific structural constraints on the learning problem. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] M. Chang, L. Ratinov, and D. Roth. Guiding [3] D. Cohn, L. Atlas, and R. Ladner. Improving [4] A.Dayanik,D.D.Lewis,D.Madigan,V.Menkov, [5] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. [6] S. Godbole, A. Harpale, S. Sarawagi, and [7] J.Graca,K.Ganchev,andB.Taskar.Expectation given the same amount of annotation time. [8] Y. Grandvalet and Y. Bengio. Semi-supervised [9] A. Haghighi and D. Klein. Prototype-driver learning [10] Y. Huang and T. M. Mitchell. Text clustering with [11] R. Jin and Y. Liu. A framework for incorporating [12] T. Joachims. Transductive inference for text [13] D. Lewis and J. Catlett. Heterogeneous uncertainty [14] B. Liu, X. Li, W. Lee, and P. Yu. Text classification [15] G. Mann and A. McCallum. Simple, robust, scalable [16] A. McCallum, G. Mann, and G. Druck. Generalized [17] H. Raghavan and J. Allan. An interactive algorithm [18] H. Raghavan, O. Madani, and R. Jones. Active [19] R. Schapire, M. Rochery, M. Rahim, and N. Gupta. [20] X. Wu and R. K. Srihari. Incorporating prior [21] X. Zhu. Semi-supervised learning literature survey.
