 Automatically extracting previously unknown behavio r patterns from videos that track animals with various physica l conditions can accelerate our understanding of animal behavior s and their influential factors, resulting in major medical and economic benefits. Unfortunately, extracting behavior patter ns from videos recordings remains as a very challenging task due t o their extensive duration and the unstructured natures. Th is task is further complicated in a completely darken animal c age with inconsistent infrared lighting, moving reflections, or other cage debris such as the cage bedding. In this research, we propose a new motion model that enables us to measure the sim ilarities among different animal movements in high precision so a clustering method can correctly separate recurring movements from infrequent random movements. More specifically , our model first transforms the spatial and temporal features of animal movements into a sequence of color images, referred to as color motion maps ( CMMs ). The task of mining recurring behavior patterns is then reduced to clustering similar colo r images in a database. We will use a real infrared video to demo nstrate the capability of our model in capturing distinguished but brief animal movements that are embedded within a sequenc e of other animal movements. Binary motion map (BMM), color motion map (CMM), co lor autocorrelogram. Observations of lab animal behaviors have been hist orically used to understand animals and their interactions with e nvironment. While labor and time intensive, behavioral observat ion has been employed in studying adverse effects of new drugs o n brain functions, learning and memory, depression, and oth er neurological disorders [1], [4], [5], [7]. Electronic monitoring of animals offers an alternat ive in screening different animal activities. For example, a sensor or switch may be attached to sensatory devices such as running wheel , water bottle or cage floor to monitor running, drinking or walki ng activities. However, since sensors can only detect the presence of animals in a specific cage area, many previously unrecognized behaviors can remain undetected under this approach. With advances in digital image technology, surveill ance cameras can be easily mounted on an animal cage to indiscri minately and continuously track all movements that an animal mig ht make. Unfortunately, current approaches rely heavily on l ocation-specific knowledge, not animal movements, in extrac ting predefined activities from videos [4], [7]. For exa mple, biologists must manually specify which artificial region of an animal cage corresponds to what activity so an analysis program can label each video image with a predefined animal activity based on animal X  X  location in the image. As a result, errant and fluc tuating activities may be generated by the program whenever animals cr oss the boundary of artificial areas which may vary in each cage. Hence, automated detection of animal activity remains as a very difficult task. Our goal in this research is to develop an efficien t approach that can recognize subtle animal activities that could n ot be obtained before. In order to achieve this goal, we need a ne w model that not only can effectively capture the spatial and te mporal features of animal movements, but also can be efficiently tr ansformed into quantitative motion descriptors for further mining processes. Additionally, this new model must be robust and fle xible enough to handle the noisy videos that are contaminated by inconsistent infrared lighting in an otherwise dark cage, moving reflections or shadow cast by animals, or other debris clutters su ch as the cage bedding. Under our approach, the bright intensity of the HSI ( H ue-S aturation-I ntensity) color space is first extracted from each video image and converted into a black-and-white image be cause there is no color information in an infrared video. The s patial movement cast by animals over two consecutive video frames can then be easily obtained by using an XOR-operation. We refer the output produced by each XOR-operation as a binary motion map ( BMM ) because it reflects the spatial movement in a bin ary format. If we stack w number of BMM s together, each element of the stacked BMM s will contain w binary numbers that can be treated a color. As a result, the stacked BMM s become a color image, or a color motion map ( CMM ), that contains both the spatial and temporal information about the body mov ements in a duration w . Next, the spatial correlation of colors in each CMM can then be measured as a numeric vector which beco mes the signature of each CMM . In other words, the spatial and temporal features o f animal movements within a temporal period are first captur ed as a color image. The features embedded in each color image ar e then transformed into a numeric vector by measuring the spatial correlation of colors in the color image. As a resu lt, the task of mining recurring behavior patterns can be reduced t o the task of clustering similar color images in a database. We w ill use a real infrared video to demonstrate the capability of our model in capturing distinguished but brief animal movements that are embedded within a sequence of other animal movement s. This paper is organized in the following sections. Section 2 reviews related works. Section 3 discusses in detai l the proposed motion model. The time and space complexity of our model are also analyzed in this section. Section 4 explains t he procedures in clustering motion patterns and creating meaningful description for each cluster. We present experiment results in Sect ion 5 and demonstrate the capability of our model in capturin g distinguished but brief animal movements that are embedded within a sequence of other animal movements. Section 6 concludes this paper. Using computer vision technology in tracking animal movements can serve as a powerful tool for monitoring sentine l cages in potential bioterrorism targets and chemical agent r esearch facilities. Authors in [4] used top-mounted cameras to monitor fishes in a fish tank that is contaminated by a tox ic agent MS222 . The goal of this work is to study the behavioral al terations relevant for ecotoxicological assays. This study co mpared the velocity, total distance traveled, space utilizatio n of fishes in the contaminated and uncontaminated fish tanks. However , the approach used in [4] relied on the location-specifi c knowledge of the fish tank because it divided each fish tank int o several artificial areas for tracking fish movement. Authors from the Stanford University and Monterey B ay Aquarium used underwater cameras to study jelly fis h [8]. The goal of this project is to understand the body move ment of jelly fish so lead information can be generated to guide the underwater cameras in following the target jelly fish. Few exi sting types of jelly fish motion are first defined as motion state s and the relationships between these motion states are then connected as a finite state machine ( FSM ). Hence, this work is built based on predefined motion knowledge of jelly fishes. An alg orithm was constructed to recognize jelly fish movement by mat ching real-time video frames against the predefined motion sta tes . There are two unique features in the study discusse d in [2]. First feature is that a camera is used to track multiple mice in a single cage. The second feature is that the camera is plac ed on the side of a cage, not mounted on the top of a cage as in most of studies, to capture the side view of animals. Authors showed th at their algorithm, based on affine transformation, can crea te an approximate blob that continuously follows individu al mouse. An algorithm is developed to classify the shape featur es of each blob into one of the four well-defined mouse activities such as moving and drinking. Authors in [6] designed a system which can separate normal motions from abnormal ones in a video. Under their approach, the amount of motions in a video frame is first compute d as a 2-D motion matrix by taking the color differences betwe en this video frame and a pre-selected background frame. The tota l motion of a video frame is then computed by summing the motion in the 2-D motion matrix. The locality of motion is also compu ted by summing the values in the individual rows and colum ns of the 2-D motion matrix. A clustering method is then used to cluster these features into different activities. Unfortunately, changes in cage bedding made by animals can be falsely identified a s a movement if motion information is obtained by comparing vide o frames to a pre-selected background frame. Moreover, although c ertain motion locality can be obtained by summing the moti ons in rows and columns of a video frame, detailed body movemen t is lost in this approximate computation. As described in Section 1 an effective motion descr iptor that can closely characterize the spatial and temporal featu res of animal movements is needed so a clustering method can dete ct frequently recurred animal movements from the descriptors extr acted from a video. We first explain the underlying concepts of our motion model in Section 3.1. The method in extracting the motion descriptors from the proposed model is discussed in Section 3.2. Under our approach, the bright intensity (of the HS I color space) is first extracted from each video frame image beca use there is no color information in an infrared video. A threshold is then applied to convert the brightness values in each video fram e into a black-and-white image. Note that although the selection o f different thresholds may result in different black-and-white images with different distributions of noises, the impact of no ises can be greatly reduced by our motion model as we will expl ain later. The spatial feature of animal movement can then be easily captured by XOR-operations which compare the pixel-to-pixel differences between two consecutive frames. We refe r the output produced this operation as a binary motion map ( BMM ) because it represents the animal X  X  spatial movement over two c onsecutive time units in a binary format. Formally, each pixel at the r th row and the c th column of a BMM constructed from the two consecutive black-and-whit e images I and I t+1 is denoted as BMM t ( r , c ) = I t ( r , c )  X  I t +1 ( r , c ) We use the following figures to illustrate this ope ration. Let Figure 1 be a sequence of six-second binary (black-and-white) video frames that are converted from an original in frared video. To simplify our discussion, we focus our attention only on the pixels that are in the upper-right and lower-left c orners of each video frame. Figure 1. Consecutive video frames over six time un its. If we apply the XOR-operation to every consecutive pair of images in Figure 1, we obtain a sequence of BMMs shown in Figure 2. Again, we show only on the pixels that ar e in the upper-right and lower-left corners of each BMM in Figure 2. Figure 2. Binary Motion Maps ( BMMs ) computed from Figure 1. If we stack w ( w  X  1) number of BMM s together, the stacked BMM s can now be treated as a color image, or a color motion map ( CMM ), that contains both the spatial and temporal info rmation about the body movements in a duration w . Moreover, we will later prove our hypothesis through a series of expe riments that different movements will result in different color images ( CMM s) and similar movements will result in similar color images ( CMM s). That is, after constructing CMM s from a video, we can reduce the task of mining recurring behavior patter ns to the task of clustering similar color images in a database as we will show in Section 4. Formally, if we stack w number of BMM s (i.e. BMM the r th row and the c th column of a CMM t is denoted as CMM t ( r , c ) =  X  For the first few frames in a video where t  X  w , CMM the above stacking formula can be efficiently imple mented by a sequence of bit-shift operations. Similarly, the  X  computation in the above formula can also be easily implemented by a sequence of bit-OR operations. As a result, each pixel in a CMM must be in the range of 0 to 2 w  X  1, where w is the number of BMMs being stacked together. For example, after stacking together the upper-righ t pixels of BMMs in Figure 2, the stacked binary number is 10011, w hich can be interpreted as a decimal unsigned integer 19 . This unsigned integer can serve as an index that referen ces to a specific color (i.e. green) defined in a customized color ma p shown at the right edge of Figure 3. Similarly, after stacking t ogether the bottom-left corner pixels of BMMs in Figure 2, the binary number is 00110, or an unsigned integer 6, which reference s to a pink color in the color map. Note that a color map can b e created such that a lower index value (i.e. lighter color) indic ates a more recent movement in a CMM . Figure 3. Stacking Binary Motion Maps ( BMM s) of Figure 2 into a Color Motion Map ( CMM ). We use the following two figures, Figure 4 and Figu re 5, to show that different animal movements will be converted a nd represented by very different color images under ou r proposed model. The sub-figures (a) X (f) of Figure 4 show a sequence of video frames that were taken 500 milliseconds apart over three seconds, during which the mouse was running on the wheel in its cage. The frame numbers are also shown on the top of each vid eo frame. The sub-figures (g) X (k) of Figure 4 show BMMs obtained by taking the XOR of the black-and-white images conver ted from sub-figures (a) X (f). For example, sub-figure (g) of Figure 4 is the XOR-result of the black-and-white images of sub-fig ures (a) and (b), and sub-figure (h) of Figure 4 is the XOR-resu lt of the black-and-white images of sub-figures (b) and (c), etc. S ub-figure (l) is the CMM obtained by stacking BMMs from (g) to (k) of Figure 4. Figure 4. (a) X (f) show a sequence of video frames i n which the mouse was running on the wheel. (g) X (f) are BMMs constructed from (a) to (f). (l) is the CMM constructed from (g) to (k). Similarly, Figure 5 shows another 3-second video se quence during which the mouse got off the wheel and moved toward the other end of its cage. Figure 5. (a) X (f) show a sequence of video frames i n which the mouse left the wheel and moved toward the other end of the cage. (g) X (f) are BMMs constructed from (a) to (f). (l) is the CMM constructed from (g) to (k). Finally, under our model, we will compute a BMM for each consecutive pair of video frames, and stack every w consecutive BMM s together to generate a sequence of CMM s. For example, frames 21180 to 21185 shown in Figure 5 are used to generate a set of BMM s and a CMM shown in subfigures (g) to (l) of Figure 5. The next CMM will be generated from BMM s that are generated from frames 21181 to 21186. This is generally refer red to as a sliding window approach with the window size equal to w , the number of BMM s being stacked together. More specifically, for a video with m video frames, we need to compute BMM and CMM for ( m  X  1) and ( m  X  w  X  1) times, respectively. Note that although the sliding window approach is u sed in computing CMM s, the system only needs to keep one latest CMM at any time during the entire video processing. Thi s is because of the following two reasons. First, a new CMM can be easily obtained by shifting one bit of all the pixels in t he current CMM followed by an OR operation to integrate the latest BMM . Second, after constructing a CMM , the CMM will be immediately transformed and saved as a motion vector (see next subsection). As a result, there is no need to keep multiple CMM s in the computer memory. Since w &lt;&lt; m is usually the case, the space and time complexity in generating CMM s from a video with m frames are O ( n ) and O ( n  X  m ), respectively, where n is the number of pixels in a video frame. The space and time complexity for computing BMM s are the same. While Figure 4 and Figure 5 show the capability of the proposed CMM technique in distinguishing different animal activ ities, a method in quantifying CMM s into numeric vectors is needed so that data mining techniques can measure motion-simi larity of CMM s and objectively cluster movements into various an imal activities. Moreover, this quantifying method must be able to tolerate noises generated by the imperfect cage env ironment and the threshold used in converting each video frame i nto black-and-white image as discussed at the beginning of Sectio n 3.1. In this research, we adopt the Color Autocorrelogram ( CAG ) approach [3] [9] in converting a CMM into a vector. The basic idea of CAG can be informally summarized as follow: First, for each pixel p of a CMM , we calculate the probability of finding the same color from neighboring pixels of p that are at the chessboard then aggregated into a CAG vector based on individual colors. As a result, a color autocorrelogram expresses how the spatial correlation of colors changes with distance. A colo r autocorrelogram is different from a color histogram which captures only the color distribution in an image an d does not include any spatial correlation information. Next, we give the formal definition of color autoco rrelogram. Let M denote a set of all possible colors in a color mot ion map M . In our case the size of M c is 2 w , where w is the number of BMMs being stacked together. Furthermore, the i th color in M = ( x 1 , y 1 ) at the chessboard distance d = | p 1  X  p  X  y 2 |) = d . The color autocorrelogram of M with all the possible colors M c is denoted as: We use the following example to illustrate the colo r autocorrelogram. Figure 6 shows two different 3  X  4 CMM s with each pixel being either black or white (i.e. w = 1). For each pixel p , the probability of finding the same color in its immediate neighbors (i.e. d = 1) is also displayed. Note that the neighboring pixels that fall outside the CMM s are not counted in the probability calculation since their colors are pres umably unknown. For example, there are three pixels ( p 1,2 distance 1 from the pixel p 1,1 , the pixel at the top-left corner of the left CMM . Since the color of p 1,1 is white and there are two white pixels among its three neighbors, the probability o f p calculated as 2/3. If d = 2, the neighbors of p 1,1 are p p 3,2 , p 3,3 and the probability of p 1,1 is 4/5. Figure 6. Two simple CMMs and the probabilities of finding the same color in the immediate neighbors of each p ixel. After summing up the probability from each pixel ba sed on individual colors, the final CAG vector for the left CMM is: V &lt;0/8, 842/120&gt;, indicating the spatial correlation of the black and white colors in the left CMM . The CAG vector for the right image is V R = &lt;2/8, 800/120&gt;. Obviously, V L  X  V R CMM s are different. Next, we use real video frames shown in Figure 4 an d Figure 5 to demonstrate that different behaviors in a video can lead to very different CAG vectors. Figure 7.(a) and Figure 7.(b) give the CAG vectors that are derived from the CMM s shown in Figure 4.( l ) and Figure 5.( l ), respectively. As we can see that the CAG vectors of the two different mouse activities appear very diff erent. Note that since color 0 is the dominated white background col or of CMM s, the probability of finding white neighboring pixels is relatively high as indicated in Figure 7. Hence, we zoom in to show the detailed probabilities of colors 2-31 in Figure 7. Figure 7. (a) CAG vector of Figure 4 (mouse running on the wheel). (b) CAG vector of Figure 5 (mouse moving in the cage). There are few important advantages in transforming a CMM by using the color autocorrelogram approach. The first advantage is that the CAG vectors can be efficiently computed: From the abov e discussion, we know that we need to search the same color for each pixel in its maximum 8  X  d neighbors. Hence, the complexity for computing a CAG vector from a CMM of n pixels is O( n  X  d ). As a result, the complexity of generating all the CAG vectors from a video with m frames is O( m  X  n  X  d ). The second advantage of CAG is that it is invariant to rotation while retaining the spatial correlation of colors i n an image [2]. For instance, we will obtain the same CAG vectors even if we rotate the images in Figure 6. This feature is very important in identifying similar/dissimilar motions for the foll owing reason: although the same movements that occur toward diffe rent directions at different locations may appear as dif ferent color images, these color images should have the same (or very similar) color correlations and be converted into the same ( or similar) CAG vectors. The distance (dissimilarity) between two CAG vectors will increase only if the corresponding CMM s have very different color correlations that represent differe nt activities. The third advantage is that the impact of noise in videos can be decreased during the CAG computation. This is because the noise caused by inconsistent infrared lighting or other f actors usually appears at random locations in different video fram es. This spatial and temporal randomness make the CAG computation harder to find the same color in the neighbors of noise pixel s, reducing the influences of noise pixels in the final CAG vectors. As we explained before, it is unlikely the color au tocorrelogram will assign similar CAG vectors to represent very different animal movements. However, it is possible that different CAG vectors may be computed for slightly different animal movem ents that are considered as similar by human beings. For example, while biologists may consider there is only one type of g rooming activity, this activity may generate different CAG vectors due to the speed and the shape of body movement. This issu e can be addressed by the users when they attached meaningfu l annotations to the video frames which we will discuss in the ne xt section. After generating CAG vectors from a video, the task of mining recurring behavior patterns can now be reduced to t he task of clustering similar color images that are represente d by CAG vectors in a database. In this section we explain h ow to cluster CAG vectors and annotate each cluster (and its constit uent video frames) with useful descriptions. Figure 8 summariz es this process. The circles in Figure 8 indicate the sub-t asks of the mining process, while the boxes represent the data entities produced by those sub-tasks. In Figure 8, each pair of consecutive gray-scale vi deo frames are first converted to a BMM , and each sliding window of w BMM s are attached together to form a CMM and a corresponding CAG vector is extracted and saved in a database as we d iscussed in Section 3. A k -means method is then used to cluster the CAG vectors into k clusters. Note that scientists may not have prior k nowledge of how many distinct behaviors an animal with differen t physical conditions may display. Hence, the purpose of using k -means method in our clustering process is not to extract exactly k -types of different behaviors. Instead, the purpose is to summarize a prohibitedly long video into a manageable number of clusters such that each cluster contains similar video frame s with highly similar animal movements. Compute BMM s/ CMM s, Generate CAG vectors These k clusters enable scientists to focus their attentio ns on studying the few representative frames extracted fr om each cluster and creating meaningful annotations (in natural lan guage) for each cluster. The annotation created for each clust er is also automatically attached to each video frame belongin g to the cluster. Figure 10 to Figure 13 in the next section show some sample video frames that are automatically annotate d with the descriptions entered by a user. The annotations ass ociated with video frames can also be saved into a database for further behavioral analysis. The representative frames of each cluster can be au tomatically selected by the following steps. First, the center of each cluster is calculated by averaging all the CAG vectors in the cluster. Secondly, the l -nearest vectors to the center of each cluster are then identified as representative vectors of each c luster. Finally, the video frames correspond to those representative vectors are displayed to assist scientists enter meaningful des cription for each cluster. If a cluster has less than l vectors, movements in the video frames of this cluster can also be identified as ra re behaviors or outliers. In this section we apply our approach to extract be havior patterns from a 30-minute surveillance video to demonstrate the applicability of our approach. We also compute the precision of our approach at the end of this section. In this experiment, we set our system to take a vid eo shot every second and set the sliding window size to w = 3. More specifically, our system stacks every three consecu tive BMM s together to form a CMM (with maximum 2 3 different colors) and to generate a CAG vector. We then use the k -means method to create k = 100 clusters and compute their centers. For each cluster, our system displays l = 3 representative frames and asks users to enter a short description to describe the animal movements in the cluster. Figure 9. (a) Three representative frames of cluste r 2. (b) Three representative frames of cluster 36. Figure 9 (a) and (b) show the three representative frames for cluster 2 and cluster 36, respectively. Although th ose frames are clustered into two clusters based on their CAG vectors (i.e. CMM s), they simply show different stages of wheel-runn ing activities. Hence, we enter the description  X  X heel X  for both clusters. If more detailed separation on the genera l wheel activity is needed, different descriptions, such as  X  X low-wh eel-activity X  and  X  X ast-wheel-activity X , can also be entered for individual clusters. After entering descriptions for all clusters, scien tists can then study the videos that are automatically annotated w ith the descriptions. We use next few figures to show few s hort periods within the 30-minute video that contain different m ouse activities. Figure 10. (a)  X  X heel X  activity that comes from two different clusters (2 and 36) over a three-second period. (b) The plot of the CAG vectors from these three frames. Figure 10.(a) shows the mouse running on the wheel over three consecutive video frames (3 seconds). The CAG vectors of these frames are also given in Figure 10.(b). Note that a lthough the descriptions for these frames indicate the  X  X heel X  activity, these frames actually belong to two different clusters be cause the CAG vector derived from the frame 40 is different from the vectors in the two other frames. Similarly, Figure 11 and Figure 12 show two differe nt kinds of mouse activities: grooming and quite (inactive). Figure 11. Grooming activity over a three-second pe riod. Figure 12. Inactive mouse over a three-second perio d from frame 491 to 493. Figure 13 at the end of this paper (next page) show s a sequence of video frames over a short 10-second period. During this period the mouse displayed multiple brief activities that are embedded in a sequence of other movements: It started with a lo ng period of running on the wheel (frame 905 X 906 and earlier fra mes), got off the wheel (frame 907 X 908), quickly turned around (f rame 909), and moved back toward the wheel (frame 910 X 912), an d eventually ended up running on the wheel again (fra me 913 X 914 and more later frames). This series of video frames demonstrates the capability of our motion model in correctly cap turing very brief but distinguished motions that are embedded i n a long sequence of video frames. Finally, after our system automatically generates t he annotated video, we went through each video frame to manually verify whether the automatically attached annotation match es the content of the frame. We found that, out of 1793 video fram es (29.88-minute video), 1576 frames are correctly clustered and annotated. This gives us 88% of precision rate. We believe tha t this precision rate can be further improved if video frames are ta ken in a higher rate (i.e. 500 ms) or a larger w is used in stacking BMM s together into a CMM . We are planning to conduct more experiments to verify our hypothesis. In this paper we present a new motion model that ca n efficiently capture the spatial and temporal features of animal movements from a video. Our model converts those spatial and temporal motion features into a sequence of color images tha t we refer to as color motion maps ( CMM s). Each CMM is then transformed into a color autocorrelogram ( CAG ) vector that measures the spatial correlation of colors in a CMM . A clustering method can then be used to cluster similar CAG vectors into clusters. The objective of each cluster is to collect video frames that contai n similar animal motions. A small number of representative frames fr om each cluster can then be automatically selected and pres ented to scientists for soliciting annotations. Solicited an notations are then automatically attached to the video frames of the c luster for further behavior study. Our experiments show that our model is able to iden tify different animal activities in high precision (88%) from surv eillance videos, even when activities are brief and embedded in between many other activities. We are planning to study the precision of our approach by taking video in a higher rate or se tting a larger w in stacking BMM s together into a CMM . Under our current approach, scientists must first c apture videos before they can perform the off-line analysis of an imal X  X  behaviors. This off-line analysis can only provide feedbacks to the scientists after an experiment ends. Moreover, this approach also requires large disk space to store long videos. Hen ce, we are also planning to apply some classification methods, such as the decision tree method, to extract rules from the CAG vectors generated in this research. We hope that the extrac ted rules can enable us to perform online analysis in which anima l movement in a video frame can be classified into certain type o f activities in real-time. This online analysis can provide much fa ster feedbacks to scientists who can then adjust the experiment se ttings based on the early results. The authors wish to express their gratitude toward the anonymous reviewers for their valuable comments. [1] S. Belongie, K. Branson, P. Dollar, and V. Rabaud, [2] R. C. Gonzalez, and R. E. Woods,  X  Digital Image [3] J. Huang, S. R. Kumar, M. Mitra, W. J. Zhu, and R. Zabih, [4] A. S. Kane, J. D. Salierno, G. T. Gipson, t. C. A. Molteno, [5] Y. Liang, V. Kobla, X. Bai, and Y. Zhang,  X  One Step [6] Junghwan Oh, and B. Bandi,  X  Multimedia Data Mining [7] L. Noldus,  X  Observing Behavior by Computer  X , Scientific [8] A. Plotnik, and S.M. Rock,  X  Quantification of cyclic motion [9] Qi Zhao and Hai Tao,  X  Object tracking using color Chih Lai received the PhD degrees in computer Scien ce from Oregon State University in 1999. He is an associate professor at University of St. Thomas. His research interests in clude data mining, multimedia databases, neuroinformatics, and real-time systems. Formally, he worked as a principle softwar e engineer on an FAA project and received various U.S. and Europe an patents on aircraft collision avoidance systems. He is a me mber of IEEE Computer Society and ACM SIGKDD. Taras Rafa is a research assistant at University of St. Thomas. He received the PhD degree in mathematical modeling an d computational methods from Ternopil State Ivan Pul X  uj Technical University (Ukraine) in 2003. He then worked as an assistant professor in the Computer Science and Biotechnical Systems department in this university. His research interes ts are in data mining, computed tomography, and image processing. He is a member of IEEE. Dwight E. Nelson is on the faculty in Biology and c onducts neuroscience research at the University of St. Thom as. His research focuses on mammalian (mouse) circadian rht yhmicity and behavior . He earned a Ph.D. from the Departm ent of Neurobiology and Physiology at Northwestern Univers ity in Evanston, IL, and conducted post-doctoral work at t he University of Virginia Center for Biological Timing (Charlotte sville, VA) and the Pharmaceutical Products Division at Abbott Laboratories (North Chicago, IL). His research is supported by the National Institute of Mental Health. 
