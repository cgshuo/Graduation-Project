 Searching for interested and/or useful info rmation has become an indispensable part providing people with adequate level of support. Consider the following example case. When a Ph.D. student, who is very interested in Dr. Wyatt X  X  talk in an academic that he has already forgotten the speaker X  X  name. The only thing he remembers is that the speaker was a student of Prof. Roger and his name began with  X  X  X . To help recall the name of Wyatt quickly, the student ma y adopt the traditional information retrieval way, say, by typing the words  X  X rof. Roger X ,  X  X tudent X  and  X  X  X  into the search bar and then the system returns a list of related pages which contain these words, as shown in Figure 1(a). However, these pages usually are not close to the user X  X  need as there are many noises in them. Thus, such word matching based search engines may cost users a lot of time to obtain the information they really want. 
An alternative way for searching is based on the relations between words. Such a kind of search engines, denoted as relation-based search engines, can provide the information associated with the input word s more effectively. For example, when  X  X rof. Roger X  is typed, the search engine may return its related terms, such as univer-sity, laboratory etc., as shown in Figure 1(b). With the help of a relation-based search engine, it is possible for users to achieve their goals more accurately. Yet only using Consider the  X  X yatt X  example again, the student may only remember the beginning character  X  X  X  of the name while the information around  X  X rof. Roger X  contains four similar terms with  X  X  X , thus it may still require the student to check all the four terms. Roger X , such as  X  X rof. Roger was born in Waadt X  etc., are well specified, it becomes much easier for the student to find out the destined name (e.g., Wyatt). In general, it can be shown that through the association relations and their specifications users will be able to reach their searching goals more quickly and accurately. Hereafter, we call this kind of search engines as association relation-based search engines. 
Based on the theories in linguistics and cognitive psychology, this paper proposes a novel mechanism for automatically learning and specifying association relations be-tween words. This mechanism, which we term as ALSAR, can obtain accurate associ-ation relations from the perspective of verb valency grammar [1], and from specifying these association relations with suitable verbs. 
The remainder of this paper is organized as follows. In Section 2, we briefly dis-cuss some related work. Section 3 gives the problem definition of our work. Section 4 presents the detailed processes of learning and specifying association relations be-tween words. Evaluations are carried out in section 5. Finally, we conclude our work in Section 6. There have been quite some works which are devoted to extracting semantic relations between words. With the consideration of different analysis methodologies, we divide these works into four categories, as discussed below. 1) Statistics-based models include EFCM (Element Fuzzy Cognitive Map) [2], PSR (Power Series Representation) [3], PLSA (Probabilistic Latent Semantic Analy-sis) [4], etc. EFCM and PSR are cognitive-based text knowledge representation mod-els, where word relations are obtained based on traditional association rule mining them need training sets, such as PLSA. 2) Search engine-based models: Bollegala et.al [6] proposes a robust semantic si-milarity measure that uses the information available on the Web to measure similarity between words. Bunescu et.al [7] proposes a supervised learning approach for relation extraction, which is based on a handful of training examples and the Web corpus. These search engine-based models are supposed to acquire more semantics than sta-tistics-based models but they are largely dependent on the results of search engines. 3) Syntax-based models: SRL (Semantic Role Labeling) [8] is a broadly employed text mining technique, as it allows the addition of structured semantic information to mining and achieves high accuracy by experimenting with support vector machine as a supervised approach. Oren Etzioni et.al [10] proposes the Open Information Extrac-tion (Open IE) paradigm to develop unlexicalized, domain-independent extractors on English corpus. These models are capable of obtaining rich semantics from texts but they depend on a generic syntactic/structural organization. 4) Expert knowledge-based models include MindNet [11], WordNet [12] and RDF (Resource Description Framework) [13]. This kind of works is excellent in acquiring of these works are based on a corpus defined by humans, which costs a lot of human efforts and time. 
With respect to the features of the above four models, we know that relations ob-tained through expert knowledge-based models possess the most abundant semantics while the ones extracted by statistics-based models have the poorest semantics. How-ever, among all of the four, only statistics-based models have an automatic and inde-pendent extraction process, which means that the other three have limitations when applied into a dynamic and large scale environment. Meanwhile, while expert know-ledge-based models have a good ability of acquiring relations with rich semantics, those relations X  semantics are constrained to a pre-established area (for example,  X  X arent Of X ,  X  X s A X ,  X  X ubject-Predicate X  and so on). Compared to the unlimited kinds of relations in real-world, the amount of relation types in expert knowledge-based models seems too small. For our work, how to automatically and independently dis-cover relations with abundant semantic types is a fundamental and most important task. In this section, we formulate the problem of automatically learning and specifying d }, a latent relation space, LRS(d i ) , which represents all of the possible specific rela-tions in d i , is defined as a six-tuple: edges that further explain specific meanings of association relations through verbs, :() N NN PE  X   X  X  is a function that maps the relations between nouns to their function that maps association relations which are specified with verbs, to their values ( P(E NV ) being the power set of E NV ). 
At the beginning of our proposed approach, each document d i is represented in the V relations in E N . However, all the values in the power sets of both E N and E NV are in-vV  X  X  , we have () ,0
Based on the theories in statistics, linguistics and cognitive psychology, we aim to whether there exists a relation between two n ouns, or justifying whether a verb is fit to specify a relation. Our proposed ALSAR is responsible for producing ALSAR  X  d relation set as output, subjected to the following condition: where  X  and  X  are some predefined thresholds. The following section gives de-tailed discussions on how to learn and specify association relations between words. As mentioned early, we make use of verb valency grammar [1] to automatically learn ments controlled by a verbal predicate. Co nsider, for example, the following two sen-tences: Sentence 1:  X  Michael 1 wins the football game 2 . X  Sentence 2:  X  Jackie 1 gives Sara 2 a flower 3 . X  
The two sentences show that the nouns depending on  X  win  X  include  X  Michael  X  and  X  flower  X . Thus, the valences of  X  win  X  and  X  give  X  are 2 and 3, respectively. Different from the theories in universal grammar proposed by Chomsky [14], verb valency grammar reflects the most basic dependence relations between nouns and verbs. 
Consequently, on the basis of verb valency grammar, the process of learning and specifying association relations is conducted to three steps, which are discussed in the following subsections. 4.1 Acquiring Verb Dependent Sets Based on Cognitive Psychology We first present a detailed discussion on how to automatically acquire verb dependent sets ( VDS ) for each sentence. Although verb valency grammar only describes the fundament dependent relations between nouns and verbs, it is still a challenge for machines to identify whether a noun is dependent on the verb around it. For example, consider a sentence as follows, Sentence 3:  X  X n order to please Mary 1 , William 2 went out and bought a bunch of roses 3 in a flower shop 4 on Friday 5 . X  
There are five noun phrases and three verbs in this sentence. Taking the verb phrases as the dependents of the verb. 
When referring to verb valency grammar, we find the valences of almost all verbs distribute in the range of 0 to 4. Many verbs have the valences of 1, 2 and 3, while a few verbs take four arguments. There is hardly any verb possessing a valence over 5. working memory. Miller X  X  Law pointed out [15] that the number of objects an aver-age human can hold in working memory is seven, plus or minus two. Also, the span is around seven for digits, around six for letters and around five for words [15]. Thus, it total amount of both the verb and its dependents will exceed the capacity of authors X  working memory. As a result, based on Miller X  X  Law in cognitive psychology, we define the Max Valency Number ( MVN ) to be 4 for each verb, i.e., MVN =4. 
However, it is still hard for machines to select suitable nouns to a verb X  X  dependent kind of nouns is the most possible arguments controlled by a verb. In linguistics, there adverbial, place adverbial and etc. It is a di fficult job for machines to understand that. Thus, through observations on abundant sentences, we conclude below three features that machines can perceive: 1) the nouns or noun phrases that are led by prepositions are always set as time ad-verbials, place adverbials or some other adverbials; tween the subjects/objects and the verb is small; 3) there is always only one verb among the nouns or noun phrases which belong to the same verb X  X  dependent set unless there are conjunctions around that verb. 
Note that the above three features show the different importance of the nouns or have more contributions to  X  bought  X  than the nouns  X  flower shop  X  and  X  Friday  X , the matically generate dependent set for each verb without involving with any human effort. Verb dependent sets ( vds ) will greatly facilitates efficient acquisitions of asso-ciation relations between words, since it a llows confining and reducing the search space to a large extent, as to be discussed next. 4.2 Acquiring Association Relations between Words association relations. Compared with traditional selections of transaction set (such as set of sentences or sliding windows), verb dependent set provides a much stronger binding among nouns or noun phrases, which is very useful to the subsequent associa-tion relation mining. In this paper, our proposed algorithm removes the threshold of support as it is affected by the size of transactions. Meanwhile, to alleviate the effects resulting from removing this threshold, we include a check strategy before picking noun pairs from verb dependent sets. Through removing support threshold and adding the check strategy, it becomes possible for ou r proposed association relation acquiring algorithm to be insensitive to the lengths of texts. 
The specific problem the check strategy needs to tackle is as follows. Given a verb other according to the characteristics of a lan guage. It should be noted that the setting of the check strategy is language dependent, and in this paper, we mainly focus on the characteristics in Chinese. Herein, the check strategy we use is described as follows: 
Through observations on Chinese news, we find that those nouns appearing in both sides of a verb tend to be associated with each other while those nouns appearing in the same side of a verb are likely to have a lower association power among them. This phenomenon can also be explained in English. Consider the following sentence: Sentence 4:  X  X oth Jackie 1 and William 2 bought Mary 3 a bunch of roses 4 . X  
Through Section 4.1, the dependent set of the verb  X  bought  X  can be obtained, which is { Jackie , William , Mary , rose }. It is apparent that the relation between  X  Jack-only consider four relations rather than six relations ( 2 4 6 C = ), which largely reduces the complexity of the subsequent learning process. 
Based on vds and the above check strategy, we treat vds as transaction set and then use Apriori [5] to further mine association relations, which compose of the learnt association relation set, denoted as lars . Compared with traditional association rule mining (such as Apriori [5] and FP-Tree [16]) whose candidate space increases exponentially with the number of words, our algorithm obtains a much smaller candidate space, thereby the complexity of learning association relations is largely reduced. 4.3 Specifying Association Re lations between Words So far there have been many works devoted to specifying the relations between words, including WordNet[12], RDF[13] etc. Relations including  X  X arent Of X ,  X  X s A X ,  X  X ubject-Predicate X  and  X  X ause-Effect X  considered by them though useful for NLP appropriate for general-purpose information retrieval tasks universally. For example, the news event  X  melamine milk powder  X  in China, the noun  X  melamine  X  was not hard for WordNet to use  X  contain  X  or  X  include  X , which is a lot closer than  X  Part Of  X  as per human X  X  understanding, in describing the relation between  X  milk powder  X  and  X  melamine  X . Additionally, all of the relations in WordNet or RDF are defined by ex-perts and thus incur a lot of human efforts. 
This subsection describes a method for sp ecifically and automatically explaining the relations from the perspective of verbs. Based on the sets of verb dependents and There are several advantages of using verb s to express the meanings between words: demands than those pre-defined relation types; 3) verbs are possible to catch up with the rapid information updates on the Web; and 4) these verb descriptions are possible to be automatically derived without any human effort. These advantages of using verbs to specify relations demonstrate a good application prospect. Detailed discus-sions are given as follows. lars i can be obtained (ref. Section 4.1 and 4.2). The objective of our association rela-sars n n v n n lars v V = X  X  X  X  . The process of acquiring sars i can be divided into two steps: 1) find verb candida te set for each association relation, and 2) select proper verbs having large contributions from each candidate set. 
Based on vds i and lars i obtained in Sections 4.1 and 4.2, we are able to use statis-tical method on these two sets to acquire verb candidate set for each association rela-After the process of acquiring vcs , we select proper verbs from it according to the co-occurrence of the association relation and the verbal descriptor. 
Overall, relations obtained by applying algorithms in Section 4-5 (which, altogeth-er, constitute our ALSAR mechanism) are very useful for machines to express the semantics in texts, thereby providing better Web services to users of diverse applica-tions including information retrieval, semantic web, personalization, news recom-mendation, etc. Evaluations on ALSAR are carried out and reported in the next section. 5.1 Evaluation Methodology Instead of trying to compare all kinds of the related algorithms reviewed in Section 2, which would be impossible due to limited accessibility and space available, we main-ly focus on comparing our proposed ALSAR with the traditional association rule min-plan to apply the relations into a specific application, namely, classification. The ex-perimental process is conducted in three steps: 1) We use words and their relations to represent each document in a given docu-grees (which include both in-degrees and out-degrees) in the network of relations between nouns. Weights of these words are defined as follows: word in d i , and s is the number of words in d i . 2) Assuming there are m topics in the data set, we randomly select a document for each topic and then mix all of the topics together. For each document d i in the mixed similarity, defined as: number of features. Herein, features are co mprised of two kinds: words, and unspeci-fied or specified asso ciation relations. Weights of words are as defined in eq.(4) while eq.(5), the similarities between each pair of the selected documents belonging to dif-ferent topics can be obtained. Through these, we allocate document d i under the topic to which the document most similar to d i belongs. We repeat this process until all of the documents are allocated. 3) Based on the above classification, we estimate the results and further evaluate the performance by using precision, recall and F-measure. Through these measures we are able to estimate the classification results for each algorithm to be compared with. 5.2 Data Set Our research group has crawled and accumulated, since 2009, more than 25,000 top-ics comprised of more than 150,000 Chinese news webpages from Baidu News (see details from http://wkf.shu.edu.cn). 
Rather than applying all of the 150,000 Chinese news webpages into our classifica-tion evaluation about fifty webpages.) Based on these Chinese news webpages and the evaluation methodology discussed in Section 5.1, the following evaluations are carried out. 5.3 On Learning Association Relations Based on the methodology discussed in Section 5.1, Figure 2 gives a comparison between the traditional association rule mining ( TARM ) algorithm [5] and our pro-posed ALSAR . Herein, WR_TARM refers to those classification results that are based on both of the words and association relations extracted by the traditional association rule mining algorithm; meanwhile, WR_ALSAR denotes the classification results should be noted that all of the words are extr acted based on association relations so as to avoid the influences brought by othe r words extraction algorithms, such as TFIDF .. In order to reduce the influences brought by the random selection of cores, all of the recalls and f-measures appearing in figures are all referred to their average values. 
From Figure 2(a), we can see that for WR_TARM under the same confidence thre-shold, all of the three metrics (precision, r ecall and f-measure) decrease very fast with the increase of support thresholds. It indicates that the performance of the traditional method is greatly affected by the settings of the support threshold. This is understand-are directly affected by the length of text, yet the lengths of Chinese news webpages in our data set vary very much. So when the support threshold increases, few associa-tion relations will be left in those large-sized news webpages, which result in the bad algorithms are probable to obtain unstable performance when applied into an envi-ronment containing various lengths of texts. For our proposed ALSAR , the argument of support has been replaced by a newly proposed check strategy, according to the characteristics of a language. This makes our proposed ALSAR to be more feasible and appliable to a practical environment in which the documents vary diversely in length. 
Figure 2(b) gives a comparison between WR_TARM -based and WR_ALSAR -based classification results. It can be seen that under different confidence thresholds, all the texts. Nevertheless, from Figure 2(b), it is also apparent that in each confidence thre-shold, WR_ALSAR obtains distinctively better performance than WR_TARM . In other ones obtained by the traditional association rule mining algorithm. 5.4 On Specifying Association Relations ALSAR . The basic idea is to compare the differe nces between the association relations that have been specified with the ones that have not been specifi ed. If specified asso-task of classification. Based on this idea, a comparison among W_ALSAR -based, WR_ALSAR -based and WSR_ALSAR -based classification results has been carried out and is shown in Figure 3. Here, WSR_ARSAR means that the news webpages in the task of classification are represented by both of the words and the specified association relations mined by ALSAR . From Figure 3, it is obvious that WSR_ALSAR -based classification obtains the best performance among all the three, while W_ALSAR -based classification has the worst performance and WR_ALSAR is in the middle. Thus, it can be inferred that have a better understanding of the texts. 
In addition, Figure 4 gives a comparison among WR_TARM -based, WR_ALSAR -based and WSR_ALSAR -based classification results under different confidence thre-sholds. It can be seen that, among all of the confidence thresholds, F-Measures obtained by WSR_ALSAR always stay above the ones obtained by WR_ALSAR, and likewise, WR_ALSAR stays above WR_TARM in terms of F-Measures. Consequently, machines to strengthen the ability of text understanding. As the Web enters big data era, it is important to provide an effective support for up-per level Web applications, such as information retrieval, knowledge representation, news recommendation etc. Based on verb valency grammar from linguistics and Mil-ler X  X  Law in cognitive psychology, we have developed an automatic association rela-tion learning and specifying mechanism, ALSAR, which incorporates two main processes: learning association relations, an d specifying the relations with verbal de-scriptors. Different from the tradition association rule mining, we apply verb valency grammar and Miller X  X  Law into the processes of ALSAR , so as to ensure that the rela-tions learnt are well specified, and can be established automatically without human efforts. Experiments on webpages crawled from Baidu News demonstrate better per-formance of ALSAR in comparison with the traditional association rule mining. 
For our future study, we plan to construct reasoning rules based on the specified association relations. We also plan to evaluate our mechanism on more data sets, in-cluding UCI KDD archive [17] and TREC corpus [18]. 
