 The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents  X  for example all pairs of a person and her birth-date. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this pa-per, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoreti-cal analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system Leila .
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing -text analysis; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Design, Experimentation, Theory Relation Extraction, Pattern Matching, Machine Learning
Many data mining tasks such as classification, ranking, recommendation, or data cleaning could be boosted by ex-plicit formalized world knowledge. Unfortunately, the man-ual construction and maintenance of such knowledge bases is a limiting factor in our modern world of  X  X xploding informa-tion X . Hence it seems tempting to exploit the World Wide Web and other poorly structured information sources for Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. automatically acquiring ontological knowledge. In this con-text, a first step could be to extract instances of a given tar-get relation from a given Web page corpus. For example, one might be interested in extracting all pairs of a person and her birth date (the birthdate -relation), all pairs of a com-pany and the city of its headquarters (the headquarters -relation) or all pairs of an entity and the class it belongs to (the instanceOf -relation).

The most promising techniques to extract information from unstructured text seem to be natural language pro-cessing (NLP) techniques. Most approaches, however, have limited the NLP part to part-of-speech tagging. This paper demonstrates that information extraction can profit signifi-cantly from deep natural language processing. It shows how deep syntactic structures can be represented suitably and it provides a statistical analysis of the pattern matching ap-proach.
There are numerous Inform ation Extraction (IE) ap-proaches. Some focus on unary relations (e.g. on extracting all cities from a given text [13, 7]). In this paper we pur-sue the more general binary relations. Some systems are designed to discover new binary relations [21]. However, in our setting, the target relation is given. Some systems are restricted to learning the instanceOf -relation [11, 4]. By contrast, we are interested in extracting arbitrary relations (including instanceOf ). Whereas there are systems that re-quire human input for the IE process [24], our work aims at a completely automated system. There exist systems that can extract information efficiently from formatted data [15, 14]. However, since a large part of the Web consists of nat-ural language text, we consider in this paper only systems that accept unstructured corpora. As initial input, some systems require a hand-tagged corpus [17, 31], manually as-sembled text patterns [34] or hand-chosen templates [32]. Since manually t agged input amounts t o huge human ef-fort, we consider here only systems that do not have this constraint. Some systems do not work on a closed corpus, but make use of the full Web for the IE process [12, 9]. De-spite the more powerful setting, these systems use extraction techniques similar to the other approaches. In order to study these extraction techniques in a controlled environment, we restrict ourselves to corpus-based systems for this paper.
One school of extraction techniques concentrates on de-tecting the boundaries of interesting entities in the text [7, 13, 35]. This usually goes along with the restriction to unary target relations. Other approaches make use of the con-text in which an entity appears [10, 5]. This school is re-stricted to the instanceOf -relation. The only group that can learn arbitrary binary relations is the group of pattern matching systems. The huge majority of them [12, 1, 23, 3, 28, 33] uses only a shallow linguistic analysis of the cor-pus. Consequently, most of them are extremely volatile to small variations in the patterns (see the conclusion of [23] for an example). Furthermore, these approaches cannot benefit from advanced linguistic techniques such as anaphora reso-lution. The few approaches that do use deep NLP [6, 27] consider only the shortest path in the dependency graph as a feature. Thus, these systems cannot deal with the dif-ference between  X  A dog is a mammal  X  (which expresses the subConcept -relation) and  X  This dog is a nag  X (whichdoes not). None of the pattern matching approaches provides an analysis of the influence of false positive patterns.
There exist different approaches for parsing natural lan-guage sentences. They range from simple part-of-speech tagging to context-free grammars and more advanced tech-niques such as Lexical Functional Grammars, Head-Driven Phrase Structure Grammars or stochastic approaches. For our implementation, we chose the Link Grammar Parser [26]. It is based on a context-free grammar and hence it is simpler to handle than the advanced parsing techniques. At the same time, it provides a much deeper semantic struc-ture than the standard context-free parsers. Figure 1 shows a linguistic structure produced by the Link Parser (a link-age ). A linkage is a connected planar undirected graph, the nodes of which are the words of the sentence. The edges (the links ) are labeled with connectors . For example, the connec-tor subj marks the link between the subject and the verb of the sentence. The linkage must fulfill certain linguistic con-straints. These are given by a link grammar , which specifies which word may be linked by which connector to preceding and following words. The parser also assigns part-of-speech tags . For example, in Figure 1, the suffix  X  .n  X  X dentifies  X  composers  X  as a noun.

We say that a linkage expresses arelation r , if the under-lying sentence implies that a pair of entities is in r .Note that the deep grammatical analysis of the sentence would allow us to define the meaning of the sentence in a theoret-ically well-founded way [22]. For this paper, however, we limit ourselves to an intuitive understanding of the notion of meaning. The problem of the corpus containing sentences that are not true is outside the scope of this paper.
We define a pattern as a linkage in which two words have been replaced by placeholders. Figure 2 shows a sample pattern with the placeholders  X  X  X  X nd X  Y  X . We call the (unique) shortest path from one placeholder to the other the bridge ,markedinboldinFigure2. Apattern matches a linkage if the bridge of the pattern appears in the linkage, although nouns and adjectives are allowed to differ. For example, the pattern in Figure 2 matches the linkage in Figure 1, because the bridge of the pattern occurs in the linkage, apart from a substitution of  X  great  X  X y X  mediocre  X .
If a pattern matches a linkage, we say that the pattern pro-duces the pair of words that the linkage contains in the posi-tion of the placeholders. In our example, the pair  X  Chopin  X  / X  composers  X  is produced.
As a definition of the target relation, our algorithm re-quires a function that decides into which of the following categories a pair of words falls:  X 
An example for the target relation. For instance, for the birthdate -relation, the examples can be given by a list of persons with their birth dates.  X 
A counterexample .Forthe birthdate -relation, the counterexamples can be deduced from the examples (e.g. if  X  Chopin  X / X  1810  X  X sanexample,then X  Chopin  X /  X  2000  X  must be a counterexample).  X 
A candidate .For birthdate , the candidates would be all pairs of a proper name and a date that are not an example or a counterexample (e.g. if  X  Mozart  X  X snotin the examples, then  X  Mozart  X / X  2000  X  is a candidate).  X  None of the above.
 The corpus should be a sequence of natural language sen-tences. These sentences are parsed, producing a deep gram-matical structure for each of them. In principle, our algo-rithm does not depend on a specific parsing technique. For example, the parse-trees produced by a context-free gram-mar can serve as grammatical structures. Here, we use link-ages. The core algorithm proceeds in three phases: 1. In the Discovery Phase , it seeks linkages in which an ex-ample pair appears. It replaces the two words by place-holders, thus producing a pattern. These patterns are collected as positive patterns . Then, the algorithm runs through the sentences agai n and finds all linkages that match a positive pattern, but produce a counterexample.
The corresponding patterns are collected as negative pat-terns 1 . 2. In the Training Phase , statistical learning is applied to learn the concept of positive patterns. The result of this process is a classifier for patterns. 3. In the Testing Phase , the algorithm considers again all sentences in the corpus. For each linkage, it generates all possible patterns by replacing two words by placeholders.
If the two words form a candidate and the pattern is clas-sified as positive, the produced pair is proposed as a new element of the target relation (an output pair ). Although usually the Discovery Phase and the Testing Phase are run on the same corpus, it is also possible to run them on two distinct corpora.
The central task of the Discovery Phase is determining patterns that express the target relation. Since the linguis-tic meaning of the patterns is not apparent to the algorithm, it relies on the following hypothesis : Whenever an example pair appears in a sentence, the linkage and the correspond-ing pattern express the target relation. This hypothesis may fail if a sentence contains an example pair merely by chance, i.e. without expressing the target relation. In this case we would use the pattern as a positive sample for the general-ization process, although it is a negative one. Analogously, Note that different patterns can match the same linkage. a pattern that does express the target relation may occa-sionally produce counterexamples. In this case, the pattern is used as a negative sample in the generalization process. We call these patterns false samples . The problem of false samples is intrinsic for pattern matching approaches in gen-eral. However, we show that false samples do not question the effectiveness of our approach.

Most learning algorithms can deal with a limited num-ber of false samples. For Support Vector Machines (SVM), the effect of false samples has been analyzed thoroughly in [8]. In general, SVM are highly tolerant to noise. There are also detailed theoretical studies [2] on how the proportion of false samples influences a PAC-learner. In essence, the number of required samples increases, but the classification is still learnable. It is also possible to understand the con-cept of positive patterns as a probabilistic concept [19]. In this setting, the pattern is not classified as either positive or negative, but it may produce pairs of the target relation with a certain fixed probability. The task of the learner is to learn the function from the pattern to its probability. [25] shows that probabilistic concepts can be learned and gives bounds on the number of required samples. The following subsection considers a particularly simple class of learners, the k-Nearest-Neighbor-classifiers.
A k-Nearest-Neighbors (kNN) classifier requires a distance function on patterns. We consider a simple variant of an adaptive kNN classifier: In the Discovery Phase, a newly discovered pattern becomes a prototype for a whole class of new patterns. Whenever another pattern is discovered, we check whether its distance to an existing prototype is be-low some threshold  X  . We say that the pattern falls on the prototype 2 . If the new pattern does not fall on an existing prototype, it becomes a prototype on its own. After the Discovery Phase, we label a prototype as positive if the ma-jority of the patterns that fell on it were positive, as negative else.

In the Testing Phase, we find for each test pattern its closest prototype. If there is no prototype within the dis-tance  X  , the pattern is classified as negative. If it falls on aprototype p , the pattern is classified as positive if p has a positive label and as negative else. We are interested in the probability that a test pattern is classified as positive, although the produced pair is not in the target relation.
In the Testing Phase, each possible pattern is generated for each sentence in the corpus (this will be a number of patterns quadratic in the number of nouns in the sentence). We model the sequence of all these patterns as a sequence of N random events. Each pattern produces a pair of words with its underlying sentence. This pair can either be an ex-ample, a counterexample or a candidate 3 . We model these events by Bernoulli random variables EX, CE, CAND ,cap-tured by a multinomial distribution: EX =1iffthepair is an example, CE = 1 iff the pair is a counterexample, CAND =1  X  EX  X  CE = 1 iff the pair is a candidate. For each prototype p , we introduce a Bernoulli random variable F , such that F p = 1 with probability f p iff a generated pattern falls on p . Note that this model also applies to the Discovery Phase.
If  X  is chosen sufficiently small, all patterns falling on p share their essential linguistic properties. Hence we assume that they all have the same probability of producing exam-ples or counterexamples.
For simplification, we assume that the 4th class of word pairs (see section 2.1) does not appear. If it does, it will only improve the bound given here.

We first concentrate on the Discovery Phase. We are in-terested in the probability that a given prototype p gets a positive label, although it does not express the target rela-tion. We define the quality of p as the relative probability of patterns falling on p to produce examples:
Since p does not express the target relation, q p &lt; 1 2 allotment of p is the share of examples and counterexam-ples produced by patterns falling on p : a p = P ( EX | F P ( CE | F p ). The better the examples and counterexamples are chosen, the more likely it is that patterns falling on p pro-duce examples or counterexamples (instead of candidates) and the larger a p will be. Let # EX p stand for the number of examples and # CE p for the number of counterexamples produced by patterns falling on p in the Discovery Phase. We are interested in the probability of p getting a positive label, namely P (# EX p &gt; # CE p ), given that q p &lt; Chernoff-Hoeffding bounds, we prove[29] that
Now we turn to the Testing Phase. We are interested in the probability that an incorrect output pair is produced by a pattern falling on p . For this to happen, a test pattern must fall on p , it must produce a candidate and p must be wrongly labeled as positive. Combined, this yields =(1  X  a p )  X  f p  X  P (# EX p &gt; # CE p ) This estimation shows that a larger allotment a p (i.e. a good choice of examples and counterexamples) decreases the probability of wrongly classifying a candidate pair. Further-more, the estimation mirrors the intuition that either many patterns fall on p in the Discovery Phase ( f p large) and then p is unlikely to have a false label, or few patterns fall on p ( f p small) and then the probability of p classifying a test pattern is small. As the number of sentences (and hence the number of generated patterns N ) increases, the bound converges to zero.
This section discusses how patterns can be represented and generalized using machine learning. The most impor-tant component of a pattern is its bridge. In the Discov-ery Phase, we collect the bridges of the patterns in a list. Each bridge is given an identification number, the bridge id . Two bridges are given the same bridge id if they dif-fer only in their nouns or adjectives (as discussed in section 1.3). Furthermore, positive patterns are given the label +1 and negative patterns  X  1. The context of a word in a link-age is the set of all its links together with their direction in the sentence (left or right) and their target words. For example, the context of  X  composers  X  in Figure 1 is the set of triples { ( det , left , X  the  X ), ( prepObj , left , X  among  X ), ( mod , right , X  of  X ) } . Each word is assigned a set of types .We distinguish nouns, adjectives, prepositions, verbs, numbers, dates, names, person names, company names and abbrevi-ations. The parser already assigns the grammatical types by its part-of-speech tagging. We assign the other types by regular expression matching. For example, any word match-ing  X  [A-Z][a-z]+ Inc  X  X sgiventhetype company . To accom-modate the role of stopwords in understanding a sentence, we make each stopword a type of its own. We represent a pattern by a quadruple of its bridge id, the context of the first placeholder, the context of the second placeholder, and its label. For example, supposing that the bridge id of the pattern in Figure 2 is 42 and supposing that the pattern is positive, we represent the pattern as follows (42, { ( subj , right , X  was  X ) } , { ( det , left , X  the  X ), ( prepObj , left , X  among  X ) } ,+1) To show that our approach does not depend on a specific learning algorithm, we implemented two machine learning algorithms: The adaptive kNN classifier discussed in 2.2.1 and an SVM classifier.
For the adaptive kNN, we need a similarity function on patterns. By x  X  y we denote the auxiliary function
Let  X  ( w ) be the set of types of a word w . The similarity of two words is the overlap of their type sets: The similarity of two contexts C 1 ,C 2 is computed by com-paring each triple in C 1 to all triples in C 2 , where each triple contains a connector, a direction and a word: sim ( C 1 ,C 2 )= Here,  X  1 , X  2 , X  3 are weighting factors that sum up to 1. We chose  X  1 =0 . 4 , X  2 =0 . 2 , X  3 =0 . 4. Two patterns have a similarity of zero if they have different bridge ids. Else, their similarity is the averaged similarity of the contexts of the first and second placeholder, respectively: sim (( b 1 ,C 11 ,C 12 ,l 1 ) , ( b 2 ,C 21 ,C 22 ,l 2 )) =
Let c p be the set of patterns that fell on a prototype p during the Discovery Phase. We compute the label of p as the sum of the labels of the patterns in c p ,weightedwith their respective similarities to p : To classify a pattern in the Testing Phase, we first determine its prototype. If there is no prototype within the distance  X  , the pattern receives the label  X  X  X  . Else, we calculate its label as the product of the similarity to the prototype and the label of the prototype.
To generalize patterns by an SVM, the patterns have to be translated to real-valued feature vectors. For this purpose, we group the patterns by their bridge ids when the Discovery Phase is completed. Each group will be treated separately so that it is not necessary to store the bridge id in the feature vector. If n is the number of connector symbols, then a feature vector for a pattern can be depicted as follows: label
R The vector consists of three parts. The first part is the label (+1 or  X  1), which occupies one dimension in the vec-tor as a real value (denoted by R in the scheme above). The second part and the third part store the context of the first and second placeholder, respectively. Each context con-tains a sub-part for each possible connector symbol. Each of these subparts contains one bit (denoted by X in the above scheme) for each possible word type. So if there are t word types, the overall length of the vector is 1 + n  X  t + n  X  encode a context as follows in the vector: If there is a link with connector con that points to a word w , we first select the sub-part that corresponds to the connector symbol con . Within this sub-part, we set all bits to 1 that correspond to atypethat w has. The vectors are still grouped according to the bridges. We pass each group separately to an SVM. We used SVMLight [18] with its default parameters. The SVM produces a model for each group, i.e. basically a function from patterns to real values (negative values for negative patterns and positive values for positive ones). To classify a new pattern in the Testing Phase, we first identify its bridge group. If the pattern does not belong to a known group, we give it the label  X  X  X  . Else, we translate the pattern to a feature vector and then apply the model of its group. Note that both the kNN classifier and the SVM classifier output a real value that can be interpreted as the confidence of the classification. Thus, it is possible to rank the output pairs by their confidence.
We implemented our approach in a system called Leila (Learning to Extract Information by Linguistic Analysis). We ran Leila on different corpora with increasing hetero-geneity 4 : Wikicomposers (all 872 Wikipedia articles about composers), Wikigeography (all 313 Wikipedia pages about the geography of countries), Wikigeneral (78141 random Wikipedia articles) and Googlecomposers (492 documents as delivered by a Google  X  X  X  X  feeling lucky X  search for com-posers X  names). Since the querying for Googlecomposers was done automatically, this corpus includes spurious advertise-mentsaswellaspageswithnopropersentencesatall.

We tested Leila on different target relations with in-creasing complexity: The birthdate -relation (e.g.  X  Chopin  X  / X  1810  X ), the synonymy -relation (e.g.  X  UN  X / X  United Na-tions  X ) and the instanceOf -relation (e.g.  X  Chopin  X /  X  composer  X ).

We compared Leila to different competitors .Weonly considered systems that, like Leila , extract the information from a corpus without using other Internet sources. We wanted to avoid running the competitors on our own cor-pora or on our own target relations, because we could not be sure to achieve a fair tuning of the competitors. Hence we ran Leila on the corpora and the target relations that our competitors have been tested on by their authors. We compare the results of Leila with the results reported by the authors. Our competitors, together with their respective corpora and relations, are the following: TextToOnto 5 can extract (i.a.) the instanceOf relation by shallow pattern matching and takes arbitrary HTML documents as input. Text2Onto [10] (currently under development) is the suc-cessor of TextToOnto. Snowball [1] uses the slot-extraction
See [30] for more details on the experimental setup and the results http://www.sourceforge.net/projects/texttoonto paradigm and has been used with the headquarters rela-tion. It was trained on a collection of some thousand doc-uments, but for copyright reasons, we only had access to the test collection (150 text documents). The CV-system [11] uses context to assign a concept to an entity. This ap-proach is restricted to the instanceOf -relation, but it can classify instances even if the corpus does not contain explicit definitions. In the original paper, the system was run on a collection of 1880 files from the Lonely Planet Internet site
For the evaluation , the output pairs of the system have to be compared to a table of ideal pairs. If O denotes the multi-set of the output pairs and I denotes the multi-set of the ideal pairs, then precision p ,recall r , and their harmonic mean F 1 can be computed as We estimated precision and recall by extracting the ideal pairs manually for a sub-portion of the corpora. We report confidence intervals for the estimates for  X  = 95% (see [30] for details). We measure precision at different levels of recall and report the values for the best F1 value. We use the original Ideal Metric for Snowball (see [1]) and the Relaxed Ideal Metric for the CV-system (see [30]).
Table 1 summarizes our experimental results with Leila on different relations. For the birthdate relation, we used Edward Morykwas X  list of famous birthdays 7 as examples. Leila performed very well. For the synonymy relation we used all pairs of proper names that share the same synset in WordNet as examples (e.g.  X  UN  X / X  United Nations  X ). As counterexamples, we chose all pairs of nouns that are not synonymous in WordNet (e.g.  X  rabbit  X / X  composer  X ). Leila performed well. For the instanceOf relation, we used all pairs of a proper name and its lowest non-compound super-concept from WordNet as examples. We used all pairs of a common noun and an incorrect super-concept from Word-Net as counterexamples. Our tough evaluation policy low-ered Leila  X  X  results: Our ideal pairs include pairs deduced by resolving semantic ambiguities, which decreases Leila  X  X  recall. Furthermore, our evaluation policy demands that non-defining concepts like friend , member or successor not be chosen as instance concepts, which decreases Leila  X  X  pre-cision. Thus, compared to the gold standard of humans, the performance of Leila can be considered reasonably good. To test whether heterogeneity influences Leila ,weranit on the Wikigeneral corpus and finally on the Googlecom-posers corpus. The performance dropped in these increas-ingly challenging tasks, but Leila could still produce useful results.

Table 2 shows the results for comparing Leila against various competitors (with Leila in boldface). Text2Onto seems to have a precision comparable to ours, although the small number of found pairs does not allow a significant con-clusion. Both Text2Onto and TextToOnto have drastically lower recall than Leila . For Snowball, we only had access to the test corpus. Hence we trained Leila on a small por-tion (3%) of the test documents and tested on the remaining ones. Leila showed a very high precision and a good recall  X  even though Snowball was trained on a much larger train-ing collection. For the CV-System, we first used the Lonely Planet corpus as in the original paper [11]. Since the ex-plicit definitions that our system relies on were sparse in the http://www.lonelyplanet.com/ http://www.famousbirthdates.com corpus, Leila performed worse than the competitor. In a second experiment, we had the CV-system run on the Wi-kicomposers corpus. This time, our competitor performed worse, because our ideal table is constructed from the defi-nitions in the text, which the CV-system is not designed to follow.
We proposed to extend the pattern matching approach for information extraction by using deep linguistic struc-tures instead of shallow text patterns. We showed how deep linguistic structures can be represented suitably for machine learning. We proved that the problem of false samples does not question the pattern matching approach. We imple-mented our approach and we demonstrated that our system Leila outperforms existing competitors.
 Our current implementation leaves room for future work. For example, the linkages allow for more sophisticated ways of resolving anaphoras or matching patterns. Leila could learn numerous interesting relations (e.g. country / president or isAuthorOf ) and build up an ontology from the results with high confidence. Leila could acquire and exploit new corpora on its own (e.g. read newspapers) and it could use its knowledge to acquire and structure its new knowledge more efficiently. We plan to exploit these possi-bilities in our future work.
We would like to thank Eugene Agichtein, Johanna V  X  olker and Philipp Cimiano for their unreserved assistance. [1] E. Agichtein, L. Gravano. Snowball :extracting [2] J.Aslam,S.Decatur.Onthesamplecomplexityof [3] S. Brin. Extracting patterns and relations from the [4] P. Buitelaar, D. Olejnik, M. Sintek. A protege plug-in [5] P. Buitelaar, S. Ramaka. Unsupervised ontology-based [6] R. C. Bunescu, R. Mooney. A Shortest Path [7] M. Califf, R. Mooney. Relational learning of [8] V. Cherkassky, M. Yunqian. Practical selection of SVM [9] P. Cimiano, G. Ladwig, S. Staab. Gimme the context: [10] P. Cimiano, J. V  X  olker. Text2onto -a framework for [11] P. Cimiano, J. V  X  olker. Towards large-scale, [12] O. Etzioni, M. Cafarella, D. Downey, S. Kok, [13] A. Finn, N. Kushmerick. Multi-level boundary [14] D. Freitag, N. Kushmerick. Boosted wrapper [15] J. Graupmann. Concept-based search on [16] A. Hearst. Automatic acquisition of hyponyms from [17] F. C. J. Iria. Relation extraction for mining the [18] T. Joachims. Learning to Classify Text Using Support [19] M. J. Kearns, R. E. Schapire. Efficient [20] D. Lin, P. Pantel. Dirt: Discovery of inference rules [21] A. Maedche, S. Staab. Discovering conceptual [22] R. Montague. Universal grammar. Formal Philosophy. [23] D. Ravichandran, E. Hovy. Learning surface text [24] E. Riloff. Automatically generating extraction [25] H. U. Simon. General bounds on the number of [26] D. Sleator, D. Temperley. Parsing english with a link [27] R. Snow, D. Jurafsky, A. Y. Ng. Learning syntactic [28] S. Soderland. Learning information extraction rules [29] F. M. Suchanek, G. Ifrim, G. Weikum. Combining [30] F. M. Suchanek, G. Ifrim, G. Weikum. LEILA: [31] S. Soderland, D. Fisher, J. Aseltine, W. Lehnert. [32] F. Xu, H. U. Krieger. Integrating shallow and deep [33] F. Xu, D. Kurz, J. Piskorski, S. Schmeier. Term [34] R. Yangarber, R. Grishman, P. Tapanainen, [35] R. Yangarber, W. Lin, R. Grishman. Unsupervised [36] L. Zhang, Y. Yu. Learning to generate CGs from
