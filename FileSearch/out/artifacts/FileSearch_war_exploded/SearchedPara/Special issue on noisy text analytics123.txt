 INTRODUCTION Craig Knoblock  X  Daniel Lopresti  X  Shourya Roy  X  L. Venkata Subramaniam Noise is an unavoidable fact of life. It can manifest itself at the earliest stages of processing in the form of degraded inputs that our systems must be prepared to handle. People are adept when it comes to pattern recognition tasks involv-ing typeset or handwritten documents or recorded speech, machines less-so. From the perspective of down-stream pro-cesses that take as their inputs the outputs of recognition systems, including document analysis and OCR, noise can be viewed as the errors made by earlier stages of processing, which are rarely perfect and sometimes quite brittle.
Noisy unstructured text data is also found in informal set-tings such as online chat, SMS, email, message board and newsgroup postings, blogs, wikis and web pages. In addi-tion to the aforementioned recognition errors, such text may contain spelling errors, abbreviations, non-standard termi-nology, missing punctuation, misleading case information, as well as false starts, repetitions, and pause-filling sounds such as  X  X m X  and  X  X h X  in the case of speech.

We define noise in text as any kind of difference between the surface form of a coded representation of the text and the intended , correct ,or original text. By its very nature, noisy text warrants moving beyond traditional text analytics techniques. Noise introduces challenges that need special handling, either through new methods or improved versions of existing ones. For example, missing punctuation and the use of non-standard words can often hinder standard natural language processing techniques such as part-of-speech tag-ging and parsing. In the case of machine learning, the chal-lenge is to learn the underlying models even in the presence of noise.

This special issue combines papers that discuss the dif-ferent problems encountered in analyzing noisy documents (where we define document broadly to mean a coherent unit of information originally intended for human consumption) and present solutions to some of these problems. The papers herein present techniques for detecting and correcting noisy text, classification of noisy text, and information extraction from noisy text sources. This special issue is the culmination of efforts that began in 2006 to create a forum for research on noisy text analytics. As a first step, the Workshop on Analyt-ics for Noisy Unstructured Text Data (AND 2007) was held in conjunction with the 20th International Joint Conference on Artificial Intelligence (IJCAI) in Hyderabad, India, in Jan-uary 2007. We selected the top papers from this workshop and vetted them through two further rounds of reviews. This special issue is the result of these efforts.

Eight papers appear here. The first three papers consider the problem of noisy text detection and correction. The first work, by Foster, focuses on the important problem of parsing noisy text, creating a Treebank of ungrammatical sentences and using this for training a more robust parser. This paper goes on to show the advantages of such an approach when inputs contain ungrammatical sentences. In the second paper, Takeuchi et al. present a method for adding sentence bound-aries to text produced through automatic speech recognition of conversational data, a first step toward further natural language processing. The third paper, by Choudhury et al., looks at the issue of correcting SMS text using a machine learning-based technique. They investigate the nature and type of compressions used in SMS texts, and develop a word-based Hidden Markov Model for the SMS language.
 The next three papers look at noisy text classification. Wang et al. in their work provide an explanation-based learn-ing method for incorporating prior domain knowledge into the learner for distinguishing handwritten characters. They study the classification of OCR X  X d Chinese text and show the value of the explanation-based learning method in the pres-ence of a large number of classes with few training exam-ples per class. In the next paper, Prasad et al. have focused on the problem of finding structure in noisy text using clas-sification and clustering. Using an HMM-based classifica-tion approach, they present a technique for spotting messages containing topics of interest. They also present a clustering methodology for the automatic conceptual organization of messages without requiring prior knowledge of the topics of interest. The next paper, by Stubbe et al., looks at genre-based document classification. A high level genre hierarchy is introduced and, using handcrafted high-level features, a system of classifiers is constructed.

The last two papers in this special issue look at the prob-lem of information extraction from noisy text. Michelson and Knoblock examine adding semantic annotations to online posts to enable accurate and structured querying on them. The final paper in the issue is by Deepak and Kummumuru and presents a technique for mining call center conversa-tions. They use a clustering-based segmentation approach to find frequent procedures or discourse structures in tran-scribed conversations.

We believe the AND 2007 workshop and the steps taken to produce this special issue of IJDAR have created a mind-share for noisy text analytics within the research community. As evidence, we note that for the TREC Question Answering Task this year, blog data, in addition to the usual newswire data, has been included. Blog text contains both  X  X lean X  Eng-lish as well as badly-formed English and spam, and mining blogs for answers introduces significant new challenges in at least two areas that are key for question-answering systems: (1) being able to handle language that is not well-formed, and (2) dealing with discourse structures that are more informal and less reliable than newswire text. Real-life data is inher-ently noisy, and techniques to process noisy text are vital if we are to get useful and actionable information from such sources. This special issue is a step in this direction, and we hope even more researchers take up the challenge.

We conclude this introduction by thanking all of the con-tributors for the work they have done. We also thank the program committee members of the AND 2007 workshop and others who carefully reviewed the papers. It is our hope that this special issue will act as a catalyst for further research in noisy text analytics.
 EDITORIAL Daniel Lopresti  X  Shourya Roy  X  Klaus Schulz  X  L. Venkata Subramaniam Noise is an ever-present challenge in the field of document analysis. We face it at all levels and stages of the process. While human readers are adept at compensating for noisy inputs, machines still stumble, sometimes badly, when con-fronted by the consequences when noise is present. The goal of building robust systems that both tolerate noisy inputs on the one hand, and minimize the errors they pass on to down-stream stages on the other, is an active area of research. The AND series of workshops on Analytics for Noisy Unstructured Text Data were established to provide a forum for addressing noise that reflects any kind of difference between the surface form of a coded representation of text and the intended, correct, or original text. Note that this definition is intentionally broad, covering differences that arise from typographic or grammatical errors, informal use of language, and errors from the machine recognition of type-set text, handwriting, and speech. By its very nature, noisy text warrants moving beyond traditional techniques for text analytics.

Thisspecialissueincludesexpandedversionsofsixpapers chosen from among those presented at the Second Workshop on Analytics for Noisy Unstructured Text Data, which was organized as part of the 31st ACM SIGIR Conference held duringJuly2008inSingapore. Similar tothefirst ANDwork-shop which also yielded a special issue of IJDAR (vol. 10, nos. 3 X 4, December 2007), AND 2008 was a successful event attended by over 40 researchers from around the world representing various academic institutions, industry, and government. Each of the invited papers selected for this issue underwent a rigorous re-review process before final accep-tance for the journal.

The first two papers deal with errors introduced as a result of failures in optical character recognition and handwrit-ing recognition, respectively. Lopresti applies a hierarchi-cal approach to dynamic programming to classify errors across the stages of a typical text analysis pipeline: sen-tence boundary detection, tokenization, and part-of-speech tagging. Errors and their cascading effects are isolated and analyzed, with the ultimate goal of understanding the varying impacts of different types of errors and ways of ameliorat-ing them. Farooq, Bhardwaj, and Govindaraju examine two methods for improving the results of unconstrained hand-writing recognition. The first of these attempts to build a reduced-size lexicon based on topic identification, while the second employs topic-specific language models to drive the outputoftherecognizer.Bothmethodsaretestedandfoundto yield significant accuracy improvements on highly degraded inputs.
 The next paper, by Reffle, Gotscharek, Ringlstetter, and Schulz, examines the problem of correcting  X  X alse friends, X  spelling mistakes that result in other words present in the lexicon. They address this challenge by building a profile for the error channel and using this to focus attention on dic-tionary words which are more likely to be errors requiring correction, thereby maximizing the number of proper cor-rections that occur and minimizing the number of false cor-rections. On the other hand, Acharyya, Negi, Subramaniam, and Roy take Short Message Service (SMS) data as their starting point. Here, noise is introduced through aggressive use of abbreviations, intentional deletions, phonetic spell-ing conventions, mixed languages, genuine misspellings, etc. They demonstrate how unsupervised methods can provide effective results at a lower cost than those requiring expen-sive manual labeling of a training set, and illustrate their discussion with tests using a large corpus of SMS messages from a real-world call center.

The article by He, Weerkamp, Larson, and de Rijke, delves into the world of blogs, where discussions can often drift from one topic to the next, severely impacting attempts to extract and distill information from such sources. The authors define a notion of topical noise, and then develop tech-niques for generating a reliable coherence score for blogs in the presence of noisy input as one typically finds in the blogosphere. In the final article, Dey and Haque study the problem of opinion mining in the context of noisy text inputs. They present a semi-supervised approach to learn domain knowledge from a training set that contains both clean and noisy data. Their targeted applications, from which they wish to extract opinion expressions, include customer blogs and feedback forms.

We conclude this introduction by thanking all those who participated in the AND 2008 workshop, as well as the reviewers who provided valuable feedback both for the work-shop articles and the extended versions that appear here. It is our hope that this special issue will continue to broaden awareness of the problems that arise in noisy text analytics, and thereby inspire those working in related areas to consider the challenges posed here as worthy topics of study.
 EDITORIAL Daniel Lopresti  X  Shourya Roy  X  Klaus Schulz  X  L. Venkata Subramaniam Noisy unstructured text data are ubiquitous in real-world communications. Text produced by processing signals intended for human interpretation, such as printed and handwritten documents, spontaneous speech, and camera-captured scene images, are prime examples. Application of Automatic Speech Recognition (ASR) systems on telephonic conversations between call center agents and customers often see 30 X 40% word error rates. Optical character recognition (OCR) error rates for hardcopy documents can range widely from 2 X 3% for clean inputs to 50% or higher depending on the quality of the page image, the complexity of the layout, and aspects of the typography. Unconstrained handwriting recognition is still considered to be largely an open problem.
Recognition errors are not the sole source of noise; nat-ural language and its creative usage can cause problems for computational techniques. Electronictext takendirectlyfrom the Internet (emails, message boards, newsgroups, blogs, wikis, chat logs, and web pages), contact centers (customer complaints, emails, call transcriptions, message summa-ries), and mobile phones (text messages) is often very noisy and challenging to process. Spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punc-tuation and case information, and pause-filling words such as  X  X m X  and  X  X h X  in the case of spoken conversations are just a few examples of the problems that can arise.

This special issue includes expanded versions of nine papers chosen from among those presented at the Third Workshop on Analytics for Noisy Unstructured Text Data, which was organized as part of the Tenth International Con-ference on Document Analysis and Recognition (ICDAR) in Barcelona, Spain, in July 2009. Building on previous AND workshops, AND 2009, was a successful event attended by over 25 researchers from various academic institutions and business organizations from different parts of the world. Each of the papers selected for this issue underwent a rigorous revision and reviewing process before final acceptance for the journal.

We are pleased to present the research described here as reflecting the current state-of-the-art in noisy text analytics. The first paper by Simone Marinai addresses some of the challenges arising in text retrieval from early printed manu-scripts. In such cases, typography and state of preservation can lead to character and word images which are difficult to segment. Marina applies Self-Organizing Maps to solve this problem and demonstrates the efficacy of his techniques on page images from the Gutenberg Bible.
 The second paper by Kesidis, Galiotou, Gatos, and Pratikakis presents a word-spotting approach for historical documents that avoids the need for optical character recog-nition. They describe a set of techniques for pre-processing, word segmentation, and word matching and test their meth-ods on early Modern Greek documents from the seventeenth and eighteenth Century.

In the third paper, Cao, Govindaraju, and Bhardwaj address the unconstrained handwritten document retrieval problem. They assume handwriting recognition will produce imperfect results and propose a term frequency estima-tion technique that incorporates segmentation information to improve precision and recall performance.

The next paper by Subramaniam, Prasad, and Natarajan examines named entity detection in the presence of errors. Since OCR output may be noisy in many applications of interest, they employ a complete lattice which is output from recognition engine. They then describe ways of dealing with issues that arise when using lattices: the high false alarm rate and the added computational cost. Their techniques are demonstrated on English videotext and handwritten Arabic.
Information retrieval in historical document collections is the subject of the paper by Gotscharek, Reffle, Ringlstetter, Schulz, and Neumann. The problem is challenging because ofthelargenumberofspellingvariationsthatarepresent.The authors study the interaction between matching procedures and specialized lexica to determine an effective approach, conducting their experiments on collections of documents spanning several centuries embodying the evolution of the German language.

Likewise, in his paper, Reynaert shows how to handle typographical variation and spelling errors in noisy text col-lections using an approach based on anagram hashing. Test results are reported on digitized Dutch Parliamentary docu-ments and the 1918 edition of the daily newspaper Het Volk . The paper by Bratus, Rumshisky, Khrabrov, Magar, and Thompson describes two approaches for extracting knowl-edge from unstructured narrative text. One of these uses domain-specific ontologies, and the other employs Hidden Markov Models trained on a small amount of annotated data. The authors present experimental results using data from the automotive industry: lexicons and ontologies of part names, along with car repair manuals.

Giannone, Basili, Naggar, and Moschitti address a differ-ent application domain in their work: criminal investigative data based on police interrogation reports. They show how text relation mining can be accomplished in such cases using kernel-based techniques. Empirical results demonstrate that the methods are effective in the presence of non-traditional language, dialects, and jargon.

Finally, the paper by Marx and Gielissen describes effi-cient storage mechanisms for scanned document images. Their method reduces the size of the documents returned to users by two orders of magnitude while maintaining the same overall visual appearance.

In addition to contributed papers, we also present a work-ing group report authored by Marinai and Karatzas. Work-shops such as AND are highly interactive, and one of their most noteworthy features is the way they bring together researcherswithdifferentperspectivesfromaroundtheworld to focus on a common topic. This working group report cap-tures the ideas and impressions of 12 attendees who chose as their topic  X  X oisy text datasets. X  As a reflection of the live discussion that took place at AND 2009, we believe it is a valuable addition to this special issue.

We conclude this introduction by thanking all those who participated in the AND 2009 workshop, as well as the reviewers who provided valuable feedback both for the work-shop papers and the extended versions that appear here. It is our hope that this special issue will continue to broaden awareness of the problems that arise in noisy text ana-lytics and thereby inspire those working in related areas to consider the challenges posed here as worthy topics of study.
