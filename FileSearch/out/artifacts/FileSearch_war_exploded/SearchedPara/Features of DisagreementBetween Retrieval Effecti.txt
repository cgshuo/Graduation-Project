 Many IR effectiveness measures are motivated from intuition, theory, or user studies. In general, most effectiveness mea-sures are well correlated with each other. But, what about where they don X  X  correlate? Which rankings cause measures to disagree? Are these rankings predictable for particular pairs of measures? In this work, we examine how and where metrics disagree, and identify differences that should be con-sidered when selecting metrics for use in evaluating retrieval systems.
 H.3.4 [ Information Storage and Retrieval ]: Systems and software X  performance evaluation IR evaluation; effectiveness measures; binary relevance
Since information retrieval systems were first evaluated, system effectiveness measures have been a topic of discussion , study and controversy. When a new evaluation metric is introduced X  X r existing measures are criticised or praised X  X he discussion is usually motivated either by theoretical conce rns with earlier metrics [5, 8, 9], or by user studies [1]. Sometime s, both approaches are combined [4].

Most IR effectiveness measures assume that the quality of results returned by a search engine can be calculated as a function of the gain vector inferred by a ranked list X  sometimes also including additional knowledge such as the total number of relevant documents available [2]. In this paper we use:
In the case of binary relevance (where documents are assumed to either be relevant, or not), the gain vector can be represented as a bit string. In simple cases, it is easy to examine these bitstrings by eye and reason about which list should be preferred. For example, where k = 3 , { 1 , 0 , 0 } is almost always better than { 0 , 0 , 1 } .

However, some cases become more contentious. Consider two vectors with k = 10 and R = 10: If asked which list is better, most researchers would say  X  X t depends on the task X . However, it X  X  not always immediately clear whether particular effectiveness metrics would agree on which list is better. In this particular example, AP prefers ranking A over B , while DCG prefers ranking B over A . Put another way, it X  X  not always clear which metric prefers which type of task, or whether particular metrics are consistent in their preferences. In this work, we perform an exhaustive search of the possible binary relevance vectors where k = 10 , and investigate where the disagreement between metrics lies.
It X  X  worth noting that many metric descriptions don X  X  com-pletely specify the details of implementation. For example, an implementation of DCG requires a selection of both a gain and a discount function. Similarly, an implementation of RBP requires a selection of the p parameter [9]. Kanoulas and Aslam examine several different possible choices for the gain and discount functions in NDCG [6]. They used a Generalisability Theory approach to find choices that pro-duced a stable ranking of systems. In this environment, they show that the optimal discount function is less steep than previously thought, and also that the optimal gain function gives nearly equal weight to relevant and highly relevant documents.

Although producing a single score for a retrieved list is convenient, a single score is not a particularly effective way of capturing system performance [7]. One illustration of this is the idea of 1-equivalence [10], which is the set of binary resul t vectors that receive the same score as a single document. The authors note that many 1-equivalent sets are contentious or at least counter-intuitive.

An interesting approach to measuring the quality of an ef-fectiveness measure is to use the Maximum Entropy Method to infer the probabilities that each rank contains a relevant document, given the effectiveness score [2]. Once this prob-ability vector is computed from the effectiveness score, a predicted precision X  X ecall curve can be produced. The error between this and the actual precision X  X ecall curve can be used to determine the quality of the metric. This approach asks the question  X  X ow good is the effectiveness score at inferring the original ranked list? X .

Motivated by the wide variety of effectiveness metrics available, Moffat [8] introduces seven properties for describing and comparing metrics. They are:
Moffat notes that it is impossible for a metric to satisfy all seven properties, as some property combinations exclude oth-ers (for example, a metric that is monotonic and convergent cannot also be realisable if k &lt; R ).
Many previous comparisons between effectiveness metrics have looked at correlation between metrics. In this work, we are interested in the specific cases where metrics do not agree. That is, the cases where we have two binary relevance vectors A and B where X  X or example X  X P says that A is the most effective result list, while DCG says that B is the most effective result list. In this work, we only focus on rankings where k = 10, since many of the metrics above include a user model, and it is common for users to only examine the first page of results in a web setting.
We generate all possible combinations for binary relevance rankings where k = 10 and R = 10. This yields a total of 1,023 possible ranked lists, containing all possible rankings of 10 documents with up to 10 relevant documents (the all 0 X  X  case is ignored). There are 1,045,506 pairs of rankings, ignoring the pairs where both rankings are identical. Each ranked list is evaluated for each metric, and for each pair of ranked lists and pair of metrics, agreement or disagreement is recorded. All metrics were computed using 64-bit floating point numbers, and equality was checked using an epsilon of 2  X  53 .
We use the same metrics as Moffat [8], with the exception of HIT, which would receive the same score for every ranked list described above.
 Table 3: Disagreement between metrics with partic-ular properties (%, lower is better). Bolded groups show significance.
 The metrics used are DCG, SP, SN-DCG [8], SN-AP [8], Prec@ k , NDCG [6], SDCG [8], RR, Recall@ k , AP [3] and RBP [9]. For RBP, 3 different values of p were used: 0.5, 0.85 and 0.95. As recommended by the authors [9], the lower bound of the score was taken to be the RBP score. ERR was also implemented, but it scores identically to RR in binary relevance [4]. Similarly, where metrics are identical except for normalisation (eg DCG and NDCG), metrics completely agree. See Table 1 for the breakdown of properties of each measure. Where choices for implementation were available, we implement the metric as described by Moffat [8].
The disagreement between each pair of measures can be seen in Table 2. In some cases (such as AP vs RR), the disagreement is not surprising, but in other cases where one would expect metrics to agree, they do not X  X uch as the high disagreement with SNDCG and most other metrics. Also of note is the high disagreement between RBP variants X  although this is not surprising, it is worth mentioning that simply changing the discount parameter dramatically affects the behaviour of the metric.
The disagreement percentages shown in Table 2 are assum-ing that each possible ranking with k = 10 can be returned by some system. However, retrieval systems aren X  X  random; the results returned depend on collection statistics and other Table 4: Features of rankings where metric pairs dis-agree, in percentages. Since features are sometimes tied, they do not add to 100. features which are not independent of relevance. To investi-gate whether these vectors are reasonable, we produced the k = 10 bitstrings for all runs submitted to the TREC4-8 ad-hoc tracks, and the 2005 and 2006 TREC robust tracks. All of the 1024 possible vectors were returned by real runs sub-mitted to the track. This validates this exhaustive method of investigation.
To investigate whether the properties of effectiveness mea-sures affect the disagreement between metrics, each disagree-ment score was sorted in to one of three buckets for each property: has-prop , where both metrics have the property; no-prop , where neither metric has the property, and cross-prop , where one metric has the property, and one does not. Table 3 shows the mean agreement between metrics in each bucket. Surprisingly, a single property is not enough to determine whether two metrics agree. Of note is the case of metrics which are not bounded, but as Table 1 shows, there are only two measures in the no-prop category for that property. With the exception of monotonicity, boundedness, and realisability, the has-prop category contains the lowest mean disagreement. However, a one way ANOVA test with a Tukey post-hoc analysis shows only the convergent/has-prop group and the realisable/no-prop group to be statistically significant predictors of agreement.
To investigate whether metrics disagreed in a predictable way that might be correlated with a task goal, all disagreeing pairs of ranked lists from each pair of metrics were examined to find out which metric preferred: recall , the list with the most relevant documents (ignoring ties); first-rel , the list with the first relevant document (ignoring ties); and first-irrel , the list with the first irrelevant document (ignoring ties). Some selected results are in Table 4. Each pairing is divided by the space in the table. When AP and DCG disagree about which ranked list is better, this table shows that in 52.41% of disagreeing cases, AP will prefer the list with more relevant documents, while DCG prefers the higher recall list only 22.09% of the time. These numbers do not sum to 100%, as sometimes there is a tie in the recall between two ranked lists, and sometimes a disagreement means that one of the two metrics gives the same score to two ranked lists (as is the case with AP when comparing { 1 , 0 , 0 , 0 } and { 0 , 1 , 0 , 1 } when R = 10).

This approach provides an alternative way to think about metric selection. When choosing say between AP and DCG (two metrics with strong agreement), this approach allows us to drill down into the differences between the metrics. If we prefer finding a relevant document quickest, then Table 4 suggests that DCG is the metric to choose, with 71.63% of cases preferring the ranking with the highest ranked initial relevant document. Alternatively, if recall is preferred, then AP is the better metric. Note that although Table 2 shows only a 3.01% disagreement between AP and DCG, this is still over 15,000 pairs of ranked lists.

The difference between the user model of RBP and DCG can also be seen in this data X  X CG prefers the list with the earliest relevant document more often than RBP. And, as the weighting factor in RBP decreases, the metric prefers earlier relevant documents. Conversely, as the weight in RBP increases, the list with higher recall is preferred.
In this work we have introduced a novel strategy for inves-tigating the disagreement between effectiveness metrics X  X y counting and examining the pairs of hypothetical rankings where the metrics disagree with each other. We validated our strategy by demonstrating that all possible rankings of 10 binary relevant documents have appeared in search results submitted to two of the TREC tracks, and we performed an initial investigation into whether the properties of effective -ness measures can be used to predict agreement. We found that two of the properties appeared to be weak predictors of agreement between metrics. Then, we used a feature-based approach to investigate whether the disagreement between a pair of metrics could be described in terms of task features. This approach allows statements like  X  X f I select AP over DCG, I am preferring recall over highly ranked documents X  to be made, allowing fine-grained selection between metrics.
As mentioned above, search engines do not produce a random result set. Although all possible rankings for k = 10 did appear in real search results during the TREC ad-hoc and robust tracks, the frequency with which each ranking appears is not uniform. It would be valuable to examine the likelihood of disagreement that each result list has. It is possible that the gain vectors produced by systems could be used to determine how contentious they are X  X ow sensitive evaluation would be to metric selection.

An obvious extension to Section 4.2 is to consider multiple groupings of properties. Although it seems that individual properties of effectiveness measures are not enough to predict agreement, perhaps some combination of the properties might be. In future work, we intend to include more variants of metrics, such as alternative discounts and gain functions. This may lead to discovery of further properties.
In Section 4.3, we only consider three features of ranked lists that map to task goals, but there are many more we could consider (such as longest consecutive run of relevant documents, or lowest ranked relevant document). Addition-ally, user preference experiments could be constructed using pairs of vectors where metrics disagree.

As noted in Section 4, many of the DCG variants com-pletely agree on individual ranked lists, as all that changes is the normalisation. However, if multiple queries (and there-fore multiple ranked lists) are used for evaluation X  X s in the case of TREC tracks X  X hen different normalisation strategies may well cause further disagreement between metrics.
Finally, this work has only considered the binary relevance case. Many of these metrics behave differently when there are multiple grades of relevance. An important next step is to repeat this analysis using graded relevance.
 In addition to the anonymous reviewers, we would like to thank Alistair Moffat for helpful comments and a correction. [1] A. Al-Maskari, M. Sanderson, and P. Clough. The [2] J. A. Aslam, E. Yilmaz, and V. Pavlu. The maximum [3] C. Buckley and E. M. Voorhees. Evaluating evaluation [4] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. [5] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [6] E. Kanoulas and J. A. Aslam. Empirical justification of [7] S. Mizzaro. The good, the bad, the difficult, and the [8] A. Moffat. Seven numeric properties of effectiveness [9] A. Moffat and J. Zobel. Rank-biased precision for [10] P. Thomas, D. Hawking, and T. Jones. What
