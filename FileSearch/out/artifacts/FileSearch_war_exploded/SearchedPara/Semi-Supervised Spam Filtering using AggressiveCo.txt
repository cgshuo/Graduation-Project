 A graph based semi-supervised method for email spam fil-tering, based on the local and global consistency method, yields low error rates with very few labeled examples. The motivating application of this method is spam filters with access to very few labeled message. For example, during the initial deployment of a spam filter, only a handful of labeled examples are available but unlabeled examples are plentiful. We demonstrate the performance of our approach on TREC 2007 and CEAS 2008 email corpora. Our results compare favorably with the best-known methods, using as few as just two labeled examples: one spam and one non-spam. H.3.3 [ Information Search and Retrieval ]: Information Filtering Experimentation, Measurement Spam, Email, Filtering, Classification
Semi-supervised methods are of special interest when there are very few training samples available. In many machine learning applications, there is always great human effort in-volved in labeling samples, while obtaining unlabeled data is fairly simple. This is the case for spam filters. During the initial deployment of spam filters, a normal user may be willing to provide only a few labeled examples for training but will still expect correct classification of a large number of emails. Another application is personalized spam filter-ing with low label cost, using per-user semi-supervised filters with few labeled examples to augment a global filter.
In this paper we address the problem of email spam fil-tering with very few correct training samples using graph based semi-supervised learning methods. Previous semi-supervised methods such as Transductive SVM and Logis-tic Regression and Dynamic Markov Compression with self training for spam filtering have yielded mixed results [4]. In this paper we are focused on the special situation in which Algorithm 1 Aggressive Consistency Learning Method (ACLM) Input: X , Y ,  X  ,  X  1: Compute Affinity matrix 2: For all j such that 4: Y = ( 1  X   X  )( I  X   X  L )  X  1 L affinity matrix A , we also propose to reduce the dimensional-ity of matrix X . By applying Singular Value Decomposition (SVD)[2] on matrix X ; we find the most informative terms in X and replace X with its approximate. In other words, X 0 = U  X  V  X  1 , we only keep the rank highest singular values of X ; so {  X  i,i = 0  X  i &gt; rank } . We compare the effectiveness of ACLM with the supervised and transductive modes of SV M light [1] (denoted SVM and TSVM). We have compared these methods on two email corpora, TREC 2007 Public Corpus 1 and CEAS 2008 Public Corpus 2 . From each corpus we have selected the first 10 , 000 from which the first 1000 were used for tuning purposes to figure out the three main parameters  X  ,  X  , and rank .
For the actual experiment, we divided the remaining 9000 messages into batches of 1000 , getting 9 batches. For each batch we used the first 100 messages to select a balanced training set (same number of spam and non-spam) and the remaining 900 messages as the test set. We report mean error rate, as average over all batches.

Each message was abstracted as a binary feature vector representing word occurrences within the whole email, in-cluding headers. We removed terms with document fre-quency of less than 5 in the training and test sets combined. Binary term frequency was then used for the terms. Raw term frequency was also investigated, but did not provide better results than binary weights.

For parameters of SVM and TSVM, several values were adjusted but no improvement over their default values was observed. The p parameter in TSVM, representing the pro-portion of spam messages to be expected, was tuned using our tuning set of emails.
 Fig. 1 shows the results of the methods on CEAS08 and TREC07 corpora. ACLM with SVD gives best performance of all methods between 4 and 32 labeled examples, mostly having less than 0 . 01 error rate. TSVM only performs best with fewer than 4 examples. We have previously seen similar results in [4] where TSVM was performing better than SVM only when the train and test sets were from two completely different sources. SVM does not give best performance on CEAS08 even with 30 labeled examples. trec.nist.gov/data/spam.html www.ceas.cc/challenge
