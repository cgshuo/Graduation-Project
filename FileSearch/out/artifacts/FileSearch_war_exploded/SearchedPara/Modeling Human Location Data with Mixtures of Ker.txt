 Location-based data is increasingly prevalent with the rapid increase and adoption of mobile devices. In this paper we address the problem of learning spatial density models, fo-cusing specifically on individual-level data. Modeling and predicting a spatial distribution for an individual is a chal-lenging problem given both (a) the typical sparsity of data at the individual level and (b) the heterogeneity of spa-tial mobility patterns across individuals. We investigate the application of kernel density estimation (KDE) to this problem using a mixture model approach that can interpo-late between an individual X  X  data and broader patterns in the population as a whole. The mixture-KDE approach is evaluated on two large geolocation/check-in data sets, from Twitter and Gowalla, with comparisons to non-KDE base-lines, using both log-likelihood and detection of simulated identity theft as evaluation metrics. Our experimental re-sults indicate that the mixture-KDE method provides a use-ful and accurate methodology for capturing and predicting individual-level spatial patterns in the presence of noisy and sparse data.
 H.2.8 [ Database Management ]: Database Application X  Data Mining, Spatial databases and GIS spatial; kernel density estimation; anomaly/novelty detec-tion; probabilistic methods; social media; user modeling
Human location data is increasingly available in the mod-ern mobile world, often in the form of geolocation tags at-tached to human behavioral data such as phone calls, text messages, social media activities, and more. With the wides-pread availability of this data there is increasing interest Figure 1: Geolocated Twitter tweets in southern California. Points over the ocean and sparsly popu-lated areas are largely a result of noise in the geolo-cation data rather than actual locations. across a variety of fields of study in creating accurate mod-els to characterize the spatial distributions of populations and individuals. For example, Sadilek et al. [23] analyze the spread of infectious diseases through geolocation data from Twitter, opening up potential new approaches for real-time computational epidemiology. Cranshaw et al. [9] use Foursquare check-in data to identify local spatial clusters within urban areas, with potential applications in urban planning to economic development and resource allocation. From a commercial perspective, location-based services and personalization are becoming increasingly important to in-dividual mobile device users, with an increasing number of applications that are location-aware, including maps, local-ized search results, recommender systems, and advertising [28].

In this paper we focus on the problem of developing ac-curate individual-level models of spatial location based on geolocated event data. The term  X  X vent X  here can be inter-preted in a broad context X  X xamples include communication events such as phone calls or text messages, check-in events, social media actions, and so on. The goal is to be able to accurately characterize and predict the spatial pattern of an individual X  X  events. The problem is challenging for two main reasons. Firstly, there is often relatively little data for many of the individuals making it difficult to build accu-rate models at the individual level. Secondly, there is often considerable variety and heterogeneity in the spatial pattern of events of individual users, rendering techniques such as clustering less than ideal for individual-level modeling. Figure 2: Gowalla checkins in southern California.
The primary contribution of our paper is a systematic approach for individual-level modeling of geolocation data based on kernel density estimates. Kernel density approaches have been relatively unexplored to date in the context of spa-tial modeling of geolocation data. While in principle they provide a flexible and general framework for spatial den-sity estimation, direct application at the level of individual event data will tend to lead to significant over-fitting (be-cause of the sparsity of data at the individual level). We propose a hierarchical extension of the traditional kernel ap-proach that can avoid the over-fitting problem by systemati-cally smoothing an individual X  X  data towards the population data. We demonstrate how adaptive bandwidth techniques provide better quality density estimates compared to fixed bandwidths. Using two large geolocation data sets from Twitter and Gowalla (see Figures 1 and 2) we show that the proposed kernel method is significantly more accurate than baselines such as Gaussian mixtures for such data, in terms of the quality of predictions on out-of-sample data.
This paper is organized as follows. Section 2 provides an overview of existing approaches for modeling human location data and elaborates on the challenges of developing spatial models in practice. In section 3 we review kernel density estimation and discuss the use of adaptive bandwidth meth-ods and in section 4 we describe our proposed mixture-KDE approach for modeling and predicting individuals X  locations. In section 5 we present empirical experiments using two dif-ferent geospatial/check-in data sets and using both test log-likelihood and accuracy in detection of simulated identity theft. Section 6 discusses scalability and online algorithms for the proposed approach and we conclude with a brief dis-cussion in Section 7.
In this paper we consider data available in the form of individual-level geotagged events, E = { E 1 ,...,E N } where E dividual, 1  X  i  X  N . Each event e j i consists of a tuple &lt; i,x,y,t &gt; , where x and y are longitude and latitude respectively and t is a time-stamp, e.g., geotagged tweets based on GPS location.

One approach in analyzing such data is to focus on the problem of sequentially predicting a user X  X  behavior in terms of their short-term trajectory, e.g., predicting where a user i  X  X  next event e j +1 i is likely to occur in terms of location &lt; x,y &gt; given their event history { e 1 i ,...,e j i may be minutes or hours apart (e.g., Song et al. [27] and Scellato et al. [24]). In this paper we focus on a different problem, that of modeling a user X  X  spatial patterns over a longer time-period in a more aggregate sense. Specifically, we focus on learning probability density models of the form f ( x,y ) that represent the spatial density of user i  X  X  events. Given an event has occurred for individual i , the probability that it lies in any area A is R R f i ( x,y ) dxdy where the inte-gral is over the region defined by A (see also [16]). In this context we focus in this paper on modeling f i ( x,y ) rather than f i ( x,y,t ), and only use the time dimension t to or-der the data. e.g. for online training and prediction. In principle it should be possible to extend the 2-dimensional spatial modeling methods proposed in this paper to include the temporal dimension, allowing for inclusion of circadian and calendar-dependent aspects of an individual X  X  behavior.
A widely-used approach in location modeling is to restrict attention to a finite set of known fixed locations, effectively discretizing space and turning the problem into a multivari-ate data analysis problem where each location represents a single dimension. One can use such representations to gen-erate a sparse matrix consisting of individuals as rows and locations as columns, where each cell i,j contains the count of the number of events for individual i that occurred at lo-cation j . The locations (columns) can be defined in different ways. For example, one can define the columns by identify-ing a set of specific locations such as shops, restaurants, and so forth (e.g., see [5, 6, 9]). An alternative approach is to discretize the spatial domain into disjoint cells (e.g., via clus-tering), and then associate a discrete set of venues with each cell (as in Cranshaw et al. [10] who used Foursquare check-in venues) or to aggregate the counts of geolocated events within each cell (as in Lee et al. [20] and Frias-Martinez et al. [15]). The advantage of these discretized representations is that they allow the application of broad set of multivariate data analysis tools, such as clustering techniques or matrix decomposition methods. However, they do not explicitly encode spatial semantics and, as such, do not provide the ability to make predictions in continuous space, which is a primary aim in our work.
In the context of continuous models, a number of authors have explored such models for individual location data in prior work. For example, Gonzalez et al. [16] and Brock-mann et al. [4] explored general distributional patterns of human mobility from location data. Eagle and Pentland [12], Li et al. [21], and Cho et al. [7] demonstrated how dif-ferent aspects of individuals X  daily routines can be effectively extracted from traces of location data.

A simple approach to modeling an individual X  X  spatial density f i ( x,y ) is to use a single Gaussian density function, i.e., where e i = ( x,y ) is a 2-dimensional longitude-latitude pair,  X  is a 2-dimensional mean vector, and  X  i is a 2  X  2 covari-ance matrix. The unimodal and elliptical density contours Figure 3: On the left (a): Geotagged events in the area between Los Angeles and Las Vegas near the city of Barstow, CA. (b): The contour lines of a Gaussian mixture model with 2 components. Figure best viewed in color. of a single Gaussian are too simple to accurately represent human location data in practice. With this in mind, a fi-nite mixture of C Gaussian densities can provide additional flexibility, defined as with parameters  X  i consisting of the C mixing weights  X  ,..., X  C , P c  X  c = 1, and means  X  trices  X  ic , 1  X  c  X  C . For example, as described in Cho et al. [7], a two-component ( C = 2) spatial model may be a use-ful model for capturing the bimodal variation due to  X  X ome X  and  X  X ork X  components in an individual X  X  spatial data.
While the mixture model can provide additional modeling power beyond that of a single Gaussian, it has a number of practical limitations. Firstly, the number of components C required for an accurate density model may vary consider-ably across different individuals, and automatically and reli-ably determining the number of components is a non-trivial problem. Secondly, the number of data points per individ-ual is usually skewed towards small counts. For example, in our Twitter data set 60% of the individuals have associated with them 5 or fewer events over a 2 month period (July and August 2013). This makes it challenging, if not impossible, to fit mixture models, even if the number of components C for each individual is known and fixed. A third limita-tion of the Gaussian mixture approach is a more pragmatic one. Human mobility is constrained by our environment resulting in sharp transitions in spatial densities, due both to natural topography (mountains, oceans) and man-made artifacts (roads, city centers, etc.). Figure 3(a) shows Twit-ter data for a region near Barstow, California. The spatial density of the data shows significant local variation, includ-ing regions of high density for the town of Barstow (bottom left), for the military base (top center), and along the var-ious major roads, with very low density in the surrounding desert. Figure 3(b) shows a fitted mixture density model with two components: it is unable to capture many of the high density patterns and  X  X astes X  considerable probability mass over sparsely populated desert regions.
To address these limitations, we investigate the use of ker-nel density estimation (KDE) methods as outlined in detail in the next section. There has been limited prior work inves-tigating the application of KDE methods in the context of human location data. Zhang and Chow [29] illustrated the advantages of KDE techniques (over Gaussian mixture mod-els) for data from location-based social networks, but used 1-dimensional kernel densities on distances rather than 2-dimensional spatial models, and Hasan et al. [18] illustrated the use of 2-dimensional spatial KDE models for exploratory analysis of check-in data. KDE methods have also been used in application areas such as epidemiology [2], ecology [14], and marketing [11], for modeling spatial densities of popu-lations of individual entities, but not for modeling spatial densities of individuals themselves.
Kernel density estimation is a non-parametric method for estimating a density function from a random sample of data [25]. Let E = { e 1 ,...,e n } be a set of historical events where e = &lt; x,y &gt; is a two-dimensional location, 1  X  j  X  n , and where we have suppressed any dependence on individual i for the moment and dropped dependence on time t . We will refer to E as the training data set. A simple approach for estimating a bivariate density function from such data is to use a single fixed bandwidth h for both spatial dimen-sions and a Gaussian kernel function K (  X  ). This results in a bivariate KDE of the following form: where e is the location for which we wish to compute the density and h &gt; 0 is a fixed scalar bandwidth parameter for all events in E . It is well known that the resulting den-sity estimate f KD can be highly sensitive to the value of the bandwidth h , producing densities that are sharply peaked around the training data points e j when h is too small, and producing an overly smooth estimate that may omit impor-tant structure in the data (such as multiple modes) when h is too large [25].

There are a number of techniques that can be used to evaluate the quality of a particular value for the bandwidth h . One straightforward data-driven option is to measure the log-probability (or log-likelihood) of a set of test data points not used in constructing the density estimate, i.e., where the n t events e r are data points not included in the training data E (e.g., a validation set). Larger values of L ( h ) are preferred since it means that higher probability is being assigned to new unseen data. Hence, a simple approach to bandwidth selection (at least for a single bandwidth param-eter h ) is to perform a grid-search on h using a validation set. We will use the above out-of-sample log-probability score function L ( h ) later in the paper for both bandwidth selection and for comparing different types of density mod-els.

One could also use various  X  X lug-in X  estimates for h , such as that of Silverman [25, pages 86-88]. These estimates are optimal (e.g. in a mean integrated squared error sense) if the true underlying density being estimated is Gaussian, and can work well in practice for other related smooth non-Gaussian densities. However, for spatial location data we found that  X  X lug-in X  estimates for h were much too large and signifi-cantly oversmoothed the KDE results in a very poor fit due to the highly multimodal nature of location data.

At this point in the discussion it is worth noting that, in addition to its advantages in terms of flexibility, kernel den-sity estimation has some well known drawbacks that have tended to limit its use in practice in the past, particularly in machine learning and data mining. The first drawback is that it is particularly susceptible to the curse of dimension-ality, essentially requiring an exponential number of data points as a function of data dimensionality d . This is not an issue for location-data modeling since we are in the low-dimensional regime of d = 2. A second drawback of kernel densities (as with related  X  X eighbor-based X  methods) is the need to store all of the training data in memory at prediction time. This was arguably a relevant point 10 years or more ago when memory was relatively expensive, but in current times it is relatively inexpensive (both computationally and financially) to keep millions (or even hundreds of millions) of points accessible in main memory at prediction time. We will return to this point in more detail later in the paper X  here it is sufficient to note that kernel density estimation is practical for 2-dimensional problems with millions of data points.
A limitation of the approach described above is that the smoothing is homogeneous, i.e., the amount of smoothing is constant through the 2-dimensional region since the band-width h is fixed for all events. This does not reflect the realities of human-location data where dense urban areas will tend to have high event density and sparsely-populated rural areas will have low event density. This limitation is clearly visible in Figure 4. The two plots in the center use the same small fixed bandwidth of h = 10 meters, which works well for the relatively dense area of Laguna Beach (upper plot), but works poorly (overfits) for the rural area near Barstow, CA in the lower plot. If the bandwidth is in-creased to h = 400 meters, as in the two plots to the right, we find that this produces a more acceptable result in the lower plot (the rural area) but is vastly oversmoothing in the upper plot.

One approach to address this issue is to use an adap-tive kernel bandwidth, several methods of which have been proposed in the literature, that relaxes the assumption of a constant fixed bandwidth parameter. Breiman et al. [3] sug-gested adapting the kernel bandwidth h j to each data point e . Using this idea, we let h j be the Euclidean distance to the k th nearest neighbor to e j in the training data. Hence Table 1: Average log-probability scores on held-out events, comparing the fixed and the adaptive ap-proaches for kernel density estimation for Twitter geolocation data. we can define an adaptive bandwidth kernel density estimate as: where K h j is defined as in Equation 4 replacing h with h
Table 1 shows the results from a series of tests on a val-idation data set, comparing the fixed and adaptive band-width approaches using different values for (a) the fixed bandwidth h , and (b) the number of neighbors k (for the adaptive method). We trained the models using 100,000 ran-domly selected events from our Twitter data set (described in more detail later in the paper) and then computed the log-probability score (Equation 5) using a set of n t = 100 , 000 randomly selected held-out events. From the results, we can see that the adaptive bandwidth models dominate the per-formance of the fixed bandwidth methods. As a sidenote, the  X  X lug-in X  methods performed significantly worse (results not shown).
So far, our predictive model f KD ( e | E,h ), does not depend on the identity of an individual i . However, our primary goal in this work is to be able to build accurate predictive spatial density models at the individual level.
To address this task, we could apply the adaptive kernel density methods described above at the level of an individual (rather than for aggregations of events across a population of individuals), computing f KD ( e | E i ) in Equation 6 where we now condition on just the individual X  X  event data E i rather than the events for the population E .

A significant challenge with building individual-level mod-els in this manner is the  X  X old-start X  problem, given that we typically have very little data for many of the individu-als for whom we wish to make predictions. To address this data sparsity problem we propose a multi-scale kernel model where we use a mixture of (a) an individual X  X  potentially noisy kernel density estimate with (b) more robust coarse-scale models 1 . More specifically we define a mixture-KDE for individual i as where  X  1 ,...,..., X  C are non-negative mixing weights with P c  X  c = 1, and f KD ( e | E ture. Here component c is a kernel density estimate com-puted as a function only of a subset of points (or events) E The component density estimates, f KD , can be any density model, including fixed or adaptive bandwidth KDEs. We use adaptive bandwidth KDEs, with k = 5 (following Table 1), for all of the components in the mixture-KDEs used in this paper.

As a specific example consider a model for individual i where C = 2, with the first component being the individual-level kernel density with E 1 = E i , and the second compo-nent being a population-level kernel density estimate with E 2 = E . This mixture will have the effect of smoothing the individual X  X  density towards the population density, with more or less smoothing depending on the relative size of the  X  weights. Note that this mixture is significantly different in nature to the use of a Gaussian mixture for an individual X  X  data (e.g., as in [7]). In that approach, each component typ-ically represents a different spatial location around which an individual X  X  activity is centered (such as  X  X ome X  and  X  X ork X ), whereas in the mixture-KDE each mixture component is re-sponsible for a broader spatial scale of activity.

For C components, where C &gt; 2, we can have the first component be an individual level density, and the C th com-ponent be the population density, where the intermediate components c = 2 ,...,C  X  1 can represent different spa-tial scales (such as neighborhoods, cities, states, and even countries). Given that the data sets we are using in this paper are from the Southern California region, we chose to use a 3-level model ( C = 3), with the first and last com-ponents being the individual and population level models respectively, and the c = 2 model representing (approxi-mately) the  X  X ome city X  for an individual (more details on this intermediate scale component are provided later in the paper). This process of selecting additional spatial compo-nents, that are  X  X etween X  the individual and the full popula-tion, is somewhat arbitrary X  X e chose a single intermediate component in the work below, but other choices could be explored for other applications.

From a generative perspective, one interpretation of Equa-tion 7 above for the mixture-KDE is as follows. Assuming that the first component is based only on individual i  X  X  data, individual i has a probability  X  1 of generating events in the future in a manner similar to his/her past behavior, and a probability  X  c ( c = 2 ,...,C ) of generating events in ac-cordance with the larger  X  X ubpopulations X  defined by events E . In this paper, in the absence of additional metadata about the individuals, we defined the larger subpopulations solely on spatial characteristics (component 2 being roughly a city, and component 3 being the whole southern California region). However, one could also define the larger subpop-
A potential alternative option would be a Bayesian hierar-chical model X  X owever Bayesian modeling with kernel den-sities is not straightforward given that kernels don X  X  have parameters that can be  X  X hrunk X  as in the usual Bayesian approach. ulations based on metadata (if it were available), such as demographics, social ties, and so forth.

Another way to interpret the mixture-KDE model is that it provides a relatively simple weighting mechanism to allow us to upweight data points from individual i in the kernel density estimate and downweight points that don X  X  belong to the individual. Given that kernel densities are defined as weighted sums over all points, the mixture-KDE can be thought of as a form of kernel density estimate where the points are given an additional level of weighting depending on what component they belong to. As a sidenote, in the results presented here we allow data points to belong to multiple sets E c , e.g., a data point for individual i can be included in the kernel density estimate for all C components. The other option would be to only allow points to belong to a single component. Either option is fine in theory: from the weighted kernel perspective it only changes (in effect) the manner in which we are assigning weights to each point in the overall weighted sum.
Given a set of components f KD ( e | E c ) ,c = 1 ...C , the next step is to compute the mixing weights  X  1 ,..., X  Equation 7. To do so, we randomly sample a validation set, disjoint from the training set, and use it to learn the weights as follows. For each event in the validation set, its density value under each component c is computed (using the train-ing set for the KDE computation). This results in a fixed set of component density values on the validation data points and we can optimize over the convex set of mixing weights  X  c to find the highest-likelihood combination. We used the Expectation-Maximization (EM) algorithm since it is easy to implement and converges quickly (one could use other op-timization techniques such as gradient descent) X  X his is in essence the same as using EM for learning finite mixtures (such as Gaussian mixtures) but where the parameters of the component densities are known and fixed and one is just learning the mixture weights (see also Smyth and Wolpert [26]). An alternative to using fixed  X   X  X  would be to allow the weights to vary by individual, in particular as a function of the number of data points for each individual. Prelimi-nary exploration of this idea suggested that any additional predictive power that might be attained would likely not be justified by the additional complexity.
For our experiments, we use two geolocation/check-in data sets: Twitter and Gowalla . Twitter is a popular micro-blogging service that allows the posting of short texts (up to 140 characters) and pictures. Using the Twitter API [1] we collected over 4 million public tweets from 230,450 unique individuals in the Southern California area over the period of July-August 2013. In the experiments in this paper we use data only from weekdays, i.e. Monday trough Friday. To remove repeated and bursty events we replaced tweets occurring with the same hour and within 50 meters of each other with a single effective tweet. Figure 1 shows the spatial distribution of our data set. The Gowalla data is the same data used by Cho et al. [7], containing 145 , 558 events from 7 , 653 unique individuals on weekdays between January and October, 2010, in the southern California area as shown in Figure 2. Training, validation, and test sets were extracted from both data sets for our experiments (details provided later).
We evaluated each of the following individual-level mod-els in our experiments. By an  X  X ndividual-level model X  we mean a model that is fit to each individual i using only their data E i , and then used for predicting future events for that individual i .
 Gaussian : A Gaussian density model. We used maximum likelihood estimates for the mean and maximum a posteriori (MAP) estimates for the covariance matrices (e.g., see [22], chapter 4.6.2):  X  =  X   X  MLE ,  X  =  X   X  0 + (1  X   X  )  X   X  MLE ,  X  = n 0 where the prior parameters were set to n 0 = 3 and  X  = 5 kilometers (for the diagonal on the prior covariance) via a grid search over the log-likelihood on a validation set across all individuals.
 Gaussian Mixture Model (GMM) : Two different Gaus-sian mixture models with C = 2 and C = 4 components. The models were fit using the EM algorithm. Again, we used maximum likelihood to estimate the  X  c and MAP estimates for the  X  c . Parameters for the priors were also determined on a validation set.
 Fixed KDE: A fixed bandwidth kernel density estimate using Equation 3. The validation set was used to determine a single fixed bandwidth, h = 5 . 3 kilometers, for all users. Adaptive KDE: An adaptive kernel density estimate us-ing Equation 6. The validation set was used to determine a nearest-neighbor value of k = 5 for all users.
 In addition, for our log-likelihood experiments we evaluated a single  X  X lobal X  population model (Population KDE) using an adaptive kernel density estimate ( k = 5) based on all data points in the training set (i.e., not an individual-level model).

For our mixture-KDE model we used 3 components, the first and last corresponding to individual i and to the full population, respectively. Each component is an adaptive bandwidth KDE with k = 5 neighbors. For the middle com-ponent we divided the southern Calfornia area into 81 re-gions corresponding to a 9  X  9 array of equal-sized grid boxes. Each individual i was associated with the region that con-tains the majority of their individual events X  X hus, the c = 2 component in the model represents the scale of a local re-gion or city. A similar approach was used in prior work for finding the  X  X ome X  location of an individual [7]. Using EM to determine the mixture weights resulted in the following values for the  X   X  X : 0.85 for  X  1 (the individual level), 0.12 for  X  2 (the region level), and 0.03 for  X  3 (the population level) for the Twitter data set, and  X  1 = 0 . 5,  X  2 = 0 . 3 and  X  3 = 0 . 2 for the Gowalla data set.
The training set for the Twitter data consists of all events recorded for the month of July, 2013. The test set for Twit-Population KDE 2.014 0.563 0.784 0.237 Table 2: Average log-probabilities on the test data for individuals and events from the Twitter data set. Population KDE 3.388 1.237 4.021 0.923 Table 3: Average log-probabilities on the test data for individuals and events from the Gowalla data set. ter is all of the events in August 2013 for a randomly se-lected set of 2000 individuals, selected from the individuals that have at least 2 events in July. To create the Gowalla training set we used the data from the months of January to June, 2010. The test set for Gowalla is all of the events from the months of July to October, 2010 for a randomly se-lected set of 1000 individuals, selected from the individuals that have at least 2 events in the months of January to June, 2010. A validation set was generated for each of the Twitter and Gowalla data sets by setting aside approximately 37,000 and 25,000 randomly selected events from each training data set.

We built models on the data from our training data set and computed the test set log-probability score of the events in the test set, under each model. We report results both in terms of the mean and median log-likelihood per event, and the mean and median log-likelihood per individual (the lat-ter giving equal weight to individuals, the former to events). The mean and median scores, for both individual and event-level scores, are shown in Tables 2 and 3 for the Twitter and Gowalla data sets respectively. A (  X  ) indicates that the test log-likelihood was not computed due to numerical under-flow. The mixture-KDE model clearly outperforms all other methods, on both data sets, for all metrics, assigning sig-nificantly higher log-probability to the test events than any of the other modeling approaches. Figures 5 and 6 show the comparison between the mixture-KDE approach and the GMM (on the left) and fixed KDE (on the right) when look-ing at each individual separately, again clearly showing the improvement of the mixture-KDE approach over the other methods.
We now compare the different models by using a simu-lated real-world application based on identity theft. Over Figure 5: Upper plots (a) and (b): scatter plots for a sample of test set log-probability scores for Twitter individuals with (a) individual Gaussian mixtures ( C = 4 ) versus the mixture-KDE, and (b) individ-ual fixed-KDE versus the mixture-KDE. Lower plots (c) and (d): histograms of the score differences per event for the mixture-KDE minus the score of the corresponding model on the y-axis in the upper plot. 8 million people are victims of identity theft every year in the United States alone, with an annual cost exceeding 4 billion dollars and over 32 million hours spent by individ-uals to resolve the problem [8]. To simulate identify theft we replaced the geolocation events for an individual over a specific time-window with the geolocation events of a differ-ent individual and then tested the ability of our models to detect the resulting anomalous spatial patterns of events. We used the Twitter data set for our experiment since the Gowalla data set did not show any significant differences for this problem between the different models (single Gaussian, mixtures of Gaussians, various forms of KDEs). We believe this may be due to the fact that our data set for Gowalla has fewer individuals than Twitter, and that these individuals use many of the same checkins, limiting the effectiveness of Gowalla data for detecting  X  X dentity switching X .

Focusing on the Twitter data set, we defined the training set to be all events in the month of July, 2013. The test set consists of two types of event streams in August, 2013: events for normal  X  X ontrol X  individuals and events with sim-ulated identity theft. The control individuals are a randomly selected set of 950 individuals that have at least 2 events in July and at least 10 events in August. The individuals with simulated identity theft correspond to a set of 50 randomly selected individuals with at least 2 events in July. For each of these 50, we then replaced their real test data events in August, with the set of events from a different randomly selected individual, among individuals who have at least 10 events in August. In this manner, our test data has 50 event sets where the spatial distribution for each sets will in general look different (in a realistic manner, as if a different individual were using the Twitter account), compared to the event sets for the  X  X rue X  individual in the training data from July.

To evaluate the different models we computed a surprise index S i for each individual i , defined as the negative log-probability of individual i  X  X  events in the test data set rela-Figure 6: Upper plots (a) and (b): scatter plots for a sample of test set log-probability scores for Gowalla individuals with (a) individual Gaussian mixtures ( C = 4 ) versus the mixture-KDE, and (b) individ-ual fixed-KDE versus the mixture-KDE. Lower plots (c) and (d): histograms of the score differences per event for the mixture-KDE minus the score of the corresponding model on the y-axis in the upper plot. tive to a model constructed on historical data: where e r i is the r th event in the test data set for individual i , and  X  f i is the density estimate for individual i constructed using the training data. The larger the surprise score S then the more anomalous the events e r i are relative to the model  X  f i . In these experiments we used all of the models that we used in the log-likelihood experiments as described earlier in the paper, except for the population model which is unable to generate rankings at the individual level.
We used a grid search on the validation set (defined in a similar manner to the training-test setup above) to deter-mine various parameters of the models, where the parame-ter values were selected to optimize precision on the identity theft task. In general, optimizing for precision results in dif-ferent parameter values for the various models compared to optimizing for likelihood. The priors of the Gaussian mix-ture model resulted in n 0 = 0 . 1 and  X  = 5 kilometers. The optimal parameters for the individual KDE were estimated to be a fixed bandwidth of h = 3 kilometers and k = 5 for the adaptive method. The optimal mixture weights for the mixture-KDE model were  X  1 = 0 . 9 (the individual level),  X  2 = 0 . 08 (the region level), and  X  3 = 0 . 02 for the popula-tion model.

For each model, we then ranked all of the individuals in the test data by their surprise index S i and computed pre-cision relative to the known ground truth in terms of which individuals correspond to simulated identity theft and which to the controls. Table 4 shows the precision at 20, the frac-tion of simulated identity theft cases correctly detected in the top 20 ranked individuals. These precision numbers are the result of averaging over 50 different randomly generated test sets, using the methodology described earlier. The rows correspond to different models and the columns correspond to 3 different scenarios: computing the surprise-index per individual based on their first n events (in time) for each Table 4: Average precision (over 50 runs) for the top 20 ranked individuals in the test data, as a function of the number of observed test events n t per indi-vidual. Table 5: Average precision (over 50 runs) for the top 20 ranked individuals in the test data, as a function of the number of observed test events per individual, for  X  X old-start X  individuals (as defined in the text). individual in the test set, with n = 1 ,n = 5 ,n = 10. Table 5 shows the same information for a  X  X old-start X  scenario, where now test sets are generated for simulated identity theft and normal individuals (using the same general procedure as be-fore) who are constrained to have between 2 and 5 events in their training data (compared to any number greater than or equal to 2 for the general case).

The results in the two tables show that the mixture-KDE model dominates all of the other methods, with a Wilcoxon signed rank p-value of p &lt; 0 . 02, except for the Gaussian model in the non-cold-case situation for n = 1. This may be due to the fact that for a simulated identity theft case, a sampled new event has a high probability of coming from a popular area.

The mixture-KDE model improves as it sees more events in the test set (as n increases from left to right in the ta-ble). However, the other methods all decrease in precision as n increases. On closer inspection we found that this was being caused by their sensitivity to false alarms, i.e., with more data points per individual there is a higher chance that a control individual (a false alarm) will have an event in the test data that is not close spatially to the individual X  X  events in the training data, resulting in a high-surprise score and a high rank for that individual. The mixture-KDE is more ro-bust to this type of variation, consistent with results earlier in the paper in terms of log-likelihood.
Our experience suggests that kernel density models are quite scalable for two-dimensional data, and can likely be scaled to millions of individuals and hundreds of millions of events relatively easily. To compute the density of a new Table 6: Predictive log-probability scores, averaged over individuals and events for a) Twitter (top table) and b) Gowalla (bottom table).  X  X nline X  is the on-line version of the Mixture-KDE model as described in the text. point e , given a training data set E , we need to compute the contribution of each training point to e  X  X  density, as shown in Equation 3. In general, storing the N training data points requires O ( N ) space and computing the density for a new event will result in time complexity O ( dN ) where d is the dimension of the data (here d = 2). In our implementation of the KDE models for the results in this paper we used k-d trees, as described in [17], to further speedup our KDE im-plementation. This effectively computes only contributions from nearby points to e , based on a k-d tree partition of the 2-dimensional space, resulting in a significant reduction in computation time. We coded our algorithm for kernel density estimation in Java 2 and ran the code on an 8-core 2.4GHz Intel Xeon CPU with 8 hyper threads and 48 GB of RAM memory. Using one million events as training data points, the average time for computing the density of a new event is 8 milliseconds, making the model tractable for large data sets. Additional speed-ups could be achieved for exam-ple by distributed computation since the density contribu-tion from different subsets of points are independent from one another. Hence, one can  X  X plit X  the training data set into disjoint groups and aggregate the density contributions in parallel.
The results presented up to this point in the paper have used a batch approach to training and testing. A useful fea-ture of the kernel density approach, including the mixture-KDE, is that it is quite easy to implement an online version that can sequentially be updated as new events arrive in streaming fashion. When a new event e arrives we simply add it to the training set. For the adaptive bandwidth ap-proach, every time the training set changes, we need to find the k neighbors of the new point, as well as potentially need-ing to find new neighbors for all N existing points. In prac-tice, however, only a very small fraction of existing points will need to have their neighbors updated. Other parame-ters of the mixture-KDE model, such as mixture weights for the components, are likely to change relatively slowly over time, and can be periodically updated on validation subsets.
Tables 6(a) and 6(b) show predictive log-probability scores for the same training and test data used earlier for likeli-hood experiments, but now, each sequential test event is included in the training data in an online fashion before
The code is available for download at http://www.datalab-.uci.edu/resources/ computing the log-probability of the next event. The online model shows a significant systematic improvement in pre-dictive scores compared to the batch model, suggesting that online adaptation is beneficial with human location data. This certainly makes intuitive sense, as we expect individual behavior to be non-stationary and changing over time. In a practical application of an online model one would likely in-corporate some downweighting (e.g., via exponential weight-ing) or windowing of events that are further back in time, allowing the model to adapt to changes in individual behav-ior.
In this paper we proposed and investigated a systematic framework for modeling human location data at an individ-ual level using kernel density estimation methods. We found that adaptive bandwidth methods had distinct advantages for this type of data over fixed bandwidth methods. To ad-dress the problem of data sparsity at the individual level we introduced the idea of mixtures of kernel density estimates at different spatial scales. This allows smoothing of an in-dividual X  X  model towards the aggregate population model, motivated by the desire for better generalization to new data. Experimental results on both Twitter and Gowalla data sets systematically illustrated the benefits of our ap-proach in terms of predictive power: kernel methods were systematically better than mixture models, adaptive band-width methods were systematically better than fixed band-width methods, and mixing individual and population esti-mates via the multi-scale mixture-KDE model outperformed all other approaches.

There are a number of extensions that could be further explored. One example is the discrete-location effect in data sets such as Twitter and Gowalla, namely that certain specific longitude-latitude locations are over-represented in the data. This suggests that additional gains in accuracy could be gained by modeling such locations as discrete delta-functions (with probability weights) in the kernel model, rather than using the simpler standard kernel approach. An-other aspect we did not pursue in this paper is including time in our models X  X or many applications (such as identity theft detection) it would beneficial to have a distribution over time as well as space for individual events. Initial ex-periments in this direction suggestion that including time is not as straightforward as simply extending the spatial ker-nel densities from 2 to 3 dimensions X  X he temporal dimen-sion, not surprisingly, has distinctly different characteristics than the spatial dimensions. Nonetheless we anticipate that spatio-temporal kernel density models can be developed in a relatively straightforward manner.
 This work was supported by a gift from the University Af-fairs Committee at Xerox Corporation, by the National Sci-ence Foundation under award IIS-1320527, and by Office of Naval Research under MURI grant N00014-08-1-1015. [1] Twitter streaming api. [2] J. Bithell. An application of density estimation to [3] L. Breiman, W. Meisel, and E. Purcell. Variable kernel [4] D. Brockmann, L. Hufnagel, and T. Geisel. The [5] J. Chang and E. Sun. Location 3: How users share [6] C. Cheng, H. Yang, I. King, and M. R. Lyu. Fused [7] E. Cho, S. A. Myers, and J. Leskovec. Friendship and [8] Federal Trade Commission Identity theft survey [9] J. Cranshaw, R. Schwartz, J. I. Hong, and N. M. [10] J. Cranshaw and T. Yano. Seeing a home away from [11] N. Donthu and R. T. Rust. Estimating geographic [12] N. Eagle and A. S. Pentland. Eigenbehaviors: [13] M. D. Escobar and M. West. Bayesian density [14] J. Fieberg. Kernel density estimators of home range: [15] V. Frias-Martinez, V. Soto, H. Hohwald, and [16] M. C. Gonzalez, C. A. Hidalgo, and A.-L. Barabasi. [17] A. G. Gray and A. W. Moore. Nonparametric density [18] S. Hasan, X. Zhan, and S. V. Ukkusuri.
 [19] K. Joseph, C. H. Tan, and K. M. Carley. Beyond local, [20] R. Lee, S. Wakamiya, and K. Sumiya. Urban area [21] Z. Li, B. Ding, J. Han, R. Kays, and P. Nye. Mining [22] K. P. Murphy. Machine Learning: A Probabilistic [23] A. Sadilek, H. A. Kautz, and V. Silenzio. Modeling [24] S. Scellato, M. Musolesi, C. Mascolo, V. Latora, and [25] B. W. Silverman. Density Estimation for Statistics [26] P. Smyth and D. Wolpert. Linearly combining density [27] L. Song, D. Kotz, R. Jain, and X. He. Evaluating [28] S. J. Vaughan-Nichols. Will mobile computing X  X  future [29] J.-D. Zhang and C.-Y. Chow. igslr: personalized
