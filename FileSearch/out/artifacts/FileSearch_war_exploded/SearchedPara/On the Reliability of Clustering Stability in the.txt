 in the main paper. For that we will need some additional notat ion. d
D ( A k ( S 1 ) , A k ( S 2 )) d
D ( A k ( S 1 ) , A k ( S 2 )) := where  X , X   X   X   X  are the solutions returned by A which without scaling simply converge to zero in probabilit y as m  X  X  X  . For some  X  &gt; 0 and a set S  X  R n , let B (namely, contain an  X  -neighborhood for some positive  X  ). We will also need to define the following variant of d m clusterings, with respect to a set B  X  R n , as In particular, d m defined with respect to f a random variable with respect to drawing and clustering S Conditions. The following conditions shall be assumed to hold: For example, condition 3c and the assumptions on F r required there.
 [ 8 ]), defined as follows. Let { X probability space. We write X X m = o p (1) we mean that X By condition 3a , f with a remainder term uniformly bounded for any x : By the asymptotic normality assumption,  X  m k  X   X   X   X  Therefore, we get from Eq. ( 2 ) that where the remainder term o of
X , (  X / X   X  ) f  X  in which  X  resides. As a result, the mapping  X   X  7 X  ((  X / X   X  ) f theorem [ 8 ] and Eq. ( 3 ), this implies that  X  m ( f process G ( ) , where D.1 A High Level Description of the Proof a general overview of the proof, without the finer details. The purpose of the stability estimator  X   X  k  X  X xpected X  value of the random variable d m d
D ( A k ( S 1 ) , A k ( S 2 )) is likely to be close to the value \ instab ( A way to go about it is to prove that \ instab ( A and then use some large deviation bound to prove that  X  m  X   X  k probability, if q is large enough. Unfortunately, computing lim d
D ( A k ( S 1 ) , A k ( S 2 )) same as lim analyze the asymptotic expectation of a different random va riable, d m was formally defined in Eq. ( 1 ). Informally, recall that d m dent samples of size m . Then d m d r/  X  that if r be large enough, then d m The advantage of the  X  X urrogate X  random variable d m for any finite r , unlike d m late lim \ instab ( A making r large enough). Using the fact that d m  X  d m  X   X  , By picking r to scale appropriately with q , our theorem follows. lined above to prove our theorem.
 A few more words are in order about the calculation of lim d recall that d m two independent samples of size m . For any x  X  X , let A Then we can write d m single strip near one of the limit cluster boundaries F take the integral over all of B of the cluster association function f D.2 Part 1: Auxiliary Result characterizing the asymptotic expected value of d m where h ( r ) = O (exp(  X  r 2 )) . To prove this result, we will need several technical lemmas. F where n Proof. Let B  X  close enough to S : Since S is orientable, then for small enough  X  &gt; 0 , B  X  particular, the map  X  : S  X  [  X   X , X  ] 7 X  B  X  that D X  ( x , 0) = 1 for every x  X  S .
 We now wish to claim that as  X   X  0 , To see this, we begin by noting that B  X  B using the fact that D X  is continuous and equals 1 on S  X { 0 } , Lemma D.2. Let ( g uniformly for all x as m  X  X  X  . Then for any i,j  X  X  1 ,...,k } ,i 6 = j , as m  X  X  X  Proof. By the assumptions on ( g converging to 0 , such that Lemma D.1 with  X  = r/  X  m , we have that as m  X  X  X  , for some constant C dependant on r and the upper bound on p ( ) . Since b that the expression in the lemma converges to 0 as well. Lemma D.3. Let ( X defined on the same probability space, and X converges in distribution to a continuous random variable Y . Then | Pr( X converges to 0 uniformly for all c  X  R .
 random variables A,B , any c  X  R and any  X  &gt; 0 , it holds that From this inequality, it follows that for any c  X  R and any  X  &gt; 0 , | Pr( X m  X  c )  X  Pr( Y m  X  c ) | X  Pr( Y m  X  c +  X  )  X  Pr( Y m  X  c ) To see this, we begin by noticing that Pr( | X convergence in probability. Next, Pr( Y to 0 uniformly for all c .
 Lemma D.4. Pr( a ,  X  m ( f any x  X  X  , any a 6 = 0 in some bounded subset of R k , and any b  X  R . Proof. By Eq. ( 3 ), 4.2 in [ 6 ], we have that if a sequence of random vectors ( X random variable X in distribution, then Pr( h y ,X extend this to cases where X clustering). Therefore, recalling that  X  m (  X   X   X   X  vector Z , we have that uniformly for all x , a ,b , Pr Here we think of a ((  X / X   X  ) f condition 3a , and assuming a 6 = 0 , we have that a ((  X / X   X  ) f
Pr a , where the convergence is uniform for any bounded a 6 = 0 , b and x  X  X  . Combining Eq. ( 10 ) and Eq. ( 11 ) gives us the required result. where o (1)  X  0 as m  X  X  X  and h ( r ) = O (exp(  X  r 2 )) . Proof. Define a  X  R k as a where o (1) converges uniformly to 0 as m  X  X  X  .
 Since G G j ( x )) &gt; 0 notation, the expression is also valid in the case where Var ( G G f 2 expression can be rewritten as: 2 that the above can be rewritten as 2 where n Let us assume for a minute that Var ( G term which is uniform for all x : p Var ( G i ( x + y n x )  X  G j ( x + y n x )) Since f to Notice that  X  ( f n sign, as As a result, denoting s ( x ) :=  X  ( f  X  =  X  =  X  In the preceding development, we have assumed that Var ( G even if Var ( G 1 O (  X  can rewrite Eq. ( 14 ) as We now perform a change of variables, letting z which is equal by the mean value theorem to for some x As a result, as r  X  X  X  , we have that the required result.
 and sample size m , define F m from any other cluster boundary (with respect to  X  Letting S As to the first integral, notice that each point in F m F disjoint for each i,j , and we can rewrite the above as: r/  X  m  X  X  X  , and we can rewrite the above as: If there were only two clusters i,j , then This is simply by definition of A independent random sample, x is more associated with cluster j . (for some i,j ) is much closer to F distance to F of at most O (1 /  X  m ) fluctuations), we have that uniformly for any x , where o (1) converges to 0 as m  X  X  X  .
 back into F interested in can be written as Now we can apply Lemma D.5 to each summand, and get the required result. D.3 Part 2: Proof of Thm. 1 For notational convenience, we will denote Proposition D.1 , we have that d m dition 3d , for any fixed q , 1 d ity as r increases). Therefore,  X  m  X   X  k Lemma D.6. Fix some r &gt; 0 . Let X distributed random variables, such that Pr( X variable on the same probability space, such that Pr( Y  X  &gt; 0 , Proof. Define an auxiliary set of random variables Z X i  X  [0 ,r ] probability in the lemma above can be written as where Z the required result.
 We now turn to the proof of the theorem. Let A m d is always in an r/  X  m -neighborhood of the limit cluster boundaries. Since p ( ) is bounded, we have that d m constants depending only on D and  X   X  &gt; 0 , Pr  X  Pr + Pr h We will assume w.l.o.g that  X / 2 &lt; \ instab ( A Pr tity  X   X  for which  X   X  / 2 &lt; \ instab ( A that  X   X  k of size m to empirically estimate P number in the range of ( \ instab ( A that the second summand in Eq. ( 22 ) is upper bounded by: where D Using the fact that D to 0 as m  X  X  X  .
 upper bound it by:
Pr By the definition of \ instab ( A Using Eq. ( 25 ) and Eq. ( 21 ), we can upper bound Eq. ( 24 ) by
Pr have that for any  X  &gt; 0 ,
Pr  X  (1  X  Pr( A m r ))  X  1 + Pr( A m r ) Pr  X  (1  X  Pr( A m r )) + 2 Pr( A m r ) exp  X  Lemma D.6 can be applied because d m If m,r are such that (1  X  Pr( A m r )) + 2 Pr( A m r ) exp  X  Let m  X  X  X  , Eq. ( 29 ) converges to (1  X  (1  X  g ( r ))) q ) + 2(1  X  g ( r )) q exp  X  is the same quantity appearing in condition 3d . It follows that (as q  X  X  X  ), and can be bounded in turn by o ( q  X  1 / 2 ) . \ instab ( A terminology used.
 Theorem E.1 (Van der Vaart) . Let  X  in probability, and such that the sequence  X  m ( X  random element Z . Let  X  7 X   X (  X  ) be Fr  X  echet-differentiable at  X   X   X  in probability, and  X   X  converges in probability to  X   X   X  linear operator A : U 7 X  V such that clustering, and the feature map from X to H in kernel clustering. Thm. E.1 is that Eq. ( 31 ) holds. Notice that it is enough to show that is a Donsker class , where this means that if we sample i.i.d m elements from D , then ( f ( x f ( ) in the set, in an appropriately defined sense. function classes, from X to R , are Donsker: to show that {h , X  ( h ) i} our class of functions can be written as { k ( , h ) } any solution close enough to  X  stability properties, via central limit theorems, is at all possible. ingredients required to apply Thm. E.1 .
 As to the asymptotic distribution of  X  m ( X  that for any i  X  X  1 ,...,k } , where x standard central limit theorem,  X  m ( X  i random vector Y , with covariance matrix Moreover, it is easily verified that Cov ( X   X  variance matrix V is composed of k diagonal blocks ( V zero.
 Thus, we can use Thm. E.1 to get that  X  m (  X   X   X   X  random vector of the form  X   X   X   X  1 the form  X   X   X  1 with respect to the parameters, namely the score function This is a random mapping based on the sample x distribution D : and  X   X  converges in probability to some  X   X  Z-estimators, such as Thm. 5.21 in [ 7 ].

