 Abstract In this paper we provide an account of the cross-lingual lexical substi-tution task run as part of SemEval-2010. In this task both annotators (native Spanish speakers, proficient in English) and participating systems had to find Spanish translations for target words in the context of an English sentence. Because only translations of a single lexical unit were required, this task does not necessitate a full blown translation system. This we hope encouraged those working specifically on lexical semantics to participate without a requirement for them to use machine translation software, though they were free to use whatever resources they chose. In this paper we pay particular attention to the resources used by the various partici-pating systems and present analyses to demonstrate the relative strengths of the systems as well as the requirements they have in terms of resources. In addition to the analyses of individual systems we also present the results of a combined system based on voting from the individual systems. We demonstrate that the system produces better results at finding the most frequent translation from the annotators compared to the highest ranked translation provided by individual systems. This supports our other analyses that the systems are heterogeneous, with different strengths and weaknesses.
 Keywords SemEval 2010 Cross lingual Lexical substitution 1 Introduction This paper provides an account of the cross-lingual lexical substitution task ( CLLS ) which was run at SemEval-2010. In CLLS , annotators and systems had to find an alternative substitute word or phrase 1 in Spanish for an English target word in context. The task is based on the English lexical substitution task (hereafter referred to as LEXSUB ) run at SemEval-2007, where both target words and substitutes were in English (McCarthy and Navigli 2007 ).

An automatic system for cross-lingual lexical substitution would be useful for a number of applications. For instance, such a system could be used to assist human translators in their work, by providing a number of correct translations that the human translator can choose from. Similarly, the system could be used to assist language learners, by providing them with the interpretation of the unknown words in a text written in the language they are learning. Last but not least, the output of a cross-lingual lexical substitution system could be used as input to existing systems for cross-language information retrieval or automatic machine translation.

As well as the practical applications, the data used in this study is useful for studies of word meaning. The test items, words in the context of a sentence, were drawn from the original LEXSUB task that allows for comparison between paraphrases and translations of the same target words in context. Erk et al. ( 2009 ) also used a portion of the LEXSUB data for their study of graded WordNet judgments and  X  X sage X  similarity judgments (how similar two usages of the same word are in a pair of sentences taken from LEXSUB ). The portion of data that is in common to both CLLS and the study by Erk et al. has already been subject to analyses to determine how well these different annotations correlate with one another (McCarthy 2011 ).
The structure of the paper is as follows. In the next section we further expand on the motivation for this task and related work. In Sect. 3 we give an overview of the task. We give a description of the participating systems in Sect. 4 paying particular attention to the resources used. Section 5 provides the results including a new analysis of system performance by part of speech (PoS) that was not provided in the original SemEval-2010 paper. In Sect. 6 we extend the analysis of system performance by considering how the system results correlate and how the different approaches contrast with one another when using a disruptive set analysis. We find that while there are systems that outperform others, there is an advantage in combining approaches since the systems are quite heterogeneous and we demonstrate this with a combination system that combines individual system outputs using voting. 2 Motivation and related work Recently, there has been a good deal of work in the field of multilingual/cross-lingual word sense disambiguation. Apidiniaki ( 2009 ) explores how useful the information found in parallel corpora is in relation to referring to a meaning of a word as a translation in another language, and presents an unsupervised system that explores the results of a data-driven sense induction method. Su and Markert ( 2010 ) apply a binary classification on words in context as to whether they are being used subjectively or objectively for cross lingual (English to Chinese) lexical substitu-tion. The intuition is that a good translation will have the same classification (subjective or objective) as the original word in context. Davidov and Rappoport ( 2009 ) attempt to extend a given concept by using translations into intermediate languages and disambiguating the translations using Web counts, where, given a set of terms (in a given language) that share a meaning (akin to WordNet synsets), the goal is to add more terms to the set that also have the same meaning. Navigli and Ponzetto ( 2012 ) use graph-based algorithms applied to a large multilingual semantic network built from Wikipedia, to perform multilingual word sense disambiguation and determine the most appropriate translations for a target word in a given context.
While there has been a lot of discussion on the relevant sense distinctions for monolingual WSD systems, for machine translation applications common practice is to use the possible translations for the target words (Apidianaki 2011 ; Carpuat and Wu 2007 ; Chan et al. 2007 ; Lefever and Hoste 2010 ; Resnik and Yarowsky 2000 ; Vickrey et al. 2005 ). One early and notable work was the S ENSEVAL -2 Japanese Translation task (Kurohashi 2001 ) that obtained alternative translation records of typical usages of a test word, also referred to as a translation memory . Systems could either select the most appropriate translation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, in our task we used annotators to provide translations for individual target instances, rather than predetermine the set of translations using lexicographers or rely on post-hoc evaluation, which does not permit evaluation of new systems after the competition.

Previous standalone WSD tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan 2007 ). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not (Sinha et al. 2009 ).

In our task, we collected a dataset that allows instances of the same word to have some translations in common, while not necessitating a clustering of translations from a specific resource into senses [as opposed to another S EMEVAL 2010 task focusing on cross-lingual representations for word senses (Lefever and Hoste 2010 )]. 2 Resnik and Yarowsky ( 2000 ) also conducted experiments using words in context, rather than a predefined sense-inventory however in those experiments the annotators were asked for a single preferred translation. In our case, we allowed annotators to supply as many translations as they felt were equally valid. This allows us to examine more subtle relationships between usages and to allow partial credit to systems that get a close approximation to the annotators X  translations. Unlike a full blown machine translation task (Carpuat and Wu 2007 ), annotators and systems are not required to translate the whole context but just the target word. Nevertheless, as we will see in Sect. 4 , some participants did make use of various machine translation technologies. 3 The cross-lingual lexical substitution task CLLS follows LEXSUB (McCarthy and Navigli 2007 ), except that the substitution annotations are lexical translations rather than lexical paraphrases. Given a target word in context, the task is to provide several correct lemmatized translations for that word in a given language. We used English as the source language and Spanish as the target language. For example, the annotators and systems were asked to find a Spanish translation for the lemma charge as it is used in the following context:
Annual fees are charged on a pro-rata basis to correspond with the standardised renewal date in December.

The annotators suggested the lemmas cargar and cobrar , which are the Spanish substitutes for the target word charged as used in this context. In this paper, we use the terms lemma and word interchangeably for the Spanish substitutes. We also refer to these as translations, though in fact they are lemmatized by both systems and annotators and further post-processing would be required for translation. Following LEXSUB , we require lemmas rather than surface forms to focus the evaluation on the lexical semantic capabilities of systems rather than surface form processing. 3.1 Background: the English lexical substitution task LEXSUB was run at SemEval-2007 (McCarthy and Navigli 2007 , 2009 ) following earlier ideas (McCarthy 2002 ) to examine the capabilities of WSD systems on a task that circumvented issues regarding semantic representation and has potential for applications. LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is possible to use alternative representations of meaning, such as those proposed in earlier work (Pantel and Lin 2002 ; Schu  X  tze 1998 ). For the event, the participants all used manually produced resources, though afterwards there has been further exploration of thesauri acquired automatically (McCarthy et al. 2010 ). The motivation for a substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while not requiring a complete system that might mask system capabilities at a lexical level and make participation in the task difficult for small research teams.

The task required systems to produce a substitute word for a word in context. The data was collected for 201 words from open class parts-of-speech (i.e., nouns, verbs, adjectives and adverbs). Words were selected that have more than one meaning with at least one near synonym. Ten sentences for each word were extracted from the English Internet Corpus (Sharoff 2006 ). There were five annotators who annotated each target word as it occurred in the context of a sentence. The annotators were each allowed to provide up to three substitutes, though they could also provide a NIL response if they could not come up with a substitute and could specify if they thought the target was part of a name. They had to indicate if the target word was an integral part of a multiword. 3.2 Data For CLLS , we provided both trial and test sets but no training data. This was essential because we did not want to presuppose anything about the inventory of translations. As for LEXSUB , any systems requiring training data had to obtain it from other sources. This we believe provides a more realistic testing environment for systems since they must not assume the availability of any required resources.

We included nouns, verbs, adjectives and adverbs in both trial and test data. We deliberately used a subset of the same underlying data from the English Internet Corpus (Sharoff 2006 ) as had been used in LEXSUB . The reason for using the same annotations. 3 We used the same set of 30 trial words as in LEXSUB , and a subset of 100 words from the LEXSUB test set, selected so that they exhibit a wide variety of substitutes. For each word, the same sentences were used as in LEXSUB . 3.3 Annotation We used four annotators for the task, all native Spanish speakers from Mexico, with a high level of proficiency in English. As in LEXSUB , the annotators were allowed to use any resources they wanted to, and were allowed to provide multiple substitutes.
While in LEXSUB annotators were restricted to a maximum of three substitutes, we did not impose such a constraint in CLLS and encouraged the annotators to provide as many valid translations as they could think of. The guidelines 4 asked the annotators to take into account the context, and provide only the lemmatized form of a substitute. Similar to the guidelines used for the annotation of the LEXSUB data, the annotators were asked to identify cases where the target word itself would be part of a multiword, or the substitute would be a multiword. For the former scenario, the annotators were asked to provide the best replacements in Spanish they could think of.

The inter-tagger agreement (ITA) was calculated as pairwise agreement ( PA ) between sets of substitutes from annotators, exactly as calculated in LEXSUB (McCarthy and Navigli 2009 ).
The calculation is formally defined as follows. Let H be the set of annotators, I be the set of test items with two or more responses (not NIL or name) and h i be the set of responses for an item i 2 I for annotator h 2 H . Let P i be the set of all possible pairwise combinations of the sets of non-empty responses from any pair of annotators in H for item i . Let f h i ; h 0 i g2 P i be one combination of annotators responses for a pair of annotators h , h 0 . Pairwise agreement between annotators is calculated as:
Thus, pairwise agreement measures the average proportion of all the paired responses (in P i ) for which the two paired annotators gave the same response. This is analogous to ITA calculations in WSD except that in WSD pairwise agreement is calculated over a set of sense annotations for each item, and usually this consists of only one item whereas we are comparing multisets from an open set of translations.
The ITA was determined as 0.2777, which is comparable with the ITA of 0.2775 achieved for LEXSUB . Pairwise agreement is lower compared to typical WSD figures due to the fact that annotators are not selecting from a fixed inventory and there are many possible translations for a given word in a given context. Pairwise agreement between each pair of annotators is shown in Table 1 . Further statistics for each annotator are provided in Table 2 . This shows for each individual, and the full gold standard, the average number of substitutes per item, the variance of this, the number of items with no substitutes and the number of items with more than one translation. In the table we see that annotators 2 and 3 were more conservative and tended to provide more NIL responses and were less likely to provide multiple translations than annotators 1 and 4. As stated above, the annotators were encouraged to provide as many valid translations as possible. From manual inspection it seems that some translations provided were of lower quality 5 however the scoring metrics described below reduce the impact of this by using the frequency distribution over the translations from the annotators and also by using the mode (the most frequent translation).
 Figure 1 shows the CLLS interface used for the annotation process. 6
Table 3 provides an example of the translations provided by the annotators for the adjective straight in five sentences. The frequency of each translation is provided after the translation just as it appears in the gold standard. The translations clearly show relationships between the various meanings in terms of shared translations yet none of the sets of translations are exactly the same, highlighting the subtle differences in meaning. 3.4 Scoring We adopted the best and out-of-ten (oot in the equations below) precision and recall scores from LEXSUB . The systems were allowed to supply as many translations as they feel fit the context. The system translations are then given credit depending on the number of annotators that picked each translation. The credit is divided by the number of annotator responses for the item so that items with less variability receive a higher weighting. For the best score the credit for the system answers for an item is also divided by the number of answers the system provides to focus attention on the translations that the system feels are truly the best in this context. Systems should only supply more than one translation if they cannot determine which one is best. More formally, the calculation is as follows. If i is an item in the set of instances I , and T i is the multiset 7 of gold standard translations from the human annotators for i , and a system provides a set of answers S i for i , then the best score for item i is:
Precision is calculated by summing the scores for each item and dividing by the number of items that the system attempted whereas recall divides the sum of scores for each item by | I |. Both scores are multiplied by 100. Thus:
Note that while our ITA metric PA , provided in Eq. 1 above, is analogous to pairwise agreement in WSD and which compares each pairing of annotator responses, the metrics for system scoring are different since they take the frequency distri-bution of annotator substitutes into account. This is important since, unlike WSD , the systems are not selecting from a fixed inventory for a given lemma and there is considerable variation in the output provided.

The out-of-ten scorer allows up to ten system responses and does not divide the credit attributed to each answer by the number of system responses. This allows a system to be less cautious and for the fact that there is considerable variation on the task and there may be cases where systems select a perfectly good translation that the annotators had not thought of. By allowing up to ten translations in the out-of-ten task the systems can hedge their bets to find the translations that the annotators supplied.
We note that there was an issue that the LEXSUB out-of-ten scorer allowed duplicates (McCarthy and Navigli 2009 ). The effect of duplicates is that systems can increase its scores because the credit for each item is not divided by the number of substitutes and because the frequency of each annotator response is used. There is also the chance that system performance is lower because systems provide erro-neous duplicates which reduces the chance of finding genuine substitutes. McCarthy and Navigli ( 2009 ) describe this oversight, identify the systems that had included duplicates and explain the implications. For our task, we decided to continue to allow for duplicates, so that systems can boost their scores with duplicates on translations with higher probability. This was made clear to participants in the trial documentation.

For both the best and out-of-ten measures, we also report a mode score, which is calculated against the mode of the annotators X  responses as was done in LEXSUB . For best , the systems have to provide the mode as their first answer for that item and for out-of-ten , it has to be listed somewhere in the 10 answers (McCarthy and Navigli 2009 ). Unlike the LEXSUB task, we did not run a separate multiword subtask although annotators were asked to indicate where the target word was part of a multiword phrase. In our task, we did not use these annotations for further analyses though have retained the data for future research. From LEXSUB there was evidence that systems did less well on instances involving multiwords but this was evident more or less for all systems and the multiwords only accounted for a small portion of the data. 3.5 Baselines and upper bound To place results in perspective, several baselines as well as the upper bound were calculated. 3.5.1 Baselines We calculated two baselines, one dictionary-based and one dictionary and corpus-based. The baselines were produced with the help of an online Spanish-English dictionary 8 and the Spanish Wikipedia. For the first baseline, denoted by DICT, for each target word, we collected all the Spanish translations of that lemmatized word provided by the dictionary, in the order returned on the online query page. The best baseline was produced by taking the first translation provided by the online dictionary, while the out-of-ten baseline was produced by taking the first 10 translations provided.

The second baseline, DICTCORP, also accounted for the frequency of the translations within a large Spanish corpus (Spanish Wikipedia). All the translations provided by the online dictionary for a given target lemma were ranked according to their frequencies in the Spanish Wikipedia, producing the DICTCORP baseline. 3.5.2 Upper bound The results for the best task reflect the inherent variability in the gold standard. Less credit is given to items with multiple translations and so the top score achievable by an oracle (the theoretical upper bound) is therefore lower than 100, as in LEXSUB . The theoretical upper bound for the best recall (and precision if all items are attempted) score is calculated as:
Note of course that this upper bound is theoretical and assumes a human could find the most frequent substitute selected by all annotators. Performance of anno-tators is lower than the theoretical upper bound because of human variability on this task (see Sect. 5 below). Since we allow for duplicates, the out-of-ten upper bound assumes the most frequent word type in T i is selected for all ten answers. Thus we would obtain ten times the best upper bound (Eq. 8 ).
If we had not allowed duplicates then the out-of-ten upper bound would have been just less than 100 % (99.97). This is calculated by assuming the top 10 most frequent responses from the annotators are picked in every case. There are only a couple of cases where there are more than 10 translations from the annotators. 4 Systems A total of nine teams participated in the task, and several of them entered two systems. We provide an overview of these systems in Table 4 and give more detail here. The systems used various resources, including bilingual dictionaries, parallel corpora such as Europarl or corpora built from Wikipedia, monolingual corpora such as Web1T or newswire collections, and translation software such as Moses, GIZA or Google. Some systems attempted to select the substitutes on the English side, using a lexical substitution framework or word sense disambiguation, whereas some systems made the selection on the Spanish side, by translating the word and then contrasting alternatives on the Spanish side.
 In what follows, we briefly describe each participating system.

CU-SMT, contributed by Columbia University, relies on a phrase-based statistical machine translation system, trained on the Europarl English-Spanish parallel corpora and news corpora. The system uses the Moses translation system, with several parameters tuned for the task. It uses BLEU and other metrics to test the translation quality. The entire English sentences are translated into Spanish and word alignment between the input and output sentences are used to isolate the candidates for substitution. 9
The UvT-v and UvT-g systems make use of k-nearest neighbor classifiers to build one word experts for each target word, and select translations on the basis of a GIZA alignment of the Europarl parallel corpus. In detail, (van Gompel 2010 ) introduces two systems built using k -nearest neighbor classifiers that are constructed using several local and global features. The systems take a parallel corpus as input. In particular the clustering algorithm used is IB1, which is a variant of k -nearest neighbor. During the first phase, word-aligned parallel corpora are read and for each instance found corresponding to a target word, contextual features are picked for machine learning. The class is the Spanish translation found aligned to the target word. The total number of classifiers therefore is equal to the number of target words. In the last phase, the classifier outputs are parsed. The classifiers yield a distribution of classes for all test instances, which are converted to the appropriate formats for best and out-of-ten . Several classifiers are built choosing subsets of features, and then a voting system operates on the class outputs of the individual classifiers. The author proposes two distinct systems based on the way the features are selected.

Two other participating systems are UBA-T and UBA-W (Basile and Semeraro 2010 ), one based on an automatic translation system and the other based on a parallel corpus. Both systems use three dictionaries to collect synonyms, namely Google Dictionary, SpanishDict and Babylon, but differ in the way they rank the candidates. The first approach relies on the automatic translation of the context sentences in order to find the best substitutes, while the other uses a parallel corpus built on DBpedia 10 to discover the number of documents in which the target word is translated by one of the potential translation candidates. The authors combine three translation supplied by a particular dictionary and using Z-score to normalize the scores. If a particular translation occurs in more than one dictionary, only the occurrence with the maximum score is taken. Then the first system uses a series of steps and heuristics based on using the Google Translate engine from English to Spanish. The second system builds a parallel corpus from the English and Spanish Wikipedia extended abstracts using DBpedia and performs queries on the corpus to find the most suitable candidates.
 SWAT-E and SWAT-S use a lexical substitution framework applied to either English or Spanish. Wicentowski et al. ( 2010 ) present the above two systems in detail. SWAT-E finds English substitutes for the target words, and then translates the substitutes into Spanish. SWAT-S first translates the English sentences into Spanish, and then finds the Spanish substitutes. Both systems depend on syntagmatic coherence to find the relative probabilities of the potential candidates, using the English and Spanish versions of the Google Web1T n-grams, and several other resources. The authors also use several backoff methods to compensate for lack of coverage by a particular resource or heuristic. Their method performs especially well on the out-of-ten subtask. Their system makes good use of duplicates to give greater weight to translations the system has more confidence in.
TYO, contributed by University of Tokyo, uses an English monolingual lexical substitution module, and then it translates the substitution candidates into Spanish using the Freedict and the Google English-Spanish dictionaries. 11 The first module produces a list of substitution candidates and their corresponding probabilities, using WordNet, Penn Treebank, and the BLIP corpus. The translations are then performed by combining with another set of translation probabilities. 12
FCC-LS (Vilarin  X  o et al. 2010 ) uses the probability of a word to be translated into a candidate based on estimates obtained from the GIZA alignment of the Europarl corpus. These translations are subsequently filtered to include only those that appear in a translation of the target word using Google Translate. The approach is a Na X   X  ve-bayes classifier for the out-of-ten subtask. Using the parallel corpus, the probabilities of each target word associated with each translation are calculated.
WLVusp determines candidates using the best N translations of the test sentences obtained with the Moses system, which are further filtered using an English-Spanish dictionary. uspWLV uses candidates from an alignment of Europarl, which are then selected using various features and a classifier tuned on the trial data. Aziz and Specia ( 2010 ) present these two systems, which are very similar to standard WSD systems; however, the components of collecting synonyms and picking the best fit for context differ from a standard WSD system. The authors focus on maximizing the best metric score. The first system (WLVusp) is based on a statistical machine translation system trained on a parallel corpus to generate the best N translations for each target word, and a dictionary is used on top of this to filter out noisy translations as well as to provide additional translations in case the statistical machine translation system proves adequate. The second system, uspWLV, uses a dictionary built from a parallel corpus using inter-language mutual information (Raybaud et al. 2009 ), and supervised machine learning (MIRA) (Crammer et al. 2006 ), to rank the options from the dictionary. Some of the features involved are mutual information between the translations and the context words. One feature of uspWLV also exploits information from WLVusp, the statistical machine translation based system. Aziz and Specia point out that other dictionaries could be used and concede that a dictionary with implicit frequency information of the translations would possibly improve performance.

IRST-1, contributed by a research team from FBK-IRST, generates the best substitute using a PoS constrained alignment of Moses translations of the source sentences, with a back-off to a bilingual dictionary. For out-of-ten , dictionary translations are filtered using the LSA similarity between candidates and the sentence translation into Spanish. IRSTbs is intended as a baseline to the IRST-1 system, and it uses only the PoS constrained Moses translation for best , and the dictionary translations for out-of-ten . 13
ColEur and ColSlm use a supervised word sense disambiguation algorithm to distinguish between senses in the English source sentences (Guo and Diab 2010 ). Translations are then assigned by using GIZA alignments from a parallel corpus, collected for the word senses of interest. The systems utilize supervised WSD, using two distinct approaches X  X n the first one they utilize English-Spanish parallel corpora from Europarl, and in the second one they build their own parallel corpus from a set of different corpora, in order to make the system less domain-specific. They use a word-sense translation table and automatic word alignment over their WSD system to generate the most suitable substitution candidates.

Table 4 summarizes the different systems that took part in the cross lingual task, and the approach and resources they used and how they ranked amongst all the participants. The detailed results are provided in the following section. 5 Results We show the original results here for best (Table 5 ) and out-of-ten (Table 6 ). These results, as in Mihalcea et al. ( 2010 ), use the official scoring as described above in Sect. 4 The rows are ordered by recall ( R ). Since out-of-ten scores can be increased by providing duplicates (McCarthy and Navigli 2009 ; Mihalcea et al. 2010 )we indicate the number of items for which there were duplicates in the dups column 14 to show the extent that the systems took advantage of the facility for weighting their substitutes. 15 Duplicates help when a system has more confidence in some of the translations and wishes to weight them accordingly. In the Tables 5 and 6 we indicate the best scoring system in bold in each respective column, and we underline all systems that are not significantly different 16 to the best performing system, again in each respective column. We also indicate for all systems scoring above the baseline (DICT in every case), whether the difference is significant at the 0.05 level ( ) or 0.01 level ( ). Both the baselines (DICT and DICTCORP) are indicated with .
 We note that some systems did better on out-of-ten , and others better on best . UBA-W and UBA-T is one such example. While UBA-T is better at finding the best translation, UBA-W is better at hedging its bets, this can be seen by the larger out-of-ten scores and due to the fact that while it does not find the mode in best , it does do a better job of finding the mode somewhere in the top 10 compared to UBA-T. While all the best performing systems on best are near the top in all four columns of Table 5 , for the out-of-ten results in Table 6 the mode scores demonstrate that a system that makes good use of duplicates (as SWAT-E and SWAT-S do) may not perform as well on the mode task as they have less chance of finding the mode in the top 10 due to having fewer than 10 substitute types. 17
Comparing systems, it seems that using dictionaries tend to give better results compared to using parallel corpora in isolation, although we note that WLVusp using a dictionary coupled with machine translation software did less well than the other system from the same team (uspWLV) which used a dictionary automatically constructed from parallel corpora. Below, and in the following section, we present further analyses to show that the systems are heterogeneous with different strengths and weaknesses. These analyses provide rationale for a system that combines the output from individual systems. We present such a system based on voting in Sect. 3 and the interested reader can skip ahead to the results in Table 12 that can be compared to the results in Tables 5 and 6 .

Pairwise annotator agreement was calculated above in Sect. 3 and measures the average proportion of all the paired substitute sets for which the two paired annotators gave the same substitute. In Table 7 we show the results each annotator would have obtained using the best scorer considering only the gold standard from the other annotators and over exactly the same set of items. This is done using the best scoring since annotators provided several translations for each item, but not ten. While these results are not strictly the same as the systems, since there is necessarily one less annotator in the gold standard for each item (sentence) and because the annotators were encouraged to provide multiple translations, they do provide more of an idea of the difficulty of the task and how well the majority of the systems are actually performing. Indeed, on the recall and precision tasks the best annotator is outperformed by the best system. This is explained by the fact that annotators tended to provide more than one answer for each item and this lowers the recall and precision scores, which are divided by the number of answers. The Mode Precision ( Mode P ) and Recall ( Mode R ) scores give us a better idea of how the annotators compare to systems when finding the best translation though there are three systems which fall within the range of the human annotators. Note that annotators 2 and 3 have low recall scores due to the higher numbers of items with NIL responses as shown in Table 3 above. With regards to the average scores, since annotators tend to provide multiple translations and many NIL responses, only the Mode P is higher than any of the systems.

In the remainder of this section, we provide analysis of the results to help establish the merits of the different approaches. We investigate how systems compare on different parts of speech. We focus on precision and recall metrics since these cover a larger portion of the data than the mode metrics. The results from the official scorer allow different credit to each item because the number of translations provided by the annotators is used in the denominator to provide more credit to items where annotators each provide the same translations and are in agreement. To facilitate a comparison across PoS we normalize the credit for each item by the upper bound for that item which is calculated as above in Eq. 8 . This is important for analyzing performance by PoS as different classes have different upper bounds and we wish to control for this in our analyses. When we normalize the credit it is easier to see where the systems reach the upper bound (1 for normalized scores). There is at least one instance for each system where this occurs. Typically this happens for lemmas such as informal and investigator where there is a translation that is much more popular than any other (i.e., more of the annotators choose it), informal and investigador for these lemmas, and where there is rarely ambiguity as can be seen by the fact that the translation is predominant over the ten sentences for that lemma.

Table 8 displays the results for each system where we analyse by the best scores by PoS, giving the precision and recall for each PoS, that is, dividing by the number of items either attempted or total for that PoS, using scores normalized by the upper bound for each item. There were a total of 110 adverb instances, 310 verb instances and 280 and 300 for adjectives and nouns respectively. The rows are ordered by recall. From these results we observe that the results were best for adjectives, then nouns, then verbs and finally adverbs. We also note that system performance varied depending on PoS. UBA-T performs exceptionally well on nouns (12 percentage points above the second best). The baseline DICT outperforms the version which includes Wikipedia data (DICTCORP) in every PoS, and for adverbs these two baselines take the first and last ranks respectively, emphasizing the importance of dictionaries over corpus data for adverbs. We note also that the system TYO does better on adverbs than other PoS.
 Table 9 displays the same analysis as Table 8 but for the out-of-ten scores. These are again normalized by the theoretical upper bound for each item, i.e. the maximum score possible given the gold standard and scoring. As the theoretical upper bound allows for duplicates the scores of the systems are lower than for best as typically they hedged their bets rather than provide duplicates. It is interesting to note the same pattern emerged over PoS for out-of-ten : overall adjectives did better than nouns, which did better than verbs while again adverbs produced the lowest results. Again TYO was one system that did better on adverbs than other PoS. It was interesting to note that there were a few teams with several systems (UvT-v and UvT-g; ColSlm and ColEur) where on best the order between the two systems remains the same across PoS, but for out-of-ten the rank performance order of the two systems varies depending on the PoS.

In addition to the PoS analysis, we attempted to look for other criteria which might distinguish approaches. One of the things we examined was length of the context provided (the sentence length). We examined the Spearman X  X  correlation between sentence length and the scores achieved by each system on that instance. Sentence length was only significantly correlated for 4 systems and 1 baseline: UvT-g, ColEur, IRST-1, CU-SMT and DICTCORP. For these the correlation ranges between 0.12 to 0.20 which though significant is quite a weak correlation. We did a similar analysis for the out-of-ten scores and found a similar pattern. 6 Further analyses In this section we present further analyses to demonstrate the similarities and differences between systems in terms of their performance and considering the methodologies employed by the systems as described above in Sect. 4 We then present a disruptive set analysis which contrasts the performance of different systems, or methodologies, on the same set of data points. In our analysis we use the test lemmas as data points but the same analysis could be conducted on individual instances. Finally in this section we provide the results obtained by combining the output from the systems to see the extent that they can collectively improve performance.
 6.1 System correlations Table 10 displays a correlation matrix 18 of the Spearman X  X  correlations between the ranks of the normalized scores per item for each pair of systems. Spearman X  X  q avoids assumptions about the parameters of the score distributions by using ranks. The correlation analysis demonstrates that similar methodologies tend to show similar performance on the same test items and tend to have larger correlation coefficients. 19 For example systems using statistical machine translation such as UBA-T and CU-SMT have higher coefficients, DICT and SWAT-E make heavy use of lexical resources and the systems uspWLV, WLVusp, ColSlm, UvT-v, UvT-g, ColEur all make use of parallel corpora though note that others, such as IRSTbs and IRST-1, also use parallel corpora but with a different approach. We find that the systems TYO, DICTCORP, SWAT-S and UBA-W seem to be outliers with Spearman X  X  q less than 0.4 when correlated with any other system. Systems built by the same team using similar approaches, e.g., IRSTbs and IRST-1, UvT-v and UvT-g, uspWLV and WLVusp all tend to be strongly correlated which is what we would expect given that the participants used much of the same technology for their two systems. 6.2 Disruptive sets We also performed an analysis using the concept of disruptive sets (Zaragoza et al. 2010 ), which provides a graphical way of comparing any two systems. The approach was originally proposed for comparing the relative performance of two search engines, and we adopt it to graphically depict how any two systems compare to one another at the task of solving a number of of  X  X ueries X , where a query (or data-point) for our task is a set of test items. Solving is a measure of performance on the task and can be defined empirically or by using intuition about an acceptable level of success. The disruptive set of a system is defined as the set of queries (data-points) that the particular system in question can handle better than the other one. We use this disruptive sets analysis to provide scatter plots with partitions determined by thresholds to show the relative performance of two systems (I and II) 20 and the extent that the systems complement one another.

For the analysis we need to determine the data points for plotting. The most straightforward options for our task are instances (1000), lemmas (100), or parts of speech (4), though some other categorization of system output would also be possible. In this analysis we used lemmas as this gives a reasonable level of granularity and we can characterize the data-points by the target lemma which would allow for predictions for unseen data as to which type of system is better for that lemma. Furthermore, there are various features of lemmas (PoS, frequency, level of abstractness) that we can use to examine the lemmas in the various partitions which may also be useful for making generalizations. In the analysis, the two axes represent any two individual systems, or the average of two mutually exclusive combinations of systems (see below).

We use a relevance metric M (the normalized best recall score) on each data-point instance (lemma) given the system X  X  output for that lemma and we determine whether each instance is above a threshold d solved or below another threshold d hard . The intersection of the solved sets of data-points from the two systems gives us a set of instances that both systems can solve ( two-system-solved ), and the intersection of the hard sets gives us a set of instances that both systems cannot solve ( two-system-hard ). What makes the disruptive sets analysis useful is that, in addition to partitioning the data-points as to whether they are solved by a particular system or not, they provide an indication of which system would be better equipped to handle particular data-points compared with the other system. That is, it provides some indication of the relative merits of the two systems for less clear cut cases. A tied region is introduced in the area not in two-system-solved or two-system-hard but where the absolute difference between the scores of the two systems for certain instances is less than a third threshold, d tied . These are instances which could be handled by either system. The remaining two regions are disruptive I and disruptive-II . Disruptive I -covers the region where those data-points (lemmas) are handled to some extent by the first system and not the second system. In this region, system I outperforms system II (the absolute difference in the scores is greater than d tied ) and the scores obtained by system I in this region are greater than the d hard for system I and less than the d solved for system II. Analogously, we have the set disruptive-II . These five sets ( two-system-solved, two-system-hard, disruptive I, disruptive-II and tied ) will depend on which systems we choose for the axes. Given that we use normalized recall scores per lemma between 0 and 10, we set the thresholds as follows. We set d solved at 6, d hard at 3, and d tied at 2. We felt these values provided appropriate partitions from manual inspection of the plots. The plots provide a depiction of the performance of the systems relative to one another. The threshold choices simply determine the partitions in the plots and the exact values of thresholds do not alter the data points but simply move these partitions.

We present here several figures that compare different systems, or groups of systems. In these figures, the per lemma performance of the first system (labelled system I) is shown by the x-axis and that of the system(s) under comparison (the second system) is shown on the y-axis (labelled system II). First we contrast the top ranking system on the best recall (UBA-T) with the second ranking system (uspWLV) in Fig. 2 . We see that while both systems have many items that they both solve, they each have a similar number of data points that they solve exclusively. We examined the exact set of lemmas in each partition to see if there was a pattern but we have not found one as yet. We examined the underlying data to determine which lemmas are universally hard and which universally solved by all the systems, and found that draw.v is universally hard while special.a is universally solved.
In addition to using disruptive sets for contrasting individual systems, we have also used the approach to help determine if there are particular advantages to particular types of approaches by grouping systems with respect to these approaches. To this end we have used the average score for each data point on the plot. We examined the average of the following two sets:  X  Machine learning approaches versus those not using machine learning (where  X  Systems that do explicitly use dictionaries compared with those that do not
Note that it is not always easy to make these partitions based on system descriptions as some of the components may rely on other resources that are not explicitly mentioned. In particular we did attempt to divide systems based on their use of statistical machine translation technology as from initial exploration it seemed that there was an advantage in doing so. However, this became problematic since nearly all systems use approaches and resources from statistical machine translation to some extent. For example, uspWLV uses features from WLVusp, which uses statistical machine translation explicitly.

We also compared systems using parallel corpora with those that did not. The plot did not show any major advantage on either side so we do not include it in the article, however from manual scrutiny of the data points in the various partitions of the plot, it did seem that methods that do not use parallel corpora solve some of the concrete words, e.g. bug.n, girl.n, shade.n, pot.n while those that use parallel corpora seem to solve more abstract words. Possibly this is because the nuances of abstract words are better handled by data whereas concrete words are easier for lexicographers to define and translate. In our analysis of the various partitions in the disruptive set analysis for the various classifications, we did not find a pattern regarding PoS or frequency of the lemmas.

Aside from disruptive sets , we also examined the merits of individual systems by looking to see which perform well, given our disruptive set threshold d solved ,on lemmas which either none or only a few other systems also solve. We present these results in Table 11 , giving also the number of unique translation types from the gold standard for each lemma. Interestingly, the number of unique translations does not necessarily reflect the difficulty of a lemma. We compared the difference between the number of translations for the difficult lemmas shown in Table 11 and the number of translations for all other lemmas and found that the average number of translations was 20.1 and 22.7 respectively, so in fact there were less translations for the more difficult lemmas and this difference was not significant according to a one-tailed Wilcoxon X  X ann X  X hitney test ( p = 0.125). We note that while UBA-T is certainly a strong system, there are also lemmas that are better handled by other approaches. 6.3 Combining systems Given the diversity of the systems participating in the task, an intuitive next step is to combine them into a meta-system that takes the output from all the systems and suggests possible lexical substitutes through some form of voting.

We implemented a simple voting mechanism, which compiles the output from the systems being considered, as well as the two baselines, and it adds up all the  X  X  X redit X  X  received by each candidate translation. It then ranks the translations in reverse order of their credit and outputs the top N translations. Here, N is set to either 1 or 10, corresponding to the best and out-of-ten evaluations respectively.
Formally, assuming K is the set of participating systems, and C is the union of candidate translations suggested by all the systems S k , we calculate the credit for a candidate translation c 2 C as: where S i k is the set of answers submitted by system S k for item i . Note that an overall credit is calculated for each candidate translation, and not for test items (sentences).
To determine the credit assigned by a system to each of the output translations for an item, we assume a credit of 1 for each item, which is divided among all the translations suggested for that item. For instance, if  X  X  X ordo;lado X  X  are two possible translations for one of the contexts of the target word  X  X  X ide, X  X  both  X  X  X ordo X  X  and  X  X  X ado X  X  will receive a credit of 0.5.

Note that in the case of out-of-ten this way of dividing credit among the translations of an item has the effect of  X  X  X illing in X  X  the translations up to the ten required translations for a complete answer. For example, the translation  X  X  X ordo X  X  will receive the same total credit whether it is listed once by itself in an out-of-ten system answer, or if it is repeated ten times in the system answer.

Whenever there are ties between the credits received by the top translations, all translations with the top score are provided. That is, if both  X  X  X ado X  X  and  X  X  X ordo X  X  end up getting the same credit from the combination of systems, both are provided.
Table 12 shows the results for the best and out-of-ten system combinations. For best , all the best system outputs are combined and the top one translation(s) is selected. For out-of-ten all the out-of-ten system outputs are combined, and the top ten translations are selected.

Comparing these results with the top scoring individual system for each of those metrics from Tables 5 and 6 , (shown in Table 12 in brackets) the combination of systems brings improvements in all measures except the basic precision and recall for the out-of-ten combination, which is explained by the fact that our combination method did not allow for duplicates.

The increase in normal recall and precision are higher but not significantly better than the best performing system for that metric, however the mode precision and recall are significantly better ( p &lt; 0.01) for both best and out-of-ten . The improvements obtained by the combined system suggest that the systems are heterogeneous which is in line with our analyses described above. 7 Conclusions and future work In this paper we have provided further results and analysis of the CLLS task with particular regard to the system approaches and performance. We have conducted this analysis by looking beyond the task results at normalized scores by PoS, system correlations by instance, and a disruptive set analysis where we contrast approaches of individual systems or sets of systems. We demonstrated that while there are better performing methods, the systems are heterogeneous as can be seen by the fact that different approaches work well on different lemmas. We demonstrate that these strengths can be brought together in a simple combination system which uses voting over the system outputs and significantly improves the results when finding the most frequent translation provided by the annotators.

There are several directions for future work. There is potential for extending the task to multiple languages or increasing the number of test cases for wider coverage. It would be possible to design a system, based on features of the best performing systems, to implement a fast, on-line analysis system that generates good Spanish substitutes for select English words in any free-form running text. This could provide various useful educational applications. For example, a user with a low level of expertise in a language could read a relatively difficult piece of text in that language and glean information from it using in-context translations provided on the fly for words in the text which the user had problems with. Such an online learning tool would be more useful than a translator, as the user would only seek translations where necessary, and better than a dictionary, as the translations would be matched to the context. There is a growing interest in such online learning tools. Duolingo 22 for example allows users to learn a language (Spanish, German, English and French at the time of writing) while translating language data on the web using the data from these language learners. The system helps users by finding other instances of words that the user does not know. The system we propose allows users to request translations for a word in context. This system could of course be an additional tool for language learners when translating language using a system such as Duolingo.
The approaches used by different systems can be used across various languages for generating similar words in different languages. If there is a way to assign those sets of words to a sense in a sense inventory, we could exploit these technique for automatically building a multilingual WordNet.
 References
