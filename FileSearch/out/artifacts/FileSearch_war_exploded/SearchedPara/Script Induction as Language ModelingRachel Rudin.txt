 Although the concept of scripts in artificial intelli-gence dates back to the 1970s (Schank and Abel-son, 1977), interest in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, Chambers and Jurafsky (2008), treats the problem of script induction as one of learning narrative chains , which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and ex-tend Chambers and Jurafsky (2008) X  X  methods for learning narrative chains, each using the narrative
In this paper, we take the position that the nar-rative cloze test, which has been treated predom-narrative event is removed; the task is to predict the missing event.
 Data Each of the models discussed in the fol-lowing section are trained and tested on chains of narrative events extracted from stories in the New York Times portion of the Gigaword corpus (Graff et al., 2003) with Concrete annotations (Ferraro et al., 2014). Training is on the entirety of the 1994 X  2006 portion (16,688,422 chains with 58,515,838 narrative events); development is a subset of the 2007 X 2008 portion (10,000 chains with 35,109 events); and test is a subset of the 2009 X 2010 por-tion (5,000 chains with 17,836 events). All ex-tracted chains are of length two or greater. Chain Extraction To extract chains of narra-tive events for training and testing, we rely on the (automatically-generated) coreference chains present in Concretely Annotated Gigaword. Each narrative event in an extracted chain is derived from a single mention in the corresponding coref-erence chain, i.e., it consists of the verb and syn-tactic dependency ( nsubj or dobj ) that governs the head of the mention, if such a dependency ex-ists. Overlapping mentions within a coreference chain are collapsed to a single mention to avoid redundant extractions. In this section we present each of the models we train for the narrative cloze evaluation. In a sin-gle narrative cloze test, a sequence of narrative events, ( e 1 ,  X  X  X  , e L ) , with an insertion point, k , for the missing event is provided. Given a fixed vocabulary of narrative events, V , a candidate se-quence is generated for each vocabulary item by inserting that item into the sequence at index k . Each model generates a score for the candidate se-quences, yielding a ranking over the vocabulary items. The rank assigned to the actual missing vo-cabulary item is the score the model receives on that cloze test. In this case, we set V to include all narrative events, e , that occur at least ten times in training, yielding a vocabulary size of 12,452. All out-of-vocabulary events are converted to (and scored as) the symbol UNK . 3.1 Count-based Methods Unigram Baseline ( UNI ) A simple but strong baseline introduced by Pichotta and Mooney (2014) for this task is the unigram model: can-Skip N-gram We tune the previous three models ( UOP , OP , and BG ) with the skip n-gram counting methods introduced by Jans et al. (2012) for this task, varying the ways in which the counts, C ( e 1 , e 2 ) , are collected. Using skip-n counting, C ( e 1 , e 2 ) is incremented every time e 1 and e 2 co-occur within a window of size n . We experiment with skip-0 (consecutive events only), skip-3 (window size 3), and skip-all (entire chain length) settings.
 For each of the four narrative cloze scoring metrics we report on (average rank, mean re-ciprocal rank, recall at 10, and recall at 50), we tune the Unordered PMI, Ordered PMI, and Bigram Probability models over the following parameter space: { skip-0 , skip-3 , skip-all }  X  { discount , no-discount }  X  { T=4 , T=10 , T=20 } , where T is a pairwise count threshold. 3.2 A Discriminative Method Log-Bilinear Language Model ( LBL ) The Log-Bilinear language model is a language model that was introduced by Mnih and Hinton (2007). Like other language models, the LBL produces a probability distribution over the next possible word given a sequence of N previously observed words. N is a hyper-parameter that determines the size of the context used for computing the prob-abilities. While many variants of the LBL have been proposed since its introduction, we use the quence E 0 made by prepending N BOS tokens and appending a single EOS token to E .

The LBL models are trained by minimizing the objective described in Equation 5 for all the sequences in the training corpus. We used the OxLM toolkit (Paul et al., 2014) which internally uses Noise-Contrastive Estimation (NCE) (Gut-mann and Hyv  X  arinen, 2010) and processor paral-lelization for speeding up the training. For this task, we train LBL models with N = 2 ( LBL 2) and N = 4 ( LBL 4). In our experiments, increas-ing context size to N = 6 did not significantly improve (or degrade) performance. Table 1 shows the results of 17,836 narrative cloze tests (derived from 5,000 held-out test chains), with results bucketed by chain length. Perfor-mance is reported on four metrics: average rank, mean reciprocal rank, recall at 10, and recall at 50.
For each of the four metrics, the best overall performance is achieved by one of the two LBL models (context size 2 or 4); the LBL models also achieve the best performance on every chain length. Not only are the gains achieved by the discriminative LBL consistent across metrics and ney and Pantel, 2010), so it is natural that these techniques have also been applied to the task of script induction. Qualitatively, PMI often identifies intuitively compelling matches; among the top 15 events to share a high PMI with ( eat, nsubj ) under the Unordered PMI model, for example, we find events such as ( overeat, nsubj ) , ( taste, nsubj ) , ( smell, nsubj ) , ( cook, nsubj ) , and ( serve, dobj ) . When evaluated by the narra-tive cloze test, however, these count-based meth-ods are overshadowed by the performance of a general-purpose discriminative language model. Our decision to attempt this task with the Log-Bilinear model was motivated by the simple ob-servation that the narrative cloze test is, in reality, a language modeling task. Does the LBL X  X  suc-cess on this task mean that work in script induc-tion should abandon traditional count-based meth-ods for discriminative language modeling tech-niques? Or does it mean that an alternative eval-uation metric is required to measure script knowl-edge? While we believe our results are sufficient to conclude that one of these alternatives is the case, we leave the task of determining which to future research.
 Acknowledgments This work was supported by the Paul Allen In-stitute for Artificial Intelligence ( Acquisition and Use of Paraphrases in a Knowledge-Rich Setting ), a National Science Foundation Graduate Research Fellowship (Grant No. DGE-1232825), the Johns Hopkins HLTCOE, and DARPA DEFT (FA8750-13-2-001, Large Scale Paraphrasing for Natural Language Understanding ). We would also like to thank three anonymous reviewers for their feed-back. Any opinions expressed in this work are those of the authors.

