 Recommender systems usually need to compare a large num-ber of items before users X  most preferred ones can be found. This process can be very costly if recommendations are fre-quently made on large scale datasets. In this paper, a novel hashing algorithm, named Preference Preserving Hashing (PPH), is proposed to speed up recommendation. Hash-ing has been widely utilized in large scale similarity search (e.g. similar image search), and the search speed with bi-nary hashing code is significantly faster than that with real-valued features. However, one challenge of applying hashing to recommendation is that, recommendation concerns users X  preferences over items rather than their similarities. To ad-dress this challenge, PPH contains two novel components that work with the popular matrix factorization (MF) algo-rithm. In MF, users X  preferences over items are calculated as the inner product between the learned real-valued user/item features. The first component of PPH constrains the learn-ing process, so that users X  preferences can be well approx-imated by user-item similarities. The second component, which is a novel quantization algorithm, generates the binary hashing code from the learned real-valued user/item fea-tures. Finally, recommendation can be achieved efficiently via fast hashing code search. Experiments on three real world datasets show that the recommendation speed of the proposed PPH algorithm can be hundreds of times faster than original MF with real-valued features, and the rec-ommendation accuracy is significantly better than previous work of hashing for recommendation.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Preference, Hashing, Recommendation, Efficiency
Recommendation has attracted intensive attention from both academia and industry [17, 1], and great progress has been made recently [13, 14]. However, it is still challenging to design an efficient recommender system that scales well with large datasets [33, 4].

In this paper, we focus on the popular matrix factoriza-tion (MF) algorithm [14], and propose new research for effi-ciency in recommendation. The recommendation process of MF can be divided into two parts: (1) real-valued features are learned for each user and item, which we call prefer-ence modeling ; (2) all candidate items are compared and ranked so that the top ones are returned as recommenda-tion, which we call preference ranking . For the majority of MF-like algorithms [13, 14, 17, 22], the cost of preference modeling for a specific user is O ( m ), where m is the number of rated items. In comparison, the cost of preference ranking is usually O ( M ), where M is the item number in the entire dataset. In general case, there is M &gt;&gt; m .

Although the efficiency of preference modeling has been extensively studied [5, 33], limited research exists for effi-ciency of preference ranking. Yet the complexity of O ( M ) can become a critical issue in practice. For instance, for large scale datasets like Netflix [1], it can take hours to do prefer-ence ranking for all users based on the pre-trained preference model of MF (i.e. the real-valued user and item features). The situation will be much worse if we consider that even the 100-million-rating Netflix dataset is only a very tiny part (less than 1%) of the real world one. Furthermore, many rec-ommender systems [36, 3, 25, 38] update users X  preference models frequently, either explicitly requested by users them-selves or implicitly updated by their item-rating behaviors. A good recommender system shall make recommendations that meet users X  latest preference models. In this case, pref-erence ranking will be conducted frequently. Although some-times engineering strategies may help, e.g. parallel comput-ing, the overall computational burden remains unreduced. Therefore new research is needed to reduce the cost of pref-erence ranking.

Based on the above consideration, in this paper we study the efficiency issue of preference ranking, and propose a novel hashing algorithm as solution. Hashing [6, 24, 31] has now become a very popular technique for large scale simi-larity search. By converting real-valued data features into binary hashing codes, hashing search can be very fast. In the best case [24, 8], the cost of finding similar data is indepen-dent of the dataset size. Such a nice property makes hash-ing outperform many other fast search techniques (e.g. kd-tree) [8]. Hence it is promising to consider utilizing hashing to speed up the preference ranking. In previous work [37], Zhou et al. directly applied traditional hashing methods for similarity search, and significant speedup (e.g. 100 times) was reported. We refer to their method as Zhou X  X  method. However, an overlooked fact is that preference ranking in recommendation is not equivalent to similarity search in traditional hashing. For MF algorithm, users X  preferences over items are calculated as the inner product between user and item features. Inner product between two vectors is fundamentally different from their similarity (see Sec. 3). Therefore, Zhou X  X  method [37] suffers great accuracy loss (see Sec. 5), despite the significant speedup.

The proposed hashing algorithm, which we name Prefer-ence Preserving Hashing (PPH), provides a new perspective of applying hashing to recommendation. Like traditional hashing, PPH also converts real-valued user and item fea-tures into binary hashing codes. Yet differently, the hash-ing code in PPH is designed to preserve users X  preferences over items (ranking order of inner product), rather than their similarities. In short, our PPH is ranking oriented while traditional hashing is similarity oriented. PPH con-sists of two novel components, the Constant Feature Norm (CFN) constraint and Magnitude and Phase Quantization (MPQ), which will be elaborated in Sec. 4. Compared with Zhou X  X  method, the proposed PPH algorithm achieves sim-ilar speedup, yet the recommendation accuracy is signifi-cantly higher. Extensive experiments are conducted on three real world datasets, i.e. MovieLens-1M, MovieLens-10M and the 100M Netflix dataset, and the results clearly demon-strate the advantage of the proposed PPH algorithm.
The rest of the paper is organized as follows. Sec. 2 re-views previous works related with traditional hashing and recommendation efficiency. Sec. 3 discusses why our PPH is not traditional hashing. The algorithm details are given in Sec. 4, and extensive experimental results are presented in Sec. 5. We conclude the paper in Sec. 6 and point out some future work directions.
We call previous hashing works that preserve data simi-larity traditional hashing , which have been widely utilized in applications such as similar images [27, 31, 19] and similar documents [34, 29] search. Generally speaking, traditional hashing consists of two steps: hashing code construction, and fast search in hashing code space.

In hashing code construction, the underlying principle is that similar data should have similar hashing codes. Di-rect hashing code derivation is NP-hard [31]. Instead, most hashing algorithms first derive real-valued hashing features according to certain criteria, then use quantization methods to get binary hashing codes. Some hashing criteria are data-independent, which means the derived hashing features do not rely on specific data distribution. For example, the fa-mous Locality Sensitive Hashing (LSH) [6] and its variants (e.g. [15]) derive hashing features via random projection. Other criteria are data-dependent, which means the deriva-tion are learned based on certain datasets. Typical algo-rithms of this kind include Iterative Quantization (ITQ) [7], Kernel Supervised Hashing (KSH) [19], Self-Taught Hashing (STH) [35], Spectral Hashing (SpH) [31], etc.

The process of quantization is quite standard. Usually the real-valued hashing features are binarized w.r.t certain thresholds. Most methods adopt one single threshold for each feature dimension, which is usually set as feature mean or median value [35]. If the data have been normalized to have zero mean, then zero will be the threshold [31]. This is known as entropy maximization principle [27, 35], be-cause such quantization makes the variance of each dimen-sion maximized so that more information can be recorded. Some other works, like Manhattan hashing [12], learn mul-tiple thresholds for each dimension.

The components of PPH, namely CFN and MPQ, cor-respond to the above two steps accordingly, yet with dis-tinct novelty. First, the CFN constraint forms the criteria of deriving real-valued hashing features, which is actually the derivation of user and item features (preference model). In particular, CFN approximates preference via similarity, while traditional hashing criteria listed above only consider data similarity. Second, the MPQ algorithm explicitly mod-els preference via hashing code, while the above quantization algorithms in traditional hashing are designed to preserve data similarity (via entropy maximization). We will present the details in Sec. 4.

After hashing codes are obtained, searching nearest neigh-bors is extremely fast. Hamming ranking (e.g. [19]) and hashing lookup [24] are two popular methods. In hamming ranking, data points are sorted by their hamming distances with the query. Although it has linear time complexity w.r.t dataset size, practically it is very fast due to the fast oper-ation of hashing code. In hashing lookup, similar data are searched within a r -radius hamming ball centered at the query hashing code. The time complexity is P r i =0 D i with D being the code length, which is constant w.r.t the dataset size. Moreover, we identify a recently proposed method, Multi-Index Hashing (MIH) [21], is more appropriate for the task of efficient recommendation. MIH can be viewed as a generalized hashing lookup method with very nice proper-ties. Further analysis and comparisons of the above search methods are given in Sec. 4.3.
As analyzed in Sec. 1, the efficiency of recommendation consists of the efficiency of preference modeling and the ef-ficiency of preference ranking. The former has been exten-sively studied [33, 5], among which stochastic optimization and parallel computing are most popular.

This paper focuses on the efficiency of preference rank-ing, which, although critical, attracts much less attention. We categorize previous works as non-hashing methods and hashing methods. We first review non-hashing methods. In [17], Linden et al. discussed the idea of item-space parti-tioning, which makes recommendation from some subsets of all items. They concluded such a naive strategy would produce recommendations of low quality. User clustering was adopted in [4, 11, 22] so that similar users share the same recommendation results. This strategy essentially re-duces the user-space, which may potentially degrade the performance of personalized recommendation. Special data structures, such as kd-tree or variants (e.g. [18, 11]), could also be solutions. However, Koenigstein et al. [11] showed that the speedup using pure tree structure is rather limited, and Grauman et al. [8] showed hashing is much more supe-rior than trees in many real world applications, particularly those that handle high-dimensional data.

Another potential disadvantage of the above algorithms is that they all need to learn a speedup model (e.g. clus-ters, trees) based on some training dataset of user and item features. Yet as analyzed in Sec. 1, if users X  preferences are frequently updated, the learned speedup model will soon be-come outdated. Extra computational burdens are imposed to learn new speedup models, which inevitably degrade the efficiency of preference ranking. In comparison, our PPH does not suffer such a problem, as will be analyzed in Sec. 4.4 and Sec. 5.3.1.

When hashing meets recommendation, it provides a new perspective that is fundamentally different from the non-hashing methods. Yet only very limited prior works exist. CF-Budget [10] might be the first work that designed binary hashing code for collaborative filtering. Yet they mainly fo-cused on reducing storage cost, and did not discuss recom-mendation speedup. Zhou et al. [37] applied hashing tech-nique for efficient recommendation, and the final speedup is substantial (e.g. 100 times was reported). However, as analyzed above, [37] did not realize the difference between preference and similarity. Therefore, they incur much larger accuracy loss than our PPH algorithm.
Assume from MF algorithm M , we derive a set of user features U = { u n  X  R D | n = 1 : N } and a set of item features V = { v m  X  R D | m = 1 : M } , where D is the feature dimension. From now on, we use notation u,v for real-valued features, while  X  u,  X  v  X  { X  1 } D 0 for binary hashing code of length D 0 .

In MF, users X  preferences over items are calculated as in-ner product u T v . Therefore, the preference ranking can be expressed as, u : v 1 v 2 ... v M  X  X  X  u T v 1 &gt; u T v 2 &gt; ... &gt; u where u : v i v j means user u prefers v i to v j . Then the inner product of u T v i should be larger than u T v j .
The goal of PPH is to find a hashing method H to con-vert U , V into binary codes  X  U ,  X  V  X  { X  1 } D 0 , so that (1) the preference order in Eq. 1 can be preserved, and (2) the ad-vantage of fast hashing code operation can be utilized. Fur-ther notice, the inner product of two hashing codes  X  u equivalent to their hamming distance Ham (  X  u,  X  v ) 1 :
Then our goal is to find H so that we have We give the following definition to summarize Eq. 3:
Definition 1. Function f ( u,v i ) is called ranking consis-tent w.r.t some hashing function H , if the descending rank-ing order of f ( u,v i ) is the same as the ascending ranking order of Ham ( H ( u ) , H ( v i )) . Namely, f ( u,v 1 and only if Ham ( H ( u ) , H ( v 1 )) &lt; Ham ( H ( u ) , H ( v use f  X  X  to represent the ranking consistency.

With this definition, the goal of PPH is to find proper H so that function f ( u,v ) = u T v is ranking consistent w.r.t H as much as possible.
Eq. 3 of our PPH algorithm is much more challenging than it appears, and directly applying traditional hashing algorithms will not work well. To see why PPH is not tra-ditional hashing, there are two main arguments. (1) PPH and traditional hashing focus on different prob-lems.

To see this, recall that the goal of traditional hashing is to preserve data similarity. Similar data should be assigned similar hashing codes; therefore, large similarity indicates small hamming distance. If we use the same notation as in Eq. 3, and let sim ( u,v ) be a valid similarity function over u,v , the goal of traditional hashing shall be expressed as
However, Eq. 4 is different from Eq. 3, since the following claim.
 Claim 1. Inner product is not a valid similarity metric.
Proof. For the sake of clarity, here we only present the most evident proof. According to [2, 16], valid similarity metric shall follow self similarity rule, i.e.
 That means, no other v is more similar with u than u itself. However,  X  v,u T u  X  u T v doesn X  X  hold in general. Therefore, inner product is not a valid similarity metric.

For most hashing algorithms, similarity is defined in the sense of small Euclidean distances (e.g. [31]); for some other works, cosine similarity is utilized (e.g. [35, 20]). It is very easy to verify Claim. 1 w.r.t inner product and Euclidean or cosine similarity. Therefore, traditional hashing and PPH are different algorithms that focus on different problems (Eq. 4 vs Eq. 3). (2) It is reasonable to assume sim ( u,v )  X  X  for some H . Yet u T v  X  X  does not hold in general.

As analyzed in Sec. 2.1, the underlying assumption in tra-ditional hashing is that, if sim ( u,v ) is high (e.g. Euclidean or cosine similarity), hashing function H will produce simi-reasonable to assume similarity functions are approximately ranking consistent w.r.t some hashing function H . This is the very reason why hashing techniques succeed in similarity search [8].

In contrast, however, since inner product is not a valid similarity metric, there is usually no guarantee to claim that for function H with two input u,v , if u T v is high then the output hashing code  X  u,  X  v are similar, thus small hamming distance Ham (  X  u,  X  v ). Therefore, hashing code and hashing function essentially only preserve data similarity rather than preference. This argument seems quite contradictory to our goal in Eq. 3. Although Eq. 3 cannot be exactly satisfied, a good approximation will be sufficient for our purpose. In the fol-lowing section, we will show how inner product (approx-imately) becomes ranking consistent w.r.t certain hashing function under certain constraint.
We propose the following two-step principle, under which our Preference Preserving Hashing (PPH) algorithm is de-signed:
We achieve step 1 by proposing a novel Constant Fea-ture Norm (CFN) constraint to MF algorithm M , which will be detailed in Sec. 4.1. Moreover, since the first step cannot be exactly satisfied, in the second step, we propose a novel quantization algorithm that further approximates inner product via hashing code. In general situation, such approximation is difficult to achieve. But with the help of step 1, the approximation can be achieved with satisfac-tory results. We call it Magnitude and Phase Quantization (MPQ), which will be explained in Sec. 4.2.
Before giving Constant Feature Norm (CFN) constraint, we first analyze what X  X  the difference between inner product and similarity.

Inner product of real-valued u,v can be expressed as u T v = || u || F || v || F cos X  u,v , where cos X  u,v is a widely adopted simi-larity metric (cosine similarity). It is the existence of norm || u || F , || v || F that deviates inner product from similarity. This is more clear if we further check the binary hashing codes  X  u and  X  v . Since their norms are constantly  X  u  X  v = Dcos X   X  u,  X  v . Inner product is now equivalent to cosine similarity, which also indicates Euclidean similarity ( ||  X  u  X   X  v || F = 2 D  X  2  X  u T  X  v ).

Therefore, to utilize hashing code to preserve inner prod-uct, we need to constrain u,v to possess constant norms, i.e. || u || F = || v || F = c . In such a way, inner product is totally equivalent to both Euclidean and cosine similarity as
What value should c be? Assume r i,j  X  [0 ,r max ] is the rating (continuous or discrete) assigned to item v j by user u , where 0 and r max represent unseen and maximal rating. In MF algorithm M , u T i v j is expected to approach r i,j much as possible (see Eq. 9). Recall u T v = c 2 cos X  [  X  c 2 ,c 2 ] while r i,j  X  [0 ,r max ]. To match the two ranges, we
We call this constraint as Constant Feature Norm (CFN) constraint. If CFN is ideally satisfied, then any traditional hashing methods for similarity search can be applied to pre-serving preference via hashing codes, because similarity is now equivalent to inner product (i.e. preference). Practi-cally, however, exactly satisfying CFN constraint is quite challenging for traditional MF to optimize. Instead of treat-ing CFN as a hard constraint, we treat CFN as a soft regu-larizer in the objective of M . Based on MF algorithm and the above analysis, we present the final objective as follows: MF-CFN : U , V = arg min where the first line is the adapted least square loss function of traditional MF objective [14], and the second line is the soft regularizer of CFN constraint. I i,j = 1 if user u i rated v j ; otherwise I i,j = 0.  X  is the regularizer coefficient. To make the comparison clear, we further present the tra-ditional MF objective with L2 regularizer, as well as the objective in [37] with SumZero regularizer: 2 MF-SumZero : U , V = arg min where r  X   X  [0 , 1] is the rescaled rating.

Since their loss objectives are similar except the regular-izer part, from now on we will use CFN, L2 and SumZero to denote the objective in Eq. 8, Eq. 9 and Eq. 10. All these objectives can be easily optimized by coordinate descent.
Remark 1 . Different regularizers will constrain U , V dif-ferently. The key question is, which constraint is more ap-propriate for hashing code to apply in recommendation? L2 regularizer constrains U , V within a sphere, since it penal-izes large magnitude of ||U|| F , ||V|| F . Obviously, it has no direct connection with hashing. SumZero is a common reg-ularizer in traditional hashing (e.g. [31]), and it constrains U , V to be symmetric around coordinate origin. In such a way, the derived U , V are expected to meet entropy maxi-mization principle [35]. However, this is a principle designed for similarity oriented hashing [31]; while in recommenda-tion, we need to preserve preference rather than similarity. Since ||U|| F , ||V|| F are constant, CFN regularizer actually constrains U , V to reside on the surface of a sphere with ra-dius p r max 2 . Based on the above analysis, we believe CFN is more appropriate for hashing code to apply in recommen-dation, which is further supported by experimental results in Sec. 5.
Remark 2 . At first glance it may appear that any c will satisfy Eq. 6. However, inappropriate c will affect the least square regression part in Eq. 8, 9, 10. In contrast, our derivation in Eq. 7 is more reasonable, where we explicitly address the issue.

In Fig. 1 we show the distribution of derived item fea-ture norms (i.e. ||V|| F ) for all three constraints, where 40 dimension real-valued features are trained on MovieLens-10M dataset. Clearly, CFN generates much more consistent norms than L2 and SumZero. Here only item norms are shown, since user norms will not affect the final preference ranking, as will be analyzed later in Eq. 15. For unseen items in the training set, their features are calculated as the average of seen item features. They form the spike in the curve of L2 (nearby CFN norms) and the spike of SumZero (nearby zero, behind L2 curve).
One drawback of treating CFN as soft regularizer is that it cannot be exactly satisfied, as shown in Fig. 1. Therefore, inner product based preference is still not exactly equivalent to similarity. However, we can expect the difference has been greatly reduced. Thereby, in this section, we will propose a novel quantization method that explicitly models preference within hashing code.
 The principle is as follows. Recall u T v = || u || F || v || If ideally we obtain || u || F = || v || F = p r max 2 via CFN, then only cos X  u,v is effective in final preference ranking. So our first task is to model  X  u,v in hashing code. Moreover, since CFN cannot be exactly satisfied, we also need to make some adjustment w.r.t || u || F , || v || F .

Firstly, we omit || u || F , || v || F and focus on  X  sume function  X  ( u,v ) = cos X  u,v , and we aim to find hashing function H  X  so that  X  ( u,v ) is ranking consistent w.r.t H Notice here smaller  X  u,v means higher  X  ( u,v ), thus higher inner product u T v . We denote the hashing code of u,v under H  X  as  X  u  X  ,  X  v  X  .

Then, we omit  X  u,v and focus on || u || F , || v || F sume function  X  ( u,v ) = || u || F || v || F , and again, we aim to find hashing function H  X  so that  X  ( u,v ) is ranking consis-tent w.r.t H  X  . Notice here larger  X  ( u,v ) means higher inner product u T v . We denote the hashing code of u,v under H as  X  u  X  ,  X  v  X  .

We propose the following linear approximation as the final hashing code of u,v : which indicates the overall hashing function H has: where
Since || u || F , || v || F are expected to be similar (like in Fig. 1), it is reasonable to assume the above linear approximation is sufficient. || X || F , X  essentially represent the magnitude and phase information of signals, therefore, we name our algo-rithm Magnitude and Phase Quantization (MPQ). In Fig. 2 we show an example code. Below we will explain how H  X  , H are designed.
We first give the phase quantization algorithm for H H  X  should guarantee small hamming distance Ham (  X  u  X  ,  X  v if  X  u,v is small. One straightforward solution is to encode the quadrants of u,v within hashing code  X  u  X  ,  X  v  X  . Formally, for the k th entry of  X  u  X  ,  X  v  X   X  X  X  1 } D , we have
The underlying assumption is that, if many quadrants of u,v are different, then  X  u,v will be large. In practice, we find such an approximation works quite well, as will be shown in Sec. 5.3.2.
The preference ranking process in Eq. 3 can be decom-posed into a series of pairwise comparison. For a specific user u and two items v 1 and v 2 , their relative preference is our concern: u
T ( v 1  X  v 2 ) = || u || F ( || v 1 || F cos X  u,v
We notice that the norm of u does not affect user X  X  pref-erence between v 1 and v 2 . 3 . Only || v || F (and  X  ) will affect the item ranking for a specific user. So for magnitude quan-tization, we should focus on || v || F rather than || u || As shown in Fig. 1, || v || F are now close to a constant value. This inspires a simple method to model the variance of || v || The distribution of || v || F is viewed as Gaussian. Then if one bit is utilized, the mean value acts as the threshold. If two bits are utilized, then we use  X   X   X  and  X  +  X  as two thresholds:
Here range [  X   X   X , X  +  X  ] corresponds to the area of 68 . 2% probability in Gaussian distribution. Encoding with more bits can be similarly deduced by setting more thresholds. But in practice we find 2 bits are sufficient for magnitude quantization (see Sec. 5.3.3).

One important issue is that,  X  u  X  cannot be directly quan-tized by Eq. 16. If both u,v are quantized by Eq. 16, similar || u || F , || v || F will produce similar  X  u  X  ,  X  v  X  distance and high preference. However, with || u || F being in-effective (see Eq. 15), it is larger || v || F that produces larger
In this case, the CFN constraint on || u || in Eq. 8 only serves as regularizer. Figure 2: An example of PPH hashing code. There are preference. It doesn X  X  matter whether || v || F is similar with || u || F or not. Therefore, we propose the following constant magnitude quantization method for u : u is assigned the same hashing code as the maximum || v || in Eq. 16. In such a way, the hamming distance between  X  v and  X  u  X  reflects how large || v || F is. Small hamming distance now equals high || u || F || v || F , thus high u T v . So far we have elaborated the design of PPH algorithm. Ideally, we expect Eq. 3 is approximately satisfied. Then for user u , the most preferred items v are the nearest neighbors  X  v from  X  u . Now we explain the adopted searching method in hashing code space.

As reviewed in Sec. 2.1, hashing lookup is widely utilized, due to its time cost that is independent of dataset size. How-ever, we find it inappropriate for recommendation for two main reasons. First, recall the search complexity of hashing lookup is P r i =1 D i . Since it depends only on the given ra-dius r , the number of returned items is unpredictable. Usu-ally there are too many items for some users but too few items for the others, as we observe from the experiments in Zhou X  X  method [37]. Yet for many recommendation tasks, the number of returned items is expected to be constant (e.g. Top-K recommendation [32, 30]). Second, hashing lookup will soon become inefficient when code length D or searching radius r becomes large: the exploration space D is prohibitive even when D &gt; 32 or r &gt; 2 [8]. Yet for MF algorithms, longer dimension is usually desired for better recommendation accuracy, and it is very common that U , V have dimensions bigger than 32 (e.g. [14, 13]).

Hamming ranking would be a better choice for recommen-dation. It ranks items according to their hamming distance with the user, and guarantees to return items of fixed num-ber. Speedup is achieved by fast hamming distance compu-tation (i.e. popcnt function). Yet one disadvantage is that its time complexity is still linear w.r.t dataset size.
We propose to utilize Multi-Index Hashing (MIH)[21] for our scenario. The basic principle of MIH is to split the orig-inal hashing code of length D into s subcodes, and conduct hashing lookup for each subcode. For r hamming searching radius, the exploration space of s  X  D/s d r/s e is much smaller than D r . For generating recommendations of fixed num-ber, MIH progressively enlarges searching radius, so that a certain number of nearest neighbors can be found. When hashing codes are uniformly distributed, the time complex-ity of MIH is sub-linear w.r.t the dataset size. Overall, MIH can be viewed as a generalized hashing lookup, with much nicer properties. Please refer to [21] for more details.
Below we give some relevant discussions for the sake of clarity and better understanding.

CFN for MF. PPH contains both CFN and MPQ: the former belongs to preference modeling, while the latter be-longs to preference ranking. Although PPH aims to speed up only preference ranking, it modifies the part of preference modeling by introducing CFN to MF objective.
 The modification is well justified for two reasons. First, Sec. 5.2.1 shows that the recommendation accuracy of CFN does not degrade too much compared with traditional L2 regularizer. Second, the optimization process of MF-CFN remains the same as MF-L2, which means CFN does not increase the time cost of preference modeling. Therefore, CFN is a reasonable alternative to L2 regularizer, yet more appropriate for hashing code derivation.

MPQ without learning. MPQ is designed to transform real-valued u,v into hashing code  X  u,  X  v without any learning process, which contrasts the trend of learning hashing codes in hashing literature (e.g. SpH [31], STH [35], ITQ [7]).
As analyzed in Sec. 2.2, we argue that the learning process may be harmful to efficient preference ranking. Once users update their preferences, the old speedup model becomes outdated and the learning process has to be repeated. For hashing, the learned hashing function is the speedup model. Previous works (e.g. [37, 22]) neglect the learning cost of speedup model when discussing recommendation efficiency, which is inappropriate. If recommendations are desired to meet users X  latest preference models, speedup model learn-ing has to be repeated frequently, whose cost can never be neglected. In contrast, MPQ does not involve any learning process. Therefore, even if users X  preferences are updated, they can be readily transformed into hashing code without extra cost. In Sec. 5.3.1 we will show a case study of MPQ vs ITQ (used in Zhou X  X  method [37]) to support our statement.
We present experimental results to answer the following questions: (1) Is PPH effective? Namely, how X  X  the rec-ommendation accuracy of PPH compared with traditional MF? (2) Is PPH efficient? Namely, how much faster it can achieve than traditional MF? Other related issues are dis-cussed when appropriate.
Recent studies (e.g. [30, 26]) argue that recommendation is better treated as learning to ranking problem, rather than a rating prediction problem. Recommendation is evaluated based on the ranking order of items regardless of their actual preference scores. We find such a perspective of recommen-dation well fits our goal in Eq. 3. Moreover, it is impossible to recover the exact ratings from hashing code.

We choose NDCG@K [9] as our evaluation metric, which has been widely utilized in learning to rank and ranking oriented recommendation (e.g. [26, 30]). We empirically set K = 10, then the average NDCG@10 over all users is our primary evaluation metric of recommendation accuracy.
We utilize the following three publicly available datasets for experiments.
For all datasets, we select 20 ratings per user for train-ing; 10 ratings for validation, on which  X  in Eq. 8, Eq. 9 and Eq. 10 is tuned; the rest (at least 10 items) are used for test-ing, due to the NDCG@10 evaluation metric. Users having less than 40 ratings are not considered. The dataset split is randomly repeated 5 times to report the average results.
As shown in Sec. 4, PPH consists of regularizer (CFN) and quantization (MPQ). The comparison will also occur in these two parts. Specifically,
For the above algorithms, we derive real-valued u,v with four different dimensions, namely, { 16 , 24 , 32 , 40 } . Notice MPQ will produce hashing code 2 bits longer than MedThresh and ITQ method. We argue this is a fair comparison, since all of MPQ, MedThresh and ITQ utilize the same real-valued features of the same dimension. Actually, as will be shown in Sec. 5.3.2, even without the two bits of magnitude codes, our phase quantization is already significantly better than MedThresh and ITQ. Interestingly, we find the performance of PPH with dim=16 is already better than other algorithms with dim=40.
We firstly examine the recommendation accuracy with real-valued feature U , V , as shown in Fig. 3. The accuracy with CFN regularizer is close to the L2 regularizer. Espe-cially when the feature dimension grows high, the difference between CFN and L2 becomes small. This well justifies the statement in Sec. 4.4, that the modification to preference modeling is reasonable.

On contrary, SumZero incurs much larger accuracy loss. It shows that the SumZero constraint, which requires features being symmetric around coordinate origin, is not helpful to maintain the accuracy of MF algorithm. Another possible reason for its large loss is the few training ratings on large scale datasets, while in [37] huge training ratings (up to 20%  X  80% of all ratings) on small datasets are utilized. Yet this is very impractical, since recommeder systems cannot ask users to rate too many items before it can make any recommendation. For example, the MovieLens website 4 only requires users to input 15 movies for start up. Therefore, it suggests that SumZero is not suitable for this cold start situation.
In Tab. 1, 2, 3, we give the recommendation accuracy with hashing code. For each feature dimension, the highest accu-racy among all methods is marked as bold. * indicates the improvement over the second highest accuracy (with sub-scripted  X  ) is statistically significant at p &lt; 0.05. We can make several observations from the results: CFN vs L2 and SumZero. Compared with L2 and SumZero, CFN achieves higher accuracy with MPQ for all dimensions, and almost all dimensions with MedThresh and ITQ (except dim=16 on Netflix). As analyzed in Sec. 4.1, CFN makes preference resemble similarity. The produced hashing codes will be more suitable to preserve preference than those produced by L2 and SumZero regularizer.
 MPQ vs MedThresh and ITQ. For CFN objective, MPQ produces much higher accuracy than MedThresh and ITQ. As analyzed above, MedThresh and ITQ are designed to preserve similarity rather than preference. As a soft reg-ularizer, CFN constraint is not fully satisfied. Therefore, for the produced hashing code of CFN, preference is not ex-actly equivalent to similarity. That X  X  why MedThresh and ITQ do not work well for CFN, because they are only suit-able for preserving similarity. In comparison, MPQ works best because it explicitly models preference within hashing code. Moreover, MPQ only boost the performance of CFN significantly. This is reasonable, because our MPQ is based on the assumption of CFN, that magnitude information is suppressed while phase information takes effect. As shown in Fig. 1, clearly L2 and SumZero do not support the as-sumption of MPQ, therefore they still do not perform well with MPQ.

Overall. Our PPH algorithm (including CFN and MPQ) achieves the best accuracy on all datasets with various fea-ture dimensions. More noticeably, the accuracy loss is much smaller than Zhou X  X  method in [37] (SumZero+MedThresh and SumZero+ITQ). http://movielens.umn.edu
Hashing Code vs Real-Valued Features. Compari-son with the results of real-valued features in Fig. 3 reveals the accuracy loss by introducing hashing code to recommen-dation. Specifically, we compare traditional MF algorithm (L2 objective) with real-valued features and PPH with hash-ing codes. In best cases (dim=32 or 40), the accuracy loss on MovieLens-1M/10M and Netflix is 0.072, 0.079 and 0.092 (for NDCG@10), which is significantly better than [37]. We argue that this is reasonable accuracy loss, since the task of replacing real-valued features with hashing code of almost the same length is very challenging. We believe there X  X  still room for further improvement, and we will discuss some fu-ture works in Sec. 6.

Moreover, the accuracy loss is a necessary trade-off for fast preference ranking. Below, we will present the efficiency experiments to show the benefits of introducing hashing code to recommendation.
For efficiency experiments, we examine three methods of preference ranking: (1)brute force linear scan with real-valued features u,v , in which inner product (preference) op-erations are conducted and preference scores are sorted to get top-10 results; (2) hamming ranking, where hamming distance are calculated with hashing code  X  u,  X  v and sorted to get top-10 results; notice hamming distance can only be in-tegers, so sorting degrades to finding items with the smallest hamming distance, which is much faster than sorting real-valued scores; (3) Multi-Index Hashing (MIH) [21], the gen-eralized hashing lookup method; its major parameter is the subcode number s , which we empirically set as { 1 , 2 , 2 , 3 } for dim= { 16 , 24 , 32 , 40 } . We do not consider basic hashing lookup because it is inappropriate for recommendation sce-nario, as explained in Sec. 4.3.

We take the Netflix dataset for illustration. To simulate real world applications, we retrieve top-10 items for every user from a pool of 17,770 candidate items. CFN and MPQ are used for the following efficiency experiments, which are conducted on a computer with AMD 64 bit Opteron CPU and 12G memory. The time cost (in seconds) over all users will be the major criteria of efficiency. Notice no parallel computing is involved, so the time cost reflects overall com-putational workload.

Time Cost vs Dimension. In Tab. 4 we give the time cost statistics when feature dimension varies on the entire item set (17,770 items). Clearly we can observe the appli-cation of hashing code can significantly speed up preference ranking. For linear scan with real-valued features, the time cost of inner product operation is O ( NMD ), and the cost of top-K item sorting is O ( NM ). Obviously, the overall cost for real-valued features is huge (42min  X  1.6 hours). Ham-ming ranking is much faster due to the fast calculation of hamming distance. On a AMD 64 bit cpu, hashing codes no longer than 64 bits can be processed by a single func-tion call ( popcnt). Its complexity of finding top-K items is O ( N ( K + D )), as analyzed above. Therefore, the time cost of hamming ranking is constant for all feature dimen-sions. Yet still, hamming ranking needs to scan all possible items, which may be inefficient. MIH achieves even higher speedup than hamming ranking, because hashing lookup is conducted for each subcode. That means, MIH doesn X  X  need to scan all candidate items.

Time Cost vs Item Number. We further investigate how different preference ranking algorithms behave when item number varies. On Netflix dataset, we randomly se-lect 12.5%, 25%, 50% of the entire 17,770 items, and show the corresponding time cost in Fig. 4. In left figure, linear scan of real-valued features scales linearly w.r.t item num-ber. In comparison, the costs of hamming ranking and MIH are almost negligible. We further show the time cost at a finer scale in the right figure. Now the time cost of real-valued features soon goes beyond the axis range. Meanwhile the difference between hamming ranking and MIH begins to emerge. We can see the cost increase of MIH is slower than hamming ranking, because MIH does not need to scan all candidate items yet hamming ranking does. As shown in [21], MIH has sub-linear time complexity for uniformly distributed hashing codes, which is better than linear com-plexity of hamming ranking.

In summary, the application of hashing code indeed can speed up preference ranking significantly. Especially with MIH, we achieve the maximal speedup and best scaling com-plexity w.r.t item number. Similar conclusions are also ob-tained from MovieLens-1M/10M datasets. Due to space lim-itation, here we simply omit the detailed time statistics. Table 4: Time Cost of Top-10 Recommendation
We further present more experimental results, regarding details like statement supporting and parameter tuning.
To support our argument in Sec. 4.4, in this section we conduct a case study of MPQ vs ITQ, to see how the learn-ing process of ITQ brings extra computational burden to preference ranking. On Netflix dataset, we test the trans-formation time for CFN objective with 40 dimension. For MPQ, the time cost is 1.014 seconds; while for ITQ, the time cost is 305.03 seconds, 300 times slower than MPQ! Moreover, the learned ITQ model will become outdated if many users update their preferences, which is very common in practice. Thus the learning has to be repeated, and the computational burden will be even heavier.

It is beyond the scope of this paper to discuss when and how users X  preferences shall be updated. But for a reason-able recommender system, such updates are believed to be conducted regularly and frequently (e.g. [25]). In this case, non-learning based algorithms (like our MPQ), will have ef-ficiency advantage than learning algorithms (like ITQ).
In Fig. 5 we show how magnitude and phase code con-tribute to the the overall MPQ performance separately. We can see the actual performances meet our expectation. Due to CFN, all || v || F become similar. Therefore, phase infor-mation contribute most to preserve users X  preferences. Yet by incorporating magnitude code, the overall MPQ achieves even better performance than phase code alone. That means, magnitude code still captures a small part of users X  prefer-ences, because CFN is only a soft regularizer.
In the above experiments, we empirically set the bit num-ber of magnitude code as 2. In Fig. 6(a) we show the recom-mendation accuracy of CFN+MPQ w.r.t different bit num-ber of magnitude code on Netflix dataset. Clearly, 2 bit has obvious improvement over 1 bit, yet the difference between 2 bit and 3 bit is very small. To make the code length small, we believe 2 bit is a good choice. Again, similar conclusions are also drawn on other datasets.
It X  X  interesting to show how regularizer coefficient  X  is se-lected. In Fig. 6(b) we show the NDCG@10 curve as  X  varies on MovieLens-10M validation set, with CFN objective and dim=40 in real-valued features. The curve is very smooth, and the accuracy is quite stable within a large range of  X  Figure 4: Time cost when item number varies. Left: around the optimal one. This means the CFN objective is very robust w.r.t  X  selection. We also observe the accuracy on validation set is much higher than that on testing set, because validation set (10 ratings per user) is much smaller than the testing set. Similar phenomena are also observed for other dimensions and datasets, and we omit them due to space limitation.
Finally, we examine the situation where less training data is utilized. In this section, we select 10, 5 and at least 10 items for training, validation and testing, where users with less than 25 items will be removed. In Fig. 6(c) we show the results of L2 (with real-valued features) and PPH (CFN+MPQ) with 20 and 10 training items for each user. It is clear that by incorporating more training data, the rec-ommendation accuracy can be significantly improved. How-ever, one drawback is that more user input is required. This tradeoff between user effort and recommendation accuracy has to be considered wisely in real world applications. In this paper, we propose a novel Preference Preserving Hashing (PPH) algorithm for efficient recommendation. We discuss the key difference between similarity and preference, and formulate PPH to approximate preference via hashing code. Extensive experiments show that our algorithm can achieve hundreds of times speedup than traditional MF algo-rithm, while suffering much less accuracy loss than previous work of hashing for recommendation.

We believe there X  X  still room for further improvement. For example, special loss functions can be designed that fit hash-ing code better. Moreover, besides CFN, there may exist other methods to better approximate preference via simi-larity. It is also interesting to test other recommendation scenarios, like tag [28] or context-aware [23] recommenda-tion, to see if PPH can generalize well. We plan to explore those possibilities in near future. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208, IIS-1017837 and CNS-1314688. This work is also partially supported by the Center for Sci-ence of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.
