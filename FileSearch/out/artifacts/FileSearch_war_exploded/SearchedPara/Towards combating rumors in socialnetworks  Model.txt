
Department of Computer Science and Engineering, I I T Delhi, New Delhi, India IBM Research-India, New Delhi, India 1. Introduction
Sociologists use the term rumor to refer to an unveri fi ed account or explanation of events circulating from person to person and pertaining to an object, event, or issue of public concern [23]. Rumor is a potentially harmful social phenomenon that has been observed in human societies in all times. In Book IV of the Aeneid , the Latin poet Virgil refers to  X  X umor, the swiftest of all evils X , going on further to give an intuitive characterization of its spread:  X  X peed lends her strength, and she fi nds vigor as she goes X  [32]. Since Virgil wrote his classic, enablin g technologies for the spread of rumor have multiplied. Speci fi cally online social net works (Twitter, Orkut, F acebook etc.) provide a p latform for the rapid interchange of information and hence, for the rapid dissemination of unsubstantiated claims that are potentially harmful. In this paper we study ways of combating rumors in social networks actuated by the realization that authoritarian methods for fi ghting rumor have largely failed. Our major insight is that in situations where populations do not answer to the same authority, it is the trust that individuals place in their friends that must be leveraged to fi ght rumor. In other words, rumor is best combated by something which act like itself, a message which spreads from one individual to another. We call such messages anti-rumors . Using mathematical models proposed in th e literature for the spread of rumor, we study different anti-rumors processes and present metrics for evaluating the ef fi cacy of these methods in fi ghting the spread of rumor.

Social networking sites have certain intrinsic properties that make them an ideal medium for the spread of rumor. They have huge and distributed user-bases, clusters of users sharing the same interests, devel-oping trust in each other, and seeking access to the same resources. Moreover, the platform openness makes it easy to deploy malicious applications. During times of crisis the use of social networking to spread information can have the harmful effect of allowing rumors to proliferate even faster and wider than they did earlier. In an era where email spam is easily detected by most users (would you send your bank account number to the widow of an assassinated African general?), mainly because it comes from people we do not know. However, it is possible for malicious users to leverage the trust we repose in our  X  X riends X  or  X  X onnections X  on social networking sites in order to spread harmful content. In fact, as of now, all popular social networking sites have experienced some level of malicious use (see e.g. [22,26]). 1.1. A note on assumptions
Our model makes one radical simplifying assumption that the anti-rumor is a message which is com-pletely convincing. There are several gaps in this assumption. The basic question that arises is: How does a person distinguish an anti-rumor from a rumor? In some cases this may be possible: for example if the anti-rumor carries a proof of its correctness with it, or if it comes from an authenticated source. But in general this may not be possible. We do not account for this lacuna in our work. Instead we present our studies as a foundational step. It may be possible in future to develop more sophisticated models that take into account the dif fi culty in distinguishing between rumor and anti-rumor, but we believe that the basic underlying dynamics will be not too different from the ones we analyze here. We discuss the possibilities of pursuing such a direction in Section 5.

Further, we assume that a person who has once heard the anti-rumor will never again believe the rumor and will in fact want to spread the anti-rumor. This is based on a somewhat rose-tinted view of human nature. While it is true that there are people who make independent efforts to quell rumors because they realize the danger they may cause (see e.g. [20]), to believe that every person will want to actively spread an anti-rumor is unduly optimistic. In fact the truth might be far from this assumption: It is even possible that malicious users might intercept the anti-rumor and, adapting the rumor so that it becomes harder to refute and start spreading the adapted rumor. It is no doubt possible to model a variable response to anti-rumor spread and that might be a way to re fi ne our model, but in this paper we only lay out the basic program, and do not study the model to that level of sophistication. 1.2. Main contributions
The main contributions are setting out three models that describe natural scenarios for the spread of anti-rumor to combat the spread of rumor. We describe these brie fl y now and in more detail in future sections.

Traditionally governments have tried to combat rumor in a centralized and sometimes authoritarian fashion. The mode has been of an authority broadcasting the anti-rumor message. This is expensive and not always effective (e.g. the USA remains plagued by rumors regarding childhood vaccinations, despite concerted advocacy through the media that it is false [17]) especially in culture or in situations where people mistrust the authority or its motives. Tripathy et al. [30] studied two authority centric models which are by nature decentralized, where decentralized is described as spreading of messages via social relationships unlike traditional authority based model which uses broadcasting for spreading the messages. The fi rst method called Delayed start model , models a situation where a local authority might discover a rumor n days after it starts and decide to spread an anti-rumor. This is a purely reactive methodology where a single agent, perhaps a local authority, takes cognizance of a rumor and decides to act against it. The other model called Beacon model , models a situation where a set of vigilant agents, beacons , are on the lookout for the spread of rumors. Once a beacon receives a rumor, it immediately starts spreading anti-rumors to combat the rumor. The Beacon model is a proactive model. In the Beacon model we assume that there is a loose confederation of agents, perhaps a set of local authorities, that realize the danger of rumor spread and have planted listening posts in the social network. These listening posts are like sleeper agents, they come alive only when they detect a rumor. In this article, we study the Delayed start model and the Beacon model more rigorously using a number of new metrics (explained in Section 2). We also study a non-authority centric m odel of an enlightened citizenry vigilant against rumors, in which any user with so me probability can detect the rumor on recei ving it and decide to warn his or her contacts about the spread of the rumor. The difference between the Neighborhood model and the previously studied models is that each agent of the social network can independently decide to start combating the rumor.

The models we study are Markovian in nature and so their evolution can be characterized by simple differential equations. This approach, known as the mean fi eld approach, follows from an in fl uential work of Kurtz [15] that shows the connection between differential equations and limit laws for Markov process. In Section 3 we present mean fi eld characterizations of our three models and validate the quality of the approximation they provide. This allows us to study the anti rumor models at scales that would be prohibitively dif fi cult to simulate. 1.3. Related work
Rumor spread bears strong similarities to virus spread since the spreading mechanisms are similar i.e. contact between individuals. Some of the models studied for virus spread are the Susceptible In-fected Susceptible (SIS) model and the Susceptible Infected Recovered (SIR) model [7,28,36], the Voter model [5,16], the Independent Cascade (IC) model [13]. We wish to clarify an important difference between our model and virus spread models. Although both rumor and virus spread through social (or physical) contacts, containing them requires fundamentally different strategies. Rumor can be combated by spreading messages (anti-rumors) on the same network, whereas combating viruses requires the vac-cination of each individual who is to be protected or barrier vaccination for protecting individuals living in a geographically bounded region. Hence, unlike the virus spread case, the same social network used by a rumor can be leveraged to fi ght against it.

The problem we study is also very similar to the well studied problem of competitive viral market-ing [3,4,29]. In our setting, there is a competition between two processes, the rumor process and the anti-rumor process. Similarly, in co mpetitive viral marketing there is competition between two products. But the key difference between these two domains is encapsulated in our assumption that rumor will eventually be removed by the anti-rumor because there is some essential distinction between falsehood and truth. Although this is de fi nitely not true for all rumors and all situations we believe that it covers a large and signi fi cant part of the space. In the case of compe titive viral marketing t here is no authority who can unequivocally decide which product is superior. Hence, in our models anti-rumor eventually cleans the network from rumor, whe reas in competitive viral marketin g situation a stable state is reached wherein all parties settle on a market share.
The problem of characterizing the rumor process in a social network has been studied (see e.g. [36]) using an epidemic like model called SIR model, however the rumor-control problem has not received wide attention. Habiba et al. [11] studied the problem of identifying good blockers to minimize the rumor spread in the network. This paper also uses the Independent Cascade model for diffusion. Ur et al. [31] show how attack strength can be increased in a social network by controlling the hubs of the network. Webb et al. [33] proposed a technique to fi nd spammer pro fi les by installing social Honeypots. They found that spam pro fi les follow a distinct temporal pattern and their geographical locations overlap with the target location. Kimura et al. [14] studied the problem of minimizing the contamination spread in network by blocking the links of the network. In this paper we study different ways of combating rumors in social networks.

We would like to observe that while sharing several aspects with the papers described above, our work X  X  fi rst main original contribution is the idea that rumor can be combated with rumor-like anti-rumor messages. Our second main original contribution is the formulation of the three anti-rumor spread models described above and a set of metrics for studying these models which will be discussed in detail in the next section. 2. Models and metrics Basic notation: We assume that the online social network is modeled as a directed graph G = { V,E } . For each node V i , the immediate neighbors are represented by the set N i and n = | V | represents the total number of nodes in the network. A variable s i , the status of the node, is maintained for each node V i  X  V . A node i with s i =0 is a susceptible node. This node is yet to believe the rumor or anti-rumor. If s i =1 , then node i is an infected node. Such nodes, after believing the rumor, will spread the rumor in the network. When s i =2 we say i is a cured node whereas if s i =3 we call i a vaccinated node. Cured nodes are those nodes which were once infected and now believe the anti-rumor. Vaccinated nodes are those susceptible nodes who now believe the anti-rumor but were never infected with the rumor. These two types of nodes, cured and vaccinated, will spread the anti-rumor message in the network. s ( t ) i denotes the status of node at time t . 2.1. Rumor spread model To model the rumor spread process, we use the Independent Cascade (IC) Model with a little variation. This is a well studied model for diffusion in social networks [13,14,24]. In the IC model, each node gets a single chance to infect its nei ghbors, whereas in our se tting each node gets multip le chances to infect its neighbors. Therefore, the rumor can spread very fast. We call this the Multi Try Independent Cascade Model (MTICM). In MTICM, at time t , each infected node V i tries to infect each of its uninfected neighbors w  X  N i and succeeds w ith probability p 1 . The rationale for choosing this model of for rumor spread is that this model is guaranteed to spread the rumor throughout the network. And in fact this model succeeds in spreading the rumor through the network in a relatively short time (see e.g. the work by Chierchetti et al. [6]). This makes this model a pessimistic estimate of the power of rumor, and hence a more dif fi cult adversary to deal with than other models, some of which cannot even guarantee network-wide spread of the rumor (see e.g. Sudbury X  X  work on the SIR model on complete graphs [27].) The hope is that if we can combat rumor which is spreading by this model then the rumor spread using a less powerful model can also be easily combated. On the optimistic side, we also use the MTICM to model the spread of anti-rumor. This assumes that an anti-rumor spreader, who is essentially an altruistic person, will be persistent in his or her actions. We postpone for the future a more sophisticated study that models closely the variations in altruistic behavior of real world actors.

Through the course of our experiments we have taken p 1 to be 0 . 01 . If it succeeds then w will become infected at time t +1 . The process starts with 10 random infected nodes. These choices of parameters are admittedly arbitrary as is the assumption that these parameters are uniform across nodes and across time. Deriving meaningful values of such parameters is a research problem in itself, one that we do not intend to focus on in this paper. Our goal is to demonstrate the fundamental characteristics of the process we study with parameter values that satisfy our general intuition. 2.2. Anti-rumor spread models
In this subsection, we present the anti-rumor spread models. In our previous study [30] we presented a preliminary analysis of the fi rst two models: The Delayed start model and the Beacon model . For making this article self-contained, we brie fl y explain these models. In this work, we present a new model and also evaluate the older models on larger dataset and the new metrics proposed in next subsection.
Delayed start model: Here we model the situation that an authority with limited jurisdiction detects the spread of rumor and then combats it by starting an independent cascade from a randomly selected infected node. We contend that there will always be a time lag between the start of rumor and its detection (and hence, the start of the anti-rumor). We parametrize our model with this delay, represented by d . Fig. 1(b) pictorially depicts the Delayed start model. The checkered node is the previously infected node from where the anti rumor starts. The information goes to all neighbors: infected (node A) as well as uninfected (Node B). The process starts from a single random infected node after a delay time d .
Beacon model: Between the time an authority detects the spread of rumor and decides to combat it, the rumor continues apace. In orde r to proactively comba t rumors, authorities m ay embed agents in the network that are capable of detecting the spread of rumor and are authorized to start anti-rumors as soon as they detect rumor. We refer these agents as beacons . The beacons spread the anti-rumor according to the Multi Try Independent Cascade Model (MTICM). Figure 1(c) shows a Beacon model. Please note that in the current state of the network, beacon B1 will be inactive since it has not yet received the rumor. In Delayed start model, the starting time of anti-rumor process is fi xed but here it depends upon the time when the beacon receives the rumor.

Neighborhood model: In the previous models the anti-rumor originates from the nodes selected by some authority either before or after the rumor starts. In the current model any node V i may decide, on receiving the rumor from a neighbor V j , to refute it. This model is similar to the Beacon model. The difference lies in choosing the set o f initial b eacons. In the Beacon model, th e initial s et of beacons are chosen by some authority whereas in the Neighborhood model, the beacons are self created with some probability durin g rumor spreading process. The Beacon model with b number of beacons (out of total n nodes) is comparable in an expected sense to a Neighborhood model where a node refutes the rumor with probability b n . 2.3. Metrics
In this section, we propose various metrics to evaluate and compare the ef fi cacy of these models. We divide these metrics into two categories: Time varying metrics and lifetime metrics. 2.3.1. Time varying metrics These metrics capture the temporal evolution of the system. We consider three time varying metrics:
Number of infected ( I ( t ) ) : This metric captures the number of infected nodes at time t . Mathemati-cally, we de fi ne this as: I ( t )= V i | V i  X  V ( G ) and s t i =1 . It provides us with a handle on the rumor growth process. T i denotes the time period for which a node V i remains infected. T i is de fi ned as:
Number of cured ( C ( t ) ) : This metric captures the number of cured nodes at time t , i.e., the number of infected nodes who have accepted the anti-rumor and now recognize the rumor as false. Formally, C ( t )= V i | V i  X  V ( G ) and s t i =2 .

Number of vaccinated ( V ( t )) : This metric captures the number of nodes who learn the truth about the rumor before the rumor reaches them and therefore, they are vaccinated against future encounters with the rumor. It can be de fi ned as: V ( t )= V i | V i  X  V ( G ) and s t i =3 .

Finally, the number of nodes which are not covered in the above metrics are known as susceptible performs two operations, it makes the infected nodes cured and makes the susceptible nodes vaccinated. Similar metrics are used in several papers related to rumors, viruses and epidemics spread [18,19,34 X 36]. 2.3.2. Lifetime metrics
These metrics capture the global properties and are calculated at the end of the process. We consider fi ve lifetime metrics:
Duration of infection ( D ( G ) ) : This metric denotes the total time required for the anti-rumor process to kill the rumor process completely. D ( G )=max { t | X  i : s ( t ) i =1 } . This metric is a well studied metric in epidemic and virus literature [10,36].

Outbreak size ( R ( G ) ) : Outbreak size captures the total number of nodes which are infected at some point of time. R ( G )= { V i | X  t D ( G ): s ( t ) i =1 } Grabowski et al. [10] also used this metric to measure the strength of epidemic spread where in SIS model.

Maximum infected time ( M ( G ) ) :The Maximum infected time measures the life time of the rumor in the network, i.e., it measures the maximum duration for which any node continues to believe the rumor. It can be de fi ned as M ( G )=max { T i | X  V i  X  V ( G ) }
Average infected time( A ( G ) ) : The average infected time captures the average time for which the users continue to believe the rumor, i.e., A ( G )= 1 | V [ G ] | V
Point of Decline ( P ( G ) ) : There are two independent cascade processes growing simultaneously: ru-mor process and anti-rumor process. If  X ( t ) 0 , then rumor process is growing and if  X ( t ) &lt; 0 ,then at which the number of users believing the rumor start to decline and therefore, marking the time when anti-rumor process gets stronger than rumor process. It can be de fi ned as P ( G )=min { t |  X ( t ) &lt; 0 } . Note that the uniqueness of the point of decline is not apriori obvious. However, our studies have shown that once the number of rumor infected individuals starts decreasing, it does not increase again. This metric is also used in epidemic literature [10]. 3. Mean fi eld characterization Clearly, our models of rumor and anti-rumor are Markovian processes. The in fl uential work of Kurtz [15] demonstrated that the limit laws of such processes conform to the solutions of ordinary differential equations. This gave rise to the  X  X ean fi eld X  way of analyzing Markov processes. The key insight can be loosely stated as follows: if we assume that neighborhood of a node behave like an  X  X ver-age X  neighborhood, the evolution of this  X  X ean fi eld X  process closely approximates the evolution of the Markov process.

Mean fi eld theory has been a central tool for theoretical physics. Several foundation papers on so-cial networks use this method e.g. Newman et al. [21] studied the small world model using mean-fi eld, Barab X si et al. [2] described mean-fi eld rate equations for the scale-free model. In the study of sociolog-ical phenomena that involves diffusion on complex networks, systems like epidemic spread (e.g. Funk et al. [9]) and competitive viral marketing [29] have also been analyzed using mean-fi eld theory. Of direct relevance to our current paper, is the work of Nekovee et al. [19] where mean-fi eld theory was used to study the spread of rumor in a complex networks using the MakiThompson (MK) model. Sathe [25] also used mean-fi eld methods to study the spreading behavior of rumor in LiveJournal. 3.1. Notation
In order to describe the mean fi eld equations for our models, we take into account the heterogeneity induced by nodes having varying num bers of neighbors by partitioning th e sets of nodes with different states according to their indegree. The number of susceptible nodes of in-degree k at time t is denoted V ( t ) denote the infected, cured and vaccinated nodes, respectively, at time t that have indegree k .The indegree distribution of the graph is denoted by { F ( k ) } k 0 i.e. the probability of a node having degree k is
F ( k ) . Recall that p 1 is probability of a node acceptin g the rumor and p 2 is the corresponding quantity for the anti-rumor message.
The mean fi eld assumption can be restated as follows: The infected nodes are spread uniformly at random in the vertex set. Under the mean fi eld assumption we have that, th e probability of a node being infected at time t is equal the fraction of the nodes of the network that are infected at time t . We denote this quantity by  X  1 ( t ) where Similarly, applying the mean fi eld assumption to the cured and vaccinated nodes, we de fi ne  X  2 ( t ) :the probability th at any given link points to an anti-rumo r spreading node (cur ed or vaccinated) 3.2. Delayed start model
Noting that a susceptible node is no longer susceptible once it accepts either the rumor or anti-rumor we have A susceptible node enters the infected set when it accepts the rumor and an already infected node leaves the infected set if it accepts the anti-rumor. This gives us A node that accepts the anti rumor enters the cured set if it was infected and the vaccinated set if it was only susceptible. Hence Solving the rate Eqs (1) X (4) analytically is dif fi cult so we resorted to solving them numerically. We vali-dated the evolution obtained from these equations by comparing the numerically obtained solution with the results of simulations conducted on a Barab X si-Albert graph with 10 6 nodes. The results, for a delay of 160, are presented in Fig. 2(a). We note that the mean fi eld evolution is a very close approximation to the simulated evolution. 3.3. Beacon model
For this model we also have to keep track of the unactivated beacons at time time. We denote by B k ( t ) the number of unactivated beacons of indegree k at time t . As in the Delayed start model we have The difference arises in  X ( C k ( t )) and  X ( V k ( t )) . A beacon, once it accepts the rumor begins to spread the anti-rumor, i.e., it begins to behave like a cured node. Hence Also, an unactivated beacon gets activated once it accepts the rumor or, by default, once it accepts the anti rumor, i.e.
 Solving these equations numerically for the case where there are 500 beacons in a 90,000 node graph mentioned above and plotting against the simulated value (see Fig. 2(b)) we fi nd that the mean fi eld results are a close approximation to the simulated values. 3.4. Neighborhood model
In the neighborhood model we add a nother probab ility, the probability of refutation. We denote it by p 3 , the probability th at a node recognizes the rumor and decides to refute it. In this model the rate equations for susceptible nodes and vaccinated nodes are as before.
 The difference occurs in the infected and cured nodes. When a susceptible node accepts the rumor, it enters the infected set with probability 1  X  p 3 else with probability p 3 it enters the cured set and becomes a spreader of anti rumor. Hence In Fig. 2 (c) we show the comparison between the simulated evolution and the mean fi eldsolutionof the Neighborhood model for refutation probability p 3 = 500 / 90000 . The approximation is found to be close to the simulation. 3.5. Discussion
The rationale for using the mean fi eld approach is to study the evolution of the rumor versus anti rumor process at scales which are too large to simulate conveniently. A similar approach was also used by Nekovee et al. [19] to understand the dynamics of rumor spread ing in scale free networks. The quality of the approximation grows with the increase in scale. In the case of the Delayed start model, the quality of approximation deteriora tes as the delay time increases. This i s because the variab ility of the number of nodes increases with increase in delay. Figure 3(a) shows the correlation between growth of infected nodes in simulation and in mean fi eld solutions. We can see that correlation coef fi cient is deteriorated as the delay time increases. Similarly in Fig. 3(b) we can see that the mean-squared error increases as the delay time increases. The quality of the approximation is poor er for the beacon model and the neighborhood model because apart from the size of the network, both these models have another scale parameter: the number of beacons and the probability of refutation, respectively. As these parameters grow the approximation improves. Figure 4 shows the quality of approximation for the growth of infected node in the Beacon model. In Fig. 4(a) we fi nd that as the number of beacons increase the correlation between the simulated results and mean fi eld results increase. Figure 4(b) shows decrease in the mean squared error between simulation and mean fi eld solutions as the number of beacons increase. The mean fi eld equations for the Neighborhood model behave similar to that of the Beacon model. In the next section we present results from simulation on the Synthetic graph described above and a network derived from Twitter. We supplement these results by deriving trends from the mean fi eld results for various metrics on Synthetic data.
 4. Experiments and analysis
In this section we discuss our simulation results. In all our results, the values used for the analysis are averaged over 50 runs. We also present the results for mean fi eld models and showcase that in most cases our simulation model and analytical model are extremely close. In some cases, we observe a deviation in actual values, however, the observed growth rate and trend are still consistent. 4.1. Data sets Twitter data: We used Twitter X  X  API to crawl the Twitter network. We started from the seed user DLF IPL , the of fi cial Twitter account of The Indian Premie r League (IPL), an Indian cr icket league sponsored by Delhi Leasing and Finance (DLF). Considering each user as a node and a  X  X ollower X  relationship as a directed link, we created a directed network containing 100,500 nodes and 2,465,836 edges. Since in this work we intend to fi nd the impact of the proposed anti-rumor models for combating rumor, therefore, we are interested in the case where a rumor can possibly spread through entire network. Therefore, we have taken the maximal strongly connected component (with 89,999 nodes) and pruned the remaining 10,501 nodes (and corresponding edges) from the original data set. The reason this pruning is done is to ensure that each node can potentially send data to every node under consideration. If this were not done then there would be nodes which can either only receive messages from some part of the network or can only send messages to some part of the network. We pruned away these nodes to get rid of degenerate behavior. After pruning, the Twitter graph cont ained 89,999 nodes and 2,262, 104 edges with average degree 24 and diameter 17.

Synthetic data: We use the Bar X basi-Albert (BA) model [1] to generate the synthetic data. The BA model is a random graph model which generates a scale-free graph (means the graph with power-law degree distribution) by incorporating growth and preferential attachment. Starting with a small number of initial vertices (say A 0 ), at each time step a new vertex with b ( A 0 ) edges added, that link the new vertex to b different vertices already present in the system. To incorporate preferential attachment, the probability P i that a new vertex will be connected to a vertex i is taken as P i = k i / j k j ,where k i denotes the degree of the vertex i .After t steps the model leads to a random network with t + A 0 vertices and b  X  t edges.

A scale-free graph of size 90,000 (same size as that of the Twitter data) and having power-law coef-fi cient of 2.54 is generated using BA model . We use BA model because, it is a widely accepted model for social network analysis due to its scale free properties which is also there in our Twitter graph and it is easy to derive the metrics for this model without simulation (using mean-fi eld equation). We have used NetworkX [12] for creating networks for both the Synthetic as well as the Twitter data. NetworkX is a Python package for creation, manipulation, and study of the structure, dynamics, and functions of complex networks. 4.2. Delayed start model
To get a preliminary idea about the ef fi cacy of the Delayed start model, we looked at the behavior of the rumor process for the Twitter graph as well as the Synthetic graph. The results for the Delayed start model with a delay time of 40 for both the Twitter graph and the Synthetic graph are shown in Fig. 5. Growth of the infected nodes are similar for both the graph.

The key thing to note in Fig. 5 is the presence of a single point of decline for both networks. The growth of number of infected nodes, I ( t ) , is slow to start with but as the time goes the I ( t ) increase very fast. Initially there are few nodes who believe the rumor but as the number of nodes who believe the rumor grows, the pr obability of accepting a rumor for a susceptible node increases which makes the rate of growth rise. This is because with the increase of infected nodes, the number infected neighbors of the susceptible nodes also increase. So, the proba bility of accepting a rumor incr eases. After certain time point the number of infected nodes start to decline, that point is called the point of decline P ( G ) . Beyond this point, not only does the rumor-affected population not grow it declines very rapidly, because the anti-rumor process is growing inside and outside the rumor process using the neighbors links. Once the anti-rumor process out-perform rumor process, in the succeeding steps the number of susceptible neighbors of the rumor accepted nodes decreases, which makes the growth reduce further. So after the point of decline, it requires very little time to completely remove the rumor. In combating rumor strategies, the point of decline is an important parameter. A detailed study of this parameter is presented in the succeeding sections.

The sharp behavior noted above is further reinforced by studying the ratio of the number of anti-rumor accepted nodes to the number of rumor accepted nodes, i.e., C ( t )+ V ( t ) to I ( t ) (Figs 6(a) and 6(b)). when all rumor infected nodes are cured. After the anti-rumor process starts, we see a sharp growth in this ratio, because the an ti-rumor process sta rts killing the rumo r process which breaks the growth of rumor process. However, in the Twitter we can observe that for higher values of delay time, after initial spurt the growth of the ratio slowsdown. The reasons for this behavior may be stated as, when the delay time increases the number of infected nodes incr eases and in the Twitter graph some infected nodes remain infected for longe r period of time, because a lot of nodes in the Twitter graph have in-degree 1. These nodes get less chance to be cured than a higher in-degree node. Finally, Fig. 6(c) shows the results obtained for the metric by solving mean fi eld equations. The analytical solution in this case matches not only in trend with the simulation results but the actual values are very close. We would like to point that the trends in the Twitter graph very closely match with the trends shown in the mean fi eld solutions. Next, we study how average infected time, A ( G ) , varies with the delay time. The results are shown in Fig. 7(a). The trend is very intuitive, as the time to start anti-rumor process increases, A ( G ) increases sub-linearly but for the higher value of delay the average infected time grows much faster. This is be-cause, the rumor accepted nodes remain infected for a longer period and also get more chances to infect other nodes, hence the delay time is an important parameter for controlling rumor. There is hardly any difference between the Twitter data and the Synthetic data. The A ( G ) values of the Twitter graph are slightly more than that of the Synthetic graph, wh ich indicates that in the Twitter graph some nodes remain infected for a longer period of time because of the lower in-degrees of these nodes. We remind the readers that both our graphs, the Twitter graph and the Synthetic graph, follow power-law degree distributions with power-law coef fi cients 2 . 34 and 2 . 54 respectively. Being a real graph, the Twitter graph contains a lot of variation in the nodes degree. The maximum infected time, M ( G ) , (Fig. 7(b)) grows very slowly. This implies that the anti-rumor is able to arrest the rumor effectively in the sense that no single node X  X  duration of believing the rumor is disproportionately large because of the increase in delay, even though the entire populations average belief time does get affected signi fi cantly by delay in starting the anti rumor. Figure 7(d) present the change in point of decline, P ( G ) , with increase in delay. It is evident that for smaller values of delay, P ( G ) increases almost linearly, i.e, the time taken to tackle the rumor depends linearly on the delay. However, for larger values of delay, P ( G ) increases but the increase is very slow because for higher delay the rumor spreads through the entire network and then stops because there are no nodes left to infect, so when the anti-rumor process starts it is the only process which is running. Therefore tracking this metric for higher values of delay does not make sense. Figure 7(c) shows the trend in out-break size ( R ( G ) ). We can see a sharp behavior for smaller values of delay. i.e., a small increase in the delay time will cause a exponential rise in the infection, which shows the importance of delay time in the combating process. Finally, Figs 7(e) and 7(f) shows the results for P ( G ) and R ( G ) for mean fi eld model. The mean fi eld model conforms to the simulation observations for the Synthetic as well as the Twitter graph. Therefore, the lifetime metrics can be studied for large scale graph with only mean fi eld equations.

We know that the out-break size, R ( G ) , represents the total number of infected nodes in the rumor captures the growth of the rumor relative to its overall reach. The values of 1 at time t implies that, the rumor has attained the maximum strength at t . In Fig. 8, we have shown the results for this ratio. In Fig. 8(a), we can see that even if both rumor and anti-rumor process start at same time, at least 70% of the total out-break size, R ( G ) is found to be infected at a particular period of time. In other words, this implies that even after the rumor begins declining, it is still able to infect al most one-third as many nodes as it did when it was on the rise. If the ratio touches 1 then the anti-rumor process is insigni fi cant, since all nodes are already infected. These curves also show the single point of decline. Comparing the results of the Twitter data and the Synthetic data we observe that the maximum value of the ratio increases with increase in delay time in both case but the maximum values occur earlier in the Twitter graph compared to the Synthetic graph which again s upports the fas ter spread in the Twitter graph. Figure 8(c) presents the results of the mean fi eld model. The overall trend for different delay factor matches with the Twitter as well as the Synthetic graph trends. However, in this result we can see that the maximum value of the ratio is less compared to the Twitter graph as well as to the Synthetic graph. The reason is that the mean fi eld is an approximation and the approximation is closer to the original solution for larger values of the scale parameter i.e. the number of nodes. 4.3. Beacon model
We start with studying the growth of infected nodes I ( t ) . The results for synthetic and real graph are shown in Fig. 9. In this case we have used only 10 beacons. As in the Delayed start model, a single point of decline is also seen in the Beacon model. In Fig. 9, we can see that the growth of I ( t ) is initially slow to start with but after certain point it grows exponentially. Because as the number of infected nodes increase, the proba bility of accepting a rumor also increase. That m eans, if the beacons are able to detect the rumor early then we can control the rumor easily. We also observe that after the point of decline there is sharp decline in the growth of infected nodes. That is, once the anti-rumor process gets hold of rumor process, it decimates it quickly. The growth of the rumor process for the Twitter graph and the Synthetic graph are almost same, but in the Twitter graph growth rate is slightly higher than the Synthetic graph. Similar to the Delayed start model we have looked at the values of S ( t ) , C ( t ) and V ( t ) together with I ( one process (rumor) until the beacon(s) get activated, after that the ratio starts to increase. When the ratio approaches 1, then there is true competition, since the number of rumor and anti-rumor spreaders are same. Hence, from that point the increase in ratio slowsdown. The bend in the upper part of the curve in Fig. 10(a) suggests that there are a few nodes in the Twitter network which remain infected for a long period of time. The variation of the ratio in the Twitter graph Fig. 10(a) is almost similar to that of the Synthetic graph Fig. 10(b), but growth of the ratio is slightly higher in the Twitter graph. The increase in the ratio may be due to decrease in the number of infected nodes (oppositely increase in the number of cured node). Figure 10(c) shows the results derived using the mean fi eld equations. It is interesting to note that, the mean fi eld solutions not only match the simulation results for the Synthetic graph but are quite close to the results for the Twitter graph. The small deviation from the mean fi eld solution is due to noisy real world data, but looking at the overall trends the mean fi eld equations provide good approximations.
 Lifetime metrics for the Beacon model are shown in Fig. 11. We start with average infected time A ( G ) shown in Fig. 11(a). The A ( G ) value decreases as the number of beacons increase, but the de-crease is slower for a higher number of beacons. This observation can be explained as follows: the anti-rumor process begins by the beacon nodes, when a beacon is activated the number of nodes who accept the anti-rumor grows centered around that beacon node, which results in a component formed by that beacon node. As the number of beacons increase these components start overlapping which make the effectiveness of some of the beacons to reduce. The Synthetic graph also shows a similar trend as the Twitter graph, but the A ( G ) values are slightly higher in the Synthetic graph because of relatively slower growth process in the Synthetic graph which is already seen in Fig. 9. The maximum infected times, M ( G ) , for different number of beacons are shown in Fig. 11(b). The maximum time of infection displays a gentler trend. As the number of beacons increases the M ( G ) values decreases but slowly, which means even though we start the anti-rumor process early by planting more number of beacons, there are some nodes which remain infected for a l onger period of time. Both the Twitter graph and the Synthetic graph follow similar trends. Almost similar results were observed for the other two metrics point of decline R ( G ) and out-break size P ( G ) (Figs 11(c) and 11(d)). Both P ( G ) and R ( G ) values decrease with increase in number of beacons. Overall trends between the Twitter graph and the Synthetic graph are similar but slightly higher P ( G ) values and lower R ( G ) values are observed for the Synthetic graph. This further added to the slower growth process in the Synthetic graph. The results for the P ( G ) and R ( G ) values of the mean fi eld equations are shown in Figs 11(e) and 11(f). The overall trend of mean fi eld model matches with the Synthetic as well as the Twitter data. The results show that, after a sharp decrease in the values as the number of beacons increase the curve is almost fl at, which suggest that there is some upper bound on the use of number of beacons in the combating process after that increasing the number of beacons does not contributes a signi fi cant change in the results. although the beacons sit on the boundary of the rumor X  X  spread (by their very de fi nition), they do not effectively encircle the rumor: even after the fraction starts decline, a large number of nodes do get infected. In fact in the Twitter graph, Fig. 12 (a ), around 40% of the total number of infected nodes infect after the decline starts (for the cases of 50-beacons and 100 beacons). This metric is insigni fi cant for the cases where the ratio is close to 1. The results for the Synthetic network Fig. 12(b) are almost similar to that of the Twitter network. The mean fi eld solutions for higher number of beacons Fig. 12(c) are not only matches with simulation using the Synthetic network but also it is very close to the Twitter network as well. 4.4. Beacon vs delayed start model
Next we compare the performance of the Beacon model with the Delayed start model. The Delayed start model is a reactive model and the Beacon model is a proactive model. In the Delayed start model we fi x a time when the anti-rumor process begins whereas in the Beacon model, we fi x the locations of the beacons but cannot know for sure when they are going to be activated. We have found that average time for the beacon (in case of 1-Beacon model) to activat e is close to 45 for the Twitter graph and close to 60 for the Synthetic graphs. Therefore, it is meaningful to compare the 1-Beacon model and the Delayed start model with delay time 45 for the Twitter graph and with delay time 60 for the Synthetic graph. We observe that the out-break size, R ( G ) , value for the 1-Beacon model (Fig. 11(c)) is around 68,000 whereas for the Delayed start model (having delay time 45) (Fig. 7(c)) the value is much higher than 68,000 (in fact it is close to 82,000). Similarly, the average infected time, A ( G ) , in case of the Beacon model is also less than the Delayed start model. These observations can be explained as follows: a beacon is activated on the way of the rumor growth process, i.e., the anti-rumor process starts at the edge of rumor growth process and hence it will limits the growth of rumor process. In case of the Delayed start model, the anti-rumor process begins at any infected node after a particular delay time. The neighbors of that node may be infected but not effective, because probably they could not able to further spread the rumor as their neighbor are already infected. Therefore from these experimental observations we may conclude that the Beacon model is able to combat the rumor more effectively. Similar results are obtained for the Synthetic data (comparing the 1-Beacon model and the Delayed start model with delay time 60). Examining the plots for the mean-fi eld solution Figs 11(e) and 11(f) for the Beacon model and the Delayed start model Figs 7(e) and 7(f) we observed that in the Delayed start model as the delay time increases, the point of decline, P ( G ) , values increase linearly but not so sharp behavior seen in the Beacon model. Similarly, the sharp behavior in out-break size, R ( G ) , for the Delayed start model is not seen in the Beacon model. From these observations, we can conclude that the Beacon model is more effective than the Delayed start model under similar settings. 4.5. Neighborhood model
In the Neighborhood model, a user may detect a message as rumor with some probability while re-ceiving it and decide to warn its neighbors about the spread of rumor. If we compare this to the Beacon model it is as if a node decides to be a beacon rather being decided a priori. We can compare the Beacon model with b beacons with the Neighborhood model with probability of detection of rumor as b n where n is the size of the graph.

First, we study the growth of infected nodes for bot h the Twitter and the Synthetic graph. The graphs are shown in Fig. 13. In this case we consider the probability of detecting the rumor is 10 90000 ,whichis equivalent to the Beacon model with 10 beacons. The overall growth process is similar to that of the other two models, i.e., there is single point of decline and there is also a sharp behavior after the point of decline. The rate of growth for the Twitter graph is faster than the Synthetic graph. Comparing this result with that of the Beacon model Fig. 9, we fi nd that rate of rumor growth in both the models is almost similar and in fact slightly lower for the Neighborhood model. This is quite interesting because, in the Beacon model we required s ome authorities to sel ect the beacons nodes but for the Neighborhood model there are no authorities involved. The slight improvement in the Neighborhood model is observed because higher degree nodes are more likely to become beacons and in social network the higher degree nodes are the most in fl uential node in term of information spread [8]. On the other hand in the Beacon model there is no control over the selection of the beacon nodes. Therefore, from these experimental observation we may conclude that the Neighborhood model is more ef fi cient in arresting rumor. similar to the Beacon model. Initia lly, the ratio is zero because there i s only one process (rumor) until any node(s) refute the rumor, after that the ratio starts to increase. Comparing these results with the results obtained by the Beacon model, Fig. 11, we have found that the time required to activate a beacon is slightly greater than the time required by the fi rst node to refute the rumor. The growth rate of the ratio is much faster initially but slowdown later in the Twitter graph, Fig. 14(a), which is not observed with the Synthetic graph, Fig. 14(b). This may be due to the noise in real data. A lot of nodes in the Twitter graph are having in-degree 1. Therefore, these nodes take more time to be cured. The mean fi eld solution, Fig. 14(c), is similar to the solution obtained by simulation.

Next, we study the behavior of the life time metrics. The results are shown in Fig. 15. The average infected time A ( G ) , Fig. 15(a), and the maximum infected time M ( G ) , Fig. 15(b), decrease very slowly with the increase in refutation probability. By comparing the results of the Twitter graph with the Syn-thetic graph we observe that, the A ( G ) values of the Twitter graph are less than that of the Synthetic graph whereas the M ( G ) values show the completely opposite behavior. This may be explained as: in the Twitter graph both rumor and anti-rumor spread very fast. Therefore, even when a larger number of nodes are infected, the infection lasts for a very short duration of time compared to the Synthetic graph. However, there are some nodes in the Twitter network which are very loosely connected to other part of the network. When such nodes get infected, this take a long time to be cured which increases the maximum infected time. Comparing these results with the results obtained by the Beacon model Figs 11(a) and 11(b), we fi nd that both the models give similar results. Similar observation can be made for the point of decline P ( G ) metric in Fig. 15(d). The P ( G ) values decrease slowly and the value is greater for the Synthetic graph than for the Twitter graph. The outbreak size, R ( G ) in Fig. 15(c) of both the graphs are quite close and decrease with the increase in the refutation probability. The mean fi eld solutions Figs 15(e) and 15(f) match the simulation quite closely.
 The last property we study is the ratio between number of infected, I ( t ) and out-break size, R ( G ) . The results for both the Twitter graph and the Synthetic graph are shown in Fig. 16. As we can see, start model (Fig. 8). However, by looking at these results closely we can see slightly lower values for the Neighborhood model. The reason for this is already discussed previously, i.e., the higher degree nodes are more likely to become beacon and also in the Neighborhood model the anti-rumor process is growing inside the region of the rumor process. The solutions of mean fi eldareshowninFig.16(c) which resembles very closely with the Synthetic as well as the Twitter graph for higher values of refute probability. 4.6. Observations
In this article, we study three natural anti-rumor processes to counter the rumor process. Simulating in a real graph (derived from Twitter) as well as synthetic graph (generated using Barab X si-Albert model), we study the temporal evolution as well as global properties of these anti-rumor processes. We have also presented mean fi eld equations that characterize the system. In all the three models, we observe a sharp growth in the rumor process after a slow start. However, once the growth of rumor starts decline, within a very short period of time the rumor is completely removed from the network. This observation suggest that, once we detect the rumor (no matter in which ways) due to fast growth power of social networks we can able to conquer the rumor. However, the life time metrics behave differently for different models. In the Delayed start model, we fi nd the point of decline P ( G ) grows linearly and the out-break size R ( G ) grows exponentially with delay time, but the other two models show different behaviors. In the Beacon model we observe that, the P ( G ) and R ( G ) values decrease very slowly as the number of beacons increase. Because, when a beacon is activated the number of nodes who accept the anti-rumor grows centered around that beacon node, which results in a component formed by that beacon node. As the number of beacons increase these components start overlapping which make the effectiveness of some of the beacons to reduce. Comparing the results of 1-Beacon model with the Delayed start model with delay 45 (average time for the beacon to active) we fi nd that the average infected time A ( G ) , the out-break size R ( G ) and the point of decline p ( G ) values are less for the 1-Beacon model compared to the values for the Delayed start model with delay 45. The reasons for this can explained as: in the Delayed start model, the anti-rumor process starts from an infected node and that node may have already been infected for a long period of time. Therefore, the node lies in the region of infected nodes. Therefore, by starting the anti-rumor process from this node may decrease the number of infected nodes but not able to contain the rumor process by vaccinating susceptible node. In the Beacon model, the beacon nodes activate themselves while receiving the rumor, i.e., the anti-rumor process actually starts at the edge of rumor process. Therefore, the beacon nodes can effectively contain rumor by vaccinating susceptible nodes as well as curing infected nodes. In the Neighborhood model similar results are obtained, i.e. the P ( G ) and R ( G ) values decrease very slowly as the number of refutation probability increases. 4.7. The beacon model versus the neighborhood model
The Beacon model and the Neighborhood model show similar results and, even sometimes the Neigh-borhood model performs better than the Beacon model. The slight improvement may be because, in the Neighborhood model, a higher deg ree node has greater probability of becoming a beacon. However, there is a major qualitative difference between the t wo of them. Even though the results for both the mod-els are same, we strongly believe that in large social networking sites the Neighborhood model is most natural and effective way to combat rumor. The reason is that the Beacon model is an authority based model. It requires a conf ederation of authorities t o select beacon nodes wher eas, the beacons are self created in the Neighborhood model because it is a non-authority based model. Selecting beacon nodes is challenging. By choosing highly reputed nodes as the beacon nodes (like government agencies) might not work, because people may thought these agencies are biased towards Government. Highly trustwor-thy independent actors are unlikely to be convinced by an authority to become beacons since this would mean that they effectively act as government agents. On a more practical note, rumors can emerge on any topic and a single beacon may not be an in fl uential actor in all different fi elds. Rumors regarding different areas of interest may follow different trajectories and so a single set of beacons may not be effective across the board. The neighborhood model is more likely to succeed in this topic-differentiated scenario, with different beacons emerging for different topics.

Essentially our conclusions can be read as follows: An enlightened citizenry is as effective, if not more, at combating rumors as a pro-active approach which is harder to implement and faces several practical obstacles. The takeaway from this conclusion could be a program to educate social networking users about the dangers of rumor spread and encouraging them to work to stem the spread of rumors. 5. Conclusions The main contribution of this paper is to study various anti-rumor strategies in online social networks. The guiding insight of our work is that since social networks span multiple nations with no governing authority, the only way that rumors can be quelled is by using the power of social relationships. In other words we study processes which are by nature decentralized and work in same fashion as rumor. We have studied a reactive situation where there is a time lag in the detection of rumor and a local authority attempts to stop the rumor by starting an anti-rumor once the rumor is detected. We found that the time lag is an important parameter. We also studied a proactive situation where beacons embedded in the network detect and fi ght rumor. Further, we studied the situation where individual citizens attempt to fi ght the rumor, no role of any authority is assumed and found that this way of fi ghting rumor is the most effective among other models. We believe our work is a fi rst step in the direction of studying the ef fi cacy of natural processes that can be employed to fi ght the spread of rumors in social networks, and may, by virtue of their distributed and organic nature succeed where authoritarian strategies have clearly failed. 6. Future work
The models we have studied are simple in conception and can be extended by including parameters such as trust, reputation and context. Trust and reputation vary with context. For example, the dynamics of rumors related to Hollywood news are completely different to those of rumors related to politics. An user A who trusts user B when it comes to gossip about movie stars may not look to the same user when it comes to political news. The key challenges in this case are to differentiate contexts and to learn trust and reputation parameters from a real-world data set. This is a challenging data mining problem in itself.
Another direction to improve this work is to focus on the dif fi culty in differentiating rumor from anti-rumor. Modeling this dif fi culty requires a deep grasp of the sociological aspects of rumor. We visualize a research project on these lines to be an interdisciplinary work wherein the expertise of social scientists, data mining specialists and mathematicians can come together to build on the foundation we have tried to present here.

Another line of work that can build on our work would involve incorporating malicious behavior from users. There are as many models of malicious behavior as there are malicious minds. We discuss one possible model. Suppose a more rumor spreader detects the anti-rumor and start to spread a new rumor message to tackle the anti-rumor message. For example, let us take a rumor about polio vaccination:  X  X olio vaccination kills ch ild X  and it X  X  corresponding anti-rumor messa ge be  X  X overnment declares that not a single child has died in polio vaccination and it is safe. X  A malicious user may counteract this by saying  X  X overnment under pressure from WHO to complete polio vaccination program. X  This example is fi ctitious but it illustrates the point that malicious users can modify a rumor to overcome an anti-rumor, in which case a stronger anti-rumor may need to spread to kill the rumor.

In conclusion, the work in this paper is presented as a starting point for a larger, more wide-ranging investigation into the dynamics of combating rumor with anti-rumor messages. Several further directions remain to be explored.
 References
