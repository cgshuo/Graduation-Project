 Department of Computer Science &amp; Automation, Indian Institute of Science, Bang alore, India can be vie wed as a way of accelerating learning by trading bias for variance. Combining them with bootstrapping is a promising avenue toward a more effecti ve method. Another approach to speeding up polic y gradient algorithms was proposed by Kakade (2002) and then rened and extended by Bagnell and Schneider (2003) and by Peters et al. (2003). The idea transforming the gradient using the inverse Fisher information matrix of the polic y. a measure of the TD error inherent in the function approximation. Due to space limitations, we use of eligibility traces but we belie ve the extension to that case would be straightforw ard. are denoted s probabilities p ( s 0 j s; a ) = Pr( s using a randomized stationary polic y ( a j s ) = Pr( a (B1) The Mark ov chain induced by any policy is irr educible and aperiodic. The long-term average reward per step under polic y is dened as dened under (B1). Our aim is to nd a polic y that maximizes the average reward, i.e., under polic y are dened as 1 In polic y gradient methods, we dene a class of parameterized stochastic policies f ( j s ; ) ; s 2 S ; 2 g , estimate the gradient of the average reward with respect to the can be written as J ( ) , d ( ; ) , V ( ; ) , and Q ( ; ; ) , respecti vely . We assume par ameter s .
 Observ e that if b ( s ) is any given function of s (also called a baseline ), then and thus, for any baseline b ( s ) , the gradient of the average reward can be written as (Greensmith et al., 2004).
 Fisher information matrix G ( ) is positi ve denite and symmetric, and is given by Sutton et al. (2000) sho wed that if the approximation ^ Q r for parameter value w , then we can replace Q with ^ Q a linear approximation ^ Q (B2). The Fisher information matrix of Eq. 4 can be written using the compatible features as Suppose E ( w ) denotes the mean squared error Let w = arg min is minimized if the baseline is chosen to be the state-v alue function itself. 3 Lemma 1 The optimum weight par ameter w for any given (policy ) satisfies 4 Pr oof Note that Equating the abo ve to zero, one obtains The last term on the right-hand side equals zero because P for any state s . No w, from Eq. 8, the Hessian r 2 The claim follo ws because G ( ) is positi ve denite for any .
 Ne xt, given the optimum weight parameter w , we obtain the minimum variance baseline in function of the baseline b , and obtain b = arg min Lemma 2 For any given policy , the minimum variance baseline b ( s ) in the action-value-function estimator corr esponds to the state-value function V ( s ) .
 Pr oof For any s 2 S , let E ;s ( w ) = P Then E ( w ) = P Equating the abo ve to zero, we obtain The rightmost term equals zero because P From Lemmas 1 and 2, w &gt; ( s; a ) is a least-squared optimal parametric representation for Q ( s; a ) . Ho we ver, because E a [ w &gt; ( s; a )] = action-v alue function.
 The TD error tively . Thus, these estimates satisfy E [ ^ V ( s t 0 . The next lemma sho ws that t is a consistent estimate of the adv antage function A . Lemma 3 Under given policy , we have E [ Pr oof Note that No w
Also r ( s By setting the baseline b ( s ) equal to the value function V ( s ) , Eq. 3 can be written as r J ( ) = of the adv antage function A ( s; a ) . Thus, d r J ( ) = Ho we ver, calculating tion. While an average reward estimate is simple enough to obtain given the single-stage reward TD algorithm. In our algorithms, we use for the TD error , where v Let V ( s ) = P estimate of the value function V ( s 0 ) that is obtained upon con vergence viz., lim probability one. Also, let a stationary estimate of the TD error with function approximation under polic y . Lemma 4 E [ Note that E [ (as was considered in Lemma 3). For the case with function approximation that we study , from Lemma 4, the quantity P assumptions (B1) and (B2), we mak e the follo wing assumption: (B3) The step-size schedules for the critic f As a consequence of Eq. 9, than the actor beyond some t 1: Input: 2: Initialization: 3: for t = 0 ; 1 ; 2 ; : : : do 4: Execution: 5: Average Reward Update: ^ J 6: TD err or: 7: Critic Update: algorithm specic (see the text) 8: Actor Update: algorithm specic (see the text) 9 : endf or 10: retur n Polic y and value-function parameters ; v We now present the critic and the actor updates of our four AC algorithms. Algorithm 1 (Regular -Gradient AC): in the number of polic y and value-function parameters.
 The next algorithm is based on the natural-gradient estimate ~ r J ( mating G 1 ( ) and sho w in Lemma 5 that our estimate G 1 matrix can be estimated in an online manner as G obtain recursi vely G Using the Sherman-Morrison matrix inversion lemma, one obtains (B4) The iter ates G Lemma 5 For any given par ameter ; G 1 probability one .
 Pr oof It is easy to see from Eq. 10 that G any given held x ed. For a x ed , by assumption (B4). The claim follo ws.
 Our second algorithm stores a matrix G 1 and two parameter vectors and v . Its per time-number of polic y parameters.
 Algorithm 2 (Natural-Gradient AC with Fisher Inf ormation Matrix): with the estimate of the inverse Fisher information matrix updated according to Eq. 11. We let 0 = k I matrices. From Eq. 10, G are con vex combinations of positi ve denite and symmetric matrices. Hence, G 1 positi ve denite and symmetric as well.
 next algorithm we tune the parameters w in such a way as to minimize an estimate of the least-squared error E ( w ) = E is thus r \ r w E ( w ) = 2[ ( s t ; a t ) ( s t ; a t ) &gt; w t ( s t ; a t )] (2005), we use the natural gradient estimate ~ r J ( in the number of value-function parameters and quadratic in the number of polic y parameters. Algorithm 3 (Natural-Gradient AC with Adv antage Parameters): Although an estimate of G 1 ( ) is not explicitly computed and used in Algorithm 3, the con-(as in Algorithm 2), and use it in the critic update for w . The overall scheme is again seen to follo w the direction of the natural gradient of average reward. Here, we let ~ r ber of value-function parameters and quadratic in the number of polic y parameters. Algorithm 4 (Natural-Gradient AC with Adv antage Parameters and Fisher Inf ormation Matrix): where the estimate of the inverse Fisher information matrix is updated according to Eq. 11. con vergence results are necessarily weak er, as indicated by the follo wing theorem. Theor em 1 For the par ameter iter ations in Algorithms 1-4, 5 we have ( ^ J f ( J ( ) ; v ; ) j 2 Zg as t ! 1 with probability one , wher e the set Z corr esponds to the set of local maxima of a performance function whose gradient is E [ For the proof of this theorem, please refer to Section 6 (Con vergence Analysis) of the ex-tended version of this paper (Bhatnag ar et al., 2007). This theorem indicates that the polic y and state-v alue-function parameters con verge to a local maximum of a performance function that corresponds to the average reward plus a measure of the TD error inherent in the function approximation. close enough to one such that when a TD ( ) critic is used, one gets lim inf (ODE) based approach for our con vergence analysis. Though we use marting ale arguments in our scheme can be vie wed as an Euler -discretization of the associated ODE. polic y is changing, and our proof techniques do not immediately extend to this case. We have introduced and analyzed four AC algorithms utilizing both linear function approximation and than Konda' s algorithm. All our algorithms performed better than Konda' s algorithm. There are a number of ways in which our results are limited and suggest future work. 1) It is
