 The problem of intelligently acquiring missing input infor-mation given a limited number of queries to enhance clas-sification performance has gained substantial interest in the last decade or so. This is primarily due to the emergence of the targeted advertising industry, which is trying to best match products to its potential consumer base in the ab-sence of complete consumer profile information. In this pa-per, we propose a novel active feature acquisition technique, to tackle this problem of instance completion prevalent in these domains. We show theoretically that our technique is optimal given the current classifier and derive a probabilistic lower bound on the error reduction achieved with our tech-nique. We also show that a simplification of our technique is equivalent to the expected utility approach, which is one of the most sophisticated solutions for this problem in existing literature. We then demonstrate the efficacy of our approach through experiments on real data. Finally, we show that our technique can be easily extended to the scenario where we have a cost matrix associated with acquiring missing infor-mation for each instance or instance-feature combinations. H.2.8 [ Knowledge Management ]: Data Mining Classification Classification, Instance completion, Missing values
In this day and age of customization and personaliza-tion of products, targeted advertising has gained prominence with the emergence of many start-ups focused solely on se-lecting the right consumer base that will buy certain prod-ucts. One of the main challenges faced here is having to deal with incomplete customer information. Hence, while you might be able to build classification models by asso-ciating available demographic features to consumer buying decisions, such classification models are usually far from per-fect. For example, we may not know the age and/or gender and/or address of some customers, which might be critical in determining their preference for a certain product. The goal then is to improve our understanding of their behaviors by obtaining this missing information. One way in which this is tackled is by identifying a subset of individuals to target so as to maximize the improvement in classification keep-ing in mind the constraints associated with obtaining such information (budget costs, number of calls you can make, etc.). This problem is not just limited to the targeted ad-vertising industry, but is also relevant to institutions such as government agencies who conduct surveys to understand social tendencies or behaviors. In such cases identifying and contacting the right people can lead to greatly improved understanding of relevant social dynamics at a manageable cost.

More formally, the problem addressed in this paper can be stated as follows: Given that we can query k instances from a dataset of size N having incomplete input informa-tion but known outputs, which instances when queried would maximize the classification performance? In the application domains described before, this would correspond to choosing k people to call that will most likely significantly enhance classification accuracy having obtained this additional in-formation. This problem has been introduced before in [12] and is commonly referred to as instance completion [13, 7, 19].

It is easy to see that this problem setting is quite dif-ferent from the traditional active learning problem [16, 2], which involves the complementary problem of missing out-puts with all inputs known with the goal now being to query those outputs that can maximize classification performance. Other related work includes work on budgeted learning [10, 8], where one is trying to find which entry (instance-feature combination) to query given a certain cost constraint. Knowl-edge gradient methods [4] are a different class of methods which are also trying to find the optimal entry to query that will maximize the gain in information. However, both these classes of methods are suited for a different setting than ours. Budgeted learning and knowledge gradient methods are gen-erally modeled as markov decision processes (MDPs), and as mentioned before, are trying to find which entry to query rather than a set of entries comprising of multiple instances. To determine the reward in querying an entry, an optimiza-tion problem has to be solved for each entry using these methods, which can be expensive. Moreover, even if one were to extend these methods in a straightforward manner for instance completion (where we want to choose k instances out of a possible N ), there would be N ! k !( N  X  k )! possible ac-tions 1 for the corresponding MDP to choose from, which would make this framework computationally infeasible.
In our setting, we seek to maximize the classification per-formance on a dataset during model building (at training time). However, there is work on active feature acquisition with the objective of getting the best possible performance during model application (at testing time) [7, 1, 17]. In [7], the authors try to query instances in the test set so as to minimize classification uncertainty, while in [1, 17] the goal is to query test instances so as to select the most important features. Clearly, both these lines of work address a goal different from ours.

The most relevant work that addresses the same problem as ours is [19, 12, 13]. In the work of [19], a method called Goal Oriented Data Acquisition (GODA) is described, which initially builds a model based only on complete instances. It then rebuilds a model for each incomplete instance that is completed based on imputed values, by adding it to the complete instances and choosing those incomplete instances that give maximum improvement in performance. In [12], a method called Error Sampling (ES) is described for pick the k instances to query which works as follows: If there are m misclassified instances with missing values and if m &gt; k , then k of these m instances are randomly picked. Otherwise if m  X  k , then after picking all of these m instances, the remaining k  X  m are randomly chosen from the correctly classified instances based on an uncertainty score [9, 15]. Not only was the accuracy obtained by ES shown to be better than GODA, but GODA was also computationally much more expensive as one has to first impute all missing values and then retrain a classifier as many times as the number of incomplete instances. In [13], the authors provide a method to score each missing entry (instance-feature combination) based on the calculation of Expected Utility (EU). It is easy to adapt this scheme to the task of instance completion by simply adding the expected utilities for the missing values corresponding to each instance and obtain a score.

One of the conceptual deficiencies of ES is that it does not rank instances, especially the misclassified ones, based on any criteria that is directly related to the goal of correctly classifying them, and therefore, most likely to improve classi-fication performance. Even the correctly classified instances are picked based on the classifier margin, which is unrelated to the likelihood that the chosen instance would improve classification upon completion. Addressing this deficiency is the primary motivation behind our technique, and we there-fore propose an approach to rank the incomplete instances by directly calculating the probability of correctly classifying each instance given the current classifier based on a function of all the corresponding missing entries. We compute the probability by taking into account all of missing features for 9 1 ! denotes factorial. an instance together, rather than looking at them indepen-dently. An important fact to notice here is that though there are multiple missing features for each instance, since we are computing a function over all of them, we only need to em-ploy a univariate probability estimation technique instead of a multivariate approach, which makes our method efficient . A further motivation for choosing instances to query with high probability of being correctly classified is that we learn useful actionable information about our data once they are completed. In particular, if we pick instances by our method to query, then after completion they may be correctly or in-correctly classified. If most of them are correctly classified, then we can be reasonably confident that the classifier we have is reliable. If they aren X  X , then that implies that our current data is not representative of the underlying data generation process and that we either need to query many more instances and/or collect additional discriminative fea-tures. On the other hand, if we had picked instances with high misclassification probability to query, which is diamet-ric to our strategy, then in either case we do not learn much in terms of improving performance. This is because, if most of them are incorrectly classified after completion, then that just confirms what we already knew, while if most of them are correctly classified after completion, then that simply tells us that the problem is in all likelihood easier than we thought.

It is also important to note that in the EU method, the expected utility score for an instance is based on marginals as the expected utility is computed independently for each missing entry. Hence, even though the decision based on their method is optimal for an entry given the current clas-sifier, it isn X  X  optimal for the instance. It turns out that computing the expected utility for each entry can be com-putationally intensive and they therefore suggest randomly choosing a subset, which makes the method sub-optimal even when choosing which entries to query. Moreover, their method is restricted to categorical input features. We demon-strate that our strategy is optimal at the instance level, and applies to continuous as well as categorical features. We also show that a simplification of our method restricted to categorical features and assuming independence is in fact equivalent to the EU method.

The rest of the paper is organized as follows. In Section 2, we describe in detail the proposed method and show how it would apply in the context of some commonly used classi-fication methods. In Section 3, we show that our strategy is optimal given the current classifier. We then derive a prob-abilistic lower bound on the error reduction for the optimal classifier on the updated dataset obtained after querying. In Section 4, we describe a simplified univariate version of our method and show that it is equivalent to the EU method when restricted to categorical features. In Section 5, we em-pirically show that our method is robust and outperforms other competing methods under varying levels of label noise. In Section 6, we describe an easy way to extend our method if a cost matrix is available and discuss promising directions for future research.
Before describing our method, we introduce some nota-tion. Let X,Y denote the input and output spaces respec-tively and D N = { ( x 1 ,y 1 ) ,..., ( x N ,y N ) } denote a dataset of size N containing missing input values. Let  X  : X  X  Y be Algorithm 1 The proposed method (JIS) to choose k in-stances to query so as to maximize classification perfor-mance.
 Input: D N , k ,  X  Output: D Q { dataset of k instances to query }
Let Q =  X  if m &gt; k { Number of misclassified and incomplete in-stances is &gt; k . } then else end if
Return D Q the optimal classifier (the classifier with the highest accu-racy) learned over D N . Let D m  X  D N be the set of m  X  N instances having missing information misclassified by  X  ( . ) while D r denote the set of r instances with missing informa-tion correctly classified by  X  ( . ). Let T ( x i ) and M ( x the set of features with known values and missing values for instance x i respectively. Correspondingly, let x T ( x x j denote the set of entries i.e. features in x j , whose values are known for x i and missing for x i respectively.
Algorithm 1 elucidates our strategy for choosing the k instances to query. Similar to some prior studies, we first focus on the misclassified instances because trying to correct them could have a greater impact on the final accuracy than those that are already correctly classified. However, it is strategy of deciding exactly which k instances to pick that is critical to the success of this task and is therefore the novel contribution of our method.

The simplest scenario is where m = k in which case we simply choose all the instances from D m to query.
 Now if m &gt; k , then for each misclassified instance x in D m we compute a function of the missing features and the corresponding set of values S x , such that if the function takes values in the set S x , which implies the missing features would take values in some other set V x , then x gets classified into the correct class by  X  . Essentially what we are trying to find is based on the current inductive function  X  , what set of values for a function of the missing features would result in  X  correctly classifying the instance (the details of how this function is computed for some popular classification techniques is described in the next section).

The criteria f ( M ( x ))  X  S x for any x can be viewed as a sufficient condition where the total error (number of mis-classified instances) reduces at least by 1 for an updated classifier that is trained on D N but now with completed in-stance x (i.e. x = x T ( x ) ). This is explained in detail in the section 3 where we discuss error reduction.

The next step in our algorithm is to compute the proba-bility of this function having values in the corresponding set. This probability signifies the likelihood of the instance to be classified correctly after querying it given the current classi-fier, and can be estimated using common parametric (normal density, exponential, etc.) or non-parametric (histograms, parzen windows [6]) density estimation techniques based on the already known values. One could also use probabilis-tic inequalities such as Markov inequality or Chebyschev X  X  inequality [5] to get fast upper bounds on the desired prob-ability.

An important fact to note here is that even though we can have multiple missing features, since we are directly comput-ing a function over all of them, we simply need to employ a univariate probability estimation on the values that the func-tion takes. This important observation makes the procedure tractable, which would not have been the case if we were to find the appropriate ranges for each missing feature sepa-rately that would correctly classify the instance, and then employ a joint probability estimation procedure.

The probabilities of these functions are used as scores for these instances and using them we query the top k instances with the highest scores. Since we compute the probability of a function of the missing features considering them together rather than analyzing them independently, we refer to our method as Joint Instance Selection (JIS).

In the case where m &lt; k , then we first choose all of D stances to query. For choosing the remaining k  X  m instances from the set D r (instances that are correctly classified hav-ing missing values), the strategy is similar to the previous case. We first compute the function with values for it that will keep the instance correctly classified . We then compute the probability for this function to have those values giving us the score for that instance. We now choose the top k  X  m instances sorted by their scores, which along with D m result in the k instances we choose to query.
In the previous subsection, we discussed our general strat-egy to choose the k instances to query. An integral part of this strategy is to compute a function of the missing features satisfying certain conditions that would result in correctly classifying the instance. In this subsection, we show how this function could be computed for some popular classification techniques.
Support vector machines (SVMs) [18] are one of the most commonly used classification techniques in academia and in-dustry. They are considered to be quite robust and can be used to perform linear and non-linear classification. Non-linear classification is accomplished by employing the kernel trick. If we denote the kernel function by K ( .,. ) and if  X  denotes the dual variable in SVM optimization correspond-ing to instance x i obtained by learning over D N which gives us the classifier  X  , then the correct classification for x encoded in the following well known inequality, where, Y = { 1 ,  X  1 } . Without loss of generality (w.l.o.g.) assume that y i = 1, then the above equation can be written as, The reason the left hand side of the above equation can be written as simply a function of the missing features in x is that, for the other missing values we can substitute the values used while building the classifier  X  (which could be class conditional means, modes, etc.). To demonstrate this, we next look at some commonly used kernel functions and further clarify our approach.
 Linear SVM: In case of Linear SVMs the kernel function is given as, Using the above equation along with Eqn.2 we have, Even though this could be further simplified by moving the terms with T ( x i ) (known values) to the right hand side leav-ing only terms with M ( x i ) on the left, we avoid doing so in order to keep the approach consistent because such further simplifications may no necessarily be possible for other ker-nel functions, as we shall see next.
 SVM with Polynomial Kernel: In case of Polynomial Kernel SVMs the kernel function is given as, Employing this in Eqn.2 we get, Notice that a further simplication to obtain an explicit func-hand side isn X  X  possible. However, this is immaterial as this is still an implicit function of x M ( x i ) i .
 SVM with Radial Basis Kernel: In this case, the kernel func-this and equation 2 we have, Similar to the earlier case, this again is an implicit function
Logistic regression [11] uses the logit link function to relate an input x i to its output Y = { 0 , 1 } as follows, where  X  is the learned parameter vector over D N . x classified into class 1 iff P ( y i = 1)  X  0 . 5. Given this and w.l.o.g. assuming y i = 1 the criteria to classify x i into the correct class is, Further expressing x i in terms of T ( x i ) and M ( x i ) we get, Here we were able to obtain an exclusive function of x M ( x on the left hand side.

These instantiations demonstrate how our strategy can be employed for standard classification techniques. We next proceed to examine the performance of the proposed method in terms of its optimality and bounds on error reduction.
In this section we prove two results. First we show that given the current information (i.e. the current classifier  X  and the input-output), the proposed method is the optimal strategy for minimizing the expected error. We then derive lower bounds on probability of total error reduction in the range of { 1 ,...,min ( m,k ) } .
W.l.o.g. assume that the first m  X  N instances denoted by D m in D N are misclassified by  X  ( . ) and have missing information. If  X  : N X N  X  X  0 , 1 } denotes the classification loss function, then the total error is given by,
The task is to query those k instances which will minimize this error. If m  X  k the best we can hope to reduce the error by is k which would occur if after querying, the k misclassified points get correctly classified. If m &lt; k , then the maximum reduction in error we can expect is m , which would require querying all m of these instances.

Formally, if D s denotes a dataset of size s then, argmin = argmin =
Thus, above we want to query instances that will minimize the error. However, since there is indeterminacy in what the missing values will be, we minimize the expectation of as follows, argmin =
Equation 13 can be interpreted as, when m  X  k the op-timal strategy is to query the k misclassified points that have the highest probability of being correctly classified and when m &lt; k , the optimal strategy is to query all misclas-sified points along with the correctly classified points that have lowest probability of being misclassified. When com-pared with algorithm 1, it is clear that this is exactly the strategy proposed in JIS, thus proving the optimality of our approach.
In this subsection, we derive probabilistic lower bounds on the error reduction of the updated optimal classifier  X  obtained after training on the updated dataset D u N . The up-dated dataset is created from the values obtained by query-ing the k instances chosen using our method.

Let u opt denote total error of  X  u opt on D u N . If u is the total error of  X  on D u N , then u opt  X  u . This is because  X  optimal classifier on D u N , whereas  X  is just one of possible classifiers which the classification algorithm considered dur-ing training. Therefore if was the original error (that is the error of  X  on D N before querying), then the error reductions of respective classifiers have the following relationship, where,  X  u opt =  X  u opt and  X  u =  X  u . This implies that whenever  X  u  X  j , then  X  u opt  X  j . Thus,
We now derive exact formulations for P ( X  u  X  j ) as well as more efficient to compute lower bounds when m  X  k and when m &lt; k . From equation 15, both of these would be the lower bounds for P ( X  u opt  X  j ), which is our goal. Let D p denote a dataset of p instances. Let L D p i and H denote i instances in the dataset D p  X  D N with the lowest and highest scores respectively based on JIS applied to D
If m  X  k , then the P ( X  u  X  j ) where j  X  X  1 ,...,k } , is just the sum of the probabilities of j or more misclassified in-stances getting correctly classified after querying. Formally, where D Q is the query set as defined in algorithm 1.
The exact formulation in the above equation requires sum-ming over all subsets of j instances in D m , which may be expensive. However, obtaining the lower bound simply re-quires multiplying the lowest j scores of the k chosen in-stances with scaling equal to the number of terms in the summation.

The case where m &lt; k is more involved. In this case, the chosen k instances also contain correctly classified instances and therefore we also have to account for the possibility of these instances getting misclassified upon completion. In other words, there are more ways than the previous case to get an error reduction of j or more. In particular, we can get an error reduction of j  X  { 1 ,...,m } (or more) by correctly classifying j (or more) misclassified instances and correctly classifying the k  X  m instances that were already correctly classified. We can also get the same error reduction by misclassifying some (say i ) previously correctly classified instances, but correctly classifying more than i + j misclassi-fied instances. Thus, to get the formulation for P ( X  u  X  j ) we sum up over the possibilities that (i) all the k  X  m cor-rectly classified instances are still correctly classified, (ii) all but one of the k  X  m correctly classified instances are still correctly classified, (iii) all but two of the k  X  m correctly classified instances are still correctly classified and so on. If D k  X  m = D r  X  D Q denotes the correctly classified instances that we want to query, then P ( X  u  X  j ) is given by, P ( X  u  X  j ) = Y +  X   X   X  Y + = m ! + P (  X  ( z ) = y z ) P (  X  ( t ) = y t )
We obtain the lower bound from the above equation in the following manner. The term before the plus sign is lower bounded using the exact same reasoning that we used in equation 16 to lower bound P ( X  u  X  j ) when m  X  k . There are two lower bounding substitutions made after the summation over i . The index i indicates the number of cor-rectly classified instances that could be misclassified after updation. Here we first lower bound P y ).
 with the smallest value is Q which is obtained by considering the misclassification prob-abilities of i instances that have the highest score in D and the probabilities to classify correctly of k  X  m  X  i in-stances that have the lowest score in D rQ k  X  m .

The second lower bounding substitution is made for P D Q equation 16. Here we just choose the lowest i + j scores of the m misclassified instances with scaling equal to the number of terms in the summation.
 Thus from equations 15, 16 and 17 we have,
P ( X  u opt  X  j )  X  if m  X  k ( m  X  j )! j ! otherwise ( m  X  j )! j ! + P (  X  ( z ) = y z ) P (  X  ( t ) = y t )
These equations provide the lower bounds for error re-duction using the proposed approach. For particular appli-cations, the above equations can be computed to quickly get a feel of how effective our querying strategy is likely to be and evaluate if they meet the application needs. If sim-ilar bounds can be computed for other querying methods, one can compare them and choose the appropriate method without empirical studies.
In this section, we first discuss a simplified univariate ver-sion of our method, where we analyze each missing feature for an instance independent of the other missing features. We then show that restricting this method to only categor-ical features makes it equivalent to the EU method of [13].
In algorithm 1, we compute a function f ( . ) over all the missing features. However, it is possible to simplify the approach and compute f ( . ) for each missing feature of an Algorithm 2 Changes to algorithm 1 for IIS during func-tion and score computation. The other steps are identical. The below steps are within the for loops which go over all x  X  D m and over all x  X  D r in algorithm 1. score x = 0 for all A  X  M ( x ) do end for
Q = Q  X  score x instance independently. This could be useful if there is in-sufficient data to accurately estimate the probabilities of the functions over multiple missing features.

In this case the function f ( . ) would be computed similarly as before except that we would compute it for each missing feature assuming the already filled in values for the other missing features for that instance. Thus, for an instance we would have as many functions as there are missing fea-tures. The criteria satisfied by each of these functions would correspond to sufficient conditions to correctly classify the instance given everything else. Hence, the score for an in-stance in this case would be the sum of the probabilities of each of those functions satisfying their respective critera. These updates to algorithm 1 that give us the IIS method are shown in algorithm 2.
In the EU method, given a dataset with categorical vari-ables we find the expected utility for each missing entry (instance-feature value). For example, if A  X  M ( x ) is a missing feature for the instance ( x,y x ) with possible cate-gorical values V A = { a 1 ,a 2 ,...,a g } and x ( A = a j instance created by filling in the missing value of A in x with a , then the utility function U ( . ) is defined as: where  X  ( .,. ) is a 0  X  1 classification loss function. Based on this definition of utility, the expected utility for querying entry x A is given by: Consequently, the score for the instance x can be defined as the sum of the expected utilities of the corresponding missing features,
From equation 18 we can see that the utility function can only have values from the set { 1 , 0 ,  X  1 } . If a misclassified instance gets correctly classified after substitution, then we obtain a 1. If there is no change in terms of being correctly or incorrectly classified, then we get a 0. Whereas if a correctly classified instance gets misclassified after substitution we get a  X  1. Thus, in the case that an instance is initially misclas-sified the value of the utility function for different combi-nations of missing features and corresponding substituted value can only be 1 or 0. This is because for a misclassi-fied instance  X  (  X  ( x ) ,y x ) = 1 and  X  (  X  ( x ( A = a Thus, the utility function is 1 only for those substitutions that result in correctly classifying the instance, else it is 0.
Given this, the expected value of this utility function com-puted according to equation 19 would signify the probability of correct classification if we were to query for the missing value. The final score would be the sum of these probabili-ties as follows,
Similarly, if an instance is correctly classified, then the utility function is  X  1 only for those substitutions that result in misclassifying the instance, else it is 0. Given this, the expected value of this utility function computed according to equation 19 would be the negative of the probability of misclassification if we were to query for the missing value. The final score again would be the sum of these probabilities as follows,
On sorting these scores, we observe that the initially mis-classified instances would always have scores no less than the correctly classified instances because from equations 21 and 22 we see that the scores for the misclassified instances are lower bounded by 0, while the scores for the correctly clas-sified instances are upper bounded by 0. This means that if we were to choose k instances to query and m  X  k , we would choose from the misclassified instances that have the highest chance of being correctly classified based on the sum of the marginals. If m &lt; k , then we would choose all the misclassified instances followed by choosing k  X  m correctly classified instances that have the least chance of being mis-classified. This is precisely what we are doing in IIS. This shows that if we restrict IIS to only categorical features it is equivalent to the EU method.
Using the techniques proposed in this paper, we next em-pirically validated them on three UCI datasets  X  credit ap-proval , adult , and spambase and two e-commerce datasets  X  priceline , and etoys which were used in previous works [14]. We also performed these experiments with varying levels of noise to test the robustness as compared to its competitors. We first describe the setup used in all the experiments fol-lowed by the experimental results and their corresponding observations.
We compared our JIS technique with its univariate ver-sion IIS and with the most sophisticated methods used for this problem in literature namely; Error Sampling and Ex-pected Utility approaches. Since EU requires categorical inputs, in datasets which contain continuous attributes, we first discretize the continuous attributes into 10 equal bins calculated from their range of values and then apply EU.
We performed 10 runs of 10 fold cross validation which is the same experimental methodology as [13], for each of the settings described next. For each instance in a dataset, we randomly set 50% of its input features to be missing. To test robustness of the different methods we introduced label noise, where we randomly changed the labels of  X  percent of the instances. We ran experiments on each dataset for three values of  X  , which are 0% (no noise), 20% and 40%. For each noise level we incrementally completed instances in steps of 10 by querying them based on the decisions of the respective methods with the model being updated after each completion. We report the total number of queried instances correctly classified up until each completion averaged across all runs (with a 90% confidence interval) as the measure of performance. We used SVM with RBF kernel (Weka X  X  im-plementation) in all our experiments. The density estima-tion for JIS and IIS was done using the well known kernel density estimation technique with gaussian kernels.
Figures 1(a), 1(b) and 1(c) show the performance of the four methods on the credit approval dataset at increasing levels of noise. We observe that JIS is consistently superior to all the other methods. In fact, the difference in per-formance becomes especially noticeable as the noise levels increase. From table 1 we see that a similar phenomenon oc-curs on all the other datasets as well, demonstrating that JIS is significantly more robust than its competitors. It also im-plies that as the classification problem gets harder, choosing which instances to complete intelligently based on joint in-formation (as we do in JIS) becomes more critical, than sim-ply picking misclassified instances randomly and correctly classified instances based on classification uncertainty (as is the case with Error Sampling), or picking instances based on marginals (as is the case with both EU and IIS).
Though IIS is better than EU on datasets with continuous attributes, the improvement is not as much as that obtained with JIS. This is not surprising since both IIS and EU are based on marginals and are equivalent when the data is cat-egorical as shown before. The improved performance of IIS can be attributed to the fact that no discretization of con-tinuous attributes is required for IIS, which is a necessary approximation for the applicability of EU.

All three methods JIS, IIS and EU are better than ES, which substantiates our intuition of picking instances to query based directly on their likelihood of being correctly classified instead of relying on other measures.
In this paper we described a novel technique for instance completion for improving classification performance on datasets containing missing input information. We proved that this technique is optimal given the currently available informa-tion. We also proved probabilistic lower bounds on the error reduction of the optimal classifier learned over the updated dataset that is obtained after following our querying proce-dure. We described a simplified univariate version of our method, which when restricted to categorical features was equivalent to the expected utility method. We then empiri-cally demonstrated that our method outperforms other com-peting methods over a wide range of completion percentages and different noise levels.

Previous works have also suggested incorporating a cost matrix C which indicates the cost of querying each entry. However, empirical studies in these works were performed on % complete 10 20 50 90 10 20 50 90 10 20 50 90 methods in the specific scenarios. real datasets where this matrix was not available. Obtaining such a cost matrix in a practical scenario is quite challenging and we therefore did not include it in our problem statement so as to make the exposition clearer. Nevertheless, both the proposed methods JIS and IIS can in fact be easily extended to scenarios where this additional information is available.
If c j i is the cost of querying the attribute j of instance x then the total cost for querying all the missing entries for an instance x i is given by, C i = P j  X  M ( x score for each instance x i obtained from JIS and IIS would just be scaled down by C i . Thus, if score x i was the original score for x i , the new score would be given by,
For the IIS method if we were just trying to find which entries to query, we would scale each marginal probability down by the corresponding cost. However, since we are in-terested in instance completion, where upon selecting an in-stance we query all of its missing features, it is appropriate to scale the score for the instance down by the total instance level cost. This also applies to the EU method, which again would be equivalent to the IIS method restricted to categor-ical features.

It is also useful to note that JIS and IIS techniques can be viewed as two ends of a spectrum where, while the JIS method computes a function over all missing features for an instance, the IIS method computes functions for each of them in isolation. Indeed, we can develop an entire spectrum of methods which compute functions over different sized sub-sets of missing features and then compute probabilties over these functions whose sum then leads to the final score. Such methods could be of interest as different methods in this spectrum might perform superior to the others for different percentages of missing values. The challenge here however, is to efficiently identify the best subsets. This would require further investigation.

From a theoretical standpoint, it would be interesting to study how our querying method compares to a method that would result in the best updated classifier. Note that our method is optimal given the current classifier but it does not guarantee that the classifier trained on the corresponding updated dataset is the best among all possible classifiers that would be produced through other querying mechanisms. In other words, there might be other querying strategies that are better than ours from the point of view of the final up-dated classifier. Even if it turns out that our method isn X  X  optimal from this other perspective, it would be interesting to see how much worse we are (i.e. find the regret). This would probably require assuming some amount of smooth-ness in the learning of the classification algorithm with re-spect to the change in inputs. An alternative strategy would be to restrict the hypothesis space to linear or some other class of functions. While these directions have been of some interest in the area of traditional active learning [3], to the best of our knowledge, there hasn X  X  been any such work re-lated to our problem of instance completion.
 We would like to thank Maytaal Saar-Tsechansky and Prem Melville for providing the e-commerce datasets. We would also like to thank the reviewers and the meta-reviewer for their constructive comments. [1] M. Bilgic and L. Getoor. Voila: Efficient feature-value [2] D. Cohn, L. Atlas, and R. Ladner. Improving [3] S. Dasgupta and J. Langford. Active learning tutorial. [4] P. Frazier. Knowledge-gradient Methods for Statistical [5] G. Grimmett and D. Stirzaker. Probability and [6] T. Hastie, R. Tibshirani, and J. Friedman. The [7] P. Kanani and P. Melville. Prediction-time active [8] A. Kapoor and R. Greiner. Learning and classifying [9] D. Lewis and J. Catlett. Heterogeneous uncertainty [10] D. J. Lizotte and O. Madani. Budgeted learning of [11] P. McCullagh and J. Nelder. Generalized Linear [12] P. Melville, M. Saar-Tsechansky, F. Provost, and [13] P. Melville, M. Saar-Tsechansky, F. Provost, and [14] M. Saar-Tsechansky, P. Melville, and F. Provost. [15] M. Saar-tsechansky and F. Provost. Active sampling [16] B. Settles. Active learning literature survey. Technical [17] V. S. Sheng and C. X. Ling. Feature value acquisition [18] V. Vapnik. Statistical Learning Theory . Wiley &amp; Sons, [19] Z. Zheng and B. Padmanabhan. On active learning for
