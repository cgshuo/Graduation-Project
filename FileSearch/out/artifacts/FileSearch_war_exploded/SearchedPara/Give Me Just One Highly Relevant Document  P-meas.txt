 We introduce an evaluation metric called P-measure for the task of retrieving one highly relevant document .Itmodels user behaviour in practical tasks such as known-item search, and is more stable and sensitive than Reciprocal Rank which cannot handle graded relevance.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Experimentation Keywords: Test Collection, Evaluation Metrics
Different IR tasks require different evaluation metrics: A patent survey task may require a recal l-oriented metric, while a known-item search task may require a precision-oriented one. When we search the Web, we often stop going through the ranked list after finding one good Web page even though the list may contain some more relevant pages, either know-ing or assuming that the rest of the retrieved pages lack novelty. Thus, finding exactly one relevant document with high precision is an important IR task.

Reciprocal Rank (RR) is commonly used for the above task: RR = 0 if the ranked output does not contain a rele-vant document; otherwise, RR =1 /r 1 ,where r 1 is the rank of the retrieved relevant document that is nearest to the top of the list. However, RR cannot distinguish between a re-trieved highly relevant document and a retrieved partial ly relevant document. In light of this, Sakai [3 ]proposed a metric called O-measure for the task of finding one highly relevant document. O-measure is a generalisation of RR, and is a variant of Q-measure which is very highly corre-lated with Average Precision (AveP) but can handle graded relevance [5]. Using well-known methods [1, 6], Sakai [3] showed that O-measure is generally more stable than (and at least as sensitive as) RR, and that finding one highly rele-vant document and finding any one relevant document may require different search strategies.

Both RR and O-measure assume that the user stops ex-amining the ranked list as soon as he finds one relevant doc-ument, even if it is only partially relevant . However, this assumption may not be valid in some IR situations such as known-item search. We therefore propose an evaluation metric called P-measure , which assumes that the user keeps
This is not necessarily a flaw. O-measure models a user who stops examining the ranked list as soon as he finds any relevant document. For example, he may be looking at a plain list of document IDs, or a list of poor-quality text snippets of retrieved documents, from which it is difficult for him to realise that the second document returned by System X is in fact highly relevant. However, if the user is looking for a known (or suspected ) item or if it is easy for him to spot a highly relevant document in the ranked list, then we probably should assess X by considering the fact that it has an S-relevant document at Rank 2.

We now define P-measure . If the system output does not contain a relevant document, P -measure =0. Otherwise, Let the preferred rank r p be the rank of the first record obtained by sorting the system output, using the relevance level as the first key and the rank as the second key. Then: For S y stem X , P -measure =(4+2) / (5 + 2) = 6 / 7. For Y , P -measure = O -measure =(3+1) / (5 + 2) = 4 / 7.
To evaluate P-measure, we used the  X  X tability X  method which examines the stability of system comparisons with re-spect to change in the topic set, and the  X  X wap X  method which estimates the performance difference required to be sure that a system outperforms another [1, 6]. We used two data sets: the NTCIR-5 CLIR Chinese and Japanese runs and test collections with 50 and 47 topics. For each data set, we used top 50 runs as measured by Mean AveP, and 1000 bootstrap samples of topics [4]. Figure 2 shows that P-measure is more stable than RR (and possibly O-measure); Table 1 shows that P-measure is more sensitive than RR (and possibly O-measure). For example, for the Chinese data, P-measure discriminates 39.2% of the 1,225,000 obser-vations (50*49/2 run pairs times 1,000 resampled topic sets) with 95% confidence, while RR discriminates only 28.7%. But even P-measure is not as stable and sensitive as Q-measure and AveP, as it does not examine all relevant doc-uments.

In practice, a document cut-off may be used with P-measure, since P-measure assumes that the user is willing to exam-ine an  X  X nlimited X  number of documents. However, a small cut-off makes IR evaluation unstable, so a large topic set should be used in such a case [1, 5].
 Table 1: Voorhees/Buckley sensitivity (95% confi-dence; NTCIR-5 CLIR Chinese and Japanese).

