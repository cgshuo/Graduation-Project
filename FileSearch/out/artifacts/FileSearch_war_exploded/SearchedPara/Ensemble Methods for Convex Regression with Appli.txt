 Lauren A. Hannah lh140@duke.edu Duke University, Durham, NC 27708 USA David B. Dunson dunson@stat.duke.edu Duke University, Durham, NC 27708 USA Convex regression, which is regression subject to a con-vexity or concavity constraint on the mean function, has received renewed attention. The regression prob-lem is x  X  X   X  R p and y  X  R , where is a mean 0 random variable and f 0 is convex, for every x 1 , x 2  X  X and  X   X  (0 , 1). Regression prob-lems with known convexity or concavity constraints occur in many areas, including economic production, consumption and preference functions (Allon et al., 2007), options pricing (A  X  X t-Sahalia and Duarte, 2003; Hannah and Dunson, 2011), value function approxi-mation in operations research and reinforcement learn-ing (Powell, 2007; Lim, 2010) and device modeling for geometric programming based circuit design in electri-cal engineering (Kim et al., 2004; Roy et al., 2007). Convex regression is particularly promising for the ma-chine learning community as a way to bridge statis-tical estimation and deterministic convex optimiza-tion. In particular, data can be used to estimate con-straints or objective functions for convex optimization problems. For instance, many reinforcement learn-ing problems that involve resource allocation or stor-age have concave value functions. If value function estimates are concave, vector-valued continuous ac-tion spaces can easily be searched. Similarly, geo-metric programming and other deterministic convex optimization problems require known constraint and objective functions. However, in many situations only noisy samples are available; convex regression can be used to generate those functions from samples. Although convex regression has been studied since the 1950 X  X  (Hildreth, 1954), computationally feasible methods for the multivariate setting have only recently been proposed by Magnani and Boyd (2009) and Han-nah and Dunson (2011). Both methods fit a piecewise linear model to the data, ( x 1 ,y 1 ) ,..., ( x n ,y n ) , under a least squares objective function by adaptively par-titioning the dataset. While efficient, the method of Magnani and Boyd (2009) does not always converge; the Convex Adaptive Partitioning (CAP) method of Hannah and Dunson (2011), however, converges, is consistent and has a worst case computational com-plexity of O ( n log( n ) 2 ).
 While piecewise linear methods are computationally efficient, the number of components and hyperplane parameters can be sensitive to training data. More-over, the resulting piecewise linear models may not be appropriate for approximating functions for convex optimization. When a piecewise linear function is used in an optimization setting, it defines a polyhedral con-straint region. When the objective function is linear, a solution lies on a vertex of the polyhedral constraint region. A vertex is created by the intersection of p + 1 hyperplanes. Because all of the parameters have esti-mation error, the location of the vertex can be highly sensitive to training data.
 These problems can be addressed by using ensem-ble methods based on the CAP estimator. Ensem-ble methods combine multiple models to produce a new predictive model. We average over multiple piecewise linear estimators to create a new estima-tor that is less sensitive to individual hyperplane parameters. Efficient estimators can be created by using many traditional ensemble methods, like bag-ging (Breiman, 1996), smearing (Breiman, 2000) and random forests (Breiman, 2001), that maintain the properties of the underlying estimator, like consistency and computational complexity. We compare these methods with CAP and the piecewise linear model of Magnani and Boyd (2009); the ensemble methods have better predictive error and produce functions that are much more stable in an optimization setting.
 We apply ensemble methods to device and constraint modeling for circuit optimization via geometric pro-gramming. Circuits are an interconnection of electrical devices, including capacitors, resistors, inductors and logic gates. Circuit optimization selects appropriate sizes for devices, gates, wires and other design vari-ables, such as threshold and power supply voltage to minimize a given objective like circuit delay or physical area, subject to a set of constraints, usually on area, power, noise or delay. Many circuit design problems can be well modeled by a geometric program (GP), which minimizes a posynomial objective function sub-ject to posynomial inequality and monomial equality constraints. Geometric programming allows the ef-ficient computation of optimal global solutions, even for large problems; see Boyd, Kim, Patil and Horowitz (2005) for a tutorial. However, constraint functions are often not available in a posynomial form and must be approximated from observational data or a known but non-posynomial function for each device. We use ensemble methods to compute device models that are more accurate and more stable than other convex re-gression methods in this setting.
 The contributions of this paper are 1) new ensem-ble methods for convex regression that are resistant to overfitting and produce a better estimator for op-timization than non-ensemble methods, 2) conditions for consistency when CAP is the underlying estima-tor, 3) strong empirical results, and 4) an application to device and constraint modeling for geometric pro-gramming based circuit optimization. 2.1. Convex regression Regression subject to a convexity constraint has been the subject of renewed interest in the past few years. One approach has been approximation by a function with a positive definite Hessian (Roy et al., 2007; Aguilera and Morin, 2009; Yongqiao and He, 2012). These methods generally result in a problem that is solved by a semidefinite program with n semidefinite constraints; solution methods are prohibitively slow for more than 1 or 2 thousand observations. Another approach relies on an alternate definition of convexity: for every x 1 , x 2  X  X , where g 0 ( x )  X   X  X  0 ( x ) is a sub-gradient of f 0 at x . Equation (1) means that a convex function lies above all of its supporting hyperplanes; with enough supporting hyperplanes, f 0 can be ap-proximately reconstructed arbitrarily well by taking the maximum over those hyperplanes.
 The least squares estimator (LSE) directly projects a least squares objective function onto the cone of con-vex functions (Hildreth, 1954), min for i,j = 1 ,...,n . Here,  X  y i and g i are the estimated values of f 0 ( x i ) and the subgradient of f 0 at x spectively. Equation (2) is a quadratic program with O ( n 2 ) constraints and cannot be solved efficiently for more than 1 or 2 thousand observations.
 To combat these computational difficulties, some re-cent methods (Magnani and Boyd, 2009; Aguilera et al., 2011; Hannah and Dunson, 2011) estimate a small set of hyperplanes by approximately solving (  X   X  , X   X  ,K  X  ) = arg min where (  X , X  )  X  R  X  R p defines a hyperplane. The re-gression function  X  f is defined as the maximum over the set of hyperplanes for a convex function, The most computationally efficient methods are given by Magnani and Boyd (2009) and Hannah and Dunson (2011). 2.2. Combining ensemble methods with Ensemble methods reduce overfitting by averaging over a collection of estimates. Here we overview tradi-tional ensemble methods and discuss how they can be combined with convex regression.
 Bagging. Bagging methods (Breiman, 1996) sub-sample the training data with replacement, which acts as a random re-weighting of the training set. We study the situation where M new training sets are created by subsampling n observations, denoted by ( x the function estimates for each subsample, where  X  f x | x ( m ) , y ( m ) is a convex regression estima-tor trained on ( x ( m ) , y ( m ) ). Bagging can be used with both CAP and the method proposed in Magnani and Boyd (2009).
 Smearing. Smearing (Breiman, 2000) works by adding mean zero, i.i.d. noise to the training data responses. These  X  X meared X  datasets are then fit with a regression method and the results are averaged to produce an estimator. M new training response sets, ( y The ensemble estimator is then created by averaging convex estimators for each of the M random training sets as in Equation (3). In Breiman (2000), the noise level was chosen to be 2.5 times the standard devia-tion of the estimator residuals, y  X   X  f ( x ). However, we also consider the situation where it is chosen by cross-validation. Both CAP and the method of Magnani and Boyd (2009) can be used with smearing.
 Random Search Directions. Random forests (Breiman, 2001) are used in tree regres-sion settings; instead of fully exploring each of the subset split directions, a split is generated in a random direction. Since the subsets in both CAP and Magnani and Boyd (2009) are defined by the hyperplane parameters, there is no direct analogy between random forests and these methods. However, we propose a method in the same spirit.
 The partitions of the CAP estimator are created in a two step process. In the first step, subsets are split along cardinal (CAP) or random directions. In the second step, the subsets are redefined by the maximal hyperplanes. Searching over a set of random directions in the first step produces a random estimator. This is done M times and an ensemble estimator is produced by averaging the estimators.
 Boosting. Boosting (Freund and Schapire, 1997; Friedman, 2002) is a popular ensemble method that constructs additive models in a greedy, forward step-wise manner. This exact method is not appropriate for convex regression since the residuals left after fitting a convex function may not maintain convexity. However, methods that iteratively weight a set of basis functions may prove useful for convex regression. Bagging, smearing and random search directions main-tain consistency if CAP is used as the convex esti-mator and a few mild conditions are imposed. Each CAP covariate subset A k has diameter d nk , where d mean for index subset C k as  X  x k = 1 | C x i  X  A k , define Let x 1 ,..., x n be i.i.d. random variables and let a superscript of ( m ) denote that a quantity is asso-ciated with random estimator m = 1 ,...,M . Let  X  f ( x | Z ( m ) ,D n ) be a random estimator based on data D n and random variable Z ( m ) . We make the following assumptions, which are the original CAP conditions for consistency applied to each random dataset:
A1. X is compact and f 0 is Lipschitz continuous and
A3. For m = 1 ,...,M , the diameter of the partition
A4. Let  X  ( m ) k be the smallest eigenvalue of
A5. For m = 1 ,...,M , the number of Proposition 3.1. Suppose that and sup x  X  X   X  f ( x | Z ( m ) ,D n )  X  f 0 ( x )  X  0 in probability. Then for every fixed M , sup x  X  X   X  f avg ( x | D n )  X  f 0 ( x )  X  0 in probability. Proof. By the triangle inequality, The result follows from the assumption for each m . Theorem 3.2. If  X  f ( x ) is generated by the CAP es-timator, ( x ( m ) , y ( m ) ) M m =1 are generated by bagging, E [ Y 2 | X = x ] &lt;  X  a.s. for all x  X  X , and A1. to A5. hold, then for every fixed M , in probability as n  X  X  X  .
 Proof. Because of the bounded second moment and and the result follows from Prop. 3.1.
 Theorem 3.3. If  X  f ( x ) is generated by the CAP es-for some B &lt;  X  , and assumptions A1. through A5. hold, then for every fixed M , in probability as n  X  X  X  .
 Proof. Fix m and consider the estimator  X  so E Since A1. through A5. hold for that estimator, it is consistent and the result follows from Prop. 3.1. Theorem 3.4. If  X  f ( x ) is generated by the CAP esti-mator with random search directions Z ( m ) and A1. to A5. hold, then for every fixed M , in probability as n  X  X  X  .
 Proof. Each estimator is consistent and the result fol-lows from Prop. 3.1. 4.1. Prediction Here x  X  R 5 with X  X  N 5 (0 ,I ). Set We compared CAP, linear fitting (Magnani and Boyd, 2009) (MB), cross-validated smearing (Sm CAP, Sm MB), smearing with 2.5 times residual noise (Sm 2.5 CAP, Sm 2.5 MB), bagging (Bag CAP, Bag MB) and random search directions (RD). All ensemble methods except RD were implemented with CAP and MB. 10 training sets and one testing set were generated; the number of training samples was varied between 100 and 5,000. For Sm CAP and Sm MB, the noise level was chosen by 5-fold cross validation from  X  = where s is the standard deviation of the residuals; each level was approximated with M = 25. Appropriate noise levels were then probabilistically chosen for each m for M = 200. The number of hyperplanes in linear fitting was chosen through 5-fold cross validation. Results are given in Table 2.
 Ensemble methods substantially reduced CAP predic-tion error for all sample sizes except for n = 5 , 000. Smearing with cross-validated noise, random search di-rections and bagging produced similar results. Ensem-ble methods produced smaller reduction in prediction error for linear fitting, with bagging producing the best results. Smearing with 2.5x standard deviation noise produced worse results than the other ensemble meth-ods, likely because the noise levels for cross-validated smearing were lower. 4.2. Optimization Approximating objective functions or constraints for use in convex optimization is one of the most promis-ing applications for convex regression. In this sub-section, we use convex regression for response surface methods in stochastic optimization; see Lim (2010) for an overview. We would like to minimize an unknown function f ( x ) with respect to x given n noisy observa-tions, ( x i ,y i ) n i =1 , where y i = f ( x i , i ), We tested the regression methods with Y = x i Qx T i + i , Q = The constraint set is  X  1  X  x j  X  1 for j = 1 , 2, and x i  X  Unif [  X  1 , 1] 2 . We used the above methods as well as the Least Squares Estimator (LSE). We generated 50 training sets, solved Equation (4) using each re-gression method, and then calculated the root mean squared error (RMSE) for the functional estimators; the number of training data was set at 100 and 500. Solutions to Equation (4) were evaluated with respect to the true function; RMSE was calculated over a grid on the constraint space. Results are given in Table 1. The ensemble methods produced significantly better quality results both in terms of solution selection and RMSE than the competing methods. The extra noise of 2.5x smearing acted as a smoother and produced a more accurate and stable minimum. 5.1. Geometric programming and circuit Geometric programming is a mathematical optimiza-tion problem where the objective function and con-straints are defined in terms of monomials, posyno-mials and generalized posynomials. A monomial g ( x ) and posynomial f ( x ) have the forms g ( x ) = cx a 1 1 x a 2 2 ...x a p p , f ( x ) = for x &gt; 0. A generalized posynomial is created through positive powers, addition, multiplication or the maxi-mum of posynomials. A GP minimizes a generalized posynomial subject to a set of generalized posynomial inequality and monomial equality constraints, where f i are generalized posynomials for i = 0 ,...,m and g j are monomials for j = 1 ,...,k . GPs can be reformulated as convex optimization problems through a change of variables, z i = log( x i ), If we take the log of the transformed function, f ( z ), we get a function that is convex in z .
 Many circuit design problems, both analog and digital, can be modeled as GPs (Kim et al., 2004; Boyd, Kim, Patil and Horowitz, 2005; Roy et al., 2007). Geometric programming offers a fast, global solution method for design problems that scales well even to large prob-lems. To use geometric programming, however, de-vices and constraints need to be modeled by general-ized posynomials. Sometimes device models are not known and need to be inferred from data in standard cell libraries; other times, constraints or device models are given, but not in a form that can be expressed as a generalized posynomial. In each of these cases, piece-wise linear convex regression can be used to produce generalized posynomial representations of these mod-els and constraints. Ensemble methods can produce models that have lower error and are more stable in an optimization setting than existing methods. 5.2. Device and constraint modeling with Functions without explicit generalized posynomial rep-resentation occur in two settings. In the device mod-eling setting, device parameters such as the inverse of transconductance, gate-source voltage, the inverse of output resistance and the intrinsic gate capacitance need to be modeled as generalized posynomial func-tions of input parameters such as device width, length, terminal voltages and drain current. These relation-ships need to be inferred from data generated by cir-cuit simulation or contained in standard cell libraries. Log convexity of such data is not guaranteed and the goal is to find a low error generalized posynomial ap-proximation. In the constraint modeling setting, some constraints, such as the power supply voltage for dif-ferent gates, are known but do not have a generalized posynomial form. In this case, the goal is to create a low error generalized posynomial approximation based on samples from the true function. While posynomials can be directly fit through Gauss-Newton type meth-ods, these methods only reach local optima and are sensitive to algorithm initialization. Piecewise linear convex regression offers a more appealing alternative. Monomials, posynomials and generalized posynomials are closely related to affine (linear) and convex func-tions. Using the transformation, z = log( x ), if a func-function f is a generalized posynomial, then log( f ( e z is convex. Conversely, if log( f ( e z )) is convex, then f can be approximated arbitrarily well by a posynomial, generalized posynomial or the maximum of a set of monomials. We use this fact and piecewise linear con-vex regression to approximate what should be posyno-mial functions by the maximum of a set of monomials. let ( z i ,  X  y i ) n i =1 be the set of transformed observations, z convex model is fit to the transformed data, then a generalized posynomial can be constructed for the original function, In an ensemble setting, the coefficients can either be constructed directly or with dummy variables. 5.3. Power modeling Here we fit a generalized posynomial to a known, but non-posynomial, function, the power dissipated as a function of gate supply and threshold voltages, V dd and V th ; this example is studied in Boyd, Kim, Patil and Horowitz (2005). The total power dissipated for a gate is the sum of the average static power and the dynamic power dissipated. The dynamic power is a function of the gate supply voltage, V dd , where f is the frequency, C int is the intrinsic capaci-tance and C L is the load capacitance. The static leak-age is a function of the supply voltage and the average current leakage,  X  I leak , P stat =  X  I leak V dd . The average current leakage is a function of both the supply and threshold voltages; a standard model is where  X  D and V 0 are constants, typically around 0 . 06 and 0 . 04, respectively. To get the total power dis-sipates, we set P = P stat + P dyn . Note that while P dyn is a posynomial, P stat is not; moreover, it is not even convex under the log transformation. Previ-ous methods have modeled power dissipated through hand-tuned monomial and generalized posynomial ap-proximations. As a numerical example, we would like to model for 1 . 0  X  V dd  X  2 . 0 and 0 . 2  X  V th  X  0 . 4 with a gen-eralized posynomial. The goal is to produce a model that has low overall error.
 We produce a generalized posynomial model using n covariate samples that are drawn uniformly from x responses are generated by evaluating those values in Equation (5). The number of observations was varied between 200 and 5,000, with 10 i.i.d. training sets. Methods were the same as in Section 4.1. All were tested by calculating RMSE from Equation (5) on a 10,000 sample testing set. For the ensemble methods, M = 200. Results are given in Table 2. The gains using ensemble methods were smaller for power mod-eling than for the synthetic problem, likely because the power modeling problem is noiseless. 5.4. LC oscillator design Here we compare convex regression methods for device modeling in geometric programming based LC oscilla-tor design. Oscillators generate an oscillating output at a constant frequency. An LC oscillator (L and C represent inductor and capacitor, respectively) sends electrons from one plate of a capacitor through a coil, or loop inductor, to reach the other plate. However, when the electrons travel around a coil, a magnetic field is created that generates a voltage across the coil in the opposite direction of the electron flow. Once the capacitor is fully discharged, the magnetic field around the coil collapses and the voltage recharges the capac-itor in the opposite direction. Additional voltage is applied to compensate for that lost to resistance. We implement the LC oscillator design problem given in Boyd, Kim and Mohan (2005). The goal is to mini-mize power consumption subject to upper bound con-straints on the phase noise, area of the loop inductor and lower bounds on the loop gain and the self res-onance frequency, and some other loop inductor and transistor-specific constraints. The variables to be op-timized are the width and diameter of the loop induc-tor, the self resonance frequency, the length and width and maximum current of the CMOS transistor, the differential voltage amplitude, the total capacitance of the oscillator, the maximum switching capacitance, the minimum variable capacitance, the bias current and the capacitor max frequency.
 We used the methods in Section 4.1 to approximate the resistance of the loop inductor. EM-based posynomial fitting gave where f , D , W are the loop inductor frequency, di-ameter and width. We used this model as truth to compare the suboptimality of the regression meth-ods in a non-trivial optimization setting. To gener-ate approximations, we sampled uniformly across log-transformed covariates, where  X  10  X  log( D )  X   X  5,  X  13  X  log( W )  X   X  10 and 22  X  log( f )  X  23 for n = 500 and n = 5 , 000, although f is fixed for op-timization. 50 different training sets were generated and the optimization problem was solved using the approximated functions in ggplab (Mutapcic et al., 2006). For all ensemble methods, M = 50 to limit the number of non-sparse GP constraints. We compared optimal power consumption for each model as a func-tion of phase noise (dBc/Hz), which was varied from  X  122 to  X  110. Due to differences in function scale, percentage error from optimal values and percentage deviations from the true resistance values are used in-stead of RMSE. Results are given in Table 3.
 For both methods, cross-validated smearing provided lower error estimators and produced solutions with comparable mean deviation and lower maximum de-viation. Random search directions produced similar results with CAP. The different measurement metric highlights the differences between ensemble methods. The Magnani and Boyd (2009) estimator became more unstable with 2.5x standard deviation smearing. CAP became more unstable when used with bagging. Both of these methods likely do poorly because they add significant noise into a noiseless situation. In this paper, we combine convex regression (CAP and Magnani and Boyd (2009)) and ensemble methods to produce a piecewise linear approximation method. CAP is a consistent, stable estimator that uses a tree-like search to produce a piecewise linear model. Ensemble methods like bagging, smearing and ran-dom search directions add uncertainty in the partition boundaries. When averaged, these uncertain estimates produce a better fit. The Magnani and Boyd (2009) method is an unstable estimator that can produce a very good fit by aligning a piecewise linear model with the data used to produce it. Smearing and bagging average over a large number of models and reduce the likelihood that the estimator will be determined by a few poorly fitting models. Although device modeling is a natural setting for ensemble convex regression, the low computational complexity, theoretical guarantees and strong empirical performance in optimization set-tings, make ensemble convex regression a promising tool for combining estimation and optimization. Lauren A. Hannah is partially supported by the Duke Provost X  X  Postdoctoral Fellowship. This work was supported by Award Number R01ES17240 from the National Institute of Environmental Health Sciences. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institute of Environmental Health Sci-ences or the National Institutes of Health.

