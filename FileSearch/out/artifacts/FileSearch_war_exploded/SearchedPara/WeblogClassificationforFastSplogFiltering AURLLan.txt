 The blogosphere, which is a subset of the web and is comprised of personal electronic journals (we-blogs) currently encompasses 27.2 million pages and doubles in size e v ery 5.5 months (T echnorati, 2006). The information contained in the blogo-sphere has been pro v en v aluable for applications such as mark eting intelligence, trend disco v ery , and opinion tracking (Hurst, 2005). Unfortunately in the last year the blogosphere has been hea vily polluted with spam weblogs (called splo gs ) which are we-blogs used for dif ferent purposes, including promot-ing af filiated websites (W ikipedia, 2006). Splogs can sk e w the results of applications meant to quan-titati v ely analyze the blogosphere. Sophisticated content-based methods or methods based on link analysis (Gy  X  ongyi et al., 2004), while pro viding ef-fecti v e splog filtering, require e xtra web cra wling and can be slo w . While a combination of approaches is necessary to pro vide adequate splog filtering, sim-ilar to (Kan &amp; Thi, 2005), we propose, as a pre-liminary step in the o v erall splog filtering, a f ast, lightweight and accurate method merely based on the analysis of the URL of the weblog without con-sidering its content.

F or quantitati v e and qualitati v e analysis of the content of the blogosphere, it is acceptable to elim-inate a small fraction of good data from analysis as long as the remainder of the data is splog-free. This elimination should be k ept to a minimum to preserv e counts needed for reliable analysis. When using an ensemble of methods for comprehensi v e splog filtering it is acceptable for pre-filtering ap-proaches to lo wer recall in order to impro v e preci-sion allo wing more e xpensi v e techniques to be ap-plied on a smaller set of weblogs. The proposed method reaches 93.3% of precision in classifying a weblog in terms of spam or good if 49.1% of the data are left aside (labeled as unknown ). If all data needs to be classified our method achie v es 78% ac-curac y which is comparable to the a v erage accurac y of humans (76%) on the same classification task.
Sploggers, in creating splogs, aim to increase the traf fic to specific websites. T o do so, the y frequently communicate a concept (e.g., a service or a prod-uct) through a short, sometimes non-grammatical phrase embedded in the URL of the weblog (e.g., w ant to b uild a statistical classifier which le v erages the language used in these descripti v e URLs in order to classify weblogs as spam or good . W e b ui lt an initial language model-based classifier on the tok ens of the URLs after tok enizing on punctuation ( . , -, , / , ? , = , etc.). W e ran the system and got an ac-curac y of 72.2% which is close to the accurac y of humans X 76% (the baseline is 50% as the training data is balanced). When we did error analysis on the misclassified e xamples we observ ed that man y of the mistak es were on URLs that contain w ords glued to-gether as one tok en (e.g., dailyfreeipod ). Had the w ords in these tok ens been se gmented the initial sys-tem w ould ha v e classified the URL correctly . W e, thus, turned our attention to additional se gmenting of the URLs be yond just punctuation and using this intra-tok en se gmentation in the classification.
T raining a se gmenter on standard a v ail able te xt collections (e.g., PTB or BNC) did not seem the w ay to procede because the le xical items used and t h e se-quence in which the y appear dif fer from the usage in the URLs. Gi v en that we are interested in unsu-pervised lightweight approaches for URL se gmenta-tion, one possibility is to use the URLs themselv es after se gmenting on punctuation and to try to learn the se gmenting (the majority of URLs are naturally se gmented using punctuation as we shall see later). W e trained a se gmenter on the tok ens in the URLs, unfortunately this method did not pro vide suf ficient impro v ement o v er the system which uses tok eniza-tion on punctuation. W e h ypothesized that the con-tent of the splog pages corresponding to the splog URLs could be used as a corpus to learn the se g-mentation. W e cra wled 20K weblogs correspond-ing to the 20K URLs labeled as spam and good in the training set, con v erted them to te xt, tok enized and used the tok en sequences as training data for the se gmenter . This led to a statistically significant im-pro v ement of 5.8% of the accurac y of the splog filter . Frequently sploggers indicate the semantic con-tent of the weblogs using des cripti v e phrases X  often noun groups (non-recursi v e noun phrases) lik e adult-video-mpegs . There are dif ferent v arieties of splogs: commercial products (especially electron-ics), v acations, mortg ages, and adult-related.
Users don X  t w ant to see splogs in their results and mark eting i n t elligence applications are af fected when data contains splogs. Existing approaches to splog filtering emplo y statistical classifiers (e.g., SVMs) trained on the tok ens in a URL after to-k enization on punctuation (K olari et al., 2006). T o a v oid being identified as a splog by such sys-tems one of the creati v e techniques that splog-gers use is to glue w ords together into longer to-k ens for which there will not be statistical informa-is unlik ely to appear in the training data while are lik ely to ha v e been seen in training). Another ap-proach to dealing with splogs is ha ving a list of splog websites (SURBL, 2006). Such an approach based on blacklists is n o w less ef fecti v e because bloghosts pro vide tools which can be used for the automatic creation of a lar ge quantity of splogs. The weblog classifier us es a se gmenter which splits the URL in tok ens and then the tok en sequence is used for supervised learning and classification. 3.1 URL segmentation The se gmenter first tok enizes the URLs on punctua-tion symbols. Then the current URL tok ens are e x-amined for further possible se gmentation. The se g-menter uses a sliding windo w of n (e.g., 6 ) charac-ters. Going from left to right in a greedy f ashion the se gmenter decides whether to split after the current third character . Figure 1 illustrates the processing of k en dietthatworks . The character  X   X   X  indicates that the left and right tri-grams are k ept together while  X   X   X  indicates a point where the se gmenter decides a break should occur . The se gmentation decisions are based on counts collected during training. F or e x-ample, during the se gmentation of dietthatworks in the case of i e t  X  t h a we essentially con-sider ho w man y times we ha v e seen in t he training data the 6-gram  X  iettha  X  v s .  X  iet tha  X . Certai n characters (e.g., digits) are generalized both during training and se gmentation. 3.2 Classification F or the weblog classification a simple Na  X   X v e Bayes classifier is used. Gi v en a tok en sequence T =  X  t 1 , . . . , t n  X  , representing the se gmented URL, the class  X  c  X  C = { spam , good } is decided as:  X  c = arg m ax In the last step we made the conditional indepen-dence assumption. F or calculating P ( t i | c ) we use Laplace (add one) smoothing (Jurafsk y &amp; Martin, 2000). W e ha v e also e xplored classification via sim-ple v oting techniques such as: a = sg n Because we are interested in ha ving control o v er the precision/recall of the classifier we introduce a score meant to be used for deciding whether to label a URL as unknown . If scor e ( T ) e xceeds a certain threshold  X  we label T as spam or good using the greater probability of P ( spam | T ) or P ( good | T ) . T o control the presi-cion of the classifier we can tune  X  . F or instance, when we set  X  = 0 . 75 we achie v e 93.3% of preci-sion which implied a recall of 50.9%. An alternate commonly used technique to compute a score is to look at the log lik elihood ratio. First we discuss the se gmenter . 10,000 spam and 10,000 good weblog URLs and their corresponding HTML pages were used for the e xperiments. The 20,000 weblog HTML pages are used to induce the se gmenter . The first e xperiment w as aimed at find-ing ho w common e xtra se gmentation be yond punc-tuation is as a phenomenon. The se gmenter w as run on the actual training URLs. The number of URLs that are additionally se gmented besides the se gmen-tation on punctuation are reported in T able 1. T able 1: Number of e xtra se gmentations in a URL The multiple se gmentations need not all occur on the same tok en in the URL afte r initial se gmentation on punctuations.

The se gmenter w as then e v aluated on a separate test set of 1,000 URLs for which the ground truth for the se gmentation w as mark ed. The results are in T able 2. The e v aluation is only on se gmentation e v ents and does not include tok enization decisions around punctuation.
 Figure 2 sho ws long tok ens which are correctly split. The weblog classifier w as then run on the test set. The results are sho wn in T able 3. The performance of humans on this task w as also e v aluated. Eight indi viduals performed the splog identification just looking at the unse gmented URLs. The results for the human annotators are gi v en in T a-ble 4. The a v erage accurac y of the humans (76%) is similar to that of the system (78%).
 From an information retrie v al perspecti v e if only 50.9% of the URLs are retrie v ed (labelled as ei-ther spam or good and the rest are labelled as unknown ) then of the spam / good decisions 93.3% are correct. This is rele v ant for cases where a URL splog filter is in casca d e follo wed by , for e x-ample, a content-based one. The system performs bet ter with the intra-tok en se g-mentation because the system is forced to guess un-seen e v ents on fe wer occasions. F or instance gi v en the input URL www.ipodipodipod.com in the sys-tem which se gments solely on punctuation both the spam and the good model will ha v e to guess the probability of ipodipodipod and the results depend merely on the smoothing technique.

Ev en if we reached the a v erage accurac y of hu-mans we e xpect to be able to impro v e the system further as the maximum accurac y among the human annotators is 90%. Among t he errors of the se g-menter the most common are related to plural nouns ( X  girl  X  s  X  vs.  X  girls  X ) and past tense of v erbs ( X  dedicate  X  d  X  vs.  X  dedicated  X ) .
 The proposed approach has ramifications for splog filtering systems that w ant to consider the outw ard links from a weblog. W e ha v e presented a technique for determining whether a weblog is splog based merely on alalyz-ing its URL. W e proposed an approach where we initially se gment the URL in w ords and then do the classification. The technique is simple, yet v ery ef fecti v e X  X ur system reaches an accurac y of 78% (while humans perform at 76%) and 93.3% of preci-sion in classifying a weblog with recall of 50.9%.
Ackno wledgements. W e wish to thank T ed Kre-mer , Ho w ard Kaushansk y , Ash Beits, Allen Bennett, Susanne Costello, Hillary Gusta v e, Glenn Meuth, Micahel Se villa and Ron W oodw ard for help with the e xperiments and comments on an earlier draft.
