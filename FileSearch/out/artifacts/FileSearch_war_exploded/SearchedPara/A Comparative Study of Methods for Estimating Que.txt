 We systematically compare five representative state-of-the-art methods for estimating query language models with pseudo feedback in ad hoc information retrieval, including two vari-ants of the relevance language model, two variants of the mixture feedback model, and the divergence minimization estimation method. Our experiment results show that a variant of relevance model and a variant of the mixture model tend to outperform other methods. We further pro-pose several heuristics that are intuitively related to the good retrieval performance of an estimation method, and show that the variations in how these heuristics are imple-mented in different methods provide a good explanation of many empirical observations.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval models General Terms: Experimentation, algorithms Keywords: Query language model, pseudo relevance feed-back, language models, feedback heuristics
As a new generation of probabilistic retrieval models, lan-guage modeling approaches to information retrieval (IR) [6, 9] have performed well empirically, with a significant amount of performance increase often coming from the use of pseudo/blind relevance feedback techniques for the esti-mation of query language models [8, 4, 7].

While many pseudo feedback techniques have been tried for estimating query language models, they have not been compared thoroughly. Previous studies [8, 4, 7] evaluate dif-ferent methods using different query sets, document collec-tions, and parameter setting, making it impossible to com-pare results across studies. A saresult,wedonotyethave a good understanding of the relative strength and weakness of different methods.

The purpose of this paper is to systematically compare these methods with the same experiment setting. We study five representative methods, including two variants of rele-vance model [4, 1], two variants of mixture model [8, 7], and a Rocchio-like method called divergence minimization model [8]. They are popular and representative methods that have already proven effective to improve retrieval accuracy, and thus a comparative study of them is very interesting.
Our basic retrieval model is the KL-divergence retrieval model [3], which scores a document D with respect to a query Q by computing the negative KL divergence between the query language model  X  Q and the document language model  X  D : where V is the set of words in our vocabulary. Obviously accurate estimation of the query language model  X  Q plays a critical role in this language modeling approach
Without feedback information, query language models are often estimated by using the MLE method on the query text: p ( the query Q ,and | Q | is the total number of words in the query. However, through exploiting feedback information (e.g., assuming the top-ranked documents F = { D 1 ...D | F | are relevant), we can re-estimate a more accurate query lan-guage model  X  Q . We now review briefly several representa-tive effective methods for query model estimation based on pseudo feedback techniques.
In the first estimation method of relevance model (often called RM1) [4], the authors essentially use the query like-lihood p ( Q | D ) as the weight for document D and take an average of the probability of word w given by each docu-ment language model. Formally, let  X  represent the set of smoothed document models in the pseudo feedback collec-tion F and Q = { q 1 ,q 2 ,  X  X  X  ,q m } . The formula of RM1 is:
In the second method (i.e., RM2), they compute the asso-ciation between each word and the query using documents containing both query terms and the word as  X  X ridges X .
The Dirichlet smoothing method with a prior of  X  fb is used to smooth the language model of each pseudo-relevant document  X  D in both RM1 and RM2.

The relevance model P ( w | Q ) can be interpolated with the original query model  X  Q to improve performance [1]. In this paper, we will only evaluate the following two interpolated versions of relevance model (called RM3 and RM4): where  X  is a parameter to control the amount of feedback.
The divergence minimization model (DMM) proposed in [8] assumes that the feedback model  X  F should be very close to the language model of every pseudo-relevant document but far away from the collection language model which can be regarded as an approximation of non-relevant language model. The following analytic solution is obtained by solving such an optimization problem: where p ( w |  X  i ) is smoothed in the same way as the smoothing of document language model in the retrieval step. Finally, the query language model is updated by interpolating  X  F with the original query model  X  Q inthesamewayasin Equations 4 and 5 with a coefficient  X  .
In the simple mixture model (SMM) [8], the words in F are assumed to be drawn from two models: (1) background model p ( w | C )and(2)topicmodel p ( w |  X  F ). Thus the log-likelihood for the entire set of feedback documents is: where c ( w, F ) is the count of word w in the set of feedback documents F ,and  X   X  [0 , 1] is the probability of choosing the background model p (  X | C ) to generate the word. The estimate of  X  F can be computed using the Expectation-Maximization (EM) algorithm to maximize the log-likelihood.
Similarly,  X  F is also interpolated with the original query model  X  Q to update the query model with a coefficient  X  .
In the regularized mixture model (RMM) proposed in [7], each feedback document is allowed to have a potentially dif-ferent amount of background words (i.e.,  X  D ). And the orig-inal query is combined with the feedback model through a conjugate (Dirichlet) prior on  X  F to respect the relevance of documents. The estimate of  X  F can be computed using the Maximum A Posteriori estimator and the EM algorithm.
Although the original RMM was proposed to eliminate the need for the interpolation parameter  X  in SMM, to make RMM comparable with other methods, we also introduce a comparable parameter  X  into RMM to indicate the amount of feedback. Parameter  X  is defined as: r  X  =  X  1  X   X  ,where and  X  are two parameters in RMM [7].
We used several standard TREC data sets in our study, including AP88-89, TREC678, and WT2G. (see Table 1). We pre-processed documents and queries with two different strategies: in the first strategy, we only stemmed words with the Porter algorithm, which is indicated as  X  X / s.w. X  (i.e., with stop words); in the second one, besides stemming, a total of 418 stop words from the standard InQuery stoplist were removed, which is labeled as  X  X /o s.w. X  (i.e., without stop words). We first compare the effectiveness of the five methods. We fix Dirichlet smoothing (  X  = 1000) for estimating the document language models. We also simply set the number of feedback documents to 10 and the number of terms in feedback model to 100, and the rest parameters are trained on the corresponding training data set (we would use the learned parameters in the rest of this paper if there is no extra specification). We summarize the results in Table 2. Overall we find SMM and RM3 most effective in our ex-periments. SMM is better on homogeneous data, e.g., AP2; while RM3 works more effectively on Web data, i.e., WT2G. For the recall, SMM dominates over all other methods ob-viously. DMM and RM4 do not work as well as other meth-ods, although RM4 appears to be effective on homogeneous data which is consistent with the observation in [4]. Indeed we observe similar performance between RMM and SMM on most collections with several exceptions (i.e., on WT2G) where RMM worked clearly worse than SMM.
In SMM and DMM, the parameter  X  controls the influ-ence of the collection language model. In RM3 and RM4, a parameter  X  fb plays a similar role. We set  X  =0 . 5and examine how  X  (or  X  fb ) affects the average precision. We observe that the performance is quite sensitive to the param-eter in DMM, but is relatively insensitive in SMM, RM3 and RM4, especially in RM3. It is interesting to see that RM3 often achieves a stable performance when we set  X  fb =0.
Recall that we interpolate the estimated feedback model  X 
F with the original query model  X  Q . The interpolation is controlled by a coefficient  X  . Our experiment results show that the setting of  X  can affect the performance significantly for all the five methods. In another exploration [5], we have studied how to adapt this parameter to the characteristics of queries and feedback documents.

We further compare the robustness of different estimation methods w.r.t. the number of documents used for pseudo feedback in Figure 1. We notice that RM3 is much more robust than the other three methods. Yet it appears that all methods work well with 10 feedback documents.
To understand why some of these methods work better than others, we propose severa l heuristics that we would like every estimation method to satisfy, and analyze how each of the five estimation methods implements these heuristics.
IDF: assigning more weights to discriminative terms. Ex-plicit IDF implementation exists in SMM, DMM and RMM, as they all use a collection language model to trim common terms from the feedback model, but RM3 and RM4 also have an implicit IDF effect due to the smoothing of docu-ment language models in the retrieval step [9].

TF: favoring frequent terms in feedback documents. The two variants of relevance model and two variants of mix-ture model employ an arithmetic mean to aggregate term frequency evidence from feedback documents, while DMM uses a geometric mean as shown in formula 6.

Document Weight: respecting important feedback doc-uments rather than taking all of them equally.

Some methods discriminate feedback documents in terms of the amount of relevance information , e.g., RM3 uses the query likelihood score, while RM4 adopts an indirect query likelihood, which differs from RM3 in that RM4 sums over feedback documents by using the likelihood of each query word as the document weight and then aggregates the ev-idence from all the query words; RMM uses the original query as a prior to assign more weights to documents more relevant to the query: one interesting thing is that RMM combines document weight and mixture noise parameter to-gether, controlled by a dynamic parameter  X  D .

Some methods favor long documents , e.g., SMM and RMM pools together terms from all the feedback documents, which is essentially using the raw document length as the document weight to combine document language models; DMM, RM3 and RM4 use Dirichlet smoothing method to estimate lan-guage models for feedback documents, which indeed assigns aweight | D | | D | +  X  to document D and thus tends to also prefer long documents.

We summarize in Table 3 how the five methods imple-ment the above heuristics. We can see that different feed-back methods implement heuristics quite differently. So we now turn to the following questions: (1) what could be the best implementation for each heuristic? and (2) is the bad retrieval performance due to the weakness of some heuristic implementation?
The mixture model and the divergence minimization pro-vide two different strategies of implementing the IDF heuris-Table 3: Heuristics analysis of different methods.  X  X /A X  means no implementation;  X  X m X  and  X  X m X  stand for arithmetic mean and geometric mean re-spectively;  X  X L X  indicates query likelihood;  X  X ir X  stands for Dirichlet smoothing. tic. To make the two strategies comparable, we design a new method Dir-SMM, which uses the same strategy as DMM to aggregate feedback documents, i.e., taking an average of the smoothed document language models, but uses the mix-ture model to factor out common terms. The results are reported in Table 4. It is interesting to see that Dir-SMM outperforms DMM consistently, which may suggest that the mixture model is better than the divergence minimization in terms of IDF effect.

Can the same IDF effect be achieved implicitly during the smoothing of document language models in the retrieval step? To seek the answer, we design another version of SMM, in which the parameter  X  is set to 0. We find that the original SMM outperforms this new version of SMM stably, no matter how we tune the Dirichlet prior.

We observe similar performance between RMM and SMM on most collections, but RMM was much worse on WT2G. This may suggest that RMM, which uses the same parameter (i.e.,  X  D ) to control IDF effect and to capture document relevance score, loses to a simple mixture model.
To compare the effectiveness of the arithmetic mean and the geometric mean as TF strategies, we design two meth-ods: Dir-SMM  X  =0 and DMM  X  =0 .  X  = 0 means that the IDF in the models is blocked. So the feedback models estimated We see from Table 4 that the fo rmer is clearly better than the latter, which may mean that the geometric mean is not a good TF measure for pseudo feedback.
Document length: We have two candidate strategies (i.e., Dirichlet smoothing and raw document length) for doc-ument weighting based on document length. The proposed Dir-SMM and the original SMM essentially use these two strategies respectively. In addition, we also introduce an-other member EDL-SMM into the comparison, which as-signs equal weights to feedback documents. To fully exploit the strength of Dir-SMM, we also train the Dirichlet pa-rameter which was fixed in the previous experiments. This enhanced version of Dir-SMM is labeled as Dir-SMM+. We compare SMM, Dir-SMM+ and EDL-SMM in Table 4. It is not surprising that Dir-SMM+ works overall the best; in fact, Dir-SMM+ takes SMM and EDL-SMM as its two ex-treme cases when setting  X  =  X  and  X  =0respectively. However, one interesting observation is that, on collections with stopwords (i.e., w/ s.w.), EDL-SMM is better than or comparable to Dir-SMM+; while on other collections (i.e., w/o s.w.), SMM is better than or comparable to Dir-SMM+. Considering the complexity of Dir-SMM+ which has more parameters to tune, a hybrid method which dynamically se-lects EDL-SMM or SMM to use could be a better strategy.
What are the reasons that document length can be ex-ploited for document weight? Why does document length work differently on different type of collections? To answer these questions, we plot the document length and document ranking of the top-10 documents on AP1 data, where doc-ument length is normalized to sum to 1. We observe that document length is distributed almost randomly on AP1 w/ s.w., but on AP1 w/o s.w., document length tends to be large for highly-ranking documents. It could be one possible reason why document length works better on AP1 w/o s.w. than on AP1 w/ s.w. This phenomenon also confirms a find-ing in [2]: if all the query terms a re discriminative words, the KL-divergence method will probably assign a higher score to a longer document, but if there are common terms in the query, longer documents are often overly-penalized and thus would not receive higher scores. Therefore, document length is more correlated to the relevance score on collections of which stopwords are removed.

Query likelihood: RM3 is most robust so far. According to our analysis, only RM3 uses the query likelihood score as the document weight, suggesting that the query likelihood could potentially lead to the robustness of RM3.
We next examine if the query likelihood score can also improve the retrieval precision. We design another family of mixture model QL-EDL-SMM, in which the weight of each feedback document model is exactly the query likeli-hood score (i.e., we do not use document length). We com-pare QL-EDL-SMM and EDL-SMM in Table 4. It is ob-served that although QL-EDL-SMM improves the MAP on TREC78 w/o s.w. slightly, it decreases the MAP on all other collections. It may suggest that the query likelihood score does not improve the retrieval precision, even though it ap-pears to increase the robustness in taking different numbers of feedback documents.
Five methods for estimating query language models were evaluated, including RM3, RM4, DMM, SMM, and RMM. We found SMM and RM3 most effective in our experiments. Using SMM yielded effective retrieval performance in both precision and recall. RM3 performed similarly to SMM on precision but worse than SMM on recall measure. However, RM3 is more robust to the setting of feedback parameters. RM4 only appeared to be effective on homogeneous docu-ment collections. DMM had relatively poor performance and was quite sensitive to parameter setting. We found similar performance between RMM and SMM on most collections, but RMM was much worse on Web data (i.e., WT2G).
We further proposed several heuristics that are intuitively related to the good retrieval performance of an estimation method, and found that the implementation of proposed heuristics in different methods provided a good explanation of many empirical observations.
We thank the anonymous reviewers for their useful com-ments. We also thank Victor Lavrenko and Donald Met-zler for valuable suggestions related to the relevance model. This material is based upon work supported by the National Science Foundati on under Gra nt Numbers I IS-0347933, IIS-0713581, and IIS-0713571.
