 JOHN P. DICKERSON and V. S. SUBRAHMANIAN, University of Maryland Geospatial Abduction Problems (GAPs) were introduced by Shakarian et al. [2010] to find a set of locations that  X  X est explain X  a given set of locations of observations. We call these inferred sets of locations  X  X xplanations. X  There are many such applications in a wide variety of domains.  X  X n criminology , serial killers carry out murders at various locations; these corre-spond to the observations we make. The goal of the police is to identify a set of locations that best  X  X xplain X  the observations. Thus, the police look for the killer X  X  home and office locations. The killer, of course, goes to considerable effort usually to ensure that he cannot be easily found by the police.  X  X n military applications , insurgents (such as those in Iraq and Afghanistan) carry out Improvised Explosive Device (IED) atta cks at various locations; these corre-sponding to our observations. Multinational forces operating in these countries would like to identify many locations associated with these attack locations; one such class of locations corresponds to the locations of weapons caches that provide logistics support for the attacks and enable the attackers to carry them out. As in the case of the serial killer, the insurgents reason carefully about their choice of weapons cache locations to minimize the probability of being detected.  X  In a wildlife application, a rare animal or bird might be spotted at several locations (observations). We would like to infer the location of the creature X  X  nest or den.
Many animals take considerable care to keep their den/nest hidden as these often hold young ones or eggs and, in some cases, food.

Shakarian et al. [2010] defined Geospatial Abduction Problems (GAPs) and studied a version of the problem where the adversary (the  X  X ad guy X  or the entity that wishes to evade detection) does not reason about the agent (the  X  X ood guy X  or the entity that wants to detect the adversary). Despite this significant omission, they were able to ac-curately predict the locations of weapons caches in real-world data about IED attacks in Baghdad. In this article, we introduce adversarial geospatial abduction problems where both the agent and the adversary reason about each other. Specifically, our contributions are as follows. (1) We axiomatically define reward functions to be any functions that satisfy certain (2) We formally define the Optimal Adversary Strategy (OAS) that minimizes chances (3) We provide a detailed set of results on the computational complexity of these prob-(4) We develop Mixed Integer Linear Programming algorithms (MILPs) for OAS and (5) We develop a prototype of our MILP algorithms to solve the OAS problem, using (6) We develop a prototype implementation that shows that both MCA-LS and MCA-
The organization of the article is as follows. Section 2 first reviews the GAP framework of Shakarian et al. [2010]. Section 3 extends GAPs to the adversarial case using an axiomatically-defined reward function (Section 2). Section 4 presents complexity results and several exact algo rithms using MILPs for the OAS problem. Section 5 provides complexity results and develops exact and approximate methods MCA, including an approximation technique that provides the best possible guarantee unless P = NP. We then briefly describe our prototype implementation and describe a detailed experimental analysis of our algorithms. Finally, related work is described in Section 7. In this section, we briefly describe the t heory of GAPs introduced by Shakarian et al. [2010]. With the exception of the counting complexity results (Lemma 2.1 and Theorem 2.1), everything in Section 2 appeared in Shakarian et al. [2010]. Through-out this article, we assume the existence of integers M , N &gt; 0 that jointly define a 2-dimensional gridded space. We use N , R , R + to respectively denote the sets of natu-ral numbers, all real numbers, and nonnegative reals.

Definition 2.1 ( Space ). Suppose M , N  X  N .The space S is the set { 1 ,..., M }  X  { 1 ,..., N } .
 Throughout this article, we assume that M , N , S are arbitrary, but fixed. This repre-sentation of the space S as a set of integer coordinates is common in most Geospatial Information Systems (GIS). We use 2 S to denote the power set of S . We assume that S has an associated distance function d which assigns a nonnegative distance to any two points and satisfies the usual distance axioms. 1 Definition 2.2 ( Observation Set ). An observation set O is any finite subset of S . For instance, in our IED application, an observation set is simply the set of locations where attacks occurred. In the serial killer example, the observation set is the set of locations where the killings occurred.
 Definition 2.3 ( Feasibility Predicate ). A feasibility predicate is any function feas : S  X  X  TRUE , FALSE } .
 Feasibility predicates encode domain knowledge. For instance, a feasibility predicate in the IED application might rule out the caches being on U.S. bases or in bodies of water or (in the case of Baghdad where our dataset contains Shiite attacks) Sunni neighborhoods. Throughout this article, we assume an arbitrary, but fixed function feas that assigns either true or false to every point in S . In our complexity results, we assume feas is computable in constant time.

Definition 2.4 ((  X ,  X  ) -explanation ). Given a finite set of observations O and real numbers  X   X  0,  X &gt; 0, a finite set of points E  X  S is an (  X ,  X  ) -explanation of O iff: (1) (  X  p  X  E ) feas ( p )= TRUE ; (2) (  X  o  X  O )(  X  p  X  E )  X   X  d ( p , o )  X   X  .

Intuitively, E is an (  X ,  X  ) -explanation of O if every point in E is feasible and every ob-servation in O is neither too close nor too far from a point in E . For a given observation, o ,wewillrefertopoint p as a partner iff feas ( p )and d ( o , p )  X  [  X ,  X  ].  X  and  X  are parameters that can be easily learned from historical data (as was done in Shakarian et al. [2010]). Both criminologists Rossmo and Rombouts [2008] and military experts U.S. Army [1994] have noted that partner locations are not too close to an observation location nor are they too far. 2 Note that having  X ,  X  actually increases the generality of our approach as users can always opt not to use them by setting  X  =0 and  X  to any number exceeding an (  X ,  X  )-explanation of cardinality k or less. Often we will fix k ; in this situation we will use the terms  X  k -explanation X  and  X  X xplanation X  interchangeably. Alternatively, another requirement that can be imposed on an explanation is irredundancy.
Definition 2.5. An explanation E is irredundant iff no strict subset of E is an explanation.
 Intuitively, if we can remove any element from an explanation, and this action causes it to cease to be a valid explanation, we say the explanation is irredundant.
Example 2.1. Figure 1 shows a map of a drug plantation depicted in a 18  X  14 grid. The distance between grid squares is 100 meters. Observation set O = { o formant or from historical data, drug enfo rcement officials know that there is a drug laboratory located 150  X  320 meters from the center mass of each field ( i.e. , in a geospa-tial abduction problem, we can set [  X ,  X  ] = [150 , 320]). Further, based on the terrain, the drug enforcement officials are able to d iscount certain areas (shown in black on Figure 1, a feasibility predicate can easily be set up accordingly). Based on Figure 1, planations.
 We now formally recall the definition of a GAP from Shakarian et al. [2010]. The k Spatial (  X ,  X  ) Explanation Problem (k-SEP ).
 INPUT: Space S ,aset O of observations, a feasibility predicate feas , reals numbers  X   X  0,  X &gt; 0, and natural number k .
 OUTPUT:  X  X es X  if there exists an (  X ,  X  )explanationfor O of size k , X  X o X  X therwise.
Shakarian et al. [2010] shows this problem to be NP-complete based on a reduction from the known NP-complete problem Geometric Covering by Discs ( GCD ) seen in Johnson [1982], also known as the Euclidean m -center on points in Masuyama and Ibaraki [1981]. The problem is defined as follows.
 Geometric Covering by Discs ( GCD ).
 INPUT: A set P of integer-coordinate points in a Euclidean plane, positive integers b &gt; 0and K &lt; | P | .
 OUTPUT:  X  X es X  if there exists k discs of diameter b centered on points in P such that there is a disc covering each point in P , X  X o X  X therwise.

As with most decision problems, we define the associated counting problem, # GCD , as the number of  X  X es X  answers to the GCD decision problem. The result that follows, which is new, shows that # GCD is #P-complete and, moreover, that there is no fully polynomial random approximation scheme for # GCD unless NP equals the complexity class RP . 3 L EMMA 2.1. # GCD is # P-complete and has no FPRAS unless NP=RP.

We can leverage the preceding result to derive a complexity result for the counting version of k -SEP .

T HEOREM 2.1. The counting version of k-SEP is # P-complete and has no FPRAS unless NP=RP. Throughout this article, we view geospatial abduction as a two-player game where an agent attempts to find an  X  X xplanation X  for a set of observations caused by the adversary who wants to hide the explanation from the agent.

Each agent chooses a strategy which is merely a subset of S . Though  X  X trategy X  and  X  X bservation X  are defined identically, we use separate terms to indicate our intended use. In the IED example, the adversary X  X  strategy is a set of points where to place his cache, while the agent X  X  strategy is a set of points that he thinks hold the weapons caches. Throughout this article, we use A (respectively B ) to denote the strategy of the adversary (respectively agent).

Given a pair ( A , B ) of adversary-agent strategies, a reward function measures how similar the two sets are. The more similar, the better it is for the agent. As reward functions can be defined in many ways, we choose an axiomatic approach so that our framework applies to many different reward functions including ones that people may invent in the future.

Definition 3.1 ( Reward Function ). A reward function is any function rf :2 S  X  2 S  X  [0 , 1] that for any k -explanation A  X  X  X  and set B  X  S , the function satisfies: (1) If B = A ,then rf ( A , B )=1. (2) For B , B then We now define the payoffs for the agent and adversary.

O BSERVATION 3.1. Given adversary strategy A , agent strategy B , and reward func-It is easy to see that for any reward function and pair ( A , B ), the corresponding game is a zero-sum game [Leyton-Brown and Shoham 2008]. Our complexity analysis assumes all reward functions are polynomially computable. All the specific reward functions we propose in this article satisfy this condition.

The basic intuition behind the reward function is that the more the strategy of the agent resembles that of the adversary, the closer the reward is to 1. Axiom 1 says that if the agent X  X  strategy is the same set as adversary X  X , then the reward is 1. Axiom 2 says that adding a point to B cannot increase the reward to the agent if that point is already in B , that is, double-counting of rewards is forbidden.

The following theorem tells us that every reward function is submodular ,thatis, the marginal benefit of adding additional points to the agent X  X  strategy decreases as the cardinality of the strategy increases.
 function is submodular, that is, if B  X  B ,andpointp  X  S such that p /  X  B and p /  X  B , then rf ( A , B  X  X  p } )  X  rf ( A , B )  X  rf ( A , B  X  X  p } )  X  rf ( A , B ) .
Some readers may wonder why rf ( A ,  X  ) = 0 is not an axiom. While this is true of many reward functions, there are reward functions where we may wish to penalize the agent for  X  X ad X  predictions. Consider the following reward function.

Definition 3.2 ( Penalizing Reward Function ). Given a distance dist , we define the penalizing reward function , prf dist ( A , B ), as follows. P ROPOSITION 3.2. prf is a valid reward function.

Example 3.1. Consider Example 2.1 and the explanation A  X { p 40 , p 46 } (resembling that the drug enforcement officials wish to search), distance dist = 100 meters. There is only one point in A that is within 100 meters of a point in B (point p 40 ) and 3 points in B more than 100 meters from any point in A (points p 38 , p 44 , p 56 ). These relationships are shown visually in Figure 2. Hence, prf dist ( A , B )=0 . 5+0 . 25  X  0 . 011 = 0 . 739. prf penalizes the agent if he poorly selects points in S .Theagentstartswitha reward of 0 . 5. The reward increases if he finds points close to elements of A ;otherwise, it decreases.

A reward function is zero-starting if rf ( A ,  X  ) = 0, that is, the agent gets no reward if he infers nothing.
 Definition 3.3. A reward function, rf ,is monotonic if (i) it is zero-starting and (ii) if B  X  B then rf ( A , B )  X  rf ( A , B ).
 We now define several example monotonic reward functions.

The intuition behind the cutoff reward function crf is simple: For a given distance dist (the  X  X ut-off  X  distance), if for every p  X  A ,thereexists p  X  B such that d ( p , p )  X  dist ,then p is considered  X  X lose to X  p .

Definition 3.4 ( Cutoff Reward Function ). Reward function based on a cut-off dis-tance, dist .
The following proposition shows that the cutoff reward function is a valid, monotonic reward function.
 P ROPOSITION 3.3. crf is a valid, monotonic reward function.

Example 3.2. Consider Example 3.1. Here, crf dist ( A , B ) returns 0 . 5 as one element of A is within 100 meters of an element in B .

By allowing a more general notion of  X  X loseness X  between points p  X  A and p  X  E , we are able to define another reward function, the falloff reward function , frf .This function provides the most reward if p = p but, unlike the somewhat binary crf , gently lowers this reward to a minimal zero as distances d ( p , p )grow.

Definition 3.5 ( Falloff Reward Function ). Reward function with value based on minimal distances between points. proportional to the square of the distance between points, as the search area required grows proportionally to the square of this distance.
 P ROPOSITION 3.4. frf is a valid, monotonic reward function.

In practice, an agent may assign different weights to points in S based on the per-ceived importance of their partner observations in O . The  X  X eighted reward function X  wrf gives greater reward for b eing  X  X loser X  to points in A that have high weight than those with lower weights.

Definition 3.6 ( Weighted Reward Function ). Given weight function W : S  X  R + , and a cut-off distance dist we define the weighted reward function to be: P ROPOSITION 3.5. wrf is a valid, monotonic reward function.

It is easy to see that the weighted reward function is a generalization of the cutoff reward function where all weights are 1.
 It is important to note that we have introduced reward functions axiomatically. There are numerous other reward functions that satisfy the axioms given in Defini-tion 3.1 that can be defined in an application. There is no guarantee that the few spe-cific instances of a reward function we have defined are the only good ones; application developers are welcome to use their own. In this section, we introduce pdfs over strategies (or  X  X ixed strategies X  [Leyton-Brown and Shoham 2008]) and introduce the notion of  X  X xpected reward. X  We first present explanation/strategy functions which return an explanation (respectively strategy) of a certain size for a given set of observations.

Definition 3.7 ( Explanation/Strategy Function ). An explanation (respectively strat-egy) function is any function ef :2 S  X  N  X  2 S (respectively sf :2 S  X  N  X  2 S )that, given a set O  X  S and k  X  N , returns a set E  X  S such that E is a k -sized explana-tion of O (respectively E is a k -sized subset of S ). Let EF be the set of all explanation functions.

Example 3.3. Following from Example 2.1, we shall define two functions ef 1 , ef 2 , which for set O (defined in Example 2.1) and k  X  3, give the following sets. These sets may correspond to explanations f rom various sources. Perhaps they corre-spond to the answer of an algorithm that drug-enforcement officials use to solve GAPs. Conversely, they could also be the result of a planning session by the drug cartel to determine optimal locations for the drug labs.

In theory, the set of all explanation functions can be infinitely large; however, it makes no sense to look for explanations containing more points than S , so we assume explanation functions are only invoked with k  X  M  X  N .

A strategy function is appropriate for an agent who wants to select points resem-bling what the adversary selected, but is not required to produce an explanation. Our results typically do not depend on whether an explanation or strategy function is used (when they do, we point it out). Therefore, for simplicity, we use  X  X xplanation function X  throughout the article. In our complexity results, we assume that explanation/strategy functions are computable in constant time.

Both the agent and the adversary do not know the explanation function (where is the adversary going to put his weapons caches? Where will US forces search for them?) in advance. Thus, they use a pdf over explanation functions to estimate their opponent X  X  behavior, yielding a  X  X ixed X  strategy.

Definition 3.8 ( Explanation Function Distribution ). Given a space S , real numbers  X ,  X  , feasibility predicate feas , and an associated set of explanation functions EF ,an explanation function distribution is a finitary 4 probability distribution efd : EF  X  [0 , 1] with ef  X  EF efd ( ef )=1.Let EFD be a set of explanation function distributions. We use | efd | to denote the cardinality of the set EF associated with efd .
Example 3.4. Following from Example 3.3, we shall define the explanation function distribution efd drug that assigns a uniform probability to explanation functions in the set ef 1 , ef 2 ( i.e. , efd drug ( ef 1 )=0 . 5).
 We now define an  X  X xpected reward X  that takes into account these mixed strategies specified by explanation function distributions.

Definition 3.9 ( Expected Reward ). Given a reward function rf , and explanation function distributions efd ad v , efd ag ,the expected reward is the function EXR rf : EFD  X  EFD  X  [0 , 1] defined as follows.

However, in this article, we will generally not deal with expected reward directly, but two special cases (expected adversarial detriment and expected agent benefit) in which the adversary X  X  and agent X  X  strategies are not mixed respectively. We explore these two special cases in the next two sections. In this section, we study how an adversary would select points (set A )inthespacehe would use to cause observations O . For instance, in the IED example, the adversary needs to select A and O so that A is an explanation for O . We assume the adversary has a probabilistic model of the agent X  X  behavior (an explanation function distribution) and that he wants to eventually find an explanation (e.g., where to put his weapons caches). Hence, though he can use expected reward to measure how close the agent will be to his explanation, only the agent X  X  strategy is mixed. The adversary X  X  actions are concrete. Hence, we introduce a special case of expected reward: expected adversarial detriment.
Definition 4.1 ( Expected Adversarial Detriment ). Given any reward function rf and explanation function distribution efd ,the expected adversarial detriment is the func-tion EXD rf : EFD  X  2 S  X  [0 , 1] defined as follows.
 Intuitively, the expected adversarial detriment is the expected number of partner lo-cations the agent may uncover if efd is correct. Consider the following example.
Example 4.1. Following from the previous examples, suppose the drug cartel is planning three drug labs. Suppose they have information that drug-enforcement agents will look for drug labs using efd drug (Example 3.4). One suggestion the adver-sary may consider is to put the labs at locations p 41 , p 52 (see Figure 1). Note that this explanation is optimal with respect to cardinality. With dist = 100 meters, they wish each explanation function (see Example 3.3). the best location for the cartel to position the labs with respect to crf and efd ,because the expected adversarial detriment of the drug-enforcement agents is large.
The expected adversarial detriment is a quantity that the adversary would seek to minimize. This is now defined as an optimal adversarial strategy next.

Definition 4.2 ( Optimal Adversarial Strategy ). Given a set of observations O ,nat-ural number k , reward function rf , and explanation function distribution efd ,an op-timal adversarial strategy is a k -sized explanation A for O such that EXD rf ( efd , A )is minimized. In this section, we formally define the Optimal Adversary Strategy (OAS) problem and study its complexity.
 OAS Problem .
 INPUT: Space S , feasibility predicate feas , real numbers  X ,  X  , set of observations O , natural number k , reward function rf , and explanation function distribution efd . OUTPUT: Optimal adversarial strategy A .
 We show that the known NP-hard problem Geometric Covering by Discs (see Section 2) is polynomially reducible to OAS, which establishes NP-hardness. T HEOREM 4.1. OAS is NP-hard.

The proof of the previous theorem yields two insights. First, OAS is NP-hard even if the reward function is monotonic (or antimonotonic). Second, OAS remains NP-hard even if the cardinality of EF is small; in the construction we only have one explanation function. Thus, we cannot simply pick an  X  X ptimal X  function from EF .Toshowan upper bound, we define OAS-DEC to be the decision problem associated with OAS. If the reward function is computable in polynomial time, OAS-DEC is in NP.
 OAS-DEC .
 INPUT: Space S , feasibility predicate feas , real numbers  X ,  X  , set of observations O , natural number k , reward function rf , explanation function distribution efd , and num-ber R  X  [0 , 1].
 OUTPUT:  X  X es X  if there exists an adversarial strategy A such that EXD rf ( efd , A )  X  R ,  X  X o X  otherwise.
 T HEOREM 4.2. If the reward function is computable in PTIME, then OAS-DEC is NP-complete.

Suppose we have an NP oracle that can return an optimal adversarial strategy; let X  X  call it A . Quite obviously, this is the best response of the adversary to the mixed strategy of the agent. Now, how does the agent respond to such a strategy? If we were to assume that such a solution were unique, then the agent would simply have to find an strategy B such that rf ( A , B ) is maximized. This would be a special case of the problem we discuss in Section 5. However, this is not necessarily the case. A natural way to address this problem is to create a uniform probability distribution over all optimal adversarial strategies and optimiz e the expected reward, again a special case of what is to be discussed in Section 5. However, obtaining the set of explanations is not an easy task. Even if we had an easy way to exactly compute an optimal adversarial strategy, finding all such strategies is an even more challenging problem. In fact, it is at least as hard as the counting version of GCD , which we already have shown #P-hard and difficult to approximate. This is shown in the following theorem.

T HEOREM 4.3. Finding the set of all adversarial op timal strategies that provide a  X  X es X  answer to OAS-DEC is # P-hard. In this section, we present several algorithms to solve OAS. We first present a simple routine for preprocessing followed by a naive enumeration-based algorithm.
We use to denote the maximum number of partners per observation and f to denote the maximum number of observations supported by a single partner. In general, is bounded by  X  (  X  2  X   X  2 ), but may be lower depending on the feasible points in S . Likewise, f is bounded by min( | O | , ) but may be much smaller depending on the sparseness of the observations.

Preprocessing Procedure. Given a space S , a feasibility predicate feas , real numbers  X   X  0 , X  &gt; 0, and a set O of observations, we create two lists (similar to a standard inverted index) as follows.  X  Matrix M. M is an array of size S . For each feasible point p  X  S , M [ p ] is a list of pointers to observations. M [ p ] contains pointers to each observation o such that feas ( p )istrueandsuchthat d ( o , p )  X  [  X ,  X  ].  X  List L. List L contains a pointer to position M [ p ] in the array M iff there exists an observation o  X  O such that feas ( p )istrueandsuchthat d ( o , p )  X  [  X ,  X  ]. It is easy to see that we can compute M and L in O ( | O | X  ) time. The next example shows how M , L apply to our running drug example.

Example 4.2. Consider our running example concerning the location of drug labo-ratories that started with Example 2.1. The set L consists of { p 1 ,..., p 67 } .Thematrix M returns lists of observations that can be associated with each feasible point. For example, M ( p 40 )= { o 3 , o 4 , o 5 } and M ( p 46 )= { o 1 , o 2 } .

Naive Approach. After preprocessing, a straightforward exact solution to OAS would be to enumerate all subsets of L that have a cardinality less than or equal to k .Letus call this set L  X  . Next, we eliminate all elements of L  X  that are not valid explanations. Finally, for each element of L  X  , we compute the expected adversarial detriment and return the element of L  X  for which this value is the least. Clearly, this approach is impractical as the cardinality of L  X  can be very large. Further, this approach does not take advantage of the specific reward functions. We now present Mixed Integer Linear Programs (MILPs) for wrf and frf and later look at ways to reduce the complexity of solving these MILPs. We present Mixed Integer Linear Programs (MILPs) to solve OAS exactly for some specific reward functions. First, we present a mixed integer linear program for the reward function wrf . Later, in Section 4.4, we show how to improve efficiency (while maintaining optimality) by reducing the number of variables in the MILP. Note that these constraints can also be used for crf as wrf generalizes crf . We also define a MILP for the frf reward function.

While these mixed integer programs may appear nonlinear, Proposition 4.4 gives a simple transformation to standard linear form. For readability, we define the MILPs before discussing this transformation.

Definition 4.3 ( wrf MILP ). Given real number dist &gt; 0 and weight function W ,as-sociate a constant w i with the weight W ( p i )ofeachpoint p i  X  L .Next,foreach p i  X  L wise. Finally, associate an integer-valued variable X i with each p i  X  L . Minimize: subject to: (1) X i  X  X  0 , 1 } ; (2) Constraint p (3) For each o j  X  O , add constraint
Example 4.3. Continuing from Examples 4.1 and 4.2, suppose the drug car-tel wishes to produce an adversarial strategy A using wrf . Consider the case where we use crf , k  X  3, and dist = 100 meters as before (see Example 4.1). of set L (as per Example 4.2). The constants c i , 1 are 1 for elements in the set { p for all others).
 We can create a MILP for frf as follows.

Definition 4.4 ( frf MILP ). For each p i  X  L and ef j  X  EF , let constant c i , j = Minimize: subject to: (1) X i  X  X  0 , 1 } ; (2) Constraint p (3) For each o j  X  O , add constraint The following theorem tells us that solving the preceding MILPs correctly yields a solution for the OAS problem under both wrf or frf .

P ROPOSITION 4.1. Suppose S is a space, O is an observation set, real numbers  X   X  0 ,  X &gt; 0 , and suppose the wrf and frf MILPs are defined as earlier. ( 1 ) Suppose A  X { p 1 ,..., p n } is a solution to OAS with wrf (respectively frf ). Consider ( 2 ) Given the solution to the constraints, if for every X i =1 ,weaddpointp i to set A ,
Setting up either set of constraints can be performed in polynomial time, where computing the c i , j constants is the dominant operation.
 P ROPOSITION 4.2. Setting up the wrf / frf constraints can be accomplished in O ( | EF | X  k  X | O | X  ) time (provided the weight function W can be computed in constant time).

The number of variables for either set of constraints is related to the size of L ,which depends on the number of observations, spacing of S ,and  X ,  X  .

P ROPOSITION 4.3. The wrf / frf constraints have O ( | O | X  ) variables and 1+ | O | constraints.
 The MILPs for wrf and frf appear nonlinear as the objective function is fractional. However, as the denominator is nonzero and strictly positive, the Charnes-Cooper transformation [Charnes and Cooper 1962] allows us to quickly (in the order of number of constraints multiplied by the number of variables) transform the constraints into a purely integer linear form. Many linear and integer linear program solvers include this transformation in their implementation.

P ROPOSITION 4.4. The wrf / frf constraints can be transformed into a purely linear integer form in O ( | O | 2  X  ) time.

We note that a linear relaxation of any of the aforesaid three constraints can yield a lower bound on the objective function in O ( | L | 3 . 5 )time.

P ROPOSITION 4.5. Given the constraints of Definition 4 . 3 or Definition 4 . 4 ,ifwe consider the linear program formed by setting all X i variables to be in [0 , 1] , then the value returned by the objective function will be a lower bound on the value returned by the objective function for the mixed integ er linear constraints, and this value can be obtained in O ( | O | 3 . 5  X  3 . 5 ) time.

Likewise, if we solve the mixed integer linear program with a reduced number of variables, we are guaranteed that the solution will cause the objective function to be an upper bound for the original set of constraints.

P ROPOSITION 4.6. Consider the MILPs in Definition 4.3 and Definition 4.4. Sup-pose L  X  L and every variable X i associated with some p i  X  L is set to 0 . The resulting solution is an upper bound on the objective function for the constraints solved on the full set of variables. As the complexity of solving MILPs is closely related to the number of variables in the MILP, the goal of this section is to reduce the number of variables in the MILP associated before with the crf reward function. We note that all results in this section applyonlyforthe crf reward function. In this section, we show that if we can find a certain type of explanation called a  X  -core optimal explanation, then we can  X  X uild-up X  an optimal adversarial strategy in polynomial time. It also turns out that finding these special explanations can be accomplished using an MILP which will often have significantly fewer variables than the MILPs of the last section. First, we consider the wrf constraints applied to crf which is a special case of wrf . The objective function for this case is and 0 otherwise. If we rearrange the objective function, we see that with each X i variable associated with point p i  X  L , there is an associated constant const i . This lets us rewrite the objective function as
Example 4.4. Continuing from Example 4.3, const i =0 . 5 for the following ele-{ p
In many covering problems where we wish to find a cover of minimal cardinality, we could reduce the number of variables in the integer program by considering equiv-alent covers as duplicate variables. However, for OAS, this technique cannot be easily applied. The reason for this is because an optimal adversarial explanation is not neces-sarily irredundant (see Definition 2.5). Consider the following. Suppose we wish to find an optimal adversarial strategy of size k .Let P be an irredundant cover of size k  X 1. Suppose there is some element p  X  P that covers only one observation o . Hence, there is no p  X  P  X  X  p } that covers o by the definition of an irredundant cover. Suppose there is also some p /  X  P that also covers o . Now, let m = p tion of an example solution to OAS that is not irredundant, we let const be the value associated with both p and p . Consider the scenario where const &lt; m k  X  2 . Suppose by way of contradiction that the optimal irredundant cover is also the optimal adversarial strategy. Then, by the definition of an optimal adversarial strategy we know that the clear that a solution to OAS need not be irredundant.

Even though an OAS is not necessarily irredundant, we are able to reduce the size of the set L by looking at certain aspects of an OAS. Our intuition is that each OAS contains a core explanation which has fewer redundant elements than the OAS and low values of const for each element in that set. Once this type of explanation is found, we can build an optimal adversarial strategy in polynomial time. First, we define a core explanation.

Definition 4.5 ( Core Explanation ). Given an observation set O and set L of possible exist p j  X  L such that: (1)  X  o  X  O if o , p i are partners, then o , p j are also partners. (2) const j &lt; const i .

We now show that any optimal adversarial strategy contains a subset that is a core explanation.

T HEOREM 4.4. If A is an optimal adversarial strategy, there exists a core explana-tion E core  X  A .

Example 4.5. Continuing from Example 4.4, consider the set A  X { p 34 , p 38 , p 57 } (which would correspond to drug lab loca tions as planned by the cartel). Later, we show that this is an optimal adversarial strategy (the expected adversarial detriment associated with A is 0). Consider the subset p 34 , p 38 .As p 34 explains observations o , o ously, it is of minimal cardinality. Hence, the set { p 34 , p 38 } is a core explanation of A .
 Suppose we have an oracle that, for a given k , O ,and efd returns a core explanation E core that is guaranteed to be a subset of the optimal adversarial strategy associated with k , O ,and efd . The following theorem says we can find the optimal adversarial strategy in polynomial time. The key intuition is that we need not concern ourselves with covering the observations as E core is an explanation. The algorithm BUILD-STRAT follows from this theorem.
 ALGORITHM 1: BUILD-STRAT
T HEOREM 4.5. If there is an oracle that for any given k, O ,and efd returns a core explanation E core that is guaranteed to be a subset of the optimal adversarial strategy associated with k, O ,and efd , then we can find an optimal adversarial strategy in O (  X | O | X  log(  X | O | )+( k  X  X  E core | ) 2 ) time.

We now introduce the notion of  X  -core optimal. Intuitively, this is a core explana-tion of cardinality exactly  X  that is optimal with respect to the expected adversarial detriment compared to all other core explanations of that cardinality.

Definition 4.6. Given an integer  X &gt; 0, an explanation distribution function efd , and a reward function rf , a core explanation E core is  X  -core optimal iff:  X  | E  X  There does not exist another core explanation E core of cardinality exactly  X  such that EXD rf ( efd , E core ) &lt; EXD rf ( efd , E core ).

We now define some subsets of the set L that are guaranteed to contain core expla-nations and  X  -core optimal explanations as well. In practice, these sets will be much smaller than L and will be used to create an MILP of reduced size.

Definition 4.7 ( Reduced Partner Set ). Given observations O and set of possible part-ners L , we define the reduced partner set L  X  X  X  as follows.
 L We define L  X  as follows.
 L
L EMMA 4.6. ( 1 ) If explanation E is a core explanation, then E  X  L  X  X  X  . ( 2 ) If explanation E is  X  -core optimal, then E  X  L  X  X  X  . ( 3 ) If for some natural number  X  , there exists an explanation of size  X  , then there exists The reduced partner set can be computed in polynomial time. We also note that under the assumption that | O | &lt;&lt; | L | , which we have found true in practice, determining the set L  X  X  X  or L  X  can be accomplished faster (in terms of time complexity) than solving even a relaxation of the original MILP.
 P ROPOSITION 4.7. Given set L, set L  X  and L  X  X  X  can be found in O ( | L | 2  X | O | 2 ) time.
Example 4.6. Let us continue from Example 4.5. Based on preprocessing and the computation of const i , we can easily produce the data of Table I in polynomial time. Based on this, we obtain a reduced partner set L  X   X { p 34 , p 38 , p 57 } .
Next, the following lemma tells us that an OAS must contain a core explanation that is  X  -core optimal.

L EMMA 4.6. Given an optimal adversarial strategy A , there exists some  X   X | A | such that there is a  X  -core optimal explanation that is a subset of A (using the crf reward function).

Thus, if we can find the  X  -core optimal explanation that is contained in an OAS, we can then find the OAS. If we know  X  , such an explanation can be found using an MILP. We now present a set of integer linear constraints to find a  X  -core optimal explanation. Of course we can easily adopt the constraints of the previous section, but this would offer us no improvement in performance. We therefore create an MILP that should have a significantly smaller number of variables in most cases.

To create this MILP, we take a given set of possible partners L and calculate the set L  X  (the reduced partner set), which often will have a cardinality much smaller than L .Next,weuse L  X  to form a new set of constraints to find a  X  -core optimal explanation. We now present these  X  -core constraints. Notice that the cardinality requirement in these constraints is  X = X  and not  X   X   X . This is because Lemma 4.6 ensures a core explanation that is  X  -core optimal, meaning that the core explana-tion must have cardinality exactly  X  . This also allows us to eliminate variables from the denominator of the objective function, as the denominator must equal  X  as well.

Definition 4.8 (  X  -core MILP ). Given parameter  X  and reduced partner set L  X  ,we define the  X  -core constraints by first associating a variable X i with each point p i  X  L  X  , then solving: Minimize: subject to: (1) X i  X  X  0 , 1 } . (2) Constraint p (3) For each o j  X  O , add constraint
Example 4.7. Using set L  X  from Example 4.6, we can create  X  -core constraints as follows: Minimize: subject to: (1) X 34 , X 38 , X 57  X  X  0 , 1 } (3) X 38  X  1 (for observation o 1 ) (4) X 38 + X 57  X  1 (for observation o 2 ) (5) X 34 + X 57  X  1 (for observation o 3 ) (6) X 34  X  1 (for observations o 4 , o 5 ) In the worst case, the set L  X   X  L . Hence, we can assert the following:
P ROPOSITION 4.8. The  X  -core constraints require O (  X | O | ) variables and 1+ | O | constraints.

P ROPOSITION 4.9. Given  X  -core constraints: ( 1 ) Given set  X  -core optimal explanation E core  X { p 1 ,..., p n } ,ifvariables
We now have all the pieces required to leverage core explanations and reduced part-ner sets to find an optimal adversarial strategy. By Theorem 4.5, we know that any optimal adversarial strategy must have a core explanation. Further, by Lemma 4.6, such a core explanation is  X  -core optimal. Using a (usually) much smaller mixed in-teger linear program, we can find such an explanation. We can then find the optimal adversarial strategy in polynomial time using BUILD STRAT . Though we do not know what  X  is, we know it must be in the range [1 , k ]. Further, using a relaxation of the OPT-KSEP-IPC constraints for solving geospatial abduction problems (as presented in Shakarian et al. [2010]), we can easily obtain a lower bound tighter than 1 on  X  .Hence, if we solve k such (most likely small) mixed integer linear programs, we are guaran-teed that at least one of them must be a core explanation for an optimal adversarial strategy. Wenotethatthese k MILPs can be solved in parallel (and the following k instances of BUILD-STRAT can also be run in parallel as well). An easy comparison of the results of the parallel processes would be accomplished at the end. As L  X  is likely to be significantly smaller than L , this could yield a significant reduction in complex-ity. Furthermore, various relaxations of this technique can be used (e.g., only using one value of  X  ).

Example 4.8. Continuing from Example 4.7, where the cartel members are at-tempting to find an OAS to best position drug laboratories, suppose they used the relaxation of OPT-KSEP-IPC (from Shakarian et al. [2010]) to obtain a lower bound on the cardinality of an explanation and found it to be 2. With k = 3, they would solve two MILPs of the form of Example 4.7: one with  X  =2andonewith  X  =3.Thesolution to the first MILP would set X 34 and X 38 both to 1 while the second MILP would set X 0, they are both optimal and running BUILD-STRAT is not necessary. Either { p 34 , p 38 } or { p 34 , p 38 , p 57 } can be returned as an OAS. Now that we have examined ways in which th e adversary can create a strategy based on probabilistic knowledge of the agent, we consider how the agent can devise an  X  X p-timal X  strategy to counter the adversary. As before, we use a special case of expected reward (Definition 3.1 from Section 3.9).

Definition 5.1 ( Expected Agent Benefit ). Given a reward function rf and explana-tion function distribution efd ,the expected agent benefit is the function EXB rf : 2 S  X  EFD  X  [0 , 1] defined as follows.

Example 5.1. Following from Examples 2.1 and 3.4, suppose drug-enforcement agents have information that the cart el is placing drug labs according to efd drug .(Such information could come from multiple runs of the GREEDY-KSEP-OPT2 algorithm of Shakarian et al. [2010]). The drug-enforcement agents wish to consider the set B  X { p function (note that k =3 , dist = 100, and rf = crf ). (Asanaside,wewouldliketopointouttheasymmetryin crf ; compare these compu-We now define a maximal counter-adversary strategy.

Definition 5.2 ( Maximal Counter-Adversary Strategy (MCA) ). Given a reward func-tion rf and explanation function distribution efd ,a maximal counter-adversary strat-egy , B , is a subset of S such that EXB rf ( B , efd ) is maximized.

Note that MCA does not include a cardinality constraint. This is because we do not require reward functions to be monotonic. In the monotonic case, we can triv-ially return all feasible points in S and be assured of a solution that maximizes the expected agent benefit. Therefore, for the monotonic case, we include an extra param-eter B  X  X  1 ,..., | S |} (for  X  X udget X ) which will serve as a cardinality requirement for B . This cardinality requirement for B is necessarily the same as for A as the agent and adversary may have different sets of re sources. Also,wedonotrequirethat B be an explanation. We discuss the special case where the solution to the MCA problem is required to be an explanation in the Appendix. We now formally define the problem of finding a maximal counter-adversary strategy. INPUT: Space S , feasibility predicate feas , real numbers  X ,  X  , set of observations O , natural numbers k , B , reward function rf , and explanation function distribution efd . OUTPUT: Maximal counter-adversary strategy B .
 MCA is NP-hard via a reduction of the GCD problem.
 T HEOREM 5.1. MCA is NP-hard.

The proof of the preceding result shows that MCA is NP-hard even if the reward function is monotonic . Later, in Section 5.3, we also show that MCA can encode the NP-hard MAX-K-COVER problem [Feige 1998] as well (which provides an alternate proof for NP-hardness of MCA). We now present the decision problem associated with MCA and show that it is NP-complete under reasonable conditions.
 INPUT: Space S , feasibility predicate feas , real numbers  X ,  X  , set of observations O , natural numbers k , B , reward function rf , explanation function distribution efd ,and number R  X  [0 , 1].
 OUTPUT: Counter-adversary strategy B such that EXB rf ( B , efd )  X  R .

T HEOREM 5.2. MCA-DEC is NP-complete, provided the reward function can be evaluated in PTIME.
 Not only is MCA-DEC NP-hard, under the same assumptions as earlier, the counting version of the problem is #P-complete and moreover, it has no fully polynomial random approximation scheme.
 T HEOREM 5.3. Counting the number of strategies that provide a  X  X es X  answer to MCA-DEC is # P-complete and has no FPRAS unless NP=RP.

Theorem 5.3 tells us that MCA may not have a unique solution. Therefore, setting up a mixed strategy of all MCAs to determine the  X  X est response X  to the MCA of an agent by an adversary would be an intractabl e problem. This mirrors our result of the previous section (Theorem 4.3). We now describe exact and approximate algorithms for finding a maximal counter-adversary strategy in the general case. Note that throughout this section (as well as in Section 5.3), we assume that the same preprocessing for OAS is used (refer to Section 4.2). We will use the symbol L to refer to the set of all possible partners.
An Exact Algorithm For MCA. A naive, exact, and straightforward approach to the MCA problem would simply consider all subsets of L and pick the one which maximizes the expected agent benefit. Obviously, this approach has a complexity O | S | i =0 | L | i and is not practical. This is unsurprising as we showed this to be an NP-complete problem.
Approximation in the General Case. Despite the impractical time complexity associated with an exact approach, it is possible to approximate MCA with guarantees, even in the general case. This is due to the fact that when efd is fixed, the expected agent benefit is submodular.
 ALGORITHM 2: ( MCA-LS )
T HEOREM 5.4. For a fixed O , k , efd , the expected agent benefit, EXB rf ( B , efd ) has the following properties: ( 1 ) EXB rf ( B , efd )  X  [0 , 1] . ( 2 ) For B  X  B and some point p  X  S where p /  X  B , the following is true:
It follows immediately that MCA reduces t o the maximization of a submodular func-tion. We now present the MCA-LS algorithm that leverages this submodularity.
The following two propositions leverage Theorem 5.4 and Theorem 3.4 of Feige et al. [2007].
 P ROPOSITION 5.1. MCA-LS has time complexity of O ( 1  X | L | 3  X  F ( efd )  X  lg( | L | ) where F ( efd ) is the time complexity to compute EXB rf ( B , efd ) forsomeset B  X  L. P ROPOSITION 5.2. MCA-LS is an ( 1 3  X  | L | ) -approximation algorithm for MCA.
Example 5.2. Let us consider our running example where drug-enforcement agents The agents have information that there are k or fewer drug laboratories that sup-port the poppy fields (set of observations O ) and that they are positioned according to efd drug (see Example 3.4). The agents wish to find a maximal counter-adversarial strategy using the prf reward function. They decide to use MCA-LS to find such a strategy with =0 . 1. Initially (at line 2), the algorithm selects point p 48 (renum-bering as p 1 , note that in this example we shall use p i and inc i numbering based on Example 2.1 rather than what the algorithm uses). Hence, inc 40 =0 . 208 and cur v al =0 . 708. As the elements are sorted, the next point to be considered in the loop at line 2 is p 40 which has an incremental increase of 0, so it is not picked. It then proceeds to point p 41 , which gives an incremental increase of 0 . 084 and is added to B so cur v al =0 . 792. Point p 45 is considered next, which gives an incremental in-point p 46 , which does not afford any incremental increase. After considering points p cremental increase (and thus, are not picked), the algorithm finds that the old incre-mental increase of the next element, p 1 , would cause the  X  X f  X  statement at line 4c to be true, thus proceeding to the inner loop inside that  X  X f  X  statement (line 4(c)iiA). This loop considers if the removal of any of the picked elements p 48 , p 41 , p 45 causes the expected agent benefit to increase. However, in this example, if any of the elements are removed, the expected agent benefit decreases. Hence, the boolean flag 1isset to false and the algorithm exits the outer loop. The algorithm then returns the set B  X { p In the previous section we showed a 1 3 approximate solution to MCA can be found in polynomial time even without any monotonicity restriction. In this section, we show that under the additional assumptions of monotonicity of reward functions, we can obtain a better 63% approximation ratio with a faster algorithm. Here, we also have the additional cardinality requirement of B for the set B (as described in Section 5). We first show that expected agent benefit is monotonic when the reward function is.
C OROLLARY 5.1. For a fixed O , k , efd , if the reward function is monotonic, then the expected agent benefit, EXB rf ( B , efd ) is also monotonic.

Thus, when we have a monotonic reward function, the MCA problem reduces to the maximization of a monotonic, normalized 5 submodular function with respect to a uniform matroid 6 ; this is a direct consequence of Theorem 5.4 and Corollary 5.1. Therefore, we can leverage the result of Nemhauser et al. [1978], to develop the MCA-GREEDY-MONO algorithm that follows. We improve performance by including  X  X azy evaluation X  using the intuition that the incremental increase caused by some point p at iteration i of the algorithm is greater than or equal to the increase caused by that point at a later iteration. As with MCA-LS , we also sort elements by the incremental increase, which may allow the algorithm to exit the inner loop earlier. In most nontriv-ial instances of MCA, this additional sorting operation will not affect the complexity of the algorithm (i.e., under the assumption that the time to compute EXB rf is greater than lg( | L | ), we make this same assumption in MCA-LS as well). ALGORITHM 3: ( MCA-GREEDY-MONO )
P ROPOSITION 5.3. The complexity of MCA-GREEDY-MONO is O ( B  X | L | X  F ( efd )) where F ( efd ) is the time complexity to compute EXB rf ( B , efd ) for some set B  X  Lof size B. In the first iteration of the algorithm, we have the next corollary. C OROLLARY 5.2. MCA-GREEDY-MONO is an ( e e  X  1 ) -approximation algorithm for MCA (when the reward function is monotonic).

In addition to the fact that MCA-GREEDY-MONO is an ( e e  X  1 )-approximation algo-rithm for MCA, it also provides the best possible approximation ratio unless P = NP . This is done by a reduction of MAX-K-COVER [Feige 1998].
 T HEOREM 5.5. MCA-GREEDY-MONO provides the best approximation ratio for MCA (when the reward function is monotonic) unless P = NP.
 The following example illustrates how MCA-GREEDY-MONO works.

Example 5.3. Consider the situation from Example 5.2, where the drug-enforcement agents are attempting to locate illegal drug labs. Suppose they want to locate the labs, but use the crf reward function, which is monotonic and zero-starting. They use the cardinality requirement B =3in MCA-GREEDY-MONO .Afterthefirst iteration of the loop at line 3, the algorithm selects point p 48 as it affords an incremen-tal increase of 0 . 417. On the second iteration, it selects point p 46 , as it also affords an incremental increase of 0 . 417, so last v al =0 . 834. Once p 46 is considered, the next point considered is p 33 , which had a previous incremental increase (calculated in the first iteration) of 0 . 25, so the algorithm can correctly exit the loop to select the final element. On the last iteration of the outer loop, the algorithm selects point p 35 ,which gives an incremental increase of 0 . 166. Now the algorithm has a set of cardinality expected agent benefit of 1, which is optimal. Note that this would not be an optimal solution for the scenario in Example 5.2 which uses prf as p 35 would incur a penalty (which it does not when using crf as in this example).
 In this section, we describe prototype implementations and experiments for solving the OAS and MCA problems. For OAS, we create an MILP for the crf case and reduce the number of variables with the techniques we presented in Section 4. For MCA, we implement both the MCA-LS and MCA-GREEDY-MONO .

We carried out all experiments for MCA on an Intel Core2 Q6600 processor running at 2.4 GHz with 8GB of memory available, using code written in Java 1.6; all runs were performed in Windows 7 Ultimate 64-bit using a 64-bit JVM, and made use of a single core. We also used functionality from the previously implemented SCARE soft-ware [Shakarian et al. 2009] to calculate, for example, the set of all possible partners L and to perform preprocessing (see the discussion in Section 4.2).

Our experiments are based on 21 months of real-world Improvised Explosive Device (IED) attacks in Baghdad 7 [Shakarian et al. 2009]. The IED attacks in this 25  X  27 km region constitute our observations. The data also includes locations of caches asso-ciated with those attacks discovered by U.S. forces. These constitute partner locations. We used data from the International Medical Corps to define feasibility predicates based on ethnic makeup, location of U.S. bases, and geographic features. We overlaid a grid of 100m  X  100m cells, about the size of a standard U.S. city block. We split the data into two parts; the first 7 months of data were used as a  X  X raining X  set to learn the [  X ,  X  ] parameters and the next 14 months of data were used for the observa-tions. We created an explanation function distribution based on multiple runs of the GREEDY-KSEP-OPT2 algorithm described in Shakarian et al. [2010]. We now present experimental results for the version of OAS, with the crf reward func-tion, based on the constraints in Definition 4.3 and variable reduction techniques of Section 4.4. First, we discuss promising real-world results for the calculation of the reduced partner set L  X  , described in Definition 4.5. Then, we show that an optimal adversarial strategy can be computed quite tractably using the methods discussed in Section 4.4. Finally, we compare our results to a set of real-world data, showing a significant decrease in the adversary X  X  exp ected detriment across various parameter settings. Our implementation was written on top of the QSopt 8 MILP solver and used 900 lines of Java code.

Reduced Partner Set. As discussed in Section 4.2, producing an optimal adversarial strategy for any reward function relies heavily on efficiently solving a (provably worst-case intractable) integer linear program. The number of integer variables in these programs is based solely on the size of the partner set L ; as such, the ability to exper-imentally solve OAS relies heavily on the size of this set.

Our real-world data created a partner set L with cardinality 22,692. We then ap-plied the method from Definition 4.5 to reduce this original set L to a smaller subset of possible partners L  X  , while retaining the optimality of the final solution. This simple procedure, while dependent on the explanation function distribution efd as well as the cutoff distance for crf , always returned a reduced partner set L  X  with cardinality be-tween 64 and 81. This represents around a 99 . 6% decrease in the number of variables required in the subsequent integer linear programs!
Figure 4 provides more detailed accuracy and timing results for this reduction. Most importantly, regardless of parameters chos en, our real-world data is reduced by orders of magnitude across the board. Of note, we see a slight increase in the size of the re-duced set L  X  as the size of the explanation function distribution efd increases. This can be traced back to the strict inequalit y in Definition 4.7. As we increase the num-ber of nontrivial explanation functions in efd , the number of nonzero constants const i increases. This results in a higher number of candidates for the intermediary set L  X  X  X  . We see a similar result as we increase the penalizing cutoff distance. Again, this is a factor of the strict inequality in Definition 4.7 in conjunction with a higher fraction of nonzero const i constants.

Interestingly, Figure 4 shows a slight de crease in the runtime of the reduction as we increase the penalizing cutoff distance. Initially, this seems counterintuitive; with more nontrivial constants const i , the construction of the intermediary set L  X  X  X  requires more work. However, this extra work pays off during the computation of the final re-duced set L  X  . In our experiments, the reduction from L to L  X  X  X  took less time than the final reduction from L  X  X  X  to L  X  . This is due to frequent short circuiting in the computa-tion of the right-hand side of the conjunction during L  X  X  X  creation. As we increase the penalizing cutoff distance, the size of L  X  X  X  actually decreases, resulting in a decrease in the longer computation of L  X  . As seen before, this decrease in L  X  X  X  did not correspond to a decrease in the size of L  X  .

Optimal Adversarial Strategy. Using the set L  X  , we now present results to find an opti-mal adversarial strategy using  X  -core optimal explanations. This is done by minimiz-ing the MILP of Section 4.4, then feeding this solution into BUILD-STRAT .Sincewedo not know the value of  X  in advance, we must perform this combined operation multiple times, choosing the best (lowest expected detriment) adversarial strategy as optimal.
A note on the lower bound for  X  : As shown by Shakarian et al. [2009], finding a minimum-cardinality explanation is NP-hard. Because of this, it is computationally difficult to find a tight lower bound for  X  . However, this lower bound can be estimated empirically. For instance, for our set of real-world data from Baghdad, an explanation of cardinality below 14 has never been returned, even across tens of thousands of runs of GREEDY-KSEP-OPT2 . Building on this strong empirical evidence, the minimum  X  used in our experiments is 14.

Figure 5 shows both timing and expected detriment results as the size of the expla-nation function | efd | and maximum strategy cardinality k are varied. Note that a lower expected detriment is better for the adversary, with zero representing no probability of partner discovery by the reasoning agent. As the adversary is allowed larger and larger strategies, its expected detriment smoothly decreases toward zero. Intuitively, as the number of nontrivially-weighted explanation functions in efd increases, the ex-pected detriment increases as well. This is a side-effect of a larger | efd | allowing the reasoning agent to cover a larg er swath of partner locations.

Recall that, as the maximum k increases, we must solve linear programs for each  X   X  X  k k low = 14. As k increases, we see a near-linear increase in the total runtime of the set of integer programs. Due to the reduced set L  X  , we are able to solve dozens of integer programs in less than 800ms; were we to use the unreduced partner set L , this would be intractable. Note that the runtime graph includes that of BUILD-STRAT which always ran in under sixteen milliseconds.

OAS Performance with respect to Real-World Adversarial Strategy. Figure 6 compares the expected number of caches found under the current, state-of-the-art X  X ED cache lo-cations based on 21 months of real-world data from Baghdad, Iraq X  X gainst the OAS strategy proposed in this article. We hold the cardinality of the adversary X  X  solution (i.e., the number of possible caches) to 14 to match the real-world data. We assume the reasoning agent uses the Spatial Cul tural Abductive Reasoning Engine (SCARE) introduced in Shakarian et al. [2009] to provide partner locations to these attacks. SCARE is the state-of-the-art method for finding IED caches.

When tested against real-world adversaries based on real-world Baghdad data, OAS significantly outperforms what adversari es have done so far in the real world (fortu-nately this is balanced by later experiment results showing that MCA-LS and MCA-GREEDY-MONO significantly outperform SCARE). The expected number of caches found by SCARE against an opponent using OAS is significantly lower than against present-day insurgents in Iraq. For instance, while SCARE (using a cutoff distance of 100 meters) detects 1.6 of the 14 possible caches against a real-world adversary, it is expected to detect only 0.11 of the caches against an adversary using OAS. This roughly order of magnitude improvement is seen across all five cutoff distances, from a minimum of approximately 7x at a cutoff distance of 200m to a maximum of over 31x at a distance of 500m. Thus, OAS significantly improves the adversary X  X  performance. First, we briefly discuss an implementation of the naive MCA algorithm discussed in Section 5.2. Next, we provide promising results for the MCA-LS algorithm using the prf reward function. Finally, we give results for the MCA-GREEDY-MONO using the monotonic crf reward function, and qualitatively compare and contrast the results from both algorithms.

MCA-Naive. The naive, exact solution to MCA (considering all subsets of L with cardinality k B or more and picking the one which maximizes the expected agent ben-efit) is inherently intractable. This approach has a complexity O ( | L | k worse by the large magnitude of the set L . In our experimental setup, we typically saw | L | &gt; 20 , 000; as such, for even the trivially small k B = 3, we must enumerate and rank over a trillion subsets. For any realistic value of k B , this approach is simply unusable. Luckily, we will see that both MCA-LS and MCA-GREEDY-MONO provide highly tractable and accurate alternatives.
 MCA-LS. In sharp contrast to the naive algorithm described previously, the MCA-LS algorithm provides (lower-)bounded approximate results in a tractable manner. Interestingly, even though MCA-LS is an approximation algorithm, in our experiments on real-world data from Baghdad using the prf reward function, the algorithm re-turned strategies with an expected benefit of 1.0 on every run. Put simply, on our practical test data, MCA-LS always completely maximized the expected benefit. This significantly outperforms the lower-bound approximation ratio of 1 / 3. We would also like to point out that this is the first implementation (to the best of our knowledge) of the nonmonotonic submodular maximization approximation algorithm of Feige et al. [2007].

Since the expected benefit was maximal for every strategy B returned, we move to analyzing the particular structure of these strategies. Figure 7 shows a relationship between the size | B | , the cutoff distance dist , and the cardinality of the expectation function distribution | efd | .Recallthat prf penalizes any strategy that does not com-pletely cover its input set of observations; as such, intuitively, we see that MCA-LS returns larger strategies as the penalizing cutoff distance decreases. If the algorithm can cover all possible partners across all expectation functions, it will not receive any penalty. Still, even when dist is 100m, the algorithm returns B only roughly twice the size as minimum-sized explanation found by GREEDY-KSEP-OPT2 (which, based on the analysis of Shakarian et al. [2010], is very close to the minimum possible ex-planation). As the cutoff dist increases, the algorithm ret urns strategies with sizes converging, generally, to a baseline: the smallest-sized explanation found by the al-gorithm of Shakarian et al. [2010], | E | . This is an intuitive soft lower bound; given enough leeway from a large distance dist , a single point will cover all expected part-ners. This is not a strict lower bound in that, given two extremely close observations with similar expected partners, a single point may sufficiently cover both.
In Figure 8, we see results comparing overa ll computation time to both the distance dist and the cardinality of efd . For more strict (i.e., smaller) values of dist ,thealgo-rithm (which, under prf , is penalized for all uncovered observations across efd )must spend more time forming a strategy B that minimizes penalization. Similarly, as the distance constraint is loosened, the algorithm completes more quickly. Finally, an in-crease in | efd | results in higher computational cost; as explained in Proposition 5.1, this is due to an increase in F ( efd ), the time complexity of computing EXB rf ( B , efd ). Comparing these results to Figure 7, we see that the runtime of MCA-LS is correlated to the size of the returned strategy B .

MCA-GREEDY-MONO. As discussed in Section 5.3, MCA-GREEDY-MONO provides tighter approximation bounds than MCA-LS at the cost of a more restrictive (mono-tonic) reward function. For these experiments, we used the monotonic rf = crf .Recall that a trivial solution to MCA given a monotonic reward function is B = L ; as such, MCA-GREEDY-MONO uses a budget B to limit the maximum size | B || L | .Wevaried this parameter B  X  X  1 ,..., 28 } .

Figure 9 shows the expected benefit EXB rf ( B , efd ) increases as the maximum al-lowed | B | increases. In general, the expected benefit of B increases as the distance constraint dist is relaxed. However, note the points with B  X  X  3 ,..., 9 } ;weseethat dist  X  100 performs better than dist &gt; 100. We believe this is an artifact of our real-world data. Finally, as | efd | increases, the expected benefit of B converges more slowly to 1 . 0. This is intuitive, as a wider spread of possible partner positions will, in general, require a larger | B | to provide coverage.
 Figure 10 shows that the runtime of MCA-GREEDY-MONO increases as predicted by Proposition 5.1. In detail, as we linearly increase budget B , we also linearly increase increases quadratically in B , for our specific reward function. Finally, note the increase in runtime as we increase | efd | =10to | efd | = 100. Theoretically, this increases F ( efd ) linearly; in fact, we see almost exactly a ten-fold increase in runtime given a tenfold increase in | efd | .

MCA Algorithms and SCARE. We now compare the efficacy of the two MCA algorithms proposed in this article to SCARE [Shakarian et al. 2009] which represents the cur-rent state-of-the-art as far as IED cache detection is concerned. Again, our experi-ments are based on real-world data from Baghdad, Iraq. For these experiments, we average results across 100 runs of SCARE; as such, we hold | efd | = 100 static for the MCA-based algorithms. Figure 11 plots the average number of predicted points within 500 meters of an actual cache for both MCA-LS and MCA-GREEDY-MONO . SCARE, plotted as a horizontal line, predicts an average of 7.87 points within 500 meters of caches. MCA-LS finds over twice as many points at low penalizing cutoff distances, and steadily converges to SCARE X  X  baseline a s the penalizing distance increases (as expected). As shown earlier in Figure 7, MCA-LS tends to find larger strategies given a smaller penalizing cutoff distance; in turn, these larger strategies yield more close points to actual caches. MCA-GREEDY-MONO shows similar behavior; as we increase the allowable budget (i.e., maximum strategy size), more points are within 500 meters of a real-world cache location. Thus, MCA-LS and MCA-GREEDY-MONO both outper-form SCARE, enabling more caches to be discovered.

We note that while the number of points in the strategy close to a real-world cache location is higher in the MCA-based algorithms than SCARE, the fraction of close points stays consistently close. SCARE returns a solution of size 14, with approximately half (7 . 87 / 14  X  56%) of these points within 500 meters of cache. Compare this to, for instance, MCA-LS with a penalizing cutoff distance of 300 meters; for these settings, the algorithm returns an average strategy size of 18, with 11 points (approximately 60%) within 500 meters of a cache location. This behavior is a product of the strategy size flexibility built into the MCA-based algorithms, and is beneficial to the reasoning agent. For example, assume the minimal solution to a problem is of size 2 and the reasoning agent has a budget of size 4. Now assume SCARE finds 1 / 2 = 50% of the points near caches, while MCA-GREEDY-MONO finds 2 / 4 = 50% of its points near caches. Both algorithms returned the same fraction of points near caches; however, the reasoning agent will spend its budget of 4 resources more effectively under MCA-GREEDY-MONO , instead of wasting two of its resources under the strategy provided by SCARE. Geospatial abduction was introduced in Shakarian et al. [2010] and used to infer a set of partner locations from a set of observations, given a feasibility predicate and an interval [  X ,  X  ]  X  [0 , 1]. The authors developed exact and approximate algorithms we study the case of geospatial abduction where there is an explicit adversary who is interested in ensuring that the agent does not detect the partner locations. This is the case with real-world serial killers and insurgents who launch IED attacks. In this article, we develop a game-theoretic framework for reasoning about the best strategy that an adversary might adopt (based on minimizing the adversary X  X  detriment) and the best strategy that the agent could adopt to counter the adversary X  X  strategy. All this is uncharted territory and represents a novel contribution of this article. In fact, everything from Section 3 onwards in this article is new.

Although abduction [Peirce 1955] has been studied in a variety of different contexts X  X edicine [Peng 1986; Peng and Reggia 1990], fault diagnosis [Console et al. 1991], belief revision [Pagnucco 1996], database updates [Console et al. 1995; Kakas and Mancarella 1990], and AI planning [do Lago Pereira and de Barros 2004] X  X e are not aware of any work in abduction where an adversary selects a ground-truth explanation with respect to a probability distribution over explanation functions that an agent would consider. Additionally, we are not aware of any related work dealing with the problem of an agent finding elements of an adversarially selected explanation (with respect to a probability distribution). However, we do believe that many of the techniques introduced here for adversarial geospatial abduction may be generalized to other forms of abduction as well.

In the field of operations research, the facility location problem [Stollsteimer 1963] is a well-studied problem dealing with optimal placement of facilities in a plane, net-work, or multidimensional space. The facilities must be positioned to optimize some sort of distance to the  X  X emand points X , most likely resembling consumers of the items being produced at the facility. In Shakarian et al. [2010], the authors outline numerous differences between facility location and geospatial abduction (difference in optimality criteria, use of feasibility predicate, nonconvexitivity of covers, etc.), even when no ad-versaries are present. However, facility location with adversaries has not really been studied, and that is the focus of this article.

Similar motivation exists in the field of (multi-)agent security, where the central idea is to protect a set of targets from adversaries. These games are typically modeled on top of graphs, with agents and adversaries competing to protect or penetrate a set of targets. Paruchuri et al. [2006] represent the adversary X  X  behavior through a probabil-ity distribution over states, indicating the probability of that state being targeted; no real graph structure is considered, much less a geospatial model. Agmon et al. [2008, 2009] consider an environment with more hidden information, and attempt to detect adversarial penetrations across the routes (represented as paths on a graph) of pa-trolling agents. Pita et al. [2009] solve Stackelberg (leader-follower) games under the assumption of bounded reasoning rationality, again on a graph network. Dickerson et al. [2010] explores protecting dynamic targets from rational adversaries on real-world road networks. Geospatial abduction was introduced in Shakarian et al. [2010] and used to infer a set of partner locations from a set of observations, given a feasibility predicate and reals  X   X  0 , X  &gt; 0. Shakarian et al. [2010] developed exact and approximate algorithms for GAPs. In particular, no adversary was assumed to exist there. In this article, we study the case of geospatial abduction where there is an explicit adversary who is interested in ensuring that the agent does not detect the partner locations. This is the case with real-world serial killers and insurgents who launch IED attacks. We develop a game-theoretic framework for reasoning about the best strategy that an adversary might adopt (based on minimizing the adversary X  X  detriment) and the best strategy that the agent could adopt to counter the adversary X  X  strategy.

We consider the adversarial geospatial abduction problem to be a two-player game: an agent ( X  X ood X  guy) and an adversary ( X  X ad X  guy). The adversary is attempting to cause certain observable events to occur (e.g., murders or IED attacks) but make it hard to detect the associated set of partner locations (e.g., location of the serial killer X  X  home/office, or the locations of weapons caches supporting the IED attacks). We use an axiomatically-defined  X  X eward function X  to determine how similar two explanations are to each other. We study the problems of finding the best response for an agent and adversary to a mixed strategy (based on a probability distribution over explanations) of the opponent. We formalize these problems as the Optimal Adversarial Strategy (OAS) and Maximal Counter-Adversary strategy (MCA) problem. We show both OAS and MCA to be NP-hard and provide exact and approximate methods for solving them.
When reasoning about the best possible strategy for the adversary, we present a mixed-integer-programming-based algorithm and show that the MILP in question can be greatly reduced through the elimination of many variables using the concept of a  X  -core explanation. Our experiments are carried out on real-world data about IED attacks over a period of 21 months in Baghdad.

When reasoning about the best possible strategy for the adversary, we present two algorithms. The MCA-LS algorithm is very general and leverages submodularity of re-ward functions. The MCA-GREEDY-MONO algorithm assumes the reward function is monotonic. Both MCA-LS and MCA-GREEDY-MONO are highly accurate and have very reasonable time frames. Though MCA-GREEDY-MONO is slightly faster than MCA-LS , we found that on every single run, MCA-LS found the exact optimal benefit even though its theoretical lower-bound approximation ratio is only 1 / 3, a truly remark-able performance. As MCA-LS does not require any additional assumptions and as its running time is only slightly slower than that of MCA-GREEDY-MONO , we believe this algorithm has a slight advantage.
 The electronic appendix for this article can be accessed in the ACM Digital Library.
