
Identifying redundant information in sentences is useful for several applications such as summarization, document provenance, detecting text reuse and novelty detection. Th e task of identifying redundant information in sentences is d e-fined as follows: Given a query sentence the task is to re-trieve sentences from a given collection that express all or some subset of the information present in the query sentence .
Sentence retrieval techniques rank sentences based on some measure of their similarity to a query. The effectiveness of such techniques depends on the similarity measure used to rank sentences. An effective retrieval model should be able to handle low word overlap between query and candi-date sentences and go beyond just word overlap. Simple language modeling techniques like query likelihood retrie val have outperformed TF-IDF and word overlap based meth-ods for ranking sentences. In this paper, we compare the performance of sentence retrieval using different language modeling techniques for the problem of identifying redun-dant information.
 Categories and Subject Descriptors: H3.3 [Information Storage and Retrieval]: Information search and Retrieval General Terms: Algorithms, Experimentation, Theory Keywords: Sentence Retrieval, Language Modeling
Previous work on novelty detection [9] and summariza-tion[3] has explored several sentence level similarity mea -sures and retrieval techniques for identifying novel infor ma-tion. Metzler et al [6] showed that query likelihood outper-formed TF-IDF and word overlap based measures for iden-tifying sentences that contain some specific facts containe d in a query sentence. Murdock [8] compared query likelihood and a mono-lingual translation based model for the task of identifying restatements. Jeon et al [4] describe a mixture model of query likelihood and a translation based model for successfully identifying similar questions. Our work exte nds [6] and [8] by considering more sophisticated language mod-eling techniques such as topic based smoothing, dependence models and relevance modeling to improve retrieval effec-tiveness for identifying redundant information.
 Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
The experiments to detect redundant information were conducted on a dataset prepared by Murdock [8]. The dataset consists of documents and 50 queries that are answer sen-tences for some TREC 2003 QA track questions. For each query documents were retrieved using query likelihood re-trieval. The top 1000 retrieved documents were sentence segmented, stemmed using Krovetz stemmer and the sen-tences were indexed. The queries were then used to retrieve sentences from their corresponding sentence indexes. For each query the top 50 retrieved sentences using query likeli -hood and translation based retrieval models were manually judged for the amount of redundant information contained in them.
We compare advanced techniques within the language mod-eling framework to improve sentence retrieval effectivenes s.
Language modeling techniques primarily rely on estimates of word generation probabilities from document and query models. The estimates of word generation probabilities fro m small units of text such as sentences are not reliable and need to be smoothed. The probabilities are smoothed using a linear interpolation of estimates from a topic model, buil t from the top 1000 documents retrieved from the document index for each query, and general English.
Modeling query term dependencies has been shown to im-prove retrieval effectiveness for document retrieval [7]. W e use the sequential dependence model described in [7] and ignore the full dependence model as it does not scale to long queries. Sequential dependence models capture term de-pendencies between adjacent terms in the query and relax the independence assumptions made by the query-likelihood model to some degree.
Relevance modeling [5] is a technique for estimating query models from top ranked retrieved documents. Diaz et al [2] show that using a larger external corpus to build relevance models performs better than building relevance models usin g the target collection alone. We compare the effectiveness of relevance models built from target and external collection s.
Murdock [8] showed that Model-S, a translation based model, is effective for sentence retrieval, especially for q ues-tion answering and novelty detection tasks. However, for the task of identifying redundant information at sentence level, Model-S is only as effective as a simple query likeli-hood retrieval. We compare the effectiveness of Model-S and a mixture model [4] using a large, relatively robust mono-lingual translation dictionary [4].
Results are reported for the following retrieval technique s. 1. Query Likelihood Baseline (QL) 2. Topic Smoothing (QL-TS) -Collection estimates lin-3. Sequential Dependence Model (DM) -A weighted com-4. Translation Model (Model-S) -A translation based 5. Mixture Model (MM) -A mixture model of query like-6. Relevance Model-Target (RM-T) -Query model built 7. Relevance Model-External + Target (RM-E) -Query 8. Interpolated Queries (RM+DM) -Best performing de-9. Two stage (DMRM) -Best performing dependence Method P@5 P@10 P@15 P@20 MAP QL 0.6776 0.5531 0.4639 0.4102 0.6066 QL-TS 0.6857 0.5694 0.4735 0.4143 0.6248 DM 0.6980 0.5653 0.4735 0.4061 0.6264 Model-S 0.6735 0.5653 0.4803 0.4143 0.6189 MM 0.6735 0.5653 0.4748 0.4153 0.6198 RM-T 0.6939 0.5714 0.4789 0 . 4276  X  X  X  0 . 6351  X  RM-E 0.6980 0.5735 0 . 4912 0 . 4276  X  X  X  0 . 6384  X  RM+DM 0.7020 0.5755 0.4857 0.4133 0 . 6417  X  DMRM 0 . 7061 0.5714 0.4789 0 . 4204  X  X  X  0 . 6438  X  X  X 
Table 1 shows retrieval effectiveness in terms of precision at the top ranks and the mean average precision (MAP). Topic based smoothing and dependence models provide mod-est improvements over the query likelihood baseline. The translation based models provide no improvements over topi c based smoothing. Using relevance models leads to small but significant gains over the best topic based smoothing run. Using a large external collection resulted in minor improve -ments over relevance models built from the smaller target collection alone. Dependence model queries provide a dif-ferent form of evidence of relevance than relevance model queries and therefore their combination yields improvment s over the individual queries. Also DMRM, which uses the best performing dependence model query to build relevance models, achieves the best performance by boosting the qual-ity of the documents used to build the relevance models.
Previous work on sentence retrieval techniques show that simple query likelihood models outperform word overlap and TF-IDF based measures. In this short paper, we compared advanced language modeling techniques for the task of iden-tifying redundant information in sentences and showed that they outperform simple query likelihood and topic based smoothing methods.
 This work was supported in part by the Center for Intelli-gent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023, and in part by NSF grant #IIS-0534383. Any opinions, findings and conclusions or recommendations expressed in this material are the author X  X  and do not nec-essarily reflect those of the sponsor. [1] P. Brown, V. Della Pietra, S. Della Pietra, and [2] F. Diaz and D. Metzler. Improving the estimation of [3] G. Erkan and D. Radev. LexRank: Graph-based [4] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [5] V. Lavrenko and W. Croft. Relevance based language [6] D. Metzler, Y. Bernstein, W. Croft, A. Moffat, and [7] D. Metzler and W. B. Croft. A Markov Random Field [8] V. Murdock. Aspects of Sentence Retrieval . PhD [9] I. Soboroff and D. Harman. Overview of the TREC
