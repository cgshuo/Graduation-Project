 In the design of practical web page classification systems one often encounters a situation in which the labeled training set is created by choosing some examples from each class; but, the class proportions in this set are not the same as those in the test distribution to which the classifier will be actually applied. The problem is made worse when the amount of training data is also small. In this paper we ex-plore and adapt binary SVM methods that make use of un-labeled data from the test distribution, viz., Transductive SVMs (TSVMs) and expectation regularization/constraint (ER/EC) methods to deal with this situation. We empir-ically show that when the labeled training data is small, TSVM designed using the class ratio tuned by minimizing the loss on the labeled set yields the best performance; its performance is good even when the deviation between the class ratios of the labeled training set and the test set is quite large. When the labeled training data is sufficiently large, an unsupervised Gaussian mixture model can be used to get a very good estimate of the class ratio in the test set; also, when this estimate is used, both TSVM and EC/ER give their best possible performance, with TSVM coming out superior. The ideas in the paper can be easily extended to multi-class SVMs and MaxEnt models.
 Categories and Subject Descriptors: I.5.2 [Pattern Recog-nition] Design Methodology-Classifier design and evaluation General Terms: Algorithms, Experimentation Keywords: Transductive and Semi-supervised learning, Clas-sification, Support Vector Machines The problem of classifying web pages into a given set of classes arises frequently in web applications. Linear classi-fiers such as support vector machines (SVMs) and maximum entropy (MaxEnt) models employing a rich feature repre-sentation (e.g., bag-of-words) are used successfully for this purpose. When labeling is expensive one is forced to work with a small set of labeled examples for training. In such cases a linear classifier trained using only labeled data does not give a good performance. Performance can be signifi-cantly boosted by employing semi-supervised methods that make effective use of unlabeled data, which is usually avail-able in plenty. The transductive support vector machine (TSVM) [10] and MaxEnt models trained with expectation regularization (ER) [12] are good examples of such meth-ods. Such semi-supervised methods assume the knowledge of auxiliary information about the underlying data such as class proportions.

The performance of semi-supervised methods such as ER and TSVM are sensitive to the class proportion values used to find their solution; see section 4.2 for a detailed empirical analysis. In many practical situations, class proportions are unknown. Often it also happens that the class proportions in the labeled training set are different from those in the actual distribution to which the classifier will be applied. A few examples of such scenarios are as follows. The main aim of this paper is to address the problem of mismatched class proportions mentioned above.

Let x denote a generic example web page and c denote a class. From a probabilistic viewpoint our problem consists of dealing with a situation in which: (i) p ( x | c ) is same in the labeled data and the actual data of interest for all c ; but, (ii) { p ( c ) } is very different in the two sets. Coupled with labeled data being small the problem becomes difficult.
Note that, even if labeled data is not small, extension of methods such as TSVM and ER to deal with mismatched class proportions requires careful thought because labeled data does not have information on actual class proportions and unlabeled data has to be suitably brought in to estimate them. There have been some previous efforts to address this problem for TSVMs [7, 14]. But these methods have some basic issues: they make adhoc changes to the training pro-cess in an active self-learning mode and are not cleanly tied to a well-defined problem formulation; they significantly de-viate from the TSVM formulation and hence perform quite worse than TSVMs when labeled data and the actual distri-bution are indeed well matched in terms of class proportions; and, they have not been demonstrated in difficult situations involving large distortions in class proportions.

In this paper we empirically analyze various issues related to the problem of mismatched class proportions, propose ways of estimating actual class proportions and demonstrate their usefulness. We only take up binary classification prob-lems in this paper; but the ideas are quite general and they have the potential for extension to multi-class settings. Since binary classification involves only two classes (positive and negative), class proportion can be represented using a single quantity f , the fraction of positive examples.
In a nutshell following are our main contributions, listed by order of treatment in the paper. 1. The ER method is introduced in the context of SVMs. 2. We empirically analyze all the methods (TSVM, ER, 3. If labeled data has distorted class proportion (even se-4. To handle the crucial case in which the actual f is un-
In this section we describe all the methods that will be analyzed in this paper. These methods are meant for binary classification. Labeled data consists of l examples { x i where the input patterns x i  X  R d are feature vectors rep-resenting web pages and the labels y i  X  X  +1 ,  X  1 } .Semi-supervised/transductive methods make use of unlabeled ex-amples in addition; unlabeled data consists of u examples { x i } l + u i = l +1 . All the methods develop a linear classification function w T x with { x : w T x&gt; 0 } denoting the positive class region. Web page classification problems involve a large fea-ture space ( d is large); the input patterns are sparse, i.e., the fraction of non-zero elements in one x i is small. Unla-beled data is always a random sample picked from the actual distribution to which the classifier will be applied; labeled data, on the other hand, may have a class proportion which is different from that in the actual distribution.
Supervised SVMs make use of labeled data and optimize the regularized large margin loss function: where o i = w T x i . An example of the large margin loss func-tion is the squared hinge loss L ( y, o )=max(0 , 1  X  yo ) ternatively one can use the hinge loss: L ( y, o )= max (0 , 1 yo ). All the experiments in this paper are done with the squared hinge loss. Irrespective of which loss is used there exist very efficient numerical methods [11, 13] for solving (1); the running time of these methods is linear in the num-ber of examples. 1 On web page and text classification tasks the performance of the SVM classifier is quite steady over a large range of values of the regularization coefficient,  X  .In all the experiments of the paper we use  X  =1.
Transductive/Semi-Supervised SVMs (TSVMs) make ef-fective use of unlabeled data to enhance the performance of SVMs. These methods perform very well on web page and text classification problems. Even with just a few labeled ex-amples they can combine this labeled data with a large un-labeled set to attain a performance equal to that of a super-vised SVM designed using a large labeled set. Through un-labeled examples many features which are even completely absent in the labeled set end up getting very good weights.
TSVMs are based on the cluster (or, low density separa-tion) assumption which states that the decision boundary should not cross high density regions, but instead lie in low density regions. Joachims [10] gave the first effective TSVM formulation. A key ingredient of this formulation is that it
Good software for supervised and transductive SVMs can be found in http://svmlight.joachims.org/ and http:// vikas.sindhwani.org/svmlin.html . assumes f , the fraction of positive examples in the actual distribution is known and also the corresponding constraint is enforced in the formulation. 2 Joachims [10] solved the following problem in which, apart from w , the set of labels of unlabeled examples, { y i } i&gt;l are also variables: where [ z ]is1if z is true and 0 otherwise. Unlike (1), (2) is a not a convex minimization problem. In general, it is hard to solve. It has been pointed out [5] that the solution of (2) can get stuck in poor local minima. Fortunately, this is not the case in linear classification problems involving large feature space, such as web page and text classification. TSVMs are routinely used to solve applied problems [4, 3]. Alternative optimization iterations (fix w and optimize { y i } i&gt;l { y i } i&gt;l and optimize w ) are usually used to solve (2).
There also exist variations of the TSVM algorithm. For example the discrete variables in { y i } i&gt;l can be eliminated from (2) to get the following equivalent problem: min Sigmoid smoothing or other methods can be applied to write [ o i &gt; 0] in a differentiable form. Then the solution can be approached via gradient based optimization of w . See [6, 13, 5] for methods of this kind. Most often this method yields a performance that is slightly better than (2).
Mann and McCallum [12] proposed the Expectation Reg-ularization (ER) method in the context of MaxEnt models. The idea is to use expectation terms related to some do-main knowledge to influence the training process. This can be easily done with SVM models too, like we do here. In our case the domain knowledge of interest is the fact that the fraction of positively classified examples in unlabeled data equals f . This fraction constraint can be used to influence the solution by including an additional regularization term in the objective function: As mentioned earlier with respect to (3), sigmoid smoothing can be used to make the expectation regularization term to be differentiable so that gradient based numerical optimiza-tion techniques can be employed. In our implementation of ER we use  X  = 50 as the default value. Later we will also study the effect of varying  X  .

Note that in (4) the expectation constraint on the fraction of positive examples is only approximately enforced since it is only included as a regularizer term. If the domain knowl-edge says that the fraction constraint holds certainly then it may be better to force the constraint rather than adding a regularizer. This leads to a new method which we call as the Expectation Constraints (EC) method. The optimiza-We will refer to the constraint as the fraction constraint . Also, when applied in a discrete setting (e.g., the fraction constraint in (2)) it is assumed that appropriate rounding of f is allowed. tion problem to be solved is: Again, sigmoid smoothing can be used to write the fraction constraint function in a differentiable form. A suitable nu-merical method that can deal with equality constraints can be used to solve for w . In our implementation we use the Augmented Lagrangian method [1].

Let us introduce another expectation based baseline method that has been ignored in the literature. The method con-sists of taking the supervised SVM solution w (i.e., solu-tion of (1)) and adding a threshold  X  to the scoring func-tion so that the fraction of unlabeled examples that satisfy w
T x +  X &gt; 0equals f . The new classifier boundary is defined by w T x +  X  = 0. We will refer to this method as SVMTh.
It is easy to see that ER and EC methods are closely re-lated. In particular, when the parameter  X  in (4) is made very large then ER and EC are expected to show very sim-ilar behavior. SVMTh is quite different from EC although both methods enforce the fraction constraint; while EC tries to balance the minimization of T sup with the fraction con-straint, SVMTh simply adjusts the threshold to satisfy the fraction constraint without worrying about its effect on T If we compare (3) and (5) we see that TSVM has an extra term (loss on unlabeled data). This term turns out to be important as it helps TSVM to place the classifier boundary more precisely by making it pass through low density re-gions of unlabeled data; as we will see later the performance improvement due to it is large especially when the size of labeled data is small.

For MaxEnt models, Grandvalet and Bengio [9] suggested including an unlabeled loss term called entropy regulariza-tion that is similar in spirit to the unlabeled loss term in (3). However, their formulation does not include the fraction constraint. Mann and McCallum [12] conduct experiments to clearly point out the fact that without this constraint this method does not do well. They also show that, if the constraint is included (even as a regularizer term) then the performance is greatly enhanced.
All the empirical analyses in this paper are done using the following five text classification datasets: gcat , ccat , aut-avn , real-sim and earn . The first four datasets are same as the ones used in [13]; earn is a binary dataset created from the Reuters-21578 dataset 3 by taking examples in the earn class as positive and the remaining examples as negative. Key properties of the datasets are given in Table 1.
Each dataset is divided into three parts: labeled data, un-labeled data and test data. Unlabeled data and test data have the same class proportion; let f actual denote the frac-tion of positive examples in test/unlabeled data. Labeled data can have a different class proportion depending on the experiment; note that the main aim of the paper is to find ways of dealing with a mismatch in class proportion between labeled data and the actual distribution. Let f lab denote the fraction of positive examples in labeled data. http://www.daviddlewis.com/resources/ testcollections/reuters21578/ Table 1: Properties of datasets. n :numberofex-amples, d : data dimensionality, f actual : positive class ratio.

Always f actual is set to the fraction of positive examples in the original, full dataset. Given f lab and the number of labeled examples, n lab , we create a random division of the full dataset into test, labeled and unlabeled data as follows. First, 50% of the examples are randomly chosen in a class-stratified fashion and kept as testing data. Let R denote the set of remaining examples. We form labeled data by choos-ing negative examples from R . Of the remaining examples, un-labeled data is randomly chosen to be the largest set obeying f actual . Since the dataset sizes are large and n lab is small, the size of unlabeled data is large and it is several times larger than n lab .
 Performance of the methods will be measured in terms of F score, which is the harmonic mean of precision and recall of the positive class.

To ease the understanding we use consistent notations in all the figures. The following colors will be used to denote the various methods: In most figures f or f lab is the horizontal axis. In such fig-ures f actual is shown as a vertical dotted line. In most figures the plots are averages over four random splits of a dataset into labeled, unlabeled and test data. Exceptions are Fig-ures 3, 8, 9, and 10 where just one random split is used. In the figures LabSize and LabFrac denote, respectively, the size of labeled data, n lab and the fraction of positive examples in labeled data, f lab .
Before we go on to deal with differences in class propor-tions in labeled data and unlabeled/test data it is useful to give some basic experimental results that help understand the methods.
Figure 1 shows the learning curves (variations of perfor-mance with n lab ) for various methods on four datasets. All methods use the value f = f actual . TSVM gives a much Figure 1: Learning curves for various datasets. All meth-ods use f = f actual . stronger performance than other methods at small values of n lab . Even at higher values of n lab it is better than or com-petitive with others. Whenever TSVM does significantly better than SVMTh it is a clear indication of the important role played by the unlabeled loss term in (2). The magnitude of performance improvement given by TSVM over others de-pends on the dataset. On gcat and aut-avn TSVM is very much superior; on ccat it is quite better than others; and, on real-sim its performance matches that of SVMTh. Af-ter TSVM, SVMTh is the next best performer, followed by EC, ER and LSVM, in that order.
Since our aim is to deal with situations in which f actual is unknown, it is useful to understand how the performance of each method is affected when f (the value of positive class fraction used in (1)-(5)) is changed away from f actual Figure 2 describes this sensitivity for n lab =90and n lab 1440. We chose these n lab values as they represent the rising and stable parts of the learning curves for most (method, dataset) combinations.

Let us first make some observations for n lab = 90. All methods suffer when f is too much lower/higher than f actual If f is sufficiently far from f actual then the performance can degrade to be even worse than that of LSVM. The fall in per-formance on the lower side is sharper since recall is badly affected when f is decreased from f actual and this has an adverse effect on the F score.

TSVM and SVMTh attain their peak performance at f values that are close to f actual . On the other hand, ER and EC attain their best performance at f values that are quite shifted away from f actual . Consistently, this shifting away from f actual is such that the fraction of the minority class is larger than its actual value in the unlabeled/test distribu-
Figure 2: Sensitivity of methods with respect to f .Thetoprowisfor n tion; in the top row of Figure 2 note the left shift of the peak for aut-avn and right shifts for all other datasets. This is due to two properties associated with LSVM, of which EC and ER are modifications. First, when n lab is small the di-rection of the LSVM classifier makes a large angle with that of the ideal SVM classifier built using a large labeled train-ing set. So the LSVM classifier boundary cuts through dense regions of the test set. Second, this cutting is more severe on the minority class examples due to lesser representation. Thus, LSVM (which optimizes T sup ) ends up choosing a clas-sifier boundary placement in which many test examples of the minority class are pushed to the wrong side. See the left bottom plot of Figure 8 (which comes later in the paper) to confirm this for the gcat dataset. Therefore, to pull the classifier towards satisfying the f actual fraction and hence do well on F score, EC and ER methods need to use a value f (see (4) and (5)) that is higher (lower) than f actual when the minority class is the positive (negative) class. The f value that is needed by ER to get the best F score on the test set is farther from f actual than the f value needed for EC because ER only loosely enforces the f constraint. SVMTh behaves differently because it doesn X  X  try to balance T sup optimization and the fraction constraint; it simply enforces the fraction constraint in post-processing without worrying about what happens to T sup in that step.

When n lab is large the behavior is different. The bottom row of Figure 2 gives sensitivity for LabSize=1440. LSVM gives a very good performance since labeled data set is suf-ficiently large. The performance of ER is close to that of
This can be explained as follows. Take the case where the positive class is the minority class. Since the positive class has a smaller number of loss terms (see (1)) the classifier boundary of LSVM is placed closer to the labeled examples of the positive class in order to reduce the total loss on the labeled examples of the negative class. Therefore the fraction of examples classified by LSVM as positive is less LSVM (see the green continuous and black dotted lines in the bottom row of Figure 2) because the  X  value (50) used in (4) is not strong enough to exert the fraction constraint. TSVM, SVMTh and EC suffer when f is moved well away from f actual because they enforce the fraction constraint.
The observations made above imply that, for EC and ER it is a good idea to employ an f value that is different from f actual ,when n lab is small. This observation is missing in previous works on ER [12]. If the class proportions in la-beled, unlabeled, and test data are all matching and n lab not too small, it is then a good idea to use cross validation to choose f to optimize performance. If n lab is not large the Leave-One-Out method can be used for cross validation. However we do not go ahead and demonstrate this in this paper since the main focus of the paper is to suggest ways of handling a mismatch between class proportions in labeled data and the actual distribution of interest.
 Figure 3 gives the sensitivity plots for the earn dataset. The experimental setup is close to that used by Zhang and Oles [15] who used the setup to point out (wrongly) that TSVM does not work well. Zhang and Oles [15] possibly optimized the TSVM objective function in (2) without the fraction constraint. Figure 3 explains what can go wrong if that is done. From the right side plot of Figure 3 it is clear that the least value of the objective function occurs at f =0 where the performance is very poor. This clearly shows the important role played by the fraction constraint in TSVM. Recall a similar comment that we made earlier at the end of subsection 2.4, with respect to MaxEnt models and entropy regularization.
The results of section 4 concern the case in which labeled data and test/unlabeled data have the same class propor-tion. We now consider situations in which the class propor-tions in the two sets are different. Sometimes it happens that even though there is a difference in those class proportions, Figure 3: Earn dataset: sensitivities of the methods with respect to f with just 20 labeled examples. The left plot shows performance of various methods. The right plot shows TSVM performance (blue) and TSVM objective function in equation (2) (magenta) (both are normalized to max value of 1 to show them on the same plot). When f actual is known TSVM gives an F score of 0.93. Figure 4: Demonstration that cost adjusted labeled loss does not change performance significantly. f actual may actually be known. Such knowledge helps the methods to achieve a good performance. In subsection 5.1 we take up this case. In subsection 5.2 we deal with the more difficult case in which f actual is unknown.
Since we know f actual as well as the number of positive and negative labeled examples in the labeled set, it is ap-propriate to modify the SVM objective function T sup so that the labeled loss term can be viewed as if it is the mean loss of labeled data picked (with replacement) from the actual distribution of interest. To do this we change T sup to T by introducing  X  , a relative weighting parameter for positive labeled examples. T Adj sup is given by T n p + n n , n p and n n are the number of positive and negative examples in labeled data, and N = n p  X  + n n is the nor-malizer chosen to make sure that the labeled loss term is the mean loss. Note that when f lab &lt;f actual ,  X  is bigger than 1, and so the weight on the positive labeled examples is increased. When f lab &gt;f actual ,  X  is smaller than 1. All Figure 5: Variation of the performance of methods with f lab (LabelFrac) when f actual is known and the methods the methods can be appropriately modified to use T Adj sup stead of T sup . However, we found that the performance of the methods are little affected when T Adj sup is used instead of T sup . Figure 4 compares the performance for one dataset, gcat . Similar behavior is seen on other datasets.
Figure 5 compares all methods on four datasets when f = f actual is used by the methods and f lab is varied over a range of values from 0 to 1. Interestingly, TSVM, SVMTh and EC are only mildly affected by changes in f lab .Thisis very much due to f actual being known and used well by the methods via the fraction constraint with f = f actual .Onthe other hand LSVM is badly affected when f lab is moved away from f actual . ER suffers when f lab &lt;f actual . The penalty weight,  X  used in (4) plays a key role in ER. Figure 6 shows the performance variation for a range of  X  values. When  X  is small the performance of ER is close to that of LSVM. For large  X  its performance is close to that of EC. Both these observations are along expected lines. For different values of f lab the optimal  X  is different.
We propose and explore some methods for estimating f actual in the presence of large variations in f lab . Note that, even when labeled data is large, it cannot be used alone to get an estimate of f actual since the class proportion in that set are distorted. We explore three methods for estimating f actual Let us refer to an estimate of f actual as f actual est .
Estimation Method 1. Simply take f actual est to be the fraction of positive examples in labeled data. This is just a baseline method.

The next two methods make use of unlabeled data to-gether with labeled data.

Estimation Method 2. Given labeled data we can solve (1) and obtain the LSVM solution w . From the good perfor-mance of SVMTh over a large range of f lab values in Figure Figure 6: Variation of ER performance with f lab (Label-Frac) for various  X  values. Dashdot:  X  = 5 (gives perfor-mance close to LSVM); Continuous:  X  = 50 (same as in Figure 5); Dotted:  X  = 500; Dashed:  X  = 5000. ER with  X   X  500 yields a performance close to that of EC. Figure 7: Estimation Method 2. Mean (with error bars) of f est versus n lab for gcat . f actual =0 . 3011 for gcat . 5 we know that the classifier direction corresponding to w is good. It is appropriate to model the classifier scoring func-tion, o = w T x applied on unlabeled data, { o i = w T x i as a mixture of two Gaussians: where g (  X  ;  X  k , X  k ) is the univariate Gaussian density func-tion with mean  X  k and standard deviation  X  k ,and  X  1 ,  X  (non-negative) mixing proportion values satisfying  X  1 +  X  1. By fitting this model to the data { o i } i&gt;l to maximize like-lihood we can get the parameter values  X  1 ,  X  2 ,  X  1 ,  X  and  X  2 . The Gaussian with the larger  X  k represents the pos-itive class; so, the corresponding  X  k can be taken as f actual an estimate of the positive class fraction.
 Figure 8 helps us understand the usefulness of this method. When the number of labeled examples is small, w ,theso-lution of (1), makes a large angle with w , the solution as-sociated with the ideal SVM classifier built using a large labeled training set having the actual class proportion; so the { o i = w T x i } i&gt;l distribution is not a clean mixture of two Gaussians, and the estimate f actual est is also poor. On Figure 8: Understanding Estimation Method 2. SVM-N denotes LSVM output for N labeled examples and FullSVM is output of SVM corresponding to a very large labeled set. the other hand, when the number of labeled examples is large (closer to the higher side of the learning curve) this estimation method performs much better.

Figure 7 plots the mean (with error bars) of f actual est puted using 10 random splits of gcat )as n lab is varied. Such a plot can help one decide when this estimation method is useful.

Estimation Method 3. In this method we explore to see if some property associated with the solution of a method canbeusedtofind f actual est . Let us take TSVM. Figure 9 gives, for gcat and ccat , the variation of the TSVM objec-tive function (see (2)), the Labeled Loss (the second term of T sup in (1)), as well as the F score with respect to f used in (2). The behavior in other datasets is similar. Clearly the TSVM objective function is not useful for tuning f .G  X  art-ner et al [8] suggest to minimize the TSVM objective func-tion to tune class proportions; but Figure 9 clearly points out that it is not the right thing to do. The plots also show the goodness of minimizing the Labeled Loss as a way of obtaining f actual est . 5 A rough explanation for this good-ness is as follows. Consider (2), the optimization problem associated with TSVM. End f values (near 0 and 1) put heavy pressure on containing all examples on one side of the classifier boundary. With such pressure Labeled Loss becomes large for such f values. Near f = f actual minimiz-ing Labeled Loss, Unlabeled Loss and satisfying the fraction constraint are all consistent, so Labeled Loss achieves small values there.

We also tried the same estimation method (i.e., minimize labeled loss along the trajectory of solutions defined by vary-ing f ) on the expectation methods. But it did not work well. Figure 10 gives, for EC and gcat and ccat , the variation of labeled loss as well as the F score as a function of f .The minimizer of the labeled loss does not coincide quite well with the maximizer of F score. Similar behavior is observed
To avoid confusion we note that (2) is always the opti-mization problem that is solved to get the TSVM solution; minimizing the Labeled Loss that we are doing here is for tuning f at a higher level treating f as a hyperparameter. Figure 9: Variation of performance (blue), TSVM objective function in (2) (magenta) and Labeled Loss (the second term of T sup in (1)) (cyan) with respect to f , for gcat and ccat . The minimizer of Labeled Loss is marked by a black circle. The rows correspond to different fraction distortions. The middle row corresponds to f lab = f actual . The bottom two rows correspond to f lab =0 . 8 f actual and f lab =0 . 6 f The top two rows correspond to (1  X  f lab )=0 . 8(1  X  f actual and (1  X  f lab )=0 . 6(1  X  f actual ). Figure 10: EC: Variation of performance (red) and La-beled Loss (the second term of T sup in (1)) (cyan) with respect to f , for gcat and ccat . The minimizer of La-beled Loss is marked by a black circle. The rows corre-spond to different fraction distortions. The middle row cor-responds to f lab = f actual . The bottom two rows corre-two rows correspond to (1  X  f lab )=0 . 8(1  X  f actual )and (1 for SVMTh and ER. For n lab = 90 Figure 11 shows the performance of Estimation Method 3 with EC and SVMTh.
Figure 12 compares the three estimation methods as ap-plied to TSVM. For n lab = 90 Estimation Method 2 doesn X  X  do even as well as Estimation Method 1. Estimation Method 3 is very good in a large range of f lab values containing f actual . It suffers only at extreme f lab values. When n lab large, e.g., 1440, Estimation Method 2 performs very well. It is appropriate to switch between the two estimation meth-ods depending on how large n lab is. Going by what we saw in Figure 7, we can use the steadiness of f actual est given by Estimation Method 2 over a range of n lab values to decide when to switch to it. Of course, n lab has to be reasonably big to notice a clear steady pattern. Until that size is reached it is best to use Estimation Method 3.

When n lab is large LSVM itself does quite well; since it is not dependent on f it becomes the preferred method for that case. Therefore, the value of a fraction estimation method should be determined by how well it performs when labeled data is small. From that viewpoint Estimation Method 3 (as applied to TSVM) is most valuable.
 Of all (Method, Estimation Method) combinations (TSVM, Estimation Method 3) is the one that takes the most com-puting time; even for that combination, the computation time for one run on any of our five datasets is less than two minutes on a 3 GHz machine.
Since the first crucial paper of Joachims [10] several works (for a sample see [15, 7, 14, 6, 13, 5]) have been published extending and exploring various aspects of the first TSVM model. Of these, the works of Chen, Wang and Dong [7] and Wang and Huang [14] are the most relevant to our work since they address the problem of mismatch in class proportions. The main drawback of these methods is that they make ad-hoc labeling of the unlabeled examples during the training process in an active self-learning mode and are not cleanly tied to a well-defined problem formulation. Due to this they significantly deviate from the TSVM formulation and hence perform quite worse than TSVMs for the normal situation in which labeled data and the actual distribution are indeed well matched in terms of class proportions. Also, the meth-ods have been demonstrated only in situations where LSVM itself gives decent performance, which is not true when there are large distortions in class proportions.

Mann and McCallum [12] empirically analyze ER in bi-nary, multi-class and structured output settings in the con-text of MaxEnt models. ER X  X  working and performance in SVMs and MaxEnt models are similar. Mann and McCal-lum [12] do a brief study of the sensitivity of ER X  X  perfor-mance with respect to class proportions. But they only fo-cus on the case of distortion towards uniform class propor-tions; in the binary case this corresponds to changing f from f actual to 0 . 5. Unfortunately their study is done only on one multi-class dataset. As we saw in section 4.2, ER shows in-teresting behavior in the binary case; in fact ER improves in performance when f is moved towards 0 . 5. SVMTh is an important baseline method that is completely missed in [12].
There is a building literature on dealing with differences in train and test distributions referred to as covariate shift (see [2] for instance), but such papers introduce and analyze new formulations and do not help to modify TSVM or the expectation methods while keeping their spirit in tact.
The main contributions of this paper are: (i) empirical analysis of TSVM and expectation methods and their sen-sitivity with respect to label proportions; and, (ii) proposal and evaluation of new methods for dealing with mismatches in label proportions between labeled and test sets. We have also done preliminary experiments to verify the ideas on Least squares and MaxEnt models of binary classification and observed behavior similar to SVM loss. With some care all the ideas can also be extended to the multi-class setting. The ideas and results of this paper are mainly for web page and text classification. More work is needed to see if they hold on other types of problems. [1] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and [2] S. Bickel, M. Br  X  uckner, and T. Scheffer.
 [3] L. Bruzzone, M. Chi, and M. Marconcini. A novel [4] O. Chapelle, B. Sch  X  olkopf, and A. Zien.
 [5] O. Chapelle, V. Sindhwani, and S. S. Keerthi.
 [6] O. Chapelle and A. Zien. Semi-supervised [7] Y. Chen, G. Wang, and S. Dong. Learning with [8] T. G  X  artner, Q. V. Le, S. Burton, A. J. Smola, and [9] Y. Grandvalet and Y. Bengio. Semi-supervised [10] T. Joachims. Transductive inference for text [11] T. Joachims. Training linear SVMs in linear time. In [12] G. S. Mann and A. McCallum. Generalized [13] V. Sindhwani and S. S. Keerthi. Large scale [14] Y. Wang and S.-T. Huang. Training TSVM with the [15] T. Zhang and F. J. Oles. A probability analysis on the 0 0.5 1 aut X  X vn LabSize: 90
F Score 0 0.5 1 aut X  X vn LabSize: 90
F Score 3: Continuous. Dashed: Use f = f actual (Upper baseline)
