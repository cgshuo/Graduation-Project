 There are many ways in which we can refer to ob-jects and people in the real world. A chair, for ex-ample, can be referred to as red, large, or seen from the front, while men may be singled out in terms of their pogonotrophy (facial hairstyle), clothing and many other attributes. This poses a problem for al-gorithms that automatically generate referring ex-pressions: how to determine which attributes to use?
One solution is to assume that some attributes are preferred over others, and this is indeed what many Referring Expressions Generation (REG) al-gorithms do. A classic example is the Incremental Algorithm (IA), which postulates the existence of a complete ranking of relevant attributes (Dale and Reiter, 1995). The IA essentially iterates through this list of preferred attributes, selecting an attribute for inclusion in a referring expression if it helps sin-gling out the target from the other objects in the scene (the distractors). Crucially, Dale and Reiter do not specify how the ranking of attributes should be determined. They refer to psycholinguistic research suggesting that, in general, absolute attributes (such as color) are preferred over relative ones (such as size), but stress that constructing a preference order is essentially an empirical question, which will dif-fer from one domain to another.

Many other REG algorithms similarly rely on preferences. The graph-based based REG algorithm (Krahmer et al., 2003), for example, models prefer-ences in terms of costs, with cheaper properties be-ing more preferred. Various ways to compute costs are possible; they can be defined, for instance, in terms of log probabilities, which makes frequently encountered properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009).

Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv  X  as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Vi-ethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determin-ing which properties to include in a referring expres-sion requires a  X  X emantically transparent X  corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Obviously, text cor-pora tend not to meet this requirement, which is why semantically transparent corpora are often collected using human participants who are asked to produce referring expressions for targets in controlled visual scenes for a given domain. Since this is a time con-suming exercise, it will not be surprising that such corpora are thin on the ground (and are often only available for English). An important question there-fore is how many human-produced references are needed to achieve a certain level of performance. Do we really need hundreds of instances, or can we al-ready make informed decisions about preferences on a few or even one training instance?
In this paper, we address this question by sys-tematically training the graph-based REG algorithm on a number of  X  X emantically transparent X  data sets of various sizes and evaluating on a held-out test set. The graph-based algorithm seems a good can-didate for this exercise, in view of its performance in the REG challenges. For the sake of compari-son, we also follow the evaluation methodology of the REG challenges, training and testing on two do-mains (a furniture and a people domain), and using two automatic metrics (Dice and accuracy) to mea-sure human-likeness. One hurdle needs to be taken beforehand. Krahmer et al. (2008) manually as-signed one of three costs to properties, loosely based on corpus frequencies. For our current evaluation experiments, this would hamper comparison across data sets, because it is difficult to do it in a manner that is both consistent and meaningful. Therefore we first experiment with a more systematic way of as-signing a limited number of frequency-based costs to properties using k -means clustering. In this section we describe our experiment with k -means clustering to derive property costs from En-glish and Dutch corpus data. For this experiment we looked at both English and Dutch, to make sure the chosen method does not only work well for English. 2.1 Materials Our English training and test data were taken from the TUNA corpus (Gatt et al., 2007). This semanti-cally transparent corpus contains referring expres-sions in two domains (furniture and people), col-lected in one of two conditions: in the -LOC con-dition, participants were discouraged from mention-ing the location of the target in the visual scene, whereas in the +LOC condition they could mention any properties they wanted. The TUNA corpus was used for comparative evaluation in the REG Chal-lenges (2007-2009). For training in our current ex-periment, we used the -LOC data from the training set of the REG Challenge 2009 (Gatt et al., 2009): 165 furniture descriptions and 136 people descrip-tions. For testing, we used the -LOC data from the TUNA 2009 development set: 38 furniture descrip-tions and 38 people descriptions.

Dutch data were taken from the D-TUNA corpus (Koolen and Krahmer, 2010). This corpus uses the same visual scenes and annotation scheme as the TUNA corpus, but with Dutch instead of English descriptions. D-TUNA does not include locations as object properties at all, hence our restriction to -LOC data for English (to make the Dutch and English data more comparable). As Dutch test data, we used 40 furniture items and 40 people items, randomly se-lected from the textual descriptions in the D-TUNA corpus. The remaining furniture and people descrip-tions (160 items each) were used for training. 2.2 Method We first determined the frequency with which each property was mentioned in our training data, relative to the number of target objects with this property. Then we created different cost functions (mapping properties to costs) by means of k -means clustering, using the Weka toolkit. The k -means clustering al-gorithm assigns n points in a vector space to k clus-ters ( S ter with the nearest centroid. The total intra-cluster variance V is minimized by the function where  X  In our case, the points n are properties, the vector space is one-dimensional (frequency being the only dimension) and  X  properties in S as follows: where S properties, S quent properties, and so on. Using this approach, properties from cluster S added  X  X or free X  to a description. Free properties are always included, provided they help distinguish the target. This may lead to overspecified descriptions, mimicking the human tendency to mention redun-dant properties (Dale and Reiter, 1995).

We ran the clustering algorithm on our English and Dutch training data for up to six clusters ( k = 2 to k = 6 ). Then we evaluated the performance of the resulting cost functions on the test data from the same language, using Dice (overlap between at-tribute sets) and Accuracy (perfect match between sets) as evaluation metrics. For comparison, we also evaluated the best scoring cost functions from Theu-ne et al. (2010) on our test data. These  X  X ree-Na  X   X ve X  (FN) functions were created using the manual ap-proach sketched in the introduction.

The order in which the graph-based algorithm tries to add attributes to a description is explicitly controlled to ensure that  X  X ree X  distinguishing prop-erties are included (Viethen et al., 2008). In our tests, we used an order of decreasing frequency; i.e., 2.3 Results For the cluster-based cost functions, the best perfor-mance was achieved with k = 2 , for both domains and both languages. Interestingly, this is the coarsest possible k -means function: with only two costs (0 and 1) it is even less fine-grained than the FN func-tions advocated by Krahmer et al. (2008). The re-sults for the k -means costs with k = 2 and the FN costs of Theune et al. (2010) are shown in Table 1. No significant differences were found, which sug-gests that k -means clustering, with k = 2 , can be used as a more systematic alternative for the manual assignment of frequency-based costs. We therefore applied this method in the next experiment. To find out how much training data is required to achieve an acceptable attribute selection perfor-mance, in the second experiment we derived cost functions and property orders from different sized training sets, and evaluated them on our test data. For this experiment, we only used English data. 3.1 Materials As training sets, we used randomly selected subsets of the full English training set from Experiment I, with set sizes of 1, 5, 10, 20 and 30 items. Be-cause the accidental composition of a training set may strongly influence the results, we created 5 dif-ferent sets of each size. The training sets were built up in a cumulative fashion: we started with five sets of size 1, then added 4 items to each of them to cre-ate five sets of size 5, etc. This resulted in five series of increasingly sized training sets. As test data, we used the same English test set as in Experiment I. 3.2 Method We derived cost functions (using k -means clustering with k = 2 ) and orders from each of the training sets, following the method described in Section 2.2. In doing so, we had to deal with missing data: not all functions, we simply assigned the highest cost (1) to the missing properties. For the order, we listed properties with the same frequency (0 for missing properties) in alphabetical order. This was done for the sake of comparability between training sets. 3.3 Results To determine significance, we calculated the means of the scores of the five training sets for each set size, so that we could compare them with the scores of the entire set. We applied repeated measures of variance (ANOVA) to the Dice and Accuracy scores, using set size (1, 5, 10, 20, 30, entire set) as a within variable. The mean results for each training set size the scores increase with the size of the training set, but the increase gets smaller as the set sizes become larger.

In the furniture domain, we found a main effect of set size (Dice: F curacy: F set sizes performed significantly different as com-pared to the entire set, we conducted Tukey X  X  HSD post hoc comparisons. For Dice, the scores of set size 10 ( p = . 141 ), set size 20 ( p = . 353 ), and set size 30 ( p = . 197 ) did not significantly differ from the scores of the entire set of 165 items. The Accu-racy scores in the furniture domain show a slightly different pattern: the scores of the entire training set were still significantly higher than those of set size 30 ( p &lt; . 05 ). This better performance when trained on the entire set may be caused by the fact that not all of the five training sets that were used for set sizes 1, 5, 10, 20 and 30 performed equally well.

In the people domain we also found a main effect of set size (Dice: F racy: F comparisons showed that the scores of set size 20 (Dice: p = . 416 ; Accuracy: p = . 146 ) and set size 30 (Dice: p = . 238 ; Accuracy: p = . 324 ) did not significantly differ from those of the full set of 136 items. Experiment II has shown that when using small data sets to train an attribute selection algorithm, results can be achieved that are not significantly different from those obtained using a much larger training set. Domain complexity appears to be a factor in how much training data is needed: using Dice as an evaluation metric, training sets of 10 sufficed in the simple furniture domain, while in the more complex people domain it took a set size of 20 to achieve re-sults that do not significantly differ from those ob-tained using the full training set.

The accidental composition of the training sets may strongly influence the attribute selection per-formance. In the furniture domain, we found clear differences between the results of specific training sets, with  X  X ad sets X  pulling the overall performance down. This affected Accuracy but not Dice, possibly because the latter is a less strict metric.

Whether the encouraging results found for the graph-based algorithm generalize to other REG ap-proaches is still an open question. We also need to investigate how the use of small training sets af-fects effectiveness and efficiency of target identifica-tion by human subjects; as shown by Belz and Gatt (2008), task-performance measures do not necessar-ily correlate with similarity measures such as Dice. Finally, it will be interesting to repeat Experiment II with Dutch data. The D-TUNA data are cleaner than the TUNA data (Theune et al., 2010), so the risk of  X  X ad X  training data will be smaller, which may lead to more consistent results across training sets. Our experiment has shown that with 20 or less train-ing instances, acceptable attribute selection results can be achieved; that is, results that do not signif-icantly differ from those obtained using the entire training set. This is good news, because collecting such small amounts of training data should not take too much time and effort, making it relatively easy to do REG for new domains and languages.
 Krahmer and Koolen received financial support from The Netherlands Organization for Scientific Re-search (Vici grant 27770007).
