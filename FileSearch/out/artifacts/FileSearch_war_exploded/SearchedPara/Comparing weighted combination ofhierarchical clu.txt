
Computer Engineering Department, Amirkabir University of Technology, Tehran, Iran
Electrical and Computer Engineering Department, University of Kashan, Kashan, Iran
Electrical and Computer Engineering Department, Isfahan University of Technology, Isfahan, Iran Computer Engineering Department, Amirkabir University of Technology, Tehran, Iran 1. Introduction
Clustering is the process of forming groups (clusters) of similar patterns from a given set of inputs. A fundamental data analysis tool in different fields such as data mining [26], image processing [33] and machine learning [12,38].

Clustering algorithms can be classified into two main groups: hierarchical and nonhierarchical clus-tering algorithms [7,29]. The inputs to nonhierarchical algorithms are the data, a distance metric and the number of clusters. These algorithms partition data into k different clusters. The popular k -means algorithm belongs to the family of nonhierarchical clustering algorithms [7]. This method starts with a based on its members, a new central point (new centroid) is calculated and pattern assignments to their closest centroids are changed, if necessary. The algorithm finishes when no pattern reassignments are needed or when certain amount of time elapses.

Hierarchical clustering algorithms merge the most similar clusters in the bottom-up fashion (agglom-tering) [35]. In Agglomerative Hierarchical Clustering (AHC) algorithms, each individual pattern is first assigned to a null cluster, and then the two clusters that are close to each other are merged into a new step of top-down algorithms, a cluster is divided into two new clusters with respect to its pattern dis-tribution. The partitioning proces s is stopped when the final clusters contain only one pattern. A tree, ing algorithm. Dendrograms offer better views of data distribution in different abstraction levels. This property makes the hierarchical clustering algorithms an ideal choice for data exploration and visual-ization. Furthermore, in some applications, the number of clusters is not known in advance, and the dendrogram could provide a visualization method for user to decide on the number of clusters. On the other hand, the performance of classification algorithms can be improved by various means. Using an ensemble of learners and combining their individual decisions is an important approach. In [20, are:  X  When there is not enough data, many different classifiers can be trained based on this data and by  X  In cases where the learning algorithm involves a local search, an ensemble of learners that start  X  When the optimal classifier is not accessible out of the basic learners in search space, the combina-Hence, performance of data clustering is boosted by combining the results of several clustering algo-rithms [2,3,8,9,13,16,22 X 24,41]. This approach enables to combine the results from multiple clustering techniques to create a single and integrated model for input dataset which improves the robustness of clustering method.

The results of clustering on a set of data using hierarchical clustering algorithms are affected by many factors, therefore producing different dendrograms. In this paper, several new combination methods for hierarchical clustering results are introduced in order to derive a consensus of hierarchical clustering.
The rest of this paper is organized as follows. Section 2 reviews the related works on clustering com-binations methods. In Section 3, the Hierarchical Clustering Combination (HCC), in its general frame-work, is introduced. In this framework, the description matrices are used as a middle structure, i.e. the hierarchy resulted from each clustering is converted to a description matrix, and then combination is performed based on description matrices. Section 4 presents the theory and a description of the pro-posed methods for aggregating different description matrices. Section 5 discusses the performance of the developed techniques and Section 6 is dedicated to the conclusions of this work. 2. Related works
When applying different clustering techniques on a set of data, the labels assigned by these clusterers are not necessarily matched with each other. As a result, to combine the clustering results from various clustering algorithms, it is necessary to solve the so-called label correspondence problem . Search algo-rithms, such as Greedy algorithm [9,41] and Hungarian algorithm [8], can be used to solve this mismatch problem.

In some clustering combination methods the label correspondence problem is solved by utilizing a similarity measure which is calculated from set of clustering algorithms that are used in combination [2, 16,21 X 24,32]. In [20] an algorithm, called Voting algorithm, was proposed based on this similarity ma-trix. A final hierarchical clustering algorithms can then be built that operate directly upon similarity matrix [3]. The result of such clustering will be the ensemble clustering.

Several researchers convert the clustering combination into another problems, such as graph partition-ing [36,39], or even to another clustering problem [34], in order to avoid the correspondence problem. obtained. In another approach, an intermediate feature space is introduced in [34]. The new feature vec-is produced by applying a clustering algorithm.

In supervised learning, popular approaches such as bagging , [19] and boosting , [28] have been known as effective approaches to improve the learning accuracy. Some researchers have applied these methods to clustering combination [1,8,10,11,14,24,30 X 32]. In these approaches, the aforementioned concepts are used to create the base clusterers, and for combinations, one of the algorithms presented above, is used [3].

Presently, most clustering combination methods are based on nonhierarchical base clusterers, i.e. the input of almost all combinational clustering algorithms is nonhierarchical and if one is interested in combining a set of hierarchical clusterings using aforementioned methods, the results of clustering tech-niques should be converted to nonhierarchical. In this conversion, only one level of the hierarchy, which this conversion, any combinational method such as stacked clustering [20] may be used to produce a consensus hierarchy. In this approach, only the information of one level of primary hierarchical cluster-to improve the quality of combination methods are ignored.

The output of a hierarchical clustering algorithm is a dendrogram. Thus, hierarchical clustering com-bination is equivalent to dendrogram combination. However, because dendrograms are tree-structured graphs their direct combination is a difficult task. Therefore, a middle structure namely Description Matrix (DM) is used to facilitate the combination [5]. There is a one to one correspondence between dendrograms and DMs. Two types of DMs exist, similarity and dissimilarity DM. The first type shows the pair-wise similarity between patterns in dendrogram and the second type shows their dissimilarity. A similarity DM is a similarity matrix in which the min-transitive property is hold. Similarly, a dis-similarity matrix with ultrametric property can be used as a dissimilarity DM. The min-transitive and ultrametric properties are dual of each other. In other words, similarity and dissimilarity of DMs can be converted to each other [6].

There potentially exist two possible approaches to combine dendrograms based on their DMs, direct and indirect approaches. In direct approach an aggregation operator is defined which involves DMs as input and produces a consensus DM as output. In this approach, because the output is a DM, the final dendrogram can be easily drawn based on this result (no further processing is needed). As an example, we can mention the work reported in [3,4] which uses similarity DMs and proposed an aggregation combine hierarchical and partition clustering into a hierarchical clustering. The only difference between these works is that the former uses similarity DMs and the latter uses dissimilarity DMs.
In indirect approach, common aggregation operators (like element wise addition) or newly defined ones are used to combine DMs to a consensus matrix. In contrast to the direct approach, the consen-sus matrix created in indirect approach will not necessarily be a DM (will not have the min-transitive/ ultrametric property) and consequently we cannot draw the resultant dendrogram directly from it. In other words, a dendrogram recovery phase is necessary to extract the final dendrogram from the consen-sus matrix. This step is equivalent to making consensus matrix ultrametric/min transitive.
The work reported in [6] can be categorized as a method of indirect approach. In this paper, the author used different dissimilarity DMs like Cophenetic Di fference (CD), Partition Membership Divergence (PMD), Cluster Membership Divergence (CMD), Sub-tree Membership Divergence (SMD) and Path Difference (PD) [18] to describe the input dendrogram. Among them, the CD descriptor performed well. Then by performing an element wise addition over DMs, a consensus matrix is created which is converted to the DM of a dendrogram by applying a conventional hierarchical clustering algorithm.
The information theory based combination method presented in [5] is another example of indirect ap-proach. In this method dissimilarity, DMs of input dendrograms are converted to probability distribution matrices. Following this step, a consensus dissimilarity matrix is created by minimizing its distance to the probability distribution matrices. To calculate the distance between two probability distribution ma-trices the R X nyi or Jensen-Shannon Divergences are used. It is worth noting that this optimization also leads to some element-wise operators. In this paper, the final dendrogram is recovered from consensus matrix by conventional clustering algorithms. 3. Hierarchical clustering combination framework The problem of hierarchical clustering combination may be stated as follows:
In order to propose an algorithm for hierarchical clustering combination, as requires in the above of a set of dendrograms is a dendrogram, which is as close as possible to all dendrograms of the set. The distances between a dendrogram and a set of dendrograms are measured by middle structures, which are called dendrogram descriptors. Various dendrogram descriptors are introduced and can be found in [3,13]. Figure 1 illustrates the general framework proposed for Hierarchical Clustering Combination ( HCC ). Different HCC methods, under certain conditions, could be represented as a special case of this framework.

In the algorithm of Fig. 1, the structural content of k th dendrogram is denoted by H ( k ) which is function f on the dendrogram. Then the description matrices, T ( k ) , 1 k L , are aggregated into a final description matrix T , which we call, hereafter, consensus matrix .

A dendrogram associated with a hierarchical clustering of N input patterns can be represented by a terminal vertices (i.e. patterns pair) in a dendrogram. If T ( k ) = { t ( hierarchy, i.e. H ( k ) , then the t ( for example [17] and the references in it).

The last step in the hierarchical clustering combination, as shown in Fig. 1, is to apply a hierarchical clustering algorithm to the consensus matrix, T , in order to create the final hierarchy. All descriptors introduced above are ultrametric. A description matrix T is ultrametric if the following inequality is satisfied [15]: for all triplets of points i , j ,and l .

Any dendrogram, which is characterized by a unique ultrametric matrix [25], can be recovered from its corresponding ultrametric matrix through hierarchical clustering. In other words, hierarchical clustering and ultrametric description of a dendrogram are the inverse of each others. Although the description matrices of base clusterings are ultrametric, their aggregation, which makes the consensus matrix, is not necessarily ultrametric. However, if T is used as input to any hierarchical clustering algorithm that operates directly on a dissimilarity matrix, a H can be recovered from it which is considered as the yield T which is equal to T or has small deviation from it. 4. Description matrix aggregation
In this paper, we use several aggregation approaches to combine the descriptor matrices. The weighted approaches that we are used are, Minimum Distance Hierarchical Clustering Combination ( MDHCC ), Hedge based Hierarchical Clustering Combination ( HHCC ), Minimum based Hierarchical Clustering Combination ( MinHCC ) and Minimum Variance Hierarchical Clustering Combination ( MVHCC ).
The MDHCC and MVHHC methods preserve the structures of basic clustering methods. As stated to all matrices of the set. The MDHCC defines error as overall distance between matrix elements and minimizes squared error. In this way, weights can be calculated. In MVHHC approach, each element of matrix T ,i.e. t ( estimates are unbiased, then we can form minimum variance estimate by taking the weighted average and restrict the coefficients w ( k ) to sum up to one. Using these weights, the aggregated estimate is guaranteed to have upper bound variance as large as the variance of any of the clustering algorithms in the ensemble. Since we assumed that the estimators are unbiased, the variance of each of the estimates is equivalent to its expected squared error.

In [6], it has been concluded that in a set of experiments the minimum aggregator is the best aggrega-tor among minimum, Harmonic mean, Geometric mean, Arithmetic mean (simple averaging), Euclidean length, and maximum aggregators. That is why, the MinHCC is chosen for comparing with other meth-ods.

The HHCC , which is derived from hedge algorithm [40], allocates weights to a set of strategies used is updated on-line after each new outcome. Strategies with the correct prediction receive more weight while the weights of the strategies with incorrect predictions are reduced.

In HH CC and MVHCC methods, an error function is required. Knowing that in a clustering algorithm, unlike classification, the label of training pattern are not known, we have used Cophenetic measure as an index for hierarchical clustering combination. The best-known index is Cophenetic correlation coefficient ( CPCC ) which compares the distance information in proximity matrix of input data and the distance information resulted from the cluster hierarchy. The CPCC values are bounded between  X  1and 1. The closer the CPCC to 1, the better the agreement between the Cophenetic and the proximity matrix. 4.1. Minimum distance hierarchical clustering combination (MDHCC)
Let Z = { z 1 ,...,z N } to be the input patterns, and H (1) and H (2) are the two dendrograms generated on the Z . The distance between two description matrices of these dendrograms could be determined by
The matrix T ( k ) is the k th description matrix with its ij th element shown by t ( Eq. (2). The L different clustering algorithms applied on pattern set Z ,yieldsasetof L dendrograms {
H (0) ,...,H ( L ) } . The dendrogram H that is a weighted combination of all clustering results is the optimum dendrogram if it minimizes the criterion function defined by: where the ij th element of description matrix T is defined by:
The resulting matrix is obtained by weighted combination of ensemble. The element of t ij from de-scription matrix which minimize D , are obtained by tacking the derivation of D with respect to w ij , The weights for combination that minimize overall distance between matrices is 1 /L . 4.2. Hedge based hierarchical clustering combination (HHCC)
The Hedge algorithm pioneers in on-line learning algorithm [20]. This algorithm allocates weights to group [20]. At first, we have no reason to prefer one clustering to other, thus we pick at random an algorithm from ensemble and take its dendrogram. For this prediction, we can obtain the CPCC value and then define the loss as 1-CPCC . Since we intend to improve the prediction for the next set we draw, it is reasonable to increase the probability of selecting one of the methods with the most appropriate predictions on the set, which are previously seen. Thus, we alter the distribution on ensemble as more authors in [40] proved an upper bound on the loss, which is not much worse than the loss of the best classifier in the ensemble. Based on their results, we select and 4.3. Minimum variance hierarchical clustering combination (MVHCC)
In [27], the weights, which are used in weighted combination, are derived so that they minimize the variance of combination. Using these weights, the aggregated estimate is guaranteed to have variance, error by using the CCPC value, we used methods in [27] for calculating the weights. 4.4. Minimum based hierarchical clustering combination (MinHCC)
A short description of this algorithm is presented here, the details can be found in [6]. Let Z = { z 1 ,...,z N Z related to H ( k ) , with its ij th element denoted by t ( The MinHCC algorithm creates the matrix T that is a combination of all matrices. Other operators such as maximum, Harmonic mean, Geometric mean, Arithmetic mean (simple averaging) and Euclidean length may also be used. However, the results presented in [6] illustrates that the minimum operator is the proper aggregator among mentioned operators.
 5. Experiments
In this section, the performance of our proposed combination methods is presented. For measure of performance, several quality evaluation indices [35] are used and applied to the resulted clusters hierarchy created from the proximity matrix of input data. The objective of using these indices is to determine the validity of clustering structure obtained by clustering algorithms.

An important index is Cophenetic Correlation Coefficient ( CPCC ). This index compares the informa-tion content of proximity matrix of input data and the distance information resulted from the clustering algorithm. The range of CPCC values are between  X  1to + 1. The CPCC value closer to + 1 indicts better proximity matrix. We used CPCC index to compare the results of the clustering algorithms, by comparing the results at two different hierarchies.

We have used different data set to generalize our results. By evaluating our proposed algorithm on are removed. Therefore, an average of CPCC , abbreviated by ACPCC , is used as a quality evaluation measure. The ACPCC is evaluated by averaging the CPCC results obtained for a particular algorithm over different data sets, as defined by where S is the number of data sets in our experiments.

In our experiments, we have used similar data sets, as applied in other paper [3]. This includes 18 real data sets, as described in Table 1. In our experiments, we have used 7 hierarchical clustering algorithms (
L = 7) to form the clustering ensemble. 5.1. Comparison of hierarchical clustering combination methods
In this paper, we have applied seven basic hierarchical clustering algorithms [21] on 18 data sets. Ta-ble 2 presents the CPCC values for each clustering algorithms for each dataset. In this table, we have included the average and variance of our results for each dataset. In another set of experiments, we have applied the HHCC , MVHCC , MDHCC and MinHCC methods to create different ensembles. Ta-ble 3 illustrates the CPCC values corresponding to these methods. In last row of table, the ACPCC and Confidence Interval are reported. The HHCC method is significantly better than MinHCC by 90%. In reference to [6] we come to conclude that in a set of experiments the Min aggregator is the best aggrega-tor among the aggregators including Min, Harmonic mean, Geometric mean, Arithmetic mean (simple averaging), Euclidean length, and Max. Therefore, we can implicitly conclude that HHCC method is also the best aggregator among them. Figure 3 presents the ACPCC of these methods graphically.
One of the parameters which is used in our implementation is denoted by J which is the number of trials, used in MVHCC method. This parameter and value of on the combination methods. Figure 4 illustrates the ACPCC over data sets in Table 1 by different values of J .Thevalueof ACPCC decreases when the value of J increases in the MVHCC .In HHCC ,thereis not considerable change in the behavior of algorithm.

The results of these experiments reveal that the HHCC method creates an ensemble that the value of its ACPCC indicator is greater than the values of ACPCC of basic clustering algorithms of ensemble. In addition, the HHCC approach has performed better than other aggregation algorithms. In another ex-periments, we investigated the effect of different parameters on the HHCC performance. In this regards, we fix the value of one parameter and change the other. In Figs 5 and 6 we have presented the results of these experiments. The smaller value of clustering algorithm, yields better result. However, its optimum value remains to be determined. When the number of trial, J , is set to 10, experiments have the best performance in all datasets. 6. Conclusion
In this paper, we presented a framework for hierarchical clustering combination problem. In this framework, each input dendrogram is represented by a dendrogram description matrix. Then the base clusterers description matrices are combined by various aggregators where the aggregators are linearly combined. Although, the concept of combining dendrograms is new and it is difficult to give compelling evaluation of the proposed methods performance, however, since we have used the Cophenetic coeffi-cient as an evaluation criterion and compared the performances of the proposed HCC methods with each other on 18 publicly available datasets we believe our results are promising. The results of this paper come in support of the idea that the combination of some hierarchical clustering perform better than average of them. References
