 The dramatic rise of time-series data in a variety of contexts, such as social networks, mobile sensing, data centre monitoring, etc., has fuelled interest in obtaining real-time insights from such data using distributed stream processing systems. One such extremely valuable insight is the discovery of correlations in real-time from large-scale time-series data. A key challenge in discovering corre-lations is that the number of time-series pairs that have to be ana-lyzed grows quadratically in the number of time-series, giving rise to a quadratic increase in both computation cost and communica-tion cost between the cluster nodes in a distributed environment. To tackle the challenge, we propose a framework called AEGIS. AEGIS exploits well-established statistical properties to dramati-cally prune the number of time-series pairs that have to be evaluated for detecting interesting correlations. Our extensive experimental evaluations on real and synthetic datasets establish the efficacy of AEGIS over baselines.
 H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Infor-mation Search and Retrieval; H.3.4 [ Distributed systems ]; G.3 [ PROBABILITY AND STATISTICS ]: Correlation and regres-sion analysis Time Series Analysis; Stream Processing; Distributed Computing; Approximate Algorithm
Time-series data is dramatically increasing [4, 13]. This trend is observed since the devices producing time-series data have ex-ploded. This has, obviously, led to a demand for accommodating more and more data for efficiently producing answers to diverse problems. One such important problem, which is the focus of this paper, is to discover correlations in real-time from massive-scale time-series data. Different correlation measures, such as the Pear-son correlation coefficient, cosine similarity, extended Jaccard co-efficient, etc., could be employed for this task.

Real-time correlation detection from large-scale time series data plays an important role in diverse applications. In data center moni-toring, correlations between performance counters (e.g. CPU, mem-ory usage, etc.) across large number of servers are typically con-tinuously queried for recognizing the servers with correlated per-formance patterns [8]. In financial applications, timely discovery of correlations among stock prices can lead stock traders to spot investment opportunities [20]. In online recommendation systems, correlation mining is used to find customers with similar shopping patterns and to provide dynamic recommendations based on how a given customer X  X  behavior correlates with other customers.
Traditional real-time data processing systems [3] designed for this task run on a standalone machine, which cannot handle the rapidly increasing amounts of time series data. This has led to the development of many distributed, fault-tolerant, and real-time com-putation systems [2, 4, 9, 18]. One such popular system is Apache Storm [2]. Even though these systems exist, using them for effi-ciently discovering correlations in time-series is still a challenging problem. Such a trend is analogous to the trend observed in map-reduce systems (e.g., Apache Hadoop); where efficiently perform-ing database operations, like joins, using map-reduce continues to remain a challenging problem [6, 12, 15].
 Distributed real-time computation.
 The problem of real-time correlation discovery has been well-studied in the centralized setting [5, 7, 8, 11, 13, 16, 17, 19, 20], where the assumption is that the time series can be stored and queried on a single machine. But when this assumption does not hold, the corre-lation discovery methods proposed in the centralized context have to be completely re-designed for the distributed environment.
Before we deep dive into the technical details of our approaches, in the following paragraphs let us briefly understand the operation of a distributed realtime computation system. The core idea of most distributed realtime computation systems is the notion of a topology [2, 9]. A topology is a directed graph where the vertexes are known as processing elements . A processing element trans-forms the incoming data according to its programmed operation and transmits it to neighbouring processing element(s) as defined by the topology. In the distributed environment, one or many in-stances of the processing elements are executed on the nodes of a cluster, where the inter-node communication is dictated by the topology. In addition to the above processing principles, below we describe useful concepts related to time series processing:  X  Stream is an unbounded sequence of tuples, where each tuple  X  Source element is a source of the streaming data. It can be used  X  Action element is a processing element that consumes tuples  X  Task is an instance of either a source or action element. One or  X  Parallelism of a given action or source element is the number  X  Shuffling function is a function defined for each edge of the Distributed correlation discovery.
 Given n time-series data streams, we are interested in discovering the pairs of time series that are correlated beyond a threshold over a sliding window of size h . We refer to such time-series pairs as -correlated pairs. Consider a toy example where we have three time-series streams s 1 , s 2 , s 3 and suppose we have the topology shown in Figure 1( a ). This topology performs the na X ve pair-wise correlation computation to obtain the correlations of all the time series pairs and then filter out the unqualified pairs. Similar to per-forming Cartesian product using MapReduce, we use two action elements, of which the first one is responsible for annotating the tuples and the second one performs the correlation computation.
Concretely, time series are read by the source task  X  S . duces tuples of the form ( i,s i,k ) , where i = (1 ,..., 3) and k is the timestamp of the tuple at time instant k . The tuples are then function H (  X  k ; S , B ( shf ) ) , which sends tuples of time series s s B ( shf ) performs two steps. First, it maintains a sliding window for each local stream. For time series i , the sliding window ending at time stamp t is denoted by s t i = ( s i,t ,...,s i,t  X  h +1 We use t to represent the current ending timestamp of the sliding window.

Second, tasks of B ( shf ) modify the keys of the local tuples as-signed to them and create output tuples of the form (  X  i,j  X  , s For example, task  X  B ( shf, 2) will emit tuples  X  8 and  X  Figure 1( b ) and Figure 1( c ). In the final phase, each task or  X 
B ( cmp, 2) receives two tuples for a specific time series pair, where each tuple contains the sliding window of one of the two time series that the key refers to. For example,  X  B ( cmp, 2) will receive  X  both of them have the same key  X  2 , 3  X  . The tasks of B ify the keys because this allows tasks of B ( cmp ) to utilize the tuples having same keys to perform pair-wise correlation computation and check the threshold .

Observe that the above approach has a major shortcoming: for large number of time series (larger n ), B ( shf ) generates and com-municates an exceptionally large number ( O ( n 2 ) ) of tuples and B ( cmp ) performs quadratic correlation computation. Thus this tremen-dously limits its scalability. In an attempt to ameliorate this situa-tion, we propose the AEGIS (A ffine Time-Serie s G rouping with Parti tion-Aware S huffling) approach. To our best knowledge, this is the first work that proposes a truly distributed and scalable solu-tion to the continuous and distributed correlation discovery prob-lem. Overall, this paper makes the following concrete contribu-tions.  X  We define the streaming threshold correlation (or STCOR) query,  X  For reducing AEGIS X  X  communication overhead, we propose  X  Built into AEGIS is a novel shuffling technique called PAS  X  We propose novel pruning techniques that operate on sliding  X  We implement AEGIS and competitor baselines on an open-
Numerous systems [2, 3, 9, 18] have been developed to process data in an high-speed environment. Storm [2] is a widely-used platform, which provides fault tolerance and tuple processing guar-antees. Unlike Storm, some systems like S4 [9] cannot guarantee that each tuple will be processed. Zaharia et al. [18] proposed a new model using micro-batches for distributed stream processing. Their approach abandons the classical topology structure in stream processing. However, such models have larger processing latency compared to the one-tuple-at-a-time model of [2, 3, 9]. However, they do not support operators for performing correlation queries.
Various indexing techniques for querying the correlations of static time-series data stored in a centralized system have been proposed in [7, 8, 13, 17]. Such techniques are not suitable for our dynamic and distributed environment. Computing real-time correlations us-ing a standalone machine has been a key focus of [5, 11, 16, 19], however these techniques are ineffective in a distributed environ-ment. The StatStream system [20] specializes in discovering corre-lations using a grid structure, but it incurs prohibitive communica-tion cost in a distributed environment. Recently, partitioning-based approaches have attracted attention for batch distributed data pro-cessing [6, 12, 15]. However, such approaches are data-dependent and need an aprori data pre-processing step to estimate the data dis-tribution. Scanning the entire data to update the data distribution is impossible in a streaming environment. In summary, this paper is the first work that efficiently discovers correlations from massive time-series data in a scalable distributed environment.
We concretely define the problem of discovering streaming cor-relations in streaming time-series data with the help of the stream-ing threshold correlation (STCOR) query . In this paper, we have used the Pearson correlation coefficient (henceforth, correlation co-efficient) to illustrate our techniques, however, our methods can be used for computing many other correlation measures (refer Sec-tion 5 for details).
The correlation coefficient  X  ( s t i , s t j ) between sliding windows s and s t j is defined as follows: where  X  t i and s t i (or  X  t j and s t j ) are the sample standard deviation and mean of sliding window s t i (or s t j ), respectively. Another im-portant concept is that of a normalized sliding window . A normal-ized sliding window of s t i is denoted as  X  s t i = ( X  s and is defined as [20], where 1 h is an all ones vector of size h . The vector  X  s also be written using the normalized sliding windows as follows:  X  ( s t i , s t j ) = (  X  s t i ) &gt;  X  s t j . Since  X  s entries  X  s i,k varies between  X  1  X   X  s i,k  X  1 . Thus, the range of variation of the normalized sliding window is known apriori, and is independent of the variation in the original sliding window s We shall later exploit this important observation to create partitions over the space of normalized sliding windows.

Additionally, there exists an important relationship between the correlation coefficient and the Euclidean distance between normal-ized sliding windows [20], where D (  X  s t i ,  X  s t j ) is the Euclidean distance between  X  s correlation coefficient between two normalized sliding windows in-creases as the Euclidean distance between them decreases.
Next, suppose the total number of time-series pushed into the system is denoted by n and the set of all the sliding windows at time t is denoted by S t , then the STCOR query is defined as follows:
D EFINITION 3.1 (STCOR QUERY ). Given S t continuously find the set of -correlated sliding window pairs for a given value of 0 &lt;  X  1 . This query is formally written as:
QC ( S t ) = { X  i,j  X  X  i 6 = j, X  ( s t i , s t j )  X  , s t Alternatively, the STCOR query can be defined using the Euclidean distance between two normalized sliding windows, and is written as follows:
QD  X  (  X  S t ) = { X  i,j  X  X  i 6 = j, D (  X  s t i ,  X  s t j where  X  is related to as  X  = p 2(1  X  ) .
 As decreases,  X  increases and vice versa. is always assumed to be greater than zero, however, in practice it could be negative. In such cases it can be shown that if the entries in one of the sliding windows are reversed, then the negative can be treated as positive [20]. Thus, without loss of generality, henceforth we only focus on the positive threshold .
We partition the space of normalized sliding windows  X  S t dimensional orthogonal regular hypercubes called  X  -hypercubes , where each hypercube has edges of length  X  . The hypercube in which a normalized sliding window  X  s t i is contained is identified by its coordinate vector , which is given as follows:
Given an hypercube c t i , the set of its neighbouring hypercubes dimensional  X  -hypercube structure for sliding window s t 3 ; thus, dimensions t , t  X  1 , and t  X  2 are shown. A normalized slid-ing window  X  s t i is mapped to the red  X  -hypercube whose coordinate vector is c t i . Figure 2( b ) shows the set of the neighbouring hyper-cubes N ( c t i ) around c t i . The number of neighbouring hypercubes of c t i , |N ( c t i ) | grows exponentially w.r.t. h ( |N ( c Figure 2: (a)  X   X  X ypercube structures over the space of all nor-malized sliding windows. (b) N ( c t i ) .

An important property of such a hypercube structure is that given a  X  s i and its c t i , all the -correlated sliding windows are either con-tained in the hypercube c t i or in any of the hypercubes in N ( c This property is due to relationship between and  X  given in Defi-nition 3.1. This property can be used for designing a shuffling strat-egy where an action task processing sliding window  X  s t i tuples of the form ( c t k ,  X  s t i ) for all c t k  X  c t although simplistic, does not work when |N ( c t i ) | is large, which is typically the case. In such cases it leads to an exponential increase in communication cost and heavily constrained scalability [6].
This section introduces our core contribution  X  the AEGIS ap-proach . As we shall experimentally demonstrate in Section 6, AEGIS exhibits orders of magnitude lower communication over-head as compared to baselines. The topology of AEGIS is depicted in Figure 3. As compared to topology of the na X ve strategy from Section 1, AEGIS X  X  topology differs in three aspects. First, for effi-cient shuffling and transmission of sliding windows, the first action element B ( shf ) A approximates and groups sliding windows (details will be discussed in Section 4.1). Second, action element B computes the correlation coefficients and removes both false nega-tives and false positives that could be introduced by B ( shf ) Section 5 for details). Third, the tuples between B ( shf ) are shuffled using a novel approach refereed to as partition-aware shuffling (PAS) approach, which essentially shuffles a sliding win-dow group from B ( shf ) A only to those action tasks of B taining correlated sliding windows (refer Section 4.2).
Each task of action element B ( shf ) A is assigned a subset of sliding windows Z t  X  S t by a source task. When a new tuple is received from a source task at timestamp t , a action task of B ( shf ) the normalized sliding windows that are present in its local slid-ing windows. This normalization is performed by incrementally updating all the quantities on the right-hand side of Eq. (2) [20]. The normalized sliding windows corresponding to Z t are denoted as  X  Figure 3: Topology architecture of the AEGIS approach.
 Sliding window grouping: The second step that a action task of B
A performs is to divide the sliding windows overlapping groups G t 1 ,..., G t f known as sliding window groups . A leader is selected for each group, which is called the pivot slid-ing window . The pivot sliding window is selected such that the non-pivot sliding windows in a group can be accurately approxi-mated using the pivot sliding window. The boundary of a sliding window group is represented by a constant bounding hypercube (CBH) . A CBH is a bounding hypercube constructed such that it includes the  X  -hypercubes of all sliding windows in a sliding win-dow group. For discovering the sliding window groups we propose an algorithm called L ocal-Correlation Gra ph Based Group ing (LAP) . LAP has two desirable properties: 1) it uses a greedy strat-egy to create a minimal number of groups, 2) the groups created by LAP are small, i.e., the length of each side of their CBH is at most a constant times  X  . The second property is extremely impor-tant for shuffling sliding window groups only to those action tasks of B ( cmp ) A that contain -correlated pairs.

Since all the action tasks of B ( shf ) A execute the same LAP al-gorithm we explain this algorithm for a single task. First, correla-tion computation over the local sliding windows  X  Z t is performed. This process generates an un-directed graph L = { V,E } referred to as the local-correlation graph . Each vertex v i 1 ,..., |  X  Z t | ) represents a normalized sliding window in edge e i,j  X  E is inserted between vertexes v i and v j if D (  X  s  X  . In practice, the overhead of computing the local-correlation graph is controlled by making sure that only a small number of sliding windows are assigned to each task of element B ( shf ) can be done by increasing the parallism of element B ( shf ) number of time-series n increases.

Then, LAP algorithm is executed on the local-correlation graph for finding the sliding window groups and pivot sliding windows. The objective function minimized by LAP is as follows: where x i = 1 indicates that a vertex v i is chosen as the pivot sliding window. The constraint ensures that each non-pivot sliding window is assigned to at least one group. The optimization problem of Eq. (7) can be reduced to a set-cover problem, which is known to be NP-complete.
 Therefore, we propose a greedy version of LAP called Greedy-LAP that efficiently finds a feasible solution and has a provable near-optimality guarantee. Greedy-LAP starts by finding the set of vertexes N v i that are connected to the vertex v i  X  L , then it ex-ecutes the following steps. S TEP 1: Choose the vertex with the highest degree, let that vertex be denoted as  X  v . S TEP window group G t  X  v is created by making the sliding window associ-ated with  X  v the pivot sliding window and the sliding windows as-sociated with the vertexes N  X  v as non-pivot sliding windows. S 3: This group of vertexes is removed from the graph L and S 1 is executed until there are no vertexes left in L . It can be shown that the above steps greedily minimize the optimization function in Eq. (7). These steps can be efficiently implemented using BFS (breadth-first-search), where we keep track of the vertexes that the algorithm is not allowed to use.
 It can be proved that Greedy-LAP is a ln( |  X  Z t | ) approximation to LAP (refer [1] for the proof). Moreover, the Greedy-LAP algorithm guarantees that the length of each side of the CBH, which contains a sliding window group, is bounded by 3  X  . We call this the 3  X  property of Greedy-LAP. A proof of this property can be found in [1]. The 3  X  property is heavily used in the PAS shuffling phase discussed in Section 4.2.
 Sliding window approximation: This step approximates each non-pivot sliding window with its pivot. We use a particular method known as the affine transformation to perform this approxima-tion [13]. Given a non-pivot sliding window  X  s t i and a pivot sliding window  X  s t m , the affine transformation between  X  s t fined as: where matrix (  X  s t m , 1 h ) is of size h -by-2 . Since h 2 , Eq. (8) is an over-determined system, and we use the least-squares method to obtain an approximate affine transformation  X  w i of w i where e i is the residual error. The advantage of using affine trans-formations is twofold: (a) the size of the affine transformation is not dependent on the length of the sliding window, (b) the affine trans-formation and the pivot sliding window can be used for estimating certain statistical measures (including the correlation coefficient). Thus, a group can be effectively represented by the pivot sliding window plus the affine transformations for approximating the non-pivot sliding windows. In the subsequent sections, we will discuss how to shuffle a sliding window group and compute the correlations between inter-group and intra-group sliding windows.
 Integration with dimensionality reduction techniques: Even though dimensionality reduction methods are not the focus of this paper, we briefly discuss how our framework can incorporate such tech-niques. Orthonormal transformation based dimensionality reduc-tion (e.g., discrete Fourier transformation (DFT), random projec-tions, etc.) can be seamlessly performed in the action element B
A . It is noteworthy that the aforementioned sliding window grouping works on such dimension-reduced sliding windows. In addition, due to the distance-preserving nature of dimensionality reduction techniques, the subsequent correlation computation can be performed on lower-dimensional sliding windows [8] without any modification.
 Emitting sliding window group tuples: Lastly, each sliding win-dow group is used for forming a output tuple of B ( shf ) is constructed as follows. For a group G t j , the key of the output tu-ple is the coordinate vector c t j corresponding to this group X  X  pivot sliding window  X  s t j . An action task of B ( cmp ) A uses c position and distribution of the sliding windows in G t j of the output tuple contains the pivot sliding window  X  s approximate solution  X  w i and error norm k e i k for each non-pivot sliding window  X  s t i in G t j . It is noteworthy that this information can be used to sufficiently reconstruct the non-pivot windows, such that correlation coefficients (and other measures) can be computed with acceptable accuracy. If the LAP algorithm, in the process of con-structing the local-correlation graph, finds that a sliding window pair is already -correlated, then that pair is directly sent to a task of B ( agg ) A as shown in Figure 3.
The na X ve key-based shuffling function, discussed in Section 1 is communication inefficient due to the quadratic data replication. On the other hand, the way of shuffling sliding windows among  X  -hypercubes, discussed in Section 3 makes the communication in-efficiency even more severe, as the sliding window size h increases. Therefore, we propose an enhanced partition-aware shuffling func-tion that (a) restricts the number of shuffling dimensions, thereby replicating sliding window groups independent of h and n , and (b) shuffles and replicates groups only to those tasks that could contain other groups with -correlated sliding windows.

The idea of PAS is to create coarse-grained partitions as com-pared to the ones created by the  X  -hypercube structure of Section 3.2. The  X  -hypercubes were created from the query threshold , on the contrary the PAS partitions are created based on the parallelism P
A of the second action element B PAS three things are ensured: 1) groups that lie in a particular PAS partition are consistently assigned to the same partition, which can-not be guaranteed in key-based shuffling, 2) each partition is han-dled by an unique action task of B ( cmp ) A , 3) the sliding window groups that are not fully contained in a PAS partition are replicated and shuffled only to the neighbouring partitions, such that the repli-cation of a sliding window group is bounded by P ( cmp ) A munication overhead is drastically minimized.
 Partitioning: The partitioning function f PAS of PAS maps each normalized sliding window  X  s t i into a vector called the partition Figure 4: Illustration of PAS shuffling: (a) Parallelism P based hypercube partitioning; (b) Controlled sliding window group replication and shuffling amongst the partitions. vector p t j . Concretely, f PAS :  X  s t i 7 X  p t i and is written as: p = f PAS (  X  s t i ) = ( sgn ( X  s i,t ) d|  X  s i,t |e ,..., sgn ( X  s where sgn ( x ) extracts the sign of its argument. Since  X  1  X   X  s 1 , each entry of the partition vector p t i is either  X  1 or 1 . Next, the number of partitions created by the partition vector is matched to the user-defined parameter P ( cmp ) A as follows: compute log 2 ( P vector is reduced by retaining only the first  X  h entries of the partition vector. In case after computing  X  h we find that  X  h &gt; h , we simply keep all the entries in the partition vector.

An example of this operation is demonstrated in Figure 4( a ) for 3D sliding windows. Suppose, P ( cmp ) A = 4 then we have Since,  X  h &lt; 3 , only dimensions t and t  X  1 are used for partitioning in Figure 4( b ). Thus, normalized sliding windows lying in the blue-grey hyper-rectangle are assigned to the blue partition (and to the  X  B
A action task) in Figure 4( b ), similarly sliding windows in the red partition are assigned to task  X  B ( cmp, 1) A , so on and so forth. Controlled group replication and shuffling: There could be more than one partition to which a sliding window group has to be as-signed in order to find -correlated pairs. The partitions to which a group should be assigned are determined as follows. Let (1 ,...,  X  h ) be a subset of dimensions referred to as the dimension subset . Given a dimension subset  X  h s , the sub-permutation set R j of a partition vector p tations of p t j , such that only the entries corresponding to the di-mensions present in the dimension subset  X  h s are permuted and the remaining are held constant. For example, if p t j = (  X  1 , 1 , 1) and  X  h s = { 2 , 3 } , then only the 2 nd and 3 rd dimension of p such that its sub-permutation set contains only the partitions that could potentially have -correlated sliding windows with one or more sliding windows from group G t j , then G t j should be assigned to all the partitions present in the sub-permutation set. Such a di-mension subset could be found using the following lemma: sliding window group G t j with a pivot  X  s t j and c t j dimension k = (1 ,  X  X  X  ,  X  h ) is added to the set  X  h s sgn ( c j,k ))  X  p t j,k &lt; 0 , otherwise it is irrelevant.
P ROOF . The proof heavily relies on the 3  X  property. Refer [1] for details. Embedded in Lemma 4.1 is also an algorithm for group replication. We essentially scan the vectors c t j and p t j of group G the condition given by Lemma 4.1 to generate the dimension subset  X  h , which is used for creating the sub-permutation set R  X 
Intuitively, the replication condition given in Lemma 4.1 can be explained as follows: a sliding window group is relevant to a par-tition if the group X  X  CBH is (a) fully contained in the partition, (b) overlaps with the partition, or (c) shares a boundary with the par-tition. For example, consider Figure 4( b ), the group G is fully contained in the partition (1 ,  X  1) , therefore it is only repli-cated and shuffled to that partition. G t 1 (in green) is contained in partition (  X  1 , 1) and shares a boundary with (1 , 1) , therefore it is replicated to those two partitions. Likewise, G t 2 is replicated to par-titions (1 , 1) , (  X  1 , 1) , (  X  1 , 1) , and (  X  1 ,  X  1) .
Since all the action tasks of B ( cmp ) A perform the same function, in the following paragraphs we only describe the actions performed by a single task. Recall that each action task of B ( cmp ) sible for processing all the sliding window groups assigned to a single PAS partition. Suppose, if exactly one group is shuffled to a task of B ( cmp ) A , then this task has no action to perform. This is because even if there were -correlated pairs in that group, the LAP algorithm would have already detected and communicated them to tasks of the final action element B ( agg ) A .

If more than one groups are shuffled to an action task, then for each group pair, the task judges whether it is necessary to examine the contents of these groups in detail or the pair can be pruned using a simple criterion. It turns out that it is indeed possible to derive a pruning criterion that only uses the pivot sliding windows of the groups to decide whether further examination is required. The following lemma discusses this criteria:
L EMMA 5.1 (P RUNING C RITERION ). Given two sliding win-dow groups G t j and G t v respectively with pivot sliding windows  X  s j and  X  s t v , if there exists a dimension k such that ( c ( c v,k  X  1) (or ( c j,k  X  2) &gt; ( c v,k +1) ), where k  X  X  t,  X  X  X  ,h  X  t +1 } , then the sliding windows in G t j and G t v are not correlated above . Otherwise, we have to compute the correlation coefficient between each pair of sliding windows in G t j and G t v and verify whether they are -correlated.
 P ROOF . Refer [1].

Intuitively, Lemma 5.1 works as follows. A sliding window group pair requires further examination if (a) the CBH of the groups in the pair overlap with each other, or (b) they share a boundary with each other. For instance, in Figure 4( b ) the CBHs of groups G 1 and G t 2 share a boundary with each other, therefore further ex-amination is required for these groups. On the contrary, the CBH of G in the blue partition does not overlap with that of G t 2 fore this group pair can be pruned. Next, using Lemma 5.1, only the sliding window group pairs that require further examination are selected. Then, for each group pair the sliding window pairs are formed by considering all the sliding windows in both groups to-gether. Now, for each sliding window pair, three possibilities could occur: (1) both the sliding windows in the pair are non-pivot win-dows, (2) the pair has one pivot and one non-pivot windows. (3) both the sliding windows in the pair are pivot windows.

Recall that in a group the non-pivot sliding windows can only be reconstructed approximately. Naturally, if such approximately reconstructed sliding windows are used for correlation computa-tion, the final query answer could contain sliding window pairs that are either false positives or false negatives. In the following para-graphs, we derive both upper and lower bounds for cases (1) and (2) , such that we can detect and delete any potential false positives or negatives in the query answer. Let us begin with the first case. (1) Given two non-pivot sliding windows  X  s t i and  X  s t proximated respectively by pivot sliding windows  X  s t j and  X  s We have,
D (  X  s t i ,  X  s t u ) = k  X  s t i  X   X  s t u k = k (  X  w i, 1  X   X  s t j  X   X  w u, 1  X   X  s t v ) + 1 h (  X  w Let  X  jv = (  X  w i, 1  X   X  s t j  X   X  w u, 1  X   X  s t v ) + 1 triangular inequality, we can derive, Since k e i  X  e u k X k e i k + k e u k , the second inequality of Eq. (12) is transformed to, Similarly, applying k e i  X  e u k  X | k e i k X  X  e u k | to the first in-equality of Eq. (12), we can write the lower bound between two non-pivot sliding windows: Let us denote the above lower bound as L 1 . Similarly, the upper bound is derived as follows:
D (  X  s t i ,  X  s t u ) = k  X  jv + e i  X  e u k X k  X  jv k + k e Let us denote the above upper bound as U 1 . (2) Suppose the pair under consideration contains one pivot slid-ing window and one non-pivot sliding window. Let  X  s t j be the pivot sliding window and  X  s t u be the non-pivot sliding window, then the lower bound is given as follows: where  X  uv =  X  w u, 1  X   X  s t v  X   X  s t j +  X  w u, 0 1 bound as L 2 . Similarly, the upper bound is given as,
Let us denote this upper bound as U 2 . (3) In this case, since both the sliding windows are pivot win-dows, which are not approximated, we compute the Euclidean dis-tance between them and verify whether it satisfies the threshold  X  .
Observe that for computing the above upper and lower bounds only the pivot sliding windows, affine transformations, and norm of the residual error are required. Therefore, in PAS only these quantities are emitted as tuples. As is shown above, even the ap-proximated sliding windows are sufficient to derive effective ac-curacy bounds. Finally, given two normalized sliding windows, if both the sliding windows are non-pivot then bounds L 1 and U computed, otherwise if one window is a pivot sliding window and the other is non-pivot, then L 2 and U 2 are computed. Then these bounds are used for eliminating false positives as follows:  X  True Positive: If U k &lt;  X  , then the sliding window pair is qual- X  False Positive : If L k  X   X   X  U k , then sliding window pair  X  True Negative : If L k &gt;  X  , then pair is not qualified and is The value of k can be 1 or 2 depending on whether case (1) or (2) is valid. In the final step all the sliding window pairs that are emitted by the tasks of B ( cmp ) A are aggregated by tasks of B ( agg ) results are continuously returned to the user as shown in Figure 3. Computing alternative correlation measures: The proposed AEGIS approach can seamlessly handle diverse correlation (or similarity) measures, such as cosine similarity, extended Jaccard similarity and Euclidean distance by adopting a measure-specific normalization processes for different measures. In the interest of space, we refer the interested reader to [1].
Here, we analyze the cost incurred by the AEGIS approach. The computational cost of the element B ( shf ) A includes (a) the cost of updating the normalized sliding windows, which can be performed in constant time, and (b) the cost of local-correlation graph con-struction and Greedy-LAP, which is significantly low as each task is assigned only a very small number of sliding windows. The cost of communicating between B ( shf ) A and B ( cmp ) decomposed as a product of the number of replicas performed by PAS and the cost of communicating each replica. The number of replicas for a sliding window group in PAS does not depend on n and h . And due to our advanced grouping and approximation meth-ods the size (and therefore the communication cost) of each replica is typically low, and only increases extremely slowly. Next, the computation cost incurred by each task of action element B depends on the number of sliding window groups assigned to it. Again, the number of groups assigned to each task of B ( cmp ) typically very small. The communication between action elements B
A and B dow pairs. Since the number of such pairs is unknown apriori, we have to omit the analysis for B ( agg ) A .
In this section, we perform extensive experimental evaluation comparing AEGIS with baseline approaches. First, we describe the baselines in Section 6.1. Then, we introduce the parameters and metrics used for the experimental evaluation, the datasets and the cluster setup. The implementation of AEGIS and the base-lines is done using Apache Storm. We choose Storm here, because Storm has lower processing latency compared to other distributed realtime computation system (e.g, S4, Spark Streaming) due to the one-at-a-time data processing model. Action and source elements are respectively implemented as bolts and spouts in Storm. Bolts and spouts in Storm have user-specified number of tasks (i.e., par-allelism) that execute in parallel in the cluster. We implement the PAS shuffling using a custom grouping function provided by Storm.
NAIVE : This is the na X ve approach described in Section 1, but is improved to incrementally compute correlations [7].

DFTCQ : This is a DFT (discrete Fourier Transform) based ap-proach proposed in [20], but we have adapted it to the distributed setting. It has a topology consisting of three action elements. The first action element shuffles a DFT-reduced sliding window in the way similar to NAIVE. The second action element computes the correlation measure and forwards qualified pairs to the last ele-ment, where duplicate removal is performed. For removing false-positives a sliding window recall phase, similar to AEGIS, is per-formed in the second action element.

LSHCQ : LSHCQ is based on locality sensitive hashing (LSH) [14]. LSH constructs multiple hash tables to find -correlated pairs. If two normalized sliding windows are correlated, they lie closer to each other in the Euclidean space. LSHCQ uses this property to replicate and shuffle potential qualified sliding window to the same task. The topology of LSHCQ consists of three action ele-ments. The first element computes the hash value of the normal-ized sliding windows for each hash table. Sliding windows that are mapped to a bucket in each hash table are shuffled to the same task of the second action element, where the correlation computa-tion is performed over the sliding windows in each bucket per hash table. LSH parameters are chosen to minimize the processing la-tency while ensuring the failure probability (i.e., the probability of not reporting a certain qualified pair) at 5% [14]. We use four evaluation parameters to establish the efficacy of AEGIS: sliding window size h , query threshold , parallelism of B
A denoted P the spout S known as the injection interval .  X  is set by adjusting the data emitting frequency of the spout S . For other parameters, we have a basic setup where:  X  = 1 sec, h = 100 , = 0 . 95 and P A = 4 .

We use three performance metrics: communication cost, pro-cessing latency, and peak capacity. Communication cost is mea-sured by the amount of data units communicated between the bolt which could be float, integer, etc. For AEGIS, the communication cost incurred during PAS and sliding window recalling during cor-relation computation is also included. Processing latency is the av-erage processing time for each task of elements B ( shf ) considered together. The processing time of B ( agg ) A is not included since it is insignificant, due to the use of hash-sets for duplicate removal. As before, for AEGIS, the time spent on recalling slid-ing windows is also included in the processing latency. The peak capacity is the maximum number of time series that an approach can simultaneously process without causing bottlenecks [2, 20]. A bottleneck is caused when sliding windows from the current time instant have to wait (in memory), for the sliding windows from a previous time instant to finish processing [2, 20]. Bottlenecks caused by any bolt tasks are detected and reported by the Storm cluster UI [2]. All the performance metrics reported in this section are computed by averaging every 20 seconds for 10 times, after the cluster reaches a stable state.
We use one synthetic and one real dataset for evaluating all the approaches. The synthetic dataset is generated as follows. Given the required number of time series n , we first generate n series. Each seed time series is generated using a random walk model [20]. From each seed time series s i , we produce  X  dataset as follows: where  X  j,t and  X  j are real random numbers between [0 , 100] , and  X  is sampled once for each time series s j , while  X  j,t is sampled once for each entry in the dataset time series s j . In our experi-ments, we set  X  = 100 and n = 10000 .

The real dataset is the Google Cluster Usage [10] data. It records extensive activities of 12K cluster nodes from a data center over a span of 29 days. We extract three parameters: CPU usage, memory usage and disk space usage for each cluster node. The total number of time series extracted is 36K. For both datasets, the source ele-ment S reads the data files stored in the local file system and then continuously pushes the data to Storm at a given injection interval. Cluster Setup: The experiments are performed using a cluster con-sisting of 1 master and 8 slaves. The master node has 64GB RAM, 4TB disk space (4 x 1TB disks in RAID5) and 12 x 2.30 GHz (In-tel Xeon E5-2630) cores. Each slave node has 6 x 2.30 GHz (Intel Xeon E5-2630) cores, 32GB RAM and 6TB disk space (3 x 2TB disks). All the nodes are connected via 1GB Ethernet.
Peak performance is the performance measured when the clus-ter is at peak capacity. The results shown in these set of experi-ments are for real data. The results for synthetic data exhibit simi-lar trends in [1] and are omitted due to space limitations. The aim of this set of experiments is to demonstrate how each metric (com-munication cost, processing latency, or peak capacity) varies as a function of one parameter, while the other parameters are set to their basic setup values, and the cluster is processing time series at peak capacity .

For instance, assume we are interested in studying the effect of injection interval, we start by setting the other parameters to their basic setup values (i.e., P ( cmp ) A = 4 , = 0 . 95 , h = 100 ). Then as we start to increase the injection interval, the cluster is able to handle more and more time series, since now it has more time to process each one of them. Therefore, we start injecting more time series into the cluster until it reaches peak capacity. Then at peak capacity we note and report the values of the metrics Peak Capacity: The peak capacity increases as a function of the injection interval and parallelism (refer Figure 5( a ) and ( b )). This is because more resources and processing time becomes available in the cluster thereby improving peak capacity. Therefore, at the high-est level of parallelism and injection interval, AEGIS and LSHCQ exhibit 3x and 2x more peak capacity than NAIVE. In addition, the increase of query threshold has very little effect on the peak capacities of NAIVE, DFTCQ and LSHCQ approaches (refer Fig-ure 5( c )). However, the peak capacity of AEGIS presents an in-creasing trend, since increasing query correlation threshold ( ) leads to the decrease of the distance threshold (  X  ), which leads to lower replication and improved communication efficiency. For maximum level of query threshold, AEGIS exhibits 3x more peak capacity than NAIVE. AEGIS achieves about 1.5x improvement over LSHCQ because LSHCQ requires a large number of hash tables to achieve high recall rate [14], and maintaining a large number of hash tables incurs high communication overhead.

On the other hand, the sliding window size affects the peak ca-pacity adversely (refer Figure 5( d )) for all approaches. This is be-cause when sliding window size increases approaches like DFTCQ typically need more DFT coefficients to retain the same amount of energy, and LSHCQ takes more time for computing the hash val-ues. And since the parallelism (or available resources) of all the action elements is fixed in this experiment the peak capacity drops to keep the system bottleneck free. However, in practice peak ca-pacity can be maintained by increasing parallelism. Concretely, at maximum value of the sliding window size AEGIS shows a 2 . 5 x improvement over NAIVE.
 Communication Cost: As peak capacity increases with injection interval and parallelism, the communication cost increases (refer Figure 5( e ) and (f) ) as well. The reason is that more communi-cation is required for maintaining that peak capacity. Even with increased communication costs, at the highest level of the injection interval, AEGIS requires 12 x and 4 x lower communication than NAIVE and DFTCQ respectively, at maximum level of parallelism it needs 12 x less communication over NAIVE, and at most selec-tive query threshold it needs 4 x less than NAIVE. Observe that with the increase of the query threshold, the communication costs are largely stable for all approaches (refer Figure 5( g )). While the communication cost significantly increases with the sliding win-dow size for NAIVE, DFTCQ, and LSHCQ (refer Figure 5( h )). At the highest value of sliding window size, AEGIS needs 9 x, 6 x, and 4 x lower communication than NAIVE, DFTCQ, and LSHCQ re-spectively.
 Processing Latency: Since peak capacity increases with increase in injection interval, the processing latency increases because more number of time series need to be processed (refer Figure 5( i )). In particular, for injection interval the lowest latencies shown by AEGIS are 1.5x better as compared to NAIVE. In Figure 5( j ), the processing latency of all approaches is relatively stable when par-allelism increases. This is because the amount of sliding windows distributed to each task is relatively stable due to the increase of parallelism. In addition, the processing latencies of all approaches varies little when query threshold increases (refer Figure 5( k )). On the other hand, the processing latency increases with increase in sliding window size (refer Figure 5( l )) since action elements spend more time on processing and parsing communicated long sliding windows. For sliding window size the maximum improvement in latency obtained is 1.5x as compared to NAIVE.
In this set of experiments we will study how sensitive the metrics are to the changes in the parameters, while the cluster is not operat-ing at peak capacity, but is processing a constant ( n = 8000 ) num-ber of time series. Since the cluster is only processing a constant number of time series, here we only consider two metrics: com-munication cost and processing latency. The sensitivity of a given performance metric (communication cost or processing latency) to a given parameter is measured by varying the parameter within a pre-defined range, while setting the other parameters to their basic setup values, and computing the mean and standard deviation of the variation observed in the performance metric.

For instance, say we are interested to measure the sensitivity of the communication cost to the sliding window size h . Then we vary the sliding window size in a pre-defined range and then com-pute the sample mean and standard deviation of the communication cost we obtained for each sliding window size. The sample mean and standard deviation are the numbers reported as sensitivity. The results for communication cost and processing latency are shown in Table 1 and Table 2 respectively. We have highlighted the last row of both tables, since communication cost and processing la-tency exhibit significant variation with respect to h . Due to limited Table 1: Mean communication cost (in millions of data units) as a function of the parameters for synthetic data. Numbers in parenthesis are standard deviations. Significant variation of the communication cost is shown in boldface.
 Table 2: Mean processing latency (in seconds) as a function of the parameters for synthetic data. Numbers in parenthesis are standard deviations. Significant variation of the processing latency is shown in boldface.
 space, we show the results only for the synthetic dataset, as the trends exhibited by real dataset are similar (refer [1]). Communication Cost: In Table 1, it can be observed that for all the parameters the mean communication cost of AEGIS is rela-tively stable and orders of magnitude lower as compared to the oth-ers. For the sliding window size h , AEGIS has nearly 20x, 10x and 2x lower cost as compared to NAIVE, DFTCQ and LSHCQ. Similarly, for the query threshold , AEGIS is nearly 60x, 10x and 3x more efficient than NAIVE, DFTCQ and LSHCQ. The injec-tion interval  X  and the parallelism of P ( cmp ) A show similar trends. Thus, AEGIS is highly effective in managing the communication cost when dealing with significantly large variations of the param-eters. This is because PAS in AEGIS robustly mitigates changes to the communication cost arising from different values of the param-eters.
 Processing Latency: The latency of AEGIS is lower and more robust as compared to NAIVE, DFTCQ and LSHCQ. Concretely, for the injection interval  X  , AEGIS approach has nearly 2x lower latency as compared to NAIVE. For the query threshold , average improvement in the latency of AEGIS w.r.t. to NAIVE and DFTCQ is approximately 2 x. When sliding window length increases, the processing latencies of all the approaches increase and thus present larger mean and standard errors. Specifically, the latency of AEGIS is about 50% lower as compared to NAIVE. Overall, AEGIS ex-hibits orders of magnitude improvement in communication cost and processing latency, and is significantly robust to changes in the pa-rameters.
This set of experiments evaluate the pruning power of AEGIS against LSHCQ. Pruning power is defined as the ratio of the num-ber of pairs that are pruned (without having to compute the cor-relation measure) to the total number of time-series pairs. Higher values of pruning power are considered better. Since the pruning Table 3: Pruning power of AEGIS and LSHCQ as a function of query threshold and sliding-window length h for real data. The upper value in each cell corresponds to AEGIS, while the lower value corresponds to LSHCQ.
 power is directly affected by the query threshold and sliding-window length h , in Table 3 we present pruning power as a func-tion of these two parameters. The results in Table 3 are on the real dataset (refer [1] for results on synthetic data). In this experiment  X  and P ( shf ) A are set to their basic set-up values.

Here we only compare AEGIS with LSHCQ, because NAIVE and DFTCQ approaches do not perform pruning and always evalu-ate all sliding window pairs. In each cell of Table 3, the upper value is for AEGIS, while the lower value is for LSHCQ. In AEGIS, PAS only shuffles together the sliding window groups that could poten-tially contain -correlated pairs, therefore many pairs are pruned during the data shuffling phase. Furthermore, higher values of and shorter sliding windows (lower h ) lead to insignificant residual errors while approximating the non-pivot sliding windows in the sliding window groups (refer Section 4), and lead to tighter bounds on the derived correlation (refer Section 5); thereby significantly increasing the pruning power.

It is observed that pruning power of AEGIS decreases as h in-creases and increases as increases. AEGIS achieves the maxi-mum pruning power 0 . 907 at the maximum = 0 . 95 and mini-mum h = 200 . The pruning power of LSHCQ is largely affected only by , while varying little under different sliding window sizes. Overall, AEGIS achieves a maximum of 50% improvement over LSHCQ when = 0 . 7 and h = 200 .
In this paper, we proposed approaches for real-time correlation discovering in large time-series data. Our main proposal the AEGIS approach, uses intelligent time series grouping, approximation, and partition-aware data shuffling methods to dramatically improve per-formance. We demonstrated that AEGIS significantly reduces the communication cost and can easily operate on a large number of time series through extensive experiments.
This work was supported by Nano-Tera.ch through the OpenSen-seII project. [1] Tech. report  X  [2] Storm. http://storm-project.net/ . [3] D. Abadi, D. Carney, U. Cetintemel, M. Cherniack, [4] L. Abraham, J. Allen, O. Barykin, V. Borkar, B. Chopra, [5] R. Cole, D. Shasha, and X. Zhao. Fast window correlations [6] S. Fries, B. Boden, G. Stepien, and T. Seidl. Phidj: Parallel [7] Y. Li, M. L. Yiu, Z. Gong, et al. Discovering longest-lasting [8] A. Mueen, S. Nath, and J. Liu. Fast approximate correlation [9] L. Neumeyer, B. Robbins, A. Nair, and A. Kesari. S4: [10] C. Reiss, J. Wilkes, and J. L. Hellerstein. Google [11] Y. Sakurai, S. Papadimitriou, and C. Faloutsos. Braid: [12] A. D. Sarma, Y. He, and S. Chaudhuri. Clusterjoin: A [13] S. Sathe and K. Aberer. AFFINITY: Efficiently querying [14] N. Sundaram, A. Turmukhametova, N. Satish, T. Mostak, [15] Y. Wang, A. Metwally, and S. Parthasarathy. Scalable [16] D. Wu, Y. Ke, J. X. Yu, S. Y. Philip, and L. Chen. Leadership [17] Q. Xie, S. Shang, B. Yuan, C. Pang, and X. Zhang. Local [18] M. Zaharia, T. Das, H. Li, S. Shenker, and I. Stoica. [19] T. Zhang, D. Yue, Y. Gu, and G. Yu. Boolean representation [20] Y. Zhu and D. Shasha. Statstream: Statistical monitoring of
