 Words are the basic units for text analysis, and therefore word segmentation is criti-cal for Chinese (as well as other Asian languages) natural language processing tasks such as parsing, information retrieval, and machine translation. Although great im-provements have been achieved for Chinese word segmentation (CWS) in recent years [Emerson 2005; Zhao and Liu 2010], the sentence accuracy rate is still low. more, OOV is the main problem [Huang and Zhao 2007] and is far from being solved; moreover, OOV is even worse for cross-domain tasks [Zhao and Liu 2010].

According to their occurrence frequencies, 2 OOV words can be sequentially classified as (1) unseen dictionary words (approximately 44.4%), which can be found in a com-mon dictionary but never appear in the training set; (2) named entities (20.7%), which include person names, location names, and organization names; (3) factoids (16.2%), which include time, numbers, foreign strings, and URLs. These factoids are usually composed of Chinese numerals, Arabic numerals, English letters, and some specific marks (e.g.,  X / X ,  X % X ); (4) suffix-derived words (6.5%), which are formed by appending specific suffixes (such as  X   X  X  X  X   X  (traveler), which is formed by appending a suffix  X   X   X  (-er) to a stem  X   X  X  X   X  (travel)) and will be defined in Section 5; (5) inconsistency (5.2%), which denotes the words that are segmented differently in the training set and the testing set in similar contexts; and (6) others (7.5%), such as abbreviations (e.g.,  X   X  X  X  X   X  is short for  X   X  X  X  X  X  X   X  (Chinese Academy of Sciences)), buzzwords (e.g.,  X   X  X   X  (android)), duplications (e.g.,  X   X  X  X  X  X   X  (happily)), prefix-derived words (e.g.,  X   X  X  X   X  (subsystem)). Among these OOV types, only factoids are handled well by current approaches with character-type information [Zhao et al. 2006a]. For the remaining types, performance is still far from satisfactory [Wang et al. 2012].
The main reason that OOV words are difficult to handle lies in the conflict between the reliability and coverage rates of character n-gram features that are currently adopted. Only large n-grams (e.g., trigrams) are sufficiently reliable for recognizing word boundaries; however, they are very sparse according to Zipf X  X  law [1949]. That is, it is very likely that the trigrams within OOV words will be unseen in the training set. As a result, additional resources such as dictionaries are essential for providing reli-able information to segment these OOV words. Because these additional resources are commonly available, it is reasonable to adopt them to further improve the performance of segmenting OOV words.

To improve performance in segmenting different types of OOV words, we propose a unified framework to incorporate additional information and resources for CWS. In this framework, various additional features (corresponding to different OOV types) extracted from additional resources are independently incorporated into a genera-tive model [Wang et al. 2009] in their corresponding submodels. Subsequently, this enhanced generative model is combined with its corresponding discriminative model [Ng and Low 2004] via log-linear combination. Within this framework, we study the following three types of OOV words: unseen dictionary words, named entities, and suffix-derived words. Our study shows that suffix information hardly improved per-formance, but the additional named-entity recognizer and dictionaries are very help-ful. After jointly incorporating identified named entities and associated dictionary en-tries, our final model achieves the best performance reported in the literature on all corpora provided by the SIGHAN-2005, CIPS-SIGHAN-2010, and Chinese Treebank for open tests. 3
The character-based generative model for incorporating dictionary information and the effect of considering suffixes are originally described in conference papers [Li et al. 2012, 2013], respectively. In this article, the enhanced generative model is further generalized to handle various types of additional information and resources (includ-ing newly-added named-entity information). Additionally, the effect of incorporating a named-entity recognizer is presented. Furthermore, detailed OOV classification statis-tics and complete error analysis for each OOV type are provided. Finally, additional dictionaries and named-entity information are jointly incorporated to further improve performance.

The article is organized as follows. Section 2 introduces the framework for in-corporating additional information. Experiment settings are described in Section 3. Sections 4, 5, and 6 study the effect of incorporating dictionary entries, named enti-ties, and suffix information, respectively, and Section 7 demonstrates the experiment results with a named-entity recognizer and dictionaries applied jointly. Related work is discussed in Section 8, and Section 9 concludes. In this section, a character-based integrated model [Wang et al. 2012], which achieved state-of-the-art performance on the closed test, is first introduced. We then propose a general framework for incorporating additional information into this integrated model for the open test. Afterwards, the training and decoding workflows for our proposed framework are presented. The character-based approach for CWS was first proposed by Xue and Shen [2003], who considered word segmentation to be a sequence-labeling problem by assigning the corresponding position to each character in its associated word. This type of approach can be formulated as follows: where t i is the position tag of character c i within the associated word and is a member of { B , M , E , S } , 4 in which B , M ,and E denote the beginning , middle ,and end of a multicharacter word, respectively, and S denotes that it is a single-character word.
P ( t n 1 | c n 1 ) can be decomposed either discriminatively or generatively. The discrimina-tive approach usually adopts the form where P t i | c i + 2 i  X  2 is estimated by the maximum entropy following widely adopted feature templates [Ng and Low 2004]: (a) C n ( n = X  2,  X  1, 0, 1, 2 ) ; (b) C n C n + 1 ( n = X  2,  X  1, 0, 1 ) ; (c) C  X  1 C 1 .

In contrast, Wang et al. [2009] propose a character-based generative model that de-composes P t n 1 | c n 1 as follows: Because P ( c n 1 ) is the same for all tag sequences, the best tag sequence can be obtained by and its associated position tag.

As noted in Wang et al. [2012], these two approaches possess different error dis-tributions and complement each other. Therefore, they can be combined via the fol-lowing log-linear interpolation. 6 For the i th character in the input sentence, the score function is where  X  is the relative weight of the two factors and can be obtained from the develop-ment set. The best tag sequence is thus
To address factoids, Wang et al. [2012] further preconverted foreign letters, Arabic numbers, and Chinese numbers into their corresponding metacharacters before apply-ing the preceding formulation. For example,  X   X  ,5m  X  X   X  was converted to  X  &lt; CN &gt; , &lt; AN &gt;&lt; FL &gt;  X  X  , X  where  X  X N X  denotes a Chinese number,  X  X N X  denotes an Arabic number, and  X  X L X  denotes a foreign letter. The score function was then reformulated as follows: where u (a unit) denotes the corresponding character after conversion, which can be a metacharacter, a punctuation mark, or a common Chinese character. This form is used as our baseline to integrate the additional resources shown next. The preceding approach only relies on the training corpus and character type informa-tion. However, there is always more that we can take advantage of in practice, such as a dictionary consisting of a large word list. Such resources can be very helpful. For example, suppose the character sequence c i c i + 1 c i + tionary; then the position tags for these three characters would likely be  X  X ME, X  even if this word entry never appears in the training set.

Formally, hints from additional resources for character c the preceding example, A i can simply be the position tag of c nary entry (i.e.,  X  X  X ). For different types of OOVs (i.e., unseen dictionary words, named entities, and suffix-derived words), A i is defined differently (see Sections 4 X 6 for de-tails). Hints from additional resources can be directly encoded as features for the dis-criminative approach, as previous works [Zhao et al. 2006a, 2010; Zhang et al. 2014] have done.

For the generative approach, A i can be incorporated in the following way: Then, P [ u , t , A ] n 1 can be approximated as where M i is the tag-matching status used to check whether the position tag t to u i ) is consistent with the preference implied by A i violate[ A i ], neutral } , where  X  X eutral X  is reserved for the case that A or unreliable. For the simple case mentioned previously, M were  X  X  X , otherwise M i would be  X  X iolate[B]. X 
There are two factors in the last line of Equation (3). The second factor is the character-tag trigram factor of the original generative model, and the first is a tag-matching factor that reflects the likelihood that the tag assigned to the character will match the given additional information. This factor is mainly introduced to give guid-ance if the original generative trigram factor cannot give a reliable prediction when the associated character-tag trigram (or even bigram) is unseen in the training cor-pus. It is reasonable to assert that these two factors should be weighted differently. Therefore, the score function of the enhanced generative model would be
After incorporating the additional information into both the generative and discrim-inative approaches, we can obtain our enhanced integrated approach via log-linear interpolation. The new score function is thus
When there is more than one type of additional information, the score function can be derived similarly. For example, suppose that there are two types of additional infor-mation (denoted by A 1 , A 2 ); in that case, the function would be The training workflow for the proposed framework is described in Figure 1. First, numbers and letters are preconverted into their corresponding metacharacters. At the same time, additional information is obtained for each character in the training sen-tence. Additional resources could be required in this process. For example, a dictionary (or a named entity recognizer) could be adopted to obtain related information. After-wards, the factors/models could be trained on the converted corpus with the additional information.

For decoding, similar to the data preparation part of the training procedure, the characters in the testing sentence are first normalized and then related information is obtained. The best tag sequence is then found via beam search, in which the score function relies on the models obtained in the training phase. Because experiment settings and the baseline system are shared by various experi-ments with different types of additional resources, they are first described as follows. All of the experiments in Sections 4 through 6 are conducted on the corpora provided by SIGHAN-2005 [Emerson 2005] and CIPS-SIGHAN-2010 [Zhao and Liu 2010], which have been widely adopted in various papers for comparing performance. There are four corpora (each includes both training and testing sets) in SIGHAN-2005: the Academia Sinica Corpus (AS), the City University of Hong Kong Corpus (CITYU), the Microsoft Research Corpus (MSR), and the Peking University Corpus (PKU). These corpora are used to compare performance on the in-domain tests. In contrast, CIPS-SIGHAN-2010 only provides one training set X  X he same one as the PKU training set from SIGHAN-2005. Additionally, there are four testing sets from different domains: Literature, Computer, Medicine, and Finance. These corpora are used to compare performance on cross-domain tests. The statistics of the SIGHAN-2005 and CIPS-SIGHAN-2010 corpora are shown in Tables I and II, respectively.

To obtain the distribution of various OOV types, we manually classify the OOV words in the SIGHAN-2005 testing corpora into the six types defined in the Intro-duction. The distributions of various OOV types are shown in Table III. For the generative model, the SRI Language Model Toolkit (SRILM) Goodman 1998]. The Factored Language Model [Bilmes and Kirchhoff 2003] in the SRILM is adopted to train P M i u i i  X  2 , and this factor sequentially backs off to P ( M For the discriminative factor, Zhang X  X  ME Package 8 is adopted to train P t and the training is conducted with Gaussian prior 1.0 and 300 iterations.
In addition, to obtain the weights of different factors in Equations (2), (4), (5) and (6) for each corpus provided by the SIGHAN bakeoffs, we randomly select 1% of the sentences from the training set as its corresponding development set; the remainder is used as the new training set. Afterwards, MERT [Och 2003] is adopted to determine the best weights of the different factors on the corresponding development sets previ-ously mentioned. After the weights are obtained, the full training set is used again to train the final model for each corpus.
 For performance evaluation, the following metrics are adopted: Precision (P), Recall (R), and F-score (F); the F-score is calculated as F we followed Zhang et al. [2004] in conducting the statistical significance tests with 2,000 resampling size and 95% confidence interval. In the following tables, an asterisk is used to indicate that the difference between the proposed approach and the corre-sponding baseline system (or the systems being compared) is statistically significant. The integrated model proposed in Wang et al. [2012], listed in Equation (2), is adopted as our baseline model because our models are derived based on it. To show the distri-bution of the various OOV error types in the baseline system, we give the error number of each OOV type in Table IV. Their corresponding ratios among total OOV errors in each corpus are shown within parentheses.

Table IV shows that the OOV errors have different distributions across various corpora. For example, there were 44% NE errors among all OOV errors in the MSR corpus, whereas this ratio is less than 18% in the other corpora. Overall, unseen dic-tionary words, named entities, and suffix-derived words form the majority of the OOV errors (following the pattern of OOV words observed in Table III after the factoids are excluded, which implies that they were not all wellhandled), and they are addressed separately in the following sections. Unseen dictionary words, especially new terms and unseen idioms, are very difficult to recognize because their associated character-tag n-grams within words are hardly seen in the training corpus, especially when the training corpus is from another domain. For example, for the medical term  X   X   X  X   X  (amino acid), its character bigrams  X   X  X  X   X  X nd  X   X  X   X  can hardly be found in the news domain corpus. Furthermore, the position tag of a character in a term or an idiom is frequently inconsistent with the distribution learned from the training corpus. For example, the character  X   X   X  is always the begin-ning character in common words such as  X   X  X   X (basic)and X   X  X   X  (base). However, it is the middle character in the previously mentioned term.

Fortunately, idioms are not productive, and most of them can be found in a gen-eral dictionary; additionally, many technical terms can also be found in a correspond-ing domain dictionary. Therefore, they can be considerably covered by dictionaries, even when these terms are not seen in the training set. In this section, we first define additional information and the corresponding matching status for adopted dictionar-ies. Afterwards, our unified framework is instantiated with the additional dictionary information. Usually, a given character in a sentence might be covered by more than one dictio-nary word. However, for simplicity, previous approaches [Zhao et al. 2006a, 2010] only consider the longest one. Such simplification will lose information when there are am-biguities, and we thus propose the following feature to denote the ambiguity status of each character. 9
Let c i be the i th character in a given sentence. To establish whether there were ambiguities (and what types of ambiguities) with those dictionary-matching words at c i , we propose the dictionary coverage status feature, which is a mem-ber of { No-Dictionary-Word, No-Ambiguity, Crossing-Ambiguity, Including-Ambiguity, Mixed-Ambiguity } and is defined here. This status depends only on the given sentence and the dictionary and is irrelevant to the position tag assigned to the character. Let D be the given dictionary that only contains multicharacter words and c note the string from c i to c j (including c j ), the conditions for  X  X ncluding-Ambiguity X  (which implies that one dictionary word is included in another dictionary word for the given string) and  X  X rossing-Ambiguity X  (which implies that one dictionary word is overlapped by another dictionary word for the given string) are defined next. (A) Conditions for Including-Ambiguity (IA) (B) Conditions for Crossing-Ambiguity (CA) . Dictionary Coverage Status at c i (denoted by DC i can then be determined as follows:
This definition implicitly implies that a character that possesses the same posi-tion tag for all associated dictionary matching-words will be assigned  X  X o-Ambiguity. X  For example, given a character sequence  X   X  X  X  X  X   X  (university biology) and a set of dictionary-matching words {  X   X   X   X  (university),  X   X  X  X  X   X  (undergraduate) acters  X   X   X  X nd X   X  , X  Condition (A.1) is satisfied but Condition (B) is not; therefore, DC 2 and DC 3 should be set to  X  X ncluding-Ambiguity. X  In contrast, if the dictionary-matching-words are {  X   X  X  X  X   X ,  X   X  X   X  (biology) } , then Condition (B.1) is satisfied but Condition (A) is not; DC 2 and DC 3 should thus be set to  X  X rossing-Ambiguity. X  How-ever, if we have all three matching words {  X   X  X  X   X , X   X  X  X  X   X , X   X  X   X  tions (A.1) and (B.1) are satisfied; therefore, DC 2 and DC Ambiguity X  in this case. Furthermore, if the matching words are DC 2 and DC 3 would be  X  X o-Ambiguity. X  With the preceding definition for dictionary coverage status, the additional information A i and tag-matching status M i for dictionary words are represented as A ( DC i , MWL i ) and M i MD i (which is the tag-matching status for dictionary-matching words and will be specified later), where MWL i denotes the maximum word length of those words that cover c i .

The score function of the generative model in Equation (4) will then be instan-tiated as where MD i is the tag-matching status of t i to D i . Because more than one word could have covered c i , MD i is a member of { MatchLongest( D MatchNone( D i ), Neutral } . Denote the set of dictionary-matching words that begin with c c (1) If W B W M W E =  X  , which indicates that this character is not covered by any (2) If W t i =  X  and W B W M W E =  X  (where W t i is the set of dictionary-matching (3) If  X  w  X  ( W B W M W E ) ,  X  w  X  W t i :len ( w )  X  len ( w ) ,thenMD (4) Otherwise, is set to  X  X atchShorter(D i ), X  indicating that the assigned tag does not
For example, when we consider the second character  X   X   X  in the sequence  X   X  X  X  X  and assume that the dictionary-matching words are {  X   X  X  X   X  (university),  X   X  X  X  X   X  (uni-versity student) } , if the position tag assigned to  X   X   X  is  X  X , X  then the corresponding MD will be  X  X atchLongest(Including-Ambiguity, 3); X  if it is  X  X , X  then the MD will be  X  X atchShorter(Including-Ambiguity, 3); X  and if it is  X  X  X  or  X  X , X  the MD will be  X  X atchNone(Including-Ambiguity, 3). X  Therefore, this candidate feature is associated with each candidate of the position tag. However, if no dictionary word covers this character, then the MD will be set to  X  X eutral X  regardless of which tag is assigned to  X   X  . X  For the discriminative approach, the following two feature templates proposed in Zhao et al. [2006a] are added to the commonly adopted feature templates (a) to (c) described in Section 2.1: (d) MWL i , dt i ; (e) C k , dt i ( k = i  X  1, i , i + 1 ) .

Let W denote the longest of those dictionary words that cover c denotes the length of W and dt i denotes the corresponding tag of c
The score function of the integrated model in Equation (5) would then be instanti-ated as where  X  and  X  are two weighting coefficients to be decided from the development set.
To train the factors in Equation (8), we need a dictionary during training. For the tag-matching factor of the generative model, this dictionary could just be the set of all of the IV words that appeared in the training set (because those words not in the dictionary are implicitly ignored by this factor, they do not need to be considered during training). However, for the discriminative model, some IV words (those occurring fewer than six times in our experiments) have to be excluded from the dictionary for training (because those words not in the dictionary are not ignored by this model, they had to be considered during training); otherwise, the condition of the testing set would seriously mismatch that of the training set. Similarly, the dictionaries used for the tag-matching factor and the discriminative model are also different during testing. For the tag-matching factor, only OOV words should be included in the dictionary, whereas for the discriminative model, the dictionary needs to include both OOV and IV words. 12 Because no dictionary can cover all OOV words for real applications, we want to know how the generative, discriminative, and integrated models with dictionary informa-tion would perform under different OOV coverage rates. Table V gives the results for different OOV coverage rates with  X  =  X  = 0.5 (weights in Equations (7) and (8)) for simplicity; the first row  X  X aseline X  denotes the original character-based approaches introduced in Section 2.1, with preconverting specific types of characters into their corresponding metacharacters.

When only the training words are contained in the dictionary, it can be seen that the performances of the discriminative and integrated approaches with dictionary in-formation are inferior to that of the baselines (0%OOV vs. Baseline), the reason be-ing that the new dictionary-related features in the discriminative model prefer dic-tionary words and the original features (adopted in the baseline) prefer IV words. Because the dictionary words are simply IV words in this case, those IV words would have been over-emphasized and would thus have hurt the OOV word perfor-mance. For example, the new discriminative model incorrectly splits the unseen word  X   X  X  X  X  X  X  X   X  (Chinese Sociological Association) into  X   X  X  /  X  X  X  X   X  , X  because the word  X   X   X   X  is both an IV word and a dictionary word, it is thus preferred by the new model. On contrary, the original discriminative model recognizes this word cor-rectly. For the integrated model, because the weight between the generative and the discriminative model is not readjusted, it is also influenced by this deterioration ef-fect. Therefore, similar errors can also be found. However, because only OOV words are used during testing for the generative model, its performance is not affected in this case.

When the dictionary begins to cover OOV words, the performances of the three mod-els rises sharply. In any event, all of the proposed models outperform the original mod-els when the dictionary covers only 20% of the OOV words. This condition is easy to satisfy in real scenarios. In addition, the weights in Equations (7) and (8) can also be adjusted to control the degree of preference for dictionary information.
Nonetheless, not every model possesses the robustness with varying dictionary cov-erage rates. For example, the corresponding result of the word-based generative tri-gram model, 13 given in the last column of Table V, shows that the trigram model is quite brittle in comparison with our model. In this model, all words kept in the dictio-nary are used to construct the word lattice in the decoding process. Those OOV words are treated as unseen events and given a very low score. However, it can be seen that although the results with the full dictionary are satisfactory, the performance drops rapidly when the OOV coverage rate decreases. This indicates that this model is quite sensitive to the coverage rate of OOV words because of its inability to identify OOV words that are not in the dictionary. This model is thus not useful for real applications because it is impossible to know the corresponding dictionary coverage rate in the test-ing set in advance. Therefore, determining the robustness of dictionary-based models for different dictionary coverage rates is important in selecting an appropriate model.
To give the performance with a true dictionary coverage rate, several publicly-accessible dictionaries are obtained first for open comparison. These dictionaries are downloaded from the Internet, 14 and they are described briefly in Table VI. For sim-plicity, we adopt the same dictionary (a combination of these dictionaries) for all testing corpora. Because the PKU and MSR corpora are in simplified Chinese, whereas the AS and CITYU corpora are in traditional Chinese, we adopt Open Chinese Convert convert the dictionary between simplified and traditional Chinese. This external dic-tionary contains approximately 1.4M words in total and covers 78%, 44%, 54%, and 51% of the OOV words (excluding factoids) in the four testing sets (PKU, AS, CITYU, and MSR) from SIGHAN-2005. Because external dictionaries are expected to be col-lected by the user in real applications, dictionary words should be consistent with the user X  X  own segmentation criterion. Therefore, to give a true evaluation that reflects the actual situation, words in the dictionary are first transformed into their correspond-ing variations according to the same criteria adopted in the various given corpora. For example,  X   X  X  X  X  X   X  (music television) was converted into  X   X  X   X  (music) and  X   X  X  X   X  (television) in the MSR corpus according to its adopted gold criterion. Also, best weights (  X  and  X  ) are obtained on the development set.

To compare the proposed models with the baseline systems described in Section 2.1 more realistically, we first show the in-domain test results of the generative, discrim-inative, and integrated models with the preceding external dictionary on all of the SIGHAN-2005 testing corpora in Table VII. 16 In the table, the boldface indicates the information results in significant improvement in all models, and larger improvements can be achieved with larger OOV coverage rates, which is consistent with the results in Table V.

In addition to these experiments conducted on in-domain corpora, we test the perfor-mance of our proposed integrated approach on cross-domain corpora provided by CIPS-SIGHAN-2010. Compared with in-domain tests, the OOV rates are always higher on the cross-domain tests (cf. Tables I and II), mainly because there are many more domain-specific terms in cross-domain corpora. Based on the experiment results shown in Table VIII for cross-domain tests, we can see that dramatic improvements can be brought about by utilizing dictionary information. Overall, the proposed integrated approach gives 0.029 improvement in F-score for cross-domain tests while only giving 0.011 for the preceding in-domain tests, which again demonstrates the importance of the OOV coverage rate of the adopted dictionary. To further establish the effectiveness of our integrated approach on the unseen dictio-nary words, Table IX gives the number of unseen dictionary word errors generated by the corresponding baseline system for each SIGHAN-2005 corpus and the number of remaining unseen dictionary word errors of our integrated approach. The correspond-ing error reduction rates for the unseen dictionary words (within parentheses) show that the proposed approach is very effective for this type of OOV error.
The remaining unseen dictionary word errors are mainly attributable to the fact that those words also consist of other shorter, high-frequency words. For example, the word  X   X  X  X  X  X   X   X  (e-Commerce) contains two shorter words,  X   X  X  X   X  (electronic) and  X   X  X   X  (commerce), which are common in the training corpus; thus, it is preferable to split this unseen word with those character n-grams, although this word is contained in the dictionary. The named entities that we address here include person names, location names and or-ganization names. Unlike terms and idioms, named entities are quite productive, and they cannot be considerably covered by a dictionary. In addition, the statistical corre-lation between characters within named entity is usually very weak. For example, the character bigram  X   X  X  X   X  in the location name  X   X  X  X  X   X  (Yangzi River) is unlikely to be seen in other words. Furthermore, location names and organization names can even be nested; for example, the location name  X   X  X  X  X  X  X  X   X  (Former Residence of Song Ching Ling) contains another person name  X   X  X  X  X   X  (Song Ching Ling), and the organization name  X   X  X  X  X  X  X   X  (Chinese Academy of Sciences) also contains another location name  X   X  X   X  (China). As a result, recognizing named entities with only character n-grams is very difficult.

Fortunately, because it is one of the key steps in fields such as information extrac-tion, question answering, and machine translation, named-entity recognition has been well studied with features much richer than character n-grams [Nadeau and Sekine 2007; Sun et al. 2002]. Thus, we propose taking advantage of these existing named-entity recognizers that could be obtained off the shelf. We choose the named-entity recognizer provided by Zhao Hai ceived three second-place rankings and one third-place ranking in the four named-entity recognition tasks in the Fourth SIGHAN Bakeoff [Jin and Chen 2008]. Aside from those character n-gram features, unsupervised segmentation outputs [Feng et al. 2004], assistant NE recognizers, 18 and additional NE lists are utilized in this approach [Zhao and Kit 2008] to improve performance. After the named entities in the sentence are identified, additional named-entity information for each character is instantiated asatuple A i = NE i ( NEtag i , NEtype i , NElen i ) , where NEtag tion tag of c i in the associated NE, which is a member of CWS; NEtype i denotes the type of the NE, which is a member of and NElen i denotes the length of the NE, which is a positive integer. In contrast, if the character is not in any NE, its associated NE i is simply represented as  X  X ot Applicable (NA). X 
With this additional named-entity information, the score function of the generative model in Equation (4) is instantiated as where MNE i is the tag-matching status of t i to NE i , and it is a member of {
Match[ NE i ],Violate[ NE i ] ,Neutral } .If NE i = NA , MNE if t i = NEtag i , MNE i is assigned as  X  X atch[ NE i ]; X  otherwise MNE  X  X iolate[ NE i ]. X  For example, assume NE i = ( B,PER,3 ) (which implies that u beginning of a 3-character person name) and the tag assigned to u would be  X  X atch[B,PER,3], X  and if u i is assigned with other tags, MNE  X  X iolate[B,PER,3]. X 
For the discriminative approach, considering that named entities only account for a small portion of the whole training data, we propose training two discriminative models 19 and then combine them for the purpose of smoothing: one is the model trained with feature templates (a) to (c) described in Section 2.1, and the other is trained on those instances where c i is in a recognized named entity, with the following feature template in addition to templates (a) to (c). (d) NEtag i , NEtype i , NElen i .

The generative model and discriminative model are then combined via log-linear interpolation. Table X shows the performance of BaseNER (the adopted named-entity recognizer) on the OOV named entities. Please note that the performances are calculated according to various word segmentation benchmarks adopted in the various corpora previously mentioned. If a named entity recognized by BaseNER is consistent with the gold seg-mentation, it is considered correct; otherwise, it is considered incorrect. In addition, for simplicity, the recall rate here is calculated based only on those OOV named en-tities. It can be seen from Table X that the performance on the PKU corpus is much lower than that on the others, the main reason being the criterion mismatch between BaseNER and PKU benchmarks. For example, the surname and the given name in a Chinese person name are considered two words in the PKU corpus, such as  X   X  whereas BaseNER takes  X   X  X  X  X   X  to be just one word.

To demonstrate the effectiveness of our approach, forced-decoding (i.e., forcing the decoder to choose the recognized named entity) is adopted for performance comparison. The experiment results of the generative, discriminative, and integrated approaches are given in Table XI, where  X  X aseline X  denotes the corresponding generative, discrim-inative, and integrated approaches mentioned in Section 2.1.

By comparing the forced-decoding approach with the baseline approach, we find that it is harmful for performance when the standards of BaseNER and gold segmentation are considerably inconsistent (see the performance of the PKU corpus in Table XI). Furthermore, even with the same standard, the improvement is still not significant because force-decoding inherits the same errors made by the named-entity recognizer. On the contrary, if the named-entity information is encoded as features and consid-ered during training (as in the proposed approach), both of these problems can be con-siderably immunized by the learned model if similar cases also occur in the training corpus. Furthermore, the discriminative model seemed less effective than the genera-tive approach, because the baseline discriminative model handles OOV better than the baseline generative model, that is, there were fewer OOV NE errors for the baseline discriminative approach.

We also test the performance of our proposed integrated approach on the cross-domain corpora provided by CIPS-SIGHAN-2010. It can be seen from Table XII that the proposed approach does not bring any improvement for cross-domain tests, one of the reasons being that there are many fewer named entities in the four cross-domain corpora than in the news (in-domain) corpora. Another reason (confusing for-eign names with Chinese names) will be studied in the next section. To further investigate the impact of incorporating the named-entity information, Table XIII gives the number of OOV NE errors generated by the baseline integrated approach, the number of those NE errors that could be recognized by the adopted NE recognizer, and the number of OOV NE errors that remain in our proposed integrated approach for each SIGHAN-2005 corpus. The corresponding error reduction rates are also given in Table XIII (within parentheses); they were calculated using (#err-baseline -#err-with-NE) / #Recognized-NE (e.g., 31% = [ ( 128  X  112 )/ 52] ) , showing the effective-ness of the proposed approach for this type of OOV error.

It can be seen that most of the OOV NE errors in the CITYU and MSR corpora can be rescued once they are correctly recognized by the adopted NE recognizer. However, the error reduction rates for the PKU and AS corpora are much lower. After identi-fying those named entities that are recognized but not rescued, we find the following conclusions.

For the PKU corpus, the Chinese family name and the given name are separated according to the gold standard, and there are many more Chinese names than foreign names. Because the NE type given by the NE recognizer does not distinguish foreign names from Chinese names, once a foreign name (which should be considered only one word) with fewer than four characters is recognized by the NE recognizer, our proposed approach tends to split it into two words. For example, our model split the foreign name  X   X  X  X   X  (Kufuor) into  X   X  /  X  X  . X  This problem could have been alleviated if the provided NE-Type could have distinguished foreign names from Chinese names. Because the four cross-domain corpora provided by CIPS-SIGHAN-2010 adopts the same criteria as the PKU corpus, this reason also explains why NE information does not help in cross-domain tests.

In contrast, for the AS training corpus, no  X  X ot X  is typically adopted to separate the given name and the surname within foreign names (e.g.,  X   X  X  X  X  X  X   X  (Michael Jordan)), and the  X  X ot X  frequently behaves as a single character word in this cor-pus. However, the testing corpus does contain the  X  X ot X  within foreign names (i.e.,  X   X  X  X  X  .  X  X   X ). Therefore, eventhough such a foreign name was recognized by the NE recognizer, it is still split into three words  X   X  X  X  X  / . /  X  X  . X  This style inconsistency problem is thus beyond the scope of our proposed model.

However, even for the CITYU and MSR corpora, their relative error reduction rates are still far from perfect, because there are many recognition errors for the NE rec-ognizer. For example, the location name  X   X  X  X  X   X  (post-horse ridge) is recognized as  X   X  [  X  X  X  ] /LOC X  in the MSR corpus. As a result, some new errors are correspondingly introduced, although they are correctly handled by the baseline system.

According to this discussion, we can conclude that our enhanced integrated model can effectively utilize the information provided by the given NE recognizer. It can also be inferred that our model performs better with a better NE recognizer. In linguistics 21 , a suffix is a morpheme that can be placed after a stem to form a new word. A suffix also cannot stand alone as a word. According to this definition, only a few characters can be considered suffixes, such as  X   X   X (-er), X   X   X  (-ize), and  X   X   X  (rate). However, the character  X   X   X  (ship) in the words  X   X  X  X  X   X  (oil storage ship) and  X   X  X  X  X   X  (space ship) can help recognize those OOV words, although it can also appear as an independent word in the phrase  X   X  /  X  /  X   X  (on the ship). We thus loosened the constraint that a suffix can not stand alone as a word in this article to cover more such characters. That is, if a character tends to locate at the end of various words, it is regarded as if it plays the role of a suffix in those words.

In Chinese, suffixes are very productive at forming new words. For example, the word  X   X  X  X  X   X  (traveler) can be formed by concatenating a stem ( X   X  X  X   X , travel) and a suffix ( X   X   X , -er). Although current approaches are able to recognize many suffix-related OOVs, they still remain an important type of error (cf. Table IV). Researchers, especially linguists [Dong et al. 2010], thus seek to further improve OOV word perfor-mance by characterizing the word formation process [Li 2011]. Furthermore, prefix-and suffix-related features are proposed as useful for CWS in some previous works [Tseng et al. 2005; Zhang et al. 2006]; nonetheless, no direct evidence is provided because prefix/suffix features are only part of the adopted features.

However, it is difficult to recognize suffix-derived OOV words, because whether a character is a suffix greatly depends on its context. For example, the character  X   X   X  isasuffixintheword X   X  X   X   X  (initialize). However, it becomes a prefix in the word  X   X  X  X   X  (chemical fiber). In addition, whether a character is a suffix varies with the different annotation standards adopted by various corpora. For example, the character  X   X   X  (factory) is a suffix in words such as  X   X  X  X  X   X  (clothing factory) in the PKU corpus provided by the SIGHAN 2005 Bakeoff [Emerson 2005]. However, it is considered a single-character word on similar occasions in the MSR corpus. Suffixes thus cannot simply be identified with some prespecified characters prepared by the linguist and should be learned from the given benchmark. Because of the difficulty in recognizing true suffixes, previous works [Tseng et al. 2005; Zhang et al. 2006] extract a suffix-like list beforehand from each corpus in a context-free manner. Specifically, Tseng et al. [2005] treat characters that frequently appear at the end of rare words as potential suffixes. In their approach, words with a number of training-set occurrences below a given threshold are selected first, and their ending characters are then sorted according to their occurrences in those rare words. Subse-quently, the suffix-like list is formed with those high-frequency characters. Zhang et al. [2006] construct their list in a similar way but without pre-extracting the rare words.
To reduce the number of suffix errors resulting from these primitive extraction pro-cedures, we propose obtaining and using the suffix list in a more prudent manner as follows.  X  Having considered that a suffix is supposed to be combined with different stems  X  According to our investigation, most suffix-derived words are composed of a
However, there were still two drawbacks to adopting these proposed suffix-like list. (1) The associated context required to decide whether a character should be considered a suffix is either completely not taken into account (in previous approaches) or treated too coarsely (in our proposed approach), and (2) the probability value (a finer informa-tion) that a given character will act as a suffix is not utilized; only a hard-decision flag (within or outside the list) is assigned to each character.

To overcome these two drawbacks, we introduce the context-dependent tagging bias level, which reflects the likelihood that the next character would tend to be the begin-ning of a new word (or be a single-character word) based on the local context. This is motivated by the following observation: if a trailing character is biased towards  X  X  X  or  X  X , X  then the current character will prefer to be tagged as  X  X  X  or  X  X ; X  on the contrary, if the trailing character is biased towards  X  X  X  or  X  X , X  then the current character will prefer to be tagged as  X  X  X  or  X  X . X 
Having considered that the surrounding context might have been unseen for the testing instances, we introduce four different tagging bias probabilities as follows.  X  Context-free tagging bias level ( qf i ) . The quantized value of P ( t  X  Left-context-dependent tagging bias level ( ql i ). Compared with qf  X  Right-context-dependent tagging bias level ( qr i ) . Compared with qf  X  Surrounding-context-dependent tagging bias level ( qs i The additional information A i can be instantiated as one of the three types of suf-fix information previously described (i.e., previous suffix-list, proposed suffix-list, and proposed tagging bias level). Furthermore, the corresponding tag-matching status (de-noted as MS i here) will be decided as follows.  X  For the previous suffix-list feature, MS i is a member of  X  For the proposed suffix-list feature, MS i is also a member of  X  For the proposed tagging bias level feature, MS i
It is reasonable to expect that the character-tag trigram factor is more reliable when c 1 is seen in the training corpus. Therefore, the scoring function for the suffix-list feature is where  X  k is selected according to whether c i i  X  1 is seen.
 For the tagging bias feature, the scoring function is where  X  q , k is selected according to which tagging bias probability factor is used and whether c i i  X  1 is seen. Therefore, we will have eight different  X 
For the discriminative approach, Tseng et al. [2005] propose an additional suffix-like list feature to utilize the suffix information.  X  s 0 . A binary feature indicating whether the current character of concern is on the In addition, we also tested the case of context-free tagging bias level (proposed in Section 6.1), under the discriminative framework, by adding the following template.  X  qf . The context-free tagging bias level.

Please note that qs (also ql and qr ) are not adopted because it is always qs in the training set (thus over-fitted). Therefore, only qf is adopted to make the training and testing conditions consistent. The segmentation results of using different generative models proposed in Section 6.2 for the SIGHAN-2005 corpora are shown in Table XIV.  X  X aseline X  in the table denotes the basic generative model corresponding to Equation (1), with preconverting num-bers and foreign letters into their corresponding metacharacters;  X  denotes the model that adopts the related suffix-like list features, corresponding to Equation (8); and each subrow to the right indicates the method used to extract the list.  X  + Tagging Bias Level X  denotes the model that adopts tagging bias level related features, corresponding to Equation (9).

Table XIV shows that the improvement brought by the tagging bias level is statis-tically significant over the original model for three out of four corpora; however, the difference is small, except for the CITYU corpus. Moreover, for the suffix-like list ap-proaches, the performance is only slightly improved when the suffix-list is extracted and used as we propose. To examine whether the quality of the suffix-list would affect performance, we manually remove those characters that should not have been consid-ered suffixes in each list (e.g., characters such as  X   X   X  X nd X   X   X  that always appear at the end of transliteration). However, the performances are nearly the same, even with the cleaned lists (not shown in the table). The reasons are identified and explained in Section 6.4.

Table XV shows the segmentation results for the various discriminative approaches.  X  X aseline X  in the table denotes the baseline discriminative model that adopts the fea-tures (a) X (c) described in Section 2.1;  X  X seng X  denotes the model with additional feature (d) in Section 6.2, and  X  X hang X  denotes the model with additional features (d) and (e). Finally,  X  X ith qf  X  denotes the model with additional feature (f) instead of features (d) and (e). Please note that qs (also ql and qr ) is not adopted (explained in Section 6.2).
The results in Table XV show that neither the suffix-like list related feature nor the context-free tagging bias level feature could provide any help for the discriminative approach. Similar to the generative approach, no significant benefit is brought about, even when the list is further cleaned by a human. This seems contradictory to the claims given in Tseng et al. [2005] and Zhang et al. [2006], and the reason is studied in the next section.

According to the experiment results, it seems that only the generative approach with the tagging bias level feature could slightly improve performance. To determine whether it truly reduced suffix-derived OOV errors, we collect the number of suffix-derived OOV errors in the baseline approach and the proposed generative approach with the tagging bias level feature, as shown in Table XVI. It can be observed that only a small number (less than 12%) of the suffix-derived OOV errors could actually be rescued. Thus, we are able to conclude that none of the approaches studied in this article effectively reduce suffix-derived OOV errors. We thus do not explore whether suffix information could help in the integrated approach. Furthermore, cross-domain tests are also not conducted. As mentioned previously, whether a character can act as a suffix is highly context-dependent. Although context is considered in our proposed suffix-list and tagging bias approaches, the preference implied by the suffix list or the tagging bias level becomes unreliable when the context is unfamiliar. Table XVII shows the percentage that the preference of different tagging bias factors matched the real tag in the training set. It can be seen that the matching rate (or the influence power) is higher with a broader context. When no context is available (the last column; which is the suffix-list ap-proach), the rate drops dramatically. As a result, many overgeneralized words are produced when qf is adopted. For example, two single-character words  X   X  /  X   X  (this bureau) are wrongly merged into a pseudo-OOV  X   X  X  X  . X  As another example, the first three characters in the sequence  X   X  X  /  X  X  X   X  (championship award tray) are wrongly merged into a pseudo-OOV  X   X  X  X   X  (championship-award). Because the related con-text  X   X  X  X   X  is never seen for the character  X   X  , X  it is thus considered a suffix in this case (as it is indeed a suffix in many other cases, such as  X   X  X  X  X   X  (medicine prize) and  X   X  X  X   X  (first prize)).

However, according to the empirical study by Zhao et al. [2010], the OOV rate can be linearly reduced only with an exponential increasing of corpus size, roughly because of Zipf X  X  law; n-gram is expected to also follow this pattern [Baroni 2009]. Therefore, the sparseness problem becomes more serious for the n-gram with a larger  X  X  X  (i.e., with a broader context) because its number of possible distinct types becomes much greater. As a consequence, there will be many more unseen bigrams than unseen unigrams in the testing set (of course, there will be even more unseen trigrams). Table XVIII shows the unseen ratios for qs , ql , qr ,and qf , in the testing set. It can be observed that the unseen ratio for qs is much larger than that for qf . However, according to the discussion in the previous section, the preference of tagging bias level is not reliable for qf . Therefore, the main reason behind the disappointing results in this section is the conflict between the reliability and the coverage of those suffix related features. That is, a more reliable suffix feature is less likely to be utilized in the testing set. As a result, no significant improvement could be achieved by using suffix-related features. It can be concluded that this problem cannot be solved with character n-gram features. This conclusion should be valuable for the relevant researchers in preventing them from wasting time on similar attempts. Based on the last three sections, the performance on unseen dictionary words and OOV named entities can be effectively improved with additional information, but suffix in-formation can hardly help without side effects. Thus, we propose combining the ap-proaches for unseen dictionary words and named entities and demonstrate how much improvement we can further accomplish with this combination. For the generative approach, the score function is instantiated as where P MD i u i i  X  2 is the tag-matching factor for dictionary information and P MNE i u i i  X  2 is the tag-matching factor for named-entity information.
For the discriminative approach, we adopt the following feature templates within one model (i.e., no smoothing as adopted in Section 5.1) for simplicity: (a) C n ( n = X  2,  X  1, 0, 1, 2 ) ,(d) MWL i , dt i , (b) C n C n + 1 ( n = X  2,  X  1, 0, 1 ) ,(e) C n dt n ( n = (c) C  X  1 C 1 ,(f) NEtag i , NEtype i , NElen i .
 The generative and discriminative approaches are then combined via log-linear interpolation: In addition to the SIGHAN-2005 and CIPS-SIGHAN-2010 corpora, the Chinese Tree-bank [Xue et al. 2005] is also a popular dataset for evaluating word segmentation methods, so we adopt it for performance comparison. We use three versions of this corpus in our tests, that is, CTB5 (LDC2005T01), CTB6 (LDC2007T36), and CTB7 (LDC2010T07). Additionally, we follow Wang et al. [2011] in splitting them into the training set, the development set, and the testing set. The statistics of these three versions of the corpus are shown in Table XIX.

The results are shown in Tables XX to XXII. The state-of-the-art performances in open tests reported in the literature are also given for comparison. The results in the tables show that our approach achieves the best performances and significantly outperforms the best results reported in the literature on all corpora for both in-and cross-domain tests, which clearly demonstrates the effectiveness of our proposed approach. In addition, as shown in the tables, better performance can always be achieved by combining dictionary and NE information, which shows that different resources can be independently utilized in their corresponding submodels under our proposed framework; we even achieved better-than-additive improvement in F-scores when the two resources are combined. Taking the performances on CTB5 as an ex-ample, using dictionary information improved F-score by 0.007 (from 0.977 to 0.984), using NE information improved the score by 0.002 (from 0.977 to 0.979), and using both, by 0.01 (from 0.977 to 0.987). Approaches for CWS can be divided into two groups according to whether any addi-tional information or resources (other than what could be extracted from the training set) is adopted. For the group that does not use any additional information (i.e., the close test), many approaches have been proposed, and they can be further divided into the following categories: only using sub-word-related features [Xue 2003; Ng and Low 2004; Peng et al. 2004; Tseng et al. 2005; Jiang et al. 2008; Xiong et al. 2009]; only using word-related features [Gao et al. 2003; Zhang et al. 2003]; and using both sub-word-and word-related features [Zhang and Clark 2007; Sun 2010]. All of these works that use sub-word-related features adopt the discriminative approach. However, Wang et al. [2009] propose a generative approach and show comparable performance; furthermore, they integrate the generative model with the discriminative model and show that it outperforms all of the close-set performances reported in the literature [Wang et al. 2012].

On the contrary, for the open test in the SIGHAN bakeoffs, in which any knowl-edge or resource can be used to improve performance (which matches the scenario of real applications), there are many fewer papers in this category, which are discussed in detail as follows. Ng and Low [2004] adopt character-type-related features to work with factoids. Wang et al. [2012] propose an alternative way to use character-type in-formation by preconverting specific types of characters into their corresponding meta-characters. Zhao et al. [2006a] introduce dictionary-related features to use external dictionaries. They also propose enlarging the training set with additional training cor-pora annotated with different segmentation standards. Jiang et al. [2009] make use of additional training corpora with annotation adaptation. Zhao et al. [2010] utilize the external dictionary, various assistant segmenters, and the output of an assistant NE recognizer in a discriminative model. Li and Sun [2009] use punctuation marks as implicit annotation. Sun and Xu [2011] enhance word segmentation with unlabeled data. More recently, Jiang et al. [2013] propose utilizing the natural annotations in Web texts to enhance word segmentation.

In addition to these approaches, which aim to improve word segmentation with ad-ditional resources, there are also approaches that jointly manage word segmentation, POS tagging, and even parsing, such as Jiang et al. [2008], Zhang and Clark [2008, 2011], Kruengkrai et al. [2009], Wang et al. [2011], Li and Zhou [2012], Qian and Liu [2012], Hatori et al. [2012], Zhang et al. [2013], and Wang et al. [2013]. As reported in these papers, word segmentation can always benefit from additional information about POS and syntax. However, the task will require a greater computation load if the infor-mation from later phases is not required in the given application, such as information retrieval.

Our work differs from previous works in several aspects. First, we propose a unified framework to incorporate various additional information and resources for the gener-ative model, whereas previous works mainly focus on adding more features to the dis-criminative models or only adopt an additional pre/post-process. Second, we study the major OOV types systematically, whereas previous works solely focus on one specific type or ignore the differences between different types. Third, we show that different resources can be independently utilized in their corresponding submodels under our proposed framework and that positive interaction can be achieved in F-scores when dictionaries and an NE recognizer are combined.
 OOV words are the main error sources for Chinese word segmentation, and different types of OOV words behave differently in various corpora. To systematically address the OOV problem, this article first classifies various OOV words into different types, which reveals that unseen dictionary words, named entities, and suffix-derived words account for the majority of OOV words. A unified framework is then proposed within which different types of additional information can be utilized independently in their corresponding submodels. Under this framework, unseen dictionary words, named en-tities, and suffix-derived words are studied separately. Experiment results and further analysis show that unseen dictionary words and OOV named entities can be effec-tively improved with additional dictionaries and an off-the-shelf named-entity recog-nizer, but suffix-related errors can be only marginally improved. Finally, we jointly utilize dictionary words and recognized named entities to outperform all of the results reported in the literature on all testing corpora.

Our main contributions are threefold. First, we propose a unified framework to in-corporate various types of additional resources. Under this framework, different types of information can be easily incorporated and independently utilized, and they can be combined in our experiments with no negative interference in F-score. Second, con-trary to claims made in some papers, we show that suffix-derived words are difficult to manage with the currently adopted character n-gram features, the reason mainly being in the conflict between the reliability and the coverage rates of those character n-grams. Third, we give the distribution over different OOV types on real corpora and note which ones are more important to solve.

