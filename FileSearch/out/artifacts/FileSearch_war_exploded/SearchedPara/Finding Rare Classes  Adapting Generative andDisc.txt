 Many real life problems are characterized by data distributed between vast yet uninteresting background classes, and sma ll rare classes of interesting instances which should be identified. In astronomy, the vast majority of sky survey image content is due to well understood phenomena, and only 0.001% of data is of inter-est for astronomers to study [12]. In financial transaction monitoring, most are ordinary but a few unusual ones indicate fraud and regulators would like to find future instances. Computer network intrusion detection exhibits vast amounts of normal user traffic, and a very few examples of malicious attacks [16]. Finally, in computer vision based security surveillance of public spaces, observed activi-ties are almost always people going about everyday behaviours, but very rarely may be a dangerous or malicious activity of interest [19]. All of these classifi-cation problems share two interesting properties: highly unbalanced frequencies  X  the vast majority of data occurs in one or more background classes, while the instances of interest for classification are much rarer; and unbalanced prior supervision  X  the majority classes are typically known a priori , while the rare classes are not. Classifying rare event instances rather than merely detecting any rare event is crucial because different c lasses may warrant different responses, for example due to different severity levels. In order to discover and learn to classify the interesting rare classes, ex haustive labeling of a large dataset would be required to ensure sufficient rare class coverage. However this is prohibitively expensive when generating each label re quires significant time of a human ex-pert. Active learning strategies might be used to discover or train a classifier with minimal label cost, but this is complicated by the dependence of classifier learning on discovery: one needs examples of each class to train a classifier.
The problem of joint discovery and cla ssification has received little atten-tion despite its importance and broad relevance. The only existing attempt to address this is based on simply applying schemes for discovery and classifier learning sequentially or in fixed iteration [16]. Methods which treat discovery and classification independently perform poorly due to making inefficient use of data, (e.g., spending time on classifier l earning is useless if the right classes have not been discovered and vice-versa). Achieving the optimal balance is crit-ical, but non-trivial given the conflict between discovery and learning criteria. To address this, we build a generative-discriminative model pair [11,4] for com-puting discovery and learning query criteria, and adaptively balance their use based on joint discovery and classification performance. Depending on the ac-tual supervision cost and sparsity of rare class examples, the quantity of labeled data varies. Given the nature of data dependence in generative and discrimi-native models [11], the ideal classifier also varies. As a second contribution, we therefore address robustness to label quantity and introduce a classifier switch-ing algorithm to optimize performance as data is accumulated. The result is a framework which significantly and consistently outperforms existing methods at the important task of discovery and classification of rare classes.
 Related Work. A common unsupervised approach to rare class detection is out-lier detection: building an unconditional model of the data and flagging unlikely instances. This has a few serious limitations: it does not classify; it fails with non-separable data, where interesting classes are embedded in the majority dis-tribution; and it does not exploit any supervision about flagged outliers, limiting its accuracy  X  especially in distinguishing rare classes from noise.

Iterative active learning approaches are often used to learn a classifier with minimal supervision [14]. Much of the active learning literature is concerned with the relative merits of different query criteria. For example, querying points that: are most uncertain [14]; reduce the version space [17]; or reduce direct approximations of the generalization error [13]. Different criteria may be suited to different datasets, e.g, uncertainty criteria are good to refine decision boundaries, but can be fatal if the classes are non-separable (the most uncertain points may be hopeless) or highly multi-modal. This has led to attempts to select dataset specific criteria online [2]. All these approaches rely on classifiers, and do not generally apply to scenarios in which the target classes are themselves unknown.
Recently, active learning has been applied to discovering rare classes using e.g., likelihood [12] or gradient [9] criteria. Solving discovery and classification problems together with active learning is challenging because for a single dataset, good discovery and classification criteri a are often completely different. Consider the toy scenarios in Figure 1. Here the color indicates the true class, and the symbol indicates the estimated class based on two initial labeled points (large symbols). The black line indicates the initial decision boundary. In Figure 1(a) all classes are known but the decision boundary needs refining. Likelihood sampling (most unlikely point under the learned model) inefficiently builds a model of the whole space (choosing first the points labeled L), while uncertainty sampling selects points closest to the boundary (U symbols), leading to efficient refine-ment. In Figure 1(b) only two classes are known. Uncertainty inefficiently queries around the known decision boundary (choosing first the points U) without dis-covering the new classes above. In contr ast, these are the first places queried by likelihood sampling (L symbols). Evidently, single-criterion approaches are insufficient. Moreover, multiple criteria may be necessary for a single dataset at different stages of learning, e.g., likelihood to detect new classes and uncertainty to learn to classify them. A simple but inefficient approach [16] is to simply iterate over criteria in fixed proportion. In contrast, our innovation is to adapt criteria online so as to select the right strategy at each stage of learning, which can dra-matically increase efficiency . Typically,  X  X xploration X  is automatically preferred while there are easily discoverable classe s, and  X  X xploitation X  to refine decision boundaries when most classes have been discovered. This ultimately results in better rare class detection p erformance than single objective, or non-adaptive methods [16].

Finally, there is the issue of what base classifier to use in the active learning algorithm of choice. One can categorize classifiers into two broad categories: gen-erative and discriminative. Discriminative models directly learn p ( y | x ) for class y and data x . Generative models learn p ( x, y ) and compute p ( y | x ) via Bayes rule. The importance of this for active learning is that for a given generative-discriminative pair (in the sense of equivalent parametric form  X  such as naive Bayes &amp; logistic regression), generative classifiers typically perform better with few training examples, while discriminative models are better asymptotically [11]. The ideal classifier is therefore likely to be completely different early and late in the active learning process . An automatic way to select the right classi-fier online as more labels are obtained is th erefore key. Existing active learning work focuses on single generative [13] or discriminative [17] classifiers. We intro-duce a novel algorithm to switch classifiers online as the active learning process progresses in order to get the best of both worlds.
 2.1 Active Learning In this paper we deal with pool-based uncertainty sampling and likelihood sam-pling because of their computational efficiency and clearly complementary na-ture. Our method can nevertheless be easily generalized to other criteria. We consider a classification problem st arting with many unlabeled instances U = ( x 1 , .., x n ) and a small set of labeled instances does not include the full set of possible labels Y in advance. We wish to learn the posterior conditional distribution p ( y | x ) so as to accurately classify the data in
U . Active learning proceeds by itera tively: i) training a classifier C on L ; ii) us-ing query function Q ( C , L , U )  X  i  X  to select unlabeled instances i  X  to be labeled and iii) removing x i  X  from U and adding ( x i  X  ,y i  X  ) to L .
 Query Criteria. Perhaps the most commonly applied query criteria are uncer-tainty sampling and variants [14]. The intuition is that if the current classification of a point is highly uncertain, it should be informative to label. Uncertainty is typically quantified by posterior entropy, which for binary classification reduces to selecting the point whose posterior is closest to p ( y | x )=0 . 5 .Theposterior p ( y | x ) of every point in U is evaluated and the uncertain points queried, Rather than selecting a single maxima, we exploit a normalized degree of pref-erence p u ( i ) for every point i can be expressed by putting the entropy into a Gibbs function (1). For non-probabilistic SVM classifiers, an approximation to p ( y | x ) can be derived from the distance to the margin from each point [14].
A complementary query criteria is that of low likelihood p ( x | y ) . Such points are badly explained by the current model, and should therefore be informative to label [12]. This may involve marginalizing or selecting the most likely class,
The uncertainty measure in (1) is in spirit discriminative (in focusing on de-cision boundaries), although p ( y | x ) can obviously be realized by a generative classifier. In contrast, the likelihood measure in (2) is intrinsically generative , in that it requires a density model of each class y , rather than just the deci-sion boundary. The uncertainty measure is generally unsuitable for finding new classes, as it focuses on known decision boundaries, and the likelihood measure is good at finding new classes, while being poorer at refining decision boundaries between known classes (Figure 1). Note that the likelihood measure can still be useful to improve known-class classification if the classes are multi-modal  X  it will explore different modes. Our adaptation method will allow it to be used in both ways. Next, we discuss specific parametric forms for our models. 2.2 Generative-Discriminative Model Pairs We use a Gaussian mixture model (GMM) for the generative model and a sup-port vector machine (SVM) for the discriminative model. These were chosen because they may both be incr ementally trained (for active learning efficiency), and they are a complementary generative-discriminative pair in that (assuming a radial basis SVM kernel) they have equivalent classes of decision boundaries [4], but are optimized with very different criteria during learning.
 Incremental GMM Estimation. For online GMM learning, we use the incremen-tal agglomerative algorithm from [15]. To summarize the procedure, for the first n =1 ..N training points observed with the same label y , { x n ,y } N n , we incremen-tally build a model p ( x | y ) for y using kernel density estimation with Gaussian kernels N ( x n , X  ) and weight  X  n = 1 n . d is the dimension of the data x . To bound the complexity, after some maximal number of Gaussians N max is reached, merge two existing Gaussians i and j by moment matching [7]. The components to merge are chosen by the selecting the pair of Gaussian kernels ( G i ,G j ) whose replacement G ( i + j ) is most similar, in terms of the Kullback-Leibler divergence. Specifically, we minimize the cost C ij , Importantly for iterative active learning online, merging Gaussians and updating the cost matrix requires constant O ( N max ) computation every iteration once the initial cost matrix is built. In contrast, learning a GMM with latent variables requires multiple expensive O ( n ) expectation-ma ximization iterations [12]. The initial covariance  X  is assumed uniform diagonal  X  = I  X  2 ,andisestimated a priori by leave-one-out cross validation on the (large) unlabeled set U : Given the learned models p ( x | y ) , we can classify  X  y  X  f gmm ( x ) ,where SVM. We use a standard SVM approach with RBF kernels, treating multi-class classification as a set of 1-vs-1 decision s, for which the decision rule [4] is given (by an equivalent form to (8)) as and p ( y | x ) can be computed based on the binary posterior estimates [18]. 2.3 Combining Active Query Criteria Given the generative GMM and discriminative SVM models defined in Sec-tion 2.2, and their respective likelihood and uncertainty query criteria defined in Section 2.1, our first con cern is how to adaptively combine the query criteria online for discovery and classification. Our algorithm involves probabilistically selecting a query criteria Q k according to some weights w ( k  X  Multi ( w ) )and then sampling the query point from the distribution i  X   X  p k ( i ) ((1) or (2)) 1 . The weights w will be adapted based on the discovery and classification perfor-mance  X  of our active learner at each iteration. In an active learning context, [2] shows that because labels are few and biased, cross-validation is a poor way to assess classification performance, an d suggest the unsupervised measure of binary classification entropy (CE) on the unlabeled set U instead. This is espe-cially the case in the rare class context where there is often only one example of a given class, so cross-validation is not well defined. To overcome this problem, we generalize CE to multi-class entropy (MCE) of the classifier f ( x ) and take it as our indication of classification performance, Here I is the indicator function that returns 1 if its argument is true, and n y is the number of classes observed so far. Importantly, we explicitly reward the discovery of new classes to jointly optimize classification and discovery. We define overall active lea rning performance  X  t ( i ) upon querying point i at time t as, The first right hand term above rewards discovery of a new class, and the second term rewards an increase in MCE (as an estimate of classification accuracy) after labeling point i at time t . The constants (1  X  e ) and (2 e  X  2) ensure the second term lies between 0 and 1 . The parameter  X  is the mixing prior for discovery vs. classification. Given this performance measure, we define an update for the future weight w t +1 of each active criterion k , Here we define an exponential decay (first term) of the weight in favor of (second term) the current performance  X  weighted by how strongly criteria k recommended the chosen point i , compared to the joint recommendation p ( i )= k p k ( i ) .  X  is the forgetting factor. The third term encourages exploration by diffusing the weights so every criterion is tried occasionally. In summary, this approach adaptively selects more frequen tly those criteria that have been suc-cessful at discovering new classes and/or increasing MCE, thereby optimizing both discovery and classification accuracy. 2.4 Adaptive Selection of Classifiers As discussed in Section 1, although we br oadly expect the generative GMM clas-sifier to have better initial performance, and the discriminative SVM classifier to have better asymptotic performance, the ideal classifier will vary with dataset and active learning iteration. The remaining question is how to combine these classifiers [10] online for best performance given any specific supervision budget. Cross-validation to determine reliability is infeasible because of lack of data; however we can again resort to the MCE over the training set U (10). In our ex-perience, MCE is indeed indicative of gene ralization performance, but relatively crudely and non-linearly so. This makes approaches based on MCE weighted posterior fusion unreliable. We therefore choose a simpler but more reliable ap-proach which switches the final classifier at the end of each iteration to the one with higher MCE, aiming to perform as well as the better classifier for any label budget. Additionally, the process of multi-class posterior estimation for SVMs [18] requires cross-validation and is inaccurate with limited data. To compute the uncertainty crit erion (1) at each iteration, we therefore use posterior of the classifier determined to be more reliable by MCE. This ensures that uncertainty sampling is as accurate as possible in both low and high data contexts. Summary. Algorithm 1 summarizes our approach. There are four parameters: Gibbs parameter  X  , discovery vs. classification prior  X  , forgetting rate  X  and exploring rate . None of these were tuned; we set them all crudely to intuitive values for all experiments,  X  = 100 ,  X  =0 . 5 , X  =0 . 6 and =0 . 01 .TheGMM and SVM classifiers both have regularization hyperparameters N max and ( C,  X  ) . These were not optimized, but set at standard values N max =32 , C =1 , X  =1 /d . Algorithm 1. Integrated Active Learning for Discovery and Classification Evaluation Procedure. We tested our method on 7 rare class datasets from the UCI repository [1] and on the CASIA gait dataset [20], for which we addressed the image viewpoint recognition problem. We unbalanced the CASIA dataset by sampling training classes in geometric proportion. In each case we labeled one point from the largest class and the goal was to discover and learn to classify the remaining classes. Table 1 summarizes the properties of each dataset. Per-formance was evaluated at each iteration by: i) the number of distinct classes discovered and ii) the average classificat ion accuracy over all classes. This accu-racy measure weights the ability to classify rare classes equally with the majority class despite the fewer rare class points. Moreover, it means that undiscovered rare classes automatically penalize accu racy. Accuracy was evaluated by 2-fold cross-validation, averaged over 25 runs from random initial conditions. Comparative Evaluations. We compared the following methods: S/R : A baseline SVM classifier making random queries. G/G : GMM classification with GMM likelihood criterion (2). S/S : SVM classifier with SVM un certainty crit erion (1). S/GSmix: SVM classifier alternating GMM likelihood and SVM uncertainty queries (corresponding to [16]). S / GSonline : SVM classifier fusing GMM like-lihood &amp; SVM uncertainty criteria by the method in [2]. S/GSadapt :SVM classification with our adaptive fusion of GMM likelihood &amp; SVM uncertainty criteria (10)-(12). GSsw / GSadapt : Our full model including online switching of GMM and SVM classifiers, as detailed in Algorithm 1.
 Shuttle. (Figure 2(a)). Our methods S/GSadapt (cyan) and GSsw/GSadapt (red), exploit likelihood sampling early for fast discovery, and hence early classi-fication accuracy. (We also outperform th e gradient and EM based active discov-ery models in [9] and [12].) Our adaptive models switch to uncertainty sampling later on, and hence achieve higher asymp totic accuracy than the pure likelihood based G/G method. Figure 2(c) illustrates this process via the query criteria weighting (12) for a typical run. The likelihood criterion discovers a new class early, leading to higher weight (11) and rapid discovery of the remaining classes. After 50 iterations, with no new classes to discover, uncertainty criteria obtains greater reward (11) and dominates, efficiently refining classification performance. Thyroid. (Figure 2(b)). Our GSsw/GSadapt model (red) is the best overall classifier: it matches the initially superior performance of the G/G likelihood-based model (green), but later achieves the asymptotic performance of the SVM classifier based models. This is because of our classifier switching innovation (Sec-tion 2.4). Figure 2(d) illustrates switching via the average (training) classifica-tion entropy and (testing) accuracy of the classifiers composing GSsw/GSadapt. The GMM classifier entropy (black dots) is higher than the SVM entropy (blue dots) for the first 25 iterations. This is approximately the period over which the GMM classifier (black line) has better performance than the SVM classifier (blue line), so switching classifier on entropy allows the pair (green dashes) to always perform as well as the best individual classifier for each iteration.
 Glass. (Figure 2(e)). GSsw/GSadapt again performs best by switching to match the good initial performance of the GMM classifier and asymptotic performance of the SVM. Note the dramatic improvement over the SVM models in the first 50 iterations. Pageblocks (Figure 2(f)). The SVM-based models outperform G/G at most iterations. Our GSsw/GSadapt correctly selects the SVM classifier throughout. Gait view (Figure 2(g)). The majority class contains outliers, so likelihood criteria is unusually weak at discovery. Additionally for this data SVM performance is generally poor, especially in early iterations. GSsw/GSadapt adapts impressively to this dataset in two ways enabled by our contributions: exploiting uncertainty sampling criteria extensively and switching to predicting using the GMM classifier.

In summary the G/G method (likelihood criterion) was usually the most ef-ficient at discovering classes as expect ed. However, it was usually asymptot-ically weaker at classifying new inst ances. This is because generative model mis-specification tends to cost more with increasing amounts of data [11]. S/S, (uncertainty criterion), was general poor at discovery (and hence classification). Alternating between likelihood and uncertainty sampling, S/GSmix (correspond-ing to [16]) did a fair job of both discovery and classification, but under-performed our adaptive models due to its inflexibility. S/GSonline (corresponding to [2]) was better than random or S/S, but was not the quickest learner. Our first model S/GSadapt, which solely adapted the multiple active query criteria, was com-petitive at discovery, but sometimes not the best at classification in early phases with very little data  X  due to exclusively using the discriminative SVM classifier. Finally, by exploiting generative-discriminative classifier switching, our complete GSsw/GSadapt model was generally the best classifier over all stages of learn-ing. Table 2 quantitatively summarizes the performance of the most competitive models for all datasets in terms of area under the classification curve. Summary. We highlighted active classifier learning with a priori unknown rare classes as an under-studied but broadly relevant and important problem. To solve joint rare class discovery and classification, we proposed a new framework to adapt both active query criteria and classifier. To adaptively switch gen-erative and discriminative classifiers online we introduced MCE; and to adapt query criteria we exploited a joint reward signal of new class discovery and MCE. In adapting to each dataset and online as data is obtained, our model signifi-cantly outperformed contemporary alte rnatives on eight standard datasets. Our approach will be of great practical value for many problems.
 Discussion. A related area of research to our present work is that of learning from imbalanced data [8] which aims to learn classifiers for classes with imbal-anced distributions, while avoiding the pitfall of simply classifying everything as the majority class. One strategy to achieve this is uncertainty based active learning [6], which works because the distribution around the class boundaries is less imbalanced than the whole dataset. Our task is also an imbalanced learning problem, but more general in that the rare classes must also be discovered. We succeed in learning from imbalanced distri butions via our use o f uncertainty sam-pling, so in that sense our method generalizes [6]. Although our approach lacks the theoretical bounds of the fusion method in [2], we find it more compelling for various reasons: it addresses a very practical and previously unaddressed prob-lem of learning to discover new classes a nd find new instances of them by jointly optimizing searching for new classes and refining their decision boundaries. It adapts based on the current state of the learning process, i.e., early on, class find-ing via likelihood may be more appropriate, and later on boundary refinement via uncertainty. In contrast [2] solely optimizes classificat ion accuracy and is not directly applicable to discovery. [5] and [3] address the fusion of uncertainty and density (to avoid outliers) criteria for classifier learning (not discovery). [5] adapts between density weighted and un weighted uncertainty sampling based on their expected future error. This is diffe rent to our situation because there is no meaningful notion of future error when an unknown number of classes remain to be discovered. [3] samples from a we ighted sum of density and uncertainty criteria. This is less powerful than our approach because it does not adapt online based on the performance of each criteria. Importantly, both [5] and [3] prefer high density points; while for rare class discovery we require the opposite  X  low likelihood. In comparison to other active rare class discovery work, our frame-work generalizes [12], (which exclusively uses generative models and likelihood criteria) to using mo re criteria and adapting betw een them. [9] focuses on a dif-ferent active discovery intuition, using local gradient to discover non-separable rare classes. We derived an analogous query criterion based on GMM local gra-dient. It was generally weaker than likelihood-based discovery (and was hence adapted downward in our framework) for our datasets, so we do not report on it here. Unlike our work here, [5,12,9] all also rely on the very strong assumption that the user at least specifies the number of classes in advance. Finally, the only other work of which we are aware which addresses both discovery and clas-sification is [16]. This uses a fixed classifier and non-adaptively iterates between discovery and uncertainty criteria (corresponding to our S/GSmix condition). In contrast, our results have shown that our switching classifier and adaptive query criteria provide compelling benefit for discovery and classification.
 Future Work. There are various interesting ques tions for future research includ-ing and how to create tighter coupling bet ween the generative and discriminative components [4], and generalizing our idea s to stream based active learning, which is a more natural setting for some practical problems.
 Acknowledgment. This research was funded by the EU FP7 project SAMU-RAI with grant no. 217899.

