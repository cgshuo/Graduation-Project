 1. Motivation and related work web mining and bioinformatics [13,21,27,22] . Web search algorithms, such as HITS [21,27] or PageRank [48] . 1.1. Markov chains and stochastic data analysis edges. The transition probability distribution for a Markov model can be represented as a matrix. The will be in state j in the next time unit; i.e., T ij  X  P  X  S
A common form of questioning in Markovian data analysis involves checking whether repeated applica-query is has the following form: Example 1.1.  X  X  X heck if  X  T k  X  1 T k  X  p 6 for a given small vector for some k . X  X  for any state s i , the greatest common divisor of the set f n P 1 j T on a power iteration method to calculate the dominant eigenvalue (which provides page rank of documents) (bounded) future time.
 initial probability distribution vector p , short of tabulating all possible T ; T approach needs to be used. For instance, if the user wants to check if  X  T the system needs to evaluate the condition sequence  X  T I  X  p 6 ,  X  T I  X  T p 6 ,  X  T I  X  T able T k is found. For example, to identify k where  X  T k  X  1 program: while ( true ){ } perfect candidates for being outsourced to powerful third party servers. 1.2. Outsourcing markovian stochastic processes and its privacy implications intensive scientific problems and large data analysis problems to be solved rapidly.
A Markovian stochastic analysis application can be outsourced as follows: (1) A compute server hosts the to the requesting party.

Note that outsourcing can help or damage privacy, depending on how the underlying privacy infrastructure hand, outsourcing also introduces new security and privacy challenges:
Although data users may have rights to perform certain operations on the data outsourced by the owner, queries on the data owners data, without leaking data to the users.
 from the third party server, outsourcing schemes can not be widely applicable. involved in outsourcing.
Naturally, without proper techniques to protect data and queries, outsourcing schemes cannot be widely that enable individuals to retain control over their information when interacting with organizations. To address the data-and access-privacy challenges within outsourcing contexts, we are implementing a pri-of Markov chain based data analysis applications in pStore. 1.3. Related work: hiding code and applications from execution environments
Providing end-to-end privacy within a shared data management environment is challenging. Coordinated required.
 encrypted data. Bouganim and Pucheral [8] proposed to use assistant hardware such as smart cards to help
Mining data without revealing sensitive information about data is naturally a challenging problem. Using overhead. Some work has been done on selective and authentic untrusted third-party distribution of XML attacks.

A related approach to providing privacy to outsourced application/data from a malicious execution envi-ify the program in goal-oriented way. The scheme they proposed is suitable for evaluating polynomial and to the hostile host was proposed. Sander and Tschudin [41] considered computing with encrypted input and stated that algebraic homomorphic one-way trapdoor functions allow not only non-interactive computing ced linear algebraic programs with conditions and loops. 1.4. Contributions of this paper mining, in this paper, we focus on access-privacy enabled Markov chain based data analysis computations, a data owner and (b) hidden queries from the clients, and executes the queries in an oblivious manner. to the Markovian process and input queries. We show that, as opposed to simple obfuscation mechanisms, on our preliminary work [35] and show that when transition matrices are diagonalizable , we can compute bound can be computed. 2. Access-private markov chain evaluation will consider the following basic iterative Markov chain based data analysis process: while ( true ){ } input. In Example 1.1 , the condition matrix, M c , was equal to  X  T I  X  and the transition matrix, M
To protect its data (i.e., the transition matrix M t , condition matrix M encrypt M t and M c before they are sent to the data store. Naturally, if M with conditions and loops. In this framework, (i) the data owner would hide its data with its own key, before outsourcing it to the data store ( M (iii) at this stage, the server has the following hidden program: (iv) the client will then decrypt the output.
 matrices, enables the following attack by a malicious server. 2.1. Attack by iteration
Let us consider the following iterative data analysis process and its encrypted version: while ( true ){ } while ( true ){ }
Note that, whatever hiding scheme is used to hide M c , M create a correct execution flow, the execution of the hidden condition M actual condition matrix, M c , and the transition matrix, M sible region of p ). In particular, the attacker can evaluate the following sequence of conditions: which is equivalent to: and observe the outcomes. Let us assume that this sequence of conditions evaluates to ality, p is assumed to be a two dimensional vector and M c linear condition with elements of p . Fig. 3 a shows the line M narrower region: using the second result and the fact that in the region above the line M uation attack .
 of the feasible region measures the degree of access-privacy of the system. 2.2. Solution: systematic enlargement of the feasible regions of the input vectors resented by extra_dim) to expand p into a three-dimensional vector, p derive from repeated evaluation of if-statement, hence guarantees that the feasible region of p become infinite.

Adding dummy dimensions (states) to enlarge feasible region involves expanding the transition and condi-rows and columns of the transition matrix correspond to the states of the Markov chain, introducing one dummy state will increase the storage cost for the transition matrix by  X  N  X  1  X  both condition evaluation and state transition calculation involve multiplying a N
N , the computation cost for condition evaluation or state transition will increase by  X  N  X  1  X  with minimal cost. 3. Enlargement of the feasible regions of the input vectors the Markov chain. However, the expanded Markov chain has to have the same semantics as the original one. transition probabilities has to be modified to maintain the original semantics of the Markov chain. learn the feasible region of the inputs. In effect, given while ( true ){ } we expand the original matrix, the condition matrix, and the input vector such that while ( true ){ } results. 3.1. Degrees of freedom of the input vectors for diagonalizable transition matrices
Let us consider an iterative Markov chain evaluation process, where p is an l -dimension vector, M l l matrix, and M c is an n l matrix. Let us further assume that the matrix M
M  X  X  eigenvalues are k 1 ; k 2 ; ... ; k l . We can then rewrite M is a diagonal matrix with its diagonal elements being eigenvalues  X  k K  X  diag  X  k 1 ; k 2 ; ... ; k l  X  . From this the following theorem follows:
Theorem 3.1 (Dimensions of the feasible region). If the diagonalizable transition matrix, M distinct eigenvalues, then it follows that if the host knows the transition matrix, M is access-privacy preserving, then l s n  X  1 &gt; 0 .

Proof 3.1. The feasible region of p the host gets from the evaluation of the condition sequence f M c p 6 ; M c M t p 6 ; M c M 2 t p 6 ; M c M 3 t p 6 ; ... ; M ger than the dimension of the feasible region of the following sequence of equations: f M c p  X  ; M c M t p  X  ; M c M 2 t p  X  ; M c M 3 t p  X  ; ... ; M
M  X  P K P 1 , we can rewrite the above equations as f M c P P P K 2 P 1 p  X  ; M c P K 3 P 1 p  X  ; ... ; M c P K j P 1 p  X  ; ... g .

Since M c  X  X  m i ; j is an n l matrix and P  X  X  p i ; j is an l l matrix, A  X  M we let y  X  P 1 p , then we can transform the above equations into equations of y , where for each j P 0, system of equations where, where the rest of the entries are all 0s.

Given this system of equations, if we define matrices V , Y region of y (if not empty) is l rank  X  Y  X  . We also know that rank  X  Y  X  6 min  X  rank  X  Y we get that rank  X  Y  X  6 n s .

So, the number of the dimensions of the feasible region of y we get from linear equations is at least get from evaluation of the condition sequence is at least l n s  X  1. h an attacker. 3.2. Degrees of freedom of the input vectors for non-diagonalizable transition matrices tor, M t is an l l non-diagonalizable transition matrix, and M diagonal elements being eigenvalues k 1 ; k 2 ; ... ; k l matrix and J is the Jordan canonical form of M t . In other words, J is an l l square matrix of the form diag  X  J 1 ; J 2 ; ... ; J s  X  , where each J i (Jordan block) is a square matrix: From this the following theorem follows:
Theorem 3.2 (Dimensions of the feasible region). If M t has s  X  s 6 l  X  Jordan blocks and if M information leakage.
 f M c p 6 ; M c M t p 6 ; M c M 2 t p 6 ; M c M 3 t p 6 ; ... ; M ger than the dimension of the feasible region of the following set of equations: M
M p  X  ; M c M 2 t p  X  ; ... ; M c M j t p  X  ; ... . Since M of M t ), we can rewrite the above set of equations as f M
J 2 P 1 p  X  ; M c P J 3 P 1 p  X  ; ... ; M c P J j P 1 p  X  ; ... g .

Once again, Since M c  X  X  m i ; j is an n l matrix and P  X  X  p n l matrix. Thus, we can rewrite each of the above equations (where j P 0) as A J further let y  X  P 1 p , then we can restate the above sequence in terms of equations of the form
Given an arbitrarily large k &gt; 0, the above equations lead to
K fact, J can be rewritten as J  X  K  X  N , where K is diagonal and N is nilpotent. In other words, where K i are diagonal matrices with k i at the diagonals and N super-diagonal. Note that a nilpotent matrix has the property that, for some p &gt; 0, N that, due to the specific structures of K and N , the commutativity property, K N  X  N K holds. Thus, for j P p , we have Let us first consider the matrix J p , which corresponds to the p th iteration: Similarly, for J p  X  1 , we can see that transition matrix, M t , then the corresponding part of the system of Eq. (2) can be rewritten as or, equivalently, as compute the rank of Y does not apply. Instead, we rewrite Y as can then further rewrite Y (subject to variations in row-orders) as
Since Y 2 consists of l independent columns, rank  X  Y 2  X  X  l . Y through V n , where each V j  X  1 6 j 6 n  X  has at least s and at most l distinct eigenvalues (where s ber of distinct eigenvalues of M t ). Thus n s 6 rank  X  Y We know that rank  X  Y  X  6 min  X  rank  X  Y 1  X  ; rank  X  Y 2 system of Eq. (5) . The reduction in the number of degrees of freedom caused by p n rows can be at most we can conclude that the feasible region of p , which the host can learn, has at least  X  l s n  X  1 p n  X  dimensions.
  X  1 p n  X  6 0 dimensions. h region unbounded. 3.3. Feasible region enlargement for access-privacy
Theorems 3.1 and 3.2 not only provide mechanisms to quantify the degrees of freedom associated with the input vectors, but they also point to the fact that the greater number, s , of distinct eigenvalues M atic way to add extra dimensions.

Insight 1 unbounding the feasible region: If we can transform matrix M dimensions. Thus the feasible region host can observe is not only enlarged, but made unbounded. ever, behaviorally, the expanded matrix is identical to the original transition matrix.
Example 3.1. Let us be given the following program: while ( true ){ } ( Step i ) In this program the initial distribution is p  X  X  xy T , the transition matrix is M t  X  the condition matrix is M c  X  2 = 32 = 3 the bound is given as  X  1 2  X  T .

Thus, in this example we have l  X  2, s  X  2, and n  X  2. Also, the transition matrix in this program is diagonalizable and its two eigenvalues are 1/3 and 1: region of the input might be bounded.
 example, here we will only show how to expand the matrix by the addition of (only) one new state. The expansion by two or more states is similar. ( Step iii ) We can use different transformations in this step. In this example, we use the following
M
Furthermore, also considering that we have we need to solve the following set of equations:
Straightforward algebraic manipulation of these equations leads to two constraints,
Hence in this step, we need to select a random invertible transformation, such that c constraints.
 three-dimensions: responding expansion of K is ( Step iv ( b )) Therefore, using this K , the equation set (9) , and the fact that M as ( Step iv ( c )) Similarly, by utilizing the chosen transformation and the equivalence between expanded condition matrix and the original one, we can compute a corresponding expanded condition matrix M as ( Fig. 7 b). Note that, due to the resulting negative (e.g., 8 leakages by if-statement attacks.
 kovian stochastic processes. 4. Conclusion and future work
This paper focuses on access-privacy enabled Markov chain evaluation where a non-trusted service pro-any possible encryption scheme for such purpose requires self-composability. Hence if the Markov chain model is known a priori by the non-trusted compute environment, the host can deduce the feasible region methodical addition of extra dimensions. We show that if the data as well as the Markov chain model are execution and outsourcing of such graph-based applications.
 Acknowledgement We thank Prof. Rida Bazzi and Zhichao Liu for their comments.

References
