 Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.). In this paper, we focus on the rank aggregation problem , also called data fusion problem , where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking. In this context, we propose a rank aggregation method within a multiple criteria framework using aggrega-tion mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another. We show that the proposed method deals well with the Information Retrieval distinctive features. Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Systems] : Information Search and Retrieval  X  Re-trieval models .
 General Terms: Algorithms, Measurement, Experimenta-tion, Performance, Theory.
 Keywords: Data fusion, Metasearch Engine, Multiple Cri-teria Approach, Outranking Methods, Rank Aggregation.
A wide range of current Information Retrieval (IR) ap-proaches are based on various search models (Boolean, Vec-tor Space, Probabilistic, Language, etc. [2]) in order to re-trieve relevant documents in response to a user request. The result lists produced by these approaches depend on the ex-act definition of the relevance concept.

Rank aggregation approaches, also called data fusion ap-proaches, consist in combining these result lists in order to produce a new and hopefully better ranking. Such ap-proaches give rise to metasearch engines in the Web context. We consider, in the following, cases where only ranks are Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. available and no other additional information is provided such as the relevance scores. This corresponds indeed to the reality, where only ordinal information is available.
Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].
Several studies argued that rank aggregation has the po-tential of combining effectively all the various sources of ev-idence considered in various input methods. For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant. Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents. Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances. These methods also tend to smooth out biases of the input meth-ods according to Montague and Aslam [22]. Data fusion has recently been proved to improve performances for both the ad hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].

The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies vot-ing algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].
Most current rank aggregation methods consider each in-put ranking as a permutation over the same set of items. They also give rigid interpretation to the exact ranking of the items. Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.
The remaining of the paper is organized as follows. We first review current rank aggregation methods in Section 2. Then we outline the specificities of the data fusion problem in the IR context (Section 3). In Section 4, we present a new aggregation method which is proven to best fit the IR context. Experimental results are presented in Section 5 and conclusions are provided in a final section.
As pointed out by Riker [25], we can distinguish two fam-ilies of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked. These two fam-ilies of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature.
We first introduce some basic notations to present the rank aggregation methods in a uniform way. Let D = { d 1 ,d 2 ,...,d n d } be a set of n d documents. A list or a rank-ing j is an ordering defined on D j  X  D ( j =1 ,...,n ). Thus, d i j d i means d i  X  is ranked better than  X  d i in When D j = D , j is said to be a full list . Otherwise, it is a partial list .If d i belongs to D j , r j i denotes the rank or position of d i in j . We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position | D j | .Let D be the set of all per-mutations on D or all subsets of D .A profile is a n -tuple of rankings PR =( 1 , 2 ,..., n ). Restricting PR to the rankings containing document d i defines PR i .Wealsocall the number of rankings which contain document d i the rank hits of d i [19].

The rank aggregation or data fusion problem consists of finding a ranking function or mechanism  X  (also called a so-cial welfare function in the social choice theory terminology) defined by: where  X  is called a consensus ranking .
This method [5] first assigns a score n j =1 r j i to each doc-ument d i . Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily.
This family of methods basically combine scores of docu-ments. When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].

For instance, Callan et al. [6] used the inference net-works model [30] to combine rankings. Fox and Shaw [15] proposed several combination strategies which are Comb-SUM, CombMIN, CombMAX, CombANZ and CombMNZ.
 The first three operators correspond to the sum, min and max operators, respectively. CombANZ and CombMNZ re-spectively divides and multiplies the CombSUM score by the rank hits. It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others. Meta-search engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings.
In this method, a consensus ranking minimizes the Spear-man footrule distance from the input rankings [21]. For-mally, given two full lists j and j , this distance is given as follows. Given a profile PR and a consensus ranking  X  , the Spearman footrule distance of  X  to PR is given by F (  X , P R )= n j =1 F (  X , j ).

Cook and Kress [8] proposed a similar method which con-sists in optimizing the distance D ( j , j )= 1 2 n d i,i r tage that it considers the intensity of preferences.
This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance. During the training process, probabilities of relevance are calculated. For subsequent queries, documents are ranked based on these probabilities. For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of rel-evance ( R )ofeachdocument d i depending on the segment k it occurs in, is computed, i.e. prob ( R | d i ,k, j ). For sub-sequent queries, the score of each document d i is given by a logistic regression approach for combining scores. Training data is needed to infer the model parameters.
The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest. Formally, let C ( d i  X d i )= PR : d i j d i } be the coalition of rankings that are con-cordant with establishing d i  X d i , i.e. with the proposition d  X  should be ranked better than  X  d i in the final ranking  X  . d beats or ties with d i iff | C ( d i  X d i ) | X | C ( d i  X d
The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Con-dorcet winner, remove it from the lists, and repeat the pre-vious two steps until there are no more documents to rank. Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26].
As in section 2.2.3, a consensus ranking minimizes a geo-metric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule dis-tance. Formally, given two full lists j and j , the Kendall tau distance is given by K ( j , j )= |{ ( d i ,d i ): i&lt;i ,r r ,r j i &gt;r j i }| , i.e. the number of pairwise disagreements be-tween the two lists. It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem.
Markov chains (MCs) have been used by Dwork et al. [11] as a  X  X atural X  method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event. In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): The consensus ranking corresponds to the stationary distri-butionofMC4.
The exact positions of documents in one input ranking have limited significance and should not be overemphasized. For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value. Indeed, in the IR context, the complete order provided by an input method may hide ties .Inthis case, we call such rankings semi orders . This was outlined in [13] as the problem of aggregation with ties. It is therefore important to build the consensus ranking based on robust information :
In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists. This was outlined in [14] as the problem of having to merge top-k results from various input lists. For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.

Rank aggregation of partial lists raises four major diffi-culties which we state hereafter, proposing for each of them various working assumptions: 1. Partial lists can have various lengths, which can favour 2. Since there are different documents in the input rank-3. Some candidate documents are missing documents in 4. When assumption H 2 k holds, each input ranking may
In the IR context, rank aggregation methods need to de-cide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties.
Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information. This consti-tutes a strong assumption that is questionable, especially when the input rankings have different lengths. Moreover, for positional methods, assumptions H 3 and H 4 ,whichare often arbitrary, have a strong impact on the results. For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents. Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H 3 yes -will give rise to very contrasted results, especially regarding the top of the consensus ranking.

Majoritarian methods do not suffer from the above-mentio-ned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information con-tained in the input rankings. Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties. Therefore, majoritarian methods base con-sensus rankings on illusory discriminant information rather than less discriminant but more robust information.
Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches ,whichwere initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion. Therefore, in order to decide whether a document d i should be ranked better than d i in the consensus ranking  X  , the two following conditions should be met: Formally, the concordance coalition with d i  X d i is where s p is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length-which draws the boundaries between an indifference and a preference situation between documents. The discordance coalition with d i  X d i is where s v is a veto threshold which is the variation of doc-ument positions -whether it is absolute or relative to the ranking length-which draws the boundaries between a weak and a strong opposition to d i  X d i .

Depending on the exact definition of the preceding con-cordance and discordance coalitions leading to the definition of some decision rules , several outranking relations can be defined. They can be more or less demanding depending on i) the values of the thresholds s p and s v , ii) the importance or minimal size c min required for the concordance coalition, and iii) the importance or maximum size d max of the dis-cordance coalition.

A generic outranking relation can thus be defined as fol-lows:
This expression defines a family of nested outranking re-c s  X  s which corresponds to the particular relation S (0 ,  X  , n also satisfies important properties of rank aggregation meth-ods, called neutrality, Pareto-optimality, Condorcet prop-erty and Extended Condorcet property, in the social choice literature [29].

Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist. Therefore, we need specific procedures in order to derive a consensus ranking. We propose the following proce-dure which finds its roots in [27]. It consists in partitioning the set of documents into r ranked classes .

Each class C h contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed. Documents within the same equivalence class are ranked arbitrarily.

Formally, let
Each class C h results from a distillation process . It corre-sponds to the last distillate of a series of sets E 0  X  E where E 0 = R \ ( C 1  X  ...  X  C h  X  1 )and E k is a reduced sub-set of E k  X  1 resulting from the application of the following procedure: 1. compute for each d i  X  E k  X  1 its qualification according
When one outranking relation is used, the distillation pro-cess stops after the first application of the previous proce-dure, i.e., C h corresponds to distillate E 1 . When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when | E k | =1.
This section illustrates the concepts and procedures of section 4.1. Let us consider a set of candidate documents PR of different rankings of the documents of R : PR =( 1 ,
Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concor-dance and discordance thresholds are set to values 2 and 1 respectively. The following tables give the concordance, dis-cordance and outranking matrices. Each entry c s p ( d i ( d v ( d i ,d i )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with d i  X d i , i.e. c s p ( d i ,d i )= | C s p ( d i  X d |
D
Table 2: Computation of the outranking relation
For instance, the concordance coalition for the assertion d  X d 4 is C 1 ( d 1  X d 4 )= { 1 , 2 , 3 } and the discordance coalition for the same assertion is D 4 ( d 1  X d 4 )=  X  .There-fore, c 1 ( d 1 ,d 4 )=3, d 4 ( d 1 ,d 4 )=0and d 1 S 1 d
Notice that F k ( d i ,R )( f k ( d i ,R )) is given by summing the values of the i th row (column) of the outranking matrix. The consensus ranking is obtained as follows: to get the first class C , we compute the qualifications of all the documents of E 0 = R with respect to S 1 . They are respectively 2, 2, 2, -2 and -4. Therefore s max equals 2 and C 1 = E 1 = { d 1 ,d Observe that, if we had used a second outranking relation S (  X  S 1 ), these three documents could have been possi-bly discriminated. At this stage, we remove documents of C 1 from the outranking matrix and compute the next class C : we compute the new qualifications of the documents of E 0 = R \ C 1 = { d 4 ,d 5 } . They are respectively 1 and -1. So C 3 = E 1 = { d 4 } . The last document d 5 is the only docu-ment of the last class C 3 . Thus, the consensus ranking is { d 1 ,d 2 ,d 3 } X  X  d 4 } X  X  d 5 } .
To facilitate empirical investigation of the proposed metho-dology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation. In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10]. In this task, there are 75 topics where only a short descrip-tion of each is given. For each query, we retained the rank-ings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams. The performances of these runs are reported in table 3.
 Table 3: Performances of the 10 best runs of the TD task of TREC-2004
For each query, each run provides a ranking of about 1000 documents. The number of documents retrieved by all these runs ranges from 543 to 5769. Their average (median) num-ber is 3340 (3386). It is worth noting that we found similar distributions of the documents among the rankings as in [11].

For evaluation, we used the  X  X rec eval X  standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision ( MAP ) and Success@n ( S@n ) for n=1, 5 and 10.
Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms. In the experi-ments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs. In the tables of the following section, statistically significant differences are marked with an as-terisk. Values between brackets of the first column of each table, indicate the parameter value of the corresponding run.
We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) com-pare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings. We set our basic run mcm with the following parameters. We considered that each input ranking is a complete or-der ( s p = 0) and that an input ranking strongly refutes d  X d i when the difference of both document positions is large enough ( s v = 75%). Preference and veto thresholds are computed proportionally to the number of documents re-tained in each input ranking. They consequently may vary from one ranking to another. In addition, to accept the assertion d i  X d i , we supposed that the majority of the rank-ings must be concordant ( c min = 50%) and that every input ranking can impose its veto ( d max = 0). Concordance and discordance thresholds are computed for each tuple ( d i ,d as the percentage of the input rankings of PR i  X  PR i .Thus, our choice of parameters leads to the definition of the out-
To test the run mcm , we had chosen the following assump-tions. We retained the top 100 best documents from each input ranking ( H 1 100 ), only considered documents which are present in at least half of the input rankings ( H 2 5 ) and as-sumed H 3 no and H 4 new . In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.
Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters. This was validated by preliminary experiments. Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered. After-wards, we study the impact of tuning parameters. Finally, we compare our model performances w.r.t. the input rank-ings as well as some standard data fusion algorithms.
Table 4 summarizes the performance variation of the out-ranking approach under different working hypotheses. In this table, we first show that run mcm22 , in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm .More-over, S@1 moves from 41 . 33% to 34 . 67% (-16 . 11%). This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm , lose this first position but remain ranked in the top 5 documents since S@5 did not change. We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings. Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.

Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar. Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.

From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25 . Therefore, whether we consider all the documents which are present in half of the rankings ( mcm24 ) or we consider all the documents which are ranked in the first 100 positions in one or more rankings ( mcm25 ), increases performances. This result was predictable since in both cases we have more de-tailed information on the relative importance of documents. Tables 5 and 6 confirm this evidence. Table 5, where val-ues between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input rank-ing leads to performance increase. It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.
 Table 5: Impact of the number of retained docu-ments
Table 6 reports runs corresponding to variations of H 2 k Values between brackets are rank hits. For instance, in the run mcm32 , only documents which are present in 3 or more input rankings, were considered successful. This ta-ble shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded. Therefore, we con-clude that many of the relevant documents are retrieved by a rather small set of IR models.
 Table 6: Performance considering different rank hits
For both runs mcm24 and mcm25 , the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds.
Table 7 shows performance variation of the outranking ap-proach when different preference thresholds are considered. We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%. Moreover, S@1 improves from 41 . 33% to 46 . 67% when preference threshold changes from 0 to 5%. We can thus conclude that the input rankings are semi orders rather than complete orders.

Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold. We can conclude that in order to put document d i before d i in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12 . 5% at least half of the input rankings of PR i  X  PR i should be concordant. Performance drops significantly for very low and very high values of the concordance threshold. In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively. Therefore, the outranking relation becomes either too weak or too strong respectively.

In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures. In fact, runs with different veto thresholds ( s v  X  [50%; 100%]) had similar performances even though there is a slight ad-vantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily. Also, tuning the discordance threshold was car-ried out for values 50% and 75% of the veto threshold. For these runs we did not get any noticeable performance varia-tion, although for low discordance thresholds ( d max &lt; 20%), performance slightly decreased.
To study performance evolution when different sets of in-put rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the in-put rankings are considered. Results reported in Table 9 are seemingly counter-intuitive and also do not support pre-vious findings regarding rank aggregation research [3]. Nev-ertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking. Therefore, when they are considered, performance decreases.
 Table 9: Performance considering different best per-forming sets of input rankings
In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the Comb-SUM and the CombMNZ strategies. We also examined the performance of one majoritarian method which is the Markov chain method (MC4). For the comparisons, we con-sidered a specific outranking relation S  X  = S (5% , 50% , which results in good overall performances when tuning all the parameters.

The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A 1 = ( H 100 ,H 2 5 ,H 4 new ): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents. For positional methods, we place missing documents at the queue of the ranking ( H 3 yes ) whereas for our method as well as for MC4, we retained hypothesis H 3 no . The three follow-ing rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A 2 =( H 1 1000 ,H 2 5 ,H i.e. changing the number of retained documents from 100 to 1000. The third row corresponds to the assumption set A 3 =( H 1 100 ,H 2 all ,H 4 new ), i.e. considering the documents present in at least one ranking. The fourth row corresponds the original ranks of successful documents.

The fifth row of Table 10, labeled A 5 , gives performance when all the 225 queries of the Web track of TREC-2004 are considered. Obviously, performance level cannot be com-pared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track. This set of runs aims to show whether relative performance of the various methods is task-dependent.
The last row of Table 10, labeled A 6 , reports performance of the various methods considering the TD task of TREC-2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A 1 of the first row. This aims to show whether relative performance of the various methods changes from year to year.

Values between brackets of Table 10 are variations of per-formance of each rank aggregation method w.r.t. perfor-mance of the outranking approach.
 Table 10: Performance (MAP) of different rank ag-gregation methods under 3 different test collections
From the analysis of table 10 the following can be estab-lished:
In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused. We noticed that the input rankings can hide ties, so they should not be considered as complete orders. Only robust information should be used from each input ranking.
Current rank aggregation methods, and especially posi-tional methods (e.g. combSUM [15]), are not initially de-signed to work with such rankings. They should be adapted by considering specific working assumptions.

We propose a new outranking method for rank aggrega-tion which is well adapted to the IR context. Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and dis-cordant in favor of a specific document. There is also no need to make specific assumptions on the positions of the missing documents. This is an important feature since the absence of a document from a ranking should not be neces-sarily interpreted negatively.

Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies. It also out-performs a good performing majoritarian methods which is the Markov chain method. These results are tested against different test collections and queries. From the ex-periments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods per-form better than positional methods.

The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.

Further work involves investigating whether the outrank-ing approach performs well in various other contexts, e.g. using the document scores or some combination of docu-ment ranks and scores.
 The authors would like to thank Jacques Savoy for his valu-able comments on a preliminary version of this paper. [1] A. Aronson, D. Demner-Fushman, S. Humphrey, [2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew. [4] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. [5] J.Borda.M  X  emoire sur les  X  elections au scrutin. [6] J. P. Callan, Z. Lu, and W. B. Croft. Searching [7] M. Condorcet. Essai sur l X  X pplication de l X  X nalyse ` ala [8] W. D. Cook and M. Kress. Ordinal ranking with [9] N. Craswell and D. Hawking. Overview of the [10] N. Craswell and D. Hawking. Overview of the [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar. [12] R. Fagin. Combining fuzzy information from multiple [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing [15] E. A. Fox and J. A. Shaw. Combination of multiple [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and [17] L. S. Larkey, M. E. Connell, and J. Callan. Collection [18] A. Le Calv  X  e and J. Savoy. Database merging strategy [19] J. H. Lee. Analyses of multiple evidence combination. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. [21] J. I. Marden. Analyzing and Modeling Rank Data . [22] M. Montague and J. A. Aslam. Metasearch [23] D. M. Pennock and E. Horvitz. Analysis of the [24] M. E. Renda and U. Straccia. Web metasearch: rank [25] W. H. Riker. Liberalism against populism . Waveland [26] B. Roy. The outranking approach and the foundations [27] B. Roy and J. Hugonnard. Ranking of suburban line [28] L. Si and J. Callan. Using sampled data and regression [29] M. Truchon. An extension of the Condorcet criterion [30] H. Turtle and W. B. Croft. Inference networks for [31] C. C. Vogt and G. W. Cottrell. Fusion via a linear
