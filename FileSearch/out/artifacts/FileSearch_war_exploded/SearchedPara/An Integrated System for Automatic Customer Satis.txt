
Text classification has matured well as a research disci-pline over the years. At the same time, business intelligence over databases has long been a source of insights for enter-prises. With the growing importance of the services indus-try, customer relationship management and contact center operations have become very important. Specifically, the voice of the customer and customer satisfaction (C-Sat) have emerged as invaluable sources of insights about how an en-terprise X  X  products and service s are percieved by customers. In this demonstration, we present the IBM Technology to Automate Customer Satisfaction analysis (ITACS) system that combines text classification technology, and a business intelligence solution along with an interactive document la-beling interface for automating C-Sat analysis. This system has been successfully deployed in client accounts in large contact centers and can be extended to any services industry setting for analyzing unstructured text data. This demon-stration will highlight the importance of intervention and interactivity in real-world text classification settings. We will point out unique research challenges in this domain re-garding label-sets, measuring accuracy, and interpretability of results and we will discuss solutions and open questions. Categories and Subject Descriptors: H.4.0 Informa-tion Systems Applications: General, I.7.0 Document and Text Processing: General General Terms: Design, Human Factors Keywords: business intelligence, csat analysis, ser-vices, text classification.
Unstructured text is emerging as the single largest source of unprocessed data growing rapidly in today X  X  customer-centric enterprises. Gartner 1 reports indicate that over 80% data in enterprises is unstructured, noisy, and doubles every three months. Processing such huge amount of data and de-riving business insights from i t has become very important http://www.gartner.com in expanding customer-centric programs like customer rela-tionship management (CRM) and customer experience man-agement (CEM). With the growing eminence of the services industry, such programs are critical to competitive growth, and sometimes even survival. It is commonly believed that structured data warehousing and mining reveal  X  what  X  is go-ing right/wrong in an enterprise, but only unstructured data can reveal  X  why  X . CRM practices, contact centers, and cus-tomer facing arms of companies are thus increasingly looking to text mining to help them understand and derive value and insights from text data.

An ever-growing amount of unstructured text data is col-lected in the customer-centric services industry such as con-tact centers. Various  X  X oice of Customer X  (VoC) channels like e-mails, feedback surveys, text messages, service re-quests, agent logs, and conversation transcripts generate lots of unstructured data that needs to be stored and processed. The VoC process aims to listen to customers, understand what is being said, and proactively alter services to provide best in class service quality. C-Sat analysis is one of the most popular techniques of analyzing VoC data.

Wikipedia defines C-Sat as: Customer satisfaction, a busi-ness term, is a measure of how products and services supplied by a company meet or surpass customer expectation. C-Sat is very relevant to the customer care and contact center busi-nesses as it is unpardonable to ignore what a customer is directly saying about a company X  X  products and services. In a typical outsourcing or contact center scenario shown in Figure 1, customers contact the service or product provider through phone or email and usually get routed to 3 rd party contact centers where agents answer these queries using do-main knowledge and knowledge bases. After this interaction, customer feedback is solicited over phone or web-based feed-back forms are sent for comments and suggestions. 10 X 20% of all customers are sent feedback requests and a fraction re-ply. For a large e-commerce account, this amounts to about 40-50 thousand feedback comments a month.
 Typically Quality Analysts (QAs) examine a sample of C-Sat comments ( verbatims ) every few weeks, dig out the in-teractions between agents and customers (text or voice), and assign reason codes (reason for being dissatisfied) to cases. QAs understandably analyze cases of dissatisfied customers, determined by quantitative ratings, while largely ignoring satisfied customers. For illustration, some real comments from customers of a Telecom company are: Figure 1: C-Sat analysis setting in the services in-dustry In the above, the first one is categorized with reason code Canned Response , indicating that the interaction should be more personalized. The second complaint is about Delayed Response , the third is categorized Sales Pitch ,andthefourth is an Accent problem . The QAs then perform aggregate BI analysis to correlate reasons codes with agents, teams, time and other important business parameters. This en-ables QAs to provide qualitative feedback to agents for op-erational improvements (personalize above response instead of copy-pasting replies) and sometimes for process improve-ments (like shift or queue management). Manual C-Sat anal-ysis requires 60% of the time of 2 X 10 people per account for contact centers. Text classification can help automate this making it consistent and exhaustive.
Our system integrates text classification with a BI solu-tion and an interactive document labeling interface. The largest component of contact center or CRM practice costs is labor; hence, technologies and processes to reduce costs or make on-board labor more efficient are crucial. ITACS is a first of its kind solution integrating text classification, BI, and interactive document labeling for services industry deployments. Details of the system can be found in [2]. The unique contributions of ITACS are: (1) a UIMA 2 based cus-tomizable text analytics engine uniquely combining statisti-cal and rule-based classifiers, (2) an integrated architecture comprising commercial (for BI) and freely available (for text analytics) products, and (3) a freely available interactive text labeling tool that helps create the classification system, and can be used for human review/validation of operational systems.
We outline here some unique challenges we faced around text classification and system building that are not often
IBM Unstructured Information Management Architecture: http://www.research.ibm.com/UIMA/ encountered in academic research or have to be viewed dif-ferently. In operational settings, simple statistical models combined with carefully hand-tuned rule-based systems out-perform sophisticated text classification models. The impor-tant requirement of human review of results of classification is usually ignored in academic research; measuring accuracy is non-trivial and often orthogonal to operational success; and the kinds of noise encountered is unique and sadly busi-ness restrictions don X  X  let interesting datasets be public.
A text classification system begins with the definition of a good set of classes or label-set on which the system is based. Label-sets refer to the collection of classes to which customers comments in the C-Sat analysis scenario are cat-egorized depending on the root cause. There is seldom a correspondence between a human proposed label-set and a clustering of documents in some vector space representation. QAs expect the label-set to be actionable i.e. labels should be convertible into exact business actions. Text classification experts on the other hand expect classes to be well-defined and separable, and not confusing with others. Another pre-requisite to training a model for text classification is the availability of a large number of documents categorized man-ually in accordance with the label-set; the larger and more accurate this training set , the better the accuracy of the fu-ture classification will be. In academic research, the training data set is assumed to be sacrosanct, and there is limited in-vestigation about its quality and consistency. On the other hand, we observed serious calibration issues in the labeling of documents causing significant detrimental effect on sys-tem accuracy. 30% inter-human disagreement in labeling has been observed[3], and we observed similar and worse inconsistencies in our training data [2].

Another challenge is presented in measuring accuracy of systems. Ground truth is usually taken to be human labeled data and accuracy compares system predictions with this la-beling. However in operational services settings, we found that comparing with human labeled truth is not too mean-ingful. We encountered very low intra-human consistency (65%) and inter-human consistency (53%) in repeated la-beling of the same set of comments. Statistically sound rep-etition of experiments is unfeasibly expensive due to human labor cost involved. We hence measured human satisfaction on hundreds of system predictions and averaged agreement instead of tradtional accuracy numbers. This is feasible and correct though it is likely to positively (or negatively) bias the expert X  X  judgment since she sees the comment and the predicted label together. However we believe this is only as bad or misleading as accuracy figures in light of the low labeling consistency numbers.

Our most important system development challenge was to output the results of the analysis (classification) in a manner understandable by various classes of business users. Reports at various granularity are expected, from detailed agent per-formance improvement clues for QAs and team leaders to bird X  X  eye views for senior management. Our solution in-cluded a full function BI product (IBM Alphablox or Cog-nos) capable of generating a comprehensive set of graphical reports which could be canned or built-up by experts. BI reporting enables improving agent performance by pointing out areas of improvement and provide operational insights for better customer experience management.

A feedback loop assumes great importance in real-world industrial settings; automated analytics solutions need to build trust in and after deployment. A text classification system becomes significantly more useful and trustworthy if there is a mechanism to inspect class assignments to docu-ments, as well as modify and correct them. Importance of trust and hence involvement of experts in text classification is unique to real world settings and absent in academic re-search. Interactivity and human review of text analysis sys-tem are not very well understood and limited research efforts have addressed some bulk labeling and feature selection is-sues respectively. These systems are research prototypes and we need to integrate interactivity and human review in all phases of a deployed text classification system.

We felt the need for an interactive document labeling sys-tem to close the human-machine feedback loop. We pro-posed and built a tool called IBM TICL (Tool for Interac-tive text Classification and Labeling). It is integrated in the architecture of our system shown in Figure 2. This labeling package acts throughout the system lifecycle: (1) to gener-ate training data and (2) to inspect and correct system label assignments after deployment.
The ITACS system is built on 3 independent but tightly coupled components. The heart of the system is a UIMA (Unstructured Information Management Architecture) based back-end that is a combination of statistical (Bayesian and SVM) and rule-based classifiers for text. UIMA is an open, industrial-strength, scalable and extensible platform for cre-ating, integrating and deploying unstructured information management solutions. UIMA applications work as pipelines of annotators for text processing. Typical applications in-gest plain text, identify named entities (persons, places, or-ganizations) and relations (works-for or located-at), and do various other text mining and feature engineering opera-tions. Our unique contribution was the development of a supervised classification engine as UIMA pipelines compris-ing loosely-coupled annotators.

Post categorization, verbatims, label assignments and other relevant back-end interaction data is stored in a DB2 database. Such back-end data includes particulars of agents and teams interacting with the customer, date of interaction, target product or service, overall score assigned in feedback, agent training information, and other relevant enterprise informa-tion. BI tools are now used to produce interpretable graph-ical reports on the stored analyzed data. The database schema is a star or snowflake schema consisting of a fact table storing the actual comments and their categorization, and referencing a few dimensions. The idea is to store facts along different dimensions for cubing operations like slice and dice, roll-up, drill-down. This helps in analysis across dimensions like agents, teams, products, dates, scores, and complaint classes.

The second component in ITACS is a BI tool which is used for visualization of data stored in the above star schema. Our customized reporting tool, IBM DB2 Alphablox, pro-duces highly interactive graphs, charts, and reports showing correlations between various dimensions of analysis (agents, labels, scores, dates etc.) as shown in Figure 3. Using these reports users can see correlation between different data fields, drill-down and roll-up the cube views of data, and slice and dice to see different aspects of the analysis at vary-ing granularity, even drilling down to the level of actual text comments. A typical report could compare label dis-tribution for under-performing agents of two teams over a few weeks, aggregated by satisfaction ratings. The agents can then be trained better depending on their assigned call-driver distribution to, say improve accent after undergoing training in voice-based helpdesks or be better at personal-ization while replying to emails. We believe a BI reporting goes a long way in providing interpretability of results in deployed text mining systems.

The need for intervention and interaction with the system is two-fold. First, the system should assist domain experts in designing label-sets and building up training data sets for classification, and second, to provide the trust factor re-quired in manual review of the quality of label assignments at any stage in deployment. The TICL package we devel-oped has a stripped down version which is freely available online 3 . It is an interactive interface to train, validate, cor-rect, and refine the classification process continuously. It aims to enable end users to start building text classification systems without knowing statistical or rule-based text clas-sification. It attempts to bridge the gap between manual and automatic classification approaches combining the tun-ability of the former with the scalability of the latter.
The TICL version available online can be used in stand-alone mode to create label-sets and training data using sim-ple text configuration files. The user interface of TICL it-self is very simple as shown and generates an HTML form listing sets of comments and their predicted labels. These can be accepted as they are or modified by experts and fed back, either just into the database for corrected reporting, or added as new training data to re-train classifiers. The functionality in both cases is on top of a learned model that predicts/assigns labels with some confidence to a set of unla-beled documents and presents them to the expert for valida-tion. TICL also helps in updating statistical models based on user feedback. It can be used to feed revised perceptions of meanings of labels back into the system over time; since corrected document labels can be treated as new training data, the classifier can be re-trained with new data. http://www.alphaworks.ibm.com/tech/ticl grated invocation of TICL for review and re-labeling of documents
We now see how the architectural components fit together in the light of the challenges described earlier. A text analyt-ics solution based on classification in the services industry setting encounters problems at the outset with labels and labeling. We described our development and deployment ef-forts around TICL to tackle some of these problems. The next operational challenge is typically presented around ac-curacy and we described human satisfaction metrics for mea-surement, and rule-based systems for improving minority classes. Another operational business requirement is that of interpretable reporting; we presented a Alphablox BI com-ponent for reporting for consumption at various levels of the people hierarchy. We described other usage modes of TICL which enabled human review of data at any point in the lifetime of the system. Our proposed system presents a first of its kind integrated C-Sat analysis tool compris-ing an end-to-end system based on text classification. It addresses all aspects of text classification systems starting from helping domain experts generate training data to hu-man review of classification results via an interactive inter-face. The proposed system offers a configurable combination of different classifiers with state-of-the-art performance. We are not aware of similar existing solutions in the services industry setting.
The demonstration will show all the architectural com-ponents of the ITACS system. These components will be shown interacting with each other. Starting with sample customer comments for a Telecom client X  X  contact center, the demo will guide visitors through classification using sta-tistical and rule-based classifiers. Results of classification can then be visualized at various levels of granularity in the Business Intelligence tool IBM DB2 Alphablox. This BI tool performs typical cubing operations like roll-up and drill-down over the underlying star schema. Visitors will be able to see various operational analytics along interesting dimensions like agents, labels, numeric scores, and trending on datelines. Some sample insights that can be drawn from the data will also be shown.

TICL is tightly integrated with ITACS and from any drill down operation (double-click) in any Alphablox grid or graph report, TICL can be invoked. Users can inspect actual comments and their labeling, even correct them and add re-labeled comments back for generating corrected reports. TICL can be shown to be useful as a stand-alone text la-beling package for active learning [1]. Figure 3 shows some screenshots and the above flow of the demonstration.
The demonstration will be completely contained on the presenter X  X  laptop. It will contain all required software in-stalled like DB2, Alphablox, TICL, and JDK. No network connectivity is needed because the sample databases and web server will reside locally.
We have described the ITACS system starting from its real world setting and researc h challenges to its architec-ture and a walkthrough of the proposed demonstration. In terms of experience, modeling and automating C-Sat anal-ysis proved to be more than designing a text classification system as the concept and meaning of C-Sat varies across business scenarios. ITACS has been successfully deployed for C-Sat analysis in client accounts in large contact centers with 85%  X  95% accuracy levels.
