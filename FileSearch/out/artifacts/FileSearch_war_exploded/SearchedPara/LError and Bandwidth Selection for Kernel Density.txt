 Kernel density estimates are a robust way to reconstruct a continuous distribution from a discrete point set. Typically their effectiveness is measured either in L 1 or L 2 error. In this paper we investigate the challenges in using L worst case) error, a stronger measure than L 1 or L present efficient solutions to two linked challenges: how to evaluate the L  X  error between two kernel density estimates and how to choose the bandwidth parameter for a kernel density estimate built on a subsample of a large data set.
Kernel density estimates ( kde s) are essential tools [33, 31, 11, 12] for understanding a continuous distribution rep-resented by a finite set of points. For instance, used in data mining amid uncertainty to provide an effec-tive intermediate representation, which captures informa-tion about the noise in the underlying data [2]. They are also used in classification problems by constructing the class of conditional probability density functions that are used in a Bayesian classifier [25]. They have many applications in other areas, such as network outlier detection [8], human motion tracking [6], financial data modeling [3] and geomet-ric inference [27].

Given a point set P  X  R d and a kernel K  X  : R d  X  R d  X  R with bandwidth parameter  X  , for any point x  X  R d , a kernel density estimate is defined as kde P ( x ) = 1 | P | P p  X  P We focus on symmetric, shift-invariant kernels which de-pend only on z = k p  X  x k and  X  , then a kernel can be writ-kde P ( x ) smoothes the effect of each p  X  P for the evalu-ation point x . For d = 1 this object can be used in place of an equi-width histogram; it removes the choice of how to shift the boundary of bins and thus kde s are more robust. Moreover, they generalize naturally to higher dimensions. Thanks to NSF Awards 1350888, 1251019, and 1443046. c  X 
The brute force solution of evaluating a kernel density estimate requires O ( | P | ) time, and is thus untenable as a data structure for large data sets. And a lot research has gone towards speeding up these queries [7, 37, 9, 40]. One of the techniques [40] is to produce a coreset representation Q of the data which can be used as proxy for the true data P while guaranteeing approximation error. The size of Q depends only on the required error, not on any properties of P ; these go beyond just randomly sampling Q from P . Written concretely, given P , and some error parameter  X  &gt; 0, the goal is to construct a point set Q to ensure
L  X  ( P,Q ) = err ( P,Q ) = max or written err ( P, X ,Q, X  ) if the bandwidths  X  and  X  for kde and kde Q, X  are under consideration. This line of work shows that an L  X  error measure, compared with L 1 or L 2 error, is a more natural way to assess various properties about ker-nel density estimates. This work (like other work [7, 37, 9]) assumes  X  is given, and then implicitly also assumes  X  =  X  . In this paper, we will investigate choosing a bandwidth  X  for kde Q under L  X  error given P, X ,Q .

Thus, we empirically study two concrete problems: 1. Given two point sets P,Q  X  R d and a kernel K , esti-2. Given two point sets P,Q  X  R d , a kernel K , and a
It should be apparent that the first problem is a key sub-problem for the second, but it is also quite interesting in its own right. We will observe that L  X  is a strictly stronger measure than L 1 or L 2 , yet can still be assessed. To the best of our knowledge, we provide the first rigorous empir-ical study of how to measure this L  X  error in practice in an efficient way, following theoretical investigations demon-strating it should be possible.

Bandwidth parameter is hugely important in the result-ing kde , and hence, there have been a plethora of proposed approaches [33, 31, 11, 12, 24, 34, 17, 28, 4, 32, 18, 29, 14, 30, 21, 36, 16, 23, 39, 10, 20, 19] to somehow automatically choose the  X  X orrect X  value. These typically attempt to min-imize the L 2 [33, 31] or L 1 error [11, 12] (or less commonly other error measures [24]) between kde P and some unknown distribution  X  that it is assumed P is randomly drawn from. Perhaps unsurprisingly, for such an abstract problem differ-ent methods produce wildly different results. In practice, many practitioners choose a bandwidth value in a ad-hoc manner through visual inspection and domain knowledge.
In this paper we argue that the choice of bandwidth should not be completely uniquely selected. Rather this value pro-vides a choice of scale at which the data is inspected, and for some data sets there can be more than one correct choice depending on the goal. We demonstrate this on real and synthetic data in 1 and 2 dimensions. As an intuitive 1-dimensional example, given temperature data collected from a weather station, there are very obvious modal trends at the scale of 1 day and at the scale of 1 year, and depending at which phenomenon one wishes to study, the bandwidth parameter should be chosen along the corresponding scale, so it is totally reasonable if we assume  X  for kde P is given.
Via examinations of problem (2), we observe that in some cases (but not all), given P,Q , and  X  , we can choose a new bandwidth  X  (with  X  &gt;  X  ) so that err ( P, X ,Q, X  ) is signif-icantly smaller than the default err ( P, X ,Q, X  ). This corre-sponds with fine-grained phenomenon disappearing with less data (as | Q | &lt; | P | ), and has been prognosticated by theory about L 2 [33] or L 1 [11] error where the optimal bandwidth for kde Q is a strictly shrinking function of | Q | . Yet, we urge more caution than this existing bandwidth theory indicates since we only observe this phenomenon in specific data sets with features present at different scales (like the daily/yearly temperature data example in Section 2.3).
 Organization. Section 2 formalizes and further motivates the problem. Section 3 addresses problem (1), and Section 4 problem (2). Then Section 5 describes detailed experimental validations of our proposed approaches. Finally, Section 6 provides some concluding thoughts.
In addition to the symmetric, shift-invariant properties of the kernels, it is convenient to enforce one of two other properties. A normalized kernel satisfies so that the kernel and the kernel density estimate are prob-ability distributions. A unit kernel satisfies which ensures that kde P ( x )  X  1. Unlike with the normal-ized kernel, the changing of bandwidth does not affect the coefficient of kernel function, so K  X  ( p,x ) = k ( k p  X  x k / X  ). Although this paper focuses on the Gaussian kernel K commonly used kernel, there are many other symmetric, shift invariant kernels such as sphere. These are shown as normalized kernels, to make them unit kernels, the coefficient is simply set to 1.
Unit kernels are more natural to estimate the L  X  errors of kernel density estimates [26, 38] since the range of values are in [0 , 1]. For normalized kernels as  X  varies, the only bound in the range is [0 ,  X  ).

Moreover, unit kernels, under a special case, correspond to the total variation distance of probability measures. In probability theory, the total variation distance for two prob-ability measures P and Q on a sigma-algebra F of subsets of sample space  X  is defined as: Terms P ( A ), resp. Q ( A ), refer to the probability restricted to subset A . If we use F as the set of all balls of radius  X  , so A is one such ball, then P ( A ) is the fraction of points of P falling in A . Hence P ( A ) can be viewed as the kde P, X  under the ball kernel, where x is the center of ball A . When Q is the coreset of P , then Q ( A ) is the fraction of points of Q falling in A , so it can be viewed as the kde Q, X  ( x ) under the ball kernel. In this sense, the total variance distance is the L  X  error, specifically err ( P,Q ) where K is the ball kernel. The total variation distance also maps to other unit kernels if F can admit weighted subsets, not just subsets.

However, normalized kernels are more useful in bandwidth selection. In this case, there is a finite value for  X   X  (0 ,  X  ) which minimizes the L 1 or L 2 error between kde P, X  and kde Q, X  , whereas for unit kernels this is minimized for  X   X  0.
But recall that unit and normalized kernels are only differ-ent in the scaling coefficient, so given one setting it is simple to convert to the other without changing the bandwidth. Hence we use both types of kernels in different scenarios: unit kernels for choosing the coresets, and normalized ker-nel for problem (1) and problem (2).
In the big data era, we are creating and accessing vastly more data than ever before. For example, mobile phones are consistently (implicitly) generating positional data along with various aspects of meta data including call duration and quality. To analyze or monitor the quality of signals or demand for this connections, we rarely need the entire data set, just an approximate version of it. A coreset can provide such a summary with accuracy guarantees, and by virtue of smaller size much more efficient and affordable access to it.
More formally, a coreset of a point set P is a subset Q such that (1) one can perform a family of queries on Q in-stead of P and the returned results are guaranteed to have bounded errors, and (2) the size of Q is much smaller than P , often independent of the size of P and only depends on the guaranteed error on the queries. For this paper, we consider coresets which preserve properties about the ker-nel density estimate, namely that for any query point x that | kde Q ( x )  X  kde P ( x ) |  X   X  for some error parameter  X  &gt; 0. The study of the worst case error was initiated by Phillips [26], and similar results under the L 2 error have been studied by Chen et al. [9] using an approach called kernel herding . Zheng et.al. [40] empirically improved these approaches to finding such a coreset in one and two dimen-sions, using methods based on random sampling, iteratively matching and halving of the data set, and Z-order curves. For instance, the experiments in [40] show that in two di-mension, a coreset of 10,000 points can be constructed in less than 5 seconds from a 160 million record data set with approximation  X  = 0 . 01.
Recall that problem (2) takes as given two point sets P and Q as well as a bandwidth  X  associated with P , and then tries to find the best bandwidth  X  for Q so that kde P, X  is close to kde Q, X  . This is different from how the  X  X andwidth selection problem X  is typically posed [11, 33]: a single point set Q is given with no bandwidth, and it is assumed that Q is drawn randomly from an unknown distribution.

We break from this formulation for two reasons. First, we often consider the point set Q chosen as a coreset from P , and this may not be randomly from P , as more intricate techniques [40] can obtain the same error with much smaller size sets Q . These non-random samples break most modeling assumptions essential to the existing techniques.

Second, the choice of bandwidth may vary largely within the same data set, and these varied choices may each high-light a different aspect of the data. As an extended example consider temperature data (here we treat a reading of 50 degrees as 50 data points at that time) from a MesoWest weather station KSLC read every hour in all of 2012. This results in 8760 total readings, illustrated in Figure 1. For three bandwidth values of 3, 72, and 1440, kde s are shown to represent daily, weekly, and yearly trends. All are useful rep-resentations of the data; there is no  X  X ne right bandwidth. X  Section 5 shows a 2-dimensional example of population den-sities where similarly there are several distinct reasonable choices of bandwidths.
 Figure 1: KDEs with different bandwidths showing daily, weekly and yearly temperature trends. Left shows the full year data, and right shows the one week data.
As mentioned the most common measures for comparing kde s are the L 1 or L 2 error, defined for p = { 1 , 2 } as Although this integral can be reasoned about, it is difficult to estimate precisely. Rather many techniques only evaluate at the points P and simply calculate These average over the domain or P ; hence if | kde P ( x )  X  kde Q ( x ) | X   X  for all x , then L p ( P,Q ) is also at most  X  . That  X  X or all X  bound is precisely what is guaranteed by L  X  ( P,Q ), hence it is a stronger bound.

Another reason to study L  X  error is that it preserves the worst case error. This is particularly important when kde P ( x ) values above a threshold trigger an alarm. For in-stance in tracking densities of tweets, too much activity in one location may indicate some event worth investigating. L 1 or L 2 error from a baseline may be small, but still have high error in one location either triggering a false alarm, or missing a real event.
There is a vast literature on bandwidth selection under the L 1 [11, 12] or L 2 [33, 31] metric. In these settings Q is drawn, often at random, from an unknown continuous distribution  X  (but  X  can be evaluated at any single point x ). Then the goal is to choose  X  to minimize k  X   X  kde Q, X  k { 1 , 2 } This can be conceptualized in two steps as k  X   X  kde  X , X  k and k kde  X , X   X  kde Q, X  k . The first step is minimized as  X   X  0 and the second step as  X   X   X  . Together, there is a value  X  { 1 , 2 }  X  (0 ,  X  ) that minimizes the overall objective.
The most common error measure for  X  under L 2 are In-tegrated Squared Errors(ISE) ISE (  X  ) = R x  X  R d ( kde  X  ) 2 dx and its expected value, the Mean Integrated Squared Error (MISE) MISE (  X  ) = E Q  X   X  [ R x  X  R d ( kde Q, X  As MISE is not mathematically tractable, often approxi-mations such as the Asymptotic Mean Integrated Squared Error (AMISE) or others [33, 34] are used. Cross-validation techniques [17, 28, 4, 32, 29, 14] are used to evaluate various parameters in these approximations. Alternatively, plug-in methods [30, 21, 36] recursively build approximations to  X  using kde Q, X  i , and then refine the estimate of  X  kde Q, X  i . Bayesian approaches [5, 16, 23, 39, 10, 20] build on these models and select  X  using MCMC approaches.
An alternative to these L 2 approaches is using an L measure, like integrated absolute error (IAE) of kde Q, X  is IAE (  X  ) = R x  X  R d | kde Q, X   X   X  | dx , which has simple inter-pretation of being the area between the two functions. De-vroye and Gy  X  o rfi [11] describe several robustness advantages (better tail behavior, transformation invariance) to these ap-proaches. Several of the approximation approaches from L can be extended to L 1 [19].

Perplexingly, however, the bandwidths generated by these methods can vary quite drastically! In this paper, we as-sume that some bandwidth is given to indicate the intended scale, and then we choose a bandwidth for a sparser point set. Hence the methods surveyed above are not directly comparable to our proposed approaches. We include the ex-periment results from some of the above methods to show that different approaches give quite different  X  X ptimal X  band-width, which in another way shows us there are more than one correct bandwidth for some data sets.
The goal of this section is to calculate For notational convenience let G ( x ) = | kde P ( x )  X  kde We focus on the case where the kernel K is a unit Gaussian. Since even calculating max x  X  R d kde P ( x ) (which is a special case of err ( P,Q ) where Q is empty) appears hard, and only constant factor approximations are known [1, 27], we will not calculate err ( P,Q ) exactly. Unfortunately these approx-imation techniques [1, 27] for max x  X  R d kde P ( x ) do not eas-ily extend to estimating err ( P,Q ). They can focus on dense areas of P , since the maximum must occur there, but in err ( P,Q ), these dense areas may perfectly cancel out. These approaches are also much more involved than the strategies we will explore.
Towards estimating err ( P,Q ), which is optimized over all of
R d , our strategy is to generate a finite set X  X  R d , and then return err X ( P,Q ) = max x  X  X G ( x ). Our goal in the generation of X is so that in practice our returned estimate
X ( P,Q ) is close to err ( P,Q ), but also so that under this process as | X |  X   X  then formally err X ( P,Q )  X  err ( P,Q ). We say such a process converges .
 We formalize this in two steps. First we show G ( x ) is Lipschitz-continuous, hence a point  X  x  X  R d close to the point x  X  = arg max given this fact, we show that our strategy will, for any radius r , as | X | X  X  X  generate a point  X  x  X  X so that k x  X   X   X  x k X  r . This will be aided by the following structural theorem on the location of x  X  , with proofs in 1 and 2 dimensions deferred to Appendix A. ( M is illustrated in Figure 2.)
Theorem 1. For K  X  a unit Gaussian kernel, and two point sets P,Q  X  R d , then x  X  = arg max x  X  R d G ( x ) must be in M , the Minkowski sum of a ball of radius  X  and the convex hull of P  X  Q . Figure 2: Illustration of the Minkowski sum of a ball of radius  X  and convex hull of P  X  Q .

We will not focus on proving theoretical bounds on the rate of convergence of these processes since they are quite data dependent, but will thoroughly empirically explore this rate in Section 5. As | X | grows, the max error value in X will consistently approach some error value (the same value for several provably converging approaches), and we can then have some confidence that as these processes plateau, they have successfully estimated err ( P,Q ). Our best pro-cess WCen6 converges quickly (e.g. | X | = 100); it is likely that the maximum error is approximately achieved in many locations.
 Now as a basis for formalizing these results we first show G ( x ) is Lipschitz continuous. Recall a function f : R is Lipschitz continuous if there exists some constant  X  such that for any two points x,y  X  R d that | f ( x )  X  f ( y ) | / k x  X  y k  X   X  . This result follows from the Gaussian kernel (as well as all other kernels mentioned in Section 1 except the Ball kernel) also being Lipschitz continuous. Then since the function f ( x ) = kde P ( x )  X  kde Q ( x ) is a finite weighted sum of Gaussian kernels, each of which is Lipschitz continuous, so is f ( x ). Since taking absolute value does not affect Lipschitz continuity, the claim holds.
We now consider strategies to generate a set of points X so that err X ( P,Q ) is close to err ( P,Q ). Recall that M , the Minkowski sum of a ball of radius  X  with the convex hull of P  X  Q must contain the point x  X  which results in err ( P,Q ). In practice, it is typically easier to use B , the smallest axis-aligned bounding box that contains M . For discussion we assume Q  X  P so P = P  X  Q .
 Rand: Choose each point uniformly at random from B .
Since x  X   X  M  X  B , eventually some point x  X  X will be close enough to x  X  , and this process converges.
 Orgp: Choose points uniformly at random from P .

This process does not converge since the maximum error point may not be in P . Yet Section 5 shows that this process converges to its limit very quickly. So many of the following proposed approaches will attempt to adapt this approach while still converging.
 Orgp+N: Choose points randomly from the original point set P then add Gaussian noise with bandwidth  X  , where  X  is the bandwidth of K .

Since the Gaussian has infinite support, points in X can be anywhere in R d , and will eventually become close enough to x  X  . So this process converges.
 Grid: Place a uniform grid on B (we assume each grid cell is a square) and choose one point in each grid. For example in 2 dimension, if four evaluation points are needed, the grid would be 2  X  2 and if nine points are needed, it would be 3  X  3. So with this method, the number of evaluation points is a non-prime integer.

Since x  X   X  B , and eventually the grid cell radius is arbi-trarily small, then some point x  X  X is close enough to x Thus this process converges.
 Cen { E[m] } : Randomly select one point p 1 from the original point set P and randomly choose m neighbor points of p within the distance of 3  X  . m is chosen through a Exponen-tial process with rate 1 /E [ m ] . Then we use the centroid of the selected neighbor points as the evaluation point. This method is inspired by [15], which demonstrates interesting maximums of kde s at the centroids of the data points.
Since P is fixed, the centroid of any combination of points in P is also finite, and the set of these centroids may not include x  X  . So this process does not converge. We next modify it in a way so it does converge.
 WCen { E[m] } : Randomly select one point p 1 from the original point set P and select the neighbor point p n  X  P as candi-date neighbor proportional to exp(  X  || p n  X  p 1 || 2 the bandwidth for K . The smaller the distance between p and p 1 , the higher probability it will be the chosen. Repeat to choose m total points including p 1 , where again m is from an Exponential process with rate 1 /E [ m ] . Now refine the m neighbor points so with probability 0 . 9 , it remains the origi-nal point p n  X  P , with the remaining probability it is chosen randomly from a ball of radius  X  centered at p n . Next, we assign each point a random weight in [0 , 1] so that all weights add to 1 . Then finally the evaluation point is the weighted centroid of these points.

This method retains much of the effectiveness of Cen, but does converge. Without the 0 . 1 probability rule of being in a ball of radius  X  around each point, this method can generate any points within the convex hull of P . That 0 . 1 probability allows it to expand to M , the Minkowski sum of the convex hull of P with a ball of radius  X  . Since x  X   X  M , by Theorem 1, this process converges. Figure 3: The example of necessary of larger band-width for coreset Q . The radius of the circle repre-sents the chosen bandwidth.
 Comb: Rand + Orgp: The combination of method Rand and Orgp, of which 20% points generated from B and 80% points generated from original points.

The 20% of points from Rand guarantees convergence, but retain most empirical properties of Orgp. This was used before with little discussion [40].

Section 5 describes extensive experiments on both syn-thetic and real data to evaluate these methods. The weighted centroid method WCen { E[m] } with large parameter (e.g. E [ m ] = 6) works very well for 1 and 2 dimensions, and also converges, so in general this technique is recommended. Al-though in some situations, it does not perform significantly better than other approaches like Rand+Orgp, which are simpler and also converge, so those may be a good option. In this section we consider being given two point sets P,Q  X  R 2 , a kernel K , and a bandwidth  X  associated with P . We consider K as a normalized Gaussian kernel, and where Q is a coreset of P . The goal is to find another band-width  X  to associate with Q so that err ( P, X ,Q, X  ) is small.
In [40], coresets are constructed assuming that kde Q uses the same bandwidth  X  as kde P . Can we improve this rela-tionship by using a different bandwidth  X  for Q ? The theory for L 1 or L 2 error (assuming Q is a random sample from P ) dictates that as | Q | decreases, the bandwidth  X  should in-crease. This intuition holds under any error measure since with fewer data points, the kde should have less resolution. It also matches the L  X  theoretical error bounds described previously [26].

We first reinforce this with a simple 2-dimensional exam-ple. Consider point set P =  X  X  P 1 ,P 2 ,P 3 ,P 4 } in Figure 3(a), the radius of the circle represents the bandwidth  X  for P . Figure 3(b) gives the coreset Q of P : Q =  X  X  Q 1 ,Q 2 ,Q each Q i contains only one black point. Now suppose our evaluation point is point e . If we use the original bandwidth  X  , kde Q, X  ( e ) = 0 with ball kernel, but if we use  X  , which is the radius of larger circle centered at e , then kde Q, X  ( e ) &gt; 0, so the error is decreased. But, we don X  X  want  X  too large, as it would reach the points in other Q i , which is not the case for  X  in P , so the error would be increased again. Thus there seems to be a good choice for  X  &gt;  X  .

But the situation of finding the  X  opt that minimizes h (  X  ) = err ( P, X ,Q, X  ) is more complicated. For each  X  , err ( P, X ,Q, X  ) is a maximization over x  X  R d . There may in fact be more than one local minimum for  X  in h (  X  ).

However, equipped with the WCen6 procedure to evaluate err ( P,Q ), we propose a relatively simple optimization algo-rithm. We can perform a golden section search over  X  , using WCen6 to obtain a set X and evaluate err X ( P, X ,Q, X  ). Such a search procedure requires a convex function for any sort of guarantees, and this property may not hold. However, we show next that h (  X  ) has some restricted Lipschitz property, so that with random restarts it should be able to find reason-able local minimum. This is illustrated in Figure 4, where the curve that is Lipschitz either has a large, relatively con-vex region around the global minimum, or has shallow local minimums. The other curve without a Lipschitz property has a very small convex region around the global minimum, and any search procedure will have a hard time finding it. Figure 4: Two curves, dark one is Lipschitz, dashed curve is not.
In general, however, h (  X  ) is not Lipschitz in  X  . But, we can show it is Lipschitz over a restricted domain, specifically when  X   X   X   X  1 /A for some absolute constant A . Define y (  X ,a ) = 1 2  X  X  2 exp(  X  a 2 / (2  X  2 )).

Lemma 1. For any  X   X   X   X  1 /A , y (  X ,a ) is  X  -Lipschitz with respect to  X  , with  X  = | a 2  X  1 / X  | A 3 .

Proof. By taking the first derivative of y (  X  ), we have And thus So the absolute value of largest slope of function y (  X ,a ) is  X  = | a 2  X  1 / X  | A 3 , thus y (  X ,a ) is  X  -Lipschitz continuous on  X  .

Theorem 2. For any  X   X   X   X  1 /A , h (  X  ) is  X  -Lipschitz with respect to  X  , for  X  = 1 | Q | P q  X  Q | ( x  X   X  q ) where x  X  = arg max x  X  R 2 | kde P, X  ( x )  X  kde Q, X  ( x ) | .
Proof. If kde P, X  ( x  X  )  X  kde Q, X  ( x  X  ) then h (  X  ) = | kde P, X  ( x  X  )  X  kde Q, X  ( x  X  ) | Since h (  X  ) is linear combination of | Q | functions of y (  X ,a ) plus a constant and y (  X ,a ) is Lipschitz continuous, based on the Lemma 1, h (  X  ) is Lipschitz continuous on  X  . We can get the same result if kde P, X  ( x  X  )  X  kde Q, X  ( x  X  directions, the first derivative of the function is bounded, so h (  X  ) is bounded.
From the above properties, we design a search procedure that will be effective in finding the bandwidth  X  minimizing err ( P, X ,Q, X  ). The random golden section search is based on the golden section search [22], a technique to find ex-tremum in a strictly unimodal function. To find a minimum, it successively narrows a range [ `,r ] with known function val-ues h ( ` ), h ( m 1 ), h ( m 2 ), and h ( r ) with ` &lt; m h ( m 1 ) &lt; h ( m 2 ) the new search range is [ `,m 2 ] and other-wise it is [ m 1 ,r ]. In either case a new fourth point is chosen according to the golden ratio in such a way that the interval shrinks by a constant factor on each step.

However, h (  X  ) in our case can be a multi-modal function, thus golden section search is not guaranteed to work. We apply random restarts as follows. Starting with a range [ ` =  X ,r = 10  X  ] we choose one middle point at m =  X  X  for  X   X  Unif (1 , 10). If h ( m ) &gt; h ( r ) we increase r by a factor 10 until it is (e.g. r = 100  X  ). Then the second middle point is chosen using the golden ratio, and the search is run deterministically. We repeat with several random values  X  .
Here we run an extensive set of experiments to validate our techniques. We compare kde P where P is in 1 and 2 dimensions with kernel density estimate under smaller core-set kde Q for both synthetic and real data. To show our methods work well in large data sets, we use the large syn-thetic data set(0.5 million) and real data set(1 million) in 2 dimension.
We consider data sets that have different features at var-ious scales, so that as more data is present using a smaller bandwidth more fine-grain features are brought out, and a larger bandwidth only shows the coarse features. Our real data set in 1 dimension is the temperature data in Figure 1, with default  X  = 72 (3 days). We use parameter  X  = 0 . 02 to generate a coreset Q with the Sort-selection technique [40].
We can also simulate data with multi-scale features. On a domain [0 , 1] we generate P recursively, starting with p and p 2 = 1. Next we consider the interval between [ p 1 and insert two points at p 3 = 2 / 5 and p 4 = 3 / 5. There are now 3 intervals [ p 1 ,p 3 ], [ p 3 ,p 4 ], and [ p interval [ p i ,p j ] we recursively insert 2 new points at p (2 / 5)  X  ( p j  X  p i ) and at p i + (3 / 5)  X  ( p j  X  p i The kde of this data set with  X  = 0 . 01 is shown in Figure 5(d), along with that of a coreset Q of size | Q | = 100.
We construct the 2-dimensional synthetic data set in a similar way. The data is in [0 , 1] 2 starting with four points p 1 = (0 , 0) ,p 2 = (0 , 1) ,p 3 = (1 , 0) ,p 4 = (1 , 1). We recurse on this rectangle by adding 4 new point in the middle m : the x -coordinates are either at the 0 . 5-quantile or 0 . 8-quantile of the x -coordinates, and same for new y -coordinates. These 4 new points creates 9 smaller empty rectangles. We further recurse on each of these rectangles until | P | = 532900. The kde P with  X  = 0 . 01 is shown in Figure 10(a). We use Grid matching [40] to generate a coreset Q with  X  = 0 . 1 and size | Q | = 1040. Under the original bandwidth  X  , the kde Q shown in Figure 10(b); due to a small bandwidth this kde has many more modes than the original, which motivates the larger bandwidth kde shown in Figure 10(c).

For real data with multiple scales in 2 dimension we con-sider OpenStreetMap data from the state of Iowa. Specifi-cally, we use the longitude and latitude of all highway data points, then rescale so it lies in [0 , 1]  X  [0 , 1]. It was recog-nized in the early 1900s [35] that agricultural populations, such as Iowa, exhibited population densities at several scales. In experiment, we use the original data of size | P | = 1155102 with  X  = 0 . 01, and Q as a smaller coreset with  X  = 0 . 1 and | Q | = 1128. These are illustrated in Figure 11.
To find the best evaluation point generation techniques, we compare the various ways to generate a set X to evaluate
X ( P,Q ). The larger numbers are better, so we want to find point sets X so that err X ( P,Q ) is maximized with | X | small. As most of our methods are random, five evaluation point sets are generated for each method and the average X ( P,Q ) is considered.

We start in 1 dimension, and investigate which parame-ter of the Cen and WCen methods work best. We will then compare the best in class against the remaining approaches. Recall the parameter E [ m ] determines the expected number of points (under a Exponential process) chosen to take the centroid or weighted centroid of, respectively. We only show the test result with E [ m ] from 2 to 7, since the results are similar when E [ m ] is larger than 7, and the larger the param-eter the slower (and less desirable) the process. The results are plotted in Figure 5 on the 1-dimensional synthetic data. Specifically, Figure 5(a) shows the Cen method and Figure 5(b) the WCen method. Both methods plateau, for some pa-rameter setting, after around | X | = 100, with WCen more robust to parameter choice. In particular WCen converges Figure 5: Choosing best evaluating point generation techniques for 1 -dimensional synthetic data. Figure 6: Choosing best evaluating point generation techniques for 1 -dimensional real data. slightly faster but with not much pattern across the choice of parameter. We use Cen6 and WCen6 as representatives. We next compare these approaches directly against each other as well as Rand, Orgp, Orgp+N, Grid, and Comb in Figure 5(c). WCen6 appears the best in this experiment, but it has been selected as best WCen technique from random trials. The Rand and Grid techniques which also converge perform well, and are simpler to implement.
 Similar results are seen on the real 1-dimensional data in Figure 6. We can take best in class from Cen and WCen parameter choices, shown as Cen6 and WCen6 in Figure 6(a) and Figure 6(b). These perform well and similar to the simpler Rand, Grid, and Orgp in Figure 6(c). Since Rand and Grid also converge, in 1 dimension we would recommend one of these simple methods.

For 2-dimensional data, the techniques perform a bit dif-ferently. We again start with Cen and WCen methods as shown in Figure 7 on real and synthetic data. The con-vergence results are not as good as in 1 dimension, as ex-pected, and it takes roughly | X | = 10000 points to converge. All methods perform roughly the same for various parame-ter settings, so we use Cen6 and WCen6 as representatives. Comparing against all techniques in Figure 7(e), most tech-niques perform roughly the same relative to each other, and again WCen6 appears to be a good choice to use. The no-table exceptions is that Grid and Rand perform worse in 2-d than in 1-d; likely indicating that the data dependent approaches are more important in this setting.
We now apply a random golden section search to find new bandwidth values for coresets on 1-dimensional and 2-dimensional synthetic and real data. In all 10 random trials we always find the same local minimum, and report this value. We will see that a value  X  &gt;  X  can often give better error results, both visually and empirically, by smoothing out the noise from the smaller coresets.

Figure 8 shows evaluation of err X ( P, X ,Q, X  ) for various  X  values chosen while running the random golden section search on synthetic and real 1-dimensional data. In both (c) Weighted centroid meth (d) Weighted centroid meth Figure 7: Choosing the best evaluation set X for 2 -dimensional synthetic (left) and real (right) data. cases, setting  X  =  X  (as  X  = 0 . 01 and  X  = 72, respectively) gives roughly twice as much error as using an omega roughly twice as large (  X  = 0 . 017 and  X  = 142, respectively).
We can see the same results in 2-dimensional data sets in Figure 9. We observe in Figure 9(a) on synthetic data that with the original  X  =  X  = 0 . 01 the error is roughly 3 . 6, but by choosing  X  = 0 . 013 that we can reduce the error to roughly 2 . 7. This is also shown visually in Figure 10, where a small coreset Q is chosen to do kde Q, X  (Figure 10(b)) and the large-scale pattern in kde P, X  is replaced by many isolated points; kde Q, X  =0 . 013 (Figure 10(c)) increases the bandwidth and the desired visual pattern re-emerges. On real data, a similar pattern is seen in Figure 9(b). The original  X  =  X  = 0 . 01 has error roughly 3 . 0, and an  X  = 0 . 024 (more than 2 times larger) gives error about 1 . 1. This extra smoothing is illustrated in Figure 11.

Thus we see that it is indeed useful to increase the band-width of kernel density estimates on a coreset, even though theoretical bounds already hold for using the same band-width. We show that doing so can decrease the error by Figure 10: Visualization of KDE P and KDE Q for 2 -dimensional synthetic data using different band-width. Figure 11: Visualization of KDE P and KDE Q for 2 -dimensional real data using different bandwidth. a factor of 2 or more. Since we consider  X  =  X  , and only decrease the error in the process, we can claim the same the-oretical bounds for the new  X  value. It is an open question of whether one can prove tighter coreset bounds by adapting the bandwidth value.
The above bandwidth selection method can be extended to minimizing the L 1 and L 2 errors. Differing from L  X  error, the L 1 and L 2 errors do not require finding a witness point of large error, but rather are the averaged over a region or, more commonly, the input points P . Figure 12 shows the L , L 2 , and L  X  errors for 2-dimensional synthetic and real data; other settings gave similar results. The results show that minimizing L  X  does not give significantly worse errors than minimizing L 1 or L 2 in our setting. For example, in Figure 12(a), we see that the choice of  X  = 0 . 013 minimizes L  X  errors,  X  = 0 . 014 gave a minimum L 2 error of 0 . 608 and  X  = 0 . 016 minimizes L 1 error of 0 . 450. Comparing instead to  X  = 0 . 013 which provided the minimum L  X  error, then we get L 2 error of 0 . 618 and L 1 error of 0 . 476; both are within 1% of the minimum solutions.
We compare against some traditional bandwidth selection methods for the 2-dimensional synthetic and real data using. Figure 12: Relations of L 1 , L 2 and L  X  error and  X  We consider the following exemplars, among those surveyed in Section 2.5: biased cross-validation (BCV), least-squares cross-validation (LSCV), plug-in (PI), and smoothed cross-validation (SCV). We use the kernel smoothing R package ( ks ), which was originally introduced by Duong in 2007 [13] and improved in 2014. In the experiment, our data set is normalized and we assume data in each dimension is inde-pendent and share the same bandwidth; so we use the largest value from the main diagonal of bandwidth matrix computed from the R package. For the 2-dimensional synthetic data set, we use the same coreset with | Q | = 1040. The four exemplar methods, respectively, resulted in the following bandwidths  X  BCV = 0 . 0085,  X  LSCV = 0 . 024,  X  PI = 0 . 0036, and  X  SCV = 0 . 0043. For the 2-dimensional real data set, with the coreset | Q | = 1128, the bandwidth chosen by the four exemplar methods, respectively, are  X  BCV = 0 . 0078,  X 
LSCV = 0 . 0003,  X  PI = 0 . 0029,  X  SCV = 0 . 004. The corre-sponding error trends compared to our method for these two data sets are shown in Figure 13, where  X  OPT denotes the optimal bandwidth from our method. Both of these figures show our method achieves the smallest error compared, and sometimes it is much (a factor of 20) smaller.
This paper considers evaluating kernel density estimates under L  X  error, and how to use these criteria to select the bandwidth of a coreset. The L  X  error is stronger than the more traditional L 1 or L 2 error, it provides approximation guarantees for all points in the domain, and it aligns with recent theoretical results [26] of kernel range space. Thus it is worth rigorously investigating, and this paper presents the first such study.

We propose several methods to efficiently evaluate the L  X  error between two kernel density estimates and provide a convergence guarantee. The method Grid works well, and is very simple to implement in R 1 . In R 2 , methods that adapt more to the data perform much better, and our technique WCen is shown accurate and efficient on real and synthetic data. We then use these technique to select a new bandwidth value for coresets which can improve the error by a factor of 2 to 3. We demonstrate this both visually and empirically on real and synthetic large data sets. [1] P. K. Agarwal, S. Har-Peled, H. Kaplan, and [2] C. C. Aggarwal. On density based transforms for [3] T. Bouezmarni and J. V. Rombouts. Nonparametric [4] A. W. Bowman. An alternative method of [5] M. J. Brewer. A bayesian model for local smoothing in [6] T. Brox, B. Rosenhahn, D. Cremers, and H.-P. Seidel. [7] P. B. Callahan and S. R. Kosaraju. Algorithms for [8] Y. Cao, H. He, H. Man, and X. Shen. Integration of [9] Y. Chen, M. Welling, and A. Smola. Super-samples [10] M. S. de Lima and G. S. Atuncar. A bayesian method [11] L. Devroye and L. Gy  X  orfi. Nonparametric Density [12] L. Devroye and G. Lugosi. Combinatorial Methods in [13] T. Duong et al. ks: Kernel density estimation and [14] T. Duong and M. L. Hazelton. Cross-validation [15] H. Edelsbrunner, B. T. Fasy, and G. Rote. Add [16] A. Gangopadhyay and K. Cheung. Bayesian approach [17] J. Habbema, J. Hermans, and K. van den Broek. A [18] P. Hall, J. Marron, and B. U. Park. Smoothed [19] P. Hall and M. P. Wand. Minimizing L 1 distance in [20] S. Hu, D. S. Poskitt, and X. Zhang. Bayesian adaptive [21] M. Jones and S. Sheather. Using non-stochastic terms [22] J. Kiefer. Sequential minimax search for a maximum. [23] K. Kulasekera and W. Padgett. Bayes bandwidth [24] J. Marron and A. Tsybakov. Visual error criteria for [25] A. P  X erez, P. Larra  X naga, and I. Inza. Bayesian [26] J. M. Phillips. eps-samples for kernels. SODA , 2013. [27] J. M. Phillips, B. Wang, and Y. Zheng. Geometric [28] M. Rudemo. Empirical choice of histograms and kernel [29] S. R. Sain, K. A. Baggerly, and D. W. Scott.
 [30] D. Scott, R. Tapia, and J. Thompson. Kernel density [31] D. W. Scott. Multivariate Density Estimation: [32] D. W. Scott and G. R. Terrell. Biased and unbiased [33] B. W. Silverman. Density Estimation for Statistics [34] G. R. Terrell. Maximal smoothing principle in density [35] E. Ullman. A theory of location for cities. American [36] M. Wand and M. Jones. Multivariate plug-in [37] C. Yang, R. Duraiswami, and L. S. Davis. Efficient [38] C. Yang, R. Duraiswami, N. A. Gumerov, and [39] X. Zhang, M. L. King, and R. J. Hyndman. A [40] Y. Zheng, J. Jestes, J. M. Phillips, and F. Li. Quality
To prove the weighted centroid method converges, we want to prove Theorem 1 in 1 and 2 dimension. For simplicity, we assume Q  X  P so P = P  X  Q .

First we work on the weighted 1-dimensional data, and extend to 2 dimension using that the cross section of a 2-dimensional Gaussian is still a 1-dimensional Gaussian. We focus on when P and Q use the same bandwidth  X  , and a unit kernel K  X  . We start to examine two points in 1 dimension, and without loss of generality, we assume p 1 = d and p 2 =  X  d for d  X  0, and that the coreset of P is Q = { p 2 } . We assign the weight for p 1 as w 1 and the weight for p 2 as w 2 . Plug in P , Q and the weight for each point, G ( x ) = | kde P ( x )  X  kde Q ( x ) | is expanded as following:
G ( x ) = 1 We assume w 1  X  w 2 , the largest error point must be closer to p . So we only need to discuss when x  X  0, then 1 2 w 1 exp  X 
G ( x ) = 1
Lemma 2. For K  X  a unit Gaussian kernel, P = { p 1 ,p 2 } and Q = { p 2 } where p 1 = d and p 2 =  X  d , when x  X  0 , function G ( x ) has only one local maximum, which is between d and d +  X  and G ( x ) is decreasing when x &gt; d .
Proof. By taking the derivative of G ( x ), we can get ing.

When x = d ,
To understand x &gt; d we examine the ratio function r ( x ) = Since both exp  X  2 xd  X  2 and x + d x  X  d are decreasing and positive, r ( x ) and thus d G ( x ) d x is decreasing when x &gt; d .
When x = d +  X  , the ratio function is We can view the above equation as a function of variable d . and take the derivative of r ( d ): With d  X  0 then d r ( d ) d d  X  0 and thus r ( d ) is a decreasing function which attains maximum w 2 w r ( d )  X  1. So when x = d +  X  , d G ( x ) d x  X  0. With the above when x &gt; d , there is only one point between d and d +  X  is only one maximum point of G ( x ) between d and d +  X  when x  X  0.
 From Lemma 2, we show that the evaluation point having largest error is between d and d +  X  . Due to the symmetry of p 1 and p 2 , when w 1  X  w 2 ,G ( x ) gets its largest error between  X  d and  X  d  X   X  .

With the results on both sides, we now show the maximum value point of G ( x ) can X  X  be outside  X  distance of Conv ( P ). Now we discuss the case for n points in 1 dimension.
Lemma 3. For K  X  a unit Gaussian kernel, P has n points and | Q | = | P | / 2 , arg max x  X  R 1 G ( x ) for 1 -dimensional data is within  X  distance of Conv ( P ) .

Proof. Suppose n = 2 k , P = { p 1 ,p 2 ,p 3 ,p 4 ,...,p 2 k  X  1 choose any k points in Q . Then pair any point in Q with any point in P not in Q , so each point in P is in exactly one pair. For simplicity we set Q = { p 1 ,p 3 ,...,p 2 k  X  1 pairs are { p 1 ,p 2 } , { p 3 ,p 4 } ,..., { p 2 k  X  1 ,p Suppose e 1 = arg max x  X  R 1 G ( x ) is not within  X  distance of Conv ( P ) and p 1 is the point closest to e 1 . Based on Lemma 2, for P has only two points, function G ( x ) is decreasing as a point outside  X  moves away from p 1 . So if we choose another point e 2 infinitesimally closer to p 1 , and we set P 1 = { p Q 1 = { p 1 } , G P 1 ,Q 1 ( e 2 ) has larger value than G P Since p 1 is the closest point in P , for any other set P { p 3 ,p 4 } , Q 2 = { p 3 } , e 2 is closer to P 2 than e 1 hence G P 2 ,Q 2 ( e 2 ) is also larger than G P 2 ,Q 2 ( e k . So G ( e 2 ) &gt; G ( e 1 ), which contradicts the assumption that e 1 = arg max x  X  R 1 G ( x ). So the largest error evaluation point should be within  X  distance of Conv ( P ).

In 2 dimensions we show a similar result. We illustrate the Minkowski sum M of a set of points P with a ball of radius  X  in Figure 2.

Theorem 3. For K  X  a unit Gaussian kernel, and two point sets P,Q  X  R 2 , | Q | = | P | / 2 , arg max x  X  R 2 be within the Minkowski sum M of a ball of radius  X  and Conv ( P ) .

Proof. Now we have n points in P  X  R 2 . Suppose the largest error position e 1 = arg max x  X  R 2 G ( x ) 6 X  M , then for some direction v no point in the convex hull of P is closer than  X  to e 1 after both are projected onto v . Then since any cross section of a Gaussian is a 1-dimensional Gaussian (with reduced weight), we can now invoke the 1-dimensional result in Lemma 3 to show that e 1 is not the largest error position along the direction v , thus e 1 6 = arg max x  X  R 2 So arg max x  X  R 2 G ( x ) should be within the Minkowski sum M of a ball of radius  X  and Conv ( P ).
