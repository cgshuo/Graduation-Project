 Simon M. Lucas 1 , Alex Panaretos 1 , Luis Sosa 1 , Anthony Tang Kazuki Ashida 2 , Hiroki Nagai 2 , Masayuki Okamoto , Hiroaki Yamamoto 2 , Hidetoshi Miyao 2 , JunMin Zhu 3 , WuWen Ou 3 , Christian Wolf 4 , Jean-Michel Jolion Lin 6 Abstract. This paper describes the robust reading competitions for ICDAR 2003. With the rapid growth in research over the last few years on recognizing text in natural scenes, there is an urgent need to establish some common benchmark datasets and gain a clear understanding of the current state of the art. We use the term  X  X obust reading X  to refer to text images that are beyond the capabilities of current commercial OCR packages. We chose to break down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system. The subproblems we chose were text locating , character reco gnition and word reco gnition . By breaking down the problem in this way, we hoped to gain a better understanding of the state of the art in each of the subproblems. Furthermore, our methodology involved storing detailed results of applying each algorithm to each image in the datasets, allowing researchers to study in depth the strengths and weaknesses of each algorithm. The text-locating contest was the only one to have any entries. We give a brief description of each entry and present the results of this contest, showing cases where the leading entries succeed and fail. We also describe an algorithm for combining the outputs of the individual text locators and show how the combination scheme improves on any of the individual systems. Keywords: Reading competition  X  Text locating  X  Camera captured 1 Introduction Fifty years of research in machine reading systems has seen great progress, and commercial OCR packages now operate with high speed and accuracy on good-quality documents. These systems are not robust, however, and do not work well on poor-quality documents or on camera-captured text in everyday scenes. The goal of general-purpose reading systems with human-like speed and accuracy remains elusive. Applications include data archive conversion of noisy documents, textual search of image and video databases, aids for the visually impaired and reading systems for mobile robots.
 eral reading systems that are able to locate and/or read text in video or natural scene images [7, 8, 11, 13, 32, 33]. So far, however, there have not been any standard pub-licly available ground-truthed datasets, which severely limits the conclusions which may be drawn regarding the relative merits of each approach.
  X  To capture and ground-truth a significant size text- X  To design or adopt standard formats for these  X  To design or adopt standard evaluation procedures  X  To run the competitions in order to get a snapshot True.xml dures used to run the Fingerprint Verification 2000 (and 2002) competitions [16]. Well in advance of the deadline we published sample datasets for each problem, the eval-uation software to be used, and the criteria for deciding the winner of each contest. To enter the contests, re-searchers had to submit their software to us in the form of a ready-to-run command-line executable. This takes a test-data input file and produces a raw results file. The raw results are then compared to the ground truth for that dataset by an evaluation algorithm, which produces a set of detailed results and also a summary. The detailed results report how well the algorithm worked on each im-age, while the summary results report the aggregate over all the images in the dataset. All these files are based on simple XML formats to allow maximum compatibility between different versions of evaluation systems, recog-nizers and file formats. In particular, new attributes and elements can be added to the markup while retaining backward compatibility with older recognition systems. The generic process is depicted in Fig. 1. 2 Data capture Images were captured with a variety of digital cameras by each of the Essex authors. Cameras were used with a range of resolution and other settings, with the particu-lar settings chosen at the discretion of the photographer. ging of the images, and with a view to possible future tagging jobs, we implemented a Web-based tagging sys-tem. This operates along similar lines to the OpenMind concept. 1 People working as taggers can log in to the sys-tem from anywhere on the Internet using a Java (1.4)-enabled Web browser. On logging in, a Java applet win-dow appears and presents a series of images. The tagger tags each image by dragging rectangles over words and then typing in the associated text. The applet then sug-gests a possible segmentation of the word into its indi-vidual characters, which the tagger can then adjust on a character-by-character basis. The tagger can also adjust the slant and rotation of the region. When the tagger has finished an image, he clicks  X  X ubmit X , at which point all the tagged rectangles are sent back to a server, where they are stored in a database. One of the parameters of the system is how many taggers should tag each image. If we had a plentiful supply of tagging effort, then we could send each image to several taggers and simply ac-cept all the images where the tags from different taggers were in broad agreement. This is somewhat wasteful of tagging effort, however, since it is much quicker to check an image than it is to tag it. We therefore adopted a two-tier tagging system of taggers and checkers, where the job of a checker was to approve a set of tags. the applet and the server. We chose to use Simple Ob-ject Access Protocol (SOAP)  X  partly to gain experience of SOAP on a real project, and partly to allow good in-teroperability with other systems. Potentially, someone could now write a tagging application in some other lan-guage, and still request images to tag, and upload tagged images to our server.
 the data we captured. This sample corresponds to the word Department in Fig. 3. The root element is tagset and consists of a sequence of image elements  X  one for each image in the dataset. The imageName element gives the relative path to the image file, and the resolution element gives the width and height of the image. The taggedRectangles element contains a taggedRectangle el-ement for each word in the image. The x , y , width and height attributes specify the location (top left corner) and size of the word, while the offset and rotation spec-ify the slant (e.g. for italicised text) and rotation of the word. The text of the word is given as the body of the tag element. The segmentation element specifies a sequence of offsets for each character segmentation point in the word. Note that this model does not exactly fit all the possible variations in camera-captured text but was an adequate model for the vast majority of images that we captured. This format was used to mark up the ground truth for the images and also as a general output for-mat for the robust reading and text-locating contests. Entries for the text-locating contest omitted the tag and segmentation elements and also the offset and rotation attributes. 3 The competitions Reading text in an image is a complex problem that may be decomposed into several simpler ones. The best way to do this decomposition is open to debate. We chose to break down the robust reading problem into three stages and run competitions for each stage and also a competition for the best overall system. The stages we chose were text locating, character recognition and word recognition. Another possible stage would have been seg-mentation of words into separate characters. This idea was rejected on the grounds that we believed the im-ages would be too difficult to segment in a way that was independent of the OCR process, and we also wanted to place some limit on the number of competitions to be run. However, the segmentation data exist for all the words in the database, so it is still possible for researchers to evaluate their segmentation algorithms on these data. 3.1 Pretrained systems For all the competitions, we debated whether to run them for trainable or non-trainable systems. We de-cided that any system training or tuning was best left to the system designers, and hence each of the contests dealt with evaluating pretrained systems. The contes-tants were advised to download the trial datasets well in advance of the competition deadline in order to tune their systems for optimal performance on this type of data.
 sirable to test the learning ability of each method. Our prime concern here, however, was to find the system that performed best on each task, irrespective of the amount of hand tuning that went into its design. Hence we justi-fied our decision to base the contests on pretrained sys-tems. 3.2 Text locating The aim of the text-locating competition was to find the system that could most accurately identify the word regions in an image.
 JPEG file as input and produces a set of rectangles as output. The preferred system interface is that both the input and output files are in a simple XML format, de-scribed on the contest Web page. Taking the example image in Fig. 3, a text-locating algorithm would ide-ally identify five rectangles in image pixel coordinates surrounding the words  X  X epartment X ,  X  X f X ,  X  X omputer X ,  X  X cience X ,  X 1 X .
  X  such as specifying that the system find complete text blocks, or individual words or characters. We chose words since they were easier to tag and describe (it would be harder to fit rectangles to text blocks since they are more complex shapes).
  X  Be easy to understand and compute;  X  Reward text-locating algorithms that would be most  X  Heavily punish any trivial solutions (e.g. such as re-tions of precision and recall, as used by the information retrieval community. An alternative form of evaluation would be a goal-directed approach [23]. In this case, the text-locating algorithms could be judged by the word recognition rate they achieve when used in conjunction with a word recognizer (or OCR package). A difficulty of this approach, however, is its dependence on the par-ticular recognizer used. A detailed description of various object detection evaluation methods is given in [17]. a retrieval system as follows. For a given query (in this case, find all the word-region rectangles in an image), we have a ground-truth set of targets T and the set returned by the system under test, which we call estimates E . The number of correct estimates we denote c .
 mates divided by the total number of estimates: are punished with a low precision score.
 divided by the total number of targets: are punished with a low recall score.
 to agree exactly with the bounding rectangle for a word identified by a human tagger. Hence we need to adopt a flexible notion of a match. We define the area match m a between two rectangles r 1 and r 2 as twice the area of intersection divided by the sum of the areas of each rectangle i.e.: where a ( r ) is the area of rectangle r . This figure has the value one for identical rectangles and zero for rectangles that have no intersection. For each rectangle in the set of estimates we find the closest match in the set of targets, and vice versa.
 set of rectangles R is defined as: and recall: precision and recall figures into a single measure of qual-ity. The relative weights of these are controlled by  X  , which we set to 0.5 to give equal weight to precision and recall: age values of p , r and f respectively over the images in the test sets.
 image on average but dropped this as some of the sys-tems submitted were unable to comply with this, and given the small number of entries, we felt it would be inappropriate to be too strict. 3.3 Robust word and character reco gnition The aim of these competitions was to find the sys-tems best able to read single words and single charac-ters, respectively, that had been extracted from camera-captured scenes. The word recognizer takes two inputs: a file of words to be recognized and a dictionary file. For these experiments a custom dictionary was supplied that had 100% coverage of the words in the images. The term word is used loosely here to mean any string of characters that the image taggers approved as a word; some of the character strings would not be in a conventional dictio-nary. To simplify our software, we designed the character recognizer interface to operate in an identical manner to the word recognizer, except that words had to be one character in length. Despite several expressions of inter-est, we received no submissions for these contests in time to include in this paper. Example word and character images are shown in Figs. 4 and 5 respectively. 3.4 Robust reading The aim of this competition was to find the best system able to read complete words in camera-captured scenes. identify five words:  X  X epartment X ,  X  X f X ,  X  X omputer X ,  X  X ci-ence X , and  X 1 X  and also specify a bounding rectangle (in image pixel coordinates) for each word.
 and robust word recognition all tackle subparts of this problem. The robust reading competition aimed to iden-tify the system that best does the complete job. The ro-bust reader took as input a scene image and produced a set of tagged rectangles as output, where each rectangle was tagged with a single word hypothesis. The standard measures of precision and recall were used to evaluate the performance of a robust reader. Unlike the text-locating contest, where we rated the quality of match between a target and estimated rectangle, we defined a strict no-tion of match between the target and estimated words: the rectangles must have an area match score m a (see above) of greater than 0.5, and the word text must match exactly. The winning system would be the one with the best f score. 4 Experimental setup We organised the data for each competition into Sample , Trial and Competition datasets. Sample datasets were provided to give a quick impression of the data and also to allow functional testing of software, i.e. researchers could check that their software could read and write the specified dataset formats but not get any statistically meaningful results.
 used to get results for ICDAR 2003 papers. For this purpose, they were partitioned into two sets: TrialTrain and TrialTest . The instructions were to use TrialTrain to train or tune algorithms, then quote results on Tri-alTest. For the competitions, the instructions were that algorithms should be trained or tuned on the entire trial set (i.e. TrialTest  X  TrialTrain).
 systems by the competition deadline of 30 April 2003. The submissions were then evaluated by running them on the competition datasets.
 problem. The downloads column shows the number of downloads of the sample dataset for each problem; in each case, the number of downloads of the trial datasets, which are much larger, was approximately half this fig-ure. Note that the text-locating dataset ( locating in the table) was the same as the robust reading dataset, but there were no expressions of interest in the robust read-ing problem. Note that only in the case of the text-locating problem did the expressions of interest (EOIs) translate to actual entries. 5 Text-locating entries Text-locating systems typically exploit standard image processing and computer vision methods to transform raw pixels into higher-level features and components. Before describing each of the text-locating systems in detail, we first list the main methods used in each sys-tem. We refer to the systems by their submitted names: Ashida, HWDavid, Wolf and Todoran.
 tinct from the others. It is the only one not to use an image pyramid and is based on the following sequence of processes:  X  Fuzzy clustering algorithm (pixel-colour based)  X  Multipass binarisation depending on cluster member- X  Connected component analysis (blobbing)  X  Bounding rectangles for blobs  X  Rectangle grouping  X  Rectangle group feature extraction  X  SVM used to classify text/non-text rectangle groups While the other methods achieve some degree of scale invariance by using an image pyramid, Ashida achieves scale invariance through its choice of rectangle group fea-tures. The trainable part of the Ashida system is the Support Vector Machine (SVM).
 parameters which can be hand-tuned to the task, es-pecially in the text/non-text component classification heuristics. The main processes involved are:  X  Image pyramid construction  X  Edge detection  X  Low-pass filtering (in the horizontal direction)  X  Morphology (closing, opening)  X  Connected component analysis  X  Application of heuristics to classify components as to that of the HWDavid system, but a few differences are worth noting. First, the classification heuristics are re-placed with an SVM. Unfortunately, the authors did not have the opportunity to train the SVM on the ICDAR 2003 training set and used a model trained on a differ-ent type of dataset (based more on text in video images). Secondly, the order of the classification and morphology operators are reversed compared with HWDavid. Either or both of these factors could account for the significant difference in the performance of the algorithms on these data. Note also that HWDavid was nearly 60 times faster than Wolf (Table 2), which is probably explained by the fact that Wolf used an SVM at an early and therefore data-intensive processing stage.
 cesses:  X  Image pyramid construction  X  Texture filtering  X  K-means clustering of filtered values  X  Tagging as text pixels those pixels that correspond  X  Edge detection  X  Morphology  X  Connected component analysis  X  Application of heuristics to classify components as The fact that the heuristics used in Todoran were tuned to return text lines or blocks rather than words put this system at a disadvantage. That said, we found cases where Todoran greatly overestimated the size of a text block.
 sonable but not entirely complete picture of the state of the art in text-region locating. For example, trainable non-linear filters (such as multilayer perceptrons) are no-tably absent from the set of methods used in this paper, though they have been applied successfully to closely re-lated tasks elsewhere e.g. [8]. 5.1 Ashida (by Ashida, Nagai, Okamoto, Yamamoto and Miyao) Methods for character extraction or text location can be broadly categorised into three types: region-based, texture-based and edge-based methods.
 character have similar colour and can be segmented from the background by colour clustering. As a result, sev-eral monochrome images are generated by thresholding on a colour space, then characters are extracted under some simple heuristic constraints, such as the size and aspect ratio of circumscribing rectangles for each image. In these methods, the clustering process plays an im-portant role and may produce irrelevant monochrome images for complex background in some cases. Texture-based methods depend on texture features which need considerable time to compute and are not robust for ac-curate localization. Edge-based methods find vertical or horizontal edges to detect character location. However, for images with a complex background, too many edges make it difficult to select only adequate ones for charac-ter extraction.
 no assumptions with regard to the illumination condi-tions and the types of objects and textures present in the scene images. This means that a clustering algo-rithm may yield many regions which do not correspond to character patterns. For this reason, we adopt a fuzzy clustering method to select colours for thresholding on a colour space. Additionally, in order to distinguish char-acter patterns from background ones, we use a Support Vector Machine (SVM) [5,25]. The SVM uses features of blobs (connected components) in the monochrome im-ages.
 fuzzy clustering is applied to a given image, resulting in a set of binary images called colour separation images. Second, some blobs in each colour separation image are grouped under simple heuristic constraints to calculate the geometric features. Finally, an SVM trained on these features selects the blobs corresponding to character pat-terns. The overall process is shown in Fig. 6. Each step will be described with the aid of examples in the follow-ing subsections. 5.1.1 Clustering in colour space. Our colour clustering algorithm assumes that characters appear with the same colour in every character string or text line in a given image. In practice, this assumption is not realistic, and even within a character various colours are seen due to dithering of the printing process and reflections etc. This fact makes the clustering process difficult, so we apply an edge-preserving smoothing process to the scene image. (red), G (green) and B (blue). However, it is important to select a suitable colour coordinate system according to each purpose. It is generally accepted that the 1976 CIE LUV uniform colour space works well for colour segmen-tation. In this colour space, two colours that are equally distant are perceived as equally distant by viewers. clustering techniques have been proposed [6,14,19]. With these methods, performance of the clustering technique could dramatically affect segmentation results. Our aim in clustering is to obtain the blobs of pixels correspond-ing to character patterns. In our clustering method, a given image with 8 bits per R, G and B is translated into LUV colour space, and the colour histogram is cre-ated in reduced LUV colour space with 17  X  45  X  40 bins. rithm. We use a fuzzy clustering algorithm which de-cides the number of clusters automatically. This is based on the well-known Fuzzy C-Means (FCM) algorithm [3]. In FCM the number of clusters c is fixed, but the ap-propriate number of clusters is different for each image. Therefore, we allow splitting, merging and discarding of clusters in our algorithm. If the standard deviation within a cluster exceeds a threshold value, the cluster is divided into two. On the other hand, if two clusters locate closely, they are merged. Also, clusters which con-tain few elements are discarded. Apart from this, our fuzzy clustering algorithm is the same as FCM. ponents for both black (foreground) and white (back-ground) pixels are labelled, and their circumscribed rect-angles are generated. The reason for this process is to increase the performance of character extraction. colour separation images generated by the clustering pro-cess and the associated circumscribed rectangles. 5.1.2 Grouping rectangles. It is more difficult to distin-guish single character patterns from background pat-terns than from word or text lines. For this reason, some blobs in colour separation images are grouped into some regions that may correspond to character strings. In this paper, we assume horizontal or slightly skewed character strings in an image and do not consider vertical strings. The blobs in the colour separation image are grouped by the following conditions (Fig. 9). Although many blobs are not grouped and remain as single regions, we treat all regions henceforth as character candidates. 1.  X  1 ,  X  2 and  X  3 (Fig. 9a) are smaller than t  X  . 2. The adjacent rectangles are overlapped in the mag-3. The ratio of areas of the adjacent rectangles is smaller tally. Figure 10 shows all of character candidate rectan-gles generated from the colour separation image of clus-ter 6 in Fig. 8. In particular, the shaded rectangles in Fig. 10 show the grouped rectangles. 5.1.3 Discrimination of character patterns. Our algo-rithm discriminates character patterns from background ones by using a support vector machines (SVM). labelled as character pattern or background in 250 scene images. We use the software programme SVMTorch [9]. The following features were calculated for each character candidate and used as input to the SVM.
 Cross-correlation feature : Cross-correlations are Run length feature : Variance of run length in each Smoothness of contour line : For the outer contour Changing point of black/white pixels : The num-LAG feature : The number and average length of path Other features : Percentage of foreground pixels 5.1.4 Discrimination by SVM. The above features may not be useful for character pattern discrimination inde-pendently, but they work well when combined using an SVM. We normalised all features in [0,1]. As a kernel function, we adopted the radial basis function ( e  X  with  X  =0 . 85, which was determined experimentally. Figure 11 shows the character extraction result for the image in Fig. 7. Most characters except  X 95 X  in the bot-tom line are extracted correctly, but some non-character patterns which look like characters are also extracted in the bottom line. 5.2 HWDavid: (by Zhu and Ou) In order to allow a degree of scale invariance (i.e. detect text of different sizes), we use an image pyramid strategy. First we extract the text blocks from each level of the image pyramid separately and then get the final results of the input image by using a special function to combine the results of each level.
 single layer of the pyramid. Based on the fact that most text in natural scenes is horizontal, we use a measure of gradient density in the horizontal direction. The first step is to apply four Sobel edge operators, as shown in Fig. 12.
 edge intensity E ( x, y ): ent density image EI ( x, y ), where w is the window width in the horizontal direction: method proposed in [18, 30] to produce a binarised im-age. We experimented with the training set and found a gradient density window size of 9 gave the best results. image, we use two morphological operators: a 7-pixel closing operation followed by a 15-pixel opening opera-tion. The closing operation is to eliminate the connected strokes, and the opening operation is to remove the iso-lated regions. Additionally, we use a conditional morpho-logical operation on the connected components which is based on a CCA (connected component analysis) algo-rithm [10].
 is the binary (thresholded) result of the gradient density image of a , which contains some background noise, c is the result of the first-turn morphological operation (closing), in which noise has been reduced greatly, and d is the result of a second-turn morphological operation (opening), where we see that the noise around the text blocks has been removed.
 height, width and ratio of white to black dots in a block, we get the candidate text block. In our pro-gramme, the white dots correspond to character strokes and black dots to background. As shown in Fig. 13d, some noise blocks should be candidate text blocks, so post-processing is essential.
 block must simultaneously satisfy the following three constraints:  X  The number of humps or valleys of candidate blocks X   X  For the characters of a text string, the height h ,  X  For x -axis projection profiles, the number of humps each level of the image pyramid is designed as follows. Because there are overlapping text blocks at different levels in the image pyramid, to improve the detection precision, we must combine these overlapping blocks. Simple union or intersection does not work well. Union makes the text blocks too fat and reduces the precision, while intersection makes the text blocks too thin and reduces the precision rate as well as the recall rate. So we use a strategy of conditional union: when two blocks from different levels of the image pyramid are similar enough, we join them, i.e. return the rectangle that ex-actly bounds both of them. Otherwise, we treat them as different text blocks of different resolution and return the two rectangles independently. 5.3 Wolf (by Wolf and Jolion) We call our method: Learning Contrast and Geometrical Features with Support Vector Machines .
 tion algorithms we have developed with different philoso-phies. 2 The first algorithm [28, 31] assumes that there is text present in the image and tries to separate the text from the non-text pixels. The second algorithm, which participated at the ICDAR competition, employs an SVM in order to learn a model consisting of con-trast and geometrical features from training data. It is described briefly in this section; for more details refer to [29] or forthcoming publications. Among other differ-ences, the choice between the two methods controls a trade-off between detection recall and precision. sified into two categories: those based on character seg-mentation, which are less suited for low-resolution text, and those based on edge or texture features. Our system has been designed to detect text in video sequences and therefore has been optimized for low-resolution images. Consequently, our text model contains contrast and tex-ture features, completed by geometrical features. metrical constraints in a post-processing step only, i.e. by using mathematical morphology after the detection step. The disadvantage of this procedure is evident: a bad de-tection cannot be corrected even by sophisticated post-processing. We integrated the calculation of the geomet-rical features directly into the detection phase. To over-come the chicken-egg problem (to calculate geometrical features we need to detect the text first), we adopted a two-step approach: 1. Perform a coarse detection without taking into ac-2. For each pixel, calculate the geometrical features of The features calculated in step 2 are, among others, the text height and its regularity in a local neighborhood. We obtain the height from the  X  X ext probability image X  calculated in step 1, estimating the peak of the vertical profile centred at the pixel to classify, and the borders of the mode containing the peak. The vertical profile may contain a single peak in the case of a single text box, or several peaks in the case of several vertically stacked text boxes, or no peak at all or a very flat peak if the box does not contain text. We pose peak detection as an op-timization problem over the space of possible peak bor-ders, maximizing a criterion which consists of the peak height (the contrast between text and background), its mean (the text probability according to step 1) and its width (the text height). The criterion favors high peaks with high contrast and low width.
 the local regularity of its width (i.e. of the text height) and the features from step 1, are learned from training data using an SVM. Figure 14 shows the coarse detec-tion result and the text height regularity image for an example image.
 processed, enforcing morphological and further geomet-rical constraints [31]. This is in order to reduce noise, to correct classification errors and to connect loose charac-ters to form complete words. In order to adapt the algo-rithm to different text sizes, the detection is performed in a hierarchical framework. The pyramid is collapsed after the post-processing of each level is complete. Addition-ally, the classification step for each level of the pyramid contains the features for the respective level as well as its parent level. The purpose of doing so is to incorporate as much information as possible very early in the detec-tion process and to facilitate the process of collapsing the pyramid.
 age dataset it used was clearly different from the image dataset we had in mind when we designed our systems: the ICDAR image dataset consisted of images taken with digital photo cameras, mostly with a resolution of 1600  X  1200 pixels. In these images, the text characters are very large, hence algorithms performing full charac-ter segmentation before detection are favored. Our hi-erarchical processing system tended to produce 5 to 6 pyramid levels for these big images. Since our detection system was designed to detect small and noisy text, we went so far as to ignore the base level of the pyramid in order to increase the precision of the algorithm. In other words, we did not take into account valuable informa-tion. As a last handicap, we did not have enough time to train the system on the ICDAR test dataset. Instead, we submitted a model trained with our own test dataset. of 88.2% of the text rectangles with a precision of 75%, measured on a test database containing 40 min of video (newscasts, commercials, cartoons). 5.4 Todoran (by Todoran and Worring) Our text localisation system uses multiscale texture and edge analysis and was inspired by the system described in Wu et al. [26] but extended with more features and an improved grouping algorithm. First, a texture filter is applied to extract the candidate text regions. For tex-ture filtering we compute a local energy estimate for each colour channel at three different scales using second or-der derivative filters. The filters used in estimation are Gaussian kernels at scale  X  =(1 , values are clustered in the 9-dimensional space using the K-means algorithm. We expect that the cluster corre-sponding to the lowest energy comprises the text region. A morphological closing fills up holes in text regions. masked with text regions provided by the texture filter step. The vertical edges representing small portions of candidate characters are merged by morphological clos-ing in the horizontal direction. From the image of filtered vertical edges we extract the blobs (connected compo-nents).
 ing geometric features we filter the set of blobs and com-bine them into text lines. For filtering and grouping we use the following features of the blobs: colour, area, as-pect ratio, height, horizontal distance to the closest blob, and amount of vertical overlapping between blobs. chosen proportional to the most common value of the blob height H . The H is determined from the histogram of blob heights over the image. We make the assumption that only one font size is present in the candidate text region. Thus, candidate blobs are accepted if: 0 . 01  X  ImageArea &lt; BlobArea &lt; 0 . 50  X  ImageArea  X  W idth &gt; Height  X  W idth &gt; 2  X  X  X  0 . 9 H &lt; Height &lt; 1 . 1  X  X  AverageColor 1= AverageColor 2  X  HorizontalDistance &lt; 1 . 2  X  X  X  V erticalOverlap &gt; 0 . 4  X  X  X  | Height 1  X  Height 2 | &lt; 0 . 85  X  X  regions due to blobs representing holes inside characters. If two text regions overlap, then the smaller one is re-moved.
 of an image pyramid. Due to good image quality and large image size, for the ICDAR 2003 competition we used only two scales in the pyramid: one-eighth and one-fourth of the original image size.
 blocks from an image, many errors are generated by splitting the desired target objects into small parts or merging two or more target objects into one. Therefore, in evaluating such systems, it is better to detect these special situations of  X  X plit X  and  X  X erge X  rather than to treat them as false alarms or misdetections.
 at a block level. Rather than trying to locate isolated characters or words, we are looking for text lines and text blocks. Therefore, the evaluation measure used at the ICDAR 2003 competition did not show our system in a favourable light. We have tested this implementation and other systems from the literature on a large dataset, using different evaluation measures [22] which are more appropriate for text blocks. There, the merge and split situations of evaluated text regions relative to the ground truth can be detected, and they are not severely pun-ished, and consequently the system achieved better re-sults. 6 Results The text-locating competition had five entries by the 30 April 2003 deadline; the other contests all had zero en-tries. Many of the originally supplied entries were miss-ing DLL or other library files  X  contestants were invited to supply any missing files, which they all did. Some of the originally supplied systems were buggy and would crash after processing several images, perhaps due to memory leaks. Again, contestants were invited to sup-ply fixed versions, which they mostly did. In the case of one of the submissions, the patched version still crashed frequently and had such a low score ( f =0 . 01) on a set of sample images that we did not run it on the full set, and hence did not include it in the results table. run these competitions by using an alternative mode of entry, where each competitor exposes their system as a Web service [4, 15] which they would be responsible for maintaining. The evaluation programme would then work by supplying images to the service, which would return its estimated set of rectangles for each image in XML. We aim to foster this approach by supplying some skeleton software and examples of how to do this. shown in Table 2. The entries are identified by the user name of the person submitting each one, and each system is described in Sect. 5.
 seconds to process each image for each system under test. This is the elapsed time when running on a 2.4-GHz Pentium 4 PC. Note that the Ful l system is the score ob-tained by returning a single rectangle for each image that covers the entire image. This could have been computed from the resolution information in the XML input file, but to give a baseline measure of the time, we computed this by retrieving and decompressing each JPEG image, then measuring the image size.
 scheme does not necessarily mean that the algorithms are poor at finding text. For example, in some of the results of Todoran that we studied, the algorithm had tagged a large block of text consisting of multiple words with a single rectangle. Our evaluator gives some credit for this, but not nearly as much as a locater that identi-fies individual words, which was the performance objec-tive.
 6.1 Results on grey-level images An interesting aspect of evaluating these text-locating systems is the extent to which they exploit colour in-formation. In order to explore this further, we ran the best performing algorithms, Ashida and HWDavid, on grey-scale versions of the competition images. We pro-duced the grey-scale images by averaging the red, green and blue components of the colour images. A significant subset of the grey-level images were inspected visually, and in all cases the text was still legible.
 was approximately the same for each algorithm as be-fore, so this information is omitted. The results indicate a slight deterioration in performance for each algorithm. In the case of Ashida, we had expected a far more drastic reduction in accuracy, since from the description of the algorithm it is clear that colour clustering plays an im-portant part in its operation. The explanation for why it still performs rather well lies in the fact that even monochrome  X  X olours X  can be clustered based on their intensity. 6.2 Results with an alternative metric Inevitably, the choice of performance metric creates a bias in the results that will typically favour one algo-rithm over another. This is especially true in the case of evaluating text-locating algorithms. There are two main aspects of evaluation in which different choices are possi-ble: allowing for imprecise matches and dealing with the correspondence problem (one-to-many etc.).
 match ground-truth rectangles, we need some way to cope with this. A continuous measure based on area of overlap was used for the competition metric. How-ever, this means that the score is non-intuitive, with no distinction between the number of rectangles impre-cisely matched and the number of rectangles completely missed. In other words, a recall score of 0.5 for a par-ticular image could mean that one out of two rectangles in the image was identified perfectly (and the other one missed completely) or that the only rectangle in an im-age was identified with an overlap score of 0.5. [27], which was inspired by the rectangle-matching al-gorithm presented in [12]. This takes into account one-to-one as well as many-to-one and one-to-many matches. However, the algorithm aims to determine, controlled by thresholds on the amount of overlap, whether a ground-truth rectangle has been detected or not. The perfor-mance of the detection method is intuitively presented as a graph, which displays the precision and recall for different overlap thresholds. The changed precision and recall measures are given by: where M atch T and M atch E are functions which take into account the different types of matches described above and which evaluate to the quality of the match:
M atch T ( T i )= The function M atch E is defined accordingly.
 is matched against a detected rectangle E j is taken based on the overlap information stored in two matrices  X  and  X  , which corresponds intuitively to the  X  X urface recall X  and  X  X urface precision X : angle E j if its surface recall  X  ij is above a threshold t and its surface precision  X  ij is above a given threshold t In the case of a one-to-many match, each single rectan-gle must satisfy this constraint, as well as the combined (scattered) area.
 cision and recall values share their intuitive meaning with the original measures introduced in the domain of content-based image retrieval. The measures repre-sent, respectively, the amount of items (text rectangles) retrieved from the  X  X atabase X  and the amount of items correctly detected among all items detected.
 performance metric. We chose the threshold values of t r =0 . 8 and t p =0 . 4 as they gave perceptually rea-sonable results. This also highlights an advantage of the more simplistic metric used for the competition results, in that the simplistic metric has no parameters to set. sures are similar to the preceding ones, despite the fact that the performance measure is rather different. The order of the two leading algorithms, which continue to return similar f scores, has now changed, but otherwise the ranking remains the same.
 total number of rectangles that were detected. To put these figures in perspective, the total number of ground-truth rectangles in these images is 2261.
 6.3 Results on sample images We viewed many of the results of each programme, es-pecially the two leaders, to gain an impression of the strengths and weaknesses of each system. In each of the following images, the ground-truth rectangles are indi-cated by long-dashed red lines, 3 while the estimated rect-angles are indicated by white dotted lines.
 where both HWDavid and Ashida performed poorly. On this test HWDavid identified lots of false text rectangles, while Ashida returned just one rectangle that had no intersection with the ground-truth rectangle ( X  X AXI X ). The reason for HWDavid X  X  false detections appears to be the strong edge features present in the windows of the building. Note that the heuristics have partially filtered out the vertical run of windows on the left of Fig. 15 but not the horizontal run of windows close to the centre of the image.
 sistent in their ability to locate text. In some cases they detected noisy, hard-to-read text, while in other cases they missed text that to the naked eye is very clear. For example, HWDavid detected some of the text in Fig. 16 while missing other parts such as SV that were in the same font and appeared equally clear. HWDavid had an f score of 0.65 for this image, while Ashida returned no rectangles and scored 0.0.
 cated the text ( X 15 X ) in the image and achieved an f score of 0.88, but HWDavid returned no rectangles and scored 0.0. Incidentally, we also tried running Ashida on a grey-scale version of this image, with the result that it scored 0.0, having failed to identify any rectangles. valid text character that the human tagger had either missed or considered not worthy of tagging. While HW-David was unfairly punished here, it still achieved an f score of 0.65 on this image, and the effect this miss-ing tag had on the overall performance score of HW-David was minuscule. Todoran returned a rectangle cov-ering the entire image for this example and scored 0.04. Ashida returned a only single rectangle, for the middle text line, and scored 0.28. Wolf, on the other hand, re-turned a rectangle for each of the three main lines of text and scored 0.51. Unfortunately, each rectangle was slightly oversized, a phenomenon we observed in many cases with Wolf, which certainly had a detrimental ef-fect on its score. Also note that HWDavid cuts  X  X an-guage X  in half with the larger rectangle. This behaviour is certainly undesirable for subsequent OCR, but it is not adequately punished with our somewhat simplistic performance measure. 7 Combining the individual text locaters (by Lin) As shown in the testing results in Table 2, the best indi-vidual text locator only achieved a precision rate of 0.55 and recall rate of 0.46. Obviously, it is still a very difficult problem. This fact prompted us to look into the com-bination of the text locators, since combination proves to be very effective in pushing the envelope in character recognition [21]. However, there is little existing research on the combination methods for text locating. Equipped with four state-of-the-art systems, we were able to ex-plore this area with initial success. 7.1 Problem definition The combination of text locators can be formulated as follows: Each text locator outputs a set of detected rect-angles R i ( i =1 , 2 ,...,m ), where m is the number of text locators. The combination algorithm will generate a new set of rectangles R , which is a function of R 1 ,R 2 ,...Rm : cating is a complex problem. Many widely used combi-nation methods such as voting cannot be directly used for text locating because there can be various cases of two-dimensional relationships of two rectangles, as illus-trated in Fig. 19:  X  Two rectangles are identical (Fig. 19a)  X  Two rectangles are very similar (Fig. 19b)  X  Two rectangles partly overlap (Fig. 19c)  X  One rectangle falls into the other one (Fig. 19d)  X  Two rectangles do not overlap at all (Fig. 19e) 7.2 Solution We have proposed a combination system specifically tar-geting the four submitted text-locating systems. We di-vided the 500 testing images into two subsets: the first 250 images are for used for designing and tuning the combination algorithm (combination training set), and the remaining 250 images are kept exclusively for testing (combination test set). The combination algorithm was designed to exploit the relative merits of the individual algorithms as observed on the training subset. The basic idea is weighted voting of results from different text lo-cators. On the other hand, the actual algorithm is much more complicated due to the different 2D relationships of rectangles mentioned above. It has the following key elements:  X  Only Ashida, HWDavid and Wolf were used in the fi- X  Then, each rectangle r  X  R 2 is compared with rect- X  If r covers most of the area of an existing rectangle p  X  Next, each rectangle in R 3 goes through the same In order to increase the precision rate, they are passed through several filters:  X  Basic filter: This filter will delete rectangles whose  X  Big block filter: If one rectangle p in R is essentially  X  Wolf filter: If one rectangle p in R originates from a 7.3 Combination results As shown in the above algorithm, there are a few param-eters, weights and thresholds to be set. These parame-ters were tuned one by one to achieve the best average f -value on the combination training set. The final com-bination weights are 1, 0.8 and 0.7 for Ashida, HWDavid and Wolf respectively, roughly proportional to their in-dividual f -values.
 scheme scores 0.60, 0.57 and 0.57 for precision, recall and f respectively.
 tion scheme performs on the combination test set, shown in Table 5. Compared to Ashida, the best individual al-gorithm, the combination scheme improves the precision by 0.02, the recall by 0.10 and the f -measure by 0.05. rules is negligible, but the extra cost of running all the individual algorithms (as compared with just running the single best one) could be significant; refer to Ta-ble 2 for timing information. If the algorithms were run in parallel on different machines, however, then this ob-jection would be overcome. The Web service model offers a straightforward and entirely platform-and language-independent way of achieving this.
 scheme improved the text-locating result on a particular image. In this case, the precision, recall and f scores were as follows: Ashida: (0.0, 0.0, 0.0); HWDavid: (0.29, 0.68, 0.41) Wolf: (0.13, 0.45, 0.20); combined: (0.49, 0.68, 0.57).
 proved the f -value over the best individual text locator, the heuristic rules are based on the three text locators. With other text locators we would have to discover new combination rules. An interesting but difficult future re-search direction would be to investigate ways of auto-matically discovering the rules. One way to approach this would be to define a suitable rule space and then use an evolutionary algorithm to search both the space of possi-ble rules and rule parameters. This kind of approach has been used successfully when using genetic programming for symbolic regression, for example. 8 Future competitions We plan to run similar text-in-scene reading competi-tions for ICDAR 2005. One difficulty in running the contests is gathering and ground-truthing the data. An interesting approach that could be pursued in parallel would be to generate synthetic images that pose simi-lar challenges to the camera-captured images. This has been done successfully for more conventional document images [2]. Synthetic text image generation programmes such as Captcha [24] have recently been developed as a reliable way to tell humans apart from machines. Such methods are termed Human Interactive Proofs (HIPS) [1]. Figure 21 shows an image 4 from the Captcha sys-tem that is currently beyond machine reading abilities. Clearly, such images could form the basis of an inter-esting future contest and are already being used as a benchmark by computer vision researchers. rently under review. For example, one point that arises in the text-locating contest is that the algorithms should give a confidence rating to each rectangle that they re-turn. This would allow more direct tuning of the trade-off between precision and recall and would also be useful information for any combination scheme.
 formance measures. Although we also ran the perfor-mance measure defined by Wolf [27], this still leaves room for improvement towards measures that more di-rectly reflect the usefulness of the extracted rectangles for subsequent OCR, without being tied to the perfor-mance of a particular OCR engine. For example, since we have both the ground-truth rectangles for each word image and the associated segmentation information, it should be possible to define a measure that strongly pe-nalises cutting a word in half while offering some leeway for algorithms to return a small border around a word without any penalty.
 was the failure to convert the expressions of interest in the character and word recognition contests into actual entries. This may be explained partly by the difficulty of the data and partly by the effort involved in adapt-ing one X  X  algorithms to the specific problems at hand. In response to the latter point, we shall offer the character recognition data in a simple fixed-size monochrome for-mat (such as the one used for MNIST. 5 While there is a risk of discarding important information in the normal-isation process, the advantage of using a standard for-mat perhaps outweighs this, and doing this would not preclude also offering the full image datasets. 9 Conclusions Our main intention in running these competitions was to gain a clear picture of the state of the art of reading text in scenes. This has so far been partially successful for the text-locating problem, but not for the other problems. The public datasets we captured and tagged should make a useful contribution to research in this area. mon characteristics, such as the use of an image pyra-mid to achieve some degree of scale invariance. In fact, the only system not to use an image pyramid was the winning method of Ashida . Even when the methods are broadly similar, there are significant differences in the details, however, such as the image processing opera-tors used to extract candidate text blocks and the fea-tures used to subsequently classify them. Some of the systems (such as Ashida and Wolf) can be directly de-ployed as trainable object detectors (e.g. to detect faces in an image), while others (such as HWDavid) have hard-coded features specifically designed for text detection. Each system involves a significant degree of tuning to the type of data it is likely to encounter. While it is a difficult notion to quantity, it appears that the amount of tuning used was a significant factor in the success of each method, with the two leading methods being better tuned than the trailing two. In particular, the HWDavid and Wolf systems are very similar, and we believe the wide disparity in their respective performances is largely due to the extent to which each system was tuned. ing set, and the entrants were not given the test data until after the competition had finished.
 particular, the leading entries, Ashida and HWDavid), all tuning was done by hand, based on human intuition and empirical evidence of training set performance. This is a labour-intensive process, and it would be interesting to investigate making these or indeed other text-locating methods self-tuning.
 optimisation algorithm, the number of evaluations of the objective function limits the extent to which a system can be optimised. A good choice of objective function would be the overall f score on the training set, f ( T ). Since there are 500 images in the training set, each eval-uation of f ( T ) takes a few minutes for HWDavid, ver-sus a few hours for Wolf, making HWDavid much more amenable to tuning. All of the systems under test are complex and have multiple stages. The extent to which the various stages in each system could be independently (and therefore more quickly) tuned is unclear, and worth further investigation.
 tentative insights into the general strengths and weak-nesses of the submitted systems. These can be sum-marised as follows:  X  Even the best-performing systems are inconsistent,  X  There are major differences in the speed of the sub- X  Variations in illumination, such as reflections from  X  Variations in scale cause significant problems, in that nificantly improved on the best individual text locator and raises the interesting possibility of developing train-able combination schemes whose parameters can be di-rectly estimated from the data.
 very much a challenging problem. We believe that the results of the ICDAR 2003 text-locating contest give an idea of the state of the art in this particular subproblem. We would like to encourage researchers to use our freely available datasets to test their systems.
 References
