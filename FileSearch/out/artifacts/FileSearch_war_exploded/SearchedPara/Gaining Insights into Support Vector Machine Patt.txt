 an important complement to automated classification algo-rithms in data mining. Pictures can provide easy to digest summaries of complex information. In classification prob-lems, graphics can help us understand the nature of the boundaries between classes and the relative importance of variables for differentiating classes. We explore the use of dynamic graphics methods called tours [1, 8, 7, 4] to exam-ine results from SVM. 
Tours provide mechanisms for displaying continuous se-quences of low-dimensional linear projections of data in high-dimensional Euclidean spaces. They are generated by con-structing an orthonormal basis that represents a linear sub-space. Tour methods are most appropriate for data that contain continuous real-valued variables. They are useful for understanding patterns, both linear and non-linear, in multi-dimensional data. However, because tours are defined as projections (analogous to an object shadow) rather than slices some non-linear structures may be difficult to detect. 
Tours are also limited to applications where the number of variables is less than 20 because otherwise the space is too large to randomly explore within a reasonable amount of time. Hence when we have more than 20 variables, it is im-portant to perform some dimensionality reduction prior to applying tour methods. 
This paper describes the use of tour methods for exploring the results of SVM for an application on classifying olive oils according to production region. The data set is one that the authors have some experience in working with. It is interesting because it is a multi-class data set and the boundaries between classes are both simple and complex. 
Let E = {(xl, yl), (x2, y2),-'" , (x/, yt)}, where xi E T~ N and yi E {-1,1} be a set of training examples for a 2-category classifier. Suppose the training data is linearly separable. Then it is possible to find a hyperplane that partitions the N-dimensional pattern space into two half-spaces R + and R-. The set of such hyperplanes (the solu-tion space) is given by fw,b(x) = sign(w.x+b). SVM selects among the hyperplanes that correctly classify the training set, the one that minimizes Ilw[I 2, which is the same as the the two sets are explanatory variables (xi) and class identity (yi) (or even predicted values). In a manual tour, the user can adjust the projection coefficient of a single variable by rotating it into or out of the current projection [7]. 
There are several approaches to exploring SVM results using tours: the location of the support vectors in the data space, the SVM predicted values in relation to the explana-tory variables, and the weight vectors, w, for examining im-portance of the explanatory variables to the classification. The grand (random) tour is used for generally exploring sup-port vectors and classification boundaries in the data space. Manually controlled tours are used for studying variable im-portance. A correlation tour (and manually controlled cor-relation tour) is used to examine predicted values in relation to explanatory variables. 
We examine the distribution of support vectors relative to the other instances in the data set to explore whether tour methods can provide some insight into the behaviour of the SVM algorithm. If the two classes are linearly sep-arable, we expect to see support vectors from each group roughly indicating a boundary between the groups in some projection. The variables contributing to the projection pro-vides an indication of relative importance to separating the groups. The coefficients of the projection (elements of P) are used to examine the variable contribution. 
We examine the predicted value wx + b for each instance x in the space of explanatory variables. By using tour meth-ods and focusing on the predicted values that are close to 0, we can explore the nature of the decision boundary in the space of the explanatory variables. Predictions in the neighborhood of 0 represent instances on the edge of the two groups in linearly separable problems. 
Examining the weights (w*, b*) of decision boundary which maximizes the margin of separation between the two classes is a way to explore the importance of variables. If the vari-ables are standardized (zero mean, unit variance) prior to fitting the SVM, the magnitude of the components of w* provide some indication of their relative importance in defin-ing the separating hyperplane between the two classes. (If the variables are not standardized then correlations between the predicted values and the variables can be used similarly. The correlation measures the strength of the linear relation-ship between predicted values and explanatory variables.) Elimination of variables that have negligible correlation with predicted values should result in simpler decision boundaries with little loss in accuracy. 
This paper illustrates these three procedures for one ex-ample data set but they generalize to any linearly separa-ble problems. Non-linearly separable problems pose further challenges not addressed here. 
The olive oil data consists of the percentage composi-tion (xl00) of 8 fatty acids (palmitic, palmitoleic, steaxic, oleic, linoleic, linolenic, arachidic, eicosenoic) found in the lipid fraction of 572 Italian olive oils. (An analysis of this data is given in [11]). There are 9 collection areas, 4 from southern Italy (region 1), 2 from Sardinia (region 2), and 3 from northern Italy (region 3). The samples from the most important variable for separating regions. We would expect that in this variable the support vectors would all be in the boundary of the regions for eicosenoic acid, that is, take values close to 10 for region 1, and values close to 3 for regions 2,3. Interestingly, the support vectors do not lie on the edge of the two groups. Using a manually controlled correlation tour we explore the influence of other variables. Rotating palmitic acid with a positive component in a linear combination with eicosenoic brings the support vectors from region 1 closer to the boundary with regions 2 and 3 (middle plot). Also, subtracting a small component of stearic acid from the linear combination of eicosenoic and palmitic brings the support vectors from regions 2,3 closer to the boundary with region 1. It appears that these three variables were detected to be important by the SVM for separating region 1 from regions 2,3. 
If we examine the correlations with predicted values (Ta-ble 2) with each variable we find that eicosenoic acid is the most important, followed by palmitic, palmitoleic, and neg-ative oleic acids. Starting with eicosenoic acid and manually touring a small number of each of these variables into the view (i.e. altering the projection coefficients to include more of these variables in the projection) gives similar results. It is clear that the SVM has used a combination of variables to generate the space of best prediction. Figure 2 shows a histogram of the predicted values from SVM, and also a his-togram of eicosenoic acid alone. Both show good separations between the two groups. On re-fitting the SVM using only one variable, eicosenoic acid, we obtain the same accuracy as that obtained using the more complex model. 
Figure 3 shows the support vectors as large solid circles in plots of oleic vs linoleic acid, and a linear combination of oleic, linoleic and arachidic acids from a manually con-trolled correlation tour. The location of the support vectors are roughly where we would expect them: on the boundary between the two groups in the variables oleic and linoleic acids. From the visual examination of the data these two variables emerged as the major variables for separating the two regions. The correlation tour is used to examine the location of the support vectors in the combination of vari-ables oleic, linoleic and ara~hidic acids. The separation is stronger in these 3 variables, and the support vectors are still roughly in the boundary between the two groups. The correlations of predicted values and the individual variables (Table 2) also indicates that these 3 variables are the most important in the SVM prediction. Refitting the model with only these variables gives the same 100% accuracy. 
Understanding the results of classifying areas within south-ern Italy are much more difficult. These are not linearly separable groups. To separate areas 1 (North Apulia) and 2 (Calabria) from areas 3 (South Apulia) and 4 (Sicily) 7 sup-port vectors are used, and 29 vectors are between the two groups (Table 1). The correlation between predicted values and variables (Table 2) suggest that variables oleic, linoleic, palmitic and palmitoleic are important. Figure 4 shows the support vectors (solid circles) and the slack vectors (open circles) in relation to all the instances for Region 1 in a pro-jection of these variables. The support vectors and slack vectors are spread throughout the view. If we ignore the area Sicily (area 4) then this view provides a good separa-tion of all three other areas. In the initial visual examination of the data we did find that Sicily was the most difficult area to classify, it overlaps with all three other areas. The other Figure 4: Olive oil data: (Left) Support vectors for the areas 1,2 vs 3,4 in the southern region are high-lighted as large solid circle. View is the combina-tion suggested by correlations between variables and predicted values. (Right) Same view with instances corresponding to Sicily are removed, revealing the neat separation between the other three areas. sets. 
In general, the tour methods can be applied for a small number of variables, not thousands of variables. Tour meth-ods can provide us better insight into the nature of class structure in data from 2 to 20 dimensions than almost any other graphics method. Hence there is a need for ways to select variables or subspaces to explore with the tour. Meth-ods such as principal component analysis are not well-suited for dimension reduction in classification problems (see [10]). The tour methods discussed here can be applied to data con-taining up to 1 million instances. The main problem with a large number of instances is that the rendering method, scatterplots, produces too much overplotting. 
Tours are defined for linear projections. Linear projec-tions are simple to understand, and there is a real-world analogy: shadows that objects make from a light source. They have the same strengths and weaknesses as shadows; we can see object curvature with a shadow but concavities and hollow, empty centers may be difficult to detect. Furnas &amp; Buja [12] discuss methods for detecting such structure us-ing sections or slices rather than projections. However this introduces considerable complexity and greatly increases the number of parameters needed to run an appropriate tour. The methods described in previous sections will work with non-linear SVM to some extent. It is possible to explore the pattern of the support vectors from a non-linear kernel, and it may be possible to detect a non-linear boundary in T~ N. However, using correlation between predicted values and ex-planatory variables may not accurately describe the relative importance of variables in non-linear SVM. Similarly explor-ing the instances with predicted values near zero may not be helpful because the relationship is non-linear. Rather a correlation tour could be used with predicted values plotted against combinations of explanatory variables to explore the non-linear dependencies. 
Lastly, the approach we have described is very labor inten-sive for the analyst. It cannot be automated because it relies heavily on the analyst's visual skills and patience for watch-ing rotations and manually adjusting projection coefficients X  However, it provides insights which we may not otherwise be able to make. However, in the quest for automating classifi-[7] D. Cook and A. Buja. Manual Controls For [8] D. Cook, A. Buja, J. Cabrera, and C. Hurley. Grand [9] C. Cortes and V. Vapnik. Support vector networks. [10] D. J. Donnell, A. Buja, and W. Stuetzle. Analysis of [11] M. Forina, C. Armanino, S. Lanteri, and E. Tiscornia. [12] G. Furnas and A. Buja. Prosection Views: [13] M. Hearst, B. Scholkopf, S. Dumais, E. Osuna, and [14] T. Joachims. Making Large-Scale SVM Learning [16] wgg. public, iastate, edu/~dicook/JSS/paper [17] Limn: w~.public.iastate.edu/ dicook/Limn 
