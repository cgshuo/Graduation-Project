 Sivan Sabato sivan.sabato@microsoft.com Adam Kalai adam.kalai@microsoft.com Microsoft Research New England, 1 Memorial Dr., Cambridge, MA In this paper we consider prediction with subjective, vague, or noisy attributes (which are also termed  X  X ea-tures X  throughout this paper). Such attributes can sometimes be useful for prediction, because they ac-count for an important part of the signal that cannot be otherwise captured. In a crowdsourcing setting, the  X  X isdom of crowds X  suggests that including multi-ple assessments of the same feature by different people may be useful. Henceforth, we refer to assessments of features as judgments . This paper introduces the prob-lem of selecting, from a set of candidate features, which ones to use for prediction, and how many judgments to acquire for each , for a given budget limiting the total number of judgments. We give theoretically jus-tified algorithms for this problem, and report crowd-sourced experimental results, in which judgments of highly subjective features (even culturally fraught ones such as attractive ) are helpful for prediction. As a toy example, consider the problem of estimating the number of jelly beans in a jar based on an image of the jar. A linear regressor with multiple judgments of features might have the form,  X  y =0 . 95(est. number of beans) / 5  X  50(round jar) / 2 Here, for binary attributes, a /r a  X  [0 , 1] denotes the fraction of positive judgments out of r a judgments of attribute a . For real-valued attributes, a /r a denotes the mean of r a judgments. The shape, number of col-ors, and attractiveness of the jar each help correct bi-ases in the estimated number of beans, averaged across five people. Our goal is to choose a regressor that, as accurately as possible, estimates the labels (i.e., jelly bean counts) on future objects (i.e., jars) drawn from the same distribution, while staying within a budget of feature judgment resources per evaluated object at test time. In the example above, notice that even though the monochromatic coefficient is greater than the beau-tiful coefficient, fewer monochromatic judgments are used, because counting the number of colors is more objective, and hence further judgments are less valu-able. While this example is contrived, similar phenom-ena are observed in the output of our algorithms. We refer to the problem of selecting the number of repetitions, r a , of each attribute, as the feature multi-selection problem, because it generalizes the feature selection problem of choosing a subset of features, i.e., r a  X  { 0 , 1 } , to choosing a multiset of features, i.e., r a  X  N . Since the feature selection problem is well known to be NP-hard (Natarajan, 1995), our prob-lem is also NP-hard in the general case. (For a for-mal reduction, one simply considers the  X  X bjective X  case where all judgments of the same feature-object pair are identical.) Nonetheless, several successful ap-proaches have been proposed for feature selection. The algorithms that we propose generalize two of these ap-proaches to the problem of feature multi-selection. Our algorithms are theoretically motivated, and tested on synthetic and real-world data. The real world data are photos extracted from the publicly available Pho-tographic Height/Weight Chart (Cockerham, 2013), where people post pictures of themselves announcing their own height and weight.
 As a more general motivation, consider a scientist who would like to use crowdsourcing as an alternative to themselves estimating a value for each of a large data set of objects. Say the scientist gathers multiple judg-ments of a number of binary or real-valued attributes for each object, and uses linear regression to predict the value of interest. In some cases, crowdsourcing is a natural source of judgments, as a great number of them may be acquired on demand, rapidly, and at very low cost. We assume the scientist has access to the following information:  X  A labeled set of objects ( o,y )  X  O  X Y (with  X  A crowd , which is a large pool of workers .  X  A possibly large set of candidate attributes A .  X  A budget B , limiting the number of attribute Our approach is as follows: 1. Collect k  X  2 judgments for each candidate at-2. Based on this data and the budget, decide how 3. Collect additional judgments (as needed) on the 4. Find a linear predictor based on the average judg-Step 4 can be accomplished by simple least-squares re-gression. The goal in Step 2 (feature multi-selection) is to decide on a number of judgments per attribute that will hopefully yield the smallest squared error af-ter Step 4.
 Interestingly, even given as few as k = 2 judgments per attribute, one can project an estimate of the squared error with more than k judgments of some features. We prove that these projections are accurate, for any fixed k  X  2, as the number of labeled objects increases. Our algorithms perform a greedy strategy for feature multi-selection, to attempt to minimize the projected loss. This greedy strategy can be seen as a generaliza-tion of the Forward Regression approach for standard feature selection (see e.g. Miller, 2002). The first al-gorithm operates under the assumption that different attributes are uncorrelated. In this case the projec-tion simplifies to a simple scoring rule, which incorpo-rates attribute-label correlations as well as a natural notion of inter-rater reliability for each attribute. In this case, greedy selection is also provably optimal. While attributes are highly correlated in practice, the algorithm performs well in our experiments, possibly because Step 4 corrects for a small number of poor choices during feature multi-selection. The second al-gorithm attempts to optimize the projection without any assumptions on the nature of correlations between features.
 While crowdsourcing is one motivation, the algorithms would be applicable to other settings such as learning from noisy sensor inputs, where one may place multi-ple sensors measuring each quantity, or social science experiments, where one may have multiple research as-sistants (rather than a crowd) judging each attribute. The main contributions of this paper are: (a) introduc-ing the feature multi-selection problem, (b) giving the-oretically justified feature multi-selection algorithms, and (c) presenting experimental results, showing that feature multi-selection can yield more accurate regres-sors, with different numbers of judgments for different attributes. Proofs of results and additional experimen-tal data are provided in Sabato &amp; Kalai (2013). Related work spans a number of fields, including Statistics, Machine Learning, Crowdsourcing, and measurement in the social sciences. A number of researchers have studied attribute-efficient prediction (also called budgeted learning ) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for in-stance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each at-tribute is judged at most once. The errors-in-variables approach (e.g., Cheng &amp; Van Ness, 1999) in statistics estimates the  X  X rue X  regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements.
 A wide variety of techniques have been studied to com-bine estimates of experts or the crowd of a single quan-tity of interest (see, e.g. Dawid &amp; Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses. Two recent works on crowdsourcing are very rele-vant. Patterson &amp; Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments. Some of their attributes are subjective, e.g., soothing . We employ their crowdsourcing protocol to label our binary attributes. Isola et al. (2011) study subjec-tive and objective features for the task of estimating how memorable an image is, by taking the mean of 10 judgments per attribute for each image. They per-form greedy feature selection over these attributes to find the best compact set of attributes for predicting memorability. The key difference between their algo-rithm and ours is that theirs does not choose how many judgments to average. Since that quantity is fixed for each attribute, their setting falls under the more stan-dard feature selection umbrella. In our experiments we compare this approach to our algorithms.
 Finally, in the social sciences, a wide array of tech-niques have been developed for assessing inter-rater reliability of attributes, with the most popular perhaps being the  X  coefficient (Cronbach, 1951). A principal use of such measures is determining, by some thresh-old, which features may be used in content analysis. For an overview of reliability theory, see (Krippendorff, 2012). Let there be d candidate attributes called A = [ d ] = { 1 , 2 ,...,d } . We assume that, for any object o and attribute a , there is a distribution over judgments P [ X [ a ] | O = o ], and we assume that the judgments of attribute-object pairs are conditionally independent given the sets of attributes and objects. This rep-resents an idealized setting in which a new random crowd worker is selected for each attribute-object judg-ment (In our experiments, we limit the total amount of work that any one worker may perform). We assume a distribution D over labeled objects, where labels are real numbers. We denote by D O the marginal dis-tribution over objects drawn according to D . We let P [ X [ a ]] = P O  X  X  O [ X [ a ] | O ]]. Labels y are assumed to be real valued. As is standard, we assume one  X  X rue X  label y i for each object o i .
 For notational ease, we assume that in the feature multi-selection phase, exactly k  X  2 judgments for each feature are collected. Our analysis trivially gen-eralizes to the setting in which different attributes are judged different numbers of times. Finally, each at-tribute a is assumed to have an expected value of E [ X [ a ]] = 0, where the expectation is taken across objects and judgments of a . This is done for ease of presentation, so that we do not have to track the mean vectors as well as the variance. When discussing im-plementation details, we describe how to remove this assumption in practice without loss of generality. Vectors will be boldface, e.g., x = ( x [1] ,...,x [ d ]), ran-dom variables will be capitalized, e.g., X , and matrices will be in black-board font, e.g., X . The i  X  X h standard unit vector is denoted by e i .
 Let r  X  N d represent the number of judgments for each feature, so that attribute a is judged r [ a ] times, and we represent the object X  X  judgments by x , defined as: where x [ a ]( j ) is the j th judgment of attribute a in x , j . We say that r is the repeat vector of x . We de-note the set of all possible representations with repeat vector r by R [ r ] .
 We denote by D r the distribution which draws ( X ,Y )  X  R [ r ]  X  R by first drawing a labeled object ( O,Y ) from D , and then drawing a random repre-sentation X  X  R [ r ] for this object. We denote by D  X  the distribution that draws ( X ,Y ) where X  X  R d by first drawing ( O,Y ) from D and then setting X [ a ] = E [ X [ a ] | O ]. We denote the expectation For k  X  2, let k = ( k,k,...,k )  X  N d be the re-peat vector used in the first training phase. The fea-ture multi-selection algorithm receives as input a la-beled training set S = (( x 1 ,y 1 ) ,..., ( x m ,y m )) where x i  X  R kd and y i  X  R , drawn from D k . This sample is generated by first drawing a set of labeled objects (( o 1 ,y 1 ) ,..., ( o m ,y m )) i.i.d. from D , and then draw-ing a random representation x i for object o i . The algo-rithm further receives as input a budget B  X  N , which specifies the total number of feature judgments allowed for each unlabeled object at test (i.e., prediction) time. The output of the algorithm is a new vector of repeats r  X  R B , where, Let o be an object with a true label y , and let  X  y be a prediction of the label of o . The squared loss for this prediction is ` ( y,  X  y ) = ( y  X   X  y ) 2 . Given a function f : Z  X  R for some domain Z , and a distribution D over Z  X  R , we denote the average loss of f on D by The final goal of our procedure is to find a predictor with a low expected loss on labeled objects drawn from D . This predictor must use only B feature judgments for each object, as determined by the test repeat vector r . We consider linear predictors w  X  R d that operate on the vector of average judgments of x  X  R [ r ] , defined as follows: For an input representation x , the predictor w predicts the label  X  w ,  X  x  X  . For vector v  X  R d , we denote by Diag( v )  X  R d  X  d the diagonal matrix with v [ a ] in the a th position.
 For a vector r  X  N d and a matrix S  X  R d  X  d , we denote by sub r ( S ) the submatrix of S resulting from deleting all rows and columns a such that r [ a ] = 0. For a vector, sub r ( u ) omits entries a such that r [ a ] = 0. Here sub r ( u )  X  R d 0 and sub r ( S )  X  R d 0  X  d 0 , where d the support size of r . We denote the pseudo-inverse of a matrix A  X  R n  X  n (see e.g. Ben-Israel &amp; Greville, 2003) by A + . The input to a feature multi-selection algorithm is a budget B and m labeled examples in which each at-tribute has been judged k times, and the output is a repeat vector r  X  R B . Our ultimate goal is to find r and a predictor w  X  R d such that ` ( w ,D r ) is min-imal. We now give intuition about the derivation of the algorithms, but their formal definition is given in Alg. 1.
 Define the loss of a repeat vector to be ` ( r )  X  min w  X  R d ` ( w ,D r ). The goal is to minimize ` ( r ) over r  X  R B . We give two forward-selection algorithms, both of which begin with r = (0 ,..., 0) and greedily increment r [ a ] for a that most decreases an estimate of ` ( r ). The key question is how does one estimate this projected loss ` ( r ) since the number of judgments can exceed k . We simplify notation by first considering only r which are positive , i.e., r [ a ]  X  1 for each a . We will shortly explain how to handle r [ a ] = 0. Define We call b [ a ] the correlation of a with the label. Note that b = E k [  X  X Y ], since linearity of expectation im-plies that b does not depend on k . Straightforward calculations show that, for any positive repeat vector r , If  X  r is non-singular, 2 ` ( r ) = min Since E [ Y 2 ] does not depend on r , minimizing ` ( r ) is equivalent to maximizing b T  X   X  1 r b (for positive r and nonsingular  X  r ). 3.1. A Scoring Algorithm The first algorithm that we propose is derived from the zero-correlation assumption, that E [ X [ a ] X [ a 0 ]] = 0 for a 6 = a 0 , or equivalently that the covariance matrix is diagonal. Perhaps the simplest approach to standard feature selection is to score each feature independently, based on its normalized empirical correlation with the label, and to select the B top-scoring features. If fea-tures are uncorrelated and the training sample is suffi-ciently large, then this efficient approach finds an opti-mal set of features. The feature multi-selection scoring algorithm that we propose henceforth is optimal un-der similar assumptions, however it is complicated by the fact that we may include multiple repetitions of each feature. Under the zero-correlation assumption,  X  r is diagonal, and its a th element, for r [ a ] &gt; 0, can be expanded as We refer to v [ a ] as the internal variance as it measures the  X  X nter-rater reliability X  of a , and we call  X  2 [ a ] the external variance as it is the inherent variance between examples. Hence for a diagonal  X  r , simple manipula-tion gives, Therefore, when  X  r is diagonal, minimizing the pro-jected loss is equivalent to maximizing the RHS above, a sum of independent terms that depend on the corre-lation and on the internal and external variance of each attribute, all of which can be estimated just once, for all possible repeat vectors. As one expects, greater cor-relation indicates a better feature, while a greater ex-ternal variance indicates a worse feature. A larger in-ternal variance indicates that more repeats are needed to achieve prediction quality. To estimate Eq. (1) we estimate each of the compo-nents on the RHS. Unbiased estimation of b is straight-forward, and unbiased estimation of v is also possible for k  X  2 samples per object, though importantly one should use the unbiased variance estimator,  X  v [ a ] =
VarEst(  X  1 ,..., X  n )  X  Using these estimates of v , we estimate the external variance using the equality  X  2 [ a ] = E k (  X  X [ a ]) A slight complication arises here, as this estimate might be negative for small samples, so we round it up to 0 when this happens. Another issue might seem to arise when the denominator of one of the summands in Eq. (1) is zero, however note that this can only oc-cur if both the internal and the external variance are zero, which implies that the feature is constantly zero, thus zeroing its correlation as well. The same holds for the estimated ratio. In such cases we treat the ratio as equal to 0. 3.2. The Full Multi-Selection Algorithm The scoring algorithm is motivated by the assumption of zero correlation between features. However, this assumption rarely holds in practice. Building on and paralleling the definitions and derivation above, the Full Algorithm similarly maximizes b T  X   X  1 r b without this assumption. For positive r , one has Where  X   X  E  X  [ X T X ] is the external covariance ma-trix, and we estimate it based on the equality  X  =  X  k  X  Diag( v ) /k . Just as in the Scoring algorithm, the estimates of  X  2 [ a ] might be negative, in the full algo-rithm it is possible that the estimate of  X  will not be positive semi-definite, so we analogously  X  X ound up X  our estimate of  X  to the nearest PSD matrix (see im-plementation details below). The estimate when some of the r [ a ] X  X  are zero is formed by deleting the corre-sponding entries in the estimate of b and the corre-sponding rows and columns in the estimate of  X  r . 3.3. Guarantees Under our distributional assumptions, we show that the estimated objective functions used by our algo-rithms converge to E [ Y 2 ]  X  ` ( r ). Thus maximizing the estimated objective approximately minimizes ` ( r ). Formally, let  X  obj f ( r ) and  X  obj s ( r ) be the objectives Algorithm 1 Feature multi-selection algorithms 1: Input: Budget B ; (( x 1 ,y 1 ) ,..., ( x m ,y m ))  X  2: Output: A repeat vector r  X  R B . 5:  X  v [ a ]  X  1 m P i VarEst( x i [ a ](1) ,...,x i [ a ]( k )). 6: if Scoring Algorithm then 7:  X  a  X  A,  X   X  2 [ a ]  X  max n 0 , 1 m P i (  X  x i [ a ]) 9: else 10:  X   X   X  MakePSD 1 m P i  X x T i  X x i  X  Diag(  X v ) /k ) 12: Define  X  obj( r )  X  sub r (  X  b ) T M + r sub r (  X  b ) 13: end if 14: r 0  X  (0 ,..., 0)  X  N d 15: for t = 1 to B do 16: Find i best  X  [ d ] such that  X  obj( r t  X  1 + e i ) is max-18: end for 19: Return r B . used in Alg. 1 for the full algorithm and the Scoring algorithm, respectively. Note that these objectives are implicitly functions of the training sample S . For a symmetric matrix S , let  X  min ( S ) be the smallest eigen-values of S . We define:  X  = min r  X  R B  X  min (sub r ( X )), and  X  B = min( B,d ).
 Theorem 3.1. Suppose that all judgments and labels are in [  X  1 , 1] . Then for any  X   X  (0 , 1) , with prob. at least 1  X   X  over m i.i.d. training samples from D k , for all r  X  R B , for m  X   X   X (  X  B ln(  X  Bd/ X  ) / X  2 ) we have If the external covariance matrix  X  is diagonal, then for m  X   X   X (ln( d/ X  ) / X  2 ) we have The convergence rate for the full algorithm stems from two bounds: (1) If the norm of the minimizing w is at most  X  , then the convergence rate is at most  X  B X  2 / minimizing w is at most of O (  X  B ln( Bd )) gets uniform convergence over r  X  R The components of this result are of the same order as the equivalent results for uniform convergence of standard least-squares regression. An improved rate of p  X 
B X  2 /m can be achieved for least-squares regression, if the algorithm exactly minimizes the sample squared loss (Srebro et al., 2010). However, our algorithm min-imizes another objective, thus this result is not directly applicable. We leave it as a challenge for future work to find out whether a faster rate can be achieved in our case.
 As always, these convergence rates are worst-case, and in practice a much smaller sample size is often suffi-cient to get meaningful results, as we have observed in our experiments. However, if the available training sample is too small to achieve reasonable results, one can limit the norm of the minimizer by adding regular-ization to the estimated covariance matrix, as in ridge regression (Hoerl &amp; Kennard, 1970). This would allow faster convergence at the expense of a more limited class of predictors.
 As Theorem 3.1 shows, when the zero-correlation as-sumption holds, the Scoring algorithm enjoys a much faster worst-case rate of convergence than the full algo-rithm. This is because it does not attempt to estimate the entire covariance matrix. This advantage is more significant for larger budgets. An additional advantage is that it finds the optimal value of r for its estimated objective: Theorem 3.2. The Scoring algorithm returns r  X  Theorem 3.2 follows since f ( r ) = a/ ( b + c/r ) is concave and increasing in r and from the following observation. Lemma 3.3. Let r  X  N d , and f ( r ) = P i  X  [ d ] g i ( r [ i ]) , where g i (  X  ) : R +  X  R are monotonic non-decreasing concave functions. Let B  X  N . The maximum of f ( r ) subject to r  X  R B is attained by a greedy algorithm which starts with r = (0 ,..., 0) , and iteratively in-creases the coordinate which increases f the most. 3.4. Implementation If our estimate of  X  is not PSD, we use the proce-dure  X  X akePSD X , which takes a symmetric matrix A as input, and returns the PSD closest to A in Frobe-nius norm. This can be done by calculating the eigen-value decomposition A = UDU T where U is orthogonal and D is diagonal, and returning U  X  DU T , where  X  D is with zeroed negative entries (Higham, 1988). If we assume a diagonal external covariance, then this pro-cedure is equivalent to rounding up the estimate of  X  ( a ) to zero, as done in the Scoring algorithm. For a budget of B , the full algorithm performs Bd SVDs to calculate pseudo-inverses. Note, however, that the largest matrix that might be decomposed here is of size min( d,B )  X  min( d,B ). Furthermore, in practice the matrices can be much smaller, since the algorithm might choose several repeats of the same features. In our experiments, the total time for decompositions, using standard libraries on a standard personal com-puter, has been negligible.
 Our description of the algorithms above assumes for simplicity that the mean of all features is zero. In practice, one adds a  X  X ree X  feature that is always 1, to allow for biased regressors. For the Scoring algorithm, one should further subtract the empirical mean from each feature. For the full algorithm, this not necessary, because when bias is allowed, adding a constant to any feature provably will not change the output of the full algorithm. We tested our approach on three regression problems. In the first problem the feature judgments were sim-ulated. In the second and third problem they were collected from the crowd using Amazon X  X  Mechanical Turk. 3 For the simulated experiment we used the UCI dataset  X  X elative location of CT slices on axial axis Data Set X  (Frank &amp; Asuncion, 2010). Here the features are histograms of spatial measurements in the image, and the label to predict is the relative location of the image on the axial axis. To simulate features with varying judgments, we collapsed each set of 8 adjacent histogram bins into a single feature, so that each judg-ment of the new feature was randomly chosen out of 8 possible values for this feature. The resulting dataset contained 48 noisy real-valued features per example. The second and third problems were to predict the height and weight of people from a photo. 880 photos with self-declared height and weight were extracted from the publicly available Photographic Height/Weight Chart (Cockerham, 2013), where peo-ple post pictures of themselves announcing their own height and weight. We chose 37 attributes that we felt the crowd could judge and might be predictive. We collected judgments for these binary attributes, mainly following the collection methodology of Pat-terson &amp; Hays (2012), by batching the images into groups of 40, to make judging efficient. To encourage honest workers, we promised (and delivered) bonuses for good work. We limited the amount of work any one person could do. We used all of the collected judgments, regardless of whether the workers received bonuses for them or not. Our pay per hour was set to average to minimum wage. We collected numeri-cal estimates of the height and the weight in a simi-lar fashion. Binary judgments took about one second per judgment and their cost was a fraction of a cent per attribute judgment. The numerical estimates took about four times as long and we paid four times as much for them. Accordingly, we adjusted all the algo-rithms to count a single numerical judgment as equal to four binary attribute judgments. Figure 1 and Fig-lected attributes. These plots demonstrate that all combinations of useful/non-useful and stable/noisy at-tributes exist in this data. The full listing of attribute properties is provided in (Sabato &amp; Kalai, 2013). We compared the test error of our algorithms, de-noted  X  X ull X  and  X  X coring X  in the plots, to those of sev-eral plausible baselines. In all comparisons, we set k = 2. The first baseline, denoted  X  X verages X  in the plots, is based on the  X  X redictive X  feature selection al-gorithm of Isola et al. (2011): We average the 2 judg-ments per attribute to create a standard data set with one value for each object-attribute pair, and greed-ily add attributes, one at a time, so as to minimize the least-squares error. The resulting regressor uses 2 judgments for each selected feature. The second base-line, denoted  X  X opies X , treats the 2 judgments of each feature-object pair as 2 different individual attributes, and again performs greedy forward selection on these features, except that the order of selecting the copies in each feature was fixed, to avoid over-fitting. The test repeat vector r was set according to the number of copies selected for each feature. These baselines perform standard Machine Learning feature selection:  X  X verages X  considers d features and  X  X opies X  consid-ers 2 d features. For height and weight prediction, we compared the results also to the test error achieved by averaging only the height or weight estimates of the crowd, respectively. Since each numerical feature costs 4 times as much as a binary feature, we averaged over B/ 4 numerical judgments when the budget was set to B . We did not use regularization anywhere, thus our algorithms and the baselines are all parameter-free. The test error in the plots was obtained as follows: r was selected based on a training set with k judg-ments. We then added judgments to features in the training set to get to r repeats. Finally we performed regular regression on the means of the enhanced train-ing set to get a predictor. This predictor was used to predict the labels of the test set with r judgments. We averaged results over 50 random train/test splits. Figures 3 and 4 show that our full algorithm achieved better test error than the baselines. The Scoring al-gorithm was usually somewhat worse than the Full Multi-Selection algorithm, and for small budgets also sometimes worse than the baselines. This is expected due to its zero-correlation assumption. However, when the sample size was small, the Scoring algorithm was sometimes better (see third plot in Fig. 4), since it suffered from less over-fitting. This is consistent with our convergence analysis in Theorem 3.1. Analysis of training errors indicates that baseline algorithms suf-fer for two different reasons: (1) they are limited to a small number of repeats per feature; and (2) they suffer from greater over-fitting, probably since our al-gorithm tends to select a sparser r than do the base-lines. We list examples of predictors learned by the full algorithm in (Sabato &amp; Kalai, 2013).
 In our last experiment we tested the tradeoff between no. of training judgments per feature and no. of train-ing examples, in the following setting: Suppose we have a budget that allows us to collect a total of M judgments for training the feature multi-selection al-gorithm, and we have access to at least M/ 2 d labeled examples. We can decide on a number k of judgments per feature, randomly select M/kd objects from our labeled pool to serve as the training set, and obtain kd judgments for each of these objects. What num-ber k should we choose? Does this number depend on the total budget M ? We compared the test error aris-ing from different values of k over different values of M , for the slice dataset using both of our algorithms,. The results are shown in Figure 3. These results show a clear preference for a small k (which allows a large m on the same budget M ). Characterizing the optimal number k is left as an open question for future work. We introduce the problem of feature multi-selection and provide two algorithms for regression with mean averaging of judgments. Future directions include other learning tasks, such as classification, and other types of feature aggregation, such as median averag-ing (majority). An additional important question for future work is how to carry out feature multi-selection in an environment with a changing crowd.
 Acknowledgments . We wish to thank Edith Law and Haoqi Zhang for several helpful discussions. Ben-Israel, A. and Greville, T.N.E. Generalized inverses: theory and applications , volume 15. Springer, 2003.
 Cesa-Bianchi, N., Shalev-Shwartz, S., and Shamir, O. Efficient learning with partially observed attributes.
J. Mach. Learn. Res. , 12:2857 X 2878, November 2011.
 Cheng, C. and Van Ness, J.W. Statistical regression with measurement error , volume 6. Arnold London, 1999.
 Cockerham, R. The photographic height and weight chart. http://www.cockeyed.com/photos/ bodies/heightweight.html , 2013. With permis-sion of Rob Cockerham.
 Cronbach, L. Coefficient alpha and the internal struc-ture of tests. Psychometrika , 16(3):297 X 334, 1951. Dawid, A. P. and Skene, A. M. Maximum likelihood estimation of observer error-rates using the em al-gorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics) , 28(1):pp. 20 X 28, 1979. Frank, A. and Asuncion, A. UCI machine learning repository, 2010. URL http://archive.ics.uci. edu/ml .
 Higham, N. J. Computing a nearest symmetric posi-tive semidefinite matrix. Linear algebra and its ap-plications , 103:103 X 118, 1988.
 Hoerl, A. E. and Kennard, R. W. Ridge regression: Bi-ased estimation for nonorthogonal problems. Tech-nometrics , 12(1):55 X 67, 1970.
 Isola, P., Parikh, D., Torralba, A., and Oliva, A. Un-derstanding the intrinsic memorability of images. In
Advances in Neural Information Processing Systems 24 , pp. 2429 X 2437, 2011.
 Krippendorff, K. Content analysis: An introduction to its methodology . Sage Publications, Incorporated, 2012.
 Miller, A. Subset selection in regression . Chapman &amp; Hall/CRC, 2002.
 Natarajan, B.K. Sparse approximate solutions to lin-ear systems. SIAM journal on computing , 24(2): 227 X 234, 1995.
 Patterson, G. and Hays, J. Sun attribute database:
Discovering, annotating, and recognizing scene at-tributes. In Proceeding of the 25th Conference on
Computer Vision and Pattern Recognition (CVPR) , 2012.
 Sabato, S. and Kalai, A. Feature multi-selection among subjective features. CoRR , abs/1302.4297, 2013. URL http://arxiv.org/abs/1302.4297 .
 Smyth, P., Fayyad, U. M., Burl, M. C., Perona, P., and Baldi, P. Inferring ground truth from subjective labelling of venus images. In NIPS , pp. 1085 X 1092, 1994.
 Srebro, N., Sridharan, K., and Tewari, A. Smoothness, low-noise and fast rates. In Advances in Neural In-formation Processing Systems (NIPS) 23 , 2010. Welinder, P., Branson, S., Belongie, S., and Perona, P. The multidimensional wisdom of crowds. In
Neural Information Processing Systems Conference
