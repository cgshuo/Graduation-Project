 The recent boom of weblogs and social media has attached increasing importance to the identification of suspicious users with unusual behavior, such as spammers or fraudulent re-viewers. A typical spamming strategy is to employ multiple dummy accounts to collectively promote a target, be it a URL or a product. Consequently, these suspicious accounts exhibit certain coherent anomalous behavior identifiable as a collection. In this paper, we propose the concept of Co-herent Anomaly Collection (CAC) to capture this kind of collections, and put forward an efficient algorithm to simul-taneously find the top-K disjoint CACs together with their anomalous behavior patterns. Compared with existing ap-proaches, our new algorithm can find disjoint anomaly col-lections with coherent extreme behavior without having to specify either their number or sizes. Results on real Twit-ter data show that our approach discovers meaningful and informative hashtag spammer groups of various sizes which are hard to detect by clustering-based methods.
 H.2.8 [ Database applications ]: Data mining Algorithms, Design, Experimentation Anomaly/Outlier Detection, Anomaly Collection/Cluster
The recent boom of weblogs and social media has pro-vided an unprecedented degree of freedom for ordinary users to generate content online. At the same time, the open-ness of the platforms leaves them highly susceptible to user abuse, and, even worse, the sheer volume of the generated data makes it infeasible to manually inspect their veracity. To find trustworthy information in these data, it is increas-ingly important to automatically identify suspicious users with unusual behavior, which are in many cases spammers or fraudulent reviewers. In real life, in order to draw attention amid this information swamp, spammers rarely operate with just a single account. Instead, they typically employ multi-ple dummy accounts to collectively promote certain targets, such as a URL or a product. For example, in Twitter, a group of users may collaboratively spam on popular hash-tags to promote their websites or businesses. Their strategy is to post a large number of tweets containing both their ad-vertisement content and the popular hashtags, so that other users querying any of these hashtags would see their spam-ming tweets. These activities are classified as spamming ac-cording to Twitter X  X  rules 1 . Figure 1 shows three collections of real spammers in Twitter detected by our approach.
These observations show that the key to detecting suspi-cious collaborative accounts is to identify their shared anoma-lous behavior patterns. We call such a user group a Coherent Anomaly Collection(CAC) , and propose an information the-ory based definition to characterize it.

Few existing studies have focused on collective anomaly detection [2], [4], [7], [5], and [8]. Furthermore, their frame-works are not appropriate for collective anomalies of extreme behaviors. The concept of an anomaly collection with ex-treme behavior was introduced in [3], which proposed algo-rithms to find top-K anomaly collections no greater than a user-specified size. However, in that work the anomaly col-lections are not optimized for the coherence in their unusual behavior, resulting in multiple spammer groups clustered in the same collection. Furthermore, the top-K anomaly col-lections heavily overlap one another and are size-bounded by user-specified constraints, offering limited information on the true anomaly collections in a data set. In this paper, we propose to simultaneously find top-K disjoint coherent anomaly collections together with their anomalous behav-ior patterns, without having to specify either the number or sizes of the target collections.

Our contributions are summarized as follows: h ttp://support.twitter.com/articles/18311-the-twitter-rules e e e age frequency of four hashtags (labeled as f 1 to f 4 ). S 1 = { e 1 , e 2 , e 3 } = { blackberrypros, randomwire-FreeCam, SEXsheylaPOrn, SHEYLLAsexPORN, LOVEsexCamFRee4 } and S
The rest of the paper is organized as follows. We formu-late our problem in Section 2. The algorithm is presented in Section 3, and Section 4 reports on experiments. We con-clude in Section 5.
To define a coherent anomaly collection, we show how to measure the anomalousness for a given collection in Section 2.1 and its coherence in Section 2.2, followed by the problem definition.
As shown in [3], capturing a collection of anomalies re-quires a measure of the anomalousness of multiple entities as a collection. We adopt the definitions in [3] to measure the anomalousness of a collection in the following exposition. The advantage of this definition is that it is defined directly at the collection level, which is different from measuring in-dividually on entity level followed by aggregating over the whole collection.

Denote the entity universe as E , and the feature universe as F . When E is ranked by a feature of F , extremity index r (1  X  r &lt; | E | / 2) is defined to indicate the top r positions in the ranking. Given a set of entities S  X  E , a feature f  X  F and r , the extreme subset of S denoted as E f ( S, r ) is the set of entities in S which appear in top-r positions w.r.t. feature f . For a given r , the extremity of S is quantified by the cardinality of E f ( S, r ), denoted as i . It turns out that i is a random variable following the hypergeometric distribution. This is because if a set S is randomly picked from | E | ranked entities, the number of entities in S that appear in top r positions follows the hypergeometric distribution. Thus the probability of observing i entities of S appearing in top r positions is prob ( i, | E | , r, | S | ) =
Th e p-value of S w.r.t. extremity index r and feature f , denoted as p f ( S, r ), is the probability of observing at least i entities of a random collection S appearing in top r positions w.r.t. f . Thus, p f ( S, r ) =
By definition, S has different p-values, each corresponding to a different r . For any given r and f , the smaller the p-value of S , the more anomalous or extremely ranked S is. Therefore, among all the choices of r , pick the one which gives the smallest p-value that S could possibly have as the representative extremity index of S , denoted as b r f ( S ). Correspondingly, the representative p-value of S w.r.t. f is denoted as b p f ( S ), i.e., b p f ( S ) = p f ( S, b r Formally, an anomaly collection is defined as follows.
Definition 1. Given an entity universe E and an en-tity set S , S  X  E , a set of independent features F and a threshold  X  , S is an Anomaly Collection (AC) w.r.t. F if (I)  X  F S  X  F such that  X  f  X  F S , b p f ( S )  X   X  ; (II) 1 &lt; | S | &lt; | E | / 2 ; (III) | F S | &gt; 1 .
The condition 1 &lt; | S | &lt; | E | / 2 is imposed, as an anomaly collection should contain more than one entity and yet re-main the minority of the population. The condition | F S | &gt; 1 requires that S is significant in at least two statistical tests. F
S is called the significant features of S . The definition also requires a set of independent features F . The indepen-dency of any two features is defined by statistics including Kendall Tau rank correlation coefficient [6].

As the representative p-value measures how anomalous an AC is for a single feature, the anomaly score of an AC S for F , denoted as  X ( S, F ), is defined as the prod-uct of the representative p-values for significant features. As the resulting score is usually small, take the log form  X ( S, F ) =  X  S is significant and the more extremely ranked S is w.r.t. each of them, the larger anomaly score S has.
By definition, the anomaly score of an entity collection is determined by the subset of its members which are most extremely ranked w.r.t. some features. It is possible that different subsets of members are extremely ranked w.r.t. dif-ferent feature subsets. However, for many applications, we are most interested in ACs whose members are extremely ranked w.r.t the same set of features. For instance, in our Twitter example in Figure 1, we prefer to identify S 1 , S and S 3 as three different ACs instead of consider them as a single AC.

To capture this important notion of coherence in our prob-lem definition, we formally define  X  X oherence X  by first rep-resenting an AC in a matrix form and using the matrix en-coding cost from information theory in [1] to evaluate the coherence of the AC.

For a given AC S and its significant feature set F S , we de-note E ( S, F S ) as the members of S that appear in the posi-tions indicated by the representative extremity index of any significant feature, i.e., E ( S, F S ) =
To tell how coherent an AC is, we represent it by a | F S b y | E ( S, F S ) | matrix. Specifically, given an AC S , its sig-nificant feature set F S and its extreme subset E ( S, F S with f a ,( a = 1 , ..., | F S | ) being the a -th feature in F e ,( b = 1 , ..., | E ( S, F S ) | ) being the b -th entity in E ( S, F the extreme matrix is M ( S ) = [ m ab ], where m
According to [1], any matrix can be encoded as one or multiple row and column clusters. The encoding cost is the sum of the code cost and description cost, where the first cost is for encoding each row and column cluster and the second cost is for describing the grouping information. If a matrix is highly homogeneous, e.g., containing all 1s or all 0s like M ( S 1 ), its encoding cost as one cluster is low. If a matrix is not homogeneous, e.g., containing multiple homogeneous clusters like M ( S 1  X  S 2 ), we should be able to find a minimum cost to encode this matrix by first encoding each homogeneous cluster within and then describing the grouping information of these clusters. Moreover, this cost is expected to be much lower than the cost of encoding the original matrix as one single cluster.

Definition 2. [CAC] Given an entity universe E and an entity set S , S  X  E , a set of independent features F and a threshold  X  , S is a Coherent Anomaly Collection (CAC) if (I) S is an anomaly collection; (II) the cost of en-coding M ( S ) as one cluster is lower than the minimum cost of encoding M ( S ) as multiple homogeneous clusters; (III) the number of 1s in its extreme matrix must be greater than half of the size of its extreme matrix.

We formally define our problem of detecting top-K dis-joint CACs as follows.

Definition 3. [TOPK CA C] Given K , the entity uni-verse E , a set F of independent features and the ranking of E on F , let S  X  = ( S 1 , S 2 , . . . , S N ) be the sequence of coherent anomaly collections ranked in descending order by their anomaly scores. The problem of TOPK CA C is to find the length-K disjoint subsequence  X  S of S  X  where  X  ( S  X  r  X  N , for 1  X  i &lt; j  X  K ; and (II) for any other length-K disjoint subsequence S 0 of S  X  where S 0 = ( S r 0 such that S r 0 j  X  K , there exists an index j, 1  X  j  X  K such that  X  r i for all 1  X  i  X  j .
We describe our algorithm for the TOPK C AC problem in this section. Conceptually, we first find the top-1 CAC and then find the next most anomalous CACs that do not overlap with any of the previously detected CACs, and so on so forth. The algorithm would mine the most anomalous CAC with the constraint of being disjoint with a set of entities C , C  X  E . We call this C the constraint set.

The high complexity of these exact algorithms leads us to propose heuristics to solve the TOPK CA C problem by sampling the candidate collections that are potentially more anomalous. We propose to sample candidates from small size to larger sizes. This is because, (I)anomalies are mi-norities and anomaly collections are in general of small sizes; (II)collections of larger sizes although may have larger anomaly scores but are less likely to be coherent. Before showing the first heuristic regarding sampling candidates of small size to large, we first define first-maximal CAC with constraint C as follows.
 top-1 CACs of size from 2 to | E | / 2  X  1, s.t. S i  X  C =  X  ,  X  2  X  i &lt; | E | / 2  X  1, the first-maximal CAC with constraint C is the S i , 2  X  i &lt; | E | / 2  X  1 such that (I)  X ( S  X ( S j +1 , F ), for all 1 &lt; j &lt; i ; (II)  X ( S i , F ) &gt;  X ( S Intuitively, first-maximal CAC with constraint C is the CAC that does not overlap with C and are more anomalous than all collections that are of smaller sizes. With this, our first heuristic is as follows.
 Heuristic 1. [first-maximal property of Top-K CACs] S i is the first-maximal CAC with constraint Al gorithm 1 CACD H , He uristically detecting top-K dis-joint CACs
I n step 4 and step 6 of Algorithm 1, we need to find the top-1 CAC of a given size n . However, the number of CACs of size n is | E | n in the worst case. We therefore propose the second heuristic regarding the importance of local extremity . Heuristic 2. [Importance of Local Extremity] Given E and F , let S be the top-1 CAC w.r.t. F . There exists a feature f  X  F and a small integer threshold  X  such that S is among the top- X  CACs w.r.t. f .
Recall that given an entity set S and a significant fea-ture f , the anomaly score of S w.r.t. f depends on the representative p-value of S w.r.t. f . If we can order the representative p-values of all possible collections of size n , and then find the set of collections corresponding to each of these p-values, we will be able to derive the collections with larger anomaly score w.r.t. f . In other words, the order-ing of collections by anomaly score can be derived from the ordering of p-values.

As stated in Section 2, given any collection S of size n , the p-value of S is determined by the extremity index r and i which is the number of entities in S that appear in top-r positions. Hence any p-value can be represented as p ( i, r, n ).
For all collections of size n , we could derive their represen-tative p-values by enumerating all possible ( i, r, n ) combina-tions with the constraints 1  X  r &lt; | E | / 2, 1  X  i  X  min ( r, n ). However, the total number of ( i, r, n ) combinations is large. Furthermore, we are more interested in those with the small p-values, as they indicate more anomalous collections. We therefore make use of the intrinsic partial orderings among p ( i, r, n ) values for deriving the next smallest p-value with-out enumerating all p-values. We define the p-value fron-tier of p ( i, r, n ) as the set of p-values that are the immediate smaller p-values according to each partial order. The partial orders lead to the following Lemmas for deriving the next smallest p ( i, r, n ) value.

Lemma 1. The anchor of column-n , i.e., p ( n, n, n ) , is the smallest p-value of all.

Lemma 2. Given any p ( i, r, n ) value, the next smallest p-value lies in the p-value frontier of p ( i, r, n ) or the frontiers of the p-values that are no greater than p ( i, r, n ) .
Now the question is how to derive the set of collections whose representative p-value is of a given p ( i, r, n ). For a given p ( i, r, n ), multiple collections may have p ( i, r, n ) as their representative p-value. In fact, the number of col-lections having the same representative p-value w.r.t. any feature can be as many as representative p-value, each feature has many correspond-ing collections. We therefore want to sample only a subset of these collections that have larger anomaly score not only w.r.t. a single feature, but also w.r.t. the whole feature set F .

Our idea is to select individual entities that are more anomalous w.r.t. F and construct the collections from them. Naturally, an individual entity e is more anomalous if its singular anomaly score , i.e.,  X ( { e } , F ) is larger. We hence take the heuristic that those collections whose ele-ments have larger sum of singular anomaly scores have larger anomaly scores. In other words, we approximate  X ( S, F ) by  X  e  X  S  X ( { e } , F ).

Next, we need a way of pinpointing individual entities in the entity list corresponding to each feature. Intuitively, we need n pointers, each points to an individual entity. We denote the list of rankings indicated by the n pointers as  X  . For a given feature f ,  X  uniquely indicates one size-n collection. We denote E f (  X  ) as the set of entities associated with  X  for f . Next, we form a collection from the first entity on each candidate entity list. We then find the collection that has the next largest sum of singular anomaly score.
Putting the ideas together, we have Algorithm 2 to heuris-tically compute the top-1 CAC of a given size n . It needs two additional parameters. The first parameter  X  c is used for selecting collections for a given p-value. Specifically, for p ( i, r, n ), we repeatedly pop a collection across multiple fea-tures and evaluate whether it is larger than the current top-1 CAC. If the top-1 CAC remains unchanged after  X  c number of times, we stop the collection selection process for p ( i, r, n ). As the algorithm searches progressively larger p-values from the small to large, the second parameter  X  p served as a ceil-ing on the number of p-values that have contributed no col-lections as the current top-1 CAC. The intuition is that if  X  number of p-values have not contribute any collection for the top-1 CAC, the unseen p-values which are even smaller are unlikely to be able to contribute collections.  X  c and  X  p be decided empirically. Larger  X  c and  X  p settings imply go-ing through more candidate collections, which necessitates a longer execution time. The setting of  X  c and  X  p will be studied in the experiments section.
 Al gorithm 2 Compute topCAC siz e ( E, F, n, C ) in algo-rithm 1 by sampling anomalous collections of size n
In this section, we present experimental studies of our approach on a Twitter dataset. We showcase the detected coherent spammer groups and compare our approach against an existing co-clustering algorithm. Data Setting.
 Our Twitter data 2 is composed of all the tweets published between September 8th 2011 and November 15th 2011, con-taining any of the 9 hashtags related to Singapore including #sg, #singapore and #sosingaporean . Altogether, there are 231,803 tweets from 21,666 users. With a total of 11,901 hashtags, a hashtag is used by 4.58 users on average. Ranked by the number of users, the top 5 percent of these hashtags are considered popular. We remove the rest of the hash-tags along with the ones that we used to collect the data. We also remove all the retweets, as hashtags in retweets do not indicate that the retweeting user is spamming on the hashtags. In addition, we filter away users who use only a hashtag only once, since they are unlikely to be spamming on hashtags. After the preprocessing, we are left with 1899 users with 587 popular hashtags, which is considered as in-dependent features. For each hashtag, we rank all the users in descending order by usage frequency. If a user never men-tions a particular hashtag, the corresponding feature value is zero. For each feature, rankings of users with identical feature values are randomized.
 Our Results.
 In this experiment, we empirically set  X  c =100 and  X  p =20 for our algorithm CACD H . Larger parameter values such as h ttp://research.larc.smu.edu.sg/palanteer/index tra cker.php  X  =500 and  X  p =50 have also been tried and give the same results with longer running time. We set K to a large value, so that CACD H sto ps when it finds all disjoint CACs.
As a result, CACD H p roduces 36 disjoint CACs. It is clear that members of a CAC collaborate in the same spamming campaign, as their tweets are often identical, with no real content other than a large number of hashtags appended with short URLs pointing to some website(s).

All members in the top-10 CACs are visualized in Figure 2 by parallel coordinates. Each member is represented by a line connecting the ranks of the user X  X  usage of all the 39 significant features( i.e., hashtags). Members of the same CAC are given the same color. It is visually telling that all the 10 CACs are both extreme and coherent in their us-age patterns of the hashtags. Moreover, our algorithm can identify subtle differences in the extreme behavior of CACs which seemingly belong to the same group. For example, the top-1st and top-2nd groups of  X  X ornographic X  spammers are in fact slightly different in their spamming patterns: (I) Besides the 8 hashtags in common, the top-1st group spams on #mongolia and #nepal while the top-2nd group spams on #brunei and #vietnam ; (II) the top-2nd CAC, despite hav-ing fewer members, uses most of the hashtags more heavily than the top-1st CAC.
 Comparison with Co-Clustering.
 As our method can simultaneously detect anomaly collec-tions and their corresponding significant features, one may suspect that similar results can be obtained by modeling the problem as a co-clustering task by clustering rows (features) and columns (users) of a matrix at the same time. We thus compare with the results of a co-clustering-based algorithm in [1]. This algorithm is chosen as it does not need the num-ber of clusters as input and is denoted as Co-clustering .
To apply the co-clustering algorithm, we need to first de-rive the input matrix. The direct way of representing the input matrix on this Twitter data is to give a value of 1 to a cell if the corresponding user has used this hashtag, and give a value of 0 otherwise. Consequently, we have a 587 (number of features) by 1899 (number of users) matrix to feed into the co-clustering algorithm [1]. In the result, the matrix is co-clustered into 8 feature groups and 10 user groups, the largest user group being of size 467 and the smallest of size 2. Of all the 10 user groups, none of them are coherent and only 5 of them can be considered as anomaly collections. We manually go through the 5 anomaly collections and find that they are anomalous only because they contain subsets of members that are ranked at extreme positions on a small number of features. This is not surprising as co-clustering aims to group users using similar sets of hashtags, not neces-sarily those who heavily use these hashtags. While identified behavior are shared, they are not necessarily anomalous.
Even if we take only the union of users ranked in the top positions of each feature, the co-clustering algorithm would not output some extremely ranked collections as expected. We choose the top-31 positions of each feature so that the input matrix contains information of all users in our top-10 CACs. Yet, out of the 10 user collections identified by co-clustering, none of them are both anomalous and coher-ent. The most anomalous collection returned are of size 124, which contains some of the pornographic spammers, and many other users that are not even sharing the same significant features with the pornographic spammers. The poor performance of the co-clustering is due to its treating every feature the same when trying to simultaneously group users and features. In contrast, our approach is able to iden-tify the significant features along with the anomalous users.
In this paper, we propose the problem of detecting top-K disjoint Coherent Anomaly Collections (CAC). We present an algorithm to identify CACs that does not need the col-lection number or collection size to be specified beforehand. Our algorithm is tested on a Twitter.com dataset to de-tect hashtag spammer collections. The experiment results demonstrate that our approach successfully finds suspicious spammer groups which are not easily identifiable with other approaches.
This research is supported by the Singapore National Re-search Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.

