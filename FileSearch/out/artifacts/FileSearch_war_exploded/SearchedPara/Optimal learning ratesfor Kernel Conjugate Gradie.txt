 Mathematics Institute, University of Potsdam sion in combination with conjugate gradient techniques. The goal is to estimate a regression function f  X  based on random noisy observations. We have an i.i.d. sample of n observations ( X i , Y i )  X  X  X  R from an unknown distribution P ( X, Y ) that follows the model We assume that the true regression function f  X  belongs to the space L 2 ( P X ) of square-integrable to find coefficients  X  such that the function defined by the normalized kernel expansion target f  X  is measured via the L 2 ( P X ) distance, k f squared error loss ` ( f, x, y ) = ( f ( x )  X  y ) 2 . erating distribution, and minimize the training squared error. This gives rise to the linear equation Assuming K n invertible, the solution of the above equation is given by  X  = K  X  1 n  X  , which yields well-known that to avoid overfitting, some form of regularization is needed. There is a considerable variety of possible approaches (see e.g. [10] for an overview). Perhaps the most well-known one is known alternatively as kernel ridge regression, Tikhonov X  X  regularization, least squares support vec-tor machine, or MAP Gaussian process regression. A powerful generalization of this is to consider where F  X  : R +  X  R + is a fixed function depending on a parameter  X  . The notation F  X  ( K n ) is to be interpreted as F  X  applied to each eigenvalue of K n in its eigen decomposition. Intuitively, F  X  should be a  X  X egularized X  version of the inverse function F ( x ) = x  X  1 . This type of regular-ization, which we refer to as linear regularization methods, is directly inspired from the theory of inverse problems. Popular examples include as particular cases kernel ridge regression, principal components regression and L 2 -boosting. Their application in a learning context has been studied extensively [1, 2, 5, 6, 12]. Results obtained in this framework will serve as a comparison yardstick in the sequel.
 In this paper, we study conjugate gradient (CG) techniques in combination with early stopping for the regularization of the kernel based learning problem (1). The principle of CG techniques is to restrict the learning problem onto a nested set of data-dependent subspaces, the so-called Krylov subspaces, defined as the K n -norm as k  X  k 2 K defined as f m := f  X  m . In the learning context considered here, regularization corresponds to early stopping. Conjugate gradients have the appealing property that the optimization criterion (5) can be computed only forward multiplication of vectors by the matrix K n . Algorithm 1 displays the computation of the CG kernel coefficients  X  m defined by (5).
 Algorithm 1 Kernel Conjugate Gradient regression Input kernel matrix K n , response vector  X  , maximum number of iterations m
Initialization :  X  0 = 0 n ; r 1 =  X ; d 1 =  X ; t 1 = K n  X  for i = 1 , . . . , m do end for Return: CG kernel coefficients  X  m , CG function f m = P n i =1  X  i,m k ( X i ,  X  ) The CG approach is also inspired by the theory of inverse problems, but it is not covered by the framework of linear operators defined in (3): As we restrict the learning problem onto the Krylov of degree  X  m  X  1 . However, the polynomial q m is not fixed but depends on  X  as well, making the CG method nonlinear in the sense that the coefficients  X  m depend on  X  in a nonlinear fashion. We remark that in machine learning, conjugate gradient techniques are often used as fast solvers in this paper, we study conjugate gradients as a regularization approach for kernel based learning, where the regularity is ensured via early stopping. This approach is not new. As mentioned in The latter method also restricts the learning problem onto the Krylov subspace K m ( X  , K n ) , but it minimizes the euclidean distance k  X   X  K n  X  k instead of the distance k  X   X  K n  X  k K n defined above 1 . Kernel Partial Least Squares has shown competitive performance in benchmark experiences (see e.g [18, 19]). Moreover, a similar conjugate gradient approach for non-definite kernels has been proposed and empirically evaluated by Ong et al [17]. The focus of the current paper is therefore not to stress the usefulness of CG methods in practical applications (and we refer to the above mentioned references) but to examine its theoretical convergence properties. In particular, we establish the existence of early stopping rules that lead to optimal convergence rates. We summarize our main results in the next section. For the presentation of our convergence results, we require suitable assumptions on the learning measurable. (This assumption is satisfied for all practical situations that we know of.) Further-sumptions on the noise : (Bounded) (Bounded Y ): | Y | X  M almost surely. The second assumption is weaker than the first. In particular, the first assumption implies that not only the noise, but also the target function f  X  is bounded in supremum norm, while the second assumption does not put any additional restriction on the target function.
 The regularity of the target function f  X  is measured in terms of a source condition as follows. The kernel integral operator is given by The source condition for the parameters r &gt; 0 and  X  &gt; 0 is defined by: We refer to r  X  1 / 2 as the  X  X nner case X  and to r &lt; 1 / 2 as the  X  X uter case X .
 The regularity of the kernel operator K with respect to the marginal distribution P X is measured in D  X  0 and the condition This notion was first introduced in [22] in a learning context, along with a number of fundamental analysis tools which we rely on and have been used in the rest of the related literature cited here. It is known that the best attainable rates of convergence, as a function of the number of examples n , are determined by the parameters r and s in the above conditions: It was shown in [6] that the minimax learning rate given these two parameters is lower bounded by O ( n  X  2 r/ (2 r + s ) ) . rule takes the form of a so-called discrepancy stopping rule: For some sequence of thresholds  X  m &gt; 0 to be specified (and possibly depending on the data), define the (data-dependent) stopping iteration It is not difficult to prove from (4) and (5) that k  X   X  K n  X  n k K stopping rule always has 2.1 Inner case without knowledge on effective dimension The inner case corresponds to r  X  1 / 2 , i.e. the target function f  X  lies in H almost surely. For sequence For technical reasons, we consider a slight variation of the rule in that we stop at step b m if q Denote Theorem 2.1. Suppose that Y is bounded (Bounded) , and that the source condition SC ( r,  X  ) holds for r  X  1 / 2 . With probability 1  X  2  X  , the estimator f stopping rule (7) satisfies We present the proof in Section 4. 2.2 Optimal rates in inner case We now introduce a stopping rule yielding order-optimal convergence rates as a function of the discrepancy stopping rule with the fixed threshold for which we obtain the following: Theorem 2.2. Suppose that the noise fulfills the Bernstein assumption (Bernstein) , that the source estimator f Due to space limitations, the proof is presented in the supplementary material. 2.3 Optimal rates in outer case, given additional unlabeled data We now turn to the  X  X uter X  case. In this case, we make the additional assumption that unlabeled stopping rule, except that the factor M is replaced by max( M,  X  ) .
 Theorem 2.3. Suppose assumptions (Bounded) , SC( r,  X  ) and ED( s, D ) , with r + s  X  1 2 . Assume unlabeled data is available with Then with probability 1  X  3  X  , the estimator f above satisfies A sketch of the proof can be found in the supplementary material. For the inner case  X  i.e. f  X   X  H almost surely  X  we provide two different consistent stopping obtained bound corresponds to the  X  X orst case X  with respect to this parameter (that is, s = 1 ). However, an interesting feature of stopping rule (7) is that the rule itself does not depend on the a priori knowledge of the regularity parameter r , while the achieved learning rate does (and with the optimal dependence in r when s = 1 ). Hence, Theorem 2.1 implies that the obtained rule is results obtained in [1] for linear regularization schemes of the form (3), (also in the case s = 1 ) for which the choice of the regularization parameter  X  leading to optimal learning rates required the knowledge or r beforehand.
 When taking into account also the effective dimensionality parameter s , Theorem 2.2 provides the order-optimal convergence rate in the inner case (up to a log factor). A noticeable difference to Theorem 2.1 however, is that the stopping rule is no longer adaptive, that is, it depends on the a regularization schemes of the form (2) in [6] and of the form (3) in [5], also rely on the a priori knowledge of r and s to determine the appropriate regularization parameter  X  .
 The outer case  X  when the target function does not lie in the reproducing Kernel Hilbert space H  X  is more challenging and to some extent less well understood. The fact that additional assumptions are techniques. Here we follow the semi-supervised approach that is proposed in e.g. [5] (to study linear regularization of the form (3)) and assume that we have sufficient additional unlabeled data in order to ensure learning rates that are optimal as a function of the number of labeled data. We remark that rates. For regularized M-estimation schemes studied in [20], availability of unlabeled data is not p  X  (0 , 1] . In [13], assumptions on the supremum norm of the eigenfunctions of the kernel integral operator are made (see [20] for an in-depth discussion on this type of assumptions). that approximate the solution of linear equations on Krylov subspaces. In the context of learning, our approach is most closely linked to Partial Least Squares (PLS) [21] and its kernel extension [18]. While PLS has proven to be successful in a wide range of applications and is considered one of the standard approaches in chemometrics, there are only few studies of its theoretical properties. In [8, 14], consistency properties are provided for linear PLS under the assumption that the target function f  X  depends on a finite known number of orthogonal latent components. These findings were recently extended to the nonlinear case and without the assumption of a latent components model [3], but all results come without optimal rates of convergence. For the slightly different CG approach studied by Ong et al [17], bounds on the difference between the empirical risks of the CG approximation and of the target function are derived in [16], but no bounds on the generalization error were derived. Convergence rates for regularization methods of the type (2) or (3) have been studied by casting kernel learning methods into the framework of inverse problems (see [9]). We use this framework for the present results as well, and recapitulate here some important facts. We first define the empirical evaluation operator T n as follows: and the empirical integral operator T  X  n as: T
T  X  n , and therefore k  X  k K normal equation (in H ) it can be interpreted as a  X  X erturbation X  of a population, noiseless version (of the equation and of are respectively replaced by their population analogues, the kernel integral operator and the change-of-space operator geometry  X  the inner product of H being defined by the kernel function k , while the inner product of L ( P X ) depends on the data generating distribution (this operator is well defined: since the kernel is bounded, all functions in H are bounded and therefore square integrable under any distribution P T
 X  , are close to the population covariance operator S = T  X  T and to the kernel integral operator applied to the noiseless target function, T  X  f  X  respectively.
 Proposition 4.1. Assume that k ( x, x )  X   X  for all x  X  X  . Then the following holds: where k . k HS denotes the Hilbert-Schmidt norm. If the representation f  X  = Tf  X  H holds, and under assumption (Bernstein) , we have the following: to H (remember that T is just the change-of-space operator). Hence, the second result (11) is valid for the case with r  X  1 / 2 , but it is not true in general for r &lt; 1 / 2 . 4.1 Nemirovskii X  X  result on conjugate gradient regularization rates We recall a sharp result due to Nemirovskii [15] establishing convergence rates for conjugate gradi-ent methods in a deterministic context. We present the result in an abstract context, then show how, combined with the previous section, it leads to a proof of Theorem 2.1. Consider the linear equation where A is a bounded linear operator over a Hilbert space H . Assume that the above equation has a an element  X  b  X  X  are known such that (with  X  and  X  known positive numbers). Consider the CG algorithm based on the noisy operator  X  A and data  X  b , giving the output at step m The discrepancy principle stopping rule is defined as follows. Consider a fixed constant  X  &gt; 1 and define constant such that  X  &lt; 1 / X  . Nemirovskii established the following theorem: some  X  &gt; 0 . Then for any  X   X  [0 , 1] , provided that 4.2 Proof of Theorem 2.1 We apply Nemirovskii X  X  result in our setting (assuming r  X  1 2 ): By identifying the approximate (13) is exactly (9), more precisely with the identification z m = f m .
 source condition, then there exists f  X  H  X  X  such that f  X  = Tf  X  H ).
 Condition (a) of Nemirovskii X  X  theorem 4.2 is satisfied with L =  X  by the boundedness of the kernel. satisfied with probability 1  X  2  X  , more precisely with  X  = 4  X   X  n q log 2  X  and  X  = 4 M we replaced  X  in (10) and (11) by  X / 2 , so that the two conditions are satisfied simultaneously, by the union bound). The operator norm is upper bounded by the Hilbert-Schmidt norm, so that the deviation inequality for the operators is actually stronger than what is needed.
 We consider the discrepancy principle stopping rule associated to these parameters, the choice  X  = 1 / (2  X  ) , and  X  = 1 2 , thus obtaining the result, since 4.3 Notes on the proof of Theorems 2.2 and 2.3 The above proof shows that an application of Nemirovskii X  X  fundamental result for CG regularization of inverse problems under deterministic noise (on the data and the operator) allows us to obtain our first result. One key ingredient is the concentration property 4.1 which allows to bound deviations in a quasi-deterministic manner.
 To prove the sharper results of Theorems 2.2 and 2.3, such a direct approach does not work unfor-tunately, and a complete rework and extension of the proof is necessary. The proof of Theorem 2.2 is presented in the supplementary material to the paper. In a nutshell, the concentration result 4.1 is too coarse to prove the optimal rates of convergence taking into account the effective dimension parameter. Instead of that result, we have to consider the deviations from the mean in a  X  X arped X  norm, i.e. of the form  X   X  introduced and used in [5, 6] to obtain sharp rates in the framework of Tikhonov X  X  regularization (2) and of the more general linear regularization schemes of the form (3). Bounds on deviations of this form can be obtained via a Bernstein-type concentration inequality for Hilbert-space valued random variables.
 On the one hand, the results concerning linear regularization schemes of the form (3) do not apply to the nonlinear CG regularization. On the other hand, Nemirovskii X  X  result does not apply to de-viations controlled in the warped norm. Moreover, the  X  X uter X  case introduces additional technical mental structure and ideas introduced by Nemirovskii, are significantly different in that context. As mentioned above, we present the complete proof of Theorem 2.2 in the supplementary material and a sketch of the proof of Theorem 2.3. In this work, we derived early stopping rules for kernel Conjugate Gradient regression that provide are adaptive with respect to the regularity of the target function in some cases. The proofs of our results rely most importantly on ideas introduced by Nemirovskii [15] and further developed by Hanke [11] for CG methods in the deterministic case, and moreover on ideas inspired by [5, 6]. Certainly, in practice, as for a large majority of learning algorithms, cross-validation remains the standard approach for model selection. The motivation of this work is however mainly theoretical, tion stands on equal footing with other well-studied regularization methods such as kernel ridge regression or more general linear regularization methods (which includes between many others L 2 boosting). We also note that theoretically well-grounded model selection rules can generally help cross-validation in practice by providing a well-calibrated parametrization of regularizer functions, or, as is the case here, of thresholds used in the stopping rule.
 One crucial property used in the proofs is that the proposed CG regularization schemes can be con-veniently cast in the reproducing kernel Hilbert space H as displayed in e.g (9). This reformulation is not possible for Kernel Partial Least Squares: It is also a CG type method, but uses the standard Euclidean norm instead of the K n -norm used here. This point is the main technical justification on why we focus on (5) rather than kernel PLS. Obtaining optimal convergence rates also valid for Kernel PLS is an important future direction and should build on the present work.
 the confidence parameter  X  . Currently, this dependence prevents us to go from convergence in high probability to convergence in expectation, which would be desirable. Perhaps more importantly, it would be of interest to find a stopping rule that is adaptive to both parameters r (target function regularity) and s (effective dimension parameter) without their a priori knowledge. We recall that linear regularization methods, the optimal choice of regularization parameter is also non-adaptive, in [7] for linear regularization methods (see also [4] for an account of the properties of hold-out in a general setup). We strongly believe that the hold-out method will yield theoretically founded adaptive model selection for CG as well. However, hold-out is typically regarded as inelegant in that selection methods that are based on using the whole data in the estimation phase. The application of Lepskii X  X  method is a possible step towards this direction. [1] F. Bauer, S. Pereverzev, and L. Rosasco. On Regularization Algorithms in Learning Theory. [2] N. Bissantz, T. Hohage, A. Munk, and F. Ruymgaart. Convergence Rates of General Regular-[3] G. Blanchard and N. Kr  X  amer. Kernel Partial Least Squares is Universally Consistent. Pro-[4] G. Blanchard and P. Massart. Discussion of V. Koltchinskii X  X   X  X ocal Rademacher complexities [5] A. Caponnetto. Optimal Rates for Regularization Operators in Learning Theory. Technical [6] A. Caponnetto and E. De Vito. Optimal Rates for Regularized Least-squares Algorithm. Foun-[7] A. Caponnetto and Y. Yao. Cross-validation based Adaptation for Regularization Operators in [8] H. Chun and S. Keles. Sparse Partial Least Squares for Simultaneous Dimension Reduction [10] L. Gy  X  orfi, M. Kohler, A. Krzyzak, and H. Walk. A Distribution-Free Theory of Nonparametric [11] M. Hanke. Conjugate Gradient Type Methods for Linear Ill-posed Problems . Pitman Research [12] L. Lo Gerfo, L. Rosasco, E. Odone, F.and De Vito, and A. Verri. Spectral Algorithms for [13] S. Mendelson and J. Neeman. Regularization in Kernel Learning. The Annals of Statistics , [14] P. Naik and C.L. Tsai. Partial Least Squares Estimator for Single-index Models. Journal of the [15] A. S. Nemirovskii. The Regularizing Properties of the Adjoint Gradient Method in Ill-posed [16] C. S. Ong. Kernels: Regularization and Optimization. Doctoral dissertation, Australian Na-[18] R. Rosipal and L.J. Trejo. Kernel Partial Least Squares Regression in Reproducing Kernel [19] R. Rosipal, L.J. Trejo, and B. Matthews. Kernel PLS-SVC for Linear and Nonlinear Classifi-[20] I. Steinwart, D. Hush, and C. Scovel. Optimal Rates for Regularized Least Squares Regression. [21] S. Wold, H. Ruhe, H. Wold, and W.J. Dunn III. The Collinearity Problem in Linear Regression. [22] T. Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural
