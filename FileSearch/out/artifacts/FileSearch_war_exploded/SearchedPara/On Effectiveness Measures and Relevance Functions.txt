 IR is one of scientific disciplin es where theoretical and empirical researches are closely intertwined. A pure theoretical retrieval model might probably fail in operational en-vironment and appropriate parameter tuning for such theoretical model is impossible without extensive experiments on benchmarks. To set up benchmarks, evaluation crite-ria have to be first of all determined. Such criteria must not only express the function-ality of a system, which is its principal aim, but also be feasible in ways to obtain test collections (document collection, information needs and relevance judgment of each topic). Relevance and measurement defin itions are therefore the essential components of an evaluation criterion.
 CLEF) share two common issues: they omit ties and use a binary relevance. Despite a lot of criticisms against their simplicity, these assumptions are employed in popular performance metrics such as MAP, R-Precision or precision at n documents retrieved (Prec@n). We formulate three reasons for that paradox. First, there is no more com-plicated evaluation proposal which has not only a theoretically sounded metric and for which relevance assessments can be easily collected. It has been reported that it is im-possible to ask a person to identify more than 11 relevance levels and all the aforemen-tioned forums find 3 or 4 levels feasible in practice [18]. Second, even if there are new metrics, there is no standard way to verify their outstanding features in practice. How to quantify the reliability and stability of an evaluation metric is still an open question in IR evaluation. In physical experiment, t he fact that measurement values vary across tests is mainly due to natural factors s uch as instrument, temperature or experimenter health. Sources of uncertainty in IR evaluation, are contrarily due to the suitability of metric assumptions and the fact of too small sample size (of topics for instance) in the particular case. Third, despite theoretical modeling for ties, they do not happen so fre-quently with automatic retrieval systems and even if they do, a simple solution such as sorting according to submission order is sufficient enough.
 by the increasing availability of documents in XML format available in Internet. This discipline is still in its infancy and XML search engine performance is still far below that of flat text retrieval. To promote research, INEX 1 aims at providing infrastructure to evaluate and compare retrieval systems dedicated to XML retrieval task. INEX states that when scaling down retrievable units from entire documents to arbitrary XML ele-ments, two interdependent challenges caused by the appearance of XML elements with parent-child relationship in retrieved lists arise while evaluating. Firstly, there are so many ties in retrieved lists which cannot be neglected. Secondly, it is necessary to use a multivalued 2 bidimensional relevance scale to express better different relevance levels rather than binary values. inex eval is the current official criterion used by INEX to rank systems [11] and tries to handle the two challenges in a unified framework. However, from the practical viewpoint of a comparative eva luation of many systems, after aver-aging evaluation scores over a sufficient amount of topics, some questions arise about the necessity and the reliability of such measure: 1. whether ties happen frequently in retrieved lists, and if this depends on system or 2. whether inex eval handles ties better than other measures which neglect this issue. 3. whether a graded scale is necessary: if a graded scale offers more information about  X  X hat works and what does not in XML content-based retrieval X . This question will never be answered until an appropriate evaluation protocol is set up. As we mentioned above, since there is no standard way to verify the reliability of an evaluation measure in IR, our proposal in this paper might therefore not only be helpful specially for INEX or even for XML retrieval but also contribu te novel and empirical ways to evaluate the reliability of an evaluation setting in IR.
 Effectiveness performance will be analysed based on the averaged measure currently employed to rank systems and not on individual topics or on theoretical aspects. We ex-amine not only pairwise effects as used in previous work such as [18,2,12] (Kendall X  X   X  , scatterplot, swap of systems in lists ordered by average values over the topic set) but also go into detail of pointwise by quantifying characteristics of each evaluation setting independent to others: interval aspect and discrimination power. The main studied ob-ject in our analysis is the inex eval measure which was chosen as the official measure in INEX. Contrary to inex eval, we choose MAP (mean of average precision over relevant documents) and Q-measure [18] which both omit ties issue in retrieved lists and handle graded relevance. These measures are quite compatib le with inex eval: they are system-oriented evaluations, single value measures, rank-based (vs set-based of R-Precision, F-measure, etc) evaluations by making use of both precision and recall facets. More-over, previous researches have shown that MAP with a binary relevance scale gives the most reliable and stable evaluation in c omparison with others such as R-Precision and Prec@n, especially with a limited size of the topic set [1] or with a very large document collection [7]. Kek  X  al  X  ainen &amp; Jarvelin [12] has extended MAP to the graded case. The Q-measure proposed by Sakai [18] is dedicated to NTCIR system comparison, either in ad hoc retrieval task or in Question-Answering task with 4-point relevance judgment. This measure corrects undernormalization weakness of a previous measure (AWP) also proposed in NTCIR for graded evaluation with an additional parameter  X  which has to be set up manually.
 evance functions used in our analysis (Section 2). Our experiment concerns two main issues about particularities of XML element retrieval evaluation: ties handling (Sec-tion 3) and graded relevance scale adaptation (Section 4). We present also results about confidence interval of systems (Section 5) and discrimination power of an evaluation setting (a combination of a relevance function and a measure) in Section 6. We review briefly some related work in Section 7 before concluding and sketching our future work (Section 8). 2.1 Effectiveness Measures Let g ( d ) be gain value of document d . In popular binary scale, g ( . ) has either a value of 0 or 1 depending on the relevance of d . With a graded scale, g ( d )  X  [0 , 1] .Let isrel ( d ) be a boolean value about the relevance status of document d , isrel ( d ) will be set up if d is relevant, otherwise 0. Assuming an ordered set of documents, count ( r ) will denote the number of relevant documents ( g ( d ) &gt; 0 ) among the first r retrieved documents ( count ( r )  X  r ); cg ( r ) is the cumulated gain of this subset of r documents and cig ( . ) for an ideal list where documents are sorted by non increasing order of their gain values. R is the number of relevant documents for the considered topic. Given these notations, we can formally describe MAP, Q-measure and inex eval measures as follows. For the sake of simplicity, the outer averaging operator over all topics is omitted in the formulas. Note that if replacing also the denominator r by cig ( r ) , one will return to AWP 3 with the drawback of freezing evaluation on relevant documents found after rank R . Without ambiguity, in this paper, we will use the same notation MAP for both binary and graded scales.  X  is a positive coefficient to control the favor of MAP aspect count ( r ) r or of AWP aspect cig ( r ) . The magnitude of  X  will be manually fixed.
 The latter is proportional to the magnitude of  X  . However, the offset magnitude is not the same across topics and across runs, the output system rankings will possibly be different.
 related to our discussion. The inex eval metric uses precall [16] which computes the probability P ( rel | retr ) that an element viewed by the user is relevant knowing that (s)he wants to see a specified amount of relevant material: where ESL x.R denotes the Expected Search Length [3], i.e. the expected number of non-relevant elements retrieved until an arbitrary recall point x is reached. The inex eval value reported in this paper has been averaged over 100 recall points: 0.01, ..., 1.00. Being adapted from the measure precall ,inex eval is expected to be an intuitive method for interpolation and to handle weakly ordered ranks correctly. Theoretically, thank to ESL x.R component, it is straighforward to extend precall (originally dedicated to binary scales) to graded quantisation functions. In practice, there are however a lot of technical problems, for instance, because of submission length limit, the number of elements of the last rank must be approximated. Moreover, in graded scales both MAP (equation 1) and inex eval are under normalized: the metric value of the ideal lists is less than 1, each topic has therefore different  X  X pper bound X  values. This raises a worry about zooming bias towards easy topics when averaging over different topics, systems with capacity to perform well on difficult topics will hardly be recompensed. 2.2 Experimental Setting We performed experiments on the INEX document collection. According to INEX, XML retrieval amounts at retrieving document elements (e.g. paragraphs, sections, etc) relevant to a query need. The relevance criterion is therefore defined at the XML ele-ment level, with two interdependent axes: exhaustivity e (describing the extent to which the element discusses the topic of request) and specificity s (describing the extent to which the element focuses on the topic of request). Each dimension contains four val-ues of { 0 , 1 , 2 , 3 } corresponding to non-relevant, marginally, fairly and highly relevant levels. Since a non-relevant element in terms of exhaustivity (specificity resp.) must be non-relevant in the other axis, there are finally ten valid combinations rather than sixteen.
 aka CO) and content-and-structure (complex queries with constraints of both content and XML structure of the target, aka CAS). While the task definition of the former is well set up, this is not the case for the latter. Our discussion concentrates therefore on the CO task. Moreover, due to limit of space, all results presented in this paper are on the data set of the second round of INEX (INEX X 03 for short). This data set involves 54 retrieved lists 4 and 32 judged topics (the last version 2.5).
 point { 0.0, 0.25, 0.5, 0.75, 1.0 } ) and sog  X  X pecificity-oriented generalized X  (7-point { 0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0 } ), and two INEX binary relevance functions: strict ( g ( e =3 ,s =3)=1 ,otherwise0),s 3 e 321 ( g ( e&gt; 0 ,s =3)=1 , otherwise 0) are ex-amined. These binary functions are considered to express user preference in retrieving specificity -oriented XML element. Note t hat although we include  X  X trict X  in the discus-sion, the question about intuitiveness and the effect of too small sample size, of very few relevant answers (as shown in table 1) of such relevance function remains unknown, any conclusion based on that function should be viewed with caution. Detail about INEX relevance functions can be consulted in [5]. We include also the most liberal relevance definition, called  X  X rec X  in this paper, where all elements with positive values of speci-ficity and exhaustivity are considered equally relevant. This relevance function results in the same number of relevant elements R as in  X  X en X  and  X  X og X . Note that  X  X rec X  has not been included in INEX official forum. From theoretical viewpoint, it seems mean-ingless to equally reward a highly relevant answer with a marginal one. However, we show later how existing metrics can id entify such difference. These relevance functions result in not only different user preferences but also in the number of right answers per topic as shown in table 1.
 functions, gen and sog. Three values of  X  for Q-measure are taken into consideration: 0.1 (favoring MAP aspect), 1 (default) and 10 (favoring AWP aspect).
 we respect submitted order of answers rather than try to simulate random distribution of answers at the same rank.
 the similarity of system rankings produced by two different settings. Kendall X  X   X  rank correlation is used to quantify the magnitude of that relationship. We adopt also 0.9 as practical threshold to suggest that two rankings highly agree in order of sorted items. First of all, let us show how often ties happen in INEX collections. Table 2 gives some statistics about ties phenomenon in INEX X 03 test collection. We define the averaged rank size for each topic as list size submission length of 1500 elements per topic, when a system lacks answer list for a topic, its rank size will be set to 1500. In our opinion, such averaged rank size is more intuitive than its recip rocal which is named  X  X esolving power X  in [13 X  X h. 4]. A thor-ough view on these systems reveals that ties phenomenon is system-dependent (even participant-dependent) rather than topic-dependent (table 3). This consolidates thus the validation of traditional assumption that ties issue is controllable. Remind that such assumption has been used in popular evaluation measures such as MAP, R-Precision, Prec@n. Moreover, examining relationship between rank size and evaluation score, we cannot draw any pattern for each category of ties: runs of very weak ordering might be at top ranks, runs at the other side can be inversely ties free.
 ranking. From table 4 and figure 1 which concern binary relevance, one can observe that inex eval behaves quite similarly to MAP. With the same relevance function, there is no clear difference in system ranking produced by inex eval and that by MAP, especially with ten runs at two ends. This suggests that ties handling of inex eval might have an impact on ranking systems but this impact seems not quite clear. In other words, in this circumstance, ties handling capacity of inex eval is not outstanding as proved theoretically. To examine how different measures handle graded relevance scales, we rely on the simi-larities of system rankings produced by a measure under difference relevance scales (ta-ble 5 and figure 2) or by different measures (table 4). As we remarked previously, there are some topics without strict relevance judgment: conclusions about strict relevance function will be drawn with caution. Moreover, from the previous section, we know that Q-measure with  X  =1 provides no particular information given the two extremes  X  =0 . 1 and  X  =10 . We therefore do not present graphics for  X  X trict X  and  X  =1 .We also neglect in table 4 some relevance pairs whose Kendall X  X   X  is inferior to 0.9 in all metrics. The low correlation of these pairs should be examined in a separate report by taking into consideration how the number of right answers affect system comparison.  X  gen, sog, trec have a high agreement on low evaluations for all measures. These  X  gen has higher correlation to trec than sog has. It suggests an approximation of gen  X  These measures do not agree about the similarity of sog to trec and to s 3 e 321 re- X  In table 4, there is a clear separate of bi nary and graded groups: given a metric pair, Now let us concentrate on MAP and Q-measure:  X  Both MAP and Q-measure discourage too short runs. A representative example is  X  On Q-measure figures, curves of s 3 e 321 are fairly different to those of gen/sog/trec,  X  In comparison with inex eval and MAP, Q-measure enlarges the distance between  X  In binary cases, correlation of MAP and Q-measure decreases when increasing Our discussions have focused so far on point estimators. We present now interval esti-mators which provides more insight on the statistical analysis of the results. the bootstrap [4], to estimate confidence interval (c.i.) of the sample mean of a run over the topic set. We got 5000 bootstrap samples and used the percentile interval technique for obtaining confidence intervals at a 95% confidence level 5 .
 trec are presented in figure 3. From the figure, one can observe that (i) despite dif-ferences in absolute values, confidence interval lengths of a system are highly similar under a performance measure and that (ii) confidence intervals of top runs are longer than those of low runs. The first issue means that although inex eval and MAP are undernormalized in graded scales, this does affect only point estimator but not distort interval estimator at all. The second however seems not intuitive from standpoint of system behavior: a poor performing system is equally bad for all topics, which is not the case for a better one. Voorhees &amp; Buckley [26] proposed to measure the sensitivity of a test collection through its capacity to distinguish retrieval systems. However, their method must be conducted on at least 50 topics for achieving s table results. Such requirement is infea-sible in the INEX case. We follow therefore the definition of discrimination power of a performance measure proposed by Tague-Sutcliffe &amp; Blustein [23], namely its capac-ity to distinguish different systems from the top run (the so-called size of group A in multiple comparison test grouping). We used the tools provided in their IR-STAT-PAK programme 6 . ANOVA prerequisite of homogeneity of variances is still violated in our case even after arcsine ( arcsin should therefore be interpreted with caution before more complicated power simula-tion such as that in [8] is carried out. Since sizes of group A after arcsine and rank transformation are qu ite close in each evaluation setti ng, only the former is presented in table 6.
 tion power is the following: from c.i. results in the previous section, runs whose con-fidence intervals overlap with the one of the top run are supposed to be in group A (this is a naive form of unpaired test). The corresponding figures are visualized in table 7.
 measures work in very similar manner in each binary relevance case, the difference in their discrimination power if happens is so small that one can think of by chance. In graded case, both tables agree that Q-measure(  X  =10 ) is less discriminating than both lower  X   X  X , inex eval and MAP. In conjunction with figures in table 4, we can suggest that the high weight of AWP factor affects not only to system ranking but also to capacity to distinguish systems of Q-measure. This is totally different to Sakai X  X  argument about theroleof  X  in [18]. In this section, we briefly review work on three issues: ties handling, metric for graded relevance scales and techniques to evaluate robustness of an effectiveness measure. call [16] which is based on Expected Search Length [3]. With the same objective to investigate ties, Kraaij described occurrence of ties in a retrieved list via  X  X esolving power X  in [13 X  X h. 4].
 provides a relatively thorough review), in our awareness, this is the first time three performance measures dedicated to graded relevance, namely inex eval, MAP and Q-measure, are together taken into consideration. Note that we do not include ADM [14] in the discussion. In our opinion, such measure, even if being shown reliable (which is still an open question), will go better in the future rather than with current human assessment protocol. It has been reported that it is very difficult for human to distinguish more than 11 relevance levels [12]. So there is no reason to encourage retrieval systems to identify strictly continuous relevance values.
 CIR context, for ad hoc retrieval task [18] and Question-Answering application [17]. However, it surprises us when he states that MAP can not be extended to graded scales without reference to work of Kek  X  al  X  ainen &amp; Jarvelin [12]. That is why his experiment concerns therefore only binary MAP and the other proposal of the two aforementioned authors, the so-called Cumulated Gain. We s hare with [18] and [13] that Cumulated Gain family, since estimating performance at document cutoff, is more suitable for vi-sual purpose from user standpoint rather than as a reliable single value measure. In the latter, the metric score must be averaged in an unbiased manner over different topics, how to determine an appropriate document cutoff when the number of relevant answers varies largely across topics (which often happens in IR test collections) is not simple at all. This is also the first time the role of  X  in Q-measure is brought to light. Relying on experiment of NTCIR systems, Sakai [18] ar gued that this parameter, like gain (rel-evance) values, has secondary impact on stability and meaning of Q-measure. We have shown that it is not the case in INEX context.
 in existing IR evaluation forums such as TREC Web and NTCIR. With two relevance dimensions (exhaustivity and specificity), not only the number of possible relevance values increases but the standard relevance axis becomes a relevance circle; relevance thresholds (to separate relevance and non relevance parts) are therefore not quite ex-plicit. Sakai suggested in [17] ways to create graded relevance judgment for Question-Answering by equivalence class of answers. We argue that such solution is not suitable for INEX case due to complicate nesting issue of XML elements and the goal of best matching, relevance assessment is therefore done in elements themselves rather than as concept judgment in Question-Answering.
 posed in INEX, such as XCG [9] with empirical results available in [10] or T2I [6]. The former, inspired from the aforementioned Cumulated Gain family, aims at a unified solution which employs graded relevance s cale and handles physical overlap of XML element in both assessment set and retrieved lists. The latter is dedicated to a new user model which is tolerant to the physical overl ap issue. Since both of these measures aim at weakening another traditional assumption of IR about independence among retrieved items, they are therefore out of scope of this paper. Moreover, to become a robust metric in practice, they also encounter the bottleneck of empirical validation which we discuss in the next paragraph.
 is still an open question. Besides simple index of Kendall X  X   X  used in most of pa-pers about IR evaluation, there are other examples such as discrimination power [23] which we adopted in this paper,  X  X rror rate X  estimator proposed by Voorhees &amp; Buck-ley [1,26,2,20]. Let us cite van Rijsbergen X  X  remark in 1979 which is unfortunately still valid  X  X here are no known statistical tests applicable to IR X  [24 X  X . 136]. The difficulty is due to various particularities of IR evaluation in comparison with standard statistical framework for instance the fact of very small sample size, the violation of some pre-requisites such as equality of variances in ANOVA. We have presented simple ways to empirically compare effect of different evaluation setting by existing statistical tools. Up to our knowledge, this is the first time confidence intervals have been taken into account in estimating discrimination power. Tague-Sutcliffe and Blustein [23], Hull et al. [8] used standard multicomparison tests that simply make use of point estimators. Savoy [21] used Bootstrap to compare each pair of runs which does not extend yet to multicomparison tests. We have gone further by using Bootstrap to estimate c.i. which reveals more detail about INEX system behavior, such as the high variance of top runs. We have taken a close view on INEX evaluation, concentrating on two stated challenges in evaluating XML element retrieval, weak ordering and graded relevance scales. Let us now summarize drawn conclusions:  X  Existing retrieval models are able to dis tinguish fine relevance status of XML ele- X  Even assuming that it is possible to obtain graded relevance assessments (continu- X  From practical standpoint, as long as human relevance assessment procedure is  X  Q-measure is not clearly more reliable t han MAP. It remains a question about rea- X  Advantage of interval confidence with respect to popular point estimator in compar-It sounds disappointing that empirical experiments do not encourage development of new evaluation measures which are expected to better describe characteristics of re-trieval systems. However, we interpret this in three different ways. First, like in research of retrieval techniques, a pure t heoretical evaluation measure must be checked in large scale testbeds before concluding about its power. Second, until new methods to collect test collections, especially to simplify the bottleneck of relevance judgment become feasible, it is not urgent to either tune graded relevance values or to propose effec-tiveness measures for graded relevance scale. Such scale makes assessment procedure more difficult, even infeasible, thus less reliable while existing measures cannot benefit later. Finally, from retrieval model viewpoi nt, the fact that graded relevance scale is not clearly necessary in IR evaluation seems to take a good news: even in the simple binary relevance case, how to design retrieval systems which optimize adaptively its perfor-mance towards some popular evaluation measures such as MAP or Prec@10 is still an open question, let alone for graded (ordinal but not continuous) relevance scale. measures in XML element retrieval context. First of all, we intend to open the box of  X  X ystery X  about confidence interval length of top runs by per topic analysis. To better understand effect of normalization per topic in measure stability, we will employ other statistics to sort systems, such as sample median [21] or geometric mean [25]. These statistics are expected to appropriately describe systems, especially for small topic set size and for heterogeneous environment. Regardings discrimination power estimation, we tend to adopt also techniques to get  X  X rror rate X  estimator proposed by Buckley &amp; Voorhees to circumstances of very small sample size such as INEX by replacing the random checking [26] by statistical significance test and the query version [1] by bootstrap samples.
 operate on retrieved lists delivered by systems but not on details of retrieval step. It is therefore straighforward to apply these techniques to compare systems in other re-trieval scenarios. We are replicating the aforementioned tests on XML document re-trieval, which seems more realistic than curre nt INEX task of retrieving arbitrary XML elements 7 . This is not a newly born task at all but has been simply considered as a sec-ondary to XML element retrieval [15]. In our understanding, such scenario is similar to topic distillation task in TREC Web Track. While the latter raises practical challenge for existing evaluation metrics in discrimination power aspect [22], we fear that this is also the case of INEX. If this is confirmed in practice, more well-designed evaluation procedure might be necessary to have reliable conclusions about INEX system perfor-mance. A recently proposed measure by [2], the so-called bpref , which is dedicated to incomplete and imperfect judgment with assumption of binary relevance and ties free in retrieved results, will also be examined in this retrieval scenario.
 The authors are thankful to both INEX participants for creating test collections and INEX organizers for letting these data be accessible. We are grateful to T. Sakai for the official version of [17], D. Hull for the unpublished report [8] and J. Blustein for providing IR-STAT-PAK programme. We are especially in debt of anonymous reviewers whose thoughtful comments make this paper much better than the initial version.
