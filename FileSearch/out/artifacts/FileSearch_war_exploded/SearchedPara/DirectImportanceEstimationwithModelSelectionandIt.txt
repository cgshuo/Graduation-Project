 hkashim a@jp.ibm.co m to the importanc e [8]: w ( x ) = p how to acc urately estimate the importanc e.
 the importan ce withou t estimating the densities would be more promising. importanc e estimation algorithms.
 issue currently .
 to its estimate b p this minimization without explicitly mode ling p samples are abund antly availab le.
 prediction performan ce in covariate shift scenarios. In this section, we propo se a new importance estimation method . 2.1 Formu lation and Notation from a training input distrib ution with density p test input distrib ution with density p the numb er n f x Our key restriction is that we avoid estimating den siti es p importanc e w ( x ) . 2.2 Kullbac k-Leibler Importan ce Estimation Pr ocedu re (KLIEP) Let us mode l the importance w ( x ) by the follo wing linear mod el: where f such that Note that b and f ' mode ls are also allo wed  X  X e explain how the basis functions f ' Using the model b w ( x ) , we can estimate the test input den sity p We determine the param eters f p te ( x ) Since the first term in the last equa tion is inde pend ent of f secon d term. We den ote it by J : where the empirical app roximation based on the test input samples f x te parame ters f test inpu t samples f x te belo w, f x tr natural to impose b w ( x ) 0 for all x 2 D , which can be achie ved by restricting is a proba bility den sity function : where the empirical approx imation based on the training input samples f x tr first line to the second line abo ve.
 No w our optimiza tion criterion is summarized as follo ws. Figure 1-(a). Note that the solution f b Estimation Proce dur e (KLIEP). 2.3 Model Selection by Lik elihoo d Cr oss Validation The performan ce of KLIEP depe nds on the choice of basis functions f ' how the y can be app ropriately cho sen from data samples. the mode l such that J is max imized. The expectation over p ples f x te fX te j g j 6 = r and app roximate the score J using X te r as We repeat this proced ure for r = 1 ; 2 ; : : : ; R , compu te the avera ge of b J average b J as an estimate of J : summarized in Figu re 1-(b) sample problem.
 the test input points f x te where K ( x ; x 0 ) is the Gaussian kernel with kernel width : The reason wh y we chose the test input points f x te input points f x tr if the training input density p setting the Gau ssi an cen ters at the test inpu t points f x te Alternati vely, we may locate ( n the compu tational cost. Since n cally propose using a subset of f x te where c In the rest of this paper , we fix the numbe r of template points at and optimize the kernel width by the abo ve CV proce dure. 3.1 Importanc e Estimation for Artificial Data Sets Let p and p The task is to estimate the importance at training input points: We compa re the follo wing method s: KLIEP( ): f w KDE(CV): f w KMM( ): f w LogR eg(CV): The kerne l width in LogRe g is cho sen based on 5 -fold CV . We fix ed the numb er of test input points at n the num ber n (a) n (b) d = 10 and n We run the experiments 100 times for each d , each n of the importanc e estimates f b w and KLIEP(CV) works significan tly better than Log Re g(CV). KLIEP(CV) tends to give significan tly smaller errors than LogRe g(CV). Ov erall, KLIEP(CV) is sho wn to be a useful method in importanc e estimation. classification ben chmark problem s (see Table 1).
 Each data set con sist s of input/ou tput samples f ( x f x follo ws. We randomly choo se one sample ( x ity min(1 ; 4( x ( c ) in each trial of expe riments; then we remo ve x cepta nce, and repe at this proce dure until we acc ept n to be lower than the training input den sity whe n x ( c ) n tr = 100 for training regressors or classifiers; the test outp ut values f y te gene ralization performanc e.
 We use the follo wing kerne l mode l for regression or classification : where K f x weighte d regularized least squar es (IWRLS) [9]: The solution b where I is the identity matrix and weighte d CV (IWCV) [9]. We com pute the IWCV score by where works reasona bly well, but it sometimes performs poo rly . tation. sets tak en from ID A.  X  X MM( ) X  den otes KMM with kernel width . KLIEP is sho wn to be a promising method for covariate shift adap tation. addition al adv antages.
 areas will be important future directions.
 Ackn owledgm ents crosoft CORE3 Project, and the IBM Faculty Award.

