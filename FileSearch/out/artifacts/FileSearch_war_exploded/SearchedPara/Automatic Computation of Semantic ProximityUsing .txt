 Taxonomic measures of semantic proximity allow us to com-pute the relatedness of two concepts. These metrics are ver-satile instruments required for diverse applications, e.g., the Semantic Web, linguistics, and also text mining. However, most approaches are only geared towards hand-crafted tax-onomic dictionaries such as WordNet , which only feature a limited fraction of real-world concepts. More specific con-cepts, and particularly instances of concepts, i.e., names of artists, locations, brand names, etc., are not covered.
The contributions of this paper are twofold. First, we in-troduce a framework based on Google and the Open Di-rectory Project (ODP), enabling us to derive the semantic proximity between arbitrary concepts and instances. Second, we introduce a new taxonomy-driven proximity metric tai-lored for our framework. Studies with human subjects cor-roborate our hypothesis that our new metric outperforms benchmark semantic proximity metrics and comes close to human judgement.
 H.3.3 [ Information Storage and Retrieval ]: Information Retrieval and Search X  Information Filtering ;I.2.6[ Artifi-cial Intelligence ]: Learning X  Knowledge Acquisition Algorithms, Experimentation, Human Factors, Measurement Semantic similarity, metrics, taxonomy, accuracy, data ex-traction Now working for Siemens AG, Corporate Technology IC 1, Munich. Contact via indicated e-mail.
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
Research on similarity of word meanings dates back to the early 60 X  X  [17]. Thenceforward, numerous semantic proxim-ity measures have been proposed, mostly operating on the taxonomic dictionary WordNet [13, 15, 11, 3, 10], exploit-ing its hierarchical structuring. The main objective of these approaches is to mimic human judgement with respect to the relatedness of two concepts, e.g., bicycles and cars . With the advent of applications that intend to make ma-chines understand and extract meaning from human-crafted information, e.g., the Semantic Web initiative or text min-ing, the necessity for tools enabling the automatic detec-tion of semantic proximity becomes even stronger. However, one severe drawback of these approaches is that their ap-plication has been confined to WordNet only. While the number of unique strings of this taxonomically organized dictionary, i.e., nouns, verbs, adjectives, and adverbs, nears 150,000, large amounts of information available on the Web and other textual sources cannot be captured by such dictio-naries. Particularly brand names, names of artists, locations, products and composed terms, in other words, specific in-stances of concepts, are beyond their scope. Examples are CIKM , Database Theory , Neil Armstrong ,or XBOX Games , to name some.
We intend to overcome the aforementioned issue and pro-pose an architecture that allows to compute the estimated semantic proximity between arbitrary concepts and concept instances. 1 The following two major contributions are made: We will abuse language by likewise denoting concepts ,e.g., Poet ,and instances ,e.g., Friedrich Schiller , by the term concept only.
Our work is structured as follows. In Section 2, we sur-vey relevant existing literature on semantic similarity. Next, we describe our system X  X  architectural framework based on Google and ODP. Section 4 presents some taxonomy-based proximity measures and introduces our new proximity met-ric. Amalgamating our system architecture with such met-rics, we conduct an extensive empirical evaluation in Section 5, involving more than 50 human subjects. Eventually, we give an outlook of future research, taking the notion of se-mantic proximity one step further.
The study of semantic proximity between two given con-cepts has largely focused on similarity , e.g., synonymy and hyponymy [13]. Proximity goes even further, also subsum-ing meronymy (part-whole) and arbitrarily typed semantic relationships.

Early taxonomy-based similarity metrics only have taken into account the shortest path  X  between two concepts within the taxonomy, and the depth  X  of the most specific common subsumer of both concepts. See [3] for an overview of these early works. Next-generation approaches were inspired by information theory and only used taxonomies in combina-tion with text corpora. Thus, the probability of a concept, its so-called information content , could be computed and used to refine the similarity measure. Resnik [15, 16] lay the foundations of this approach, followed by Jiang and Conrath [9] and Lin [11], both largely similar to Resnik X  X . Lin X  X  ap-proach is extended to handle graphs rather than mere trees by Maguitman et al. [12].

Li et al. [10] have conducted an extensive study that re-vealed that the usage of information content does not yield better performance. Moreover, they proposed a metric that combines shortest path length  X  and subsumer depth  X  in a non-linear fashion and outperformed traditional taxonomy-based approaches. Chirita et al. [5]usedavariationofLi et al.  X  X  metric for personalizing Web search.

Taxonomy-based metrics with collaborative filtering sys-tems in mind have been proposed by Ganesan et al. [8] and Ziegler et al. [22].

The exploitation of Web search engine results for comput-ing similarity between concepts or, more generally, queries, has been tried before. Chien and Immorlica [4] and Vlachos et al. [20] attempted to detect similarity via temporal cor-relation, reporting mixed results. Wen et al. [21] computed semantic query similarity based on query content, user feed-back, and some simple document hierarchy. No empirical analyses were provided, though. Cimiano et. al. [6,7]make use of linguistic patterns along with search engine leverage to automatically identify class-instance relationships and dis-ambiguate word senses.
In this section, we describe the framework we built in or-der to compute semantic proximity between arbitrary con-cepts or instances. The approach rests upon ODP and Google Directory, which we use in order to provide us with the re-quired background knowledge. The two services are required so we can compose semantic profiles of the concepts we want to compare. The following step then necessitates a proximity metric to match these profiles against each other. Open Directory Project. The so-called DMOZ Open Di-Google Directory. Building upon ODP, Google Directory
The main task is to compute the proximity s between two concepts c x , c y , i.e., s ( c x ,c y ). In order to match c c , we need to build semantic profiles for both concepts first.
To this end, we send these two concepts c x and c y ,for instance Boris Becker and Wimbledon , to Google Direc-tory and obtain two ranked result lists q c x : L n x and q L n y , respectively. We define L n z := { 1 , 2 ,...,n z where z  X  X  x, y } , n z the number of documents returned for query term c z ,and D the set of topics in the ODP taxon-omy. Hence, we only consider the ODP referral associated with each document returned for the query rather than the textual summary. For example, q c x (1) gives the ODP topic that the top-ranked result document for query term c x is categorized into.

Next, the two profiles are forwarded to the proximity met-ric, which then computes the estimated semantic proximity score s ( c x ,c y ), using the ODP as background knowledge to look up topics q c z ( i ), where z  X  X  x, y } and i  X  X  1 , 2 ,...,n within the taxonomy.

The approach is very versatile and not only extends to the computation of semantic proximity for pairs of concepts, but effectively pairs of arbitrary queries in general.
In order to use our framework, we need to install an actual taxonomy-based proximity metric s that compares q c x with q c y .Since q c x and q c y are ranked lists of topics, its type must be s :( L n x  X L n y )  X  S , where S is an arbitrary scale, e.g., [  X  1 , +1].

We will first review several popular metrics that have been proposed in the context of WordNet and then proceed to propose our own taxonomy-based proximity metric.
In general, WordNet metrics compare the similarity of word senses , where each word sense is represented by a topic from the taxonomy. Hence, the metric X  X  functional layout is s : D  X  D  X  S rather than s :( L n x  X L n y )  X  S .The reason is that we are comparing two topics from the taxon-omy rather than instances , which are arranged into multiple topics within the taxonomy. For example, Batman is an in-stance of several topics, e.g. Movie and Cartoon Hero . We will show later on how to circumvent this issue.
The simplest WordNet metric only computes the short-est path  X  ( d x ,d y ) between two topics d x ,d y in the taxonomy. Leacock and Chodorow [3] modify this basic metric by scal-ing the path length by the overall depth  X  of the taxonomy:
Though the Leacock-Chodorow metric uses little infor-mation to compute the similarity estimate, its accuracy has been shown only insignificantly inferior to more informed approaches based on information theory, e.g., Resnik [15, 16] or Jiang and Conrath [9].

Li et al. [10] have conducted an extensive survey compar-ing numerous existing WordNet metrics and proposed a new metric which combines shortest path length  X  ( d x ,d and most specific subsumer depth  X  ( d x ,d y )ina non-linear fashion, outperforming all other benchmark metrics. The most specific subsumer of topics d x and d y is defined as the topic that lies on the taxonomy paths from the root topic to both d x and d y and has maximal distance from the root topic. Li et al.  X  X  metric also features two tuning parameters  X  and  X  whichhavetobelearnedinordertoguarantee the metric X  X  optimal performance. The metrics is defined as follows: s
As has been stated before, the above metrics only measure the distance between two singleton topics d x and d y rather than two lists of topics q c x : L n x and q c y : L n y ,respectively. The issue has been addressed [5] by computing the average similarity of all unordered pairs { d x ,d y } X  ( q c x ) where d x = d y . 2
As has been outlined in Section 3.2, multi-class categoriza-tion of concepts/instances into several topics is essential for our approach, since more than one query result and its taxo-nomic referral are used to describe one concept/instance c Existing WordNet metrics can be tailored to support prox-imity computations for topic lists, but their performance is non-optimal (see Section 5). We therefore propose a new metric with multi-class categorization in mind. Our approach is substantially different from existing metrics and can be subdivided into two major phases, namely profiling and prox-imity computation . The first phase takes the list of topics describing one concept/instance c z ,e.g., Batman Begins , and creates a flat profile vector, based on the ODP taxonomy as background knowledge. The second phase then takes the profile vectors for both c x and c y and matches them against each other, hence computing their correlation.

As input, our metric expects two ranked topics lists q c z L n z ,z  X  X  x, y } , and three fine-tuning parameters,  X  ,  X  ,and  X  . These parameters have to be learned from a training set before applying the metric.
Foreachconcept c z for which to build its profile, we cre-ate a new vector v z  X  R | D | , i.e., the vector X  X  dimension is exactly the number of topics in the ODP taxonomy. Next, we accord a certain score  X   X  i , where i  X  X  1 , 2 ,...,n each topic q c z ( i )  X  ( q z ). The amount of score depends on the rank i of topic q c z ( i ). The earlier the topic appears in ( f ) denotes the image of map f : A  X  B ,i.e., ( f : A  X  B ):= { f ( x ) | x  X  A } . the result list q c z of query c z , the more weight we assign to that topic, based upon the assumption that results further down the list are not as valuable as top-list entries. For the weight assignment, we assume an exponential decay , inspired by Breese et al.  X  X  half-life utility metric [2]:
Parameter  X  denotes the impact weight half-life ,i.e.,the number of the rank of topic q c z (  X  )onlist q c z for which the impact weight is exactly half as much as the weight  X   X  1 top-ranked result topic. When assuming  X  =  X  , all ranks are given equal weight.

Having computed the score  X   X  i foreachtopic q c z ( i ), we now start to assign score to all topics d i, 0 ,d i, 1 ,...,d along the path from q c z ( i ) to the taxonomy X  X  root node. Hereby,  X  ( i ) denotes the depth of topic q c z ( i )= d taxonomy, and d i, 0 is the root node. The idea is to propagate score from leaf topic d i, X  ( i ) to all its ancestors, for each d is also  X  X  type of X  d i,j  X  1 , owing to the taxonomy X  X  nature of being composed of hierarchical  X  X s-a X  relationships.
Note that the ODP taxonomy also features some few links of types other than  X  X s-a X , namely  X  X ymbolic X  and  X  X elated X . These types were not considered in our model so far. When upward-propagating score for each q c z ( i ), we first assign d i, X  ( i )  X  1 then depends on four factors, namely parameters  X  and  X  , the number of siblings of d i, X  ( i ) , denoted  X  ( d and the score  X  i, X  ( i ) of d i, X  ( i ) . The general score propaga-tion function from taxonomy level j to level j  X  1isgiven as follows:
Informally, the propagated score depends on a constant factor  X  and the number of siblings that topic d i,j has. The more siblings, the less score is propagated upwards. In order to not overly penalize nodes d i,j  X  1 that have numerous chil-dren, we chose logarithmic scaling. Parameter  X  controls the influence that the number of siblings has on upward propa-gation. Clearly, other functions could be used likewise.
Next, we normalize all score  X  i,j , where i  X  X  1 , 2 ,...,n and j  X  X  0 , 1 ,..., X  ( i ) } , so that values  X  i,j sum up to unit score. Values of vector v z at positions d i,j are eventually increased by  X  i,j , yielding the final profile vector for concept c .
 Algorithm 1 summarizes the complete profiling procedure. Function pathvec( q c z ( i )  X  D ) returns the vector containing the path of topic q c z ( i ) X  X  ancestors, from q c z ( i )itselftothe root node. The vector X  X  size is  X  ( i ). Function id( d  X  the index that topic d is mapped to within profile vector v
Profile generation for concepts c x ,c y and their respective ranked topic lists q c x , q c y appears as the major task of our approach; the eventual proximity computation is straight-forward. Mind that the profiling procedure generates plain feature vectors , so we can apply generic statistical tools for measuring vector similarity. We opted for Pearson X  X  correla-tion coefficient, particularly prominent in collaborative fil-tering applications [18, 23]. Hence, the final proximity value is computed as follows: s
Where v x and v y denote the mean values of vectors v x and v .Moreover, v x and v y are assumed to have been computed according to Algorithm 1.
In Section 3, we have proposed a framework to compute semantic proximity between arbitrary concepts/instances. In order to evaluate which proximity metric best fits our approach, we conducted an extensive empirical study involv-ing 51 human subjects and necessitating the creation of two novel benchmark sets, featuring 30 and 25 concept pairs.
The evaluation method follows the methodology used for comparing the performance of WordNet metrics, e.g., [15], [11], [3], and [10], based on mainly two benchmark sets, namely Rubenstein-Goodenough [17] and Miller-Charles [14]. The first set features 65 concept pairs, e.g., rooster vs. age , furnace vs. stove , and so forth. Miller-Charles is a mere subset of Rubenstein-Goodenough and only contains 30 word pairs. These 30 word pairs were given to a group of 38 people, asking them to judge the semantic similarity of each pair of words on a 5-point scale [14]. For benchmarking, these human ratings were used as an  X  X rbiter X  for all Word-Net metrics, and the correlation for each metric X  X  computed word/concept pair similarities with human judgement was measured. The closer the metric X  X  results, the better its ac-curacy.
Neither Miller-Charles X  nor Goodenough-Rubenstein X  X  set features concept instances or composed concepts, e.g., loca-tions, book titles, names of actors, etc.; however, the com-parison of semantic proximity for these specific terms repre-sents the core capability of our framework. We hence needed to create own benchmark sets. Since some of the metrics presented in Section 4, namely Li et al.  X  X  [10] metric and our own approach, require parameter learning, we created two lists of concept pairs. The first, denoted B 0 ,contains25 pairs of concepts, e.g., Food Network vs. Flowers ,or IMDB vs. Blockbuster , and serves for training and pa-rameter learning. The second benchmark, denoted B 1 ,fea-tures 30 concept pairs such as Easyjet vs. Cheap Flights and Holiday Inn vs. Valentine X  X  Day . The conception of both benchmark sets and their respective human subject studies is identical. They only vary in their concept pair lists, which are disjoint from each other. The sets, along with the average ratings of human subjects per concept pair and the respective standard deviations, are given in Table 1 and 2. In order to obtain these two lists of concept pairs, we used Google X  X  Suggest service, still in its beta version at the time of this writing ( http://www.google.com/webhp?complete=1 ): For each letter in the alphabet (A-Z), we collected the list of most popular search queries proposed by Google Sug-gest, giving us 260 different queries, e.g., Cheap Flights Inland Revenue ,and Cars . We took all 33,670 possible query pair combinations and computed the semantic prox-imity for each query according to our proposed framework, using the simple Leacock-Chodorow [3] metric. Next, for the test set B 1 , we sorted all pairs according to their metric score in descending order and randomly picked ten pairs from the 5% most similar concept pairs, ten concept pairs from mid-range, and ten pairs from the bottom. For B 0 , we proceeded in a similar fashion.

We still had to manually weed out unsuitable pairs, i.e., those terms that people were deemed to be unfamiliar with. For instance, the famous US series Desperate Housewives is largely unknown to Germans, who represented 87% of all participants.

Though comparatively laborious, we opted for the largely automatized and randomized method presented above rather than for manual selection, which might have incurred per-sonal bias into the design of both benchmark sets.
Both online studies exhibited an identical make-up, differ-ing only in the concept pairs contained. Participants were required to rate the semantic relatedness of all concept pairs on a 5-point likert scale, ranging from no proximity to syn-onymy . 51 people completed B 1 and23ofthemalsofilled out B 0 . 3 87% of B 1  X  X  participants were German, while the remaining 13% were Italian, Turkish, US-American, and Is-raeli. 27% of all participants were CS PhD students or fac-ulty, much lower than for Resnik X  X  replication of Miller-Charles.

The results of each survey B z , z  X  X  0 , 1 } , were regarded as the proximity rating vector v i  X  X  1 , 2 ,..., 5 } | B z | respective participant i . We thus computed the inter-subject correlation , i.e., the Pearson correlation coefficient p ( v
Owing to B 1  X  X  major importance, we asked people to com-plete B 1 first. Correlation
Figure 2: Parameter learning curve for Li et al. (see Section 4.2.2) for every unordered pair { i, j } X  B z of human subjects, represented by their proximity rating vectors. Pair similarity scores were summed up and averaged afterwards: For set B 1 , we obtained an average correlation p 1 =0 . 7091. For B 0 , we had an average correlation p 0 of 0 . 7027. These values bear strong indication for judgement correlation, but are still considerably lower than Resnik X  X  human judgment replication of Miller-Charles, which had an inter-subject cor-relation of 0 . 8848 [15].

We identify the following points as driving forces behind this observation:
For measuring proximity, we compared several strategies which can be categorized into two larger classes, namely tax-onomy-based (see Section 4) and text-based . While focusing on the group of taxonomy-based metrics, the second group served as an indication to verify that traditional text-based methods cannot provide better performance, thus rendering the new breed of taxonomy-based metrics obsolete for our purposes.
For the taxonomy-based category, we opted for the met-rics presented in Section 4, namely Leacock-Chodorow [3], Li et al. [10], and our own approach, henceforth  X  X iegler et al.  X . For b oth WordNet metrics, i.e., Li et al. and Leacock-Cho-dorow, we employed the strategy presented in Section 4.1 for comparing concept lists rather than singletons.
Besides taxonomy-driven metrics, we also tested various classic text-based similarity measures for achieving the task, based upon the well-known vector-space paradigm [19, 1]. To this end, we tested four setups, using two different types of data to run upon:
First, instead of using the taxonomic description q c z ( i ) of each search query result i for query c z , we used its brief textual summary , i.e., the snippet returned by Google to describe the respective query result. These snippets typically contain the title of the result page and some 2-3 lines of text that summarizes the content X  X  relevance with respect to the query. Second, instead of using the snippet only, we downloaded the full document associated with each query result i for query concept c z .

Next, we applied Porter-stemming [1] and stop-word re-moval [1] to the first 100 search results (both snippets and full document) for all 260 queries crawled from Google Sug-gest. Both setups, i.e., snippet-and document-based, were further subdivided into two variations each: While term fre-quency (TF) [1] was always applied to all index term vec-tors, inverse document frequency (IDF) was first switched on and then off for snippet-and document-based. Hence, we get four different setups.
For comparing the performance across all metrics, we again followed the approach proposed by Resnik [15, 16] and Li et al. [10], i.e., we computed the predicted proximity of all concept pairs for set B 0 as well as B 1 , thus obtaining the re-spective metric X  X  rating vector. Next, we computed the Pear-son correlation p ( v m , v i )ofeachmetric m  X  X  rating vector v with the proximity rating vectors of all participants i  X  for one given experiment B z and averaged the summed co-efficients:
The correlation thus measures the metric X  X  compliance with human ratings. The higher the average correlation, the better. 4
Since Li et al. and Ziegler et al. demand tuning parameters, we conducted two separate runs. The first one, operating on B , was meant for parameter learning. The learned optimum parameters were then used for the second run, based on B 1 i.e., the actual test set.
Opposed to [15], the inter-subject correlation does not rep-resent an upper bound for metric correlation with human ratings, as can be shown easily.
Li et al. [10] give  X  =0 . 2and  X  =0 . 6 as optimal param-eters for their approach. However, since we are supposing a different data set, we ran parameterization trials for  X  and  X  again. We thereby assumed | q c z | = 30 for all concepts c i.e., 30 query results were considered for defining each con-cept/instance c z .Forboth  X  and  X  , we tested the interval [0 , 1] in . 05 increments on B 0 . The two-dimensional curve is shown in Figure 2. As optimal parameters we obtained  X  =0 . 2and  X  =0 . 8, which comes close to the values found by Li et al. [10]. The peak correlation amounts to 0 . 6451.
For our own approach, three parameters had to be learned, namely coefficients  X  ,  X  and half-life  X  . Again, we assumed | q c z | = 30. Parameters  X  and  X  were determined first, having  X  = 10, see Figure 3(a). The optimal values were  X  =0 . 7 and  X  =0 . 15, giving the curve X  X  peak correlation of 0 . 6609. As Figure 3(a) shows, all higher values are settled around some fictive diagonal. For probing half-life  X  , we therefore selected two points spanning the fictive diagonal, with the optimal parameters  X  =0 . 7and  X  =0 . 15 in the middle. The results for increasing  X  over all three 2D-points are shown in Figure 3(b). Again, the peak is reached for  X  =0 . 7and  X  =0 . 15, when assuming  X  =7.

The learned values were then used in the actual evaluation runs performed on B 1 .
First, we evaluated the proximity prediction performance across all taxonomy-based metrics. To this end, we tested all four metrics on varying query result sizes | q ( c z ) concepts c z , ranging from 1 to 80 documents. Results are dis-played in Figure 4(a), giving the average correlation with hu-man ratings for each metric and | q c z | X  [1 , 80]. The number of topics/documents for characterizing one concept/instance appears to have little impact on Leacock-Chodorow. When | q c z | &gt; 40, the correlation seems to worsen. For Li et al. ,an increase of | q c z | has merely marginal positive effects. We owe these two observations to the fact that WordNet met-rics are not designed to compare sets or lists of topics, but rather two singleton topics only (see Section 4.1). Besides, Figure 4(a) also shows that Li et al. has much higher correla-tion with human ratings than Leacock-Chodorow X  X  simplis-tic metric.

For our own approach, we tested the metric X  X  performance when using half-life  X  = 7, which had been found the opti-mal value before, and  X  =  X  . Note that an infinite half-life  X  =  X  effectively makes all topics obtain equal weight, no matter which list position they appear at. For  X  =7,the curve flattens when | q c z | &gt; 25. The flattening effect appears since all topics with low ranks i&gt; 7havesolittleweight,less than 50% of the top position X  X  weight. Adding more topics, considering that additional topics have increasingly worse ranks, therefore exerts marginal impact only. On the other hand, for  X  =  X  , smaller fluctuations persist. This makes sense, for every added topic has equal impact. However, the curves for  X  =7and  X  =  X  exhibit differences of smaller extent only. When assuming more than 40 topics per con-cept, correlation worsens somewhat, indicated through the  X  =  X  curve. As opposed to both WordNet metrics, our metric performs better when offered more information, i.e., more topics per concept. The latter finding backs our design goal geared towards multi-class categorization (see Section 4.2).
Moreover, Figure 4(a) shows that Ziegler et al. performs significantly better than both other taxonomy-based bench-marks. With  X  =  X  , the peak correlation of 0 . 7505 is reached for | q c z | = 31. Curve  X  = 7 levels out around | q c z | ing a correlation of 0 . 7382. For comparison, Li et al.  X  X  peak value amounts to 0 . 6479, Leacock-Chodorow X  X  maximum lies at 0 . 5154.

Next, we compared the performance of text -based prox-imity metrics, shown in Figure 4(b). All metrics, except for full text-based with TF and IDF, drastically improve when offered more documents for representing one concept. How-ever, fluctuations are much stronger than for the taxonomy-based metrics. For more than 20 documents per concept, snippets-based with TF and IDF performs best, reaching its maximum correlation of 0 . 6510 for 73 documents. This performance is comparable to Li et al.  X  X , but while the text-based metric becomes more accurate for document numbers &gt; 20, the mentioned taxonomy-based metric exhibits better performance for topic numbers &lt; 20.
We have shown that our novel metric outperforms both state-of-the-art taxonomy-based proximity metrics as well as typical text-based approaches. For reasonably large num-bers of topics , i.e., | q c z | &gt; 20, the correlation with human ratings lies between 0 . 72 and 0 . 75, indicating strong corre-lation. Leacock-Chodorow, being an utter simplistic taxon-omy-based approach, and the full text-based approach with TF and IDF, both exhibited correlations below 0 . 5formore than 20 topics/documents. Opposed to our approach, their performance was better when using less information, i.e., &lt; 20 topics/documents, still peaking only slightly above 0 . 5. The other three metrics, i.e., Li et. al. , snippet-based with and without IDF, and full text-based without IDF, had correlation scores between 0 . 55 and 0 . 65 for more than 20 topics/documents.
Semantic proximity metrics are becoming an increasingly important component for frameworks geared towards the machine X  X  understanding of human-created sources of infor-mation, such as the Web. Currently, only for small portions of information fragments, namely words and simple concepts stored in thesauri and dictionaries such as WordNet ,se-mantic similarity measures are applicable. By harnessing the combined power of both Google and ODP, we were able to extend semantic proximity to arbitrary concepts, e.g., names of persons, composed concepts, song titles, and so forth. Moreover, we introduced a new taxonomy-based proximity metric that showed significantly better performance than ex-isting state-of-the-art approaches and comes close to human judgement.

For the future, we would like to steer our research to-wards the nature of proximity itself. In other words, besides revealing that two concepts are somewhat related, we would like to reveal the type of their mutual relationship. For in-stance, when supposing the concepts Space Shuttle and Cape Canaveral , an algorithm should inform us that both are related because Space Shuttles are located in Cape Canaveral, and that they are both from the NASA universe. First of all, we would like to express our gratitude towards all the people that have participated in our studies, for devoting their time and giving us many invaluable comments.
In addition, the authors would like to thank Thomas Hor-nung, Karen Tso, Matthias Ihle, and Paolo Massa for fruitful discussions and careful proofreading.
