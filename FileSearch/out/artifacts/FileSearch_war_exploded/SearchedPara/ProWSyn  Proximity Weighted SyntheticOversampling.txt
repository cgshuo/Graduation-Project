 Imbalanced learning problem is to deal with imbalanced data sets where one or more classes have much higher number of data samples than other classes. Generally speaking, imbalanced learni ng occurs whenever some types of data distribution significantly dominate the sample space compared to other data distributions. By convention, in imbalanced data sets, we call the classes having more samples the majority classes and the ones having fewer samples the mi-nority classes. Classifiers tend to produce greater classification errors over the minority class samples [1 X 3]. Many real world problems suffer from this phe-nomenon such as medical diagnosis [4], information retrieval [5], detection of fraudulent telephone calls [6] and oil spills in radar images [7], data mining from direct marketing [8], and helicopter fault monitoring [9]. Thus the identification of the minority class samples is of utmost importance.

There have been many attempts in solving imbalanced learning problems, such as various oversampling and undersampling methods [10]. Roughly speaking, an undersampling method remove some majority class samples from an imbalanced data set with an aim to balance the distribution between the majority class and minority class samples [11 X 14], while an oversampling method does the oppo-site, i.e, generates synthetic minority class samples and adds them to the data set [15 X 19]. Both undersampling and oversampling methods have been shown to improve classifiers X  performance on imbalanced data sets. While comparing oversampling and undersampling one natural observation favoring oversampling is that undersampling may remove essential information from the original data, while oversampling does not suffer from t his problem. It has been shown that oversampling is lot more useful than undersampling and the performance of over-sampling was shown to improve dramatically even for complex data sets [20].
Most of the existing oversampling methods (e.g. [17 X 19]) first try to estimate the difficulty levels of the minority class samples in generating synthetic minority class samples. In doing so, they assign weights to the minority class samples based on a computed value, i.e.,  X  . The methods then use  X  to decide how many synthetic samples are to be generated for a particular minority class sample. In Sect. 2, we illustrate that the way  X  is computed is not reasonable in many scenarios. Consequently, the sample generation process is not able to generate synthetic samples appropriately, which affects classifiers performance.
In this paper, we present a new flexible oversampling method, named Prox-imity Weighted Synthetic Oversampling Technique (ProWSyn). Unlike previous work, ProWSyn uses the distance information of the minority class samples from the majority samples in assigning weights to the minority class samples. The effectiveness of the proposed technique has been evaluated on several bench-mark classification problems with high imbalanced ratio. It has been found that ProWSyn performs better compared to some other existing methods in most of the cases.

The remainder of this paper is divided into five sections. In Sect. 2, we present the related works for solving imbalanced learning problems. Section 3 describes the details of the proposed algorithm. I n Sect. 4, we present the experimental study and simulation results. Finally, in Sect. 5, we provide some future aspects of this research and conclude the paper. The main objective of oversampling methods is to oversample the minority class samples to shift the classifier learning bias toward the minority class. Synthetic Minority Over-sampling Technique (SMOTE) is one such method [15]. For every minority class sample, this technique first finds its k (which is set to 5 in SMOTE) nearest neighbors of the same class and then randomly selects some of them according to the over-sampling rate. Fi nally, SMOTE generates new synthetic samples along the line between the min ority sample and its selected nearest neighbors. The problem of SMOTE is that it does not consider the importance of minority class samples, thus generates an arbitrary equal number of synthetic minority class samples. However, all the minority class samples are not equally important (hard). The minority class samples that are surrounded by many majority class samples or closer to the classifier X  X  decision boundary are more important than the other ones.

Adaptive synthetic (ADASYN) oversampling [17], Ranked Minority Oversam-pling (RAMO) [18] and CBSO [19] try to address the aforementioned problem in dealing with imbalanced learning problems. These methods are based on the idea of assigning weights to minority class samples according to their impor-tance. These weights are used for generating synthetic samples. More synthetic samples are generated for a large weight than for a small weight. CBSO, like earlier approaches [17, 18], uses a parameter  X  , the number of majority samples among the k nearest neighbors of a minority sample x , for assigning weight to x .Let, N k ( x )isthesetof k nearest neighbors of x . Then,  X  for x equals to the number of the majority class samples in N k ( x ). If this number is large, then  X  is high, resulting a large weight assignment to the minority sample [17 X 19]. However, the use of  X  for assigning weights to individual minority samples may not be appropriate in the situations described below. 1.  X  may be inappropriate for assigning weights to the minority class samples 2.  X  may be insufficient to discover the difference of minority samples w.r.t The above scenarios confirms that earlier approaches based on  X  e.g. [17 X 19] cannot effectively assign weights to the mi nority class samples. In most scenarios, if the parameter k is not sufficiently large, then most of the minority samples will get zero weight [17, 19] or the lowest weight [18]. Minority class samples having zero or the lowest weight will get no or very few synthetic samples around them. It may seem the problem can be a voided by increasing the value of k to contain the majority class samples. However, the required value of k cannot be determined in advance. In some regions, a small k may suffice, while in other regions a large k is to be required. Even if k is increased, it cannot solve the skewed distribution of synthetic samples. For example, suppose we increase k to 6 to contain the majority class samples for the minority class sample A (Fig. 1(a)). For this case, N 6 ( A )= { B,C,D,E,F,P } , which contains one majority sample P . Hence, A  X  X  delta will be 1. However, M  X  X  delta will be 4, because N 6 ( M ) contains four majority class samples (Fig. 1(a)). Still, A  X  X   X  is smaller compared to that for M and more synthetic samples will be generated in the neighborhood of M . This justifies that increasing k cannot effectively solve the problem at all. Motivated by problems stated in Sect. 2 , we have extended the recently pro-posed CBSO [19] algorithm and proposed a new improved algorithm, named Proximity Weighted Synthetic Oversampling Technique (ProWSyn). The new algorithm uses a different weight generat ion technique to alleviate the problems described earlier. The complete algorithm is shown in [Algorithm ProWSyn]. Our ProWSyn differs from CBSO in Steps 2 to 7 in which each minority sample x is now weighted based on the proximity level of x i.e., PL x .Wemeasure PL x using the Euclidean distance of x from the majority class samples. Steps 8 to 11 create the synthetic samples and produce an output oversampled minority data set, S omin . The details of weight generation procedure are discussed below. [ Algorithm ProWSyn ] Input: Training data samples D tr with m samples { x i ,y i } , i =1  X  X  X  m ,where x i is an instance in n dimensional feature space X ,and y i  X  X  X  1 , 1 } is the class identity level associated with x i . Define S maj and S min to be the majority and minority class set respectively and m l and m s as the number of majority class and mi-nority class samples re spectively. Therefore, m s  X  m l and m s + m l = m . Procedure: 1. Calculate the number of synthetic samples that need to be generated for the 2. Initialize, P = S min 3. For i =1to L  X  1dothefollowing: 4. Form partition P L with the remaining unpartitioned samples in P . 5. Set proximity level of each x in P L to be L : 6. For each x , calculate a weight w x from its proximity level PL x defined as: 7. Normalize w x according to w x = w x / z  X  S 8. Calculate the number of synthetic samples g x that need to be generated for x : 9. Find the clusters of minority set, S min 10. Initialize set, S omin = S min 11. For each x , generate g x synthetic minority class samples according to the Output: Oversampled minority data set, S omin 3.1 Weight Generation Mechanism of ProWSyn The goal of our weight generation mechanism is assign appropriate weights to the minority class samples according to their importance in learning. To do this, ProWSyn works in two phases. In first phase (Steps 2-5), it divides the minority data set in a number of partitions (say, L ) based on their distance from the decision boundary. Each partition is assigned a proximity level where the level increases with increasing distance from the boundary. Minority class samples with lower proximity levels are the difficult samples and therefore are important for learning, while they with higher proximity levels are less important and may not have significant importance at all. In second phase, ProWSyn generates synthetic minority class samples using the proximity information so that it can generate more synthetic samples in lower proximity regions, i.e., regions that are very near to the decision boundary. The whole procedure is described below (Steps 2 to 7 of [Algorithm ProWSyn]).

From each majority sample ProWSyn finds the nearest K minority class sam-ples according to the Euclidean distan ce. The set of all these minority class samples form the first partition, P 1 (of proximity level 1), the nearest level of samples from the boundary. Then, it finds the next K minority samples accord-ing to distance from each majority. These samples together form the second partition, P 2 (of proximity level 2). In this way, the procedure is repeated for L  X  1 such partitions of proximity levels 1 to L  X  1. The rest of the unpartitioned samples will form partition L , the farthest set of samples from the boundary. A simulated partitioning is shown in Fig. 1(b) for K =3and L = 3. It is seen that the minority class samples are properly identified and partitioned accord-ing to their distance from the boundary, which also signifies their importance in oversampling.

Minority class samples are given weig ht according to their proximity level (Step 6 of [Algorithm ProWSyn]). All minority samples in the same partition, i.e., having same proximity level, gets the same weight. As the level increases, weight of the minority samples also decreases exponentially (Eqn. 3). The pa-rameter  X  in (3) controls the rate with whic h weight decreases with respect to levels.

The above weight generation technique of ProWSyn has several advantages over earlier  X  -based approaches. First, our Pro WSyn can effectively find weight for a minority sample according to its position from the decision boundary, while earlier approaches fail to do so (Sect. 2). Secondly, the proposed method successfully partitions the minority class samples based on the distance from the decision boundary. So, samples closer to boundary get higher weight than samples that are further. This is not guaranteed in earlier approaches (Sect. 2). Thirdly, while earlier approaches may lead to generation of synthetic samples in a few small regions due to positive weights of a few minority class samples (Sect. 2), ProWSyn can avoid it by assigning proper weight values for all minority class samples. Fourthly, the procedure of Pro WSyn is a more general one, we can easily control the size of each partition and nu mber of partitions to be created based on the problem domain by varying the parameters K and L . However, there is no such scope in earlier approaches. In this section, we evaluate the effectiv eness of our proposed ProWSyn algorithm and compare its performance with ADASYN [17], RAMO [18], and CBSO [19] methods. We use two different classifier models: backpropagation neural network and C4.5 decision tree [21]. We collect ten datasets from UCI machine learning repository [22]. The data sets were chosen in such a way that they contained a varied level of imbalanced distribution of samples. Some of these original data sets were multi-class data. Since, we are onl y interested in two-class classification problem, these data sets were transform ed to form two-class data sets in a way which ensures a certain level of imbalance. Table 1 shows the minority class com-position (these classes in the original dataset were combined to form the minority class and rest of the classes form the majority class) and other characteristics of the data sets such as the number of features, the number of total samples, and the number of majority and minority class samples. As evaluation metrics, we use the most popular measure for imbalanced problem domains i.e., the area under the Receiver Operating Characteristics (ROC) graph [23], usually known as AUC. Furthermore, we use two other popular performance metrics such as F-measure and G-mean [10].

We run single decision tree classifier and single neural network classifier on the selected datasets described in Table 1. For the neural network classifier, the number of hidden neurons is randomly set to 5, the number of input neurons is set to be equal to the number of features in the dataset and the number of output neurons is set to 2. We use Sigmoid function as an activation function for neural network. The number of training epochs is randomly set to 300 and learning rate is set to 0.1. For ADASYN and CBSO, the value of the nearest neighbors, K , is set to 5 [17, 19]. The values of the nearest neighbors, i.e., k 1and k 2, of RAMO are chosen as 5 and 10 respectively [18]. The scaling coefficient,  X  , for RAMO is set to 0.3 [18]. For ProWSyn and CBSO, C p = 3 [17, 19]. For ProWSyn, other parameter values are  X  =1, K =5,and L =5.Thesevalues are chosen after some preliminary simulation runs and they are not meant to be optimal. Number of synthetic samples generated is set by  X  = 1 for ADASYN, ProWSyn, and CBSO. Same number of synthetic samples were generated for RAMOforfaircomparison.

Simulation results of F-measure (F-m eas), G-mean, averaged AUC, and stan-dard deviation of AUC values (SDAUC) are presented in Table 2. Each result is found after a 10-fold cross-validation. The best result in each category is high-lighted with a bold-face type. We obtain averaged AUC values by averaging the AUC values of multiple simulation runs [18]. We also provide the number of times an algorithm win (win time) against any other method we compare here. It is observed from Table 2 that for both classifiers, ProWSyn outperforms CBSO, ADASYN, and RAMO algorithms in terms of F-measure, G-mean, and averaged AUC in most of the datasets. As described in Sect. 2,  X  based weight generation technique cannot create appropriate distribution of synthetic minor-ity class samples. Generation of appropriate weight values by the ProWSyn approach leads to better distribution of synthetic samples along the difficult mi-nority class regions making its performance better than other approaches. We apply Wilcoxon signed-rank test [24] to statistically compare the performance (AUC metric) of the four methods. The test is applied to compare our proposed ProwSyn with each of the other methods in a pairwise manner. For 10 datasets, the test statistic, i.e. T should be less than or equal to critical value 8 [25] to reject the null hypothesis at the significance level of 0.05. Table 3 shows the detailed computation of the Wilcoxon statistic, i.e. T for ProsSyn vs. ADASYN results of Decision tree classifier. The obtained value of T = 3 is less than the critical value 8. This proves that ProwSyn is statistically better than the ADASYN. For space consideration, we avoid the detailed computation and show only the test statistic values for all other comparisons in Table 4. We see that ProwSyn statistically outperforms other methods for all comparisons except ProWSyn vs. RAMO for neural network classifier ( T = 10 is larger than critical value 8). In this case, ProWSyn can not statistically outperform RAMO. However, in Table 2, AUC column under Neural Network Classifier shows that ProWSyn wins in 6 cases while RAMO wins 2 cases. This difference in winning time shows that ProWSyn performs better than RAMO for neural network classifier.
 In this paper, we try to identify the problems related to the synthetic sample generation process of oversampling methods in dealing with imbalanced data sets. Existing algorithms [17 X 19] generate inaccurate weights for the minority samples that results in very poor and skewed distribution of synthetic samples (Sect. 2). We thus present a new synthet ic oversampling algorithm, ProWSyn, for balancing the majority and minority class distributions in an imbalanced data set. Our ProWSyn avoids the aforementioned problem by assigning effec-tive weights to the minority class samples using their Euclidean distance from the majority class samples in the data set. ProWSyn partitions the minority data set into several partitions based on samples X  proximity from the decision bound-ary. and uses this proximity information for the minority samples in such a way that the closest sample get highest weight and the furthest ones get the lowest weight. By doing so, our method ensures a proper distribution of weights among the minority samples according to their position from the decision boundary. This results in a effective distribution of g enerated synthetic samples across the minority data set. The simulation results show that ProWSyn can statistically outperform ADASYN, CBSO, and RAMO algorithms in terms of a number of performance metrics such as AUC, F-me asure, and G-mean. Several other re-search issues can be investigated using ProWSyn such as application of ProWSyn in multi-class problems, integration of ProWSyn with some other undersampling methods, and integration of ProWSyn with an ensemble technique such as Ad-aboost.M2 boosting ensemble.
 Acknowledgments. The work has been done in the Computer Science &amp; En-gineering Department of Bangladesh University of Engineering and Technology (BUET). The authors would like to acknowledge BUET for its generous support.
