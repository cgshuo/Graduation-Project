 Despite its intuitive appeal, the hypothesis that retrieval at the level of  X  X oncepts X  should outperform purely term-based approaches remains unverified empirically. In addition, the use of  X  X nowledge X  has not consistently resulted in perfor-mance gains. After identifying possible reasons for previous negative results, we present a novel framework for  X  X oncep-tual retrieval X  that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Exper-iments show that an EBM-based scoring algorithm dramat-ically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval Models Measurement, Experimentation question answering, semantic models, reranking
Although the field of information retrieval has made enor-mous progress in the last half century, virtually all systems are still built on the remarkably simple concept of  X  X ounting words X . Fundamentally, the vector space [35], probabilis-tic [33], inference network [26], language modeling [30], and Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. divergence from randomness [1] approaches can be viewed as sophisticated  X  X ookkeeping X  techniques for matching words from queries with words in documents, under strong assump-tions of term independence. Although these methods have been empirically validated (e.g., in TREC evaluations), it is a simple fact that words alone cannot capture the semantic content of documents and information needs.

This assertion translates naturally into the hypothesis that retrieval systems operating at a level above terms (e.g., con-cepts, relations, etc.) should outperform purely term-based approaches. Unfortunately, studies along these lines, some dating back nearly two decades, have failed to conclusively support this claim (see Section 2). Here, we provide a novel approach to this age-old problem and demonstrate that large gains in retrieval effectiveness are possible in restricted do-mains if semantic knowledge is appropriately utilized.
Our work, which lies at the intersection between document retrieval and question answering, has the ambitious goal of developing knowledge-rich  X  X onceptual retrieval X  algo-rithms. This is accomplished in three steps: first, we outline a general framework that identifies the types of knowledge important to information seeking (Section 3). Then, we in-stantiate this framework in the domain of clinical medicine, mirroring a paradigm of practice known as evidence-based medicine [34] (Sections 4 and 5). Document reranking ex-periments using a collection of real world clinical questions (Section 6) demonstrate that our approach significantly out-performs a state-of-the-art baseline (Section 7). Finally, we explore the contributions of different knowledge sources (Section 8) and discuss how our ideas can be applied to other domains (Section 9).
Research on more sophisticated retrieval models can gen-erally be grouped into attempts to go beyond simple term-matching and attempts to relax term independence assump-tions. Due to space limitations, we only discuss representa-tive works here.

Deeper linguistic analysis of documents and queries rep-resents a popular avenue of exploration. Typical approaches involve application of NLP techniques such as query expan-sion (using ontological resources), word-sense disambigua-tion, and parsing. Previous work has shown that use of lexi-cal semantic relations for query expansion does not increase retrieval performance [38], and neither does indexing syn-tactic structures [14, 37]; although see [9]. Whether word-sense disambiguation helps retrieval is subject to debate [22, 27, 36, 39], but even positive results show modest improve-ments at best. More encouraging is recent work on formal models that attempt to capture term dependencies [16, 25]; experiments have yielded gains, suggesting that the problem lies not with the ideas but their implementation. Neverthe-less,asBelkin[3]pointedoutandBuckleyandHarman[4] confirmed empirically, many difficulties surrounding infor-mation retrieval are not linguistic in nature. It has been suggested by many researchers that information seeking ex-ists in a much broader context involving real-world tasks, different search strategies, users X  cognitive structures, etc. Retrieval models often neglect to account for these impor-tant factors.

We believe that little headway has been made in leverag-ing semantic knowledge in IR because nearly all attempts have occurred in unrestricted domains. Reasoning on any-thing other than a few lexical relations (e.g., using Word-Net) in the open domain is exceedingly difficult because there is a vast amount of world and commonsense knowl-edge that must be encoded, either manually or automati-cally. As an example, the massive commonsense knowledge store Cyc [23] was found to have negligible impact on ques-tion answering performance in a recent TREC evaluation [6]. A promising approach is the use of abductive inferencing techniques to  X  X ustify X  candidate answers [28], which, with substantial knowledge engineering, has produced impressive performance on simple fact-based questions. Nevertheless, it is unclear if these methods can be applied to more com-plex information needs. A possible solution is to sacrifice breadth for depth, as exemplified by recent work on ques-tion answering in restricted domains [29], e.g., terrorism. In a more restricted semantic space, it is much easier to ex-plicitly encode the body of knowledge necessary to support conceptual retrieval.

Our approach differs from previous work in two impor-tant ways: First, we identify linguistic knowledge as one of three types of knowledge critical to the information-seeking process. Second, within a general framework for conceptual retrieval, we present a case study in the domain of clinical medicine, where existing resources can be effectively lever-aged to improve retrieval effectiveness. Through a series of ablation studies, we gain a better understanding of how these different types of knowledge interact.
The idea that information should be retrieved at the con-ceptual level predates the existence of computers themselves; librarians have been building conceptual structures for or-ganizing information long before the invention of computer-ized retrieval systems. Even after the development of full-text search engines, it was well known that  X  X ags of words X  make poor query representations [3]. The idea that systems for retrieval (computer or otherwise) serve to bring the cog-nitive representations (of the user and the collection)  X  X nto alignment X  has been explored within the framework of cog-nitive information retrieval [20], but this line of work has not resulted in computationally implementable models.
Our attempts to develop a framework for conceptual re-trieval begin with an outline of the types of knowledge im-portant to the information-seeking process. In particular, we hypothesize that there are three broad categories of knowl-edge that should be captured by retrieval algorithms:
Based on this framework, we envision retrieval as a pro-cess of  X  X emantic unification X  between representations that encode user information needs and corresponding represen-tations automatically derived from a text collection. This work describes a specific instantiation of this idea in the domain of clinical medicine.
The domain of clinical medicine is an appropriate area in which to explore conceptual retrieval algorithms because the problem structure, task knowledge, and domain knowl-edge are all relatively well-understood. Furthermore, the need to answer questions related to patient care at the point of service has been well-studied and documented [8, 13, 17]. MEDLINE, the authoritative repository of abstracts from the medical and biomedical primary literature main-tained by the U.S. National Library of Medicine (NLM), provides the clinically-relevant sources for answering physi-cians X  questions, and is commonly used in that capacity [7, 10]. However, studies have shown that existing systems for searching MEDLINE (such as PubMed, NLM X  X  online search service) are often unable to provide clinically-relevant an-swers in a timely manner [5, 17]. Better access to high-quality evidence represents a high-impact decision-support application for physicians.

The centerpiece of our approach is a widely-accepted par-adigm for medical practice called evidence-based medicine (EBM), which calls for the explicit use of current best evi-dence, i.e., the results of high-quality patient-centered clin-ical research, in making decisions about patient care. Nat-urally, such evidence, as reported in the primary medical literature, must be suitably integrated with the physician X  X  own expertise and patient-specific factors. It is argued that practicing medicine in this manner leads to better patient outcomes and higher quality health care. One of our goals is to develop accurate retrieval systems that support physi-cians practicing EBM.

Evidence-based medicine specifies three orthogonal facets of the clinical domain, that, when taken together, describe a model for addressing complex clinical information needs. The first facet, shown in Table 1 (left column), describes the four main tasks that physicians engage in. The second facet pertains to the structure of a well-built clinical ques-tion. Richardson [31] identifies four key elements, as shown in Table 1 (middle column). These four elements are of-ten referenced with the mnemonic PICO, which stands for Problem/Population, Intervention, Comparison, and Out-come. Finally, the third facet serves as a tool for appraising the strength of evidence (SoE), i.e., how much confidence should a physician have in the results? For this work, we adopted a taxonomy with three levels of recommendations, as shown in Table 1 (right column).

It should be apparent that evidence-based medicine pro-vides two of the three types of knowledge necessary to sup-port conceptual retrieval. The four clinical tasks ground information needs in broader user activities, and strength of evidence considerations model the pertinence (i.e., non-topical aspects of relevance) in a real-world clinical context. The PICO representation provides a problem structure for capturing clinical information needs. In addition to being a cognitive model for problem analysis (as physicians are trained to decompose complex situations in terms of these elements), PICO frames lend themselves nicely to a compu-tational implementation.

Finally, substantial understanding of the clinical domain has already been codified in the Unified Medical Language System (UMLS) [24]. The 2004 version of the UMLS Meta-thesaurus contains information about over 1 million biomed-ical concepts and 5 million concept names from more than 100 controlled vocabularies. In addition, software for uti-lizing this ontology already exists: MetaMap [2] identifies concepts in free text, while SemRep [32] extracts relations between concepts. In summary, the three types of knowl-edge identified in the previous section already exist in an accessible form.

Integrating these three perspectives of EBM, we concep-tualize retrieval as  X  X emantic unification X  between needs expressed in a PICO frame and corresponding structures extracted from MEDLINE abstracts. This matching pro-cess, naturally, should be sensitive to task-based consider-ations. As a concrete example, the question  X  X n children with an acute febrile illness, what is the efficacy of single-medication therapy with acetaminophen or ibuprofen in re-ducing fever? X  might be represented as:
This frame representation explicitly encodes the clinical task and the PICO structure of the question. After process-ing MEDLINE citations, automatically extracting PICO ele-ments from the abstracts, and matching these elements with the query, a system might produce the following answer:
Many components are required to realize the above ques-tion answering capability: first, knowledge extractors for au-tomatically identifying PICO elements in MEDLINE ab-stracts; second, a citation scoring algorithm that opera-tionalizes the principles of evidence-based medicine; third, an answer generator that produces responses for physicians. This work focuses on the second: an algorithm that inte-grates knowledge-based and statistical techniques to assess the relevance of MEDLINE citations with respect to a clini-cal information need. For identifying PICO frame elements in free text abstracts, we employ previously-developed com-ponents, as described in [11, 12]. By leveraging a combi-nation of semantic and lexical features, we demonstrated methods for very precisely extracting clinically-relevant ele-ments: populations, problems, and interventions, which are short phrases, and outcomes, which are sentences that as-sert clinical findings, e.g., efficacy of a drug for a disease or a comparison between two drugs. The output of these knowledge extractors serves as the input to our algorithm for scoring MEDLINE citations.
What is the relevance of a MEDLINE abstract with re-spect to a clinical question? Evidence-based medicine out-lines the need to consider three separate facets, each of which contributes to the total score:
The relevance of a particular citation is a weighted linear combination of contributions from matching PICO frames, the strength of evidence of the citation, and associated MeSH terms that are indicative of appropriateness for certain clin-ical tasks. In the simplest model, each component is equally weighted, but we also experimented with learning optimal  X   X  X  from training data. Computing S PICO requires knowl-edge about the problem structure, while S SoE and S MeSH both reflect knowledge about user tasks. For a more de-tailed description of the scoring algorithm, see [12].
The following subsections describe how each of these indi-vidual scores are computed. We readily concede that our ci-tation scoring algorithm is quite ad hoc , since many weights are heuristic reflections of our intuition and domain knowl-edge. However, we know of no comparable scoring algorithm in the clinical domain, and no suitable data set (of suffi-cient size) from which to derive model parameters in a more principled fashion. This particular scoring implementation serves as a proof-of-concept, and we leave the development of a more formal model for future work. Furthermore, the primary focus of this paper is not the algorithm itself, but rather an exploration of how different types of knowledge interact in a framework for conceptual retrieval.
The score of an abstract based on extracted PICO ele-ments, S PICO , is broken up into individual components based on each frame element: S
The first component in the above equation, S problem ,re-flects a match between the problem in the query frame and the primary problem identified in the abstract. A score of 1 is given if the problems match based on their UMLS concept id as provided by MetaMap, which essentially performs ter-minological normalization automatically. Failing an exact match of concept ids, a partial string match is given a score of 0 . 5. If the primary problem in the query has no overlap with the primary problem from the abstract, a score of  X  1 is given. Finally, if our problem extractor could not identify a problem (but the query frame does contain a problem), a score of  X  0 . 5isgiven.

Scores based on population and intervention, S population and S intervention , respectively, count the lexical overlap be-tween the query frame elements and corresponding elements extracted from abstracts. A point is given to either a match-ing intervention or a matching population. Our framework collapses the processing of interventions and comparisons because it is often difficult to separate the two (e.g., in an abstract that compares the efficacy of two drugs, which is the  X  X aseline X  and which is the comparison?). A single ex-tractor identifies all interventions under consideration.
The outcome-based score, S outcome , is the value assigned to the highest-scoring outcome sentence, as determined by the knowledge extractor. As outcomes are rarely specified ex-plicitly in clinical questions, we decided to omit matching on them. Our citation scoring algorithm simply considers the inherent quality of the outcome statements in an abstract, independent of the query (akin to changing document pri-ors). Given a match on the primary problem, all clinical outcomes are likely to be of interest to the physician.
Two components of the EBM score take into account task knowledge. The first quantifies the strength of evidence:
Citations published in core and high-impact journals such as Journal of the American Medical Association (JAMA) get a score of 0 . 6for S journal , and 0 otherwise. In terms of the study type, S study , clinical trials, such as randomized controlled trials, receive a score of 0 . 5; observational studies, e.g., case-control studies, 0 . 3; all non-clinical publications,  X  1 . 5; and 0 otherwise. The study type is directly encoded as metadata associated with each MEDLINE citation. Finally, recency factors into the strength of evidence; a mild penalty decreases the score of a citation proportionally to the time difference between the date of the search and the date of publication.

The other scoring component that encodes task knowl-edge is based on MeSH (Medical Subject Headings) terms, which are manually-assigned controlled-vocabulary concepts associated with each MEDLINE citation. For each clinical task, we have gathered a list of terms that are positive or negative indicators of relevance. This score is given by:
The function  X  ( t ) maps a MeSH term to a positive score if the term is a positive indicator for that particular task, or a negative score if the term is a negative indicator. For ex-ample, genomics-related terms such as  X  X enetics X  and  X  X ell physiology X  are negative indicators for all tasks, while  X  X rug administration routes X  and any of its children are strong positive indicators for the therapy task. We have manually identified several dozen indicators and manually assigned weights; see [12] for more details.
Ideally, we would like to apply our scoring algorithm di-rectly to MEDLINE citations. However, this would involve pre-extracting and indexing PICO elements from the 15 plus million entries in the complete MEDLINE database. Unfor-tunately, we do not have access to the computational re-sources necessary to accomplish this. As an alternative, we evaluate our EBM-based citation scoring algorithm in an abstract reranking task. This corresponds to a two-stage processing pipeline commonly seen in question answering systems [19]: retrieval of an initial set followed by postpro-cessing. Our experiments employed PubMed, NLM X  X  gate-way to MEDLINE.

Since no suitable test collection for evaluating our algo-rithm exists, we had to first manually create one. Fortu-nately, collections of clinical questions (representing real-world information needs of physicians) are available on-line. Table 2: Sample clinical questions and frames.
 From two sources, the Journal of Family Practice 2 and the Parkhurst Exchange 3 , we randomly sampled 50 questions, which were manually classified according to clinical task and coded into PICO-based query frames. Our collection was di-vided into a development set and a blind held-out test set (24 and 26 questions, respectively). The exact distribution of the questions over the task types is shown in the head-ings of Table 3; these figures roughly follow the prevalence of question types observed by Ely et al. [13]. One example from each clinical task is shown in Table 2.

For each question, the second author, who is a medical doctor, manually crafted PubMed queries to fetch an ini-tial set of hits. The queries took advantage of PubMed X  X  advanced features and represent  X  X est effort X  from an ex-perienced user; it was verified that each hit list contained at least some relevant abstracts. The process of generat-ing queries averaged about forty minutes per question. The top fifty results for each query were retained for our experi-ments. In total, 2309 citations were retrieved because some queries returned fewer than fifty citations.

All abstracts gathered by the above process were then exhaustively evaluated by the same author. Since all ab-stracts were judged, we did not have to worry about biases when comparing different systems in a reranking setup. In total, the relevance assessment process took approximately 100 hours, or about an average of 2 hours per question.
Our reranking experiment compared four different con-ditions: the baseline PubMed results; hits reranked using Indri, a state-of-the-art language modeling toolkit [26] (us-ing the questions verbatim as queries); hits reranked by the EBM-scoring algorithm described in Section 5; and hits reranked by combining Indri and EBM scores,  X S EBM +(1  X   X  ) S in the crafting of the citation scoring algorithm (especially in the manual determination of weights).

To evaluate retrieval effectiveness, we collected the fol-lowing metrics: mean average precision (MAP), precision http://www.jfponline.com/ http://www.parkhurstexchange.com/qa/ at ten retrieved documents (P10), and mean reciprocal rank (MRR). Mean average precision is the most widely-accepted single-point retrieval metric. Precision at top documents is particularly important in a real-world clinical setting be-cause physicians are often under intense time pressure. Mean reciprocal rank, a metric often used for question answering, quantifies the expected position of the first relevant hit.
Results of our reranking experiment are shown in Ta-ble 3. For the EBM run, each scoring component was equally weighted (i.e.,  X  1 =  X  2 =1 / 3). For the EBM+Indri run, we settled on a  X  of 0 . 85, which optimized performance over the development set. The Wilcoxon signed-rank test was employed to determine the statistical significance of the re-sults; significance at the 1% level is indicated by either or , depending on the direction of change; significance at the 5% level, or ; n.s. is denoted by  X  .

All three conditions (Indri, EBM, EBM+Indri) signifi-cantly outperform the PubMed baseline on all metrics. In many cases, the differences are very dramatic, e.g., the EBM algorithm more than doubles MAP and P10 on the test set (vs. PubMed). There are enough therapy questions to achieve statistical significance in the task-specific results; however, due to a smaller number of questions for the other tasks, those results are not statistically significant. Are differences in performance between Indri, EBM, and EBM+Indri statistically significant? Results of the Wilcoxon signed-rank test are shown in Table 4. For all but one case (MRR on the development set), our EBM scoring algo-rithm significantly outperforms Indri alone X  X hich supports our claim that appropriate use of semantic knowledge can yield substantial improvements over state-of-the-art ranking methods based solely on term statistics. Furthermore, com-bining term-based statistical evidence from Indri with EBM scores results in a small but statistically significant increase in MAP on both the development and test set.

For the above experiments, the PICO, SoE, and MeSH components of the EBM score were weighted equally. Sep-arate experiments reported in [12] attempted to optimize  X  1 and  X  2 using the development set. However, optimal weights did not result in statistically significant differences, suggesting that the performance of the EBM-scoring algo-rithm is relatively insensitive to specific weight settings. We conclude that retrieval performance can be attributed pri-marily to the use of different semantic resources, as opposed to a fortunate setting of parameters.

Nevertheless, it is important to determine the performance contributions of each knowledge component within our con-ceptual retrieval framework. The results of ablation studies that isolate each score component are shown in Table 5. As can be seen, each component contributes significantly to the overall performance, given the fact that using S PICO , S and S MeSH individually results in significantly lower perfor-mance (vs. all three components). In general, the PICO score alone outperforms Indri, but not SoE or MeSH alone.
The domain of medicine represents a fortunate confluence of circumstances in which problem structure, task knowl-edge, and domain knowledge are all readily available. In many domains, one or more components may be missing or 0.502 (+30%)  X  0.686 (+13%)  X  0.630 (+47%) 0.467 (+17%)  X  0.613 (+15%)  X  0.565 (+50%) 0.833 (+13.6%)  X  1.000 (+11%)  X  0.876 (+34%) 0.667 (  X  9.1%)  X  1.000 (+11%)  X  0.910 (+39%) 0.667 (  X  9.1%)  X  1.000 (+11%)  X  0.910 (+39%) 0.533 (+127%)  X  0.439 (+20%)  X  0.544 (+53%) 0.367 (+83%)  X  0.400 (+25%)  X  0.500 (+78%) 0.833 (+83%)  X  0.380 (  X  30%)  X  0.683 (+30%) 1.000 (+119%)  X  1.000 (+85%)  X  0.936 (+78%) 1.000 (+119%)  X  1.000 (+85%)  X  0.936 (+78%) not (yet) computationally accessible. Statistical term-based ranking algorithms have the advantage that minimal effort is required to move from domain to domain. In the cases where only a limited amount of knowledge is available, is it possible to obtain the best of both worlds by combining term-based and knowledge-derived evidence?
Additional experiments with our EBM algorithm shed some light on this question. We conducted a number of runs that combined Indri scores with components of the EBM score by linear weighting,  X S Indri +(1  X   X  ) S EBM* , where represents different ablated variants of the EBM scoring al-gorithm. The weights were tuned using the development set. Results of these experiments are shown in Table 6.
We can see that the availability of any individual source of evidence improves Indri results. In this specific domain, problem structure contributes the greatest, although task knowledge also plays an important role. We can view SoE and MeSH scores as modeling non-uniform priors on the relevance of specific document types, based on the particu-lar task at hand. To conclude, not only can a knowledge-based approach to retrieval yield significant improvements over purely term-based methods, but fragmentary evidence from individual knowledge sources can still be useful.
Since the primary thrust of this research is a general frame-work for conceptual retrieval X  X ith our EBM citation scor-ing algorithm as an illustrative instantiation X  X t is impor-tant to demonstrate the generality of our ideas. In this sec-tion, we briefly discuss how other applications might benefit from similar semantic modeling.

The genomics domain represents a straightforward exten-sion to the work presented here X  X nstead of a physician, the target user would be a biomedical researcher. Domain coverage could be provided by UMLS and other specialized sources, e.g., the Gene Ontology (GO) or Online Mendelian Inheritance in Man (OMIM). As with the clinical domain, there exist generalized categories of information needs, as exemplified by query templates in the TREC 2005 genomics track [18]. An example is  X  X hat is the role of [gene] in [disease] X , fully instantiated in  X  X hat is the role of the gene Transforming growth factor-beta1 (TGF-beta1) in the dis-ease Cerebral Amyloid Angiopathy (CAA)? X  Finally, task knowledge is not difficult to obtain, given the existence of well-defined task models, e.g., determining the genetic basis of a disease or drug discovery.

Beyond life sciences, our framework for conceptual re- X  6.0%  X  +16.2%  X  0.903  X  0.8%  X  +3.0%  X   X  31.0%  X  14.7%  X  0.674  X  25.9%  X  23.1%  X  34.6%  X  19.2% 0.714  X  21.6%  X  18.6%  X   X  24.5%  X  6.6%  X  0.781  X  14.2%  X   X  10.9%  X   X  7.4%  X  +25.4% 0.847  X  9.5%  X  +24.0%  X   X  36.9%  X  14.6%  X  0.644  X  31.1%  X  5.7%  X   X  35.8%  X  13.1%  X  0.663  X  29.2%  X  3.0%  X   X  28.4%  X  3.1%  X  0.677  X  27.6%  X  0.9%  X  trieval is broadly applicable to other domains as well. Here, we briefly discuss three others: patent search, enterprise search, and QA in the terrorism/warfighting domain. For patent search [21], the USPTO maintains an extensive clas-sification system that comprises the core of a domain model. Search tasks and information needs are specific and well-defined, e.g., discovery of prior art. In the realm of enterprise search in workplace settings, Freund et al. [15] have identi-fied four broad categories of information needs ( X  X ow to X ,  X  X hy X ,  X  X hat X , and  X  X how me X ) and patterns of association between tasks (e.g.,  X  X erformance tuning X ) and genres (e.g.,  X  X ookbook X  or  X  X emo X ). The appropriateness of different genres to different tasks parallels Strength of Evidence con-siderations in medicine, and categories of information needs translate naturally into template-based problem structures. In summary, existing resources in the patent and enterprise domains also support a knowledge-based treatment.
Question answering in the terrorism/warfighting domain has become a widely-researched topic, given current fund-ing priorities in the United States, as exemplified by research programs such as AQUAINT and GALE. In this domain, the triplet of problem structure, task model, and domain knowl-edge is available. In terms of problem structure, well-known query prototypes have been studied (paralleling query tem-plates in the genomics track), as well as the representations for reasoning about such problems, e.g.,  X  X ecipes X  for acquir-ing specific weapons of mass destruction. These information needs can be decomposed into simpler structures, which can serve as the basis for a network of related semantic frames that cover the problem domain (e.g., acquire radiological material, build device, etc.). Task models are relatively well specified and functional boundaries are clearly delineated; for example, the interaction between intelligence and opera-tional planning is well understood. Finally, domain-specific ontologies have already been built. All of these elements provide the foundation for conceptual retrieval algorithms that incorporate rich sources of knowledge.
The contributions of this paper are a general framework for conceptual retrieval and a concrete instantiation of the approach in the clinical domain. We have identified three types of knowledge that are important in information seek-ing: problem structure (PICO frames), task knowledge (clin-ical tasks and SoE considerations), and domain knowledge (UMLS). Experiments show that a citation scoring algo-rithm which operationalizes the principles of evidence-based medicine dramatically outperforms a state-of-the-art base-line in retrieving MEDLINE citations. In addition, ablation studies help us better understand the performance contribu-tions of each scoring component. This work provides a tan-talizing peek at the significant advances that can be made in information retrieval based on appropriate use of semantic knowledge, and hopefully paves the way for future work.
This work was supported in part by the National Library of Medicine, where the first author was a visiting scientist during the summer of 2005. We X  X  like to thank D. Oard and D. Soergel for helpful comments. The first author would like to thank Esther and Kiri for their loving support. [1] G. Amati and C. van Rijsbergen. Probabilistic models [2] A. Aronson. Effective mapping of biomedical text to [3] N. Belkin. Anomalous states of knowledge as a basis [4] C. Buckley and D. Harman. Reliable information [5] M. Chambliss and J. Conley. Answering clinical [6] J. Chu-Carroll, J. Prager, C. Welty, K. Czuba, and [7] K. Cogdill and M. Moore. First-year medical students X  [8] D. Covell, G. Uman, and P. Manning. Information [9] H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua. [10] S. De Groote and J. Dorsch. Measuring use patterns [11] D. Demner-Fushman and J. Lin. Knowledge [12] D. Demner-Fushman and J. Lin. Answering clinical [13] J. Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy, [14] J. Fagan. Experiments in Automatic Phrase Indexing [15] L. Freund, E. Toms, and C. Clarke. Modeling [16] J. Gao, J.-Y. Nie, G. Wu, and G. Cao. Dependence [17] P. Gorman, J. Ash, and L. Wykoff. Can primary care [18] W. Hersh, A. Cohen, J. Yang, R. Bhupatiraju, [19] L. Hirschman and R. Gaizauskas. Natural language [20] P. Ingwersen. Cognitive information retrieval. ARIST , [21] N. Kando and M.-K. Leong. Workshop on patent [22] S.-B. Kim, H.-C. Seo, and H.-C. Rim. Information [23] D. Lenat. CYC: A large-scale investment in knowledge [24] D. Lindberg, B. Humphreys, and A. McCray. The [25] D. Metzler and W. Croft. A Markov random field [26] D. Metzler and W. Croft. Combining the language [27] R. Mihalcea and D. Moldovan. Semantic indexing [28] D. Moldovan, M. Pa  X  sca, S. Harabagiu, and [29] S. Narayanan and S. Harabagiu. Question answering [30] J. Ponte and W. Croft. A language modeling approach [31] W. Richardson, M. Wilson, J. Nishikawa, and [32] T. Rindflesch and M. Fiszman. The interaction of [33] S. Robertson, S. Walker, S. Jones, [34] D. Sackett, S. Straus, W. Richardson, W. Rosenberg, [35] G. Salton. A vector space model for information [36] M. Sanderson. Word-sense disambiguation and [37] A. Smeaton, R. O X  X onnell, and F. Kelledy. Indexing [38] E. Voorhees. Query expansion using lexical-semantic [39] E. Voorhees. Using WordNet to disambiguate word
