 Billions of online display advertising spots are purchased on a daily basis through real time bidding exchanges (RTBs). Advertising companies bid for these spots on behalf of a company or brand in order to purchase these spots to dis-play banner advertisements. These bidding decisions must be made in fractions of a second after the potential pur-chaser is informed of what location (Internet site) has a spot available and who would see the advertisement. The entire transaction must be completed in near real-time to avoid de-lays loading the page and maintain a good users experience. This paper presents a bid-optimization approach that is im-plemented in production at Media6Degrees for bidding on these advertising opportunities at an appropriate price. The approach combines several supervised learning algorithms, as well as second price auction theory, to determine the cor-rect price to ensure that the right message is delivered to the right person, at the right time.
 I.5.4 [ Computing Methodologies ]: Pattern Recognition-Applications Probability Estimation, Online Advertising, Causality, Bid Optimization, Logistic Regression
In recent years online advertising has seen a major shift toward real time bidding (RTB) ad-exchanges [7]. As the term suggests, RTBs allow advertisers to bid on the oppor-tunity to show an ad to a specific browser on a specific site with a display ad slot. Advertisers are integrated into the exchanges through APIs and collect different data to decide whether or not they want to bid and at what price. RTBs provide liquidity on both supply and demand and have cre-ated a complex economic ecosystem of publishers, adver-tisers, marketers, and data providers [12]. Media6Degreed (m6d) is one of the players in the online display targeting sector with a main focus on prospecting  X  finding new cus-tomers for a product or brand. We find and target browsers for over 100 marketers delivering millions of ads daily. Be-fore we get into some of the inner workings of this ecosystem, lets have a short and intuitive clarification of the terminol-ogy of the players in the display advertising ecosystem:
Strategies for targeting and bidding vary greatly between different advertisers and are primarily a function of the data an advertiser is willing and capable of using. The ultimate goal from the marketer X  X  perspective is the holy grail of ad-vertising:
We want to address the right browser with the right message at the right moment and preferably at the
Consider for instance search-based advertising: When a browser is entering  X  X ar insurance X  as a search term, it is pretty straightforward to decide that this is a great oppor-tunity for Geico or Allstate to show an ad for their insurance products. However, matching a search term to a product is typically not that simple. The  X  X ight browser X  is any browser who is currently searching for a keyword that is matched ei-ther manually or using machine learning [19] to the product or brand. The browser also defines the right time: the time of the search since he is obviously thinking about the prod-uct at that moment.

Contextual advertising has a similar flavor: if the browser is currently reading a review about used cars, right now might be a good opportunity to show him a car or car insur-ance ad. The challenge is again to associate the content of the page with the product or message. This works well only for a small set of well-understood content and very clearly-defined products.

Both of the above strategies suffer from high competition and limited scale. The number of total searches is finite and determined by the browsers. In cases where the match between the keyword and the product is obvious, the compe-tition for showing the browser an ad is high and as a result the winning bid price in the auction is high as well. The same is true for very well defined content with clear product association. In addition, the latter scenario is typically man-aged through a direct agreement between a larger publisher and the relevant marketers. As a result, few such opportu-nities appear in the auctions. For the majority of products or things one might want to advertise, the association of content and product is highly non-trivial [19].
 The m6d targeting strategy goes beyond this information. Through our data partners we collect historical informa-tion on browsers and use machine learning to build high-dimensional supervised ranking models to identify browsers with a high interest in the brand or high likelihood of buying the product. The actual targeting model is not the focus of this paper and is presented in more detail in [14, 15]. An important distinction between the above strategies and the m6d approach is that the latter targets browsers, not just singluar impression opportunities. In other words, even be-fore the browser gets to the inventory, we already know some measure of fit between him and all our marketers. Using su-pervised machine learning on data that includes positively labeled browsers who already took some relevant brand ac-tion typically provides more relevant information (for con-version prediction) than the match between the content of the current inventory and an advertiser. At the point of the auction, we already have a good measure on the fit between browser and marketer, what we now want to do is adjust the bid price to reflect the last piece of the puzzle: is this the right place and time?
One important consideration in the discussion of the value of an advertising opportunity is the evaluation. We may have some high-level intuition about this, but in reality we have to focus on measurable outcomes to evaluate oppor-tunities as well as advertising strategies. Primarily due to convenience, the industry has historically focused on clicks. This is rather unfortunate, as many studies have shown a rather loose if not inverse relationship between clicks and purchases for display advertising [1, 4]. Instead m6d mea-sures post-view conversions that allow for a certain time pe-riod between the ad impression and some relevant action (e.g. visiting the marketer X  X  website, downloading code, sub-scribing to some service, or purchasing a product). Keep in mind that both the advertiser and the marketer can trace the events of a browser through time using cookies (of course only as long as they do not get deleted).

Yet another layer to the optimization puzzle is attribu-tion. My incentive as an advertiser is not just to increase the conversion rate or brand recognition of my customer, but to be fairly compensated for it. Attribution in the con-text of multi-touch and view through conversion is an entire research topic by itself [16, 5] and we will leave it for future work to tie our bid strategy to the attributed value. The biggest problem from our perspective is the complete lack of visibility. We do not know who else might have shown an ad to the browser and ultimately receives the credit for the conversion. Unfortunately, short-sighted attribution can have a strong impact on optimization strategies that are overall suboptimal for the marketer (and the industry in general). Last touch attribution leads for instance to a  X  X ar-pet bombing X  strategy where some advertisers might try to buy huge volumes of cheap inventory that is not even seen by the browser. Even though attribution is not the focus of this paper, we are assuming a  X  X air X  attribution method where the advertiser has the incentive to evaluate opportu-nities with respect to the impact on a browser X  X  conversion probability.

So to recast, we want to bid on the right browser with the right message at the right inventory at a price that reflects the true value of the opportunity to the marketer.
The rest of the paper is organized as follows: Section 2 will give a brief overview of the m6d targeting and bidding to clarify the constraints under which the bid optimization solution was developed. Section 3 initially presents consid-eration on why and how inventory affects the value of an opportunity and proposes a formal measure for opportunity value. Section 4 covers the estimation task and some mod-eling results. Finally, section 5 focuses on translating model predictions into bid strategies and measuring the actual per-formance impact on 15 advertising campaigns.
In order to perform bid optimization one must first be able to determine the value of a particular inventory that is placed up for auction. However, determining the quality of a particular inventory is a challenging task because past decisions to show ads on a given inventory depend in part on the browser X  X  propensity to convert. In other words, inven-tory quality is confounded by the many different decisions that m6d has made as a company that make it possible to operate our large scale targeting machine. As a result, the quality of browsers that we observe at a piece of inventory is not equal across all inventory. Understanding the different pieces of the m6d infrastructure and how the components of that infrastructure contribute the quality of browsers at a particular inventory is essential for valuing each piece of in-ventory. For the purpose of this paper we will take a browser centric view and visit the sequence of events that ultimately lead to an impression of a particular ad to a particular user at a particular inventory. By illuminating this process one can better understand what issues contribute to the differ-ential quality of browsers across the different inventories on which m6d receives bid requests. We are deliberately skip-ping the core m6d targeting component that builds a con-version propensity model for every campaign. Details can be found in [15]. The following steps highlight the process of browsers through the m6d system: 1. Initiate -A browser is first observed by m6d either 2. Monitor -m6d anonymously tracks the browser X  X  In-3. Score and Segment -The browsing history is used 4. Sync with Exchange -Once a browser is placed into 5. Activate Segment -Account managers determine 6. Receive Bid Request -Subsequently, m6d begins 7. Bid -m6d first determines internally the set of active 8. Show Impression -If the m6d bid was the highest 9. Track Conversion -Weobservethebrowserforsome 10. The Cycle -The entire process repeates where more 11. Cookie Deletion -The browser at some point deletes
We typically observe browsers/users multiple times dur-ing the day browsing different sites. The goal of this work is to quantify the value of a specific opportunity O where an opportunity is given by a triplet O =( U, A, I )ofanad A for a marketer to a user U at a particular inventory I . Given the value of this opportunity we want to adjust the bid price B accordingly. So what is the relationship between the inventory and the browser X  X  probability of converting? Inventory relates to the probability of conversion in two dis-tinct ways. The first is independent of the ad whereas the second reflects an interaction between the inventory and the ad experience.
 Organic Propensity In some cases, the mere fact that the person visits a particu-lar inventory can change our best estimate of his conversion probability. Reasons for this are: 1. Contextual Relevance: There is a correlation be-2. Current Intentions: If the browser is already en-3. Life Expectation: Presumably, there can be a corre-Causal Impact The objective of advertising is of course to increase the browser X  X  propensity to convert. The effects below capture interactions between the inventory and the causal effect of showing an ad. 1. Perceptiveness: Even beyond the current intentions, 2. Impression Quality: There has been recently a lot
While it might be possible to quantify and model all 5 different factors independently, the true reason why a piece of inventory is particularly good is not nearly as relevant as a reliable measurement of the relative value of one piece of inventory over another.
 Optimization Objective So far we have focused on the conversion probability. How-ever, the value of an opportunity depends on the objectives of the marketer and the advertiser. In an auction-theoretic framework [18], the optimal bid price of a second-price auc-tion is determined by the expected value of the outcome where V ( c ) is the value of a conversion. In reality, for online targeting, V ( c ) ends up playing only a minor role in bid optimization. For all intents and purposes, V ( c ) is typically unknown or simply assumed to be constant across all users. In that case, the value of the opportunity is proportional to the probability of conversion. This implies that the bid price should be proportional to the conversion probability as well.
In conclusion, we want to modify our bid proportionally to the effect of the inventory. If showing an ad on a particular piece of inventory doubles the probability of conversion over some random inventory, the bid price should be twice the average bid price as well.

The formulation in Equation 2 has another advantage: it is independent of a campaign X  X  base conversion rate and can be handled by our system the same way no matter what the base rate of the campaign is. In particular, we can easily integrate it with the base bid price B determined by the account management team as a multiplier:
One last observation on Equation 2 is its close relation-ship to the notion of relative impact in the context of causal analysis. Our definition of value compares the outcome of servinganadatinventory i to the expectations over all counterfactual events of showing the ad at different inven-tories j . However, in order for a counterfactual analysis to produce unbiased estimates of a true causal effect (both in observational and experimental settings), several strict as-sumptions must be made about the data that we are not fully considering here [17].
Equation 2 states a well defined supervised modeling prob-lem. We need to estimate p ( c | u, i, a ) for all pieces of inven-tory i . We will estimate this quantity for each advertising campaign a independently leaving us with p a ( c | u, i ). The inventory i is originally provided in form of an URL. M6d parses the URL to a meaningful level. A subset of relatively common canonical URLs are assigned unique inventory IDs and the remaining long tail distribution of URLs is typi-cally collected in an exchange-specific DEFAULT inventory. Note that the inventory is exchange-specific. This implies that the same hostname (e.g., www.facebook.com )canmap into multiple inventories. In total we maintain about 5,000 unique inventories.
What do we know about the user at the time of the auc-tion? We know the identity of the browser and could in principle retrieve all relevant information from the cookie to estimate p a ( c | u, i ). But is a tall order for sophisticated targeting in real time. However, we already have a highly op-timized and pre-calculated estimate of the user X  X  propensity to convert p a ( c | u ) from the m6d targeting that has assigned the browser into a segment of only the good prospects for a given campaign. So given the time constraint of a real time auction  X  where one of the worst case scenarios is timing out and not being able to bid  X  the only really relevant browser information is his segment s . And since trafficking and base price are affected by segment, we have to control for it anyway. Consider if all the best prospects are targeted in exchange E 1 and the lower prospects in E 2 .Thiswould induce a major bias that makes the inventory in exchange E 1 look great unless we control during model estimation for all the variables that drive the trafficking decision.
To estimate inventory values for a given campaign, we need a sample of impressions delivered across multiple inven-tories, along with the segment information of each targeted user. In addition, we track for each impression whether or not the user converted according to the rules of the cam-paign. For a campaign where the marketer insists on eval-Figure 1: Number of impressions in a 3 week period and conversion rates for the 100 largest campaigns. uating the click through rate, we can alternatively use only clicks as positives. Note that this leaves us with an interest-ing sparse representation where every example has exactly 2 binary non-zero features: one for the segment and one for the inventory.

Before we discuss the data for estimation, let us consider the dimensionality of the problem. We currently have 5,000 unique inventories across 20 exchanges, about 1,000 of which have notable volume. In addition, the average campaign uses between 10 and 50 differen t segments. The estimation task in its simplest form has an effective dimensionality of  X  1,000. If one was to consider interaction terms, the dimen-sionality grows easily by an order of magnitude.

Conversion rates on the other hand are typically low and range between 0.04 in the best case and below 0.001 for others. This implies that the desirable amount of data for estimation is rather large. As a result, we typically use three weeks of impression data. Figure 1 shows typical impression counts and conversion rates for several of our campaigns. On average we show 15 million impressions, observe 250,000 conversions (mostly site visits) at an average conversion rate of 1%.

Our models are by default built against post-view conver-sion for a 7 day conversion window. The m6d system auto-matically assigns conversions to the last impression prior to the conversion. At this point it can be debated whether the implicit  X  X ast touch X  attribution of this labeling is adequate. To make the estimation process efficient, we downsample the negative set from to a base conversion rate of 20% in the training set of each campaign. Note that this will affect the raw predictions and in turn the performance ratio, so we have to post-correct the model predictions [10].
 Model Estimation and Feature Selection We estimate p ( c | s, i ) using L1-constrained logistic regression (with the default penalty weight) as implemented in the bbr software package [8]. Having an L1 penalty implicitly selects Segment 0.729 1.802 1.744 1.461 Seg&amp;Inventory 0.7636 2.121 1.952 1.524
Delta 0.0346 0.318 0.207 0.062 %Improved 97% 96% 95% 95% %Delta 4.7% 17.6% 11.8% 4.3% p-value 0 0 0 0 Table 1: Out of sample performance of the segment &amp; inventory models is significantly better across dif-ferent metrics (AUC and Lift at different percentage cutoffs) than the segment-only model. The p-value is for a paired t-test is always below 10  X  15 . Account-ing for inventory almost never hurts targeting per-formance. only the relevant features and provides a sparse solution. For many inventories, the parameter is set to zero and as a re-sult the estimated impact factor is 1 and will not change the bid price (see the formal explanation below in Equation 4). Additionally, we applied an initial filter that included an inventory feature only if its volume was sufficient to yield at least 5 conversions in expectation given the base conversion rate of the campaign. Otherwise it was assigned to a de-fault  X  X ther X  bucket. We need to include all observations to ensure the calibration of the model when we implicitly inte-grate over all inventories to calculate the expected value.
The careful reader might have noticed some built-in de-pendence at this point. In the current setting, the sign of the inventory effect is bound to be the same across all seg-ments. In a linear model without interaction terms, the relative effect of the inventory will always point in the same direction (either greater or smaller than one) because the es-timated parameter on the inventory indicator is either pos-itive or negative. As a result, for a given campaign, the inventory score from Equation 2 can only differ in relative magnitude across segments, but this is purely a function of the different conversion rates of the segments. This effect is intuitively appealing if we consider the drivers of inventory impact from section 3. It is hard to imagine that there is a very strong non-monotonic interaction effect between the conversion rate p ( c | s ) and the inventory i . Either the inven-tory is of high quality with the impression in view or not. Something similar can be said for the current intention of the browser and his state of mind. Nevertheless we also build the models allowing for interactions between the segment and the inventory. Note that this increases the parameter space significantly (about a factor of 10) and typically hurts model performance due to overfitting.
 Estimating  X   X  i We do not need to explicitly integrate over all inventories to estimate E j [ p ( c | u, j, a )]. Instead we use the empirical distribution of j in the data. All that is needed is to build the same models as before on the exact same dataset but using only the segment variables as features.

This is also a great baseline to evaluate whether there is indeed any predictive information in the inventory i that can be picked up by a model (see next paragraph). Only if Table 2: Example inventory scores  X   X   X  i for a hotel chain campaign. inventory can provide additional explanatory power above and beyond the segment would we consider to use it for bid optimization.

At this point we have two models for each marketer a , one that can predict  X  p a ( c | s, i ) and one that predicts  X  Both are miscalibrated due to the initial downsampling of negatives. We correct both probability estimates according to the sample rate [10] and arrive at: Model Performance Before using this in production we would like to assess how much information the inventory can provide beyond ordi-nary m6d targeting. Table 1 compares the out-of-sample performance (using cross validation) of the estimates of  X  and  X  p ( c | s ). The results for both AUC and lift at different percentage cutoffs are very encouraging with significantly higher performance of the inventory model. The AUC in-creases on average by 0.03, which corresponds to a 5% in-crease in performance due to the additional information from inventory. Similarly, lift (more relevant to targeting) in-creases as well, obviously more so for higher cutoffs. Lift 20 calculates the lift of the top 20% of browsers who saw and ad compared to all browsers who were shown an ad. Note that the reported lift is only within the 1-3% browsers out of all browsers known to our system, that actually saw an ad for the specific campaign. Recall that the set of eligible browsers are identified by our main targeting model based on their browsing history and a re selected because they have the highest conversion probabilities. So the reported lift is not the lift of our full targeting and bid optimization, but only the lift of inventory selection on top of the regular selec-tion. As a results, this is a vast understatement of the true lift of our entire campaign where the baseline set of browsers contains the other 97-99% that we deliberately chose not to show ads to.
 Example Predictions Figure 2 shows the distribution of the estimates of  X   X   X  all inventories i for a particular hotel chain campaign. The scores are well-calibrated around 1 and show some promis-ing signal for inventory that represents travel sites. Table 2 shows some example scores for the hotel chain campaign. Not surprisingly, travel sites have an extremely high score. They meet most of the influence criteria outlined in section 3 for the organic category. Dating and social media sites, on the other hand, do not seem very suitable for hotel advertise-ment. They probably score low across multiple dimensions of inventory impact (bad target, low on current intent and low on perceptiveness).
 Figure 2: Distribution of the estimates of  X   X   X  i for a hotel chain campaign. The mean score was 1.03.
 The high-scoring inventories are all travel sites.
Satisfied by the above results, we integrated the inventory scores into the m6d production system and studied the ef-fectiveness of several inventory-based bid strategies on cam-paign performance. We present results on 15 campaigns come from a variety of industries including telecommunica-tions, services, retail, travel, and others. The main selection criterion for choosing campaigns was a sufficient post-view site-visit rate to allow for significance in the results on ap-proximately 2 weeks of data. We anticipated a performance increase of 5% lift. We back out the required number of impressions given the empirical conversion rate that we see for that campaign and include only campaigns that deliver a sufficient number of impressions such that a 5% increase should be significant under the assumption of a binomial dis-tribution. Some subset of campaigns meeting this criterion were excluded since the account management team felt that they were unfit for experimentation. The characteristics of the the trial campaigns are very similar to those shown in Figure 1.
Using the inventory score in real time requires a very short response time. The technical implementation uses a lookup table by campaign, inventory, and segment that only keeps non-one scores to limit the size and increase the speed of the lookup. We have not observed a notable increase in response time or timeouts since we implemented the inventory-based bid strategies. M6d has a number of other bid strategies that are inventory independent and are not discussed further. However, the design of the bid optimization as a multiplicative modifier allows for any arbitrary combination of strategies provided they are all centered at 1 for neutrality.

During fall 2011, m6d implemented a flexible bidding plat-form that can test multiple bid strategies on the same cam-paign simultaneously. For this purpose, we assign each browser randomly to exactly one strategy even if they qualify for multiple segments of the same campaign.
We evaluate the bid strategies with respect to: 1. Conversion Rate (PVSVR): Percentage of impres-2. Cost per Acquisition (CPA): This metric combines
Both measures are of practical relevance and related to the previous discussions. The majority of m6d X  X  customer agree-ments specify a cost per mille (CPM) payment model where we are paid independently of the conversion rate. However, the conversion rate is considered by the customer as a perfor-mance metric to compare us against other vendors. CPA is directly relevant to our margin. For a given conversion per-formance we would like to minimize the CPA or alternatively for a given cost, we will try to deliver more conversions. Note that from the marketer X  X  perspective on a CPM campaign there is a direct relationship between the PVSVR the CPA. In more general terms, CPA measures economic efficiency and should be closely related to strategy 1. Lets assume for the moment that the actual cost is equal or proportional to the bid price. Under strategy 1 we would expect that the tradeoff between CPA and PVSVR is better than the subop-timal strategy 0  X  either CPA is lower for approximately the same conversion rate or higher conversion rate for similar CPA. Setting the bid price proportional to the conversion probability does not maximize PVSVR. As a matter of fact, the optimal strategy for maximizing the conversion rate is to neglect any cost calculation and simply bid the practical equivalent of infinity for every opportunity above a certain threshold that is determined by the total desired number of impressions.

However, this is clearly not in our best interest consider-ing margins. Strategy 2 is closer to this extreme whereas
Campaign Measure Str 0 Str 1 Str 2 Table 3: Bid optimization performance in terms of PVSVR and CPA for identifying new customers (Prospecting). Bold indicates better performance, while not necessarily significant. strategy 1 is aimed at economic efficiency in terms of the cost of acquisition. We have a number of campaigns that are evaluated with respect to the cost per acquisition.
Tables 3 and 4 show detailed results for the three bid strategies. A  X  X A X  indicates that this marketer did not use that particular strategy. Given the results m6d decided that overall strategy 2 held more promise and ran only strategy 2 during the second stage of our experiment. We have sep-arated the results into two browser groups: those with very high conversion rate (retargeting) and those with lower con-version rates (prospecting). Recall that the model is esti-mated jointly over both populations.

In both populations we observe that the more aggressive strategy 2 outperforms strategies 0 and 1 in terms of the post-view site visit rate (PVSVR) by a large margin. This is very consistent with our expectations. The performance increase over the baseline is on average 24% for retargeting and 21% for prospecting. For comparison, a lift of 20% is considered quite substantial in marketing and this does not even include the core m6d targeting component. So the performance gains realized through strategy 2 are very impressive and as a result this strategy got rolled out to a larger number of campaigns than strategy 1.

In terms of CPA, we see that strategy 2 is on average 5%
Campaign Measure Str 0 Str 1 Str 2 Table 4: Bid optimization performance in terms of PVSVR and CPA for predicting the return of an ex-isting customer (Retargeting). Bold indicates better performance, while not necessarily significant. lower than strategy 0 in retargeting, beating the baseline in 7 of 15 cases, but 18% higher for prospecting and exceeds the baseline in all but one campaign. The decline in the CPA performance was expected for strategy 2. It is a positive surprise that strategy 2 is actually better both in terms of CPA and PVSVR for the retargeting group.

The reason for the CPA increase for prospecting is that the model estimates are affected more by the retargeting population due to the notably higher conversion rate and as a result, the model is more accurate for retargeting. But even in the prospecting case, the cost increase for strategy 2 reflects only a 10-20% reduction of our margin, which is ac-ceptable when we need to drive conversions. This suggests that we may not implement this strategy across all cam-paigns but rather use it as a tool to manage performance more effectively when renewal or the size of a followup deal is under consideration.

Strategy 1 beats strategy 0 on 7 out of 8 campaigns in terms of PVSVR for both browser groups and realizes an average lift of 6% on prospecting and 3% on retargeting. However, it provides economic efficiency gains (CPA) over the the baseline in only 4 out of 8 cases -clearly not sig-nificant in either group. In summary we observe a higher conversion rate at equal CPA: an overall better economic position.

We would like to finish the discussion with an interesting observation. In most CPM campaigns where we are paid for a fixed number of impressions independently of the conver-sions, we are still subject to comparative evaluation against other advertisers in terms of the implied CPA. Note that our PVSVR translates directly into the marketer X  X  measure of cost per acquisition. So while our direct incentive may not be to increase the conversion rate, in a repeated game where we have to win renewal or a larger budget in direct compar-ison to other advertisers, it is very much in our interest to trade short term margin for long term growth.
Ad exchanges and real time bidding are fairly new and have revolutionized the advertising industry only in the last 1-2 years. Most reports are short blogs or white papers with very limited technical and performance details. The ma-jority of published related work is in the more established application area of search advertising [2, 11, 6, 13]. In search term bidding the auction is for a given search term and the bid price determines the relative position in the ranking of the returned offers. One major difference in this setting is the payout structure. In our case, we always pay if we have the winning bid whereas in most of search advertising the bidder only pays his exact bid price if the offer was clicked on by the browser. That puts a larger burden of analytical op-timization on the search provider (Google, Yahoo, etc.) and gives them the incentive to develop more elaborate auction styles where the bid price may not be the sole decision cri-terion. In addition, the bid price is not really determined in real time and is not browser specific. The advertisers could typically submit and change their bid price for a given term at any time throughout the day. The predictive modeling component typically evolves around estimating the condi-tional click propensity and on the side of the advertiser the expected revenue given click.
Bid optimization has become one standard component of the production system of m6d targeting. We developed a predictive modeling approach that estimates reliably and efficiently the relative impact of inventory on the conversion probability. It is interesting to note the scale of the under-lying models: We are estimating close to 1,000 parameters by downsampling from an average of 50 million impressions per campaign semi-automatically with a model refresh rate of about 2 weeks. The scores can be applied as factors to the base-prices previously set by the account management team. The empirical results suggest a clear improvement of the campaign performance in terms of conversion rate that can be used to tune the campaign to satisfy the perfor-mance expectations of the marketer. One factor that could currently limit our performance is the internal selection of the campaign with the highest (post-adjustment) bid price. Since we are picking the max of a distribution, this can lead to an over-inflation of the bid price and amplify overfitting (the highest scores in most models are not necessarily the most reliable). Understanding this effect is one of out imme-diate future work efforts and should lead to reliable cross-campaign optimization. In addition we are embarking on a deep dive into the modeling process with a stronger focus on the prospecting group. The main advantage of pooling the two groups is a favorable bias -variance tradeoff. How-ever we think there is potentially a better solution that can improve the prospecting CPA further. Finally we see great potential in expanding the scope of bid optimization to bring together in real time additional information from the cookie, such as age and activity level, and combine the performance-centric optimization approach with other tuning parameters. One such parameter is the frequency of ad exposure for a given marketer. There is presumably a diminishing causal impact of each subsequent impression on browser conversion. Future bid optimization will consider the impression history for that browser in addition to inventory considerations. [1] L. Abraham. Is the click the right currency for display [2] G. Aggarwal, A. Goel, and R. Motwani. Truthful [3] C. Chabris and D. Simons. The Invisible Gorilla . [4] B. Dalessandro, F. Provost, C. Perlich, and R. Hook. [5] B. Dalessandro, O. Stitelman, C. Perlich, and [6] B. Edelman and M. Ostrovsky. Strategic bidder [7] B.Edelman,M.Ostrovsky,andM.Schwarz.Internet [8] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale [9] M. Hughes. Taking issues with viewable impressions, [10] G. King and L. Zeng. Logistic regression in rare events [11] B. Kitts and B. LeBlanc. Optimal bidding on keyword [12] M. Muthukrishnan. Data mining problems in internet [13] B. Edelman M. Ostrovsky and M. Schwartz. Internet [14] F. Provost, B. Dalessandro, R. Hook, X. Zhang, and [15] T. Raeder, B. Dalessandro, O. Stitelman, C. Perlich, [16] X. Shao and L. Li. Data-driven multi-touch [17] A.A. Tsiatis. Semiparametric theory and missing data [18] W. Vickrey. Counterspeculation, auctions, and [19] C. Xiong, T. Wang, W. Ding, Y. Shen, and T.Y. Liu.
