
This paper presents a data oriented approach to model-ing the complex computing systems, in which an ensemble of correlation models are discovered to represent the sys-tem status. If the discovered correlations can continually hold under different user scenarios and workloads, they are regarded as invariants of the information system. In our previous work [12], we have developed an algorithm to au-tomatically search the invariants between any pair of sys-tem attributes, which we call local invariants. However that method is unable to deal with the high order dependency models due to the combinatorial explosion of search space. In this paper we use Bayesian regression technique to dis-cover those high order correlation models, called global in-variants. We treat each attribute as a response variable in turn and express its dependency with the other attributes in a regression model. By adding the prior constraint of Laplacian distribution to the regression coefficients, we can find the solution in which only the correlated attributes with respect to the response have nonzero regression coefficients. After that we further consider the temporal dependencies of those extracted attributes by incorporating their past obser-vations. We also provide a confidence metric and a valida-tion procedure to measure the reliability of learned models. If the model does not break down in the validation, it is regarded as a true invariant of the system. Experimental results on a real wireless networking system show that the discovered invariants can be used to effectively detect sys-tem failures as well as provide valuable information about the failure source.
With the rapid advances in networking and computing technology, we are facing an explosive growth of com-plexity in networked applications and information services. Typical systems such as Google, Yahoo! and Amazon con-sist of thousands of components including operating sys-tems, applications software, servers, and storage and net-working devices, which are widely distributed across the system. Since the system complexity appears to be ap-proaching the limits of human capability, it is crucial to de-velop autonomic management tools to reduce the burden of system operators.

Meeting the grand challenges of such self-management requires advanced techniques t o correctly characterize the system state. One approach to achieving this relies on the expert knowledge of system structure and turns such prior knowledge into a set of event-condition-action rules to model the system [1][8]. However, the models and rules provided by those approaches are usually system dependent and may be incomplete and in accurate. Recently statistical techniques [5][4][6] have been drawing a surge of interest in modeling the system activity given that large amount of measurement data can be co llected from system monitor-ing tools. This data contains many implicit evidences about the system structure and activity. By discovering those evi-dences from the data and correlating them to the high level system behavior, we can effectively characterize the system status and hence facilitate many management tasks. How-ever, there are usually huge amount of attributes and ob-servations in the measurement data due to the large scale of computing systems, which makes the data modeling a big challenge. For instance, commercial frameworks such as HP X  X  OpenView [7] and IBM X  X  Tivoli [11] aggregate at-tributes from a variety of sources including hardware, net-working, operating systems, application servers, and so on. It is hard to create a single powerful model that can effec-tively describe the system and efficiently deploy in practice.
To address this issue, this paper presents an invariants based solution to model the behavior of information sys-tems. We learn an ensemble of correlation models among system attributes from a large amount of measurement data. If the dependency models con tinually hold under different user scenarios and workloads, they are regarded as invari-ants of the computing system. For example, the frequency of SQL queries issued in the backend database in a web based system is usually correlated with the frequency of HTTP requests issued at the front end as a result of fixed business logic of the application. Such relationship is usu-ally difficult to be quantitatively described by the system op-erators. Our algorithm can automatically discover those im-plicit constraints in the system to assist operators to under-stand the system internal status. As a result, many manage-ment tasks can be facilitated such as failure detection, ca-pacity planning, policy generation, and so on. For instance, by online checking the number of  X  X ealthy X  invariants, i.e., those models that fit the online measurements well, we can identify anomalies during system operation.

In our previous work [12], a brute force search method has been developed to explore all the pair-wised correlation models in the system, which we call local invariants. In dis-tributed systems, however, some attributes may depend on more than one other attributes. For instance, the input of a load balancer is always equal to the summation of multiple output variables. If we use the method in [12] to explore high order correlation models, which we call global invari-ants, we need to check all the models with various combi-nations of system attributes. In order to avoid the problem of  X  X ombinatorial explosion X  for large scale computing sys-tems, this paper uses a statistical mining based approach to discovering global invariant models for the system. Given the measurement data, we use the regression model to ex-press the correlation between each attribute and its related partners. That is, we treat each attribute as a response vari-able in turn and express its dependency with the correlated attributes in a regression model. Since we do not know how many attributes are relevant to the response and what those attributes are, we put all the rest attributes in the regression as possible independent variables at the beginning. We add Laplacian priors on the regression coefficients and formu-late the dependency discovery in the Bayesian regression framework. As a result, we can expose a subset of attributes that have statistically significant relationship with the re-sponse. If such high order relationship is strong enough, we further explore the temporal dependency between the response and discovered subset of relevant attributes by in-corporating their past observations. We also provide a confi-dence metric and a validation procedure to measure the reli-ability of learned models. If the model does not break down in the validation, it is regarded as a true invariant of the sys-tem. Comparing with the local invariants as described in [12], the global invariants reveal more complex dependency relationships among the system attributes.

We have successfully applied the discovered invariants to the management of a 3G te lecommunication system. It shows that the discovered global invariants are necessary additions to the previous learned local invariants. Combin-ing local and global invariant s can present a more compre-hensive and effective characterization of the system behav-ior. The discovered invariants have been successfully ap-plied to detect and localize the system failures. Experimen-tal results based on the real system data demonstrate that the percentage of broken invariants is an effective indication of the failure occurrence. In addition, it is easy to pinpoint the suspicious attributes from the broken invariant set, which significantly facilitates the failure localization task.
To support the autonomic management of computing systems, several statistical approaches have been proposed in the literature. In the project of Magpie [2], Barham and his colleagues collected request traces from software com-ponents in the distributed system, and used the stochastic context free grammar to mode l the request X  X  control flow for the purpose of detecting component failures as well as localizing perform ance bottlenecks. The Pinpoint project [5], a close relative to Magpie, used request path shapes and component interactions as two features to represent the user request flow across the system. The probabilistic con-text free grammar (PCFG) and  X  2 statistical test were em-ployed to model those features for detecting high level soft-ware failures in application servers. In the same context of component interaction analysis as in [5], [4] put forward a subspace decomposition based algorithm to explore the correlations between interaction profiles of multiple compo-nents. A simplified Bayesian network model was presented by Cohen et al. [6] to discover the root cause of service level agreement (SLA) violations. In the work [3], Bodik et al. made use of the user access behavior as an evidence of sys-tem X  X  health, and applied several statistical approaches such as the Naive Bayes to mining such information for detecting application level failures.
Figure 1. Examples of invariants in informa-tion systems. (a)The input-output relation-ship in a load balancer; (b)Packet volume vs. number of SQL query in a database.

The above approaches mos tly focused on either a spe-cific type of system measurements such as the request traces, or a specific system management task. Compared with them, our invariant bas ed solution can accept the data measurements from various sources such as software log files and network traffic statistics, and generalize well for many system management scenarios. We observe that there exist invariant properties of system measurements since most of the attributes in measurement data are strongly cor-related. For example, the resource utilizations of the sys-tem such as CPU and memory usages always increase or decrease in accordance with the change of system work-loads. Furthermore, the system structure and design also introduce a lot of correlated attributes. As shown in Fig-ure 1(a), assuming that a load balancer has one input I and three outputs O 1 , O 2 and O 3 , we should always have I = O 1 + O 2 + O 3 because this is the physical property of the load balancer. Meantime, in a web system, the volume of network packets V 1 going through the database, as shown in Figure 1(b), should always be correlated with the number of queries issued in the datab ase because of the underlying logic of system components. We believe there are many of such constant relationships among the measured attributes. As a consequence, an ensemble of invariant models can be discovered to correlate the large amount of monitoring data collected from various points o f the system. Each invariant contains a specific dependency relationship among the mea-surement attributes, which reveals some specific aspects of system activity. If we put all the discovered invariants to-gether we can achieve a comprehensive view of the system situation and facilitate many management tasks. For exam-ple,  X  By monitoring the number of  X  X ealthy X  invariants dur- X  Since each invariant is usually represented as a depen- X  The discovered invariants can assist system adminis-
The invariants are usually identified by analyzing the his-torical system measurements . Given a set of monitoring data z l , l =1 ,...,n , with each measurement containing p attributes, z i  X  R p , we divide the data into two parts: one is used for training, and the other is for the validation. The purpose of training is to build dependency relationships among the data attributes, which serve as the hypotheses of system invariants. The validation process uses the second part of data to mimic actual system dynamics and sequen-tially validate the effectiveness of hypotheses under those situations. The hypotheses that pass the validation process are regarded as true invariants of the information system. In Section 3 we describe the work [12] of modeling the local invariants in the training process. Section 4 then presents our proposed method of discovering system global invari-ants. The validation of those discovered correlation models are described in Section 5.
The proposed system invariant model has to consider both spatial and temporal dependencies among system at-tributes. While spatial relationship concerns the correla-tion across different attributes, the temporal correlation is related to dependency in consecutive measurements of each attribute along time. In the work [12], a brute force search method is used to build correlation models between any pair of attributes x and y from the data z , y = f ( x ) .Therela-tionship between x ( t ) and y ( t ) is learned by the AutoRe-gressive model with eXogenous inputs (ARX) [13] where [ n, m ] is the order of the model, which determines how many previous steps are affecting the current output. a i and b j are the coefficient parameters that reflect how strongly a previous step is affecting the current output. Let X  X  denote then Equation (1) can be rewritten as:
Assuming that we have observed the measurements over a time interval 1  X  t  X  N , O N = { x (1) ,y (1) , ..., x ( N ) ,y ( N ) } , the parameter  X  can be ob-tained by the least squares solution
In order to evaluate how well the learned model (1) fits the measurement data, the follo wing fitness score is defined where  X  y ( t |  X  ) is the estimated value of y based on equation (4) and  X  y is the mean of the real output y ( t ) . A higher score F (  X  ) indicates that the model fits the observed data better and its upper bound is 100. Only a model with high fitness score is selected to characterize the data relationship. We can set a range of the order [ n, m ] to learn a list of model candidates and select a right model from them according to the highest fitness score. For more detailed description of local invariants extraction, please see [12].

Given measurements with p attributes, the above method needs to construct p ( p  X  1) / 2 models as all possible can-didates of local invariants, i.e., pair wise dependency mod-els. If we want to discover the high order dependency mod-els among attributes, however, the above brute force search does not scale well because it re quires the search of all the models with various combinations of system attributes. In the following we propose a statistical mining based method to discover those high order correlations.
While the local invariants reveal the correlations between any pair of measurement attributes, the global method ex-plores the relationship between each attribute y and multi-ple other attributes  X  x =[ x 1 ,x 2 ,  X  X  X  ,x s ] , 2  X  s where p is the total number of attributes in the measurement data z . Since all the attributes in the data are measured as time series, we have to model both the spatial and temporal correlations among the attributes. To simplify the problem, we separate these two stages in the algorithm. In the first step, we explore all the contemporary dependencies among attributes, e.g. identify the subsets of attributes that exhibit strong spatial correlations. Given the measurements z with p attributes, we treat each attribute y as the response in turn and express the dependency between y and other attributes in a regression model. Since we do not know how many attributes are relevant to y and what those attributes are, at the beginning we put the other p  X  1 attributes in the model as possible independent variables y ( t )=  X  1 x 1 ( t )+  X  2 x 2 ( t )+  X  X  X  +  X  p  X  1 x p  X  1 We use the Bayesian technique to trim the regression coef-ficients  X  i s. That is, by choosing proper prior distribution for the regression coefficients, we can force the coefficients whose associated attributes have no relationship with the re-sponse y to be exactly zeroes. As a result, we can extract a subset of variables, whose corresponding coefficients are not zero, as correlated variables with respect to the response y .

We use the fitness score as described in (6) to measure the significance of the statistical relationship between y and the extracted attributes  X  x . If it is strong enough, we further explore their dependency by inc orporating their past obser-vations
Note this time only a small number of attributes, x 1 ,  X  X  X  x , are involved in the time series modeling. We still use the Bayesian approach but with slight modification to identify the function (8). Eventually we obtain a model that contains both the spatial and temporal correlations between y and its related attributes.

Figure 2. The workflow of extracting global invariant hypotheses.

Figure 2 presents the workflow of extracting global de-pendency models. In Section 4.1, we focus on the extraction of spatial correlations among attributes. Section 4.2 then models the temporal correlations among attributes.
This section focuses on the modeling of spatial depen-dencies among attributes. We rewrite equation (7) in a sim-plified way where  X  =[  X  1 , X  2 ,  X  X  X  , X  p  X  1 ] is the regression coeffi-cients. The common solution such as the least squares esti-mation for equation (9) is to maximize the likelihood where [ x l y l ] is the l th observation of measurement data and  X  2 is the variance of residuals l = y l  X  x l  X  , l = 1 ,  X  X  X  ,n . In order to extract a subset of variables that have relationship with the response y , we make a prior assump-tion of the distribution on regression coefficients  X  and use Bayesian method to find the maximum a posterior solution of (9). It has been shown in [9] that if we choose the Lapla-cian prior on each coefficient  X  i we can shrink the coefficients of irrelevant input vari-ables to exactly zero. That is, given the data set D = [( lowing posterior distribution instead of equation (10), we will get a model in which the coefficients of irrelevant attributes are zeroes. Since the variance  X  2 in (10) is also unknown, we incorporate it into the optimization process and revise the posterior (12) as The parameter  X  in the Laplacian prior (11) is a predefined constant. Here we choose  X  =0 . 1 .

Since the Laplacian distribution (11) is non-convex, we rewrite the Laplacian prior (11) as a hierarchical decom-position of two other distributions: a zero-mean Gaussian prior p (  X  i |  X  i )= N (  X  i | 0 , X  i ) with a variance  X  exponential hyper prior As a result, the distribution (13) can be rewritten as
Suppose we could observe the values of new parameter  X  =[  X  1 , X  2 ,  X  X  X  , X  p  X  1 ] ,then p (  X  |  X  )=1 and the poste-rior distribution (15) is simplified because both p ( y | and p (  X  |  X  ) in the right side of equation (15) are Gaussian distributions. We write the log-posterior of the right side in equation (15) where the matrix H =[ x 1 , x 2 ,  X  X  X  , x n ] ,and  X (  X  )= verse variances of all  X  i s. By taking the derivatives with respect to  X  and  X  2 respectively, we obtain the solution that maximizes (16).

In reality, however, since we do not know the values of  X  (and hence the matrix  X (  X  ) in (16)), we can not maximize the equation (16) directly. Instead the follow-ing expectation-maximization (EM) algorithm is used to find the solution. The EM algorithm is an iterative process which computes the expectation of hidden variables  X  and uses such expectation as the estimation of  X  to find the op-timal solution. Each iteration consists of two steps.  X  the E-step computes the conditional expectation of  X  The M-step carries out the maximization of (16) with The EM algorithm is easy to implement, and converges to the maximum of posterior probability (13) quickly.
The coefficient vector  X   X  computed from the above EM algorithm contains many zeroes. By selecting those at-tributes with nonzero coefficients, we obtain a subset of variables  X  x =[ x 1 ,x 2 ,  X  X  X  ,x s ] that are correlated with y as well as a function  X  f to express such dependency We measure the fitness of function  X  f using the metric (6), in which  X  y is the estimated value of y based on equation (22). If the confidence score is low, we discard that model. Other-wise, we further consider the temporal dependency between y and the subset  X  x =[ x 1 ,x 2 ,  X  X  X  ,x s ] by incorporating their past observations. Such dependency can be described in the following multivariate autoregressive model with eX-ogenous inputs
The values [ n, m ] denote the maximum time lags for y and x s respectively in the autoregressive modeling. If we de-fine the equation (23) can be rewritten in the same format as (4), and its solution can be obtained by the equation (5).
Note we put the same maximum time lag m for all attributes x 1 ,  X  X  X  ,x s in equation (23). In reality, how-ever, different attributes may have temporal dependency with y ( t ) up to different maximum time lags. For exam-ple, some attributes may only have contemporaneous de-pendency with y ( t ) ,e.g. m =0 . If we place the same maximum order m for all variables and solve the equation (23) by traditional least squares method, the results may not be accurate because many irrelevant variables are involved in the model (23). Those irrelevant parts have to be removed in the estimation process.
Figure 3. The effects of different  X  values on the Laplacian distribution.

We still use the Bayesian method in Section 4.1 to prune the irrelevant variables. The difference here is that we are more interested in pruning those inputs with longer time de-lays because those inputs are more likely to be irrelevant to the response y ( t ) . As a result, we choose different prior pa-rameter  X  values in the Laplacian distribution (11) for dif-ferent coefficients to control the level of penalties for those coefficients. Figure 3 plots the shape of Laplacian distribu-tion with different  X  values. It shows that a large  X  value strengthens the constraint that the coefficient  X  will be zero. On the other hand, small  X  value conveys our expectation that the corresponding variable is correlated to the response. We utilize such knowle dge and choose different  X  values for the variables with different time lags in (23). While the coefficients a 1 ,  X  X  X  ,a n and b 01 ,  X  X  X  ,b 0 s in (24) are added with Laplacian prior (11) with parameter  X  , the coefficients b ,  X  X  X  ,b is , i =1 ,  X  X  X  ,m are imposed with the same prior distribution with parameter observations are penalized more severely. The same EM al-gorithm described in Section 4.1 is used to obtain the MAP solution  X  . The only modification is that in the E-step the matrix V ( t ) in equation (19) is replaced with because of different  X  values. We learn a list of model can-didates from a range of orders [ n, m ] and then select the one with the highest fitness score (6) as the final global invariant hypothesis associated with y ( t ) .
Given the measurement data, we treat each attribute as the response variable in turn and use the algorithm pre-sented in Section 4 to obtain following high order corre-lation model where k 1 ,  X  X  X  , k s are the maximum time lags for each vari-able. As a consequence we get a set of correlation models. However, those models only fit the training data well. They may not work properly for other data that contains different user scenarios and system workloads. The purpose of val-idation is to use another data set called  X  X alidation data X  to test the validity of invariant hypotheses and hence remove the unqualified ones. Those correlation models that survive the validation process are rega rded as system invariants. For simplicity we express the equation (27) as the y ( t )= F (  X  ) , where  X  denotes all the coefficients in the model  X  =[  X  a 1 ,  X  X  X  ,  X  a n ,b 0 , 1 ,  X  X  X  ,b k 1 , 1 ,  X  X  X  ,b
We divide the validation data into K windows, each of which contains l measurements. For every window W i ,we use its data to fit each hypothesis  X  and calculate the fit-ness score F (  X  ) given by Equation (6). Further we select a threshold F to determine whether the model fits that seg-ment of data or not After receiving k such windows of data, 1  X  k  X  l , we can calculate an overall confidence score for the invariant hy-pothesis  X  which is the average of fitness scores computed from previous k windows We compare the value p k (  X  ) with predefined threshold P .If p (  X  ) &lt;P , the invariant hypothesis  X  is discarded. Those hypotheses that survive in all the test windows are finally regarded as the invariants of system. Note in the implemen-tation the confidence p k (  X  ) in (30) can be online updated for each window of dataset We use the real monitoring data from a Universal Mobile Telecommunication System(UMTS) to demonstrate the re-sults of extracting both local and global invariants in the system. We also apply the discovered invariants to detect and localize failures in the system.

The UMTS is the third generation (3G) wireless net-work, which is divided into a Core Network (CN) and UMTS Terrestrial Radio Access Network (UTRAN) as shown in Figure 4. The CN is responsible for switch-ing/routing calls and data connections to external networks. The UTRAN consists of a set of radio network subsys-tems(RNSs) connected to the CN. Figure 4 plots one of the RNS subsystems, which is composed of a radio net-work controller (RNC) and one or more Node Bs. While RNC is in charge of controlling the use of radio resources, the Node B is responsible for rad io transmission/reception to/from the User Equipment(UE). Asynchronous transfer mode (ATM) is chosen as the transport technology to inter-connect the UTRAN elements such as Node Bs and RNCs.
Two sets of data have been collected from two radio net-work subsystems(RNSs) in the UTRAN network system. Table 1 gives a rough description of the two data sets. We see that both RNSs cover over 550 radio cells and the data are sampled at 15 minutes interval. While the data from RNS-A contains 556 attributes, there are 731 attributes in the RNS-B data set. They contain comprehensive infor-mation about the system behavior such as load, capacity, resource availability and accesses, and service quality. Fig-ure 5 plots some of those attribute curves. With this large amount of data, it is hard for operators to check each indi-vidual attribute and their correlations manually. In the fol-lowing we extract both the local and global invariants in each RNS and use those invariants to monitor the status of RNSs.

Figure 5. Some attribute curves in the data from RNS-A.
We divide each data set into thr ee parts for training, val-idation and testing respectively. For the data from RNS-A, we use the first 5 days measurements as training data, the next 6 days data as the validation set, and the last 4 days as the test set. For the data from RNS-B, both training and val-idation sets contain 2 days meas urements and the test data contains 1 day measurements.

Figure 6(a) presents the results of invariants extraction in RNS-A. After initial training, we have extracted 6449 in-variant hypotheses from the data, in which 6282 candidates are learned by the local method and the remaining 167 are from the global method. The 6 days validation data is then divided into 6 validation pha ses, with each phase contain-ing 1 day measurements. In each validation window, we calculate the confidence score for every invariant hypothe-sis and discard those hypotheses with confidence lower than 85. The curve in Figure 6 exhibits the number of remaining invariant hypotheses after each sequential validation phase. We see that although the number of valid hypotheses shows a rapid decrease at the first three validation windows, it changes a little during the last three validation days. This tells us that the remaining candidates after validation are stable under different system variations and can be regarded as invariants of the system. Eventually we discover 565 in-variant models, in which 423 are local invariants and 142 are global ones. Compared with the initial number of in-variant hypotheses, we see that a large percent of local hy-potheses disappear after the validation due to their sensitiv-ity to system dynamics. This is because many local rela-tionships identified from the training data only reveal par-tial dependencies between attributes, which can not always hold under different system variations. On the other hand, the global models are not likely to be affected by various system dynamics. Most of the global hypotheses continue to hold after the validation process.
Figure 6. The invariants sequential validation results in (a) RNS-A and (b) RNS-B.

The invariants extraction in RNS-B presents similar re-sults, which are shown in Figure 6(b). After training with 2 days data, we obtain 7524 invariant hypotheses includ-ing 7349 local models and 175 global candidates. We then divide the validation data into 9 windows to sequentially validate those hypotheses. It shows that the number of valid hypotheses becomes almost stable after the third validation phase. Eventually 851 invariants have been discovered in which 706 invariants are local ones and the remaining are global invariants. One interesting observation is that the percentages of survived local and global invariant hypothe-ses are very close to those from the RNS-A data. It is prob-ably because of the similar underlying physical properties of two systems even though they have different capacities and configurations.
Since the learned invariants reflect the system internal property and are robust to normal system dynamics such as the workload variations, we can monitor the system runtime status by checking the consistencies of those invariants dur-ing system operation. That is, we use the test data as sam-ples to test the invariant model. As shown in Figure 7, given a new measurement z new , we select relevant attributes from z new to get x new and y new based on the invariant equation y = f ( x ) .The x new is then fed to the invariant model to generate the estimated output  X  y new . We calculate the differ-ence between  X  y new and the real value y new as the residual We set a threshold for the residual R to determine whether the invariant model is broken or not. The threshold value is set as 1.2 times the maximum value of historical residu-als collected from the validation phase. We count the num-ber of broken invariants for each coming measurement as the evidence of system X  X  health. Only when that number is larger than certain threshold, which is the 10 percent of total invariants in our experiment, we regard that the system has encountered an unexpected failure.

Figure 7. Residual generation for each new measurement based on the invariant model.

Figure 8(a) presents the number of broken invariants for each test sample from RNS-A, i n which the X axis is the index of test samples and the Y axis denotes the number of broken invariants. The maximum of Y axis is the to-tal number of discovered invariants in RNS-A. From that Figure, we see that only a small portion of invariants (less than 5 percent) are occasionally broken for the test sam-ples. Therefore, we conclude that the system works well during the testing period, which is actually consistent with the groundtruth we got from the original data source.
Figure 8. The number of broken invariants on the test data from (a)RNS-A (b)RNS-B.

We use the same strategy to monitor the status of RNS-B on one day test samples. In this case, we are told that the test data of RNS-B contains a failure which happened at 7:30am and was resolved two hours later. The results of our detector for that test data are presented in Figure 8(b). It shows that the number of broken invariants suddenly increases to over 200 at the 30th test sample, which is exactly the time when the actual failure occurred g iven that each measurement is sampled at 15 minutes interval. Furthermore the number of broken invariants immediatel y drops at the 38th test sample, two hours after the detected failure which coincides with the actual time of failure settlement. From this example we see that the set of discovered invariants does provide a real time view of the system internal state. We also plot the num-ber of broken global and local invariants in Figure 9(a) and (b) respectively. It shows that both global and local invari-ants present strong evidences about the incidence of failure. Combining the global and local invariants can present more comprehensive description of the system behavior.
Figure 9. The number of (a) broken global in-variants and (b) broken local invariants on the test data from RNS-B.

After a failure has been detected, we look into the bro-ken invariants and their related attributes to provide infor-mation about the failure source. We build a dependency matrix and use the market data analysis [10] to determine a list of attributes that are likely to co-occur with the failure. An example is shown in Figure 10 to illustrate such process. There are five attributes m 1 ,m 2 ,  X  X  X  ,m 5 in the figure, and six invariants have been discovered among those attributes which are plotted as lines connecting the related attributes. After the failure, four invariants are broken. To find the most suspicious attributes, we build a dependency matrix in Table 2. Each row in that matrix presents the information of one invariant: the column titled  X  X roken X  represents whether the invariant is broken ( X 1 X ) or not ( X 0 X ), and the other five columns show whether each of th e five attributes is involved in the invariant ( X 1 X ) or not ( X 0 X ). We define the values in the  X  X roken X  column as a vector T b and those in other columns as T i ,i =1 ,  X  X  X  , 5 . The Jaccard coefficient is applied to indicate the relevance of each attribute to the failure which is based on the number of 1 s in the intersection set divided by the number of 1 s in the union set between T b T . The attributes with higher Jaccard coefficient are con-sidered more suspicious with respect to the failure. In the example of Figure 10, the Jaccard coefficients for the five attributes are 0.2, 0.2, 1, 0.2, and 0.2 respectively. Obvi-ously m 3 is the most suspicious attribute.

Figure 10. Illustration of broken invariants af-ter the component m 3 breaks down.

We then calculate the Jacarrd coefficient for each at-tribute after we detected the RNS-B failure shown in Figure 9. Based on the system operator, such failure is caused by the broken DHT component in the system. In our diagno-sis results, the DHT related attribute is ranked as the third highest among all system attributes. Note the attribute with the highest Jaccard coefficient may not be the exact failure root cause because of the cascad ing effects of failures. Our strategy is to provide a list of highly suspicious attributes to the system operator to help them for the failure localization. Although the final root cause is determined by the system operator, our ranked attributes can still provide useful clues to guide the localization.
Besides the previous two data sets, we have tested our in-variant based failure detection technique by using more than one year X  X  system operation data (ranging from 12/2004 through 4/2006) from nine radio network systems(RNSs). Along with the data, a trouble ticket file is provided by the system operator in which 337 different system failures have been recorded. The logged failures belong to either of two categories based on their confi dence level. The first cate-gory, which includes 46 cases, is related to the failures with strong confidence. Each failure in that category has a clear problem description such as DHT failure, partial power out-age and so on. The second failure category contains the remaining 291 cases, which do not have clear problem de-scriptions and may be just false alarms. According to the original source, only 30 percent of the second category fail-ures are real ones and the others are false alarms.
We first extract and validate t he invariants for each RNS based on their normal data. The test data for each failure case is retrieved based on the failure equipment ID, prob-lem start time, and the resolved time recorded in the trouble ticket file. We use the percentage of broken invariants as the anomaly score to determine whether there exists a failure or not. Figure 11 plots the histogram of the anomaly score for the two categories of failures respectively. We see that the anomaly score for the first category of failures shows a
Figure 11. Histogram of the anomaly score for (a) the first category and (b) the second category of failures. uni-modal distribution with the peak at around 0.25. On the other hand the histogram for the second category of failures shows a bi-modal distribution. This is consistent with our original knowledge because the second category contains a large amount of false alarms. If we choose the anomaly score 0.1 as the threshold, around 84.8 percent of recorded cases in the first category are reported as failures, and 33.6 percent of cases in the second category are regarded as fail-ures. Here we assume the first category failures are all true failures. Therefore our tec hnique can achieve the detection rate 0.848. For the second failure category, however, we do not have any groundtruth for the recorded cases. If we assume 30 percent of recorded cases are true failures, as described by the field operators, we can achieve the false positive rate 0.336 -0.3*0.848 = 0.08. Compared with the current detection tool used by the system operators, our de-tection technique can reduce the false alarms significantly while still keep the satisfactory detection rate. Due to the confidentiality of the test data, we do not plan to describe the testing results in more detail.
This paper has presented an invariants based framework to model complex information systems. In addition to the brute force local invariant search, we have proposed a sta-tistical mining based algorithm to extract global invariants that represent multivariate dep endencies among attributes. Combining local and global i nvariants can provide a more comprehensive and effective characterization of the system behavior, and hence facilitate many system management tasks. Experimental results from a real wireless system have demonstrated that the discovered system invariants can be used to effectively detect and localize failures in the system.
Since the discovered invariants capture the essentials of system state, the set of broken invariants under failure can serve as a  X  X ignature X  of system anomaly condition. One direction of our future work is to associate each signature with its related problem and solution, and build a database to store such information. By defining an appropriate metric to measure the similarity between different signatures, we can quickly identify and solve recurrent system failures by retrieving similar signatures in the database.

