 Text reuse occurs in many different types of documents and for many different reasons. One form of reuse, duplicate or near-duplicate documents, has been a focus of researchers because of its importance in Web search. Local text reuse occurs when sentences, facts or passages, rather than whole documents, are reused and modified. Detecting this type of reuse can be the basis of new tools for text analysis. In this paper, we introduce a new approach to detecting local text reuse and compare it to other approaches. This comparison involves a study of the amount and type of reuse that oc-curs in real documents, including TREC newswire and blog collections.
 H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods Algorithms, Measurement, Experimentation Text reuse, fingerprinting, information flow Text reuse and duplication can occur for many reasons. Web collections, for example, contain many duplicate or near-duplicate versions of documents because the same in-formation is stored in many different locations. Local text reuse, on the other hand, occurs when people borrow or pla-giarize sentences, facts, or passages from various sources. The text that is reused may be modified and may be only a small part of the document that is being created.
Near-duplicate document detection has been a major fo-cus of researchers because of the need for these techniques in Web search engines. These search engines handle enormous collections with a great number of duplicate documents. The duplicate documents make the system less efficient in that they consume considerable system resources. Further, users typically do not want to see redundant documents in search results. Many efficient and effective algorithms for near-duplicate document detection have been described in the literature [1, 4, 5, 6].

The obvious application involving local text reuse is pla-giarism detection, but being able to detect local reuse would be a powerful new tool for other possible applications involv-ing text analysis. For example, Metzler et al. [15] discussed tracking information flow, which is the history of statements and  X  X acts X  that are found in a text database such as news. This application was motivated by intelligence analysis, but could potentially be used by anyone who is interested in ver-ifying the sources and  X  X rovenance X  of information they are reading on the Web or in blogs.

Local text reuse detection requires different algorithms than have been developed for near-duplicate document de-tection. The reason for this is that, in the case of local text reuse, only a small part (or parts) of a document may have been taken from other sources. For example, state-of-art near-duplicate detection algorithms like the locality sensitive hash [5] assume a transitive relation between doc-uments. That is, if a document A is a near-duplicate of document B, which is a near-duplicate of document C, then document A should be a near-duplicate of document C. A text reuse relationship based on parts of documents, how-ever,violatesthisassumption,asshowninFigure1.

In this paper, we focus on algorithms for detecting local text reuse based on parts of documents. In Section 2, we dis-cuss the related literature. In Section 3, we expand on the idea of local text reuse by introducing categories of reuse. These categories are the basis of our experimental evalua-tion. In Section 4, we introduce a novel algorithm for local text reuse detection called DCT fingerprinting . This algo-rithm is evaluated for efficiency and effectiveness in Section 5. In Section 6, the local reuse detection algorithm is used to measure the amount and type of text reuse that occurs in TREC news and blog collections.
There have been broadly two approaches to text reuse detection. One approach is using document fingerprints through hashing subsequences of words in documents. This approach is known to work well for copy detection. Shiv-akumar and Garcia-Molina [19, 20] and Broder [3] intro-duced efficient frameworks. Since handling many finger-prints is too expensive, various selection algorithms for fin-Figure 1: Text reuse by partial text of documents (local text reuse), where  X  is an indicator function that is 1 if two documents have no text reuse rela-tionship. A document A and a document B have a text reuse relationship by text T1. Document B and a document C have a relationship by text T3. But, Document A and C have no such relationship. gerprints were proposed by Manber [14], Heintze [9], Brin et al. [2] and Schleimer [18]. Broder et al. [4] suggested an efficient near-duplicate algorithm generating new finger-prints (super-shingles) by hashing sequences of fingerprints again. Charikar [5] introduced a hashing algorithm based on random projections of words in documents. Henzinger [10] empirically compared a variant of Broder et al X  X  algorithm and Charikar X  X  algorithm on a large scale Web collection. Chowdhury et al. [6] and Bernstein and Zobel [1] proposed filtration algorithms for fast near duplicate detection.
Another approach is computing similarities between doc-uments in the Information Retrieval sense. Shivakumar and Garcia-Molina [19] and Hoad and Zobel [11] suggested simi-larity measures based on relative frequency of words between documents. Metzler et al. [15] compared similarity measures using an evaluation corpus that was developed for studies of local text reuse.
Most algorithms for near-duplicate detection use a chunk or a shingle as a text unit [2, 4, 9, 14, 18]. A chunk or a shingle generally represents a substring or a subsequence of words. Since these chunks are based on small pieces of text, they may be expected to also work well for our local text reuse detection task. Therefore, the techniques mentioned later use this approach. The drawback with this approach is that some forms of text modification will not be captured.
In order to efficiently perform text reuse detection, the string form of a chunk must be converted into a numeric form. Generally, the conversion is performed by hashing algorithms such as MD5 [17] or Rabin fingerprinting [16]. For convenience, we refer to the hash value as the chunk instead of the string. We use a subset of a set of chunks generated in a document to represent the document for text reuse detection. Chunk values in the subset are referred to as fingerprints and the process for obtaining them is referred to as fingerprinting .
A text reuse relationship is a pairwise relationship. Given a pair of documents, we need to estimate the amount of text shared between the two documents. The amount of Table 1: Definitions of text containment terms Term Most Considerable Partial Range C ( A, B )  X  0 . 8 C ( A, B )  X  0 . 5 C ( A, B )  X  0 . 1 text of document A that is shared with document B can be represented as a ratio of the number of shared fingerprints to the number of fingerprints of document A. The ratio, containment of A in B [3] is estimated as follows: where F A and F B are sets of fingerprints of document A and B, respectively.

Note that the shared fingerprint ratio is a non-symmetric metric, i.e. C ( A, B ) = C ( B, A ). Generally, symmetric met-rics like resemblance [3] have been used for near-duplicate detection because it has to be determined whether the es-timated value is greater than a threshold in order to easily check if the document pair has a near-duplicate relationship. Since our goal is to understand more ge neral forms of text reuse rather than simply judging near-duplicate documents, we use the non-symmetric metric that contains more infor-mation.

We divide containment values into three ranges as shown in Table 1. That is, if greater than 80%, 50% or 10% of the total fingerprints of document A are shared with a document B, then we say that most , considerable or partial text of document A is reused by document B. These thresholds are not fixed but may be changed based on the properties of collections or goals of the text reuse application. Here, we set the values based on reviewing results for various collections.
General text reuse occurs in various levels. Most of the text of a document might be shared with other documents, or only several words of a document might be shared with other documents. As a basis for evaluating local detection techniques and investigating the frequency of text reuse, we classify text reuse relationships into six categories as shown in Table 2. For example, if partial text of document A is shared with document B and the shared text is most text of document B, then document A and document B have a C 3 type relationship.

Note that in a broad sense, C 1, C 2and C 4 correspond to near-duplicate cases, whereas C 3, C 5and C 6 correspond to local text reuse. We now briefly describe each category or type. An analysis of real collections based on these cate-gories is presented in Section 6.
The fingerprints of a document are numerical representa-tions for text reuse detection and, for local reuse detection, should represent as much as possible of the content of the document. For example, if the fingerprints of documents are hash values of the first three sentences of each document, then they do not capture enough of the content. Documents can be very different even if the fingerprints of the docu-ments are identical.

For efficient text reuse detection, an inverted index is gen-erally built with fingerprints extracted from documents. To find all documents which have text reuse relationships with a document A, we first read all inverted lists of the fingerprints of document A, then merge the lists, and finally, find text reuse relationships according to the rules in Section 3. The first step is the most critical in time complexity because it requires significant I/O access, whereas the other steps can be performed in main memory. Since the maximum length of the inverted list is the number of documents in the collec-tion, this can be naively thought as an O ( Mn ) algorithm, where M and n are the number of the fingerprints of doc-ument A and the number of documents in the collection, respectively.

On real collections, however, the length of the inverted list is at most the occurrence count of the most frequent finger-prints in the collection. Moreover, we can restrict the upper bound of the length by setting very common fingerprints to stop-fingerprints in the same way as stop-words in Informa-tion Retrieval. Therefore, the practical time complexity is O ( Ml ), where l is the restricted length of the inverted list such that l n .

When we try to discover all text reuse relationships in the collection, the above process is repeated n times, where n is the number of documents in the collection. This is an O ( nml ) algorithm, where m is the average number of the fingerprints of a document.

Since the length of the inverted list is not only invariant regardless of the fingerprinting methods but also generally small, the average number of the fingerprints m is more crit-ical than the length of the list l for efficiency.
In sum, good fingerprinting techniques for text reuse de-tection should satisfy the following properties.
Considering accuracy and efficiency, we introduce several fingerprinting methods. There are broadly two kinds of fingerprinting techniques for text reuse detection: overlap methods and non-overlap methods. We first review the over-lap methods and introduce the non-overlap methods later. 1
Overlap methods use a sliding window. Basically, the win-dow is shifted by a word, and a word sequence in the window or its hash value is handled as a chunk. If the size of the window is k , i.e. the i th window contains the i th word to the i + k  X  1 th word in the document, then the i th chunk in document D is computed as follows:
C ( D, i )= h ( t ( D, i ) ,t ( D, i +1) ,  X  X  X  ,t ( D, i + k where h and t ( D, i ) are the hash function and the i th in document D .

As you see, k  X  1 of the words that appeared in the pre-vious window appear in the current window again. That is, methods based on the sliding window are referred to as overlap methods in that the adjacent windows overlap each other.

Generally, overlap methods generate many chunks, but show good performances. However, since processing a large number of chunks as fingerprints can be too expensive, chunk selection techniques have been introduced [9, 14, 18]. k -gram is the simplest technique of the overlap methods. It uses all the chunks generated from each sliding window as fingerprints. Thus, the number of the fingerprints of docu-ment D is computed as follows: where L ( D ) is the term count of document D .

As k -gram uses all chunks, it generally shows good per-formance. However, it might be infeasible in big collections because of too many fingerprints.
Instead of using all the chunks generated by the sliding window, 0modp tries to select some of them as fingerprints [14]. A random selection of chunks would reduce the number but we cannot predict which chunks would be selected. If different chunks are selected each time, then two documents may be determined to be different even when they are iden-tical. Therefore, all chunk selection methods have to satisfy the property that the same chunks should be selected for identical documents. 0modp selects only chunks such that C ( D, i ) mod p  X  0. When two documents are identical, chunks in the documents are the same. Assuming that the chunk values are uniformly distributed, the expected number of selected chunks, i.e. the number of fingerprints of document D ,isgivenby: That is, 0modp can reduce the number of the fingerprints by a factor p .

This method may however not represent the whole doc-ument accurately. For example, although the upper halves
Although we only describe cases of using subsequences of words for fingerprinting in this paper, all methods intro-duced here can be used for fingerprinting based on sub-strings. of two documents are identical, the lower halves might be different. In the worst case, if chunks in either the upper halves or the lower halves are selected, then the detection algorithms would falsely determine the reuse relationship.
Winnowing is another selection method based on k -gram [18]. Winnowing adopts another fixed size window, i.e. a winnowing window over the sequence of chunks generated by the original window, and it selects a chunk whose value is the minimum in each winnowing window. If there is more than one minimum value in the winnowing window, then the rightmost minimum value in the window is selected.
Schleimer et al. [18] showed that winnowing performs better than 0modp in practice. Further, they showed that the expected number of fingerprints has a lower bound as follows: where w is the size of winnowing window.
A main idea of non-overlap methods is splitting text into a few meaningful text segments such as phrases or sentences instead of generating many subsequences of words. Thus, sliding windows are not used, and accordingly, there is no overlap between chunks. We refer to a process of splitting text segments as breaking . A word position where a break occurs is referred to as a breakpoint .

A chunk value of non-overlap method is computed as fol-lows: where b ( i ) is a breakpoint for the i th text segment.
A simple breaking method is using punctuation characters like  X .,!? X . In case of well form atted text, punctuation char-acters play an significant role in forming sentence bound-aries. However, in general Web text including blogs, there is substantial noise and text segments are often defined by HTML tags rather than by punctuation. In such environ-ments, where text reuse detection algorithms are often used, breaking by punctuation does not work well. Therefore, the non-overlap methods to be introduced here are based on mathematical properties of text instead of punctuation.
Hash-breaking [2] is a non-overlap version of 0modp .A hash value h ( w )foreachword w is computed, and hash values such that h ( w ) mod p  X  0 are selected as breakpoints for text segments. That is, a sequence of words from the next word of the previous breakpoint to the current breakpoint is considered as a meaningful text segment. We can get a chunk by applying Equation (6) to the text segment. Since the number of chunks generated in a document is relatively small, any selection method does not need to be used and all the chunks are used as fingerprints of the document. The expected number of fingerprints is given by L ( D ) /p . That is, the average length of text segments is p .Inbad cases, the length of text segments might be much shorter than we expected. The worst case is that the text segment includes only a very common word such as  X  X  X  or  X  X he X . Then, the fingerprint is noise and hurts the text reuse performance. To address this problem, we suggest a simple revision of the original hash-breaking algorithm. When we set a p value, we expect the length of text segment to be p .Thus,wecan ignore text segments whose lengths are shorter than p .We can reduce noisy fingerprints through this approach.
A weak point that still remains is that hash-breaking is too sensitive to small modifications of text segments. In case of overlap methods, even if there is a small change in a sen-tence, then adjacent chunks without the change as well as chunks with the change are generated by window overlap-ping. Accordingly, the effect of the change can be minimized. On the other hand, since there is no overlap between chunks split by hash-breaking and the number of chunks is small, the change might be overestimated.
We propose a robust method called DCT fingerprinting to address the sensitivity problem of hash-breaking. The Discrete Cosine Transform (DCT) is a real valued version of Fast Fourier Transform (FFT) and transforms time domain signals into coefficients of frequency component. By exploit-ing a characteristic that high frequency components are gen-erally less important than low frequency components, DCT is widely used for data compression like JPEG or MPEG. DCT is formulated as follows: where x n and X k are the n th value in the time domain signal sequence and a coefficient of the k th frequency component, respectively. Note that the length of the time domain sig-nal sequence N isthesameasthenumberofthefrequency domain components.

A main idea of DCT fingerprinting is that a sequence of hash values of words can be considered as a discrete time domain signal sequence. That is, we can transform the hash value sequence into the coefficients of frequency components by using DCT.

The process of DCT fingerprinting is composed of seven steps. ii. Compute hash values for words in the text segment, iii. Perform a vertical translation of the hash values so iv. Normalize the hash values by the maximum value. v. Perform DCT with the normalized hash values. vi. Quantize each coefficient to be fitted in a small number vii. Form a fingerprint with the quantized coefficients Q
DCT fingerprinting is expected to be more robust against small changes than hash-breaking. As you see in Equation (7), when there is a small change of an input value, i.e. a hash value of a word, the change is propagated over all coeffi-cients by a reduced effect. Since we quantize the coefficients, the final fingerprint value can be kept unchanged. That is, this robustness can be interpreted as an advantage of data reduction. The following examples show the robustness of DCT fingerprinting. The numbers in [] are the fingerprints for the right string sequences. [0x295D0A52] one woman comedy by person Willy [0x295D0A52] one woman show by person Willy [0xF1315F87] company scheduled another money [0xF1315F87] company slated another money
It is difficult to show theoretically how many changes DCT fingerprinting can be tolerant of because input signal values are almost randomly mapped to by hashing. That is, while a minor change, e.g., a change from  X  X roduct X  to  X  X roducts X  might cause a big change of the hash value of the word, a word replacement might be coincidentally mapped to the same value. Nevertheless, a single word change tends to change a few high frequency components, and we can ignore the high frequency components by the formatting scheme. Thus, we can expect that DCT fingerprinting sometimes handles a single word change. When more than one word is changed, the input signal shape is likely to be distorted and the DCT coefficients are changed. Moreover, if words are added to or removed from the text segment, then even the number of the coefficients is changed. Therefore, we con-clude that DCT fingerprinting can be tolerant of at most a single word replacement.

Note that DCT fingerprinting and hash-breaking generate the same number of fingerprints. Although computation of DCT fingerprinting might seem somewhat expensive, it re-quires only p 2 more multiplications for each fingerprint com-pared to hash-breaking. In practical, since running time of text reuse detection mostly depends on the amount of I/O access, computation complexity of DCT fingerprinting has little impact on the time.

We should mention that there have been some efforts to use DCT as a robust hash function for images [12] or videos [7]. However, they have totally different schemes and con-texts from ours and there is no similarity except for using the DCT approach.
To compare fingerprinting techniques, we designed exper-iments on a test dataset. To build the dataset, we ran each technique on TREC newswire collection and collected de-tected document pairs. Then, by manually judging them, we obtained 100 labeled document pairs for each text reuse type defined in Section 3.3.

We first optimized parameters for each fingerprinting tech-nique with 50 document pairs of the dataset. Then, with the remaining 50 document pairs, we ran our text use detec-tion engine using each technique with the tuned parameters to evaluate the performance. Small values were chosen for the parameters, e.g., k =3for k -gram, p =6for 0modp , w = 10 for winnowing and p = 3 for hash-breaking and DCT Figure 3: Overall performance of fingerprinting techniques. F 1 and m represent the average of F 1 of the six categories and the average number of finger-prints of a document, respectively. fingerprinting. For near-duplicate detection, big values for the parameters are generally used to avoid false detections and to reduce the number of fingerprints. However, in order to catch local text reuse, smal l values are more desirable.
For our experiments, we used 32 bit integers for finger-prints. Since we used MD5 [17], i.e. a 128 bit algorithm for hashing, we had to choose the upper 32 bits of the MD5 value. We used the harmonic mean of recall and precision, F 1 as an evaluation metric for accuracy in that this task could be considered as a classification problem with six cat-egories. The number of fingerprints was used as another metric to evaluate efficiency.

Table 3 and Figure 3, 4 and 5 show the experimental re-sults. Overall, in accuracy, k -gram outperformed the others as shown in Figure 3. However, it generated too many fin-gerprints as we predicted. DCT fingerprinting showed the second best accuracy with a small number of fingerprints. For near-duplicate cases ( C 1, C 2and C 4), both k -gram and winnowing showed good performance as shown in Figure 4. For the local text reuse cases ( C 3, C 5and C 6), DCT finger-printing worked as well as k -gram as shown in Figure 5. It is also noticeable that DCT fingerprinting generated many fewer fingerprints than k -gram while they showed similar performance.

Based on the results, if there is enough resources and the target collection is small, then k -gram is the best. Other-wise, DCT fingerprinting would be the most practical choice. When the main purpose is near-duplicate detection, win-nowing might be a good choice. For local text reuse, DCT fingerprinting is most desirable considering both accuracy and efficiency.
DCT fingerprinting needs to be validated in practice in that its robustness might be misunderstood as a result of hash collisions and false detection. The best way to validate would be to directly examine all detected pairs, but that is impossible. Thus, we chose an indirect method: com-parison with k -gram. We know that k -gram is the most accurate method. Moreover, it has the lowest probability of false detection because it uses all chunks. Although testing on a larger collection where the false detection probabil-ity tends to be higher is preferable, running k-gram on big collections is too expensive. Therefore, we used a TREC m represents the average number of fingerprints. Figure 4: Near-duplicate detection performance of fingerprinting techniques. F 1 and m represent the average of F 1 of C 1 , C 2 and C 4 and the average num-ber of fingerprints of a document, respectively. Figure 5: Local text reuse detection performance of fingerprinting techniques. F 1 and m represents the average of F 1 of C 3 , C 5 and C 6 and the average number of fingerprints of a document, respectively. newswire collection containing about 758,224 documents as the test collection. The collection is comprised of Associated Press (1988-1990), the Wall Street Journal (1987-1992), the Financial Times (1991-1994) and the Los Angeles Times (1989-1990).

Table 4 presents the results of the two techniques. They show almost same results for each type. The fact that DCT fingerprinting detected slightly more documents means that there might be false detections in some degree, but that would not be significant. Further, while the size of the index for k -gram was 1.4 gigabytes, that for DCT fingerprinting was only 239 megabytes. The running time for DCT fin-gerprinting (1 hour) was also shorter than that of k-gram (3 hours). Thus, we conclude that DCT fingerprinting is comparable in effectiveness and more efficient than k-gram. Table 4: Text reuse detection results of k -gram and DCT fingerprinting in TREC newswire collection.  X #Sibling X  represents the average number of docu-ments which are related to the detected document through a category Type #Doc %#Sibling #Doc %#Sibling C 1 21087 2.78% 1.68 20173 2.66% 1.48 C 2 21579 2.85% 1.93 21080 2.78% 1.60 C 3 9338 1.23% 1.59 8533 1.13% 1.33 C 4 33448 4.41% 21.96 31477 4.15% 20.67 C 5 41693 5.50% 4.90 41406 5.46% 3.74 C 6 99219 13.09% 59.72 101934 13.44% 62.38
Total 171320 22.59% 40.60 170874 22.54% 42.36
In this section, we analyze the amount and type of text reuse in two collections using DCT fingerprinting. We use two metrics to analyze the collections. One metric, the num-ber of documents in each text reuse type, shows how many documents involve text reuse. Another metric is the aver-age number of siblings . The siblings of a document represent documents which have text reuse relationships with the doc-ument.
News articles are public and official documents written by professional reporters or editors. Text reuse happens most frequently when stories are revised or when journalists  X  X or-row X  from earlier stories. The results in Section 5.2 and in Table 4 were based on news collections.

We sampled 50 document pairs for each type from the detection results and manually classified the text reuse into three more classes based on the style of text reuse (rather than the amount of text). These classes are  X  X ext reuse X  ,  X  X ommon Phrase X  and  X  X emplate X  . The results are shown in Table 5.  X  X ext reuse X  patterns correspond to actual text reuse cases. That is, a document pair with these patterns is derived from the same source or has a direct relation. For example, while an article A written on April 4, 1988 describes an event with four sentences, another article B written on April 5, 1988 has six sentences in addition to the four sentences. We can infer an information flow from article A to article B.  X  X ommon phrase X  patterns are caused by common phrases. Thus, we might not infer any actual relation. For example,  X  X ow Jones average of 30 industrials X  and  X  X he New York Stock Exchange X  are commonly used in many articles about the stock market. If two documents share these phrases, we Table 5: Text reuse in the TREC newswire collec-tion.
 Pattern C 1 C 2 C 3 C 4 C 5 C 6 Total Text Reuse 64% 68% 100% 0% 48% 6% 48% Common Phrase 0% 0% 0% 0% 12% 84% 16%
Template 36% 32% 0% 100% 40% 10% 36% cannot say that they have a text reuse relationship.
The most interesting patterns are  X  X emplate X  patterns, which make up one third of the total detected relationships. In these patterns, there are reusable templates where only a few words are replaced. The following is an example where only highlighted numbers are changed with the remaining words unchanged.

U.S. automakers were scheduled to make 167,791 cars this week, compared with 170,279 a week ago and 152,751 in the same week in 1987 . [AP880616-0266] Such template patterns are easily found in the economic news about stock market indexes, foreign exchange rates, or bank interest rates. These patterns are observed in most text reuse types and the class C4 is particularly dominated by these patterns.

Consequently, the news collection has a small amount of text reuse if we exclude C 4and C 6 which are dominated by noisy patterns like  X  X ommon Phrase X  and  X  X emplate X  .That is, to build an accurate text reuse detection system, local text reuse detection which can identify this type of noise is necessary. Further, each document typically has few sib-lings. That is, the reused text is not likely to move through many documents and the propagation paths are limited.
Blogs are generally operated by individuals, in contrast to news organizations. We use the TREC Blogs06 collection [13], which contains about 3 million postings. Since the collection size is about 100 gigabytes, it is difficult to process using the k -gram technique.

When we find text reuse in blog collections, there is a problem to be considered. In most blogs, navigation bars are located on the top or the side of each page and advertisement links like Google AdSense 2 or links to the previous postings occupy the corners of each page. Text in such frames is repeated in most of the postings of a blog. As a result, blog postings could be falsely detected as text reuse relationships even though their actual contents are not related to each otheratall. Werefertothisas frame noise .Toremove such noise, we employed a Document Slope Curve (DSC) content selection algorithm [8]. The algorithm plots a curve as follows:
DSC [ k ]= where T [ k ]isthe k th tokeninanHTMLpage. Byexploiting the observation that there are fewer HTML tags in content bodies than in the other areas, we regard the lowest slope area of the curve as the content body.
 We ran our detection engine using DCT fingerprinting. Despite the collection size, the index size was only 1.3 giga-bytes, which can be easily handled in main memory. The http://www.google.com/adsense Table 6: Text reuse detection result in TREC Blogs06 collection.  X #Sibling X  represents the aver-age number of documents which are related to the detected document through a category.
 run took 10 hours with eight 3.2 MHz CPUs. We also tried to use k -gram for time comparison, for which the index size was 10 gigabytes. In addition, we employed a speed-up algo-rithm for k -gram,  X  X PEX X  [1], which filters out unnecessary unique fingerprints while building the index. Nevertheless, processing had not finished after several days of elapsed time because of heavy I/O load.

Table 6 shows the text reuse detection result. Many more documents are involved in text reuse relationships compared to the newswire collection. In fact, the numbers were over-estimated as we see later.

We sampled 50 document pairs for each type from the results and identified five styles of reuse, i.e.,  X  X ext Reuse X  ,  X  X ommon Phrase X  ,  X  X pam X  ,  X  X rame X  and  X  X RL Aliasing X  .The result is shown in Table 7.

In  X  X ext Reuse X  patterns, text reuse o riginated from au-thoritative sources such as news articles or academic papers. This appears more frequently than text reuse based on other blog postings. That is, many bloggers seem to still trust au-thoritative sources more than blog documents.

Most  X  X ommon phrase X  patterns are composed of boiler-plate text, in contrast to the newswire collection. For ex-ample, the following paragraph is a representative example of boilerplate text which is located below content text with the highlighted date changed.

This entry was posted on Friday, January 13th, 2006 at 12:00 pm and is filed under XXX. You can follow any responses to this entry through the RSS 2.0 feed. The boiler plate text is different from  X  X emplate X  patterns in that the text forms a small part of the document, e.g., a header or footer rather than the content of the document and is observed in most postings.  X  X rame X  patterns correspond to f rame noise. Although we preprocessed the collection by using the DSC content selec-tion algorithm, a considerable amount of frame noise still remains. Since this noise is almost evenly distributed over all types, we cannot distinguish it easily by classification.
Another new pattern is  X  X pam X  . Spam phrases such as  X  X ree gift X  and  X  X oker casino X  tend to be repeated in or between spam postings, and accordingly, they could be detected as text reuse.

Another special pattern is  X  X RL Aliasing X  which has been reported in near-duplicate studies on Web [20]. While two postings have different URLs, they correspond to the same document. Since their contents are identical, these patterns are observed in only the C1 type.

As you see in Table 7, noisy patterns like  X  X rame X  and  X  X pam X  account for 50  X  70% of each class, which causes most Table 7: Text reuse in the TREC Blogs06 collection. Pattern C 1 C 2 C 3 C 4 C 5 C 6 Total Text Reuse 16% 20% 20% 6% 12% 18% 15% Common Phrase 2% 12% 12% 24% 28% 28% 18% Spam 30% 22% 20% 8% 12% 20% 19% Frame 36% 46% 48% 62% 48% 34% 46%
URL Aliasing 16% 0% 0% 0% 0% 0% 3% of the overestimation of text r euse. Therefore, to more ac-curately investigate text reuse in blog or Web collections, better content selection techniques and spam filtering algo-rithms are required.

Taking the overestimation into account, the blog collec-tion does not contain more text reuse than the newswire collection. However, the number of the siblings in the blog collection is still greater than that in the newswire collec-tion. This shows that the reused text is easily spread over the blogs.

In addition,  X  X ext Reuse X  patterns are almost equally dis-tributed over all text reuse types. That is, we need to con-sider all text reuse types in order to accurately infer rela-tionships between documents. Therefore, for text reuse de-tection applications like information flow tracking, local text reuse detection is likely to more effective than near-duplicate detection which can detect only a few text reuse types. We defined a general framework for text reuse detection. The six categories for text reuse detection can be flexibly applied to various tasks including near-duplicate detection and local text reuse detection.

We reviewed several fingerprinting techniques for the frame-work and introduced a robust technique, DCT fingerprint-ing. Through performance comparison and empirical valida-tion, we showed that DCT fingerprinting is one of the best candidates for general or local text reuse detection with high accuracy and efficiency.

Finally, using this algorithm, we investigated the text reuse aspects of a newswire collection and a blog collection. Through the analysis, we showed that the text reuse pat-terns of the two collections are different from each other and local text reuse detection will be more effective than near-duplicate detection for applications like information flow tracking on such collections.
This work was supported in part by the Center for Intel-ligent Information Retrieval, in part by NHN Corp. and in part by NSF grant #IIS-0534383. Any opini ons, findings and conclusions or recommendations expressed in this ma-terial are the authors X  and do not necessarily reflect those of the sponsor. [1] Y. Bernstein and J. Zobel. Accurate discovery of [2] S.Brin,J.Davis,andH.Garc  X   X a-Molina. Copy [3] A. Z. Broder. On the resemblance and containment of [4] A.Z.Broder,S.C.Glassman,M.S.Manasse,and [5] M. S. Charikar. Similarity estimation techniques from [6] A.Chowdhury,O.Frieder,D.Grossman,andM.C.
 [7] B. Coskun, B. Sankur, and N. Memon.
 [8] A. Finn, N. Kushmerick, and B. Smyth. Fact or [9] N. Heintze. Scalable document fingerprinting. In 1996 [10] M. Henzinger. Finding near-duplicate web pages: a [11] T. C. Hoad and J. Zobel. Methods for identifying [12] C.-Y. Lin and S.-F. Chang. A robust image [13] C. Maconald and I. Ounis. The TREC blogs06 [14] U. Manber. Finding similar files in a large file system. [15] D. Metzler, Y. Bernstein, W. B. Croft, A. Moffat, and [16] M. O. Rabin. Fingerprinting by random polynomials. [17] R. Rivest. The MD5 Message-Digest Algorithm, RFC [18] S. Schleimer, D. S. Wilkerson, and A. Aiken. [19] N. Shivakumar and H. Garc  X   X a-Molina. SCAM: A copy [20] N. Shivakumar and H. Garc  X   X a-Molina. Finding
