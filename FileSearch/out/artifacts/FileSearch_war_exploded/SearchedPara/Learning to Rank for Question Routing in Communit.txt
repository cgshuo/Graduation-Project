 This paper focus es on the problem of Question Routing (QR) in Community Question Answering (CQA), which aims to route newly posted questions to the potential answerers who are most likely to answer them. Traditional methods to solve this problem only consider the text similarity features between the newly pos t-ed question and the user profile, while ignoring the important statistical features , including the question-specific statistical fea-ture and the user-specific statistical features . Moreover, traditio n-al methods are based on unsupervised learning, which is not easy to introduce the rich features into them. This paper proposes a general framework based on the learning to rank concepts for QR. lected . Then, by introducing the intrinsic relationships between the asker and the answerers in each CQA session to capture the intrinsic labels/orders of the users about their expertise degree of the question q , two different methods, including the SVM-based and RankingSVM-based methods, are presented to learn the mo d-els with different example creation processes from the training set. Finally, the potential answerers are ranked using the trained mo d-els. Extensive experiments conducted on a real world CQA d a-taset from Stack Overflow show that our proposed two methods can both outperform the traditional query likelihood language model (QLLM) as well as the state-of -the-art Latent Dirichlet Allocation based model (LDA). Specifically, the RankingSVM-based method achieves statistical significant improvements over the SVM-based method and has gained the best performance. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Community Question Answering; Question Routing; Learning to Rank Community Question Answering (CQA) has become a popular type of web services where users can ask and answer questions. Examples of such CQA services include the general sites like Yahoo! Answers 1 , Baidu Zhidao 2 and the focused sites like Stack Overflow 3 and TurboTax 4 , etc. 
As time passed , CQA services not only have accumulated large archives of questions and their answers, but also have attracted in-creasing number of users for information seeking and knowledge sharing. However, in recent years, the efficiency of CQA services is greatly challenged by the serious gap between the posted que s-tions and the potential answerers due to the fast increase of posted questions, and the lack of an effective way to find interesting questions for the potential answerers. This will lead to the follo w-ing three problems. (1) From the asker  X  s perspective, the askers have to passively wait for answerers to answer their questions, which may take hours or days to get the ir questions resolved. Previous study [ 5] has shown that more than 80% of the new questions cannot be resolved efficiently within 48 hours. Th ere-fore, it will make the askers be not willing to post new questions into the CQA services, but to try other means to seek their info r-mation. (2) From the answerer  X  s perspective, the potential a n-swerers, who know well the answers of questions in a particular domain and are willing to contribute their knowledge to the com-munity, are not easy to find their interested questions because they are easily overwhelmed by the large number of open questions. Therefore, it will reduce the answerers  X  enthusia sm in providing answers to share their knowledge. (3) From the CQA service  X  s perspective, this will degrade the services  X  performance as well as reduce the user s X  loyalty to the system, and will undoubtedly lose lots of users, who are the bases of the web services. 
To bridge the gap between the newly posted question and the potential answerers, it is essential to automatically route the newly posted questions to the potential answerers, who may answer the questions. This linking of the newly posted questions with the potential answerers will improve the user s X  satisfaction as well as the performance of the CQA services, thus will make the askers be more willing to contribute knowledge to the CQA services and the answerers be more enthusiastic in providing answers. 
Question Routing (QR) in CQA is the task of routing newly po sted questions to the potential answerers, who are interest ed in the questions and are most likely to answer them [ 5, 6, 11-14], to make the questions get answered as soon as possible. 
Traditional methods to solve this problem usually first build a profile for each potential answerer based on the archives of her previously answered questions, and then use the information r e-trieval based methods to compute the text similarity between the http://answers.yahoo.com/ http://zhidao.baidu.com/ http://stackoverflow.com/ https://ttlc.intuit.com/ user profile and the newly posted question. This process is also called expertise estimation [ 5]. Finally, all the candidate a n-swerers are ranked based on their expertise score on the newly posted question. These methods include the traditional query lik e-lihood language model (QLLM) [5 ], the category-sensitive lan-guage model [ 6] and the Latent Dirichlet Allocation [2 ] based model (LDA) [ 8], which has gained the state-of -the-art perfo r-mance. 
However, all of these approaches only consider the text sim i-larity features between the newly posted question and the user profile, while ignoring the important statistical features , including the question-specific statistical feature and the user-specific sta-tistical features . 
What is more, traditional approaches are based on unsupe r-vised learning to estimate the potential answerers  X  expertise given a newly posted question, thus are not easy to introduce the rich features into them. 
In this paper, we propose a general framework based on the learning to rank concepts for QR . To the best of our knowledge, it rank concepts for solving this problem. swerers ). Each triple is constructed from a CQA session, where q is the question asked by the asker and answered by the answerers . Secondly, we identify eight typical features, including the que s-tion-specific statistical feature , the user-specific statistical fea-of questions, users, and their relationships. Thirdly , we present two different learning methods, including the SVM-based and RankingSVM-based methods, to learn the ranking models of the candidate answerers for a given newly posted question. Then, we introduce the intrinsic relationships between the asker and the answerers in each CQA session to capture the intrinsic l a-bels/orders of the users about their expertise degree of the que s-answerers in one CQA session should have more expertise degree of the question than that of the asker. By capturing the intrinsic labels/orders of the users, it will be easy to construct the training examples to be used in supervised learning, and of course be available to introduce the rich features into the learning process. Finally, we rank the potential answer ers using the trained ranking models for the newly posted question. 
We conduct extensive experiments on a real world CQA d a-taset from Stack Overflow website for QR . The results show that our proposed two methods can both outperform the traditional QLLM as well as the state-of -the-art LDA . In addition, the Ran k-ingSVM-based method achieves statistical significant improv e-ments over the SVM-based method and has gained the best pe r-formance. 
The rest of the paper is organized as follows. Section 2 details our proposed learning to rank framework for QR . Section 3 d e-scribes the experimental study. Finally, we conclude and offer the future work in section 4. Figure 1 shows the overall structure of the learning to rank framework for question routing. answerers ). Each triple is constructed from a CQA session, where q is the question asked by the asker and answered by the a n-swerers . (2) Secondly, we identify features to capture the different a s-pects of the question ( q ), the users ( asker and answerers ), and the relationships between them. (3) Thirdly, we will choose two different learning algorithms to learning the ranking model. (4) Then, according to the different learning algorithms, we will create different training examples, which is crucial for the supervised learning. Specifically, we introduce the intrinsic rel a-tionships between the asker and the answerers in each CQA se s-expertise degree of the question q . The intrinsic labels/orders of the users mean that all the answerers in one CQA session should have more expertise degree of the question than that of the asker. By capturing the intrinsic labels/orders of the users, it will be easy to construct the training examples to be used in supervised lear n-ing, and of course be available to introduce the rich features into the learning process. (5) Finally, we will use the learned ranking model to rank the candidate answerers according to their expertise degree to the newly posted question. 
In the following subsections, we will describe the feature s e-lection, the ranking algorithms with their example creation pr o-cesses and the candidate answerers ranking in more detail. Given a question and a user, three types of features listed in Table 1 are used in our work. They are the question-specific statistical feature (feature 1), the user-specific statistical features (feature 2~5 ) and the text similarity features (feature 6~8). Feature 1~5 are the statistical features , which have not yet been investigated to be used in the ranking model for question routing. Feature 6~8 are the text similarity features which are the only features QLLM and LDA have considered before. # Feature Description 1 Q: Title Length Length of the question title. 3 U : # of best answers # of best answers the user has provided. 4 U : # of answers # of answers the user has provided. 5 U : # of asked questions # of questions asked by the user. 7 QU: P (question, LM ( answered and asked questions )) For question-specific statistical feature , we consider the question could be considered as an important feature to measure the quality of the question. In other words, a question of good quality is eas i-er to get answered. For user-specific statistical features , we consider the number of answers and best answers the user has provided and the number of questions the user has asked (feature 2~5). If a user has provided a lot of answers with large amount of them as best answers (of course, with higher percentage of best answers), then we think that the user has more authority. And more authoritative users will probably provide more authoritative answers. In addition, if a user has also asked a lot of questions, it indicates that the user is very active. Active users will probably learn a lot from others and will probably provide answers to others if she will in the future. Feature 6~8 are the text similarity features which are the only features that QLLM and LDA have considered before. Feature 6 is the probability of generating the question from the user X  X  a n-swered questions with language model. Feature 7 is the probabi l-ity of generating the question from the user X  X  a nswered and asked questions with language model. Feature 8 is the probability of generating the question from the user X  X  answered questions with LDA model. There are three major approaches to learning to rank, i.e., the pointwise, pairwise, and listwise approaches [7 , 9 ]. In our work, we apply the pointwise and pairwise approaches, as it is difficult to obtain the complete order of users from the existing dataset, but between two users. Specifically, in the pointwise way, we care about the label for each user to denote whether the user has enough expertise to answer the question and reduce the ranking of users as a classification problem; and in the pairwise way, we care concept of ranking than the pointwise way. The label of each user means whether the user has enough expe r-tise to answer the newly posted question. Clearly, the asker does not have, the label is -1; but the answerer does, the label is 1. We first reduce the ranking of candidate answerers as a classific a-tion problem, which is a pointwise approach to learning to rank. 
Su pport Vector Machine (SVM) is a widely used approach to build a classifier based on a set of labeled objects . Given a set of labeled objects, some belong to a positive class and the others belong to a negative class, SVM tries to build a hyper-plane that separates objects belonging to the positive class from those of the negative class with the largest margin. The resultant model could then be used to classify other unknown data points in vector re p-resentation and label them as either positive or negative. 
Creating Examples : To directly use SVM to train the model, the key step is to create the positive and negative examples . As we have collected a training set consists of triples ( q , asker , an-( q , asker ) as a negative example, and if a user is the answerer of q , we consider ( q , answerer ) as a positive example. For clarity, we list the definition of negative and positive examples in Table 2. Table 2 : Negative and positive examples for SVM-based method 
With the definition of positive and negative examples and the training set consists of triples ( q , asker , answerers ), we create two sets of examples to be used for model training . 
Training Models : Before training, an extra action is taken to normalize the values of all features in the training set to within the bigger range dominate those in the smaller range [ 3]. The same normalization will also be applied when the trained model is used to classifying unknown pairs (question, user). Then, we use LIBSVM [ 3], which is a popular implementation of SVM, with linear kernel to train the model. Next, we introduce a pairwise approach to learning to rank for question routing. 
RankingSVM [ 4], one of the pairwise ranking methods, is a classical algorithm for ranking, which transforms ranking into pairwise classification and employ the SVM technology to pe r-form the learning task. 
Creating Examples : To directly use RankingSVM to train the model, the key step is to create the partial order between two users . We also use the collected training set consists of triples ( q , the expertise degree of each answerer to answer the question q should be ranked higher than that of the asker . Thus the following partial order should hold: 
With the definition of the partial order between two users and the training set consists of triples ( q , asker , answerers ), we create a partial order set to be used for model training. 
Training Models : Before training, we also perform the no r-malization of all the features in the training set to within the range [-1, 1]. The same normalization will also be applied in the test set. Then, we use RankingSVM [ 4] with linear kernel to train the model. Having the trained models at hand, including the SVM-based and RankingSVM-based methods, now we should perform candidate answerer ranking for question routing, and then route the newly posted question to the candidate answerers that are ranked higher. (1) A typical SVM classifier would only give binary results. However, we are not interested in the binary results. We are inte r-ested in knowing the expertise degree of the users for ranking for the newly posted question. Thus we enable the probability estim a-tion functionality of LIBSVM to train the model which is able to produce a probability of the user having enough expertise to a n-swer the questio n. Then the final predicted probabilities can be used directly for ranking. (2) When using RankingSVM to perform the pairwise a p-proach to learning to rank, the final predicted ranking score can be used directly for ranking. Dataset : The experimental dataset is based on a snapshot of the focused CQA service Stack Overflow, which focuses on the que s-tions and answers on a wide range of topics in computer pr o-gramming. An archive of the whole content of this website is released every three months. For our experiments, we use the January 2011 data dump (since its launch on July 31, 2008 to December 31, 2010). First, we select a representative subset of the whole dataset. Table 3 shows the detail. It is from January 1, 2009 to January 31, 2010, exactly 13 months, which is the time span used in [ 10 ]. Tags are the only elements that categorize different topics in Stack Overflow. However, they belong to a very diverse topic set. Therefore, we need to create a subset of the dataset that exhibits the same properties as in the original one. Following [ 10], we use bution is maintained. The representative subset is selected as fo l-lows: resolved questions with at least 2 answers and tagged with at least 1 of the 21 selected tags are picked. All the questions are lowercased and stopwords are removed using a standard list of 418 words. Then only the questions with at least 2 words are left. Finally, we get 92 ,411 CQA sessions for the subset. 
Then, we split the subset into a training set and a test set a c-cording to the timestamp when the question is posted. The trai n-ing set is from January 1, 2009 to December 31, 2009, exactly 12 months with 81 ,295 sessions and the test set is from January 1, 2010 to January 31, 2010, exactly 1 month with 11,116 sessions. Table 3 : The subset used in our experiments. "Start" and "End" mean the start and end dates we split the dataset in the January 2011 data dump from Stack Overflow. Table 4: The statistics of the user set, training question set and the test question set ( ) for the subset. X means to choose the users who have provided at least X answers . 
Finally, we construct the user set, the training question set and the test question set ( ) for question routing as follows: (1) We first select the users who have answered at least (we choose in our work) questions in the training set as the user set . (2) Then we collect questions that each question has the asker, the best answerer and at least one other answerer in from the training set and test set as the training question set and test question set , respectively. The statistics of the sets ( ) are shown in Table 4. 
Take for example, there are 5,761 users who have a n-swered at least 10 questions in the training set. The asker, the best answerer and at least one other answerer for each question in the 16 ,033 training questions are within the 5,761 users. For each of the 1,150 test questions, they are routed to the 5,761 users. Ground Truth : Following [5 , 6, 12], the actual answerers for each test question are viewed as the ground truth. Baselines : To evaluate the performance of our proposed learning to rank framework for question routing, we use the following two baselines: (1) QLLM : the traditional query likelihood language model, which is just the linear combination of feature 6 and 7 in Table 1. (2) LDA : the state-of -the-art LDA-based model, which only considers feature 8 in Table 1. 
Note that, we also combine QLLM and LDA (LDALM, which can be seen as the linear combination of feature 6, 7 and 8) to see the interpolation performance. However, using the LDA alone achieves the best performance among QLLM, LDA and LDALM in our experiments. Thus, we only report LDA as the state-of-the-art method in our work. 
Metrics : We evaluate the performance of all the ranking met h-ods using the following three metrics. Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision@n (P@n). We also perform a significance test using a paired t -test at the 0.05 significance level. There are several parameters to be set in our experiments. (1) We use GibbsLDA++ 6 to conduct LDA training and infe r-ence. The parameters for LDA we used are as follows: the hyper-parameters , and the topic size . http://gibbslda.sourceforge.net/ X QLLM (baseline1) LDA (baseline2) SVM (f7) %chg (  X  LDA ) %chg (  X  LDA) 10 15 20 (2) We use LIBSVM 7 as our SVM implementation with linear kernel, and we use 5-fold cross-validation and employ a grid search on SVM parameter over the following values to find the best parameter. 
Finally is found to be optimal in all our exper i-ments, and of course we use this value to report all the results. (3) We use SVM Rank 8 as our RankingSVM implementation with linear kernel, and we also use 5-fold cross-validation and employ a grid search on RankingSVM parameter over the same values as LIBSVM to find the best parameter. Finally, is found to be optimal in all our experiments, and of course we use this value to report all the results. While desi gning and conducting the experiments, we have the following two questions in mind: (1) How effective are our proposed learning to ranking based methods, including SVM-based and RankingSVM-based methods? (2) Which method is better, the SVM-based method or the RankingSVM-based method? 
Specifically, to answer the two questions, we differentiate each of our two methods with 7 features that excludes the LDA feature and 8 features that includes the LDA feature. Table 5 presents the comparisons of different methods for que s-tion routing . There are some clear trends in the results: (1) LDA significantly outperforms QL LM . This indicates that using the latent semantic topics from LDA to represent the user profile is much better than that of QLLM, and thus the state-of -the-art LDA achieves better performance than QLLM. The results are consistent with the former work [ 6, 12]. (2) The SVM-based method with 7 features significantly ou t-performs QLLM. When compared with LDA, the method is a litter better in all the metrics except that in MAP when . In addition, the method achieves statistical improvements over LDA in the metric of P@10. The result indicates that incorporating the http://www.csie.ntu.edu.tw/~cjlin/libsvm/ http://www.cs.cornell.edu/People/tj/svm_light/svm_rank.html question-specific statistical feature and the user-specific statistical features into the model can improve the question routing perfo r-mance effectively. (3) The SVM-based method with 8 features does not improve much than that of the SVM-based method with 7 features. More o-ver, it degrades the performance in some metrics. Even so, the performance of this method is a little better than LDA, and achieves statistical improvements in the metric of P@10. (4) The RankingSVM-based method with 7 features statistica l-ly outperforms LDA in almost all of the metrics. This demo n-strates that the RankingSVM-based method can significantly ou t-perform LDA with only the text similarity features and our intr o-duced statistical features without LDA-based semantics. This also indicates that the RankingSVM-based learning method can learns the ranking model of the candidate answerers much better. This is the superiority of pairwise-based learning to rank method over the pointwise-based learning to rank method. (5) The RankingSVM-based method with 8 features can fu r-ther improve the performance when includes the LDA-based s e-mantic feature, and achieves the best performance in all the me t-rics. However, this effectiveness of incorporating the LDA-based semantic feature is not the case when use the SVM-based method. (6) Note that, when , the best value of MRR in our e x-periments is 0.1299, which means that on average each test que s-tion will get answered if we route it to the top 8 users. However, the traditional QLLM should route it to the top 17 users (MRR=0.0618), the state-of-the-art LDA should route it to the top 11 users (MRR=0.0967) and the SVM-based method should route it to the top 9 users (MRR=0.1149 or 0.1152). From the above results, we have already noticed that our proposed SVM-based and RankingSVM-based methods can significantly improve the question routing performance. These methods mainly benefit from our introduced statistical features. In addition, the RankingSVM-based method is much better than the SVM-based method and achieves the best performance when using 8 features. This mainly benefits from the superiority of the pairwise-based learning to rank method over the pointwise-based learning to rank improvements ( using a paired t -test) over SVM. f7 12.6 * 10.1 * -0.8 11.4 * 9.1 7.2 0.0 f8 16.5 * 22.7 * 7.5 16.4 * 12.8 * 21.1 * 7.6 method. Therefore, it is necessary to do a further analysis and comparison of performance of the two methods. 
Table 6 presents the comparison of SVM-based and Ran k-ingSVM-based methods in two aspects: (1 ) Different number of features, 7 or 8; (2 ) Different size of the chosen candidate a n-swerers, which is decided by the value of X , the smaller the value of X , the more candidate answerers are chosen. From the table, we can see that the RankingSVM-based method is much better than the SVM-based method and achieves the statistically significant improvements in most of the comparisons. The improvements can be seen from the two aspects. (1 ) The more features (from 7 to 8), the more improvements of RankingSVM-based method over SVM-based method. (2 ) The more the candidate answerers to be routed (decrease X from 20 to 10), the more improvements of RankingSVM-based method over SVM-based method. 
In this paper, we propose a general framework based on the learning to rank concepts for Question Routing (QR) in Comm u-nity Question Answering (CQA).We conduct experiments on a real world CQA dataset from Stack Overflow website for QR . The results show that our proposed two methods can both outperform the traditional QLLM as well as the state-of -the-art LDA . More o-ver, the RankingSVM-based method achieves statistical signif i-cant improvements over the SVM-based method and has gained the best performance. 
Some interesting future work could be continued. First, more features should be introduced into our proposed framework, such as the badges and reputation of the users in Stack Overflow. S e-cond, we will try to investigate our proposed methods for other types of CQA dataset, such as the general sites like Yahoo! A n-swe rs. This work is supported by the National Science Foundation of China under Grant No. 61070111 and the Strategic Priority R e-search Program of the Chinese Academy of Sciences under Grant No. XDA06030200. [1] Lada A. Adamic, Jun Zhang, Eytan Bakshy, and Mark S. [2] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent [3] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for [4] R. Herbrich, T. Graepel, and K. Obermayer. Large margin [5] Baichuan Li and Irwin King. Routing questions to [6] Baichuan Li, Irwin King, and Michael R. Lyu. Question [7] Hang Li. Learning to rank for information retrieval and [8] Mingrong Liu, Yicen Liu, and Qing Yang. Predicting best [9] Tie-Yan Liu. Learning to rank for information retrieval. [10] Fatemeh Riahi, Zainab Zolaktaf, Mahdi Shafiei, and [11] Fei Xu, Zongcheng Ji, and Bin Wang. Dual role model for [12] Guangyou Zhou, Kang Liu, and Jun Zhao. Joint relevance [13] Tom Chao Zhou, Michael R. Lyu, and Irwin King. A [14] Y. Zhou, G. Cong, B. Cui, C.S. Jensen, and J. Yao. Routing 
