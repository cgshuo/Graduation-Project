 Amirkabir University of Technology, Tehran, Iran 1. Introduction
Reinforcement learning (RL) paradigm [28] is a popular way to address sequential decision problems with limited environmental feedback. Unlike supervised learning which uses correctly labeled data in learning, in RL, there is no such information available. The only feedback is the positive and negative rewards the agent receives from the environment regarding the actions it takes. The goal of the agent is to maximize the cumulative discounted reward, i.e. expected return, over time.

Many of the reinforcement learning algorithms are value based methods that learn optimal policies using a value function. State (or state-action) value function, which predicts the result of following a task or using sample experiences. Then the optimal policy can easily be constructed from the optimal value function [12].

Although RL has proved its performance in several domains [7,19], when it comes to the real world and more complex tasks, the speed of learning is unsatisfactory. There are four common approaches to address this issue, any of which can be either automatic or based on the domain expertise and human-provided knowledge. The first and the oldest approach is function approximation [34]. The goal is to substitute the tabular representation of the value function by a parametric function approximator. As a result, the agent can generalize its experience over the state or action space more efficiently. Most of the work in this area focuses on selecting a suitable function approximator and a way to adapt its parameters during learning [18]. The second method is state abstraction [1,17]. Instead of learning in is temporal abstraction, which is also known as hierarchical RL [29,30]. This approach suggests defining higher level, temporally abstract actions rather than learning by simple one step actions. The last and relatively new approach is transfer learning [15,31]. While all the three former approaches are centered around learning improvement in a single task, this approach exploits the experience gained from learning other tasks (source tasks) to improve learning in a different, but related task (target task). Granularity of the information being transferred from the source tasks to the target task may be at different levels. Samples, value functions, and partial policies are examples of the low-level information. On the other hand, features of value function approximators, rules and advices, and task representation are high-level knowledge inferred from low-level information.
 In some cases, it is useful to combine two or more of the above approaches to achieve a better result. To improve and speed up function approximation, Lazaric [13] transfers the features of approximators as high-level knowledge to the new tasks. Some works try to overcome the difficulties of traditional state abstraction by transferring the state abstraction to the other tasks and making the cost of discovering the abstract task worth it. While trying to make more efficient algorithms for state abstraction, Wolfe and Barto [37] consider a special multi task domain. In that domain, all tasks have completely same transition functions but the reward functions can be obtained from a fixed output function. They do domain and speeding up their learning. Their work is similar to ours since they transfer state abstraction to the new tasks. However, they assume the model of tasks in domain is somehow known a priori, so they do not need to abstract lots of tasks to extract the structure of the domain. Walsh et al. [35] try to speed up abstraction formation using the prior knowledge of several previously learned tasks of a multi-task setting. In fact, there are some works which try to use prior knowledge to form abstraction and do not waste time on finding proper abstractions. For example Seijen et al. [24] provide a bunch of representations (abstractions) for the agent and let the agent to switch between these representations during learning, so that it can have a more flexible abstraction. Walsh and colleagues assume that such a prior knowledge is not available by the designer. Instead, they assume there are some solved tasks of the domain. The models or values of these tasks are used to recognize irrelevant features for each task. The future tasks. The advantage of Walsh et al. approach is that knowledge transfer can be done automatically and it is not dependent on human interaction and expertise.

In this paper, we propose two new approaches to transfer abstraction and approximation automati-cally without needing to have expertise about the model of the MDPs or their similarities. We use the knowledge obtained from previously seen and solved tasks to extract similarities between tasks of the multi-task setting; then we leverage this knowledge for improving and speeding up abstraction or ap-proximation of the future tasks. Unlike Walsh et al. framework, we do not restrict the domain to factored MDPs and our method is capable of extracting finer similarities than just recognizing irrelevant features of the state space. Since we use optimal Q-values for extracting similarities, first, we extend the avail-able theoretical literature for optimal Q-based state abstraction and fuzzy function approximation. We prove that the error in state clustering can propagate in a natural way into the error between values of the ground states and value estimates of the abstract space (Section 3). We also prove that learning in the fuzzy approximated state space converges to the results close enough to the results of learning in the ground space (Section 4). Second, instead of transferring the exact value functions which are low-level previous tasks as higher level of information to the new tasks. Especially, we use different fuzzy t-norms to combine fuzzy clusters obtained from different tasks of the domain for speeding up the learning in the new tasks of the domain (Section 5). 2. Background
Many of the sequential tasks for which reinforcement learning algorithms are applied, can be stated as Markov Decision Processes (MDPs). An MDP is a stochastic process with Markov property i.e. the history doesn X  X  provide more information than the current decision and state to predict next situation. An MDP is a 5-tuple S, A,  X  ,T,R ,where S is the set of states; A is the set of actions;  X  is the set of acceptable state action pairs; T : S  X  A  X  S  X  [0 , 1] is the transition function; and R : S  X  S  X  A  X  R is the bounded reward function. T ( s, a, s ) is the probability of transition from state s to s under consider finite discrete MDPs, where S and A are finite and discrete sets. The goal for many MDPs is to learn a policy to maximize discounted cumulative reward (with discount factor 0 &lt; X  1 ) continuing from any of the starting states. Many of the solutions proposed for MDPs belong to a category named value-based methods. In these methods, first, the values of states are estimated; V ( s )=  X  k r k ;and then in each state, the best policy chooses the action which takes it to the next state with the maximum value. It X  X  also possible to consider the action value, Q ( s, a ) , which is discounted cumulative reward starting from state s and action a and then choosing actions with best values in each state. If we now the model of MDP, then we can use Bellman optimality equation [2] to find optimal state or state-action value functions which are exactly the same for all optimal policies. Bellman optimality equation for Q  X  is: Value iteration [20] is an iterative method for estimating the solution of Bellman optimality equation pairs are updated,
When the model of state space is not known or it is not learned explicitly, other learning algorithms like Q-learning [36] are used to estimate optimal values: where  X  t is the learning rate which should be decreased gradually, satisfying t  X  t =  X  and t  X  2 t &lt;  X  , in order to let the algorithm converge to the optimal value.
Factored MDPs are the group of MDPs in which instead of explicit representation of all states, the Cartesian product of the domains of the state features. Most of the RL algorithms suffer from the curse of dimensionality. As the number of features of the state space increases, the number of states increases exponentially. The goal of state abstraction is to map the ground large space MDP to a smaller space, and use the solution of the abstract space to find the solution of the ground space. The first step of the abstraction process is to find a surjection mapping  X  : S  X  space. Li et al. [16] formulate the problem of state abstraction in MDPs as follows: Definition 1 .Let M = S, A, T, R be the ground MDP and its abstract version be Define the abstraction function as  X  : S  X  state s , and the inverse image  X   X  1 (  X  s ) ,with  X  s  X  abstraction function  X  . Note that under these assumptions, {  X   X  1 (  X  s ) |  X  s  X  space S . For each  X  s  X  w definitions at hand, we can define the transition and reward functions of the abstract MDP as follows:  X  T (  X  s, a,  X  s )=
It X  X  easy to verify that this new transition function is a well-defined next state distribution. Finally, value functions for the abstract MDP  X  V  X  (  X  s ) and  X  Q  X  (  X  s, a ) , respectively [16].

A natural definition for the weighting function is which is the probability of being in state s when the abstract state is  X  s .
 Definition 2 . Consider an N -dimensional data set X . A fuzzy partition (clustering) on X with c clusters can be represented in terms of a membership function by u : { 1 ,...,c } X  X  X  [0 , 1] ,where u ( i, x ) is the degree of membership of element x to the i th cluster. Conditions on the membership function are given by Ruspini [23]: Definition 3 . A hard (crisp) clustering on data set X has the same definition as fuzzy partition except that the first cond ition replaces with u ( i, x )  X  X  0 , 1 } , 1 i c, x  X  X . That means each element of X completely belongs to exactly one cluster. Any hard partition defines an equivalence relation on the data set so that all elements which belong to the same cluster form an equivalence class; in other words x j  X  x k if and only if  X  1 i c ; u ( i, x j )= u ( i, x k )=1 .

There are different fuzzy clustering algorithms any of which are suitable for special environments and data. One of the famous ones is fuzzy c-means clustering (FCM) which was developed by Dunn in 1973 [10] and improved by Bezdek in 1981 [4]. It is an iterative algorithm; given the number of clusters Where m is any real number greater than 1 , u ( i, x ) is the degree of membership of N -dimensional membership u ( i, x ) and the cluster centers c i are updated by between 0 and 1, and t is the iteration step.

The fuzzifier m determines the level of fuzziness of clusters. A large m results in smaller memberships u ( i, x ) and fuzzier clusters. As m converges to 1, the memberships u ( i, x ) converge to 0 or 1, which implies a hard partition. A sample fuzzy cluster and the effect of fuzzifier are shown in Fig. 1 adapted from [33]. 3. Q  X  -based state abstraction
Li et al. [16] classify different available state abstraction functions in RL into five main groups and provide a unified theory to determine the relation between them.
 Definition 4 . Given an MDP M = S, A, T, R,  X  ,andanystates s 1 ,s 2  X  S we define two types of abstraction as below, with an arbitrary but fixed weighting function w ( s ) : 1. A model-irrelevance abstraction  X  model is such that for any action a and any abstract state  X  The converse of lemma is not true necessarily. That means it X  X  possible to have states with identical Q  X  -values for all actions, but with nonequivalent models. This relation means clustering and abstraction based on Q  X  can lead to a smaller abstract space than clustering based on T,R . The problem is that while the MDP model can be available a priori, Q  X  values are not available and they will be obtained after learning. The popular way to avoid this bottleneck is to aggregate states between iterations of learning based on the estimations of Q  X  values, using Bellman residuals [25] or statistical tests [6]. In this way, as the estimations change, states are allowed to be aggregated or disaggregated adaptively in learning instead of leaning the original space without abstraction. Some of the works like soft state aggregation [25] even do not discuss the computational effectiveness of their method. This is the reason that more recent works concentrate on model-based abstraction and not on Q  X  -based abstraction [8,11, 21,32].
 Considering the advantages of Q  X  -based abstraction, which does not need to access the model of the MDP and leads to a more abstract space than model-based abstraction, we use a setting to benefit the advantages of this kind of abstraction and avoid the above mentioned problems. Instead of abstraction based on the estimations of Q-values during learning, we aggregate states after finishing learning and obtaining the Q  X  -values. As learning has already been finished, this abstraction doesn X  X  provide any benefit for the current task. However, in a setting where many tasks of the same domain are going to be learned over time, this abstraction can be done off-line and be used for the future tasks.
According to the unified theory of state abstraction, if we cluster states with completely same Q  X  -because of the convergence conditions of learning algorithms. Considering some approximate criteria for abstraction solves this problem and provides a smaller abstract space to solve. Because of these rea-sons, many of the works propose approximate versions of abstraction [3,11,22,32]. For example, Fern et the approximate state-action similarity. However, all of these works are approximate versions of model-based abstraction.
 Lemma 2 .Let d Fern : S  X  S  X  [0 , 1] be the model-based Fern metric on the state space. Assume within an -neighborhood. | by learning in the abstract space and c R (1  X   X  ) is the weight of impact of reward distances on the Fern metric [11].

In this section, we extend the results of the unified theory of state abstraction and show theoretically tency, we use some of the definitions and results of the previous works (denoted by references) and add new definitions and results to them.
 1. s = s  X  d ( s, s )=0 . 2. d ( s, s )= d ( s ,s ) . 3. d ( s, s ) d ( s, s )+ d ( s ,s ) .
 Any pseudometric d on a state space S defines a partition (clustering) on the state space so that s, s  X  S belong to the same cluster iff d ( s, s )=0 .
 Definition 7 .Let Q  X  : S  X  A  X  R be the optimal Q-value function of a task on state space S .Define It can easily be verified that the partition induced by d Q  X  is the same as the partition induced by the where 0 .Let the abstract state space and the original state space, respectively. Also, let  X  contributes to the abstract state  X  s .
 Lemma 3 . Suppose we start iterative steps of value iteration algorithm in both the ground and the abstract space by 0 for all states and actions. For all iteration k , abstract state  X  s  X   X  Q Proof. The proof is by induction. Base case ( k =0 ) is trivial according to assumptions. inductive step: Suppose the inequality holds for all iterations less than or equal to k for all  X  s, a .
Now, considering Eqs (2) and (3), we have: Lemma 5 . Suppose we start iterative steps of value iteration algorithm in both the ground and the abstract space by 0 for all states and actions. For all iteration k , abstract state  X  s  X   X  Q Proof. The proof is by induction. Base case ( k =0 ) is trivial according to assumptions. inductive step: Suppose the inequality holds for all iterations less than or equal to k for all  X  s, a . any two functions f,g .
 Theorem 1 . For any state s  X  S and its corresponding abstract state  X  s  X   X  Q  X  (  X  s, a ) Q  X  ( s, a )+ .
 Proof. According to Lemma 3,  X  k, So, According to Lemma 4, for any n  X  N there exists a k 0 such that for all i&gt;k 0 , d i &lt; + n .So, So, lim k  X  X  X  A k  X  1  X   X  . According to lemma 5,  X  k, According to the clustering criteria, the difference between Q  X  -values of any two states of each cluster that cluster by at most .
 Therefore, Q  X  ( s, a )  X  1 1  X   X 
Note that the convergence of value iteration method is independent of the start values in the first does [36].
 Corollary 1 . If we cluster states which have completely identical Q  X  -value for all actions, =0 ,then  X  s  X  S, a  X  A, Q  X  ( s, a )= is exactly what already has been stated in the unified theory of state abstraction ([16], Theorem 4). Experiments Grid world
The first environment we consider is the 5  X  5 grid world introduced by Ferns et al. [11]. There are 5 actions, stay, north, south, east and west. Transitions for each cell are uniformly distributed among adjacent cells. Rewards are distributed as follows. Moving south from rows 1 X 4 to rows 2 X 5 yields rewards of 0.1, 0.2, 0.3, 0.4 respectively. Moving east from columns 1 X 4 to columns 2 X 5 yields rewards of 0.5, 0.53, 0.56, 0.59 respectively. Finally, staying in the southeast corner yields a reward of 1. All other actions give 0 reward [11] (Fig. 2).

We compare the size of aggregated MDP as a function of using Fern X  X  approximate model-base abstraction [11] and our approximate Q  X  -based abstraction. For both cases, we start from some seed states and cluster states within an -neighborhood either based on d Q  X  or d Fern distance metric. We change the allowed distance between states of each cluster by varying from 0.0 to the maximum distance. Note that low (close to 0) means that only very close states (in terms of their distance) are allowed to be aggregated. Hence, at this end of the spectrum, very little aggregation will occur and the value function in the aggregated MDP should be very close (or identical) to the one in the original MDP. On the other hand, when grows, more and more states can be aggregated, finally resulting in a poor approximation of the optimal value function. Figures 3 and 4 show the size of the aggregated MDPs for  X  =0 . 9 , X  =0 . 5 , X  =0 . 1 , obtained using d
We apply value iteration algorithm in the original space and the abstract spaces with  X  =0 . 9 , X  = 0 . 5 , X  =0 . 1 . Table 1 compares the metrics d error bounds. In both cases, the increase of  X  leads to looser error bounds because of the 1 1  X   X  factor. more accurate than learning in the abstract space obtained by d Fern . Figure 5 compares these approaches for  X  =0 . 9 by showing the amount of error in value function after same amount of state space reduction offered by two approaches.
 N -Bit world
The second environment we consider is Binary 6-bit task (see Fig. 2) which is a new version of BitFlip some probability flips one or both adjacent bits, too. Each action results an immediate reward 0 for none-goal states and 1000 for reaching the goal which is a movement from a special combination of values Fern metric d Fern , we normalize reward function. Figures 6 and 7 show the size of the aggregated MDPs Table 2 compares the metrics d Fern and d Q  X  in terms of the actual errors for learned value functions. Figure 8 compares these approaches for  X  =0 . 9 by showing the amount of error in value function after same amount of state space reduction offered by two approaches. Therefore, state abstraction based on Q  X  values is preferable to abstraction based on Fern metric, first because of not needing the model of MDP, and second because of more accurate abstraction. 4. Fuzzy Q  X  -based clustering and state abstraction
Hard clustering might not provide a very useful abstraction in some domains. States which have similar to have much smaller number of clusters. With fuzzy clustering, elements are allowed to belong to several clusters with different degrees of membership, simultaneously. In many real situations, some data lay on the boundaries between clusters. With hard clustering, they are forced to fully belong to exactly one cluster. So, fuzzy clustering is more natural than hard clustering, as these data can have partial memberships with degrees between 0 and 1 to different clusters.

In the field of reinforcement learning, fuzzy logic is often used for the purpose of generalization and function approximation. Singh et al. [25] propose an algorithm for soft probabilistic (fuzzy) clustering of the state space (with fixed number of clusters) during learning based on the criteria of minimizing the probabilities (degrees of membership). The model of MDP and the value function of the original space are unknown. So, the novelty of the work is in finding the best soft (fuzzy) clustering during learning. The authors prove that under some conditions, Q-learning in the abstract space converges to the solution of Bellman optimality equa tions in that space. While traversing t he original state s pace, Q-values of abstract states are updated with different probabilities (degrees); and finally weighted average of the Q-values of abstract states are used to estimate the Q-values of the original states. The weight matrix (membership matrix) is updated based on the Bellman error and the process continues until the error is less than some bound. The advantage of this method is that only Q-values of the abstract states are saved and updated. One of the disadvantages is that the number of clusters should be fixed at first. The other problem is that for computing the Bellman error, because of not knowing the model of the original MDP, the average of the sample Bellman error should be computed. Singh et al. also do not prove if the algorithm.

Sorg and Singh [27] introduce the concept of soft homomorphism which assigns to each target state of a target task some degrees of membership to source states of a source task, so that the target transi-tion and reward functions be equal to the weighted sum of the source transition and reward functions, respectively. They prove that the weighted sum of the source optimal Q-values approximate the target optimal Q-values by an error bound. Knowing the model of the source MDP, They propose an online algorithm which learns degrees of membership of the target states to the source states whenever such a are actually considered as the fuzzy cluster centers to which different target states belong with degrees of membership, satisfying the homomorphism conditions. The first disadvantage is needing to know the model of the source MDP. The second drawback is that for an unrelated source task, the method does not work properly and there is no way to recognize this before trying to find the mapping. In other words, the designer should be sure about the relationship.

Busoniu et al. [5] propose a fuzzy Q-iteration algorithm for MDPs with known models. The authors apply fuzzy clustering for Q  X  value approximation. In their work, they assume the state space is parti-tioned into fuzzy clusters. The degrees of membership of states to clusters are considered as the basis functions, having a parameter corresponding to each cluster and action. The parameters corresponding niu et al. prove the convergence and consistency of their approach theoretically and experimentally. One drawback of this approach is that it assumes the number of fuzzy clusters and degrees of memberships of states to clusters are known a priori.

All of these works try to learn membership functions during learning or during iterations of value iteration method. Because of that, they consider many restrictions. For example, the number of fuzzy clusters are fixed for all of these works. There is also no freedom for choosing the best fuzzifer. We apply fuzzy c-means clustering on previously learned optimal Q-values. We try to find suitable fuzzifier and minimum cluster numbers so that the membership functions and clusters X  centers obtained by FCM estimate optimal Q-values with at most error . Then we modify Q-learning algorithm so that it converges to the clusters X  centers.
 Suppose the state space is partitioned into c fuzzy clusters such that where is the fuzzy center of optimal Q-values of states which belong to cluster i and m is the fuzzifier. In each time step, the agent is in all clusters simultaneously but with different degrees of membership. For center values for all clusters. Define Theorem 2 .Forall i =1 ...c,a  X  A , Proof. We start from the right hand side (RHS) of the equation, As hence, according to definitions, RHS Corollary 2 . If the transition function in the original space, T : S  X  A  X  S  X  [0 , 1] be deterministic, then for all i =1 ...c,a  X  A , where f ( s, a ) is the state for which T ( s, a, f ( s, a )) = 1 .
 proof is obvious following the proof of Theorem 2.

Busoniu et al. [5] consider MDPs with deterministic transition functions and actually use this corol-lary to propose their fuzzy Q-iteration algorithm which iteratively computes fuzzy centers of Q-values for all clusters and actions. The only difference is that for simplicity of learning, they assume that for considering all states in the above equation, they only consider the peak state:
As Q-learning update is applied for a deterministic transition of a step, the inequality which might happen for Q-iteration doesn X  X  occur for Q-learning. We show that by approximating optimal Q-values and learning in fuzzy abstract space, we can increase the speed of learning and reduce the sample cost. Our proposed algorithm is shown in Fig. 9. While learning in this fuzzy clustered space, we update Q-centers of clusters. There is a main difference between learning in this abstract space and learning in the hard abstract space. In hard abstract space, in each step the Q-center of just one cluster, to which the observation belongs, is updated. In fuzzy abstract space, in each step, the Q-center values of all clusters to which the observation belongs should be updated based on the degree of membership of that observation to those clusters. So, the necessary computations for each step is more that learning in the original space. However, as the number of fuzzy clusters is less than the number of states and the optimal Q-values of all states are estimated based on the Q-center values of clusters, it seems that this batch updating is worth.
 Experiments
We apply fuzzy c-means clustering on optimal Q-values of both the 5  X  5gridtaskandthe 6 -bit task, with  X  =0 . 9 , mentioned in Section 3. We modify FCM algorithm to choose the fuzzifier and the number of clusters from a range so that the estimations of the Q  X  -values by the fuzzy centers justify an error bound. Figure 10 compares the number of clusters obtained by hard and fuzzy clustering for different allowed error bounds for estimations. In all cases, the chosen fuzzifiers are in range 1.1 X 1.3. As shown in the figure, we can approximate the value function using less number of fuzzy clusters compared to hard clusters. This is because states can belong to multiple clusters and their values can be estimated using these fewer fuzzy centers. Since any update of the value of a cluster center leads to the update of values of its cluster members, in each iteration of learning in the fuzzy abstract space, more states are updated compared to the hard abstract space.

For 5  X  5gridtask,  X  =0 . 9 , A choice for the fuzzifier and the number of clusters which leads to estimation of optimal Q-values with upper bound error =0 . 5 is m =1 . 1 ,c =8 . The mean return per episode are represented in Fig. 11. Figure 12 compares the Q  X  -values of action east obtained by applying Q-learning and the estimations of Q  X  -values obtained by learned fuzzy Q-center values.
For 6-bit task,  X  =0 . 9 , A choice for the fuzzifier and the number of clusters which leads to estimation of optimal Q-values with upper bound error =60 is m =1 . 2 ,c =16 . The mean return obtained in the per episode are represented in Fig. 13. 5. Knowledge discovery State abstractions done in the previous sections don X  X  have any benefit for learning the task whose Q  X  -values are the source of abstraction. That task has already been learned and its original Q  X  -values are available. The usage of that abstraction is actually in a multi-task setting.

In multi-task reinforcement learning (MTRL), the source and target tasks are drawn independently from a domain which is a distribution over tasks [31]. It is assumed that all tasks of the domain have similar components like reward structure, dynamics, or value function. In this way, the target and source tasks are closely related, providing a nice candidate for transfer learning. In addition to Walsh approach for transferring abstraction, various other approaches to MTRL exist, any of which transfers one form of knowledge and uses the knowledge in target task(s) in a special way. Snel and Whiteson [26] use source tasks to form shaping function which guides and speeds up learning of the target task(s). Lazaric and Ghavamzadeh [14] consider a multi-task setting in which all tasks share state and action space and also structure in their value functions. The authors use hierarchical Bayesian models to model value functions similarities. These models are used for efficiently transferring related information from similar source tasks to the target task(s).

Consider a multi-task setting with same state and action spaces from which we have learned several tasks before and still there will be new tasks which should be learned. If we see similarities and kind of structure in previous learned tasks, we can be sure with a high probability that the structure will be available in the new tasks of the domain, too. Wide range of applications of clustering algorithms prove the potential of these methods to reveal the underlying structure in data. We propose two kinds of useful knowledge which can be extracted from clusterings in previous sections. First, suppose we have computed d Q  X  (defined in Section 3) for all previously seen tasks of the setting. Definition 8 . Suppose Define be used for abstraction of the future tasks of the setting. However, with hard partitioning, the amount of overall abstraction may not be considerable in some domains. The second kind of knowledge we propose is based on fuzzy partitioning of the tasks of the setting.
 measure and the minimum similarity measure prodSim, minSim : X  X  X  X  [0 , 1] between two elements x 0 . Note that it X  X  not much applicable to define such a measure based on hard clustering. In hard clustering, each state belongs to exactly one cluster and the similarity would be 0 or 1 . The contour map of the product and the minimum similarity measures between states of 5  X  5gridtask(  X  =0 . 9 ) obtained by fuzzy clustering with c =10 ,m =1 . 1 and between states of the 6-bit task (  X  =0 . 9 ) obtained by fuzzy clustering with c =8 ,m =1 . 3 are represented in Figs 14 and 15, respectively. As we expected, in 6 bit 27, ... ; and in summary all states which differ a multiple of 8. The reason is that states of each group have same first three bit values and these are the only bits to which the goal depends.

Suppose the product similarity measure (or minimum similarity measure) between all states are computed for all previously learned tasks of the setting. We define a new similarity measure of learning and reduce samples cost in the future tasks of the setting. As we saw in Section 4, by updat-ing centers of clusters to which an observation belongs, the values of all other states belonging to those clusters are updated indirectly. This batch updating reduces the sample cost. Here we use the same idea so that in each step, in addition to Q-values of the observation, we update Q-values of all other states based on their similarity to that observation. This batch updating can be vanished gradually by applying a decreasing coefficient for learning states similar to the observation.
 Experiments if first four bits change from 1011 to 0011, regardless of what other bits are, the episode ends and the agent receives the 1000-reward. The product similarity measure between states are available based on the fuzzy clustering of Q  X  -values (with fuzzifier m =1 . 3 and = 100 ). First, we apply this knowledge for the same task to check whether it reduces samples cost or not. Figure 16 proves the correctness of this approach.

Now, consider three other 8-bit tasks with  X  =0 . 9 , goal  X  X  X  X  X  X  1010  X  X  X  X  X  X  X  1011 for 8-bit task 2, goal  X  X  X  X  X  X  0011  X  X  X  X  X  X  X  0010 for 8-bit task 3, and goal  X  X  X  X  X  X  0111  X  X  X  X  X  X  X  0011 fuzzy clustering on their learned Q  X  -values and compute product states similarities for them. For fuzzy clustering, we modified FCM algorithm to choose the fuzzifier and the number of clusters from a range so that the estimations of the Q  X  -values by the fuzzy centers justify the error bound . Then we compute The contour map of the product similarity measure between states of Test 4 obtained by fuzzy clustering with = 100 ,m =1 . 3 ,c =26 and the minimum of product similarity measure of states obtained from tasks 1, 2, and 3 are represented in Fig. 17. Figure 18 compares the results obtained by learning task 4 without any knowledge and learning it using the knowledge obtained from task 1, task 2, and task 3. We sample cost.
 6. Conclusions and future work
In this paper, we propose two ways to leverage Q  X  -values of already leaned tasks of a domain to improve learning in the future similar tasks of that domain. In our first proposed method, we combine state abstraction and transfer learning. We show theoretically how hard abstraction based on Q  X  -values preserves the optimal value function in the abstract space with an error bound and speeds up learning in that same task in the future. Then, for each previously learned task, We do hard clustering on the state space based on Q  X  -values of that task. We extract a final hard partitioning from all clusterings of the tasks of the domain and transfer this abstraction to the new tasks of the domain to improve their learning. In the second approach, we combine function approximation and transfer learning. We examine theoretically and algorithmically how using the knowledge extracted by fuzzy value approximation of a single task improves learning of that same task in the future. Then, for each previously learned task, We approximate Q  X  -values by fuzzy clustering. After that, we define a similarity measure between states based on their memberships in fuzzy clusters. Finally, we define a similarity measure between states based on the similarities obtained from different tasks. We show empirically that batch learning based on this similarity measure can speed up learning in the future tasks of the setting.

An important next step is extracting other kinds of knowledge from Q  X  -values. we are going to extract relevant features of the factored MDPs based on Q  X  -values. Also, we are going to test these approaches for other domains. While in this paper only discrete MDPs are considered, our proposed approach can be extended to continuous MDPs. The knowledge obtained from tasks of the domain can be used for function approximation in the future tasks.
 Acknowledgments
A special thank you goes to those who contributed to this paper: Mozhgan Mombeini for her useful ideas and sharing her knowledge; Mahdi Hamzehei for his guidance, encouragement, and valuable com-ments. We also would like to thank Brian Tanner for the worthwhile RL-Glue framework and sample codes he has provided. References
