 Motivated by the enormous amounts of data collected in a large IT service provider organization, this paper presents a method for quickly and automatically summarizing and extracting meaningful insights from the data. Termed Clus-tered Subset Selection (CSS), our method enables program-guided data explorations of high-dimensional data matrices. CSS combines clustering and subset selection into a coher-ent and intuitive method for data analysis. In addition to a general framework, we introduce a family of CSS algorithms with different clustering components such as k-means and Close-to-Rank-One (CRO) clustering, and Subset Selection components such as best rank-one approximation and Rank-Revealing QR (RRQR) decomposition.

From an empirical perspective, we illustrate that CSS is achieving significant improvements over existing Subset Se-lection methods in terms of approximation errors. Com-pared to existing Subset Selection techniques, CSS is also able to provide additional insight about clusters and cluster representatives. Finally, we present a case-study of program-guided data explorations using CSS on a large amount of IT service delivery data collection.
 G.1.3 [ Mathematics of Computing ]: Numerical Analy-sis X  Numerical Linear Algebra ; H.4.0 [ Information Sys-tems ]: Information Systems Applications X  General Algorithms, Management subset selection, clustering, low rank matrix approximation, service delivery, service provider
Today, the vast majority of IT-enabled business processes generate large amounts of data that is stored and analyzed to provide useful insights and decision support. A typical scenario is to represent these data sets as high-dimensional data points in data matrices. Linear algebra and matrix techniques become necessary to process and analyze these data collections. In this paper, we present a novel matrix algorithm that combines clustering and subset selection into a coherent method for low rank matrix approximations. We utilize this novel method to analyze matrices that arise from a real IT service provider application.

In the modern statistical data analysis and data mining literature, there have been several efforts to analyze data using linear algebra and matrix techniques. Streams and co-evolving sequences can be envisioned as matrices; each data source (sensor) corresponds to a row of the matrix, and each time-tick to a column. We can perform multivariate analysis or Singular Value Decomposition [ 38 ],  X  X ketches X  and random projections [ 30 ] to find patterns and outliers. Information retrieval systems, where the data can be turned into document-term matrices, routinely utilize SVD or Latent Semantic Analysis (LSI) [ 19 , 37 ]. In web applica-tions we have matrices where both rows and columns repre-sent web-pages; we can apply the HITS [ 34 ] or the pageR-ank [ 6 ] algorithm to find hubs, authorities and influential nodes. Finally, while analyzing social networks , and in fact, any graph (with un-labeled edges), we have matrices where people are rows and columns, and edges correspond to the non-zero entries in the adjacency matrix of the graph. The network value of a customer [ 29 ] has close ties to the first eigenvector of this matrix. In a similar setting, graph partitioning [ 32 ] is often done through matrix algebra (e.g. spectral clustering [ 31 ]).

In the process of developing techniques to help IT pro-gram executives to analyze their data, we came up with two basic challenges. The first was the development of simple, user-friendly methods. The standard techniques described above require advanced mathematical skills such as Singular Value Decomposition (SVD), Eigen-Decomposition, or Prin-cipal Components Analysis (PCA), which prohibits many domain experts, such as IT program executives, to employ them for data explorations. The second was to find tech-niques that analyze the data using concrete concepts that are familiar to the users, not some abstract notions of latent variables or principal components. For example, since users X  expertise are from the original data, the analysis should also be represented directly in terms of the original data.
Consider an information technology (IT) services environ-ment where the IT service provider is hosting and managing the IT infrastructure for a number of clients. The services include a diverse set of offerings, e.g., data center opera-tions, application, server, storage and network management, and end user support. By outsourcing these IT services the client reduces its operational cost while maintaining a pre-dictable level of service quality through contractual service level agreements (SLAs).

The IT service provider continuously monitors a large set of metrics spanning applications, servers, networks, and the effectiveness (quality) of its service management opera-tion. One aspect of service management is the problem and change management processes. Service requests arrive in the form of tickets, which are then routed to specific service matter experts (SMEs) for resolution. The service quality provided to a client depends on how efficiently these tickets are handled. There are a number of metrics to track quality (e.g., the time to resolve a ticket) and service level agree-ments are typically drafted on top of these metrics. The challenge for the service provider is to manage its resources effectively in order to meet all the committed SLAs.
Typical metrics for problem and change management in-clude workload metrics such as number of opened or closed problems or changes and performance metrics such as the Mean Time To Resolve (MTTR). Each metric is further as-sociated with the context from which it was obtained. This would include the timestamp, the business process, the busi-ness unit, and the client, to name a few.

The total number of accounts for a large service provider can be in the order of thousands. For example, the data used in the experiments presented in this paper, come by monitoring a set of ten metrics over a set of more than 1000 accounts. There is an enormous wealth of insights and busi-ness optimization decisions that can be inferred from the data. However, given the scale and complexity, it is difficult for common users without the right tools to mine this hid-den value and discover latent relations. Of particular inter-est is the discovery of relationships between the time series between IT metrics and the clustering according to similar levels of performance. The discovery of  X  X imilarity X  relation-ships in the data leads to particularly useful insights with respect to how quality can be maintained and improved.
To address these issues from an engineering standpoint, this paper answers the following questions: given n service accounts over time (modeled as an m -by-n matrix), how to cluster the accounts into groups, how to identify the key accounts in each group, and how to represent the others using the key accounts?
In this paper, we leverage techniques from the numerical linear algebra to develop a powerful framework for program-guided data explorations. More specifically, we present a novel Subset Selection technique, named Clustered Subset Selection (CSS). CSS provides an algorithmic framework with the following properties: ( i ) rigorous algorithm : it is a novel deterministic Subset Selection method on matri-ces with consistent improvement over existing techniques, ( ii ) comprehensive solution : it provides a comprehensive process for clustering data dimensions, selecting key dimen-sions, and synthesizing the rest dimensions, and ( iii ) intu-itive mining result : the results of CSS are in the form of actual dimensions of the data matrix, instead of abstract no-tion of latent variables or principal components. The actual dimensions can provide intuitive explanation to the users. Finally, in addition to this algorithmic framework, a case study shows how the results of CSS can be used for data exploration on real IT service metrics.

Organization: The rest of the paper is organized as fol-lows. In section 2, we review related work and basic facts from linear algebra. Section 3 presents existing (determin-istic and randomized) Subset Selection algorithms. In sec-tion 4, we present a novel Subset Selection method while in section 5 we experimentally evaluate the the new method. Finally, in section 6 we discuss how CSS can be leveraged to quickly explore and extract insights of real IT service data.
We start by briefly presenting some basic definitions and facts from linear algebra; more details can be found in [ 24 ]. We then review two important stepping stones of our Clus-tered Subset Selection (CSS) algorithm, i.e. two low rank matrix approximation schemes that proceed with a cluster-ing preprocessing step of the input matrix. More related work is presented in section 3, where we discuss existing Subset Selection algorithms for matrices. Our CSS frame-work combines ideas of the above domains to offer a novel way of quickly exploring, summarizing, and understanding large and/or complex datasets. Let [ n ] denote the set { 1 , 2 , . . . , n } . For any matrix A  X  R m  X  n , let A ( i ) , i  X  [ m ] denote the i -th row of A as a row vector, and let A ( j ) , j  X  [ n ] denote the j -th column of A as a column vector. In addition, let k A k 2 F = note the square of its Frobenius norm, and let k A k 2 = denote an n  X  n permutation matrix. Also for an m  X  n ma-trix A , A i,j arises if we interchange the i -th and j -th columns of A . We occasionally use Matlab notation, like A (: , 1 : k ) , to denote the submatrix of A with its k first columns. matrix. The Singular Value Decomposition (SVD) of A can be written as A = U A  X  A V T A = In this expression,  X   X  min { m, n } denotes the rank of A , U
A  X  R m  X   X  is an orthonormal matrix,  X  A is a  X   X   X  diagonal matrix, and V A  X  R n  X   X  is an orthonormal matrix. Also,  X  k denotes the k  X  k diagonal matrix containing the top k singular values of A ,  X   X   X  k denotes the (  X   X  k )  X  (  X   X  k ) matrix containing the bottom  X   X  k singular values of A , V k denotes the n  X  k matrix whose columns are the top k right singular vectors of A , and V  X   X  k denotes the n  X  (  X   X  k ) matrix whose columns are the bottom  X   X  k right singular vectors of A , etc. The m  X  k orthogonal matrix U consisting of the top k left singular vectors of A is the  X  X est X  k -dimensional basis for the column space of A since A k = U  X  k V T k = U k U T k A is the  X  X est X  rank-k approximation to A . In particular, A k minimizes k A  X  A 0 k  X  , for both  X  = 2 and F , over all m  X  n matrices A 0 whose rank is at most k . We will use the notation k X k  X  when writing an expression that holds for both the spectral and the Frobenius norm. We will subscript the norm by 2 and F when writing expressions that hold for one norm or the other.
 QR decomposition. Another matrix decomposition that is useful in our discussion is the QR decomposition. Let A be an m  X  n matrix. The QR decomposition of A can be written as where Q  X  R m  X  n is an orthonormal matrix and R is an n  X  n upper triangular matrix. The QR decomposition of A can be computed for example by applying the Gram-Schmidt orthogonalization process on the columns of A .
 Pseudoinverse. The Moore-Penrose generalized inverse, or pseudoinverse, of A , denoted by A + , may be expressed for example in terms of the SVD as A + = V A  X   X  1 A U T A . For matrices A  X  R m  X  n and C  X  R m  X  k : i.e. the matrix X = C + A is the solution to the general-ized regression problem (least squares problem with multiple right hand sides) with inputs C and A .
Given an m  X  n matrix A , an important problem in linear algebra and scientific computing is to find a  X  X ood X  low rank matrix approximation to A , that can be expressed as the product of an m  X  k matrix D and an k  X  n matrix G , as A  X  DG , for some k  X { m, n } . As we discussed above, SVD provides the optimal solution to the appropriate optimiza-tion problem. Other suboptimal solutions to this problem are also of great theoretical and practical interest. In this section we present two suboptimal techniques that find the matrix D of the above factorization by first clustering the columns of A into k clusters and then computing a vector (column of D ) from each cluster. Our method, that is pre-sented in section 4, was motivated by these two algorithms in the sense that it also proceeds in a similar way for com-puting the matrix D .

Given a matrix A  X  R m  X  n and an integer k , Modha and Dhillon ([ 15 ]) present a low rank matrix approxima-tion scheme where one ( i ) clusters the input matrix A into k clusters using the Spherical k-means algorithm, ( ii ) selects the centroid vector (concept vector) of each cluster to be a column of an m  X  k matrix D , and ( iii ) approximates A as the rank-k matrix A  X  DD + A . Empirical results establish the efficiency of the above technique (also called Concept Decomposition), in the sense that the Frobenius norm of the residual error A  X  DD + A is close to the Frobenius norm of the residual error from the best rank-k approximation of A .

Along the same lines, Zeimpekis and Gallopoulos ([ 23 ]) present a low rank matrix approximation scheme where one ( i ) clusters the input matrix A into k clusters using the Spherical k-means algorithm, ( ii ) selects the leading left sin-gular vector of each cluster to be a column of an m  X  k matrix D , and ( iii ) approximates A as the rank-k matrix A  X  DD + A . Empirical results establish the efficiency of this technique (also called CLSI, an abbreviation of Clus-tered Latent Semantic Indexing) in a similar sense to the one described above. The authors also propose a variant of the above technique that first forms ` clusters of the columns of A , for some ` &lt; k , and then selects the leading k gular vectors from each cluster for 1 : ` , for some positive integers k 1 , k 2 , ..., k ` such that that Zeimpekis and Gallopoulos discuss similar ideas in [ 42 ].
The clustering aspect of both works is similar to the algo-rithm presented in this paper. However, our focus is on se-lecting actual columns from each cluster as representatives, not abstract notions of centroids or singular vectors.
In this section we discuss Subset Selection algorithms for matrices. We do emphasize here that the term  X  X ubset Se-lection X , has appeared in many different settings in the lit-erature (see example [ 39 , 11 , 12 ]). Our study focus on the family of algorithms from the numerical linear algebra lit-erature, introduced in the seminal work of Gene Golub in [ 25 ]. To the best of our knowledge, these techniques be-sides their  X  X ood X  empirical behavior, are equipped with a sound theoretical framework for their performance. Other subset/feature selection techniques that are commonly used by practitioners for similar purposes still remain pure heuris-tical choices from a theoretical perspective.

Given an m  X  n matrix A , the Subset Selection algorithms discussed in this paper select a small subset of k ( k  X  n ) columns of A , to minimize the distance || A  X  CC + A ||  X  C is an m  X  k matrix that contains the selected columns, and C + A = arg { min X  X  R k  X  n || A  X  CX ||  X  } . Intuitively, A is summarized to its column submatrix C , which contains only a small subset of the columns of A ; A can then be reconstructed as A  X  CC + A . Formally, a Subset Selection algorithm attempts to (approximately) solve the following combinatorial optimization problem:
Problem 1. Subset Selection Problem : Given a ma-trix A  X  R m  X  n and a positive integer k , pick k columns of A forming a matrix C  X  R m  X  k such that the residual is minimized over all possible Here  X  = 2 or F denotes the spectral norm or Frobenius norm.

Intuitively, a Subset Selection algorithm identifies the X  X ost X  linearly independent columns of the matrix A . Although other qualitative measures might be suitable for evaluat-ing the performance of a Subset Selection Algorithm (e.g see section 4 in [ 8 ] and the discussion in [ 26 ]) we focus on the spectral norm or Frobenius norm of the residual error A  X  CC + A .

Algorithms for constructing such a matrix C go back to the seminal work of Gene Golub in [ 25 ] on pivoted QR fac-torizations. Since then, much work has be done on Subset Selection algorithms. Current research spans two different fields of computer science and mathematics. In the numeri-cal linear algebra literature, one can find deterministic Sub-set Selection algorithms, while in the theoretical computer science literature the technique of randomization has lead to efficient randomized Subset Selection techniques. Within the numerical linear algebra community, Subset Selection algorithms leverage the so-called Rank Revealing QR (RRQR) factorization.

Definition 3.1. (The RRQR factorization) Given a matrix A  X  R m  X  n ( m  X  n ) and an integer k ( k  X  n ), assume partial QR factorizations of the form: where Q  X  R m  X  n is an orthonormal matrix, R  X  R n  X  n R above factorization is called a RRQR factorization if it sat-isfies where p 1 ( k, n ) and p 2 ( k, n ) are functions bounded by low de-gree polynomials in k and n .
 Subset Selection Algorithms as described above are linked to another matrix factorization, where the columns of the input matrix A are (approximately) expressed as linear combina-tions of actual columns of A .

Definition 3.2. (The CX factorization) Given a ma-trix A  X  R m  X  n and an integer k ( k  X  n ), assume an (ap-proximate) matrix factorization of the form where C and X are m  X  k and k  X  n matrices, respectively. The above is a column-based or a CX factorization of the matrix A if the matrix C consists of exactly k columns of A , and X is any k  X  n matrix.
 We now draw the connection between a RRQR and a CX factorization of a matrix. We do emphasize here that the result of Lemma 1 is not a new theoretical contribution. Although most of the papers discussing RRQR techniques did not explicitly state the aforementioned connection, re-sults presented in this Lemma are essentially implied in most manuscripts. Since, to the best of our knowledge, a techni-cal proof of this connection has not appeared in any paper discussing RRQR methods, we include a proof of Lemma 1 for completeness of our discussion and for helping readers better understand how RRQR algorithms lie in the core of the Subset Selection methods discussed in this paper.
Lemma 1. Given a matrix A  X  R m  X  n ( m  X  n ) and an integer k ( k  X  n ), for any RRQR factorization of A , R form with C = A  X  I n  X  k  X  R m  X  k ( I n  X  k is the n  X  k identity ma-trix), and X = ( A  X  I n  X  k ) + A  X  R k  X  n , such that and Here C consists of the k leading columns of A  X  . Proof: Let Q = R R The QR factorization of A  X  can be expressed as with Z  X  R ( n  X  k )  X  k an all-zeros entries matrix. Then since Q 2 is an orthonormal matrix, and || UX ||  X  = || X || for any orthonormal matrix U and any unitarian invariant norm  X  , such as the Frobenius and 2 norm. Also note that standard perturbation theory for matrices in [ 41 ] implies that padding a matrix A with all-zeros columns does not affect the Frobenius norm or the 2 norm of A .

The proof concludes by proving that || A  X  CC + A ||  X  = || A  X   X  Q 1 R 1 ||  X  .
In the above, we used that R 1 = Q T 1 A  X  , C = Q 1 R 11 that  X  is an orthonormal matrix. Also note that R 11 is a square matrix with linear independent columns (thanks to the RRQR of A ); thus is invertible. Clearly, any algorithm that constructs a RRQR factoriza-tion of the matrix A with provable guarantees for the leading singular value of the factor R 22 , also provides a CX factor-ization of A with provable performance for the spectral norm of the residual error A  X  CC + A : k A  X  CX k 2  X  p 2 ( k, n )  X  k +1 ( A ) = p 2 ( k, n ) k A  X  A C and X can be selected as indicated in Lemma 1 .
Table 1 summarizes existing deterministic Subset Selec-tion methods ( RRQR factorization algorithms). Algorithms are listed in a chronological order. You can evaluate these algorithms in terms of their approximation error theoretical behavior (column 2), and their asymptotical running time bound (column 3). Method p 2 ( k, n ) Time
Pivoted QR [ 25 ]
High RRQR [ 20 ]
High RRQR [ 7 ]
RRQR [ 28 ]
Low RRQR [ 9 ]
Hybrid-I RRQR [ 10 ]
Hybrid-II RRQR [ 10 ]
Hybrid-III RRQR [ 10 ]
Strong RRQR [ 27 ]
Strong RRQR [ 27 ] O (
DGEQPY [ 4 ] O (
DGEQPX [ 4 ] O ( SPQR [ 40 ] --
PT Algorithm 1 [ 36 ] O (
PT Algorithm 2 [ 36 ] O (
PT Algorithm 3 [ 36 ] O (
Pan Algorithm 2 [ 35 ] O ( Table 1: Deterministic Subset Selection Algorithms. A dash means that the algorithm either runs in O ( n k ) time, or the authors do not provide a running time bound.
Within the theoretical computer science community, Sub-set Selection Algorithms introduced from Frieze, Kannan, and Vempala in [ 22 ] and popularized by Despande et al. in [ 13 , 14 ], and Drineas et al. in [ 16 , 17 , 18 , 5 ]. The algo-rithms from this community are randomized, which means that they come with a failure probability, and focus mainly on bounding the Frobenius norm of A  X  CC + A . The best such algorithm ([ 5 ]) needs O ( mn 2 ) time to find k columns of A such that with high probability ysis purposes, deterministic algorithms are often preferred by practitioners. Deterministic techniques can guarantee that the same result will be obtained on the same data, a fundamental property on real data mining applications. On the other hand, randomized algorithms typically have bet-ter theoretical properties and are simpler to be implemented and used. This paper focus on deterministic Subset Selec-tion techniques since our proposed framework is motivated by a real data application.
We now present the Clustered Subset Selection (CSS) problem and an approximation algorithm (CSS Algorithm) for this problem, which are our main contributions of this pa-per. Clustered Subset Selection is a relaxation of the Subset Selection problem discussed above. We basically propose to find an approximate solution to the Subset Selection prob-lem by finding an approximate solution of this relaxation.
Problem 2 (Clustered Subset Selection). Given a matrix A  X  R m  X  n and a positive integer k , ( i ) find a number `  X  k to be the number of clusters of A , ( ii ) partition the columns of A into ` clusters, denoted by A i , and ( iii ) select k representative columns C i  X  R m  X  k i from each cluster A for 1  X  i  X  ` , such that 1) is minimized.
 In the sequel we present an algorithm, named Clustered Subset Selection (CSS), to find an approximate solution to the above problem. Our Algorithm has the following prop-erties: ( i ) empirically it consistently outperforms existing deterministic subset selection methods on the datasets used in this paper, and ( ii ) it provides a novel powerful framework for quick data exploration through clusters and representa-tives.
Due to the complexity of the problem (see discussion be-low), the optimal solution might be infeasible to obtain for practical applications. Therefore, we propose a top-down greedy approach to independently (approximately) solve the clustering and subset selection sub-problems of Problem 2. Our approach is presented in Algorithm 1 .
 Algorithm 1 CSS general 1: in: data matrix A  X  R m  X  n 2: in: number of clusters and representatives ` , k 3: cluster the columns of A into ` clusters. 4: determine the number of representatives k i such that P 5: for n = 1 , . . . , ` do { find representatives } 6: select k i columns C i from the cluster A i 7: end for 8: return cluster assignments and representatives [ C i ] Design choices. Depending on the clustering and subset selection algorithms, many different instances of algorithms can be designed. Moreover, the relative size of ` and k also affects the designs. When ` = k , CSS will select one rep-resentative column per cluster. More generally, if `  X  k , first it has to determine the number of representatives from each cluster and then to select a number of representatives independently from each cluster. A simple heuristic for de-termining the number of representatives from each cluster is to make them proportional to the size of the cluster. Table 2 illustrates the design choices of the CSS meta-algorithm.
Having discussed the general framework, we now present specific design options for clustering and subset selection.
Virtually all clustering algorithms can be applied for the clustering step in Algorithm 1 . Here we present two options that are used in our experiments: 1) deterministic k-means and 2) Close-to-Rank-One clustering (CRO) [ 33 ]. Kmeans clustering. : k-means is a fairly standard method for clustering. The only customization that is needed in order to have a deterministic Clustered Subset Selection method is to start with a deterministic initialization of the cluster centers. For example, we can pick the first ` columns of A as the initial cluster centers. The assumption/objective of k-means is to put points close to each other into the same cluster. For completeness to our discussion we present the standard Loyd X  X  algorithm as Algorithm 2 in this paper. CRO clustering. : Another option is to partition the ma-trix A into clusters of close to rank-one matrices; then we can easily select one or a few columns to approximate the rest well. To the best of our knowledge, the only algorithm available in the literature for satisfying the above objective is the technique described in [ 33 ]. Algorithm 3 gives a high level description of this technique. We refer to the original paper for more details. Note that the CRO value of a ma-trix X is defined as CRO ( A ) =  X  2 max ( X ) rank one) value of a matrix X essentially measures how close to a rank-1 matrix X is. The larger the CRO the closest to a rank-1 matrix X is.
 Algorithm 2 k-means Clustering 1: in: data matrix A  X  R m  X  n 2: in: number of clusters `  X  n 3: Initialize vectors c 1 , c 2 , ..., c ` , with the first ` columns 4: repeat 5: For i = 1 : ` , set the cluster A i to be the columns of A 6: For i = 1 : ` , c i = 1 | A 7: until no changes occur to the clusters A i 8: Return final clusters A 1 , A 2 , ..., A ` .
 Algorithm 3 Close-to-rank-one Clustering 1: in: data matrix A  X  R m  X  n 2: in: number of clusters `  X  n 3: Calculate CRO for every pair of clusters. Here a pair of 4: Start with every column of A as its own cluster. 5: repeat 6: Merge the pair of clusters with the largest CRO. 7: Compute CRO between the new cluster and the re-8: until ` clusters remain 9: Return final clusters.
Many subset selection algorithms have been proposed in the literature (see Section 2 for a survey). In this paper, we focus on deterministic algorithms which are often pre-ferred in the exploratory data analysis. More specifically, Algorithm 4 Best column selection in: Data matrix A  X  R m  X  n for i = 1 : n do { all columns } end for Return x i such that i = arg min 1  X  i  X  n e i Algorithm 5 RRQR Subset Selection 1: in: Data matrix A  X  R m  X  n , 2: in: number of representatives k 3: Initialize R = A ,  X  = I n 4: for n = 1 , . . . , k do { greedy selection } 5: 6: Let R 22 denote the ( n  X  i + 1)  X  ( n  X  i + 1) trailing 10: end for 11: return C = A  X (: , 1 : k ) we present two fast deterministic algorithms for ` = k (one representative per cluster) and ` &lt; k (more than one repre-sentative per cluster), respectively.
 to select exactly one column from each cluster that best represents all the columns. This can be formalized as the following combinatorial problem: Given a cluster A i , select a column x opt  X  A i such that: Note here that from basic linear algebra the pseudo-inverse of a vector x is x + = x T x T x . In terms of algorithm, one could select the best column x opt from the cluster A i by enumer-ating through all columns in A i (see running time analysis below for a justification of the efficiency of this approach). The pseudocode is presented in Algorithm 4 .
 we potentially need to select more than one column from each cluster. In particular, we need to answer two questions. How many representatives does each cluster have? How to select those representatives from each cluster? To answer the first question, we select the number of representatives based on the size of the cluster. More specifically, the size can be proportional to the number of columns of A i . To answer the second question, we adopt a fast deterministic Subset Selection algorithm from Table 1. The method of [ 25 ] is presented as the Algorithm 5 of this paper. We start be briefly commenting on the complexity of the Clustered Subset Selection problem defined above. The Clus-tered Subset Selection problem is a hard combinatorial opti-mization problem. Since clustering and subset selection are hard by themselves (e.g. k-means is provably NP-hard), we do believe that the Clustered Subset Selection prob-lem is NP-hard. Though, we didn X  X  attempt to prove the NP-hardness of the Clustered Subset Selection problem and leave this issue as an open question.
 We now discuss the running time of our approximation Clustered Subset Selection Algorithm. Let X  X  assume that A is an m  X  n matrix and that it is clustered into ` clusters A for 1  X  i  X  ` . The cluster A i has dimensions m  X  n i , such that proportional to the cost of the Clustering Algorithm. We denote this cost as O ( clustering ) . This cost obviously de-pends on the clustering technique selected in this step of our Clustered Subset Selection meta-algorithm. In our experi-ments we basically run twenty iterations of the aforemen-tioned described iterative k-means algorithm; thus this step takes polynomial time on the size of the input matrix. Note that since we don X  X  really care to find the optimal cluster-ing assignments, twenty iterations suffice for our purposes. For the second step, when ` = k , for each cluster A i we need O ( mn 2 i ) time to find the best column x i via exhaustive enumeration. This might be of the order O ( mn 2 ) , because n i = O ( n ) . This implies that the second step of the Clus-tered Subset Selection algorithm needs O ( mn 2 ) time, and the overall process costs O ( clustering + mn 2 ) time. When ` &lt; k , we asymptotically need O ( clustering + mnk ) , mainly because the pivoted QR technique (Algorithm 5 of this pa-per) needs O ( mn i k i ) time to select k i columns from A and k i = O ( k ) , n i = O ( n ) . Overall, if we carefully select the clustering algorithm of the Clustered Subset Selection meta-algorithm, the running time of our algorithm is poly-nomial on the size of the input matrix. Thus our algorithm can handle large scale datasets in reasonable time.
In this section we experimentally evaluate our CSS method with the existing deterministic Subset Selection techniques described in section 3. Note that our CSS algorithm is typi-cally slower than the classic deterministic algorithms due to the clustering overhead. Despite that overhead, CSS meth-ods can finish in seconds to minutes. The complexity of our method was analyzed in the previous section. Therefore, we do not report the detailed running time experiments; our comparison focus on the errors incurred by the related low rank matrix approximations.
We use two well-known synthetic matrices from the nu-merical analysis literature and two matrices that arise from real world data applications (Finance and Image Process-ing). See below for a detailed description of our datasets. 1. Kahan : This is an n  X  n matrix which is constructed as 2. GKS : This is an n  X  n upper triangular matrix whose 3. sandp : Yahoo! provides historical stock prices for the 4. digits : This is a 320  X  390 pixel-by-digit matrix. In
We now describe the Subset Selection methods that are used in our experiments. We also provide pointers to pub-licly available software implementing these methods. Again, our study focus on deterministic Subset Selection techniques. 1. Pivoted QR: We employed MATLAB X  X  qr function. This 2. SPQR: The Semi Pivoted QR (SPQR) method described 3. qrxp: qrxp is the algorithm implemented as the LA-4. qryp: qryp is the algorithm implemented as the LA-5. CSS1: CCS1 stands for an instance of our Clustered 6. CSS2: CCS2 stands for an instance of our Clustered 7. CSS3: CCS3 stands for an instance of our Clustered 8. CSS4: CCS4 stands for an instance of our Clustered
Our experimental results are depicted in Table 4 . Each column of this table corresponds to a Subset Selection algo-rithm and each row to a dataset. The column and row ids of this table coincide with the above description. The ( i, j ) -th entry of Table 4 , represents the value || A  X  CC + A || F A || F . We use the same k and ` for all methods and the value of k and ` are presented in Table 3.

The CSS family algorithms are very competitive compared to the other deterministic methods. In particular, CSS1 consistently outperforms the other methods (up to 4X) in all cases. The results confirm that besides providing more information (clusters plus representatives), CSS family al-gorithms opens a new direction for the classical Subset Se-lection problem.

Finally, we report a numerical experiment for a compar-ison of our method with the CLSI method of [ 23 ] and the Concept decomposition method of [ 15 ]. We ran all three methods, CSS1, CLSI, and Concept Decomposition on the sandp dataset with k = 12 . The results (in the form of rel-ative error as in Table 4) are 1.31, 1.12, and 1,13 for CSS1, CLSI, and Concept Decomposition methods, respectively.
Having shown the superiority of CSS methods on general datasets, now we present more fine-grained analysis and case study on IT service data.
As we motivated in the introduction, IT service providers provide diverse service lines for various companies (accounts) across the globe. The combination of geography, account and service line forms a monitoring basis called WID.
It is of key importance for them to monitor and correlate all the service metrics and to identify potential problems quickly.

Example metrics are presented in the following categories: As a service provider, the total number of metrics across accounts and service lines can be gigatic. It is very hard for the program exectutives to monitor all the metrics for all accounts and service lines. The goal of applying CSS on service metrics is to summarize the service data into man-ageable clusters based on their behaviors and to highlight the representatives from each cluster. Finally, the represen-tatives can be easily utilized to predict other metrics.
The data used in the paper is obtained from the web-based data mining tool for IT service delivery monitoring, called  X  X rism Insight X . It monitors problem and change data of 11 quality metrics for over 1,000 WIDs. Among which, we focus on two service quality metrics and two work characteristics metrics: These metrics are computed on a daily basis. For each of the four metrics, we construct a timestamp-by-WID matrix A . Since the measurements across WIDs are not directly com-parable 1 , we normalize each WID (a column in the matrix) by its norm, i.e. A i = A i k A i k . Next we remove the all zero columns and rows. As shown in Table 5, we obtain four ma-trices of size 240  X  749 (MTTR), 240  X  671 (SLA), 240  X  679 (Opened changes) and 240  X  669 (Closed changes).

Next we first present a quantitative comparison as the one presented in the previous section, and then discuss the mining results of the application of CSS on the service data.
Recall that the two parameters in the CSS methods are the number of clusters ` and the number of representatives k . Since CSS1-2 always outperform CSS3-4, we only include the results for CSS1-2. First, we vary ` by fixing k = 100 . Results are presented in Table 6. Notice that ` do not affect the error much when k is fixed. Second, we vary k and fix Table 6: Relative residual error vs. the number of clusters ` for k = 100 ` = k as shown in Figure 1 . For this experiment, we present the absolute errors instead of relative errors. As expected, the error monotonically decreases smoothly as k increases for all methods. Note that our CSS1 method is significantly better than the others.
The metric values are highly dependent on the particular service line and account size. rest of methods. The immediate application of the CSS method is to cluster WIDs and then to identify representatives within each clus-ter. We will show some example clusters and its representa-tives by using CSS1 on the MTTR matrix with k = ` = 100 . First, Figure 2 shows the distribution of the cluster sizes. Note that, most of clusters are fairly small but with two ex-ceptions. In particular, the largest cluster has 180 members. We manually investigate that cluster and find out that all the members have very few nonzero entries, which means they are the  X  X ealthy X  WIDs 2 .
Second, for the small clusters, we further investigate each one of them. Most of the WIDs either belong to the same service line or come from the same accounts. The repre-sentative WID from each cluster are the one that can best approximate the rest ones in the group. As shown in Table 7, some small clusters include service line such as Mainframe,
Healthy WIDs have very few problems or the problems got resolved quickly with small MTTR.
 DBDC and Storage and work streams (sub-service line) such as MVS, IMS and Dist Storage SAN. We are unable to show the account names due to the privacy complaint, but CSS does successfully group several relevant accounts into the same cluster.
 Representative WID Cluster members Mainframe -MVS Mainframe -MVS DBDC-IMS DBDC-CICS Storage-Dist Storage SAN Storage-Dist Storage B/R Table 7: Example clusters and their representa-tives: Each WID consists of service line and work stream. Accounts are not presented due to privacy constraints.
Modern applications generate complex data that are typ-ically represented as high-dimensional points in data matri-ces. How to identify automatic summary and insight of such matrices? How to help users quickly gain intuitive under-standing of complex data? To address the above questions, we present a powerful framework for program-guided data explorations named Clustered Subset Selection (CSS).
The Clustered Subset Selection approach provides a co-herent method to quickly summarize a data matrix using the clusters and the representative columns. Various ver-sions of the CSS general technique have been discussed in this paper. The proposed methods are highly competitive against several other subset selection algorithms. In par-ticular, one instance of our general CSS meta-algorithm, the CSS1 method, consistently outperforms others on all datasets. Finally, the Clustered Subset Selection approach demonstrates a great potential for data mining on summariz-ing IT service data. Future work includes developing time-evolving models for the CSS method as well as generalizing it to high-order data. Another interesting issue is to perform a theoretical analysis of our algorithm to derive bounds sim-ilar with those of Table 1.
