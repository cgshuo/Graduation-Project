 As data-driven techniques tackle more and more complex natural language processing tasks, it be-comes increasingly unfeasible to use complete, ac-curate, hand-annotated data on a large scale for training models in all languages. One approach to addressing this problem is to develop methods that automatically generate annotated data by transfer-ring annotations in parallel corpora from languages for which this information is available to languages for which these data are not available (Yarowsky et al., 2001; Fung et al., 2007; Pad  X  o and Lapata, 2009).
Previous work on the cross-lingual transfer of se-mantic annotations (Pad  X  o, 2007; Basili et al., 2009) has produced annotations of good quality for test sets that were carefully selected based on seman-tic ontologies on the source and target side. It has been suggested that these annotations could be used to train semantic role labellers (Basili et al., 2009).
In this paper, we generate high-quality broad-coverage semantic annotations using an automatic approach that does not rely on a semantic ontol-ogy for the target language. Furthermore, to our knowledge, we report the first results on using joint syntactic-semantic learning to improve the quality of the semantic annotations from automatic cross-lingual transfer. Results on correlations between syntax and semantics found in previous work (Merlo and van der Plas, 2009; Lang and Lapata, 2010) have led us to make use of the available syntactic anno-tations on the target language. We use the seman-tic annotations resulting from cross-lingual transfer combined with syntactic annotations to train a joint syntactic-semantic parser for the target language, which, in turn, re-annotates the corpus (See Fig-ure 1). We show that the semantic annotations pro-duced by this parser are of higher quality than the data on which it was trained.

Given our goal of producing broad-coverage an-notations in a setting based on an aligned corpus, our choices of formal representation and of labelling scheme differ from previous work (Pad  X  o, 2007; Basili et al., 2009). We choose a dependency repre-sentation both for the syntax and semantics because relations are expressed as direct arcs between words. This representation allows cross-lingual transfer to use word-based alignments directly, eschewing the need for complex constituent-alignment algorithms. We choose the semantic annotation scheme defined by Propbank, because it has broad coverage and in-cludes an annotated corpus, contrary to other avail-able resources such as FrameNet (Fillmore et al., 2003) and is the preferred annotation scheme for a joint syntactic-semantic setting (Merlo and van der Plas, 2009). Furthermore, Monachesi et al. (2007) showed that the PropBank annotation scheme can be used for languages other than English directly. Data-driven induction of semantic annotation based on parallel corpora is a well-defined and feasible task, and it has been argued to be particularly suit-able to semantic role label annotation because cross-lingual parallelism improves as one moves to more abstract linguistic levels of representation. While Hwa et al. (2002; 2005) find that direct syntactic de-pendency parallelism between English and Spanish concerns 37% of dependency links, Pad  X  o (2007) re-ports an upper-bound mapping correspondence cal-culated on gold data of 88% F-measure for in-dividual semantic roles, and 69% F-measure for whole scenario-like semantic frames. Recently, Wu and Fung (2009a; 2009b) also show that semantic roles help in statistical machine translation, capi-talising on a study of the correspondence between English and Chinese which indicates that 84% of roles transfer directly, for PropBank-style annota-tions. These results indicate high correspondence across languages at a shallow semantic level.
Based on these results, our transfer of semantic annotations from English sentences to their French translations is based on a very strong mapping hy-pothesis, adapted from the Direct Correspondence Assumption for syntactic dependency trees by Hwa et al. (2005).
 The relationships which we transfer are semantic role dependencies and the properties are predicate senses. We introduce one constraint to the direct se-mantic transfer. Because the semantic annotations in the target language are limited to verbal predicates, we only transfer predicates to words the syntactic parser has tagged as a verb.

As reported by Hwa et al. (2005), the direct cor-respondence assumption is a strong hypothesis that is useful to trigger a projection process, but will not work correctly for several cases.

We used a filter to remove obviously incomplete annotations. We know from the annotation guide-lines used to annotate the French gold sentences that all verbs, except modals and realisations of the verb  X  etre , should receive a predicate label. We define a filter that removes sentences with missing predicate labels based on PoS-information in the French sen-tence. 2.1 Learning joint syntactic-semantic We know from previous work that there is a strong correlation between syntax and semantics (Merlo and van der Plas, 2009), and that this correla-tion has been successfully applied for the unsuper-vised induction of semantic roles (Lang and Lap-ata, 2010). However, previous work in machine translation leads us to believe that transferring the correlations between syntax and semantics across languages would be problematic due to argument-structure divergences (Dorr, 1994). For example, the English verb like and the French verb plaire do not share correlations between syntax and seman-tics. The verb like takes an A0 subject and an A1 direct object, whereas the verb plaire licences an A1 subject and an A0 indirect object.

We therefore transfer semantic roles cross-lingually based only on lexical alignments and add syntactic information after transfer. In Figure 1, we see that cross-lingual transfer takes place at the se-mantic level, a level that is more abstract and known to port relatively well across languages, while the correlations with syntax, that are known to diverge cross-lingually, are learnt on the target language only. We train a joint syntactic-semantic parser on the combination of the two linguistic levels that learns the correlations between these structures in the target language and is able to smooth out errors from automatic transfer. We used two statistical parsers in our transfer of semantic annotations from English to French, one for syntactic parsing and one for joint syntactic-semantic parsing. In addition, we used several cor-pora. 3.1 The statistical parsers For our syntactic-semantic parsing model, we use a freely-available parser (Henderson et al., 2008; Titov et al., 2009). The probabilistic model is a joint generative model of syntactic and semantic depen-dencies that maximises the joint probability of the syntactic and semantic dependencies, while building two separate structures.

For the French syntactic parser, we used the de-pendency parser described in Titov and Hender-son (2007). We train the parser on the dependency version of the French Paris treebank (Candito et al., 2009), achieving 87.2% labelled accuracy on this data set. 3.2 Data To transfer semantic annotation from English to French, we used the Europarl corpus (Koehn, 2003) 1 . We word-align the English sentences to the French sentences automatically using GIZA++ (Och and Ney, 2003) and include only intersective align-ments. Furthermore, because translation shifts are known to pose problems for the automatic projection of semantic roles across languages (Pad  X  o, 2007), we select only those parallel sentences in Europarl that are direct translations from English to French, or vice versa. In the end, we have a word-aligned par-allel corpus of 276-thousand sentence pairs. Syntactic annotation is available for French. The French Treebank (Abeill  X  e et al., 2003) is a treebank of 21,564 sentences annotated with constituency an-notation. We use the automatic dependency conver-sion of the French Treebank into dependency format provided to us by Candito and Crabb  X  e and described in Candito et al. (2009).

The Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels (Meyers, 2007) is used to train the syntactic-semantic parser described in Subsec-tion 3.1 to annotate the English part of the parallel corpus. 3.3 Test sets For testing, we used the hand-annotated data de-scribed in (van der Plas et al., 2010). One-thousand French sentences are extracted randomly from our parallel corpus without any constraints on the se-mantic parallelism of the sentences, unlike much previous work. We randomly split those 1000 sen-tences into test and development set containing 500 sentences each. We evaluate our methods for automatic annotation generation twice: once after the transfer step, and once after joint syntactic-semantic learning. The comparison of these two steps will tell us whether the joint syntactic-semantic parser is able to improve semantic annotations by learning from the syntactic annotations available. We evaluate the models on unrestricted test sets 2 to determine if our methods scale up.

Table 1 shows the results of automatically an-notating French sentences with semantic role an-notation. The first set of columns of results re-ports labelling and identification of predicates and the second set of columns reports labelling and iden-tification of arguments, respectively, for the predi-cates that are identified. The first two rows show the results when applying direct semantic transfer. Rows three and four show results when using the joint syntactic-semantic parser to re-annotate the sentences. For both annotation models we show re-sults when using the filter described in Section 2 and without the filter.
 The most striking result that we can read from Table 1 is that the joint syntactic-semantic learning step results in large improvements, especially for argument labelling, where the F-measure increases from 54% to 65% for the unfiltered data. The parser is able to outperform the quality of the semantic data on which it was trained by using the infor-mation contained in the syntax. This result is in accordance with results reported in Merlo and Van der Plas (2009) and Lang and Lapata (2010), where the authors find a high correlation between syntactic functions and PropBank semantic roles.

Filtering improves the quality of the transferred annotations. However, when training a parser on the annotations we see that filtering only results in better recall scores for predicate labelling. This is not sur-prising given that the filters apply to completeness in predicate labelling specifically. The improvements from joint syntactic-semantic learning for argument labelling are largest for the unfiltered setting, be-cause the parser has access to larger amounts of data. The filter removes 61% of the data.

As an upper bound we take the inter-annotator agreement for manual annotation on a random set of 100 sentences (van der Plas et al., 2010), given in the last row of Table 1. The parser reaches an F-measure on predicate labelling of 55% when us-ing filtered data, which is very close to the up-per bound (59%). The upper bound for argument inter-annotator agreement is an F-measure of 74%. The parser trained on unfiltered data reaches an F-measure of 65%. These results on unrestricted test sets and their comparison to manual annotation show that we are able to scale up cross-lingual se-mantic role annotation. A more detailed analysis of the distribution of im-provements over the types of roles further strength-ens the conclusion that the parser learns the corre-lations between syntax and semantics. It is a well-known fact that there exists a strong correlation be-tween syntactic function and semantic role for the A0 and A1 arguments: A0s are commonly mapped onto subjects and A1s are often realised as direct ob-jects (Lang and Lapata, 2010). It is therefore not surprising that the F-measure on these types of ar-guments increases by 12% and 15%, respectively, after joint-syntactic semantic learning. Since these arguments make up 65% of the roles, this introduces a large improvement. In addition, we find improve-ments of more than 10% on the following adjuncts: gether comprise 9% of the data.

With respect to predicate labelling, comparison of the output after transfer with the output after parsing (on the development set) shows how the parser smooths out transfer errors and how inter-lingual divergences can be solved by making use of the variations we find intra-lingually. An exam-ple is given in Figure 2. The first line shows the predicate-argument structure given by the English syntactic-semantic parser to the English sentence. The second line shows the French translation and the predicate-argument structure as it is transferred cross-lingually following the method described in Section 2. Transfer maps the English predicate la-bel CONTINUE .01 onto the French verb rester , be-cause these two verbs are aligned. The first oc-currence of services is aligned to the first occur-rence of services in the English sentence and gets the A1 label. The second occurrence of services gets no argument label, because there is no align-ment between the C-A1 argument to , the head of the infinitival clause, and the French word services . The third line shows the analysis resulting from the syntactic-semantic parser that has been trained on a corpus of French sentences labelled with automat-ically transferred annotations and syntactic annota-tions. The parser has access to several labelled ex-amples of the predicate-argument structure of rester , which in many other cases is translated with remain and has the same predicate-argument structure as rester . Consequently, the parser re-labels the verb with REMAIN .01 and labels the argument with A3.
Because the languages and annotation framework adopted in previous work are not directly compara-ble to ours, and their methods have been evaluated on restricted test sets, results are not strictly com-parable. But for completeness, recall that our best result for predicate identification is an F-measure of 55% accompanied with an F-measure of 60% for argument labelling. Pad  X  o (2007) reports a 56% F-measure on transferring FrameNet roles, know-ing the predicate, from an automatically parsed and semantically annotated English corpus. Pad  X  o and Pitel (2007), transferring semantic annotation to French, report a best result of 57% F-measure for argument labelling given the predicate. Basili et al. (2009), in an approach based on phrase-based machine translation to transfer FrameNet-like anno-tation from English to Italian, report 42% recall in identifying predicates and an aggregated 73% recall of identifying predicates and roles given these pred-icates. They do not report an unaggregated number that can be compared to our 60% argument labelling. In a recent paper, Annesi and Basili (2010) improve the results from Basili et al. (2009) by 11% using Hidden Markov Models to support the automatic semantic transfer. Johansson and Nugues (2006) trained a FrameNet-based semantic role labeller for Swedish on annotations transferred cross-lingually from English parallel data. They report 55% F-measure for argument labelling given the frame on 150 translated example sentences. In this paper, we have scaled up previous efforts of annotation by using an automatic approach to se-mantic annotation transfer in combination with a joint syntactic-semantic parsing architecture. We propose a direct transfer method that requires nei-ther manual intervention nor a semantic ontology for the target language. This method leads to semanti-cally annotated data of sufficient quality to train a syntactic-semantic parser that further improves the quality of the semantic annotation by joint learning of syntactic-semantic structures on the target lan-guage. The labelled F-measure of the resulting an-notations for predicates is only 4% point lower than the upper bound and the resulting annotations for ar-guments only 9%.
 The research leading to these results has received funding from the EU FP7 programme (FP7/2007-2013) under grant agreement nr 216594 (CLAS-SIC project: www.classic-project.org), and from the Swiss NSF under grant 122643.

