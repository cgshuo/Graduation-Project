 The explosion of web image scale inevitably brings overwhelming number of du-plicate images. Accordin g to a recent study [18], more than 8.1% of web images have more than ten near duplicates and more than 20% of web images have duplicate images. Without proper processing, such duplicates can cause redun-dancies in web search results and reduces user experiences. Therefore, detecting duplicate images from a large image coll ection becomes a hot research topic.
As defined in work[17,9], the duplicate image detection task is to find all visually duplicate image groups from an image collection. According to the def-inition, it is a cluster style task rather than a search style task. The input is a gigantic collection of images and the output is a cleaned version of that image collection(all duplicate ones are grouped into one cluster).

To cope with large scale, efficiency is the core consideration in designing algo-rithms. State-of-the-art approaches usually exploit hash-code representation of images [9,17] and put images with the same hash-code together. Since duplicate images could be represented with different hash codes, such approaches achieve high precision but relatively low recall.
 In this paper, we propose to improve recall while preserving high precision. We design a probing style algorithm to look for more duplicate images. Different from single probing sequence in image search [13], multiple probing sequences exist in duplicate image detection. This raises a new challenge: how to merge multiple probing sequences. In this paper, we formulate this challenge into a rank aggregation problem. Under such formulation, each probing sequence is first represented as a partial ordered list and then aggregated into an unified probing sequence for execution.

In experiments, we compare the Principal Components Analysis (PCA) code and Locality sensitive hashing (LSH) code on millions of images in this task. It could reflect that PCA-code is more suit able in this task. Then we demonstrate the proposed approach with PCA-code. In developing the rank aggregation on probing sequences, we introduce three different distance measurements and com-pare them with the baseline. All these th ree measurements are proved to increase the recall while preser ving high precision.
 The rest of the paper is organized as follo ws. Section 2 discusses related work; Section 3 introduces the framework of the algorithm. Section 4 generates the PCA code; Section 5 proposes the aggregation-based probe algorithm. Section 6 provides the experimental results and section 7 concludes this paper. 2.1 Duplicate Image Detection Plenty of works [14,19,9,17,22] are devoted to duplicate image detection. Accord-ing to different feature representations o f an image, these works can be classified into two categories.  X  Local Method: Typical works[3,9,22,16] are bases on SIFT[12]. The min-hash  X  Global Method: [10,15,17] are based on global features. Typical global fea-2.2 Rank Aggregation A lot of relevant works on rank aggregation methods has been published [4,8,5,2,21,11]. In general, there are two main types: score-based aggregation and rank-based aggregation. Recent work f ocuses on rank-based methods [4,5], which means the order of the list is signific ant but the actual score of each element in the list is not. There are some common metrics for rank-based aggregation such as Canberra distance[7], Spearman footrule distance [4] and Kendall X  X  tau distance [4].

In our task, the probing score to a given code represents the probability of the existence of duplicate images corresponding to it. Thus we use a score-based rank aggregation method to handle the problem. The existing score-based methods are often based on supervised learning [21,2]. Due to the large volume of web images, it X  X  very expensive to acquire manual labeling. Therefore we adopt an unsupervised approach. Different from Klementiev[8] , which measures the agreement between the rankers, we design s ome distance measurements naturally from the existing rank-based measu rement and directly optimize them. In this section, we introduce the framework for the duplicate image detection task, which contains two steps:
At first, all images in the entire data set will be represented by gray value and indexed according to their PCA-code; Then we use the PCA codes to divide the image set into smaller groups, the images in each group share the same PCA code. We refer to each group as a bucket in the rest of this paper. In the generation of PCA-code, similar images may vary a lot in their PCA codes. The slight shift of the features may cause totally different code, thus put into different buckets. To improve the performance, the P CA-codes representing similar image features should be identified and related. The traditional similar metric is the hamming distance, which means the PCA-codes with small hamming distance will be grouped together. But it suffers a low efficiency to probe all images in the dataset.
 To increase the efficiency, for each bucket, we generate a probing sequence. The probing sequence can be regarded as similarity measurement of the buckets. The later element in the sequence means the less probability that contains the duplicate image. If we can generate the p robing sequence, it could effectively and efficiently find the duplicate image groups.

Recently, a multi-probe method [13] is proposed to generate the probing se-quence for LSH. Since the Gaussian distribution assumption of PCA is equal to that of LSH in [13], we can apply [13] to the PCA-code. But [13] method relays on the query as the input parameter, however in duplicate image detection, there are more than one images in each bucket, therefore, we cannot apply [13] directly. We develop a two-step algorithm to handle the problem. First, for each image in the bucket, we generate the basic probing sequence. Second, we apply the rank aggregation algorithm to generate the merged probing sequence. Because the length of the candidate list is limited, it is a partial list rank aggregation.
Since we target to cope with large scale web images, it X  X  hard and expensive to acquire the labeled data, thus it is hard to use supervised rank aggregation method. We turn to an unsupervised score-based method which is displayed in Figure 1. In this section, we introduce feature ex traction and PCA hash-code generation. For feature extraction, an image is divided into a k  X  k grid, where k is set to 8 from previous work [17]. The mean gray value feature is extracted from each grid as a robust image representation for duplicate detection. It can tolerate most image transformations such as resizing, recoloring, compression, and a slight amount of cropping and rotation. Each feature element is calculated by the Equation (1) where gray ( i ) j is the j  X  th gray value in grid i and N i is the number of pixels in grid i . So an image could be represented by a k 2  X  1 vector.

Before code generation, the raw features should be compressed into a compact representation with noises reduced. The PCA hash-code algorithm uses a PCA-based hash function to map a high-dimensional feature space to a low-dimensional feature space. The PCA model is used to compress the features extracted from the image database. After the dimension reduction, visual features are transformed to a low-dimensional feature space. Then the hash code H for an image is built as follows: each dimension is transformed to one if its value in this dimension is greater than mean value, a nd zero otherwise. This is s ummarized in Equation (2). where the mean i is the mean value of the dimension i . The flowchart of PCA-code component is shown in Figure 2.
 The input of our aggregation-based probe algorithm is a set of images in hash code and feature representation. The output is duplicate image groups(code groups). The algorithm traverses all the buckets. For a given bucket, it generates a merged probing sequence of buckets and then gathers them with the given bucket into a group. Then these buckets will not be probed again.
 The merged probing sequence is generated in two steps.
 Step 1: Basic Probing Sequence Generation. In this step, a probing se-quence of buckets for each image in the s tarting bucket is generated. The top-K elements are selected into the sequence based on a score function. The score func-tion measures the probability of containing duplicate images. A smaller score means larger probability of finding a duplicate image.
 Step 2: Merged Probing Sequence Generation. In this step, the different basic probing sequences generated from the starting bucket are merged. The merging is formulated as a partial rank aggregation problem as shown in Table 1. For elements in basic probing sequence, it is assigned with a probability value. We treat probability values as scores in our solution. Additionally, given that there is no labeled data available, we use an unsupervised approach in solving the aggregation problem. 5.1 Step 1: Basic Probing Sequence Generation At the beginning, each image in collect ion are encoded by PCA-code. And all of them will generate a probing sequence by a score function. From [13], the score function defined for one image in starting bucket h q to bucket h j is as in Equation (3) where f is the compressed feature value of the query image and  X  is the set of bits which are different between buckets h q and h j .

Then we apply this equation to PCA-code. Since we will compare the com-pressed feature values between different dimensions in PCA-code, we normalize them by X  X  X  X   X   X  . Considering the assumption that each dimension obeys the Gaussian distribution, the normalized value conforms to a standard Gaussian distribution. Then the score function of PCA is as in Equation (4) where the  X  k is the mean value of dimension k and  X  k means the standard deviation of dimension k . It is used to estimate the probability that images in h j duplicate the query image. A lower score means that h j is more likely to contain duplicate images. For each image in the starting bucket, we get a basic probing sequence based on score PCA . 5.2 Step 2: Merged Probing Sequence Generation Before modeling the score-based rank aggregation problem, we do some pre-processing to turn the partial ordered list into a full ordered list. We combine all the elements in partial ordered lists, to get a set T  X  . Then we calculate scores for all the elements in T  X  for each image in the starting bucket. It becomes a full ordered list problem.

Now we define some symbols to give a mathematical formulation for the score-based rank aggregation problem. We assume that the V i represents the i  X  th full ordered list and V  X  is the result list. Finally, we cast the score-based rank aggregation problem into the optimization problem shown in Equation (5). The dis ( V i ,V  X  ) measures the distance between V i and V  X  .

In the formulation, different distance measurement dis ( V i ,V  X  ) leads to dif-ferent result list V  X  . The following three measurements are designed in this problem. 1. Footrule distance 2. Canberra Distance 3. Discounted Canberra Distance Optimization Method. In this section, we show the details of optimization method for different distance measurements in the score-based rank aggregation. 1. In footrule distance , the optimal score of each element in T  X  is the median value of the scores given by the rankers.
 Where N is the number of the rankers. 2. The problem of Canberra Distance canbesolvedbyageneralgradient descent algorithm. From Equation (8) we know that the elements in T  X  are separated from each other. So we can calculate the optimal value of each element independently. The formulation for the gradient descent step is listed in the following equation.
 Where I ( x ) is the Indicator function and  X  is the learning rate(which is usually setting as 0.01). The smallest k elements in V  X  are the result list. 3. Because the rank adjustment factor of each element in T  X  has relativity, the problem of Discounted Canberra Distance cannot be handled by the previous al-gorithm. We add a restriction that the V  X  = i W i  X  V i . Then we follow the idea from the existing lambdarank optimization algorithm[1] (which approximately optimizes NDCG as its object) and this problem can be optimized directly by a approximate gradient descent algorithm. The gradient process is listed in the following equation.
 where  X  is the learning rate (which is usually setting to 0.01) and I ( x )isthe Indicator function. The Pos V  X  j will be updated after each iteration process. Then we use the W to calculate the V  X  and the smallest k elements in V  X  are the result list. 6.1 Experimental Setting The dataset contains 1,003,440 images crawled from the web. It is split into two parts, the ground truth and the distractor set. The ground truth consists 239 groups of 1260 images. It is labeled manually. The distractor set is selected randomly from the web with images in the ground truth removed. The size of the distractor set is 1,002,180. All the images are resized to 160 pixels on their larger side. We use 100,000 images to train a stable PCA for dimension reduction. 6.2 Evaluation Measurements To measure the performance of the methods, we use the same metrics as [17]: The DetectedImageP air denotes image pairs in a detected group. The CorrectImageP airs denotes the detected image pairs belong to the same group of the ground truth. The GroundT ruthImageP airs represents the image pairs in the same group of the ground truth. 6.3 Comparison of PCA-Code and LSH In this experiment, the PCA-code is compared to LSH on duplicate image de-tection tasks. To the best of our knowledge, the result of such comparison has not been shown before. The length of P CA-code and LSH are set as 40. Figure 3 shows the P-R curves of the PCA-code and LSH. The buckets in both codes are grouped by hamming distance. It can be seen that the curve of the LSH is under the curve of the PCA-code. It means that with the same precision, we can get a higher recall performance. So PC A-code is more effective than LSH. From another perspective, the curve of LSH declines rapidly. On the contrary, the curve of PCA declines slowly. It means that the buckets divided by PCA-code containing duplicate images are close to each other. And the buckets containing un-duplicate image are not close.

The reason is analyzed as follows. PCA model could be seen as a particular list of projections of the original feature space. Therefore it will perform better than LSH, whose projectio n is randomly generated.
 6.4 Comparison of Different Aggregation-Based Probe Method In this experiment, we show the improvement of our aggregation-based probe method on three distances measurements. Then we evaluate the algorithm X  X  stability on the partial ordered list length. The starting bucket is fixed for all experiments.

In evaluating the improvement of our aggregation-based probe method, we use two baselines. The first baseline is the PCA-code, which means the codes are di-vided by Hamming distance. Comparisonwith this baseline show the improvement of probing methods over non-probing ones. The second baseline is MC 4[4],astate-of-the-art rank-based rank aggregation method, which is the best method in [4]. Comparison with this baseline shows the improvement of our score-based rank ag-gregation methods to the rank-based rank aggregation methods.

Table 2 shows the performance of our aggregation-based probe method when the length of partial ordered list is fixed in probing. In this experiment, the length of result list is equal to the length of the partial ordered list.
In the table, all the rank aggregation methods improve the recall and main-tain the precision. The FDst and MC 4 methods show similar performance and the CaDst is slightly better than either of those on precision performance.The improvement of DCaDst on recall performance is higher than all the others.
When we fix the number of buckets at 40, Recall/P recision of Baseline is only 0 . 556 / 0 . 998 but DCaDis reaches 0 . 813 / 0 . 994, improving the recall by 40%. The other methods are around 0 . 775 / 0 . 993, which is better than the baseline but lower than DCaDst .When we fix the recall score at 0.813, Baseline probes 780 buckets and DCaDst probes only 40 buckets. DCaDst probes far fewer buckets to reach the same recall. The other methods probe more than 50 buckets. This good performance of DCaDst can be explained by its design principle: it focuses on the top-K items of the result list and optimizes them. This design is helpful for optimizing the top-k items of the resul t list. When the length of result list is shorter, the performance is more better than the others. So the DCaDst could probe less buckets than the other methods to reach the same performance.
We conclude that DCaDst is better than the other methods for this task. It effectively reduces the number of the bucke ts to be probed and increase the recall while preserving the precision performance. It also shows that the score function in Equation 3 is a reasonable inference, which could reflect the probability of the bucket containing the duplicate images.

Table 3 shows the stability of our aggregation-based probe algorithm on three distances when the partial list length is changed.

From Table 3, we can see that the recall performance is stable as the result list length fixed, which indicated that our proposed methods are robust as the length of partial ordered list varies. It is a exciting property in our task. So that we can set the length of the partial ordered list as length of the result list while maintaining the performance. It could reduce the computation and easy to apply to the next application.

To sum up, the experiments show that our aggregation-based probe algo-rithm can effectively increase the recall performance while maintaining the high precision with high robustness. In this paper, we propose a probing based method to detect duplicate images from a large image collection. Compared to previous works, the proposed approach re-serves the advantages in high precision and at the same time improves the rela-tively low recall. To increase the recall, th e proposed method generate the probing sequence to find the duplicate images more intelligently. Different from state-of-art probing methods for image search, multiple probing sequences exist in duplicate image detection. To unify multiple probing sequences, we present an unsupervised score-based rank aggregation algorithm. As shown in experimental results on mil-lions of images, relatively low recall is increased and high precision is preserved.
In future work, we plan to construct a larger dataset and explore more im-age representations to analyze the content of image. Also we will discuss the convergence of the gradient descent and more rank aggregation methods.
