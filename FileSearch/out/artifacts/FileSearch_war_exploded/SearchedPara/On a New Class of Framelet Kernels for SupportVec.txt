 perturbed by noise, evaluated at a fini te set of points. Kernel methods, such as support vector machines (SVMs) [4,15,16], regularization networks (RN) [1], pacity in handling nonlinear relations and learning form sparse data. Kernel function plays an important role in such methods. However, there are a number functions and selection of kernel parameters [3,5].

Recently, some researchers respectivel y proposed classes of frame-based ker-tions in the framework of SVM regression (SVR) and RN [8,10,11,17]. The re-dundant property of frames make them well-suited to deal with noisy samples in a robust way. These kind of kernels are good at approximating multiscale func-which is always difficult to compute for a given frame. The effect of choosing based tight wavelet frame, its dual frame is itself. They combine the power of MRA and the flexibility of redundant representations. We will use them in SVR and RN for function approximation. The usefulness of framelet-based kernels presenting some basic concepts of SVR and RN in section 2. Then we introduce in section 5, and section 6 concludes the paper. onto labels y . When new input x is presented the target output y is predicted kernel-based techniques for solving regression problems of learning from exam-tion (SRM) framework [7,13]. Next we b riefly review the concepts of SVR and RN. 2.1 SVM for Regression from the noisy observations where the additive measurement errors  X  i are uncorrelated zero-mean Gaussian SV algorithm for regression computes a linear function in the high dimensional feature space F . Thereby this algorithm can compute a nonlinear function by minimizing the following functional: where f 2 K is a norm in a Reproducing Kernel Hilbert Space (RKHS) H K around the regression function within which errors are not penalized. 2.2 Regularization Networks Regularization theory is a classical way to solve the ill-posed problem of ap-formulates the regression problem as the following variational problem where the second term is a penalty functional called stabilizer [9]. tem but with the same fundamental structure as wavelet system. It has both the of framelets and the approximation properties of the system.
  X   X  i =(  X  i  X   X  )(  X  0 called refinable mask such that  X   X  =(  X  0  X   X  )( 2 jd/ 2  X  (2 j  X  X  k ) and the dyadic wavelet system The following is the fundamental tool to construct framelets: function  X  is defined as Proposition 1 (The Oblique Extension Principle (OEP) [6]). Suppose continuous at the origin with  X  (0) = 1 . And for every  X   X  X  X   X ,  X  } d and  X   X  { X   X ,  X  } d then the wavelet system X (  X  ) defined by  X  is a tight frame. For  X   X  1, proposition 1 reduces to the Unitary Extension Principle .
The approximation order of the framelet system is proved to be strongly connected to the number of vanishing mo ment of the mother wavelet system [6]. Let X (  X  ) be an framelet system, then Proposition 2. Assume that the system has vanishing moments of order m 1 , and the MRA provides approximation order m 0 . Then, the approximation order of X (  X  ) is min { m 0 , 2 m 1 } . Thechoiceofthekernel K ( x , y ) determines the function space in which the norm f 2 K in equation (1) and (2) is defined. It also determines the form of data points under a nonlinear map  X  where  X  defines the feature space. In practice, the kernel K can be defined directly without explicitly defining the map  X  .Itisthispropertythatmakesthe kernel methods so attractive.
 as Proof: We prove that framelet kernel (3) is admissible reproducing kernel. of the form for  X  i,j,k = f,  X  i,j,k  X  R , and define the scale product in our space to be given by K ( x , y ). In fact, we have The framelet  X  i,j,k is a frame for the RKHS H K which is called the feature space induced by the kernel K . Hence, the framelet-based kernel K and the corresponding space H K can be used within the framework of SVR and RN.
Most constructions of framelets based on a spline MRA structure whose re-used in the experiments in next section.
 A. Linear Spline Framelet Kernel K 1 with vanishing moments 1 and the approximation order 2. We use {  X  1 , X  2 } to construct framelet kernel K 1 B. Cubic Spline Framelet Kernel K 2
Let  X  be the B-spline function of order 4 supported on [0 , 4], which is a Let construct our framelet kernel function K 2 C. Cubic Spline Framelet Kernel K 3 The third one is also a cubic spline framelet kernel. It is based on the same MRA structure as in K 2 but has different vanishing moments and approximation order. We take where t = 317784 / 7775 + 56 11113747578360  X  245493856965 t/ 62697600 ,t 2 = t = struct a framelet kernel function K 3
The sum of infinite terms can be truncated into a sum of finite terms in corresponding to the shift index k are summed because the framelet elements K 1 , K 2 and K 3 with j min = (c) respectively, (d) is the Gaussian kernel.
 lems using SVM regression and regularization networks methods. We illustrate classical Gaussian kernel in the simulated regression experiments I and II.

For both SVR and RN, some hyperparameters have to be tuned. The perfor-mance of SVR in (1) depends on the hyperparameters such as  X  , regularization factor  X  . The performance of RN in (2) depends on the choice of regulariza-selection problem [3,5]. The idea is to find the parameters that minimize the not been used for learning (hold-out testing or cross-validation techniques).
In our experiments, the hyperparameters were optimized from a range of finely sampled values, where the generalization error was estimated by the 10-errors.  X 10  X 5 0 5 10  X 2 0 2 4 6 5.1 Experiment I In this experiment, the sample set { ( x i ,y i ) } come from the function the true function.

The performance of the framelet kernel strongly depends on the value of the error for the two learning machines decreases monotonously as j min reduced. lower bound parameter j min to be  X  5. The scale upper bound parameter j max two learning machines and the different kernels using the optimal hyperparame-j kernel. For RN the optimal parameters are j min =  X  5, j max =1,  X  =0 . 4for K and  X  =0 . 35,  X  =0 . 1 for Gaussian kernel. Figure 3 shows the SVR results of lines represent the approximation functions.

As can be seen, the framelet kernels achieve good performance in SVR. The best performance in SVR is given by the framelet kernel K 3 which has the highest approximation order. In RN, the framelet kernels do not give better performance compared to Gaussian kernel. The best result in RN is achieved by Gaussian kernel and it is better than the best one in SVR. This may be because the sample set came from a smoothly changed function and large sample size give enough information for approximation, and the SVM algorithm loses its superiority. 5.2 Experiment II the following function which contains multiple scales as shown in Figure 4. The tips of the cones at from each other of the width and position. A data set of 150 is generated by corrupted by the zero-mean Gaussian noise with variance 0 . 2. Figure 4 shows the sample set and the true function.

Table 2 shows the generalization error for the two learning machines and the different kernels using the optimal hyperparameter setting. For SVR the  X  5, j max =0,  X  =0 . 2,  X  =0 . 06 for K 2 ; j min =  X  5, j max =0,  X  =0 . 6, the optimal parameters are j min =  X  5, j max =0,  X  =0 . 1for K 1 ; j min =  X  5, j max =0,  X  =0 . 1for K 2 ; j min =  X  5, j max =0,  X  =0 . 1for K 3 ;and  X  =0 . 3,  X  =0 . 1 for Gaussian kernel. Figure 5 shows the RN results of the optimal parameters. The slender lines depict the true function and the bold lines represent the approximation functions.

The neighborhood of the singular points is the most difficult part to approxi-Gaussian kernel in that place. In both SVR and RN, Gaussian kernel achieved the poorest performance and framelet kernl K 1 is the best. This suggests that for the learning methods of SVM regression and regularization networks. This resentation from the framelet system. They are good at approximating functions extensions to two dimensions will be considered in our future work. 60575004), the Ministry of Education of China(NCET-04-0791), NSF of Guang-Dong (05101817) and the Hong Kong Research Grant Council(project CityU 122506).

