
Many applications of collaborative filtering (CF), such as news item recommendation and bookmark rec-ommendation, are most naturally thought of as one-class collaborative filtering (OCCF) problems. In these problems, the training data usually consist simply of bi-nary data reflecting a user X  X  action or inaction, such as page visitation in the case of news item recommenda-tion or webpage bookmarking in the bookmarking sce-nario. Usually this kind of data are extremely sparse (a small fraction are positive examples), therefore am-biguity arises in the interpretation of the non-positive examples. Negative examples and unlabeled positive ex-amples are mixed together and we are typically unable to distinguish them. For example, we cannot really at-tribute a user not bookmarking a page to a lack of inter-est or lack of awareness of the page. Previous research addressing this one-class problem only considered it as a classification task. In this paper, we consider the one-class problem under the CF setting. We propose two frameworks to tackle OCCF. One is based on weighted low rank approximation; the other is based on negative example sampling. The experimental results show that our approaches significantly outperform the baselines.
Personalized services are becoming increasingly in-dispensable on the Web, ranging from providing search results to product recommendation. Examples of such systems include recommending products at Ama-zon.com 1 , DVDs at Netflix 2 , News by Google 3 etc. The central technique used in these systems is collabo-rative filtering (CF) which aims at predicting the pref-erence of items for a particular user based on the items previously rated by all users. The rating expressed in different scores (such as a 1-5 scale in Netflix) can be ex-plicitly given by users in many of these systems. How-ever, in many more situations, it also can be implicitly expressed by users X  behaviors such as click or not-click and bookmark or not-bookmark. These forms of im-plicit ratings are more common and easier to obtain.
Although the advantages are clear, a drawback of implicit rating, especially in situations of data spar-sity, is that it is hard to identify representative negative examples. All of the negative examples and missing positive examples are mixed together and cannot be distinguished. We refer to collaborative filtering with only positive examples given as One-Class Collabora-tive Filtering (OCCF). OCCF occurs in different sce-narios with two examples as follows.  X  Social Bookmarks : Social bookmarks are very  X  Clickthrough History : Clickthrough data are
There are several intuitive strategies to attack this problem. One approach is to label negative examples to convert the data into a classical CF problem. But this is very expensive or even intractable because the users generating the preference data will not bear the burden. In fact, users rarely supply the ratings needed by traditional learning algorithms, specifically not neg-ative examples [23]. Moreover, based on some user studies [14], if a customer is asked to provide many positive and negative examples before the system per-forms well, she would get a bad impression of it, and may decline to use the system. Another common solu-tion is to treat all the missing data as negative exam-ples. Empirically, this solution works well (see Section 4.6). The drawback is that it biases the recommen-dation results because some of the missing data might be positive. On the other hand, if we treat missing as unknown, that is, ignore all the missing examples and utilize the positive examples only and then feed it into CF algorithms that only model non-missing data (as in [24]), a trivial solution arising from this approach is that all the predictions on missing values are posi-tive examples. All missing as negative (AMAN) and all missing as unknown (AMAU) are therefore two ex-treme strategies in OCCF.

In this paper, we consider how to balance the ex-tent of treating missing values as negative examples. We propose two possible approaches to OCCF. These methods allow us to tune the tradeoff in the interpre-tation of so-called negative examples and actually re-sult in better performing CF algorithms overall. The first approach is based on weighted low rank approx-imation [24]. The second is based on negative exam-ple sampling. They both utilize the information con-tained in unknown data and correct the bias of treat-ing them as negative examples. While the weighting-based approach solves the problem deterministically, the sampling-based method approximates the exact so-lution with much lower computational costs for large scale sparse datasets.

Our contributions are summarized as follows. First we propose two possible frameworks for the one-class collaborative filtering problem and provide and char-acterize their implementations; second, we empirically study various weighting and sampling approaches using several real world datasets. Our proposed solutions significantly outperform the two extremes (AMAN and AMAU) in OCCF problems, with at least 8% improve-ment over the best baseline approaches in our experi-ments. In addition, we show empirically that these two proposed solution frameworks (weighting and sampling based) for OCCF have almost identical performance.
The rest of the paper is organized as follows. In the next section, we review previous works related to the OCCF problems. In Section 3, we propose two ap-proaches for OCCF problems. In Section 4, we empir-ically compare our methods to some baselines on two real world data sets. Finally, we conclude the paper and give some future works.
In the past, many researchers have explored collabo-rative filtering (CF) from different aspects ranging from improving the performance of algorithms to incorporat-ing more resources from heterogeneous data sources [1]. However, previous research on collaborative filtering still assumes that we have positive (high rating) as well as negative (low rating) examples. In the non-binary case, items are rated using scoring schemes. Most pre-vious work focuses on this problem setting. In all the CF problems, there are a lot of examples whose rat-ing is missing. In [2] and [19], the authors discuss the issue of modeling the distribution of missing values in collaborative filtering problems. Both of them cannot handle the case where negative examples are absent.
In the binary case, each example is either positive or negative. Das et al. [8] studied news recommendation, while a click on a news story is a positive example, and a non-click indicates a negative example. The authors compare some practical methods on this large scale bi-nary CF problem. KDD Cup 2007 hosted a X  X ho rated What X  recommendation task while the training data are the same as the Netflix prize dataset (with rating). The winner team [15] proposed a hybrid method com-bining both SVD and popularity using binary training data.
Algorithms for learning from positive-only data have been proposed for binary classification problems. Some research addresses problems where only examples of the positive class are available [22] (refer to one-class classification) where others also utilize unlabeled exam-ples [16]. For one-class SVMs [22], the model is describ-ing the single class and is learned only from positive examples. This approach is similar to density estima-tion [4]. When unlabeled data are available, a strat-egy to solve one-class classification problems is to use EM-like algorithms to iteratively predict the negative examples and learn the classifier [28, 17, 26]. In [9], Denis show that function classes learnable under the statistical query model are also learnable from positive and unlabeled examples if each positive example is left unlabeled with a constant probability.

The difference between our research and previous studies on learning from one-class data is that they aim at learning one single concept with positive examples. In this paper, we are exploring collaboratively learning many concepts in a social network.
Our work is also related to the class imbalance prob-lem which typically occurs in classification tasks with more instances of some classes than others. The one-class problem can be regarded as one extreme case of a class imbalance problem. Two strategies are used for solving the class imbalance problem. One is at the data level. The idea is to use sampling to re-balance the data [3] [18]. Another one is at the algorithmic level where cost-sensitive learning is used [10] [27]. A comparison of the two strategies can be found in [20].
As discussed above, AMAN and AMAU (no missing as negative) are two general strategies for collaborative filtering, which can be considered to be two extremes. We will argue that there can be some methods in be-tween that can outperform the two strategies in OCCF problems; examples include  X  X ll missing as weak nega-tive X  or  X  X ome missing as negative X . In this section, we introduce two different approaches to address the issue of one-class collaborative filtering. They both balance between the strategies of missing as negative and miss-ing as unknown. The first method uses weighted low rank approximations [24]. The idea is to give differ-ent weights to the error terms of positive examples and negative examples in the objective function; the sec-ond one is to sample some missing values as negative examples based on some sampling strategies. We first formulate the problem and introduce the main notation in this paper. In the next two subsections, we discuss the two solutions in details.
Suppose we have m users and n items and the pre-vious viewing information stored in a matrix R . The element of R takes value 1, which represents a positive example, or  X ? X , which indicates an unknown (missing) positive or negative example. Our task is to identify po-tential positive examples from the missing data based on R . We refer to it as One-Class Collaborative Fil-tering (OCCF). Note that, in this paper, we assume that we have no additional information about users and items besides R . In this paper, we use bold capi-tal letters to denote a matrix. Given a matrix A , A ij represents its element, A i. indicates the i -th row of A , A .j symbolizes the j -th column of A , and A T stands for the transpose of A .
Our first approach to tackle the one-class collabora-tive filtering problem is based on a weighted low-rank approximation [11, 24] technique. In [24], weighted low-rank approximations (wLRA) is applied to a CF problem with a naive weighting scheme assigning X 1 X  X o observed examples and X 0 X  X o missing (unobserved) val-ues, which corresponds to the AMAU. Another naive method for OCCF is to treat all missing values as neg-ative examples. However, because there are positive examples in missing values, this treatment can make mistakes. We address this issue by using low weights on the error terms. Next, we propose the weighted al-ternating least squares (wALS) for OCCF. We further discuss various weighting schemes different from naive schemes AMAU ([24]) and AMAN.

Given a matrix R = ( R ij ) m  X  n  X  X  0 , 1 } m  X  n with m users and n items and a corresponding non-negative weight matrix W = ( W ij ) m  X  n  X  R m  X  n + , weighted low-rank approximation aims at approximating R with a low rank matrix X = ( X ij ) m  X  n minimizing the objec-tive of a weighted Frobenius loss function as follows. In the above objective function L ( X ) (Eq. (1)), ( R seen in low-rank approximations, and W ij reflects the contribution of minimizing the term to the overall ob-jective L ( X ). In OCCF, we set R ij = 1 for positive examples; for missing values, we posit that most of them are negative examples. We replace all the miss-ing values with zeros. As we have high confidence on the observed positive examples where R ij = 1, we set the corresponding weights W ij to 1. In contrast, differ-ent from the simple treatment of missing as negative, we lower the weights on X  X egative X  X xamples. Generally we set W ij  X  [0 , 1] where R ij = 0. Before discussing the weighting schemes for X  X egative X  X xamples, we show how to solve the optimization problem argmin X L ( X ) effectively and efficiently.
 Consider the decomposition X = UV T where U  X  R of features d  X  r where r  X  min ( m, n ) is the rank of the matrix R . Then we can re-write the objective function (Eq. (1)) 3 as Eq. (2) To prevent overfitting, one can append a regularization term to the objective function L (Eq. (2)): or In Eq. (3) and Eq. (4), k . k F denotes the Frobenius norm and  X  is the regularization parameter which, in practical problems, is determined with cross-validation. Note that Eq. (4) subsumes the special case of regu-larized low-rank approximation in [21, 30]. Zhou et al. [30] show that the alternating least squares (ALS) [11] approach is efficient for solving these low rank ap-proximation problems. In this paper, we extend this approach to weighted ALS (wALS). Now we focus on minimizing the objective function L (Eq. (4)) to illus-trate how wALS works.

Taking partial derivatives of L with respect to each entry of U and V , we obtain Then we have Algorithm 1 Weighted Alternating Least Squares (wALS) Require: data matrix R , weight matrix W , rank d Ensure: Matrices U and V with ranks of d
Initialize V repeat until convergence. return U and V ments of W i. on the diagonal, and I is a d  X  d identity matrix.
 U Notice that the matrix V T g W i. V +  X  strictly positive definite, thus invertible. It is not dif-ficult to prove that without regularization, V T g W i. V can be a degenerate matrix which is not invertible. Similarly, given a fixed U , we can solve V as follows. V elements of W .j on the diagonal.

Based on Eq. (6) and Eq. (7), we propose the follow-ing iterative algorithm for wLRA with regularization (based on Eq. (4)). We first initialize the matrix V with Gaussian random numbers with zero mean and small standard deviation (we use 0 . 01 in our experi-ments). Next, we update the matrix U as per Eq. (6) and then update the matrix V based on Eq. (7). We repeat these iterative update procedures until conver-gence. We summarize the above process in Algorithm 1 which we refer to as Weighted Alternating Least Squares (wALS). Note that for the objective function Eq. (3) with a uniform regularization term, we only need to change both in Eq. (7) to 1. 3.2.1 Weighting Schemes: Uniform, User Ori-As we discussed above, the matrix W is crucial to the performance of OCCF. W = 1 is equivalent to the case of AMAN with the bias discussed above. The basic idea User-Oriented W ij = 1 W ij  X  Item-Oriented W ij = 1 W ij  X  m  X  of correcting the bias is to let W ij involve the credibility of the training data ( R ) that we use to build a collab-orative filtering model.d For positive examples, they have relative high likeliness to be true. We let W ij = 1 for each pair of ( i, j ) that R ij = 1. For missing data, it is very likely that most of them are negative examples. For instance, in social bookmarking, a user has very few web pages and tags; for news recommendation, a user does not read most of the news. That is why pre-vious studies make the AMAN assumption although it biases the recommendations. However, we notice that the confidence of missing values being negative is not as high as of non-missing values being positive. There-fore, essentially, we should give lower weights to the  X  X egative X  examples. The first weighting scheme as-sumes that a missing data being a negative example has an equal chance over all users or all items, that is, it uniformly assign a weight  X   X  [0 , 1] for  X  X egative X  ex-amples. The second weighting scheme posits that if a user has more positive examples, it is more likely that she does not like the other items, that is, the missing data for this user is negative with higher probability. The third weighting scheme assumes that if an item has fewer positive examples, the missing data for this item is negative with higher probability. We summarize these three schemes in Table 1. A parameter for the three schemes is the ratio of the sum of the positive example weights to the sum of the negative example weights. We will discuss the impact of the parameter in Section 4.6. In the future, we plan to explore more weighting schemes and learn the weight matrix W it-eratively.
As we state above, for one-class CF, a naive strategy is to assume all missing values to be negative. This implicit assumption of most of the missing values being negative is roughly held in most cases. However, the main drawback here is that the computational costs are very high when the size of the rating matrix R is large. wALS has the same issue. We will analyze its computational complexity in the next subsection. Another critical issue with the naive strategy is the imbalanced-class problem discussed in Section 2.3. In this subsection, we present a stochastic method
Figure 1. A diagram that illustrates an ensem-ble based on negative exampling sampling for OCCF based on negative example sampling for OCCF as shown in Figure 1. In phase I, we sample negative examples from missing values. Based on an assumed probability distribution, we generate a new matrix e R ( i ) including all positive examples in R . In phase II, for each e R ( i ) , we re-construct the rating matrix b R ( i ) special version of wALS which we discuss in Section 3.2. Finally, we combine all the b R ( i ) with equal weights gen-erating a matrix b R which approximates R . We refer to this method as sampling ALS Ensemble (sALS-ENS). 3.3.1 Sampling Scheme Since there are too many negative examples (compared to positive ones), it is costly and not necessary to learn the model on all entries of R . The idea of sampling can help us to solve the OCCF problem. We use a fast ( O ( q )) random sampling algorithm [25] to generate new training data b R from the original training data R by negative example sampling given a sampling proba-bility matrix b P and negative sample size q . As OCCF is a class-imbalanced problem, where positive examples are very sparse, we transfer all positive examples to the new training set. We then sample negative examples from missing data based on b P and the negative sample size q .

In this algorithm, b P is an important input. In this paper, we provide three solutions which correspond to the following sampling schemes: 1. Uniform Random Sampling: b P ij  X  1. All the miss-2. User-Oriented Sampling: b P ij  X  3. Item-Oriented Sampling: b P ( i, j )  X  1 / 3.3.2 Bagging After generating a new training matrix by the above algorithm, we can use a low-rank matrix b R to approx-imate e R using wALS. Because e R is stochastic, b R can also be biased and unstable. A practical solution to the problem is to construct an ensemble. In particular, we use the bagging technique [6] ( Algorithm 2 ). Algorithm 2 Bagging Algorithm for OCCF ple size q , number of single predictor  X  Ensure: Reconstructed matrix b R for i = 1 :  X  do end for b
R = return b R
Next, we analyze the running time of wALS and sALS-ENS. Recall that U is a m  X  d matrix, V is a n  X  d matrix, and d  X  min { m, n } . For wALS, each step of updating U (or M ) takes time O d 2 nm (based on Eqs. 6 and 7). The total running time of wALS is assuming that it takes n t rounds to stop.

For sALS-ENS similarly, assuming that ALS takes on-average n t rounds to stop, and the number of NES predictors is  X  , then its total running time is
In practice, n t ,  X ,  X  are small constants ( n t ranges from 20 to 30,  X   X  20, and  X   X  5), and ( n + m ) d  X  n r (1 +  X  ). Therefore the running time of wALS is O ( mn ), while the running time of sALS-ENS is O ( n r ). Thus sALS-ENS is more scalable to large-scale sparse data compared to wALS. To be pre-cise, the running time ratio of wALS to sALS-ENS mn  X  1 , then ALS-ENS takes less time to finish than wALS; otherwise, wALS is faster.
We use two test datasets to compare our proposed algorithms with possible baselines. The first dataset, the Yahoo news dataset, is a news click through record stream 4 . Each record is a user-news pair which con-sists of the user id and the URL of the Yahoo news article. After preprocessing to make sure that the same news always gets the same article id, we have 3158 unique users and 1536 identical news stories. The second dataset is from a social bookmarking site. It is crawled from http://del.icio.us. The data contains 246 , 436 posts with 3000 users and 2000 tags.
As a most frequently used methodology in machine learning and data mining, we use cross-validation to estimate the performance of different algorithms. The validation datasets are randomly divided into training and test sets with a 80/20 splitting ratio. The train-ing set contains 80% known positive examples and the other elements of the matrix are treated as unknown. The test set includes the other 20% known positive and all unknown examples. Note that the known positives in the training set are excluded in the test process. The intuition of good performance of a method is that the method has high probabilities to rank known posi-tives over unknown examples most of which are usually negative examples. We evaluate the performance on the test set using MAP and half-life utility which will be discussed below. We repeat the above procedure 20 times and report both mean and standard devia-tion of the experimental results. The parameters of our approaches and the baselines are determined by cross-validation.

MAP (Mean Average Precision) is widely used in information retrieval for evaluating the ranked docu-ments over a set of queries. We use it in this paper to assess the overall performance based on precisions at different recall levels on a test set. It computes the mean of average precision (AP) over all users in the test set, where AP is the average of precisions computed at all positions with a preferred item: where i is the position in the rank list, N is the number of retrieved items, prec ( i ) is the precision (fractions of retrieved items that are preferred by the user) of a cut-off rank list from 1 to i , and pref ( i )is a binary indicator returning 1 if the i -th item is preferred or 0 otherwise.
Half-life Utility (HLU): Breese et al. [5] introduced a half-life utility [13] ( X  X faccuracy X  [12]) to estimate of how likely a user will view/choose an item from a ranked list, which assumes that the user will view each consecutive item in the list with an exponential decay of possibility. A half-life utility over all users in a test set is defined as in Eq. (9).
 where R u is the expected utility of the ranked list for user u and R max u is the maximally achievable utility if all true positive items are at the top of the ranked list. According to [5], R u is defined as follows. where  X  ( j ) equals 1 if the item at position j is pre-ferred by the user and 0 otherwise, and  X  is the half-life parameter which is set to 5 in this paper, which is the same as in [5].
We evaluate our approaches weighting/sampling negative examples by comparing with two categories of baselines treating all missing as negative (AMAN) or treating all missing as unknown (AMAU). 4.3.1 AMAN In AMAN settings, most traditional collaborative fil-tering algorithms can be directly applied. In this paper, we use several well-known collaborative filter-ing algorithms combined with the AMAN strategy as our baselines, which include the alternating least squares with the missing as negative assumption (ALS-AMAN), singular value decomposition (SVD) [29], and a neighborhood-based approach including user-user similarity[1] and item-item similarity algorithms[1]. 4.3.2 AMAU Following the AMAU strategy, it is difficult to adapt traditional collaborative filtering algorithms to obtain non-trivial solutions, as we discussed in Section 1. In this case, ranking items by their overall popularity is a simple but widely used recommendation method. An-other possible approach is to convert the one-class col-laborative filtering problem into a one-class classifica-tion problem. In this paper, we also include one such algorithm, namely the one-class SVM [22] into our pool of baseline methods. The idea is to create a one-class SVM classifier for every item, which takes a user X  X  rat-ings on the remaining set of items as input features and predicts if the user X  X  rating on the target item is positive or negative. The set of training instances for a SVM classifier consists of the rating profiles of those users who have rated the target item, which should consists of positive examples only in the one-class col-laborative filtering setting, which could be used to train a one-class SVM classifier for each target item.
Figure 2 shows the impact of the number of features (parameter d ) on SVD and wALS. We can see for SVD, the performance will first increase and then drop as we increase the number of features. But for wALS, the performance is much more stable and keeps increas-ing. The performance of wALS usually converge at around 50 features. In our following experiments, we will use the optimal feature number for SVD (10 for Yahoo news data and 16 for user-tag data) and wALS (50). In Section 3, we introduced two approaches to OCCF based on sampling and weighting. For each ap-proach, we proposed three types of schemes, that is uni-form, user-oriented and item-oriented. In this section, we will compare the three schemes for both sampling and weighting.

Table 2 compares these schemes. Among the weight-ing schemes, the user-oriented weighting scheme is the best and the item-oriented weighting scheme is the worst. The uniform weighting lies in between. This may be due to the imbalance between the number of users and the number of items. For the current datasets, the number of users is much larger than the number of items.
Figure 3 shows the performance comparisons of dif-ferent methods based on the missing as unknown strat-egy (Popularity and SVM), methods based on the miss-ing as negative strategy (SVD and ALS-AMAN) and our proposed methods(wALS, sALS). The x-axes  X  is defined as  X  = ( comparison consideration, given  X  , the negative sam-pling parameter q in Algorithm 2 (sALS-ENS) is set to  X   X  n r , where n r indicates the number of total posi-tive examples. The baseline popularity will not change with the parameter  X  , therefore it is shown in a hori-zontal line in the figures. The same holds for the base-lines SVD and ALS-AMAN. It can be seen from the figures that our methods outperform all the methods based on missing as negative and missing as unknown strategies.

Parameter  X  controls the proportion of negative ex-amples. As  X   X  0, the methods are approaching the AMAU strategies and as  X   X  1, the methods are ap-proaching the AMAU strategies. We can clearly see that the best results lie in between. That is to say both weighting and sampling methods outperform the baselines. The weighting approach slightly outperform the sampling approach. But as indicated in Table (3), the sampling approach is much more efficient when  X  is relative small.
Table 3. Runtime (seconds) of wALS, sALS ans sALS-ENS with 20 sALS combinations on the Yahoo News data.

We can also see that compared to the AMAU strate-gies the missing as negative strategy is more effective. This is because although the label information of un-labeled examples are unknown, we still have the prior knowledge that most of them are negative examples. Disregarding such information does not lead to com-petitive recommendations. This is somewhat differ-ent with the conclusion in class imbalance classification problems where discarding examples of the dominating class often produces better results [7].
Motivated by the fact that negative examples are often absent in many recommender systems, we for-mulate the problem of one-class collaborative filtering (OCCF). We show that some simple solutions such as the  X  X ll missing as negative X  and  X  X ll missing as un-known X  strategies both bias the predictions. We cor-rect the bias in two different ways, negative example weighting and negative example sampling. Experimen-tal results show that our methods outperform state of the art algorithms on real life data sets including social bookmarking data from del.icio.us and a Yahoo news dataset.

For future work, we plan to address the problem how to determine the parameter  X  . We also plan to test other weighting and sampling schemes and to identify the optimal scheme. We also plan to study the rela-tionship between wALS and sALS-ENS theoretically.
