 Ali Jalali alij@mail.utexas.edu Nathan Srebro nati@uchicago.edu Clustering as the problem of partitioning data into clusters with strong similarity inside the clusters and strong dissimilarity across different clusters is one of the main problems in machine learning. In this paper, we consider the problem of cut-based, or correlation , clustering (Bansal et al., 2002) that has received a lot of attention recently (Ailon et al., 2011; Mathieu &amp; Schudy, 2010; Bagon &amp; Galun, 2011): Given a graph G ( V , E ) on n nodes with normalized symmetric affinity matrix A (for all u, v  X  V : 0  X  A uv  X  1 and A uu = 1), we want to partition V into clusters C = { C 1 , . . . , C so as to minimize the total disagreement
The first term, captures the internal disagreement inside clusters, and the second term captures the ex-ternal agreement between nodes in different clusters. In an ideal cluster, the affinities between all members of the same cluster are 1 and the affinities between members of two different clusters are zero and hence the objective is zero. This objective does not require the number of clusters to be known ahead of time X  we may decide to use any number of clusters, and this is accounted for in the objective. Unfortunately, find-ing a clustering minimizing the disagreement D ( C ) is NP-Hard (Bansal et al., 2002).
 We formulate this problem as an optimization of a convex disagreement objective over a non-convex set of valid clustering matrices (Section 2) and then consider convex relaxations of this constraint. Recently, Jalali et al. (2011) suggested a trace-norm (aka nuclear-norm) relaxation, casting the problem as minimizing an  X  1 loss and a trace-norm penalty, and providing conditions under which the true underlying clustering is recovered. Instead of trace-norm, we propose us-ing the max-norm (aka  X  2 norm) (Srebro et al., 2005), which is a tighter convex relaxation than the trace-norm. Accordingly, we establish an exact recovery guarantee for our max-norm based formulation that is strictly better than the trace-norm based guaran-tee. We show that if the affinity matrix is a corrup-tion of an  X  X deal X  clustering matrix, with a certain bound on the corruption, then the optimal solution of the max-norm bounded optimization problem is ex-actly the ideal clustering (Section 3.1). We also discuss even tighter convex relaxations related to the max-norm, and suggest augmenting the convex relaxation with a single-linkage post-processing step in case of non-exact recovery, showing the empirical advantages of these approaches (Section 5).
 The approach we suggests relies on optimizing an  X  1 objective subject to a max-norm constraint. A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the sub-ject of some interest in the context of  X  X obust PCA X  (Candes et al., 2011; Xu et al., 2012) and recovering the structure of graphical models with latent variables (Chandrasekaran et al., 2010). As with the trace-norm regularized variant, the  X  1 + max-norm problem can be formulated as an SDP and solved using standard solvers, but this is only applicable to fairly small scale problems. In Section 4, we discuss various optimiza-tion approaches to this problems, including approaches which preserve the sparsity of the solution.
 1.1. Relationship to the Goemans Willimason Our convex relaxation approach is related to the classic SDP relaxations of max-cut (Goemans &amp; Williamson, 1995) and more generally the cut-norm (Alon &amp; Noar, 2006). In fact, if we are interested in a partition to ex-actly two clusters, the correlation clustering problem is essentially a max-cut problem, though with both pos-itive and negative weights (i.e. a symmetric cut-norm problem), and our relaxation is essentially the classic SDP relaxation of these problems. Our approach and results differ in several ways.
 First, we deal with problems with multiple clusters, and even when the number of clusters is not pre-determined. If the number of clusters k is pre-determined, the correlation clustering problem can be written as an integer quadratic program, with a k vari-ables per node, and can be relaxed to an SDP. But this SDP will be very different from ours, and will involve a matrix of size nk  X  nk , unlike our relaxation where the matrix is of size n  X  n regardless of the number of clusters. Consequently, the rounding techniques based on (random) projections typically employed for classic SDP relaxations do not seem relevant here. Instead, we employ a single-linkage post-processing as a form of  X  X ounding X  imperfect solutions.
 Second, the type of guarantees we provide are very different from those in the Theory of Computation lit-erature. Most of the SDP relaxation work we are aware of (including the classical work cited above) focuses on worst case constant factor approximation guarantees. On one hand, this means the guarantee needs to hold even on  X  X razy X  inputs where there is really no reason-able clustering anyway, and second, and on the other hand it is not clear how approximating the objective to within a constant factor translates to recovering an underlying clustering. Instead, we prove that when the affinity matrix is close enough to following some underlying  X  X rue X  clustering, the true clustering will be recovered exactly . This type of guarantee is more in the spirit of compressed sensing, which where ex-act recovery of a support set is guaranteed subject to conditions on the input (Jalali et al., 2011). 1.2. Other Clustering Approaches There are several classes of clustering algorithms with different objectives. In hierarchical clustering algo-rithms such as UPGMA (Sneath &amp; Sokal, 1973), SLINK (Sibson, 1973) and CLINK (Defays, 1977), the goal is to generate a sequence of clusterings by merg-ing/splitting two clusters at each step of the sequence according to a local disagreement objective as opposed to our global D ( C ). Because of this locality, these methods are known to be very sensitive to outliers. Cut-based clustering algorithms such as k -means/medians (Steinhaus, 1957; Jain &amp; Dubes, 1981), ratio association (Shi &amp; Malik, 2000), ratio cut (Chan et al., 1994) and normalized cut (Yu &amp; Shi, 2003) try to optimize an objective function globally. The main issue is that they are typically NP-Hard and need to know the number of clusters in advance. In contrast, spectral clustering algorithms(von Luxburg, 2007) try to find the first k principal component of the affinity matrix or a transformed version of that (Meil X a &amp; Shi, 2001). These methods require the number of clusters in advance and has been shown to be tractable (convex) relaxations to NP-Hard cut-based algorithms (Dhillon et al., 2005). These methods are again very sensitive to outliers. Our approach is based on representing a clustering C through its incidence matrix K ( C )  X  R n  X  n where K uv = 1 iff u and v belong to the same cluster in C (i.e. u, v  X  C i for some i ), and K uv = 0 otherwise (i.e. if u and v belong to different clusters). The ma-trix K ( C ) is thus a permuted block-diagonal matrix, and can also be thought of as the edge incidence ma-trix of a graph with cliques corresponding to clusters in C . We will say that a matrix K is a valid clus-tering matrix , or sometimes simply valid , if it can be written as K = K ( C ) for some clustering C (i.e. if it is a permuted block diagonal matrix, with 1s in the diagonal blocks).
 The disagreement can then be written as either: or as: where the term tering C and can thus be dropped.
 We now phrase the correlation clustering problem as matrix problem, where we would like to solve min The problem is that even though the objectives (1) and (2) are convex, the constraint that K is valid is certainly not convex. Our approach to correlation clus-tering will thus be to relax this non-convex constraint (the validity of K ) to a convex constraint.
 We note that although both the absolute error ob-jective (1) and the linear objective (2) agree on valid clustering matrices (or more generally, on binary ma-trices K ), they can differ when K is fractional, and especially when A is also fractional. The choice of objective can thus be important when relaxing the va-lidity constraint to a convex constraint. More specif-ically, as long as A is binary (i.e. A uv  X  { 0 , 1 } ), and 0  X  K uv  X  1, even if K is fractional, the two objec-tives agree. Non-negativity of K uv is ensured in some, but not all, of the convex relaxations we study. When non-negativity is not ensured, the absolute error ob-jective (1) would tend to avoid negative values, but the linear objective might certainly prefer them. More importantly, once the affinities A uv are also fractional, the two objectives differ even for 0  X  K uv  X  1. While the linear objective would tend to not care much about entries with affinities close to 1 / 2, the absolute error objective would tend to encourage fractional values in thees cases.
 The linear objective also has some optimization ad-vantages over the absolute function as well. From a numerical optimization point of view, dealing with the linear objective function is easier since we do not need to compute the sub-gradients of the  X  1 -norm. As discussed in the previous Section, we are interested in optimizing over the non-convex set of valid clus-tering matrices. The approach we discuss here is to relaxing this set to the set of matrices with bounded max-norm (Srebro et al., 2005). The max-norm of a matrix K is defined as where, k  X  k  X  , 2 is the maximum of the  X  2 norm of the rows, and the minimization is over factorization of any internal dimensionality. It is not hard to see that if K is a valid clustering matrix, with K = K ( C ), then k K k max = 1. This is achieved, e.g., by a factorization with R = L , and where each row R u of R is a (unit norm) indicator vector with R ui = 1 for u  X  C i and zero elsewhere.
 Relaxing the validity constraint to a max-norm con-straint, and using the absolute error objective, we ob-tain the following convex relaxation of the correlation clustering problem: Alternatively, we could have used the linear objective (2) instead. In any case, after finding b K , it is easy to check whether it is valid, and if so recover the clus-tering from its block structure. If  X  K is valid, we are assured the corresponding clustering is a globally op-timal solution of the correlation clustering problem. 3.1. Theoretical Guarantee Assuming there exists an underlying true clustering, we provide a worst-case (deterministic) guarantee for exact recovery of that clustering in the presence of noise when the affinity matrix A is a binary 0  X  1 ma-trix using absolute objective. The flavor of our result is similar to (Jalali et al., 2011) for trace-norm, except that we show the max-norm constraint problem recov-ers the underlying clustering with larger noise com-paring to trace-norm constraint. This matches our intuition that max-norm is a tighter relaxation than trace-norm for valid clustering matrices.
 To present our theoretical result, we start by intro-ducing an important quantity that our main result is based upon. Suppose C  X  = { C  X  1 , . . . , C  X  k } is the under-lying true clustering. For a node u and a cluster C  X  i , let otherwise and be the maximum of the disagreement ratios on the ad-jacency matrix. This definition is inspired by (Jalali et al., 2011) but is slightly different. Notice that the larger D max ( A, K ) is, the more noisy (comparing to ideal clusters) the graph is; and hence, the harder the clustering becomes. In particular for ideal clusters (fully connected inside and fully disconnected outside clusters), we have D max ( A, K ) = 0.
 We would like to ensure that when D max ( A, K ) is small enough, our method can recover K . The following lemma helps us understand the information theoretic limit of D max ( A, K ), i.e. what value of D max is cer-tainly not enough to ensure recovery, even information theoretically: Lemma 1. For any clustering C = { C 1 , . . . , C k } and for all  X  &gt; 2 5+ r with r = n 2 P affinity matrix A such that D max ( A, K ( C )) =  X  and the combinatorial program (3) does not output C . Note that the minimum of 2 5+ r is attained when all clusters have equal sizes. If we have k  X  clusters of size , then r = k  X  and the bound in Lemma 1 asserts that if D max ( A, K ) &gt; 2 k  X  +5 , then there are examples for which the original clustering cannot be recovered by the combinatorial program (3). This implies that D max ( A, K ) cannot be scaled better than  X ( 1 k  X  ) in general even without convex relaxation.
 Suppose there exist a true underlying clustering C  X  with k  X  clusters. Let C min be the smallest size under-lying true cluster and we are given an affinity matrix A with D max = D max ( A, K ( C  X  )). Introducing lagrange multiplier  X  , we consider the optimization problem The following theorem characterizes the noise regime under which the simple max-norm relaxation (5) re-covers C  X  .
 to (5) ) is unique and equal to the matrix K  X  = K ( C  X  ) (the solution to (3) ).
 in the theorem. Notice that for a balanced underlying clustering ( k  X  clusters of size n/k  X  ), this parameter is 1 and as the underlying clustering gets more and more unbalanced, this parameter increases. That motivates to call it unbalanceness of the clustering. It is clear that as unbalanceness parameter increases, the region of D max for which our theorem guarantees the cluster-ing recovery shrinks. We plot the admissible region of D max due to unabalanceness in Fig 1.
 Remark 2: According to the Lemma 1, the bound on D max is order-wise tight and can be only improved by a constant in general. 3.2. Comparison to Trace-Norm Clustering Since the max-norm constraint is strictly a tighter re-laxation to the trace-norm constraint, we expect the max-norm algorithm to perform better. Our theo-rem shows improvement over the guarantees provided for trace-norm clustering. Comparing to the result of (Jalali et al., 2011) on trace-norm ( D max  X  | C min | 4 n the max-norm tolerates more noise. To see this, consider a balanced clustering, then trace-norm re-quires D max  X  1 4 k  X  and max-norm requires D max  X  min( 1 k  X  +1 , 0 . 1789) which is larger than 1 4 k  X  for all k The difference gets more clear for unbalanced cluster-ing. Suppose we have one small cluster of constant size | C min | and other clusters are approximately of size n k  X  . As ( n, k  X  ) scales, trace-norm guarantee requires that D max = o ( 1 n ) which is inverse proportional to the size of the smallest cluster, whereas, max-norm guarantee requires D max = o ( k  X  n ) which is inverse proportional to the size of the largest cluster. This is a huge theo-retical advantage in our theorem.
 Further, we compare our algorithm with trace-norm algorithm (Jalali et al., 2011) and SLINK on a proba-bilistic setup. Start from two different ideal clusters on 100 nodes: a) Balanced clusters: four ideal clusters of size 25, b) Unbalanced clusters: three ideal clusters of size 30 and one ideal cluster of size 10. Then, gradually increase D max on both graphs and run all algorithms and report the probability of success in exact recov-ery of the underlying clusters. Although our theoret-ical guarantee is for binary affinity matrices, here, we run the same experiment for fractional affinity matrix. We run all experiments for both absolute and linear objectives. Fig. 3.1 shows that in all cases max-norm outperforms the trace-norm and the improvement is more significant for unbalanced clustering with frac-tional affinity matrix. Moreover, this experiments re-veal that the absolute objective has slight advantage if the affinity matrix is binary and clusters are balanced; otherwise, the linear objective is better. In this Section we consider optimization problems of the form (4). This problem recovers a sparse and low-rank matrix from their sum, considering max-norm as a proxy to rank. In Section 4.1, we discuss how (4) can be formulated as an SDP, allowing us to easily solve it using standard SDP solvers, as long as the problem size is relatively small. We then propose three other methods to numerically solve the optimization problem (4). 4.1. Semi-Definite Programming Method Following Srebro et al. (2005), we introduce dummy variables L, R  X  R n  X  n and reformulate (4) as the fol-lowing SDP problem These constraints are equivalent to the condition k K k max  X  1. This SDP can be solved using generic SDP solvers, though is very slow and is not scalable to large problems. 4.2. Factorization Method Motivated by Lee et al. (2010), we introduce dummy variables L, R  X  R n  X  n and let K = LR T . With this change of variable, we can reformulate (4) as
This problem is not convex, but it is guaranteed to have no local minima for large enough size of the prob-lem (Burer &amp; Choi, 2006). Furthermore, if we now the optimal solution  X  K has rank at most r , we can take L, R to be R n  X  ( r +1) . In practice, we truncate to some reasonably high rank r even without a known guar-antee on the rank of the optimal solution. To solve this problem iteratively, Lee et al. (2010) suggest the following update The projection P max (  X  ) operates on rows of L and R ; if  X  2 -norm of a row is less than one, it remains un-changed, otherwise it will be rescaled so that the  X  2 -norm becomes one.
 A possible problem with the above formulation is the lack of  X  X parsity X  in the following sense: The  X  1 ob-jective is likely to yield and optimal solution K  X  with many non-zeros in A  X  K  X  , i.e. where K  X  is exactly equal to A on some of the entries. However, gradient steps on the factorization are not likely to end up in exactly sparse solutions, and we are not likely to see any such sparsity in solutions of this method. 4.3. Loss Function Method There are gradient methods such as truncated gradi-ent (Langford et al., 2009) that produce sparse solu-tion, however, these methods cannot be applied to this problem. We introduce a surrogate optimization prob-lem to (4) by adding a loss function. For some large  X   X  R , solve Here, the matrix Z is sparse and includes the disagree-ments. For sufficiently large values of  X  , the loss func-tion ensures that the matrix A  X  Z is close to the matrix LR T that is a bounded max-norm matrix. To solve this problem iteratively, we use the update Here, P  X  same sign before and after the update, it remains un-changed; otherwise, it will be set to zero. Solving di-rectly for large values of  X  might cause some problems due to the finite numerical precision. In practice, we start with some small value say  X  = 1 and double the value of  X  after some iterations. This way, we gradu-ally put more and more emphasis on the loss function. 4.4. Dual Decomposition Method Inspired by Rockafellar (1970), we first reformulate (4) by introducing a dummy variable Z  X  R n  X  n as follows Then, introducing a Lagrange multiplier  X   X  R n  X  n , we propose the following equivalent problem: Here, hh X  ,  X ii is the trace of the product. This problem is a saddle-point convex problem in ( Z, K,  X ). To solve this, we iteratively fix  X  and optimize over ( K, Z ) and then, using those optimal values of ( K, Z ), update  X . For a fixed  X , the problem can be separated into two optimization problems over K and Z as which can be solved using factorization method dis-cussed above, and which is a soft thresholding; if |  X  ij | &gt; 1 then, b K ( X ) ij =  X  Sign ( X  ij ); otherwise b K ( X ) ij = A ij . Using b K ( X  k ) and b Z ( X  k ), we update  X  as follows until it converges. One criterion for the convergence of this method is to round both matrices b K, b Z and check if they are equal. To use this criterion, we need to initialize the two matrices very differently to avoid the stopping due to the initialization. 4.5. Numerical Comparison We compare the performance of these methods. For three ideal clusters of size 20 with noise level D max , we run all three algorithms for 2000 iterations. We consider an initial step size  X  = 1 for all methods, and, for the loss function method, we doubel  X  every 100 iterations. For the dual method, we update  X  for 20 times and run 100 iterations of the factorization method for the max-norm sub-problem at each update. We report the sparsity of the solution A  X  b K as well as the  X  1 -norm of the error k b K  X  K  X  k 1 for each algorithm in Fig 3. This result shows that there is a trade-off between sparsity and the error  X  the dual optimization method provides consistently a sparse solution, where, factorization and loss function methods provide small error. The sparsity of loss function method gets worse as the noise increases. In this section, we improve our basic algorithm in two ways: first, we use a tighter relaxation for valid cluster-ing constraint and second, we add a single-linkage step after we recovered the clustering matrix. Although max-norm is a tighter relaxation comparing to trace-norm, we would like to go further and introduce tighter relaxations. Figure 4 summarizes different possible re-laxations based on max-norm. The tightest relaxation we suggest is { K = RR T : k R k  X  , 2  X  1 , R  X  0 } based on the intuition that a clustering matrix is symmetric and has a trivial factorization R  X  R n  X  k , where, R ij is non-zero if node i belongs to cluster j . Next lemma formalizes this result.
 Lemma 2. All relaxation sets shown in Fig. 4 are convex and the strict subset relations hold.
 This suggests using the tightest convex relaxation, that is constraining to K such that there exists R &gt; = 0 , k R k  X  , 2 &lt; = 1 with K = RR T (the set of matrices K with a factorization K = RR T , R &gt; = 0 is called the set of completely positive matrices and is convex (Berman &amp; Shaked-Monderer, 2003)). We optimize over this relaxation by solving the following optimiza-tion problem over R : and setting b K = b R b R T . Although the constraint on b is convex, the problem (6) is not convex in R . 5.1. Single-linkage Post Processing The matrix e K extracted from (6) might diverge from a valid clustering matrix in two ways: firstly, it might not have the structure of a valid clustering and sec-ondly, even if it has the structure, the values might not be integer. We run SLINK on e K as a  X  X ounding scheme X  to fix both of the above problems. SLINK gives a sequence of clusterings C 1 , . . . , C n . To pick the best clustering, we choose The matrix e K can be viewed as a refined version of the affinity matrix A and hence the second step of the algorithm can be replaced by other hierarchical clustering algorithms. 5.2. Comparison with Other Algorithms We compare our enhanced algorithm with the trace-norm algorithm (Jalali et al., 2011) followed by SLINK and SLINK itself. In all cases we pick a clustering from SLINK hierarchy using (7). The setup is identical to the experiment explained in Section 3.2. Fig 3.1 summarizes the results and shows that our algorithm outperforms all competitive methods significantly. Besides the exact recovery, we would like to investi-gate that as noise level D max increases, how bad the output of our algorithm gets. Using  X  X ariation of in-formation X  (Meil X a, 2007) as a distance measure, we compare our algorithm with trace-norm, SLINK and spectral clustering (von Luxburg, 2007) for both bal-anced and unbalanced clusterings. For the spectral clustering method, we first find the largest k = 4 prin-cipal components of A and then, run SLINK on princi-pal components. Fig 5 shows the result indicating that max-norm, even when the noise level is high, outputs a clustering that is not far from the true clustering. 5.3. MNIST Dataset To demonstrate our method in a realistic and larger scale data set, we run our enhanced algorithm, trace-norm and spectral clustering on MNIST Dataset (Le-Cun et al., 1998). For each experiment, we pick a total of n data points from 10 different classes ( n/ 10 from each class) and construct the affinities using Gaussian kernel as explained in (B  X uhler &amp; Hein, 2009). We re-port the time complexities and clustering errors as pre-vious experiment in Fig 5.1. For the spectral cluster-ing, we take SVD using Matlab and pick the top 10 principal components followed by k -means.

