 Given a collection of mentions of entities extracted from a body of text, coreference or entity resolu-tion consists of clustering the mentions such that two mentions belong to the same cluster if and only if they refer to the same entity. Solutions to this problem are important in semantic analysis and knowledge discovery tasks (Blume, 2005; Mayfield et al., 2009). While significant progress has been made in within -document coreference (Ng, 2005; Culotta et al., 2007; Haghighi and Klein, 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010), the larger prob-lem of cross -document coreference has not received as much attention.

Unlike inference in other language processing tasks that scales linearly in the size of the corpus, the hypothesis space for coreference grows super-exponentially with the number of mentions. Conse-quently, most of the current approaches are devel-oped on small datasets containing a few thousand mentions. We believe that cross-document coref-erence resolution is most useful when applied to a very large set of documents, such as all the news ar-ticles published during the last 20 years. Such a cor-pus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size.

Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pair-wise mention similarity or distance function based on mention text, context, and document-level statis-tics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6.
Other previous work attempts to address some of the above concerns by mapping coreference to in-ference on an undirected graphical model (Culotta et al., 2007; Poon et al., 2008; Wellner et al., 2004; Wick et al., 2009a). These models contain pair-wise factors between all pairs of mentions captur-ing similarity between them. Many of these mod-els also enforce transitivity and enable features over Times articles, with the true entities shown on the right. entities by including set-valued variables . Exact in-ference in these models is intractable and a number of approximate inference schemes (McCallum et al., 2009; Rush et al., 2010; Martins et al., 2010) may be used. In particular, Markov chain Monte Carlo (MCMC) based inference has been found to work well in practice. However as the number of men-tions grows to Web scale, as in our problem of cross-document coreference, even these inference tech-niques become infeasible, motivating the need for a scalable, parallelizable solution.

In this work we first distribute MCMC-based in-ference for the graphical model representation of coreference. Entities are distributed across the ma-chines such that the parallel MCMC chains on the different machines use only local proposal distribu-tions. After a fixed number of samples on each ma-chine, we redistribute the entities among machines to enable proposals across entities that were pre-viously on different machines. In comparison to the greedy approaches used in related work, our MCMC-based inference provides better robustness properties.

As the number of mentions becomes large, high-quality samples for MCMC become scarce. To facilitate better proposals, we present a hierarchi-cal model. We add sub-entity variables that repre-sent clusters of similar mentions that are likely to be coreferent; these are used to propose composite jumps that move multiple mentions together. We also introduce super-entity variables that represent clusters of similar entities; these are used to dis-tribute entities among the machines such that similar entities are assigned to the same machine. These ad-ditional levels of hierarchy dramatically increase the probability of beneficial proposals even with a large number of entities and mentions.

To create a large corpus for evaluation, we iden-tify pages that have hyperlinks to Wikipedia, and ex-tract the anchor text and the context around the link. We treat the anchor text as the mention, the con-text as the document, and the title of the Wikipedia page as the entity label. Using this approach, 1 . 5 million mentions were annotated with 43 k entity la-bels. On this dataset, our proposed model yields a B 3 (Bagga and Baldwin, 1998) F1 score of 73 . 7% , improving over the baseline by 16 % absolute (corre-sponding to 38 % error reduction). Our experimen-tal results also show that our proposed hierarchical model converges much faster even though it contains many more variables. The problem of coreference is to identify the sets of mention strings that refer to the same underlying en-tity. The identities and the number of the underlying entities is not known. In within-document corefer-ence, the mentions occur in a single document. The number of mentions (and entities) in each document is usually in the hundreds. The difficulty of the task arises from a large hypothesis space (exponential in the number of mentions) and challenge in resolv-ing nominal and pronominal mentions to the correct named mentions. In most cases, named mentions are not ambiguous within a document. In cross-document coreference, the number of mentions and entities is in the millions, making the combinatorics even more daunting. Furthermore, naming ambigu-ity is much more common as the same string can refer to multiple entities in different documents, and distinct strings may refer to the same entity in differ-ent documents.
 We show examples of ambiguities in Figure 1. Resolving the identity of individuals with the same name is a common problem in cross-document coreference. This problem is further complicated by the fact that in some situations, these individ-uals may belong to the same field. Another com-mon ambiguity is that of alternate names, in which the same entity is referred to by different names or aliases (e.g.  X  X ill X  is often used as a substitute for  X  X illiam X ). The figure also shows an example of the renaming ambiguity  X   X  X ovebug Starski X  refers to  X  X evin Smith X , and this is an extreme form of al-ternate names. Rare singleton entities (like the fire-fighter) that may appear only once in the whole cor-pus are also often difficult to isolate. 2.1 Pairwise Factor Model Factor graphs are a convenient representation for a probability distribution over a vector of output vari-ables given observed variables. The model that we use for coreference represents mentions ( M ) and en-tities ( E ) as random variables. Each mention can take an entity as its value, and each entity takes a set of mentions as its value. Each mention also has a feature vector extracted from the observed text men-tion and its context. More precisely, the probability of a configuration E = e is defined by where factor  X  a represents affinity between men-tions that are coreferent according to e , and factor  X  r represents repulsion between mentions that are not coreferent. Different factors are instantiated for different predicted configurations. Figure 2 shows the model instantiated with five mentions over a two-entity hypothesis.

For the factor potentials, we use cosine sim-ilarity of mention context pairs (  X  mn ) such that Figure 2: Pairwise Coreference Model: Factor graph for a 2 -entity configuration of 5 mentions. Affinity factors are shown with solid lines, and re-pulsion factors with dashed lines.  X  a ( m,n ) =  X  mn  X  b and  X  r ( m,n ) =  X  (  X  mn  X  b ) , where b is the bias. While one can certainly make use of a more sophisticated feature set, we leave this for future work as our focus is to scale up inference. However, it should be noted that this approach is agnostic to the particular set of features used. As we will note in the next section, we do not need to calculate features between all pairs of mentions (as would be prohibitively expensive for large datasets); instead we only compute the features as and when required. 2.2 MCMC-based Inference Given the above model of coreference, we seek the maximum a posteriori (MAP) configuration:  X  e = arg max e p ( e )
Computing  X  e exactly is intractable due to the large space of possible configurations. 1 Instead, we employ MCMC-based optimization to discover the MAP configuration. A proposal function q is used to propose a change e 0 to the current config-uration e . This jump is accepted with the following Metropolis-Hastings acceptance probability: where t is the annealing temperature parameter.
MCMC chains efficiently explore the high-density regions of the probability distribution. By slowly reducing the temperature, we can decrease the entropy of the distribution to encourage con-vergence to the MAP configuration. MCMC has been used for optimization in a number of related work (McCallum et al., 2009; Goldwater and Grif-fiths, 2007; Changhe et al., 2004).

The proposal function moves a randomly chosen mention l from its current entity e s to a randomly chosen entity e t . For such a proposal, the log -model ratio is: log Note that since only the factors between mention l and mentions in e s and e t are involved in this com-putation, the acceptance probability of each proposal is calculated efficiently.

In general, the model may contain arbitrarily complex set of features over pairs of mentions, with parameters associated with them. Given labeled data, these parameters can be learned by Percep-tron (Collins, 2002), which uses the MAP config-uration according to the model (  X e ). There also exist more efficient training algorithms such as SampleR-ank (McCallum et al., 2009; Wick et al., 2009b) that update parameters during inference. However, we only focus on inference in this work, and the only parameter that we set manually is the bias b , which indirectly influences the number of entities in  X e . Un-less specified otherwise, in this work the initial con-figuration for MCMC is the singleton configuration, i.e. all entities have a size of 1 .

This MCMC inference technique, which has been used in McCallum and Wellner (2004), offers sev-eral advantages over other inference techniques: (a) unlike message-passing-methods, it does not require the full ground graph, (b) we only have to exam-ine the factors that lie within the changed entities to evaluate a proposal, and (c) inference may be stopped at any point to obtain the current best con-figuration. However, the super exponential nature of the hypothesis space in cross-doc coreference ren-ders this algorithm computationally unsuitable for large scale coreference tasks. In particular, fruit-ful proposals (that increase the model score) are ex-tremely rare, resulting in a large number of propos-als that are not accepted. We describe methods to speed up inference by 1) evaluating multiple pro-posal simultaneously (Section 3), and 2) by aug-menting our model with hierarchical variables that enable better proposal distributions (Section 4). The key observation that enables distribution is that the acceptance probability computation of a pro-posal only examines a few factors that are not com-mon to the previous and next configurations (Eq. 2). Consider a pair of proposals, one that moves men-tion l from entity e s to entity e t , and the other that moves mention l 0 from entity e 0 s to entity e 0 t . The set of factors to compute acceptance of the first pro-posal are factors between l and mentions in e s and e , while the set of factors required to compute ac-ceptance of the second proposal lie between l 0 and mentions in e 0 s and e 0 t . Since these set of factors are completely disjoint from each other, and the re-sulting configurations do not depend on each other, these two proposals are mutually-exclusive . Differ-ent orders of evaluating such proposals are equiv-alent, and in fact, these proposals can be proposed and evaluated concurrently. This mutual-exclusivity is not restricted only to pairs of proposals; a set of proposals are mutually-exclusive if no two propos-als require the same factor for evaluation.

Using this insight, we introduce the following ap-proach to distributed cross-document coreference. We divide the mentions and entities among multiple machines, and propose moves of mentions between entities assigned to the same machine. These jumps are evaluated exactly and accepted without commu-nication between machines. Since acceptance of a mention X  X  move requires examining factors that lie between other mentions in its entity, we ensure that all mentions of an entity are assigned the same ma-chine. Unless specified otherwise, the distribution is performed randomly. To enable exploration of the complete configuration space, rounds of sampling are interleaved by redistribution stages, in which the entities are redistributed among the machines (see Figure 3). We use MapReduce (Dean and Ghe-Figure 3: Distributed MCMC-based Inference : Distributor divides the entities among the machines, and the machines run inference. The process is re-peated by the redistributing the entities. mawat, 2004) to manage the distributed computa-tion.

This approach to distribution is equivalent to in-ference with all mentions and entities on a single machine with a restricted proposer, but is faster since it exploits independencies to propose multiple jumps simultaneously. By restricting the jumps as described above, the acceptance probability calcu-lation is exact. Partitioning the entities and propos-ing local jumps are restrictions to the single-machine proposal distribution; redistribution stages ensure the equivalent Markov chains are still irreducible. See Singh et al. (2010) for more details. The proposal function for MCMC-based MAP infer-ence presents changes to the current entities. Since we use MCMC to reach high-scoring regions of the hypothesis space, we are interested in the changes that improve the current configuration. But as the number of mentions and entities increases, these fruitful samples become extremely rare due to the blowup in the possible space of configurations, re-sulting in rejection of a large number of proposals. By distributing as described in the previous section, we propose samples in parallel, improving chances of finding changes that result in better configura-tions. However, due to random redistribution and a naive proposal function within each machine, a large fraction of proposals are still wasted. We address these concerns by adding hierarchy to the model. 4.1 Sub-Entities Consider the task of proposing moves of mentions (within a machine). Given the large number of mentions and entities, the probability that a ran-domly picked mention that is moved to a random entity results in a better configuration is extremely small. If such a move is accepted, this gives us ev-idence that the mention did not belong to the pre-vious entity, and we should also move similar men-tions from the previous entity simultaneously to the same entity. Since the proposer moves only a sin-gle mention at a time, a large number of samples may be required to discover these fruitful moves. To enable block proposals that move similar men-tions simultaneously, we introduce latent sub-entity variables that represent groups of similar mentions within an entity, where the similarity is defined by the model. For inference, we have stages of sam-pling sub-entities (moving individual mentions) in-terleaved with stages of entity sampling (moving all mentions within a sub-entity). Even though our con-figuration space has become larger due to these ex-tra variables, the proposal distribution has also im-proved since it proposes composite moves. 4.2 Super-Entities Another issue faced during distributed inference is that random redistribution is often wasteful. For ex-ample, if dissimilar entities are assigned to a ma-chine, none of the proposals may be accepted. For a large number of entities and machines, the probabil-ity that similar entities will be assigned to the same machine is extremely small, leading to a larger num-ber of wasted proposals. To alleviate this problem, we introduce super-entities that represent groups of similar entities. During redistribution, we ensure all entities in the same super-entity are assigned to the same machine. As for sub-entities above, inference switches between regular sampling of entities and sampling of super-entities (by moving entities). Al-though these extra variables have made the config-uration space larger, they also allow more efficient distribution of entities, leading to useful proposals. 4.3 Combined Hierarchical Model Each of the described levels of the hierarchy are sim-ilar to the initial model (Section 2.1): mentions/sub-entities have the same structure as the entities/super-entities, and are modeled using similar factors. To represent the  X  X ontext X  of a sub-entity we take the union of the bags-of-words of the constituent men-tion contexts. Similarly, we take the union of sub-factor touches). The boxes on factors were excluded for clarity. entity contexts to represent the context of an entity. The factors are instantiated in the same manner as Section 2.1 except that we change the bias factor b for each level (increasing it for sub-entities, and decreasing it for super-entities). The exact values of these biases indirectly determines the number of predicted sub-entities and super-entities.

Since these two levels of hierarchy operate at separate granularities from each other, we combine them into a single hierarchical model that contains both sub-and super-entities. We illustrate this hi-erarchical structure in Figure 4. Inference for this model takes a round-robin approach by fixing two of the levels of the hierarchy and sampling the third, cycling through these three levels. Unless specified otherwise, the initial configuration is the singleton configuration, in which all sub-entities, entities, and super-entities are of size 1 . We evaluate our models and algorithms on a number of datasets. First, we compare performance on the small, publicly-available  X  X ohn Smith X  dataset. Sec-ond, we run the automated Person-X evaluation to obtain thousands of mentions that we use to demon-strate accuracy and scalability improvements. Most importantly, we create a large labeled corpus using links to Wikipedia to explore the performance in the large-scale setting. 5.1 John Smith Corpus To compare with related work, we run an evalua-tion on the  X  X ohn Smith X  corpus (Bagga and Bald-win, 1998), containing 197 mentions of the name  X  X ohn Smith X  from New York Times articles (la-beled to obtain 35 true entities). The bias b for our approach is set to result in the correct number of entities. Our model achieves B 3 F1 accuracy of 66 . 4% on this dataset. In comparison, Rao et al. (2010) obtains 61 . 8% using the model most similar to ours, while their best model (which uses sophis-ticated topic-model features that do not scale easily) achieves 69 . 7% . It is encouraging to note that our approach, using only a subset of the features, per-forms competitively with related work. However, due to the small size of the dataset, we require fur-ther evaluation before reaching any conclusions. 5.2 Person-X Evaluation There is a severe lack of labeled corpora for cross-document coreference due to the effort required to evaluate the coreference decisions. Related approaches have used automated Person-X evalu-ation (Gooi and Allan, 2004), in which unique person-name strings are treated as the true entity labels for the mentions. Every mention string is replaced with an  X  X  X  for the coreference system. We use this evaluation methodology on 25 k person-name mentions from the New York Times cor-pus (Sandhaus, 2008) each with one of 50 unique strings. As before, we set the bias b to achieve the same number of entities. We use 1 million samples in each round of inference, followed by random re-distribution in the flat model, and super-entities in the hierarchical model. Results are averaged over five runs. Figure 5: Person-X Evaluation of Pairwise model : Performance as number of machines is varied, aver-aged over 5 runs.
 Table 1: Wikipedia Link Corpus Statistics . Size of an entity is the number of mentions of that entity.
Figure 5 shows accuracy compared to relative wallclock running time for distributed inference on the flat, pairwise model. Speed and accuracy im-prove as additional machines are added, but larger number of machines lead to diminishing returns for this small dataset. Distributed inference on our hi-erarchical model is evaluated in Figure 6 against in-ference on the pairwise model from Figure 5. We see that the individual hierarchical models perform much better than the pairwise model; they achieve the same accuracy as the pairwise model in approx-imately 10% of the time. Moreover, distributed in-ference on the combined hierarchical model is both faster and more accurate than the individual hierar-chical models. 5.3 Wikipedia Link Corpus To explore the application of the proposed approach to a larger, realistic dataset, we construct a corpus based on the insight that links to Wikipedia that ap-pear on webpages can be treated as mentions, and since the links were added manually by the page au-thor, we use the destination Wikipedia page as the Figure 6: Person-X Evaluation of Hierarchical Models : Performance of inference on hierarchical models compared to the pairwise model. Experi-ments were run using 50 machines. entity the link refers to.

The dataset is created as follows: First, we crawl the web and select hyperlinks on webpages that link to an English Wikipedia page. 2 The anchors of these links form our set of mentions , with the sur-rounding block of clean text (obtained after remov-ing markup, etc.) around each link being its con-text. We assign the title of the linked Wikipedia page as the entity label of that link. Since this set of mentions and labels can be noisy, we use the following filtering steps. All links that have less than 36 words in their block, or whose anchor text has a large string edit distance from the title of the Wikipedia page, are discarded. While this results in cases in which  X  X resident X  is discarded when linked to the  X  X arack Obama X  Wikipedia page, it was nec-essary to reduce noise. Further, we also discard links to Wikipedia pages that are concepts (such as  X  public_domain  X ) rather than entities. All enti-ties with less than 6 links to them are also discarded.
Table 1 shows some statistics about our automat-ically generated data set. We randomly sampled 5% of the entities to create a development set, treating the remaining entities as the test set. Unlike the John Smith and Person-X evaluation, this data set also contains non-person entities such as organiza-tions and locations.

For our models, we augment the factor potentials with mention-string similarity: where STREQ is 1 if mentions m and n are string identical ( 0 otherwise), and w is the weight to this feature. 3 In our experiments we found that setting w = 0 . 8 and b = 1 e  X  4 gave the best results on the development set.

Due to the large size of the corpus, existing cross-document coreference approaches could not be ap-plied to this dataset. However, since a majority of related work consists of using clustering after defining a similarity function (Section 6), we pro-vide a baseline evaluation of clustering with Sub-Square (Bshouty and Long, 2010), a scalable, dis-tributed clustering method. Subsquare takes as in-put a weighted graph with mentions as nodes and similarity between mentions used as edge weights. Subsquare works by stochastically assigning a ver-tex to the cluster of one its neighbors if they have significant neighborhood overlap. This algorithm is an efficient form of approximate spectral cluster-ing (Bshouty and Long, 2010), and since it is given the same distances between mentions as our models, we expect it to get similar accuracy. We also gen-erate another baseline clustering by assigning men-tions with identical strings to the same entity. This mention-string clustering is also used as the initial configuration of our inference.
 Figure 7: Wikipedia Link Evaluation : Perfor-mance of inference for different number of machines ( N = 100 , 500 ). Mention-string match clustering is used as the initial configuration. Table 2: F1 Scores on the Wikipedia Link Data . The results are significant at the 0.0001 level over Subsquare according to the difference of proportions significance test.

Inference is run for 20 rounds of 10 million sam-ples each, distributed over N machines. We use N = 100 , 500 and the B 3 F1 score results obtained set for each case are shown in Figure 7. It can be seen that N = 500 converges to a better solu-tion faster, showing effective use of parallelism. Ta-ble 2 compares the results of our approach (at con-vergence for N = 500 ), the baseline mention-string match and the Subsquare algorithm. Our approach significantly outperforms the competitors. Although the cross-document coreference problem is challenging and lacks large labeled datasets, its ubiquitous role as a key component of many knowl-edge discovery tasks has inspired several efforts.
A number of previous techniques use scoring functions between pairs of contexts, which are then used for clustering. One of the first approaches to cross-document coreference (Bagga and Bald-win, 1998) uses an idf-based cosine-distance scor-ing function for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of con-texts only if the mentions are deemed  X  X mbiguous X  using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) inte-grate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clus-tering. Niu et al. (2004) incorporate information ex-traction into the context similarity model, and anno-tate a small dataset to learn the parameters. A num-ber of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering al-gorithm used. Daum  X  e III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to as-sist within-document coreference.

Since many related methods use clustering, there are a number of distributed clustering algorithms that may help scale these approaches. Datta et al. (2006) propose an algorithm for distributed k-means. Chen et al. (2010) describe a parallel spectral clustering algorithm. We use the Subsquare algo-rithm (Bshouty and Long, 2010) as baseline because it works well in practice. Mocian (2009) presents a survey of distributed clustering algorithms.
Rao et al. (2010) have proposed an online deter-ministic method that uses a stream of input mentions and assigns them greedily to entities. Although it can resolve mentions from non-trivial sized datasets, the method is restricted to a single machine, which is not scalable to the very large number of mentions that are encountered in practice.

Our representation of the problem as an undi-rected graphical model, and performing distributed inference on it, provides a combination of advan-tages not available in any of these approaches. First, most of the methods will not scale to the hundreds of millions of mentions that are present in real-world applications. By utilizing parallelism across ma-chines, our method can run on very large datasets simply by increasing the number of machines used. Second, approaches that use clustering are limited to using pairwise distance functions for which ad-ditional supervision and features are difficult to in-corporate. In addition to representing features from all of the related work, graphical models can also use more complex entity-wide features (Culotta et al., 2007; Wick et al., 2009a), and parameters can be learned using supervised (Collins, 2002) or semi-supervised techniques (Mann and McCallum, 2008). Finally, the inference for most of the related ap-proaches is greedy , and earlier decisions are not re-visited. Our technique is based on MCMC inference and simulated annealing, which are able to escape local maxima. Motivated by the problem of solving the corefer-ence problem on billions of mentions from all of the newswire documents from the past few decades, we make the following contributions. First, we intro-duce distributed version of MCMC-based inference technique that can utilize parallelism to enable scal-ability. Second, we augment the model with hierar-chical variables that facilitate fruitful proposal distri-butions. As an additional contribution, we use links to Wikipedia pages to obtain a high-quality cross-document corpus. Scalability and accuracy gains of our method are evaluated on multiple datasets. There are a number of avenues for future work. Although we demonstrate scalability to more than a million mentions, we plan to explore performance on datasets in the billions. We also plan to examine inference on complex coreference models (such as with entity-wide factors). Another possible avenue for future work is that of learning the factors. Since our approach supports parameter estimation, we ex-pect significant accuracy gains with additional fea-tures and supervised data. Our work enables cross-document coreference on very large corpora, and we would like to explore the downstream applications that can benefit from it.
 This work was done when the first author was an intern at Google Research. The authors would like to thank Mark Dredze, Sebastian Riedel, and anonymous reviewers for their valuable feedback. This work was supported in part by the Center for Intelligent Information Retrieval, the Univer-sity of Massachusetts gratefully acknowledges the support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime con-tract no. FA8750-09-C-0181., in part by an award from Google, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by NSF grant #CNS-0958392, and in part by UPenn NSF medium IIS-0803847. Any opin-ions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
