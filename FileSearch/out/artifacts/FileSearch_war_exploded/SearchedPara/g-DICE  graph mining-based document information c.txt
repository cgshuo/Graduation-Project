
ORIGINAL PAPER K. C. Santosh 1 Abstract In this paper, we present document information content (i.e. text fields) extraction technique via graph min-ing. Real-world users first provide a set of key text fields from the document image which they think are important. These fields are used to initialise a graph where nodes are labelled with the field names in addition to other features such as size, type and number of words, and edges are attributed with relative positioning between them. Such an attributed relational graph is then used to mine similar graphs from document images which are used to update the initial graph iteratively each time we extract them, to produce a graph model. Graph models, therefore, are employed in the absence of users. We have validated the proposed technique and eval-uated its scientific impact on real-world industrial problem with the performance of 86.64% precision and 90.80% recall by considering all zones, viz. header, body and footer. More specifically, the proposed technique is well suited for table processing (i.e. extracting repeated patterns from the table) and it outperforms the state-of-the-art method by approxi-mately more than 3%.
 Keywords Document information content  X  Table processing  X  Key text fields  X  Spatial relations  X  Attributed relational graph  X  Graph mining 1.1 Motivation Document image analysis or processing has been explored for several decades [ 2 , 31 , 36 , 44 ]. According to [ 44 ], docu-ment analysis is related to document image analysis (DIA) since the overall research works have been concerned with document image interpretation. It deals with zone separa-tion or segmentation, layout understanding, localisation and recognition. Broadly speaking, two major areas have been considered: graphics and text analysis/processing [ 14 , 31 ]. Separating text from graphics in a single document is not just an issue, but subsequent processes are still required to understand complete document. For example, document information content exploitation (DICE) has been receiving an important attention. In this paper, the term  X  X ontent X  refers to text. DICE tools are required to reduce or eliminate paper-based manual works since they are not only unsecure and subject to human error but also time-consuming and costly. As reported in [ 36 ], the cost of manual data extraction can go up to 9 EUR for a single invoice. In this situation, one can think of a commercial system where thousands of data have to be processed in every single day. DICE, however, does not provide a complete solution, as it requires a priori knowledge of which texts are meaningful. The term  X  X ean-ingful X  makes the DICE intelligent, and it is user-defined. Inspired by the real-world application, users select a set of key text fields, allowing the machine exploit similar sets from other test documents without further user intervention. In other words, extracting information content from documents in accordance with the users X  specifications is the core theme of the paper.

In this paper, users first select a set of key text fields (i.e. a pattern) from a few (or limited) document images. These fields are used to initialise a graph where fields are labelled with their field names (plus basic features such as size and type) and edges are attributed with relative posi-tioning between them. Such an attributed relational graph (ARG) is then used to exploit similar patterns via graph min-ing and is transformed into a model graph each time we extract them. The graph mining process is entirely based on the pivotal selection and node induction process via relation validation. We have learnt model graphs from a limited num-ber of document images (maximum 10 document images per class). We, thus, refer the proposed method as graph mining -based document information content exploitation (g-DICE). We address a set of key text fields in terms of pattern exploita-tion regardless of a document X  X  structure. Further, in this proposed method, the user can not only modify the input pat-terns by amending fields within the set in case wrong fields are selected but also provide a complete replacement. This makes our method easy to learn, cost effective to adopt and flexible to extend. 1.2 State of the art In document text processing, most methods require optical character recognition (OCR) technology so that one can work on word and sometimes on character level to offer an intel-ligent solution for information content exploitation. In this framework, table processing has been an extensively studied 65 ].Theauthorsdescribedtableintermsofeitherrulinglines, text lines and (un)analysed text blocks, a set of cells resem-bling the 2 D grid or a set of strings that are integrated with each other via relations [ 15 , 29 , 33 , 34 ]. Generally speaking, we have (a) graphics-based, (b) OCR-based and (c) hybrid (best of the both worlds) approaches to detect zone and/or to extract document information content. Based on the state-of-the-art literature, these approaches are based on (a) ruling lines, (b) boundary limits, (c) column and row alignments, (d) projection profiles, (e) white spaces, (f) syntax-based text search, (g) full text search (key word search like table header detection), and (h) pre-defined layout (style), header and trailer patterns.

Structural information as in table can be useful for index-ing and retrieving document information content [ 11 , 42 ]. To analyse table-forms structure, rulings techniques lack a priori knowledge about table organisation [ 21 , 40 , 65 ]. For exam-ple, in [ 61 ], authors used interest points detection based on intersections (crossings) and corners. Such a technique is not robust to handle broken rulings. In such a case, in [ 26 ], authors proposed a technique to deal with broken rulings, but limited to a small gap. Furthermore, a table is localised based on the intersection of horizontal and vertical graph-ical lines [ 7 , 19 ]. But, the concepts are failed since not all interesting zones possess lines.

For primary works on document structure analysis (e.g. table detection and recognition), we refer to fundamental schemes presented in [ 32  X  34 ]. In their works, authors devel-opedatablespottingandstructureextractionsystem(T-Recs) using word bounding boxes as the input. Since it relies only on boxes, it may not be effective on tables with multi-column layouts. In [ 39 , 58 ], they use a set of text lines and table lines (based on white space between consecutive words) so that adjacent lines with large white spaces and horizontal adja-cent words can be grouped together. The statistical concept assumes that the maximum number of columns is two and three templates of page layout, viz. single, double and mixed column. For training, it requires a large amount of labelled data. Similarly, in [ 27 ], the authors used segmented text lines (via horizontal projection profiles) and assumed single col-umn input document image. Like previous approaches, it does not offer solutions for multi-column documents. In the literature, one can find extensive use of projection profile-based techniques since 1990 because it is easy to separate fields in columns and rows [ 8 ]. Further, authors made a binary decision based on white spaces [ 27 , 59 ]. In [ 41 ], researchers observed that the tables have distinct columns. This implies that gaps between the fields in the tables are substantially larger than the gaps between the words in other text lines. Even though it is limited to a specific document layouts, authors found that the column alignment study is still inter-esting to exploit heterogeneous documents by integrating application-dependent heuristics [ 52 ].

A new hybrid page layout analysis algorithm has been developed where an initial data-type hypothesis and the tab-stops location are used to format the studied page [ 54 ]. This means that the detected tab-stops are used to deduce the col-umn layout of the page. The column layout is then applied in a top-down manner to impose structure and reading order. Recently, researchers use the common intersection points between four neighbouring boxes (facing each other) to allo-cate and identify fields in the corresponding columns [ 9 ]. The concept shows interesting results on handwritten documents. But content variations of any fields induce significant change in the overall column structure (e.g. deviation of the white spaces in columns). Such a change includes field appearance like height, width and position. As a consequence, many cues typically used for data extraction become difficult to exploit. A basic example is that the variation in the periodicity of text lines does not let method to cope data extraction without hav-ing dynamism in relative positioning between the fields. In such a case, simple column alignments do not offer an effec-tive solution. Another example is that the white spaces used to separate fields may fail due to the possible overlapping. Again, a concept like [ 9 ] may fail. Most of the existing meth-ods assume that fields are organised in a widely separated columns where there is no fields overlapping. Eventually, the decision after integrating such cues is still weak since the scores are all determined by using hard thresholds (and assumptions).
 In [ 53 ], tables are recognised via pre-defined table styles. The test documents containing tables in a known layout are analysed by using information like table location, table record identification, field location and context. The kernel of the table recognition procedure is based on text line projec-tion. The concept is interesting, but it may not provide clear separation when sub-headings appear in the table. To clarify this, as an example, description fields (which are associated with the table) are not always in the regular format may pro-vide difficulty in separating rows or columns. Additionally, obstacles include small print, tight line spacing, poor-quality text (suchas photocopies) andline-art or backgroundpatterns that touch the text. In such a case, simple text line projec-tion may bring overlapping between two different rows (and columns). Eventually, local rules are inevitable. In addition, in [ 63 ], they built a model by taking table header informa-tion (i.e. knowledge reference or list of key words). This allows them to accurately start table extraction, including information about tabular structure, if the reference model (table header) is correctly detected. Subsequently, table col-umn separators are used by taking coordinates of column rows that are relative to table header. The concept is diffi-cult to extend up to heterogeneous tabular structure having non-obvious rows and column alignments and to documents having table in different locations, viz. header, body and footer.

Several methods are employed for extracting information from invoices. Study based on analysis of physical struc-ture [ 11 ], cartridge headers or structural redundancy [ 5 , 22 , 23 ] is still worth-taking. In [ 23 ], consecutive text lines shar-ing similar token structure are extracted. In brief, similarity is measured by token content type (numeric, alphabetic and alphanumeric) and by token alignment. In [ 22 ], main text lines are first identified by using real numbers such as price (using regular expression) and projection profiles are then used to separate columns. Periodicity structure of the main lines is used to assign invoices to one of the pre-defined types. This concept is inspired from the previous work presented in [ 35 ] where a database document of known invoice struc-ture is used to study its family and token alignment is used to identify repeated structure from test invoices. These meth-ods [ 5 , 23 ] possess a major difficulty in modelling knowledge to extract tables of all types since they contain fixed knowl-edge which is purely specific and thus they are limited to certain types of document structure.

Detecting columns, lines and headers, and representing them in terms of a graph is another interesting work [ 46 ]. However, considering real-world applications, the concept requires some heuristics to optimise performance. State-of-the-art methods which are focussed on document structure recognition, interesting zone/region (e.g. table) and layout detection do not provide a complete solution in case one is required to extract important information from any specified zone. In other words, they do not extract key text fields, nor do they explicitly perform content understanding. To detect and understand the important content, relations between the fields must be considered. In this context, recently, graph matching techniques were studied [ 24 , 25 ]. In [ 24 ], a single pdf document is represented by an ARG, in which nodes represent physical items and edges represent spatial and log-ical relationships. A pdf wrapper is then defined to create a sub-graph so that the required data (regular tabular struc-tures) can be retrieved. In other words, the major drawback of the approach is that their system does not learn from the annotation specified by the user. Instead, the user manually specifies the conditions for subgraphs to match. This requires significant effort and technical expertise. Further, in the real-world problem, false detection is its another bottleneck. This work can benefit from the recently published work on graph matching [ 51 ], where a complete document image can be treated in the form of sub-graphs through the construction of a graph lattice, a hierarchy of related sub-graphs linked in a lattice. Likewise, detecting repeated structures is still an important contribution in the field, rather than just detecting a complete boundary of the specific zone [ 4 ]. The concept is based on one shot structure learning where an example is provided to extract similar ones from the same document. It uses cues such as token alignments, which are extensively used in the state of the art. Further, it fails to extract the pat-terns if at least one field is missing in an item. The source of missing fields can be either from the document itself or from the OCR error.

To fully exploit the document information content, we cannot rely on the pre-defined layout and structure of the documents and the state-of-the-art cues like ruling lines, col-umn alignments, white spaces and projection profiles, since they do not meet the requirement of the heterogeneous doc-uments. Further, we must be able to reject fields which are not meaningful for the users from any zone of interest. 1.3 Contribution As shown in Fig. 1 , users first provide a set of key text fields which they deem important. An input pattern graph (or query graph) is now initialised, where the fields are labelled with their names and features, like size and type, and edges are attributed with spatial relations. This query graph is then used to exploit similar graphs via graph mining and is transformed into a model graph iteratively each time we extract them.
Graph mining algorithms can be based either on a pri-ori knowledge or on pattern-growth. Since association rules are used in a priori-based algorithms [ 1 ], structured pattern mining which is associated with pattern-growth algorithms is interesting in our case, where graph-based pattern rep-resentation can be taken as a special case [ 12 , 60 ]. In this case, the matches that are strictly based on structural similarity between the graphs yield computational com-plexity polynomial time [ 18 , 43 , 57 , 60 ]. Computational time can be speeded up by using labelled nodes and attributed edges [ 10 , 17 , 20 , 60 ]. Thus, pattern graph mining must be made in such a way so that it assures edge connectivity constraint [ 55 , 64 ]. Connectivity is only meaningful when it provides a complete structure of the pattern. Based on this, our graph mining starts with a pivotal node selection in a document (with respect to each labelled node in the query graph). From each pivotal node, relation assignment will guide to compute feature score between the pair of nodes. Relation validation leads to a node induction process. Such a node induction process results a set of nodes that are similar to the set of query nodes in the query graph. Our relations constrained feature score computation is fast since the search space is limited to the degree of the node associated with the query graph. The extracted similar graphs are now verified and used to update the query graph as a model graph. On the whole, a semi-supervised graph learning method is pro-posed to update the query graph, where learning documents are limited to maximum 10 document samples per class.
This paper is the thorough extension of the work presented in [ 48  X  50 ] where they provide a proof of a concept, validating on limited input patterns that are mostly linear and are more focussed on the table extraction problems. Based on this, in this paper, we integrate dynamic labels at nodes instead of just not relying on a pre-defined list and introduce a possible swinging (i.e. back and forth, and up and down) capabil-ity of spatial relations in graph mining scheme. In addition, in-depth evaluations that are not just limited to extract infor-mation content associated with tables are studied. Overall, our model depends on the structure of the query graph and it does not follow how the document is structured. Therefore, no matter what the structure of the document is, our principal evaluation has been made separately over document zones, viz. header, body and footer.

In the following section, the major three phases, as depicted in Fig. 1 , are explained.
 2.1 Graph initialisation The user selects interested fields to help the system to extract similar sets of fields from documents. To fully exploit the selected information, one needs to label them and formalise the visual links that exist between them. Selecting two or more fields will provide an information about a complete structural representation. Therefore, their visual links (i.e. connectivities) must be meaningful. In addition, field labels are useful. In Fig. 2 , we illustrate a complete idea how a set of selected fields can be transformed into a complete ARG. In this illustration, we provide a bidirectional graph in which one can change the direction if the reference field changes (see Fig. 5 ).

As inFig. 2 , inwhat follows, weexplainhow: (a) input pat-ternsarerepresented,(b)thefieldsarerepresentedbyfeatures and (c) relative positioning between the fields is computed. The complete three-step process then yields graph-based pat-tern representation. 2.1.1 Input pattern Any document d is composed of a set Z of zones, Z = { header, body, footer } . In document image processing, this zone categorisation will make the problem simpler (cf. Sect. 1.2 ). In general, we have doc. d = { zone z , z  X  X  For any zone z , users provide input pattern(s), zone z = { pattern p , p  X  X  1 , P ]} , where the total number of input pat-terns P can be varied from one zone to another.

To input several different patterns, users may not neces-sarily use the same document (belonging to the particular class). Further, we provide flexibility to the users since they can change, amend and/or provide different input patterns at any time. Two documents illustrating different input patterns are shown in Fig. 3 . In this illustration, we zoom in on a por-tion of the input pattern (in the first example) from body zone where table-like document structure is present. Formally, an ( a ) ( a ) (b) (c) input pattern is a set of selected key text fields, pattern p ={ field i } i = 1 ,..., A . (1) These input patterns can be either linear or zig-zag. A set of fields that are appeared in a single line is said to be a linear pattern. Zig-zag and/or nonlinear patterns refer to those patterns where fields are found to be in multiple lines (see body pattern in Fig. 3 b). Missing fields and missing lines in any pattern are possible. Further, the number of words and lines each field composed of (in the input pattern) can vary. The input pattern selection depends on the user X  X  interest, not the document X  X  structure. 2.1.2 Feature representation To represent a field, we define a feature set F F = feature where features to represent a field can be rectangular box, its content (i.e. OCR value), string type, string size, number of words, words separation within a field, number of lines it is composed of and its corresponding label. In our case, for any i th field, we can formally represent a feature as field F i = , price and reference , for instance ). Features are application dependent. If we consider table processing, the labels in our feature set are the derivatives of the table headers, in general. We, therefore, avoid direct use of table headers and can avoid their column alignment since table headers are not always correctly aligned with their own corresponding fields. For example, table header named  X  code TVA  X  and its corresponding field values in Fig. 3 cdo not possess perfect alignment.

In this work, a dynamic field analyser is studied in addition totheconventionalschemewherefixedsetsofregularexpres-sions are used. In case of the fixed sets, confusions may occur fromoneclass(orsupplier)toanother,sincetheirtextformats vary. As an example, reference or code article text formats are different from one class to another. As a consequence, the pre-defined list does not cover all incoming documents text formats and their styles. The dynamic regular expression provides the specific text format/style in accordance with the particular class and thus it avoids confusions with other classes. In Fig. 4 , an example with the label  X  code article  X  X s transformed into  X  \ d { 4 }[ X  X \ d { 6 }  X . Such labels are updated the set L in that particular class, in addition to the standard list. Using regular expressions, we are able to avoid OCR errors if characters are broken or connected with graphics. 2.1.3 Relative positioning To compute relative positioning between the key text fields, we use bounding box and its projection into 3  X  3 parti-tions [ 45 ]: R = where field  X  represents the reference field. Bounding box projection is supported by their corresponding centroids (i.e. centroid from any box should pass through another). For A number of fields in the provided pattern, we have A  X  ( A 1 )/ 2 relations defined in R , where loop does not exist, r  X  if i = j (see Fig. 2 ).

To compute relative positioning between the fields, we name the fields (i.e. from 1 to A ) based on ordering from left to right, line by line. In Fig. 3 c, relations between field a reference) and { field i } i = 2 ,..., A can be right ( Relations for all fields with respect to field 1 are identical. Therefore, to differentiate those pairs, we take the level of neighbourhood k into account in addition to the basic set of spatial relation predicates defined in R . Then, the relation for field j with respect to field i can be expressed as r The parameters k 1 and k 2 refer to the number of horizontal and vertical jumps from the reference field, respectively. For an adjacent field, k = 0. Using Fig. 5 , adjacent and non-adjacent jumps are explained with examples by taking both (a) linear and (b) nonlinear input patterns. In Fig. 5 a, r adjacent to each other, and r 13 = right 1 , 0 ( field 1 , single horizontal jump is required. 2.1.4 Graph-based pattern representation Formally, the complete input pattern is represented by a com-plete 4-tuple ARG G = ( V , E , F where V is a finite set of nodes (fields); E  X  V  X  V , i.e. a finite set of edges and each r ij  X  E is a pair of (v i since v i ,v j  X  V ; F V : V  X  L V , L V represents a set of nodes, their features and their labels in the set L , defined in the particular domain (cf. Sect. 2.1.2 ); and F E : E  X  R E , R E represents the edges via relations, defined in Sect. 2.1.3 ).

To complete a graph, we introduce (a) missing and (b) neighbouring fields. Missing fields are presented between the selected fields. Neighbouring fields are taken before and after the first and the last selected fields. When taking neigh-bouring fields into account, we must know how many fields are appropriate. Using all fields from left to right may not be interesting because fields from different context or regions can be taken. For example, in table extraction, fields from another table can be taken if two tables exist side by side, spanning horizontally. Based on our dataset experience, we fix the maximum number of neighbouring fields as three. For example, in Fig. 3 c, missing field  X  quantity  X  and neighbour-ing fields  X  prix total H.T.  X  and  X  code TVA  X  are included in a graph. To determine the missing and neighbouring fields, intra-field and inter-field separation are used: intra-field i = max intra-wSep i , inter-field ij = min inter-wSep ij , (7) where intra-wSep i = max {  X ( word w , word w + 1 ) : word w word w + 1  X  field i } (i.e. the maximum distance between two consecutive words from any i th field); inter-wSep ij min  X ( word w , word w ) : word w  X  field i , word w  X  field (i.e. the minimum distance between two consecutive words from any consecutive i th and j th fields); and field = { word w , box w } , 1  X  w  X  W .

These are computed either from the selected text fields or from the fields belonging to neighbouring blocks. Fields in the neighbouring blocks are only used in case the selected fields do not contain two or more words. In a graph, to sepa-rate the selected fields from others, an activation key is used (as a binary operator), v = 1 The binary value  X 1 X  represents its activation (for selected fields) and  X 0 X , otherwise. 2.2 Graph mining Graph-based input patterns (query graphs) developed in Sect. 2.1 are now used to exploit similar graphs from the studied documents. Given the query graph Q , to extract sim-ilar graphs from a document, we start with pivotal nodes selection and perform relation validation to compute the fea-ture score between the pairs of nodes. Such pivotal selection and relation validation strategies dramatically reduce mining complexity (cf. Sect. 2.5 ). 2.2.1 Pivotal node selection For every node v q i in the query graph Q , the correspond-ing label q i is defined [cf. Sect. 2.1.2 and F V in Eq. ( 6 )]. Similarly, for any document d , fields are labelled as i .The number of fields in the document is derived by using inter-and intra-word separation knowledge [cf. Eq. ( 7 )] of the respective input pattern(s). The sets L q and L (including their nodes) respectively for a query graph and a complete document can be expressed as Having these labelled nodes in Q , the target is to select nodes sharing identical labels ( i = q i ) from the set L of labels in the studied document d . Consider { i } are taken as pivotal nodesandtheset L t isupdated, L t  X  L (seeAlgorithm 1 ).In this process, instead of taking all labelled nodes from a query graph, we start with nodes (fields) having a single word and nodes that regularly appear in that particular class of docu-ment. For example, labels like price , quantity , amount , code article , code TVA , date or date of birth , facture number (or invoice number ), order number and reference from a single class (as shown in Fig. 3 a) are used. However, labels like description are never used because the number of words, number of lines and their regular expressions are varied.
The pivotal node selection is entirely based on the node query list, and it varies from class to class, relying on the text field selection. 2.2.2 Relation validation and feature score computation AsshowninAlgorithm 2 , each pivotal node from the set L t is used to validate relations with neighbouring nodes in the studied document. Relation validation allows to connect nodes so that the document graph is structurally similar to the query graph. It uses all possible relations { r q ij } the set [cf. Sect. 2.1.3 and F E in Eq. ( 6 )] defined in query graph Q .
Algorithm 1 : Pivotal node selection using all possible labels of a query graph.

Algorithm 2 : Relation validation and feature score compu-tation from one pivotal node.

For V q number of nodes in the query graph Q , there exists at least V q number of ways to extract similar sets of nodes. But, to extract a graph exactly similar to Q, one of them is sufficient. In general, for any i th pivotal node, the set of relations calR that are required to validate is: R In Fig. 6 , we simplify the relation validation set by transform-ing the query graph into the corresponding adjacency graph. Again, we illustrate it with real-world examples in Fig. 7 , where labels like (a) code article and (b) quantity have been applied separately. From any pivotal node, one can extract exactly the same set of fields.

In every relation validation (i.e. r q ij = r ij ), we update r . score ( r q ij , r ij ) = 1 since the pairs of nodes are structurally identical. We then compute feature score between the pair of nodes (v i ,v j ) in a document with respect to (v q i Q . More formally, we can compute feature score f . score between two corresponding nodes v q and v as f . score (v q ,v) = where  X  f  X  X  0 , 1 ] . Given two strings: x and y (representing v q and v , respectively), we compute feature matching scores from string type , noW and size [see Eq. ( 3 )]: s s s where, in case of Levenshtein dist. type (.) , we treat numer-als { 0  X  9 } , all alphabets { A  X  Z , a  X  z } and symbols equally, and dist. noW (.) and dist. size (.) refer to the differences in number of words and sizes of the studied strings, respec-tively. For example, consider x = NIKE 2000 . 52 and y = NIKE 1999 . 01 , then Levenshtein dist. type ( x , y ) = dist. noW ( x , y ) = abs ( 2  X  2 ) = 0 and dist. size ( x abs ( 10  X  10 ) = 0 .

Our algorithm does not just rely on the exact relation validation ( r q ij = r ij ), but also introduces possible swing-ing to immediate/neighbouring relations that go back and forth, and up and down. The idea is elaborated in Fig. 8 , where three neighbouring nodes are taken into account from the node sharing exact relation. To elaborate in detail, con-sider any node v j at position ( a , b ) , v a , b j , then the most appropriate node will be the one that results the maximum f . score (.) , f . ( a ) (b) (c) ( d ) 2.2.3 Graph matching score Feature scores are computed between the query node and the node from the document validating the relation. Relation val-idation allows matching straightforward (i.e. isomorphism). Aggregating both scores (relation validation score and fea-ture score) determines the level of similarity between the graphs. In general, the total matching score S for data graph G with respect to Q can be expressed as S (
Q , G ) =  X  where  X   X  X  0 , 1 ] . In our case, we keep  X  = 0 . 5 to avoid biasing between them.

Until now, our relation validation aims to return fully matched pattern. Given the graphs: Q and G , an exact graph matching problem is to find a one-to-one mapping, i.e.  X  : V  X  V q such that (v Such a graph isomorphism problem needs a priori knowledge about the labels at nodes and or possible relations between them. In practice, this is not always the case and therefore, an exact portion of the query graph matching is also possible, i.e. inexact matching where we can have V  X  V q and E  X  E q and then mapping between them can be  X  : V  X  V q such ( a ) that (v i ,v j )  X  E iff ( X  (v i ),  X  (v j ))  X  E q . To obtain max-imum common sub-graph [ 6 ], a graph edit distance is used, inspired from [ 37 ]. In general, the graph edit distance (GED) between Q and G is defined as the cost of the best edit path by taking the number of edit operations that transforms Q into G [ 56 ].
 GED ( Q , G ) = min where  X ( Q , G ) denotes the set of edit paths transforming G into Q and c denotes the cost function measuring the strength c ( o ) of edit operation o . In general, the possible edits, defined for both nodes and edges, are divided into three categories: substitutions, additions and deletions. We use node deletions to make the data graph structurally identical with the query graph Q and therefore, the c ( o ) applies to node addition. In our case, instead of taking GED (.) separately, we integrate it into f . score (.) computation and we are limited to maximum two nodes addition (to avoid false detection). To illustrate it, as shown in Fig. 9 , consider fully labelled nodes V q = { v q G . To match them, we first insert a node v d 3 in graph G .This insertion makes matching straightforward and their matching score is, S (
Q , G ) = In this example, node labels are identical and  X   X  refers to virtual node and edges. As soon as a single node is added, the edges associated with it make the query graph complete. Therefore, in our case, we are limited to penalise an addition of a single node [ 3 ]. 2.2.4 Output graphs For pivotal node selection, if we consider all labelled nodes from the set v q i from the query graph, we have V q number of possibilities to extract similar sets of nodes in the form of graph. To avoid such possible redundant graphs at the output, we take a single graph (unique graph U ) from which the matching score is produced, U = argmax Since we force word-level pivotal node selection, labels like description are not applied. These are extracted with the help of neighbouring labels, their relations and features like size , noW and noL . Therefore, faster graph isomorphism like [ 62 ] may not directly fit into our application since we do not extract all fields by pivotal selection.

On the whole, we are able to extract a set U of unique similar graphs [cf. Eq. ( 16 )] plus their matching scores [cf. Eq. ( 14 )], U = { U 2.3 Graph learning (models) Our graph learning mechanism is semi-supervised. To update the query graph, a very few learning documents (limited to maximum 10 document samples per class) are taken. The mechanism uses unlabelled fields with the help of small num-ber of labelled fields (input patterns).

In our case, all mined graphs may not be used for updating the query graph. The mined graphs { U u , S u } are needed to bepruned.Tohandlethis,weapplythefollowingtwocriteria: a) graph consistency and b) matching score.
 The graph is said to be consistent if node labels i  X  L . In case nodes are not labelled, the graph is chosen if S (.)  X  threshold (i.e. 0.80, empirically designed).

After pruning, our graph modelling focusses on updating node features in a query graph since we do not intend to change the structural organisation. The assumption (based on semi-supervised learning) is that identically connected nodes tend to have the same label. It does not always hold true, and, therefore, we take node features such as wSep , size , noW and noL into account for updating, including their label s. For any node v F i [cf. Eq. ( 3 )], new features values are added as v and maxVal refer to the minimum and the maximum values from the query graph and the mined graph(s). For example, in Fig. 10 , Keeping relations as they are in query graph, features are updated as v M ( V , E ,  X  F graph(s) help in augmenting the matching score since we take feature range into account in matching process. Con-sider node strings x (in model graph) and y (in data graph). The distance in terms of their sizes is computed as size  X  y size )  X  abs ( initVal  X  newVal ) The similar process occurs in other features like wSep , noW and noL [cf. Eq. ( 12 )].

Considering a set Z of zones, we have a set M of models in any particular class k as, M From each model, a confidence score (CS) is computed by aggregating all matching scores from the mined graphs, CS z p k = 1 U u S u . As an example, in Fig. 10 ,wehave CS
These models are used to exploit similar information from test documents. In case of multiple models, outputs are ranked based on the order of confidence score. 2.4 Sample outputs In Fig. 11 , starting with input pattern, pivotal node selection and outputs are provided. In this example, node label price (where debit is considered as price , in general) is used to select pivotal nodes from the studied documents, and both regular and translated document images are taken into con-sideration to extract similar patterns. Considering the output from the translated document, the proposed method does not require any knowledge about document zone as long as pat-tern structure remains the same and it scans whole document page to select pivotal nodes (and sets of pivotal nodes remain identical for both document images).

In Fig. 12 , both linear and zig-zag-type patterns are shown including missing fields and lines. In this illustration, body patterns appear in all documents. These outputs show the robustness of the method.

The method does not offer good performance when OCR does not read the text and when the query graph is poor (see Fig. 13 ). In Fig. 13 a, the first field is missed from the last body pattern due to OCR error. But on the other hand the detected fields (in that pattern) maintain the overall pat-tern structure similar to the query pattern regardless of its alignment X  X hich, in fact, demonstrates the interesting part of the proposed concept because it does not require informa-tion about field alignment.

False detection is another issue that can result from the structural similarity. We have two nodes, representing the pattern graph. Aside from the first two graphs in the tabular structure, the last three graphs are well spotted because labels and the graph structures remain identical (see Fig. 13 b). But these are not found in the ground-truths. Such false detection requires post-processing. Like in the state-of-the-art approaches, white space gap can be one of the options to overcome it. In this paper, no post-processing has been employed. 2.5 Computational cost Consider V q and V be the finite sets of nodes in a query graph Q and data graph G , respectively. The maximum time complexity needed for pivotal node selection from any docu-ment d will be O ( V q  X  V ). This could be substantially reduced sincenodelabellike price and amount selectsidenticalnodes. Further, we do not employ all query nodes for pivotal node selection (cf. Sect. 2.2.1 ). To compute matching between two graphs: Q and G , we rely on relation validation to com-pute feature score. Thus, the maximum time complexity is O ( deg (v q edges (relations) including back and forth, and up and down swings (see Fig. 8 ). This means that the search space, for any pivotal node v i in a document, is limited to the degree of corresponding query node deg (v q i ) . Additionally, a single pivotal node is sufficient to extract the identical set of nodes in a data graph. Now, for a complete document image, it is O ( E  X  V  X  L t ), where L t is the set of pivotal nodes.
Considering the aforementioned issues, on average, our graph mining algorithm takes 2s per document image, on average, even when the pivotal list is 100 (Fig. 14 ). This means that time complexity is not linearly proportional to the pivotal list because they are rejected in the first two iterations, as soon as no relation validation happens. 3.1 Dataset, input pattern and ground-truths We worked on a real-world industrial problem in direct collaboration with the ITESOFT , 1 France. In our dataset, (a) both printed and handwritten texts including graphics are present; (b) broken strokes in characters, noise due to touched handwritten texts (e.g. signatures) and overlapping of graphics such as stamps are considerable issues; (c) tabu-lar alignments are not always regular even when headers are found to be relatively well positioned; (d) graphical lines are not necessarily appeared in tables; and (e) tables appeared side by side (spanning horizontally). Our current dataset is composed of 90 classes with 15 X 100 images per class. Alto-gether, we have 2215 document images. In this set, some samples are translated (in a random fashion) for our research purpose (see Fig. 11 d).

In each class, we have received input patterns from first two to 10 documents. Altogether, 230 documents are used. The number of input patterns per document varies from one to three. Input patterns sometimes are unknowingly repeated (i.e. unintentional). In this case, to increase the number of input patterns, some of the fields are removed automatically but in a random manner. This automatic field removal process results new input patterns. Altogether, we have 431 input pat-( a ) (b) terns, where 211 input patterns are created in the laboratory, and the remaining 220 input patterns are directly taken from the real-world users. The number of fields per pattern varies between two and seven. There are 1123 fields from all input patterns, where 543 fields are from those input patterns that are created in the laboratory. Figure 15 shows a few exam-ples. For more details, we refer to Sect. 3.3 . The number of input patterns varies from zone to zone. In header and footer zones, input patterns are sometimes missed. In contrast, in the body zone, multiple input patterns are provided. As a result, 83.98% (i.e. 362) of the total input patterns are taken from table-like structure (structured zone).

For each document, users provide ground-truths (i.e. all corresponding similar patterns) that are anticipated. The number of ground-truth patterns per document varies from one to nine. As a result, considering the whole dataset, we have altogether 3471 ground-truth patterns (composed of approximately 7232 fields, on the whole). 3.2 Evaluation protocol Precision and recall metrics are used to check how success-fully the patterns are retrieved. Precision is the fraction of retrieved (ret.) patterns that are relevant (rel.) while recall is the fraction of the patterns that are relevant to the query and are successfully retrieved. In general, precision = In our test, to compute precision and recall, we use area-ratio-based and string-matching-based measures between the fields of the corresponding patterns.

For a single document, we have a set U of the mined graphs, representing an output, O ={ U u } U u = 1 ,withaset U  X  of ground-truths, O  X  ={ U  X  number of fields that can be represented by iconic boxes, {
B evaluation measures are provided.
 Area-ratio-based measure (ARM) We use the overlapping ratio between the two key field boxes (using Dice coefficient measure [ 13 ], cf. Fig. 16 ), ( a ) (b) where | B  X  b  X  B b | is the intersected or common area of them three different variants (v.1 X  X .3) by using threshold values. v.1 OR 1 (,) = v.2 OR 1 (,) = v.3 OR 1 (,) = Variants 1 and 2 refer to the soft decision and take the output when overlapping score is more than 80 and 90%, respec-tively. Variant 3 refers to the hard decision and takes the output when overlapping scores are 100%. In other words, variant 3 represents a  X  X inary decision X  (i.e.  X 1/0 X  or  X  X es/no X ) and thus brings the worst-case scenario of the performance of the studied algorithm.

We aggregate all OR 1 (, ) to compute overlapping ratio between the graphs, = 1 Then for a complete set of graphs (i.e. an output), we can express evaluation metric as = 1
String-matching-based measure (SMM) In this measure, we use normalised Levenshtein distance (NLD) [ 38 ] between two strings, ( a ) (b) (c) NLD 0 ( str  X  st  X  , str st ) = 1  X  Levenshtein distance Based on this, we use three variants by using threshold val-ues as in Eq. ( 21 ) to produce NLD 1 (, ) . All three variants replace Eq. ( 22 ) and therefore, we can update Eq. ( 23 ) with Eval SMM (, ) . 3.3 Results Table 1 shows an overall performance of the system, evalu-ated over 90 different classes. It is composed of two different ways of validation.
 The first validation demonstrates how well the algorithm works when the input patterns are clean. The second con-veys how robust it is. In Fig. 15 , some sample fields from both tests are shown, which are used in both tests.
In Table 1 , we observe that the cleaner the input pattern, the better the performance. This happens to be in test no. 1 since input patterns are created in accordance with box sizes from OCR. In contrast, in case of the user input patterns (in test no. 2), the fields selection has been made directly on doc-ument images by drawing rectangular boxes. Therefore, one field selection may take word(s) or character(s) from other, closer fields (can be left, right, top and bottom) and multi-ple lines. This will then change feature properties like wSep , noW and noL . Further, in that selected box (from users), since OCR reads some dots (due to noise) as  X  X ull-stop X ,  X  X olon X  and  X  X emi-colon X , features representing the graph nodes can possibly be varied. 3.4 Extension 3.4.1 Table processing In Table 1 , separate results for header, body and footer are provided. We observed better performance in the body zone compared to the footer and header because a) it is structured and b) there are more chances (and multiple pivotal nodes) to have multiple patterns. The latter issue is connected with the quality of the graph models. The higher the number of patterns in the output, the higher the graph model quality. In our experimental dataset, we barely have multiple patterns in header and footer zones. This causes the performance dif-ferences with body zone because unique pattern extraction from those zones does not help to learn the model.
On the whole, table processing is found to be the better suited application, where the focussing point is that the table extraction does not always mean only to detect the presence and to spot the area where table(s) is(are) located but also to select important key text fields. Furthermore, considering body zone, frequently used labels analysed and their corre-sponding scores are provided in Table 2 . In our test, labels like descriptions arenotcompletelydetected(i.e.somewords or letters may be missed). In contrast, other fields are well spotted. 3.4.2 Comparative study As mentioned in Sect. 1.3 , the only reference and the closest state-of-the-art method is document information extraction based on the repeated structure [ 4 ]. In their study, they focussed on repeated structure detection: a very specific and interesting table processing problem. Therefore, in our comparison set up, a subset of the documents containing multi-line table(s) is selected from 2215 documents, which is composed of 1019 documents (i.e. 46.01% of the whole dataset). These results come from 65 classes (i.e. 10 X 40 doc-uments per class) out of 90. Similarly, their corresponding ground-truth patterns, which are composed of 2474, are sep-arated. For the work reported in [ 4 ], it requires at least a single input pattern per document page. Therefore, we have taken the first ground-truth pattern as an input pattern. In contrast, the proposed technique does not require the same set up. Instead, as said before, it uses a few input patterns from maximum 10 documents per class to develop graph models(s). Even though 83.98% of the total input patterns are taken from table-like structure (cf. Sect. 3.1 ), they do not always have multi-line tables. Therefore, we have 219 input patterns that correspond to 65 classes of 1019 docu-ments. These input patters are collected from both tests: nos. 1 and 2 (cf. Sect. 3.3 ). For comparison, the same evaluation protocol (see Sect. 3.1 ) of variant 1 (v.1) is used.
In Table 3 , comparative tests are provided. In this compar-ative study, we observe that the proposed technique performs the best, where, like before, SMM provides better scores compared to ARM measure. Unsurprisingly, OCR errors affect the performance of the system. OCR frequently fails to read or mis-read the texts. In such a case, an omission of a single text (or word), i.e. a missing field makes their sys-tem [ 4 ], fails to detect the complete pattern. Further, in Bart and Sarkar [ 4 ], missing fields cannot be recovered. Align-ments, on the other hand, affect performance since every pattern may not be perfectly aligned. As said before, one of the prominent algorithmic differences is that their work requiresinputpattern(s)foreverytestdocumentimage,while the proposed technique learns graph models from a few train images (i.e. maximum 10 document images per class). This means that they use location information of the provided patterns to start a complete search. As a consequence, their system requires significant manual effort.

Further, user-guided pdf documents wrapping based on graph matching [ 24 , 25 ] can be taken for comparison. But, considering a large database, inputting a wrapper is a tedious task (cf. Sect. 1.2 ). However, their tool is well suited for table detection. In this paper, we have presented graph mining-based docu-ment information content (i.e. text fields) exploitation. We have initialised the query graph(s) based on the users X  spec-ifications, employed a simple and effective graph mining technique to extract similar patterns from the documents and transformed the query graph into model graphs, which are used in the absence of the users. We have developed an intel-ligent solution for document information exploitation that excels in the following points: a) simplicity and ease of use, and b) accuracy, ease of deployment and flexibility. It does not require a large set of document images to learn graph models. Further, the input patterns can be changed, amended and replaced, since model learning takes less than 10s for a singleinputpatternperclass,onaverage.Overallinformation exploitation performance is 86.64% (precision) and 90.80% (recall) in less than 2s per document image, on average. This allows industry standard integration possible.

To learn the model(s), unique pattern extraction does not help. This happens to be in header and footer zones. Body zone is still interesting where there exists similar repeated patterns in a single document. Based on this, in this paper, we have also addressed table processing problem, where we have validated that the table processing does not always mean only to detect the presence and to spot the area where table(s) is(are) located but also to select important key text fields withinit.Inthisframework,wehaveachievedtheprecisionof 89.30%andrecallof94.87%.Thisperformanceoutperforms the state of the art by approximately more than precision of 3%.

Regarding limitations, the proposed method does not provide complete and accurate solutions for those patterns having a large set of fields in a zig-zag format due to query graph complexity. To manage this, one of the solutions is to take advantage of the recent work reported in [ 51 ]. Another solution is to transform a complex graph into several linear graphs (i.e. G ={ g 1 , g 2 ,..., g G } ). Such a graph transforma-tion is based on the total number of lines, the input pattern is composed of. Every linear graph is derived from those fields that are lying in a single line. Connecting them requires an intelligent technique, which is in our plan. Another interest-ing future work is to include relevance feedback, a pertinent issue in the framework of  X  X uery learning X .

