 Mining discrete patterns in binary data is important for sub-sampling, compression, and clustering. We consider rank-one binary matrix approximations that identify the domi-nant patterns of the data, while preserving its discrete prop-erty. A best approximation on such data has a minimum set of inconsistent entries, i.e., mismatches between the given binary data and the approximate matrix. Due to the hard-ness of the problem, previous accounts of such problems em-ploy heuristics and the resulting approximation may be far away from the optimal one. In this paper, we show that the rank-one binary matrix approximation can be reformulated as a 0-1 integer linear program (ILP). However, the ILP formulation is computationally expensive even for small-size matrices. We propose a linear program (LP) relaxation, which is shown to achieve a guaranteed approximation error bound. We further extend the proposed formulations using the regularization technique, which is commonly employed to address overfitting. The LP formulation is restricted to medium-size matrices, due to the large number of variables involved for large matrices. Interestingly, we show that the proposed approximate formulation can be transformed into an instance of the minimum s-t cut problem, which can be solved efficiently by finding maximum flows. Our empirical study shows the efficiency of the proposed algorithm based on the maximum flow. Results also confirm the established theoretical bounds.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms Binary matrix factorization, rank-one, integer linear pro-gram, regularization, minimum cut, maximum flow
Many applications involve data with discrete attributes and high dimensionality, such as binary images and document-term associations. For such applications, it is desirable to retain the discrete nature of the original data in the factor-ized components. PCA [8] is a commonly used method for reducing the dimension of continuous data, and it has been used widely in computer vision and many other applications [16, 18]. PCA computes a set of orthogonal projection vec-tors by the Singular Value Decomposition (SVD) [4], which capture the largest variations in the data. PCA and many other matrix-based techniques [12] deal with data of con-tinuous attributes. Analysis of discrete data sets, however, generally leads to NP-complete/hard problems, especially when physically interpretable results in discrete spaces are desired. For discrete data sets, where all the entries are ei-ther 0 or 1, it is desirable to find decomposition where all the entries involved are from { 0 , 1 } .

A technique, called PROXIMUS [13] was proposed to mine discrete patterns from binary data. PROXIMUS is based on the binary matrix factorization, which separates a data set based on the entries of a binary vector and on recursive par-tition in the direction of such vectors. The final product for a given binary matrix by PROXIMUS is a tree struc-ture, representing various degrees of similarity among data points. Similar to SVD, the rank-one approximation is at the core of PROXIMUS. A binary rank-one approximation is given by the outer product of two binary vectors, one asso-ciated with data points and the other with their attributes. Starting from the root node of a tree, the entries in the data vector serves as indicators for bisection. That is, data points with 1 are made into one group while those with 0 are into the other, e.g., two disjoint submatrices. By this, two child nodes of the root are constructed. The tree is completed by this way recursively for each submatrix until some termina-tion criterion is met, e.g., bisection is not possible for a data vector of all ones. PROXIMUS has been applied for mining high-dimensional discrete-attribute data [13, 14, 15].
The problem of binary rank-one approximations for a given binary matrix is to find two binary vectors x 1 and x 2 such that the outer product of x 1 and x 2 is at the minimum Ham-ming distance from the given binary matrix [13]. In particu-lar, for a binary matrix A  X  X  0 , 1 } I 1  X  I 2 , where I some positive integers, x 1  X  { 0 , 1 } I 1 and x 2  X  { 0 , 1 } computed by solving the following optimization problem: where || X || F denotes the Frobenius norm [4] of a matrix. The problem in (1) is conjectured to be NP-hard [14], and an iterative heuristic for rank-one approximation was pro-posed in PROXIMUS [13], whose solutions are sensitive to the initialization. Eight heuristics were proposed to initialize the approximation vectors. However, there is no theoretical guarantee on the quality of the approximations [15].
In this paper, we propose high-quality low-rank approxi-mations of discrete data sets with guaranteed error bounds. Our main contributions include:
In this section, the problem of rank-one approximation is transformed into a maximum weight problem (definition follows). The simple form of the objective function in this problem permits elegant mathematical formulations for op-timal solutions.
 Figure 1: The 0-1 integer linear program (0-1 ILP) including constraints (3)(4)(5) for the maximum weight problem and its relaxation (LP1) including constraints (3)(4)(6).

Since every entry of the given matrix A is either 0 or 1, the objective function of the binary rank-one approximation can be simplified as follows:
X a given binary matrix A  X  { 0 , 1 } I 1  X  I 2 as W i,j = 1 , if A i,j = 1 , and W i,j =  X  1, otherwise. Since A is fixed in equation (2), an optimal solution can be found by solv-ing max x 1 , x 2 x T 1 Wx 2 . We call this the  X  X aximum Weight Problem X  (MWP) as formally stated below: Maximum Weight Problem (MWP) : Given a weight matrix W  X  { X  1 , 1 } I 1  X  I 2 , find two vectors x is maximized.

A 0-1 integer linear program (ILP) is given for the exact solution of MWP in Figure 1. We will show that solving this 0-1 ILP is equivalent to solving MWP. The constraint (5) is employed in the 0-1 ILP instead of (6), which will be used for the LP relaxation in Section 3.

There are three types of binary variables x 1 i , x 2 j , and z in the ILP for 1  X  i  X  I 1 and 1  X  j  X  I 2 . Variables x x 2 j represent the i -th and j -th entries of solution vectors x 1 and x 2 respectively. Variable z i,j is an auxiliary variable whose value is 1, if and only if both the values of x 1 i x 2 j are 1. The objective function of MWP can therefore be rewritten as and (4) are for the relation between z i,j and the pair of x and x 2 j . Notice that variable z i,j for each pair of i and j appears exactly twice in two places of the ILP: ( i ) the term W i,j z i,j in the objective function and ( ii ) the one constraint for W i,j . This implies that the value of z i,j is determined by the sign of W i,j and the other two variables involved in the constraint for W i,j . Figure 2: LP2: A linear program for the relaxed problem.

For each positive W i,j , constraint (3) ensures that z i,j equals zero if the value of x 1 i or x 2 j is zero. If x 1 i both equal to one, the value of z i,j is free to be either 0 or 1; however, the maximization forces the value of z i,j to be one since W i,j is positive. For each negative W i,j , constraint (4) makes the value of z i,j one if x 1 i and x 2 j both equal to one. If any of x 1 i and x 2 j equals zero, then the value of z is free to be either 0 or 1; nevertheless, at optimality, z must equal zero since W i,j has negative contribution to the objective in this case. Consequently, the relation between z i,j and the pair x 1 i and x 2 j for some i and j at optimality is as follows: z i,j = 1 , if and only if x 1 i = 1 and x relation between the ILP and MWP is summarized below.
Proposition 1. For a given matrix A  X  X  0 , 1 } I 1  X  I 2 , the 0-1 ILP in Figure 1 defines an optimal solution S = { x x x T 2 is an optimal rank-one approximation for A , where x k A k 2 F  X  sible pairs of x 1 and x 2 for A .
The ILP formulation in Figure 1 defines the optimal solu-tions for MWP. However, it is computationally expensive to solve even for small-scale problems. In this section, we pro-pose an efficient algorithm based on the linear programming relaxation for computing error-bounded rank-one approxi-mations. The error by this algorithm is shown to be no more than twice of that by the optimal solution.
We relax the integral constraint (5): x 1 i , x 2 i , z ij program (LP) in Figure 1. We call this LP as LP1 . Each entry of x 1 and x 2 in LP1 is allowed to have any nonnegative value no more than 1.
 Solving LP1 is less difficult than solving its original ILP in Figure 1. Linear programs are polynomial-time solvable [10]. Simplex Method [2] and its variants are widely used to solve LP X  X . Despite their exponential worst-case complexity [11], simplex algorithms have persistently shown to perform very well in practice [17]. A randomized polynomial-time simplex algorithm is given by [9]. We use simplex algorithms to solve LP relaxations.

A compact LP, named LP2 , that defines the same set of optimal solutions for LP1 is given in Figure 2. LP2 has fewer constraints and variables, i.e., more efficient than LP1 in terms of running time. The main difference between the two formulations is that all the constraints at (3) in LP1 are incorporated into the objective function of LP2. This
Algorithm 1 : LPIT  X  Algorithm for binary rank-one approximation Data : matrix A  X  X  0 , 1 } I 1  X  I 2 Result : a pair of vectors in { 0 , 1 } I 1 and { 0 , 1 } Phase One: Formulate an instance of LP2 for A ; Find an optimal solution x 1  X  R I 1 , x 2  X  R I 2 ; Phase Two: Construct a weight matrix W from A ;
Apply the iterative procedure to improve x 1 and x 2 ; can be done because of the following facts: ( i ) LP1 is a maximization problem, ( ii ) the W i,j for the z i,j in (3) is positive, ( iii ) each z i,j appears in exactly one constraint, every variable in LP1 can have a value in the interval [0 , 1]. Those facts imply that the equality at (3) must hold for LP1 at optimality. Suppose, as contradiction, that there exists a case that the equality at (3) dose not hold at op-the facts that W i,j = 1 and z i,j does not involve in any other constraint, increasing the value of z i,j gives a feasible solution with better objective value, a contradiction. On the other hand, the equality of each constraint at (4), for W i,j =  X  1, may not hold at optimality, e.g., z i,j =  X  1 for x 1 i = 0 and x 2 j = 0 if the equality would hold but z
Each variable z i,j for positive W i,j in LP1 becomes re-dundant and is removed from LP2. However, their values can be obtained from the solution for LP2 as Since removing or relaxing a constraint of a maximization program does not reduce an optimal value, the relations among the optimal values l ILP , l LP1 , and l LP2 of the ILP, LP1, and LP2, respectively, are follows: l ILP  X  l LP1 = l for the same given weight matrix.

Proposition 2. For the same problem instance, the op-timal value of MWP is no more than that of LP2.
Our algorithm for binary rank-one approximation consists of two phases: ( i ) find a solution by LP2, and ( ii ) apply an iterative procedure to further improve the solution. The iterative procedure [13] can be done as follows: Since the objective value for MWP is x T 1 Wx 2 , one of the vectors x and x 2 can be quickly determined if the other is fixed. The procedure alternatively finds one of the vectors, while fixing the other, until a termination criterion is met. The method, named LPIT , is summarized in Algorithm 1.

Two questions arise from the algorithm LPIT: (1) Is a solution found in Phase One of LPIT valid? That is, does LP2 find a pair of binary vectors? and (2) What is the quality of solutions found by LPIT?
Every variable in LP2 is allowed to have any value within the interval between 0 and 1. A matrix with fractional values cannot be a binary approximation. The solution by simplex algorithms for LP2 is shown in Section 3.3 to have binary property; namely, every entry in x 1 and x 2 is either 0 or 1.
In addition to feasibility, the quality of solutions by ap-proximation is also important. The second phase of the algorithm LPIT is a heuristic, whose improvement on so-lutions is less predictable. The assurance on error-bounded approximations therefore depends on the initial vectors, i.e., optimal solutions for LP2. The discrepancy between objec-tive values of MWP and LP2 is the focus of analysis on the error bound, which is covered in Section 3.4.
The main result of this section is summarized in the the-orem below.

Theorem 3. The value of every variable found by the simplex algorithm for LP2 is either 0 or 1.
 The following definitions are relevant to the coefficient ma-trix of the constraints in LP2. A square matrix is unimod-ular if its determinant is  X  1 or 1. A matrix A is totally unimodular if every square submatrix B of A is unimodular or singular, i.e., det B  X  { X  1 , 0 , 1 } . A matrix A is totally unimodular if the condition below holds [5].
 Sufficient Condition 1 (Total Unimodularity).
 Suppose that every column of matrix A  X  { X  1 , 0 , 1 } I 1 has two nonzero entries or fewer. There exists a pair of disjoint sets S 1 and S 2 , with S 1  X  S 2 = { 1 , 2 , . . . , I that the following condition holds: For every column j with two nonzero entries A i,j and A k,j , where i  X  S p and k  X  S p, q  X  X  1 , 2 } , ( i ) A i,j 6 = A k,j for p = q , and ( ii ) A for q 6 = p .

A standard unit vector e is a vector with exactly one en-try of one and the others are of zero, e.g., [0 , 0 , 1 , 0] lemma below can be proved by using basic facts about de-terminants.
 ular. The following matrices are totally unimodular: (a) its transpose A T , (b) the matrix [ A | X  e ]  X  R I 1  X  ( I 2 (c) a matrix obtained from A by interchanging two rows or columns.

Proof. (a) is due to the fact that det B T = det B for any square matrix B . (c) is due to the fact that interchanging two rows or columns changes only the sign of a determinant. For any matrix C  X  R n  X  n and any row i of C , det C = P submatrix of C obtained by removing row i and column k from C . A square submatrix C of the matrix [ A | X  e ]  X  R or ( iii ) [ D | X  e ], where D  X  R n  X  ( n  X  1) is a submatrix of A for some n , 1  X  n  X  min { I 1 , I 2 + 1 } . The matrix C in case ( i ) is unimodular due to the total unimodularity of A . Since matrix C in case ( ii ) has a column consisting entirely of zeros, det C = 0 , which means C is unimodular. For a row i corresponding to the nonzero entry of e in case ( iii ), D [ i,  X  ] is a square submatrix of A . Thus, [ A | X  e ] is totally unimodular.

Now consider the coefficient matrix C of the constraints in LP2. Rearranging constraints and variables in different way results in a distinct matrix for C . However, due to (c) in Lemma 4, total unimodularity is invariant under such operations. Without loss of generality, the rows and columns of C are assumed to be properly ordered with respect to the format in Figure 2. Let e i,n denote a standard unit vector of size n with i -th entry being 1. Suppose that a given I 1 weight matrix W has negative entries W i 1 ,j 1 , W i 2 ,j and W i k ,j k for some k , 1  X  k  X  I 1 I 2 . Thus, C can be partitioned into constraints of variable bounds, e.g., (8) and (9) in Figure 2, are not included in a coefficient matrix.

Lemma 5. The coefficient matrix of constraints in LP2 is totally unimodular.

Proof. Let X be the matrix [ X 1 | X 2 ], where X 1 and X are submatrices defined for C . Its transpose X T is totally unimodular since it satisfies the total unimodularity condi-tion in Sufficient Condition 1. By Lemma 4, X is totally unimodular; so does the matrix [ X | X  I k ] = C .
To characterize an optimal solution by simplex algorithms, consider the following. A polyhedron is a solution set of some finite number of linear equalities and inequalities. A bounded polyhedron is called a polytope . Let S  X  { 0 , 1 } be the feasible solution set for some problem instance of the ILP in Figure 1. The convex hull of S is the smallest convex set containing S . Since S is a finite set, the convex hull is a polytope; that is, the feasible region of LP1 is a polytope; so does LP2. A point x in a polytope P is an extreme point if and only if x is not a convex combination of any pair of distinct points in P . If an LP has optimal solutions, then one of the optimal solutions must be an extreme point due to linear property. Simplex algorithms are based on repeat-edly searching for a better extreme point on a polyhedron. Suppose that a polytope P is defined by c extreme point of P are integral if the following holds [7]. Sufficient Condition 2 (Integral Extreme Points). The coefficient matrix A is totally unimodular, and the vec-tors b l , b u , c l , and c u are integral.
 Proof of Theorem 3 . For a given matrix of all 1 X  X , LP2 finds vectors of all 1 X  X , due to empty set of constraints (7) in this case. For other cases, the coefficient matrix of LP2 is totally unimodular, by Lemma 5. All the constants involved in the constraints of LP2 are integral. This also includes an implicit lower bound 0 for each constraint (7) of LP2. Thus, the Sufficient Condition 2 is satisfied. It follows that every extreme point of a polytope for LP2 is integral. More specifi-cally, each of those integers is either 0 or 1 due to the bounds on variables in LP2. The feasible region of an instance for LP2 is a polytope, i.e., a bounded region. A simplex algo-rithm always finds an extreme point in the optima if the LP is feasible and bounded.
 We assume that an optimal solution for LP2 hereinafter has this binary property.
The main result of this section is summarized in the fol-lowing theorem.

Theorem 6. Let x 1 x T 2 be the binary rank-one approxi-mation by the algorithm LPIT for a binary matrix A . The Hamming distance for binary vectors p and q .
 The corollary below follows directly from this theorem.
Corollary 7. For a given rank-one binary matrix A , the algorithm LPIT finds a pair of vectors x 1 and x 2 such that x 1 x T 2 = A .
 Theorem 6 shows an error-bound on solutions by LPIT. Sev-eral definitions, relevant to its proof, are defined as follows: Definition 1. Given an optimal solution for LP2 by a simplex algorithm with an input matrix A  X  4. W opt = The objective value of S for LP2, and 5. W app = The objective value of a solution for MWP by the algorithm LPIT for the matrix A .

Proof. Note that variable z i,j = 1 for W i,j =  X  1 if and only if x 1 i + x 2 j = 2 at optimality. By Theorem 3, every variable in the solution equals 0 or 1. Using the objective function of LP2, W opt can be expressed, by Definition 1, as Proof. Let x 1 and x 2 be the two vectors found in Phase One of the algorithm LPIT. The objective value is x T 1 Wx by equation (2). After the completion of LPIT, we have W app  X  x T 1 Wx 2 due to the iterative procedure in Phase Two. Moreover, by the definition of MWP, the value can be expressed, by Definition 1, as
Lemma 10. W 1  X  1 2 input matrix for the algorithm LPIT.

Proof. Suppose that { z with weight matrix W of A . Since every entry in A is bi-nary and A i,j = 1 if and only if W i,j = 1 , by Definition 1, it follows that
Proof of Theorem 6 . The Hamming distance by an optimal approximation for A is min = min  X k A k 2 F  X  W opt by Proposition 2 = k A k 2 F  X  W 2  X  W (  X  )  X  W 1 by Lemma 8 = 1  X  1  X  1 = 1
Regularization is one of key techniques underlying many well-known data mining and machine learning methods, in-cluding support vector machines (SVM) and ridge regres-sion. In the regularization framework, the smoothness of solution functions is typically optimized along with the er-ror term and a regularization parameter is used to control the tradeoff between them. This generally results in the fol-lowing objective function: f = f err +  X f reg , where f err and  X  are the original error function, regularizer, and regu-larization parameter, respectively. Specifically, we propose to solve the following optimization problem: where A  X  { 0 , 1 } I 1  X  I 2 , x 1  X  { 0 , 1 } I 1 , and x The value matches between the given matrix A and the solution matrix x x T 2 . The nonnegative regularization parameter  X  in (11) controls the balance between the approximation error and the solution complexity. The problem at (1) is a special case of (11) when  X  = 0 .

It follows from (2) that an optimal solution to (11) can be found by solving the following maximization problem: max where U  X  X  X  (1 +  X  ) , 1  X   X  } I 1  X  I 2 is called the  X  -regularized weight (  X  -RW) matrix of A in this paper and is defined as U i,j = 1  X   X  , if A i,j = 1 , and U i,j =  X  (1 +  X  ), otherwise. The equivalent relation in (12) is due to the equality below, with 1 being a vector of all 1 X  X : x
Next, we present the proposed methods for exact and error-bounded solutions for regularized rank-one approxi-mation at (11). Our methodology is similar to the one in Section 3: the exact solutions can be found by integer lin-ear programming; a relaxed integer program, called ILP2 , is proposed for error-bounded solutions.
A 0-1 integer linear program, called ILP1 , for exact so-lutions of the problem at (12) is given in Figure 3. There are three types of binary variables x 1 i , x 2 j , and z ILP1 for 1  X  i  X  I 1 and 1  X  j  X  I 2 . Variables x 1 i and x represent the i -th and j -th entries of solution vectors x x 2 respectively. Variable z i,j is an auxiliary variable whose value is 1, if and only if both the values of x 1 i and x 1. The objective function at (12) can therefore be rewritten as
Constraints (13) and (14) enforce the relationship between z for any pair of i and j , z i,j = 1 if and only if x 1 i = 1 and x 2 j = 1 at optimality.

The corresponding variable z i,j for each positive A i,j is bounded above by 1 2 ( x 1 i + x 2 j ) at (13). Replacing every such z optimal value is not less than that of the ILP1. Figure 4 shows this resulting formulation ILP2. The constraint for A i,j = 1 becomes redundant and is removed from Figure 4. Since removing or relaxing a constraint does not make an objective value of the same instance worse, the proposition below follows:
Proposition 11. The optimal value at (12) is no more than that of ILP2 for the same problem instance.

The main result of the error bounded approximation is given as Theorem 12 (the proof follows similar techniques in Section 3 and is omitted).

Theorem 12. Let x 1 x T 2 be the solution matrix for reg-ularized rank-one approximation found by ILP2 for a bi-nary matrix A and a regularization parameter  X   X  R + . The cost vectors p and q .

Similar to ILP1 in Section 3, we relax the integral con-responding linear program (LP). The solution found by LP relaxation of ILP2 is feasible for ILP2 mainly because the coefficient matrix of ILP2 is totally unimodular [5]. That is, solving the LP relaxation.

The algorithm, called rLPIT for (regularized) binary rank-one approximation is similar to LPIT and it consists of two phases: ( i ) find an error-bounded solution, and ( ii ) apply the iterative procedure [13] to further improve the solution.
In Sections 3 and 4, we compute the binary matrix ap-proximation by solving liner programs. However, solving the LP for a large matrix is still computationally expensive, e.g., too many variables. In this section, we transform the problem into an instance of minimum s-t cut problem, which can be solved efficiently by finding maximum flows.
Consider the Generalized Independent Set Problem (GIS) [6]: The input of GIS is ( i ) an undirected graph G = ( V, E ), ( ii ) a nonnegative weight w ( v ) for each vertex v  X  V , and ( iii ) a nonnegative penalty p ( e ) for each edge e  X  E . The problem of GIS is to find a vertex subset S  X  V so as to max-imize the gain: Observation. ILP2 in Figure 4 defines an instance for GIS, and the corresponding graph is bipartite : 1. Every vertex x 1 i is in the same partite while a vertex 2. The weight of a vertex x 1 i (or x 2 j ) is 1  X   X  2 3. By constraint (16), z i,j = 1 if and only if x 1 i = 1 and 4. The penalty of each edge { x 1 i , x 2 j } is 1 +  X  . 5. The values of entries in x 1 and x 2 of a solution for It follows that the objective function of this instance for GIS is the same as the one in Figure 4. GIS is at least as dif-ficult as maximum independent set problem ; consequently, GIS is NP-hard. However, it has been shown that GIS for bipartite graphs can be solved exactly in polynomial time by transformation [6] to an instance of minimum st-cut prob-lem. Techniques in [6] implies the following transformation from problem at (11) to the maximum flow problem: Given A  X  X  0 , 1 } I 1  X  I 2 and  X   X  R + , the input is reduced to a net-work G = ( V, E ), a source s  X  V , a destination t  X  V , and a capacity c ( e ) for each link e  X  E as follows: Nodes: Create ( i ) a node set V 1 of size I 1 for rows in A , ( ii ) Links: For each node pair v i  X  V 1 and v j  X  V 2 , create a link Capacities: Denote s row ,i and s col ,j as the sums of the row i For example, Figure 5(c) depicts the network from the input matrix in Figure 5(a) and  X  = 1 2 . Figure 5: (a) A matrix consists of 4 data points and a regularization parameter 0.5. (b) The solution found by the minimum s-t cut. (c) The network transformed from the input in (a). (d) The residual network showing the cut and its assignment.

The network can be solved efficiently by any algorithm for finding maximum flows. The result partitions the node set V \{ s, t } into two sets S 1 and S 2 , where the destination t is reachable from every node v 2  X  S 2 in the final residual network but not for any node v 1  X  S 1 (page 928, Goldberg &amp; Tarjan [3]). Figure 5(d) shows the residual network as a result of applying a maximum flow algorithm on the network in Figure 5(c).
 The solution vectors x 1  X  { 0 , 1 } I 1 and x 2  X  { 0 , 1 } ILP2 is obtained from the two sets S 1 and S 2 as follows: 1. The value of x 1 i is 1 if and only if its corresponding 2. The value of x 2 j is 1 if and only if its corresponding 3. All the other entries have values of zero.
 Figure 5(b) shows the corresponding assignments and the solution for the example.
In this section, we present experimental results to eval-uate the proposed algorithms. We present comparisons for the results by the minimum s-t cut in Phase 1 (P1) of the rLPIT algorithm, the improvement by the iterative pro-cess in Phase (P2) of rLPIT , theoretical upper bounds, and running time of regularized rank-one approximations by the minimum s-t cut. All algorithms are implemented using C++ .
We use both the synthetic and real data in the experi-ments. We collected 3000 Drosophila gene expression pat-tern images of lateral view from three early developmen-tal stage ranges, i.e., 1-3, 4-6, 7-8, from the FlyExpress database 1 . The images from this database had gone through labor intensive segmentation, alignment, and enhancement. The collection contains 128  X  320 black-and-white images of staining patterns and were subsampled down to 64  X  160 to reduce the computational cost for the purpose of this experi-ment. Each image is represented as a vector of length 10240. We randomly generate binary matrices with the density of non-zeros around 30%, which is close to the average density of gene pattern expression images. We present the cases up to 1000  X  1000 in size, and 1000 cases for distinct size for non-regularization. For regularization, we varied the parameter  X  from 0.0 up to 1.0 with an increment of 0.1. The optimal objective values for ILP2 were used to provide bounds for our test cases, since the optimal value of (12) is no more than that of ILP2 as summarized in Proposition 11. For example, if the ratio by the minimum s-t cut to ILP2 is 1 . 5, then the exact ratio is 1 . 5 or less.

To figure out error ratios in acceptable time, i.e., avoiding integer programming, the linear programming (LP) relax-that an optimal solution for the LP is also an optimal so-lution for ILP2, mainly due to the total unimodularity of ILP2 X  X  coefficient matrix, assuming a simplex method for the LP is applied. The implementation was coded in C ++ with the LP solvers lpsolve 2 , which has a simplex-base algo-rithm. For large test cases, solving the corresponding LP X  X  becomes a challenge. For example, there are one million variables or more for the size of 1000  X  1000 . For such cases, the errors are compared with iterative heuristic, e.g., with random initial vectors proposed in [14].
Each chart of Figure 6 shows the histogram of an error-bound distribution for a given set of fixed-size test cases. The bound is a ratio of the Hamming distance by solution from a minimum s-t cut to the objective value of ILP2. In a case that a distance by ILP2 is zero, the ratio is 1 because the distance by a minimum s-t cut is also zero for all the test http://www.flyexpress.net http://sourceforge.net/projects/lpsolve (c) 50  X  50 test cases (c) 50  X  50 test cases cases. All the figures indicate that none of the test cases has any ratio exceeding 2. This is consistent with the result in Theorem 12.

Figure 6(a) depicts that the iterative procedure has negli-gible improvement on the solution quality in this case. Com-paring this result with those of Figures 6(b) and 6(c), we found that the role of iterative procedure is getting impor-tant as the input matrix gets complex. This is due to the lack of dominant patterns in randomly generated large ma-trices. In all those test cases, the mean of error bounds is steady in the range of 1.45 to 1.70 by the algorithm. The vector pair found by the minimum s-t cut serves good start-ing points for the iterative procedure in those test cases. Figure 6(d) shows the error ratios of the algorithm to the iterative heuristic given in [15].
Figure 7 shows the solution quality with varying regu-larization parameters for the test cases. In all those figures, none of the test cases has any error ratio exceeding the theo-retical bound 2 1+  X  , 0  X   X   X  1. The bounds are tight for some of the test cases. For highly complex matrices, the proposed algorithm obtains solutions with errors close to theoretical bounds, e.g., the results in Figures 7(b) and 7(c). The im-provement by the iterative procedure appears less persistent for regularization. This phenomenon is likely due to shorter Hamming distances of optimal solutions as penalty gets very high for complex solutions.
We use the gene expression images for evaluating the run-ning time of finding error-bounded approximations using minimum s-t cuts. We used the implementation of a maxi-mum flow algorithm given in [1], which is reported to have good average performance in the context of image applica-tions. In our test, there were two scenarios, each we fixed one of the dimensions while incrementally changed the other. Any of the fixed dimensions has a size of 1000. The regular-ization parameter was set to 0.5.

Figure 8(a) shows the running time for increasing the number of features. The running time is linear with respect to the change for our test cases. Figure 8(b) shows the run-ning time for increasing the number of data points. It is clear that solving a matrix with a higher dimension in data points is more costly by the algorithms, and such cost gets higher faster. However, we can always improve the perfor-mance by solving the transpose of such a matrix or simply switching the source and destination sides of the network.
A total of three trees, each for a stage range, were con-structed. Each tree has 1000 leaf nodes, the number of input images for a stage range. Extracted patterns of a tree were represented as an image. To illustrate the typical traits of trees built by the proposed approximation algorithm, Fig-ure 9 shows a result for a set of 40 input images from stage range 4-6. We observe that the shape of a tree relies on the regularization parameter  X  and the degree of variation of the input images. When  X  = 0 , i.e., without regularization as in [13, 15], both x 1 and x 2 tend to be either all zeros or all ones, which is undesirable for tree construction. In our experiments, we set  X  = 0 . 4. We can observe from the figure that images from the same branch share similar patterns.
In this paper, we study the problem of computing rank-one binary matrix approximations, which have been pre-viously used for subsampling, compression, and clustering. Specifically, we reformulate the rank-one binary matrix ap-proximation problem as a maximum weight problem, based on which we propose a linear program (LP) formulation, which is shown to achieve a guaranteed approximation error bound. We further extend the proposed formulations using the regularization technique. In addition, we show that the proposed approximate formulation can be transformed into an instance of minimum s-t cut problem, which can be solved efficiently by finding maximum flows. Our empirical evalu-ation shows the efficiency of the proposed algorithm based on minimum s-t cuts. Results also confirm the consistency and tightness of the established theoretical bounds.
As a sample application, we apply the proposed algo-rithm for the hierarchy construction and pattern discovery for Drosophila gene expression pattern images. We plan to examine the biological significance of the resulting hierarchy. We plan to apply the proposed algorithm to other applica-tions involving binary data. We are currently investigating how the solutions of the proposed algorithm depend on the regularization parameter as well as its estimation. Figure 9: The hierarchy and patterns for a set of 40 images from developmental stage range 4-6.
