
Evolutionary Clustering has emerged as an important research topic in recent literature of data mining, and so-lutions to this problem have found a wide spectrum of ap-plications, particularly in social network analysis. In this paper, based on the recent literature on Dirichlet processes, we have developed two different and specific models as so-lutions to this problem: DPChain and HDP-EVO. Both models substantially advance the literature on evolutionary clustering in the sense that not only they both perform bet-ter than the existing literature, but more importantly they are capable of automatically learning the cluster numbers and structures during the evolution. Extensive evaluations have demonstrated the effectiveness and promise of these models against the state-of-the-art literature.
Evolutionary clustering is a relatively new research topic in data mining. Evolutionary clustering refers to the sce-nario where a collection of data evolves over the time; at each time, the collection of the data has a number of clus-ters; when the collection of the data evolves from one time to another, new data items may join the collection and exist-ing data items may disappear; similarly, new clusters may appear and at the same time existing clusters may disap-pear. Consequently, both the data items and the clusters of the collection may change over the time, which poses a great challenge to the problem of evolutionary cluster-ing in comparison with the traditional clustering. On the other hand, solutions to the evolutionary clustering problem have found a wide spectrum of applications for trend de-velopment analysis, social network evolution analysis, and dynamic community development analysis. Potential and existing applications include daily news analysis to observe news focus change, blog analysis to observe community de-velopment, and scientific publications analysis to identify the new and hot research directions in a specific area. Due to these important applications, evolutionary clustering has recently become a very hot and focused research topic.
Statistically, each cluster is associated with a certain dis-tribution at each time. A solution to the evolutionary clus-tering problem is to make an inference to a sequence of dis-tributions from the data at different times.

A reasonable solution to the evolutionary clustering problem must have a clustering result consistent with the original data distribution. Consequently, the following two properties must be satisfied to reflect a reasonable evolu-tionary clustering problem: (1) The number of clusters as well as the clustering structures at different evolutionary times may change. (2) The clusters of the data between neighboring times should stay the same or have a smooth change; but after a long time, clusters may drift substan-tially.

In this paper, we propose a statistical approach to solv-ing the evolutionary clustering problem. We assume that the cluster structure at each time follows a mixture model of the clusters for the data collection at this time; clusters at dif-ferent times may share common clusters; further, these clus-ters evolve over the time and some may become more pop-ular while others may become outdated, making the clus-ter structures and the number of clusters change over the time. Consequently, we use Dirichlet Process (DP) [11] to model the evolutionary change of the clusters over the time. Specifically, we propose two Dirichlet process based mod-els as two different solutions to the evolutionary clustering problem: DPChain and HDP-EVO.

DPChain is based on the Dirichlet Process Mixture (DPM) model [2, 10], which automatically learns the num-ber of the clusters from the evolutionary data; in addition, the cluster mixture proportion information at different times is used to reflect a smooth cluster change over the time. HDP-EVO is developed based on the Hierarchical Dirichlet Process (HDP) model [21] with a set of common clusters on the top level of the hierarchy to explicitly address the clus-ter correspondence issue in order to solve the evolutionary clustering problem; the middle level is for the clusters at each different time, which are considered as the subsets of the top level clusters; the relationship between the top level clusters and the middle level clusters is obtained through the statistical inference under this model, resulting in explicitly addressing the cluster correspondence issue for the clusters at different times.

The specific contributions of this work are highlighted as follows: (1) We have applied the recent literature on Dirich-let process based statistical learning to solve the evolution-ary clustering problem by developing two specific models: DPChain and HDP-EVO as two different solutions. (2) Both models for evolutionary clustering substantially ad-vance the literature in the sense that they are capable of au-tomatically learning the number of clusters and the cluster structure at each time during the evolution, which makes these solutions practical in many evolutionary clustering applications. (3) We have demonstrated the superiority of these solutions to the existing state-of-the-art literature in both synthetic data and real Web daily news data for the evolutionary document clustering application.
Evolutionary Clustering is a recently emerging research topic in data mining. Due to its very short history, there is not much literature on this topic at this time.

Chakrabarti et al. in 2006 [7] were probably consid-ered as the first to address the evolutionary clustering prob-lem in the data mining literature. In their work, a general framework was proposed and two specific clustering algo-rithms within this framework were developed: evolution-ary k-means and evolutionary agglomerative hierarchical clustering. The framework attempted to combine the two properties of evolutionary clustering for the development of these two algorithms; one is the snapshot quality, which measures how well the current data fit the current cluster-ing; and other is the history quality, which measures how smooth the current clustering is with the previous cluster-ing.

Recently, Chi et al. [8] presented an evolutionary spectral clustering approach by incorporating the temporal smoothness constraint into the solution. In order to fit the current data well into the clustering but at the same time not to deviate the clustering from the history too dramatically, the temporal smoothness constraint is incorporated into the overall measure of the clustering quality. Based on the spec-tral clustering approach, two specific algorithms, PCM and PCQ, were proposed.

These two algorithms were developed by explicitly in-corporating the history clustering information into the exist-ing classic clustering algorithm, specifically, k-means, ag-glomerative hierarchical clustering, and spectral clustering approaches [16, 19]. While incorporating the history infor-mation into the evolutionary clustering certainly advances the literature on this topic, there is a very restrictive as-sumption in their work  X  it is assumed that the number of the clusters over the time stays the same. It is clear that in many applications of evolutionary clustering, this assump-tion is obviously violated.

Dirichlet Process [11] is a statistical model developed in the statistics literature to capture the distribution uncertain-ties in the space of probability measure. When a random measure is no longer a single distribution, but a mixture dis-tribution, DPM [2, 10] is used to extend DP. Statistically, the clustering problem indeed fits into a mixture model, making it natural to use DPM model.

More importantly, DPM allows an infinite number of mixture components, shedding the light on solving the clus-tering model selection problem. Sethuraman [18] gives a constructive definition of Dirichlet distribution for an arbi-trarily measurable base space. This stick-breaking construc-tion is very useful to model the weight of mixture compo-nents in the clustering mixture model. Besides the capabil-ity of learning the number of clusters from the data auto-matically, HDP model [21] is further developed for sharing the mixture components across different data collections, making it possible to capture the relationship between the clusters at different times.

Recently in the machine learning community, DP related models are developed and used to solve the clustering prob-lems such as document topic analysis [5, 21], image clus-tering [17], and video surveillance activity analysis [22]. Blei et al. [5] developed the Latent Dirichlet Allocation (LDA) model that automatically learns the clustering of top-ics given a document corpus. However, LDA assumes that the number of clusters is given in advance and is a paramet-ric constant.

Blei et al. [4] designed a family of time series probabilis-tic models to analyze the evolving topics at different times; they assumed a fixed number of topics and did not consider the clusters X  birth or death during the evolution. Griffiths et al. [12] studied the PANS proceedings by LDA model to identify  X  X ot topics X  and  X  X old topics X  by examining tempo-ral dynamics of the documents; they used Bayesian model selection to estimate the number of the topics. Wang et al. [23] presented an LDA-style topic model in which time is an observed continuous variable instead of a Markov dis-cretization assumption. This model is able to capture the trends of the temporal topic evolution; however, the num-ber of topics is still assumed fixed. Zhu et al. [25] further developed a time-sensitive Dirichlet process mixture model for clustering documents, which models the temporal corre-lations between instances. Nevertheless, a strong assump-tion was made that there is only one cluster and one docu-ment at each time, which is too restrictive to handle prob-lems with a collection of clusters and documents at a time. More recently, Xu et al. [24] proposed a statistical model HDP-HMM to provide a solution to evolutionary clusterng, which is able to learn the number of clusters and the cluster structure transitions during the evolution.
In the following text, boldface symbols are used to de-note vectors or matrices, and non-boldface symbols are used to denote scalar variables. Also for all the variables we have defined, adding a symbol  X  s either in the subscript or in the superscript to a defined variable means the whole scope the variable is defined for except for the item indicated as s .
The first model we propose is based on the DPM model [2, 10], which is called DPChain model in this paper. For DPChain model, we assume that at each time t a collection of data has K t clusters and each cluster is derived from a unique distribution. K t is unknown and is learned from the data. We denote N t as the number of the data items in this collection at time t .
Figure 1 illustrates the DPChain model. We use the in-dicator variable to represent the DPChain model. First we introduce the notations.  X  denotes the concentration param-eter for a Dirichlet distribution. H denotes the base measure of a Dirichlet distribution with the pdf as h . F denotes the distribution of the data with the pdf as f .  X  t,k denotes the parameter of cluster k of the data at time t . At time t ,  X  is a sample from distribution H , represented as a parameter of F .  X  t is the cluster mixutre proportion vector at time t .  X  t,k is the weight of the corresponding cluster k at time t . Con-sequently,  X  t is distributed as stick (  X  ) [18] which is de-scribed as follows.  X  Let z t,i be the cluster indicator at time t for data item i . z follows a multinomial distribution with parameter  X  t . Let x t,i denote data item i from the collection at time t . x is modeled as being generated from F with parameter  X  t,k by the assignment z t,i .
In evolutionary clustering, cluster k is smoothly changed from time t  X  1 to t . With this change of the clustering, the number of the data items in each cluster may also change. Consequently, the cluster mixture proportion is an indica-tor for the population of a cluster. In the classic DPM model,  X  t represents the cluster mixture. We extend the classic DPM model to the DPChain model by incorporating the temporal information into  X  t . With a cluster smooth change, more recent history has more influence on the cur-rent clustering than less recent history. Thus, a cluster with a higher mixture proportion at the present time is more likely to have a higher proportion at the next time. Hence, the cluster mixture at time t may be constructed as follows. where  X  is a smooth parameter.
 This relationship is further illustrated by an extended Chinese Restaurant Process (CRP) [3, 1]. We denote n t,k as the number of data items in cluster k at time t , and n  X  the number of data items belonging to cluster k except x t,i w t,k is the smooth prior weight for cluster k at the begin-ning of time t . According to (2), w t,k has the relationship to n  X ,k at the previous time  X  : Then, similar to CRP, the prior probability to sample a data item from cluster k given history assignment { z 1 ... z t and the other assignment at time t , z t,  X  i = z t \ z t,i follows. where n t  X  1 is the number of the data items at time t ex-cept for x t,i , and x t,i is considered as the last data item in the collection at time t . With (4), an existing cluster appears again with a probability proportional to w t,k + n  X  i t,k new cluster appears at the first time with a probability pro-portional to  X  . If at time t as well as the times before t ,the data of cluster k appear infrequently, cluster k has a rela-tively small weight to appear again in the next time, which leads to a higher probability of becoming death for cluster k . Consequently, this model has the capability to describe the birth or death of a cluster over the evolution. The data item generation process for DPChain model is listed as fol-lows. 1. Sample cluster parameter  X  t,k from the base measure 2. First, sample the cluster mixture vector  X  t from 3. At time t , sample the cluster assignment z t,i for data 4. Finally, a data item x t,i is generated from distribution
At each time t , the concentration parameter  X  may be different. In the sampling process, we just sample  X  from a Gamma Distribution at each iteration. For a more sophisti-cated model,  X  may be modelled as a random variable vary-ing with time, as the rate of generating a new cluster may change over the time. Given the DPChain model, we use Markov Chain Monte Carlo (MCMC) method [14] to sample the cluster assign-ment z t,i for each data item at time t . Specifically, follow-ing Gibbs Sampling [6], the aim is to sample the posterior cluster assignment z t,i , given the whole data collection at time t , the history assignment { z 1 ... z t  X  1 } , and other assignment z t,  X  i at the current time.

We denote x t,  X  i as all the data at time t except for x The posterior of the cluster assignment is determined by Bayes rule: p ( z p ( x where x k  X  i = { x t,j : z t,j = k, j = i } donates all the data at time t assigned to cluster k except for x t,i .
Since z t,i is conditionally indenpent of x t,  X  i given all the history assignment and the current time assignment except for x t,i ,weomit x t,  X  i at the second term of the right hand right hand side of (5), which is the conditional likelihood of x t,i on cluster k , given the other data associated with k and other cluster assignment.

If k is an existing cluster: f tribution of parameter  X  t,k given observation { x t,j : z k, j = i } .If F is conjugate to H , the posterior of  X  t,k still in the distribution family of H . Then we can integrate out  X  t,k to compute f  X  i k ( x t,i ) . Here we only consider the conjugate case because our experiments reported in this pa-per are based on this case. For the non-conjugate case, a similar inference method may be obtained [15].

For a new cluster k , it is equivalent to compute the marginal likelihood of x t,i by integrating out all the param-eters sampled from H .
Finally, the posterior cluster assignment in the conjugate case is given as: p ( z  X   X   X   X   X 
While DPChain model advances the existing literature on evolutionary clustering in the sense that it is capable of learning the cluster numbers over the time, this model fails to have an explicit representation on the cluster correspon-dence over the time. In order to explicitly capture the clus-ter correspondence between the data collections of different times, we further develop the HDP Evolutionary Clustering model, which we call HDP-EVO.
HDP-EVO model is illustrated in Figure 2. Again, we use the indicator variable representation to describe the HDP-EVO model. First, we introduce the notations.  X  is the concentration parameter of the Dirichlet distribution of  X  . Common clusters for all the collections at different times are shared with the global cluster set with mixture propor-tion vector  X  .  X  k is the parameter for a cluster with i.i.d. sampled from a distribution H . The clusters appearing at time t are a subset of the common clusters with a local cluster mixture parameter vector  X  t where  X  is the concentration parameter. At different times, a different  X  t shares the common global clusters which es-tablish the correspondence between the local clusters at dif-ferent times and the global clusters.

Similar to DPChain model, the mixture proportion of the clusters evolves over the time, favoring recent history. We assume again an exponential smooth transition: where  X  is a smooth parameter. We denote z t,i as the cluster assignment at time t for the data item x t,i , and follows a multinomial distribution of  X  t . Finally, x t,i is modeled as being drawn from the distribution F with the parameter  X  k under cluster k .
Now, the data generation process is described as follows. 1. The common global clusters X  parameter vector  X  is 2. Sample global cluster mixture proportion  X  from 3. At time t , first sample the local clusters X  mixture pro-4. z t,i , the assignment of the cluster for x t,i , is sampled 5. Finally, we sample x t,i from distribution F with pa-
Based on the above generation process, the cluster num-ber can be automatically learned through the inference from the data at each time. All the local clusters at different times are capable of establishing a correspondence relation-ship among themselves from the top level of the commonly shared global clusters. With the introduction of the expo-nentially weighted smoothness of the mixture proportion vector at different times, the cluster may smoothly evolve over the time.
The indicator variable representation of HDP-EVO di-rectly assigns clusters to data. In order to design the Gibbs sampling process for HDP-EVO, we further illustrate HDP-EVO model as a 2-level CRP.

Under the standard CRP model [3, 1], each table cor-responds to one cluster. Here, we further categorize the clusters into a higher level, global clusters that are com-monly shared across all data collections at different times, and the lower lever, local clusters, i.e., the tables of a Chi-nese Restaurant with data items sitting around, at each time. We use k to denote the k -th global cluster and use tab to de-note the tab -th local cluster (Figure 3).

At each time t , the data collection is modeled as be-ing generated from the local clusters with the param-eters {  X  t, 1 ,..., X  t,tab ,... } , each of which is sampled from the commonly shared global clusters with parameters {  X  1 ,..., X  k ,... } in the CRP style [3, 1]. We use tab t,i denote the table (i.e., the local cluster) at time t for x assign global cluster k to table tab , if all the data clustered into local cluster tab at time t are distributed with parameter  X  . We explicitly introduce k t,tab to represent this mapping relationship. Similarly, we introduce tab t,i to denote the mapping that x t,i is clustered into table tab at time t .Let n t,tab be the number of the data items at table tab at time for x t,i , and n t be the total number of the data items at time t .Let m t,k be the number of the tables at time t belonging to the global cluster k , m  X  tab t,k be number of the tables in cluster k except for tab , and m t be the total number of the tables at time t ,
Under the 2-level CRP, at time t , we first sam-ple which table tab x t,i belongs to, given the history { tab t, 1 ,...,tab t,i  X  1 } in which by the exchangeability x may be considered as the last data item at time t : where  X  is the concentration parameter. To ensure the smooth transition over the history, we also denote w t,k as the smooth prior weight for cluster k at time t . Thus, we have
Denoting K as the all the history global cluster assign-ment mapping up to time t inclusive, the likelihood of hav-ing the assignment mapping k t,tab is:  X   X   X   X   X  where  X  is the concentration parameter.
Again we use Gibbs Sampling [6] for the 2-level CRP for HDP-EVO inference. First, we specify how to assign x t,i (which may be considered as the last data item by the exchangability) to tab : p ( tab p ( tab ,
K )
For the second level CRP, We denote the conditional side of (13).

For an existing table tab which belongs to global cluster k t,tab , the conditional likelihood of x t,i given other data un-as that is Eq. (6) with cluster k replaced with k t,tab .
For a new table tab , we first sample the table from the global cluster, the conditional likelihood of x t,i under the cluster k becomes:  X   X   X   X   X  where f  X  i k ( x t,i ) under new cluster k is the marginal like-lihood for a new global cluster k from Eq. (7) with  X  t,k replaced with  X  k .

Finally, we sample x t,i from table tab as follows:
Similarly, to sample a table tab from a global cluster k , we have: where x t,tab denotes all the data belonging to table tab at time t , and x t,  X  tab = x t \ x t,tab denotes the remaining data except those in table tab .

We denote the second term of the right hand side of (16) all the data in table tab , given other tables X  data at time t , under cluster k .

For an existing global and new cluster k ,wehavethe likelihood :
Finally, we assign a table tab to a global cluster k as follows: p ( k  X   X   X   X   X  where tab t is the set of all the tables at time t .
For both models we have developed in this paper, there are hyperparameters that must be estimated. We use the EM method [9] to learn these parameters. Specifically, for DPChain, the hyperparameters are (  X ,  X  ) . According to (3),
Figure 3. The illustrated example of global and local cluster correspondence updating  X  results directly in updating w t,k . Consequently, we actually update the hyperparameters  X =(  X , w t,k ) . Following [10],  X  is sampled from the Gamma Distribution at each iteration in the Gibbs sampling in the E-step. In the M-step, similar to [25], we update w t,k by maximizing the cluster assignment likelihood. Suppose that, at an iteration, there are K clusters. Thus, the EM framework is as follows:  X  At time t , initialize parameters  X  and z  X  E-Step: Sample  X  from Gamma Distribution. Sample  X  M-Step: Update w  X  Iterate the E-Step and the M-Step until the EM con-For HDP-EVO, the hyperparameters are  X =(  X ,  X ,  X  ) , Similar parameter learning may be obtained using an EM again.
We have extensively evaluated the two models in com-parison with the state-of-the-art literature, the PCM and PCQ algorithms developed in [8]. For the experiments in text data evolutionary clustering, we have also evaluated the two models in comparison with LDA [5, 13] in addi-tion. The evaluations are performed in three datasets, a syn-thetic dataset, the 20 NewsGroups dataset, and a Google daily news dataset we have collected over a period of 5 con-tinuous days.
We have generated a synthetic dataset according to our assumption of the evoluationary data. At each time, the data are a collection of mixture models with the number of the clusters as an unknown prior; the data evolve over the time under a smooth transition. Specifically, in the dataset, we have 10 different data collections corresponding to 10 different times, with each collection according to the DPM model with 200 2-dimensional Gaussian distribution points. 10 Gaussian points in N ( 0 , 2I ) are set as the 10 global clus-ters X  mean parameters  X  ; then 200 Gaussian points within a cluster are sampled with this cluster X  X  mean parameter and deviation parameter sampling from N ( 0 , 0 . 2I ) , where identify matrix. At each time, part of the clusters are cho-sen from the previous collections, with a weight inversely proportional to their difference in time; other clusters are sampled from the multinomial distribution with the current mixture proportion vector, which is a sample from a sym-metric DP with parameter 0.1. Consequently, each time, we sample 200 2-dimensional data points from Gaussian distri-bution according to the corresponding cluster parameters  X  we have chosen at time t . Thus, new and existing clusters of Gaussian distribution appear at the coming times, according to their history. After the generation of such dataset, we ob-tain the number of the clusters and the cluster assignment as the ground truth. We intentionally generate different num-bers of the clusters at different times, as shown in Figure 6.

In the inference process, we tune the hyperparameters as follows. In each iteration, we use vague gamma pri-ors [10] to update  X  and  X  from  X (1 , 1) . Smoothing pa-rameter  X  (consequently w t as well) is updated according to (19). Figure 4 shows an example of the clustering re-sults between HDP-EVO and PCQ at time 8 for the syn-thetic data. Clearly, HDP-EVO has a much better perfor-mance than PCQ in this synthetic data. For a more system-atic evaluation on this synthetic dataset, we use NMI (Nor-malized Mutual Information) [20] to qantitatively compare the clustering performances among all the four algorithms (DPChain, HDP-EVO, PCM, and PCQ). Figure 5 docu-ments the performance comparison. From this figure, the average NMI values across the 10 times for DPChain and HDP-EVO are 0.74 and 0.85, respectively, while those for PCQ and PCM are 0.70 and 0.71, respectively. DPChain works worse than HDP-EVO for the synthetic data. The reason is that DPChain model is unable to accurately cap-ture the cluster correspondence among the data collections across the time in this case, but still performs better than PCQ and PCM. Since one of the advantages of the two pro-
Figure 5. The NMI performance compari-son of the four algorithms on the synthetic dataset posed models is to be able to learn the number of of the clus-ters and the clustering structures during the evolution, we report this performance for the two models on this synthetic dataset in Figure 6. Here, we define the expected number of the clusters at each time as the average number of the clus-ters in all the posterior sampling iterations after the burn-in period. Thus, these numbers are not necessarily integers. Clearly, both models are able to learn the cluster numbers, with HDP-EVO better in performance than DPChain. Since both PCQ and PCM do not have this capability, they are not included in this evaluation.
In order to demonstrate and evaluate the proposed mod-els on a real dataset, we construct a real dataset based on a subset of the 20 Newsgroups data 1 . We intentionally set the number of the clusters at each time the same num-ber to accommodate the comparing algorithms PCQ and PCM which have this assumption of the same cluster num-ber over the evolution. In order to compare the text clus-
Figure 6. The cluster number learning perfor-mance of the two proposed models on the synthetic dataset tering capability of LDA [5, 13] with a known topic num-ber, we here set the topic number for LDA at each time collection as the ground truth 10. Consequently, we se-lect 10 clusters (i.e., topics) from the dataset (alt.atheism, comp.graphics, rec.autos, rec.sport.baseball, sci.crypt, sci.electronics, sci.med, sci.space, soc.religion.christian, talk.politics.mideast), with each having 100 documents. To  X  X imulate X  the corresponding 5 different times, we then split the dataset into 5 different collections, each of which has 20 documents randomly selected from each clusters. Conse-quently, each collection at a time has 10 topics to generate words. All the documents are preprocessed using the stan-dard text processing techniques for removing the stop words and stemming the remaining words.

To apply the DPChain and HDP-EVO models, a sym-metric Dirichlet distribution is used with the parameter 0.2 for the prior base distribution H . In each iteration, we up-date  X  and  X  from the gamma priors  X (0 . 1 , 0 . 1) ,  X  (or from (19). For LDA,  X  is set 0.1 and the prior distribution of the topics on the words is a symmetric Dirichlet distribution with concentration parameter 1. Since LDA only works for one data collection with a known cluster number, in order
Figure 7. The NMI performance comparison of the five algorithms on the 20 Newsgroups dataset to compare with LDA, we explicitly apply LDA to the data collection with the ground truth cluster number as input at each time.

Figure 7 reports the overall performance comparison among all the five methods using NMI metric again. Clearly both proposed models substantially outperform PCQ, PCM, and LDA almost at all the times. HDP-EVO has a better performance than DPChain except at time 1 where there is no history information. Figure 8 further reports the perfor-mance on learning the cluster numbers at different times for the two proposed models. Both models have a reasonble performance in automatically learning the cluster number at each time in comparison with the ground truth. Again, HDP-EVO has a better performance than DPChain.

In order to truly demonstrate the performance of the pro-posed models in comparison with the state-of-the-art liter-ature on a real evolutionary clustering scenario, we have manually collected Google News articles for a continuous window of five days (Feb. 10 -14, 2008) where both the data items (i.e., words in the articles) and the clusters (i.e., the news topics) evolve over the time. We select a series number of clusters (ground truth in Table 1) at each day to reflect the evolving process of the clusters. We select 10 documents for each cluster everyday. Again, in order to compare the text clustering capability of LDA [5, 13] with a known topic number, we use the ground truth cluster num-ber at each time as the input to LDA. The parameter tun-ing process is similar to that in the experiment using the 20 newsgroup dataset.

Figure 9 reports the NMI based performance evaluations among the five algorithms. Again, both proposed methods substantially outperform PCQ, PCM, and LDA in average, and HDP-EVO has a better performance than DPChain, ex-cept for at time 1 where there is no history information; PCQ and PCM fail completely in most of the cases as they assume that the number of the clusters remains the same during the evolution, which is not true in this scenario.
Figure 10 further reports the performance on learning
Figure 8. Cluster number learning perfor-mance of the two models on the 20 News-groups dataset Table 1. Ground Truth of Google News Dataset Num. Documents 50 60 50 60 60 the cluster numbers for different times for the two proposed models. Again, HDP-EVO has a much better performance than DPChain even though both methods are able to learn the cluster numbers automatically.
In this paper, we have addressed the evolutionary clus-tering problem. Based on the recent literature on DP based models, we have developed two separate models as two dif-ferent solutions to this problem: DPChain and HDP-EVO. Both models substantially advance the evolutionary cluster-ing literature in the sense that they not only perform better
Figure 9. The NMI performance comparison for all the five algorithms on the Google News dataset than the existing literature, but also are able to automati-cally learn the dynamic cluster numbers and the dynamic clustering structures during the evolution, which is a com-mon scenario in many real evolutionary clustering applica-tions. Extensive evaluations demonstrate the effectiveness of these models as well as their promise in comparison with the state-of-the-art literature. This work is supported in part by NSF (IIS-0535162 and IIS-0812114). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.
Figure 10. The cluster number learning per-formance of the two models on the Google
News dataset
