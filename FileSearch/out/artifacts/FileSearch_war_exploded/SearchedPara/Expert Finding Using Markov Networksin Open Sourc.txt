 A requirement for a system defines what this system should achieve to meet the expectations of the stakeholders who depend on it. In Requirements Engineering (RE), researchers deal with requirement-related issues such as eliciting, modelling and analysing, documenting and checking that the requirements are fulfilled [2,9]. Spe-cific difficulties in this area are, for exam ple, the huge amount of stakeholders to deal with, the stakeholders X  heterogeneity and distribution, the difficulty to express needs and solve their conflicts [16]. Although methodologies exist to support the analyst in dealing with the full RE process, a broad mastering is generally infeasible for a single person [4,8] and makes RE processes human-and knowledge-intensive [1,12]. These difficulties show the need to support the management of the information about the re-quirements in an efficient and customised way. One such way is to rely on available stakeholders considered as experts to provide reliable information or to analyse the in-formation provided by others.

Aiming at recommending stakeholders, two main approaches have been considered in requirements elicitation: forum-based approaches, which rely on the contribution of stakeholders in forums to evaluate their kno wledge [1], and social network-based ap-proaches, which exploit relationships between stakeholders to evaluate them relatively to the others [11]. However, while these two approaches show interesting results in their specific contexts, they are not designed to exploit the information provided by a hybrid context like an Open Source Software (OSS) project. In such a context, a large community of anonymous stakeholders prov ide few relations between each others and OSS-related companies can participate through some representatives only.

In this paper, we propose a novel approach exploiting concepts borrowed from both forum-and social network-based works to fill this gap. In particular, we show how they relate to two complementary perspectives to evaluate expertises, that we call content-based and social-based perspectives, which justifies their use in a unified way. While this approach could be considered in a broader scope than RE, we mainly inspire from works and build on concepts used in the RE field, justifying the scope of this paper. In our approach, we basically reuse the concept of role provided in social networks, the concepts of topic and term provided in forum-based works, and the concept of stake-holder common in both (and more broadly in RE). By relating instances of these con-cepts depending on evidences extracted from available data sources, we build a new model where specific stakeholders are relat ed to instances of the o ther concepts, al-lowing us to evaluate their expertise. We translate this model into a Markov network, which allows us to produce inferences based on the modelled expertise, in order to rank a stakeholder based on selected topics or roles for instance.

In the following, Section 2 provides an overview of the state of the art in expert finding and stakeholders recommendation in RE, focusing particularly on the two ap-proaches mentioned previously. Then, we highlight the main limitations we want to tackle in Section 3 and describe how we do so in Section 4. We present in Section 5 two preliminary experiments, one with a small, illustrative example where our approach has been successfully applied, and another on an existing OSS project. Finally, we highlight the limits of our results and discuss how our approach can be improved in Section 6 be-fore to conclude. 2.1 Expert and Expertise By looking at different d ictionary definitions of expert , we can identify a broad agree-ment on the concept among dictionaries like the Collins 1 , Oxford 2 , Cambridge 3 and Merriam-Webster 4 dictionaries. Considering the last one, more precise, you can be iden-tified as an expert by  X  X aving or showing special skill or knowledge because of what you have been taught or what you have experienced X . This definition shows two per-spectives: when one has special skills or knowledge, whether people assess them or not, and when one shows special skills or knowledge, whether he actually has them or not. Ericsson [5] investigates more deeply three criteria (p. 14): a lengthy domain-related experience , based on evidences that one has extended knowledge ; a reproducibly su-perior performance , based on evidences that one has extended skills ; a social criteria, where a community agree on the status of expert that one could have. We retrieve the two former criteria in the expertise notion provided by Sonnentag et al. [5] (p. 375) with the long experience and the high performance . These two criteria are specialisations of the  X  has  X  definition, that we will call content-based perspective, while the social crite-ria represents the  X  shows  X  definition, that we will call social-based perspective. Several works already provide approaches to model expertises and retrieve experts. Pavel and Djoerd [17] exploit documents produced by people to build a language model for each person and infer to which extent thi s person has contributed to the document, which helps to evaluate the expertise of this person in the topics related to the document. Similarly, Mockus and Herbsleb [14] tackle the expert finding problem in collaborative software engineering, where the amount of code written in a piece of a software appears as a good evidence of his or her expertise i n this piece. Taking a m ore social point of view, Zhang et al. [20] compare several algorithms used to retrieve experts using social networks built from forums of online communities, identifying askers and repliers and exploiting evaluations of the replies provided by participants. Finally, Karimzadehgan et al. [7] exploit at the same time the organisational relationships between employees of a company and the content of e-mails they have sent in mailing lists. This makes it an hybrid solution to expert finding and the closest work to our approach, to the best of our knowledge. All these works can be classified as taking a content-based perspective [14,17], a social-bas ed perspective [20] or both [7]. 2.2 Stakeholder Recommendation in RE A recommendation system (RS) is a software application which provides items esti-mated to be valuable for a given user task in a given context. RSs have been widely used in e-commerce to provide users with personalised product, content or service rec-ommendations (e.g. Amazon product recommendation, MovieLens movie recommen-dation) [12]. While a few works consider the expert finding problem in RE [18], by generalising to stakeholder recommendations one can find works and literature reviews comparing them [15]. Relying on these re views, we can identify two main approaches that we can relate to the content-based and so cial-based perspectives identified so far.
A first approach comes from Castro-Herrera et al. [1], where the participation of the stakeholders in a forum is exploited to evaluate their knowledge on different topics. Since several threads can be related to the same topic or one thread can mix several topics, they cluster the messages by topic depending on their common terms (resulting in abstract topics represented as vectors of terms). Then, looking at the authors of the messages, the stakeholders are related to the t opics in which they participate. The result is that other stakeholders can be recommended to participate in a new topic by identi-fying its similarity with already existing ones. From the expert finding point of view, we can see this approach as a way to exploit the knowledge provided by the stake-holders through their contribu tions to identify their topics of expertise, illustrating the content-based perspective.

In a second approach, Lim et al. have worked on StakeNet [11] for the aim to priori-tise the requirements to implement dependi ng on how the stakeholders rate them. For this aim, starting from a reduced set of well-identified stakeholders, each of them sug-gests people that he or she assumes to have some influence on the project. A role , like student, security guard or director, and a level of salience , a value on a scale between 1 and 5, are provided to describe how the suggested stakeholder influences the project. Based on these suggestions, a social network is built and classical measures are ap-plied to evaluate the global influence of each stakeholder. From th e expert finding point of view, we can see this approach as a way to evaluate the expertise of a stakeholder by aggregating the suggestions of ot her stakeholders, illustrating the social-based per-spective. In the RE literature, we can find two approaches illustrating well the content-based and social-based perspective separately. Ho wever, due to the complementarity of these perspectives, a hybrid context needs to consider both of them. For instance, in the con-text of OSS projects, it is usual to use forums where the community can exchange ideas and discuss about issues or answer questions from newcomers [10], supporting the forum-based approach and its content-ba sed perspective. However, companies pro-viding OSS-related services (e.g. integration, adaptation and training) can be involved in these communities, with only a few members (e.g. representatives) actually partici-pating in the forum, leading to have a whole set of stakeholders ignored. On the other way, companies can exploit the roles of t heir employees and the feedback from their co-workers to identify who are the relevant people to contact for specific issues [11,19], supporting the social-based perspective. But when considering the participants of a fo-rum or a mailing list, where several thousands of anonymous people can join and leave at any time, personal suggestions of other stakeholders is unable to cover the whole pic-ture. Especially when indirect evaluations, such as message evaluation, is not available, like in mailing lists.

Consequently, we aim at improving expert finding in RE by designing a more com-prehensive approach, also integrating the c ontent-based and social-based perspectives already developed in the current state of the art. To do so, we provide a new model which reuses the concepts exploite d in the existing approaches, namely stakeholders , roles , topics and terms , and relate them based on evidences extracted from available sources of data. Then, we use Markov networks (MN), a technique computing proba-bilities based on graphical models (probabilistic models based on graphs), to infer the probability for each stakeholder to have s ome expertise and build corresponding rank-ings. Karimzadehgan et al. [7] also provides an hybrid solution which considers topics and terms in a probabilistic way, but we additionally consider roles in our model, while they consider social relationships only as a way to post-process the probabilities. An-other main difference is that, using MNs, we can adapt the query to the needs of the user, such as considering several topics or integrate specific terms and roles, while the inference technique in their approach is based on a single topic. 4.1 Concepts and Relations As we aim to recommend experts, we have to consider the people who will be recom-mended. In RE, the people involved in a project are usually called stakeholders and, because we consider that any person involved in a project is a potential expert to work with, we will use the same term in our approach. Each stakeholder can have one or several roles , such as being a developer or a manager in a company, but also being a contributor in the forum of an OSS (more sp ecific ones can be considered). Each stake-holder can also know about some topics , such as security, community management, in-terface or specific features of the OSS. Going f urther, we can see that each stakeholder uses terms , whether it is in his contributions in the forum or in official documents he redacts as employee of a company involved in an OSS.

At this point, we have stakeholders who are related to roles, topics and terms. In our approach, we go further by exploiting the fact that knowing about a topic, like interface , implies generally to know some terms related to this topic, like interface (the name of the topic itself), button , screen and so on. In the same way, having a specific role, like developer , implies generally to know about some specific topics, like interface and programming , and to use specific terms. We exploit all these relations in our model to describe the expertises of each stakeholder.

Once all the concepts we consider and their relations have been presented, we have to consider the outcomes we build from them. First of all, we define an expert using a relative point of view: being more expert than another person means having more expertise compared to this person. This definition takes the side of relative experts rather than of absolute ones, as described by Chi [5] (chapter 2), with the latter considering people above a threshold to be experts even if nobody reaches this threshold in the considered community. For the notion of expertise , we use the definition provided by the Oxford Dictionaries by considering the expert knowledge or skill in a particular field, thus the topics she knows, the terms she uses, but also the roles she has, which supports the presence of both knowledge an d skills. Consequently, someone having more expertise than someone else in a particular field is considered as more expert in this specific field, and similarly someone ranked as more expert in a field is assumed to have more expertise.

All the concepts introduced here are descr ibed in Figure 1. The relations are directed to clarify their interpretation, but we consider the relations in both directions (e.g. a term is related to a topic as well as the topic is related to the term). 4.2 Model In order to model the experts and their expertises, we use a weighted graph representing the instances of the concepts and relations previously defined. We have a set of stake-holders S ,asetofroles R , a set of topics T and a set of terms C which correspond to the nodes of the graph, and a weighted e dge for each relation between these nodes. Basically, each stakeholder in S is related to all elements in R , T and C , each role in R is related to all elements in S , T and C and equivalently for each topic in T and each term in C , forming a complete 4-partite graph, as shown in Figure 2. The weight of an edge represents the amount of evidences supporting the corresponding relation. For instance if we have no evidence that a stakeholder s  X  S knows about a topic t  X  T , these nodes are related by an edge with a zero weight written as a tuple s, t, 0 .Having the tuples s 1 ,t, 5 and s 2 ,t, 10 describes two relations showing that we have twice the amount of evidence that s 2 knows about the topic t compared to s 1 .
The actual value of the weight depends on the interpretation of evidence .Limet al. [11], in their social network, use the salience elicited from the stakeholders to weight their edges, while Castro-Herrera et al. [1] exploit the frequencies of appearance of terms and normalise them in vectors. Both these approaches as well as others can be exploited, with the main challenge being to have consistent weights depending on the sources used to retrieve them. Considering the technique we use, described in Subsec-tion 4.3, the inference is independent on the scale, so having the weights 5 and 10, or respectively 1 and 2, have no influence on the results. Consequently, any arbitrary scale can be chosen for a given category of weight, and the remaining challenge is to merge weights representing different categories, like merging salience and frequencies. This is still an open problem in our approach, thus we consider that the weights are already consistent and can be simply added to have the total amount of evidences. 4.3 Expert Ranking Based on Markov Networks Before to introduce the technique we use to build expert rankings, we introduce some basic notions of the MN, also called Markov random field. A random variable is a variable having several possible states, each with a specific probability, such as a binary random variable x having a state in V x = { ,  X } with P ( x = )=0 . 8 and P ( x =  X  )=0 . 2 . By representing the variables as nodes in a graph and by linking them, we can identify for each complete sub-graph contai ned in this graph a set of fully-connected variables called a clique . On each clique g = { x 1 , ..., x n } ,where x i can take any state in V i , we can define a potential function f g : V 1  X  ...  X  V n  X  R + which returns a value based on the state of the variables in the clique. Finally, a MN N =( X,F ) is defined via a set of random variables, X = { x 1 , ..., x n } , and a set of potential functions over cliques in X , F = { f 1 , ..., f m } .

Notice that we do not explicit the links between the nodes in the definitions of the network, as they are already defined through the cliques (each clique implies that we have all the possible links between the variables concerned). In the specific case where all the potential functions are defined on pairs of nodes, the MN represents a weighted graph, where the weights of the links depe nd on the states of the nodes. In this paper, we translate a tuple n 1 ,n 2 ,w in a potential function f such as f ( n 1 =  X  ,n 2 =  X  )= f ( n 1 =  X  ,n 2 = )= f ( n 1 = ,n 2 =  X  )=0 and f ( n 1 = ,n 2 = )= w (we consider other types of potential functions for future works). Consequently, we can represent our model as a MN, where the nodes are represented by binary random variables and the weighted edges are translated into potential functions, building a MN involving a lot of loops due to the completen ess of the 4-partite graph used. The state of each node tells whether the node (stakeholde r, role, topic or term) is relevant or not (respectively or  X  ), and, in the specific case of stakeholders, meaning that he or she is an expert or not.

The aim of MNs is to comput e probabilities based on th ese random variables and potential functions. Considering the nodes X = { x 1 , ..., x n } ,where x i is assigned a state v i  X  V i , and each clique g i assigned to a potential function f i , the probability to be in a specific state  X  = { v 1 , ..., v n } is computed as P (  X  )= m i =1 f i ( g i ) Z where Z =  X  m i =1 f i ( g i ) is the normalisation factor which allows to build a probability (  X  P (  X  )=1 ). If we are interested in a subset of variables, it is possible to compute a partial probability by summing all the cases for the remaining variables, for instance x = { x 1 ,x 2 } ,P ( x 1 = )= P ( x 1 = ,x 2 = )+ P ( x 1 = ,x 2 =  X  ) . Finally, assuming that some variables have a given state, we can compute a conditional proba-bility, where the computation is done only with the configurations where the given states hold (including the normalisation factor). For instance, with the combinations having x 2 = :
An interesting property is its scale independence : if we apply a scaling factor  X  on the potential functions f i =  X .f i and compute the probability P basedonthese functions, we can see that we get the same results: This property is of particular importance becau se it reduces the problem of data merging by allowing us to choose any arbitrary scale as a reference and to re-scale data extracted with different scales to this reference. The r emaining problem is to consider the trust or reliability of the data, which is still an open problem in our approach.

Using this technique on our model, we build a query based on which kind of expert is searched, for instance someone knowing about the topics t security and t cryptography . These topics provide the condition we want to match, thus the probability for a stake-holder s to be an expert is P ( s = | t security = ,t cryptography = ) . By computing these probabilities for all the stakeholders, we are able to rank them from the most to the least probable expert on the corresponding topics. It is possible to combine as much topics as wanted for the query, as well a s other elements like roles and terms. 4.4 Recommendation Process In order to recommend stakeholders as experts, we need to build our model based on sources of data , from which we should be able to retrieve all the elements used as nodes in our graph-based models (stakeholders, roles, topics and terms) and their weighted re-lations. These sources can be document-based, like forums, e-mails or reports, or other models, like goal-models or social networks built from social recommendations. Based on these sources, the necessary extractors have to be designed: a node extractor ,which retrieves the nodes, and a relation extractor , which retrieves the weighted relations be-tween these nodes (e.g. algorithms 1 and 2 in Section 5). With these extractors and the sources as inputs, we first extract all the nodes using the node extractors on each source of data, before to extract all the relations using the relation extractors. We split the ex-traction process because we do not make any a ssumption on which source will provide the relevant nodes and relations and in which order they will be parsed: by extracting the nodes first, we ensure that the relation extraction step will consider all the relevant nodes. After each extraction step we aggregate t he elements extracted from the different sources: a simple union for the nodes and a merging of the similar relations by summing their weights, merging from instance s, t, 2 and s, t, 3 into s, t, 5 .
 After the nodes and relations are extracted, we can build the MN, as described in Subsection 4.3, and query it. To build the query, the properties searched for the experts to recommend (having some roles, knowing about some topics or using some terms) should be present in the network nodes. For instance, if the network contains a topic security , it is possible to query for an expert in this topic, possibly combining it with other topics, but also with roles and terms. If the element is not present, it cannot be queried and an equivalent need to be found, e.g. cryptography , which is in the network. In our approach, when we look for an expert in a topic which is not in the network, we replace this topic by the corresponding term if it exists, otherwise we ignore it. Notice that querying for an expert with a given role does not mean that only people having this role will be considered (it is not a filtering function), but that people being more related to this role (directly or indirectly, as described in the model) will be considered as more experts.

Once the MN and the query Q = { x 1 , ...x q } are built, the probability of each stake-holder s  X  S to be an expert is computed as a conditional probability based on the query P ( s = | x 1 = , ..., x q = ) . The stakeholders are then ranked by decreasing order of probability to infer the ranking of recommendation. The recommendation can be enriched with the probabilities to provide an evaluation of the recommendations, the first two as potential experts because of their high probability. An important remark is that, as we consider relative expertise, the main information provided by our rank-ings is not which rank is assigned to which stakeholder, but which stakeholder is ranked higher or lower than another. In particular, the rankings ( A, B, C, D ) and ( D,A,B,C ) fully agree on ( A, B, C ) and disagree on the ordering of D compared to the others. By considering rank comparisons, these two rankings are completely disagreeing, which is not our interpretation here. Moreover, having a partial ordering (several stakeholders at the same rank) leads to have less informa tive rankings, because no order is provided between some stakeholders and the interpretation of lack of information to rank them is more natural than a strictly equal expertise.
 The complete process is illustrated in Figure 3.
 4.5 Supporting Tool The approach has been implemented in Java a nd external libraries have been used to extract the data from the sources and compute the MN. We retrieve nouns to identify terms and topics using the software GATE [3], a free and open source Java software to manage text processing with natural languages. It was chosen because it appears as a reference regarding natural language processing, aggregating well known tools like Lucene and WordNet and providing a complete extraction process. The MN is built and the queries are evaluated using libDAI [6], a free and open source C++ library made to compute graphical models. It was chosen becau se, among all the tools or libraries able to compute graphical models like Bayesian networks and MNs, it was one of the few able to compute MNs in particular and t he only one explicitly supporting loops, which is a major constraint considering our loop-intensive models described in Subsection 4.3. The global architecture of our tool is presented in Figure 4.
 5.1 Illustrative Example In order to stress our approach in a controlled situation, we have lead an experiment with 3 participants Alice, Bob and Carla, discussing via e-mail about 2 cooking-related threads : Asian food and European dessert (the names and topics have been renamed to preserve anonymity). The experiment has started just after its presentation to the participants and has lasted 2 days during which 30 messages were exchanged, with 8 contributions from Alice (4 for Asian food, 4 for European dessert), 9 from Bob (4 for Asian food, 5 for European dessert) and 13 from Carla (6 for Asian food, 7 for European dessert). To build a gold standard, the participants were asked to fill a form for each discussion after the experiment, asking for their level of knowledge in the discussion (newbie, advanced, expert) and the most knowledgeable participant from their point of view. Bob and Alice were identified as experts in European dessert while Carla was the expert in Asian food.

In order to build our model, we have designed a node extractor and a relation ex-tractor which fit to our source of data: the set of e-mails exchanged. The node extractor uses Algorithm 1 to consider an authors as a stakeholder , the nouns in the subject of the Algorithm 1. Node extractor for e-mails.
 Algorithm 2. Relation extractor for e-mails.
 e-mails as topics , and the nouns in the body as terms . We did not consider roles in this experiment, but the MN does not differentiate the types of nodes: using roles in place of topics for instance, assuming the wei ghts are the same, leads to the exactly same result. Thus, while they should be considered for a proper validation, we claim that it is not critical for our preliminary experiments. The relation extractor uses Algorithm 2 to relate the terms in the body and the topics in the subject to the author, as well as the terms and topics together.

The extraction process has identified 3 stakeholders, 4 topics, 293 terms and 2063 re-lations, building a MN of 300 nodes and 2063 functions. The 4 topics include the 2 more than the expected ones because the participants were asked to launch the discussions by themselves, letting them formulate the discussion subjects. The results, displayed in Table 1, consider an empty query aiming to ask for expert without specifying any constraint (all the nodes in the MN can have any state), while the query with  X  X uro-pean dessert X  or  X  X sian food X  implies to ask for experts in the corresponding topics (restricting these topic nodes to the state ). Alice and Bob appear as more expert than Carla on European dessert, while it is the opposite on Asian food, as shown by our gold standard. 5.2 OSS-Based Experiment: XWiki While the previous experiment allows to have a better control on the data, another ex-periment has been run on an OSS project to fit better to the contexts targeted by this approach. Our tool has been applied on the mailing list of the XWiki 5 OSS commu-nity, which involves also a company selling support and training on this OSS. We have used the mailing list archives 6 from January to May 2013, retrieving 805 e-mails in 255 threads. We did not use roles in this preliminary experiment, but we plan to do so in fu-ture works by exploiting some data available from the XWiki community, as discussed in Section 6.

In order to build our model, we have used the Algorithm 1 and Algorithm 2 to extract the nodes and relations from the e-mails. An additional effort has been made to clean the data, especially to identify unique authors by aggregating different e-mail addresses for similar names of author, and to remove noise in the body of the e-mails like quotations. However, this process still need to be impr oved because some noise, like huge source code excerpts, is removed manually by forbidding special terms which appear in this noise. In order to make the computation tractable, we systematically remove the nodes having the smallest total weight (i.e. summing all the weights of its relations) and their relations. We do so in an iterative way until we reach a targeted configuration (e.g. 10 stakeholders, 10 topics and 100 terms).

The extraction process has identified 120 stakeholders, 216 topics, 4854 terms and 75470 relations, and different reduction policies and potential functions have been ap-plied to build the MN. Some preliminary experiments have shown that we are able to build a ranking from this dataset and we were able to assess the coherency of some results by having obvious experts like main contributors generally highly ranked, and participants of specific discussions highly ranked for the topics of their discussions. We did not build a proper gold standard, but we plan as future work to build one based on the rankings provided by some requirement analysts. For instance, by reducing the network to 5 stakeholders, 10 topics and 100 terms, we come with two committers, committer 1 (a main committer) and committer 2 , and three other forum participants, participant 1 , participant 2 and participant 3 . During the time span considered, commit-ter 1 , committer 2 and participant 1 have discussed about data migration issues during an XWiki upgrade on the forum. By querying the MN on  X  X ata migration X , which are topics available after the reduction process, these 3 stakeholders are generally ranked as the top 3. More investigation is needed to improve and assess the validity of our approach in this context, but these preliminary experiments provide a good support. The illustrative example and the OSS-based experiment suffer a lot of limitations due to, respectively, their restricted context and preliminary state. However, the aim of this pa-per is to present our approach and to illustrate it and show potential benefits by exploit-ing the results we were able to get from these e xperiments, saving the proper validation for future work. In particular, the OSS-based experiment provides a relevant context for our purpose but, due to the amount of noise (e.g. systematic quotations, huge source code excerpts) and the difficulty to remove it systematically, we need to use more ad-vanced techniques to improve our results. Moreover, while the presented experiments do not use any role, the OSS-based experiment provide some sources of data that we can use to retrieve and relate them, such as a Hall of Fame which describes specific types of committers and contributors, and organisational models which describe the different types of actors involved.

Looking at the approach, several points can be discussed. First of all, a MN compu-tation is not scalable due to the computation of the full graph. We can see it particularly well in the OSS experiment where the reduction of the network was mandatory to use the libDAI library. Another way to compute our model could be to compute part of the MN in a smarter way, using optimisation techniques, or to use other techniques which are more local like social network measures (e.g. PageRank, degree centrality) or search based techniques (e.g. hill climbing, genetic algorithms). Another point is the lack of re-lation between two nodes of the same type in our model: we could consider for instance that topics are co-related, such as  X  X ecurity X  and  X  X ryptography X , or two stakeholders working in the same office or on the same OSS module are related. We can also dis-cuss the querying process, where asking for an expert in a specific role does not mean that only people having this role will be ranked due to the probabilistic property of our approach. Such filtering behaviour could be considered to improve its adaptivity, for in-stance weighting the elements of the query to give them some importance. Finally, we can discuss the interpretation of the probabilities used to infer the ranking, especially their proximity leading to a probable lack of robustness, or the inability to identify a clear threshold to differentiate actual experts from novices. We are currently looking at other potential functions and information to exploit in our data to tackle this problem. We can also consider related works to improve or extend our approach. For instance, Massa and Avesani [13] describe trust metrics for recommendations based on collab-orative filtering, which could inspire us for the merging of the weights coming from different sources. In particular, the frequency of the term in a forum, which can be above thousand, compared to roles provided by official documents, which can be one or two evidences, implies to use some normalisation methods and trust metrics. Yarosh et al [19] provide a taxonomy which includes roles and topics, but also other concepts and classifies them as selection criteria or t asks to achieve, which could be interesting to extend the expressiveness of our approach. Finally, we consider that the scope of this approach can also be discussed because , although we focus on RE works to inspire us, we could imagine to use it in other domains or to use more restrictive assumptions which holds in RE to improve the performance of our approach. This paper focuses on expert finding to improve the support of RE processes in large and dynamic contexts like OSS projects. We show how current approaches in RE, based on forums and social networks, relate to two complementary perspectives to evaluate expertises, namely content-based and social -based perspectives. We provide a novel ap-proach by combining and enriching concepts from these works to build a model that we translate into a Markov network to infer the probability that stakeholders have some searched expertises. We show in an illustrative example how this approach can be suc-cessfully applied and present a prelimin ary experiment in a real OSS case, before to discuss the results and limitations of our approach.

As future work, we plan to improve our approach by considering normalisation and trust metrics to manage heterogeneous sources of information as well as to optimise the MN computation or to use more local techniques to improve the scalability. The query expressiveness should be also improved to allow a better control of the inference process and adapt the results to the needs of the user of our approach. Other potential functions are investigated in order to improve the confidence and robustness of our rankings. Finally, we plan to further exploit the data provided by the OSS case to exploit roles and to design a complete case study.
 Acknowledgements. The authors are grateful to Itzel Morales-Ramirez for her help in exploiting the tool GATE for the nouns extrac tion. This work is a result of the RISCOSS project, funded by the EC 7th Framework Programme FP7/2007-2013, agreement num-ber 318249.

