 Clustering provides a better understanding of the data by dividing data sets into clusters so that objects in the same cluster are more similar than those in different clusters. It is one of the most important techniques of data mining and has been studied for decades and many algorithms have been proposed [ 14 ]. Nevertheless, uncertain data and high-dimensional data pose huge challenges to traditional clustering algorithms.
 subspace and the detection of clusters are circular dependent. Subspace clus-tering algorithms for high dimensional data can be divided into two groups according to the search technique: top-down subspace clustering methods and bottom-up subspace clustering methods. CLIQUE [ 4 ], ENCLUS [ 9 ], SUBACLU [ 15 ], MAFIA [ 18 ] and so on are all typical methods of bottom-up subspace clus-tering methods. And representative algorithms of top-down subspace clustering methods are like PROCLUS [ 1 ], ORCLUS [ 2 ], FINDIT [ 19 ], etc.
 tering, such as SC-MINER [ 11 ], CDCDD [ 21 ], CLWC [ 10 ] and so on. With the emergence of new application domains such as location-based ser-vices and sensor monitoring, uncertainty is ubiquitous due to reasons such as outdated sources or imprecise measurement [ 3 ]. As uncertainty is ubiquitous in many cases, uncertain data clustering causes more and more attention. In uncertain data, the attribute value may vary in a certain range, so that tradi-tional clustering algorithms do not work. Uncertain clustering algorithms also can be divided into two groups: partition-based clustering methods such as UK-means [ 8 ], UK-medoids [ 12 ], etc. and density-based clustering methods such as FDBSCAN [ 17 ], FOPTICS [ 16 ], PDBSCANi [ 20 ], etc.
 It is even more challenging for clustering high dimensional uncertain data. To our knowledge, there exists only one subspace clustering algorithm for high dimensional uncertain data [ 13 ], which is an extension of a bottom-up subspace clustering algorithm for handling data uncertainty.
 In this paper, based on the classical FINDIT [ 19 ] subspace clustering algo-rithm for high dimensional data, we propose a constraint based subspace cluster-ing algorithm for high dimensional uncertain data, UFINDIT. We extend both the distance functions and dimension voting rules of FINDIT to deal with high dimensional uncertain data. We also use the constraints to improve FINDIT both in eliminating parameters X  effect on the process of merging medoids and in improving the effectiveness of soundness. Furthermore, we propose some meth-ods such as sampling to get an more efficient algorithm. Experimental results on synthetic and real data sets show that our proposed UFINDIT algorithm outperforms the existing subspace clustering algorithm for uncertain data. FINDIT adopts dimension-oriented distance (dod) measure as its dissimilarity measure and determines the correlated dimensions for each cluster by dimension voting. FINDIT is composed of three phases: sampling phase, cluster forming phase and data assigning phase. In the first phase, sample set S and medoid set M (representatives of original clusters) are generated by a random sampling method. And in the second phase, correlated dimensions of all medoids are deter-mined by the V nearest neighbors X  voting and the neighbors of each medoid are calculated in dod measure. The medoids which are near from each other in dod measure are grouped together. As different medoid cluster sets are generated for each iteration on , an evaluation criteria is used to choose the best and the corresponding medoid cluster set MC . Then the selected medoid cluster set is given to the following phase as the best summary of the original clusters. In data assignment phase, all points are assigned to their nearest medoid clusters, and the points not assigned to any medoid cluster are regarded as outliers. The inputs for FINDIT are the dataset and two user parameters C D mindist . The first parameter reflects the users wish about the minimum size of clusters and the second one is the merge threshold between two resultant clusters. That is, if the dod distance between two clusters is smaller than D selected , they are regarded as one cluster by FINDIT and are subsequently merged.
 but it is sensitive to parameter D mindist and . Though it selects the best from 25 different values by its soundness criteria, the soundness criteria itself is flawed. In view of its merit and demerit, this paper extends it to uncertain data clustering and give solutions to overcome its demerit. 3.1 Problem Formulation The attribute of each object in the database of uncertain data is described by a probability density function (pdf) which limits the range the attribute value varies in. We are given a data set DB in a D -dimensional space. And the pdf p ( x ) with x  X  R D denotes the i -th data point which satisfies that its integral is equal to 1. To address the components of a vector x , we use the notation p ( x 1 ,  X  X  X  ,x D ) instead of p i ( x ).
 we define the projection of p i ( x ) to a subspace Sub as p which is defined by: where Sub = { 1 ,  X  X  X  ,s } . We define a subspace cluster as a set of p denotes the set of constraints, and c is the number of constraints, i.e., c = ML =( x i ,x j ) denotes a must-link between data points x x ,x j &gt; denotes a cannot-link between data points x i and x are linked by must-links and cannot-links are called medoids. And M denotes a points set which contains all medoids in C . 3.2 Algorithm Framework The inputs for our method are the data set X with N points in D-dimensional space, a user parameter C minsize and constraints set C based on the actual ground-truth clusters. According to C , we can easily get M . The parameter C minsize reflects the user s wish about the minimum size of clusters, in another word, our method does not report the clusters smaller than C cluster forming phase and data assigning phase. In the first phase, we randomly generate the set of sampled points S . The second phase includes five parts: determining key dimensions for each medoid, assigning every sampled point to its nearest medoid, merging similar medoids to get medoid clusters, refining medoid clusters and evaluation measuring medoid clusters. And the last phase is a process that assign every point in dataset to each medoid cluster in the best medoid clusters to obtain the finally clustering result. The algorithm framework is described in Algorithm 1.
 3.3 Dimension-Oriented Distance for Uncertain Data Dimension-oriented distance ( dod ) is an unique distance measure proposed in FINDIT which utilizes dimensional difference information and value difference information together. We extend this distance measure to solve uncertain data clustering problems, and we call it udod . We proposed two different methods to extend this distance measure. In case 1, we mainly compute the probability that the distance between point p and q is less than distance threshold .Ifthe probability is greater than the given  X  , then we believe that the two points are  X  X qual X  on that dimension. In this case, we define directed probability-based udod from a point p to a point q as: where | p ( d )  X  q ( d ) | denotes the Manhattan distance between point p and q in dimension d , D p is the subspace of point p , P ( | p ( d ) probability that the distance between point p and q is less than distance threshold , p ( x )isthepdfofpoint p in dimension d and  X  denotes probability threshold. where D d ( p, q ) denotes the distance integral between point p and q . between p and q .Thatis: 3.4 Sampling, Cluster Forming and Data Assigning Sampling Phase. In this phase, we randomly generate samples set S . Accord-ing to FINDIT, any original cluster larger than C minsize a certain number of points in S and have at least one point in M . According to FINDIT, we also use Chernof f bounds to solve the minimum size problem of sampled set S . The minimum size assures that every cluster in S have more than  X  sampled points by the probability of 1  X   X  , and is computed by the following equation: where constant k is the number of clusters, and  X  is a value satisfying C N/k X  (  X   X  1), N is the size of the dataset and C minsize cluster in the dataset.
  X  = 30,  X  =0 . 01, k = N/C minsize and  X  = 1 according to FINDIT. We use the notation S minsize to indicate the value used for  X  ,so S number of points in constraints ( M ) is also computed by using Chernof f bounds with  X  =1.
 Cluster Forming Phase. Cluster forming phase is iterated several times with an increasing . Throughout the whole process, is a very important para-meter which determines the computation of udod . To obtain an optimal , according to FINDIT, we try 25 different values from (1 / 100) valuerange to (25 / 100) valuerange where valuerange is a normalized value for all dimensions. Determining Key Dimensions: The purpose of this step is to determine the related dimensions for all medoids. For each medoid in set M , we compute its V nearest neighbors in set S by calculating udod ( p, m ) with As mentioned in FINDIT, we regard this V nearest neighbors as voters and dimension voting is a process for D independent questions. For each dimension of each medoid, in distance-based case, we count the number of voters which vote for  X  X ES X . While in the probability-based case, we compute the sum of probability that voters vote for  X  X ES X . If a neighbor X  X  d -th dimensional distance from m is less than or equal to the given , it is considered that it votes for  X  X ES X  in distance-based case. And in probability-based case, we regard the probability that a neighbor X  X  d -th dimensional distance is less than or equal to the given as the probability that voter votes for  X  X ES X .
 As the maximum number of reliable voters is S minsize (mentioned in [ 19 ]) which is equal to 30, we set V = 20. For each dimension of each medoid, as mentioned in FINDIT, if the computation value is greater than 12, then we believe this dimension is a relevant dimension for this medoid.
 Assigning Sampled Points: Since the set of key dimensions have been generated for each medoid in last step, each medoid is meaningful in its own subspace composed of its related dimensions. So to verify whether there is really a cluster in the subspace, we assign points in S to medoids in M . We refer to the point assigned to medoid m as member of m . For each point p in S and each medoid m in M , p is assigned to the nearest medoid which satisfies the member assignment condition as follows: Merging Medoids: In this phase, we improve the medoid clustering step of FINDIT by using constraints. After the last step, we get medoids with their own key dimensions and members. We group similar medoids together gradually. And the grouped medoid clusters are original clusters used for finally clustering. To cluster medoids, we also use the groupwise average clustering method ( gac )[ 19 ]. It starts by regarding all medoids as independent medoid clusters. And the distance between any two medoid clusters is computed as the weighted average distance between the medoids belonging to them as follows: where | m i | means the number of m i s members which is used as weight and mc means a medoid cluster A . To improve the effectiveness of gac , for any two medoids, if the number of dimensions on which they are considered equal is no more than 2, the distance is set to D as a penalty(note that D is the maximum distance). The process stops until there is a cannot-link between the closest pair. We believe that each cluster should have at least one must-link, so we remove the medoid clusters which do not contain a must-link before later step. The detailed process is described in Algorithm 2.
 Refining Medoid Clusters: Since each medoid cluster obtained in last step has information inherited from its medoids and each medoid m in mc can have slightly different related dimensions, we obtain the finally subspace for each medoid cluster mc by averaging information of each medoid in mc . Similar to FINDIT, the average selection ratio for a dimension d is obtained as follows: Algorithm 2. MergeMedoids Algorithm Input: Medoid set M
Output: Medoid cluster set MC 1. begin 2. Merge each two medoids linked by a must-link and add the result to MC ; 3. Merge medoid clusters in MC which have common medoids; 4. Get mc A and mc B between which the distance is shortest from MC ; 5. while There is not a cannot-link between medoid in mc A 6. Merge mc A and mc B and push the merged result into MC ; 7. Get mc A and mc B between which the distance is shortest from MC ; 8. end 9. for each medoid cluster mc in MC do begin 10. if There is not a must-link between medoids in mc 11. then Remove mc from MC ; 12. end 13. return MC ; 14. end where  X  i is a binary function which is set to 1 when the dimension d is a key dimension of m i and set to 0 otherwise. In this paper, we regard d as a key dimension of the medoid cluster, if avg d is lager than 0.95. When the finally subspace is obtained, all medoids in mc are only meaningful in this common subspace rather than their own subspaces.
 whose number of key dimensions is smaller than 2 are removed because their corresponding subspace is unreasonable.
 Evaluation: Because the cluster forming phase is iterated several times, so we should select the best medoid cluster set MC as the final result. According to FINDIT, the soundness criteria of MC is measured by the following equation: where | mc | is the size of a medoid cluster mc and | KD mc common key dimensions of mc . And the MC with the greatest soundness is chosen as the best one ( MC bestepsilon ) which has the most dimension and mem-ber information.
 Data Assigning Phase. In this phase, the points in the dataset are assigned to either a medoid cluster in MC bestepsilon or a noisy cluster. The principle of point assignation in this phase is the same as that used in member assignation step. If a point is assigned to a medoid m in medoid cluster mc ,itisequalto that it is assigned to mc . The only difference is that all medoids are meaningful in the subspace of the medoid cluster mc rather than their own subspaces. 3.5 Pruning Based on Constraints and Sampling Pruning. To solve the soundness X  X  problem of FINDIT and reduce the run-time, we propose a pruning algorithm by using constraints. The principle is that the average distance between points linked by cannot-link should be greater than distance between points linked by must-link. So we compute the difference between the mean of cannot-link distances and the mean of must-link distances using udod as dif f . Then select n epsilons whose values of dif f are greater than unselected ones as the epsilons set rather than 25 epsilons. In this paper, we set n as 5. What X  X  more, as udod costs is time-consuming, we propose a pruning method to reduce its runtime. Theoretically, if the max distance between point p and q in dimension d is less than , the real distance in dimension d must be less than and if the min distance between point p and q is more than ,the real distance must be more than . So if anyone of the two conditions is fulfilled, there is no need for us to compute P ( | p ( d )  X  q ( d ) reduce some computation of udod .
 Sampling. In some scenarios, it might be possible to compute the exact proba-bility by solving the integral of probability-based udod . However, the probability may be incomputable. To solve this problem, we use Monte-Carlo sampling to approximately calculate the integrals. Each pdf p ij of point i in dimension j is represented by a set of objects, drawn from the distribution obtained by p Given two pdfs p ij 1and p ij 2 and their sampled objects set I 1, I 2, we compute the distance between each object in I 1 and each object in I 2. Then compute the percentage of the objects pairs that have a smaller distance than as the prob-ability that the distance between the two points on dimension j is less than . For the integral of distance, we use an approximate method called Riemann integral to compute it. As we need to compute the integral with two variables, we divide both variables intervals into several equal sub-intervals, and the inte-gral can be approximately regarded as the sum of several cuboids X  volumes.The integral is computed as follows: 4.1 Datasets and Baselines We conduct a series of experiments to evaluate the performance of UFINDIT in terms of accuracy, efficiency and sacabitity. These experiments were performed on several synthetic dataset and real world datasets. The synthetic datasets are generated based on the method described in [ 1 , 13 ], but we make some modifi-cations to generate high-dimensional uncertain data. Firstly, we generate high-dimensional data as described in [ 1 ]. Then we generate uncertain range for the dataset with a random length between 0 and 0.1 data range in the relevant dimensions and a random length between 0 and 0.5 data range in the non-relevant ones.
 and summarize the details in Table 2 . We generate uncertain range for the UCI dataset with a random length between 0 and 0.1 data range in all dimensions. Two kinds of uncertainty, uniform distribution and Guassian distribution are conducted on synthetic datasets and UCI datasets. Our two algorithms are respectively called UFINDIT PRO which denotes the probability-based case, and UFINDIT DIS which denotes the distance-based case. We call the algo-rithm proposed in [ 13 ] USC, and FINDIT algorithm using data removing uncer-tainty by computing expectation of uncertain data as a real value FINDIT+EXP. We compare the accuracy, efficiency and scalability of UFINDIT PRO and UFINDIT DIS with USC and FINDIT+EXP. As there are 6 algorithms in USC, we experiment on all the 6 algorithms and report the best result. We use the para-meters suggested in the original paper except  X  in USC. We set  X  =0 . 05. And for UFINDIT, we use | C | = 100 according to Chernoff bounds, C according to FINDIT. 4.2 Accuracy We measure the accuracy by using F1-value criterion which is one of the most often used criterion to evaluate the accuracy of high dimensional data clustering algorithms. Figure 1 shows the accuracy comparison on synthetic datasets. Results show that FINDIT+EXP performs worst while both two UFINDIT per-forms best. The soundness of FINDIT becomes invalid which makes FINDIT worst and as we improve the soundness of FINDIT by using constraints, the result of UFINDIT performs well. USC performs well when data size and dimen-dataset. Figure 2 shows the accuracy comparison on UCI datasets. Results show that USC has the worst performance, while UFINDIT still performs best. In this case, the soundness of FINDIT performs well which makes results different. Comparing the result on synthetic datasets with that on real datasets, we find that the soundness of FINDIT is not always invalid. Overall we conclude that UFINDIT (UFINDIT PRO and UFINDIT DIS) performs better than the other two algorithms on both synthetic datasets and real datasets. 4.3 Efficiency and Scalability UFINDIT PRO and UFINDIT DIS with USC algorithm on a series of synthetic datasets. As FINDIT+EXP algorithm does not contain uncertain information, it will be much faster than uncertain algorithms, we do not compare with it here. Figure 3 shows the time comparison on different datasets. Figure 3 (a) shows the time scalability with the increasing of data set size. The runtime of USC increases sharply with the increasing of dataset size which shows that it is not scalable. Whereas our algorithms scale well with dataset size. Our algorithms have advan-tages over USC on clustering large quantity of data. Figure 3 (b) shows time scalability with respect to increasing of dimensionality. It can be seen that the slopes of all algorithms are similar, but USC increases faster than our algorithms. Figure 3 (c) shows time scalability with respect to the increasing of constraints. As USC using no constraints, its runtime does not change with the increasing of constraints. Our algorithms almost increase linearly as the number of constraints increases. In this paper, based on the classical subspace clustering algorithm FINDIT, we propose an effective subspace clustering algorithm UFINDIT for uncertain high dimensional data. We introduce a pruning algorithm to improve FINDIT by using constraints and also modify the medoids clustering process of FINDIT. To the best of our knowledge, this is the second (and the first top-down) subspace clustering algorithm for uncertain high dimensional data. Experimental results have shown the superiority of our method in terms of accuracy, efficiency and scalability.

