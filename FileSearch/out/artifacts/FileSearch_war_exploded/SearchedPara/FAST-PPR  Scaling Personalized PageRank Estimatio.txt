 We propose a new algorithm, FAST-PPR, for computing personalized PageRank: given start node s and target node t in a directed graph, and given a threshold  X  , FAST-PPR computes the Personalized PageRank  X  s ( t ) from s to t , guar-anteeing a small relative error as long  X  s ( t ) &gt;  X  . Existing algorithms for this problem have a running-time of  X (1 / X  ); in comparison, FAST-PPR has a provable average running-time guarantee of O ( p d/ X  ) (where d is the average in-degree of the graph). This is a significant improvement, since  X  is often O (1 /n ) (where n is the number of nodes) for applica-tions. We also complement the algorithm with an  X (1 / lower bound for PageRank estimation, showing that the de-pendence on  X  cannot be improved.

We perform a detailed empirical study on numerous mas-sive graphs, showing that FAST-PPR dramatically outper-forms existing algorithms. For example, on the 2010 Twit-ter graph with 1.5 billion edges, for target nodes sampled by popularity, FAST-PPR has a 20 factor speedup over the state of the art. Furthermore, an enhanced version of FAST-PPR has a 160 factor speedup on the Twitter graph, and is at least 20 times faster on all our candidate graphs. F.2.2 [ Nonnumerical Algorithms and Problems ]: Com-putations on Discrete Structures; G.2.2 [ Graph Theory ]: Graph Algorithms Algorithms,Theory Personalized PageRank; Social Search
The success of modern networks is largely due to the abil-ity to search effectively on them. A key primitive is PageR-ank [1 ], which is widely used as a measure of network im-portance. The popularity of PageRank is in large part due to its fast computation in large networks. As modern social network applications shift towards being more customized to individuals, there is a need for similar ego-centric measures of network structure.

Personalized PageRank (PPR) [ 1] has long been viewed as the appropriate ego-centric equivalent of PageRank. For a node u , the personalized PageRank vector  X  u measures the frequency of visiting other nodes via short random-walks from u . This makes it an ideal metric for social search , giv-ing higher weight to content generated by nearby users in the social graph. Social search protocols find widespread use  X  from personalization of general web searches [ 1, 2, 3], to more specific applications like collaborative tagging net-works [4 ], ranking name search results on social networks [ 5], social Q&amp;A sites [6 ], etc. In a typical personalized search application, given a set of candidate results for a query, we want to estimate the Personalized PageRank to each candi-date result. This motivates the following problem: Personalized PageRank  X  s ( t ) up to a small relative error. Since smaller values of  X  s ( t ) are more difficult to detect, we parameterize the problem by threshold  X  , requiring small relative errors only if  X  s ( t ) &gt;  X  . Current techniques used for PPR estimation (see Section 2.1 ) have  X (1 / X  ) running-time  X  this makes them infeasible for large networks when the desired  X  = O (1 /n ) or O ((log n ) /n ).

In addition to social-search, PPR is also used for a variety of other tasks across different domains: friend recommenda-tion on Facebook [ 7], who to follow on Twitter [ 8], graph partitioning [ 9], community detection [ 10 ], and other ap-plications [ 11]. Other measures of personalization, such as personalized SALSA and SimRank [ 12 ], can be reduced to PPR. However, in spite of a rich body of existing work [ 2, 13 , 9, 14 , 15 , 16, 17 ], estimating PPR is often a bottleneck in large networks. We develop a new algorithm, Frontier-Aided Significance Thresholding for Personalized PageRank (FAST-PPR), based on a new bi-directional search technique for PPR estimation:  X  Practical Contributions: We present a simple imple-mentation of FAST-PPR which requires no pre-processing and has an average running-time of O ( p d/ X  ) 1 . We also pro-pose a simple heuristic, Balanced FAST-PPR, that achieves a significant speedup in practice.

In experiments, FAST-PPR outperforms existing algorithms across a variety of real-life networks. For example, in Figure 1, we compare the running-times and accuracies of FAST-PPR with existing methods. Over a variety of data sets, FAST-PPR is significantly faster than the state of the art, with the same or better accuracy.

To give a concrete example: in experiments on the Twitter-2010 graph [ 19 ], Balanced FAST-PPR takes less than 3 sec-onds for random source-target pairs. In contrast, Monte Carlo takes more than 6 minutes and Local Update takes more than an hour. More generally in all graphs, FAST-PPR is at least 20 times faster than the state-of-the-art, without sacrificing accuracy.  X  Theoretical Novelty: FAST-PPR is the first algorithm for PPR estimation with O ( p d/ X  ) average running-time, where d = m/n is the average in-degree 2 . Further, we modify FAST-PPR to get O (1 / by pre-computing and storing some additional information, with a required storage of O ( m/
We assume here that the desired relative error and teleport probability  X  are constants  X  complete scaling details are provided in our technical report [18 ].
Formally, for ( s,t ) with  X  s ( t ) &gt;  X  , FAST-PPR re-turns an estimate b  X  s ( t ) with relative error c , incurring to our technical report [18 ] for complete details.
We also give a new running-time lower bound of  X (1 / for PPR estimation, which essentially shows that the depen-dence of FAST-PPR running-time on  X  cannot be improved.
Finally, we note that FAST-PPR has the same perfor-mance gains for computing PageRank with arbitrary pref-erence vectors [2], where the source is picked from a distri-bution over nodes. Different preference vectors are used for various applications [2, 1]. However, for simplicity of pre-sentation, we focus on Personalized PageRank in this work.
Given G ( V,E ), a directed graph, with | V | = n, | E | = m , and adjacency matrix A . For any node u  X  V , we denote N out ( u ) ,d out ( u ) as the out-neighborhood and out-degree re-spectively; similarly N in ( u ) ,d in ( u ) are the in-neighborhood and in-degree. We define d = m n to be the average in-degree (equivalently, average out-degree).

The personalized PageRank vector  X  u for a node u  X  V is the stationary distribution of the following random walk starting from u : at each step, return to u with probability  X  , and otherwise move to a random out-neighbor of the current node. Defining D = diag( d out ( u )), and W = D  X  1 A , the personalized PageRank (PPR) vector of u is given by: where e u is the identity vector of u . Also, for a target node t , we define the inverse-PPR of a node w with respect to t as  X   X  1 t ( w ) =  X  w ( t ). The inverse-PPR vector {  X  of t sums to n X  ( t ), where  X  ( t ) is the global PageRank of t .
Note that the PPR for a uniform random pair of nodes is 1 /n  X  thus for practical applications, we need to consider  X  of the form O (1 /n ) or O (log n/n ). We reinforce this choice of  X  using empirical PPR data from the Twitter-2010 graph in Section 6.2  X  in particular, we observe that only 1% of ( s,t ) pairs have PPR greater than 4 /n . In order to process real-time queries, one option would be precompute all answers, and for each node u , store a list of all targets with PPR at least  X  . This results in O (1) running-time at query, but requires  X (1 / X  ) storage per node, which is not feasible in large networks. The other extreme is to eschew storage and compute the PPR at query time  X  however, as we discuss below, existing algorithms for computing PPR have a worst-case running-time of  X (1 / X  ). For the relevant values of  X  , this is infeasible.
There are two main techniques used to compute PageR-ank/PPR vectors. One set of algorithms use the power it-eration. Since performing a direct power iteration may be infeasible in large networks, a more common approach is to use local-update versions of the power method, similar to the Jacobi iteration. This technique was first proposed by Jeh and Widom [2 ], and subsequently improved by other re-searchers [ 20 , 9]. The algorithms are primarily based on the following recurrence relation for  X  u : Another use of such local update algorithms is for estimat-ing the inverse-PPR vector for a target node. Local-update algorithms for inverse-PageRank are given in [14] (where inverse-PPR is referred to as the  X  X ontribution PageRank vector X ), and [ 21] (where it is called  X  X usceptibility X ). How-ever, one can exhibit graphs where these algorithms need a running-time of O (1 / X  ) to get additive guarantees on the order of  X  .

Eqn. 2 can be derived from the following probabilistic re-interpretations of PPR, which also lead to an alternate set of randomized or Monte-Carlo algorithms. Given any random variable L taking values in N 0 , let RW ( u,L ) , { u,V 1 ,V 2 ,...,V L } be a random-walk of random length L  X  Geom (  X  ) 3 , starting from u . Then we can write: In other words,  X  u ( v ) is the probability that v is the last node in RW ( u,L ). Another alternative characterization is: i.e.,  X  u ( v ) is proportional to the number of times RW ( u,L ) visits node v . Both characterizations can be used to estimate  X  (  X  ) via Monte Carlo algorithms, by generating and storing random walks at each node [ 13 , 15, 16 , 17 ]. Such estimates are easy to update in dynamic settings [ 15 ]. However, for estimating PPR values close to the desired threshold  X  , these algorithms need  X (1 / X  ) random-walk samples.
The problem with the basic Monte Carlo procedure  X  generating random walks from s and estimating the dis-tribution of terminal nodes  X  is that to estimate a PPR which is O (  X  ), we need  X (1 / X  ) walks. To circumvent this, we introduce a new bi-directional estimator for PPR: given a PPR-estimation query with parameters ( s,t, X  ), we first work backward from t to find a suitably large set of  X  X ar-gets X , and then do random walks from s to test for hitting this set.

Our algorithm can be best understood through an analogy with the shortest path problem. In the bidirectional shortest path algorithm, to find a path of length l from node s to node t , we find all nodes within distance l 2 of t , find all i.e., P [ L = i ] =  X  (1  X   X  ) i  X  i  X  N 0 nodes within distance l 2 of s , and check if these sets intersect. Similarly, to test if  X  s ( t ) &gt;  X  , we find all w with  X  (we call this the target set ), take O (1 / start node, and see if these two sets intersect. It turns out that these sets might not intersect even if  X  s ( t ) &gt;  X  , so we go one step further and consider the frontier set  X  nodes outside the target set which have an edge into the target set. We can prove that if  X  s ( t ) &gt;  X  then random walks are likely to hit the frontier set.

Our method is most easily understood using the charac-terization of  X  s ( t ) as the probability that a single walk from s ends at t (Eqn. ( 3)). Consider a random walk from s to t  X  at some point, it must enter the frontier. We can then de-compose the probability of the walk reaching t into the prod-uct of two probabilities: the probability that it reaches some node w in the frontier, and the probability that it reaches t starting from w . The two probabilities in this estimate are typically much larger than the overall probability of a ran-dom walk reaching t from s , so they can be estimated more efficiently. Figure 2 illustrates this bi-directional scheme.
To formalize our algorithm, we first need some additional definitions. We define a set B to be a blanket set for t with respect to s if all paths from s to t pass through B . Given blanket set B and a random walk RW ( s,L ), let H B be the first node in B hit by the walk (defining H B =  X  if the walk does not hit B before terminating). Since each walk corresponds to a unique H B , we can write P [ V L = v ] as a sum of contributions from each node in B . Further, from the memoryless property of the geometric random variable, the probability a walk ends at t conditioned on reaching w  X  B before stopping is exactly  X  s ( t ). Combining these, we have: In other words, the PPR from s to t is the sum, over all nodes w in blanket set B , of the probability of a random walk hitting B first at node w , times the PPR from w to t .
Recall we define the inverse-PPR vector of t as  X   X  1 (  X  w ( t )) w  X  V . Now we introduce two crucial definitions:
Definition 1 (Target Set). The target set T t ( r ) for a target node t is given by:
Definition 2 (Frontier Set). The frontier set F t ( r ) for a target node t is defined as: The target set T t ( r ) thus contains all nodes with inverse-PPR greater than r , while the frontier set F t ( r ) contains all nodes which are in-neighbors of T t ( r ), but not in T The next proposition illustrates the core of our approach.
Proposition 1. Set r &lt;  X  . Fix vertices s,t such that s /  X  T t ( r ) . 1. Frontier set F t ( r ) is a blanket set of t with respect to Figure 2: The FAST-PPR Algorithm: We first work backward from the target t to find the frontier F t (with inverse-PPR estimates). Next we work for-ward from the source s , generating random-walks and testing for hitting the frontier. 2. For random-walk RW ( s,L ) with length L  X  Geom (  X  ) :
Proof. For brevity, let T t = T t ( r ) ,F t = F t ( r ). By definition, we know  X  t ( t )  X   X   X  thus t  X  T t , since r The frontier set F t contains all neighbors of nodes in T which are not themselves in T t  X  hence, for any source node u /  X  T t , a path to t must first hit a node in F t .
For the second part, since F t is a blanket set for s with re-spect to t , Eqn. 4 implies  X  s ( t ) = P w  X  F where H F t is the first node where the walk hits F t . Note that by definition,  X  w  X  F t we have w /  X  T t  X  thus  X  w ( t )  X  Applying this bound, we get:  X  s ( t )  X  r X Rearranging, we get the result.

The aim is to estimate  X  s ( t ) through Eqn. 4. By the pre-vious proposition, F t ( r ) is a blanket set. We will determine the set F t ( r ) and estimate all quantities in the right side of Eqn. 4, thereby estimating  X  s ( t ).

We perform a simple heuristic calculation to argue that setting r  X  shows that T t ( r ) can be found in O ( d/ r ) time [14, 21 ]  X  using this we can find F t ( r ). Now suppose that we know all values of  X  w ( t ) (  X  w  X  F t ( r )). By Eqn. 4, we need to estimate the probability of random walks from s hitting vertices in F t ( r ). By the previous proposition, the prob-ability of hitting F t ( r ) is at least  X / r  X  hence, we need O ( r / X  ) walks from s to ensure we hit F t ( r ). All in all, we require O ( d/ r + r / X  )  X  setting r = time bound of O ( p d/ X  ). In reality, however, we only have (coarse) PPR estimates for nodes in the frontier  X  we show how these estimates can be boosted to get the desired guar-antees, and also empirically show that, in practice, using the frontier estimates gives good results. Finally, we show that 1 /  X 
We note that bi-directional techniques have been used for estimating fixed-length random walk probabilities in regular undirected graphs [22, 23 ]. These techniques do not extend to estimating PPR  X  in particular, we need to consider di-rected graphs, arbitrary node degrees and walks of random length . Also, Jeh and Widom [ 2] proposed a scheme for PageRank estimation using intermediate estimates from a fixed skeleton of target nodes. However there are no running-time guarantees for such schemes; also, the target nodes and partial estimates need to be pre-computed and stored. Our algorithm is fundamentally different as it constructs separate target sets for each target node at query-time.
We now develop the Frontier-Aided Significance Thresh-olding algorithm, or FAST-PPR, specified in Algorithm 1. The input-output behavior of FAST-PPR is as follows:  X  Inputs: The primary inputs are graph G , teleport prob-ability  X  , start node s , target node t , and threshold  X   X  for brevity, we suppress the dependence on G and  X  . We also need a reverse threshold r  X  in subsequent sections, we discuss how this parameter is chosen.  X  Output: An estimate
The algorithm also requires two parameters, c and  X   X  the former controls the number of random walks, while the latter controls the quality of our inverse-PPR estimates in the target set. In our pseudocode (Algorithms 1 and 2), we specify the values we use in our experiments  X  the theoretical basis for these choices is provided in Section 3.2 . Algorithm 1 FAST-PPR( s,t, X  ) Inputs: graph G , teleport probability  X  , start node s , tar-1: Set accuracy parameters c ,  X  (in our experiments we use 2: Call FRONTIER( t, r , X  ) to obtain target set 3: if s  X  T t ( r ) then 4: return  X   X  1 t ( s ) 5: else 6: Set number of walks k = c r / X  (See Theorem 3 for 7: for index i  X  [ k ] do 8: Generate L i  X  Geom (  X  ) 9: Generate random-walk RW ( s,L i ) 10: Determine H i , the first node in F t ( r ) hit by RW 11: end for 12: return b  X  s ( t ) = (1 /k ) P i  X  [ k ]  X   X  1 t ( H i 13: end if FAST-PPR needs to know sets T t ( r ) ,F t ( r ) and inverse-(approximately) from existing algorithms of [ 14 , 21 ]. For the sake of completeness, we provide pseudocode for the proce-dure FRONTIER (Algorithm 2) that obtains this informa-tion. The following combines Theorems 1 and 2 in [ 21].
Theorem 1. FRONTIER ( t, r , X  ) algorithm computes es-| b  X  t ( w )  X   X  all choices of t ) is O ( d/ (  X  r )) , where d = m/n is the average degree of the graph.

Observe that the estimates b  X   X  1 t ( w ) are used to find ap-proximate target and frontier sets. Note that the running Algorithm 2 FRONTIER( G, X ,t, r , X  ) [14, 21 ] Inputs: graph G , teleport probability  X  , target node t , re-1: Define additive error inv =  X  r 2: Initialize (sparse) estimate-vector b  X   X  1 t and (sparse) 3: Initialize target-set b T t = { t } , frontier-set b F 4: while  X  w  X  V s.t.r t ( w ) &gt;  X  inv do 5: for u  X  X  in ( w ) do 7: b  X   X  1 t ( u ) = b  X   X  1 t ( u ) +  X  ,r t ( u ) = r t 8: if b  X   X  1 t ( u ) &gt; r then 9: b T t = b T t  X  X  u } , b F t = b F t  X  X  in ( u ) 10: end if 11: end for 12: r t ( w ) = 0 13: end while 14: b F t = b F t \ b T t 15: return b T t , b F t , ( b  X   X  1 t ( w )) w  X  time for a given t is proportional to the frontier size | It is relatively straightforward to argue (as in [ 21 ]) that P
In the subsequent subsections, we present theoretical anal-yses of the running times and correctness of FAST-PPR. The correctness proof makes an excessively strong assump-tion of perfect outputs for FRONTIER, which is not true. To handle this problem, we have a more complex variant of FAST-PPR that can be proven theoretically  X  refer to our technical report [18 ] for details. Nonetheless, our empiri-cal results show that FAST-PPR does an excellent job of estimating  X  s ( t ).
Theorem 2. Given parameters  X , r , the running-time of the FAST-PPR algorithm, averaged over uniform-random pairs s,t , is O (  X   X  1 ( d/ r + r / X  )) .

Proof. Each random walk RW ( u,Geom (  X  )) takes 1 / X  steps on average, and there are O ( r / X  ) such walks per-formed  X  this is the forward time . On the other hand, from Theorem 1, we have that for a random ( s,t ) pair, the aver-age running time of FRONTIER is O ( d/ (  X  r ))  X  this is the reverse time . Combining the two, we get the result. Note that the reverse time bound above is averaged across choice of target node; for some target nodes (those with high global PageRank) the reverse time may be much larger than average, while for others it may be smaller. However, the forward time is similar for all source nodes, and is pre-dictable  X  we exploit this in Section 5.2 to design a balanced version of FAST-PPR which is much faster in practice. In terms of theoretical bounds, the above result suggests an obvious choice of r to optimize the running time:
Corollary 1. Set r = average per-query running-time of O (  X   X  1 p d/ X  ) .
We now analyze FAST-PPR in an idealized setting, where we assume that FRONTIER returns exact inverse-PPR esti-mates  X  i.e., the sets T t ( r ), F t ( r ), and the values {  X  are known exactly. This is an unrealistic assumption, but it gives much intuition into why FAST-PPR works. In partic-ular, we show that if  X  ( s,t ) &gt;  X  , then with probability at least 99%, FAST-PPR returns an estimate with relative er-ror at most 1 / 4; furthermore, if  X  ( s,t ) &lt;  X  , then FAST-PPR returns an estimate with additive error at most  X / 4. Note that the above guarantees are chosen for ease of exposition  X  in our main theoretical result (provided in our technical re-port [ 18]), we show that FAST-PPR can achieve any desired relative error target and success probability.

Theorem 3. For any s,t, X , r , FAST-PPR outputs an estimate b  X  s ( t ) such that with probability &gt; 0 . 99 :
Proof. We choose c = max (48  X  8 e ln(100) , 4 log 2 (100)); this choice of parameter c is for ease of exposition in the computations below, and has not been optimized.

To prove the result, note that FAST-PPR performs k = c / X  i.i.d. random walks RW ( s,L ). We use H i to denote the first node in F t ( r ) hit by the i th random walk. Let X i =  X   X  1 t ( H i ) and X = P k i =1 X i . By Eqn. 4,  X  |  X  ( t )  X  b  X  s ( t ) | is exactly | X  X  E [ X ] | /k .
It is convenient to define scaled random variables Y X / r and Y = X/ r before we apply standard Chernoff bounds. We have |  X  s ( t )  X  b  X  s ( t ) | = ( r /k ) | Y  X  (  X /c ) | Y  X  E [ Y ] | . Also,  X  s ( t ) = (  X /c ) E cause H i  X  F t ( r ), X i =  X  H i ( t ) &lt; r , so Y i we can apply the following two Chernoff bounds (refer to Theorem 1.1 in [ 24]): 1. P [ | Y  X  E [ Y ] | &gt; E [ Y ] / 4] &lt; exp(  X  E [ Y ] / 48) 2. For any b &gt; 2 e E [ Y ] , P [ Y &gt; b ]  X  2  X  b Now, we perform a case analysis. Suppose  X  s ( t ) &gt;  X / (4 e ). Then E [ Y ] &gt; c/ (4 e ), and
P [ |  X  s ( t )  X  b  X  s ( t ) | &gt;  X  s ( t ) / 4] = P Suppose  X  s ( t )  X   X / (4 e ). Then,  X / 4 &gt; 2 e X  s ( t ) implying c/ 4 &gt; 2 e E [ Y ]. By the upper tail: The proof is completed by trivially combining both cases.
In this section, we prove that any algorithm that accu-rately estimates PPR queries up to a threshold  X  must look at  X (1 / the optimal dependence on  X  . The numerical constants be-low are chosen for easier calculations, and are not optimized.
We assume  X  = 1 / 100 log(1 / X  ), and consider random-ized algorithms for the following variant of Significant-PPR, which we denote as Detect-High (  X  )  X  for all pairs ( s,t ): We stress that the probability is over the random choices of the algorithm, not over s,t . We now have the following lower bound:
Theorem 4. Any algorithm for Detect-High(  X  ) must ac-cess  X (1 /
Proof Outline. The proof uses a lower bound of Gol-dreich and Ron for expansion testing [25]. The technical con-tent of this result is the following  X  consider two distributions G 1 and G 2 of undirected 3-regular graphs on N nodes. A graph in G 1 is generated by choosing three uniform-random perfect matchings of the nodes. A graph in G 2 is generated by randomly partitioning the nodes into 4 equally sized sets, V ,i  X  { 1 , 2 , 3 , 4 } , and then, within each V i , choosing three uniform-random matchings.

Consider the problem of distinguishing G 1 from G 2 . An adversary arbitrarily picks one of these distributions, and generates a graph G from it. A distinguisher must report whether G came from G 1 or G 2 , and it must be correct with probability &gt; 2 / 3 regardless of the distribution chosen. The-orem 7.5 of Goldreich and Ron [25 ] asserts the following:
Theorem 5 (Theorem 7.5 of [25 ]). Any distinguisher must look at We perform a direct reduction, relating the Detect-High (  X  ) problem to the Goldreich and Ron setting  X  in particular, we show that an algorithm for Detect-High (  X  ) which requires less than 1 / guisher which violates Theorem 5 . The complete proof is provided in our technical report [18 ].
The previous section describes vanilla FAST-PPR, with a proof of correctness assuming a perfect FRONTIER. We now present some variants of FAST-PPR. We give a theo-retical variant that is a truly provable algorithm, with no as-sumptions required  X  however, vanilla FAST-PPR is a much better practical candidate and it is what we implement. We also discuss how we can use pre-computation and storage to obtain worst-case guarantees for FAST-PPR. Finally, from the practical side, we discuss a workload-balancing heuristic that provides significant improvements in running-time by dynamically adjusting r .
The assumption that FRONTIER returns perfect esti-mates is theoretically untenable  X  Theorem 1 only ensures that each inverse-PPR estimate is correct up to an additive factor of inv =  X  r . It is plausible that for every w in the frontier b F t ( r ),  X   X  1 t ( w ) &lt; inv , and FRONTIER may return a zero estimate for these PPR values. It is not clear how to use these noisy inverse-PPR estimates to get the desired accuracy guarantees.

To circumvent this problem, we observe that estimates  X  t ( w ) for any node w  X  b T t ( r ) are in fact accurate up to a multiplicative factor . We design a procedure to bootstrap these  X  X ood X  estimates, by using a special  X  X arget-avoiding X  random walk  X  this modified algorithm gives the desired accuracy guarantee with only an additional log(1 / X  ) factor in running-time. The final algorithm and proof are quite intricate  X  due to lack of space, the details are deferred to our technical report [ 18 ].
In FAST-PPR, the parameter r can be chosen freely while preserving accuracy. Choosing a larger value leads to a smaller frontier and less forward work at the cost of more reverse work; a smaller value requires more reverse work and fewer random walks. To improve performance in practice, we can optimize the choice of r based on the target t , to bal-ance the reverse and forward time. Note that for any value of , the forward time is proportional to k = c r / X  (the num-ber of random walks performed)  X  for any choice of r , it is easy to estimate the forward time required. Thus, instead of committing to a single value of r , we propose a heuristic wherein we dynamically decrease r until the estimated re-maining forward time equals the reverse time already spent .
We now describe this Balanced FAST-PPR algorithm in brief. Instead of pushing from any node w with residual r ( w ) above a fixed threshold  X  inv , we now push from the node w with the largest residual value  X  this follows a sim-ilar algorithm proposed in [21 ]. From [ 14, 21 ], we know that a current maximum residual value of r max implies an additive error guarantee of r max  X   X  this corresponds to a dy-namic r value of r max  X  X  . At this value of r , the number of forward walks required is k = c r / X  . By multiplying k by the average time needed to generate a walk, we get a good prediction the amount of forward work still needed  X  we can then compare it to the time spent on revere-work and adjust till they are equal. Thus Balanced Fast-PPR is able to dynamically choose r to balance the forward and reverse running-time. In Section 6.5 , we experimentally show how this change balances forward and reverse running-time, and significantly reduces the average running-time.
All our results for FAST-PPR have involved average-case running-time bounds. To convert these to worst-case running-time bounds, we can precompute and store the frontier for all nodes, and only perform random walks at query time. To obtain the corresponding storage requirement for these Fron-tier oracles , observe that for any node w  X  V , it can belong to the target set of at most 1 Summing over all nodes, we have:
To further cut down on running-time, we can also pre-compute and store the random-walks from all nodes, and perform appropriate joins at query time to get the FAST-PPR estimate. This allows us to implement FAST-PPR on any distributed system that can do fast intersections/joins. More generally, it demonstrates how the modularity of FAST-PPR can be used to get variants that tradeoff between dif-ferent resources in practical implementations. We conduct experiments to explore three main questions: 1. How fast is FAST-PPR relative to previous algorithms? 2. How accurate are FAST-PPR X  X  estimates? 3. How is FAST-PPR X  X  performance affected by our de- X  Data-Sets: To measure the robustness of FAST-PPR, we run our experiments on several types and sizes of graph, as described in Table 1. Pokec and Twitter are both so-cial networks in which edges are directed. The LiveJournal, Orkut (social networks) and DBLP (collaborations on pa-pers) networks are all undirected  X  for each, we have the largest connected component of the overall graph. Finally, our largest dataset with 3 . 7 billion edges is from a 2007 crawl of the UK domain [ 26, 27 ]. Each vertex is a web page and each edge is a hyperlink between pages.

For detailed studies of FAST-PPR, we use the Twitter-2010 graph, with 41 million users and 1 . 5 billion edges. This presents a further algorithmic challenge because of the skew of its degree distribution: the average degree is 35, but one node has more than 700 , 000 in-neighbors.

The Pokec [28 ], Live Journal [29 ], and Orkut [29 ] datasets were downloaded from the Stanford SNAP project [ 30 ]. The DBLP-2011 [26 ], Twitter-2010 [26 ] and UK 2007-05 Web Graph [ 26 , 27 ] were downloaded from the Laboratory for Web Algorithmics [19 ].  X  Implementation Details: We ran our experiments on a machine with a 3.33 GHz 12-core Intel Xeon X5680 pro-cessor, 12MB cache, and 192 GB of 1066 MHz Registered ECC DDR3 RAM. Each experiment ran on a single core and loaded the graph used into memory before beginning any timings. The RAM used by the experiments was domi-nated by the RAM needed to store the largest graph using the SNAP library format [ 30 ], which was about 21GB.
For reproducibility, our C++ source code is available at: http://cs.stanford.edu/~plofgren/fast_ppr/  X  Benchmarks: We compare FAST-PPR to two bench-mark algorithms: Monte-Carlo and Local-Update.

Monte-Carlo refers to the standard random-walk algo-rithm [ 13, 15 , 16 , 17 ]  X  we perform c MC  X  walks and estimate  X  ( v ) by the fraction of walks terminating at v . For our ex-periments, we choose c MC = 35, to ensure that the relative errors for Monte-Carlo are the same as the relative error bounds chosen for Local-Update and FAST-PPR (see be-low). However, even in experiments with c MC = 1, we find that FAST-PPR is still 3 times faster on all graphs and 25 times faster on the largest two graphs (refer to our technical report [18 ] for additional plots).

Our other benchmark, Local-Update, is the state-of-the-art local power iteration algorithm [ 14 , 21 ]. It follows the same procedure as the FRONTIER algorithm (Algorithm 2), but with the additive accuracy inv set to  X / 2. Note that a backward local-update is more suited to computing PPR forward schemes [9 , 2] as the latter lack natural performance guarantees on graphs with high-degree nodes.  X  Parameters: For FAST-PPR, we set the constants c = 350 and  X  = 1 / 6  X  these are guided by the Chernoff bounds Figure 3: Complementary cumulative distribution for 10,000 (s,t) pairs sampled uniformly at random on the Twitter graph. we use in the proof of Theorem 3. For vanilla FAST-PPR, we simply choose r =
For all our experiments, we use  X  = 4 n . To understand the importance of this threshold, we study the distribution of PPR values in real networks. Using the Twitter graph as an example, we choose 10,000 random ( s,t ) pairs and compute  X  s ( t ) using FAST-PPR to accuracy  X  = n 10 . The complementary cumulative distribution function is shown on a log-log plot in Figure 3. Notice that the plot is roughly linear, suggesting a power-law. Because of this skewed dis-tribution, only 2.8% of pairs have PPR above 1 n = 2.4e-8, and less than 1% have PPR over 4 n = 9.6e-8.
After loading the graph into memory, we sample 1000 source/target pairs ( s,t ) uniformly at random. For each, we measure the time required for answering PPR-estimation queries with threshold  X  = 4 /n , which, as we discuss above, is fairly significant because of the skew of the PPR distribu-tion. To keep the experiment length less than 24 hours, for the Local-Update algorithm and Monte-Carlo algorithms we only use 20 and 5 pairs respectively.

The running-time comparisons are shown in Figure 4  X  we compare Monte-Carlo, Local-Update, vanilla FAST-PPR, and Balanced FAST-PPR. We perform an analogous exper-iment where target nodes are sampled according to their global PageRank value. This is a more realistic model for queries in personalized search applications, with searches bi-ased towards more popular targets. The results, plotted in Figure 4(b) , show even better speedups for FAST-PPR. All in all, FAST-PPR is many orders of magnitude faster than the state of the art.

The effect of target global PageRank: To quantify the speedup further, we sort the targets in the Twitter-2010 graph by their global PageRank, and choose the first tar-get in each percentile. We measure the running-time of the four algorithms (averaging over random source nodes), as shown in Figure 5. Note that FAST-PPR is much faster than previous methods for the targets with high PageRank. Significant-PPR queries ( s,t, X  ) with threshold  X  = 4 n for 1000 ( s,t ) pairs. For each pair, the start node is Figure 5: Execution time vs. global PageRank of the target. Target nodes were sorted by global PageR-ank, then one was chosen from each percentile. We use  X  = 0 . 2 , and 5 -point median smoothing.
 Note also that large PageRank targets account for most of the average running-time  X  thus improving performance in these cases causes significant speedups in the average run-ning time. We also see that Balanced FAST-PPR has signif-icant improvements over vanilla FAST-PPR, especially for lower PageRank targets.

To give a sense of the speedup achieved by FAST-PPR, consider the Twitter-2010 graph. Balanced FAST-PPR takes less than 3 seconds for Significant-PPR queries with targets sampled from global PageRank  X  in contrast, Monte Carlo takes more than 6 minutes and Local Update takes more than an hour. In the worst-case, Monte Carlo takes 6 min-utes and Local Update takes 6 hours, while Balanced FAST-PPR takes 40 seconds. Finally, the estimates from FAST-PPR are twice as accurate as those from Local Update, and 6 times more accurate than those from Monte Carlo.
We measure the empirical accuracy of FAST-PPR. For each graph, we sample 25 targets uniformly at random, and compute their ground truth inverse-PPR vectors by running a power iteration up to an additive error of  X / 100 (as before, we use  X  = 4 /n ). Since larger PPR values are easier to com-pute than smaller PPR values, we sample start nodes such that  X  s ( t ) is near the significance threshold  X  . In particular, for each of the 25 targets t , we sample 50 random nodes from the set { s :  X / 4  X   X  s ( t )  X   X  } and 50 random nodes from the set { s :  X   X   X  s ( t )  X  4  X  } .

We execute FAST-PPR for each of the 2500 ( s,t ) pairs, and measure the empirical error  X  the results are compiled in Table 2. Notice that FAST-PPR has mean relative error less than 15% and max relative error less than 65% on all graphs  X  this is sufficiently accurate to make it useful for personalized search.

To make sure that FAST-PPR is not sacrificing accuracy for improved running-time, we also compute the relative er-ror of Local-Update and Monte-Carlo, using the same pa-rameters as for our running-time experiments. For each of the 2500 ( s,t ) pairs, we run Local-Update, and mea-sure its relative error. For testing Monte-Carlo, we use our knowledge of the ground truth PPR, and the fact that each random-walk from s terminates at t with probability p =  X  s ( t ). This allows us to simulate Monte-Carlo by di-rectly sampling from a Bernoulli variable with mean  X  s ( t )  X  this statistically identical to generating random-walks and testing over all pairs. Note that actually simulating the walks would take more than 50 days of computation for 2500 pairs. The relative errors are shown in Figure 1(b) . Notice that FAST-PPR is more accurate than the state-of-the-art competition on all graphs. This shows that our running time comparisons are using parameters settings that are fair.  X  Necessity of the Frontier: Another question we study experimentally is whether we can modify FAST-PPR to compute the target set T t ( r ) and then run Monte-Carlo walks until they hit the target set (rather than the fron-tier set F t ( r )). This may appear natural, as the target set is also a blanket set, and we have good approximations for inverse-PPR values in the target set. Further, using only the frontier results in significantly worse accuracy.
 Figure 7: Forward and reverse running-times (on log-scale) for FAST-PPR (in red) and Balanced FAST-PPR (in blue) as we vary the global PageR-ank of the target node on the x-axis. Data is for the Twitter-2010 graph and is smoothed using median-of-five smoothing. Notice how there is a significant gap between the forward and backward work in Fast-PPR, and that this gap is corrected by Balanced Fast-PPR. target set would reduce the dependence of the running-time on d , and also reduce storage requirements for an oracle-based implementation.

It turns out however that using the frontier is critical to get good accuracy. Intuitively, this is because nodes in the target set may have high inverse-PPR, which then increases the variance of our estimate. This increase in variance can be visually seen in a scatterplot of the true vs estimated value, as shown in Figure 6(b)  X  note that the estimates generated using the frontier set are much more tightly clustered around the true PPR values, as compared to the estimates generated using the target set.  X  Balancing Forward and Reverse Work: Balanced FAST-PPR, as described in Section 5.2 , chooses reverse thresh-old r dynamically for each target node. Figure 4 shows that Balanced FAST-PPR improves the average running-time across all graphs.

In Figure 7, we plot the forward and reverse running-times for FAST-PPR and Balanced FAST-PPR as a function of the target PageRank. Note that for high global-PageRank targets, FAST-PPR does too much reverse work, while for low global-PageRank targets, FAST-PPR does too much for-ward work  X  this is corrected by Balanced FAST-PPR. Peter Lofgren is supported by a National Defense Science and Engineering Graduate (NDSEG) Fellowship.

Ashish Goel and Siddhartha Banerjee were supported in part by the DARPA XDATA program, by the DARPA GRAPHS program via grant FA9550-12-1-0411 from the U.S. Air Force Office of Scientific Research (AFOSR) and the De-fense Advanced Research Projects Agency (DARPA), and by NSF Award 0915040.

C. Seshadhri is at Sandia National Laboratories, which is a multi-program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy X  X  National Nuclear Security Administration under contract DE-AC04-94AL85000. [1] L. Page, S. Brin, R. Motwani, and T. Winograd,  X  X he [2] G. Jeh and J. Widom,  X  X caling personalized web [3] P. Yin, W.-C. Lee, and K. C. Lee,  X  X n top-k social [4] S. A. Yahia, M. Benedikt, L. V. Lakshmanan, and [5] M. V. Vieira, B. M. Fonseca, R. Damazio, P. B. [6] D. Horowitz and S. D. Kamvar,  X  X he anatomy of a [7] L. Backstrom and J. Leskovec,  X  X upervised random [8] P. Gupta, A. Goel, J. Lin, A. Sharma, D. Wang, and [9] R. Andersen, F. Chung, and K. Lang,  X  X ocal graph [10] J. Yang and J. Leskovec,  X  X efining and evaluating [11] H. Tong, C. Faloutsos, and J.-Y. Pan,  X  X ast random [12] T. Sarl  X os, A. A. Bencz  X ur, K. Csalog  X any, D. Fogaras, [13] K. Avrachenkov, N. Litvak, D. Nemirovsky, and [14] R. Andersen, C. Borgs, J. Chayes, J. Hopcraft, V. S. [15] B. Bahmani, A. Chowdhury, and A. Goel,  X  X ast [16] C. Borgs, M. Brautbar, J. Chayes, and S.-H. Teng, [17] A. D. Sarma, A. R. Molla, G. Pandurangan, and [18] P. Lofgren, S. Banerjee, A. Goel, and C. Seshadhri, [19]  X  X aboratory for web algorithmics. X  [20] D. Fogaras, B. R  X acz, K. Csalog  X any, and T. Sarl  X os, [21] P. Lofgren and A. Goel,  X  X ersonalized pagerank to a [22] O. Goldreich and D. Ron,  X  X n testing expansion in [23] S. Kale, Y. Peres, and C. Seshadhri,  X  X oise tolerance [24] D. Dubhashi and A. Panconesi, Concentration of [25] O. Goldreich and D. Ron,  X  X roperty testing in [26] P. Boldi, M. Rosa, M. Santini, and S. Vigna,  X  X ayered [27] P. Boldi, M. Santini, and S. Vigna,  X  X  large [28] L. Takac and M. Zabovsky,  X  X ata analysis in public [29] A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, [30]  X  X tanford network analysis platform (snap). X 
