 We introduce a unified graph representation of the Web, which includes both structural and usage information. We model this graph using a simple union of the Web X  X  hyperlink and click graphs. The hyperlink graph expresses link struc-ture among Web pages, while the click graph is a bipartite graph of queries and documents denoting users X  searching behavior extracted from a search engine X  X  query log.
Our most important motivation is to model in a unified way the two main activities of users on the Web: searching and browsing , and at the same time to analyze the effects of random walks on this new graph. The intuition behind this task is to measure how the combination of link structure and usage data provide additional information to that containe d in these structures independently.

Our experimental results show that both hyperlink and click graphs have strengths and weaknesses when it comes to using their stationary distribution scores for ranking W eb pages. Furthermore, our evaluation indicates that the uni-fied graph always generates consistent and robust scores tha t follow closely the best result obtained from either individ ual graph, even when applied to  X  X oisy X  data. It is our belief that the unified Web graph has several useful properties for improving current Web document ranking, as well as for generating new rankings of its own. In particular station-ary distribution scores derived from the random walks on the combined graph can be used as an indicator of whether structural or usage data are more reliable in different situa-tions.
 H.3.3 [ Information Search and Retrieval ]; H.2.8 [ Information Systems ]: Data Mining Algorithms, Experimentation, Human Factors Search Engine Queries, Usage Mining, Structure Mining, Random-Walks, Web Graphs
In recent years, significant amount of research has been devoted to studying the Web graph (which we refer to as hy-perlink graph to avoid ambiguity) and the click graph . The hyperlink graph is the directed graph among Web pages in which edges represent hyperlinks. The click graph is a view of the information contained in query logs, i.e., a bipartit e graph between queries and Web pages, in which edges con-nect a query with the documents that were clicked by users as a result.

At an intuitive level, these two graphs capture two of the most common tasks of users on the Web: browsing and searching . A user who browses the Web essentially follows edges on the hyperlink graph, while a user who searches and consequently clicks on the result pages, is following edges on the click graph. Searching and browsing together are equiv-alent to the two prototypical actions of information seekin g and exploration.

The edges of these two graphs can capture certain seman-tic relations between the objects they represent. An ex-ample of such a relation is similarity : two pages connected together by a hyperlink, or a query and a page connected together by a click, are more likely to be similar than two non-connected objects [7]. Another presumed semantic re-lation is authority endorsement : a hyperlink from a page u to a page v , or a click from a query q to the page v , can both be viewed as implicit  X  X otes X  for page v [14]. These hypotheses provide a foundation for the research of several Web information retrieval problems, for instance clusteri ng of Web pages, queries and users, on-line community dis-covery. Similarity search exploits the similarity hypothe ses, while ranking leverages the authority-endorsement theori es.
Unfortunately both the hyperlink graph and the click graph have certain disadvantages. For example, Google X  X  PageR-ank [4] uses links in the hyperlink graph to compute im-portance scores for Web pages. As a result substantial ad-versarial effort has been put into artificially increasing th e PageRank score of Web pages. This adversarial effort takes the form of spam pages or link farms [10, 9].

Similarly, the click graph has its own disadvantages. One of those disadvantages is its sparsity: a page that is clicke d for a certain query must first appear in the list of results for that query. This may not be trivial considering the vast number of pages available for each query. Also, there is an issue of an inherent bias in any rankings produced by this graph, favoring already highly ranked Web pages. Another related problem is its large dependency on textual match-ing: typically search engines emphasize precision at the ex -pense of recall, and display only results which match exactl y all the query terms, causing many relevant pages not to be connected with queries if they are not exact matches. Fur-thermore, the click graph is also prone to spam, but in this case click spam which aims towards taking advantage of us-age mining algorithms to improve search ranking.
 Contributions of this work. In this paper we propose a new type hybrid Web graph, which combines the existing hyperlink and the click graphs, and we apply Web mining and link-analysis algorithms to it. This new graph, which we call the hyperlink-click graph , is a simple graph union: it has two types of nodes, pages and queries , with directed edges between pages according to the hyperlink graph, and undirected edges between queries and pages according to the click graph.

The union of these two graphs combines the traditional hyperlink graph, based on connectivity structure, and the click graph, based on search engine usage information. The purpose of this graph is to extend the traditional hyperlink graph into a graph which reflects more accurately users X  nat-ural behavior in the Web.

In particular we define and study random walks on the unified graph. We show that ranking according to the scores obtained from the hyperlink-click graph is similar to ranki ng using the score of the non-combined graph with the highest performance. The unified graph compensates where either the hyperlink or click graph execute poorly, being overall more robust and fail-safe. It is important to note that in modern Web search engines, link analysis scores in the style of PageRank might be only small components of the overall ranking function. Nevertheless, we compare directly to tho se scores in order to isolate the effect of the hyperlink-click graph.

Combining usage and content information in one structure can improve the quality of many Web mining algorithms. From our point of view, the two graph structures are com-plementary and each of them can be used to alleviate the shortcomings of the other. For example, using clicks to in-clude user feedback on the Web graph improves its resistance against link-spam. On the other hand, by considering hy-perlinks and browsing patterns we increase the density and connectivity of the click graph, and we can account for pages that users might visit after issuing particular queries. Applications of the hyperlink-click graph. There are several Web mining tasks in which the hyperlink-click graph can be used:
We plan to investigate some of these applications in future work. The main focus of this paper is the first application: enhancing the ranking of Web documents.
 Roadmap. The rest of the paper is organized as follows. In Section 2 we present the related work. In Section 3 we in-troduce our notation and provide a formal description of the graphs used in this paper. Section 4 discusses the random walk model, which is mainly used for ranking. In Section 5 we discuss our experimental results, and finally, in Section 6 summarizes our results and conclusions.
The Web is an extremely rich and highly interconnected source of information, which makes Web mining a very ac-tive research field. Given the space limitations, our covera ge of the topic is by no means complete.

In general, the information found on the Web can be ana-lyzed from three main points of view associated to the pre-dominant types of data found in it [18].
 Content: The information that the Web documents were Structure: The description of the organization of the con-Usage: This data describes the history of usage of a Web
There are several models for representing the information on the Web. The most popular view is the one based on structure. This approach sees the Web as a graph in which documents are nodes that are connected to each other when there is at least one hyperlink from one document to the other. This graph structure has been exploited by link-based ranking algorithms such as [4] and [12]. Both methods rank pages according to their importance and authority , es-timated by analyzing the endorsements or links from other documents.

In the work presented in [1] there is an overview of many other possible graph-based representations based on the co n-tent and usage data found on the Web. The focus is on the analysis of queries from search engines and their semantic relations, as well as their relations given by the clicks on c om-mon documents. Relations between queries can be inferred from common keywords or common clicked documents. In a similar way, relations between documents can be found by looking at shared links or words. The incorporation of docu-ment contents into these types of graphs is introduced from the words in queries, their selected documents, and also by the relations induced among documents with similar words.
With respect to usage data, a common model for query logs from search engines is in the form of a bipartite undi-rected graph. This graph includes two types of nodes: querie s and documents. Links between the two types of nodes are generated by user clicks from queries to documents in the process of selecting a search result. This type of represent a-tion was presented in [3] and used for agglomerative cluster -ing to find related queries and documents. Later, this view was expanded in [5] where weights were added to the undi-rected edges, based on the number of clicks from the query to a document. This graph is referred to as click graph . They study the effect of forward and backward random walks on this model for document ranking. They discuss that queries should be considered as soft relevance judgments, and that query logs give noisy and sparse data. The work of [5] sug-gest that an effective method is a backward random walk. On the other hand, the notion of unification of different Web data sources is not a new one. In [19] a framework is proposed for link analysis. This framework allows to model inter-type and intra-type links between different Web ob-jects. They discuss that any link-based model can be stud-ied within their framework and they focus their work on users and their browsing behavior. In particular they apply this to extend the HITS algorithm by incorporating users browsing patterns.

Noise and malicious manipulation of Web content affect both the click graph and hyperlink graphs. The most typical type of manipulation is link spam on the hyperlink graph [10, 9]. In this approach artificial links are created to induce higher link-based ranks on documents. In a similar way, clic k graph manipulation can be produced from artificial clicks on search engine results [16, 9]. The aim of this attack is to manipulate learned ranking functions that are based on click through information. Another type of noise that can be found in click through data is the bias of clicks due to the position of the search result. This bias has been studied and modeled, e.g. by [8, 6].

Another perspective on query logs is to avoid considering queries individually, but use them as sequences of actions. This is explored in [17] and serves a dual purpose: it reduces the noise due to single queries, and it allows the connection of different actions of users over time.
In this section we describe three types of Web graphs: the hyperlink graph, the click graph, and the hyperlink-click graph. We introduce the notation that is used in the paper, and describe the random walks that are performed over the graphs.
 The hyperlink graph. Given a set of N Web documents D we consider the hyperlink graph G H = ( D, H ) as a di-rected graph, where there is an edge ( u, v )  X  H if and only if document u has a hyperlink to document v , for u, v  X  D .
For a document u  X  D , the set of in-neighbors of u (the documents that point to u ) and the set of out-neighbors of u (the documents that are pointed to by u ) are denoted by N IN ( u ) and N OUT ( u ), respectively. That is, N IN ( u ) = { v  X  D | ( v, u )  X  H } and N OUT ( u ) = { v  X  D | ( u, v )  X  H } . For u  X  D , d IN ( u ) = | N IN ( u ) | is the in-degree of document u , and d OUT ( u ) = | N OUT ( u ) | is its out-degree . The click graph. Let Q = { q 1 , . . . , q M } be the set of M unique queries submitted to a search engine during a specific period of time. In practice, in order to construct the set of unique queries we assume some simple normalization, such as normalizing for space, letter case, and ordering of the query terms. For a query q  X  Q we denote by f ( q ) the frequency of the query q , that is, how many times the query was submitted to the search engine.

In a large-scale search engine query log, in addition to the information about which queries have been submitted, there is information about which documents are clicked by the users who submit those queries. Let D = { d 1 , . . . , d be the set of N Web documents clicked for those queries.
The click graph G C = ( Q  X  D, C ) is an undirected bipartite graph that involves the set of queries Q , the set of documents D , and a set of edges C . For q  X  Q and d  X  D , the pair ( q, d ) is an edge of C if and only if there is a user who clicked on document d after submitting the query q . The obvious prerequisite is that the document d is in the set of results computed by the search engine for the query q . To each edge ( q, d )  X  C we associate a numeric weight c ( q, d ) that measures the number of times the document d was clicked when shown in response to the query q .

As before, we define N ( q ) = { a | ( q, a )  X  C } the set of neighboring documents of a query q  X  Q , and N ( a ) = { q | ( q, a )  X  C } the set of neighboring queries of a document a  X  D . We then define the weighted degree of a query q  X  Q of a document a  X  D as d ( a ) = P q  X  N ( a ) c ( q, a ). The hyperlink-click graph. Quite simply, the hyperlink-click graph G HC can be seen as the union of the hyperlink graph and the click graph. There is a directed edge of weight 1 between documents u and v if there is a hyperlink from u to v , and there is an undirected weighted edge between query q and document d if there are clicks from q to d , and the weight of the edge is equal to the number of clicks c ( q, d ).
Given a graph G = ( V, E ) a random walk on G is a process that starts at a node v 0  X  V and proceeds in discrete steps by selecting randomly a node of the neighbor set of the node at the current step. A random walk on a graph of N nodes can be fully described by an N  X  N matrix P of transition probabilities . The i -th row and the i -th column of P cor-respond both to the i -th node of the graph, i = 1 , . . . , N . The P ij entry of P is the probability that the next node will be the node j given that the current node is the node i . Thus, all rows of P sum to 1, and P is called row-stochastic matrix.

Under certain conditions (irreducibility, finiteness, and aperiodicity, see [15] for definitions and more details) a ra n-dom walk is characterized by a steady-state behavior, which is known as the stationary distribution of the random walk. Formally, the stationary distribution is described by an N -dimensional vector  X  that satisfies the equation  X  P =  X  . Alternatively, the i -th coordinate  X  i of the stationary-distri-bution vector  X  measures the frequency in which the i -th node of the graph is visited during the random walk, and thus, it has been used as an intuitive measure of the impor-tance of each node in the graph.

Next we will consider random walks in the three different graphs we have introduced: the hyperlink graph, the click graph, and the hyperlink-click graph. We will denote the stationary distributions in those three graphs by  X  H ,  X  C and  X  HC , respectively. We will refer to the values of the stationary distribution vectors as scores .
 Random walk on the hyperlink graph. The random walk on the hyperlink graph corresponds to surfing the Web by following hyperlinks at random from the current Web page. The concept has been popularized through the sem-inal paper by Brin and Page [4], and its application to the Google search engine. The stationary distribution is also known as the PageRank vector. In the PageRank model, a step of following a random hyperlink is performed with prob-ability  X  , while the walk  X  X umps X  ( X  X eleports X  or  X  X esets X ) to a random page with probability 1  X   X  . Additionally, special care is taken when reaching a dangling node , a node with no outgoing edges. A common assumption is that upon reach-ing to a dangling node the random walk continues by select-ing a target node uniformly at random. Consequently, if A is the adjacency matrix of the Web graph G H , define N H be the normalized version A H so that all rows sum to 1. As-sume that N H is defined to take care of the dangling nodes, so that if a row of A H has all 0s, then the corresponding row of N H has all values equal to 1 /N . Finally, let 1 H matrix that has the value 1 /N in all of its entries. Then the transition-probability matrix P H of the random walk on the Web graph is given by P H =  X  N H + (1  X   X  ) 1 H .
In addition to yielding a better model of surfing the Web graph, performing the random jumps with probability (1  X   X  ) 6 = 0 ensures the sufficient conditions for the stationary distribution to be defined.

Random walk on the click graph. Random walk on the click graph is similar, except for the fact that the click graph is bipartite and undirected. Being bipartite creates periodicity in the random walk, while being undirected has the consequence that the stationary distribution is propor -tional to the degree of each node. However, assuming that we also perform random jumps with probability (1  X   X  ), then the random walk is aperiodic and irreducible (every node can be reached from every other node), and also the stationary distribution at each node is not a direct functio n of its degree.

Formally, the random walk on the click graph is described as follows. Let A C be an M  X  N matrix, whose M rows correspond to the queries of Q and the N columns corre-spond to the documents of D , and whose ( q, d ) entry has value c ( q, d ), the number of clicks between query q  X  Q and document d  X  D . Let A  X  C be an ( M + N )  X  ( M + N ) matrix defined by and let N C be the row-stochastic version of A  X  C . Here again we assume that N C is defined to take care of the dan-gling nodes, so that if a row of A C has all 0s, then the corresponding row of N C has all values equal to 1 / ( M + N ). Finally, let 1 C be an ( M + N )  X  ( M + N ) matrix that has value 1 / ( M + N ) in all its entries. Then the transition-probability matrix that describes the random walk on the click graph is P C =  X  N C + (1  X   X  ) 1 C .

Note that in [5] a backward random walk is used, while we consider instead a forward random walk.
 Random walk on the hyperlink-click graph. Using the notation that we introduced in the previous paragraphs, the random walk on the hyperlink-click graph is defined as follows: First overwrite A H to be an ( M + N )  X  ( M + N ) matrix, including also the M queries and assuming that all rows that correspond to queries are 0s. Then let N H be the row-stochastic version of A H , normalizing for dangling nodes X  X ote that all newly introduced queries correspond to dangling nodes X  X hile let N C be as before. Finally, let 1 = 1 C .

For combining the graphs introduce a querying probability  X  , which determines the rate at which a user switches be-tween querying a surfing behavior. The transition-probabil ity matrix for the random walk on the hyperlink-click graph is then given by Let us also describe at a high-level the random walk defined by the above equation. First, with probability (1  X   X  ) the walk goes to a random query or to a random document. With probability  X  , the walk follows a link in the hyperlink-click graph. The exact action depends on whether the cur-rent state is a document or a query. If the current state is a document u , then with probability  X  the next state is a query q for which there are clicks to u , while with probabil-ity 1  X   X  the next state is a document v pointed by u . If the current state is a query, then with probability  X  the next state is document for which there are clicks from the query, while with probability 1  X   X  the next state is any random document.

For our experiments, while we investigate the effects of the value of the parameter  X  to the results, we fix the value of  X  to be 0 . 85, since it is a value widely used for PageRank computation.
In this section we present the experiments performed in order to validate the utility of the scores produced by ran-dom walks on the hyperlink-click graph. We compare these scores to those generated by the hyperlink and click graphs independently. The objective of this section is to discover new information for improving the ranking of Web docu-ments.

For the comparison of the different random walk scores, we focus on two tasks in which a good ranking method should perform well. These tasks are: ranking high-quality doc-uments and ranking pairs of documents . The evaluation is centered on analyzing the dissimilarities among the differe nt models.

We begin by describing the datasets used.
As a data source we use an in-house query log. Due to the enormous size of the Web, we use only a small sample of documents and queries. Thus, we use only partial graphs instead of the full graphs. No publicly available Web doc-ument collections are used, because there are no collection s with query log information associated to them, which is a fundamental constraint for our experiments.

We create the graph data by using the query log as the starting point. First let us denote by D QL the set of all documents contained the query log. We parse the query log and we find all the documents that have 10 or more clicks. There are about 9 000 such documents in our sample, and we refer to them as seed documents D S .

We then use a Web crawl to find all documents that point to and are pointed to by the seed documents. Let D sets of documents with outlinks to and inlinks from D S , re-spectively, and let D ALL = D S  X  D IN  X  D OUT be the set of all documents encountered. The above expansion pro-cess increases the number of total documents (documents in D ALL ) to approximately 144 million.

It should be noted that documents gathered through this expansion process might also exist in D QL . We then define D C to be the documents in the intersection of D ALL and D QL , that is D C = D ALL  X  D QL .

Finally, the set of queries Q C that we consider are the queries that have at least one click in the set of documents D
C . In total, there are about 61 000 such queries. The dataset construction described above is shown in Figure 1. Given the above sets, we then define the three graphs we con-sider, the hyperlink graph, the click graph, and the hyperli nk-click graph as follows: Hyperlink graph: The nodes of the hyperlink graph G H Click graph: The nodes of the click graph G C are the doc-Hyperlink-click graph: The hyperlink-click graph G HC is The selected dataset reflects a consistent sample of the Web graph, although highly popular documents are chosen as a seed set, this is further expanded to include most of the neighboring documents. This expansion allows to include in the dataset an heterogeneous sample of documents which are connected to the initial set. Additionally, the query log da ta is processed very quickly using the MG4J 1 and fastutil 2 available on-line. This computational cost is almost negli gi-ble compared to that of processing the hyperlink graph.
As described in Section 5.1, our experimental datasets are partial and they only represent a sample of the whole Web. Hence, to make the obtained results comparable, we analyze only the results for the documents contained in the intersec -tion of the click, hyperlink and combined graphs (which we refer to as D C ). However, it is important to note that we use all of the nodes in each graph to compute the random walk results, and not only the ones contained in D C .
We compute  X  H ,  X  C and  X  HC for the values of  X  = count that even for very large values of  X  , random walks on G
HC are quite different from those on G C . This is due to the high influence of G H on the combined graph and is observed in throughout the evaluation.
 To compare the random walk results, we decided to focus on high-quality Web documents and how they score within the different models. The hypothesis we sustain is that it is desirable for a good model to score high-quality documents above other documents. To measure this, we use documents from the dmoz document directory. 3 Our working hypothe-sis is that since dmoz is editorially maintained, on average, documents in this directory are of higher quality than doc-uments not in the directory. Consequently, we use D Z to denote the set of documents in the evaluation set D C that belong also to the dmoz directory. Following our working hypothesis, we postulate that the graph that produces the best ranking results is the graph that ranks documents in D Z higher than the rest of the documents in D C .

To quantitatively measure the agreement of the rankings produced from the different graphs with the dmoz directory, we use two measures:  X  : Our first measure is the normalized sum of the  X  scores http://mg4j.dsi.unimi.it http://fastutil.dsi.unimi.it http://dmoz.org Algorithm 1 Micro-evaluation 1. define a set of queries Q  X  Q C  X  G C that have at least 2. Compute the average values of  X  Z and  X  Z .
  X  : The second measure we use is inspired by the Goodman-
We evaluate the proposed measures  X  Z and  X  Z in two levels of granularity, which are are defined as follows: Macro-evaluation: This evaluation intends to capture the Micro-evaluation: This evaluation is performed at query
The results obtained in the macro and micro evaluations are shown in Table 1 and Table 2, respectively. The macro-evaluation results show that for the  X  Z the best value is obtained for G H and the worst for G C . On the other hand, in the  X  Z the roles are reversed with G C being the overall graph with less inverted elements and G H the one with the most number of inverted elements. The results of the G HC follow closely the best performing scores with less than 0 . 003 difference for  X  Z and 0 . 085 difference for  X  Z .
The micro-evaluation results, in Table 2, shows that for the  X  Z metric, G HC obtains the best value followed by G For the  X  Z metric G C is the best, and G H is the worst in both  X  Z and  X  Z .

These metrics observe the performance of the random walk scores using different perspectives. From our point of view a good scoring method should perform well both at macro and micro level. The results obtained show that the random walk scores on the G HC follow closely the best scores generated by the non-combined graphs.
 In addition to evaluating our rankings using the measures  X  and  X  Z , which are based on the assumption that documents in dmoz are on average of high quality, we also perform a user study.

We evaluated a set of triples of the form ( q , d 1 , d 2 is a query with at least 10 clicks in total, d 1 and d 2 are two distinct documents returned by the search engine for that query. Also, we limited the evaluation to cases in which the ordering of d 1 and d 2 was different according to at least two scoring methods in {  X  H ,  X  C ,  X  HC } . The evaluation interface is shown in Figure 2. Users where presented a randomly selected triple and asked:  X  Is one of these pages clearly better for the query q ?  X . They were also given the option to say that the two documents were about the same, or that they could not be compared.

A group of 13 human assessors participated in the eval-uation. A total of 1 , 710 assessments were collected, from which 515 (32%) expressed preference for one of the two documents. There were 82 cases in which more than one evaluator assessed the same triple and expressed a preferen ce for one of the two documents. In those cases, the agreement among the evaluators was 70%. Still, the assessment process proved to be very difficult since many of the selected pairs of documents have only a marginal difference in their scores.
The results of the user study are shown in Table 4, using again the  X  statistic to measure the agreement between the rankings of the algorithms and the rankings induced by the human evaluators.
 Table 4:  X  of ranking functions with human prefer-ences Table 5:  X  of ranking functions with human prefer-ences for  X   X  4 . 5 10  X  7 (38% of unique pairs)
Due to the marginal difference in scores between many pairs of documents, we study the behavior of  X  for the pairs of documents which have a greater difference between their scores. For this we evaluate only the pairs of documents ( d , d j ) for which all scoring methods have a minimum  X  = |  X  ( d i )  X   X  ( d j ) | . This allows to evaluate pairs that are less ambiguous to assess for humans. As a result we found that for values of  X   X  4 . 5 10  X  7 the  X  values of  X  C and  X  HC are reversed and that  X  HC produces the best performance at this point (shown in Table 5).

If we continue to increase the minimum value of  X  we obtain the results shown in Figure 3. Figure 3: Behavior of  X  in the user study when re-stricting the minimum allowed value of  X  .
 The click graph, just as the hyperlink graph can be prone to induced variations, which can affect the scores of the random walk. For the hyperlink graph it is well know that typical variations are produced by link-spam. In the case of the clic k graph, undesirable modifications in the random walk scores can be the consequence of different methods that increase the number of clicks, such as click-spam. Other variations o n the click-through data can occur from sponsored placement of search engine results, in general these do not represent a practical problem, since in general they can be filtered from a query log. Nevertheless we will study the effects of induced variations by using clicks on sponsored results to simulate click-spam.

In the previous part of the evaluation sponsored clicks were filtered from the G C and G HC . We repeat this evalua-tion introducing sponsored click-through data into G C and G
HC . Tables 6 and 7 show the results of the high-quality document evaluation with this variation. We can observe that in the macro-evaluation the order prevails with respec t to the original results. On the other hand, in the micro-evaluation G HC performs better for both metrics.
The user study was repeated with 5 judges, which did 1 , 576 assessments in total, from which 588 (37%) expressed a preference for one of the two documents. Unlike the results of the user study without sponsored clicks, in this case user s agreed more with  X  H and less  X  C , i.e.: results were reversed. Nevertheless, the results for  X  HC remained in the middle (see Table 8).
 Table 8:  X  of ranking functions with human prefer-ences using click-through data with sponsored clicks In Tables 9 and 10 we provide a concise summary of the metrics and types of evaluations used to measure the quality of the different random walk scores. The convention that we use is that G A &gt; G B means that the ranking generated using the graph G A is better than the ranking generated using the graph G B (according to our measures), while G A  X  G B means that the difference between the two rankings is less than 0 . 1.
 In Figures 4 and 5 we show a comparison of metrics  X  Z and  X 
Z with click variations and without variations. In this Fig-ures we can observe that the values for G HC are always very close or better than the best result from the non-combined graphs. This result is independent on whether or not click variations where induced into the data.

Overall the different tasks evaluated reflect consistency in the results. The values obtained for the study performed with dmoz documents are coherent for the variations in the value of  X  , and furthermore, they agree with the results obtained from the user evaluation. We consider this as an indicator of the usefulness of the evaluation and its metric s.
In this paper we studied the effects of a random walk on a unified Web graph. This Web graph combines both hy-perlinks between documents and clicks from queries to doc-uments, and was created to capture more completely users X  searching and browsing behavior in the Web.

Our main motivation for studying this unified graph is to analyze the new information that it can provide. As a first approach, we focus on the task of using this model to en-hance Web document ranking. For this we used a number of different evaluation metrics in order to assess the rankin g produced on tour graph with respect to rankings produced by the hyperlink and click graphs. We evaluated by analyz-ing useful tasks for ranking, such as, ranking high-quality documents and also ranking pairs of documents. For the later, we conducted a user study which provided consistent results with the rest of the evaluation. On the other hand, we also tested the tolerance of our model to click variations or noisy data.

Our experimental evaluation shows that the scores gener-ated by random walks on the combined Web graph have sev-eral useful properties for document ranking. Overall these scores produce good quality results which are very stable and tolerant to noisy clickthrough data. Additionally, our results show that the unified graph is always close to the best performance of either the click or hyperlink graph. Further -more the results on the combined graph never approximate the lower bound according to any metric, while the non-combined graphs do not generate good results in all cases.
It is our belief that these properties of the unified graph are useful for improving current ranking techniques. Partl y as an indicator of how reliable link-based ranks and click-based ranks are for different tasks. As well as an indepen-dent indicator of document quality.

As part of future work we would like to analyze how to deal with the inherent bias that exists in any ranking technique based on usage mining. This is, that pages with already high stationary distribution scores are presented to users more often as a query result. Thus, high ranked pages tend to be more clicked. In long term use, this could create a self-reinforcing ranking. This is not an easy problem to solve and it depends mainly on other underlying ranking techniques. Therefore we recommend any click-based ranking as as a complement to other independent ranking methods.

Also in the future we would like to analyze other Web mining applications for the hyperlink-click graph, such as : link and click spam detection and similarity search.
The code used for the weighted random walk described in this paper is available at http://law.dsi.unimi.it/satellite-software/
Acknowledgments. The authors thank Claudio Corsi from the University of Pisa for his help with data preprocess -ing. Also we thank Debora Donato and Vanessa Murdock from Yahoo! Research for valuable discussions and feedback . [1] R. Baeza-Yates. Graphs from search engine queries. [2] L. Becchetti, C. Castillo, D. Donato, S. Leonardi, and [3] D. Beeferman and A. Berger. Agglomerative clustering [4] S. Brin and L. Page. The anatomy of a large-scale [5] N. Craswell and M. Szummer. Random walks on the &gt; G &gt; G [6] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [7] B. D. Davison. Topical locality in the web. In [8] G. Dupret, V. Murdock, and B. Piwowarski. Web [9] D. Fetterly. Adversarial information retrieval: The [10] Z. Gy  X  ongyi and H. Garcia-Molina. Spam: It X  X  not just [11] G. Jeh and J. Widom. Simrank: a measure of [12] J. M. Kleinberg. Authoritative sources in a [13] W. Kruskal and L. Goodman. Measures of association [14] M. Lifantsev. Voting model for ranking Web pages. In [15] R. Motwani and P. Raghavan. Randomized [16] F. Radlinski. Addressing malicious noise in [17] F. Radlinski and T. Joachims. Query chains: learning [18] J. Srivastava, R. Cooley, M. Deshpande, and P.-N. [19] W. Xi, B. Zhang, Z. Chen, Y. Lu, S. Yan, W.-Y. Ma,
