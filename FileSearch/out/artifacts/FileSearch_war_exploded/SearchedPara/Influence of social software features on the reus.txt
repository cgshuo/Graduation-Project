 1. Introduction Knowledge is a company X  X  most important resource in today X  X  knowledge-based economy ( Grant, 1997; Nickerson &amp;
Zenger, 2004 ). While each employee possesses knowledge individually, it is the primary task of the company to manage all available knowledge and to integrate it into products and services ( Grant, 1996 ). Therefore, firms aim to implement effec-tive knowledge management (KM) processes including knowledge creation, capture, distribution, and reuse ( Alavi &amp; Leidner, 2001 ). The first three processes form the basis of KM, whereas the effective reuse of existing knowledge assets can help to gain competitive advantage ( Davenport &amp; Prusak, 2000 ) because it helps to prevent employees from re-creating redundant knowledge and thereby saves time and money ( Akg X n, Byrne, Keskin, Lynn, &amp; Imamoglu, 2005 ). Knowledge does not refer only to a single chunk of knowledge but can also refer to complex digital assets such as program code, system design infor-mation, an instruction manual, the description of a case and its solution, or a report. Probably the most studied type of  X  knowledge reuse in the context of information systems (IS) is software reuse ( Frakes &amp; Fox, 1995 ). Three phases can be dis-tinguished in knowledge reuse: (1) Retrieval of potentially relevant knowledge. (2) Evaluation of knowledge usefulness for the task at hand. (3) Actual use incl. possible adaptations, if the knowledge was considered useful.

Phase 1 has been studied extensively and phase 3 has also received some attention (e.g., in case-based reasoning ( Aamodt &amp; Plaza, 1994 ). The second phase did not receive much attention; it is usually implicitly included in phase 1. However, human decision makers do not all interpret the facts and signals they receive in the same way. They often perceive them differently or pay attention to a different subset of signals. Therefore, we concentrate on the second phase to better under-stand whether and which facts and signals may influence their decision on reuse.

One important part of an effective knowledge management is Business Intelligence (BI) ( Gold, Malhotra, &amp; Segars, 2001 ) which is defined as a  X  X  X trategic approach, for systematically targeting, tracking, communicating and transforming relevant weak signs into actionable information on which strategic decision-making is based X  X  ( Rouibah &amp; Ould-ali, 2002 ). It includes the generation and distribution of reusable reports as well as statistical and mathematical analyses (e.g., data mining). In the early days of electronic data processing, reports have been developed by programmers based on user specifications. Nowa-days, knowledge workers have been empowered to create reports by themselves which is called self-service BI (SSBI) ( Evelson, 2012 ). Self-service BI is supported by user-friendly tools and an increasing amount of data and new data sources which enable knowledge workers to perform more detailed analyses than previously possible ( McAfee &amp; Brynjolfsson, 2012 ).
In addition to strategic and tactical analyses, which are standard tasks of BI systems, operational analyses are increasingly performed ( B X hringer, Gluchowski, Kurze, &amp; Schieder, 2009 ). More granular and current data are available for this purpose, while new and often inexperienced user groups are trying to use the technology. In this situation, organizations have to find ways to disseminate new information more effectively ( Bevanda &amp; Pavletic  X  , 2007 ) and to increase its reuse. How important this issue may become show the figures reported in Eckerson (2008) . An energy company found after a few years of adopting
SSBI 26,000 reports stored by only one department. This huge amount of reports precluded people from using them rather than attracting them. After perusal of the reports, the number was cut down to 300 reports containing almost the same infor-mation. This indicates that with better reuse the growth of the number of (possibly redundant) reports would not have been so dramatic.

Therefore, we study the cognitive preconditions of reuse of previously created reports. People will use them if they find them useful to (partly) satisfy their information needs. Reuse of a report may mean the use of a report as it is, the application of the same reporting procedures to another data set (e.g., a report designed for country A is executed on the data of country
B), or an adaptation of the report to include, for example, an additional calculation. Of course, combinations of the latter two adaptations are also possible, i.e., a change of the data set and calculations.

Another advancement of the last years is the rise of Web 2.0 or social media ( O X  X eilly, 2007 ). First, they became popular in the private realm but meanwhile they have entered the corporate world where they are supporting the move toward Enter-prise 2.0 ( McAfee, 2006 ). This integration of social software tools in corporate intranets offers one possible solution to the challenge of targeted report dissemination and reuse. BI reports can be enriched by social software features such as tagging, rating, information on frequency of use, comments, and information on the identity of the report author or other report users. This is possible if report creators make their reports available on a BI portal so that information on their use by other users can be added over time, partly automatically (e.g., frequency of use). Software vendors like SAP ( SAP StreamWork, 2013 ), IBM ( IBM Connections, 2013 ), and Microsoft ( Microsoft SharePoint, 2013 ) have already expanded their BI portals to incorporate some of the mentioned features. From a research perspective, Meredith and O X  X onnell (2010) present a mock-up of an analysis tool with social media functionality while B X hringer et al. (2009) describe a design prototype which inte-grates social aspects in a BI portal (see Fig. 1 ).

However, these prototypes are based on conceptual thinking and the suggested features were not tested regarding their influence on report reuse. Thus, our study aims to examine if the enrichment of BI reports by social software features influ-ences their reuse and to reveal the underlying patterns of the influence processes.
 The most commonly used theory to examine influence processes in IS research is the elaboration likelihood model (ELM).
ELM studies how people form and change attitudes based on the information they receive. Perceived usefulness of an IS or a report is such an attitude. Perceived usefulness of an IS artifact can be defined as the degree to which people believe that the use of this artifact would improve their working performance ( Davis, Bagozzi, &amp; Warshaw, 1989 ). ELM distinguishes two per-suasion processes by the type of information processed and explains under which circumstances an information recipient might be more influenced by one process or the other ( Petty &amp; Cacioppo, 1986 ). Utilizing ELM to address the above men-tioned research gap can help us to answer the following research questions in particular: (1) Which social software features influence the perceived usefulness of reports designed by other people? (2) Are these influences moderated by job-related characteristics of the report user and if so, how?
Answering the two questions is both theoretically and practically important. On the one hand, we advance theoretical knowledge by examining the role and nature of influence processes of social software features in a software environment initially designed to present purely objective data. On the other hand, the study can help practitioners to adjust the imple-mentation of these features in order to fit the company X  X  BI user X  X  needs.

The paper is organized as follows: first, we provide the conceptual background and give an overview of previous appli-cations of ELM in the IS context. Second, we describe the core factors of this study and derive the hypotheses. Then, the research model is empirically tested using data from an experiment with knowledge workers who regularly receive or gen-erate BI reports. We end with a discussion of the study X  X  results and practical implications. 2. Background 2.1. Theoretical background
A combination of two major research streams provides the theoretical foundations for our study. On the one hand, lit-erature on technology acceptance provides us with a strong dependent variable proven to influence behavior and, on the other hand, literature on attitude change serves as a starting point to identify factors influencing the adoption of information and variables affecting this process.

The Technology Acceptance Model (TAM) is an extension of the theory of reasoned action ( Fishbein &amp; Ajzen, 1975 ) and was introduced to assess the adoption process of IS users ( Davis et al., 1989 ). TAM suggests that user adoption of a new tech-nology is strongly influenced by perceived usefulness and perceived ease of use. While perceived usefulness is defined as the degree to which people believe that the use of a specific IS artifact would improve their working performance, perceived ease of use is the degree to which people believe that the use of a specific information systems artifact would be free of effort ( Davis et al., 1989 ). These two independent variables are related to the attitude toward the technology which is itself a pre-dictor of behavioral intention to use the technology and the actual technology use. However, TAM does not examine how perceptions on usefulness are formed or, in other words, how information is evaluated in the formation of attitudes.
Attitude change and formation is usually conceptualized using the ELM ( Petty &amp; Cacioppo, 1981, 1986 ). Since an extensive body of extant empirical literature found support for ELM (a detailed overview is given in the next section), we focus on this approach to develop our research model. ELM is based on the central assumption that information recipients build their atti-tude about given information via two different routes (central and peripheral) depending on their thoroughness of informa-tion assessment and assumes the exclusivity of either route. ELM suggests that whether information recipients take the central route or the peripheral route to form an attitude depends on their elaboration likelihood. Elaboration likelihood describes the motivation and ability to evaluate given information and elaborate the main arguments ( Bhattacherjee &amp; Sanford, 2006 ). In information systems research, elaboration likelihood is typically operationalized as a combination of the information recipients X  expertise (reflecting ability) and job relevance (reflecting motivation) ( Bhattacherjee &amp; Sanford, 2006; Sussman &amp; Siegal, 2003 ).

The first conceptual route of ELM is the so called central route. It can only be taken if an information recipient is able (e.g., because of a high expertise in this field) and motivated (e.g., because of a high personal job relevance) to scrutinize the pre-sented information and assess the quality of the arguments included. When a recipient X  X  expertise and job relevance are high, it is more likely that s/he is able to base their judgment on the argument X  X  usefulness rather than relying on heuristic cues.
When elaboration likelihood is low, information recipients resort to a second route which is called the peripheral route. It encompasses peripheral cues which automatically exist (e.g., the personal characteristics of the messenger) or are purposely provided (e.g., a quality seal) in addition to the actual message or information. These cues do not necessitate a deep cognitive elaboration of the message. Neither expertise on the particular topic nor strong motivation is needed to evaluate them. The peripheral cues often represent source credibility which is commonly identified as a combination of expertise and trustwor-thiness of the person, originating the information ( Pornpitakpan, 2004 ). It can be defined as the belief that the information provider is a reliable information source.

Combining these literature streams can help to gain knowledge on information influence processes which in turn can guide the design of IS to improve the reuse of knowledge assets in internal databases. 2.2. Previous research
Many articles that apply ELM in the context of IS have been published meanwhile. We identified about 70 such articles in major journals and conferences that were published in the last 16 years. They can be classified into two groups: (1) Research that studies attitude formation (and in few cases attitude change) like in the original ELM. (2) Studies of information influence that use the concept of perceived usefulness or a similar construct instead of attitude
Since perceived usefulness is a constituent part of attitude ( Rosenberg &amp; Hovland, 1960 ), these studies can also be char-acterized as measuring the cognitive part of attitude (see discussion in Section 3 ). Such studies usually do not attempt to measure the general attitude of users toward a technology but to understand how they perceive or react to information received from or via the system. For example, Sussman and Siegal (2003) study the reaction of employees to advices given by co-workers via e-mail. The attitude toward e-mail is not the focus of the research although it is the underlying technology. The majority of the articles fall into this group.
 Another way to classify the articles is by the organizational setting in which the technology is used ( Kayhan &amp;
Bhattacherjee, 2010 ). The majority of the articles study information systems that are used for private purposes. In terms of e-business, these settings could be categorized as B2C or C2C settings. For example, Tam and Ho (2005) research person-alization as a persuasion mechanism on web sites, Angst and Agarwal (2009) study the attitude of patients to electronic health records, and Luo, Luo, Schatzberg, et al. (2013) analyze factors of credibility of recommendations in online communities.

Here, we review articles that study information influence based on ELM within an organization, since this corresponds to our research setting. This includes studies in which the research does not take place within an organization that uses the IS under observation but the research is positioned in such a scenario. This can be the case, for example, when an experiment is carried out with student respondents. The papers are reviewed in the chronological order of their appearance.
Mak, Schmitt, and Lyytinen (1997) conduct an ELM-based experiment in the context of expert systems (ES). They use the ambiguity of the decision setting as the variable determining the central route of influence while perceived credibility of the experts building the system represents a peripheral cue. They study whether the influence of these variables on the user deci-sion to accept the system X  X  advice is moderated by user perceived participation in knowledge update of the ES which represents user motivation, i.e., elaboration likelihood. They determine that ELM predictions fully apply for a high level of participation in knowledge update but only partially for a low level of participation. Dijkstra (1999) also conducted several experiments with a mock-up ES using ELM. The setting in these experiments is not quite clear but we consider the work as  X  X  X rganization-al X  X  following the interpretation of Kayhan and Bhattacherjee (2010) and review the latest experiment here. It reveals that students with low elaboration likelihood accept wrong advices more often than those with high elaboration likelihood which also corresponds to ELM.
 The influence of advices (arguments) given to a consultant by a co-worker through e-mail is studied by Sussman and
Siegal (2003) . They evaluate a survey of consultants from one city office of a multinational public accounting company. Each consultant answered the survey based on a self-selected e-mail. They use perceived usefulness (of advices) as the dependent variable as mentioned above. The consultants were influenced by both, argument quality and source credibility. In general, perceived argument quality can be defined as the extent to which a user of an application perceives the provided information to be complete, unambiguous, meaningful, and correct ( Wand &amp; Wang, 1996 ). Perceived source credibility is the degree to which people believe that the information provider is a reliable information source. The influence was significantly moder-ated by expertise (representing cognitive ability) and only slightly by involvement (representing motivation), the two indi-cators of elaboration likelihood. In addition, the effect of argument quality and source credibility on the perceived adoption of the advice is mediated by its perceived usefulness.
Angst and Agarwal (2004) combine ELM with the theories of social learning ( Bandura, 1977 ) and social influence ( Kelman, 1961 ). They posit that internalization influences users via the central route to commence using a system and keep using it in the long run. They expect that compliance and identification work via the peripheral route but that their influence fades over time. Their study of the use of a customer relationship management system (CRM) within a bank only partially confirms their expectations. Compliance led to an enduring, and even increasing use of the CRM. Identification also led to an increased system use via the peripheral route.

Bhattacherjee and Sanford (2006) study the intended adoption of a document management system (DMS). The authors survey personnel from a city government in a Ukrainian city. Their model combines ELM with TAM in such a way that argu-ment quality and source credibility influence perceived usefulness which influences (affective) attitudes toward the system.
Attitudes are also directly influenced by source credibility. Perceived usefulness and attitudes influence usage intentions. All main effects are confirmed as predicted by ELM and TAM. This is also true for moderating effects except in the case of job relevance, which represents motivation, where the effect on the relationships between source credibility and perceived use-fulness/attitude is unexpectedly positive. The authors find an explanation for this observation, but they point out the com-plex nature of the construct source credibility.

Fadel, Durcikova, and Cha (2008) study information influence on the perceived usefulness in a knowledge management system (KMS) following the approach of Sussman and Siegal (2003) . They basically use the same research model leaving out the adoption variable. The information whether the document was already validated (by a committee) or not was added as a peripheral cue. Source credibility is represented by the position and an experience rating of the document author. The experiment is conducted with undergraduate business students. The findings are surprising since higher argument quality led to lower usefulness ratings. The authors explain this with more length of documents with higher argument quality. Source credibility had no effect on perceived usefulness. Only the validation variable positively affected usefulness ratings.
Kayhan and Bhattacherjee (2010) study the use of knowledge repositories (KR) under two different governance regimes, expert and community governance. Accordingly, they add credibility of governance mechanism as a peripheral cue to the ori-ginal ELM. Their dependent variable is the intention to use the knowledge which leads to actual use. The paper does not report empirical results since it is a research in progress.

Li (2013) analyzes the effect of training in an Enterprise Resource Planning (ERP) system on the intention of trainees to use it. She does not model elaboration variables of ELM, cognitive ability and motivation, but adds normative and informa-tional influence as variables. These are influenced by the elements of source credibility and argument quality and influence themselves the formation of attitude that is modeled as a tripartite construct (consisting of affective response, cognitive response, and behavior intention). The results confirm ELM predictions and show, in addition, that cognitive response is much more crucial for the formation of behavior intention than affective response. Jung, Srite, Haseman, and Jung (2013) also studied the attitude toward an ERP system but the emphasis of the research was on students education in ERP systems in a university rather than on their use at work.

In summary, except for Fadel et al. (2008) , ELM was mostly but not completely confirmed. Some uncertainty remains about certain moderation effects and peripheral cues: Communication between trainer and trainee ( Bhattacherjee &amp;
Sanford, 2006 ) or information exchange via e-mail ( Sussman &amp; Siegal, 2003 ) typically happen among familiar partners. This means, the source of information is personally known to the information recipient. If the informant is known to the infor-mation recipient, task-specific criteria may be overshadowed by the personal relationship or  X  X  X ource likeability X  X  ( Bhattacherjee &amp; Sanford, 2006 ). Sympathy or antipathy, competition among individuals, or other aspects that are not related to the task are mixed with task-related criteria. Therefore, it cannot be known whether the influence is based on the personal relationship or on (cues about) the informant X  X  knowledge, experience, or another task-related feature. Task-related credibil-ity of informants should depend on task-specific criteria like problem relevant experience or on the community view, esp. in a collaborative setting.

We advance knowledge on information influence by designing an experiment, as described below, for cases where co-workers do not know each other personally. This is not an unrealistic situation since in big organizations computer-mediated communication and collaboration often take place between partners who do not know each other personally. This is espe-cially the case in organizations with many (international) locations. We design the experiment in such a way that source credibility can be measured without interference from personal relationships and possible prejudices, i.e. source likeability.
Research on information influence can also be based on other conceptual models ( Rieh, 2002 ). We concentrate on ELM-based research because this allows direct comparisons and in order to support the building of cumulative knowledge on informational influence within IS and across disciplines. Table 1 briefly summarizes the reviewed literature. 3. Model development and hypotheses
ELM models attitude as the dependent variable which is influenced by argument quality and peripheral cues. However, recent IS research building on this framework extends or exchanges the dependent variable with variables such as useful-ness, intention to use, and (actual) use rather than measuring attitude before and after a stimulus ( Bhattacherjee &amp; Sanford, 2006; Fadel et al., 2008; Kayhan &amp; Bhattacherjee, 2010; Sussman &amp; Siegal, 2003 ). The aim is to combine the explanation of influence processes of ELM with the strong effects of perceived usefulness on the actual behavior proven by models like TAM ( Davis et al., 1989 ). This modification of the original ELM is justifiable and can clarify the results of persuasion studies as described below. Social psychological research has shown that attitude is a broadly defined construct consisting of three underlying components: affect, cognition, and behavior ( Rosenberg &amp; Hovland, 1960 ). While affect describes an emotional response and behavior means an overt response to a certain stimulus, the cognitive component is characterized by beliefs, knowledge structures, or perceptions as a response to the stimuli. Given this tripartite structure of attitude, it is ambiguous to measure attitude as one construct and, therefore, necessary to specify the component which is the focal point of a research project ( Breckler, 1984 ) if not all components are considered.

Most persuasion studies in social psychology rely on the cognitive component of attitude because the stimuli are logical lus, a report in a BI portal, contains information that supports a cognitive task, we apply this well-proven approach and con-centrate on perceived usefulness as the cognitive component of attitude as our dependent variable. In the context of IS and
ELM, this is also strongly supported by results in Li (2013) as reported above. Since we cannot measure actual long-run report reuse in an experiment, we employ perceived usefulness as its strongest predictor.

As described above, the central route of persuasion is based on the quality of information. In cases when more than a sim-ple piece of information is conveyed, like a whole report in our case, this aspect is usually described as argument quality. A BI report  X  X  X rgues, X  X  for example, that a plan is (not) being met, that some organizational units perform better than others, that a relationship between two variables exists, or something similar. As defined above, perceived argument quality relates to information completeness, lack of ambiguity, and correctness. Obviously, an incomplete or unclear report (e.g., missing col-umn headings in a table) would be difficult to comprehend and use. Hence, we hypothesize: H1. A high argument quality has a positive effect on the perceived usefulness of the report.

In general, ELM suggests that if the information recipient X  X  expertise on the topic is not sufficient, he resorts to peripheral cues in the evaluation process ( Petty &amp; Cacioppo, 1986 ). The above mentioned studies on information adoption have iden-tified perceived source credibility as a major peripheral cue in IS. However, in the two most cited studies, the informant and the information recipient have known each other. Consultants received advice via e-mail from peers in their company ( Sussman &amp; Siegal, 2003 ) and course participants received information from their course instructor ( Bhattacherjee &amp;
Sanford, 2006 ). We cannot know in these cases whether credibility was based on sympathy toward the informant or her expertise as perceived by the information recipient. If an organization desires that information is not judged based on source likeability, but supports credibility cues, a different experimental set-up and different cues must be used. Kelman (1961) attributes source effects to three kinds of social influences: internalization, identification, and compliance.
Accepting information from sources with high expertise and integrating this information into one X  X  own cognitive system is called internalization. Identification describes the phenomenon that individuals tend to hold similar opinions as reference persons or groups. Compliance refers to conforming to a powerful source (a person or a group of people) on the basis of rewards and punishments ( Karahanna &amp; Straub, 1999 ). Our experiment is designed in such a way that respondents do not know each other personally and, thus, do not stand in a hierarchical relationship. Compliance cannot be assumed under such conditions since rewards and punishments imply a hierarchical relationship between the parties concerned ( Petty &amp; Wegener, 1998 ). Power in an organization is often legitimated by the hierarchical relationship between people ( Koslowsky &amp; Schwarzwald, 2001 ). Thus, social influence processes in our research context can be assigned to identification and internalization but not to social compliance. We operationalize them as explained in the following.

First, we turn to possible identification cues. The usefulness of a report to other users can be displayed via usage or sub-scription statistics. Within BI portals usage or subscription statistics can be easily collected. Usage statistics disclose actual demand for the content. For example, they are given in social networks in the form of  X  X  X  users have looked up your profile last week X  X  or in article repositories in the form of  X  X  X his paper has been downloaded x times last month. X  X  The figures can be interpreted as indicators of interestingness of content. Subscription usually indicates an evaluation based on the consump-tion of the content. The number of users subscribing to some content or distributing it can be found in various social net-works and micro blogging services (e.g., retweets or following people on Twitter). We choose a subscription figure because it can be interpreted as the extent of endorsement for the respective report ( Pee, 2012 ). Hence, we hypothesize: H2. A high number of report subscriptions has a positive effect on the perceived usefulness of the report.

More explicit evaluations of a specific content by other users are commonly displayed on e-commerce sites in form of user ratings and recommendations (e.g., amazon.com). These reflect the users X  opinions of the quality of products or content.
Meredith and O X  X onnell (2010) suggest that this function should also be included in BI systems. Research on the effect of ratings in B2C-settings suggests that ratings are especially effective if the rating is supplemented by a rationale ( Willemsen, Neijens, Bronner, &amp; de Ridder, 2011 ). However, an evaluation of report content may be also perceived as an  X  X  X -valuation X  X  of the report author. In an organizational setting, authors may be hesitant to contribute reports fearing that nega-tive report ratings may reflect on them unfavorably. But raters, too, may be hesitant and biased in rating their peers X  contributions ( Toegel &amp; Conger, 2003 ). Therefore, we do not use user ratings as an identification cue. Obviously, this is a question of organizational culture which different organizations may answer differently.

Internalization is the second relevant social influence process that can be observed in our study. An information recipient (the BI report user in this case) internalizes the opinion of others more likely if the information was created by an expert source ( Kelman, 1961 ). In enterprises, the hierarchical level can be seen as an indicator of experience and expertise which means that a high position of an informant signals a high task-related credibility. Weisband, Schneider, and Connolly (1995) have shown in several experiments that persons with high status exhibit more influence than persons with a lower status.
The disclosure of the hierarchical level of the report author has been recommended for BI systems ( B X hringer et al., 2009 ). It can be signaled, for example, through the display of the position title. Hence, we hypothesize: H3. A high hierarchical level of the report author has a positive effect on the perceived usefulness of the report.
An alternative cue that supports internalization could be the expert level of the report author. In privately used social media the expert status of a contributor may be indicated, for example, by the number of stars, a label (e.g.,  X  X  X ovice X  X  or  X  X  X x-pert X  X ), or a  X  X  X arma X  X  figure. The status is gained either just based on activity within the application and/or based on other users X  evaluations of the author X  X  contributions (e.g.:  X  X  X ate how valuable this review was to you X  X ). We did not incorporate such cues because the number of contributed reports may be strongly correlated with the job function (which is already partly contained in the position title) and peer evaluations could again lead to unintended effects.

A central assumption of ELM is that users take the central or the peripheral route depending on their ability and moti-vation to elaborate information ( Petty &amp; Cacioppo, 1986 ). Accordingly, these are the two characteristics of information reci-pients we want to analyze in this context. In IS research, individual motivation has been operationalized by job relevance ( Bhattacherjee &amp; Sanford, 2006 ). It is assumed that the motivation of employees to elaborate information produced by an application rises if this very application is an important part of their daily job and may influence their work performance.
In contrast, users in jobs where this application is perceived to be less relevant may spare the effort of elaborating the argu-ment and rely on peripheral cues. In other words, job relevance moderates information influences.

The ability dimension of ELM is captured in IS research as recipient expertise, user expertise, or prior knowledge ( Bhattacherjee &amp; Sanford, 2006; Pee, 2012; Sussman &amp; Siegal, 2003 ). We choose the term user expertise. Expert users can assess the provided information well on the basis of their domain and, in our case, based on methodological and software knowledge. For example, understanding the display of multidimensional data in a two-dimensional table requires some experience or expertise. Experienced users do not need to rely on peripheral cues as novices ( Bhattacherjee &amp; Sanford, 2006 ). Novice users do not have enough expertise to assess the information in detail. They are forced to search for other indi-cators and rely on peripheral cues and heuristics ( Petty &amp; Cacioppo, 1986 ). Hence, we hypothesize:
H4. A high elaboration likelihood, formed by job relevance and user expertise, has a positive moderating effect on the relationship between argument quality and perceived usefulness.

H5a. A high elaboration likelihood has a negative moderating effect on the relationship between report subscriptions and perceived usefulness.

H5b. A high elaboration likelihood has a negative moderating effect on the relationship between hierarchical level of report author and perceived usefulness.
 Fig. 2 summarizes our research model.

Our model mostly resembles the model of Fadel et al. (2008) but the unexpected results reported in that study (negative effect of argument quality, no moderation effects) call for a reexamination. There is also an important difference in periph-eral cues. The validation cue used in Fadel et al. (2008) in addition to cues about the document author represents a formal approval by a small group. Therefore, there is no construct that relates to identification with the community. We measure this aspect by the construct report subscriptions which indicates the report usage by the user community. Finally, surveying undergraduate students is a good research start, but if findings should be transferable to companies, it is mandatory to con-duct research with working professionals as is done here. 4. Method 4.1. Experimental design
We asked selected participants to evaluate a screen shot of a BI portal mock-up containing a report supplemented by social cues. The cognitive task was to compare the sales performance of individual branches among each other and with the average sales of the company based on the presented report. It is a slightly changed example from a source on good prac-tices of information presentation in reports, widely respected in companies in German speaking countries ( Hichert, 2007 ). We chose it because it presents a good and a poor quality report based on the same data. Both reports are given in Appendix
B . The  X  X  X ood quality X  X  report explicitly compares each branch with the average performance and displays exact sales figures for each year. The  X  X  X oor quality X  X  report displays the exact sales figures only for the most current year while all others are only shown as bar charts. Average branch performance is not displayed but needs to be induced from eight individual bar charts. The displayed information has been reduced compared to the original source in order to ensure that it is visible on all devices; the displayed dates have been adapted so that the report appears to have been generated in 2013. The survey was conducted in German and has been translated for presentation in this article.

As indicated above, we want to create a situation where no personal relationship exists between informant and informa-tion recipient. This set-up is automatically given in our experiment since the report author is a fictive person. However, even if people do not know each other, a photo of the report author may induce feelings like sympathy, trust, or prejudices based on gender, age, race, or other aspects. Since we want to avoid such effects, we only show a silhouette of the report author instead of a picture. This is not unusual but a custom in social media when people do not want to show their photo to every-body. Since the name often indicates the gender and may indicate ethnicity or a certain national origin, we provide only the initial of the first name and a last name that is common in Germany.

The peripheral cue relating to internalization, hierarchical level, does not disclose any personal characteristics of the report author. These considerations also apply to the identification cue. B X hringer et al. (2009) , for example, provide in their prototype a photo and further information about report users. We do not incorporate this suggestion in our experiment since it creates similar issues like providing a photo of the report author. In addition, in Germany, for example, tracking of employ-ees in corporate social media or in other applications is usually not allowed if it is not demanded by the work context. Dis-playing names of users who ran a report would not be allowed without the (unlikely) consent of the working council. All social data we use are or can be easily utilized in real systems without conflict with even strong data privacy regulations.
Due to the limited number of participants, only a limited number of scenarios can be employed. Therefore, we set for each of the variables just two values, a high and a low value (see also Lim, 2013 ), so that the participants could easily interpret these cues, if they wanted to include them in their assessment. The cue  X  X  X umber of people who use this report X  X  was set to a low number (1) or a high number (30) and the hierarchical level was set to either a junior or a senior position. The two report versions and the two social cues are all displayed randomly with a high or a low value. This design results in a 2 2 2-matrix with 8 treatment combinations. Each respondent was randomly assigned to one configuration for assessment.
Fig. 3 shows one scenario. 4.2. Experiment participants and data collection procedure
The survey was conducted in Spring 2013. The target group consisted of knowledge workers from different functional areas (finance, marketing, logistics, IT) and different hierarchical levels (staff, line, and managers). They live in German-s-peaking areas, including the countries Germany, Austria, Switzerland, and Belgium. 1750 addressees were randomly chosen from a contact list of customers and prospects of a medium-size BI consultancy and contacted by e-mail. They were char-acterized in the data base as BI report designers (26%), BI users (66%), or having no BI experience (9%). The e-mail contained a link to the online-survey which also contained the mock-up. Participation was on a voluntary basis without any reward. All participants were advised that this was a scientific survey and promised a summary of the results. 334 persons opened the link and 178 completed the survey (53.3%). Some responses had to be eliminated because answers to measurement items were incomplete. Records in which the processing time was so short that it cannot be assumed that the survey was pro-cessed with care (less than 60 s) and records that were apparently filled out following a certain pattern (e.g. always the same value) were also eliminated. The only rational explanation for such behavior is that the participants were interested in the survey results.

The data cleansing process resulted in 141 useable records. When we compare the BI proficiency of these respondents with all addressed contacts, then there is no statistically significant difference between the distribution of the respondents and the above given distribution of contacts. Given the eight scenarios, there are about 17.6 observations on average for each 1997 ). The average age of respondents is slightly less than 40 years. The majority of participants held a senior position (as a manager or specialist) and was male. The job position and the age of the participants show a high variation indicating a suf-ficient amount of variance in elaboration likelihood based on these variables. An overview of the participants X  characteristics is shown in Table 2 . 4.3. Measurement
The research model contains three manipulated constructs which were captured by dichotomous variables. Argument quality, hierarchical level, and report subscriptions were coded 0 for the poor quality report/low hierarchical level/low num-ber of report subscriptions and 1 for good quality report/high hierarchical level/high number of report subscriptions. Dummy coding was applied since the number of participants for each scenario is unequal (cf. Streukens, Wetzels, Daryanto, &amp; de Ruyter, 2010 ).

We also measured perceived argument quality and used this variable in the calculations rather than the quality level set by ( Hichert, 2007 ) (as low or high) because the perceived quality of identical reports may be different by different people ( Lee, Strong, Kahn, &amp; Wang, 2002 ). Furthermore, perceived source credibility was measured to check the manipulation of both peripheral cues. Perceived usefulness and elaboration likelihood (expressed by expertise and job relevance) need to be operationalized with appropriate items and measurement scales. Reliable and established scales were taken from previ-ous studies while the item wording was tailored to the specific research context. The items of argument quality and per-ceived usefulness were adopted from Sussman and Siegal (2003) . Items measuring perceived source credibility were drawn from ( Kubiszewski, Noordewier, &amp; Costanza, 2011 ) and elaboration likelihood was captured using items from
Bhattacherjee and Sanford (2006) . All statements could be rated on seven-point agreement scales with the extremes being 1 (strongly disagree) and 7 (strongly agree). All constructs are measured with reflective items except for user expertise which is formed by the items knowledge of Business Intelligence and knowledge of collaborative systems . Thus, user expertise relates to methodological or tools knowledge rather than domain knowledge. The latter was not necessary for the chosen cognitive task as explained in Section 4.1 . With respondents from different functional areas and different industries, a task requiring extensive domain knowledge could not have been selected. Job relevance referred to BI in general since no fictive report could have been really relevant for the participants. However, the experimental setting was clear to the participants and there can be no doubt that they were able to relate the example to their own company setting. Supporting figures are pre-sented below.An overview of all items is included in Appendix A . 5. Results 5.1. Measurement model
Partial least squares (PLS) were used to test the presented research model because PLS has several advantages over (co)variance-based approaches to test factorial designs: PLS is less demanding regarding the sample size and distribution ( Chin, Marcolin, &amp; Newsted, 2003; Reinartz, Haenlein, &amp; Henseler, 2009 ), measurement models can be controlled for errors, and both reflective and formative indicators can be calculated simultaneously ( Streukens et al., 2010 ). Path coefficients, moderating effects, and quality criteria were computed with SmartPLS 3 ( Ringle, Wende, &amp; Becker, 2014 ). The quality of the reflective measurement models can be assessed by the criteria indicator reliability, composite reliability, convergent validity, and discriminant validity ( Henseler &amp; Fassott, 2010 ). The results are shown in Table 4 . The recommended minimum value of indicator reliability is 0.7. In our study, all indicators load well above this threshold (0.87 and higher) and can there-fore be described as highly significant. Composite reliability is calculated via the internal consistency reliability (ICR) and should surpass the value of 0.7 ( Nunnally &amp; Bernstein, 1994 ). All constructs fulfill this criterion and reach ICR scores of 0.93 and above. Convergent validity can be assessed using the average variance extracted (AVE) ( Fornell &amp; Larcker, 1981 ).
If 50% of the construct X  X  variance is explained AVE reaches 0.5 which is regarded as a sufficient level of convergent validity ( G X tz, Liehr-Gobbers, &amp; Krafft, 2010 ). All examined constructs show very good AVE scores above 0.81. Discriminant validity is determined by the Fornell X  X arcker criterion which measures if the AVE of a specific latent variable is higher than any model fulfill this criterion.

Elaboration likelihood was calculated using a second-order construct (cf. Jarvis, Mackenzie, Podsakoff, Mick, &amp; Bearden, 2003 ) formed by user expertise and job relevance. For this, latent variable scores were computed for each of the two com-ponents of elaboration likelihood in smartPLS. Afterward, the latent variable scores (EXL and JRL) were used as formative indicators for elaboration likelihood. This approach complies with the recommendations to compute hierarchical constructs given in Wetzels, Odekerken-Schr X der, and van Oppen (2009) .

The formative constructs user expertise and elaboration likelihood can be assessed by looking at multicollinearity and the significance of indicator weights ( Chin, 1998; Diamantopoulos &amp; Winklhofer, 2001; Tenenhaus, Vinzi, Chatelin, &amp; Lauro, 2005 ). The variance inflation factor (VIF) measures multicollinearity and should not surpass the value of 10 ( Reinartz et al., 2009 ). The values of 4.22 and 2.01 are well under the recommended upper limit and indicate low multicollinearity. One of two indicators for user expertise and elaboration likelihood is significant at a 95% and the other at 99% level. However,
EX2 and JRL show very small negative indicator weights and low significances. Cenfetelli and Bassellier (2009) argue that indicators with a small contribution can be interpreted as absolutely but not relatively important and should remain in the measurement model under the conditions that the bivariate correlation is high and that they cover distinct facets of a construct. Since both indicators fulfill these criteria, they were kept in the model.

Age and position are personal characteristics that could impact elaboration likelihood. However, they correlate each sig-nificantly with expertise and cannot be added but they could be used instead of expertise to reflect ability. This leads to less robust results and is, therefore, not reported here. Gender could be considered by conducting separate calculations for each gender in order to compare path coefficients. However, this leads to too few observations for most scenarios. We are also not aware of any literature suggesting gender effect in this context. 5.2. Preliminary analyses
The descriptive statistics of each scenario are given in Table 3 showing that the participants perceived a lower argument quality in all scenarios with the low quality report than in all scenarios with the high quality report. The computation of the bivariate correlation between the latent variable scores of perceived argument quality and the dichotomous manipulation variable argument quality confirms this observation. A value of 0.27 (significant at p &lt; 0.01) indicates that participants could well differentiate between the good and poor quality report.

The computation of the correlation between the latent variable scores of both peripheral cues (modeled as one formative manipulation of the peripheral cues was perceived by the participants in the manner intended.

There is a significant correlation ( p &lt; 0.05) between job relevance and questionnaire completion time. This supports ELM assumptions and the experimental design: participants with higher job relevance elaborated longer on the questions. This would not have happened if they did not see any relationship between the fictive report and their job demands. 5.3. Structural model
A two-step approach was applied to test the structural model. The research model was firstly evaluated without moder-ating effects and secondly moderating effects were included before calculating the results.

The path coefficients of influence of argument quality, report subscriptions, and hierarchical level of report author on per-ceived usefulness are positive and significant supporting the Hypotheses H1, H. and H3. These results persist when interac-tion terms are included. The explained variance of perceived usefulness by our research model is 0.37 with direct effects only. Including direct and indirect effects results in an increased R tial explained variance according to Chin and Newsted (1999) .

In Hypotheses H4, H5a, and H5b, we theorize that elaboration likelihood has a positive moderating effect on the path from argument quality to perceived usefulness and a negative effect on the relationships between the peripheral cues and perceived usefulness. The moderating effects of elaboration likelihood are confirmed empirically. All path coefficients of moderating effects are significant. However, the interaction effect between report subscriptions and elaboration likelihood is positive contrary to Hypothesis H5a. The detailed results for all hypotheses are summarized in Table 5 . 6. Discussion 6.1. Interpretation
The influence of argument quality as the central route and the influence of report subscriptions and hierarchical level of report author as peripheral cues conform to ELM. Argument quality has clearly the highest impact on perceived usefulness.
Identification and internalization processes, modeled by peripheral cues, also took place. This also answers our first research question about factors that influence perceived usefulness of a report.

These influences are moderated by BI systems expertise and job relevance of BI as job-related characteristics in the fol-lowing way (research question two): Both characteristics form elaboration likelihood which increases the impact of argu-ment quality on perceived usefulness. The theory-based expectations are also met for the moderation of the impact of the hierarchical level. However, the effects of elaboration likelihood on the influence of report subscriptions do not confirm our expectations based on ELM. Report subscriptions represent the community view of the report. Respondents with high elaboration likelihood pay attention to this view despite the low quality of the report. Our analysis of the respondents shows that those with high expertise, a formative part of elaboration likelihood, are often report designers. They seem to give credit to the low quality report based on report subscriptions, perhaps following the proverb  X  X  X ait the hook to suit the fish, not the fisherman. X  X 
While ELM assumes that decision makers exclusively take either one route or the other, Petty and Cacioppo (1986) admit the possibility that arguments and peripheral cues may also occur simultaneously. The suspicion of additive impacts of both routes is backed by research using the heuristic X  X ystematic model of information processing ( Maheswaran &amp; Chaiken, 1991 ).
Decision makers seek information to reduce their uncertainty ( Shannon &amp; Weaver, 1949 ) and may not want to ignore any piece of information. Therefore, we hypothesize that participants with high elaboration likelihood also observe peripheral cues when argument quality is low rather than immediately rejecting the message, here a BI report. Similar to job relevance, there is a small but significant correlation (at p &lt; 0.05) between expertise and questionnaire completion time. This means that participants with higher expertise used more time to answer the questions although they should be able to assess the usefulness of the report quicker than participants with lower expertise. This is a further indication that users with high expertise possibly use peripheral cues in addition to argument quality.

Our results confirm the above cited assumption of Petty and Cacioppo (1986) that the central route and peripheral route may co-occur. In the IS context, this means, that if the analysis of argument quality leads to doubts, information recipients may still resort to peripheral cues. In other words, while information recipients with low expertise can only take the periph-eral route, information recipients with high expertise may still take this route, possibly if they remained in doubt after taking the central route. For them, the central and the peripheral route are not alternatives but may complement each other. 6.2. Limitations and future research
The study has several limitations. To obtain acceptable results with a limited number of participants, the study was lim-ited to two social cues. Future research should investigate the impact of additional (or other) cues and a combination of task-related and task-unrelated cues. However, affective attitudes toward report authors (or other report users) that may be based on prejudices will be difficult to determine in an organizational setting since respondents will be more inclined to answer politically correct fearing that their identity could be easier disclosed than in a public survey.

Our result that the two routes of persuasion can complement each other for information recipients with high elaboration likelihood leads to the question about the sequence of information examination. It could be that they first examine the argu-ments because they have the ability and motivation to do so and then, if not persuaded yet, use peripheral cues. However, it could also be that they first notice peripheral cues that are usually easy to assess and then examine the arguments. The experiment would need to be designed in such a way that respondents also reveal the sequence of information use.
It is further of interest to discover what weights are assigned to the two routes if they complement each other. In a setting of two directly communicating parties and based on a moral hazard model that includes payoffs for the informant and infor-mation recipient, Dewatripont and Tirole (2005) show that extreme values of peripheral cues may  X  X  X rowd out X  X  the commu-nication of task-relevant information while intermediate values enhance it. In an experiment like ours, this would mean, for example, that the variable report subscriptions should be set to several values ranging from very low to intermediate to very high values. This would, however, lead to more scenarios and require more respondents.

The study also needs to be extended to other cultural environments since cultural aspects play a major role in the behav-ior of people ( Hofstede, Hofstede, &amp; Minkov, 1991 ). For example, people in Germany have a low power distance and may, therefore, be less influenced by the hierarchical level of an informant than people in countries with high power distance. 7. Managerial implications and conclusion
The results of our study have direct implications for the enhancement of BI systems by selected social media features. Our research has shown that disclosure of the hierarchical level of the author and the number of report subscriptions are periph-eral cues that receive attention from users. The community view could be strengthened by adding votes for reports. How-ever, the use of voting schemes should be carefully considered since it may create unintended effects (e.g., asking close colleagues to vote favorably for one X  X  report even if they do not use it) and adaptive behavior (e.g., not contributing reports because of fear of poor voting scores) that diminish the positive value of crowd wisdom expressed through voting. Other cues may also grab user attention but care should be taken not to overload users with peripheral cues and distract their attention from report content. Therefore, managers should consciously choose how many and which cues to offer. Report reuse reduces time and cost for report design but it is most important that users with low expertise use the right report. Therefore, great care has to be taken that the previous phase of report retrieval delivers a good fit between user needs and available reports. This can be supported, for example, through tagging, another feature made popular through social media. In this case, each report designer could be asked to generate a set of descriptors for the reports she develops.

Organizations should also consciously decide whether they want to add information that may invoke source likeability (or the opposite). While organizations usually prohibit anonymous contributions in internal applications, there are also con-texts where less disclosure about personal characteristics of contributors helps to concentrate on task-related issues.
In practice, supporting evaluation of report usefulness leads to another benefit. As shown in Hertzum and Pejtersen (2000) , employees search for documents not only because of their content but also to find document authors who may be experts on the subject of the document. Then, they interact with them directly. In our case, a user who discovers relevant reports may be assured of report authors X  expertise if he receives information on the number of subscriptions for the different reports as one cue.

We have developed a model to analyze influence processes in the context of BI. The model is based on ELM and it can explain a considerable percentage of variance in perceived usefulness. Users of BI reports are mostly influenced by argument quality but peripheral cues, here the number of report subscriptions and the hierarchical level of the report author, also influence their perception of report usefulness. We ensured with our research design that these results are not biased by source likeability.
 The number of report subscription reflects a community view that has been often researched in public social networks.
We have identified that it also receives considerable attention in an organizational setting, even when information recipients do not need to rely on it to evaluate the information.

We have empirically confirmed the suspicion that the central and peripheral routes can complement each other. More research is needed to clarify how they exactly complement each other, in terms of sequence of their evaluation and how their influences are aggregated to derive at an overall assessment of the information received.
 Appendix A. Construct definitions and respective items A.1. Perceived usefulness
This is the degree to which people believe that the use of a specific IS artifact would improve their working performance ( Davis et al., 1989 ). The items are adapted from Sussman and Siegal (2003) where they were taken from Bailey and Pearson (1983) and range from  X  X  X trongly disagree X  X  to  X  X  X trongly agree X  X  on a seven-point agreement scale.
 PU1 . The processing of data by this report is valuable to solve the task.
 PU2 . The processing of data by this report is informative to solve the task.
 PU3 . The processing of data by this report is helpful to solve the task.
 A.2. Perceived argument quality
This is the extent to which a user of an application perceives the provided information to be complete, unambiguous, meaningful, and correct ( Wand &amp; Wang, 1996 ). The items are adapted from Sussman and Siegal (2003) where they were tak-en from Bailey and Pearson (1983) and range from  X  X  X trongly disagree X  X  to  X  X  X trongly agree X  X  on a seven-point agreement scale. AQ1 . The processing of data by this report is complete.
 AQ2 . The processing of data by this report is consistent.
 AQ3 . The processing of data by this report is accurate.
 A.3. Perceived source credibility
This is the degree to which people believe that the information provider is a reliable information source. The items are adapted from Kubiszewski et al. (2011) and are measured on a seven-point agreement scale ranging from  X  X  X trongly disagree X  X  to  X  X  X trongly agree. X  X  SC1 . The report is trustworthy.
 SC2 . The report is believable.
 SC3 . The report is reliable.
 A.4. Elaboration likelihood
This relates to the motivation and the ability to evaluate given information and elaborate the main arguments ( Bhattacherjee &amp; Sanford, 2006 ). Ability is represented by expertise ( Bhattacherjee &amp; Sanford, 2006 ) which is measured on a seven-point agreement scale ranging from  X  X  X trongly disagree X  X  to  X  X  X trongly agree. X  X  EX1 . I am a knowledgeable user of Business Intelligence systems.
 EX2 . I am a knowledgeable user of collaborative software.

Motivation is represented by job relevance ( Bhattacherjee &amp; Sanford, 2006 ) which is measured on a seven-point agree-ment scale ranging from  X  X  X trongly disagree X  X  to  X  X  X trongly agree. X  X  JR1 . Using Business Intelligence systems is important for my job.

JR2 . Using Business Intelligence systems is appropriate for my job. Appendix B References
