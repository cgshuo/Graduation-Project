 1. Introduction
Broken links in the World Wide Web, i.e., links pointing to an unavailable page, are usually the result of pages that have disappeared or have been moved. Broken links affect PageRank ( Green, 2004 ) and discourage page visitors, who might regard number of free systems such as Xenu X  X  Link Sleuth 1 or W3C Link Checker,
Link Validator. 4 Once a page owner has detected a broken link  X  either manually or using one of these checking systems  X  it is his own responsibility to update or erase the link. However, it is a time-consuming task that has to be performed frequently.
Furthermore, when navigating through the web we often reach links that are apparently very interesting for us, but that do  X  not work any more. In those cases we can try a web search using information that the broken link and the page that contains it,
We can realize that in many cases the page linked by the broken link has not disappeared, but it has been moved inside the web site structure, or to another web site. We have also observed that even in the cases in which the page has in fact disappeared, it is possible to find highly related pages that can be used to repair the broken link.

In most cases the page containing the broken link provides a lot of information regarding the page it points to: the anchor text, the surrounding anchor text, the URL and the text in the page. But we can also use other resources from the Web Infra-structure, such as a cached page stored in a search engine or Internet archive ( Wayback Machine )( Mearian, 2009 ), available utilities from search engines and information provided by social tagging websites.

In this work we propose a system that recommends candidate pages to repair a broken link. Our system checks the links of the page given as input. For those which are broken, the system determines if we have available enough information to perform a reliable recommendation. If so, the system provides to the user with a set of candidate pages to replace the broken link. Our method applies information retrieval techniques to extract the most relevant data from several information sources. Some of them, such as the anchor text and the page text have been previously studied in our preliminary works ( Martinez-Romo &amp; Araujo, 2008, 2009, 2010 ). This work extends the previous findings by introducing an exhaustive analysis of different information retrieval techniques in order to use them in two very important phases of the system: extraction of relevant terminology and ranking of results. The candidate pages are obtained by submitting queries to a search engine com-posed of terms extracted from the different sources. In order to tune the results, the pages recovered in this way are filtered and ranked according to relevance measures obtained by applying information retrieval techniques. The resulting list of pages is presented to the user. Of course, we do not know the actual purpose of an editor when he includes a link in his web page, or the actual interest of a web searcher. Therefore, our system does not automatically associate a page with a bro-ken link, but recommends a ranked list of candidate pages that are highly related to the missing page.

The first step of our work has been the analysis of a large number of web pages and their links in order to determine which ones are the most useful sources of information and which of them are the most appropriate in each case. This study has allowed us to extract criteria to determine, for a particular broken link, whether it makes sense to look for candidate pages to recommend to the user, or whether the available information is not enough to attempt the recovering. Sometimes we can recover a broken link by entering the anchor text as a user query in a search engine. However, there are many cases in which the anchor text does not contain enough information to do that. In these cases, we can compose queries by adding terms extracted from other sources of information (the text of the web page that contains the link, a cached page stored in a search engine  X  if it exists  X  the URL, etc.) to the anchor text.

In order to evaluate the different approaches considered, we have developed a method which mainly relies on the random selection of pages and on the use of links that are not true broken, thus allowing us to check whether in the case they were true broken, our techniques would be able to recover the correct page. Later, we have performed both a manual and an auto-matic evaluation of the resulting system on two different collections of pages containing true broken links.
The remainder of the paper proceeds as follows: Section 2 provides an overview of related work; Section 3 presents a scheme of the proposed model; Section 4 describes the methodology we have followed to evaluate the suitability of the sources of information considered; Section 5 analyzes the utility of the anchor text of the links to recover the page; Section 6 studies the suitability of different sources of information to provide terms for the query expansion process; Section 7 pre-sents several resources from the Web infrastructure that can be used to obtain more information; Section 8 describes the process to rank the candidate documents; Section 9 analyzes some parameter settings of the system; Section 10 presents the scheme resulting from the previous analysis, as well as the results of applying it to two sets of truly broken web links;
Finally, Section 11 draws the main conclusions. 2. Related work
Despite the problem of broken links was considered the second most serious problem on the Web ( Markwell &amp; Brooks, 2002 ) several years ago, missing pages are still frequent when users surfing the Internet. Previous works quantified this problem: Kahle (1997) reported the expected life-time of a web page is 44 days. Koehler (2002) performed a longitudinal study of web page availability and found the random test collection of URLs eventually reached a  X  X  X teady state X  X  after approx-imately 67% of the URLs were lost over a 4-year period. Markwell and Brooks (2002) monitored the resources of three authentic courses during 14 months, and 16.5% of the links had disappeared or were non-viable.

Most previous attempts to recover broken links are based on information annotated in advance with the link. Davis (2000) studied the causes that provoke the existence of broken links and proposed solutions focused on collecting infor-mation on the links in its creation or modification. One of the first works which tried to solve the problem of broken links
Web resources, using for this task a novel object-oriented approach. The Webvise system ( Gr X nb X k, Sloth, &amp;  X rb X k, 1999 ), integrated with Microsoft software, stores annotations in hypermedia databases external to the web pages. This allows the system to provide a certain degree of capacity to recover integrated broken links. The information is stored when the links are created or modified. Shimada and Futakata (1998) designed the Self-Evolving Database ( SEDB ), which stores only links in a centralized way while documents are left in their native formats at their original locations. When a document is missing, the SEDB reorganizes all links formerly connected to the missing document in order to preserve the topology of links.

Thought with a purpose different to repair broken links, other works have investigated mechanisms to extract informa-tion from the links and the context they appear in. Some of these mechanisms have been tested in our system for recovering broken links. McBryan (1994) proposed to use the anchor text as a help to the search of web resources. This work describes the tool WWWW intended to locate resources on the Internet. The WWWW program surfs the Internet locating web re-sources and builds a database of these. Each HTML file found is indexed with the title string used in there. Each URL refer-enced in an HTML file is also indexed. The system allows searching on document titles, reference hypertext, or within the components of the URL name strings.

Dai and Davison (2009) , considered outdated links, alongside broken links, building a classification model based on a of vetting automatically many links of the web. For the evaluation, the authors used the ODP data set, which was based on the external pages cited by DMOZ Open Directory Project and corresponding to historical snapshots provided by the Wayback lected external pages which had complete historical snapshots since the year in which they were first observed in the ODP directory up to year 2007.

The INEX Link-the-Wiki task ( Huang, Geva, &amp; Trotman, 2009 ) aims at evaluating the state of the art in automated discov-ery of document hyperlinks using Wikipedia as reference collection. The objective of the task is to create a reusable resource for evaluating and comparing different state of the art systems and approaches to automated link discovery.
Many works also appeared using different techniques in the TREC-10 Web Track ( Craswell &amp; Hawking, 2001 ) that intro-duced the homepage finding task, where the queries were the name of an entity whose homepage was included in the col-lection. The challenge in this task was to return all the homepages at the top of the ranking. Among the most relevant works,
Westerveld, Kraaij, and Hiemstra (2001) used language models along with four sources of information: page content, num-ber of inlinks, URL depth and the anchor texts of outlinks. Xi, Fox, Tan, and Shu (2002) fulfilled a machine learning study divided into three stages: in a first stage they used the vector space model for getting the similarity between a query and several sources of information. Afterwards they filtered the collection pages by means of a decision tree. Finally, a logistic regression model ranked the remaining pages. Amati, Carpineto, and Romano (2001) used pseudo-relevance feedback and divergence from randomness to extract relevant terms and perform expanded queries.

Although all these works in the TREC environment have in common the page finding task, our work differs in the follow-ing respects: (i) Techniques used every year were specific for the intended track. (ii) In the homepage finding task the num-ber of potential candidates is much more reduced and most of papers used the URL depth as a main factor for the page selection. (iii) Mixed tasks used queries that differed from the anchor text and the applied techniques were specific to this scenario. (iv) The size of the collection is much smaller than the whole Internet, and the collections used on the mentioned tracks allow extracting information on the links structure, which is impossible in our study.

Closest to our research are other works (Morishima, Nakamizo, Iida, Sugimoto, &amp; Kitagawa, 2008, 2009a, 2009b; Nakam-izo, Iida, Morishima, Sugimoto, &amp; Kitagawa, 2005) which have developed a software tool that finds new URLs of web pages after pages are moved. The tool outputs a list of web pages sorted by their plausibility of being link authorities. This system attributes concerning the relations among links and directories.

As the authors state in the paper, the mechanism they propose to exploit locality in the problem is to provide some meth-ods complementary to the content-based approach. For the evaluation in Morishima, Nakamizo, Iida, Sugimoto, and Kitag-awa (2009a) , the authors collected links contained in Web sites of nine university domains and selected outgoing links from them. Their system found 858 broken links in the collection, 259 of which were identified as having been caused by page movement. These were the only ones used in the evaluation.

Popitsch and Haslhofer (2010) , consider a link as broken not only when the target page can not be accesses any more, but also when representations of the target resource were updated in such a way that they underwent a change in meaning that the link-creator had not in mind. The approach proposed in this work for fixing broken links is based on an indexing infra-structure. A monitor periodically accesses considered data sources, creates an item for each resource it encounters and ex-tracts a feature vector from these the representation of these items. The authors have created a dbpedia-eventset that was derived from the person datasets of the DBpedia snapshots 3.2 and 3.3, selecting some particular subsets, appropriate to evaluate their approach.

Harrison and Nelson (2006), Nelson, McCown, Smith, and Klein (2007), and Klein and Nelson (2008) built a framework for pages that they want to preserve, and these terms are used to query a search engine if some of those pages disappear in the the pages joined to the project.

Our work differs from previous proposals since it does not rely on any information about the links annotated in advance, and it can be applied to any web page. Furthermore, we do not try to recover just moved or outdated pages, but any kind of disappeared page. We use a great amount of Web resources to find a missing page by employing new technologies that have arisen recently. Furthermore, we have defined a methodology to evaluate the system without resorting to user judgments, thus increasing the objectivity of the results.
 3. Proposed model
Fig. 1 presents a scheme of the proposed model. The broken link and the web page that contains it, provide terms that may be related with the disappeared web page content. The most relevant of those terms are used to build queries that are submitted to a search engine. Our system performs a form of query expansion ( Efthimiadis, 1996 ), a well-known method to improve the performance of information retrieval systems. In this method, the expansion terms have to be very carefully selected to avoid worsening the query performance. Many approaches proposed for query expansion use external collections ( Voorhees, 2006, 2005, 2003 ), such as Web documents, to extract candidate terms for the expansion. There exist other meth-ods to extract the candidate terms from the same collection that the search is performed on. Some of these methods are based on global analysis where the list of candidate terms is generated from the whole collection, but they are computation-chor text, and the sources of expansion terms are the elements of the web page containing the broken link (text, URL, con-text, etc.), and also, if they exist, a cached page corresponding to the disappeared page that can be stored in the search engine, and other resources from social tagging websites and search engines. There exist many works which have analyzed the importance of the anchor text like a source of information. Eiron and McCurley (2003) carried out a study which com-pared the usefulness of the anchor text and the content page in a web search task. This study shows several aspects of the anchor text, including the frequency of queries and the most frequent terms in the anchor text, content and titles.The anchor text has also been used for the site finding task ( Craswell, Hawking, &amp; Robertson, 2001 ) in TREC, but this source of information has generally been used as a way to represent a page, while in this work, the anchor text is used as a source of information in order to find a missing web page. We have investigated the performance of different approaches to extract the expansion terms from the mentioned sources. Some of these approaches are based on term frequencies, while others are language modeling approaches based on the differences between the probability distribution of terms in a collection and in the considered source of information.

After the term extraction step, a query is submitted to the considered search engine for every one of the expansions. Then, top ranked documents are retrieved in each case and the whole set is ranked. We have also investigated different approaches for this process. One of them is the vector space model, according to which the candidate documents are ranked by similarity with elements from the parent or the cached page. Similarity is measured using different normalized measures ( Cosine , Dice and Tanimoto ) between the corresponding vectors. Also, language modeling is considered again for this task, which builds a probabilistic language model for each document, and ranks documents according to the probability of the model that gen-erates the query. 4. Evaluation methodology of information, if we try to recover broken links directly. Therefore, we have employed random web links which are not really broken, and we called pseudobroken links. Thus, we had available the page at which they point and we were able to evaluate the recommendation of our system. 4.1. Selection of links to recover
To carry out the analysis, we took links from pages randomly selected by means of successive requests to www.random-website.com , a site that provides random web pages. Certain requisites were imposed to our test pages. The language of web least 250 words were required for using its text to characterize it. Moreover, the text was required to contain at least ten terms that were not stop words, that is, words that are so common that they are ignored in information retrieval tasks (e.g., articles, pronouns, etc.). We also demanded that the page had at least five potentially analyzable links , which means: The system analyzes external links, therefore links that pointed to the same site were discarded.
 The anchor text had to neither be empty nor be a number or a URL.
 If the anchor text was only composed of one character and it was a punctuation mark, this link was discarded.
Some preliminary experiments indicated that it is frequent to find pages in which over 95% of the links are alive and oth-ers in which most of them are broken. In our data, pages have an average of 17 links, although only 9% of them have more than 100 links and 57% do not exceed 10. When these pages have many links (e.g., 1000 links), they bias the results in some way or another. Because of this, we decided to limit to ten the number of links taken per page. This subset of links was ran-domly chosen among the analyzable links in the page. Finally, we have a collection of 900 links to study and evaluate our system in the next sections. 4.2. Automatic evaluation from the page candidate to replace the link matches the analyzed link (remember that in this first analysis the link is not really broken). Nevertheless, we have found some cases in which the recovered page has the same content as the pseudobrok-en link, but different URL. Therefore, if the URLs do not match, we verify whether the web page content is the same. We have also found several cases in which the page content is not identical, but they were very similar: there are some small changes like advertisements, dates, etc. For this reason, if the contents are not exactly the same, we apply the vector space model ( Manning, Raghavan, &amp; Sch X tze, 2008 ), i.e we represent each page by a term vector and calculate the cosine distance be-tween them (similarity). Other methods could be used for this task such as shingling techniques ( Brin, Davis, &amp; Garc X  na, 1995 ), which take a set of contiguous terms or shingles of documents and compare the number of matching shingles, or vector space model since shingling techniques focus on copy detection and PageSim or SimRank comes at a higher computa-tional cost. We performed a study which can be observed in Table 1 . This table presents the number of cases in which the broken links have been recovered, querying the search engine with the terms extracted from each anchor text.
As we can see in Table 1 , if we use a similarity threshold higher than 0.9, 253 hits are recovered in the first position and
Besides, lowering this value the number of wrong results increases, sometimes recovering different pages. This means that using a lower threshold in the evaluation methodology, the system in some cases could incorrectly indicate that the link has been successfully recovered. For these reasons we have set the similarity threshold value to 0.9. 4.3. Manual evaluation
Each candidate web page for each broken link was judged by three different human judges (assessors). These judges were cached page of the missing page to evaluate if the candidate page was similar enough to replace the missing page.
Reviewers were provided a guidelines consisting of a list of similarity aspects, such as structure, content, title, and data, and a set of examples.

In our experiments, we restricted the broken links considered as  X  X  X ecovered X  X  to those for which all assessors agreed that the system had provided the correct page.

The results of the manual evaluation were statistically analyzed and we conclude that there was a strong agreement be-tween assessor judgments on the comparison of the first valid candidate page detected. 5. Information provided by the anchor text
In many cases the words which compose the anchor text of a hyperlink are the main source of information to identify the
This table presents the number of pseudobroken links that are recovered among the top ten hits returned by the search en-66% of the recovered links appears in the first position. These results prove that the anchor text is an important source of information to recover a broken link. Accordingly we use the terms extracted from the anchor text to compose a query that can be later extended with some additional terms from other sources. 5.1. Named entities in the anchor text Sometimes the anchor terms provide little or not descriptive value. Let us imagine a link whose anchor text is  X  X  X lick here X  X .
In this case, finding the broken link might be impossible. For this reason it is very important to analyze these terms so as to be able to decide which tasks should be performed depending on their quantity and quality.

In this work we have carried out a recognition of named entities (persons, organizations or places) on the anchor text in order to extract certain terms whose importance is higher than the remaining ones. There exist several software solutions for liminary experiments, none of these solutions applied to the anchors have provided precise results, perhaps because we are working in a wide domain. In addition, the size of the anchor texts is too small for the kind of analysis usually performed by these systems.

Accordingly, we have decided to use the opposite strategy. Instead of finding named entities, we have chosen to compile a set of dictionaries to discard the common words and numbers, assuming that the rest of words are pseudo named entities , which we will from now on call named entities. Although we have found some false negatives, as for example the company Apple , we have obtained better results using this technique.

Table 2 shows the number of pseudobroken links recovered depending on the presence of named entities in the anchors, and on the number of anchor terms. We can see that when the anchor does not contain any named entity, the number of links that are not recovered is much higher than the number of the recovered ones, whereas both quantities are similar when there exist named entities. This proves that the presence of any named entity in the anchor favors the recovery of the link.
The most prominent result is the very small number of cases in which the correct document is recovered when the anchor consists of just a term and it is not a named entity. 5 When the anchor contains named entities, even if there is only one, the number of retrieved cases is significant. Another fact that we can observe is that from two terms, the number of anchor terms does not represent a big change in the results.
 6. Additional terminology for the web search
We consider the anchor text as the main source of information to recover a broken link, but we can extract additional terminology in order to complete the information provided by the anchor text. There are several sources of information in the context of a link that can help to recover it. These sources of information have different characteristics such as the amount of text or the relation to the broken link. In some cases we may have sources of information with a large amount of text such as the whole page in which the broken link appears, or just a few words such as a URL. In both cases, our main goal is to synthesize this information and to extract the minimum number of terms that represent a link in the most compact way. Thus, we have applied classical information retrieval techniques to extract the most representative terms. After remov-ing the stop words, we generate a ranked term list. First terms of this list are used to expand the query formed by the anchor text, i.e., the query is expanded with each of those terms, and the top hits are retrieved in each case. 6.1. Different approaches for the extraction of terms
According to the source of information that we analyze, the quality of the extracted terms will be different depending on the used method. Thus, we have used two types of extraction methods: frequency-based and language-model-based approaches. 6.1.1. Frequency-based approaches to select terms
Frequency-based are the most simple approaches to select the most relevant expansion terms. We have considered two mation. There are some terms with very little or no discriminating power as descriptors of the source, despite they are fre-quent on it. The reason is that those terms are also frequent in many other documents of the collection considered or in any other web page in our case. To take into account these cases we have also applied the well-known Tf-Idf weighting scheme for a term, where Idf(t) is the inverse document frequency of that term: an English Wikipedia articles dump 6 as reference collection. 6.1.2. Terminology extraction using language modeling
One of the main approaches to query expansion is based on studying the difference between the term distribution in the whole collection and in the subsets of documents that can be relevant for a query. One would expect that terms with little informative content have a similar distribution in any document of the collection. On the contrary, representative terms of a page or document are expected to be more frequent in that page than in other subsets of the considered collection.
One of the most successful methods based on term distribution analysis uses the concept of Kullback X  X iebler Divergence (KLD) ( Cover &amp; Thomas, 1991 ) to compute the divergence between the probability distributions of terms in the whole col-lection and the specific considered documents. The most likely terms to expand the query are those with a high probability in the document, which is the source of terms, and low probability in the whole collection. For the term t this divergence is: where P P ( t ) is the probability of the term t in the considered page, and P collection.

Computing this measure requires a reference collection of documents. The relation between this reference collection and the analyzed document, is an important factor in the results obtained with this approach. Because of general web pages are we can not use the whole web as a corpus. To study the impact of this factor on the results we have used three different collections of web pages indexed with Lucene ( Gospodnetic &amp; Hatcher, 2004 ):
Enwiki . This collection contains articles, templates, image descriptions, and primary meta-pages from an English Wikipe-dia dump. The size of this collection is around 3.6 million of documents.
 Dmoz . This collection is the result of a crawling process on the set of URLs from the DMOZ Open Directory Project (ODP) .
The whole set is around 4.5 million of sites. We set the crawling depth to zero, so just a document has been retrieved from each site.

Dmoz URLs . In this work, we have used the terms that compose a URL to extract relevant information. As the previous collections are based in common texts, we have compiled a new collection composed only of URL terms. Thus, we have parsed each URL in the 4.5 million of sites from ODP and we have indexed these terms, taking every URL as a document. 6.2. Sources of information
Both in the context of a link and the Web, there exist sources of information which provide relevant terms to recover a link that no longer exists. These sources of information have different characteristics both in the vocabulary component and length. For this reason, we consider the best approach to extract the most relevant information in a minimal number of terms for each of these sources. 6.2.1. Information provided by the URL
Apart from the anchor text, the URL is the only information directly provided by a link. Terms from a URL are very often highly representative of the content of the pointed page. In fact, most search engines take the URL terms into account in deciding whether a page is relevant to a query.

In addition, URL terms can be a very useful source of information whether a page has been moved within the same site, or the page is in another site but it maintains the same user id, service, application name, resource, etc.

A URL is mainly composed of a protocol, a domain, a path and a file. These elements are composed of terms that can pro-vide rich information about the target page. Moreover in last years, because of the increasing use of search engines, there exist Search Engine Optimization (SEO) techniques that try to exploit the importance of URL terms in a request.
In order to select the most relevant terms from a URL, it is important to exclude terms that are very frequent in URLs, e.g., we have built a language model with terms from the DMOZ URLs collection. Afterwards, with help of this collection of URLs, we have used different term extraction methods in order to know the most relevant terms in a certain URL. 6.2.2. Information provided by the page that contains the link
The most frequent terms of a web page are a way to characterize the main topic of the cited page. This technique requires link to a personal page is frequently formed by the name of the person to whom the page corresponds. However, in many are very common. If we perform a query to a search engine with only the forename and the surname, the personal page of this person probably will not appear among the first retrieved pages. However, if we expand the query using some terms related to that person, which can be extracted from his web page, then his personal web page will go up to the top positions. 6.2.3. Information provided by the context of a link
Sometimes anchor texts have not enough terms or these terms are too generic. Let us imagine a link whose anchor text is  X  X  X lp group X  X . For this reason, text surrounding a link can provide contextual information about the pointed page and the re-quired information to complete a more specific search. Moreover, Bencz X r, B X  relation between a link context and the pointed Web page, getting a good performance when the anchor text was extended with neighboring words. In our experiments, we have used several words around the anchor text to extend it, though we took into account HTML block-level elements and punctuation marks as in Pant (2003) and Chauhan and Sharma (2007) .
Although the format of Web documents changes over time and there are more and more links contained in a sentence or a paragraph (Blogs, Wikipedia , etc.), the main problem of this source is the limited number of times that you can find a con-its usefulness is very high. 6.2.4. Information provided by a cached page
One of the most useful sources of information is a cached page stored in a search engine or in an Internet archive ( Way-back Machine ). These websites, with different purposes, store the copy of a large number of Web pages with most of their available resources. The system first tries to retrieve a search engine cached page because it corresponds to the most recent version that can be obtained. However, if this cached page can not be retrieved, the system tries to find the latest cached version stored in the Wayback Machine . However, this source of information has two drawbacks: Web is bigger and bigger and servers of these websites are not able to store all the Web pages, although in our experiments we achieved a cached version from 50% to 60% of the analyzed links. On the other hand, the date of the latest version of a stored page may be old, even a year in the case of Internet archives. Although in the latter case, the essence of a site does not usually change in spite of the content has changed slightly. There are exceptions such as sites like slashdot.com , which varies almost com-terms are used to expand the query formed by the anchor text. 6.3. Comparing different approaches for the extraction of terms
Fig. 2 shows the results obtained using Frequency , Tf-Idf and KLD for the extraction of the expansion terms from different sources of information. In case of URL as source of information, because of the number of terms that we can extract is very small, we have replaced the Frequency approach with a KLD approach based on a collection of URLs. According to the obtained results depicted in Fig. 2 i, the approach that has achieved the best performance is the KLD approach which has used the URL collection. It seems clear that a language model formed by the terms that usually appear in a URL is the most appro-priate in this case. Both the URL and the page that contains the broken link (parent page) are two sources of information that can always be found, but the parent page, in many cases, has a lot of information not related to the broken link. Thus, the efficiency of the terms extracted from this source is limited. Fig. 2 ii shows how the system recovers more links using the
Frequency approach. As the extracted terms are not precise for this task, a simple method based on Frequency , that gets more generic terms, works better than another that gets greater specificity in terms like KLD .

Both context and cached page are two very important sources of information, since context usually contains the required terms to complete the information present in the anchor text, and a cached page shows a snapshot of what we are really looking for. The main problem in these two cases is that they are not always available but in the case of context is even more significant. In both cases, presented in Fig. 2 iii and iv, the extraction method that achieves the best performance is KLD by using Wikipedia as reference collection, because the terms that can be extracted are highly accurate.

From these results we can conclude than the best term extraction approach for the recovery of broken links is KLD .We can observe that when we use KLD , the results obtained with the Wikipedia as reference collection are better (the total num-ber of correct recovered pages). The reason is probably that this collection provides a wider range of topics, although the possible existence of some spam page in DMOZ could also distort the language model, and hence the term extraction.
In spite that Frequency is the method that obtains the best results by extracting terms from the parent page, there exists a limited relationship between the terms and the required page in many cases. Thus, this task is a random approximation with the aim of finding the closest terms to the link context or most related to the required page.

In addition, parent page content is sometimes not closely related to the page to recover, and thus refining the methods to select the more representative terms of the parent page does not improve the results.

Accordingly, in the remaining experiments we have used KLD with the English Wikipedia , as reference collection, for extracting terms from the context and cached page; KLD with a URL terms collection for extracting terms from the URL; and the Frequency method for extracting terms from the parent page. 6.4. Effect of query expansion in the relationship between success@1 and success@10
In order to study the effect of query expansion methods in the relationship between Success at one (S@1) and Success at ten (S@10), a new comparison is presented. Success at one measures in how many cases the first recommended document is considered an useful replacement of the missing page. In the case of Success at ten, only the first ten recommended docu-ments are considered. In Table 3 we can observe that expansion considerably increases the number of links recovered in the sion approach is used. Accordingly, we think that the most suitable mechanism is to recover with and without a expansion approach, and later ranking the whole set of results to present the user the most important ones in top positions. 7. Using the web to track a page trail
We have established the anchor text and the terms extracted from several sources in the context of a link as the main sources of information to recover a broken link. But we can also use other resources from the Web infrastructure, such as available utilities of search engines and information provided by social tagging websites.

Web offers us new resources every day that can be used to obtain more information. Our purpose is to use the resources available in the Web for obtaining more information about those pages that no longer exist, but for which there still exists and useful, and they can help us to recover a missing page. Another growing phenomenon are the social tagging systems, where a huge community is willing to generate information in a collaborative way. In this case, the point of view of a group of people on a social website can be a very important source of information to discover something new about a web page. 7.1. Web search utilities of search engines
Search engines work every day for giving us access to information in a simple way. Furthermore, the fact that only a few companies have the ability to index the whole Web (at least the vast majority) means that some of the services they offer have a great usefulness because of their global scope.

Recently, the open search web services platform of Yahoo! (BOSS
Terms . The technology used in Key Terms is the same used for Search Assist, searchers to explore concepts related to the query. The Key Terms feature uses term frequency and positional and contextual heuristics to return ordered lists that describe a web page. Each result returned for a query includes associated meta-data of up to 20 terms that describe that result.

We query to Yahoo! with the URL of a broken link to get an ordered list with these terms, and we use the first N terms to expand the anchor text of the broken link. Table 4 shows an use case after querying to Yahoo! with www.sigir.org , the web-provides this location. 7.2. Social tagging systems
Nowadays, the phenomenon Social Tagging is revolutionizing the searches in Internet, since behind of websites such as del.icio.us or www.bibsonomy.org there exists a community of users that classify pages using labels and also that share their bookmarks with other users. These websites store a huge collection of tagged web pages and they become a very important improve web searches.

Bao et al. (2007b) explored the use of social annotations in del.icio.us to improve web search. They pointed out that anno-tations are usually good summaries of corresponding web pages and count of annotations indicates the popularity of web pages. Yanbe, Jatowt, Nakamura, and Tanaka (2007) proposed combining the widely used link-based ranking metric with the one derived using social bookmarking data. They also introduced the SBRank algorithm which captures the popularity of a page and implemented a web search application by using this social tagging. Noll and Meinel (2008) proposed a person-alization technique which separates data collection and user profiling from the information system whose contents and in-dexed documents are being searched and used social bookmarking and tagging to re-rank web search results.
Our system also considers this source of information and searches in social tagging sites such as del.icio.us or www.bibs-onomy.org to extract a sorted tag list for every missing page according to the number of users that have used this label to tag that page. Afterwards, first N tags are used to expand the anchor text of the broken link. 8. Ranking the recommended links
After retrieving a set of candidate pages querying to search engines, the system needs to present the results to the user in decreasing order of relevance. To calculate this relevance we have considered different sources of information related to the broken link and different sources of information from the candidate pages. In order to establish the best ranking function for the candidate pages, we performed an analysis to compare different similarity approaches and elements from parent, cache and candidate pages. 8.1. Vector space model and co-occurrence coefficients
Within the comparative study among different ranking approaches, we have used the vector space model Manning et al. (2008) to represent the documents and several co-occurrence Rijsbergen (1977) coefficients to rank them. Methods based on term co-occurrence have been used very frequently to identify semantic relationships among documents. In our experiments we have used the well-known Tanimoto , Dice and Cosine co-occurrence coefficients to measure the similarity between the vectors representing the reference document D 1 and the candidate document D 8.2. Language model approach
We have also considered another model to represent the documents and rank them. Thus, we have used a language mod-eling to represent the documents and a non-symmetric probabilistic measure to rank the set of candidate documents. In this case we look at the difference between two probability distributions, computing the Kullback X  X eibler Divergence (KLD) be-tween two documents: where P D 1  X  t  X  is the probability of the term t in the reference document, and P didate document. 8.3. Comparing different approaches for the ranking of candidate pages
We have applied the approaches described above to different elements from the parent or the cached pages and from the candidate page. Specifically, we have studied the similarity among the following pairs of elements: (i) parent anchor text and candidate title, (ii) parent anchor text and candidate content, (iii) parent content and candidate content, and (iv) cache con-tent and candidate content.

In addition to these comparisons, we also used the anchor text and the snippet of the candidate document, but the results did not improve those showed in Fig. 3 . We can observe that in Fig. 3 i and ii the results obtained with KLD are worse than those obtained with the co-occurrence measures, especially in Fig. 3 i. In these figures we are studying the similarity between content ( Fig. 3 ii). On the contrary, KLD performs better than the co-occurrence measures in Fig. 3 iii and Fig. 3 iv, where we are measuring similarity between the content of two pages, the parent or the cached page, and the candidate page. So we can conclude that KLD performs better that the co-occurrence methods only if it is applied to texts long enough, such as the page content. In Fig. 3 iv we can observe that, as expected, the obtained results ranked by similarity with the cached page are the best. However, in many cases this page is not available (near a 40 X 50%). Comparing the remaining cases we can observe that the best results are obtained applying the similarity between the anchor text of the broken link and the title of the candidate page, and using co-occurrence methods. According to these results, if we can retrieve a cached page, we will apply KLD to rank the candidates pages with respect to the cached page. Otherwise, we will use the similarity between the anchor text and the candidate page title measured with a co-occurrence method, such a Dice , which performs slightly better in some cases. 9. Usage statistics and sources of information effectiveness
Extraction and use of terminology, as well as the recovery of the results from the search engine and subsequent ranking means a significant computational cost for the system. For these reasons, we present a comparative study on different parameters which affect this cost. The system will use 900 links to measure the contribution of each source of information.
Furthermore, sources of information have different availability and effectiveness degrees. For this reason, we present a study of these characteristics from every source of information.

An important issue to investigate is the trade-off between the amount of collected information to recover the broken links, and the required time to do it. We can expect that, in general, the more information available, the better the recom-mendation. However it is important to know the cost incurred for each increment of the collected data. 9.1. Impact on the results according to the number of terms and hits
The amount of information collected mainly depends on two parameters: the number of terms (extracted from several sources of information) used to expand the query (anchor text), and the number of hits taken from the results of the search engine for each query. We performed a set of experiments to evaluate how these parameters affect to the results.
Fig. 4 shows the number of recovered links for different numbers of terms used in the expansion. In this figure are shown the results for the different sources of information employed by the system. In these experiments, ten hits are taken from the results of the search engine. Obviously, the approach without expansion ( X  X  X nchor Text X  X ) is not affected by the number of terms used in the expansion. But it is interesting to notice that an expansion with parent terms beats the  X  X  X nchor Text X  X  approach when expanding with six or more terms. The main reason is that using more terms increases the probability of expanding with the appropriate term. In addition, it is remarkable the ascending slope when parent terms are used, since these terms are very imprecise. The  X  X  X ll X  X  approach is a combination of all sources of information, but this approach is not the addition of the results from all the sources information since some of these sources recover the same links. Apart from this, we can observe that the number of recovered links increases with the number of expanding terms for all the sources of information. However, the improvement is quite small from 10 to 25, specifically in the  X  X  X ll X  X  approach. Thus, according to this experiment, we can conclude that the best number of terms used to expand the query is 10.
Fig. 5 shows the number of recovered links when different numbers of hits are taken from the results provided by the search engine. This figure presents the results for the different sources of information employed by the system. Ten terms are used to expand the query formed by the anchor text in these experiments. It is remarkable the sharp slope when seven
URL terms are used, and the low recovery when a single term is used to expand the query. We can observe that the number of recovered links increases with the number of taken hits, though the improvement is much smaller from 10 to 15 hits. In to 15, we can conclude that the best number of hits taken from the results of the search engine is 10. 9.2. Effectiveness of sources of information
There exists a great diversity in the availability of information sources previously discussed. Sources such as the anchor text, the URL of the missing page or the page containing the broken link are always available. However, a cached page of the missing page or the context of the link that we are trying to recover, are only available in some cases. In addition from the obtained resources from the Web, search engines do not have a list of key-terms for each page neither all pages on the Inter-net are labeled in a social tagging website.

On the other hand and independently of the availability of sources of information, its effectiveness varies greatly from one source to another. That is, there are sources of information that when available, retrieve the missing page over a 90%, while the recovery degree with other sources of information is below 60%.

Fig. 6 shows the availability, the number of recovered (1 X 10 positions) links and the recovered links in the first position of the returned hits by the search engine, according to the source of information employed.

Fig. 6 depicts the great effectiveness of Context , Cache and Tagging terms. Sometimes the anchor text is too generic and and Tagging terms. It is remarkable the high level of recovery obtained by Cache and Keyterm terms. These sources are able to expand the query with very precise terms. In addition, as we mentioned above, there are some sources of information that the precision of each source, in such a way that the more links recovered from the first hit extracted from the search engine, the more precise the source. In addition, the more precise the source of information, the faster the system. Thus, Context and
Tagging are presented in the Fig. 6 as the more precise sources. 9.3. Time impact according to the number of terms and hits
Fig. 7 shows the average execution time to recover all the links using the previously defined sources of information and different number of hits and terms. Results show the recovery according to the number of taken hits, and the number of term used for expansion. In the experiments with different set of hits, the number of terms was fixed to 10, and in the experiments with different sets of terms, the number of hits was fixed to 10. We can observe that, as expected, the time increases with both, the number of hits and the number of terms. Though we expect to reduce this execution time with different improve-ments in the current implementation, according to the results of this section, in the experiments showed in this work we have fixed to 10 the number of taken hits of each search, and the number of terms used for the expansion, as a trade-off between the improvement in the performance and the execution time.

This paper presents a system that can be applied in different use cases. The one closer to the experiments corresponds to an online web application where a user can obtain a real-time response to his query. However, there are other use cases where time is not as important, such as a system that works offline. This is the case of a website that runs the recovery sys-tem periodically, looking for new broken links and a possible replacement for every one of them, sending that information to the user by email later. Thus, an average time of about ten seconds for every broken link would not be a problem for such a task.

Moreover, returning to the use case of the online web application, we have investigated the highest degree of quality of the results that we can obtain for the problem. However, not all the sources of information are equally useful for the recovery. Thus, the time could be reduced significantly by excluding some of the sources of information the system uses. For instance, context , tags and keyterms could be excluded without significantly affecting the results.

In general, the system is designed using a parametric approach which allows either improving the response time by reducing the number of sources of information, the number of expansion terms and the number of hits retrieved, or optimiz-ing the precision of the results using a larger number of resources. 9.4. Search engines
In our system, a search engine is used to be queried according to the method described above. Because our system de-pends on a proprietary technology, we conducted a comparative performance analysis among three of the most used search engines. We have used a collection of 750 broken links to perform these experiments.

Table 5 shows the number of recovered links using each search engine. In addition, it can be seen the links recovered by using the method without expansion (NE), but which are not recovered when expanding the query. It also shows the links recovered exclusively through the use of query expansion (E). For this table, the experiments were performed with a reduced query expansion approach, using only terms from the page where the broken link was found.

According to the data shown in Table 5 , Yahoo! gets the best results. Despite this search engine shows a better perfor-mance in the recovery of links, the main reasons for using it in our system have been the limitations that the APIs of other search engines present and the possibility of obtaining additional information, such as the  X  X  X ey Terms X  X  data from Yahoo! , with the same query. 10. Algorithm for automatic recovery of links
Results from the analysis described in previous sections suggest several criteria to decide in which cases there is enough information to try the retrieval of the link and which sources of information to use. According to them, we propose the recov-ery process which appears in Fig. 8 . First of all, it is checked whether the anchor number of terms is just one ( chor) = 1 ) and whether it does not contain named entities ( attempted if the missing page has an available cached version ( to verify that the proposal presented to the user can be useful. Otherwise, the user is informed that the recommendation is not possible ( No _ recovered ). If the page has an available cached version, then the recovery is performed, expanding a query formed by the anchor terms with terms extracted from the cache, context and URL using KLD , and from the Tags and Key-terms otherwise. Then the results are ranked (by similarity between the candidate page and the cached page computed with
KLD ) and only if any of them is sufficiently similar to the cache content ( is recommended this list of candidate documents. In the remaining cases, that is, when the anchor has more than one term or when it contains some named entity, the recovery is performed expanding the query with the terms from the context and
URL (applying KLD ), and from the Tags and Keyterms otherwise. If the page has an available cached version, the query is ex-panded with terms from the cached page using KLD or with terms from the page that contains the link (using Frequency ) otherwise. After that, all documents are grouped and ranked according to the cached page ( ing the Dice co-occurrence coefficient ( rank(docs, anchor 10.1. Results applying the system to broken links
We have applied this algorithm to links that are really broken. We have only used those that had an available stored ver-sion of the missing page. The reason is that only in this case, we can evaluate the results in an objective way. In some cases, the system is able to gather at most 700 candidate pages (7 sources of information 10 terms 10 hits) for every broken link. Then, according to the algorithm defined above, the system ranks these candidate pages and shows to the user the best 100 candidate pages as a ranked list. In order to evaluate the system, we performed both a manual evaluation by three hu-man judges and an automatic evaluation according to the methodology previously proposed in this work.

To evaluate the recovery system of broken links deeply, we have used two collections with different characteristics. On the one hand, we selected at random a set of active pages to simulate the behavior of the system in a real environment, given by a heterogeneous set of pages that are available online. On the other hand, we have tried to test the performance of the system on a set of very old broken links. In this way, we have conducted a set of experiments that reflect in a realistic way the typical conditions to which the system could face. 10.2. Random website collection
Following the methodology presented in Section 4.1 , we took every broken link from pages randomly selected by means of successive requests to www.randomwebsite.com , a site that provides random web pages. To evaluate the results in an objective way, we have only used pages that had an available cached version. The cached version is compared to every can-didate page to evaluate the quality of the results. However, we do not use this cached page as a source of information.
Results are shown in Table 6 where the measure used was the fraction of recovered links with the best candidate to re-place a broken link somewhere in the top 10 candidate pages proposed by the system ( X  X  X uccess at 10 X  X  or S@10). In addition, we have considered that a broken link is recovered when a valid candidate page is present among the 100 first documents proposed by the system. Valid pages are those similar enough to the cached one according to the judge opinion or the sim-ilarity measure obtained.

According to the manual evaluation, the system recovered 1619 from 1998 links (81% of the total links). On the other hand, the system recovered 78% of the total links using the automatic evaluation. Table 6 shows the ranking obtained for these recovered links. We have verified that in some cases the original page was found (it had been moved to other URL).
In some other cases, the system retrieved pages with very similar content. Besides, the system is able to provide useful replacements for web pages among the first 10 positions in 47% of the recovered links, and among the 20 first ones in 71% of the cases. 10.3. UK-domain collection
With the aim of completing a set of experiments that shows the performance of the system, we have also tested the sys-the missing page is available in the collection, although our system has not used this source of information for the recovery of the missing page. The main drawback is that the available version of the missing page is more than three years old. This obsolescence may cause some failures in the recovery because the pages that could replace the broken links have been dis-appearing gradually.

This new corpus is a publicly available Web Spam collection ( Castillo et al., 2006 ) based on crawls of the .uk Web domain done in May 2007. This reference collection was tagged by a group of volunteers labeling domains as  X  X  X on-spam X  X ,  X  X  X pam X  X  or  X  X  X orderline X  X . In our experiments, we restricted the dataset using only domains labeled as  X  X  X on-spam X  X . We imposed this restriction because spammers use techniques to artificially modify the content of web pages and this could introduce a bias in the results.

From the results shown in Table 7 , and according to the manual evaluation, we can see that the system recovered 449 from 615 links (73% of the total links). On the other hand, the system recovered 62% of the total links using the automatic evaluation. Table 7 shows the ranking of these recovered links. Furthermore, the system is able to provide useful replace-ments for web pages among the first 10 positions in 49% of the recovered links, and among the 20 first ones in 76% of the cases.

We have verified that in most cases the system recommended pages with very similar content. In the remaining cases, the system is able to propose pages whose content has changed over time, but have a similar structure and some signs of unique identity, generally Key Terms or sentences.

Comparing these results with those obtained with the previous collection, it can be seen from Table 7 that a lower num-ber of recovered links is achieved. However, this fact can be explained by considering that the pages used in the evaluation methodology are more than three years older than the current candidate pages. Moreover, if we consider the difference be-tween manual and automatic evaluation, there is a greater difference from the previous collection. This fact can be explained taking into account that a judge does not check literally the content but the structure, the most relevant information and some signs of identity. It is very important to note that although the system has only managed to retrieve a cached version in 4% of cases, it has been able to obtain a version stored on the Web Archive in 58% cases.

Furthermore, we have used the UK-Spam collection to identify broken links in the ClueWeb09 goal of doing an evaluation of the algorithm that is entirely reproducible using only algorithms known to the scientific commu-nity. We have used the default options of the Indri search engine
ClueWeb09 dataset which consists of roughly the first 500 million English web pages. The system has recovered 54% of the total number of links (332 out of 615 links) with manual evaluation and 51% with automatic evaluation. 10.4. Comparison with other approaches
Although many studies have addressed the issue of broken links, only the work of Morishima et al. (2009a) has shown performance results. The problem that the authors try to solve is different from ours, because they focus on pages that have been moved to other location, while we try to recover all types of links, included disappeared pages. Furthermore, results from both systems have been obtained for different collections. For these reasons, the comparison between both works should be taken with a grain of salt. They represent complementary methods that address problems of different nature. Mor-ishima et al. collected 127,109 links contained in Web sites of nine university domains, and selected outgoing links from them. Their system found 858 broken links in the collection and 259 of them were identify as having been caused by page movement. The data shown in Table 8 corresponds only to the recovery of the latter links.

According to the results shown in Table 8 , and considering the manual evaluation, it can be seen how PageChaser obtained a 74.9%, while our system recovered a 81% on the random web links collection and 73% on the UK-domain collection. These results indicate that a combination of both methods would give rise to an orthogonal system that could improve the results from both systems separately.
 10.5. Influence of sources of information on the recovery of broken links
Section 9.2 presented a comparative study which showed both availability and amount of links that each source of infor-10.2 . It is important to analyze each source of information separately.

Looking first at the Cache source, it can be observed note that its effectiveness (available links/links recovered) is lower though it preserves the precision (recovered links/links recovered in first position).

It is also noticeable that the source Parent has become the second best source of information after Cache source. One of the main reasons is that the effectiveness of the terms of the URL has been reduced very much, since now the links are really broken. This is the case when you can really assess the effectiveness of the terms of the URL. As for Keyterms , it can be seen that their effectiveness has also been increased and they become a really relevant source of information. On the other hand, it is also observed an increase in the effectiveness of the anchor text terms which could be somewhat hidden in the experiments with links were not really broken. Finally, the terms extracted from the context and tags maintain their effectiveness. 11. Conclusions
In this work we have analyzed different sources of information that we can use to carry out an automatic recovery of web links that are not valid anymore. Results indicate that the anchor terms can be very useful, especially if there are more than one and if they contain some named entity. We have studied the effect of using different terminology extraction approaches on sources of information, such as the page that contains the link, the surrounding anchor text, the URL anchor terms, and a cached page stored in some digital library (search engine or web archive). We have also employed resources from the Web infrastructure for obtaining more information about those pages that no longer exist, but for which there still exists infor-mation in Internet. Specifically, the system has extracted terms from recently Web technologies such as social tagging sys-tems and available tools from search engines.

This study has shown that the results are better when the query is expanded. Thus, the query expansion reduces the ambiguity that would entail the limited quantity of anchor terms. We have compared different methods for the extraction of terms to expand the anchor text. Experiments have shown that the best results are obtained using a language modeling from sources of information such as the cached page, the context and the URL of a link. On the other hand, a frequency ap-proach should be used to extract terms from the page that contains the link.

We have decided to combine all terminology extraction methods and use a novel ranking approach, in order to present to the user the candidate pages as a ranked list. We have also carried out a comparative study among different ranking ap-proaches by using several sources of information from the source and target pages; We have used several co-occurrence coefficients and a divergence approach based on language models. Thus, the best ranking is obtained applying KLD between language models from the cached page and the candidate page, if a cached page is available, and applying a co-occurrence method as Dice between the anchor text of the broken link and the title of the candidate page, otherwise.
To evaluate the recommender system, we have developed a novel methodology without resorting to user judgments, thus increasing the objectivity of the results, including two web page collections with true broken links to test the full system.
Through our proposed evaluation methodology, we have been able to determine the optimal amount of both terms used for expanding a query and retrieved hits from the search engine. In addition, this methodology has allowed us to perform an empirical evaluation of the availability and effectiveness of sources of information. We also performed a manual evalu-ation by three human judges to complete the evaluation methodology and to receive feedback about the system perfor-mance by assessors.

The result of this analysis has allowed us to design a strategy that has been able to recover a page that could replace the missing one in 81% of the broken links (1618 of 1998) in the case of a random web page collection and in 73% of the broken links (449 of 615) in the case of a UK-domain collection. Moreover, the system is able to provide 47% and 49% from these recovered links in the top ten of the results, and among the top 20 in 71% and 76% using a random web page and a UK-do-main collection, respectively.
 Acknowledgments
This work has been partially supported by the Spanish Ministry of Science and Innovation within the project Holopedia (TIN2010-21128-C02-01) and the Regional Government of Madrid under the Research Network MA2VICMR (S2009/TIC-1542).
 References
