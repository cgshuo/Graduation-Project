 Keywords: Chinese segmentation, bilingual information, statistical machine transla-tion Different from most of the western languages, Chinese sentences are written without any spaces between the words. Word segmentation is therefore one of the most im-portant steps of Chinese natural language processing tasks, such as statistical machine translation (SMT). than those treating each character as one  X  word  X  . While it is difficult to define what is a  X  correct  X  Chinese word segmentation (CWS), a generally accepted point is that the definition of  X  correct  X  segmentation should vary with different tasks. For exam-ple, Chinese information retrieval systems call for a segmentation that generates shorter words, while automatic speech recognition benefits from having longer words. Howev-SMT systems. [2] and [3] showed that the F-score, which is used generally to measure the performance of a segmentation on monolingual corpus, had nothing to do with the effect of the segmentation on SMT systems as a very high F-score may produce rather poor quality translations.
 the-shelf monolingual CWS method. For instance, [4] proposed the N-gram generative language modeling based approach. [5] used the hierarchical hidden Markov Model (HHMM) based method. [6] applied a sliding-window maximum entropy classifier to take CWS as a task of character tagging. Then [7] used Linear-chain conditional random fields (CRFs) [8] instead to take on the role of classifier and got a better result. model training process even if they are sub-optimal and raise a series of problems as follows:  X  Firstly, the specifications of monolingual CWS systems are not suitable for SMT.  X  Secondly, monolingual CWS methods often make a large number of mistakes on  X  Thirdly, monolingual CWS methods are not good at dealing with the ambigui-studied how to optimize the CWS in the machine translation system. [2] proposed two approaches to combine multiple word segmentations. [3] showed that neither character-level segmentation granularity nor Chinese-Treebank-style segmentation granularity was suitable for SMT systems and it introduced a new feature to shorten the aver-age word length produced by its CRF segmenter. However, the optimization in these papers is based on monolingual information and still keeps the problems above. [9] described a generative model which consisted of a unigram language model and an alignment model of both directions. Then it treated the word segmentation as a Hidden Markov Modeling problem of inserting and deleting spaces with the initial segmenta-tions. But the approach suffers from the problems of local optimum because of the lack of linguistic specifications which introduces some mistaken alternatives. Furthermore, it couldn  X  t address the issues of monolingual CWS systems only by the information of word alignment and called for multi-source information to be integrated. joint translation model to integrate multi-source bilingual information into monolingual CWS methods to address the issues above. Firstly we apply a word-based translation model to rescore the alternative segmentations. We get the alternative set by the combi-nation of CRF-based CWS system and N-gram language model based CWS system and rescore them by the way of cross-validation. Secondly, we take use of a phrase-based named entity (NE) transliteration model to integrate the information of bilingual NE into the model. Thirdly, we employ an English-Chinese dictionary and a Chinese syn-onym dictionary to make the model more accurate and effective. Finally, we propose an algorithm to improve the segmentation iteratively.
 rect segmentation for SMT systems and is very effective in improving the performance of machine translations. 2.1 Previous Work on Monolingual CWS CRF-based Model for CWS CRF is an undirected graphical model trained to maxi-mize a conditional probability [8] and is first used for CWS task by [10], which treats CWS task as a sequence tagging question. For instance, Chinese characters that begin a new word are given the START tag and Characters in the end of the words are given END tag.CRF-based model overcomes the problem of marking bias in generative mod-els but has a shortage of prone to generate much longer word than other methods, which is harmful to SMT because it causes data sparseness.
 N-gram Language Model for CWS N-gram language model based method [4] treats CWS task as a hidden Markov modeling problem of inserting spaces into text. It defines two states between every pair of the characters of Chinese text: have a space or don  X  t have a space between the pair of characters. N-gram language model has much weaker ability of recognizing OOV word than CRF-based model but it generates significant shorter words than CRF-based model, which meets our demand greatly. 2.2 Combination of CWS Systems In order to produce the set of alternative effectively and accurately, we propose an approach to combine the two models above. First, we are given a Chinese sentence c two segmentations by CWS models above: f J 1 CRF = ( f 1 , f 2 , ,, f J ) produced by CRF-based model and f J: 1 N gram = ( f 1 , f 2 , ,, f J: ) produced by N-gram language model. In the sentence, we call a character as a word boundary when it is the ending (not the beginning) of a word in one of the segmentations. According to the description, we define four states of a character as follows: boundary in f J 1 CRF .
 as follows: keep the original state in the sentence (see Figure 2(a)). keep the original state in the sentence (see Figure 2(b)).
 by combining each character  X  s possible states and the set of alternatives in Figure 2(a) can be described as a graph (see Figure 3).
 segmentation and each alternative segmentation will be given a fixed value as their monolingual segmentation probability for the next process. 3.1 Word-based Translation Model SMT systems, we use monolingual CWS methods to select a  X  best  X  segmentation by assuming that the probability of the segmentation is conditional independent with the English text as follows: however, it is proved by [9] that the assumption is harmful to the translation perfor-mance. Ignoring the assumption, we can select the  X  best  X  segmentation by the bilin-gual CWS probability as follows: a fixed value as mentioned above, it can be ignored and the bilingual CWS probability thus is with our word-based translation model. Considering the computing complexity, we take use of IBM model-1 in the process. As we can  X  t obtain the fixed alignment of each alternative, we take every possible alignment into account. Then the translation proba-bility is derived by where  X  is the normalization factor to make all alternative segmentations  X  probability sum to one and  X  X  X  indicates the number of the words of English sentence e I 1 while  X  X  X  translation probability from Chinese word f j to English word e i which is given by our word-based translation model.
 in the process of computing translation probability. That is, we compute the translation sidering efficiency, we divide the corpus into two subsets and compute the probability of one using the translation model trained on the other subset. 3.2 English-Chinese Phrase-based Named Entity Transliteration Model As we mentioned in Section 1, it is really difficult for monolingual CWS methods to segment the proper names or technical terms which are defined as named entity (NE) correctly and suitably. As many different words can be the transliteration of the same English named entity since they pronounce in the same way, it causes a big problem of data sparseness, which can  X  t be solved by the translation model in Section 3.1. the gap.
 letters and f j by characters and train a standard English-Chinese phrase-based translit-eration model using the open source translation system moses.
 English  X  sentence  X  l Y 1 and derive the best transliteration of it as: As an English named entity is generally transliterated from left to right, we don  X  t need to reorder the translation and the value of reordering feature d ( start x end x 1 1) is fixed to d (0) . What  X  s more, as we mentioned above, many different words pro-nounce in the same way. It doesn  X  t matter which character is chosen and each will be a  X  correct  X  transliteration of the English word ei. So the value of language model is derive as follows: segmentations is given by: where  X  1 and  X  2 indicate the weights of word translation feature and named entity is given by: and the c X 1 best is given above. 3.3 Integrating the Information of Dictionary In order to promote the joint model to be more accurate, we put forward a dictionary-based model in this Section.
 indicates the number of items in the dictionary. Each item consists of an English word e and a set of the word  X  s translations T i .
 e in the set T i . What  X  s more, it  X  s common that replace the translation of English word with a synonym which may have a little difference in meaning with the English word.
 the similarity of two Chinese words. The dictionary has five category  X  s levels and every word is given one or more codes to indicate the categories of the word. The words given the same code have the almost same meaning. Based on the tree, we define the semantic distance of two codes SemDist ( S 1 , S 2 ) as the shortest distance from the point S 1 to point S 2 in the tree. For example, SemDist ( Ah 08 B 01 , Ah 08 B 02) = 2 , SemDist ( Ah 08 B 01 , Ah 08 A 01) = 4 . Then we define the similarity of two codes SemSim ( S 1 , S 2 ) as follows: defined by where the function categoryOf ( W 1 ) return the set of codes of word W 1 . Finally, we extend the translation model described in Section 4.2 to p ( e I 1 j f J 1 ) = where the  X  1 ,  X  2 and  X  3 indicate the weights of word-based translation model, named entity transliteration model and dictionary-based model. In this Section, as the algorithm showed in Algorithm 1, we present an iterative process to optimize the joint model and our segmentation in an unsupervised way. the word-based translation model M trans (1) , M trans (2) and NE transliteration model M NE by the optimized segmentations and the new NE dictionary D NE .
 tion and current iteration is lower than a threshold h. 5.1 Data Set and Evaluation We take the IWSLT machine translation task [11] for our experiment and our mod-el is evaluated on the data track from two aspects: the segmentation performance on the training data set and the final translation performance on evaluation data set. The bilingual training corpus is a superset of corpora in the multi-domain collected from different sources including the training data of IWSLT task. 5.2 Baseline System and Translation System We take CRF-based CWS method [7] as a baseline CWS method.
 (2010-8-13 version) framework using GIZA++ [12] and minimum error rate training [13] to train and tune the feature weights of SMT systems. GIZA++ is used to get alignments from the bilingual training corpus with grow-diag-final-and option. The 4-gram LM is estimated by the SRILM toolkit [14] with interpolated modified Kneser-Ney discounting. We use the Moses decoder to produce all the system outputs, and score them with the BLEU-4 [15] score. Algorithm 1 Iterative joint model training 6.1 Segmentation Performance on Training Data Set Firstly, we compare our model on the segmentation performance with currently widely-used monolingual CWS methods.
 SMT systems and the CWS are related to SMT by a series of factors such as the spec-ifications, OOVs, lexicons. None of these factors can be directly related to the SMT. Therefore, we compare our method with others in multiple factors on the training data as shown in Table 1.Considering computational complexity, we perform our method using only one iteration.
 to the others. However, it produces a much smaller vocabulary than CRF and ICT [5] methods while keeps a high rate of unique words, which means that our method not only avoid data sparseness by shortening the common words, but also recognize the OOVs more accurate as the example shown in Figure 4. 6.2 Translation Performance on Task IWSLT Then, we evaluate our method for word segmentation on the IWSLT machine translation task. The bilingual training corpus includes the training data of task and other corpus in the multi-domain collected from different sources. We take the open-source translation system moses in the evaluation and use the evaluation corpus of (IWSLT 2005) [16] to optimize the model weights of moses. Finally, we take the evaluation corpus of (IWSLT 2007) [11] to evaluate the translation performance.
 [5], CRF-based method [7], N-gram language model based method [4], GS [9] and our method as shown in Table 2.
 and N-gram language model based method, with another two monolingual methods, and then integrate parts of our joint model or our full model into them to evaluate the translation performance using the same evaluation corpus as above. The results are shown in Table 3.
 mance effectively. It also can be found that even if ICT system has a better translation performance than N-gram, it obvious that N-gram method are more adaptive to com-bine with CRF method using the joint model because N-gram is prone to segment the OOVs into characters and thus is fit for our method.
 and CRF methods) over the training corpus and evaluate the translation performance for each iteration as shown in Figure 5. We get the final BLEU at iteration 6 in Figure 5 is 41.58.
 list two examples in Table 4.
 In this paper, we showed that it is effective to improve the performance of SMT sys-tem by introducing multi-source bilingual information to CWS system.We proposed a joint model and an iterative algorithm and our experiments showed that our method outperformed the other CWS approaches in terms of not only the word segmentation performance but also the translation quality. It is also proved that each sub-model of our joint model is effective and the iterative algorithm works well. In future work, we plan to make our joint model more accurate to select the segmentation for SMT system better.

