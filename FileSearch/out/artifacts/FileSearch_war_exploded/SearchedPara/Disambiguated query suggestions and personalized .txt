 1. Introduction
This paper focuses on a well known problem of Web searches, known as the ranked list problem , derived from both the fact that search engines present the results by means of ranked lists, and the semantic ambiguity of users X  queries submitted to search engines. Most queries are very short, consisting of just one or two terms: as a consequence of this rough and ambig-distinct semantics of query terms. Furthermore, since most users analyze only the first one or two pages of ranked results, they potentially can miss many relevant documents retrieved in subsequent and unread pages.

To overcome these drawbacks, users search on the Web by performing an iterative process based on trial and error cycles in the attempt to capture new and more relevant documents in the top k ranked positions. However, this behavior leads to  X  uments in the top k positions of the ranked results, such that they cover the topics of interest.

Although the issues described above jointly cause the ranked list problem , in the literature they have been investigated separately. Several techniques for Web search result disambiguation were proposed: many techniques are based on results X 
Etzioni, 1999); other techniques personalize the search results by exploiting external knowledge, such as user context and been mainly conceived as a measure of diversification of the search results, and is used to compute the ranking of documents
Ieong, 2009; Carbonell &amp; Goldstein, 1998). Novelty has also been regarded in relation with document summarization ( Swee-well as for relevance feedback exploitation ( Lad et al., 2007; Xu &amp; Chen, 2006 ).

The overall objective of our proposal is to support the user in optimizing Web searches, by reducing the need for long of disambiguated queries, that potentially may retrieve both new and relevant documents. The retrieved documents are organized into clusters which reflect distinct meanings of the original query terms, meanings that emerge from within retrieved documents.
 Our mechanism follows three main phases.

Phase 1. First of all, the retrieved documents are clustered, on the basis of words extracted from their titles and snippets ( Osinski &amp; Weiss, 2005 ).

Phase 2. For each cluster, a personalized rank is computed, based on the aggregation of two criteria: the novelty of con-query. Notice that the notion of novelty on which our proposal is based, is not used for content diversification of single documents (like in Carbonell &amp; Goldstein (1998) ), but is a measure of the amount of new documents in the cluster, i.e., documents never retrieved before. A preference on novelty with respect to content-relevance can be specified by the user to personalize the ranking on the basis of what she/he has already retrieved and analyzed in past iterations of the query. This way, the top ranked clusters both are relevant to the query and contain new documents.

Phase 3. From each cluster X  X  representation, a disambiguated query is generated and suggested to the user to deepen the search. The disambiguated queries are computed based on terms in titles and snippets of documents in the clusters: they should be able to highlight the main contents of the cluster and potentially may retrieve further relevant documents. In particular, depending on the content of the cluster, the suggested query might be either more focused or more general (when documents in the cluster are too heterogeneous or too focused, respectively).

The originality of the proposal is the joint exploitation of two features: suggested disambiguated queries, and cluster nov-elty and similarity ranking. Disambiguated queries provide suggestions to the user for executing further more specific re-quests on a topic of interest that emerged, possibly unexpectedly, from within retrieved documents belonging to the same cluster. These queries might potentially retrieve new documents. To immediately identify the new relevant docu-ments, the user can rank clusters based on a combination of novelty and content similarity, thus achieving an optimization of this approach: by the aid of this mechanism, the number of query reformulation cycles, usually needed to obtain relevant information in a traditional search session, can be reduced.

The paper is organized as follows: in Section 2 , the related works are presented. In Section 3 the model of the disambig-the main results are summarized in the conclusion. 2. Related works
A common experience of users when searching the Web by submitting queries to search engines is the inability of finding pens because they generally formulate queries consisting of two or three terms, that are often ambiguous to gather appro-priate Web pages (Sanderson reports that from 7% up to 23% of Web searches in query logs consists of less than three terms only ( Sanderson, 2008 )).
Several papers have been published on the topics of query disambiguation based on extracting terms from retrieved doc-uments: the idea is to replace user-provided query terms with more specific ones, to narrow the search to the context users have in mind ( Lawrence, 2000; Teevan et al., 2008 ). In order to obtain these terms, some approaches exploit external knowl-contexts (Patwardhan, Banerjee, &amp; Pedersen, 2005; Fellbaum, 1998). A problem that may occur with these approaches is that the analysis of short query sessions. Some methods apply collaborative analysis evaluating the similarity between query logs Nevertheless, these techniques suffer from the start up problem, when no recommendations are available.
The approaches which generate groups of terms based on either terms co-occurrence analysis ( Liu, Jin, &amp; Chai, 2006 ), or
Differently from the previous approaches, our goal is to optimize the whole iterative process of query reformulation, by aid-ing the user to submit more specific disambiguated queries, and favoring the discovery of novel documents focused on the retrieved topics of interest.

The issue to provide novel contents in the top ranked results is also known as document diversification and has become diversification of documents is based on the comparison of documents one against another, assuming that similar docu-ments deal with similar topics. Then, to reduce the redundancy of top-ranked results, documents are ordered based on a combination of their diversity and query relevance, which in Carbonell and Goldstein (1998) and Zhai et al. (2003) is the Maximal Marginal Relevance (MMR) criterion, in Zhang et al. (2005) is the affinity ranking , that considers richness as well.
Novelty detection has been investigated also in relation with automatic text summarization ( Sweeney et al., 2008 ) and in some novel Web pages that focus on the interesting topics in the first positions ( Lad et al., 2007; Xu &amp; Chen, 2006 ).
In our proposal, the novelty metric that we define is related to clusters of documents, and measures the amount of new documents contained in a cluster with respect to documents previously seen by the user. Clusters can be ranked based on a tees the content-relevance of results. As discussed in Section 5.1 , the conjunctive use of suggested disambiguated queries, and a personalized combination of cluster novelty and content-similarity ranking aids users in discovery new relevant doc-uments, thus reducing the need of long iterative cycles of query reformulation. 3. The query disambiguation process 3.1. Process overview
The proposed disambiguation process, that revises the previous version reported in Bordogna, Campi, Psaila, and Ronchi (2009) , consists of six steps (see Fig. 1 ) realizing the three main phases presented in the Introduction (Section 1 ) and implemented by the meta-search system Matrioshka . The process is activated by an initial query q submitted by the user uments in the cluster; furthermore, clusters are ranked (and sorted) based on a personalized combination of their novelty with respect to previously retrieved and already seen documents, and their similarity to the initial query. The disambiguated queries in Q are generated so as to potentially retrieve new documents focused on the same contents of the associated clus-ters and are provided as suggestions to deepen the search. Submitting one of the suggested disambiguated queries to the newly retrieved documents.

In the following, we describe the process with more details, by explaining what happen at each step, and then by formal-izing the involved decisions functions.
 Step 3. Cluster candidates terms extraction . For each cluster c , a set
Step 4. Disambiguated query generation . This step generates a disambiguated query qe Step 5. Results visualization . The ranked clusters and the associated disambiguated queries are shown to the user.
Step 6. History Updating . The user can submit one of the suggested queries ( qe
In order to formalize each process step, we introduce some basic notions. 3.2. Basic notions
The ingredients that are generated and manipulated by the process are the following: (I) ( disambiguated ) query , (II) doc-ument , (III) cluster , and (IV) history . They are defined as follows. (I) A query q ,or disambiguated query qe , is a set of p terms t (II) A document d is a 4-uple (III) A cluster c is a 5-tuple (IV) The History is a 2-uple 3.3. Process steps in details
In the following subsections, the process steps are formalized. 3.3.1. Step 1. Clustering submit a query q to one search engine: the first N top ranked documents retrieved by the search engine are then grouped into homogeneous-labeled clusters. The fact that only the top N documents are considered for clustering example, if we choose N = 80, we consider the first 8 Web pages of results, which is a great improvement with respect to the time needed for clustering, ranking and generating the disambiguated queries.

We compute the documents content ranks (similarity rank) R independence of the actual ranking scores computed by the used search engine as follows:
This way, the first ranked document gets a value R d = 1, while the last one gets a value R
We use the Lingo clustering algorithm to perform an efficient flat clustering on the basis of the titles Tit the documents ( Osinski &amp; Weiss, 2005 ). Lingo also automatically associates a label L tents. Note that other clustering algorithms can be used, provided that they associate a label to the generated clusters.
It is worth noticing that we do not need to access the full text of the documents for extracting the features necessary to cluster them. We parse the result pages containing the first N results and extract all the information which constitute the 2008 ) performs lexicographic analysis, stop-words removal, conflation of terms having the same stem (recognized by apply-ing the Porter X  X  Algorithm, ( Lucene, XXXX )), terms expansion by using wordnet ( Fellbaum, 1998 ) to favor the clustering of documents; the URI strings is used to uniquely identify documents. Finally, it computes a ranking value w as the average of the ranking values of its documents: where D c are the documents belonging to the cluster c .

At the end of this step, in each tuple c = h D c , L c , Re (the cluster label) and w c , (the cluster rank) are instantiated, while Re val Status Value ( RSV c ) have an undefined value that will be computed in Step 3 and Step 2 , respectively. 3.3.2. Step 2. Personalized novelty and similarity ranking
This step is aimed at computing the Retrieval Status Value RSV w of the cluster with respect to the query q . It is defined as follows ( Dubois &amp; Prade, 1985 ): where Tnorm is a T-norm operator. In our experiment, we adopt the fuzzy AND operator defined by the min .
The combination is based on a preference degree k 2 [0,1], that defines the user desired preference for novelty with re-spect to content-similarity. The specification of distinct k makes it possible distinct personalization of the ranking.
If k = 0, the novelty of the cluster content is considered irrelevant by the user and thus RSV solely the content-similarity w c of the content of c with respect to the query content. Thus, we obtain:
On the contrary, k = 1 means that novelty has the same importance of content-similarity, and in this case the relevance score two criteria is guaranteed by the T-norm aggregation: novelty threshold 1 k , while for lower values the relevance degree is computed solely based on content similarity w that the novelty threshold is the minimum value above which the novelty of a cluster starts to become meaningful for the user. The greater is the preference of novelty to the user, the smaller are the novelty values that the user considers mean-ingful for the ranking.

Novelty. The novelty of a cluster c is defined as the proportion of new documents in cluster c with respect to those present in the History = h H , Rh i : where H  X fh U d 1 ; l H  X  d 1  X i ; ... ; h U d Z ; l H  X  d the content of the History . These documents are identified by the URI U 6: History Updating , in Section 3.3.6 ).

Notice that l c ( d )=1if d belongs to the cluster c (i.e., l crease, since it can happen that some documents in H are also present in cluster c .
 Notice that in the case in which we do not model the decaying of the History l other side, if we introduce a process to model the decaying/reinforcement of the History it can happen that 0 occurs in the situation in which a document was added to the History H many iterations before the current one and was forgotten it, but to a lesser extent than a recently-added document. 3.3.3. Step 3. Cluster candidates terms extraction
The third step of the process is devoted to the extraction of the r most representative terms associated to each cluster c , denoted as:
This step is crucial in the process, since the quality of the disambiguated queries strongly depends on the quality of these the best results were obtained by defining tf j , c based on the following multiple criteria.

Term Relative Frequency. The absolute frequency occ t , c titles and snippets of the documents in cluster c ; assuming that the titles are a more objective representation of the doc-ument contents than the snippet, a single occurrence in the title is weighted double than a single occurrence in the snip-pet; then, the absolute frequency is normalized with respect to the maximum frequency max extracted from cluster c . This way, the Relative Frequency of a term in a cluster is computed as follows:
Representativeness of the entire cluster. Let us denote with D representative of a cluster X  X  content, a term must be present in most of the documents of the cluster, not just in a few of them. To evaluate the satisfaction of this qualitative criterion, we impose a fuzzy restriction on the cardinality of D function most : [0,1] ? [0,1]: in which m is the upper-bound percentage of documents considered insufficient to characterize the majority of docu-ments in c , and M is the lower-bound percentage defining when the majority of the documents in c is full. We then com-
Experimentally, we tried several settings for m and M ; we achieved the best results with m = 0.5 and M = 0.7. With this of the documents in c , the restriction is not satisfied at all. For intermediate percentage values (in between 50% and 70%), the satisfaction increases linearly.
 cluster with respect to the others. Provided that C is the set of all clusters and C of their documents, it is defined by: where j C j is the total number of clusters and j C t j is the number of clusters containing t . ICF containing t are only in one cluster. The greater is the number of clusters containing t , the smaller is ICF
These three criteria are aggregated by a product to compute the representativeness degree tf where MaxICF = max t 2 c ( ICF t ) is the greatest among the values ICF
In this aggregation, the contribution of the inverse cluster frequency is lower-bounded to 0.5 to favor the selection of the query terms as representative terms. In fact, in spite of the fact that the query terms have null inverse cluster frequency, since they appear in all the snippets of all the retrieved documents, most of the times they could be selected as representa-tive terms, because they have a high relative term frequency rel _ freq selection criteria, the following properties are satisfied by the candidate set of terms.
 Most of the query terms are selected in the candidate set as explained above.

The majority of the documents of a cluster contain the selected terms: this is achieved by imposing that the selected terms are contained in most documents of the cluster (as defined by the fuzzy quantifier most );
The selected terms are meaningful in synthesizing the contents of the cluster with respect to other clusters ( Xu, Uszkoreit, the documents, (the occurrences in the titles have a double weight with respect to the occurrences in the snippets), fur-ther these terms appear in most documents of the cluster and have a high inverse cluster frequency. ments in cluster c . 3.3.4. Step 4. Disambiguated query generation
During this fourth step, a new disambiguated query qe c is generated as a subset of the set Re ciated with a cluster c produced in the third step.

We start from the assumption that Re c represents the main contents of cluster c as accurately as we can. As a conse-quence, submitting a query consisting of all the terms in Re on the contents of cluster c .
 documents, we generalize the representation of the cluster contents by generating a disambiguated query qe subsets of terms in Re c .

The function that determines the number of terms n c to select from Re length of the disambiguated query is inversely proportional to the Retrieval Status Value RSV rion, we can expect to retrieve other relevant documents by submitting a more general and shorter query that synthesizes topics of the cluster we need a more detailed and longer query.

Once the number of terms n c is determined, qe c  X  X  t 1 ; c terms in Re c , where their representativeness is defined by their weights, i.e., t
The terms in Re c with greatest representativeness weights are those in common with the query of the previous cycle, iteration, while they may be discarded in further iterations. So the procedure generates more specific disambiguated queries of the original ambiguous query at the first iteration cycle, while it may generate disambiguated queries expanding the meanings of the original one in subsequent iterations. This behavior can be observed in the example of Section 4 , by com-paring the disambiguated queries of the first iteration (in Fig. 3 ) with those of the second iteration (in Fig. 4 ). 3.3.5. Step 5. Results visualization
In this step, the ranked list of clusters and associated disambiguated queries are shown to the user, that can choose one of
The user is made aware of the clusters that contain more novel and/or more focused contents. By increasing the preference among which content-relevance alone, novelty alone, and a weighted balance of novelty and content-relevance based on the definition given in Step 2 . 3.3.6. Step 6. History updating is updated. Our assumption is that if a user selects a disambiguated query that was generated from a given cluster, it means (or many of) the documents contained in that cluster. In order to keep information about the contents already considered by the user, we store the representations d of the documents in the cluster from which the selected disambiguated query has been generated (visited cluster) into H .
 ing/reinforcement of the History is modeled, at each updating step, we monotonically decrease the membership degrees to the history for documents that appeared only in past iterations and increase the membership degrees to the history for doc-uments that appear in new iterations as follows: where d (0 6 d 6 1) is the decaying factor.

When d = 0, the above formula simply adds the new documents belonging to c (where c is the visited cluster associated with the selected query qe c ) to the history H without any decaying, i.e., with full membership degree l
When d &gt; 0, the decaying of the membership degrees of documents already in H is modeled. If d 2 D have that document d is added to the History H with full membership degree l ( d ) = 0, the current membership degree of d in the history is decreased by a factor that is proportional to j D ber of documents in c : in doing so, the more new documents are added to the history, the faster is the decaying of old doc-uments. This mechanism also serves to control the growth of the History H .
 Further, Rh as well, i.e., the set of representative terms of the History, is updated.

If a term t R Rh but t 2 Re c , it is added to Rh with tf tativeness degree is incremented with the current value of representativeness in Re not in Re c , its representativeness is decreased by a decaying factor d . The updating of tf defined as follows.
The set Rh can be shown to the user who wants to be aware of the main contents in the history. 4. Examples of query disambiguation
In this section, we describe an example of search process performed by using the Matrioshka meta search system. Fig. 2 depicts an overview of the main functional components of Matrioshka architecture.

Matrioshka is constituted by three main parts: client side , server side and communication layer . The client side components handle the user interaction. The server side component interfaces the search engines and executes the clustering task; fur-disambiguated queries. Finally, the Communication Layer dispatches the messages between client and server. Specifically, the query) or executes the operations on previously generated groups of clusters.

Let us describe the query disambiguation process described in the previous section by the aid of an example carried out through the use of Matrioshka . Let us consider to perform a search for information in order to spend a weekend in Bergamo (city in Northern Italy). The search process starts by submitting the generic query Bergamo to Yahoo! through Matrioshka Graphic User Interface (see Fig. 3 ) that is composed of three functional panels:
The top row panel allows a user to select a search engine to query (in figure, Yahoo! API was chosen), to submit a query by typing one or more words, and then by pressing the search button (in figure, the keyword  X  X  Bergamo  X  X  was submitted).
Furthermore, it allows users to set the maximum desired number of clusters to generate (20 in Fig. 3 ) and the maximum desired number of documents to consider (100 in Fig. 3 ).

The left-hand panel shows each submitted query and the corresponding ranked list of clusters. In the figure, query  X  X  Berg-expressing different topics of interest related to the original query  X  X  Bergamo  X  X .
 titles of the documents it contains, which are related to the topics of the cluster.
 a balance content/novelty with a preference weight for novelty equal to 100%. This means that a ranking of clusters defined as the min between content-similarity and novelty is applied. Since this is the first search, and consequently the history repository is empty, the resulting clusters are ranked solely according to the value of their content-similarity with respect to the initial query, since their novelty is maximal for all the clusters.

The right-hand panel can show either the ranked list of clusters with all details, in the case the user clicks on the query user clicks on the label of a cluster.

In the first case, the user obtains the list of clusters, together with the associated disambiguated queries (the gray but-tons). Clicking on the button, the user will submit the related disambiguated query to the chosen search engine, in order to deepen the search on the contents of the corresponding cluster.

The reader can see that the disambiguated queries contain the original query term  X  X  Bergamo  X  X  and are composed of selected words whose global meaning resembles the meaning of the cluster label.
 In this panel, for each cluster one can also see the values of content similarity and novelty.
 because novelty is irrelevant for clusters obtained by the first query.

Suppose now that, looking at the presented clusters and consulting the documents of the most interesting ones, we de-cide to make a deeper search with respect to the emerged topics described by the disambiguated query  X  X  Ski Bergamo Hotels  X  X  are not interested in the novelty criterion: so, we decide to set k = 0. The obtained results are presented in Fig. 4 .
The clusters obtained as a result of this second iteration show more specific contents focused on the topics related with solana  X  X , contain documents indeed related with the query, in the specific example hotels on the Alps near Bergamo. tents with respect to the original query, so the ranking is not informative with respect to the new contents of the retrieved clusters.

Furthermore, if we consult the documents contained in the top ranked clusters, we can observe that some of them were already seen in the previous results, and this could be a waste of time. If we change the preference for novelty to 100%, we can take the novelty of clusters into full consideration, and, as a consequence, we can change the presentation of results so not retrieved (see Fig. 5 ).

By considering the novelty criterion, the user can avoid the waste of time spent in consulting old documents (because he/ she is aware that they are contained in the lower ranked clusters), but, more importantly, the user obtains disambiguated queries that potentially can retrieve other relevant documents on more specific topics. The example described above high-lights how our technique modifies the interaction between the user and search engines. In particular, the proposed approach can be used to reduce the number of documents retrieved more than once in the top positions during a search session.
As it will be reported in Section 5 , the interaction framework realized by the aid of Matrioshka and exemplified above, allows users to optimize their Web searches. In fact, by submitting suggested queries and adopting a novelty and con-tent-similarity ranking, they can retrieve, in the top positions, the documents that satisfy their needs in a smaller number of query reformulations with respect to directly querying a search engine. 5. User evaluation and performance study
In this section, we describe the experiments we performed in order to evaluate the proposed Web search process based on the selection of disambiguated queries and the content-similarity/novelty re-ranking of clusters.
First we made a user X  X  evaluation of the utility of the proposed mechanism, and successively a performance evaluation by scaling the number of retrieved documents to assess the feasibility of the proposal. 5.1. User evaluation
We wanted to evaluate the advantages/disadvantages of using the proposed mechanism to perform Web searches. This is not a simple task since in the literature there is nothing similar that we could adopt as a baseline. So, we had to design an original user evaluation experiment.

The main objective was not to evaluate the accuracy of the clustering algorithm, since we adopted an algorithm proposed have a positive impact on the proposed approach. If clusters are more homogeneous, we expect that our algorithm generates more specific disambiguated queries, but this shall be proved by future experiments.

The user evaluation has been conducted with two distinct goals. The first aim was to analyze the difference between the disambiguated queries generated by Matrisohka and those suggested by a search engine. We report in Fig. 6 the results of this comparison for a set of 10 original queries submitted to Google directly and by the aid of Matrioshka . The terms that disam-biguate the original queries are in bold characters, while the disambiguated queries that have a correspondent equivalent extended query suggested by Google are highlighted in gray. Some considerations can be drawn: each query suggested by
Furthermore, for each original query, Matrioshka generates a greater number of suggested queries with respect to Google , for which the number of suggested queries is fixed to 10. If we consider the disambiguated queries provided by Matrioshka , we can observe that several of them generated for the same original query can be considered  X  X  X pecializations X  X  of an ex-aid of Matrioshka , one can more directly focus on a specific search.

The second observation is that Matrioshka suggested queries have a variable length (from two to five terms) that also de-pends on the original query length, as expected. This characteristics contributes to determine the greatest specificity of
Matrioshka suggested queries with respect to Google extended queries (see, for example, the suggested query  X  X  X orld cup facebook South Africa X  X  that would be very ambiguous by considering only the first three terms). At the same time, the crite-rion to determine the query length allows better disambiguating the meanings of the suggested queries (see, for example, the two disambiguated queries  X  X  X edding invitations customs X  X  and  X  X  X edding invitations Cards X  X  ).

The second goal was to analyze the benefits derived by the use of the suggested queries and the ranking of clusters based on a personalized balance of content-similarity and novelty.

A factor that we think can measure the benefit derived by the use of the whole process is the comparison between the number of query reformulations that the user engages in retrieving documents that satisfy her/his needs by our process and without our process (just by submitting manually generated queries to the search engine).

We proposed to 17 users, undergraduate students at Politecnico of Milano, to perform five search processes, each one on a distinct theme described as a TREC X  X  topic (reported in Fig. 7 ), by submitting a first general query and then iterating the search by expanding the original query both manually (or by picking up an extended query when using Google ) and by pick-ing up a query suggested by Matrioshka . The users were free to choose a personalized balance between novelty and similarity ranking of clusters when performing their searches with Matrioshka . In all experiments the decaying of the history was not modeled (the decaying factor was set to zero, d = 0). They had to go on by adopting the following decision criteria: continue searching if relevant information is retrieved that matches the topic needs; stop searching when feeling that better findings each manual query submitted directly to the search engine, as well as submitted through Matrioshka . The ordinal scale for
The first column indicates the used search engine, in the second column the queries submitted directly to the search en-tion of the user with the obtained results; column fourth reports the selected Matrioshka  X  X  suggested queries that have been submitted through Matrioshka to the same search engine specified in the first column, and in the fifth column the user X  X  sat-isfaction for the corresponding obtained results.
 Fig. 9 reports the summary of the results of this experiment for all the users on all search processes.
From the analysis of the results of all users, the first thing that we could notice is that generally, by using Matrioskha , one needs a smaller number of query cycles to obtain satisfactory results, since most of the times the clusters retrieved by the initial query contain a greater number of relevant documents than those retrieved by the search engine in the first result pages. We have computed, for each user, the iteration gain, defined as the ratio between the average number of disambiguated queries submitted through Matrioshka and the average number of reformulated queries submitted directly to the search engine for the five search processes. Only four users out of 17 employed the same number of iterations with the search engine and Matrioshka , while the majority of the users saved query reformulations with Matrioshka . The aver-age value of the iteration gain for all users and all search processes (85 search processes) is 0.84, which means that the average number of cycles with Matrioshka is 84% of the query cycles with the direct use of the search engines. We also observed that the novelty and similarity ranking of clusters was not used in relation with specific topics, but was used by specific users for all the topics with preference k = 1, and these 10 users were also those who achieved maximum gain.
From this observation, we can state that users that better understood the features of Matrioshka are those who could achieve greatest benefits in using it.

Second, for each user we computed the ratio between the average of the best satisfaction degrees on each search process obtained with Matrioshka and with the search engine, independently of the number of iterations. Only two users were better satisfied with the results of the search engine than with Matrioshka . The average on all users is 1.18 which means a fairly greater satisfaction by using directly Matrioshka than the search engine.

Third, for each user we evaluated the ratio between the average of the satisfaction degrees obtained with Matrioshka and with the search engine for all search processes. Again, two users were averagely better satisfied by the direct use of the search engine. The average on all users is 1.35 which means that, overall, Matrioshka performs much better than the search engine alone.
 choose to pick up a Google extended query to deepen their search instead of formulating manually a refined query: three these observations is that users prefer a free reformulation of refined queries, but when they are bound to resubmit sug-gested queries they achieve greater satisfaction.
 5.2. Performance
A critical point of a system like Matrioshka is the performance of the server component and, consequently, the scalability when the number of retrieved documents grows.

To evaluate this aspect, we repeated several experiments on different search engines, with a different maximum number of retrieved documents. We measured the execution times for all the components in the server side that are critical and un-der our control, i.e., all server components. However, we also measured the communication time needed by the server to contact the search engine, in order to have a full understanding of the overall system performance. We performed tests in a real settings, thus the maximum number of documents was limited by API provided by search services. In particular,
Google JSON (JSON interface to Google), provides a maximum number of 64 documents, Microsoft Bing API provides at most 50 documents, while Yahoo! API provides at most 100 documents.
 The experiments were performed on a PC powered by an Intel Pentium 4 641 3.2 GHZ processor, equipped with 1 GByte
RAM (of type DDR2 PC2-4200 SYNCH DRAM NON-ECC), a 256 GByte Hard Disk (Serial ATA II). The installed operating system is Linux Fedora 6 Core Distribution (kernel version 2.6.20-1.2952.fc6). Java classes were compiled with JDK version 1.6.0 03. Classes were executed using the Java Runtime Environment JRE1.6.0 03.

Scalability Issue. The first set of experiments, devoted to verify the scalability issue when the number of retrieved docu-ments increases, was performed on Yahoo! API . Table 1 shows the results.
 The table shows several execution times. They are the following.

Server component Times . This is a pool of measures performed on several server components.  X  Expansion (column Exp. ). The expansion time is spent by preprocessing titles and snippets for lexical analysis opera- X  Indexing (column Index ). During this phase, the Lucene library (version 2.9.3 ( Lucene, XXXX )) builds the inverted index  X  Clustering (column Clust ). Based on the inverted index, the Lingo Algorithm aggregates documents in clusters.  X  Tag Generation (column Tag ). During this phase, disambiguated queries are generated.  X  Ranking (column Rank ). This column measures all the time spent for ranking the clusters.
 Note that column Server is the sum of the previous columns and is the time spent in all server activities.
Communication Time (column Comm. ). This is the time needed by the server components to get the ranked list from the search engine.
 Total . This is the total time, that includes both the server and the communication time.
 Fig. 10 graphically shows the execution times reported in Table 1 .

First of all, we can observe that the connection time, i.e., the time spent by the Matrioshka server to connect to search engines, dominates the overall time. This result is important, since this means that the server can easily deal with several parallel requests implemented by Java threads, in that most of time is wasted waiting for the search engine.
Looking at the diverse server components, it is possible to notice that the most time-consuming phase is the expansion , due to the fact that the WordNet thesaurus is accessed to expand the terms extracted from titles and snippets. Furthermore, it is worth noticing how the tagging phase, that actually builds the disambiguated queries, is rather efficient.
The result is that the server components needs 154 msec. for processing 20 documents and, increasing in a quasi linear way, needs 1141 msec. for 100 documents. Notice that the server used for experiments is a normal PC, without an excep-tional computational power (the processor is a single core CPU, the amount of RAM is just 1 GByte); anyway, the overall time is dominated by the communication time: this is ten times greater than the server time in the case of 20 documents, and decreases to three times in the case of 100 documents.

The reader can observe, in Fig. 10 , that the server scales well with the number of retrieved documents, in all the measured phases.

Comparison among Search Engines. The second set of experiments we conducted, was aimed at understanding if there are different behaviors changing the search engine. We always considered the case of a maximum of 50 documents retrieved by the search engine, since all connected search engines are able to provide them. Table 2 reports the results.
The execution times reported in Table 2 are graphically depicted in Fig. 11 (server components) and Fig. 12 (server, con-nection and overall times).

The general consideration that we can do is the following: the behavior of the server components which do not depend on the interaction with the search engine and on the way the search engine provides the retrieved documents, are stable. Only system, meaning that the necessary computational effort is not too high and the specific implementation performs correctly.
In contrast, the most interesting points concern the interaction with the search engine. The first two search engines in the rather fast, if compared with the other two engines (i.e., Google and Google Scholar ) that provide a standard HTML pages, to that JSON representation is quite efficient and compact.
The compactness of the JSON representation is confirmed by the connection time: Google JSON is the search engine that of an XML document, and the expansion phase takes advantage of this fact, while the connection time is really too high if compared to the other search engines.
 The final consideration we can do is the following: as far as the performance is concerned, it is convenient to adopt
Matrioshka or an equivalent system for Web searches. In fact, it significantly reduces the communication costs, in that it reduces the number of queries sent to search engines, and consequently the network overload. As outlined in the previous subsection, by using Matrioshka one needs to submit 84% of the queries submitted directly to a search engine: if we suppose that five requests must be sent to a search engine to understand the next effective query to send, we have to multiply the connection time of each search engine by 5. The result is greater than the connection time needed for processing four
Matrioshka requests, that provide the set of disambiguated queries, thus probably addressing the user to the right direction. 6. Conclusion
In this paper, we described a novel approach to build disambiguated queries in a Web meta search process. Our approach exploits document clusters, dynamically generated, in order to identify the candidate contexts of the query, and uses key-word extracted by these clusters to generate more focused queries. These queries can be used as suggestions by the user for possible new focused searches. One important contribution of this work is the proposal of a mechanism to compute a viously seen documents, and the content-similarity with respect to the query. This is done not to the purpose of diversifying the contents of top ranked clusters as in Carbonell and Goldstein (1998) and Agrawal et al. (2009) , but to optimize the re-trieval and discovery of new documents on the topics of the cluster.

The evaluation experiments we carried out measured the benefits for the user deriving from the use of suggested queries and the novelty ranking of clusters in relation with the reduction of query reformulations needed to achieve satisfactory re-sults for the target searches.

We can envision some extensions of our work: first of all, the computation of the content-similarity of clusters to the query, that in the present version relies solely on the search engine X  X  ranking, could be improved. We intend to compute a content-similarity measure based on the representation of documents by their titles and snippets to complement the search engine rank. This would not require to access the Web pages but could give more information on the document con-tent. The proposal then must be evaluated more extensively by analyzing the impact of the decaying of the history on the novelty ranking as the query cycles increase.
 References
