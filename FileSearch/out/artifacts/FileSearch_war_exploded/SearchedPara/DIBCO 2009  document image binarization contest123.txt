 ORIGINAL PAPER B. Gatos  X  K. Ntirogiannis  X  I. Pratikakis Abstract DIBCO 2009 is the first International Document Image Binarization Contest organized in the context of ICDAR 2009 conference. The general objective of the con-test is to identify current advances in document image bina-rization using established evaluation performance measures. This paper describes the contest details including the eval-uation measures used as well as the performance of the 43 submitted methods along with a short description of the top five algorithms.
 Keywords Binarization  X  Performance evaluation  X  Document image preprocessing 1 Introduction Document image binarization is an important step in the document image analysis and recognition pipeline. There-along with an objective evaluation methodology in order to capture the efficiency of current document image bina-rization practices. To this end, we organized the first Inter-national Document Image Binarization Contest (DIBCO 2009) in the context of ICDAR 2009 conference. In this contest, we focused on the evaluation of document image binarization methods using a variety of scanned machine-printed and handwritten documents for which we created the binary image ground truth following a semi-automatic procedure based on Ref. [ 1 ]. The authors of submitted meth-ods registered in the competition and downloaded represen-tative samples along with the corresponding ground truth. At a next step, all registered participants were required to submit their binarization executable. After the evaluation of all candidate methods, the testing dataset (5 machine-printed and 5 handwritten images with the associated ground truth) became publicly available ( http://www.iit.demokritos. gr/~bgat/DIBCO2009/benchmark ).

The remainder of the paper is structured as follows: All the participants are listed in Sect. 2 . The evaluation measures are detailed in Sect. 3 . The experimental results are shown in Sect. 4 . In Sect. 5 , the top five methods are detailed while in Sect. 6 conclusions are drawn. 2 Participants Thirty-five (35) research groups have participated in the com-petition with forty-three (43) different algorithms (several participants submitted more than one algorithm). Table 1 presents all the participants sorted by the date of submission. A brief description of each participating method is given in Ref. [ 2 ]. 3 Evaluation measures The evaluation measures used comprise an ensemble of mea-sures that have been widely used for evaluation purposes. These measures consist of (i) F -measure; (ii) PSNR; (iii) negative rate metric and (iv) misclassification penalty metric. 3.1 Definitions  X  F -measure F -measure = where recall = TP TP+FN , precision = TP TP+FP
TP , FP and FN denote the true-positive, false-positive and false-negative values, respectively.  X  PSNR PSNR = 10 log where MSE = PSNR is a measure of how close is an image to another. Therefore, the higher the value of PSNR, the higher the sim-ilarity of the two images. We consider that the difference between foreground and background equals to 255 ( C = 255).  X  Negative rate metric (NRM)
The negative rate metric (NRM) is based on the pixel-wise mismatches between the GT and prediction. It combines the false-negative rate NR FN and the false-positive rate NR FP It is denoted as follows: NRM = itives, false positives, true negatives and false negatives, respectively.

In contrast to F -measure and PSNR, the binarization qual-ity is better for lower NRM.  X  Misclassification penalty metric (MPM) The misclassification penalty metric (MPM) evaluates the prediction against the ground truth (GT) on an object-by-object basis. Misclassification pixels are penalized by their distance from the ground truth object X  X  border.
 MPM = where MP FN = d i FN and d j FP denote, respectively, the distance of the i th false-negative and the j th false-positive pixel from the con-tour of the GT segmentation. The normalization factor D is the sum over all the pixel-to-contour distances of the GT object. A low MPM score denotes that the algorithm is good at identifying an object X  X  boundary. 4 Experimental results The DIBCO testing dataset consists of five machine-printed and five handwritten images resulting in a total of 10 images for which the associated ground truth was built for the evalua-tion. Representative example images of the dataset for printed and handwritten images are shown in Figs. 1 a and 2 a, respec-tively. The documents of this dataset originate from the col-lections of the following libraries: The Goettingen State and University Library, The Bavarian State Library, the British Library and the Library of Congress. The selection of the images in the dataset was made so that should contain repre-sentative degradations which appear frequently (e.g. variable background intensity, shadows, smear, smudge, low contrast, bleed-through or show-through).

The evaluation was based upon the four distinct measures presented in Sect. 3 . Apart the overall ranking (Table 2 ), we present the ranking on the machine-printed and the hand-written test images, separately (Table 3 ). For each type, the ranking was calculated after accumulating the ranking value of each method for all measures. Let R ( i,j ) be the rank of the i -th method using the j -th measure, where i =1... t , t denotes the number of the binarization techniques used in the evaluation and j = 1 ... m , m denotes the number of the evaluation measures. For each binarization method, the final ranking is achieved by the summation of the four rank-ings S i = 4 j = 1 R ( i , j ) . The smaller value of S i performance is achieved by the corresponding binarization method i . We further provide graphs that show the perfor-mance of the binarization algorithms in terms of F -measure and NRM (Fig. 3 ). Overall, the best performance is achieved by Algorithm 26 which has been submitted by S. Lu and C.L. Ta n o f t h e Institute for Infocomm Research in Singapore. To further provide a comparison with representative state-of-the-art binarization algorithms, Otsu [ 3 ] and Sauvola et al. [ 4 ] algorithms were applied at the DIBCO 2009 dataset using the DIBCO 2009 measures. Results are shown in Tables 2 , 3 . Example binarization results of the top five algorithms for machine-printed and handwritten images are shown in Figs. 1 b X  X  and 2 b X  X . 5 Top five best performing algorithms 5.1 Algorithm No. 26: Institute for Infocomm Research, The algorithm includes four parts, which deal with document background extraction, stroke edge detection, local thres-holding and post-processing, respectively. The local thresh-old is estimated by averaging the detected edge pixels within a local neighborhood window. A detailed description follows.
The background is estimated in a two-round strategy. In the first round, the row-scanned background surface is gen-erated by fitting a polynomial for each row of the document image. In the second round, we smooth the first-round back-ground surface by applying the polynomial smooth on that surface column by column. In each round of the smooth-ing, the data is 1-D image signal that is sampled from one row/column of the document image under study. Equation 5 below specifies the sampled data.
 X where X sm (j) refers to the origin pixel value, X sp (i) refers to the sampled pixel value, and step denotes the sampling step. This polynomial smooth procedure is employed itera-tively for each row/column. After each round of smoothing, the sampled data that is farthest from the fitted smoothing polynomial is removed. The smoothing proceeds iteratively until the maximum difference between the sampled data and the fitted smoothing polynomial is smaller than a pre-defined threshold.

When the background surface is estimated, the gradient information (GI) of each pixel is calculated as follows: First, the pixel differences between one pixel and its eight neigh-bors are calculated in four directions as specified in Eq. 6 . Then, the GI value of one pixel is defined as sum of the mag-nitude of the four direction differences. In addition, the GI ( 7 ) is further normalized by mdn/bg to deal with the uneven illumination.  X   X   X   X   X   X   X   X   X   X   X  A = I ( i , j + 1 ) + I ( i , j )  X  2  X  I ( i , j  X  1 ) B = I ( i + 1 , j ) + I ( i , j )  X  2  X  I ( i  X  1 , j )
C = I ( i + 1 , j + 1 ) + I ( i , j )  X  2  X  I ( i  X  1 , j
D = I ( i  X  1 , j + 1 ) + I ( i , j )  X  2  X  I ( i + 1 , j GI ( i , j ) = where I refers to the intensities of document image, A, B, C and D denote the pixel intensity difference of four directions, mdn is median value of BG , and bg refers to the intensity of that point in the estimated document background surface. The GI of document images usually has a bimodal pattern because GI of stroke edge points is generally larger than that of other points. The stroke edges can therefore be extracted by Otsu X  X  global thresholding method.

When the stroke edges are detected, document image pix-els can be classified by Eq. 8 as follows: R ( i , j ) = E where I refers to the input document image, E refers to the binary edge image, (i, j) refer to the corresponding pixels in document image. The threshold N min is defined by users, N refers to the number of edge points within the neighborhood window. E mean ( 9 ) is the sum of the mean of the intensi-ties of the edge points within the neighborhood window. So, if N e is larger than N min and I(i, j) is smaller than E R(i, j) issetat1.Otherwise, R(i, j) issetat0. 5.2 Algorithm No. 14: Universit X  Pierre et Marie Curie &amp; The algorithm is based on the toggle mapping [ 5 ] morpho-logical operator, and it is further developed to TMMS  X  X og-gle Mapping Morphological Operation X  for text localization in natural scenes [ 6 ]. According to Ref. [ 6 ], the algorithm outperforms standard thresholding criteria like Niblack X  X  or Sauvola X  X . In the following, a detailed description of the algo-rithm is given.

The toggle mapping morphological operator maps a func-tion f on a set of n other functions g i by replacing the value in each point of the function by the value of the local nearest function. The result r of the mapping is defined by the following Eq. ( 10 ).  X  x : r ( x ) = g i ( x ) ; min A common use of this operator is contrast enhancement. The background-to-foreground segmentation is based on the tog-gle mapping operator. They choose to map the image I on two functions: the morphological erosion E of the image and the morphological dilation D of the image. Then, for each pixel, if the given pixel value is closer to the erosion, it is marked as background and if the pixel is closer to the dilation it is marked as foreground.

The aforementioned strategy handles efficiently bound-aries of patterns but generates salt and pepper noise on homo-geneous regions. To avoid this issue, pixels whose erosion and dilation are too close are excluded from the analysis. In other words, every pixel with the difference between the dilation and the erosion is under a threshold t min is consid-ered as included in a homogeneous region and is excluded from the analysis. Pixels are then classified into three clas-ses: foreground (F), background (B) and homogeneous (H). Finally, homogeneous regions are assigned to foreground or background according to the class of their boundaries. This special treatment of homogeneous regions avoids a lot of noise in the background but may lead to miss major regions. Quality of results depends on the choice of t min for which open contours while a low value keeps noise in homogeneous regions. A hysteresis threshold is used in order to reduce the critical effect of the threshold parameter. This hysteresis thresholding comprises two thresholds instead of one: (a) A high threshold (for DIBCO 2009, a multiple of the distance of the two modes of the histogram was assigned) to select regions and (b) a lower threshold to define the boundaries of the selected regions. Foreground regions or background regions in the low thresholded image are kept if and only if they have a seed (marker) in the high thresholded result. Otherwise, if low thresholded regions do not contain at least one pixel on the high thresholded image, they are classified as homogeneous. In order to improve the quality of the output, a parameter p is added to manage the thickness of patterns. tmms(x)= Notice that in this definition, F and B can be interchanged whether the text is on a darker or a lighter background. 5.3 Algorithm No. 24: University of Quebec, Canada The method takes advantage of local probabilistic models and the calculus of variation. The statistics of the input image are used for the automatic estimation of the stroke width. Based on this, very small regions with small confidence scores are removed. The produced stroke map is eroded using a curve evolution approach implemented in the level-set framework using an energy term which measures the fitness of the stroke pixels with respect to the stroke gray level map. A detailed description of the algorithm is given in the following.
At first, an initialization map is required to identify high-probable text pixels. The remaining text pixels which may be degraded will be recovered by the local-linear evolution of the level-set function. For this purpose, we use one of multilevel classifiers [ 7 ], the stroke map (SM). In multilevel classifiers, there are many classifiers which uses different features to locate text pixels. Although the information at the pixel-level is helpful, a major part of the document image information is hidden in the spatial correlations. The classi-fiers at the content level, such a the SM, the stroke profile (SP) [ 7 ] and the stroke cavity map (SCM) [ 8 ], seek for this infor-mation based on the stroke-based features. In other words, theses classifiers try to use the document-related nature of the images. In the case of the SM, likelihood of having a stroke around the pixel under question is examined based on the structure of the text pixels around it. In this analysis, the average stroke width ws [ 9 ] is used to determine the pos-sibility of a stroke around the pixel. In a new kernel-based approach, on a neighborhood of size 2 ws +1, a score is calcu-lated based on which a SM value is assigned to the pixel. The SM can operate on different operation regimes. For the pur-pose of this work, which is to avoid as much as false-positive pixels, the SM is set to internal high-confidence operating mode. As it is obvious, the SM itself needs an initialization map to have an pre-estimation of the text pixels. We use the grid-based Sauvola X  X  method [ 9 ] to generate a fast and ade-quate initialization map for the SM.

In a next step, the stroke map is eroded in order to keep only the pixel that can be considered as stroke with an even higher confidence level and to remove salt and pepper noise. This process that helps in the subsequent creation of an accu-rate stroke gray level map is done by using a level-set based curve evolution approach. Within this scheme, we use two distinct local-linear models to represent the expected inten-sity of the strokes and that of the background, respectively. Also, a curvature-based term, which tends to smooth the con-tour, and a balloon force, which reduces the area of the stroke region, are used. The level-set methodology that is used is similar to that presented in Ref. [ 8 ] and [ 10 ] where a level-set function  X  is evolved with respect to an artificial time vari-able t . The initial position of the contour,  X  ( t = 0 ) by the SM obtained at step 1 and we set  X  so that  X   X  0 indicates the strokes and  X &lt; 0 indicates the background.
At a final step, a dense stroke gray level map (SGL) is created to represent the expected intensity of stroke pixels at any spatial position. Let  X ( t = t Erosion ) corresponds to the level-set function at the end of the preceding step. Then, this map is computed by using only the information of the stroke pixel, i.e. where  X  t = t Erosion  X  0. After that, the binarization is refined by using a level-set function similar to that of the previous step. However, in this case, the ini-tialization is given by  X  t = t Erosion , the balloon force is canceled and a term that measure the fitness of the stroke pixels with respect to the stroke gray level map is added to the curve evolution scheme. The final segmentation is given by a thresholding of the resulting level-set function  X   X  5.4 Algorithm No. 10: Tsinghua University, China, It is a three-step binarization algorithm. The first step locates the text area according to edge information, the second step binarizes the text area, and the third step modifies the bina-rized result from semantic perspective.

At the first step, we convert the original image I ori to gray scale I gray , then count gradient image I grd from it. We bina-rize I grd to get an edge pixel set S edg = p : I grd ( p with threshold T 1 = 120. We extract connected component from S edg . Area within bounding boxes of all the connected components are regarded as  X  X ext area X , denoted as S text
At the second step, we first find a global threshold V of text area using adaptive thresholding. For all the pixels p  X  S The following iteration will find the optimal threshold. (a) Divide all the pixel to two sets: S 1 = p : p  X  S text (b) Count mean value m 1 , m 2 of S 1 , S 2 (c) Update threshold V g = ( m 1 + m 2 ) / 2 (d) Go back to (a). Repeat until the new threshold matches The optimal threshold achieved by the aforementioned pro-cedure is in fact the iterative threshold proposed by Trussell the non-text-area pixels as background to get a rough binary result I b w .

At the final step, connected component analysis is run on all the black pixels in I b w , and count the average bounding box height L . L can be regarded as character size in the whole document. Image height H , image width W and L give us some semantic information about the document. We have three steps to modify I b w . (i) To handle non-even illumination, we add local thresh-(ii) Once again, we extract connected components from 5.5 Algorithm No. 9a: Universit X  de Lyon, INSA, France The method features an adaptive thresholding algorithm which has been designed to increase the local contrast in the text image.

The authors propose to normalize the different elements used in the equation which calculates the threshold T at Niblack X  X  algorithm [ 12 ], i.e. to formulate the binarization decision in terms of contrast instead of in terms of gray val-ues, which is a natural way considering the motivation behind Niblack X  X  technique. How can contrast be defined? In Niblack [ 12 ], the local contrast of the center pixel of a window of gray levels is defined as C where I is the gray value of the center pixel and, as men-tioned earlier, m is the mean and s is the standard deviation of the gray values in the window. Since dark text is consid-ered on bright background, points having a gray value which is higher than the local mean are not considered; therefore, the absolute value can be eliminated. Denoting by M the minimum value of the gray levels of the whole image, the maximum value of this local contrast is given by C It is difficult to formulate a thresholding strategy defined only on the local contrast and its maximum value, since this does not take into account the variation of the window with respect to the rest of the image. This is the reason why the defini-tion of a more global contrast is proposed; the contrast of the window centered on the given pixel is denoted as: C where R = max(s) is the maximum value of the standard deviations of all windows of the image. This contrast indi-cates whether the window is rather dark or bright with respect to the rest of the image (a high value implies the absence of text).

At a final step, a simple thresholding criterion is used to keep only pixels which have a high local contrast compared to its maximum value corrected by the contrast of the window centered on this pixel: I : C where a is a gain parameter. Developing this equation, the following threshold is obtained: T = ( 1  X  a ) m + aM + a s In the case that the given pixel is the center of a window with maximum contrast, i.e. s=R , we get T=m , the algo-rithm is forced to keep the maximum number of points of the window. On the other hand, if the variation is low (s R), then the probability that the window contains text is very low. Therefore, a pixel is only kept if its local contrast is very high. The threshold is denoted as T  X  ( 1  X  a ) m + aM .The gain parameter a allows to control the uncertainty around the mean value. A simple solution is to fix it at 0.5, which situates the threshold between m and M . 6 Conclusions DIBCO 2009 attracted 35 research groups that are currently active in document image analysis. The increased interest in this competition is a twofold proof: first, it shows the impor-tance of binarization as a step toward effective document image recognition and second, the need for pursuing a bench-mark that will lead to a meaningful and objective evaluation. References
