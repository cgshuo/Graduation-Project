 1. Introduction Query suggestion is an effective technique to help users by providing relevant query examples ( Baeza-Yates, Hurtado, &amp;
Mendoza, 2004; Jones, Rey, Madani, &amp; Greiner, 2006 ). This technique has been widely adopted for many Information Retrie-relevant to their current work. Since a scientific study is related to a number of research topics, people typically use many queries for retrieving a comprehensive list of related papers. In this situation, query suggestion can reduce the complexity of the search by providing effective query examples. In addition, sometimes scientists need to find relevant papers outside their specific area of expertise, and example queries can be a good guideline for exploring new areas.
 Despite the potential effectiveness of query suggestion in literature search, there has not been much research in this area.
Instead, prior work has focused on elaborating retrieval models that find related papers based on features derived from a  X  ier to formulate multiple effective queries. Furthermore, we can integrate these queries with state-of-the-art retrieval mod-els, leading to further improvement.

To develop effective suggestions for literature search, we need to consider its unique characteristics. In contrast to general in a very specific environment, and a query suggestion method should be designed for the unique characteristics of that envi-ronment. In literature search, one unique characteristic is that phrasal concepts and terminology (e.g.,  X  X  X exicon acquisition using bootstrapping X  X ) are frequently used as keywords in target documents (i.e., research papers). Since scientists use longer technical terms to describe their research ideas, phrasal concepts are frequently observed in academic writing. It follows that queries that emphasize phrasal concepts should be more effective for discriminating relevant documents from non-relevant documents in retrieval. In addition, typical users of literature search may prefer using phrasal-concept queries because phrases and terminology tend to have clear meanings, and users can more easily understand the areas that the suggested queries are targeting. Another typical characteristic of literature search is the lack of query log data. Many query suggestion methods for general web search rely heavily on large query logs (e.g., Baeza-Yates et al., 2004; Jones et al., 2006; Ma, Yang, search traffic.

Given that phrasal concepts are important for literature search and query log data is generally not available, we propose a query suggestion method that can generate phrasal-concept queries by exploiting pseudo-labeled documents. Specifically, we collect candidate (phrasal) concepts from pseudo-relevant documents (i.e., the top k documents retrieved by a baseline query), and identify  X  X  X ey concepts X  X   X  effective phrasal-concepts for finding relevant documents  X  by using the labeling prop-ciated candidate concepts. Once key concepts are found, we provide a context by extracting  X  X  X elated concepts X  X  that are statistically associated with a key concept. We then construct phrasal-concept queries by combining key concepts with their related concepts, and suggest a list of phrasal-concept queries in descending order of predicted effectiveness of their key concepts. Note that we use the terms phrasal-concept and concept interchangeably.
 cles are identified by asking the same scientists. However, no such data is available, and there have been alternatives pro-example, He, Pei, and Kifer (2010) develop an initial query using the sentences containing citations from a published paper, and regard the citations as the relevant articles. This approach favors a local recommendation because it only considers local contexts of the query paper (i.e., published paper) ( He, Nie, Lu, &amp; Zhao, 2012 ). On the other hand, the settings used in mary written by the user, and the list of references cited in the paper is the set of relevant documents. This method uses the global context of the query paper for retrieval. In this paper, we adopt this approach for the retrieval experiments.
We evaluate our phrasal-concept query suggestion method based on user preference as well as retrieval effectiveness. We conduct user experiments to verify that users prefer the queries suggested by our technique, compared to other effective query suggestion and query expansion methods. To assess the retrieval effectiveness of our method, we compare the retrie-val performance to other query expansion methods in simulated literature search environments.

The rest of this paper is organized as follows. In Section 2 , we outline previous work in query suggestion, query expansion, and literature search. Section 3 defines a phrasal-concept query suggestion task for literature searches, and Section 4 de-scribes our proposed techniques. In Section 5 , we provide experimental results and discussions. We summarize our contri-butions and future work in Section 6 . 2. Related work 2.1. Query suggestion
Query suggestion is a technique that recommends alternative queries for users X  initial queries, which has been proven to be useful for improving users X  search experience ( Baeza-Yates et al., 2004; Jones et al., 2006 ). Since large scale web search engines can easily gather search logs that include user-issued queries and interaction information (e.g., clickthrough statis-tics), many previous suggestion techniques, especially for the web domain, use such resources ( Baeza-Yates et al., 2004; Jiang clustering approach to extract similar queries from search logs for suggestion. They first cluster the aggregated queries using are extracted and suggested to the user. In a related approach Jiang &amp; Sun, 2011 ) exploit query-hashing algorithms, which can map similar queries into the same hash code. Given search logs, they first generate prior-knowledge that indicates pair-wise similarity and dissimilarity of the queries by using a hierarchical clustering, and formulate a hashing function which returns the same hash value for similar queries by minimizing the empirical error calculated using the prior-knowledge (while minimizing the distance between similar queries and maximizing the gap between dissimilar queries). Jones et al. (2006) generate query suggestions by reformulating the original query using substitutions. They substitute initial query terms by synonyms, generalization, specification, and related terms. To do this, they develop a binary classifier which can tuted (re-written) query&gt;) extracted from query logs. Another approach for query suggestion uses clickthrough data to iden-(user-query and query-URL bipartite graphs) are formed using clickthrough data, and based on these bipartite graphs and a
Markov random walk algorithm, a query similarity graph is generated and similarities between queries are propagated. Thus, given an initial query, similar queries, highly ranked by the similarity propagation of the initial query, are suggested to the
In this work, alternative queries are extracted from query logs considering both relatedness to an initial query and diversi-fication in the search results of the suggestions.

While query logs are readily available for web search, these data resources are generally not available in academic search environments. Some researchers have proposed query suggestion techniques that do not rely on query log data. For example
Bhatia, Majumdar, and Mitra (2011) suggest relevant n-gram phrases for an initial query without using query logs. They ex-tract n-grams from the corpus that are highly correlated with the partially input user query. In other words, relevant n-grams are suggested on the fly by completing the query that the user is typing. In our experiments, we use this approach as a base-line to compare with our approach. In the patent domain Kim et al. (2011) developed a Boolean query suggestion system that generates Boolean queries for an initial keyword query. They trained binary decision trees using the pseudo-relevant docu-ments retrieved by the initial query, and extracted decision rules that determine whether a new document is pseudo-rele-vant or not. By doing this, a Boolean query can be formulated as a decision rule, i.e., a sequence of terms associated by conjunction where each term can be prefixed by negation. 2.2. Query expansion
Automatic query expansion ( Mitra, Singhal, &amp; Buckley, 1998 ) has been studied as a means to bridge the vocabulary gap between users X  queries and relevant documents. In this process, initial queries are iteratively refined by including more terms that are potentially related to relevant documents. This area has been a focus of researchers for many years (e.g., Mitra query expansion is definitely related. However, the main difference is that query expansion methods place more emphasis on improving retrieval performance, while query suggestion techniques consider utility from the users X  perspective as well as retrieval effectiveness.

Among many different approaches, Pseudo-Relevance Feedback (PRF) ( Rocchio, 1971 ) is known as one of the most effective. This approach is based on the assumption that the top-ranked documents from an initial retrieval are relevant to the query. ( Xu &amp; Croft, 2000 ) extract expansion terms from the top retrieved documents for an initial query based on their co-occurrences with the initial query terms. Relevance models proposed by Lavrenko and Croft (2003) incorporate the pseudo-relevance assumption into the language modeling framework ( Ponte &amp; Croft, 1998 ). In this method, pseu-do-relevant documents can be used for estimating a query model by deriving a multinomial distribution over the terms in pseudo-relevant documents. Terms that have high probability of occurrence in documents strongly related to the query are highly likely to be selected as expansion terms. The Latent Concept Expansion (LCE) method ( Metzler &amp; Croft, 2007 )is the most closely related work to our method because it uses latent concepts extracted from pseudo-relevant documents to expand initial queries. However, this model works with short initial queries (e.g., the TREC query  X  X  X ubble telescope achievements X  X ) whereas we assume that academic users provide longer queries which describe their new papers or projects. In addition, the Markov Random Field (MRF) framework used in ( Metzler &amp; Croft, 2007 ) was less effective using related study is the query expansion method proposed by Fonseca, Golgher, Possas, Riveriro-Neto, and Zibiani (2005) . They view a past query in a query log as a concept, and past queries related to the current query are suggested to users to find more related concepts. However, their system relies on a sufficient volume of query log data, which cannot be easily ac-quired in typical small, domain-specific search systems. Other techniques related to our work involve modeling queries the initial query, and such concepts are less useful in our work because we focus on finding citations that can contain quite different terms from the initial query. 2.3. Literature search
Literature search is widely used by scientists for finding prior work that is relevant to their current research papers and projects. Previous studies in this area have improved retrieval models by extracting features from meta information (e.g., research interests of authors ( Basu, Hirsh, Cohen, &amp; Nevill-Manning, 2001 )). For example Bradshaw, Scheinkman, and
Hammond (2000) used only citing snippets to index articles cited at least once, and showed that this scheme could outper-form a model that indexes the whole text of articles. In addition Ritchie et al. (2008) also showed that combining the citing the initial retrieval results in their experiments. Using the MEDLINE database, documents by the number of times they were cited can lead to gains in precision. In a different approach Strohman et al. (2007) used statistical learning frameworks to combine various meta features such as citation counts, common authors, and distances in a citation graph, and this approach achieved significant improvements over the baseline that used only simple keywords.
More recently Bethard and Jurafsky (2010) developed citation behavior-based features. Since many academic authors appear lished by the query authors or their co-authors.

We improve literature search in several ways. First, we focus on making improvements to the query by providing query suggestions, whereas most existing work has elaborated retrieval models by meta information (i.e., information not based on the query text). Next, query suggestion techniques can be more practical and can help searches in real environments because the number of new articles is growing rapidly, and extracting all features from the corpus is complicated. Finally, the phra-sal-concept queries generated by our method can be incorporated with existing retrieval features in the best models, which may lead further improvements.

Another piece of related work in literature search is context-aware citation recommendation. This task has been proposed surrounding the citation of the query paper. To solve this He et al. (2010) used a non-parametric probabilistic model which measures the similarity between a given context and target article by concept-based likelihood distribution. As another solu-tion He et al. (2012) suggested a translation model to bridge the vocabulary gap between the context and retrieved docu-ments. This type of local recommendation is effective if the authors describe detailed contexts for citations. In our work, tions) are recommended by retrieval. Global recommendation is a different problem than local recommendation, and has been the focus of most prior work (e.g., ( Bethard &amp; Jurafsky, 2010 )). 3. Problem formulation
In this section, we provide term definitions that we will use throughout this paper, and formulate the phrasal-concept query suggestion problem for literature search.
 to a new research work. This search task is helpful for scientists when they initiate new research projects or write up their work. In this paper, we assume that the users provide a research summary or paper abstract as an initial starting point, and focus on suggesting effective queries that can retrieve documents relevant to the research described in the summary. line query to generate more effective query suggestions.

Definition 3 ( Pseudo-relevant documents ). Pseudo-relevant documents are the top k documents retrieved by the baseline query. A state-of-the-art retrieval model is used to generate the ranking, and we extract phrasal concepts used for sugges-tions from the pseudo-relevant documents.
 Definition 4 ( Phrasal concept ). A phrasal concept is a syntactic expression recognized as a noun phrase in a document.
Syntactically-based phrases will be more recognizable to users in general than term sequences of some length (e.g., bigrams names such as  X  X  X arkov Random Field X  X ), and noun phrase concepts have been shown to be effective for improving retrieval effectiveness ( Bendersky &amp; Croft, 2008 ). In this paper, we use the terms phrasal-concept and concept , interchangeably.
Definition 5 ( Key concept and related concept ). A key concept is an effective phrasal-concept for finding relevant documents, and a related concept is a phrasal-concept related to a key concept, which helps users to understand the key concept better.
For example,  X  X  X ext classification via WordNet X  X  can be a key concept, and  X  X  X upport Vector Machine X  X  and  X  X  X ordNet similarity feature X  X  could be related concepts. A key concept can have multiple related concepts, and to measure the relation between a concept and the key concept, various statistical similarity measures can be used (see Section 4.2.2).
 documents. We assume that the top n ranked concepts are the key concepts.
To improve the understandability of each suggestion and maximize retrieval performance, we include only a single key con-cept and its related concepts in a phrasal-concept query.
 to users. We suggest up to n queries which are sorted in descending order of predicted retrieval effectiveness of their key concepts. Since the key concepts in Problem 1 are ranked by their predicted retrieval effectiveness, we can address this prob-lem by solving Problem 1. 4. Phrasal concept query suggestion
In this section, we describe our method to generate phrasal-concept queries. Given an initial query that the users input, we generate a list of n phrasal-concept queries in the following steps: Step 1 : Generate a baseline query, BQ and gather the pseudo-relevant documents of BQ .
 Step 2 : Extract candidate concepts from the pseudo-relevant documents .
 Step 3 : Identify n key concepts by ranking the candidate concepts using BQ . Related concepts may be also extracted. Step 4 : Construct a list of n concept queries as query suggestions.

The first step is improving the initial query to generate effective phrasal-concept queries. In this step, we use existing query expansion methods (e.g., Latent Concept Expansion ( Metzler &amp; Croft, 2007 )) for the improvement. Since we assume that the users just input a bag of words as an initial query, such an initial query may perform poor and is not helpful for obtaining effective pseudo-relevant documents where phrasal concepts are extracted in the next step. To alleviate this, we use query expansion methods to generate a more effective set of pseudo-relevant documents. The query weighting schemes corresponding to the expansion method can also be applied. To formulate better baseline queries, we conducted preliminary experiments with several query expansion and generation methods and found that the LCE ( Metzler &amp; Croft, 2007 ) and machine learning-based approaches ( Huang et al., 2006 ) performed significantly better in our search environments. So, in our experiments, we use these methods to generate baseline queries. However, any other query improvement method (e.g., relevance model ( Lavrenko &amp; Croft, 2003 ) or the dependence model ( Metzler &amp; Croft, 2005 )) can be applied. Once a baseline query is formulated, we can obtain the top k pseudo-relevant documents from the retrieval result.
 Next, we extract candidate (phrasal) concepts by ranking the phrases recognized from the pseudo-relevant documents.
Then, in the third step, we rank the candidates with respect to their retrieval effectiveness predicted from the baseline query terms. After ranking, we assume that the top n (phrasal) concepts are key concepts, and combine each key concept with the related concepts that have high co-occurrence with the key concept. Finally, we can construct a list of phrasal-concept que-ries, each of which includes a single key concept and multiple related concepts. Fig. 1 shows an example of phrasal-concept query generation following this process, and the details of each step are described in the following sections. 4.1. Extracting candidate phrasal-concepts In the second step, we collect candidate (phrasal) concepts used for identifying key concepts and their related concepts.
By retrieving documents with the baseline query, we obtain pseudo-relevant documents, and then use them to extract can-didate phrasal-concepts. As we consider a noun phrase (NP) as a phrasal concept (see Definition 4 in Section 3 ), we apply an
NP recognizer 2 to the pseudo-relevant documents. However, due to the long length of academic articles (such as journal pa-pers), too many phrasal-concepts are recognized from whole text of an article. Therefore, to reduce the size of the candidate set, we assume that a title and abstract contain important phrasal-concepts which can represent the whole article. Accordingly,
N important phrasal-concepts from titles and abstracts of pseudo-relevant documents; among all the recognized phrasal-con-cepts, we can use n-gram language models to estimate the importance of each phrasal-concept recognized from the titles and abstracts of pseudo-relevant documents. In the experiments, we use 300 phrasal-concepts extracted by using tri-gram language models. The ranking function based on the model is given as: where w 1 w 2 ... w l is a concept whose word-length is l and k
To avoid the sparseness problem, the tri-gram language models are smoothed by bigram and unigram language models, and for each model we use maximum likelihood estimations based on term frequencies in the pseudo-relevant documents.
We empirically set the biases as k 1  X  0 : 7, k 2 = 0.2 and k multiple tri-grams from the phrasal-concept (see the first part of Eq. (1) ), and sum up the probability of each tri-gram to estimate the probability of the whole concept. 4.2. Identifying key phrasal-concepts
After collecting candidate phrasal-concepts, we identify key concepts by ranking the candidate (phrasal) concepts w.r.t. their predicted retrieval effectiveness. Given a set of candidate concepts and the baseline query, we assume that the concepts more similar to the baseline query will be more effective because the baseline query is effective for retrieving relevant doc-uments. As an example, in Fig. 1 , the initial query describes some graph-theoretic constraints for non-projective dependency parsing. In the baseline query,  X  X  X ependency X  X  and  X  X  X arse X  X  are effective keywords and highly weighted, and we can infer that among many related phrasal-concepts for this paper,  X  X  X on-projective dependency parsing X  X  is one of the most important phrasal-concepts. Since this phrasal-concept intuitively looks very similar to the keywords in the baseline query (i.e.,  X  X  X ependency X  X  and  X  X  X arse X  X ), it may have higher retrieval effectiveness. To identify this phrasal-concept as a key concept, we use the similarity between the phrasal concept and keywords. Thus, in ranking, we place the phrasal concepts more similar to many baseline query terms at higher ranks, and the highly ranked phrasal concepts are regarded as  X  X  X ey concepts X  X . query terms are propagated to the candidate concepts through a similarity matrix which defines the similarities between the candidate concepts and baseline query terms.

Suppose that we construct two vectors: (i) the vector of baseline query terms, concepts, v c . Define a term vector, V ,as V =[ v b , v c each v b 2 v b and each y c 2 y c is mapped to each v c 2 fine a | V | | V | similarity matrix, W which represents the similarities between " sim( v i , v j ), we can use one of the following similarity measures.

Point-wise Mutual Information (PMI) is a statistical measure which quantifies the discrepancy between the co-occur-rence probability in the joint distribution of v i and v j distributions. Using a corpus, the PMI of two terms (i.e., where v i , v j 2 V , df( ) denotes the document frequency in a corpus, and N is the number of all documents in the corpus.
Chi-square statistics ( v 2 ) is a statistical method that determines whether observed co-occurrence frequencies with the expected frequencies assuming independence. where a = df( v i , v j ), b = df( v i ) a , c = df( v j ) a , and d = N a b c .

Likelihood (LK) measures the likelihood of v j to v i , i.e., how much
Unlike the other measures, LK is directional, i.e., LK( v
With V , Y , and W , we perform the concept ranking algorithm ( Fig. 2 ) which produces a ranked list of the candidate (phra-sal) concepts. In ranking, an initial output vector Y (0) values of y b are 1.0 which indicates  X  X  X abeled X  X  (the highest retrieval effectiveness) and the values of y  X  X  X nlabeled X  X . Given a number of iterations (i.e., t ), the propagation runs iteratively, and the values of y more similar to the baseline query terms may have higher values than the others less similar to the baseline query terms. would be converged, i.e., the values of all candidate concepts are equal. Therefore, an appropriate value of t can be found by retrieval experiments (described in Section 5.1.2 ). After t iterations, the algorithm ranks v and the phrasal concepts with greater values are placed at higher positions in the output list. In the output list, we assume that the top n phrasal concepts are  X  X  X ey concepts X  X .

After identifying key concepts, we extract related concepts for each key concept. Since a similarity measure (e.g., PMI) can be defined between two phrasal concepts, we use it to extract  X  X  X elated concepts X  X  among all candidate phrasal-concepts. In extraction, for each key concept, v KC , we determine the set of  X  X  X elated concepts X  X , where h is the cut-off value, v KC is a key concept, v is a candidate phrasal-concept, set h as 0.01, 0.02, and 0.01 for PMI, v 2 , and LK, respectively.

Note that key concepts are identified as highly effective for retrieval, whereas related concepts are just strongly related to a key concept and provide additional context to the key concept for the users.
 4.3. Constructing phrasal-concept queries
Given the top n key (phrasal) concepts, we construct n phrasal-concept queries by associating each key concept with its related concepts. As defined in Section 3 , we ensure that a phrasal-concept query contains only a single key concept because a long query which contains several key concepts may be too complex to understand as a query suggestion. In addition, to further simplify the suggestions, we select the l most related concepts in the set of related concepts, experiments, we empirically set l as 4, i.e., we make a query contain at most 5 phrasal-concepts including a key concept.
Finally, the n phrasal-concept queries are suggested to users, where each query is formed as h Key Concept , Related Con-cept 1 , Related Concept 2 , ... i . The queries are listed in descending order of predicted retrieval effectiveness of their key concepts. 5. Experiments
This section describes evaluations for our method. In Section 5.1 , we conduct retrieval experiments to verify the retrieval effectiveness of our approach, and Section 5.2 describes user experiments on preferences. 5.1. Retrieval experiments: literature search simulation 5.1.1. Experimental setup
In the experiments, we use MontyLingua 3 to identify phrasal concepts from the pseudo-relevant documents. Queries and documents are stemmed by the Krovetz stemmer. To simulate literature searches, we set up experimental environments as follows: (Search Tasks) We conduct two different search tasks considering two domains of interest: the academic and medical domains. The task for the academic domain is finding academic papers relevant to a current research project. In this task, we assume that a scientist (user) inputs a summary of his research (i.e., we use title and abstract texts of his paper as the initial query) as an initial query, and we automatically generate a list of queries that can help to retrieve existing papers relevant to the research project. The search task for the medical domain is reference retrieval for an information need from physicians. We assume that physicians provide a statement of information about their patients as well as their information need, and we generate a list of queries that can retrieve relevant medical references for the information request. (Collections) For our search tasks, we use two different collections consisting of academic and medical literature. For the academic literature collection, we used the ACL anthology corpus ( Bird et al., 2008 ) which includes 10,921 academic papers published from 1975 to 2007. The full text of each article is available, and metadata (e.g., author names, venues, titles, and citations) is provided. We removed stop-words including frequently used acronyms (e.g., fig.) and section names (e.g.,  X  X  X ntroduction X  X  and  X  X  X elated work X  X ) from the documents. To develop initial queries, we randomly selected 183 query papers query by concatenating a query paper X  X  title and abstract. As done in previous research ( Bethard &amp; Jurafsky, 2010; Ritchie et al., 2006; Strohman et al., 2007 ), we consider the articles cited in each query paper as  X  X  X elevant X  X  and 12.19 citations are listed on average. In addition, we discarded the references to articles outside of the collection that is searched, and the query papers are removed from the collection and relevance judgments for other papers.

For the medical literature collection, we used the OHSUMED collection ( Hersh, Buckley, Leone, &amp; Hickam, 1994 ), which consists of 348,566 medical references (abstracts) and 106 queries. Each query contains the statement of patient information and information need from physicians. This test collection contains relevance judgments manually annotated using three  X  X  X elevant. X  X  (Assumptions for Experiments) To implement a literature search simulation, we made the following assumptions. First, searchers directly use suggested queries without reformulation. We believe this helps to show the lower bound of perfor-mance that the proposed technique can achieve. Second, in a multiple-query session, searchers try the queries in the sug-gestion order. Since modeling user behavior is beyond the scope of this paper, we simply assume that searchers sequentially examine the queries starting from the first one. (Evaluation Measures) To measure retrieval performance, we use traditional IR evaluation metrics as well as session-based measures. We adopt Mean Average Precision (MAP) and normalized Discounted Cumulative Gain (nDCG) ( Jarvelin &amp; Kekalainen, 2002 ) at the top 30 and 100 retrieval results. Also, normalized session Discounted Cumulative Gain (nsDCG) queries in a session. We use nsDCG to optimize our suggestion technique, and other traditional IR metrics (i.e., MAP and nDCG) for comparing our method with baselines. The metrics are calculated as follows.

First, MAP is defined in terms of Precision and Average Precision. Precision, P, is the fraction of retrieved results (documents) that are relevant , which can be calculated as: where D is the retrieved results and R is the set of relevant documents.
 Average Precision, AveP, is the average of precision at each point where a relevant document is found and is computed as: where D i is an i -th ranked result in D .
 Based on these, for a given set of queries, Q , MAP is calculated by: where q is a query in Q , R q is the retrieval results of q , and D
Second, nDCG is defined by Discounted Cumulative Gain (DCG) which discounts the documents placed at the lower ranks in the retrieval list. The DCG of a particular rank, DCG @ k , is defined as: where rel i is the relevance of the result at position i and rel
Using this, the nDCG at position k , nDCG @ k , can be computed as: where IDCG is an ideal DCG score, i.e., when every relevant document is placed at the top of the retrieval list.
Third, we adopt a session-based metric that can measure the overall effectiveness of multiple queries because we suggest multiple queries for a search session. ( Jarvelin et al., 2008 ) proposed the normalized session Discounted Cumulative Gain (nsDCG) which discounts documents that appear lower in a ranked list of an individual query as well as documents retrieved by the later suggested query. Given a session, nsDCG@ k is calculated as follows.

First, a rank list is constructed by concatenating the top k documents from each ranked list of the session. For each rank i in the concatenated list, the discounted gain (DG) is computed as: where rel i 2 {0, 1}.

We then apply an additional discount to documents retrieved by later suggestions. For example, the documents ranked between 1 and k are not discounted at all, but the documents ranked between k + 1 and 2 k are discounted by 1/log (2 + ( bq 1)) where bq is the log base and determined by search behavior. A larger base, e.g., 10, indicates that a searcher is patient and willing to examine more suggestions, while a smaller base, e.g., 2, represents an impatient searcher. In this paper, we use bq = 10 because academic searchers would use many queries to investigate more relevant articles. Then,
Session Discounted Cumulative Gain (sDCG) at top k is calculated by: where j = b ( i 1)/ k c and n is the number of suggestions (queries) in a session.

Accordingly, the final formula for nsDCG@ k is given as: where Ideal sDCG @ k is an  X  X  X deal X  X  score of sDCG obtained by an optimal ranked list in decreasing order of relevance. (Retrieval Model) For retrieval, we implement a learning-to-rank retrieval model using SVM model can efficiently learn the weights of retrieval features from training data. To compose a feature vector in the retrieval meta features extracted from the document (e.g., age, venue, and citation information), proposed in ( Bethard &amp; Jurafsky, used for the learning-to-rank model. (Baselines) Two different baseline approaches are employed for retrieval experiments. As baselines, we use the pseudo-relevance feedback techniques proposed in ( Huang et al., 2006; Metzler &amp; Croft, 2007 ), and the details of each method are summarized as follows. Latent Concept Expansion (LCE) ( Metzler &amp; Croft, 2007 ) is a robust pseudo-relevance feedback technique based on a
Markov Random Field framework. Comparing to relevance models ( Lavrenko &amp; Croft, 2003 ), this method is more generalized and can model term dependencies in a pseudo-relevance feedback process. To obtain feedback terms, we first obtain the top k pseudo-relevant documents (ranked using the sequential dependence model), and then the terms in the set of pseudo-relevant documents, R D , are ranked by: frequency in a collection, C , and c i is a free parameter.
 | C |) to avoid highly common terms in C .

We select 10 documents for R D and 80 (unigram) terms for feedback, and free parameters are set by 3-fold cross valida-tion. In addition, we used bigrams for the feedback, but could not obtain any significant improvements relative to just using unigrams.

Machine Learning-based Expansion (MLE) is a method using a statistical learner for pseudo-relevance feedback, in-spired by Huang et al. (2006) that exploits supervised learning algorithms. Given an initial query, to obtain a set of feedback terms, a linear regressor is trained with a set of features where each feature corresponds to a (unigram) term appearing in training documents (pseudo-relevant documents obtained by the initial query). Then, the trained regressor estimates the (pseudo-) relevance score of a new document, and the terms corresponding to highly weighted features are predicted to be effective for predicting pseudo-relevance. Note that this is a totally unsupervised procedure in that we do not use human-labeled samples.

We generate a set of training examples by using the top 100 pseudo-relevant documents and randomly sampled non-relevantdocumentswhicharenotinthetop100aspositiveandnegativesamples.Wescale(pseudo)relevancetoaninter-1.0}, and generate 11 distinct sets, each of which contains an equal number of training examples where each set is mapped to to 10 documents are assigned to 1.0) and the beyond-100 documents are used for 0.0 (non-relevant). A feature set contains all words(exceptstop-words)fromthepseudo-relevantdocuments,andafeaturevalueiscalculatedbythe tf X  X df ofaterm ineach document.After training,a weightvector, b is obtained,andamongall componentsof b , wecan select thetop k features(terms) by ranking them in descending order of their weight values in b . To formulate an expanded query, the initial query is combined with the top k feedback terms, and the weight value from b is used for feedback term weighting. The bias to feedback terms against the initial query is set as 0.5, and 120 terms are selected as feedback terms. We tested this method with the features of noun phrases (longer than unigram), extracted from the training examples using a phrase recognizer. settingofunigramtermscouldsignificantlyoutperformthecaseofnounphrases,andthusweusetheunigram-basedexpansionas a more robust baseline. 5.1.2. Optimizing parameters
The first experiment is conducted to optimize the parameters of our method. In the phrasal-concept ranking algorithm ( Fig. 2 ), the number of iterations and a similarity measure which defines a similarity matrix can influence the determination of key phrasal concepts. In addition, for academic literature search, we can use two different sets of candidates for ranking: (i) phrasal concepts only from titles of pseudo-relevant documents and (ii) phrasal concepts from titles or abstracts of pseu-do-relevant documents (see Section 4.2.1). Thus, we test with different numbers of iterations, combinations of 2 candidate sets, and three different similarity measures. However, for medical reference retrieval, we use all phrasal concepts identified from pseudo-relevant documents because the OHSUMED collection does not provide section information, but the three different similarity measures can be tested.

Fig. 3 depicts the average nsDCG@100 over 1 X 20 iterations using the ACL collection. In retrieval, we used the Indri search engine ( Strohman et al., 2005 ) to run the queries generated from each setting, each session, we generated 10 phrasal-concept queries using the 6 different combinations. First, as the number of iterations increases, the performance reached a peak and afterward slightly decreases. Second, among the three proposed similarity measures, LK (likelihood) shows significantly better performance than PMI and v titles only (TTL) can reach the maximum more quickly and are slightly better than the queries using the concepts from titles or and noisy. To find an optimal combination, we compared the average nsDCG@100 of every combination, and the queries generated using TTL, LK and 5 iterations significantly outperformed most of the other cases (statistical significance in p -value &lt; 0.05). Experiments using the OHSUMED collection showed similar tendencies. 5.1.3. Retrieval results
With the optimized parameters, we verify the retrieval effectiveness of our method on the two different search tasks. We use 3-fold cross-validation for evaluations, and LCE and MLE queries are used as baselines. As another baseline, we can con-sider the n-gram suggestion method (NGram) ( Bhatia et al., 2011 ). However, we do not use it for this experiment because
NGram focuses on finding relevant phrases for an initial query rather than improving their performance. Instead, we use that for user experiments (see Section 5.2 ). Besides, since the query expansion methods can significantly outperform n-gram sug-gestion in retrieval effectiveness, they can provide stronger baselines for retrieval experiments.

For academic literature search, we use the 20 features described in Tables 1 and 2 for our phrasal-concept queries, and the 16 features ( Table 1 ) for baseline queries since the baseline queries do not contain phrasal concepts so we cannot use the 4 concept-specific features ( Table 2 ). In the experiments of medical reference retrieval, we only use query-based features among the features in Table 1 because OHSUMED does not provide the meta information that is essential to implement
Query in Table 1 ) are used with LCE and MLE queries, and 4 concept-specific features are additionally included for phrasal-concept queries in OHSUMED experiments.
To compare the performance between our method (PHRASAL-CONCEPT) and the baseline, we use the best average precision scores of the top-1 to 10 ranked phrasal-concept queries for each session, e.g., if the users browse the top 10 sug-gestions, we select the best query whose average precision score is the highest. Since our method generates multiple queries for a session, we select a single best query by the assumption that users examine the search results by all the top-n queries and identify the best query among them. In other words, we report an upper bound of the performance achieved by our method. Since authors sometimes need to use many queries to explore more relevant articles to their papers, browsing all of the top-n suggestions is not unusual, and they can subsequently recognize the most effective query among them.
Besides, the baseline method can only generate a single best query, and the metric for multiple-query session (i.e., nsDCG) is not applicable.

Table 3 shows the average nDCG@100 and MAP of the results obtained by the best-performing query within the top-1 to 10 suggestions. First, in ACL, from the first suggestion, users can find an effective phrasal-concept query which can signifi-cantly outperform any baselines. Second, in OHSUMED, users need to examine the top two or more queries to find an effec-tive phrasal-concept query that can perform significantly better than the best baseline (i.e., LCE). Third, phrasal-concept queries are significantly better than the baselines in most cases. Unlike the baseline queries, phrasal-concept queries can exploit the concept-specific features, and this leads to significant improvements over the baselines. For example, in Table 4 , phrasal concepts in the concept query can effectively work with the concept-specific features for retrieval, whereas those features are not applied to the baseline query. This result is quite significant because we can identify that phrasal concepts can be new effective features for the literature search task, and are complementary to the previously developed features. 5.1.4. Further analysis
In Table 5 , we show the number of improved or degraded queries w.r.t. the best baseline (i.e., MLE), within the top-10 suggestions for the 183 queries in the ACL collection. From this table, we can study the robustness of the proposed approach.
About 70.6% of the queries generated by our method are more effective than the baseline. Moreover, about 44.4% of the gen-erated queries dramatically outperform the baseline (i.e., improvements are greater than or equal to 25%). 5.2. User experiments: preference survey
In the user experiments, we conduct a questionnaire survey to identify preferences among a number of query sugges-tions. In other words, we ask users to select the most effective suggestion among many query examples generated by several methods. By doing this, we intend to identify which methods can generate more useful queries for users. We first describe the details of the survey, and then provide the results. 5.2.1. Survey settings
In our survey, we assume a situation where users (assessors) need to construct a list of articles relevant to a given paper per. Then, we list 8 different query suggestions generated by 4 different methods (NGram suggestion (NGram; Bhatia et al., 2011 ), Relevance Model (RM; Lavrenko &amp; Croft, 2003 ), Machine Learning-based Expansion (MLE; Huang et al., 2006 ; see Sec-tion 5.1.1 ), and our method (PHRASAL-CONCEPT)) to an assessor. That is, two suggestions per method were provided. Finally, we ask them to select one or two queries that they believe would be more useful to retrieve relevant articles among the 8 suggestions. By doing this, the methods that can generate more effective queries for users would be chosen.
Fig. 4 shows an example of a question in the survey. To collect query papers, we selected 15 papers among the 183 query papers in our ACL collection (described in Section 5.1.1 ). For a fair comparison, the 15 papers were selected considering the results of retrieval experiments ( Table 5 ); first, we selected 5 papers for which our proposed method worked significantly better than the baseline method in retrieval experiments (i.e., MLE); second, 5 papers were chosen for which the baseline method outperformed our method; finally, 5 papers were randomly selected among the papers for which our method per-formed as well as the baseline. This survey was done by the help of 20 volunteers who were graduate students majoring in computer science and familiar with the topics in computational linguistics (on which the ACL query papers focus). The de-tails of each method are described as follows. (NGram) While most existing query suggestion methods require query logs, the method proposed in ( Bhatia et al., 2011 ) can suggest relevant n-grams without leveraging query logs. Since the original method aims at providing relevant n-grams when a user partially typed an initial query, we modify the method to fit in our search environments; we assume that a user finished typing the initial query and query completion is unnecessary. Note that this model is not used in the retrieval exper-iments (Section 5.1 ) because it focuses on suggesting correlated terms for an initial query rather than extracting effective ones for improving retrieval performance (as we explained in Section 5.1.3 ).

Similar to ( Bhatia et al., 2011 ), given an n-gram, we use the log-likelihood ranking function based on phrase-query correlations. where p i is an n-gram phrase, np is a noun phrase and df( ) denotes the document frequency in a corpus.
For an initial query, Q 0 , we use the title of a query paper, but in query ranking, as we see in Eq. (15) , we count only noun phrases (longer than unigram) in Q 0 because counting correlation of every term in Q grams) from pseudo-relevant documents, and generate two queries by selecting the top-1 to 5 and top-6 to 10 n-grams ranked by this method.

The other baselines are query expansion methods proposed by Lavrenko and Croft (2003) and Huang et al. (2006) , i.e., RM and MLE. For each baseline, we generate two different queries by selecting the top-1 to 5 and top-6 to 10 terms ranked by the method. We also use the top-1 and 2 phrasal-concept queries generated by our method with the optimal parameters (see
Section 5.1.2 ). As a result, 8 queries are suggested, and to prevent assessors from inferring methods by the order of sugges-tions, we randomly shuffle the suggestion order. 5.2.2. Survey results and quality analysis
In the survey, a total of 484 responses was collected, and for each question (query paper), a respondent selected 1.61 que-ries on average, out of 8 queries (we asked to select only one or two of the best queries).

We first analyze the quality of queries generated by each method. Table 6 shows the top one and two suggested queries by each method for two research papers. First, it is clear that our phrasal-concept queries can present more plausible phrases than the baselines. For instance,  X  X  X xtracting structural paraphrases X  X  refers to a task while  X  X  X ultiple sequence alignment X  X  refers to a technique used in the field of paraphrase recognition (paper 1). Also,  X  X  X xtracting product features and opinions X  X  and  X  X  X earning subjective nouns X  X  are important tasks in the study of opinion analysis (paper 2). Thus, these key concepts are related to many citations of each query paper. Second, the quality of NGram suggestions looks poor. Most of the suggested phrases are too general, and their meanings are vague since this method simply counts only correlations between the initial query and phrases without considering properties needed for queries in a specific domain. Another interesting point is that
MLE tends to suggest the names of important authors who published frequently cited papers, e.g.,  X  X  X egina Barzilay X  X  (for paper 1) and  X  X  X heresa Wilson X  X  (for paper 2). This is because MLE uses statistical leaning to extract highly discriminative terms, e.g., author name.

Next, we provide the average number of responses that selected queries generated by each method per question, as shown in Table 7 . First, users strongly prefer to use our phrasal-concept queries, i.e., PHRASAL-CONCEPT accounted for 62% of the all responses. Second, although NGram can suggest phrases to the user, NGram suggestions are significantly less preferred because of their poor quality. As discussed above, the concepts suggested by our method look more readable and effective to retrieve relevant documents, and thus the assessors in the survey could show preferences on phrasal-concepts.
However, user preferences in the survey may not reflect the exact effectiveness of suggestions in retrieval. Nevertheless, these preference results reveal that phrasal-concepts are more preferred by academic search users. Accordingly, our method is more useful than the baseline methods from the user perspective. 6. Conclusion
In this paper, we proposed a phrasal-concept based query suggestion technique for literature search. To generate more effective queries, we identified key concepts from pseudo-relevant documents by exploiting a label propagation technique and baseline query. By combining the key concept and its related concepts, a phrasal-concept query is generated. Through user studies and retrieval experiments, we showed that users strongly prefer to use our method and phrasal-concept queries can improve retrieval performance in literature search environments.

The merit of our approach is reproducibility and generalizability. To generate effective suggestions, we mainly use the concepts identified from pseudo-relevant documents, and similarities recognized within the corpus; any external resources or manually constructed data are not required. However, as Bai, Nie, Bouchard, and Cao (2007) studied, query contexts mined from external ontologies may help to identify more effective concepts and their relationships. Thus, for future work, we explore global information-based approaches applicable for the queries in academic literature search. In addition, we plan to use  X  X  X emantic X  X  concepts which cover semantic entities such as author names and domain-specific terminology in academic papers because such entities may be crucial to creating more effective and more  X  X  X nteresting X  X  queries from the user X  X  perspective.
 Acknowledgements sions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
 References
