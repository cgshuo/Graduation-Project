 n -gram representations of documents may improve over a simple bag-of-word representation by relaxing the indepen-dence assumption of word and introducing context. How-ever, this comes at a cost of adding features which are non-descriptive, and increasing the dimension of the vector space model exponentially.
 We present new representations that avoid both pitfalls. They are based on sound theoretical notions of stringology, and can be computed in optimal asymptotic time with algo-rithms using data structures from the suffix family. While maximal repeats have been used in the past for similar tasks, we show how another equivalence class of repeats  X  largest-maximal repeats  X  obtain similar or better results, with only a fraction of the features. This class acts as a minimal gen-erative basis of all repeated substrings. We also report their use for topic modeling, showing easier to interpret models. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing document representation; stringology; maximal repeats
Documents are not per se mathematical objects. In order to treat them like that, the first step is define a mapping that represents them as a data structure. For most type of doc-uments, the most natural such data structure seems to be a sequence. While this is standard in fields like bioinformat-ics, the very high dimensionality and lack of flexibility makes such an approach impopular for natural language applica-tions. In these applications the most popular representation is the vector space model, where a document d is mapped into a vector v ( d )  X  X  D . Normally, such a mapping proceeds in two steps: at first a set of defined set of features are ex-tracted from d , which are in second place weighted through a scoring scheme. For text documents the standard is to use the bag-of-words, where each dimension in the vector space model represents one word. This standard approach of just counting words (or unigrams) has some well-known short-comings. It is a lossy representation and as such can map different documents into the same representation (as in: the department chair couches offers and the chair department offers couches [14] ).

While less problematic in query systems (queries are short and modelling them as simple set of words is a good approx-imation of the real use), this may become an issue when comparing full-length documents between them. Another drawback of counting single words is that multi-words ex-pressions (collocations) are missed. A document where  X  X ew Haven X  occurs is probably different from one which contains separate occurrences of the words  X  X ew X  and  X  X aven X .
Both examples are consequences of the independence as-sumption of the unigram model. A common way of bypass-ing these issue is by using a higher level language model ( n  X  grams), which however introduces other errors, such as counting differently Sally has and Sally is . Any choice of n will cut constituents that are of size n + 1, while adding noise to constituents of size smaller than n .Atthesame time the dimension of the vocabularies increases exponen-tially with n (which is bad for efficiency reasons) and the vector becomes much sparser (which is bad for computing similarities). In general, for a n  X  gram models with n&gt; 3 this becomes so problematic that performance decreases con-siderably (although this is of course application dependent, for string kernels for instance [8] this threshold is reached at n = 5). The general solution to this is to combine several n -gram models: this requires extensive cross-checking to find the right subset of n  X  X  and increases even more the feature space.

In this paper we propose to use repeats (exact substrings that appear more than once in the documents) as basic fea-tures and to represent documents by a bag-of-repeats. They carry the same advantage as n -grams of providing an higher context for each term, while avoiding having to specify the value of n . Moreover, the length of a repeat is unbounded (except by the length of the document) and this can there-fore be considered as an infinite-gram model. The length of each repeat is simply determined by how long the sub-string is repeated. The use of maximal repeats has already been proposed in the past for the task of classification [11] and clustering for languages without explicit word bound-aries [9]. They permit to control the quadratic explosion of repeats with an equivalence class of repeats that contains all of them. We improve upon this by using largest-maximal repeats , another class of repeats that acts as a basis (in the algebraic sense of generator) of all substrings. We report experiments classifying and clustering text documents from various domains (20 newsgroups, TDT and HR forms) which show improved performance. Moreover, largest-maximal re-peats permit easier to interpret topics, addressing one of the major disadvantages of probabilistic topic models.
A sequence s is a concatenation of symbols s [1] ...s [ n ], which s [ i ]  X   X , the alphabet. The length of s , | s | is the num-bers of symbols, which we will generally denote by n .With-out loss of generality, we will assume that s starts and ends with a special symbol, not appearing elsewhere. Another sequence r is said to occur in s at position k if r [ i ]= s [ k + i ] for i =1 ... | r | . The set of occurrences of r in s is denoted by pos s ( r ). If pos s ( r )  X  2, r is called a repeat of s .Thesetof all repeats of s is denoted by R ( s ). While in general we will work with a set of sequences, this can always be represented as one big sequence concatenating all individual sequences intercalating unique separators.

The left (right) context of a repeat r in s for a subset of its occurrences p is defined as lc s ( r, p )= { s [ i p } ( rc s ( r, p )= { s [ i + | r | ]: i  X  p } ). For one occurrence o of r , we define it to be left-context unique if it is the only occurrence with this left-context: lcu s ( r, o )=( lc s ( r, lc ( r, pos s ( r ) \{ o } )). For a given r , a subset p  X  is said to be left-context diverse ( lcd s ( r, p )) if | Note that a subset of occurrences can be left-context diverse and not having any occurrence that is left-context unique. We will also use the respective definitions for right-context diverse ( rcd s ( r, p )) and right-context unique ( rcu s
Maximal repeats appear in the literature [7] as a compact representation of all repeats. Differently from normal re-peats, the number of maximal repeats inside a sequence is linear in n and it is trivial to recover all repeats from the set of maximal repeats.

A maximal repeat is a repeat such that if it would be extended to its left or right it would lose some of its occur-rences. Formally:
Definition 1 (Maximal Repeats). The set of maxi-mal repeats ( MR ) is the set of repeats that are left-and right-context diverse:
MR ( s )= { r  X  X  ( s ): lcd s ( r, pos s ( r ))  X  rcd s ( r, pos The property of maximality is strongly related to the context of a repeat. If the symbol to the left (right) of any occurrence of r is always the same, then r is not a maximal repeat because it could be extended to its left (right) without losing any occurrence.

As running example we will suppose the set of documents as shown in Table 1. For simplicity, we assume that the text represented by the dots there is unique. As in the rest of our experiments, we take the words to be symbols (not the characters). There are 32 repeats, but only 4 maximal ones ( Dear valued customer , Dear valued customer X , Dear val-ued customer Y , Dear valued customer Y with respect to ). The Table 1: Running example. Each line is a different document, and the dots represent unique text. Table 2: Upper and lower bounds for the number of normal, maximal, largest-maximal; and for the total number of occurrences of these classes. repeat with respect for instance is not maximal, as it can be extended to with respect to without reducing its number of occurrences.

However, suppose that all occurrences of a repeat r are covered not by one, but by two or more other repeats r 1 ...r . In this case r is still a maximal repeat, even if can be consid-ered as redundant by using repeats r 1 ...r . In our example, the repeat Dear valued customer can be covered by combin-ing the occurrences of Dear valued customer X and Dear valued customer Y . Such is the intuition captured by the following definition:
Definition 2 (Largest-maximal Repeats). The set of largest-maximal repeats ( LMR )isthesetofrepeatswhich have at least one occurrence right-and left-context unique : LMR ( s )= { r  X  X  ( s ):  X  o  X  pos s ( r ): lcu s ( r, o ) Any such occurrence is called a largest-maximal one.
In our example, Dear valued customer is not a LMR al-though the remaining three maximal repeats do belong to this set. Largest-maximal repeats cover the whole sequence (except for unique symbols), which is not necessarily true for more restrictive classes of repeats (like super-maximal repeats [7]). But they do it in a less redundant way than maximal repeats. Even more, as in the case of maximal re-peats, all occurrences of all repeats can be recovered from the set of largest-maximal occurrences. This condition does not hold any more for any strict subset. In this sense, largest-maximal repeat form a minimal basis in the algebraic sense, as they can been seen as a smallest generator of all repeats.
Table 2 gives an overview on the known bounds for these classes of repeats.  X  X ( n ) is the upper bound on the number of members of class X : max s : | s | = n {| X ( s ) |} where X stands for one of R , MR , LMR .  X  X ( n ) is the total number of oc-currences of repeats in X : max s : | s | = n { Of course  X  LMR ( n ) is upper-bounded by O ( n 2 ). It is how-ever an open problem if this bound is tight.

There are well-known algorithms for computing maximal repeats in linear time, using a data structure from the suffix family (like suffix tree or suffix array) [12] and Gusfield [7] outlines an algorithm to compute largest-maximal repeat. For our experiments we used in all cases a linear implemen-tation using the suffix array which processes roughly 500K-Figure 1: Regularization parameter versus mean ac-curacy for 5-fold cross validation on HR Forms 1.5M words per second on a commodity machine, depending on the type of sequence.
We first evaluated if using repeats as document repre-sentation instead of unigram does impact information re-trieval tasks. For this, we used three types of datasets. The first two are well-understood classical text collections, whose classification is rather easy (this is, where the stan-dard performance is close to 90%). In this group we used the 20 newsgroup dataset 1 (18 774 posts from 20 newsgroups) and the TDT5 news articles collection 2 (6 496 news arti-cles labeled with 126 stories). The other group consists of a non-public collection of scanned and OCR X  X d Human Resources (HR) forms of a multinational conglomerate, of 4 615 forms belonging to 34 categories. In all three cases, we learned a multi-label classifier through logistic regres-sion ( 2 -normalized, using the liblinear software 3 ). We re-port the results in Fig. 1 for the HR forms. The results on the other two datasets are similar, although the high per-formance achieved with the baselines makes the differences less substantial. As evaluation measure, we report the mean accuracy on 5-fold cross-validation (y-axis) for different val-ues of c , the regularization parameter in logistic regression (x). Note that  X  except for low values of c  X  the representa-tion of maximal and largest-maximal repeats perform better than any of the n -grams approaches. Statistical significance tests (Wilcoxon signed-rank) on the performances between any of the baselines ( n -grams) and the repeat-based meth-ods gave a p-value less than 5% for all values of c  X  0 . 1. Also note that while there where 527 K features in the combined uni-, bi-and trigram model; there were only 112 K maxi-mal repeats and 80 K largest-maximal ones (6 . 5 times less). Adding n -grams for higher values of n slowly approaches the performance of the maximal repeats, which is expected as the learning process of the classifiers starts filtering those n -grams which are meaningful.

The better performance of maximal repeats confirms ex-isting results [11], but the comparable results of largest-maximal repeats with respect to maximal ones are new. http://qwone.com/~jason/20Newsgroups/ www.ldc.upenn.edu/Projects/TDT5 http://www.csie.ntu.edu.tw/~cjlin/liblinear/ Table 3: Clustering results on 20NG and HR forms Figure 2: Clustering Performance ( F 1 )onTDT5 Remember that the latter is a strict subset of the former and if provided enough test data the supervised algorithm could learn to dismiss redundant features. We therefore per-formed an unsupervised experiment and clustered all three datasets. For the HR forms and the 20NG, we used k-means , reporting the average over 50 runs. We sampled a subset of 2 000 documents in the case of 20NG (different for each run). For TDT5 it is known that cluster methods where the number of clusters is fixed perform poorly [6] and we used the simple incremental clustering algorithm [13], that takes as parameter a threshold on t he similarity and which is a standard method for TDT tasks. Instead of the number of clusters, this algorithms takes as input a threshold on the cosine similarity for a document to belong to a cluster. The order of the documents is important here, so we report the average on 100 runs of random permutations. As measure we use precision and recall of the best mapping between pro-posed clusters and ground-truth clusters. In Table 3 are the results for 20NG and the HR Forms (using k-means )and Fig. 2 shows the evolution of the F 1 measure with changing threshold parameter for TDT5. Again, all the results are statistically significant (Wilcoxon signed-rank). Note that in all cases the results using LMR are better, while at the same time using less features (77% for 20NG and 73% for HR, compared to MR)
Because they tend to be larger than simple words, we tried how the expressiveness of repeats could be used directly, instead of indirectly as in the classification and clustering task we studied until now. Probabilistic Topic Models are one of the most popular unsupervised methods to explore a document collection. One of the main drawbacks of these methods is their interpretability [2]. The standard way is to show the k most probable words for each topic, but this un-igram representation may be redundant or not informative enough. Instead of using word counts as input for them, we provided repeat counts and used LDA [3], the most popu-lar of the existing probabilistic topic models. The vanilla-LDA model tends to favor very frequent features over less frequent ones, a known phenomenon that makes stop-word removal crucial, for example. Because shorter repeats tend to appear more frequent than longer, this therefore favors Table 4: 20 most probable words for a selection of topics, learned on a vanilla-LDA inferred with 50 topics, using Gibbs sampling (1000 iterations). repeated single words. To balance this phenomena, and also to reduce any bias due to over-counting the same words, we counted only largest-maximal occurrences (see Def. 2). As we said there, these occurrences are the minimal necessary information from which the fre quency information for all re-peats can be deduced. In Fig. 4 we report the most probable repeats for some topics, inferred on the AP dataset [3].
Repetitions have a long history in linguistics [16] and it is common to eliminate unique words in practical applications
The analysis of genetic sequences lacks of a precise defi-nition of words, and it is there where notions of repeats to represent documents have been used the most. The growth of available sequences has popularized alignment-free se-quence comparison methods: Apostolico [1] in particular maps the maximal repeats inside one sequence to a vector-space model, and compares those vectorial representations, although the repeats there are calculated intra-document/ sequence, while we propose to calculate them inter-document, taking into consideration the whole collection.

The use of MR for natural language documents has been particular useful in languages lacking a clear word segmen-tation [9], although the improvement they offer for classi-fication has already been reported previously [11]. LMR are called near-supermaximal repeats by Gusfield [7] but there they are used only to facilitate the definition of su-permaximal repeats. Recently, this class of repeats were re-discovered [10] and successfully applied to the automatic detection of certain genomic structures.

With respect to improving the interpretability of proba-bilistic topic models with phrases, good results have been obtained with the Turbotopics [4] where the representative multi-words expressions are co mputed a-posterior. Two vari-ants of the vanilla-LDA have been proposed that integrate an n -gram modeling: [14] combines an hierarchical bigram model with LDA, while [15] extends LDA by modeling the generation of documents as a process of either generating a unigram or an n  X  gram. Of course, this approaches heritates all the drawback of using n -grams for document modeling.
We presented a way of modeling documents fitting the vector space model that generalizes the n -gram approach by providing a potential infinite context. Moreover, by using a less well known class of repeats that acts as minimal ba-sis of all repeated substrings we show how to obtain better performance for clustering and comparable one for classi-fication tasks than using maximal repeats and using less features. Finally, their use for topic modeling permits in a straightforward way to obtain topics that are easier to interpret. Right now, for clustering we used a standard TF-IDF weighting schema, which does not take into account the length of the repeats: we plan to address this in future work. [1] A. Apostolico, O. Denas, and A. Dress. Efficient tools [2] D. Blei. Probabilistic Topic Models. CACM , [3] D. M. Blei and M. I. Jordan. Modeling annotated [4] D. M. Blei and J. D. Lafferty. Visualizing Topics with [5] M. Gall  X  e. Searching for Compact Hierarchical [6] M. Gall  X  e and J.-M. Renders. Full and semi-batch [7] D. Gusfield. Algorithms on Strings, Trees, and [8] H. Lodhi, C. Saunders, J. Shawe-Taylor, [9] T. Masada, A. Takasu, Y. Shibata, and K. Oguri. [10] J. Nicolas, C. Rousseau, A. Siegel, P. Siegel, F. Coste, [11] D. Okanohara and J.-I. Tsujii. Text Categorization [12] S. J. Puglisi, W. F. Smyth, and M. Yusufu. Fast [13] C. van Rijsbergen. Information Retrieval .
 [14] H. M. Wallach. Topic Modeling : Beyond [15] X. Wang, A. McCallum, and X. Wei. Topical [16] J. G. Wolff. Learning syntax and meanings through
