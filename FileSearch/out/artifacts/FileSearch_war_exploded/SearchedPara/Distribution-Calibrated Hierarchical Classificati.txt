 given domain. For example, consider the task of assigning a t opic to each document in a corpus. If a training set of labeled documents is available, then a mu lticlass classifier can be trained using a supervised machine learning algorithm. Often, large labe l-sets can be organized in a taxonomy. Examples of popular label taxonomies are the ODP taxonomy of web pages [2], the gene ontology [6], and the LCC ontology of book topics [1]. A taxonomy is a hi erarchical structure over labels, where some labels define very general concepts, and other lab els define more specific specializations of those general concepts. A taxonomy of document topics cou ld include the labels MUSIC , CLAS -SICAL MUSIC , and POPULAR MUSIC , where the last two are special cases of the first. Some label taxonomies form trees (each label has a single parent) while others form directed acyclic graphs. When a label taxonomy is given alongside a training set, the m ulticlass classification problem is often called a hierarchical classification problem. The label taxonomy defines a structure over the classification problem, and in the design of learning algori thms to solve this problem. Most hierarchical classification learning algorithms trea t the taxonomy as an indisputable definitive model of the world, never questioning its accuracy. However , most taxonomies are authored by human editors and subjective matters of style and taste play a major role in their design. Many arbitrary decisions go into the design of a taxonomy, and whe n multiple editors are involved, these arbitrary decisions are made inconsistently. Figure 1 show s two versions of a simple taxonomy, both equally reasonable; choosing between them is a matter of per sonal preference. Arbitrary decisions that go into the taxonomy design can have a significant influen ce on the outcome of the learning algorithm [19]. Ideally, we want learning algorithms that a re immune to the arbitrariness in the taxonomy. The arbitrary factor in popular label taxonomies is a well-k nown phenomenon. [17] gives the ex-ample of the Library of Congress Classification system (LCC), a widely adopted and constantly updated taxonomy of  X  X ll knowledge X , which includes the cat egory WORLD HISTORY and four of its direct subcategories: ASIA , AFRICA , NETHERLANDS , and BALKAN PENINSULA . There is a clear imbalance between the the level of granularity of ASIA versus its sibling BALKAN PENINSULA . The Dewey Decimal Classification (DDC), another widely accepted taxonomy of  X  X ll knowledge X  , classes. The rigid choice of a decimal fan-out is an arbitrar y one, and stems from an aesthetic ideal rather than a notion of informativeness. Incidentally, the ten subclasses of RELIGION in the DDC include six categories about Christianity and the addition al category OTHER RELIGIONS , demon-strating the editor X  X  clear subjective predilection for Ch ristianity. The ODP taxonomy of web-page topics is optimized for navigability rather than informati veness, and is therefore very flat and often unbalanced. As a result, two of the direct children of the lab el GAMES are VIDEO GAMES (with over 42 , 000 websites listed) and PAPER AND PENCIL GAMES (with only 32 websites). These ex-amples are not intended to show that these useful taxonomies are flawed, they merely demonstrate the arbitrary subjective aspect of their design.
 Our goal is to define the problem such that it is invariant to ma ny of these subjective and arbitrary design choices, while still exploiting much of the availabl e information. Some older approaches to hierarchical classification do not use the taxonomy in the de finition of the classification problem number of classification mistakes. More recent approaches [ 3, 8, 5, 4] exploit the label taxonomy as
CLASSICAL MUSIC when its true topic is actually JAZZ is not nearly as bad as classifying that document as COMPUTER HARDWARE . When this interpretation of the taxonomy can be made, ignoring it is effectively wasting a valuable signal in the p roblem input. For example, [8] define the loss of predicting a label u when the correct label is y as the number of edges along the path between the two labels in the taxonomy graph.
 Additionally, a taxonomy provides a very natural framework for balancing the tradeoff between specificity and accuracy in classification. Ideally, we woul d like our classifier to assign the most specific label possible to an instance, and the loss function should reward it adequately for doing so. However, when a specific label cannot be assigned with suf ficiently high confidence, it is often example, classifying a document on JAZZ as the broader topic MUSIC is better than classifying it as the more specific yet incorrect topic COUNTRY MUSIC . A hierarchical classification problem should be defined in a way that penalizes both over-confidence and und er-confidence in a balanced way. above, but it is very sensitive to arbitrary choices that go i nto the taxonomy design. Once again consider the example in Fig. 1: each hierarchy would induce a different graph-distance, which would lead to a different outcome of the learning algorithm. We can make the difference between the two outcomes arbitrarily large by making some regions of the taxonomy very deep and other regions very flat. Additionally, we note that the simple grap h-distance based loss works best when the taxonomy is balanced, namely, when all of the splits in th e taxonomy convey roughly the same amount of information. For example, in the taxonomy of Fig. 1 , the children of CLASSICAL MU -SIC are VIVALDI and NON -VIVALDI , where the vast majority of classical music falls in the latt er. If the correct label is NON -VIVALDI and our classifier predicts the more general label CLASSICAL MUSIC , the loss should be small, since the two labels are essential ly equivalent. On the other hand, if the correct label is VIVALDI then predicting CLASSICAL MUSIC should incur a larger loss, since important detail was excluded. A simple graph-distance bas ed loss will penalize both errors equally. On one hand, we want to use the hierarchy to define the problem. On the other hand, we don X  X  want arbitrary choices and unbalanced splits in the taxonomy to h ave a significant effect on the outcome. Can we have our cake and eat it too? Our proposed solution is to leave the taxonomy structure Namely, the loss of predicting u when the true label is y is defined as the sum of edge-weights along the shortest path from u to y . We use the underlying distribution over labels to set the ed ge Figure 1: Two equally-reasonable label taxonomies. Note th e subjective decision to include/exclude the label ROCK , and note the unbalanced split of CLASSICAL to the small class VIVALDI and the much larger class NON -VIVALDI . weights in a way that adds balance to the taxonomy and compens ates for certain arbitrary design choices. Specifically, we set edge weights using the informa tion-theoretic notion of conditional self-information [7]. The weight of an edge between a label u and its parent u  X  is the log-probability of observing the label u given that the example is also labeled by u  X  .
 Others [19] have previously tried to use the training data to  X  X ix X  the hierarchy, as a preprocessing data twice: once to fix the hierarchy and then again in the actu al learning procedure. The problem is that the preprocessing step may introduce strong statist ical dependencies into our problem. These dependencies could prove detrimental to our learning algor ithm, which expects to see a set of inde-pendent examples. The key to our approach is that we can estim ate our distribution-dependent loss using the same data used to define it, without introducing any significant bias. It turns out that to accomplish this, we must deviate from the prevalent binomia l-type estimation scheme that currently dominates machine learning and turn to a more peculiar geome tric-distribution-type estimator. A binomial-type estimator essentially counts things (such a s mistakes), while a geometric-type esti-mator measures the amount of time that passes before somethi ng occurs. Geometric-type estimators have the interesting property that they might occasionally fail, which we investigate in detail below. Moreover, we show how to control the variance of our estimate without adding bias. Since em-pirical estimation is the basis of supervised machine learn ing, we can now extrapolate hierarchical learning algorithms from our unbiased estimation techniqu e. Specifically, we present a reduction geometric-type estimator.
 This paper is organized as follows. We formally set the probl em in Sec. 2 and present our new distribution-dependent loss function in Sec. 3. In Sec. 4 we discuss how to control the variance of our empirical estimates, which is a critical step towards th e learning algorithm described in Sec. 5. We conclude with a discussion in Sec. 6. We omit technical pro ofs due to space constraints. We now define our problem more formally. Let X be an instance space and let T be a taxonomy of U both general labels and specific labels. Specifically, we ass ume that U contains the special label ALL , and that all other labels in U are special cases of ALL .  X  : U X  X  is a function that defines a more general label than u that contains u as a special case. In other words, we can say that  X  u is function  X  n : U X  X  is defined by recursively applying  X  to itself n times. Formally For completeness, define  X  0 as the identity function over U . T is acyclic , namely, for all u 6 = ALL ancestors, and is defined as  X   X  ( u ) = S  X  parent X  X  parent, and so on. We assume that T is connected and specifically that ALL is an ancestor of all labels, meaning that ALL  X   X   X  ( u ) for all u  X  X  . The inverse of the ancestor function is the descendent function  X  , which maps u  X  X  to the subset { u  X   X  X  : u  X   X   X  ( u  X  ) } . In other words, u is a descendent of u  X  if and only if u  X  is an ancestor of u . Graphically, we can depict T as a rooted tree: U defines the tree nodes, ALL is the root, and { u, X  ( u ) : u  X  X \ ALL } is the set of representation, we define the graph distance between any two labels d ( u,u  X  ) as the number of edges along the path between u and u  X  in the tree. The lowest common ancestor function  X  : U X U X  X  maps any pair of labels to their lowest common ancestor in the taxonomy, where  X  X owest X  is in the We denote the set of leaves by Y and note that Y X  X  .
 Now, let D be a distribution on the product space X X Y . In other words, D is a joint distribution over instances and their corresponding labels. Note that we assume that the labels that occur in the distribution are always leaves of the taxonomy T . This assumption can be made without loss of generality: if this is not the case then we can always add a lea f to each interior node, and relabel all of the examples accordingly. More formally, for each lab el u  X  X \Y , we add a new node y to U we do not know anything about D , other than the fact that it is supported on X X Y . We sample m independent points from D , to obtain the sample S = { ( x i ,y A classifier is a function f : X X  X  that assigns a label to each instance of X . Note that a classifier is allowed to predict any label in U , even though it knows that only leaf labels are ever observed in the real world. We feel that this property captures a funda mental characteristic of hierarchical more general label when it cannot confidently give a specific p rediction. The quality of f is measured using a loss function  X  : U X Y X  R require  X  to be weakly monotonic, in the following sense: if u  X  lies along the path from u to y then it is not what we have in mind. Another fundamental character istic of hierarchical classification , we actually expect that  X  ( u  X  ,y ) &lt;  X  ( u,y ) . distribution D , through its empirical proxy S . In other words, we want D to differentiate between informative splits in the taxonomy and redundant ones. We fo llow [8] in using graph-distance to define the loss function, but instead of setting all of the edg e weights to 1 , we define edge weights using D .
 each u  X  X  , define p ( u ) = P observing any descendent of u . We assume henceforth that p ( u ) &gt; 0 for all u  X  X  . With these weight is essentially the definition of conditional self inf ormation from information theory [7]. telescopes between u and  X  ( u,y ) and between u and  X  ( u,y ) , and becomes is completely invariant to the the number of labels along the path from u or y . It is also invariant to the addition or subtraction of new leaves or entire subtre es, so long as the marginal distributions Recalling the example in Fig. 1 where CLASSICAL is split into VIVALDI and NON -VIVALDI , the edge to the former will have a very high weight, whereas the edge to the latter will have a weight close to zero.
 Now, define the risk of a classifier h as R ( f ) = E examples sampled from D . Our goal is to obtain a classifier with a small risk. However, before we tackle the problem of finding a low risk classifier, we address the intermediate task of estimating the risk of a given classifier f using the sample S . The solution is not straightforward since we cannot even compute the loss on an individual example,  X  ( f ( x i ) ,y naive way to estimate  X  ( f ( x i ) ,y and to plug these values into the definition of  X  . This estimator tends to suffer from a strong bias, due to the non-linearity of the logarithm, and is considered to be unreliable 1 . Instead, we want an unbiased estimator.
 Define q ( f,u ) = Pr( f ( X ) = u ) , the probability that f outputs u when X is drawn according to that the lowest common ancestor of f ( X ) and Y is u , when ( X,Y ) is drawn from D . R ( f ) can be rewritten as Notice that the second term in the definition of risk is a const ant, independent of f . This constant is simply H ( Y ) , the Shannon entropy [7] of the label distribution. Our ulti mate goal is to compare constant, and we can discard it henceforth. From here on, we f ocus on estimating the augmented risk  X  R ( f ) = R ( f )  X  H ( Y ) .
 The main building block of our estimator is the estimation te chnique presented in [14]. Assume for a moment that the sample S is infinite. Recall that the harmonic number h with h For example, A the subtree rooted at f ( x 1 ) , and B label is contained in the subtree rooted at  X  ( f ( x 1 ) ,y L Theorem 1. L Proof. We have that E L 1 f ( X 1 ) = u,Y 1 = y = p ( u ) Using the fact that for any  X   X  [0 , 1) it holds that P  X  u,Y 1 = y ] =  X  log p ( u ) + 2 log p (  X  ( u,y )) . Therefore, We now recall that our sample S is actually of finite size m . The problem that now occurs is that A happens, we say that the estimator L and when f does not output labels with tiny probabilities. Formally, l et  X  ( f ) = min be the smallest probability of any label that f outputs.
 Theorem 2. The probability of failure is at most e  X  ( m  X  1)  X  ( f ) .
 The estimator E [ L ically, since we are after a classifier f with a small risk, we prove an upper-bound on  X  R ( f ) . Theorem 3. It holds that E L For example, with  X  = 0 . 01 and m = 2500 , the bias term in Thm. 3 is less than 0 . 0004 . With m = 5000 it is already less than 10  X  14 . Say that we have k classifiers and we want to choose the best one. The estimator L an unnecessarily high variance because it typically uses a s hort prefix of the sample S and wastes the remaining examples. To reliably compare k empirical risk estimates, we need to reduce the variance of each estimator. The exact value of Var( L non-trivial way, but we can give a simple upper-bound on Var( L Theorem 4. Var( L We reduce the variance of the estimator by repeating the esti mation multiple times, without reusing any sample points. Formally, define S S and uses A untouched example in the sequence. The second estimator, L examples, namely, the examples S chose some threshold t , the random variables L and therefore the aggregate estimator L = 1 Since L In the finite-sample case, aggregating multiple estimators is not as straightforward. Again, the event where the estimation fails introduces a small bias. Additio nally, the number of independent estima-tions that fit in a sample of fixed size m is itself a random variable T . Moreover, the value of T depends on the value of the risk estimators. In other words, i f L will take a small value. The precise definition of T should be handled with care, to ensure that the individual estimators remain independent and that the aggr egate estimator maintains a small bias. For example, the first thing that comes to mind is to set T to be the largest number t such that S t  X  m certainty that A and T , which both interferes with the variance reduction and intr oduces a bias. Instead, we define T as follows: choose a positive integer l  X  m and set T using the last l examples in S , as follows, set In words, we think of the last l examples in S as the  X  X anding strip X  of our procedure: we keep jumping forward in the sequence of samples, from S land on the landing strip. Our new failure scenario occurs wh en our last jump overshoots the strip, and no S L = P T i =1 L i . Note that we are summing L i rather than averaging them; we explain this later on. Theorem 5. The probability of failure of the estimator L is at most e  X  l X  ( f ) .
 We now prove that our definition of T indeed decreases the variance without adding bias. We give a rare case where S the predefined limit m ). We note that a very similar theorem can be stated in the finit e-sample case, at the price of a significantly more complicated analysis. Th e complication stems from the fact that we are estimating the risk of k classifiers simultaneously, and the failure of one estimato r depends on the values of the other estimators. We allow ourselves to i gnore failures because they occur with such small probability, and because they introduce an insig nificant bias.
 Theorem 6. Assuming that S is infinite, but T is still defined as in Eq. (3), it holds that E L ] = E T ]  X  R ( f ) and Var( L )  X  E [ T ]  X  2 , where  X  2 = Var L i ) .
 The proof follows from variations on Wald X  X  theorem [15].
 Recall that we have k competing classifiers, f risk. We overload our notation to support multiple concurre nt estimations, and define T ( f stopping time (previously defined as T in Eq. (3)) of the estimation process for  X  R ( f L we redefine T = min the same number of estimators for each classifier. We then cho ose the classifier with the smallest risk estimate, arg min L ( F definition of T remains a stopping time for each of the individual estimatio n processes. Although and the variance of L . We note that finding j that minimizes L ( f minimizes L ( f the variance of each L ( f unbiased estimate decreases like 1 / E [ T ] , which is what we would expect. Using the one-tailed Chebyshev inequality [11], we get that for any  X  &gt; 0 , Pr  X  R ( f bound). The variance of the estimation depends on E [ T ] , and we expect E [ T ] to grow linearly with m . For example we can prove the following crude lower-bound.
 Theorem 7. E [ T ]  X  ( m  X  l ) /c , where c = k + P k In this section, we propose a method for learning low-risk hi erarchical classifiers, using our new definition of risk. More precisely, we describe a reduction f rom hierarchical classification to cost-sensitive multiclass classification . The appeal of this approach is the abundance of existing cos t-sensitive learning algorithms. This reduction is itself an algorithm whose input is a training set of m examples and a taxonomy over d labels, and whose output is a d  X  m matrix of non-negative reals, the original training set, are given to a cost-aware multicl ass learning algorithm, which attempts to find a classifier f with a small empirical loss P m For example, a common approach to multiclass problems is to t rain a model f label u  X  X  and to define the classifier f ( x ) = arg max a cost sensitive classifier is to assume that the functions f where C &gt; 0 is a parameter and [  X  ] an empirical loss, justified by the fact that M ( i,f ( x i ))  X  P Coming back to the reduction algorithm, we generate M using the procedure outlined in Fig. 2. mate. The purpose of the random permutation at each step is to hopefully decrease the variance of the overall estimate, by decreasing the dependencies bet ween the different individual estimators. We profess that a rigorous analysis of the variance of this es timator is missing from this work. Ide- X  challenging problem due to the complex dependencies in the e stimator.
 The learning algorithm used to solve this problem can (and sh ould) use the hierarchical structure to guide its search for a good classifier. Our reduction to an uns tructured cost-sensitive problem should not be misinterpreted as a recommendation not to use the stru cture in the learning process. For example, following [10, 8], we could augment the SVM approac h described in Eq. (4) by replacing regularizer. learning problem. As a consequence, our focus was on the fund amental aspects of the hierarchical problem definition, rather than on the equally important alg orithmic issues. Our discussion was restricted to the simplistic model of single-label hierarc hical classification with single-linked tax-onomies, and our first goal going forward is to relax these ass umptions.
 We point out that many of the theorems proven in this paper dep end on the value of  X  ( f ) , which is defined as min is tiny and much of our analysis breaks down. This provides a s trong indication that an empirical estimate of  X  ( f ) would make a good regularization term in a hierarchical lear ning scheme. In other words, we should deter the learning algorithm from choosing a classifier that predicts very rare labels. As mentioned in the introduction, the label taxonom y provides the perfect mechanism for backing off and predicting a more common and less risky ances tor of that label.
 We believe that our work is significant in the broader context of structured learning . Most structured learning algorithms blindly trust the structure that they a re given, and arbitrary design choices are likely to appear in many types of structured learning. The id ea of using the data distribution to calibrate, correct, and balance the side-information exte nds to other structured learning scenarios. The geometric-type estimation procedure outlined in this p aper may play an important role in those settings as well.
 characteristics of hierarchical loss functions like weak m onotonicity. The author also thanks Ohad Shamir, Chris Burges, and Yael Dekel for helpful discussion s. [1] The Library of Congress Classification. http://www.loc .gov/aba/cataloging/classification/. [2] The Open Directory Project. http://www.dmoz.org/abou t.html. [3] L. Cai and T. Hofmann. Hierarchical document categoriza tion with support vector machines. [4] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Hierarchi cal classification: combining bayes [5] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Increment al algorithms for hierarchical classi-[6] The Gene Ontology Consortium. Gene ontology: tool for th e unification of biology. Nature [7] T. M. Cover and J. A. Thomas. Elements of Information Theory . Wiley, 1991. [8] O. Dekel, J. Keshet, and Y. Singer. Large margin hierarch ical classification. In Proceedings of [9] S. T. Dumais and H. Chen. Hierarchical classification of W eb content. In Proceedings of [10] T. Evgeniou, C.Micchelli, and M. Pontil. Learning mult iple tasks with kernel methods. Journal [11] W. Feller. An Introduction to Probability and its Applications , volume 2. John Wiley and Sons, [12] D. Koller and M. Sahami. Hierarchically classifying do cuemnts using very few words. In [13] A. K. McCallum, R. Rosenfeld, T. M. Mitchell, and A. Y. Ng . Improving text classification by [14] S. Montgomery-Smith and T. Schurmann. Unbiased estima tors for entropy and class number. [15] S.M. Ross and E.A. Pekoz. A second course in probability theory . 2007. [16] E. Ruiz and P. Srinivasan. Hierarchical text categoriz ation using neural networks. Information [17] C. Shirky. Ontology is overrated: Categories, links, a nd tags. In O X  X eilly Media Emerging [18] A. S. Weigend, E. D. Wiener, and J. O. Pedersen. Exploiti ng hierarchy in text categorization. [19] J. Zhang, L. Tang, and H. Liu. Automatically adjusting c ontent taxonomies for hierarchical
