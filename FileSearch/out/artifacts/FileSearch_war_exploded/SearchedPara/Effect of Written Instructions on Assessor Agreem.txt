 Assessors frequently disagree on the topical relevance of docu-ments. How much of this disagreement is due to ambiguity in as-sessment instructions? We have two assessors assess TREC Legal Track documents for relevance, some to a general topic descrip-tion, others to detailed assessment guidelines. We find that detailed guidelines lead to no significant increase in agreement amongst as-sessors or between assessors and the official qrels.
 H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation .
 Retrieval experiment, evaluation, e-discovery Measurement, performance, experimentation
Assessors frequently disagree on the relevance of a document to a topic. Voorhees [2000] finds that TREC adhoc assessors have mu-tual F1 scores of around 0 . 6 , while Roitblat et al. [2010] report mu-tual F1 as low as 0 . 35 for professional e-discovery reviewers. Such low agreement is of serious practical concern in e-discovery, where large-scale, delegated manual review is still widely used. Possi-ble causes of disagreement include assessor error and ambiguity in instructions. We examine whether detailed relevance guidelines in-crease agreement amongst assessors and with the guideline author, and find no significant increase in either form of agreement
We measure inter-assessor agreement by Cohen X  X   X  ,forwhich is perfect and 0 is random agreement [Cohen, 1960]. Unassessable documents (too long or misrendered) are ignored. A trial experi-ment of 75 documents per treatment, on Topic 301 from the TREC 2010 Legal Track, indicated that a sample size of 215 documents per treatment, with even proportions relevant and irrelevant, was required to achieve 80% power for a true  X  delta of 0 . 23 difference in agreement with official assessments between first and second tercile assessors in the TREC 2009 Legal Track. Work performed while interns at the University of Maryland.
Message type Messages Documents &gt; 1 relevant document 5 13 12 0
Relevant appealed 170 170 82 4 " unappealed 38 38 7 0
Irrelevant appealed 58 0 78 1 " unappealed returned 32 0 44 1 "" unreturned 16 0 17 1 Total 319 221 240 7 Table 1: Number and types of messages and documents sam-pled from Topic 204 for re-assessment. A message is classed as  X  X elevant X  if it contains a single relevant document (body or attachment). Counts of relevant, irrelevant, and unassessable documents are using the official, post-appeal assessments. Topic 204 from the interactive task of the TREC 2009 Legal Track [Hedin et al., 2009] was used for the full experiment. The corpus is the EDRM Enron emails. Whole messages were sam-pled, but each email body and attachment was separately assessed. A stratified sample was taken, as described in Table 1. The strata were divided evenly and randomly into two batches. Each batch was assessed in document id order, with the parts of a message be-ing assessed sequentially, as in TREC.

At TREC, a senior lawyer called the topic authority develops the topic, writes the detailed guidelines, and adjudicates appeals against first-round assessments. The appeals process for this topic was thorough [Webber, 2011], and the majority of sampled doc-uments were appealed; we regard the assessments as an accurate representation of the topic authority X  X  conception of relevance. We measure agreement for each batch between the two experimental assessors and the official, post-appeal assessments.

The latter two authors of this paper acted as experimental as-sessors. Each assessor assessed all documents in each batch. For the first batch, assessors were given the 42 -word topic statement to guide their assessments; for the second, they received the detailed relevance guidelines. A third pass was then made, in which the two assessors jointly reviewed both batches, in light of the de-tailed guidelines, and tried to agree on a conception of relevance.
Table 2 shows the results of our experiments. The provision of detailed assessment guidelines (Batch 2) did not improve agree-ment, significantly or otherwise, over topic-only instructions (Batch 1), either amongst assessors or with the official assessments, in either the full or the trial experiment. Message-level analysis (in
Batch A v BA v OB v OA v BA v OB v O Jnt-1 0 . 992 0 . 677 a 0 . 686  X  X  X  X  Jnt-2 0 . 950 0 . 665 b 0 . 674  X  X  X  X  Ta b l e 2 : C o h e n  X  X   X  values between official and two experimen-tal assessors, for full and trial experiments, on single-assessed Batch 1 (with topic statement only), single-assessed Batch 2 (with detailed guidelines), and (for full experiment only) joint-assessed Batches 1 and 2 (with topic guidelines and consultation between assessors). Columnar value pairs significantly differ-ent at  X  =0 . 05 (excepting inter-experimenter joint review) are marked by superscripts.
 Table 3: Two-tailed 95% normal-approximation confidence in-tervals on the true change in  X  between Batch 1 and Batch 2 amongst different assessor pairs, for the full experiment. which a message is relevant if any part of it is relevant) gives simi-lar results. Inter-assessor  X  values are high for the full experiment X  X  joint assessment, since assessors reached agreement on all save a handful of documents (1 for Batch 1, and 5 for Batch 2). Assessor A X  X  agreement with the official assessments increases significantly under joint review, but this may be due to Assessor A X  X  assessments moving closer to Assessor B X  X ; Assessor A X  X  self-agreement on Batch1is 0 . 399 post-consultation, whereas Assessor B X  X  is
Table 3 gives 95% confidence intervals on the true change in  X  values with the addition of assessor guidelines. A substantial improvement is still plausible in agreement between Assessor A and the official assessments, but not for Assessor B and official, nor for inter-assessor agreement.

Agreement between the original TREC assessors and the author-itative assessment on the documents examined in our experiment is 0 . 102 for Batch 1 and 0 . 024 for Batch 2, much lower than for our experimental assessors; however, this is a biased comparison, since sampling was heavily weighted towards appealed documents. Over the 7 , 289 documents sampled for assessment at TREC, though, the original assessors achieved a  X  of 0 . 320 , still well below that of the experimental assessors. The relatively high reliabilty of the asses-sor is reflected in their high mutual F1 scores (Table 4).
Qualitatively, the experimental assessors described the full ex-periment topic description by itself as being clear, and the detailed guidelines as being very clear and easy to relate to the documents. As can be seen in Table 2, agreement for this topic was generally higher than for the trial experiment.
Our initial, seemingly common-sense, hypothesis was that more detailed instructions would raise agreement between assessors and the authoritative conception of relevance, and therefore amongst assessors themselves. The results of this experiment have failed to confirm this hypothesis, or even to show a general trend in this direction. The only significant improvement occurred when Asses-
Table 4: Assessor mutual F1 scores for the full experiment. sor A consulted with Assessor B, but that may be attributable to the former X  X  assessments moving closer to the latter X  X . Indeed, confi-dence intervals indicate that a substantial increase in agreement is not plausible, except possibly between one assessor and the official view. We can conclude that, for this topic and these assessors, the provision of more detailed assessment guidelines did not lead to any marked increase in assessor reliability.

It is also notable that our experimental assessors, who were high school students with no legal training, appear to have produced assessments much more in line with the authoritative conception of relevance than the original TREC assessors, who were legally trained, professional document reviewers.

Our findings are not reassuring for the widespread practice of using delegated manual review in e-discovery. If assessors do no better with detailed guidelines than with a general outline of the topic, then there is an irreducible loss of signal in transmitting the relevance conception of an authoritative reviewer into the minds of other assessors. E-discovery practice is moving towards the use of statistical classification tools [Grossman and Cormack, 2011]; it may well be that the lawyer overseeing a case is better able to con-vey their conception of relevance by personally training a machine classifier, than by instructing delegated human reviewers.
