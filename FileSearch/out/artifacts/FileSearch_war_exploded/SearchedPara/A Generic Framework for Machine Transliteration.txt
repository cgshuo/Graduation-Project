 H.3.3 [ Information Search and Retrieval ]: CLIR, Ma-chine Transliteration Algorithms Machine Transliteration, Cross-language Information Retrieval
Machine Transliteration deals with the conversion of text strings from one orthography to another, while preserving the phonetics of the strings in the two languages. Translit-eration is an important problem in machine translation or crosslingual information retrieval, as most proper names and generic iconic terms are out-of-vocabulary words, and there-fore need to be transliterated. There are numerous methods explored in the literature for machine transliteration, rang-ing from rule-based techniques to statistical learning tech-niques. Here we focus our attention on language-independent techniques that potentially can scale well with a large num-ber of languages. In this paper, we present a modular, sta-tistical learning framework that lends itself for easy exper-imentation with transliteration tasks between a variety of different languages, in a language-independent manner. The workbench includes a variety of components  X  algorithms, data-sets and transliterations scripts  X  for a quick assembly of an effective transliteration system across langauges. We believe that such workbenches would be important in an in-creasingly multilingual world, for building systems that span a number of languages, quickly and effectively.
We present our machine transliteration framework based on a core algorithm modeled as a noisy channel [3]; the model poses the transliteration problem as a noisy-channel, where the source string gets garbled into target string. The transliteration is learned by estimating the parameters of the distribution that maximizes the likelihood of observing the garbling seen in the training data. Subsequently, given a target language string t ,wedecodeaposteriorithemost probable source language string s that gave raise to t ,as:
The transliteration model P ( t | s ) is learnt from the train-ing corpus and the P ( s ) is the language model for the source language strings. We segment the source and target strings as S = { s 1 s 2 ...s n } and T = { t 1 t 2 ...t m } ,where s are source and target language segments respectively, and estimate the P ( t | s ) approximately as:
Clearly, choosing automatically the right granularity for s and t i without language-specific tools or information, is a major challenge. Hence, we explore an expectation maxi-mization approach, which exploits the only information we know about the alignment, that some prefix (or suffix) of the source string must map to some prefix (or suffix, re-spectively) of the target string, in each of the paired strings in the training set. We extract substrings of up to a spec-ified length from both ends of each of the paired strings, as the initial hypothesis for our segment alignment; the fre-quencies of such alignments are used for estimating their alignment probabilit ies. Subsequent ly, we use viterbi algo-rithm to find the optimal alignment for each of the paired strings using the estimated alignment probabilities from the first step; the segment alignment probabilities are recalcu-lated in each re-training step, based on the counts of seg-ment pairs in the optimal alignments obtained in that step. At each re-training step, a test set is used to compute the transliteration accuracy, and the training is continued till the point when transliteration accuracy starts decreasing, due to over-fitting. The resulting transliteration model is used subsequently for that specific language pair. Related Work Our work parallels the language-independent approaches discussed in [5, 1, 4]. These works are primar-ily based on source-channel models, and their results shown on specific language pairs; in some cases the transliteration was modeled through the phonemic domain. Our frame-work is closest to that presented in [4], where the transliter-ation task was modeled as a source-channel [3] based on the orthographic n -grams of the source and target languages. However, we take an additional step making the framework modular and scalable, together with a scripting language for modeling training procedures.

We implemented a generic tra nsliteration framework, in which different algorithmic modules or any pre-or post-processing routines may be integrated easily. In addition, we defined a scripting language that can be used for defin-ing a transliteration task as a script. The languages to be handled and the relevant resource and data files may be read in dynamically. A script engine executes a user-defined script, and as directed, record the statistics during the run.
The quality of a transliteration measured as the fraction of correctly transliterated stri ngs is recorded, parameterized by training size, exact or fuzzy matching (the reference and result strings match fuzzily when the edit-distance between them is  X  20 % of the reference string size), recall in the top-n most-probable transliterated strings, etc.

Single Language Pairs Figure 1 provides a sample out-put of a transliteration task from English to Arabic. The script is written to record the results for various parameters, such as, training data size, exact vs fuzzy, top-n results, etc. From a result as above, several trends may be observed eas-ily: first, that the quality of transliteration (  X  80 %ofthe English names matched fuzzily with one of the top-10 re-sults, when a training set of about 20, 000 name pairs); sec-ond, that the transliteration quality improves with the size of the training data, but only asymptotically above 5, 000 ; third, fuzzy matches improve quality significantly, but intro-duces ambiguity in the results. However, introduction of a post-processing module that can resolve the ambiguity may be employed to boost the accuracy of the results (for ex-ample, we tried accessing the index of a search engine with fuzzy strings, which resulted in the correct identification of the transliterated string in vast majority of the cases.
Diverse set of languages Figure 2 provides the results of transliteration experiments on a set of languages, namely, English, Arabic, Japanese and 2 Indic (Hindi and Tamil) languages. In this experiment, we experimented with a spe-cific transliteration algorithm (as presented in Section 2), on transliteration tasks from English to one of the 4 target languages, and in the reverse direction. A template script is used for each task and the results are generated for the exact and fuzzy matches in the top-10 results. The objective of this experiment is to profile the transliteration performance of a given algorithm on a diverse set of language pairs. The results indicate that the algorithm exhibited a varying de-gree of quality in each language pair: For example, translit-eration accuracy from English to Arabic was  X  90 %, to Indic languages were  X  70  X  80 % and to Japanesewas  X  42 %.
Manual analysis indicate that the low quality of translit-erating Indic languages were due to improper segmentation, and that in Japanese was due to the weakly learned model Figure 2: Transliteration among a Set of Languages as most Japanese strings were concatenated strings (even when the corresponding English string is, say, Sir Issac New-ton ). We introduced a pre-processing module to force the alignment boundaries for Indic scripts to a pre-specified set (by observing the fact that no alignment boundaries can occur between a base character and a combining diacritic), which resulted in significant improvement in the transliter-ation accuracy. Though such additions are not generic, the workbench allows us to experiment with language-specific modules to boost transliteration accuracy on specific cases.
In this paper we outlined a generic framework for exper-imenting with transliteration tasks, in a modular manner. While our system displays a reasonable quality in translit-eration, a shade below the state-of-the-art for specific lan-guage pairs, its strength lies in the fact that new languages may be added easily, and an effective transliteration mech-anism may be developed quickly by experimentation with reusable components, a scripting language and language-specific modules. We plan to release this system as a work-bench for pedagogical purposes to let users explore various algorithms and training processes in a systematic manner on a wide variety of languages. In addition, it may provide a generic transliteration system for resource-poor languages. [1] AbdulJaleel, N. and Larkey, L. Statistical [2] Al-Onaizan, Y. and Knight, K. Machine [3] Brown, F. B. et al. The mathematics of statistical [4] Haizhou, L., Min, Z. and Jian, S. A joint [5] Virga, P. and Khudanpur, S. Transliteration of proper
