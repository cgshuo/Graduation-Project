 De veloping a dialogue management strate gy for a spok en dialogue system is often a comple x and time-consuming task. This is because the number of unique con versations that can occur between a user and the system is almost unlimited. Consequently , a system developer may spend a lot of time antic-ipating how potential users might interact with the system before deciding on the appropriate system re-sponse.

Recent research has focused on generating dia-logue strate gies automatically . This work is based on modelling dialogue as a mark ov decision process, formalised by a finite state space S , a finite action set A , a set of transition probabilities T and a re-ward function R . Using this model an optimal dia-logue strate gy  X   X  is represented by a mapping be-tween the state space and the action set. That is, for each state s  X  S this mapping defines its optimal ac-tion a  X  approaches have emplo yed reinforcement learning (RL) algorithms to estimate an optimal value func-tion Q  X  (Le vin et al., 2000; Frampton and Lemon, 2005). For each state this function predicts the fu-ture reward associated with each action available in that state. This function mak es it easy to extract the optimal strate gy ( policy in the RL literature).
Progress has been made with this approach but some important challenges remain. For instance, very little success has been achie ved with the lar ge state spaces that are typical of real-life systems. Similarly , work on summarising learned strate gies for interpretation by human developers has so far only been applied to tasks where each state-action pair is explicitly represented (Lec X uche, 2001). This tab ular representation severely limits the size of the state space.

We propose an alternati ve approach to finding op-timal dialogue policies. We mak e use of XCS, an evolutionary reinforcement learning algorithm that seeks to represent a polic y as a compact set of state-action rules (W ilson, 1995). We suggest that this al-gorithm could overcome both the challenge of lar ge state spaces and the desire for strate gy inspectability . In this paper , we focus on the issue of inspectabil-ity. We present a series of experiments that illustrate how XCS can be used to evolv e dialogue strate gies that are both optimal and easily inspectable. Learning Classifier Systems were introduced by John Holland in the 1970s as a frame work for learn-ing rule-based kno wledge representations (Holland, 1976). In this model, a rule base consists of a popu-lation of N state-action rules kno wn as classifier s . The state part of a classifier is represented by a ternary string from the set { 0,1,# } while the action part is composed from { 0,1 } . The # symbol acts as a wildcard allo wing a classifier to aggre gate states; for example, the state string 1#1 matches the states 111 and 101. Classifier systems have been applied to a number of learning tasks, including data mining, optimisation and control (Bull, 2004).

Classifier systems combine two machine learning techniques to find the optimal rule set. A genetic algorithm is used to evaluate and modify the popu-lation of rules while reinforcement learning is used to assign rewards to existing rules. The search for better rules is guided by the str ength parameter as-sociated with each classifier . This parameter serv es as a fitness score for the genetic algorithm and as a predictor of future reward ( payof f ) for the RL algo-rithm. This evolutionary learning process searches the space of possible rule sets to find an optimal pol-icy as defined by the reward function.

XCS (X Classifier System) incorporates a num-ber of modifications to Holland X  s original frame-work (W ilson, 1995). In this system, a classifier X  s fitness is based on the accurac y of its payof f predic-tion instead of the prediction itself. Furthermore, the genetic algorithm operates on actions instead of the population as a whole. These aspects of XCS result in a more complete map of the state-action space than would be the case with strength-based classi-fier systems. Consequently , XCS often outperforms strength-based systems in sequential decision prob-lems (K ovacs, 2000). In this section we present a simple slot-filling sys-tem based on the hotel booking domain. The goal of the system is to acquire the values for three slots: the check-in date, the number of nights the user wishes to stay and the type of room required (single, twin etc.). In slot-filling dialogues, an optimal strate gy is one that interacts with the user in a satisf actory way while trying to minimise the length of the dialogue. A fundamental component of user satisf action is the system X  s pre vention and repair of any miscommuni-cation between it and the user . Consequently , our hotel booking system focuses on evolving essential slot confirmation strate gies.

We devised an experimental frame work for mod-elling the hotel system as a sequential decision task and used XCS to evolv e three beha viours. Firstly , the system should execute its dialogue acts in a log-ical sequence. In other words, the system should greet the user , ask for the slot information, present the query results and then finish the dialogue, in that order (Experiment 1). Secondly , the system should try to acquire the slot values as quickly as possible while taking account of the possibility of misrecog-nition (Experiments 2a and 2b). Thirdly , to increase the lik elihood of acquiring the slot values correctly , each one should be confirmed at least once (Experi-ments 3 and 4).

The reward function for Experiments 1, 2a and 2b was the same. During a dialogue, each non-terminal system action recei ved a reward value of zero. At the end of each dialogue, the final reward comprised three parts: (i) -1000 for each system turn; (ii) 100,000 if all slots were filled; (iii) 100,000 if the first system act was a greeting. In Experiments 3 and 4, an additional reward of 100,000 was as-signed if all slots were confirmed.

The transition probabilities were modelled using two versions of a handcoded simulated user . A very lar ge number of test dialogues are usually required for learning optimal dialogue strate gies; simulated users are a practical alternati ve to emplo ying human test users (Schef fler and Young, 2000; Lopez-Cozar et al., 2002). Simulated user A represented a fully cooperati ve user , always giving the slot information that was ask ed. User B was less cooperati ve, giving no response 20% of the time. This allo wed us to perform a two-fold cross validation of the evolv ed strate gies.

For each experiment we allo wed the system X  s strate gy to evolv e over 100,000 dialogues with each simulated user . Dialogues were limited to a maxi-mum of 30 system turns. We then tested each strat-egy with a further 10,000 dialogues. We logged the total reward (payof f) for each test dialogue. Each experiment was repeated ten times.
In each experiment, the presentation of the query results and closure of the dialogue were combined into a single dialogue act. Therefore, the dialogue acts available to the system for the first experi-ment were: Gr eeting , Query+Goodbye , Ask(Date), Ask(Dur ation) and Ask(RoomT ype) . Four boolean variables were used to represent the state of the di-alogue: Gr eetingF irst , DateF illed , Dur ationF illed , RoomF illed .
 Experiment 2 added a new dialogue act: Ask(All) . The goal here was to ask for all three slot values if the probability of getting the slot values was rea-sonably high. If the probability was low, the sys-tem should ask for the slots one at a time as be-fore. This information was modelled in the sim-ulated users by 2 variables: Prob1SlotCorr ect and Prob3SlotsCorr ect . The values for these variables in Experiments 2a and 2b respecti vely were: 0.9 and 0.729 ( =0 . 9 3 ); 0.5 and 0.125 ( =0 . 5 3 ).
Experiment 3 added three new dialogue acts: Ex-plicit Confirm(Date) , Explicit Confirm(Dur ation) , Explicit Confirm(RoomT ype) and three new state variables: DateConfirmed , Dur ationConfirmed , RoomConfirmed . The goal here was for the sys-tem to learn to confirm each of the slot val-ues after the user has first given them. Experi-ment 4 sought to reduce the dialogue length fur -ther by allo wing the system to confirm one slot value while asking for another . Two new di-alogue acts were available in this last experi-ment: Implicit Confirm(Date)+Ask(Dur ation) and Implicit Confirm(Dur ation)+Ask(RoomT ype) . Table 1 lists the total reward (payof f) averaged over the 10 cross-v alidated test trials for each experiment, expressed as a percentage of the maximum payof f. In these experiments, the maximum payof f repre-sents the shortest possible successful dialogue. For example, the maximum payof f for Experiment 1 is 195,000: 100,000 for filling the slots plus 100,000 for greeting the user at the start of the dialogue mi-nus 5000 for the minimum number of turns (five) tak en to complete the dialogue successfully . The av-erage payof f for the 10 trials trained on simulated user A and tested on user B was 193,877  X  approxi-mately 99.4% of the maximum possible. In light of
Table 1: Payof f results for the evolv ed strate gies. these results and the stochastic user responses, we suggest that these evolv ed strate gies would compare favourably with any handcoded strate gies.

It is instructi ve to compare the rate of con vergence for dif ferent strate gies. Figure 1 sho ws the average payof f for the 100,000 dialogues trained with sim-ulated user A in Experiments 3 and 4. It sho ws that Experiment 3 approached the optimal polic y after approximately 20,000 dialogues whereas Ex-periment 4 con verged after approximately 5000 dia-logues. This is encouraging because it suggests that XCS remains focused on finding the shortest suc-cessful dialogue even when the number of available actions increases. Figure 1: Con vergence towards optimality during training in Experiments 3 and 4 (simulated user A).
Finally , we look at how to represent an optimal strate gy. From the logs of the test dialogues we ex-tracted the state-action rules (classifiers) that were executed. For example, in Experiment 4, the op-timal strate gy is represented by 17 classifiers. By comparison, a purely RL-based strate gy would de-fine an optimal action for every theoretically pos-sible state (i.e. 128). In this example, the evolu-tionary approach has reduced the number of rules from 128 to 17 (a reduction of 87%) and is therefore much more easily inspectable. In fact, the size of the optimal strate gy can be reduced further by select-ing the most general classifier for each action (Table 2). These rules are suf ficient since the y cover the 60 states that could actually occur while follo wing the optimal strate gy. We have presented a novel approach to generating spok en dialogue strate gies that are both optimal and easily inspectable. The generalizing ability of the evolutionary reinforcement learning (RL) algorithm, XCS, can dramatically reduce the size of the opti-mal strate gy when compared with con ventional RL techniques. In future work, we intend to exploit this generalization feature further by developing systems that require much lar ger state representations. We also plan to investig ate other approaches to strate gy summarisation. Finally , we will evaluate our ap-proach against purely RL-based methods.

