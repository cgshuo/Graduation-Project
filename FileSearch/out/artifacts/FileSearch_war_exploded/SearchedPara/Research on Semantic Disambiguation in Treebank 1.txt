 applications, and is heavily used in Machine Translation, information extraction, QA tic phenomena (for example, diachronic corpora can be used to study the time course of syntactic change). Once parsed, a corpus will contain frequency evidence showing resources, existing analysis algorithms and optimization methods of treebank. 
In previous studies, significant improvement has been made in the English syntac-word clustering, and words are represented as corresponding Category identification. the data sparseness that treebank brought. coding. All of this provides significant improvement. Combining with the characteris-sparseness problem in parsing. In traditional parsing process, we use lexical sequence, contextual words and speech sparseness problem. Table 1 shows three categories of data sparseness in parsing. 
Example Training sen-
From Table 1, as example 1 shows,  X   X  X  X   X  in training corpus and  X   X  X  X  X   X  in test-parser model, data sparseness may cause in testing process when there is not  X   X  X  X  X   X  but  X   X  X  X   X  in testing corpus. In this case,  X   X  X  X  X   X  is an unknown word. As example 2 shows, two  X   X  X  X   X  in two sentences, the former is a verb to make something quiet; 2, which will cause serious data sparseness. tion in different contexts can solve the data sparseness caused by above situations. In this paper, we use HIT Information Retrieval Laboratory Synonyms Cilin Ex-thesaurus consists of not only synonyms, but also a number of the words of the same classification, namely broadly related words. 3.1 Lexical and Semantic Representation Table 2 shows part of the lexical coding in HIT Information Retrieval Laboratory Synonyms Cilin Extended Version . row in Fig. 1 is a class of words with the same semantic. 
In HIT Information Retrieval Laboratory synonyms Cilin extended version , categories as following.  X  Univocal word  X  a word corresponds to only one coding, that is, there is only one word meaning even in different contexts. multiple meanings in different contexts . 3.2 Cilin Representation of Set can express Cilin using  X  X ODE TO WORD X  and  X  X ORD TO CODE X  as shown following. represents Cilin. word word word word = represents the Cilin corresponding to word represents the j-th sents set of all the encoding in Cilin. sents set of all the words in Cilin. 
SYWC word code word code word code =... represents Cilin. word represents the i-th word. Cilin corresponding to the i-th word th synonymous word in Cilin Cilin. In this paper, we divide the words into two categories, which are univocal word and polysemous word. In Cilin, a univocal word has only one coding, namely, a word has one meaning in different contexts. Thus, words can map to its corresponding encod-ing directly. syntax and structural information. Data sparseness problems stem from the ambiguity meaning words and mapping words into semantic encoding based on its surrounding context, the impact caused by word sense ambiguity can be reduced. 
For convenience, SC is defined as univocal word coding set. treebank. 4.1 Build Contextual Dictionary text and encoding context are introduced as two concepts. set of preceding n words and succeeding m words. And n&gt;0, m&gt;0, if n=0, nm_ lex-means lexical context of preceding.  X  Definition 3: nm_ encoding context: In the training corpus, set 
SYCW corresponds to a code ical context of all the univocal words in set context means encoding context of preceding. CONTEXTDIC code C C code C C code C C = i-th encoding in Cilin. 
Set the set of preceding context words corresponding to in context words of text word of code of succeeding context words corresponding to set is the same as weight W of context words using the following function: Among this, TF (cword) is the frequency of the current encoding preceding (succeed-ing) context word cword. P is the numbers of all encoding rences of word cword encoding in preceding (succeeding) context. 
Because of the limitation of the Cilin, we require every encoding of the word to be disambiguated can be able to find at least one corresponding univocal word in Cilin, in order to ensure the accuracy of word sense disambiguation. 4.2 Contextual Vector Representation nm_ lexical context and nm_ encoding context are consist of a series of words. Words are expressed by weights in the calculation process, then, word sense disambiguation word here is nm_ lexical context of the words in treebank and nm_ encoding context word in Cilin. In set CONTEXTDIC, set tual vectors are needed to represent nm_ encoding context of also need m contextual vectors to represent nm_ encoding context of position of preceding (succeeding) context of text word in the k-th position of preceding (succeeding) context of vector represents the weight of code . Each context word needs a contextual vector of trasting elements of same dimension as V V G 4.3 Semantic Similarity Calculation namely, the word is mapped into its encoding. of polysemous word and context of encoding by the method of similarity calculation, as the function following: Ultimately, function calculating the similarity of word ing: polysemous word is disambiguated, semantic similarity of polysemous word
SimCos Threshold &gt; , and its encoding encoding of ty, and SC is encoding set of univocal words. We trained and tested on two kinds of treebanks. One is Chinese Treebank (CTB)5.0, SIGHAN2012 parsing evaluation Tsinghua Chinese Treebank (TCT), training corpus 666 L. Miao et al. Baseline method of corpus1. Corpus 2 is unoptimized TCT, and act as baseline meth-od of corpus 2 after training. 5.1 Evaluation Indicators parsing. The algorithm is as following: value. 5.2 Selection of Sim ilarity Threshold and semantic in current context. Then we need to select similarity threshold. Thresh-threshold. 
Actually, with the increase of the threshold, the number of polysemous word to be rules. The results of threshold selection are shown in Fig. 4. 
After analysis of Fig. 4, is 0.2. Therefore, we use 0. 2 5.3 Experiments and A Comparative Experiment-b is parsed by Berkeley Pars e ble 3. 
It seems that when thre s precision is higher than b a s to precisely express the w o crease in accuracy of pars i recognition. When thresho l baseline1 method shows th a ical elements of the word i 668 L. Miao et al. semantic information can enhance parsing and optimize treebank. improvement. We use unoptimized TCT as corpus 2, and get 0.6 as its threshold by experiment. Table 4 shows the results of TCT parsing by Berkeley Parser. 
Table 4 shows a great success over baseline2 method. This proves semantic encod-we will get remarkable improvement as the increasing of corpus. success with a decrease of 8.17% over error rate in corpus 1 and an improvement of treebank building and parser enhancing. At the same time, it performs well in phrase phrase recognition. mation is useful to syntactic analysis of the non-lexical. other is to optimize the method of parsing to resolve data sparseness problems. Foundation of China under Grants No. 61271304 and Beijing Natural Science Foun-dation of Class B Key Project under Grants No. KZ201311232037.

